file_path,api_count,code
deepmosaic.py,0,"b""import os\nimport sys\nimport traceback\nfrom cores import Options,core\nfrom util import util\nfrom models import loadmodel\n\nopt = Options().getparse()\nutil.file_init(opt)\n\ndef main():\n    \n    if os.path.isdir(opt.media_path):\n        files = util.Traversal(opt.media_path)\n    else:\n        files = [opt.media_path]        \n    if opt.mode == 'add':\n        netS = loadmodel.bisenet(opt,'roi')\n        for file in files:\n            opt.media_path = file\n            if util.is_img(file):\n                core.addmosaic_img(opt,netS)\n            elif util.is_video(file):\n                core.addmosaic_video(opt,netS)\n            else:\n                print('This type of file is not supported')\n\n    elif opt.mode == 'clean':\n        netM = loadmodel.bisenet(opt,'mosaic')\n        if opt.traditional:\n            netG = None\n        elif opt.netG == 'video':\n            netG = loadmodel.video(opt)\n        else:\n            netG = loadmodel.pix2pix(opt)\n        \n        for file in files:\n            opt.media_path = file\n            if util.is_img(file):\n                core.cleanmosaic_img(opt,netG,netM)\n            elif util.is_video(file):\n                if opt.netG == 'video' and not opt.traditional:            \n                    core.cleanmosaic_video_fusion(opt,netG,netM)\n                else:\n                    core.cleanmosaic_video_byframe(opt,netG,netM)\n            else:\n                print('This type of file is not supported')\n\n    elif opt.mode == 'style':\n        netG = loadmodel.style(opt)\n        for file in files:\n            opt.media_path = file\n            if util.is_img(file):\n                core.styletransfer_img(opt,netG)\n            elif util.is_video(file):\n                core.styletransfer_video(opt,netG)\n\n            else:\n                print('This type of file is not supported')\n\n    util.clean_tempfiles(tmp_init = False)\n        \n# main()\nif __name__ == '__main__':\n    try:\n        main()\n    except Exception as ex:\n        print('--------------------ERROR--------------------')\n        ex_type, ex_val, ex_stack = sys.exc_info()\n        print('Error Type:',ex_type)\n        print(ex_val)\n        for stack in traceback.extract_tb(ex_stack):\n            print(stack)\n        input('Please press any key to exit.\\n')\n        util.clean_tempfiles(tmp_init = False)\n        exit(0)\n\n"""
cores/__init__.py,0,b'from .options import *'
cores/core.py,0,"b""import os\nimport numpy as np\nimport cv2\n\nfrom models import runmodel,loadmodel\nfrom util import mosaic,util,ffmpeg,filt,data\nfrom util import image_processing as impro\n\n'''\n---------------------Video Init---------------------\n'''\ndef video_init(opt,path):\n    util.clean_tempfiles()\n    fps,endtime,height,width = ffmpeg.get_video_infos(path)\n    if opt.fps !=0:\n        fps = opt.fps\n    ffmpeg.video2voice(path,'./tmp/voice_tmp.mp3')\n    ffmpeg.video2image(path,'./tmp/video2image/output_%05d.'+opt.tempimage_type,fps)\n    imagepaths=os.listdir('./tmp/video2image')\n    imagepaths.sort()\n    return fps,imagepaths,height,width\n\n'''\n---------------------Add Mosaic---------------------\n'''\ndef addmosaic_img(opt,netS):\n    path = opt.media_path\n    print('Add Mosaic:',path)\n    img = impro.imread(path)\n    mask = runmodel.get_ROI_position(img,netS,opt)[0]\n    img = mosaic.addmosaic(img,mask,opt)\n    impro.imwrite(os.path.join(opt.result_dir,os.path.splitext(os.path.basename(path))[0]+'_add.jpg'),img)\n\ndef addmosaic_video(opt,netS):\n    path = opt.media_path\n    fps,imagepaths = video_init(opt,path)[:2]\n    # get position\n    positions = []\n    for i,imagepath in enumerate(imagepaths,1):\n        img = impro.imread(os.path.join('./tmp/video2image',imagepath))\n        mask,x,y,size,area = runmodel.get_ROI_position(img,netS,opt)\n        positions.append([x,y,area])      \n        cv2.imwrite(os.path.join('./tmp/ROI_mask',imagepath),mask)\n        print('\\r','Find ROI location:'+str(i)+'/'+str(len(imagepaths)),util.get_bar(100*i/len(imagepaths),num=35),end='')\n    print('\\nOptimize ROI locations...')\n    mask_index = filt.position_medfilt(np.array(positions), 7)\n\n    # add mosaic\n    for i in range(len(imagepaths)):\n        mask = impro.imread(os.path.join('./tmp/ROI_mask',imagepaths[mask_index[i]]),'gray')\n        img = impro.imread(os.path.join('./tmp/video2image',imagepaths[i]))\n        if impro.mask_area(mask)>100:    \n            img = mosaic.addmosaic(img, mask, opt)\n        cv2.imwrite(os.path.join('./tmp/addmosaic_image',imagepaths[i]),img)\n        print('\\r','Add Mosaic:'+str(i+1)+'/'+str(len(imagepaths)),util.get_bar(100*i/len(imagepaths),num=35),end='')\n    print()\n    ffmpeg.image2video( fps,\n                        './tmp/addmosaic_image/output_%05d.'+opt.tempimage_type,\n                        './tmp/voice_tmp.mp3',\n                         os.path.join(opt.result_dir,os.path.splitext(os.path.basename(path))[0]+'_add.mp4'))\n\n'''\n---------------------Style Transfer---------------------\n'''\ndef styletransfer_img(opt,netG):\n    print('Style Transfer_img:',opt.media_path)\n    img = impro.imread(opt.media_path)\n    img = runmodel.run_styletransfer(opt, netG, img)\n    suffix = os.path.basename(opt.model_path).replace('.pth','').replace('style_','')\n    impro.imwrite(os.path.join(opt.result_dir,os.path.splitext(os.path.basename(opt.media_path))[0]+'_'+suffix+'.jpg'),img)\n\ndef styletransfer_video(opt,netG):\n    path = opt.media_path\n    positions = []\n    fps,imagepaths = video_init(opt,path)[:2]\n\n    for i,imagepath in enumerate(imagepaths,1):\n        img = impro.imread(os.path.join('./tmp/video2image',imagepath))\n        img = runmodel.run_styletransfer(opt, netG, img)\n        cv2.imwrite(os.path.join('./tmp/style_transfer',imagepath),img)\n        print('\\r','Transfer:'+str(i)+'/'+str(len(imagepaths)),util.get_bar(100*i/len(imagepaths),num=35),end='')\n    print()\n    suffix = os.path.basename(opt.model_path).replace('.pth','').replace('style_','')\n    ffmpeg.image2video( fps,\n                './tmp/style_transfer/output_%05d.'+opt.tempimage_type,\n                './tmp/voice_tmp.mp3',\n                 os.path.join(opt.result_dir,os.path.splitext(os.path.basename(path))[0]+'_'+suffix+'.mp4'))  \n\n'''\n---------------------Clean Mosaic---------------------\n'''\ndef get_mosaic_positions(opt,netM,imagepaths,savemask=True):\n    # get mosaic position\n    positions = []\n    for i,imagepath in enumerate(imagepaths,1):\n        img_origin = impro.imread(os.path.join('./tmp/video2image',imagepath))\n        x,y,size,mask = runmodel.get_mosaic_position(img_origin,netM,opt)\n        if savemask:\n            cv2.imwrite(os.path.join('./tmp/mosaic_mask',imagepath), mask)\n        positions.append([x,y,size])\n        print('\\r','Find mosaic location:'+str(i)+'/'+str(len(imagepaths)),util.get_bar(100*i/len(imagepaths),num=35),end='')\n    print('\\nOptimize mosaic locations...')\n    positions =np.array(positions)\n    for i in range(3):positions[:,i] = filt.medfilt(positions[:,i],opt.medfilt_num)\n    return positions\n\ndef cleanmosaic_img(opt,netG,netM):\n\n    path = opt.media_path\n    print('Clean Mosaic:',path)\n    img_origin = impro.imread(path)\n    x,y,size,mask = runmodel.get_mosaic_position(img_origin,netM,opt)\n    cv2.imwrite('./mask/'+os.path.basename(path), mask)\n    img_result = img_origin.copy()\n    if size != 0 :\n        img_mosaic = img_origin[y-size:y+size,x-size:x+size]\n        if opt.traditional:\n            img_fake = runmodel.traditional_cleaner(img_mosaic,opt)\n        else:\n            img_fake = runmodel.run_pix2pix(img_mosaic,netG,opt)\n        img_result = impro.replace_mosaic(img_origin,img_fake,mask,x,y,size,opt.no_feather)\n    else:\n        print('Do not find mosaic')\n    impro.imwrite(os.path.join(opt.result_dir,os.path.splitext(os.path.basename(path))[0]+'_clean.jpg'),img_result)\n\ndef cleanmosaic_video_byframe(opt,netG,netM):\n    path = opt.media_path\n    fps,imagepaths = video_init(opt,path)[:2]\n    positions = get_mosaic_positions(opt,netM,imagepaths,savemask=True)\n    # clean mosaic\n    for i,imagepath in enumerate(imagepaths,0):\n        x,y,size = positions[i][0],positions[i][1],positions[i][2]\n        img_origin = impro.imread(os.path.join('./tmp/video2image',imagepath))\n        img_result = img_origin.copy()\n        if size != 0:\n            img_mosaic = img_origin[y-size:y+size,x-size:x+size]\n            if opt.traditional:\n                img_fake = runmodel.traditional_cleaner(img_mosaic,opt)\n            else:\n                img_fake = runmodel.run_pix2pix(img_mosaic,netG,opt)\n        mask = cv2.imread(os.path.join('./tmp/mosaic_mask',imagepath),0)\n        img_result = impro.replace_mosaic(img_origin,img_fake,mask,x,y,size,opt.no_feather)\n        cv2.imwrite(os.path.join('./tmp/replace_mosaic',imagepath),img_result)\n        print('\\r','Clean Mosaic:'+str(i+1)+'/'+str(len(imagepaths)),util.get_bar(100*i/len(imagepaths),num=35),end='')\n    print()\n    ffmpeg.image2video( fps,\n                './tmp/replace_mosaic/output_%05d.'+opt.tempimage_type,\n                './tmp/voice_tmp.mp3',\n                 os.path.join(opt.result_dir,os.path.splitext(os.path.basename(path))[0]+'_clean.mp4'))  \n\ndef cleanmosaic_video_fusion(opt,netG,netM):\n    path = opt.media_path\n    N = 25\n    if 'HD' in os.path.basename(opt.model_path):\n        INPUT_SIZE = 256\n    else:\n        INPUT_SIZE = 128\n    fps,imagepaths,height,width = video_init(opt,path)\n    positions = get_mosaic_positions(opt,netM,imagepaths,savemask=True)\n    \n    # clean mosaic\n    img_pool = np.zeros((height,width,3*N), dtype='uint8')\n    for i,imagepath in enumerate(imagepaths,0):\n        x,y,size = positions[i][0],positions[i][1],positions[i][2]\n        \n        # image read stream\n        mask = cv2.imread(os.path.join('./tmp/mosaic_mask',imagepath),0)\n        if i==0 :\n            for j in range(0,N):\n                img_pool[:,:,j*3:(j+1)*3] = impro.imread(os.path.join('./tmp/video2image',imagepaths[np.clip(i+j-12,0,len(imagepaths)-1)]))\n        else:\n            img_pool[:,:,0:(N-1)*3] = img_pool[:,:,3:N*3]\n            img_pool[:,:,(N-1)*3:] = impro.imread(os.path.join('./tmp/video2image',imagepaths[np.clip(i+12,0,len(imagepaths)-1)]))\n        img_origin = img_pool[:,:,int((N-1)/2)*3:(int((N-1)/2)+1)*3]\n        \n        if size==0: # can not find mosaic,\n            cv2.imwrite(os.path.join('./tmp/replace_mosaic',imagepath),img_origin)\n        else:\n\n            mosaic_input = np.zeros((INPUT_SIZE,INPUT_SIZE,3*N+1), dtype='uint8')\n            mosaic_input[:,:,0:N*3] = impro.resize(img_pool[y-size:y+size,x-size:x+size,:], INPUT_SIZE)\n            mask_input = impro.resize(mask,np.min(img_origin.shape[:2]))[y-size:y+size,x-size:x+size]\n            mosaic_input[:,:,-1] = impro.resize(mask_input, INPUT_SIZE)\n\n            mosaic_input = data.im2tensor(mosaic_input,bgr2rgb=False,use_gpu=opt.use_gpu,use_transform = False,is0_1 = False)\n            unmosaic_pred = netG(mosaic_input)\n            img_fake = data.tensor2im(unmosaic_pred,rgb2bgr = False ,is0_1 = False)\n            img_result = impro.replace_mosaic(img_origin,img_fake,mask,x,y,size,opt.no_feather)\n            cv2.imwrite(os.path.join('./tmp/replace_mosaic',imagepath),img_result)\n        print('\\r','Clean Mosaic:'+str(i+1)+'/'+str(len(imagepaths)),util.get_bar(100*i/len(imagepaths),num=35),end='')\n    print()\n    ffmpeg.image2video( fps,\n                './tmp/replace_mosaic/output_%05d.'+opt.tempimage_type,\n                './tmp/voice_tmp.mp3',\n                 os.path.join(opt.result_dir,os.path.splitext(os.path.basename(path))[0]+'_clean.mp4'))        """
cores/options.py,1,"b""import argparse\nimport os\nimport torch\n\nclass Options():\n    def __init__(self):\n        self.parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n        self.initialized = False\n\n    def initialize(self):\n\n        #base\n        self.parser.add_argument('--use_gpu',type=int,default=0, help='if -1, do not use gpu')\n        # self.parser.add_argument('--use_gpu', action='store_true', help='if input it, use gpu')\n        self.parser.add_argument('--media_path', type=str, default='./imgs/ruoruo.jpg',help='your videos or images path')\n        self.parser.add_argument('--mode', type=str, default='auto',help='Program running mode. auto | add | clean | style')\n        self.parser.add_argument('--model_path', type=str, default='./pretrained_models/mosaic/add_face.pth',help='pretrained model path')\n        self.parser.add_argument('--result_dir', type=str, default='./result',help='output media will be saved here')\n        self.parser.add_argument('--tempimage_type', type=str, default='jpg',help='type of temp image, png | jpg, png is better but occupy more storage space')\n        self.parser.add_argument('--netG', type=str, default='auto',\n            help='select model to use for netG(Clean mosaic and Transfer style) -> auto | unet_128 | unet_256 | resnet_9blocks | HD | video')\n        self.parser.add_argument('--fps', type=int, default=0,help='read and output fps, if 0-> origin')\n        self.parser.add_argument('--output_size', type=int, default=0,help='size of output media, if 0 -> origin')\n        self.parser.add_argument('--mask_threshold', type=int, default=64,help='threshold of recognize clean or add mosaic position 0~255')\n\n        #AddMosaic\n        self.parser.add_argument('--mosaic_mod', type=str, default='squa_avg',help='type of mosaic -> squa_avg | squa_random | squa_avg_circle_edge | rect_avg | random')\n        self.parser.add_argument('--mosaic_size', type=int, default=0,help='mosaic size,if 0 auto size')\n        self.parser.add_argument('--mask_extend', type=int, default=10,help='extend mosaic area')\n        \n        #CleanMosaic     \n        self.parser.add_argument('--mosaic_position_model_path', type=str, default='auto',help='name of model use to find mosaic position')\n        self.parser.add_argument('--traditional', action='store_true', help='if specified, use traditional image processing methods to clean mosaic')\n        self.parser.add_argument('--tr_blur', type=int, default=10, help='ksize of blur when using traditional method, it will affect final quality')\n        self.parser.add_argument('--tr_down', type=int, default=10, help='downsample when using traditional method,it will affect final quality')\n        self.parser.add_argument('--no_feather', action='store_true', help='if specified, no edge feather and color correction, but run faster')\n        self.parser.add_argument('--all_mosaic_area', action='store_true', help='if specified, find all mosaic area, else only find the largest area')\n        self.parser.add_argument('--medfilt_num', type=int, default=11,help='medfilt window of mosaic movement in the video')\n        self.parser.add_argument('--ex_mult', type=str, default='auto',help='mosaic area expansion')\n        \n        #StyleTransfer\n        self.parser.add_argument('--preprocess', type=str, default='resize', help='resize and cropping of images at load time [ resize | resize_scale_width | edges | gray] or resize,edges(use comma to split)')\n        self.parser.add_argument('--edges', action='store_true', help='if specified, use edges to generate pictures,(input_nc = 1)')  \n        self.parser.add_argument('--canny', type=int, default=150,help='threshold of canny')\n        self.parser.add_argument('--only_edges', action='store_true', help='if specified, output media will be edges')\n\n        self.initialized = True\n\n\n    def getparse(self):\n        if not self.initialized:\n            self.initialize()\n        self.opt = self.parser.parse_args()\n\n        model_name = os.path.basename(self.opt.model_path)\n\n        if torch.cuda.is_available() and self.opt.use_gpu > -1:\n            self.opt.use_gpu = True\n        else:\n            self.opt.use_gpu = False\n\n        if self.opt.mode == 'auto':\n            if 'clean' in model_name or self.opt.traditional:\n                self.opt.mode = 'clean'\n            elif 'add' in model_name:\n                self.opt.mode = 'add'\n            elif 'style' in model_name or 'edges' in model_name:\n                self.opt.mode = 'style'\n            else:\n                print('Please input running model!')\n                input('Please press any key to exit.\\n')\n                exit(0)\n\n        if self.opt.output_size == 0 and self.opt.mode == 'style':\n            self.opt.output_size = 512\n\n        if 'edges' in model_name or 'edges' in self.opt.preprocess:\n            self.opt.edges = True\n\n        if self.opt.netG == 'auto' and self.opt.mode =='clean':\n            if 'unet_128' in model_name:\n                self.opt.netG = 'unet_128'\n            elif 'resnet_9blocks' in model_name:\n                self.opt.netG = 'resnet_9blocks'\n            elif 'HD' in model_name and 'video' not in model_name:\n                self.opt.netG = 'HD'\n            elif 'video' in model_name:\n                self.opt.netG = 'video'\n            else:\n                print('Type of Generator error!')\n                input('Please press any key to exit.\\n')\n                exit(0)\n\n        if self.opt.ex_mult == 'auto':\n            if 'face' in model_name:\n                self.opt.ex_mult = 1.1\n            else:\n                self.opt.ex_mult = 1.5\n        else:\n            self.opt.ex_mult = float(self.opt.ex_mult)\n\n        if self.opt.mosaic_position_model_path == 'auto':\n            _path = os.path.join(os.path.split(self.opt.model_path)[0],'mosaic_position.pth')\n            self.opt.mosaic_position_model_path = _path\n            # print(self.opt.mosaic_position_model_path)\n\n        return self.opt"""
make_datasets/cut_video.py,0,"b'import os\nimport numpy as np\nimport cv2\nimport random\nimport csv\n\nimport sys\nsys.path.append("".."")\nfrom util import util,ffmpeg\nfrom util import image_processing as impro\n\nfiles = util.Traversal(\'/media/hypo/Media/download\')\nvideos = util.is_videos(files)\n\n\n\nuseable_videos = []\nvideo_dict = {}\nreader = csv.reader(open(\'./csv/video_used_time.csv\'))\nfor line in reader:\n    useable_videos.append(line[0])\n    video_dict[line[0]]=line[1:]\n\nin_cnt = 0\nout_cnt = 1\nfor video in videos:\n    if os.path.basename(video) in useable_videos:\n\n        for i in range(len(video_dict[os.path.basename(video)])):\n            ffmpeg.cut_video(video, video_dict[os.path.basename(video)][i], \'00:00:05\', \'./video/\'+\'%04d\'%out_cnt+\'.mp4\')\n            out_cnt +=1\n        in_cnt += 1          \n'"
make_datasets/draw_mask.py,0,"b'import cv2\nimport numpy as np\nimport datetime\nimport os\nimport random\n\nimport sys\nsys.path.append("".."")\nfrom cores import Options\nfrom util import util\nfrom util import image_processing as impro\n\n\nopt = Options()\nopt.parser.add_argument(\'--datadir\',type=str,default=\' \', help=\'your images dir\')\nopt.parser.add_argument(\'--savedir\',type=str,default=\'../datasets/draw/face\', help=\'\')\nopt = opt.getparse()\n\nmask_savedir = os.path.join(opt.savedir,\'mask\')\nimg_savedir = os.path.join(opt.savedir,\'origin_image\')\nutil.makedirs(mask_savedir)\nutil.makedirs(img_savedir)\n\nfilepaths = util.Traversal(opt.datadir)\nfilepaths = util.is_imgs(filepaths)\nrandom.shuffle(filepaths)\nprint(\'find image:\',len(filepaths))\n\n# mouse callback function\ndrawing = False # true if mouse is pressed\nix,iy = -1,-1\nbrushsize = 20\ndef draw_circle(event,x,y,flags,param):\n    global ix,iy,drawing,brushsize\n\n    if event == cv2.EVENT_LBUTTONDOWN:\n        drawing = True\n        ix,iy = x,y\n\n    elif event == cv2.EVENT_MOUSEMOVE:\n        if drawing == True:\n            cv2.circle(img_drawn,(x,y),brushsize,(0,255,0),-1)\n\n    elif event == cv2.EVENT_LBUTTONUP:\n        drawing = False\n        cv2.circle(img_drawn,(x,y),brushsize,(0,255,0),-1)\n\ndef makemask(img_drawn):\n    # starttime = datetime.datetime.now()\n    mask = np.zeros(img_drawn.shape, np.uint8)\n    for row in range(img_drawn.shape[0]):\n        for col in range(img_drawn.shape[1]):\n            # if (img_drawn[row,col,:] == [0,255,0]).all(): #too slow\n            if img_drawn[row,col,0] == 0:\n                if img_drawn[row,col,1] == 255:\n                    if img_drawn[row,col,2] == 0:\n                        mask[row,col,:] = [255,255,255]\n    return mask\n\ncnt = 0\nfor file in filepaths:\n    try:\n        cnt += 1\n        img = impro.imread(file,loadsize=512)\n        img_drawn = img.copy()\n        cv2.namedWindow(\'image\')\n        cv2.setMouseCallback(\'image\',draw_circle) #MouseCallback\n        while(1):\n\n            cv2.imshow(\'image\',img_drawn)\n            k = cv2.waitKey(1) & 0xFF\n            if k == ord(\'s\'):\n                \n                img_drawn = impro.resize(img_drawn,256)\n                mask = makemask(img_drawn)\n                cv2.imwrite(os.path.join(mask_savedir,os.path.splitext(os.path.basename(file))[0]+\'.png\'),mask)\n                cv2.imwrite(os.path.join(img_savedir,os.path.basename(file)),img)   \n                print(\'Saved:\',os.path.join(mask_savedir,os.path.splitext(os.path.basename(file))[0]+\'.png\'),mask)\n                # cv2.destroyAllWindows()\n                print(\'remain:\',len(filepaths)-cnt)\n                brushsize = 20\n                break\n            elif k == ord(\'a\'):\n                brushsize -= 5\n                if brushsize<5:\n                    brushsize = 5\n                print(\'brushsize:\',brushsize)\n            elif k == ord(\'d\'):\n                brushsize += 5\n                print(\'brushsize:\',brushsize)\n            elif k == ord(\'w\'):\n                print(\'remain:\',len(filepaths)-cnt)\n                break\n    except Exception as e:\n        print(file,e)\n\n'"
make_datasets/get_edges_pix2pix_dataset.py,0,"b'import numpy as np\nimport cv2\nimport os\nimport sys\nsys.path.append("".."")\nfrom util import image_processing as impro\nfrom util import util\n\nimg_dir = \'./datasets_img/pix2pix/edges2cat/images\'\noutput_dir = \'./datasets_img/pix2pix/edges2cat/train\'\nutil.makedirs(output_dir)\n\nimg_names = os.listdir(img_dir)\nfor i,img_name in enumerate(img_names,2000): \n    try:\n        img = impro.imread(os.path.join(img_dir,img_name))\n        img = impro.resize(img, 286)\n        h,w = img.shape[:2]\n        edges = cv2.Canny(img,150,250)\n        edges = impro.ch_one2three(edges)\n        out_img = np.zeros((h,w*2,3), dtype=np.uint8)\n        out_img[:,0:w] = edges\n        out_img[:,w:2*w] = img\n        cv2.imwrite(os.path.join(output_dir,\'%05d\' % i+\'.jpg\'), out_img)\n    except Exception as e:\n        pass\n'"
make_datasets/get_image_from_video.py,0,"b'import os\nimport sys\nsys.path.append("".."")\nfrom cores import Options\nfrom util import util,ffmpeg\n\nopt = Options()\nopt.parser.add_argument(\'--datadir\',type=str,default=\'\', help=\'your video dir\')\nopt.parser.add_argument(\'--savedir\',type=str,default=\'../datasets/video2image\', help=\'\')\nopt = opt.getparse()\n\nfiles = util.Traversal(opt.datadir)\nvideos = util.is_videos(files)\n\nutil.makedirs(opt.savedir)\nfor video in videos:\n    ffmpeg.continuous_screenshot(video, opt.savedir, opt.fps)'"
make_datasets/make_pix2pix_dataset.py,0,"b'import os\nimport random\nimport sys\nimport datetime\nimport time\nimport shutil\nimport threading\nimport warnings\nwarnings.filterwarnings(action=\'ignore\')\n\nimport numpy as np\nimport cv2\n\nsys.path.append("".."")\nfrom models import runmodel,loadmodel\nimport util.image_processing as impro\nfrom util import util,mosaic,data\nfrom cores import Options\n\n\nopt = Options()\nopt.parser.add_argument(\'--datadir\',type=str,default=\'../datasets/draw/face\', help=\'\')\nopt.parser.add_argument(\'--savedir\',type=str,default=\'../datasets/pix2pix/face\', help=\'\')\nopt.parser.add_argument(\'--name\',type=str,default=\'\', help=\'save name\')\nopt.parser.add_argument(\'--mod\',type=str,default=\'drawn\', help=\'drawn | network | irregular | drawn,irregular | network,irregular\')\nopt.parser.add_argument(\'--square\', action=\'store_true\', help=\'if specified, crop to square\')\nopt.parser.add_argument(\'--irrholedir\',type=str,default=\'../datasets/Irregular_Holes_mask\', help=\'\')  \nopt.parser.add_argument(\'--hd\', action=\'store_true\', help=\'if false make dataset for pix2pix, if Ture for pix2pix_HD\')\nopt.parser.add_argument(\'--savemask\', action=\'store_true\', help=\'if specified,save mask\')\nopt.parser.add_argument(\'--outsize\', type=int ,default= 512,help=\'\')\nopt.parser.add_argument(\'--fold\', type=int ,default= 1,help=\'\')\nopt.parser.add_argument(\'--start\', type=int ,default= 0,help=\'\')\nopt.parser.add_argument(\'--minsize\', type=int ,default= 128,help=\'when [square], minimal roi size\')\nopt.parser.add_argument(\'--quality\', type=int ,default= 40,help=\'when [square], minimal quality\')\n\nopt = opt.getparse()\n\nutil.makedirs(opt.savedir)\nutil.writelog(os.path.join(opt.savedir,\'opt.txt\'), \n              str(time.asctime(time.localtime(time.time())))+\'\\n\'+util.opt2str(opt))\nopt.mod = (opt.mod).split(\',\')\n\n#save dir\nif opt.hd:\n    train_A_path = os.path.join(opt.savedir,\'train_A\')\n    train_B_path = os.path.join(opt.savedir,\'train_B\')\n    util.makedirs(train_A_path)\n    util.makedirs(train_B_path)\nelse:\n    train_path = os.path.join(opt.savedir,\'train\')\n    util.makedirs(train_path)\nif opt.savemask:\n    mask_save_path = os.path.join(opt.savedir,\'mask\')\n    util.makedirs(mask_save_path)\n\n#read dir\nif \'drawn\' in opt.mod:\n    imgpaths = util.Traversal(os.path.join(opt.datadir,\'origin_image\'))\n    imgpaths.sort()\n    maskpaths = util.Traversal(os.path.join(opt.datadir,\'mask\'))\n    maskpaths.sort()\nif \'network\' in opt.mod or \'irregular\' in opt.mod:\n    imgpaths = util.Traversal(opt.datadir)\n    random.shuffle (imgpaths)\nif \'irregular\' in opt.mod:\n    irrpaths = util.Traversal(opt.irrholedir)\n\n\n#def network\nif \'network\' in opt.mod:\n    net = loadmodel.bisenet(opt,\'roi\')\n\n\n# def checksaveimage(opt,img,mask):\n    \n#     #check\n#     saveflag = True\n#     x,y,size,area = impro.boundingSquare(mask, random.uniform(1.4,1.6))\n#     if area < 1000:\n#         saveflag = False\n#     else:\n#         if opt.square:\n#             if size < opt.minsize:\n#                 saveflag = False\n#             else:\n#                 img = impro.resize(img[y-size:y+size,x-size:x+size],opt.outsize,interpolation=cv2.INTER_CUBIC)\n#                 mask =  impro.resize(mask[y-size:y+size,x-size:x+size],opt.outsize,interpolation=cv2.INTER_CUBIC)\n#                 if impro.Q_lapulase(img)<opt.quality:\n#                     saveflag = False         \n#         else:\n#             img = impro.resize(img,opt.outsize,interpolation=cv2.INTER_CUBIC)\n    \n#     if saveflag:\n#         # add mosaic\n#         img_mosaic = mosaic.addmosaic_random(img, mask)\n#         global savecnt\n#         savecnt += 1\n\n#         if opt.hd:\n#             cv2.imwrite(os.path.join(train_A_path,opt.name+\'%06d\' % savecnt+\'.jpg\'), img_mosaic)\n#             cv2.imwrite(os.path.join(train_B_path,opt.name+\'%06d\' % savecnt+\'.jpg\'), img)\n#         else:\n#             merge_img = impro.makedataset(img_mosaic, img)\n#             cv2.imwrite(os.path.join(train_path,opt.name+\'%06d\' % savecnt+\'.jpg\'), merge_img)\n#         if opt.savemask:\n#             cv2.imwrite(os.path.join(mask_save_path,opt.name+\'%06d\' % savecnt+\'.png\'), mask)\n\n\nprint(\'Find images:\',len(imgpaths))\nstarttime = datetime.datetime.now()\nfilecnt = 0\nsavecnt = opt.start\nfor fold in range(opt.fold):\n    for i in range(len(imgpaths)):\n        filecnt += 1\n        try:\n            # load image and get mask\n            img = impro.imread(imgpaths[i])\n            if \'drawn\' in opt.mod:\n                mask_drawn = impro.imread(maskpaths[i],\'gray\')\n                mask_drawn = impro.resize_like(mask_drawn, img)\n                mask = mask_drawn\n            if \'irregular\' in opt.mod:\n                mask_irr = impro.imread(irrpaths[random.randint(0,12000-1)],\'gray\')\n                mask_irr = data.random_transform_single(mask_irr, (img.shape[0],img.shape[1]))\n                mask = mask_irr\n            if \'network\' in opt.mod:\n                mask_net = runmodel.get_ROI_position(img,net,opt,keepsize=True)[0]\n                if not opt.all_mosaic_area:\n                    mask_net = impro.find_mostlikely_ROI(mask_net)\n                mask = mask_net\n            if opt.mod == [\'drawn\',\'irregular\']:\n                mask = cv2.bitwise_and(mask_irr, mask_drawn)\n            if opt.mod == [\'network\',\'irregular\']:\n                mask = cv2.bitwise_and(mask_irr, mask_net)\n\n                #checkandsave\n                # t=threading.Thread(target=checksaveimage,args=(opt,img,mask,))\n                # t.start()\n\n                saveflag = True\n                x,y,size,area = impro.boundingSquare(mask, random.uniform(1.4,1.6))\n                if area < 1000:\n                    saveflag = False\n                else:\n                    if opt.square:\n                        if size < opt.minsize:\n                            saveflag = False\n                        else:\n                            img = impro.resize(img[y-size:y+size,x-size:x+size],opt.outsize,interpolation=cv2.INTER_CUBIC)\n                            mask =  impro.resize(mask[y-size:y+size,x-size:x+size],opt.outsize,interpolation=cv2.INTER_CUBIC)\n                            if impro.Q_lapulase(img)<opt.quality:\n                                saveflag = False         \n                    else:\n                        img = impro.resize(img,opt.outsize,interpolation=cv2.INTER_CUBIC)\n                \n                if saveflag:\n                    # add mosaic\n                    img_mosaic = mosaic.addmosaic_random(img, mask)\n                    # global savecnt\n                    savecnt += 1\n\n                    if opt.hd:\n                        cv2.imwrite(os.path.join(train_A_path,opt.name+\'%06d\' % savecnt+\'.jpg\'), img_mosaic)\n                        cv2.imwrite(os.path.join(train_B_path,opt.name+\'%06d\' % savecnt+\'.jpg\'), img)\n                    else:\n                        merge_img = impro.makedataset(img_mosaic, img)\n                        cv2.imwrite(os.path.join(train_path,opt.name+\'%06d\' % savecnt+\'.jpg\'), merge_img)\n                    if opt.savemask:\n                        cv2.imwrite(os.path.join(mask_save_path,opt.name+\'%06d\' % savecnt+\'.png\'), mask)\n                \n                # print(""Processing:"",imgpaths[i],"" "",""Remain:"",len(imgpaths)*opt.fold-filecnt)\n                # cv2.namedWindow(\'image\', cv2.WINDOW_NORMAL)\n                # cv2.imshow(\'image\',img_mosaic)\n                # cv2.waitKey(0)\n                # cv2.destroyAllWindows()   \n        except Exception as e:\n            print(imgpaths[i],e)\n        if filecnt%10==0:\n            endtime = datetime.datetime.now()\n            # used_time = (endtime-starttime).seconds\n            used_time = (endtime-starttime).seconds\n            all_length = len(imgpaths)*opt.fold \n            percent = round(100*filecnt/all_length,1)\n            all_time = used_time/filecnt*all_length\n\n            print(\'\\r\',\'\',str(filecnt)+\'/\'+str(all_length)+\' \',\n                util.get_bar(percent,30),\'\',\n                util.second2stamp(used_time)+\'/\'+util.second2stamp(all_time),\n                \'f:\'+str(savecnt),end= "" "")'"
make_datasets/make_video_dataset.py,0,"b'import os\nimport random\nimport sys\nimport datetime\nimport time\nimport shutil\nimport threading\n\nimport numpy as np\nimport cv2\n\nsys.path.append("".."")\nfrom models import runmodel,loadmodel\nimport util.image_processing as impro\nfrom util import util,mosaic,data,ffmpeg\nfrom cores import Options\n\nopt = Options()\nopt.parser.add_argument(\'--datadir\',type=str,default=\'your video dir\', help=\'\')\nopt.parser.add_argument(\'--savedir\',type=str,default=\'../datasets/video/face\', help=\'\')\nopt.parser.add_argument(\'--interval\',type=int,default=30, help=\'interval of split video \')\nopt.parser.add_argument(\'--time\',type=int,default=5, help=\'split video time\')\nopt.parser.add_argument(\'--minmaskarea\',type=int,default=2000, help=\'\')\nopt.parser.add_argument(\'--quality\', type=int ,default= 45,help=\'minimal quality\')\nopt.parser.add_argument(\'--outsize\', type=int ,default= 286,help=\'\')\nopt.parser.add_argument(\'--startcnt\', type=int ,default= 0,help=\'\')\nopt.parser.add_argument(\'--minsize\', type=int ,default= 96,help=\'minimal roi size\')\nopt = opt.getparse()\n\n\nutil.makedirs(opt.savedir)\nutil.writelog(os.path.join(opt.savedir,\'opt.txt\'), \n              str(time.asctime(time.localtime(time.time())))+\'\\n\'+util.opt2str(opt))\n\nvideopaths = util.Traversal(opt.datadir)\nvideopaths = util.is_videos(videopaths)\nrandom.shuffle(videopaths)\n\n# def network\nnet = loadmodel.bisenet(opt,\'roi\')\n\nresult_cnt = opt.startcnt\nvideo_cnt = 1\nstarttime = datetime.datetime.now()\nfor videopath in videopaths:\n    try:\n        timestamps=[]\n        fps,endtime,height,width = ffmpeg.get_video_infos(videopath)\n        for cut_point in range(1,int((endtime-opt.time)/opt.interval)):\n            util.clean_tempfiles()\n            ffmpeg.video2image(videopath, \'./tmp/video2image/%05d.\'+opt.tempimage_type,fps=1,\n                start_time = util.second2stamp(cut_point*opt.interval),last_time = util.second2stamp(opt.time))\n            imagepaths = util.Traversal(\'./tmp/video2image\')\n            cnt = 0 \n            for i in range(opt.time):\n                img = impro.imread(imagepaths[i])\n                mask = runmodel.get_ROI_position(img,net,opt,keepsize=True)[0]\n                if not opt.all_mosaic_area:\n                    mask = impro.find_mostlikely_ROI(mask)\n                x,y,size,area = impro.boundingSquare(mask,Ex_mul=1)\n                if area > opt.minmaskarea and size>opt.minsize and impro.Q_lapulase(img)>opt.quality:\n                    cnt +=1\n            if cnt == opt.time:\n                # print(second)\n                timestamps.append(util.second2stamp(cut_point*opt.interval))\n        util.writelog(os.path.join(opt.savedir,\'opt.txt\'),videopath+\'\\n\'+str(timestamps))\n        #print(timestamps)\n\n        # util.clean_tempfiles()\n        # fps,endtime,height,width = ffmpeg.get_video_infos(videopath)\n        # # print(fps,endtime,height,width)\n        # ffmpeg.continuous_screenshot(videopath, \'./tmp/video2image\', 1)\n\n        # # find where to cut\n        # print(\'Find where to cut...\')\n        # timestamps=[]\n        # imagepaths = util.Traversal(\'./tmp/video2image\')\n        # for second in range(int(endtime)):\n        #     if second%opt.interval==0:\n        #         cnt = 0 \n        #         for i in range(opt.time):\n        #             img = impro.imread(imagepaths[second+i])\n        #             mask = runmodel.get_ROI_position(img,net,opt)[0]\n        #             if not opt.all_mosaic_area:\n        #                 mask = impro.find_mostlikely_ROI(mask)\n        #             if impro.mask_area(mask) > opt.minmaskarea and impro.Q_lapulase(img)>opt.quality:\n        #                 # print(impro.mask_area(mask))\n        #                 cnt +=1\n        #         if cnt == opt.time:\n        #             # print(second)\n        #             timestamps.append(util.second2stamp(second))\n\n        #generate datasets\n        print(\'Generate datasets...\')\n        for timestamp in timestamps:\n            savecnt = \'%05d\' % result_cnt\n            origindir = os.path.join(opt.savedir,savecnt,\'origin_image\')\n            maskdir = os.path.join(opt.savedir,savecnt,\'mask\')\n            util.makedirs(origindir)\n            util.makedirs(maskdir)\n\n            util.clean_tempfiles()\n            ffmpeg.video2image(videopath, \'./tmp/video2image/%05d.\'+opt.tempimage_type,\n                start_time = timestamp,last_time = util.second2stamp(opt.time))\n            \n            endtime = datetime.datetime.now()\n            print(str(video_cnt)+\'/\'+str(len(videopaths))+\' \',\n                util.get_bar(100*video_cnt/len(videopaths),35),\'\',\n                util.second2stamp((endtime-starttime).seconds)+\'/\'+util.second2stamp((endtime-starttime).seconds/video_cnt*len(videopaths)))\n\n            imagepaths = util.Traversal(\'./tmp/video2image\')\n            imagepaths = sorted(imagepaths)\n            imgs=[];masks=[]\n            mask_flag = False\n\n            for imagepath in imagepaths:\n                img = impro.imread(imagepath)\n                mask = runmodel.get_ROI_position(img,net,opt,keepsize=True)[0]\n                imgs.append(img)\n                masks.append(mask)\n                if not mask_flag:\n                    mask_avg = mask.astype(np.float64)\n                    mask_flag = True\n                else:\n                    mask_avg += mask.astype(np.float64)\n\n            mask_avg = np.clip(mask_avg/len(imagepaths),0,255).astype(\'uint8\')\n            mask_avg = impro.mask_threshold(mask_avg,20,64)\n            if not opt.all_mosaic_area:\n                mask_avg = impro.find_mostlikely_ROI(mask_avg)\n            x,y,size,area = impro.boundingSquare(mask_avg,Ex_mul=random.uniform(1.1,1.5))\n            \n            for i in range(len(imagepaths)):\n                img = impro.resize(imgs[i][y-size:y+size,x-size:x+size],opt.outsize,interpolation=cv2.INTER_CUBIC) \n                mask = impro.resize(masks[i][y-size:y+size,x-size:x+size],opt.outsize,interpolation=cv2.INTER_CUBIC)\n                impro.imwrite(os.path.join(origindir,\'%05d\'%(i+1)+\'.jpg\'), img)\n                impro.imwrite(os.path.join(maskdir,\'%05d\'%(i+1)+\'.png\'), mask)\n\n            result_cnt+=1\n\n    except Exception as e:\n        video_cnt +=1\n        util.writelog(os.path.join(opt.savedir,\'opt.txt\'), \n              videopath+\'\\n\'+str(result_cnt)+\'\\n\'+str(e))\n    video_cnt +=1\n'"
models/BiSeNet_model.py,25,"b'# This code clone from https://github.com/ooooverflow/BiSeNet\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom . import components\nimport warnings\nwarnings.filterwarnings(action=\'ignore\')\n\ndef flatten(tensor):\n    """"""Flattens a given tensor such that the channel axis is first.\n    The shapes are transformed as follows:\n       (N, C, D, H, W) -> (C, N * D * H * W)\n    """"""\n    C = tensor.size(1)\n    # new axis order\n    axis_order = (1, 0) + tuple(range(2, tensor.dim()))\n    # Transpose: (N, C, D, H, W) -> (C, N, D, H, W)\n    transposed = tensor.permute(axis_order)\n    # Flatten: (C, N, D, H, W) -> (C, N * D * H * W)\n    return transposed.contiguous().view(C, -1)\n\n\nclass DiceLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.epsilon = 1e-5\n\n    def forward(self, output, target):\n        assert output.size() == target.size(), ""\'input\' and \'target\' must have the same shape""\n        output = F.softmax(output, dim=1)\n        output = flatten(output)\n        target = flatten(target)\n        # intersect = (output * target).sum(-1).sum() + self.epsilon\n        # denominator = ((output + target).sum(-1)).sum() + self.epsilon\n\n        intersect = (output * target).sum(-1)\n        denominator = (output + target).sum(-1)\n        dice = intersect / denominator\n        dice = torch.mean(dice)\n        return 1 - dice\n        # return 1 - 2. * intersect / denominator\n\nclass resnet18(torch.nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = components.resnet18(pretrained=pretrained)\n        self.conv1 = self.features.conv1\n        self.bn1 = self.features.bn1\n        self.relu = self.features.relu\n        self.maxpool1 = self.features.maxpool\n        self.layer1 = self.features.layer1\n        self.layer2 = self.features.layer2\n        self.layer3 = self.features.layer3\n        self.layer4 = self.features.layer4\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.relu(self.bn1(x))\n        x = self.maxpool1(x)\n        feature1 = self.layer1(x)  # 1 / 4\n        feature2 = self.layer2(feature1)  # 1 / 8\n        feature3 = self.layer3(feature2)  # 1 / 16\n        feature4 = self.layer4(feature3)  # 1 / 32\n        # global average pooling to build tail\n        tail = torch.mean(feature4, 3, keepdim=True)\n        tail = torch.mean(tail, 2, keepdim=True)\n        return feature3, feature4, tail\n\n\nclass resnet101(torch.nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.features = components.resnet101(pretrained=pretrained)\n        self.conv1 = self.features.conv1\n        self.bn1 = self.features.bn1\n        self.relu = self.features.relu\n        self.maxpool1 = self.features.maxpool\n        self.layer1 = self.features.layer1\n        self.layer2 = self.features.layer2\n        self.layer3 = self.features.layer3\n        self.layer4 = self.features.layer4\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.relu(self.bn1(x))\n        x = self.maxpool1(x)\n        feature1 = self.layer1(x)  # 1 / 4\n        feature2 = self.layer2(feature1)  # 1 / 8\n        feature3 = self.layer3(feature2)  # 1 / 16\n        feature4 = self.layer4(feature3)  # 1 / 32\n        # global average pooling to build tail\n        tail = torch.mean(feature4, 3, keepdim=True)\n        tail = torch.mean(tail, 2, keepdim=True)\n        return feature3, feature4, tail\n\ndef build_contextpath(name,pretrained):\n    model = {\n        \'resnet18\': resnet18(pretrained=pretrained),\n        \'resnet101\': resnet101(pretrained=pretrained)\n    }\n    return model[name]\n\nclass ConvBlock(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2,padding=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU()\n\n    def forward(self, input):\n        x = self.conv1(input)\n        return self.relu(self.bn(x))\n\nclass Spatial_path(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.convblock1 = ConvBlock(in_channels=3, out_channels=64)\n        self.convblock2 = ConvBlock(in_channels=64, out_channels=128)\n        self.convblock3 = ConvBlock(in_channels=128, out_channels=256)\n\n    def forward(self, input):\n        x = self.convblock1(input)\n        x = self.convblock2(x)\n        x = self.convblock3(x)\n        return x\n\nclass AttentionRefinementModule(torch.nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.sigmoid = nn.Sigmoid()\n        self.in_channels = in_channels\n        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n\n    def forward(self, input):\n        # global average pooling\n        x = self.avgpool(input)\n        assert self.in_channels == x.size(1), \'in_channels and out_channels should all be {}\'.format(x.size(1))\n        x = self.conv(x)\n        # x = self.sigmoid(self.bn(x))\n        x = self.sigmoid(x)\n        # channels of input and x should be same\n        x = torch.mul(input, x)\n        return x\n\nclass FeatureFusionModule(torch.nn.Module):\n    def __init__(self, num_classes, in_channels):\n        super().__init__()\n        # self.in_channels = input_1.channels + input_2.channels\n        # resnet101 3328 = 256(from context path) + 1024(from spatial path) + 2048(from spatial path)\n        # resnet18  1024 = 256(from context path) + 256(from spatial path) + 512(from spatial path)\n        self.in_channels = in_channels\n\n        self.convblock = ConvBlock(in_channels=self.in_channels, out_channels=num_classes, stride=1)\n        self.conv1 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n\n\n    def forward(self, input_1, input_2):\n        x = torch.cat((input_1, input_2), dim=1)\n        assert self.in_channels == x.size(1), \'in_channels of ConvBlock should be {}\'.format(x.size(1))\n        feature = self.convblock(x)\n        x = self.avgpool(feature)\n\n        x = self.relu(self.conv1(x))\n        x = self.sigmoid(self.conv2(x))\n        x = torch.mul(feature, x)\n        x = torch.add(x, feature)\n        return x\n\nclass BiSeNet(torch.nn.Module):\n    def __init__(self, num_classes, context_path, train_flag=True):\n        super().__init__()\n        # build spatial path\n        self.saptial_path = Spatial_path()\n        self.sigmoid = nn.Sigmoid()\n        # build context path\n        if train_flag:\n            self.context_path = build_contextpath(name=context_path,pretrained=True)\n        else:\n            self.context_path = build_contextpath(name=context_path,pretrained=False)\n\n        # build attention refinement module  for resnet 101\n        if context_path == \'resnet101\':\n            self.attention_refinement_module1 = AttentionRefinementModule(1024, 1024)\n            self.attention_refinement_module2 = AttentionRefinementModule(2048, 2048)\n            # supervision block\n            self.supervision1 = nn.Conv2d(in_channels=1024, out_channels=num_classes, kernel_size=1)\n            self.supervision2 = nn.Conv2d(in_channels=2048, out_channels=num_classes, kernel_size=1)\n            # build feature fusion module\n            self.feature_fusion_module = FeatureFusionModule(num_classes, 3328)\n\n        elif context_path == \'resnet18\':\n            # build attention refinement module  for resnet 18\n            self.attention_refinement_module1 = AttentionRefinementModule(256, 256)\n            self.attention_refinement_module2 = AttentionRefinementModule(512, 512)\n            # supervision block\n            self.supervision1 = nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=1)\n            self.supervision2 = nn.Conv2d(in_channels=512, out_channels=num_classes, kernel_size=1)\n            # build feature fusion module\n            self.feature_fusion_module = FeatureFusionModule(num_classes, 1024)\n        else:\n            print(\'Error: unspport context_path network \\n\')\n\n        # build final convolution\n        self.conv = nn.Conv2d(in_channels=num_classes, out_channels=num_classes, kernel_size=1)\n\n        self.init_weight()\n\n        self.mul_lr = []\n        self.mul_lr.append(self.saptial_path)\n        self.mul_lr.append(self.attention_refinement_module1)\n        self.mul_lr.append(self.attention_refinement_module2)\n        self.mul_lr.append(self.supervision1)\n        self.mul_lr.append(self.supervision2)\n        self.mul_lr.append(self.feature_fusion_module)\n        self.mul_lr.append(self.conv)\n\n    def init_weight(self):\n        for name, m in self.named_modules():\n            if \'context_path\' not in name:\n                if isinstance(m, nn.Conv2d):\n                    nn.init.kaiming_normal_(m.weight, mode=\'fan_in\', nonlinearity=\'relu\')\n                elif isinstance(m, nn.BatchNorm2d):\n                    m.eps = 1e-5\n                    m.momentum = 0.1\n                    nn.init.constant_(m.weight, 1)\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, input):\n        # output of spatial path\n        sx = self.saptial_path(input)\n\n        # output of context path\n        cx1, cx2, tail = self.context_path(input)\n        cx1 = self.attention_refinement_module1(cx1)\n        cx2 = self.attention_refinement_module2(cx2)\n        cx2 = torch.mul(cx2, tail)\n        # upsampling\n        cx1 = torch.nn.functional.interpolate(cx1, size=sx.size()[-2:], mode=\'bilinear\')\n        cx2 = torch.nn.functional.interpolate(cx2, size=sx.size()[-2:], mode=\'bilinear\')\n        cx = torch.cat((cx1, cx2), dim=1)\n\n        if self.training == True:\n            cx1_sup = self.supervision1(cx1)\n            cx2_sup = self.supervision2(cx2)\n            cx1_sup = torch.nn.functional.interpolate(cx1_sup, size=input.size()[-2:], mode=\'bilinear\')\n            cx2_sup = torch.nn.functional.interpolate(cx2_sup, size=input.size()[-2:], mode=\'bilinear\')\n\n        # output of feature fusion module\n        result = self.feature_fusion_module(sx, cx)\n\n        # upsampling\n        result = torch.nn.functional.interpolate(result, scale_factor=8, mode=\'bilinear\')\n        result = self.conv(result)\n\n        if self.training == True:\n            return self.sigmoid(result), self.sigmoid(cx1_sup), self.sigmoid(cx2_sup)\n\n        return self.sigmoid(result)'"
models/__init__.py,0,b'\n'
models/components.py,7,"b'import torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, planes)\n        self.bn1 = norm_layer(planes)\n        self.conv2 = conv3x3(planes, planes, stride)\n        self.bn2 = norm_layer(planes)\n        self.conv3 = conv1x1(planes, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, norm_layer=None):\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, norm_layer=norm_layer)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, norm_layer=norm_layer)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, norm_layer=None):\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model'"
models/loadmodel.py,8,"b'import torch\nfrom .pix2pix_model import define_G\nfrom .pix2pixHD_model import define_G as define_G_HD\nfrom .unet_model import UNet\nfrom .video_model import MosaicNet\nfrom .videoHD_model import MosaicNet as MosaicNet_HD\nfrom .BiSeNet_model import BiSeNet\n\ndef show_paramsnumber(net,netname=\'net\'):\n    parameters = sum(param.numel() for param in net.parameters())\n    parameters = round(parameters/1e6,2)\n    print(netname+\' parameters: \'+str(parameters)+\'M\')\n\ndef __patch_instance_norm_state_dict(state_dict, module, keys, i=0):\n    """"""Fix InstanceNorm checkpoints incompatibility (prior to 0.4)""""""\n    key = keys[i]\n    if i + 1 == len(keys):  # at the end, pointing to a parameter/buffer\n        if module.__class__.__name__.startswith(\'InstanceNorm\') and \\\n                (key == \'running_mean\' or key == \'running_var\'):\n            if getattr(module, key) is None:\n                state_dict.pop(\'.\'.join(keys))\n        if module.__class__.__name__.startswith(\'InstanceNorm\') and \\\n           (key == \'num_batches_tracked\'):\n            state_dict.pop(\'.\'.join(keys))\n    else:\n        __patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)\n\ndef pix2pix(opt):\n    # print(opt.model_path,opt.netG)\n    if opt.netG == \'HD\':\n        netG = define_G_HD(3, 3, 64, \'global\' ,4)\n    else:\n        netG = define_G(3, 3, 64, opt.netG, norm=\'batch\',use_dropout=True, init_type=\'normal\', gpu_ids=[])\n    show_paramsnumber(netG,\'netG\')\n    netG.load_state_dict(torch.load(opt.model_path))\n    netG.eval()\n    if opt.use_gpu:\n        netG.cuda()\n    return netG\n\n\ndef style(opt):\n    if opt.edges:\n        netG = define_G(1, 3, 64, \'resnet_9blocks\', norm=\'instance\',use_dropout=True, init_type=\'normal\', gpu_ids=[])\n    else:\n        netG = define_G(3, 3, 64, \'resnet_9blocks\', norm=\'instance\',use_dropout=False, init_type=\'normal\', gpu_ids=[])\n\n    #in other to load old pretrain model\n    #https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/models/base_model.py\n    if isinstance(netG, torch.nn.DataParallel):\n        netG = netG.module\n    # if you are using PyTorch newer than 0.4 (e.g., built from\n    # GitHub source), you can remove str() on self.device\n    state_dict = torch.load(opt.model_path, map_location=\'cpu\')\n    if hasattr(state_dict, \'_metadata\'):\n        del state_dict._metadata\n\n    # patch InstanceNorm checkpoints prior to 0.4\n    for key in list(state_dict.keys()):  # need to copy keys here because we mutate in loop\n        __patch_instance_norm_state_dict(state_dict, netG, key.split(\'.\'))\n    netG.load_state_dict(state_dict)\n\n    if opt.use_gpu:\n        netG.cuda()\n    return netG\n\ndef video(opt):\n    if \'HD\' in opt.model_path:\n        netG = MosaicNet_HD(3*25+1, 3, norm=\'instance\')\n    else:\n        netG = MosaicNet(3*25+1, 3,norm = \'batch\')\n    show_paramsnumber(netG,\'netG\')\n    netG.load_state_dict(torch.load(opt.model_path))\n    netG.eval()\n    if opt.use_gpu:\n        netG.cuda()\n    return netG\n\ndef bisenet(opt,type=\'roi\'):\n    \'\'\'\n    type: roi or mosaic\n    \'\'\'\n    net = BiSeNet(num_classes=1, context_path=\'resnet18\',train_flag=False)\n    show_paramsnumber(net,\'segment\')\n    if type == \'roi\':\n        net.load_state_dict(torch.load(opt.model_path))\n    elif type == \'mosaic\':\n        net.load_state_dict(torch.load(opt.mosaic_position_model_path))\n    net.eval()\n    if opt.use_gpu:\n        net.cuda()\n    return net\n\n# def unet_clean(opt):\n#     net = UNet(n_channels = 3, n_classes = 1)\n#     show_paramsnumber(net,\'segment\')\n#     net.load_state_dict(torch.load(opt.mosaic_position_model_path))\n#     net.eval()\n#     if opt.use_gpu:\n#         net.cuda()\n#     return net\n\n# def unet(opt):\n#     net = UNet(n_channels = 3, n_classes = 1)\n#     show_paramsnumber(net,\'segment\')\n#     net.load_state_dict(torch.load(opt.model_path))\n#     net.eval()\n#     if opt.use_gpu:\n#         net.cuda()\n#     return net\n'"
models/pix2pixHD_model.py,12,"b""# This code clone from https://github.com/NVIDIA/pix2pixHD\n# LICENSE file : https://github.com/NVIDIA/pix2pixHD/blob/master/LICENSE.txt\nimport torch\nimport torch.nn as nn\nimport functools\nfrom torch.autograd import Variable\nimport numpy as np\n\n###############################################################################\n# Functions\n###############################################################################\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm2d') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n\ndef get_norm_layer(norm_type='instance'):\n    if norm_type == 'batch':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n    elif norm_type == 'instance':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n    else:\n        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n    return norm_layer\n\ndef define_G(input_nc, output_nc, ngf, netG, n_downsample_global=3, n_blocks_global=9, n_local_enhancers=1, \n             n_blocks_local=3, norm='instance', gpu_ids=[]):    \n    norm_layer = get_norm_layer(norm_type=norm)     \n    if netG == 'global':    \n        netG = GlobalGenerator(input_nc, output_nc, ngf, n_downsample_global, n_blocks_global, norm_layer)       \n    elif netG == 'local':        \n        netG = LocalEnhancer(input_nc, output_nc, ngf, n_downsample_global, n_blocks_global, \n                                  n_local_enhancers, n_blocks_local, norm_layer)\n    elif netG == 'encoder':\n        netG = Encoder(input_nc, output_nc, ngf, n_downsample_global, norm_layer)\n    else:\n        raise('generator not implemented!')\n    # print(netG)\n    if len(gpu_ids) > 0:\n        assert(torch.cuda.is_available())   \n        netG.cuda(gpu_ids[0])\n    netG.apply(weights_init)\n    return netG\n\ndef define_D(input_nc, ndf, n_layers_D, norm='instance', use_sigmoid=False, num_D=1, getIntermFeat=False, gpu_ids=[]):        \n    norm_layer = get_norm_layer(norm_type=norm)   \n    netD = MultiscaleDiscriminator(input_nc, ndf, n_layers_D, norm_layer, use_sigmoid, num_D, getIntermFeat)   \n    print(netD)\n    if len(gpu_ids) > 0:\n        assert(torch.cuda.is_available())\n        netD.cuda(gpu_ids[0])\n    netD.apply(weights_init)\n    return netD\n\ndef print_network(net):\n    if isinstance(net, list):\n        net = net[0]\n    num_params = 0\n    for param in net.parameters():\n        num_params += param.numel()\n    print(net)\n    print('Total number of parameters: %d' % num_params)\n\n##############################################################################\n# Losses\n##############################################################################\nclass GANLoss(nn.Module):\n    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n                 tensor=torch.FloatTensor):\n        super(GANLoss, self).__init__()\n        self.real_label = target_real_label\n        self.fake_label = target_fake_label\n        self.real_label_var = None\n        self.fake_label_var = None\n        self.Tensor = tensor\n        if use_lsgan:\n            self.loss = nn.MSELoss()\n        else:\n            self.loss = nn.BCELoss()\n\n    def get_target_tensor(self, input, target_is_real):\n        target_tensor = None\n        if target_is_real:\n            create_label = ((self.real_label_var is None) or\n                            (self.real_label_var.numel() != input.numel()))\n            if create_label:\n                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n                self.real_label_var = Variable(real_tensor, requires_grad=False)\n            target_tensor = self.real_label_var\n        else:\n            create_label = ((self.fake_label_var is None) or\n                            (self.fake_label_var.numel() != input.numel()))\n            if create_label:\n                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n            target_tensor = self.fake_label_var\n        return target_tensor\n\n    def __call__(self, input, target_is_real):\n        if isinstance(input[0], list):\n            loss = 0\n            for input_i in input:\n                pred = input_i[-1]\n                target_tensor = self.get_target_tensor(pred, target_is_real)\n                loss += self.loss(pred, target_tensor)\n            return loss\n        else:            \n            target_tensor = self.get_target_tensor(input[-1], target_is_real)\n            return self.loss(input[-1], target_tensor)\n\nclass VGGLoss(nn.Module):\n    def __init__(self, gpu_ids):\n        super(VGGLoss, self).__init__()        \n        self.vgg = Vgg19().cuda()\n        self.criterion = nn.L1Loss()\n        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]        \n\n    def forward(self, x, y):              \n        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n        loss = 0\n        for i in range(len(x_vgg)):\n            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())        \n        return loss\n\n##############################################################################\n# Generator\n##############################################################################\nclass LocalEnhancer(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=32, n_downsample_global=3, n_blocks_global=9, \n                 n_local_enhancers=1, n_blocks_local=3, norm_layer=nn.BatchNorm2d, padding_type='reflect'):        \n        super(LocalEnhancer, self).__init__()\n        self.n_local_enhancers = n_local_enhancers\n        \n        ###### global generator model #####           \n        ngf_global = ngf * (2**n_local_enhancers)\n        model_global = GlobalGenerator(input_nc, output_nc, ngf_global, n_downsample_global, n_blocks_global, norm_layer).model        \n        model_global = [model_global[i] for i in range(len(model_global)-3)] # get rid of final convolution layers        \n        self.model = nn.Sequential(*model_global)                \n\n        ###### local enhancer layers #####\n        for n in range(1, n_local_enhancers+1):\n            ### downsample            \n            ngf_global = ngf * (2**(n_local_enhancers-n))\n            model_downsample = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf_global, kernel_size=7, padding=0), \n                                norm_layer(ngf_global), nn.ReLU(True),\n                                nn.Conv2d(ngf_global, ngf_global * 2, kernel_size=3, stride=2, padding=1), \n                                norm_layer(ngf_global * 2), nn.ReLU(True)]\n            ### residual blocks\n            model_upsample = []\n            for i in range(n_blocks_local):\n                model_upsample += [ResnetBlock(ngf_global * 2, padding_type=padding_type, norm_layer=norm_layer)]\n\n            ### upsample\n            model_upsample += [nn.ConvTranspose2d(ngf_global * 2, ngf_global, kernel_size=3, stride=2, padding=1, output_padding=1), \n                               norm_layer(ngf_global), nn.ReLU(True)]      \n\n            ### final convolution\n            if n == n_local_enhancers:                \n                model_upsample += [nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0), nn.Tanh()]                       \n            \n            setattr(self, 'model'+str(n)+'_1', nn.Sequential(*model_downsample))\n            setattr(self, 'model'+str(n)+'_2', nn.Sequential(*model_upsample))                  \n        \n        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n\n    def forward(self, input): \n        ### create input pyramid\n        input_downsampled = [input]\n        for i in range(self.n_local_enhancers):\n            input_downsampled.append(self.downsample(input_downsampled[-1]))\n\n        ### output at coarest level\n        output_prev = self.model(input_downsampled[-1])        \n        ### build up one layer at a time\n        for n_local_enhancers in range(1, self.n_local_enhancers+1):\n            model_downsample = getattr(self, 'model'+str(n_local_enhancers)+'_1')\n            model_upsample = getattr(self, 'model'+str(n_local_enhancers)+'_2')            \n            input_i = input_downsampled[self.n_local_enhancers-n_local_enhancers]            \n            output_prev = model_upsample(model_downsample(input_i) + output_prev)\n        return output_prev\n\nclass GlobalGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, n_downsampling=3, n_blocks=9, norm_layer=nn.BatchNorm2d, \n                 padding_type='reflect'):\n        assert(n_blocks >= 0)\n        super(GlobalGenerator, self).__init__()        \n        activation = nn.ReLU(True)        \n\n        model = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0), norm_layer(ngf), activation]\n        ### downsample\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n                      norm_layer(ngf * mult * 2), activation]\n\n        ### resnet blocks\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, activation=activation, norm_layer=norm_layer)]\n        \n        ### upsample         \n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n                       norm_layer(int(ngf * mult / 2)), activation]\n        model += [nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0), nn.Tanh()]        \n        self.model = nn.Sequential(*model)\n            \n    def forward(self, input):\n        return self.model(input)             \n        \n# Define a resnet block\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, padding_type, norm_layer, activation=nn.ReLU(True), use_dropout=False):\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, activation, use_dropout)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, activation, use_dropout):\n        conv_block = []\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n                       norm_layer(dim),\n                       activation]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n                       norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out\n\nclass Encoder(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=32, n_downsampling=4, norm_layer=nn.BatchNorm2d):\n        super(Encoder, self).__init__()        \n        self.output_nc = output_nc        \n\n        model = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0), \n                 norm_layer(ngf), nn.ReLU(True)]             \n        ### downsample\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n                      norm_layer(ngf * mult * 2), nn.ReLU(True)]\n\n        ### upsample         \n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n                       norm_layer(int(ngf * mult / 2)), nn.ReLU(True)]        \n\n        model += [nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0), nn.Tanh()]\n        self.model = nn.Sequential(*model) \n\n    def forward(self, input, inst):\n        outputs = self.model(input)\n\n        # instance-wise average pooling\n        outputs_mean = outputs.clone()\n        inst_list = np.unique(inst.cpu().numpy().astype(int))        \n        for i in inst_list:\n            for b in range(input.size()[0]):\n                indices = (inst[b:b+1] == int(i)).nonzero() # n x 4            \n                for j in range(self.output_nc):\n                    output_ins = outputs[indices[:,0] + b, indices[:,1] + j, indices[:,2], indices[:,3]]                    \n                    mean_feat = torch.mean(output_ins).expand_as(output_ins)                                        \n                    outputs_mean[indices[:,0] + b, indices[:,1] + j, indices[:,2], indices[:,3]] = mean_feat                       \n        return outputs_mean\n\nclass MultiscaleDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n                 use_sigmoid=False, num_D=3, getIntermFeat=False):\n        super(MultiscaleDiscriminator, self).__init__()\n        self.num_D = num_D\n        self.n_layers = n_layers\n        self.getIntermFeat = getIntermFeat\n     \n        for i in range(num_D):\n            netD = NLayerDiscriminator(input_nc, ndf, n_layers, norm_layer, use_sigmoid, getIntermFeat)\n            if getIntermFeat:                                \n                for j in range(n_layers+2):\n                    setattr(self, 'scale'+str(i)+'_layer'+str(j), getattr(netD, 'model'+str(j)))                                   \n            else:\n                setattr(self, 'layer'+str(i), netD.model)\n\n        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n\n    def singleD_forward(self, model, input):\n        if self.getIntermFeat:\n            result = [input]\n            for i in range(len(model)):\n                result.append(model[i](result[-1]))\n            return result[1:]\n        else:\n            return [model(input)]\n\n    def forward(self, input):        \n        num_D = self.num_D\n        result = []\n        input_downsampled = input\n        for i in range(num_D):\n            if self.getIntermFeat:\n                model = [getattr(self, 'scale'+str(num_D-1-i)+'_layer'+str(j)) for j in range(self.n_layers+2)]\n            else:\n                model = getattr(self, 'layer'+str(num_D-1-i))\n            result.append(self.singleD_forward(model, input_downsampled))\n            if i != (num_D-1):\n                input_downsampled = self.downsample(input_downsampled)\n        return result\n        \n# Defines the PatchGAN discriminator with the specified arguments.\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, getIntermFeat=False):\n        super(NLayerDiscriminator, self).__init__()\n        self.getIntermFeat = getIntermFeat\n        self.n_layers = n_layers\n\n        kw = 4\n        padw = int(np.ceil((kw-1.0)/2))\n        sequence = [[nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]]\n\n        nf = ndf\n        for n in range(1, n_layers):\n            nf_prev = nf\n            nf = min(nf * 2, 512)\n            sequence += [[\n                nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=2, padding=padw),\n                norm_layer(nf), nn.LeakyReLU(0.2, True)\n            ]]\n\n        nf_prev = nf\n        nf = min(nf * 2, 512)\n        sequence += [[\n            nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=1, padding=padw),\n            norm_layer(nf),\n            nn.LeakyReLU(0.2, True)\n        ]]\n\n        sequence += [[nn.Conv2d(nf, 1, kernel_size=kw, stride=1, padding=padw)]]\n\n        if use_sigmoid:\n            sequence += [[nn.Sigmoid()]]\n\n        if getIntermFeat:\n            for n in range(len(sequence)):\n                setattr(self, 'model'+str(n), nn.Sequential(*sequence[n]))\n        else:\n            sequence_stream = []\n            for n in range(len(sequence)):\n                sequence_stream += sequence[n]\n            self.model = nn.Sequential(*sequence_stream)\n\n    def forward(self, input):\n        if self.getIntermFeat:\n            res = [input]\n            for n in range(self.n_layers+2):\n                model = getattr(self, 'model'+str(n))\n                res.append(model(res[-1]))\n            return res[1:]\n        else:\n            return self.model(input)        \n\nfrom torchvision import models\nclass Vgg19(torch.nn.Module):\n    def __init__(self, requires_grad=False):\n        super(Vgg19, self).__init__()\n        vgg_pretrained_features = models.vgg19(pretrained=True).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        for x in range(2):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(2, 7):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(7, 12):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(12, 21):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(21, 30):\n            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h_relu1 = self.slice1(X)\n        h_relu2 = self.slice2(h_relu1)        \n        h_relu3 = self.slice3(h_relu2)        \n        h_relu4 = self.slice4(h_relu3)        \n        h_relu5 = self.slice5(h_relu4)                \n        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n        return out\n"""
models/pix2pix_model.py,13,"b'# This code clone from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n# LICENSE file : https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/LICENSE\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport functools\nfrom torch.optim import lr_scheduler\n\n\n\ndef set_requires_grad(nets, requires_grad=False):\n    """"""Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n    Parameters:\n        nets (network list)   -- a list of networks\n        requires_grad (bool)  -- whether the networks require gradients or not\n    """"""\n    if not isinstance(nets, list):\n        nets = [nets]\n    for net in nets:\n        if net is not None:\n            for param in net.parameters():\n                param.requires_grad = requires_grad\n\n\n###############################################################################\n# Helper Functions\n###############################################################################\n\n\nclass Identity(nn.Module):\n    def forward(self, x):\n        return x\n\n\ndef get_norm_layer(norm_type=\'instance\'):\n    """"""Return a normalization layer\n\n    Parameters:\n        norm_type (str) -- the name of the normalization layer: batch | instance | none\n\n    For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev).\n    For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.\n    """"""\n    if norm_type == \'batch\':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n    elif norm_type == \'instance\':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n    elif norm_type == \'none\':\n        norm_layer = lambda x: Identity()\n    else:\n        raise NotImplementedError(\'normalization layer [%s] is not found\' % norm_type)\n    return norm_layer\n\n\ndef get_scheduler(optimizer, opt):\n    """"""Return a learning rate scheduler\n\n    Parameters:\n        optimizer          -- the optimizer of the network\n        opt (option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\xef\xbc\x8e\xe3\x80\x80\n                              opt.lr_policy is the name of learning rate policy: linear | step | plateau | cosine\n\n    For \'linear\', we keep the same learning rate for the first <opt.niter> epochs\n    and linearly decay the rate to zero over the next <opt.niter_decay> epochs.\n    For other schedulers (step, plateau, and cosine), we use the default PyTorch schedulers.\n    See https://pytorch.org/docs/stable/optim.html for more details.\n    """"""\n    if opt.lr_policy == \'linear\':\n        def lambda_rule(epoch):\n            lr_l = 1.0 - max(0, epoch + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\n            return lr_l\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n    elif opt.lr_policy == \'step\':\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n    elif opt.lr_policy == \'plateau\':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\'min\', factor=0.2, threshold=0.01, patience=5)\n    elif opt.lr_policy == \'cosine\':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=opt.niter, eta_min=0)\n    else:\n        return NotImplementedError(\'learning rate policy [%s] is not implemented\', opt.lr_policy)\n    return scheduler\n\n\ndef init_weights(net, init_type=\'normal\', init_gain=0.02):\n    """"""Initialize network weights.\n\n    Parameters:\n        net (network)   -- network to be initialized\n        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n\n    We use \'normal\' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n    work better for some applications. Feel free to try yourself.\n    """"""\n    def init_func(m):  # define the initialization function\n        classname = m.__class__.__name__\n        if hasattr(m, \'weight\') and (classname.find(\'Conv\') != -1 or classname.find(\'Linear\') != -1):\n            if init_type == \'normal\':\n                init.normal_(m.weight.data, 0.0, init_gain)\n            elif init_type == \'xavier\':\n                init.xavier_normal_(m.weight.data, gain=init_gain)\n            elif init_type == \'kaiming\':\n                init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\')\n            elif init_type == \'orthogonal\':\n                init.orthogonal_(m.weight.data, gain=init_gain)\n            else:\n                raise NotImplementedError(\'initialization method [%s] is not implemented\' % init_type)\n            if hasattr(m, \'bias\') and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find(\'BatchNorm2d\') != -1:  # BatchNorm Layer\'s weight is not a matrix; only normal distribution applies.\n            init.normal_(m.weight.data, 1.0, init_gain)\n            init.constant_(m.bias.data, 0.0)\n\n    #print(\'initialize network with %s\' % init_type)\n    net.apply(init_func)  # apply the initialization function <init_func>\n\n\ndef init_net(net, init_type=\'normal\', init_gain=0.02, gpu_ids=[]):\n    """"""Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights\n    Parameters:\n        net (network)      -- the network to be initialized\n        init_type (str)    -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n        gain (float)       -- scaling factor for normal, xavier and orthogonal.\n        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n\n    Return an initialized network.\n    """"""\n    if len(gpu_ids) > 0:\n        assert(torch.cuda.is_available())\n        net.to(gpu_ids[0])\n        net = torch.nn.DataParallel(net, gpu_ids)  # multi-GPUs\n    init_weights(net, init_type, init_gain=init_gain)\n    return net\n\n\ndef define_G(input_nc, output_nc, ngf, netG, norm=\'batch\', use_dropout=False, init_type=\'normal\', init_gain=0.02, gpu_ids=[]):\n    """"""Create a generator\n\n    Parameters:\n        input_nc (int) -- the number of channels in input images\n        output_nc (int) -- the number of channels in output images\n        ngf (int) -- the number of filters in the last conv layer\n        netG (str) -- the architecture\'s name: resnet_9blocks | resnet_6blocks | unet_256 | unet_128\n        norm (str) -- the name of normalization layers used in the network: batch | instance | none\n        use_dropout (bool) -- if use dropout layers.\n        init_type (str)    -- the name of our initialization method.\n        init_gain (float)  -- scaling factor for normal, xavier and orthogonal.\n        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n\n    Returns a generator\n\n    Our current implementation provides two types of generators:\n        U-Net: [unet_128] (for 128x128 input images) and [unet_256] (for 256x256 input images)\n        The original U-Net paper: https://arxiv.org/abs/1505.04597\n\n        Resnet-based generator: [resnet_6blocks] (with 6 Resnet blocks) and [resnet_9blocks] (with 9 Resnet blocks)\n        Resnet-based generator consists of several Resnet blocks between a few downsampling/upsampling operations.\n        We adapt Torch code from Justin Johnson\'s neural style transfer project (https://github.com/jcjohnson/fast-neural-style).\n\n\n    The generator has been initialized by <init_net>. It uses RELU for non-linearity.\n    """"""\n    net = None\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if netG == \'resnet_9blocks\':\n        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9)\n    elif netG == \'resnet_6blocks\':\n        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6)\n    elif netG == \'unet_128\':\n        net = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n    elif netG == \'unet_256\':\n        net = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n    else:\n        raise NotImplementedError(\'Generator model name [%s] is not recognized\' % netG)\n    return init_net(net, init_type, init_gain, gpu_ids)\n\n\ndef define_D(input_nc, ndf, netD, n_layers_D=3, norm=\'batch\', init_type=\'normal\', init_gain=0.02, gpu_ids=[]):\n    """"""Create a discriminator\n\n    Parameters:\n        input_nc (int)     -- the number of channels in input images\n        ndf (int)          -- the number of filters in the first conv layer\n        netD (str)         -- the architecture\'s name: basic | n_layers | pixel\n        n_layers_D (int)   -- the number of conv layers in the discriminator; effective when netD==\'n_layers\'\n        norm (str)         -- the type of normalization layers used in the network.\n        init_type (str)    -- the name of the initialization method.\n        init_gain (float)  -- scaling factor for normal, xavier and orthogonal.\n        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n\n    Returns a discriminator\n\n    Our current implementation provides three types of discriminators:\n        [basic]: \'PatchGAN\' classifier described in the original pix2pix paper.\n        It can classify whether 70\xc3\x9770 overlapping patches are real or fake.\n        Such a patch-level discriminator architecture has fewer parameters\n        than a full-image discriminator and can work on arbitrarily-sized images\n        in a fully convolutional fashion.\n\n        [n_layers]: With this mode, you cna specify the number of conv layers in the discriminator\n        with the parameter <n_layers_D> (default=3 as used in [basic] (PatchGAN).)\n\n        [pixel]: 1x1 PixelGAN discriminator can classify whether a pixel is real or not.\n        It encourages greater color diversity but has no effect on spatial statistics.\n\n    The discriminator has been initialized by <init_net>. It uses Leakly RELU for non-linearity.\n    """"""\n    net = None\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if netD == \'basic\':  # default PatchGAN classifier\n        net = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer)\n    elif netD == \'n_layers\':  # more options\n        net = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer)\n    elif netD == \'pixel\':     # classify if each pixel is real or fake\n        net = PixelDiscriminator(input_nc, ndf, norm_layer=norm_layer)\n    else:\n        raise NotImplementedError(\'Discriminator model name [%s] is not recognized\' % net)\n    return init_net(net, init_type, init_gain, gpu_ids)\n\n\n##############################################################################\n# Classes\n##############################################################################\nclass GANLoss(nn.Module):\n    """"""Define different GAN objectives.\n\n    The GANLoss class abstracts away the need to create the target label tensor\n    that has the same size as the input.\n    """"""\n\n    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):\n        """""" Initialize the GANLoss class.\n\n        Parameters:\n            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n            target_real_label (bool) - - label for a real image\n            target_fake_label (bool) - - label of a fake image\n\n        Note: Do not use sigmoid as the last layer of Discriminator.\n        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n        """"""\n        super(GANLoss, self).__init__()\n        self.register_buffer(\'real_label\', torch.tensor(target_real_label))\n        self.register_buffer(\'fake_label\', torch.tensor(target_fake_label))\n        self.gan_mode = gan_mode\n        if gan_mode == \'lsgan\':\n            self.loss = nn.MSELoss()\n        elif gan_mode == \'vanilla\':\n            self.loss = nn.BCEWithLogitsLoss()\n        elif gan_mode in [\'wgangp\']:\n            self.loss = None\n        else:\n            raise NotImplementedError(\'gan mode %s not implemented\' % gan_mode)\n\n    def get_target_tensor(self, prediction, target_is_real):\n        """"""Create label tensors with the same size as the input.\n\n        Parameters:\n            prediction (tensor) - - tpyically the prediction from a discriminator\n            target_is_real (bool) - - if the ground truth label is for real images or fake images\n\n        Returns:\n            A label tensor filled with ground truth label, and with the size of the input\n        """"""\n\n        if target_is_real:\n            target_tensor = self.real_label\n        else:\n            target_tensor = self.fake_label\n        return target_tensor.expand_as(prediction)\n\n    def __call__(self, prediction, target_is_real):\n        """"""Calculate loss given Discriminator\'s output and grount truth labels.\n\n        Parameters:\n            prediction (tensor) - - tpyically the prediction output from a discriminator\n            target_is_real (bool) - - if the ground truth label is for real images or fake images\n\n        Returns:\n            the calculated loss.\n        """"""\n        if self.gan_mode in [\'lsgan\', \'vanilla\']:\n            target_tensor = self.get_target_tensor(prediction, target_is_real)\n            loss = self.loss(prediction, target_tensor)\n        elif self.gan_mode == \'wgangp\':\n            if target_is_real:\n                loss = -prediction.mean()\n            else:\n                loss = prediction.mean()\n        return loss\n\n\ndef cal_gradient_penalty(netD, real_data, fake_data, device, type=\'mixed\', constant=1.0, lambda_gp=10.0):\n    """"""Calculate the gradient penalty loss, used in WGAN-GP paper https://arxiv.org/abs/1704.00028\n\n    Arguments:\n        netD (network)              -- discriminator network\n        real_data (tensor array)    -- real images\n        fake_data (tensor array)    -- generated images from the generator\n        device (str)                -- GPU / CPU: from torch.device(\'cuda:{}\'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device(\'cpu\')\n        type (str)                  -- if we mix real and fake data or not [real | fake | mixed].\n        constant (float)            -- the constant used in formula ( | |gradient||_2 - constant)^2\n        lambda_gp (float)           -- weight for this loss\n\n    Returns the gradient penalty loss\n    """"""\n    if lambda_gp > 0.0:\n        if type == \'real\':   # either use real images, fake images, or a linear interpolation of two.\n            interpolatesv = real_data\n        elif type == \'fake\':\n            interpolatesv = fake_data\n        elif type == \'mixed\':\n            alpha = torch.rand(real_data.shape[0], 1)\n            alpha = alpha.expand(real_data.shape[0], real_data.nelement() // real_data.shape[0]).contiguous().view(*real_data.shape)\n            alpha = alpha.to(device)\n            interpolatesv = alpha * real_data + ((1 - alpha) * fake_data)\n        else:\n            raise NotImplementedError(\'{} not implemented\'.format(type))\n        interpolatesv.requires_grad_(True)\n        disc_interpolates = netD(interpolatesv)\n        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolatesv,\n                                        grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n                                        create_graph=True, retain_graph=True, only_inputs=True)\n        gradients = gradients[0].view(real_data.size(0), -1)  # flat the data\n        gradient_penalty = (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean() * lambda_gp        # added eps\n        return gradient_penalty, gradients\n    else:\n        return 0.0, None\n\n\nclass ResnetGenerator(nn.Module):\n    """"""Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.\n\n    We adapt Torch code and idea from Justin Johnson\'s neural style transfer project(https://github.com/jcjohnson/fast-neural-style)\n    """"""\n\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type=\'reflect\'):\n        """"""Construct a Resnet-based generator\n\n        Parameters:\n            input_nc (int)      -- the number of channels in input images\n            output_nc (int)     -- the number of channels in output images\n            ngf (int)           -- the number of filters in the last conv layer\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers\n            n_blocks (int)      -- the number of ResNet blocks\n            padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero\n        """"""\n        assert(n_blocks >= 0)\n        super(ResnetGenerator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):  # add downsampling layers\n            mult = 2 ** i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2 ** n_downsampling\n        for i in range(n_blocks):       # add ResNet blocks\n\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n\n        for i in range(n_downsampling):  # add upsampling layers\n            mult = 2 ** (n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1,\n                                         bias=use_bias),\n                      norm_layer(int(ngf * mult / 2)),\n                      nn.ReLU(True)]\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        """"""Standard forward""""""\n        return self.model(input)\n\n\nclass ResnetBlock(nn.Module):\n    """"""Define a Resnet block""""""\n\n    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        """"""Initialize the Resnet block\n\n        A resnet block is a conv block with skip connections\n        We construct a conv block with build_conv_block function,\n        and implement skip connections in <forward> function.\n        Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n        """"""\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        """"""Construct a convolutional block.\n\n        Parameters:\n            dim (int)           -- the number of channels in the conv layer.\n            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers.\n            use_bias (bool)     -- if the conv layer uses bias or not\n\n        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n        """"""\n        conv_block = []\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.ReLU(True)]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        """"""Forward function (with skip connections)""""""\n        out = x + self.conv_block(x)  # add skip connections\n        return out\n\n\nclass UnetGenerator(nn.Module):\n    """"""Create a Unet-based generator""""""\n\n    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n        """"""Construct a Unet generator\n        Parameters:\n            input_nc (int)  -- the number of channels in input images\n            output_nc (int) -- the number of channels in output images\n            num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7,\n                                image of size 128x128 will become of size 1x1 # at the bottleneck\n            ngf (int)       -- the number of filters in the last conv layer\n            norm_layer      -- normalization layer\n\n        We construct the U-Net from the innermost layer to the outermost layer.\n        It is a recursive process.\n        """"""\n        super(UnetGenerator, self).__init__()\n        # construct unet structure\n        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)  # add the innermost layer\n        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n        # gradually reduce the number of filters from ngf * 8 to ngf\n        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)  # add the outermost layer\n\n    def forward(self, input):\n        """"""Standard forward""""""\n        return self.model(input)\n\n\nclass UnetSkipConnectionBlock(nn.Module):\n    """"""Defines the Unet submodule with skip connection.\n        X -------------------identity----------------------\n        |-- downsampling -- |submodule| -- upsampling --|\n    """"""\n\n    def __init__(self, outer_nc, inner_nc, input_nc=None,\n                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n        """"""Construct a Unet submodule with skip connections.\n\n        Parameters:\n            outer_nc (int) -- the number of filters in the outer conv layer\n            inner_nc (int) -- the number of filters in the inner conv layer\n            input_nc (int) -- the number of channels in input images/features\n            submodule (UnetSkipConnectionBlock) -- previously defined submodules\n            outermost (bool)    -- if this module is the outermost module\n            innermost (bool)    -- if this module is the innermost module\n            norm_layer          -- normalization layer\n            user_dropout (bool) -- if use dropout layers.\n        """"""\n        super(UnetSkipConnectionBlock, self).__init__()\n        self.outermost = outermost\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n        if input_nc is None:\n            input_nc = outer_nc\n        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n                             stride=2, padding=1, bias=use_bias)\n        downrelu = nn.LeakyReLU(0.2, True)\n        downnorm = norm_layer(inner_nc)\n        uprelu = nn.ReLU(True)\n        upnorm = norm_layer(outer_nc)\n\n        if outermost:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1)\n            down = [downconv]\n            up = [uprelu, upconv, nn.Tanh()]\n            model = down + [submodule] + up\n        elif innermost:\n            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            down = [downrelu, downconv]\n            up = [uprelu, upconv, upnorm]\n            model = down + up\n        else:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            down = [downrelu, downconv, downnorm]\n            up = [uprelu, upconv, upnorm]\n\n            if use_dropout:\n                model = down + [submodule] + up + [nn.Dropout(0.5)]\n            else:\n                model = down + [submodule] + up\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        if self.outermost:\n            return self.model(x)\n        else:   # add skip connections\n            return torch.cat([x, self.model(x)], 1)\n\n\nclass NLayerDiscriminator(nn.Module):\n    """"""Defines a PatchGAN discriminator""""""\n\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n        """"""Construct a PatchGAN discriminator\n\n        Parameters:\n            input_nc (int)  -- the number of channels in input images\n            ndf (int)       -- the number of filters in the last conv layer\n            n_layers (int)  -- the number of conv layers in the discriminator\n            norm_layer      -- normalization layer\n        """"""\n        super(NLayerDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n            use_bias = norm_layer.func != nn.BatchNorm2d\n        else:\n            use_bias = norm_layer != nn.BatchNorm2d\n\n        kw = 4\n        padw = 1\n        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):  # gradually increase the number of filters\n            nf_mult_prev = nf_mult\n            nf_mult = min(2 ** n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2 ** n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        """"""Standard forward.""""""\n        return self.model(input)\n\n\nclass PixelDiscriminator(nn.Module):\n    """"""Defines a 1x1 PatchGAN discriminator (pixelGAN)""""""\n\n    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d):\n        """"""Construct a 1x1 PatchGAN discriminator\n\n        Parameters:\n            input_nc (int)  -- the number of channels in input images\n            ndf (int)       -- the number of filters in the last conv layer\n            norm_layer      -- normalization layer\n        """"""\n        super(PixelDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n            use_bias = norm_layer.func != nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer != nn.InstanceNorm2d\n\n        self.net = [\n            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n            norm_layer(ndf * 2),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n\n        self.net = nn.Sequential(*self.net)\n\n    def forward(self, input):\n        """"""Standard forward.""""""\n        return self.net(input)\n'"
models/runmodel.py,0,"b'import cv2\nimport sys\nsys.path.append("".."")\nimport util.image_processing as impro\nfrom util import mosaic\nfrom util import data\nimport torch\nimport numpy as np\n\ndef run_segment(img,net,size = 360,use_gpu = True):\n    img = impro.resize(img,size)\n    img = data.im2tensor(img,use_gpu = use_gpu,  bgr2rgb = False,use_transform = False , is0_1 = True)\n    mask = net(img)\n    mask = data.tensor2im(mask, gray=True,rgb2bgr = False, is0_1 = True)\n    return mask\n\ndef run_pix2pix(img,net,opt):\n    if opt.netG == \'HD\':\n        img = impro.resize(img,512)\n    else:\n        img = impro.resize(img,128)\n    img = data.im2tensor(img,use_gpu=opt.use_gpu)\n    img_fake = net(img)\n    img_fake = data.tensor2im(img_fake)\n    return img_fake\n\ndef traditional_cleaner(img,opt):\n    h,w = img.shape[:2]\n    img = cv2.blur(img, (opt.tr_blur,opt.tr_blur))\n    img = img[::opt.tr_down,::opt.tr_down,:]\n    img = cv2.resize(img, (w,h),interpolation=cv2.INTER_LANCZOS4)\n    return img\n\ndef run_styletransfer(opt, net, img):\n\n    if opt.output_size != 0:\n        if \'resize\' in opt.preprocess and \'resize_scale_width\' not in opt.preprocess:\n            img = impro.resize(img,opt.output_size)\n        elif \'resize_scale_width\' in opt.preprocess:\n            img = cv2.resize(img, (opt.output_size,opt.output_size))\n        img = img[0:4*int(img.shape[0]/4),0:4*int(img.shape[1]/4),:]\n\n    if \'edges\' in opt.preprocess:\n        if opt.canny > 100:\n            canny_low = opt.canny-50\n            canny_high = np.clip(opt.canny+50,0,255)\n        elif opt.canny < 50:\n            canny_low = np.clip(opt.canny-25,0,255)\n            canny_high = opt.canny+25\n        else:\n            canny_low = opt.canny-int(opt.canny/2)\n            canny_high = opt.canny+int(opt.canny/2)\n        img = cv2.Canny(img,opt.canny-50,opt.canny+50)\n        if opt.only_edges:\n            return img\n        img = data.im2tensor(img,use_gpu=opt.use_gpu,gray=True,use_transform = False,is0_1 = False)\n    else:    \n        img = data.im2tensor(img,use_gpu=opt.use_gpu,gray=False,use_transform = True)\n    img = net(img)\n    img = data.tensor2im(img)\n    return img\n\ndef get_ROI_position(img,net,opt,keepsize=True):\n    mask = run_segment(img,net,size=360,use_gpu = opt.use_gpu)\n    mask = impro.mask_threshold(mask,opt.mask_extend,opt.mask_threshold)\n    if keepsize:\n        mask = impro.resize_like(mask, img)\n    x,y,halfsize,area = impro.boundingSquare(mask, 1)\n    return mask,x,y,halfsize,area\n\ndef get_mosaic_position(img_origin,net_mosaic_pos,opt):\n    h,w = img_origin.shape[:2]\n    mask = run_segment(img_origin,net_mosaic_pos,size=360,use_gpu = opt.use_gpu)\n    # mask_1 = mask.copy()\n    mask = impro.mask_threshold(mask,ex_mun=int(min(h,w)/20),threshold=opt.mask_threshold)\n    if not opt.all_mosaic_area:\n        mask = impro.find_mostlikely_ROI(mask)\n    x,y,size,area = impro.boundingSquare(mask,Ex_mul=opt.ex_mult)\n    #Location fix\n    rat = min(h,w)/360.0\n    x,y,size = int(rat*x),int(rat*y),int(rat*size)\n    x,y = np.clip(x, 0, w),np.clip(y, 0, h)\n    size = np.clip(size, 0, min(w-x,h-y))\n    # print(x,y,size)\n    return x,y,size,mask'"
models/unet_model.py,3,"b""# This code clone from https://github.com/milesial/Pytorch-UNet\n# LICENSE file : https://github.com/milesial/Pytorch-UNet/blob/master/LICENSE\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass double_conv(nn.Module):\n    '''(conv => BN => ReLU) * 2'''\n    def __init__(self, in_ch, out_ch):\n        super(double_conv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass inconv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(inconv, self).__init__()\n        self.conv = double_conv(in_ch, out_ch)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass down(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(down, self).__init__()\n        self.mpconv = nn.Sequential(\n            nn.MaxPool2d(2),\n            double_conv(in_ch, out_ch)\n        )\n\n    def forward(self, x):\n        x = self.mpconv(x)\n        return x\n\nclass Upsample(nn.Module):\n    def __init__(self,  scale_factor):\n        super(Upsample, self).__init__()\n        self.scale_factor = scale_factor\n    def forward(self, x):\n        return F.interpolate(x, scale_factor=self.scale_factor,mode='bilinear', align_corners=True)\n\n\nclass up(nn.Module):\n    def __init__(self, in_ch, out_ch, bilinear=True):\n        super(up, self).__init__()\n\n        #  would be a nice idea if the upsampling could be learned too,\n        #  but my machine do not have enough memory to handle all those weights\n        if bilinear:\n            self.up = Upsample(scale_factor=2)\n        else:\n            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n\n        self.conv = double_conv(in_ch, out_ch)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        \n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n                        diffY // 2, diffY - diffY//2))\n        \n        # for padding issues, see \n        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n\n        x = torch.cat([x2, x1], dim=1)\n        x = self.conv(x)\n        return x\n\n\nclass outconv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(outconv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes):\n        super(UNet, self).__init__()\n        self.inc = inconv(n_channels, 64)\n        self.down1 = down(64, 128)\n        self.down2 = down(128, 256)\n        self.down3 = down(256, 512)\n        self.down4 = down(512, 512)\n        self.up1 = up(1024, 256)\n        self.up2 = up(512, 128)\n        self.up3 = up(256, 64)\n        self.up4 = up(128, 64)\n        self.outc = outconv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        x = self.outc(x)\n        return x"""
models/videoHD_model.py,7,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .pix2pixHD_model import *\n\n\nclass encoder_2d(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, n_downsampling=3, n_blocks=9, norm_layer=nn.BatchNorm2d, \n                 padding_type='reflect'):\n        assert(n_blocks >= 0)\n        super(encoder_2d, self).__init__()        \n        activation = nn.ReLU(True)        \n\n        model = [nn.ReflectionPad2d(3), nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0), norm_layer(ngf), activation]\n        ### downsample\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.ReflectionPad2d(1),nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=0),\n                      norm_layer(ngf * mult * 2), activation]\n\n        self.model = nn.Sequential(*model)\n    def forward(self, input):\n        return self.model(input)  \n\nclass decoder_2d(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, n_downsampling=3, n_blocks=9, norm_layer=nn.BatchNorm2d, \n                 padding_type='reflect'):\n        assert(n_blocks >= 0)\n        super(decoder_2d, self).__init__()        \n        activation = nn.ReLU(True)        \n\n        model = []\n\n        ### resnet blocks\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, activation=activation, norm_layer=norm_layer)]\n        \n        ### upsample         \n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n\n            # model += [  nn.Upsample(scale_factor = 2, mode='nearest'),\n            # nn.ReflectionPad2d(1),\n            # nn.Conv2d(ngf * mult, int(ngf * mult / 2),kernel_size=3, stride=1, padding=0),\n            # norm_layer(int(ngf * mult / 2)),\n            # nn.ReLU(True)]\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2, padding=1, output_padding=1),\n                       norm_layer(int(ngf * mult / 2)), activation]\n        model += [nn.ReflectionPad2d(3), nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0), nn.Tanh()]        \n        self.model = nn.Sequential(*model)\n            \n    def forward(self, input):\n        return self.model(input)\n\n\nclass conv_3d(nn.Module):\n    def __init__(self,inchannel,outchannel,kernel_size=3,stride=2,padding=1,norm_layer_3d=nn.BatchNorm3d,use_bias=True):\n        super(conv_3d, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(inchannel, outchannel, kernel_size=kernel_size, stride=stride, padding=padding, bias=use_bias),\n            norm_layer_3d(outchannel),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\nclass conv_2d(nn.Module):\n    def __init__(self,inchannel,outchannel,kernel_size=3,stride=1,padding=1,norm_layer_2d=nn.BatchNorm2d,use_bias=True):\n        super(conv_2d, self).__init__()\n        self.conv = nn.Sequential(\n            nn.ReflectionPad2d(padding),\n            nn.Conv2d(inchannel, outchannel, kernel_size=kernel_size, stride=stride, padding=0, bias=use_bias),\n            norm_layer_2d(outchannel),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass encoder_3d(nn.Module):\n    def __init__(self,in_channel,norm_layer_2d,norm_layer_3d,use_bias):\n        super(encoder_3d, self).__init__()\n        self.inconv = conv_3d(1, 64, 7, 2, 3,norm_layer_3d,use_bias)\n        self.down1 = conv_3d(64, 128, 3, 2, 1,norm_layer_3d,use_bias)\n        self.down2 = conv_3d(128, 256, 3, 2, 1,norm_layer_3d,use_bias)\n        self.down3 = conv_3d(256, 512, 3, 2, 1,norm_layer_3d,use_bias)\n        self.down4 = conv_3d(512, 1024, 3, 1, 1,norm_layer_3d,use_bias)\n        self.pool = nn.AvgPool3d((5,1,1))\n        # self.conver2d = nn.Sequential(\n        #     nn.Conv2d(256*int(in_channel/4), 256, kernel_size=3, stride=1, padding=1, bias=use_bias),\n        #     norm_layer_2d(256),\n        #     nn.ReLU(inplace=True),\n        # )\n\n\n    def forward(self, x):\n\n        x = x.view(x.size(0),1,x.size(1),x.size(2),x.size(3))\n        x = self.inconv(x)\n        x = self.down1(x)\n        x = self.down2(x)\n        x = self.down3(x)\n        x = self.down4(x)\n        #print(x.size())\n        x = self.pool(x)\n        #print(x.size())\n        # torch.Size([1, 1024, 16, 16])\n        # torch.Size([1, 512, 5, 16, 16])\n\n\n        x = x.view(x.size(0),x.size(1),x.size(3),x.size(4))\n\n       # x = self.conver2d(x)\n\n        return x\n\n    # def __init__(self, input_nc, output_nc, ngf=64, n_downsampling=3, n_blocks=9, norm_layer=nn.BatchNorm2d, \n    #              padding_type='reflect')\n\nclass ALL(nn.Module):\n    def __init__(self, in_channel, out_channel,norm_layer_2d,norm_layer_3d,use_bias):\n        super(ALL, self).__init__()\n\n        self.encoder_2d = encoder_2d(4,3,64,4,norm_layer=norm_layer_2d,padding_type='reflect')\n        self.encoder_3d = encoder_3d(in_channel,norm_layer_2d,norm_layer_3d,use_bias)\n        self.decoder_2d = decoder_2d(4,3,64,4,norm_layer=norm_layer_2d,padding_type='reflect')\n        # self.shortcut_cov = conv_2d(3,64,7,1,3,norm_layer_2d,use_bias)\n        self.merge1 = conv_2d(2048,1024,3,1,1,norm_layer_2d,use_bias)\n        # self.merge2 = nn.Sequential(\n        #     conv_2d(128,64,3,1,1,norm_layer_2d,use_bias),\n        #     nn.ReflectionPad2d(3),\n        #     nn.Conv2d(64, out_channel, kernel_size=7, padding=0),\n        #     nn.Tanh()\n        # )\n\n    def forward(self, x):\n\n        N = int((x.size()[1])/3)\n        x_2d = torch.cat((x[:,int((N-1)/2)*3:(int((N-1)/2)+1)*3,:,:], x[:,N-1:N,:,:]), 1)\n        #shortcut_2d = x[:,int((N-1)/2)*3:(int((N-1)/2)+1)*3,:,:]\n\n        x_2d = self.encoder_2d(x_2d)\n        x_3d = self.encoder_3d(x)\n        #x = x_2d + x_3d\n        x = torch.cat((x_2d,x_3d),1)\n        x = self.merge1(x)\n\n        x = self.decoder_2d(x)\n        #shortcut_2d = self.shortcut_cov(shortcut_2d)\n        #x = torch.cat((x,shortcut_2d),1)\n        #x = self.merge2(x)\n\n        return x\n\ndef MosaicNet(in_channel, out_channel, norm='batch'):\n\n    if norm == 'batch':\n        # norm_layer_2d = nn.BatchNorm2d\n        # norm_layer_3d = nn.BatchNorm3d\n        norm_layer_2d = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n        norm_layer_3d = functools.partial(nn.BatchNorm3d, affine=True, track_running_stats=True)\n        use_bias = False\n    elif norm == 'instance':\n        norm_layer_2d = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n        norm_layer_3d = functools.partial(nn.InstanceNorm3d, affine=False, track_running_stats=False)\n        use_bias = True\n\n    return ALL(in_channel, out_channel, norm_layer_2d, norm_layer_3d, use_bias)\n"""
models/video_model.py,7,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .pix2pix_model import *\n\n\nclass encoder_2d(nn.Module):\n    """"""Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.\n\n    We adapt Torch code and idea from Justin Johnson\'s neural style transfer project(https://github.com/jcjohnson/fast-neural-style)\n    """"""\n\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type=\'reflect\'):\n        """"""Construct a Resnet-based generator\n\n        Parameters:\n            input_nc (int)      -- the number of channels in input images\n            output_nc (int)     -- the number of channels in output images\n            ngf (int)           -- the number of filters in the last conv layer\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers\n            n_blocks (int)      -- the number of ResNet blocks\n            padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero\n        """"""\n        assert(n_blocks >= 0)\n        super(encoder_2d, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):  # add downsampling layers\n            mult = 2 ** i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n        #torch.Size([1, 256, 32, 32])\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        """"""Standard forward""""""\n        return self.model(input)\n\n\nclass decoder_2d(nn.Module):\n    """"""Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.\n\n    We adapt Torch code and idea from Justin Johnson\'s neural style transfer project(https://github.com/jcjohnson/fast-neural-style)\n    """"""\n\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type=\'reflect\'):\n        """"""Construct a Resnet-based generator\n\n        Parameters:\n            input_nc (int)      -- the number of channels in input images\n            output_nc (int)     -- the number of channels in output images\n            ngf (int)           -- the number of filters in the last conv layer\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers\n            n_blocks (int)      -- the number of ResNet blocks\n            padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero\n        """"""\n        super(decoder_2d, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = []\n\n        n_downsampling = 2\n        mult = 2 ** n_downsampling\n        for i in range(n_blocks):       # add ResNet blocks\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n        #torch.Size([1, 256, 32, 32])\n\n        for i in range(n_downsampling):  # add upsampling layers\n            mult = 2 ** (n_downsampling - i)\n            # model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n            #                              kernel_size=3, stride=2,\n            #                              padding=1, output_padding=1,\n            #                              bias=use_bias),\n            #           norm_layer(int(ngf * mult / 2)),\n            #           nn.ReLU(True)]\n            #https://distill.pub/2016/deconv-checkerboard/\n            #https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/190\n\n            model += [  nn.Upsample(scale_factor = 2, mode=\'nearest\'),\n                        nn.ReflectionPad2d(1),\n                        nn.Conv2d(ngf * mult, int(ngf * mult / 2),kernel_size=3, stride=1, padding=0),\n                        norm_layer(int(ngf * mult / 2)),\n                        nn.ReLU(True)]\n        # model += [nn.ReflectionPad2d(3)]\n        # model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        # model += [nn.Tanh()]\n        # model += [nn.Sigmoid()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        """"""Standard forward""""""\n        return self.model(input)\n\n\n\nclass conv_3d(nn.Module):\n    def __init__(self,inchannel,outchannel,kernel_size=3,stride=2,padding=1,norm_layer_3d=nn.BatchNorm3d,use_bias=True):\n        super(conv_3d, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(inchannel, outchannel, kernel_size=kernel_size, stride=stride, padding=padding, bias=use_bias),\n            norm_layer_3d(outchannel),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\nclass conv_2d(nn.Module):\n    def __init__(self,inchannel,outchannel,kernel_size=3,stride=1,padding=1,norm_layer_2d=nn.BatchNorm2d,use_bias=True):\n        super(conv_2d, self).__init__()\n        self.conv = nn.Sequential(\n            nn.ReflectionPad2d(padding),\n            nn.Conv2d(inchannel, outchannel, kernel_size=kernel_size, stride=stride, padding=0, bias=use_bias),\n            norm_layer_2d(outchannel),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass encoder_3d(nn.Module):\n    def __init__(self,in_channel,norm_layer_2d,norm_layer_3d,use_bias):\n        super(encoder_3d, self).__init__()\n        self.down1 = conv_3d(1, 64, 3, 2, 1,norm_layer_3d,use_bias)\n        self.down2 = conv_3d(64, 128, 3, 2, 1,norm_layer_3d,use_bias)\n        self.down3 = conv_3d(128, 256, 3, 1, 1,norm_layer_3d,use_bias)\n        self.conver2d = nn.Sequential(\n            nn.Conv2d(256*int(in_channel/4), 256, kernel_size=3, stride=1, padding=1, bias=use_bias),\n            norm_layer_2d(256),\n            nn.ReLU(inplace=True),\n        )\n\n\n    def forward(self, x):\n\n        x = x.view(x.size(0),1,x.size(1),x.size(2),x.size(3))\n        x = self.down1(x)\n        x = self.down2(x)\n        x = self.down3(x)\n\n        x = x.view(x.size(0),x.size(1)*x.size(2),x.size(3),x.size(4))\n\n        x = self.conver2d(x)\n\n        return x\n\n\n\nclass ALL(nn.Module):\n    def __init__(self, in_channel, out_channel,norm_layer_2d,norm_layer_3d,use_bias):\n        super(ALL, self).__init__()\n\n        self.encoder_2d = encoder_2d(4,-1,64,norm_layer=norm_layer_2d,n_blocks=9)\n        self.encoder_3d = encoder_3d(in_channel,norm_layer_2d,norm_layer_3d,use_bias)\n        self.decoder_2d = decoder_2d(4,3,64,norm_layer=norm_layer_2d,n_blocks=9)\n        self.shortcut_cov = conv_2d(3,64,7,1,3,norm_layer_2d,use_bias)\n        self.merge1 = conv_2d(512,256,3,1,1,norm_layer_2d,use_bias)\n        self.merge2 = nn.Sequential(\n            conv_2d(128,64,3,1,1,norm_layer_2d,use_bias),\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(64, out_channel, kernel_size=7, padding=0),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n\n        N = int((x.size()[1])/3)\n        x_2d = torch.cat((x[:,int((N-1)/2)*3:(int((N-1)/2)+1)*3,:,:], x[:,N-1:N,:,:]), 1)\n        shortcut_2d = x[:,int((N-1)/2)*3:(int((N-1)/2)+1)*3,:,:]\n\n        x_2d = self.encoder_2d(x_2d)\n\n        x_3d = self.encoder_3d(x)\n        x = torch.cat((x_2d,x_3d),1)\n        x = self.merge1(x)\n        x = self.decoder_2d(x)\n        shortcut_2d = self.shortcut_cov(shortcut_2d)\n        x = torch.cat((x,shortcut_2d),1)\n        x = self.merge2(x)\n\n        return x\n\ndef MosaicNet(in_channel, out_channel, norm=\'batch\'):\n\n    if norm == \'batch\':\n        # norm_layer_2d = nn.BatchNorm2d\n        # norm_layer_3d = nn.BatchNorm3d\n        norm_layer_2d = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n        norm_layer_3d = functools.partial(nn.BatchNorm3d, affine=True, track_running_stats=True)\n        use_bias = False\n    elif norm == \'instance\':\n        norm_layer_2d = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n        norm_layer_3d = functools.partial(nn.InstanceNorm3d, affine=False, track_running_stats=False)\n        use_bias = True\n\n    return ALL(in_channel, out_channel, norm_layer_2d, norm_layer_3d, use_bias)\n'"
util/__init__.py,0,b''
util/clean_cache.py,0,"b""import os\nimport shutil\n\ndef findalldir(rootdir):\n    dir_list = []\n    for root,dirs,files in os.walk(rootdir): \n        for dir in dirs:\n            dir_list.append(os.path.join(root,dir))\n    return(dir_list)\n\ndef Traversal(filedir):\n    file_list=[]\n    dir_list = []\n    for root,dirs,files in os.walk(filedir): \n        for file in files:\n            file_list.append(os.path.join(root,file)) \n        for dir in dirs:\n            dir_list.append(os.path.join(root,dir))\n            Traversal(dir)\n    return file_list,dir_list\n\ndef is_img(path):\n    ext = os.path.splitext(path)[1]\n    ext = ext.lower()\n    if ext in ['.jpg','.png','.jpeg','.bmp']:\n        return True\n    else:\n        return False\n\ndef is_video(path):\n    ext = os.path.splitext(path)[1]\n    ext = ext.lower()\n    if ext in ['.mp4','.flv','.avi','.mov','.mkv','.wmv','.rmvb']:\n        return True\n    else:\n        return False\n\ndef cleanall():\n    file_list,dir_list = Traversal('./')\n    for file in file_list:\n        if ('tmp' in file) | ('pth' in file)|('pycache' in file) | is_video(file) | is_img(file):\n            if os.path.exists(file):\n                if 'imgs' not in file:\n                    os.remove(file)\n                    print('remove file:',file)\n\n    for dir in dir_list:\n        if ('tmp'in dir)|('pycache'in dir):\n            if os.path.exists(dir):\n                shutil.rmtree(dir)\n                print('remove dir:',dir)"""
util/data.py,2,"b""import random\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nimport cv2\nfrom .image_processing import color_adjust,dctblur\n\ntransform = transforms.Compose([  \n    transforms.ToTensor(),  \n    transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5))  \n    ]  \n)  \n\ndef tensor2im(image_tensor, imtype=np.uint8, gray=False, rgb2bgr = True ,is0_1 = False):\n    image_tensor =image_tensor.data\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    \n    if not is0_1:\n        image_numpy = (image_numpy + 1)/2.0\n    image_numpy = np.clip(image_numpy * 255.0,0,255) \n\n    # gray -> output 1ch\n    if gray:\n        h, w = image_numpy.shape[1:]\n        image_numpy = image_numpy.reshape(h,w)\n        return image_numpy.astype(imtype)\n\n    # output 3ch\n    if image_numpy.shape[0] == 1:\n        image_numpy = np.tile(image_numpy, (3, 1, 1))\n    image_numpy = image_numpy.transpose((1, 2, 0))  \n    if rgb2bgr and not gray:\n        image_numpy = image_numpy[...,::-1]-np.zeros_like(image_numpy)\n    return image_numpy.astype(imtype)\n\n\ndef im2tensor(image_numpy, imtype=np.uint8, gray=False,bgr2rgb = True, reshape = True, use_gpu = True,  use_transform = True,is0_1 = True):\n    \n    if gray:\n        h, w = image_numpy.shape\n        image_numpy = (image_numpy/255.0-0.5)/0.5\n        image_tensor = torch.from_numpy(image_numpy).float()\n        if reshape:\n            image_tensor = image_tensor.reshape(1,1,h,w)\n    else:\n        h, w ,ch = image_numpy.shape\n        if bgr2rgb:\n            image_numpy = image_numpy[...,::-1]-np.zeros_like(image_numpy)\n        if use_transform:\n            image_tensor = transform(image_numpy)\n        else:\n            if is0_1:\n                image_numpy = image_numpy/255.0\n            else:\n                image_numpy = (image_numpy/255.0-0.5)/0.5\n            image_numpy = image_numpy.transpose((2, 0, 1))\n            image_tensor = torch.from_numpy(image_numpy).float()\n        if reshape:\n            image_tensor = image_tensor.reshape(1,ch,h,w)\n    if use_gpu:\n        image_tensor = image_tensor.cuda()\n    return image_tensor\n\ndef shuffledata(data,target):\n    state = np.random.get_state()\n    np.random.shuffle(data)\n    np.random.set_state(state)\n    np.random.shuffle(target)\n\ndef random_transform_video(src,target,finesize,N):\n\n    #random crop\n    h,w = target.shape[:2]\n    h_move = int((h-finesize)*random.random())\n    w_move = int((w-finesize)*random.random())\n    # print(h,w,h_move,w_move)\n    target = target[h_move:h_move+finesize,w_move:w_move+finesize,:]\n    src = src[h_move:h_move+finesize,w_move:w_move+finesize,:]\n\n    #random flip\n    if random.random()<0.5:\n        src = src[:,::-1,:]\n        target = target[:,::-1,:]\n\n    #random color\n    alpha = random.uniform(-0.1,0.1)\n    beta  = random.uniform(-0.1,0.1)\n    b     = random.uniform(-0.05,0.05)\n    g     = random.uniform(-0.05,0.05)\n    r     = random.uniform(-0.05,0.05)\n    for i in range(N):\n        src[:,:,i*3:(i+1)*3] = color_adjust(src[:,:,i*3:(i+1)*3],alpha,beta,b,g,r)\n    target = color_adjust(target,alpha,beta,b,g,r)\n\n    #random blur\n    if random.random()<0.5:\n        interpolations = [cv2.INTER_LINEAR,cv2.INTER_CUBIC,cv2.INTER_LANCZOS4]\n        size_ran = random.uniform(0.7,1.5)\n        interpolation_up = interpolations[random.randint(0,2)]\n        interpolation_down =interpolations[random.randint(0,2)]\n\n        tmp = cv2.resize(src[:,:,:3*N], (int(finesize*size_ran),int(finesize*size_ran)),interpolation=interpolation_up)\n        src[:,:,:3*N] = cv2.resize(tmp, (finesize,finesize),interpolation=interpolation_down)\n\n        tmp = cv2.resize(target, (int(finesize*size_ran),int(finesize*size_ran)),interpolation=interpolation_up)\n        target = cv2.resize(tmp, (finesize,finesize),interpolation=interpolation_down)\n\n    return src,target\n\ndef random_transform_single(img,out_shape):\n    out_h,out_w = out_shape\n    img = cv2.resize(img,(int(out_w*random.uniform(1.1, 1.5)),int(out_h*random.uniform(1.1, 1.5))))\n    h,w = img.shape[:2]\n    h_move = int((h-out_h)*random.random())\n    w_move = int((w-out_w)*random.random())\n    img = img[h_move:h_move+out_h,w_move:w_move+out_w]\n    if random.random()<0.5:\n        if random.random()<0.5:\n            img = img[:,::-1]\n        else:\n            img = img[::-1,:]\n    if img.shape[0] != out_h or img.shape[1]!= out_w :\n        img = cv2.resize(img,(out_w,out_h))\n    return img\n\ndef random_transform_image(img,mask,finesize,test_flag = False):\n    #random scale\n    if random.random()<0.5:\n        h,w = img.shape[:2]\n        loadsize = min((h,w))\n        a = (float(h)/float(w))*random.uniform(0.9, 1.1)\n        if h<w:\n            mask = cv2.resize(mask, (int(loadsize/a),loadsize))\n            img = cv2.resize(img, (int(loadsize/a),loadsize))\n        else:\n            mask = cv2.resize(mask, (loadsize,int(loadsize*a)))\n            img = cv2.resize(img, (loadsize,int(loadsize*a)))\n\n    #random crop\n    h,w = img.shape[:2]\n    h_move = int((h-finesize)*random.random())\n    w_move = int((w-finesize)*random.random())\n    img_crop = img[h_move:h_move+finesize,w_move:w_move+finesize]\n    mask_crop = mask[h_move:h_move+finesize,w_move:w_move+finesize]\n\n    if test_flag:\n        return img_crop,mask_crop\n    \n    #random rotation\n    if random.random()<0.2:\n        h,w = img_crop.shape[:2]\n        M = cv2.getRotationMatrix2D((w/2,h/2),90*int(4*random.random()),1)\n        img = cv2.warpAffine(img_crop,M,(w,h))\n        mask = cv2.warpAffine(mask_crop,M,(w,h))\n    else:\n        img,mask = img_crop,mask_crop\n\n    #random color\n    img = color_adjust(img,ran=True)\n\n    #random flip\n    if random.random()<0.5:\n        if random.random()<0.5:\n            img = img[:,::-1,:]\n            mask = mask[:,::-1]\n        else:\n            img = img[::-1,:,:]\n            mask = mask[::-1,:]\n\n    #random blur\n    if random.random()<0.5:\n        img = dctblur(img,random.randint(1,15))\n        \n        # interpolations = [cv2.INTER_LINEAR,cv2.INTER_CUBIC,cv2.INTER_LANCZOS4]\n        # size_ran = random.uniform(0.7,1.5)\n        # img = cv2.resize(img, (int(finesize*size_ran),int(finesize*size_ran)),interpolation=interpolations[random.randint(0,2)])\n        # img = cv2.resize(img, (finesize,finesize),interpolation=interpolations[random.randint(0,2)])\n    \n    #check shape\n    if img.shape[0]!= finesize or img.shape[1]!= finesize or mask.shape[0]!= finesize or mask.shape[1]!= finesize:\n        img = cv2.resize(img,(finesize,finesize))\n        mask = cv2.resize(mask,(finesize,finesize))\n        print('warning! shape error.')\n    return img,mask\n\ndef showresult(img1,img2,img3,name,is0_1 = False):\n    size = img1.shape[3]\n    showimg=np.zeros((size,size*3,3))\n    showimg[0:size,0:size] = tensor2im(img1,rgb2bgr = False, is0_1 = is0_1)\n    showimg[0:size,size:size*2] = tensor2im(img2,rgb2bgr = False, is0_1 = is0_1)\n    showimg[0:size,size*2:size*3] = tensor2im(img3,rgb2bgr = False, is0_1 = is0_1)\n    cv2.imwrite(name, showimg)\n"""
util/ffmpeg.py,0,"b'import os,json\n\n# ffmpeg 3.4.6\n\ndef video2image(videopath,imagepath,fps=0,start_time=0,last_time=0):\n    if start_time == 0:\n        if fps == 0:\n            os.system(\'ffmpeg -i ""\'+videopath+\'"" -f image2 \'+\'-q:v -0 \'+imagepath)\n        else:\n            os.system(\'ffmpeg -i ""\'+videopath+\'"" -r \'+str(fps)+\' -f image2 \'+\'-q:v -0 \'+imagepath)\n    else:\n        if fps == 0:\n            os.system(\'ffmpeg -ss \'+start_time+\' -t \'+last_time+\' -i ""\'+videopath+\'"" -f image2 \'+\'-q:v -0 \'+imagepath)\n        else:\n            os.system(\'ffmpeg -ss \'+start_time+\' -t \'+last_time+\' -i ""\'+videopath+\'"" -r \'+str(fps)+\' -f image2 \'+\'-q:v -0 \'+imagepath)\n\n\ndef video2voice(videopath,voicepath):\n    os.system(\'ffmpeg -i ""\'+videopath+\'"" -f mp3 \'+voicepath)\n\ndef image2video(fps,imagepath,voicepath,videopath):\n    os.system(\'ffmpeg -y -r \'+str(fps)+\' -i \'+imagepath+\' -vcodec libx264 \'+\'./tmp/video_tmp.mp4\')\n    #os.system(\'ffmpeg -f image2 -i \'+imagepath+\' -vcodec libx264 -r \'+str(fps)+\' ./tmp/video_tmp.mp4\')\n    os.system(\'ffmpeg -i ./tmp/video_tmp.mp4 -i ""\'+voicepath+\'"" -vcodec copy -acodec copy \'+videopath)\n\ndef get_video_infos(videopath):\n    cmd_str =  \'ffprobe -v quiet -print_format json -show_format -show_streams -i ""\' + videopath + \'""\'  \n    #out_string = os.popen(cmd_str).read()\n    #For chinese path in Windows\n    #https://blog.csdn.net/weixin_43903378/article/details/91979025\n    stream = os.popen(cmd_str)._stream\n    out_string = stream.buffer.read().decode(encoding=\'utf-8\')\n\n    infos = json.loads(out_string)\n    try:\n        fps = eval(infos[\'streams\'][0][\'avg_frame_rate\'])\n        endtime = float(infos[\'format\'][\'duration\'])\n        width = int(infos[\'streams\'][0][\'width\'])\n        height = int(infos[\'streams\'][0][\'height\'])\n    except Exception as e:\n        fps = eval(infos[\'streams\'][1][\'r_frame_rate\'])\n        endtime = float(infos[\'format\'][\'duration\'])\n        width = int(infos[\'streams\'][1][\'width\'])\n        height = int(infos[\'streams\'][1][\'height\'])\n\n    return fps,endtime,height,width\n\ndef cut_video(in_path,start_time,last_time,out_path,vcodec=\'h265\'):\n    if vcodec == \'copy\':\n        os.system(\'ffmpeg -ss \'+start_time+\' -t \'+last_time+\' -i ""\'+in_path+\'"" -vcodec copy -acodec copy \'+out_path)\n    elif vcodec == \'h264\':    \n        os.system(\'ffmpeg -ss \'+start_time+\' -t \'+last_time+\' -i ""\'+in_path+\'"" -vcodec libx264 -b 12M \'+out_path)\n    elif vcodec == \'h265\':\n        os.system(\'ffmpeg -ss \'+start_time+\' -t \'+last_time+\' -i ""\'+in_path+\'"" -vcodec libx265 -b 12M \'+out_path)\n\ndef continuous_screenshot(videopath,savedir,fps):\n    \'\'\'\n    videopath: input video path\n    savedir:   images will save here\n    fps:       save how many images per second\n    \'\'\'\n    videoname = os.path.splitext(os.path.basename(videopath))[0]\n    os.system(\'ffmpeg -i ""\'+videopath+\'"" -vf fps=\'+str(fps)+\' -q:v -0 \'+savedir+\'/\'+videoname+\'_%06d.jpg\')\n'"
util/filt.py,0,"b""import numpy as np\n\ndef less_zero(arr,num = 7):\n    index = np.linspace(0,len(arr)-1,len(arr),dtype='int')\n    cnt = 0\n    for i in range(2,len(arr)-2):\n        if arr[i] != 0:\n            arr[i] = arr[i]\n            if cnt != 0:\n                if cnt <= num*2:\n                    arr[i-cnt:round(i-cnt/2)] = arr[i-cnt-1-2]\n                    arr[round(i-cnt/2):i] = arr[i+2]\n                    index[i-cnt:round(i-cnt/2)] = i-cnt-1-2\n                    index[round(i-cnt/2):i] = i+2\n                else:\n                    arr[i-cnt:i-cnt+num] = arr[i-cnt-1-2]\n                    arr[i-num:i] = arr[i+2] \n                    index[i-cnt:i-cnt+num] = i-cnt-1-2\n                    index[i-num:i] = i+2\n                cnt = 0\n        else:\n            cnt += 1\n    return arr,index\n\ndef medfilt(data,window):\n    if window%2 == 0 or window < 0:\n        print('Error: the medfilt window must be even number')\n        exit(0)\n    pad = int((window-1)/2)\n    pad_data = np.zeros(len(data)+window-1, dtype = type(data[0]))\n    result = np.zeros(len(data),dtype = type(data[0]))\n    pad_data[pad:pad+len(data)]=data[:]\n    for i in range(len(data)):\n        result[i] = np.median(pad_data[i:i+window])\n    return result\n\ndef position_medfilt(positions,window):\n\n    x,mask_index = less_zero(positions[:,0],window)\n    y = less_zero(positions[:,1],window)[0]\n    area = less_zero(positions[:,2],window)[0]\n    x_filt = medfilt(x, window)\n    y_filt = medfilt(y, window)\n    area_filt = medfilt(area, window)\n    cnt = 0\n    for i in range(1,len(x)):\n        if 0.8<x_filt[i]/(x[i]+1)<1.2 and 0.8<y_filt[i]/(y[i]+1)<1.2 and 0.6<area_filt[i]/(area[i]+1)<1.4:\n            mask_index[i] = mask_index[i]\n            if cnt != 0:\n                mask_index[i-cnt:round(i-cnt/2)] = mask_index[i-cnt]\n                mask_index[round(i-cnt/2):i] = mask_index[i] \n                cnt = 0\n        else:\n            mask_index[i] = mask_index[i-1]\n            cnt += 1\n    return mask_index\n\n# def main():\n#     import matplotlib.pyplot as plt\n#     positions = np.load('../test_pos.npy')\n#     positions_new = np.load('../test_pos.npy')\n#     print(positions.shape)\n#     mask_index = position_medfilt(positions.copy(), 7)\n#     x = positions_new[2]\n\n#     x_new = []\n#     for i in range(len(x)):\n#         x_new.append(x[mask_index[i]])\n\n#     plt.subplot(211)\n#     plt.plot(x)\n#     plt.subplot(212)\n#     plt.plot(x_new)\n#     plt.show()\n\n# if __name__ == '__main__':\n#     main()"""
util/image_processing.py,0,"b'import cv2\nimport numpy as np\nimport random\n\nimport platform\n\nsystem_type = \'Linux\'\nif \'Windows\' in platform.platform():\n    system_type = \'Windows\'\n\nDCT_Q = np.array([[8,16,19,22,26,27,29,34],\n                [16,16,22,24,27,29,34,37],\n                [19,22,26,27,29,34,34,38],\n                [22,22,26,27,29,34,37,40],\n                [22,26,27,29,32,35,40,48],\n                [26,27,29,32,35,40,48,58],\n                [26,27,29,34,38,46,56,59],\n                [27,29,35,38,46,56,69,83]])\n\ndef imread(file_path,mod = \'normal\',loadsize = 0):\n    \'\'\'\n    mod:  \'normal\' | \'gray\' | \'all\'\n    loadsize: 0->original\n    \'\'\'\n    if system_type == \'Linux\':\n        if mod == \'normal\':\n            img = cv2.imread(file_path)\n        elif mod == \'gray\':\n            img = cv2.imread(file_path,0)\n        elif mod == \'all\':\n            img = cv2.imread(file_path,-1)\n    \n    #In windows, for chinese path, use cv2.imdecode insteaded.\n    #It will loss EXIF, I can\'t fix it\n    else: \n        if mod == \'gray\':\n            img = cv2.imdecode(np.fromfile(file_path,dtype=np.uint8),0)\n        else:\n            img = cv2.imdecode(np.fromfile(file_path,dtype=np.uint8),-1)\n    \n    if loadsize != 0:\n        img = resize(img, loadsize, interpolation=cv2.INTER_CUBIC)\n\n    return img\n\ndef imwrite(file_path,img):\n    \'\'\'\n    in other to save chinese path images in windows,\n    this fun just for save final output images\n    \'\'\'\n    if system_type == \'Linux\':\n        cv2.imwrite(file_path, img)\n    else:\n        cv2.imencode(\'.jpg\', img)[1].tofile(file_path)\n\ndef resize(img,size,interpolation=cv2.INTER_LINEAR):\n    \'\'\'\n    cv2.INTER_NEAREST \xc2\xa0 \xc2\xa0 \xc2\xa0\xe6\x9c\x80\xe9\x82\xbb\xe8\xbf\x91\xe6\x8f\x92\xe5\x80\xbc\xe7\x82\xb9\xe6\xb3\x95\n    cv2.INTER_LINEAR \xc2\xa0 \xc2\xa0 \xc2\xa0 \xc2\xa0\xe5\x8f\x8c\xe7\xba\xbf\xe6\x80\xa7\xe6\x8f\x92\xe5\x80\xbc\xe6\xb3\x95\n    cv2.INTER_AREA \xc2\xa0 \xc2\xa0 \xc2\xa0 \xc2\xa0 \xe9\x82\xbb\xe5\x9f\x9f\xe5\x83\x8f\xe7\xb4\xa0\xe5\x86\x8d\xe5\x8f\x96\xe6\xa0\xb7\xe6\x8f\x92\xe8\xa1\xa5\n    cv2.INTER_CUBIC \xc2\xa0 \xc2\xa0 \xc2\xa0 \xc2\xa0\xe5\x8f\x8c\xe7\xab\x8b\xe6\x96\xb9\xe6\x8f\x92\xe8\xa1\xa5\xef\xbc\x8c4*4\xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9a\x84\xe8\xa1\xa5\xe7\x82\xb9\n    cv2.INTER_LANCZOS4     8x8\xe5\x83\x8f\xe7\xb4\xa0\xe9\x82\xbb\xe5\x9f\x9f\xe7\x9a\x84Lanczos\xe6\x8f\x92\xe5\x80\xbc\n    \'\'\'\n    h, w = img.shape[:2]\n    if np.min((w,h)) ==size:\n        return img\n    if w >= h:\n        res = cv2.resize(img,(int(size*w/h), size),interpolation=interpolation)\n    else:\n        res = cv2.resize(img,(size, int(size*h/w)),interpolation=interpolation)\n    return res\n\ndef resize_like(img,img_like):\n    h, w = img_like.shape[:2]\n    img = cv2.resize(img, (w,h))\n    return img\n\ndef ch_one2three(img):\n    res = cv2.merge([img, img, img])\n    return res\n\ndef color_adjust(img,alpha=1,beta=0,b=0,g=0,r=0,ran = False):\n    \'\'\'\n    g(x) = (1+\xce\xb1)g(x)+255*\xce\xb2, \n    g(x) = g(x[:+b*255,:+g*255,:+r*255])\n    \n    Args:\n        img   : input image\n        alpha : contrast\n        beta  : brightness\n        b     : blue hue\n        g     : green hue\n        r     : red hue\n        ran   : if True, randomly generated color correction parameters\n    Retuens:\n        img   : output image\n    \'\'\'\n    img = img.astype(\'float\')\n    if ran:\n        alpha = random.uniform(-0.1,0.1)\n        beta  = random.uniform(-0.1,0.1)\n        b     = random.uniform(-0.05,0.05)\n        g     = random.uniform(-0.05,0.05)\n        r     = random.uniform(-0.05,0.05)\n    img = (1+alpha)*img+255.0*beta\n    bgr = [b*255.0,g*255.0,r*255.0]\n    for i in range(3): img[:,:,i]=img[:,:,i]+bgr[i]\n    \n    return (np.clip(img,0,255)).astype(\'uint8\')\n\ndef makedataset(target_image,orgin_image):\n    target_image = resize(target_image,256)\n    orgin_image = resize(orgin_image,256)\n    img = np.zeros((256,512,3), dtype = ""uint8"")\n    w = orgin_image.shape[1]\n    img[0:256,0:256] = target_image[0:256,int(w/2-256/2):int(w/2+256/2)]\n    img[0:256,256:512] = orgin_image[0:256,int(w/2-256/2):int(w/2+256/2)]\n    return img\n\ndef spiltimage(img,size = 128):\n    h, w = img.shape[:2]\n    # size = min(h,w)\n    if w >= h:\n        img1 = img[:,0:size]\n        img2 = img[:,w-size:w]\n    else:\n        img1 = img[0:size,:]\n        img2 = img[h-size:h,:]\n\n    return img1,img2\n\ndef mergeimage(img1,img2,orgin_image,size = 128):\n    h, w = orgin_image.shape[:2]\n    new_img1 = np.zeros((h,w), dtype = ""uint8"")\n    new_img2 = np.zeros((h,w), dtype = ""uint8"")\n\n    # size = min(h,w)\n    if w >= h:\n        new_img1[:,0:size]=img1\n        new_img2[:,w-size:w]=img2\n    else:\n        new_img1[0:size,:]=img1\n        new_img2[h-size:h,:]=img2\n    result_img = cv2.add(new_img1,new_img2)\n    return result_img\n\ndef block_dct_and_idct(g,QQF):\n    T = cv2.dct(g)\n    IT = np.round(cv2.idct(np.round(np.round(16.0*T/QQF)*QQF/16)))\n    return IT\n\ndef image_dct_and_idct(I,QF):\n    h,w = I.shape\n    QQF = DCT_Q*QF\n    for i in range(int(h/8)):\n        for j in range(int(w/8)):\n            I[i*8:(i+1)*8,j*8:(j+1)*8] = block_dct_and_idct(I[i*8:(i+1)*8,j*8:(j+1)*8],QQF)\n    return I\n\ndef dctblur(img,Q):\n    \'\'\'\n    Q: 1~20, 1->best\n    \'\'\'\n    h,w = img.shape[:2]\n    img[:8*int(h/8),:8*int(w/8)]\n    img = img.astype(np.float32)\n    if img.ndim == 2:\n        img = image_dct_and_idct(img, Q)\n    if img.ndim == 3:\n        h,w,ch = img.shape\n        for i in range(ch):\n            img[:,:,i] = image_dct_and_idct(img[:,:,i], Q)\n    return (np.clip(img,0,255)).astype(np.uint8)\n    \ndef find_mostlikely_ROI(mask):\n    contours,hierarchy=cv2.findContours(mask, cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)\n    if len(contours)>0:\n        areas = []\n        for contour in contours:\n            areas.append(cv2.contourArea(contour))\n        index = areas.index(max(areas))\n        mask = np.zeros_like(mask)\n        mask = cv2.fillPoly(mask,[contours[index]],(255))\n    return mask\n\ndef boundingSquare(mask,Ex_mul):\n    # thresh = mask_threshold(mask,10,threshold)\n    area = mask_area(mask)\n    if area == 0 :\n        return 0,0,0,0\n\n    x,y,w,h = cv2.boundingRect(mask)\n    \n    center = np.array([int(x+w/2),int(y+h/2)])\n    size = max(w,h)\n    point0=np.array([x,y])\n    point1=np.array([x+size,y+size])\n\n    h, w = mask.shape[:2]\n    if size*Ex_mul > min(h, w):\n        size = min(h, w)\n        halfsize = int(min(h, w)/2)\n    else:\n        size = Ex_mul*size\n        halfsize = int(size/2)\n        size = halfsize*2\n    point0 = center - halfsize\n    point1 = center + halfsize\n    if point0[0]<0:\n        point0[0]=0\n        point1[0]=size\n    if point0[1]<0:\n        point0[1]=0\n        point1[1]=size\n    if point1[0]>w:\n        point1[0]=w\n        point0[0]=w-size\n    if point1[1]>h:\n        point1[1]=h\n        point0[1]=h-size\n    center = ((point0+point1)/2).astype(\'int\')\n    return center[0],center[1],halfsize,area\n\ndef mask_threshold(mask,ex_mun,threshold):\n    mask = cv2.threshold(mask,threshold,255,cv2.THRESH_BINARY)[1]\n    mask = cv2.blur(mask, (ex_mun, ex_mun))\n    mask = cv2.threshold(mask,threshold/5,255,cv2.THRESH_BINARY)[1]\n    return mask\n\ndef mask_area(mask):\n    mask = cv2.threshold(mask,127,255,0)[1]\n    # contours= cv2.findContours(mask,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)[1] #for opencv 3.4\n    contours= cv2.findContours(mask,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)[0]#updata to opencv 4.0\n    try:\n        area = cv2.contourArea(contours[0])\n    except:\n        area = 0\n    return area\n\n\ndef Q_lapulase(resImg):\n    \'\'\'\n    Evaluate image quality\n    score > 20   normal\n    score > 50   clear\n    \'\'\'\n    img2gray = cv2.cvtColor(resImg, cv2.COLOR_BGR2GRAY)\n    img2gray = resize(img2gray,512)\n    res = cv2.Laplacian(img2gray, cv2.CV_64F)\n    score = res.var()\n    return score\n\ndef replace_mosaic(img_origin,img_fake,mask,x,y,size,no_father):\n    img_fake = cv2.resize(img_fake,(size*2,size*2),interpolation=cv2.INTER_LANCZOS4)\n    if no_father:\n        img_origin[y-size:y+size,x-size:x+size]=img_fake\n        img_result = img_origin\n    else:\n        #color correction\n        RGB_origin = img_origin[y-size:y+size,x-size:x+size].mean(0).mean(0)\n        RGB_fake = img_fake.mean(0).mean(0)\n        for i in range(3):img_fake[:,:,i] = np.clip(img_fake[:,:,i]+RGB_origin[i]-RGB_fake[i],0,255)      \n        #eclosion\n        eclosion_num = int(size/5)\n        entad = int(eclosion_num/2+2)\n\n        # mask = np.zeros(img_origin.shape, dtype=\'uint8\')\n        # mask = cv2.rectangle(mask,(x-size+entad,y-size+entad),(x+size-entad,y+size-entad),(255,255,255),-1)\n        mask = cv2.resize(mask,(img_origin.shape[1],img_origin.shape[0]))\n        mask = ch_one2three(mask)\n        \n        mask = (cv2.blur(mask, (eclosion_num, eclosion_num)))\n        mask_tmp = np.zeros_like(mask)\n        mask_tmp[y-size:y+size,x-size:x+size] = mask[y-size:y+size,x-size:x+size]# Fix edge overflow\n        mask = mask_tmp/255.0\n\n        img_tmp = np.zeros(img_origin.shape)\n        img_tmp[y-size:y+size,x-size:x+size]=img_fake\n        img_result = img_origin.copy()\n        img_result = (img_origin*(1-mask)+img_tmp*mask).astype(\'uint8\')\n\n    return img_result'"
util/mosaic.py,0,"b""import cv2\nimport numpy as np\nimport os\nimport random\nfrom .image_processing import resize,ch_one2three,mask_area\n\ndef addmosaic(img,mask,opt):\n    if opt.mosaic_mod == 'random':\n        img = addmosaic_random(img,mask)\n    elif opt.mosaic_size == 0:\n        img = addmosaic_autosize(img, mask, opt.mosaic_mod)\n    else:\n        img = addmosaic_base(img,mask,opt.mosaic_size,opt.output_size,model = opt.mosaic_mod)\n    return img\n\ndef addmosaic_base(img,mask,n,out_size = 0,model = 'squa_avg',rect_rat = 1.6,father=0):\n    '''\n    img: input image\n    mask: input mask\n    n: mosaic size\n    out_size: output size  0->original\n    model : squa_avg squa_mid squa_random squa_avg_circle_edge rect_avg\n    rect_rat: if model==rect_avg , mosaic w/h=rect_rat\n    father : father size, -1->no 0->auto\n    '''\n    n = int(n)\n    if out_size:\n        img = resize(img,out_size)      \n    h, w = img.shape[:2]\n    mask = cv2.resize(mask,(w,h))\n    img_mosaic=img.copy()\n\n    if model=='squa_avg':\n        for i in range(int(h/n)):\n            for j in range(int(w/n)):\n                if mask[int(i*n+n/2),int(j*n+n/2)] == 255:\n                    img_mosaic[i*n:(i+1)*n,j*n:(j+1)*n,:]=img[i*n:(i+1)*n,j*n:(j+1)*n,:].mean(0).mean(0)\n\n    elif model=='squa_mid':\n        for i in range(int(h/n)):\n            for j in range(int(w/n)):\n                if mask[int(i*n+n/2),int(j*n+n/2)] == 255:\n                    img_mosaic[i*n:(i+1)*n,j*n:(j+1)*n,:]=img[i*n+int(n/2),j*n+int(n/2),:]\n\n    elif model == 'squa_random':\n        for i in range(int(h/n)):\n            for j in range(int(w/n)):\n                if mask[int(i*n+n/2),int(j*n+n/2)] == 255:\n                    img_mosaic[i*n:(i+1)*n,j*n:(j+1)*n,:]=img[int(i*n-n/2+n*random.random()),int(j*n-n/2+n*random.random()),:]\n\n    elif model == 'squa_avg_circle_edge':\n        for i in range(int(h/n)):\n            for j in range(int(w/n)):\n                img_mosaic[i*n:(i+1)*n,j*n:(j+1)*n,:]=img[i*n:(i+1)*n,j*n:(j+1)*n,:].mean(0).mean(0)\n        mask = cv2.threshold(mask,127,255,cv2.THRESH_BINARY)[1]\n        _mask = ch_one2three(mask)\n        mask_inv = cv2.bitwise_not(_mask)\n        imgroi1 = cv2.bitwise_and(_mask,img_mosaic)\n        imgroi2 = cv2.bitwise_and(mask_inv,img)\n        img_mosaic = cv2.add(imgroi1,imgroi2)\n\n    elif model =='rect_avg':\n        n_h=n\n        n_w=int(n*rect_rat)\n        for i in range(int(h/n_h)):\n            for j in range(int(w/n_w)):\n                if mask[int(i*n_h+n_h/2),int(j*n_w+n_w/2)] == 255:\n                    img_mosaic[i*n_h:(i+1)*n_h,j*n_w:(j+1)*n_w,:]=img[i*n_h:(i+1)*n_h,j*n_w:(j+1)*n_w,:].mean(0).mean(0)\n    \n    if father != -1:\n        if father==0:\n            mask = (cv2.blur(mask, (n, n)))\n        else:\n            mask = (cv2.blur(mask, (father, father)))\n        mask = ch_one2three(mask)/255.0\n        img_mosaic = (img*(1-mask)+img_mosaic*mask).astype('uint8')\n    \n    return img_mosaic\n\ndef get_autosize(img,mask,area_type = 'normal'):\n    h,w = img.shape[:2]\n    size = np.min([h,w])\n    mask = resize(mask,size)\n    alpha = size/512\n    try:\n        if area_type == 'normal':\n            area = mask_area(mask)\n        elif area_type == 'bounding':\n            w,h = cv2.boundingRect(mask)[2:]\n            area = w*h\n    except:\n        area = 0\n    area = area/(alpha*alpha)\n    if area>50000:\n        size = alpha*((area-50000)/50000+12)\n    elif 20000<area<=50000:\n        size = alpha*((area-20000)/30000+8)\n    elif 5000<area<=20000:\n        size = alpha*((area-5000)/20000+7)\n    elif 0<=area<=5000:\n        size = alpha*((area-0)/5000+6)\n    else:\n        pass\n    return size\n\ndef get_random_parameter(img,mask):\n    # mosaic size\n    p = np.array([0.5,0.5])\n    mod = np.random.choice(['normal','bounding'], p = p.ravel())\n    mosaic_size = get_autosize(img,mask,area_type = mod)\n    mosaic_size = int(mosaic_size*random.uniform(0.9,2.1))\n\n    # mosaic mod\n    p = np.array([0.25, 0.25, 0.1, 0.4])\n    mod = np.random.choice(['squa_mid','squa_avg','squa_avg_circle_edge','rect_avg'], p = p.ravel())\n\n    # rect_rat for rect_avg\n    rect_rat = random.uniform(1.1,1.6)\n    \n    # father size\n    father = int(mosaic_size*random.uniform(0,1.5))\n\n    return mosaic_size,mod,rect_rat,father\n\n\ndef addmosaic_autosize(img,mask,model,area_type = 'normal'):\n    mosaic_size = get_autosize(img,mask,area_type = 'normal')\n    img_mosaic = addmosaic_base(img,mask,mosaic_size,model = model)\n    return img_mosaic\n\ndef addmosaic_random(img,mask):\n    mosaic_size,mod,rect_rat,father = get_random_parameter(img,mask)\n    img_mosaic = addmosaic_base(img,mask,mosaic_size,model = mod,rect_rat=rect_rat,father=father)\n    return img_mosaic"""
util/util.py,0,"b'import os\nimport shutil\n\ndef Traversal(filedir):\n    file_list=[]\n    for root,dirs,files in os.walk(filedir): \n        for file in files:\n            file_list.append(os.path.join(root,file)) \n        for dir in dirs:\n            Traversal(dir)\n    return file_list\n\ndef is_img(path):\n    ext = os.path.splitext(path)[1]\n    ext = ext.lower()\n    if ext in [\'.jpg\',\'.png\',\'.jpeg\',\'.bmp\']:\n        return True\n    else:\n        return False\n\ndef is_video(path):\n    ext = os.path.splitext(path)[1]\n    ext = ext.lower()\n    if ext in [\'.mp4\',\'.flv\',\'.avi\',\'.mov\',\'.mkv\',\'.wmv\',\'.rmvb\',\'.mts\']:\n        return True\n    else:\n        return False\n\ndef is_imgs(paths):\n    tmp = []\n    for path in paths:\n        if is_img(path):\n            tmp.append(path)\n    return tmp\n\ndef is_videos(paths):\n    tmp = []\n    for path in paths:\n        if is_video(path):\n            tmp.append(path)\n    return tmp  \n\ndef is_dirs(paths):\n    tmp = []\n    for path in paths:\n        if os.path.isdir(path):\n            tmp.append(path)\n    return tmp  \n\ndef  writelog(path,log,isprint=False):\n    f = open(path,\'a+\')\n    f.write(log+\'\\n\')\n    f.close()\n    if isprint:\n        print(log)\n\ndef makedirs(path):\n    if os.path.isdir(path):\n        print(path,\'existed\')\n    else:\n        os.makedirs(path)\n        print(\'makedir:\',path)\n\ndef clean_tempfiles(tmp_init=True):\n    if os.path.isdir(\'./tmp\'):   \n        shutil.rmtree(\'./tmp\')\n    if tmp_init:\n        os.makedirs(\'./tmp\')\n        os.makedirs(\'./tmp/video2image\')\n        os.makedirs(\'./tmp/addmosaic_image\')\n        os.makedirs(\'./tmp/mosaic_crop\')\n        os.makedirs(\'./tmp/replace_mosaic\')\n        os.makedirs(\'./tmp/mosaic_mask\')\n        os.makedirs(\'./tmp/ROI_mask\')\n        os.makedirs(\'./tmp/ROI_mask_check\')\n        os.makedirs(\'./tmp/style_transfer\')\n\ndef file_init(opt):\n    if not os.path.isdir(opt.result_dir):\n        os.makedirs(opt.result_dir)\n        print(\'makedir:\',opt.result_dir)\n    clean_tempfiles(True)\n\ndef second2stamp(s):\n    h = int(s/3600)\n    s = int(s%3600)\n    m = int(s/60)\n    s = int(s%60)\n\n    return ""%02d:%02d:%02d"" % (h, m, s)\n\ndef get_bar(percent,num = 25):\n    bar = \'[\'\n    for i in range(num):\n        if i < round(percent/(100/num)):\n            bar += \'#\'\n        else:\n            bar += \'-\'\n    bar += \']\'\n    return bar+\' \'+str(round(percent,2))+\'%\'\n\ndef copyfile(src,dst):\n    try:\n        shutil.copyfile(src, dst)\n    except Exception as e:\n        print(e)\n\ndef opt2str(opt):\n    message = \'\'\n    message += \'---------------------- Options --------------------\\n\'\n    for k, v in sorted(vars(opt).items()):\n        message += \'{:>25}: {:<35}\\n\'.format(str(k), str(v))\n    message += \'----------------- End -------------------\'\n    return message\n'"
train/add/train.py,10,"b'import sys\nimport os\nimport random\nimport datetime\nimport time\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport cv2\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nfrom torch import optim\n\nsys.path.append("".."")\nsys.path.append(""../.."")\nfrom cores import Options\nfrom util import mosaic,util,ffmpeg,filt,data\nfrom util import image_processing as impro\nfrom models import unet_model,BiSeNet_model\n\n\n\'\'\'\n--------------------------Get options--------------------------\n\'\'\'\nopt = Options()\nopt.parser.add_argument(\'--gpu_id\',type=int,default=0, help=\'\')\nopt.parser.add_argument(\'--lr\',type=float,default=0.001, help=\'\')\nopt.parser.add_argument(\'--finesize\',type=int,default=360, help=\'\')\nopt.parser.add_argument(\'--loadsize\',type=int,default=400, help=\'\')\nopt.parser.add_argument(\'--batchsize\',type=int,default=8, help=\'\')\nopt.parser.add_argument(\'--model\',type=str,default=\'BiSeNet\', help=\'BiSeNet or UNet\')\n\nopt.parser.add_argument(\'--maxepoch\',type=int,default=100, help=\'\')\nopt.parser.add_argument(\'--savefreq\',type=int,default=5, help=\'\')\nopt.parser.add_argument(\'--maxload\',type=int,default=1000000, help=\'\')\nopt.parser.add_argument(\'--continuetrain\', action=\'store_true\', help=\'\')\nopt.parser.add_argument(\'--startepoch\',type=int,default=0, help=\'\')\nopt.parser.add_argument(\'--dataset\',type=str,default=\'./datasets/face/\', help=\'\')\nopt.parser.add_argument(\'--savename\',type=str,default=\'face\', help=\'\')\n\n\n\'\'\'\n--------------------------Init--------------------------\n\'\'\'\nopt = opt.getparse()\ndir_img = os.path.join(opt.dataset,\'origin_image\')\ndir_mask = os.path.join(opt.dataset,\'mask\')\ndir_checkpoint = os.path.join(\'checkpoints/\',opt.savename)\nutil.makedirs(dir_checkpoint)\nutil.writelog(os.path.join(dir_checkpoint,\'loss.txt\'), \n              str(time.asctime(time.localtime(time.time())))+\'\\n\'+util.opt2str(opt))\ntorch.cuda.set_device(opt.gpu_id)\n\ndef Totensor(img,use_gpu=True):\n    size=img.shape[0]\n    img = torch.from_numpy(img).float()\n    if opt.use_gpu:\n        img = img.cuda()\n    return img\n\ndef loadimage(imagepaths,maskpaths,opt,test_flag = False):\n    batchsize = len(imagepaths)\n    images = np.zeros((batchsize,3,opt.finesize,opt.finesize), dtype=np.float32)\n    masks = np.zeros((batchsize,1,opt.finesize,opt.finesize), dtype=np.float32)\n    for i in range(len(imagepaths)):\n        img = impro.resize(impro.imread(imagepaths[i]),opt.loadsize)\n        mask = impro.resize(impro.imread(maskpaths[i],mod = \'gray\'),opt.loadsize)      \n        img,mask = data.random_transform_image(img, mask, opt.finesize, test_flag)\n        images[i] = (img.transpose((2, 0, 1))/255.0)\n        masks[i] = (mask.reshape(1,1,opt.finesize,opt.finesize)/255.0)\n    images = Totensor(images,opt.use_gpu)\n    masks = Totensor(masks,opt.use_gpu)\n\n    return images,masks\n\n\n\'\'\'\n--------------------------checking dataset--------------------------\n\'\'\'\nprint(\'checking dataset...\')\nimagepaths = sorted(util.Traversal(dir_img))[:opt.maxload]\nmaskpaths = sorted(util.Traversal(dir_mask))[:opt.maxload]\ndata.shuffledata(imagepaths, maskpaths)\nif len(imagepaths) != len(maskpaths) :\n    print(\'dataset error!\')\n    exit(0)\nimg_num = len(imagepaths)\nprint(\'find images:\',img_num)\nimagepaths_train = (imagepaths[0:int(img_num*0.8)]).copy()\nmaskpaths_train = (maskpaths[0:int(img_num*0.8)]).copy()\nimagepaths_eval = (imagepaths[int(img_num*0.8):]).copy()\nmaskpaths_eval = (maskpaths[int(img_num*0.8):]).copy()\n\n\'\'\'\n--------------------------def network--------------------------\n\'\'\'\nif opt.model ==\'UNet\':\n    net = unet_model.UNet(n_channels = 3, n_classes = 1)\nelif opt.model ==\'BiSeNet\':\n    net = BiSeNet_model.BiSeNet(num_classes=1, context_path=\'resnet18\')\n\nif opt.continuetrain:\n    if not os.path.isfile(os.path.join(dir_checkpoint,\'last.pth\')):\n        opt.continuetrain = False\n        print(\'can not load last.pth, training on init weight.\')\nif opt.continuetrain:\n    net.load_state_dict(torch.load(os.path.join(dir_checkpoint,\'last.pth\')))\n    f = open(os.path.join(dir_checkpoint,\'epoch_log.txt\'),\'r\')\n    opt.startepoch = int(f.read())\n    f.close()\nif opt.use_gpu:\n    net.cuda()\n    cudnn.benchmark = True\n\noptimizer = torch.optim.Adam(net.parameters(), lr=opt.lr)\n\nif opt.model ==\'UNet\':\n    criterion = nn.BCELoss()\nelif opt.model ==\'BiSeNet\':\n    criterion = nn.BCELoss()\n    # criterion = BiSeNet_model.DiceLoss()\n\n\'\'\'\n--------------------------train--------------------------\n\'\'\'\nloss_plot = {\'train\':[],\'eval\':[]}\nprint(\'begin training......\')\nfor epoch in range(opt.startepoch,opt.maxepoch):\n    random_save = random.randint(0, int(img_num*0.8/opt.batchsize))\n    data.shuffledata(imagepaths_train, maskpaths_train)\n\n    starttime = datetime.datetime.now()\n    util.writelog(os.path.join(dir_checkpoint,\'loss.txt\'),\'Epoch {}/{}.\'.format(epoch + 1, opt.maxepoch),True)\n    net.train()\n    if opt.use_gpu:\n        net.cuda()\n    epoch_loss = 0\n    for i in range(int(img_num*0.8/opt.batchsize)):\n        img,mask = loadimage(imagepaths_train[i*opt.batchsize:(i+1)*opt.batchsize], maskpaths_train[i*opt.batchsize:(i+1)*opt.batchsize], opt)\n\n        if opt.model ==\'UNet\':\n            mask_pred = net(img)\n            loss = criterion(mask_pred, mask)\n            epoch_loss += loss.item()\n        elif opt.model ==\'BiSeNet\':\n            mask_pred, mask_pred_sup1, mask_pred_sup2 = net(img)\n            loss1 = criterion(mask_pred, mask)\n            loss2 = criterion(mask_pred_sup1, mask)\n            loss3 = criterion(mask_pred_sup2, mask)\n            loss = loss1 + loss2 + loss3\n            epoch_loss += loss1.item()\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if i%100 == 0:\n            data.showresult(img,mask,mask_pred,os.path.join(dir_checkpoint,\'result.png\'),True)\n        if  i == random_save:\n            data.showresult(img,mask,mask_pred,os.path.join(dir_checkpoint,\'epoch_\'+str(epoch+1)+\'.png\'),True)\n    epoch_loss = epoch_loss/int(img_num*0.8/opt.batchsize)\n    loss_plot[\'train\'].append(epoch_loss)\n\n    #val\n    epoch_loss_eval = 0\n    with torch.no_grad():\n    # net.eval()\n        for i in range(int(img_num*0.2/opt.batchsize)):\n            img,mask = loadimage(imagepaths_eval[i*opt.batchsize:(i+1)*opt.batchsize], maskpaths_eval[i*opt.batchsize:(i+1)*opt.batchsize], opt,test_flag=True)\n            if opt.model ==\'UNet\':\n                mask_pred = net(img)\n            elif opt.model ==\'BiSeNet\':\n                mask_pred, _, _ = net(img)\n            # mask_pred = net(img)\n            loss= criterion(mask_pred, mask)\n            epoch_loss_eval += loss.item()\n    epoch_loss_eval = epoch_loss_eval/int(img_num*0.2/opt.batchsize)\n    loss_plot[\'eval\'].append(epoch_loss_eval)\n    # torch.cuda.empty_cache()\n\n    #savelog\n    endtime = datetime.datetime.now()\n    util.writelog(os.path.join(dir_checkpoint,\'loss.txt\'),\n                \'--- Epoch train_loss: {0:.6f} eval_loss: {1:.6f} Cost time: {2:} s\'.format(\n                    epoch_loss,\n                    epoch_loss_eval,\n                    (endtime - starttime).seconds),\n                True)\n    #plot\n    plt.plot(np.linspace(opt.startepoch+1,epoch+1,epoch+1-opt.startepoch),loss_plot[\'train\'],label=\'train\')\n    plt.plot(np.linspace(opt.startepoch+1,epoch+1,epoch+1-opt.startepoch),loss_plot[\'eval\'],label=\'eval\')\n    plt.xlabel(\'Epoch\')\n    plt.ylabel(\'BCELoss\')\n    plt.legend(loc=1)\n    plt.savefig(os.path.join(dir_checkpoint,\'loss.jpg\'))\n    plt.close()\n    #save network\n    torch.save(net.cpu().state_dict(),os.path.join(dir_checkpoint,\'last.pth\'))\n    f = open(os.path.join(dir_checkpoint,\'epoch_log.txt\'),\'w+\')\n    f.write(str(epoch+1))\n    f.close()\n    if (epoch+1)%opt.savefreq == 0:\n        torch.save(net.cpu().state_dict(),os.path.join(dir_checkpoint,\'epoch\'+str(epoch+1)+\'.pth\'))\n        print(\'network saved.\')\n'"
train/clean/train.py,15,"b'import os\nimport numpy as np\nimport cv2\nimport random\nimport torch\nimport torch.nn as nn\nimport time\n\nimport sys\nsys.path.append("".."")\nsys.path.append(""../.."")\nfrom util import mosaic,util,ffmpeg,filt,data\nfrom util import image_processing as impro\nfrom cores import Options\nfrom models import pix2pix_model,pix2pixHD_model,video_model,unet_model,loadmodel,videoHD_model\nfrom matplotlib import pyplot as plt\nimport torch.backends.cudnn as cudnn\n\n\'\'\'\n--------------------------Get options--------------------------\n\'\'\'\n\nopt = Options()\nopt.parser.add_argument(\'--gpu_id\',type=int,default=0, help=\'\')\nopt.parser.add_argument(\'--N\',type=int,default=25, help=\'\')\nopt.parser.add_argument(\'--lr\',type=float,default=0.0002, help=\'\')\nopt.parser.add_argument(\'--beta1\',type=float,default=0.5, help=\'\')\nopt.parser.add_argument(\'--gan\', action=\'store_true\', help=\'if specified, use gan\')\nopt.parser.add_argument(\'--l2\', action=\'store_true\', help=\'if specified, use L2 loss\')\nopt.parser.add_argument(\'--hd\', action=\'store_true\', help=\'if specified, use HD model\')\nopt.parser.add_argument(\'--lambda_L1\',type=float,default=100, help=\'\')\nopt.parser.add_argument(\'--lambda_gan\',type=float,default=1, help=\'\')\nopt.parser.add_argument(\'--finesize\',type=int,default=256, help=\'\')\nopt.parser.add_argument(\'--loadsize\',type=int,default=286, help=\'\')\nopt.parser.add_argument(\'--batchsize\',type=int,default=1, help=\'\')\nopt.parser.add_argument(\'--perload_num\',type=int,default=16, help=\'number of images pool\')\nopt.parser.add_argument(\'--norm\',type=str,default=\'instance\', help=\'\')\n\nopt.parser.add_argument(\'--dataset\',type=str,default=\'./datasets/face/\', help=\'\')\nopt.parser.add_argument(\'--maxiter\',type=int,default=10000000, help=\'\')\nopt.parser.add_argument(\'--savefreq\',type=int,default=10000, help=\'\')\nopt.parser.add_argument(\'--startiter\',type=int,default=0, help=\'\')\nopt.parser.add_argument(\'--continuetrain\', action=\'store_true\', help=\'\')\nopt.parser.add_argument(\'--savename\',type=str,default=\'face\', help=\'\')\n\n\n\'\'\'\n--------------------------Init--------------------------\n\'\'\'\nopt = opt.getparse()\ndir_checkpoint = os.path.join(\'checkpoints/\',opt.savename)\nutil.makedirs(dir_checkpoint)\nutil.writelog(os.path.join(dir_checkpoint,\'loss.txt\'), \n              str(time.asctime(time.localtime(time.time())))+\'\\n\'+util.opt2str(opt))\ntorch.cuda.set_device(opt.gpu_id)\n\nN = opt.N\nloss_sum = [0.,0.,0.,0.]\nloss_plot = [[],[]]\nitem_plot = []\n\n# list video dir \nvideonames = os.listdir(opt.dataset)\nvideonames.sort()\nlengths = [];tmp = []\nprint(\'Check dataset...\')\nfor video in videonames:\n    if video != \'opt.txt\':\n        video_images = os.listdir(os.path.join(opt.dataset,video,\'origin_image\'))\n        lengths.append(len(video_images))\n        tmp.append(video)\nvideonames = tmp\nvideo_num = len(videonames)\n#def network\nprint(\'Init network...\')\nif opt.hd:\n    netG = videoHD_model.MosaicNet(3*N+1, 3, norm=opt.norm)\nelse:\n    netG = video_model.MosaicNet(3*N+1, 3, norm=opt.norm)\nloadmodel.show_paramsnumber(netG,\'netG\')\n\nif opt.gan:\n    if opt.hd:\n        #netD = pix2pixHD_model.define_D(6, 64, 3, norm = opt.norm, use_sigmoid=False, num_D=1)\n        netD = pix2pixHD_model.define_D(6, 64, 3, norm = opt.norm, use_sigmoid=False, num_D=2,getIntermFeat=True)    \n    else:\n        netD = pix2pix_model.define_D(3*2, 64, \'basic\', norm = opt.norm)\n    netD.train()\n\nif opt.continuetrain:\n    if not os.path.isfile(os.path.join(dir_checkpoint,\'last_G.pth\')):\n        opt.continuetrain = False\n        print(\'can not load last_G, training on init weight.\')\nif opt.continuetrain:     \n    netG.load_state_dict(torch.load(os.path.join(dir_checkpoint,\'last_G.pth\')))\n    if opt.gan:\n        netD.load_state_dict(torch.load(os.path.join(dir_checkpoint,\'last_D.pth\')))\n    f = open(os.path.join(dir_checkpoint,\'iter\'),\'r\')\n    opt.startiter = int(f.read())\n    f.close()\n\noptimizer_G = torch.optim.Adam(netG.parameters(), lr=opt.lr,betas=(opt.beta1, 0.999))\ncriterion_L1 = nn.L1Loss()\ncriterion_L2 = nn.MSELoss()\nif opt.gan:\n    optimizer_D = torch.optim.Adam(netD.parameters(), lr=opt.lr,betas=(opt.beta1, 0.999))\n    if opt.hd:\n        criterionGAN = pix2pixHD_model.GANLoss(tensor=torch.cuda.FloatTensor)\n    else:\n        criterionGAN = pix2pix_model.GANLoss(gan_mode=\'lsgan\').cuda()   \n\nif opt.use_gpu:\n    netG.cuda()\n    if opt.gan:\n        netD.cuda()\n        criterionGAN.cuda()\n    cudnn.benchmark = True\n\n\'\'\'\n--------------------------preload data & data pool--------------------------\n\'\'\'\ndef loaddata(video_index):\n    \n    videoname = videonames[video_index]\n    img_index = random.randint(int(N/2)+1,lengths[video_index]- int(N/2)-1)\n    \n    input_img = np.zeros((opt.loadsize,opt.loadsize,3*N+1), dtype=\'uint8\')\n    # this frame\n    this_mask = impro.imread(os.path.join(opt.dataset,videoname,\'mask\',\'%05d\'%(img_index)+\'.png\'),\'gray\',loadsize=opt.loadsize)\n    input_img[:,:,-1] = this_mask\n    #print(os.path.join(opt.dataset,videoname,\'origin_image\',\'%05d\'%(img_index)+\'.jpg\'))\n    ground_true =  impro.imread(os.path.join(opt.dataset,videoname,\'origin_image\',\'%05d\'%(img_index)+\'.jpg\'),loadsize=opt.loadsize)\n    mosaic_size,mod,rect_rat,father = mosaic.get_random_parameter(ground_true,this_mask)\n    # merge other frame\n    for i in range(0,N):\n        img =  impro.imread(os.path.join(opt.dataset,videoname,\'origin_image\',\'%05d\'%(img_index+i-int(N/2))+\'.jpg\'),loadsize=opt.loadsize)\n        mask = impro.imread(os.path.join(opt.dataset,videoname,\'mask\',\'%05d\'%(img_index+i-int(N/2))+\'.png\'),\'gray\',loadsize=opt.loadsize)\n        img_mosaic = mosaic.addmosaic_base(img, mask, mosaic_size,model = mod,rect_rat=rect_rat,father=father)\n        input_img[:,:,i*3:(i+1)*3] = img_mosaic\n    # to tensor\n    input_img,ground_true = data.random_transform_video(input_img,ground_true,opt.finesize,N)\n    input_img = data.im2tensor(input_img,bgr2rgb=False,use_gpu=opt.use_gpu,use_transform = False,is0_1=False)\n    ground_true = data.im2tensor(ground_true,bgr2rgb=False,use_gpu=opt.use_gpu,use_transform = False,is0_1=False)\n    \n    return input_img,ground_true\n\nprint(\'Preloading data, please wait...\')\n\nif opt.perload_num <= opt.batchsize:\n    opt.perload_num = opt.batchsize*2\n#data pool\ninput_imgs = torch.rand(opt.perload_num,N*3+1,opt.finesize,opt.finesize).cuda()\nground_trues = torch.rand(opt.perload_num,3,opt.finesize,opt.finesize).cuda()\nload_cnt = 0\n\ndef preload():\n    global load_cnt   \n    while 1:\n        try:\n            video_index = random.randint(0,video_num-1)\n            ran = random.randint(0, opt.perload_num-1)\n            input_imgs[ran],ground_trues[ran] = loaddata(video_index)\n            load_cnt += 1\n            # time.sleep(0.1)\n        except Exception as e:\n            print(""error:"",e)\nimport threading\nt = threading.Thread(target=preload,args=()) \nt.daemon = True\nt.start()\ntime_start=time.time()\nwhile load_cnt < opt.perload_num:\n    time.sleep(0.1)\ntime_end=time.time()\nprint(\'load speed:\',round((time_end-time_start)/opt.perload_num,3),\'s/it\')\n\n\'\'\'\n--------------------------train--------------------------\n\'\'\'\nutil.copyfile(\'./train.py\', os.path.join(dir_checkpoint,\'train.py\'))\nutil.copyfile(\'../../models/videoHD_model.py\', os.path.join(dir_checkpoint,\'model.py\'))\nnetG.train()\ntime_start=time.time()\nprint(""Begin training..."")\nfor iter in range(opt.startiter+1,opt.maxiter):\n\n    ran = random.randint(0, opt.perload_num-opt.batchsize-1)\n    inputdata = input_imgs[ran:ran+opt.batchsize].clone()\n    target = ground_trues[ran:ran+opt.batchsize].clone()\n\n    if opt.gan:\n        # compute fake images: G(A)\n        pred = netG(inputdata)\n        # update D\n        pix2pix_model.set_requires_grad(netD,True)\n        optimizer_D.zero_grad()\n        # Fake\n        real_A = inputdata[:,int((N-1)/2)*3:(int((N-1)/2)+1)*3,:,:]\n        fake_AB = torch.cat((real_A, pred), 1)\n        pred_fake = netD(fake_AB.detach())\n        loss_D_fake = criterionGAN(pred_fake, False)\n        # Real\n        real_AB = torch.cat((real_A, target), 1)\n        pred_real = netD(real_AB)\n        loss_D_real = criterionGAN(pred_real, True)\n        # combine loss and calculate gradients\n        loss_D = (loss_D_fake + loss_D_real) * 0.5\n        loss_sum[2] += loss_D_fake.item()\n        loss_sum[3] += loss_D_real.item()\n        # udpate D\'s weights\n        loss_D.backward()\n        optimizer_D.step()\n\n        # update G\n        pix2pix_model.set_requires_grad(netD,False)\n        optimizer_G.zero_grad()\n        # First, G(A) should fake the discriminator\n        real_A = inputdata[:,int((N-1)/2)*3:(int((N-1)/2)+1)*3,:,:]\n        fake_AB = torch.cat((real_A, pred), 1)\n        pred_fake = netD(fake_AB)\n        loss_G_GAN = criterionGAN(pred_fake, True)*opt.lambda_gan\n        # Second, G(A) = B\n        if opt.l2:\n            loss_G_L1 = (criterion_L1(pred, target)+criterion_L2(pred, target)) * opt.lambda_L1\n        else:\n            loss_G_L1 = criterion_L1(pred, target) * opt.lambda_L1\n        # combine loss and calculate gradients\n        loss_G = loss_G_GAN + loss_G_L1\n        loss_sum[0] += loss_G_L1.item()\n        loss_sum[1] += loss_G_GAN.item()\n        # udpate G\'s weights\n        loss_G.backward()\n        optimizer_G.step()\n\n    else:\n        pred = netG(inputdata)\n        if opt.l2:\n            loss_G_L1 = (criterion_L1(pred, target)+criterion_L2(pred, target)) * opt.lambda_L1\n        else:\n            loss_G_L1 = criterion_L1(pred, target) * opt.lambda_L1\n        loss_sum[0] += loss_G_L1.item()\n\n        optimizer_G.zero_grad()\n        loss_G_L1.backward()\n        optimizer_G.step()\n\n    if (iter+1)%100 == 0:\n        try:\n            data.showresult(inputdata[:,int((N-1)/2)*3:(int((N-1)/2)+1)*3,:,:],\n             target, pred,os.path.join(dir_checkpoint,\'result_train.jpg\'))\n        except Exception as e:\n            print(e)\n    # plot\n    if (iter+1)%1000 == 0:\n        time_end = time.time()\n        if opt.gan:\n            savestr =\'iter:{0:d} L1_loss:{1:.4f} G_loss:{2:.4f} D_f:{3:.4f} D_r:{4:.4f} time:{5:.2f}\'.format(\n                iter+1,loss_sum[0]/1000,loss_sum[1]/1000,loss_sum[2]/1000,loss_sum[3]/1000,(time_end-time_start)/1000)\n            util.writelog(os.path.join(dir_checkpoint,\'loss.txt\'), savestr,True)\n            if (iter+1)/1000 >= 10:\n                loss_plot[0].append(loss_sum[0]/1000)\n                loss_plot[1].append(loss_sum[1]/1000)\n                item_plot.append(iter+1)\n                try:\n                    plt.plot(item_plot,loss_plot[0])\n                    plt.plot(item_plot,loss_plot[1])\n                    plt.savefig(os.path.join(dir_checkpoint,\'loss.jpg\'))\n                    plt.close()\n                except Exception as e:\n                    print(""error:"",e)\n        else:\n            savestr =\'iter:{0:d}  L1_loss:{1:.4f}  time:{2:.2f}\'.format(iter+1,loss_sum[0]/1000,(time_end-time_start)/1000)\n            util.writelog(os.path.join(dir_checkpoint,\'loss.txt\'), savestr,True)\n            if (iter+1)/1000 >= 10:\n                loss_plot[0].append(loss_sum[0]/1000)\n                item_plot.append(iter+1)\n                try:\n                    plt.plot(item_plot,loss_plot[0])\n                    plt.savefig(os.path.join(dir_checkpoint,\'loss.jpg\'))\n                    plt.close()\n                except Exception as e:\n                    print(""error:"",e)\n        loss_sum = [0.,0.,0.,0.]\n        time_start=time.time()\n\n    # save network\n    if (iter+1)%opt.savefreq == 0:\n        if iter+1 != opt.savefreq:\n            os.rename(os.path.join(dir_checkpoint,\'last_G.pth\'),os.path.join(dir_checkpoint,str(iter+1-opt.savefreq)+\'G.pth\'))\n        torch.save(netG.cpu().state_dict(),os.path.join(dir_checkpoint,\'last_G.pth\'))\n        if opt.gan:\n            if iter+1 != opt.savefreq:\n                os.rename(os.path.join(dir_checkpoint,\'last_D.pth\'),os.path.join(dir_checkpoint,str(iter+1-opt.savefreq)+\'D.pth\'))\n            torch.save(netD.cpu().state_dict(),os.path.join(dir_checkpoint,\'last_D.pth\'))\n        if opt.use_gpu:\n            netG.cuda()\n            if opt.gan:\n                netD.cuda()\n        f = open(os.path.join(dir_checkpoint,\'iter\'),\'w+\')\n        f.write(str(iter+1))\n        f.close()\n        print(\'network saved.\')\n\n        #test\n        if os.path.isdir(\'./test\'):  \n            netG.eval()\n            \n            test_names = os.listdir(\'./test\')\n            test_names.sort()\n            result = np.zeros((opt.finesize*2,opt.finesize*len(test_names),3), dtype=\'uint8\')\n\n            for cnt,test_name in enumerate(test_names,0):\n                img_names = os.listdir(os.path.join(\'./test\',test_name,\'image\'))\n                img_names.sort()\n                inputdata = np.zeros((opt.finesize,opt.finesize,3*N+1), dtype=\'uint8\')\n                for i in range(0,N):\n                    img = impro.imread(os.path.join(\'./test\',test_name,\'image\',img_names[i]))\n                    img = impro.resize(img,opt.finesize)\n                    inputdata[:,:,i*3:(i+1)*3] = img\n\n                mask = impro.imread(os.path.join(\'./test\',test_name,\'mask.png\'),\'gray\')\n                mask = impro.resize(mask,opt.finesize)\n                mask = impro.mask_threshold(mask,15,128)\n                inputdata[:,:,-1] = mask\n                result[0:opt.finesize,opt.finesize*cnt:opt.finesize*(cnt+1),:] = inputdata[:,:,int((N-1)/2)*3:(int((N-1)/2)+1)*3]\n                inputdata = data.im2tensor(inputdata,bgr2rgb=False,use_gpu=opt.use_gpu,use_transform = False,is0_1 = False)\n                pred = netG(inputdata)\n     \n                pred = data.tensor2im(pred,rgb2bgr = False, is0_1 = False)\n                result[opt.finesize:opt.finesize*2,opt.finesize*cnt:opt.finesize*(cnt+1),:] = pred\n\n            cv2.imwrite(os.path.join(dir_checkpoint,str(iter+1)+\'_test.jpg\'), result)\n            netG.train()\n'"
