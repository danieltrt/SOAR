file_path,api_count,code
etc/converter/convert-to-FfDL.py,0,"b'#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport json\nfrom ruamel.yaml import YAML\nimport sys\nimport getopt\n\n#global variables\nlanguage = \'\'\ninputfile = \'\'\noutputfile = \'\'\nsamplefile = \'\'\nyaml=YAML(typ=\'safe\')\n\ndef getFfDL():\n\ttry:\n\t\tf = open(inputfile, ""r"")\n\t\tresponse = f.read()\n\t\tdata = yaml.load(response)\n\t\tf.close()\n\t\treturn data\n\texcept:\n\t\tprint(""Missing {}"".format(inputfile))\n\ndef getSampleJob():\n\ttry:\n\t\tif samplefile:\n\t\t\tf = open(samplefile,""r"")\n\t\telse:\n\t\t\tf = open(""sample-FfDL.yaml"",""r"")\n\t\tresponse = f.read()\n\t\tresYaml = yaml.load(response)\n\t\tf.close()\n\t\treturn resYaml\n\texcept:\n\t\tprint(""Missing sample-FfDL.yaml"")\n\ndef createJob(sample,data):\n\ttry:\n\t\tsample[\'framework\'][\'name\'] = data[\'model_definition\'][\'framework\'][\'name\']\n\t\tsample[\'name\'] = data[\'model_definition\'][\'name\']\n\t\tsample[\'description\'] = data[\'model_definition\'][\'description\']\n\t\tsample[\'framework\'][\'command\'] = data[\'model_definition\'][\'execution\'][\'command\']\n\t\tsample[\'data_stores\'][0][\'id\'] = data[\'training_data_reference\'][\'name\']\n\t\tsample[\'data_stores\'][0][\'connection\'][\'auth_url\'] = data[\'training_data_reference\'][\'connection\'][\'endpoint_url\']\n\t\tsample[\'data_stores\'][0][\'connection\'][\'user_name\'] = data[\'training_data_reference\'][\'connection\'][\'access_key_id\']\n\t\tsample[\'data_stores\'][0][\'connection\'][\'password\'] = data[\'training_data_reference\'][\'connection\'][\'secret_access_key\']\n\t\tsample[\'data_stores\'][0][\'training_data\'][\'container\'] = data[\'training_data_reference\'][\'source\'][\'bucket\']\n\t\tsample[\'data_stores\'][0][\'training_results\'][\'container\'] = data[\'training_results_reference\'][\'target\'][\'bucket\']\n\t\tpy2 = False\n\t\tCPU = False\n\t\ttry:\n\t\t\tif data[\'model_definition\'][\'framework\'][\'name\'] == \'tensorflow\':\n\t\t\t\tif \'2.\' in data[\'model_definition\'][\'framework\'][\'runtimes\'][\'version\']:\n\t\t\t\t\tpy2 = True\n\t\texcept:\n\t\t\tpy2 = False\n\n\t\ttry:\n\t\t\tsample[\'learners\'] = int(data[\'model_definition\'][\'execution\'][\'compute_configuration\'][\'nodes\'])\n\t\texcept:\n\t\t\tsample[\'learners\'] =  1\n\n\t\t# Detect T-shirt requirements\n\t\tif data[\'model_definition\'][\'execution\'][\'compute_configuration\'][\'name\'] == ""k80"":\n\t\t\tsample[\'cpus\'] = 4\n\t\t\tsample[\'gpus\'] = 1\n\t\t\tsample[\'memory\'] = \'24Gb\'\n\t\telif data[\'model_definition\'][\'execution\'][\'compute_configuration\'][\'name\'] == ""p100"":\n\t\t\tsample[\'cpus\'] = 8\n\t\t\tsample[\'gpus\'] = 1\n\t\t\tsample[\'memory\'] = \'24Gb\'\n\t\telif data[\'model_definition\'][\'execution\'][\'compute_configuration\'][\'name\'] == ""v100"":\n\t\t\tsample[\'cpus\'] = 26\n\t\t\tsample[\'gpus\'] = 1\n\t\t\tsample[\'memory\'] = \'24Gb\'\n\t\telif data[\'model_definition\'][\'execution\'][\'compute_configuration\'][\'name\'] == ""k80x2"":\n\t\t\tsample[\'cpus\'] = 8\n\t\t\tsample[\'gpus\'] = 2\n\t\t\tsample[\'memory\'] = \'48Gb\'\n\t\telif data[\'model_definition\'][\'execution\'][\'compute_configuration\'][\'name\'] == ""p100x2"":\n\t\t\tsample[\'cpus\'] = 16\n\t\t\tsample[\'gpus\'] = 2\n\t\t\tsample[\'memory\'] = \'48Gb\'\n\t\telif data[\'model_definition\'][\'execution\'][\'compute_configuration\'][\'name\'] == ""v100x2"":\n\t\t\tsample[\'cpus\'] = 52\n\t\t\tsample[\'gpus\'] = 2\n\t\t\tsample[\'memory\'] = \'48Gb\'\n\t\telif data[\'model_definition\'][\'execution\'][\'compute_configuration\'][\'name\'] == ""k80x4"":\n\t\t\tsample[\'cpus\'] = 16\n\t\t\tsample[\'gpus\'] = 4\n\t\t\tsample[\'memory\'] = \'96Gb\'\n\t\telse:\n\t\t\tCPU = True\n\t\t\tsample[\'cpus\'] = 1\n\t\t\tsample[\'gpus\'] = 0\n\t\t\tsample[\'memory\'] = \'1Gb\'\n\n\t\t# Detect Framework version\n\t\ttry:\n\t\t\tif data[\'model_definition\'][\'framework\'][\'name\'] == \'tensorflow\':\n\t\t\t\tif \'1.3\' in data[\'model_definition\'][\'framework\'][\'version\']:\n\t\t\t\t\tif py2:\n\t\t\t\t\t\tif CPU:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.3.0""\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.3.0-gpu""\n\t\t\t\t\telse:\n\t\t\t\t\t\tif CPU:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.3.0-py3""\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.3.0-gpu-py3""\n\t\t\t\telif \'1.4\' in data[\'model_definition\'][\'framework\'][\'version\']:\n\t\t\t\t\tif py2:\n\t\t\t\t\t\tif CPU:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.4.0""\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.4.0-gpu""\n\t\t\t\t\telse:\n\t\t\t\t\t\tif CPU:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.4.0-py3""\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.4.0-gpu-py3""\n\t\t\t\telif \'1.5\' in data[\'model_definition\'][\'framework\'][\'version\']:\n\t\t\t\t\tif py2:\n\t\t\t\t\t\tif CPU:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.5.0""\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.5.0-gpu""\n\t\t\t\t\telse:\n\t\t\t\t\t\tif CPU:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.5.0-py3""\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.5.0-gpu-py3""\n\t\t\t\telif \'1.6\' in data[\'model_definition\'][\'framework\'][\'version\']:\n\t\t\t\t\tif py2:\n\t\t\t\t\t\tif CPU:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.6.0""\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.6.0-gpu""\n\t\t\t\t\telse:\n\t\t\t\t\t\tif CPU:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.6.0-py3""\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.6.0-gpu-py3""\n\t\t\t\telif \'1.7\' in data[\'model_definition\'][\'framework\'][\'version\']:\n\t\t\t\t\tif py2:\n\t\t\t\t\t\tif CPU:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.7.0""\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.7.0-gpu""\n\t\t\t\t\telse:\n\t\t\t\t\t\tif CPU:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.7.0-py3""\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""1.7.0-gpu-py3""\n\t\t\t\telse:\n\t\t\t\t\tif py2:\n\t\t\t\t\t\tif CPU:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""latest""\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""latest-gpu""\n\t\t\t\t\telse:\n\t\t\t\t\t\tif CPU:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""latest-py3""\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsample[\'framework\'][\'version\'] = ""latest-gpu-py3""\n\t\t\telif data[\'model_definition\'][\'framework\'][\'name\'] == \'caffe\':\n\t\t\t\tif CPU:\n\t\t\t\t\tsample[\'framework\'][\'version\'] = ""cpu""\n\t\t\t\telse:\n\t\t\t\t\tsample[\'framework\'][\'version\'] = ""gpu""\n\t\t\telif data[\'model_definition\'][\'framework\'][\'name\'] == \'pytorch\':\n\t\t\t\tsample[\'framework\'][\'version\'] = ""latest""\n\t\texcept:\n\t\t\tprint(""Wrong framework.version contents in {}"".format(inputfile))\n\n\t\tif data[\'model_definition\'][\'framework\'][\'name\'] != ""tensorflow"":\n\t\t\tsample.pop(\'evaluation_metrics\', None)\n\texcept:\n\t\tprint(""Missing contents in {}"".format(inputfile))\n\ttry:\n\t\tif outputfile:\n\t\t\tf = open(outputfile, ""w"")\n\t\telse:\n\t\t\tf = open(""manifest-FfDL.yaml"", ""w"")\n\t\tyaml.default_flow_style = False\n\t\tyaml.dump(sample, f)\n\t\tf.close()\n\texcept:\n\t\tif outputfile:\n\t\t\tprint(""Cannot write contents to {}"".format(outputfile))\n\t\telse:\n\t\t\tprint(""Cannot write contents to manifest-FfDL.yaml."")\n\n\nif __name__ == ""__main__"":\n\targv = sys.argv[1:]\n\ttry:\n\t  opts, args = getopt.getopt(argv,""i:o:s:"",[""ifile="",""ofile="",""sfile=""])\n\texcept getopt.GetoptError:\n\t  print(\'Format Error: Wrong format.\')\n\t  print(\'convert-to-FfDL.py -i <inputfile> -o <outputfile> -s <samplefile>\')\n\t  sys.exit(2)\n\tfor opt, arg in opts:\n\t\tif opt in (""-i"", ""--ifile""):\n\t\t inputfile = arg\n\t\telif opt in (""-o"", ""--ofile""):\n\t\t outputfile = arg\n\t\telif opt in (""-s"", ""--sfile""):\n\t\t samplefile = arg\n\tif not inputfile:\n\t  print(\'Input Error: inputfile cannot be empty.\')\n\t  print(\'convert-to-FfDL.py -i <inputfile> -o <outputfile> -s <samplefile>\')\n\t  sys.exit(2)\n\tdata = getFfDL()\n\tsample = getSampleJob()\n\tcreateJob(sample,data)\n'"
etc/converter/convert-to-WML.py,0,"b'#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport json\nfrom ruamel.yaml import YAML\nimport sys\nimport getopt\n\n#global variables\nlanguage = \'\'\ninputfile = \'\'\noutputfile = \'\'\nsamplefile = \'\'\nyaml=YAML(typ=\'safe\')\n\ndef getFfDL():\n\ttry:\n\t\tf = open(inputfile, ""r"")\n\t\tresponse = f.read()\n\t\tdata = yaml.load(response)\n\t\tf.close()\n\t\treturn data\n\texcept:\n\t\tprint(""Missing {}"".format(inputfile))\n\ndef getSampleJob():\n\ttry:\n\t\tif samplefile:\n\t\t\tf = open(samplefile,""r"")\n\t\telse:\n\t\t\tf = open(""sample-WML.yaml"",""r"")\n\t\tresponse = f.read()\n\t\tresYaml = yaml.load(response)\n\t\tf.close()\n\t\treturn resYaml\n\texcept:\n\t\tprint(""Missing sample-WML.yaml"")\n\ndef createJob(sample,data):\n\ttry:\n\t\tsample[\'model_definition\'][\'framework\'][\'name\'] = data[\'framework\'][\'name\']\n\t\tsample[\'model_definition\'][\'name\'] = data[\'name\']\n\t\tsample[\'model_definition\'][\'description\'] = data[\'description\']\n\t\tsample[\'model_definition\'][\'execution\'][\'command\'] = data[\'framework\'][\'command\']\n\t\tsample[\'training_data_reference\'][\'name\'] = data[\'data_stores\'][0][\'id\']\n\t\tsample[\'training_data_reference\'][\'connection\'][\'endpoint_url\'] = data[\'data_stores\'][0][\'connection\'][\'auth_url\']\n\t\tsample[\'training_data_reference\'][\'connection\'][\'access_key_id\'] = data[\'data_stores\'][0][\'connection\'][\'user_name\']\n\t\tsample[\'training_data_reference\'][\'connection\'][\'secret_access_key\'] = data[\'data_stores\'][0][\'connection\'][\'password\']\n\t\tsample[\'training_data_reference\'][\'source\'][\'bucket\'] = data[\'data_stores\'][0][\'training_data\'][\'container\']\n\t\tsample[\'training_results_reference\'][\'name\'] = \'\'.join([data[\'data_stores\'][0][\'id\'],""-results""])\n\t\tsample[\'training_results_reference\'][\'connection\'][\'endpoint_url\'] = data[\'data_stores\'][0][\'connection\'][\'auth_url\']\n\t\tsample[\'training_results_reference\'][\'connection\'][\'access_key_id\'] = data[\'data_stores\'][0][\'connection\'][\'user_name\']\n\t\tsample[\'training_results_reference\'][\'connection\'][\'secret_access_key\'] = data[\'data_stores\'][0][\'connection\'][\'password\']\n\t\tsample[\'training_results_reference\'][\'target\'][\'bucket\'] = data[\'data_stores\'][0][\'training_results\'][\'container\']\n\t\tif data[\'framework\'][\'name\'] == \'tensorflow\':\n\t\t\tif \'py3\' in data[\'framework\'][\'version\']:\n\t\t\t\tsample[\'model_definition\'][\'framework\'][\'runtimes\'][\'version\'] = ""3.5""\n\t\t\telse:\n\t\t\t\tsample[\'model_definition\'][\'framework\'][\'runtimes\'][\'version\'] = ""2.7""\n\t\telse:\n\t\t\tsample[\'model_definition\'][\'framework\'][\'runtimes\'][\'version\'] = ""3.5""\n\t\tsample[\'model_definition\'][\'execution\'][\'compute_configuration\'][\'nodes\'] = int(data[\'learners\']) if data.get(""learners"") else 1\n\t\tgpus = int(data[\'gpus\']) if data.get(""gpus"") else 0\n\t\tif gpus <= 1:\n\t\t\tif int(data[\'cpus\']) <= 4:\n\t\t\t\tsample[\'model_definition\'][\'execution\'][\'compute_configuration\'][\'name\'] = ""k80""\n\t\t\telif int(data[\'cpus\']) <= 8:\n\t\t\t\tsample[\'model_definition\'][\'execution\'][\'compute_configuration\'][\'name\'] = ""p100""\n\t\t\telse:\n\t\t\t\tsample[\'model_definition\'][\'execution\'][\'compute_configuration\'][\'name\'] = ""v100""\n\t\telif gpus <= 2:\n\t\t\tif int(data[\'cpus\']) <= 8:\n\t\t\t\tsample[\'model_definition\'][\'execution\'][\'compute_configuration\'][\'name\'] = ""k80x2""\n\t\t\telif int(data[\'cpus\']) <= 16:\n\t\t\t\tsample[\'model_definition\'][\'execution\'][\'compute_configuration\'][\'name\'] = ""p100x2""\n\t\t\telse:\n\t\t\t\tsample[\'model_definition\'][\'execution\'][\'compute_configuration\'][\'name\'] = ""v100x2""\n\t\telse:\n\t\t\tsample[\'model_definition\'][\'execution\'][\'compute_configuration\'][\'name\'] = ""k80x4""\n\texcept:\n\t\tprint(""Missing contents in {}"".format(inputfile))\n\ttry:\n\t\tif data[\'framework\'][\'name\'] == \'tensorflow\':\n\t\t\tif \'1.3\' in data[\'framework\'][\'version\']:\n\t\t\t\tsample[\'model_definition\'][\'framework\'][\'version\'] = ""1.3""\n\t\t\telif \'1.4\' in data[\'framework\'][\'version\']:\n\t\t\t\tsample[\'model_definition\'][\'framework\'][\'version\'] = ""1.4""\n\t\t\telif \'1.5\' in data[\'framework\'][\'version\']:\n\t\t\t\tsample[\'model_definition\'][\'framework\'][\'version\'] = ""1.5""\n\t\t\telse:\n\t\t\t\tsample[\'model_definition\'][\'framework\'][\'version\'] = ""1.5""\n\t\telif data[\'framework\'][\'name\'] == \'caffe\':\n\t\t\tsample[\'model_definition\'][\'framework\'][\'version\'] = ""1.0""\n\t\telif data[\'framework\'][\'name\'] == \'pytorch\':\n\t\t\tsample[\'model_definition\'][\'framework\'][\'version\'] = ""0.3""\n\texcept:\n\t\tprint(""Wrong framework.version contents in {}"".format(inputfile))\n\ttry:\n\t\tif outputfile:\n\t\t\tf = open(outputfile, ""w"")\n\t\telse:\n\t\t\tf = open(""manifest-WML.yaml"", ""w"")\n\t\tyaml.default_flow_style = False\n\t\tyaml.dump(sample, f)\n\t\tf.close()\n\texcept:\n\t\tif outputfile:\n\t\t\tprint(""Cannot write contents to {}"".format(outputfile))\n\t\telse:\n\t\t\tprint(""Cannot write contents to manifest-WML.yaml."")\n\n\nif __name__ == ""__main__"":\n\targv = sys.argv[1:]\n\ttry:\n\t  opts, args = getopt.getopt(argv,""i:o:s:"",[""ifile="",""ofile="",""sfile=""])\n\texcept getopt.GetoptError:\n\t  print(\'Format Error: Wrong format.\')\n\t  print(\'convert-to-WML.py -i <inputfile> -o <outputfile> -s <samplefile>\')\n\t  sys.exit(2)\n\tfor opt, arg in opts:\n\t\tif opt in (""-i"", ""--ifile""):\n\t\t inputfile = arg\n\t\telif opt in (""-o"", ""--ofile""):\n\t\t outputfile = arg\n\t\telif opt in (""-s"", ""--sfile""):\n\t\t samplefile = arg\n\tif not inputfile:\n\t  print(\'Input Error: inputfile cannot be empty.\')\n\t  print(\'convert-to-WML.py -i <inputfile> -o <outputfile> -s <samplefile>\')\n\t  sys.exit(2)\n\tdata = getFfDL()\n\tsample = getSampleJob()\n\tcreateJob(sample,data)\n'"
metrics/log_collectors/__init__.py,0,b'from .training_data_service_client import *\nfrom .regex_extractor import extract_from_log\nfrom .tensorboard import extract_tb\n'
community/FfDL-H2Oai/h2o-model/h2o3_baseline.py,0,"b'#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport h2o\nfrom h2o.automl import H2OAutoML\nimport os\nimport sys\nimport socket\nimport time\n\n\ntrain_data_file = None\nmemory = None\ntarget = None\n\n\ndef parse_args(argv):\n    if len(argv) < 6:\n        sys.exit(""Not enough arguments provided"")\n\n    global train_data_file, memory, target\n\n    i = 1\n    while i <= 6:\n        arg = str(argv[i])\n        if arg == ""--trainDataFile"":\n            train_data_file = str(argv[i+1])\n        elif arg == ""--memory"":\n            memory = str(argv[i+1])\n        elif arg == ""--target"":\n            target = str(argv[i+1])\n        i += 2\n\n\nif __name__ == ""__main__"":\n    parse_args(sys.argv)\n\nh2o.init(ip=socket.gethostbyname(socket.gethostname()), port=""54321"", start_h2o=False)\n\ntrain = h2o.import_file(train_data_file)\n\nx = train.columns\ny = target\nx.remove(y)\n\ntrain[y] = train[y].asfactor()\n\naml = H2OAutoML(max_runtime_secs=60)\naml.train(x=x, y=y, training_frame=train)\n\nlb = aml.leaderboard\nprint(lb)\n\nsave_path = os.environ[""RESULT_DIR""]\nmodel_path = h2o.save_model(model=aml.leader, path=save_path)\nprint(model_path)\n\nh2o.cluster().shutdown()\n'"
community/FfDL-Seldon/onnx-model/ONNXMnist.py,0,"b'#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport onnx\nimport os\nfrom ngraph_onnx.onnx_importer.importer import import_onnx_file\nimport ngraph as ng\nimport numpy as np\n\nclass ONNXMnist(object):\n\n    def __init__(self):\n        # Initialize variables\n        training_id = os.environ.get(""TRAINING_ID"")\n        model_file_name = os.environ.get(""MODEL_FILE_NAME"")\n        mountPath = os.environ.get(""MOUNT_PATH"")\n\n        # Replace with path of trained model\n        model_path = mountPath + \'/\' + training_id + \'/\' + model_file_name\n\n        # Load model from the FLEXVolume\n        self.models = import_onnx_file(model_path)\n        self.runtime = ng.runtime(backend_name=\'CPU\')\n        self.model = self.models[0]\n        self.net = self.runtime.computation(self.model[\'output\'], *self.model[\'inputs\'])\n        print(""Model loaded"")\n\n    def predict(self,X,features_names):\n        return self.net(X)\n'"
community/FfDL-Seldon/pytorch-model/PyMnist.py,7,"b'#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport torch\nimport numpy as np\nfrom torchvision import datasets, transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport boto3\nimport botocore\nimport tarfile\nimport os\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\n\nclass PyMnist(object):\n    def __init__(self):\n        training_id = os.environ.get(""TRAINING_ID"")\n        endpoint_url = os.environ.get(""BUCKET_ENDPOINT_URL"")\n        bucket_name = os.environ.get(""BUCKET_NAME"")\n        bucket_key = os.environ.get(""BUCKET_KEY"")\n        bucket_secret = os.environ.get(""BUCKET_SECRET"")\n        print(""Training id:{} endpoint URL:{} key:{} secret:{}"".format(training_id,endpoint_url,bucket_key,bucket_secret))\n        \n        self.class_names = [""class:{}"".format(str(i)) for i in range(10)]\n        #self.class_names = [""prediction""]\n\n        # Define S3 resource and download the model file\n        client = boto3.resource(\n            \'s3\',\n            endpoint_url=endpoint_url,\n            aws_access_key_id=bucket_key,\n            aws_secret_access_key=bucket_secret,\n        )\n\n        KEY = training_id + \'/saved_model.tar.gz\' # replace with your object key\n\n        try:\n            client.Bucket(bucket_name).download_file(KEY, \'saved_model.tar.gz\')\n        except botocore.exceptions.ClientError as e:\n            if e.response[\'Error\'][\'Code\'] == ""404"":\n                print(""The object does not exist."")\n            else:\n                raise\n\n        # Untar model file\n        tar = tarfile.open(""saved_model.tar.gz"")\n        tar.extractall()\n        tar.close()\n\n        self.model = Net()\n        self.model.load_state_dict(torch.load(""./model.dat""))\n        \n\n    def predict(self,X,feature_names):\n        tensor = torch.from_numpy(X).view(-1, 28, 28)\n        t = transforms.Normalize((0.1307,), (0.3081,))\n        tensor_norm = t(tensor)\n        tensor_norm = tensor_norm.unsqueeze(0)\n        out = self.model(tensor_norm.float())\n        predictions = torch.nn.functional.softmax(out)\n        print(predictions)\n        return predictions.detach().numpy()\n\n\n'"
community/FfDL-Seldon/tf-model/TFMnist.py,0,"b'#\r\n# Copyright 2017-2018 IBM Corporation\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the ""License"");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n# http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an ""AS IS"" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n#\r\n\r\nimport tensorflow as tf\r\nimport boto3\r\nimport botocore\r\nimport tarfile\r\nimport os\r\n\r\nclass TFMnist(object):\r\n    def __init__(self):\r\n        training_id = os.environ.get(""TRAINING_ID"")\r\n        endpoint_url = os.environ.get(""BUCKET_ENDPOINT_URL"")\r\n        bucket_name = os.environ.get(""BUCKET_NAME"")\r\n        bucket_key = os.environ.get(""BUCKET_KEY"")\r\n        bucket_secret = os.environ.get(""BUCKET_SECRET"")\r\n        print(""Training id:{} endpoint URL:{} key:{} secret:{}"".format(training_id,endpoint_url,bucket_key,bucket_secret))\r\n        \r\n        self.class_names = [""class:{}"".format(str(i)) for i in range(10)]\r\n        #self.class_names = [""prediction""]\r\n\r\n        # Define S3 resource and download the model file\r\n        client = boto3.resource(\r\n            \'s3\',\r\n            endpoint_url=endpoint_url,\r\n            aws_access_key_id=bucket_key,\r\n            aws_secret_access_key=bucket_secret,\r\n        )\r\n\r\n        KEY = training_id + \'/saved_model.tar.gz\' # replace with your object key\r\n\r\n        try:\r\n            client.Bucket(bucket_name).download_file(KEY, \'saved_model.tar.gz\')\r\n        except botocore.exceptions.ClientError as e:\r\n            if e.response[\'Error\'][\'Code\'] == ""404"":\r\n                print(""The object does not exist."")\r\n            else:\r\n                raise\r\n\r\n        # Untar model file\r\n        tar = tarfile.open(""saved_model.tar.gz"")\r\n        tar.extractall()\r\n        tar.close()\r\n\r\n        # Load the model into tf session and run predictions.\r\n        self.sess = tf.Session()\r\n        #tf.Session(graph=tf.Graph()) as sess:\r\n        # Load saved model into tf session\r\n        tf.saved_model.loader.load(self.sess, [tf.saved_model.tag_constants.SERVING], ""./"")\r\n        graph = tf.get_default_graph()\r\n        self.input = graph.get_tensor_by_name(""x_input:0"")\r\n        #self.predictor = graph.get_tensor_by_name(""predictor:0"")\r\n        self.output = graph.get_tensor_by_name(""y_output:0"")\r\n        self.keep_prob = tf.placeholder(tf.float32)\r\n\r\n    def predict(self,X,feature_names):\r\n        predictions = self.sess.run(self.output, feed_dict = {self.input:X, self.keep_prob:1.0})\r\n        return predictions\r\n\r\n\r\n    \r\n\r\n'"
demos/fashion-mnist-training/fashion-mnist-webapp/app.py,0,"b'#!/usr/bin/env python\n\n#\n# Copyright 2018 IBM Corp. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport collections\nimport json\nimport logging\nimport os\nimport requests\nimport signal\nimport time\nimport threading\nfrom tornado import httpserver, ioloop, web\nfrom tornado.options import define, options, parse_command_line\nimport numpy\nfrom PIL import Image\nimport io\n\nmodelEndpoint = os.environ.get(""MODEL_ENDPOINT"")\n# Command Line Options\ndefine(""port"", default=8088, help=""Port the web app will run on"")\n# define(""ml-endpoint"", default=""http://0.0.0.0:5000/predict"",\n#        help=""The Image Caption Generator REST endpoint"")\ndefine(""ml-endpoint"", default=modelEndpoint,\n       help=""The Image Caption Generator REST endpoint"")\n\n# Setup Logging\nlogging.basicConfig(level=os.environ.get(""LOGLEVEL"", ""INFO""),\n                    format=\'%(levelname)s: %(message)s\')\n\n# Global variables\nstatic_img_path = ""static/img/images/""\ntemp_img_prefix = ""MAX-""\nimage_captions = collections.OrderedDict()\nVALID_EXT = [\'png\', \'jpg\', \'jpeg\', \'gif\']\nFASHION_LIST = [\'T-shirt/top\',\'Trouser\',\'Pullover\',\'Dress\',\'Coat\',\'Sandal\',\'Shirt\',\'Sneaker\',\'Bag\',\'Ankle boot\']\n\n\nclass MainHandler(web.RequestHandler):\n    def get(self):\n        self.render(""index.html"", image_captions=image_captions)\n\n\nclass DetailHandler(web.RequestHandler):\n    def get(self):\n        image = self.get_argument(\'image\', None)\n        if not image:\n            self.set_status(400)\n            return self.finish(""400: Missing image parameter"")\n        if image not in image_captions:\n            self.set_status(404)\n            return self.finish(""404: Image not found"")\n        self.render(""detail-snippet.html"", image=image,\n                    predictions=image_captions[image])\n\n\nclass CleanupHandler(web.RequestHandler):\n    def get(self):\n        self.render(""cleanup.html"")\n\n    def delete(self):\n        clean_up()\n\n\nclass UploadHandler(web.RequestHandler):\n    def post(self):\n        finish_ret = []\n        new_files = self.request.files[\'file\']\n        for file_des in new_files:\n            file_name = temp_img_prefix + file_des[\'filename\']\n            if valid_file_ext(file_name):\n                rel_path = static_img_path + file_name\n                output_file = open(rel_path, \'wb\')\n                output_file.write(file_des[\'body\'])\n                output_file.close()\n                caption = run_ml(rel_path)\n                finish_ret.append({\n                    ""file_name"": rel_path,\n                    ""caption"": caption[0][\'caption\']\n                })\n        if not finish_ret:\n            self.send_error(400)\n            return\n        sort_image_captions()\n        self.finish(json.dumps(finish_ret))\n\n\ndef valid_file_ext(filename):\n    return \'.\' in filename and filename.split(\'.\', 1)[1].lower() in VALID_EXT\n\n\n# Runs ML on given image\ndef run_ml(img_path):\n    img_file = {\'image\': open(img_path, \'rb\')}\n    image = preprocess_object(img_path)\n    # Run curl command to send json to seldon\n    features = [str(i+1) for i in range(0,784)]\n    req = {""data"": {""names"": features, ""ndarray"": image}}\n    results = requests.post(ml_endpoint, json=req)\n    request_results = results.json()\n    # Run postprocessing and retrieve results from returned json\n    return_json = request_results[\'data\'][\'ndarray\']\n    results = postprocess(return_json)\n    cap_json = postpostprocess(results)\n    print(cap_json)\n    caption = cap_json[\'predictions\']\n    image_captions[img_path] = caption\n    return caption\n\n# preprocess and post processing images\ndef preprocess_object(image_path, target_shape=(28,28)):\n    """"""\n\n    image_path:    File path to image\n    target_shape:  Shape that the model is expecting images to be in\n                   by default the expected shape is (28,28)\n    returns:       An array containing the raw data from the image at\n                   \'image_path\' resized to \'target_shape\'\n    """"""\n\n    image = read_image(image_path, target_shape)\n\n    # Changes the image from a 1d array of size 784 to 2d of 28x28\n    feature_list = []\n    row = []\n    for x in range(0,28):\n        range1 = 28 * x\n        row = []\n        for vals in range(0,28):\n            pixel = image[range1 + vals]\n            row.append([pixel])\n        feature_list.append(row)\n\n    return [feature_list]\n\ndef read_image(image_path, target_shape):\n    """"""\n\n    image_path:    File path to image\n    target_shape:  Shape that the model is expecting images to be in\n                   by default the expected shape is (28,28)\n    returns:       Returns a list of pixel values stored in \'L\' mode\n                   (black and white 8bit pixels)\n    """"""\n    image = Image.open(image_path)\n    image = image.resize(target_shape)\n    # Force image to be stored in \'L\' mode (Black and White 8bit pixels)\n    image = image.convert(\'L\')\n    raw = list(image.getdata())\n    return raw\n\ndef postprocess(results):\n    """"""\n\n    results:    List of results where each result is a list of confidences\n                in the image being predicted being of a certain class\n    returns:    A list of 2-tuples where each 2-tuple corresponds to a result\n                and is a 2-tuple where the second element is a list of sorted\n                confidence values (from max to min) and the first element is\n                the argsorted list of classes corresponding to the confidences\n    """"""\n\n    post_results = []\n    for result in results:\n        argsort_rev = numpy.argsort(result)[::-1]\n        result_rev_sort = sorted(result)[::-1]\n        post_results.append((argsort_rev, result_rev_sort))\n    return post_results\n\ndef postpostprocess(post_results):\n    """"""\n\n    post_results:   An array of confidences that the image is of the class\n                    with the same key as the index of the confidence value\n    """"""\n\n    FASHION_LIST = [\'T-shirt/top\',\'Trouser\',\'Pullover\',\'Dress\',\'Coat\',\'Sandal\',\'Shirt\',\'Sneaker\',\'Bag\',\'Ankle boot\']\n\n    cap_array = []\n    for i in range(0,3):\n        cap_array.append({""index"": i, ""caption"": FASHION_LIST[post_results[0][0][i]],""probability"": str(round(post_results[0][1][i]*100, 4))})\n    cap_json = {\'predictions\': cap_array}\n    return cap_json\n\n\ndef sort_image_captions():\n    global image_captions\n    image_captions = collections.OrderedDict(\n        sorted(image_captions.items(), key=lambda t: t[0].lower()))\n\n\n# Gets list of images with relative paths from static dir\ndef get_image_list():\n    image_list = sorted(os.listdir(static_img_path))\n    rel_img_list = [static_img_path + s for s in image_list]\n    return rel_img_list\n\n\n# Run all static images through ML\ndef prepare_metadata():\n    threads = []\n\n    rel_img_list = get_image_list()\n    for img in rel_img_list:\n        t = threading.Thread(target=run_ml, args=(img,))\n        threads.append(t)\n\n    for t in threads:\n        t.start()\n\n    for t in threads:\n        t.join()\n\n    sort_image_captions()\n\n\n# Deletes all files uploaded through the GUI and removes them from the dict\ndef clean_up():\n    img_list = get_image_list()\n    for img_file in img_list:\n        if img_file.startswith(static_img_path + temp_img_prefix):\n            os.remove(img_file)\n            image_captions.pop(img_file)\n\n\ndef signal_handler(sig, frame):\n    ioloop.IOLoop.current().add_callback_from_signal(shutdown)\n\n\ndef shutdown():\n    logging.info(""Cleaning up image files"")\n    clean_up()\n    logging.info(""Stopping web server"")\n    server.stop()\n    ioloop.IOLoop.current().stop()\n\n\ndef make_app():\n    handlers = [\n        (r""/"", MainHandler),\n        (r""/upload"", UploadHandler),\n        (r""/cleanup"", CleanupHandler),\n        (r""/detail"", DetailHandler)\n    ]\n\n    configs = {\n        \'static_path\': \'static\',\n        \'template_path\': \'templates\'\n    }\n\n    return web.Application(handlers, **configs)\n\n\ndef main():\n    parse_command_line()\n\n    global ml_endpoint\n    ml_endpoint = options.ml_endpoint\n    # if \'/model/predict\' not in options.ml_endpoint:\n    #     ml_endpoint = options.ml_endpoint + ""/model/predict""\n\n    logging.info(""Connecting to ML endpoint at %s"", ml_endpoint)\n\n    # try:\n    #     requests.get(ml_endpoint)\n    # except requests.exceptions.ConnectionError:\n    #     logging.error(\n    #         ""Cannot connect to the Image Caption Generator REST endpoint at "" +\n    #         options.ml_endpoint)\n    #     raise SystemExit\n\n    logging.info(""Starting web server"")\n    app = make_app()\n    global server\n    server = httpserver.HTTPServer(app)\n    server.listen(options.port)\n    signal.signal(signal.SIGINT, signal_handler)\n\n    logging.info(""Preparing ML metadata (this may take some time)"")\n    start = time.time()\n    prepare_metadata()\n    end = time.time()\n    logging.info(""Metadata prepared in %s seconds"", end - start)\n\n    logging.info(""Use Ctrl+C to stop web server"")\n    ioloop.IOLoop.current().start()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
demos/fashion-mnist-training/seldon-deployment/MyModel.py,0,"b'from keras.models import load_model\nimport numpy\nimport boto3\nimport botocore\nimport os\n\nclass MyModel(object):\n\n    def __init__(self):\n        # Initialize variables\n        training_id = os.environ.get(""TRAINING_ID"")\n        endpoint_url = os.environ.get(""BUCKET_ENDPOINT_URL"")\n        bucket_name = os.environ.get(""BUCKET_NAME"")\n        bucket_key = os.environ.get(""BUCKET_KEY"")\n        bucket_secret = os.environ.get(""BUCKET_SECRET"")\n        print(""Training id:{} endpoint URL:{} key:{} secret:{} bucket name:{}"".format(training_id,endpoint_url,bucket_key,bucket_secret,bucket_name))\n\n        # Define S3 resource and download the model file\n        client = boto3.resource(\n            \'s3\',\n            endpoint_url=endpoint_url,\n            aws_access_key_id=bucket_key,\n            aws_secret_access_key=bucket_secret,\n        )\n\n        KEY = training_id + \'/model.h5\' # replace with your object key\n\n        try:\n            client.Bucket(bucket_name).download_file(KEY, \'model.h5\')\n        except botocore.exceptions.ClientError as e:\n            if e.response[\'Error\'][\'Code\'] == ""404"":\n                print(""The object does not exist."")\n            else:\n                raise\n\n        # Replace with path of trained model\n        model_path = \'model.h5\'\n        self.model = load_model(model_path)\n\n    def predict(self,X,features_names):\n        X = numpy.array(X)\n        return self.model.predict(X)\n'"
etc/examples/caffe2-model/mnist.py,0,"b'# Copyright (c) 2016-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n# Module caffe2.python.examples.resnet50_trainer\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport logging\nimport numpy as np\nimport time\nimport os\n\nfrom caffe2.python import core, workspace, experiment_util, data_parallel_model\nfrom caffe2.python import data_parallel_model_utils, dyndep, optimizer\nfrom caffe2.python import timeout_guard, model_helper, brew\nfrom caffe2.proto import caffe2_pb2\n\nimport caffe2.python.models.resnet as resnet\nfrom caffe2.python.modeling.initializers import Initializer, pFP16Initializer\nimport caffe2.python.predictor.predictor_exporter as pred_exp\nimport caffe2.python.predictor.predictor_py_utils as pred_utils\nfrom caffe2.python.predictor_constants import predictor_constants as predictor_constants\n\n\'\'\'\nParallelized multi-GPU distributed trainer for Resnet 50. Can be used to train\non imagenet data, for example.\n\nTo run the trainer in single-machine multi-gpu mode by setting num_shards = 1.\n\nTo run the trainer in multi-machine multi-gpu mode with M machines,\nrun the same program on all machines, specifying num_shards = M, and\nshard_id = a unique integer in the set [0, M-1].\n\nFor rendezvous (the trainer processes have to know about each other),\nyou can either use a directory path that is visible to all processes\n(e.g. NFS directory), or use a Redis instance. Use the former by\npassing the `file_store_path` argument. Use the latter by passing the\n`redis_host` and `redis_port` arguments.\n\'\'\'\n\nlogging.basicConfig()\nlog = logging.getLogger(""resnet50_trainer"")\nlog.setLevel(logging.DEBUG)\n\ndyndep.InitOpsLibrary(\'@/caffe2/caffe2/distributed:file_store_handler_ops\')\ndyndep.InitOpsLibrary(\'@/caffe2/caffe2/distributed:redis_store_handler_ops\')\n\n\ndef AddImageInput(model, reader, batch_size, img_size, dtype, is_test):\n    \'\'\'\n    The image input operator loads image and label data from the reader and\n    applies transformations to the images (random cropping, mirroring, ...).\n    \'\'\'\n    data, label = brew.image_input(\n        model,\n        reader, [""data"", ""label""],\n        batch_size=batch_size,\n        output_type=dtype,\n        use_gpu_transform=True if model._device_type == 1 else False,\n        use_caffe_datum=True,\n        mean=128.,\n        std=128.,\n        scale=256,\n        crop=img_size,\n        mirror=1,\n        is_test=is_test,\n    )\n\n    data = model.StopGradient(data, data)\n\n\ndef AddNullInput(model, reader, batch_size, img_size, dtype):\n    \'\'\'\n    The null input function uses a gaussian fill operator to emulate real image\n    input. A label blob is hardcoded to a single value. This is useful if you\n    want to test compute throughput or don\'t have a dataset available.\n    \'\'\'\n    suffix = ""_fp16"" if dtype == ""float16"" else """"\n    model.param_init_net.GaussianFill(\n        [],\n        [""data"" + suffix],\n        shape=[batch_size, 3, img_size, img_size],\n    )\n    if dtype == ""float16"":\n        model.param_init_net.FloatToHalf(""data"" + suffix, ""data"")\n\n    model.param_init_net.ConstantFill(\n        [],\n        [""label""],\n        shape=[batch_size],\n        value=1,\n        dtype=core.DataType.INT32,\n    )\n\n\ndef SaveModel(args, train_model, epoch):\n    prefix = ""[]_{}"".format(train_model._device_prefix, train_model._devices[0])\n    predictor_export_meta = pred_exp.PredictorExportMeta(\n        predict_net=train_model.net.Proto(),\n        parameters=data_parallel_model.GetCheckpointParams(train_model),\n        inputs=[prefix + ""/data""],\n        outputs=[prefix + ""/softmax""],\n        shapes={\n            prefix + ""/softmax"": (1, args.num_labels),\n            prefix + ""/data"": (args.num_channels, args.image_size, args.image_size)\n        }\n    )\n\n    # save the train_model for the current epoch\n    model_path = ""%s/%s_%d.mdl"" % (\n        args.file_store_path,\n        args.save_model_name,\n        epoch,\n    )\n\n    # set db_type to be ""minidb"" instead of ""log_file_db"", which breaks\n    # the serialization in save_to_db. Need to switch back to log_file_db\n    # after migration\n    pred_exp.save_to_db(\n        db_type=""minidb"",\n        db_destination=model_path,\n        predictor_export_meta=predictor_export_meta,\n    )\n\n\ndef LoadModel(path, model):\n    \'\'\'\n    Load pretrained model from file\n    \'\'\'\n    log.info(""Loading path: {}"".format(path))\n    meta_net_def = pred_exp.load_from_db(path, \'minidb\')\n    init_net = core.Net(pred_utils.GetNet(\n        meta_net_def, predictor_constants.GLOBAL_INIT_NET_TYPE))\n    predict_init_net = core.Net(pred_utils.GetNet(\n        meta_net_def, predictor_constants.PREDICT_INIT_NET_TYPE))\n\n    predict_init_net.RunAllOnGPU()\n    init_net.RunAllOnGPU()\n\n    assert workspace.RunNetOnce(predict_init_net)\n    assert workspace.RunNetOnce(init_net)\n\n    # Hack: fix iteration counter which is in CUDA context after load model\n    itercnt = workspace.FetchBlob(""optimizer_iteration"")\n    workspace.FeedBlob(\n        ""optimizer_iteration"",\n        itercnt,\n        device_option=core.DeviceOption(caffe2_pb2.CPU, 0)\n    )\n\n\ndef RunEpoch(\n    args,\n    epoch,\n    train_model,\n    test_model,\n    total_batch_size,\n    num_shards,\n    expname,\n    explog,\n):\n    \'\'\'\n    Run one epoch of the trainer.\n    TODO: add checkpointing here.\n    \'\'\'\n    # TODO: add loading from checkpoint\n    log.info(""Starting epoch {}/{}"".format(epoch, args.num_epochs))\n    epoch_iters = int(args.epoch_size / total_batch_size / num_shards)\n    for i in range(epoch_iters):\n        # This timeout is required (temporarily) since CUDA-NCCL\n        # operators might deadlock when synchronizing between GPUs.\n        timeout = 600.0 if i == 0 else 60.0\n        with timeout_guard.CompleteInTimeOrDie(timeout):\n            t1 = time.time()\n            workspace.RunNet(train_model.net.Proto().name)\n            t2 = time.time()\n            dt = t2 - t1\n\n        fmt = ""Finished iteration {}/{} of epoch {} ({:.2f} images/sec)""\n        log.info(fmt.format(i + 1, epoch_iters, epoch, total_batch_size / dt))\n        prefix = ""{}_{}"".format(\n            train_model._device_prefix,\n            train_model._devices[0])\n        accuracy = workspace.FetchBlob(prefix + \'/accuracy\')\n        loss = workspace.FetchBlob(prefix + \'/loss\')\n        train_fmt = ""Training loss: {}, accuracy: {}""\n        log.info(train_fmt.format(loss, accuracy))\n\n    num_images = epoch * epoch_iters * total_batch_size\n    prefix = ""{}_{}"".format(train_model._device_prefix, train_model._devices[0])\n    accuracy = workspace.FetchBlob(prefix + \'/accuracy\')\n    loss = workspace.FetchBlob(prefix + \'/loss\')\n    learning_rate = workspace.FetchBlob(\n        data_parallel_model.GetLearningRateBlobNames(train_model)[0]\n    )\n    test_accuracy = 0\n    if (test_model is not None):\n        # Run 100 iters of testing\n        ntests = 0\n        for _ in range(0, 100):\n            workspace.RunNet(test_model.net.Proto().name)\n            for g in test_model._devices:\n                test_accuracy += np.asscalar(workspace.FetchBlob(\n                    ""{}_{}"".format(test_model._device_prefix, g) + \'/accuracy\'\n                ))\n                ntests += 1\n        test_accuracy /= ntests\n    else:\n        test_accuracy = (-1)\n\n    explog.log(\n        input_count=num_images,\n        batch_count=(i + epoch * epoch_iters),\n        additional_values={\n            \'accuracy\': accuracy,\n            \'loss\': loss,\n            \'learning_rate\': learning_rate,\n            \'epoch\': epoch,\n            \'test_accuracy\': test_accuracy,\n        }\n    )\n    assert loss < 40, ""Exploded gradients :(""\n\n    # TODO: add checkpointing\n    return epoch + 1\n\n\ndef Train(args):\n    # Either use specified device list or generate one\n    if args.gpus is not None:\n        gpus = [int(x) for x in args.gpus.split(\',\')]\n        num_gpus = len(gpus)\n    else:\n        gpus = list(range(args.num_gpus))\n        num_gpus = args.num_gpus\n\n    log.info(""Running on GPUs: {}"".format(gpus))\n\n    # Verify valid batch size\n    total_batch_size = args.batch_size\n    batch_per_device = total_batch_size // num_gpus\n    assert \\\n        total_batch_size % num_gpus == 0, \\\n        ""Number of GPUs must divide batch size""\n\n    # Round down epoch size to closest multiple of batch size across machines\n    global_batch_size = total_batch_size * args.num_shards\n    epoch_iters = int(args.epoch_size / global_batch_size)\n\n    assert \\\n        epoch_iters > 0, \\\n        ""Epoch size must be larger than batch size times shard count""\n\n    args.epoch_size = epoch_iters * global_batch_size\n    log.info(""Using epoch size: {}"".format(args.epoch_size))\n\n    # Create ModelHelper object\n    train_arg_scope = {\n        \'order\': \'NCHW\',\n        \'use_cudnn\': True,\n        \'cudnn_exhaustive_search\': True,\n        \'ws_nbytes_limit\': (args.cudnn_workspace_limit_mb * 1024 * 1024),\n    }\n    train_model = model_helper.ModelHelper(\n        name=""resnet50"", arg_scope=train_arg_scope\n    )\n\n    num_shards = args.num_shards\n    shard_id = args.shard_id\n\n    # Expect interfaces to be comma separated.\n    # Use of multiple network interfaces is not yet complete,\n    # so simply use the first one in the list.\n    interfaces = args.distributed_interfaces.split("","")\n\n    # Rendezvous using MPI when run with mpirun\n    if os.getenv(""OMPI_COMM_WORLD_SIZE"") is not None:\n        num_shards = int(os.getenv(""OMPI_COMM_WORLD_SIZE"", 1))\n        shard_id = int(os.getenv(""OMPI_COMM_WORLD_RANK"", 0))\n        if num_shards > 1:\n            rendezvous = dict(\n                kv_handler=None,\n                num_shards=num_shards,\n                shard_id=shard_id,\n                engine=""GLOO"",\n                transport=args.distributed_transport,\n                interface=interfaces[0],\n                mpi_rendezvous=True,\n                exit_nets=None)\n\n    elif num_shards > 1:\n        # Create rendezvous for distributed computation\n        store_handler = ""store_handler""\n        if args.redis_host is not None:\n            # Use Redis for rendezvous if Redis host is specified\n            workspace.RunOperatorOnce(\n                core.CreateOperator(\n                    ""RedisStoreHandlerCreate"", [], [store_handler],\n                    host=args.redis_host,\n                    port=args.redis_port,\n                    prefix=args.run_id,\n                )\n            )\n        else:\n            # Use filesystem for rendezvous otherwise\n            workspace.RunOperatorOnce(\n                core.CreateOperator(\n                    ""FileStoreHandlerCreate"", [], [store_handler],\n                    path=args.file_store_path,\n                    prefix=args.run_id,\n                )\n            )\n\n        rendezvous = dict(\n            kv_handler=store_handler,\n            shard_id=shard_id,\n            num_shards=num_shards,\n            engine=""GLOO"",\n            transport=args.distributed_transport,\n            interface=interfaces[0],\n            exit_nets=None)\n\n    else:\n        rendezvous = None\n\n    # Model building functions\n    def create_resnet50_model_ops(model, loss_scale):\n        initializer = (pFP16Initializer if args.dtype == \'float16\'\n                       else Initializer)\n\n        with brew.arg_scope([brew.conv, brew.fc],\n                            WeightInitializer=initializer,\n                            BiasInitializer=initializer,\n                            enable_tensor_core=args.enable_tensor_core,\n                            float16_compute=args.float16_compute):\n            pred = resnet.create_resnet50(\n                model,\n                ""data"",\n                num_input_channels=args.num_channels,\n                num_labels=args.num_labels,\n                no_bias=True,\n                no_loss=True,\n            )\n\n        if args.dtype == \'float16\':\n            pred = model.net.HalfToFloat(pred, pred + \'_fp32\')\n\n        softmax, loss = model.SoftmaxWithLoss([pred, \'label\'],\n                                              [\'softmax\', \'loss\'])\n        loss = model.Scale(loss, scale=loss_scale)\n        brew.accuracy(model, [softmax, ""label""], ""accuracy"")\n        return [loss]\n\n    def add_optimizer(model):\n        stepsz = int(30 * args.epoch_size / total_batch_size / num_shards)\n\n        if args.float16_compute:\n            # TODO: merge with multi-prceision optimizer\n            opt = optimizer.build_fp16_sgd(\n                model,\n                args.base_learning_rate,\n                momentum=0.9,\n                nesterov=1,\n                weight_decay=args.weight_decay,   # weight decay included\n                policy=""step"",\n                stepsize=stepsz,\n                gamma=0.1\n            )\n        else:\n            optimizer.add_weight_decay(model, args.weight_decay)\n            opt = optimizer.build_multi_precision_sgd(\n                model,\n                args.base_learning_rate,\n                momentum=0.9,\n                nesterov=1,\n                policy=""step"",\n                stepsize=stepsz,\n                gamma=0.1\n            )\n        return opt\n\n    # Define add_image_input function.\n    # Depends on the ""train_data"" argument.\n    # Note that the reader will be shared with between all GPUS.\n    if args.train_data == ""null"":\n        def add_image_input(model):\n            AddNullInput(\n                model,\n                None,\n                batch_size=batch_per_device,\n                img_size=args.image_size,\n                dtype=args.dtype,\n            )\n    else:\n        reader = train_model.CreateDB(\n            ""reader"",\n            db=args.train_data,\n            db_type=args.db_type,\n            num_shards=num_shards,\n            shard_id=shard_id,\n        )\n\n        def add_image_input(model):\n            AddImageInput(\n                model,\n                reader,\n                batch_size=batch_per_device,\n                img_size=args.image_size,\n                dtype=args.dtype,\n                is_test=False,\n            )\n\n    def add_post_sync_ops(model):\n        """"""Add ops applied after initial parameter sync.""""""\n        for param_info in model.GetOptimizationParamInfo(model.GetParams()):\n            if param_info.blob_copy is not None:\n                model.param_init_net.HalfToFloat(\n                    param_info.blob,\n                    param_info.blob_copy[core.DataType.FLOAT]\n                )\n\n    # Create parallelized model\n    data_parallel_model.Parallelize(\n        train_model,\n        input_builder_fun=add_image_input,\n        forward_pass_builder_fun=create_resnet50_model_ops,\n        optimizer_builder_fun=add_optimizer,\n        post_sync_builder_fun=add_post_sync_ops,\n        devices=gpus,\n        rendezvous=rendezvous,\n        optimize_gradient_memory=False,\n        cpu_device=args.use_cpu,\n    )\n\n    if args.model_parallel:\n        # Shift half of the activations to another GPU\n        assert workspace.NumCudaDevices() >= 2 * args.num_gpus\n        activations = data_parallel_model_utils.GetActivationBlobs(train_model)\n        data_parallel_model_utils.ShiftActivationDevices(\n            train_model,\n            activations=activations[len(activations) // 2:],\n            shifts={g: args.num_gpus + g for g in range(args.num_gpus)},\n        )\n\n    data_parallel_model.OptimizeGradientMemory(train_model, {}, set(), False)\n\n    workspace.RunNetOnce(train_model.param_init_net)\n    workspace.CreateNet(train_model.net)\n\n    # Add test model, if specified\n    test_model = None\n    if (args.test_data is not None):\n        log.info(""----- Create test net ----"")\n        test_arg_scope = {\n            \'order\': ""NCHW"",\n            \'use_cudnn\': True,\n            \'cudnn_exhaustive_search\': True,\n        }\n        test_model = model_helper.ModelHelper(\n            name=""resnet50_test"", arg_scope=test_arg_scope, init_params=False\n        )\n\n        test_reader = test_model.CreateDB(\n            ""test_reader"",\n            db=args.test_data,\n            db_type=args.db_type,\n        )\n\n        def test_input_fn(model):\n            AddImageInput(\n                model,\n                test_reader,\n                batch_size=batch_per_device,\n                img_size=args.image_size,\n                dtype=args.dtype,\n                is_test=True,\n            )\n\n        data_parallel_model.Parallelize(\n            test_model,\n            input_builder_fun=test_input_fn,\n            forward_pass_builder_fun=create_resnet50_model_ops,\n            post_sync_builder_fun=add_post_sync_ops,\n            param_update_builder_fun=None,\n            devices=gpus,\n            cpu_device=args.use_cpu,\n        )\n        workspace.RunNetOnce(test_model.param_init_net)\n        workspace.CreateNet(test_model.net)\n\n    epoch = 0\n    # load the pre-trained model and reset epoch\n    if args.load_model_path is not None:\n        LoadModel(args.load_model_path, train_model)\n\n        # Sync the model params\n        data_parallel_model.FinalizeAfterCheckpoint(train_model)\n\n        # reset epoch. load_model_path should end with *_X.mdl,\n        # where X is the epoch number\n        last_str = args.load_model_path.split(\'_\')[-1]\n        if last_str.endswith(\'.mdl\'):\n            epoch = int(last_str[:-4])\n            log.info(""Reset epoch to {}"".format(epoch))\n        else:\n            log.warning(""The format of load_model_path doesn\'t match!"")\n\n    expname = ""resnet50_gpu%d_b%d_L%d_lr%.2f_v2"" % (\n        args.num_gpus,\n        total_batch_size,\n        args.num_labels,\n        args.base_learning_rate,\n    )\n\n    explog = experiment_util.ModelTrainerLog(expname, args)\n\n    # Run the training one epoch a time\n    while epoch < args.num_epochs:\n        epoch = RunEpoch(\n            args,\n            epoch,\n            train_model,\n            test_model,\n            total_batch_size,\n            num_shards,\n            expname,\n            explog\n        )\n\n        # Save the model for each epoch\n        SaveModel(args, train_model, epoch)\n\n        model_path = ""%s/%s_"" % (\n            args.file_store_path,\n            args.save_model_name\n        )\n        # remove the saved model from the previous epoch if it exists\n        if os.path.isfile(model_path + str(epoch - 1) + "".mdl""):\n            os.remove(model_path + str(epoch - 1) + "".mdl"")\n\n\ndef main():\n    # TODO: use argv\n    parser = argparse.ArgumentParser(\n        description=""Caffe2: Resnet-50 training""\n    )\n    parser.add_argument(""--train_data"", type=str, default=None, required=True,\n                        help=""Path to training data (or \'null\' to simulate)"")\n    parser.add_argument(""--test_data"", type=str, default=None,\n                        help=""Path to test data"")\n    parser.add_argument(""--db_type"", type=str, default=""lmdb"",\n                        help=""Database type (such as lmdb or leveldb)"")\n    parser.add_argument(""--gpus"", type=str,\n                        help=""Comma separated list of GPU devices to use"")\n    parser.add_argument(""--num_gpus"", type=int, default=1,\n                        help=""Number of GPU devices (instead of --gpus)"")\n    parser.add_argument(""--model_parallel"", type=bool, default=False,\n                        help=""Split model over 2 x num_gpus"")\n    parser.add_argument(""--num_channels"", type=int, default=3,\n                        help=""Number of color channels"")\n    parser.add_argument(""--image_size"", type=int, default=227,\n                        help=""Input image size (to crop to)"")\n    parser.add_argument(""--num_labels"", type=int, default=1000,\n                        help=""Number of labels"")\n    parser.add_argument(""--batch_size"", type=int, default=32,\n                        help=""Batch size, total over all GPUs"")\n    parser.add_argument(""--epoch_size"", type=int, default=1500000,\n                        help=""Number of images/epoch, total over all machines"")\n    parser.add_argument(""--num_epochs"", type=int, default=1000,\n                        help=""Num epochs."")\n    parser.add_argument(""--base_learning_rate"", type=float, default=0.1,\n                        help=""Initial learning rate."")\n    parser.add_argument(""--weight_decay"", type=float, default=1e-4,\n                        help=""Weight decay (L2 regularization)"")\n    parser.add_argument(""--cudnn_workspace_limit_mb"", type=int, default=64,\n                        help=""CuDNN workspace limit in MBs"")\n    parser.add_argument(""--num_shards"", type=int, default=1,\n                        help=""Number of machines in distributed run"")\n    parser.add_argument(""--shard_id"", type=int, default=0,\n                        help=""Shard id."")\n    parser.add_argument(""--run_id"", type=str,\n                        help=""Unique run identifier (e.g. uuid)"")\n    parser.add_argument(""--redis_host"", type=str,\n                        help=""Host of Redis server (for rendezvous)"")\n    parser.add_argument(""--redis_port"", type=int, default=6379,\n                        help=""Port of Redis server (for rendezvous)"")\n    parser.add_argument(""--file_store_path"", type=str, default=""/tmp"",\n                        help=""Path to directory to use for rendezvous"")\n    parser.add_argument(""--save_model_name"", type=str, default=""resnet50_model"",\n                        help=""Save the trained model to a given name"")\n    parser.add_argument(""--load_model_path"", type=str, default=None,\n                        help=""Load previously saved model to continue training"")\n    parser.add_argument(""--use_cpu"", type=bool, default=False,\n                        help=""Use CPU instead of GPU"")\n    parser.add_argument(\'--dtype\', default=\'float\',\n                        choices=[\'float\', \'float16\'],\n                        help=\'Data type used for training\')\n    parser.add_argument(\'--float16_compute\', action=\'store_true\',\n                        help=""Use float 16 compute, if available"")\n    parser.add_argument(\'--enable-tensor-core\', action=\'store_true\',\n                        help=\'Enable Tensor Core math for Conv and FC ops\')\n    parser.add_argument(""--distributed_transport"", type=str, default=""tcp"",\n                        help=""Transport to use for distributed run [tcp|ibverbs]"")\n    parser.add_argument(""--distributed_interfaces"", type=str, default="""",\n                        help=""Network interfaces to use for distributed run"")\n\n    args = parser.parse_args()\n\n    Train(args)\n\nif __name__ == \'__main__\':\n    workspace.GlobalInit([\'caffe2\', \'--caffe2_log_level=2\'])\n    main()\n'"
etc/examples/horovod/pytorch_mnist.py,14,"b'# Copyright 2018 Uber Technologies, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n#!/usr/bin/env python\n\nfrom __future__ import print_function\nimport argparse\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\nimport torch.utils.data.distributed\nimport horovod.torch as hvd\n\n# Training settings\nparser = argparse.ArgumentParser(description=\'PyTorch MNIST Example\')\nparser.add_argument(\'--batch-size\', type=int, default=64, metavar=\'N\',\n                    help=\'input batch size for training (default: 64)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=1000, metavar=\'N\',\n                    help=\'input batch size for testing (default: 1000)\')\nparser.add_argument(\'--epochs\', type=int, default=10, metavar=\'N\',\n                    help=\'number of epochs to train (default: 10)\')\nparser.add_argument(\'--lr\', type=float, default=0.01, metavar=\'LR\',\n                    help=\'learning rate (default: 0.01)\')\nparser.add_argument(\'--momentum\', type=float, default=0.5, metavar=\'M\',\n                    help=\'SGD momentum (default: 0.5)\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'disables CUDA training\')\nparser.add_argument(\'--seed\', type=int, default=42, metavar=\'S\',\n                    help=\'random seed (default: 42)\')\nparser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'N\',\n                    help=\'how many batches to wait before logging training status\')\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nhvd.init()\ntorch.manual_seed(args.seed)\n\nif args.cuda:\n    # Horovod: pin GPU to local rank.\n    torch.cuda.set_device(hvd.local_rank())\n    torch.cuda.manual_seed(args.seed)\n\n\nkwargs = {\'num_workers\': 1, \'pin_memory\': True} if args.cuda else {}\ntrain_dataset = \\\n    datasets.MNIST(\'data-%d\' % hvd.rank(), train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ]))\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(\n    train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=args.batch_size, sampler=train_sampler, **kwargs)\n\ntest_dataset = \\\n    datasets.MNIST(\'data-%d\' % hvd.rank(), train=False, transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ]))\ntest_sampler = torch.utils.data.distributed.DistributedSampler(\n    test_dataset, num_replicas=hvd.size(), rank=hvd.rank())\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.test_batch_size,\n                                          sampler=test_sampler, **kwargs)\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\n\nmodel = Net()\n\nif args.cuda:\n    # Move model to GPU.\n    model.cuda()\n\n# Horovod: broadcast parameters.\nhvd.broadcast_parameters(model.state_dict(), root_rank=0)\n\n# Horovod: scale learning rate by the number of GPUs.\noptimizer = optim.SGD(model.parameters(), lr=args.lr * hvd.size(),\n                      momentum=args.momentum)\n\n# Horovod: wrap optimizer with DistributedOptimizer.\noptimizer = hvd.DistributedOptimizer(\n    optimizer, named_parameters=model.named_parameters())\n\n\ndef train(epoch):\n    model.train()\n    train_sampler.set_epoch(epoch)\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                epoch, batch_idx * len(data), len(train_sampler),\n                100. * batch_idx / len(train_loader), loss.data[0]))\n\n\ndef metric_average(val, name):\n    tensor = torch.FloatTensor([val])\n    avg_tensor = hvd.allreduce(tensor, name=name)\n    return avg_tensor[0]\n\n\ndef test():\n    model.eval()\n    test_loss = 0.\n    test_accuracy = 0.\n    for data, target in test_loader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        # sum up batch loss\n        test_loss += F.nll_loss(output, target, size_average=False).data[0]\n        # get the index of the max log-probability\n        pred = output.data.max(1, keepdim=True)[1]\n        test_accuracy += pred.eq(target.data.view_as(pred)).cpu().float().sum()\n\n    test_loss /= len(test_sampler)\n    test_accuracy /= len(test_sampler)\n\n    test_loss = metric_average(test_loss, \'avg_loss\')\n    test_accuracy = metric_average(test_accuracy, \'avg_accuracy\')\n\n    if hvd.rank() == 0:\n        print(\'\\nTest set: Average loss: {:.4f}, Accuracy: {:.2f}%\\n\'.format(\n            test_loss, 100. * test_accuracy))\n\n\nfor epoch in range(1, args.epochs + 1):\n    train(epoch)\n    test()\n'"
etc/examples/horovod/tensorflow_mnist.py,0,"b'#!/usr/bin/env python\n\n#example picked from https://raw.githubusercontent.com/uber/horovod/master/examples/tensorflow_mnist.py\nimport os, sys\nimport tensorflow as tf\nimport horovod.tensorflow as hvd\nlayers = tf.contrib.layers\nlearn = tf.contrib.learn\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\ncheckpoint_path = os.getenv(""RESULT_DIR"")\ntraining_data_dir = os.getenv(""DATA_DIR"")\nlearner_id = os.getenv(""LEARNER_ID"")\n\nprint(""checkpint path is {} and training_data_dir is {} and learner_id {}"".format( checkpoint_path, training_data_dir, learner_id))\nif checkpoint_path == None:\n    exit(1)\n\nif training_data_dir == None:\n    exit(1)\n\nif learner_id == None:\n    exit(1)\n\n\ndef conv_model(feature, target, mode):\n    """"""2-layer convolution model.""""""\n    # Convert the target to a one-hot tensor of shape (batch_size, 10) and\n    # with a on-value of 1 for each one-hot vector of length 10.\n    target = tf.one_hot(tf.cast(target, tf.int32), 10, 1, 0)\n\n    # Reshape feature to 4d tensor with 2nd and 3rd dimensions being\n    # image width and height final dimension being the number of color channels.\n    feature = tf.reshape(feature, [-1, 28, 28, 1])\n\n    # First conv layer will compute 32 features for each 5x5 patch\n    with tf.variable_scope(\'conv_layer1\'):\n        h_conv1 = layers.conv2d(\n            feature, 32, kernel_size=[5, 5], activation_fn=tf.nn.relu)\n        h_pool1 = tf.nn.max_pool(\n            h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\')\n\n    # Second conv layer will compute 64 features for each 5x5 patch.\n    with tf.variable_scope(\'conv_layer2\'):\n        h_conv2 = layers.conv2d(\n            h_pool1, 64, kernel_size=[5, 5], activation_fn=tf.nn.relu)\n        h_pool2 = tf.nn.max_pool(\n            h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\')\n        # reshape tensor into a batch of vectors\n        h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n\n    # Densely connected layer with 1024 neurons.\n    h_fc1 = layers.dropout(\n        layers.fully_connected(\n            h_pool2_flat, 1024, activation_fn=tf.nn.relu),\n        keep_prob=0.5,\n        is_training=mode == tf.contrib.learn.ModeKeys.TRAIN)\n\n    # Compute logits (1 per class) and compute loss.\n    logits = layers.fully_connected(h_fc1, 10, activation_fn=None)\n    loss = tf.losses.softmax_cross_entropy(target, logits)\n\n    return tf.argmax(logits, 1), loss\n\n\ndef main(argv):\n    # Initialize Horovod.\n    hvd.init()\n    # Download and load MNIST dataset.\n    mnist = learn.datasets.mnist.read_data_sets(training_data_dir)\n\n    # Build model...\n    with tf.name_scope(\'input\'):\n        image = tf.placeholder(tf.float32, [None, 784], name=\'image\')\n        label = tf.placeholder(tf.float32, [None], name=\'label\')\n    predict, loss = conv_model(image, label, tf.contrib.learn.ModeKeys.TRAIN)\n\n    opt = tf.train.RMSPropOptimizer(0.01)\n\n    # Add Horovod Distributed Optimizer.\n    opt = hvd.DistributedOptimizer(opt)\n\n    global_step = tf.contrib.framework.get_or_create_global_step()\n    train_op = opt.minimize(loss, global_step=global_step)\n\n    # BroadcastGlobalVariablesHook broadcasts initial variable states from rank 0\n    # to all other processes. This is necessary to ensure consistent initialization\n    # of all workers when training is started with random weights or restored\n    # from a checkpoint.\n    hooks = [hvd.BroadcastGlobalVariablesHook(0),\n             tf.train.StopAtStepHook(last_step=100 // hvd.size()),\n             tf.train.LoggingTensorHook(tensors={\'step\': global_step, \'loss\': loss},\n                                        every_n_iter=10),\n             ]\n\n    # Pin GPU to be used to process local rank (one GPU per process)\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.visible_device_list = str(hvd.local_rank())\n\n    # Save checkpoints only on worker 0 to prevent other workers from corrupting them.\n    checkpoint_dir = checkpoint_path if hvd.rank() == 0 else None\n\n    # The MonitoredTrainingSession takes care of session initialization,\n    # restoring from a checkpoint, saving to a checkpoint, and closing when done\n    # or an error occurs.\n    with tf.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir,\n                                           hooks=hooks,\n                                           config=config) as mon_sess:\n        while not mon_sess.should_stop():\n            # Run a training step synchronously.\n            image_, label_ = mnist.train.next_batch(100)\n            mon_sess.run(train_op, feed_dict={image: image_, label: label_})\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
etc/examples/keras-test/mnist_cnn.py,0,"b'#-----------------------------------------------------------------------\n#                                                                   \n# This information contains sample code provided in source code form.\n# You may copy, modify, and distribute these sample programs in any \n# form without payment to IBM for the purposes of developing, using,\n# marketing or distributing application programs conforming to the     \n# application programming interface for the operating platform for     \n# which the sample code is written. Notwithstanding anything to the \n# contrary, IBM PROVIDES THE SAMPLE SOURCE CODE ON AN \'AS IS\' BASIS \n# AND IBM DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING,     \n# BUT NOT LIMITED TO, ANY IMPLIED WARRANTIES OR CONDITIONS OF          \n# MERCHANTABILITY, SATISFACTORY QUALITY, FITNESS FOR A PARTICULAR      \n# PURPOSE, TITLE, AND ANY WARRANTY OR CONDITION OF NON-INFRINGEMENT.\n# IBM SHALL NOT BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,     \n# SPECIAL, EXEMPLARY OR ECONOMIC CONSEQUENTIAL DAMAGES ARISING OUT     \n# OF THE USE OR OPERATION OF THE SAMPLE SOURCE CODE. IBM SHALL NOT     \n# BE LIABLE FOR LOSS OF, OR DAMAGE TO, DATA, OR FOR LOST PROFITS,     \n# BUSINESS REVENUE, GOODWILL, OR ANTICIPATED SAVINGS. IBM HAS NO     \n# OBLIGATION TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS \n# OR MODIFICATIONS TO THE SAMPLE SOURCE CODE.                        \n#                                                                   \n#-------------------------------------------------------------------\n\n\'\'\'Trains a simple convnet on the MNIST dataset.\nGets to 99.25% test accuracy after 12 epochs\n(there is still a lot of margin for parameter tuning).\n16 seconds per epoch on a GRID K520 GPU.\n\'\'\'\n\nfrom __future__ import print_function\nimport keras\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\nimport sys\nimport os\n\nbatch_size = 128\nnum_classes = 10\nepochs = 1 # Only one epoch, to keep it quick\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\nmodel_path = os.environ[""RESULT_DIR""] + ""/keras_mnist_cnn.hdf5""\n\ndef main(argv):\n    if len(argv) < 2:\n        sys.exit(""Not enough arguments provided."")\n        \n    global image_path\n\n    i = 1\n    while i <= 2:\n        arg = str(argv[i])\n        if arg == ""--mnistData"":\n            image_path = str(argv[i+1])\n        i += 2\n\nif __name__ == ""__main__"":\n    main(sys.argv)\n\n# the data, shuffled and split between train and test sets\nf = np.load(image_path)\nx_train = f[\'x_train\']\ny_train = f[\'y_train\']\nx_test = f[\'x_test\']\ny_test = f[\'y_test\']\nf.close()\n\nif K.image_data_format() == \'channels_first\':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype(\'float32\')\nx_test = x_test.astype(\'float32\')\nx_train /= 255\nx_test /= 255\nprint(\'x_train shape:\', x_train.shape)\nprint(x_train.shape[0], \'train samples\')\nprint(x_test.shape[0], \'test samples\')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation=\'relu\',\n                 input_shape=input_shape))\nmodel.add(Conv2D(64, (3, 3), activation=\'relu\'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation=\'relu\'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation=\'softmax\'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=[\'accuracy\'])\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n          verbose=1, validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\'Test loss:\', score[0])\nprint(\'Test accuracy:\', score[1])\n\nmodel.save(model_path)\nprint(""Model saved in file: %s"" % model_path)\n'"
etc/examples/pytorch-launch-dist/train_dist_launcher.py,11,"b'""""""\nDistributed Learning using Pytorch\'s torch.distributed.launcher\n""""""\n\nimport time\nimport argparse\nimport sys\nimport os\nimport threading\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom math import ceil\nfrom random import Random\nfrom torch.multiprocessing import Process\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\n\nclass Partition(object):\n    """""" Dataset-like object, but only access a subset of it. """"""\n\n    def __init__(self, data, index):\n        self.data = data\n        self.index = index\n\n    def __len__(self):\n        return len(self.index)\n\n    def __getitem__(self, index):\n        data_idx = self.index[index]\n        return self.data[data_idx]\n\n\nclass DataPartitioner(object):\n    """""" Partitions a dataset into different chuncks. """"""\n\n    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n        self.data = data\n        self.partitions = []\n        rng = Random()\n        rng.seed(seed)\n        data_len = len(data)\n        indexes = [x for x in range(0, data_len)]\n        rng.shuffle(indexes)\n\n        for frac in sizes:\n            part_len = int(frac * data_len)\n            self.partitions.append(indexes[0:part_len])\n            indexes = indexes[part_len:]\n\n    def use(self, partition):\n        return Partition(self.data, self.partitions[partition])\n\n\nclass Net(nn.Module):\n    """""" Network architecture. """"""\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef partition_dataset(batch_size, is_distributed):\n    """""" Partitioning MNIST """"""\n    vision_data = os.environ.get(""DATA_DIR"") + ""/data""\n    dataset = datasets.MNIST(\n        vision_data,\n        train=True,\n        download=True,\n        transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307, ), (0.3081, ))\n        ]))\n    if is_distributed:\n        size = dist.get_world_size()\n    else:\n        size = 1\n    bsz = int(batch_size / float(size))\n    partition_sizes = [1.0 / size for _ in range(size)]\n    partition = DataPartitioner(dataset, partition_sizes)\n    if is_distributed:\n        partition = partition.use(dist.get_rank())\n    else:\n        partition = partition.use(0)\n    train_set = torch.utils.data.DataLoader(\n        partition, batch_size=bsz, shuffle=True)\n    return train_set, bsz\n\n\ndef average_gradients(model):\n    """""" Gradient averaging. """"""\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM, group=0)\n        param.grad.data /= size\n\n\ndef run(rank, world_rank, batch_size, is_gpu):\n    """""" Distributed Synchronous SGD Example """"""\n    torch.manual_seed(1234)\n    size = os.environ.get(""WORLD_SIZE"")\n    train_set, bsz = partition_dataset(batch_size, (not (size == 1)))\n    # For GPU use\n    if is_gpu:\n        device = torch.device(""cuda:{}"".format(rank))\n        model = Net().to(device)\n    else:\n        model = Net()\n        model = model\n#    model = model.cuda(rank)\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\n    num_batches = ceil(len(train_set.dataset) / float(bsz))\n    for epoch in range(10):\n        epoch_loss = 0.0\n        for data, target in train_set:\n            # For GPU use\n            if is_gpu:\n                data, target = data.to(device), target.to(device)\n            else:\n                data, target = Variable(data), Variable(target)\n#            data, target = Variable(data.cuda(rank)), Variable(target.cuda(rank))\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.item()\n            loss.backward()\n            if not (size == 1):\n                average_gradients(model)\n            optimizer.step()\n        print(\'Process \', world_rank,\n              \', epoch \', epoch, \': \',\n              epoch_loss / num_batches)\n\n# Change \'backend\' to appropriate backend identifier\ndef init_processes(local_rank, world_rank, fn, batch_size, is_gpu, backend):\n    """""" Initialize the distributed environment. """"""\n    print(""World Rank: "" + str(world_rank) + ""  Local Rank: "" + str(local_rank)  + "" connected"")\n    dist.init_process_group(backend, init_method=""env://"")\n    print(""GROUP CREATED"")\n    fn(local_rank, world_rank, batch_size, is_gpu)\n\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--local_rank"", type=int)\n    parser.add_argument(""--batch_size"", type=int)\n    args = parser.parse_args()\n    local_rank = args.local_rank\n    batch_size = args.batch_size\n\n    world_rank = os.environ.get(""RANK"")\n\n    backend = \'gloo\'\n    start_time = time.time()\n\n    init_processes(local_rank, world_rank, run, batch_size, True, backend)\n    print(""COMPLETION TIME: "" + str(time.time() - start_time))\n\n    if int(os.environ.get(""LEARNER_ID"")) == 1:\n        print(""Destroying Process Group"")\n        torch.distributed.destroy_process_group()\n    else:\n        while True:\n            time.sleep(1000000)\n'"
etc/examples/pytorch-model/mnist.py,14,"b'#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport os\nimport gzip\nimport codecs\nimport sys\n\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\ndef get_int(b):\n    return int(codecs.encode(b, \'hex\'), 16)\n\ndef parse_byte(b):\n    if isinstance(b, str):\n        return ord(b)\n    return b\n\ndef read_label_file(path):\n    with open(path, \'rb\') as f:\n        data = f.read()\n        assert get_int(data[:4]) == 2049\n        length = get_int(data[4:8])\n        labels = [parse_byte(b) for b in data[8:]]\n        assert len(labels) == length\n        return torch.LongTensor(labels)\n\n\ndef read_image_file(path):\n    with open(path, \'rb\') as f:\n        data = f.read()\n        assert get_int(data[:4]) == 2051\n        length = get_int(data[4:8])\n        num_rows = get_int(data[8:12])\n        num_cols = get_int(data[12:16])\n        images = []\n        idx = 16\n        for l in range(length):\n            img = []\n            images.append(img)\n            for r in range(num_rows):\n                row = []\n                img.append(row)\n                for c in range(num_cols):\n                    row.append(parse_byte(data[idx]))\n                    idx += 1\n        assert len(images) == length\n        return torch.ByteTensor(images).view(-1, 28, 28)\n\n# Training settings\nparser = argparse.ArgumentParser(description=\'PyTorch MNIST Example\')\nparser.add_argument(\'--batch-size\', type=int, default=64, metavar=\'N\',\n                    help=\'input batch size for training (default: 64)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=1000, metavar=\'N\',\n                    help=\'input batch size for testing (default: 1000)\')\nparser.add_argument(\'--epochs\', type=int, default=10, metavar=\'N\',\n                    help=\'number of epochs to train (default: 10)\')\nparser.add_argument(\'--lr\', type=float, default=0.01, metavar=\'LR\',\n                    help=\'learning rate (default: 0.01)\')\nparser.add_argument(\'--momentum\', type=float, default=0.5, metavar=\'M\',\n                    help=\'SGD momentum (default: 0.5)\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'disables CUDA training\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'N\',\n                    help=\'how many batches to wait before logging training status\')\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\nfiles=os.listdir(os.environ[""DATA_DIR""])\nfor filename in files:\n    if \'.gz\' in filename:\n        print(filename)\n        file_path=os.environ[""DATA_DIR""]+\'/\'+filename\n        print(file_path)\n        with open(file_path.replace(\'.gz\', \'\'), \'wb\') as out_f, gzip.GzipFile(file_path) as zip_f:\n            out_f.write(zip_f.read())\n\ntraining_set = (read_image_file(os.environ[""DATA_DIR""]+\'/train-images-idx3-ubyte\'),read_label_file(os.environ[""DATA_DIR""]+\'/train-labels-idx1-ubyte\'))\ntest_set = (read_image_file(os.environ[""DATA_DIR""]+\'/t10k-images-idx3-ubyte\'),read_label_file(os.environ[""DATA_DIR""]+\'/t10k-labels-idx1-ubyte\'))\n\nos.makedirs(os.environ[""RESULT_DIR""]+\'/processed\')\n\nwith open(os.environ[""RESULT_DIR""]+\'/processed/training.pt\', \'wb\') as f:\n    torch.save(training_set, f)\n\nwith open(os.environ[""RESULT_DIR""]+\'/processed/test.pt\', \'wb\') as f:\n    torch.save(test_set, f)\n\nkwargs = {\'num_workers\': 1, \'pin_memory\': True} if args.cuda else {}\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(os.environ[""RESULT_DIR""], train=True, download=False,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=args.batch_size, shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(os.environ[""RESULT_DIR""], train=False, transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\nmodel = Net()\nif args.cuda:\n    model.cuda()\n\noptimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.data[0]))\n\ndef test():\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    test_loss /= len(test_loader.dataset)\n    print(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\nfor epoch in range(1, args.epochs + 1):\n    train(epoch)\n    test()\n\n    \nos.makedirs(os.environ[""RESULT_DIR""]+\'/model\')    \nmodel_path = os.environ[""RESULT_DIR""]+""/model/model.dat""\nprint(""Saving model in {}"".format(model_path))\ntorch.save(model.state_dict(), model_path)\n\nos.system(""(cd $RESULT_DIR/model;tar cvfz ../saved_model.tar.gz .)"")\nprint(str(os.listdir(os.environ[""RESULT_DIR""])))\nprint(os.environ[""RESULT_DIR""])\nsys.stdout.flush()\n'"
etc/examples/tf-distributed/launcher.py,0,"b'#!/usr/bin/env python\n\nimport os\nimport subprocess\nimport sys\n\n#wrapper loop which will run the actual training command\ndef run_training(cmd):\n   process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n   print (""process should have started executing now... {}"".format("" "".join(cmd)))\n   while process.poll() is None:\n      output = process.stdout.readline()\n      if output:\n         print(output.strip().decode(""utf-8""))\n         sys.stdout.flush()\n   rc = process.returncode\n   if rc != 0:\n      raise ValueError(""cmd: {0} exited with code {1}"".format("" "".join(cmd), rc)) \n            \n\nif __name__ ==""__main__"":\n    number_of_learners_str = os.getenv(""NUM_LEARNERS"") \n    number_of_learners = int(number_of_learners_str)\n\n    learner_name_prefix = os.getenv(""LEARNER_NAME_PREFIX"") #base name for all learners created, individual learners as separate by <>-1, <>-2 and so on\n    number_of_ps_hosts_str = os.getenv(""PS_HOSTS_COUNT"",1)\n    number_of_ps_hosts = int(number_of_ps_hosts_str)\n    \n    learner_id_str = os.getenv(""LEARNER_ID"") # id for the learner. depending on the learner on which this code executes you get a value of  1, 2, 3..\n    learner_id = int(learner_id_str)\n\n    # TODO \n    # error out if any of the above values are not defined\n\n    job_name = ""ps"" if int(learner_id) <= number_of_ps_hosts else ""worker""\n    task_index = learner_id - 1  if job_name == ""ps"" else learner_id - 1 - number_of_ps_hosts  #learner_id starts from 1, so minus 1\n    \n    ps_hosts = []\n    worker_hosts = []\n    #host name needs to be in the format of host.domain learner_name_prefix-<id> is the host and learner_name_prefix is the domain\n    # and hence the format learner_name_prefix-<id>.learner_name_prefix\n    for i in range(0, number_of_ps_hosts):\n        ps_hosts.append(""{}-{}.{}:2222"".format(learner_name_prefix, i, learner_name_prefix))\n\n    for i in range(number_of_ps_hosts, number_of_learners):\n       worker_hosts.append(""{}-{}.{}:2222"".format(learner_name_prefix, i, learner_name_prefix)) \n    \n    command = sys.argv[1:] # base command\n    command.append(""--job_name={}"".format(job_name))\n    command.append(""--ps_hosts={}"".format("","".join(ps_hosts)))\n    command.append(""--worker_hosts={}"".format("","".join(worker_hosts)))\n    command.append(""--task_index={0}"".format(task_index)) \n\n    print (""executing command {}"".format(command))\n    run_training(command)\n\n\n    #the above should output the command like below\n\n    # desginating learner with ID 1 as ps and everyone else as worker\n    #python mnist_dist.py \\\n    # --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \\\n    # --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \\\n    # --job_name=ps --task_index=0\n'"
etc/examples/tf-distributed/mnist_dist_savemodel.py,0,"b'#-----------------------------------------------------------------------\n# This information contains sample code provided in source code form.\n# You may copy, modify, and distribute these sample programs in any \n# form without payment to IBM for the purposes of developing, using,\n# marketing or distributing application programs conforming to the     \n# application programming interface for the operating platform for     \n# which the sample code is written. Notwithstanding anything to the \n# contrary, IBM PROVIDES THE SAMPLE SOURCE CODE ON AN \'AS IS\' BASIS \n# AND IBM DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING,     \n# BUT NOT LIMITED TO, ANY IMPLIED WARRANTIES OR CONDITIONS OF          \n# MERCHANTABILITY, SATISFACTORY QUALITY, FITNESS FOR A PARTICULAR      \n# PURPOSE, TITLE, AND ANY WARRANTY OR CONDITION OF NON-INFRINGEMENT.\n# IBM SHALL NOT BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,     \n# SPECIAL, EXEMPLARY OR ECONOMIC CONSEQUENTIAL DAMAGES ARISING OUT     \n# OF THE USE OR OPERATION OF THE SAMPLE SOURCE CODE. IBM SHALL NOT     \n# BE LIABLE FOR LOSS OF, OR DAMAGE TO, DATA, OR FOR LOST PROFITS,     \n# BUSINESS REVENUE, GOODWILL, OR ANTICIPATED SAVINGS. IBM HAS NO     \n# OBLIGATION TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS \n# OR MODIFICATIONS TO THE SAMPLE SOURCE CODE.                        \n#-------------------------------------------------------------------\n\nimport os\nimport sys\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nflags = tf.app.flags\nflags.DEFINE_string(""data_dir"", os.getenv(""DATA_DIR"", ""/tmp/mnist-data""), ""Directory for storing mnist data"")\nflags.DEFINE_string(""model_dir"", os.getenv(""RESULT_DIR"", "".""), ""Directory for storing trained model"")\nflags.DEFINE_string(""ps_hosts"", """", ""Comma-separated list of hostname:port pairs"")\nflags.DEFINE_string(""worker_hosts"", """", ""Comma-separated list of hostname:port pairs"")\nflags.DEFINE_string(""job_name"", """", ""One of \'ps\', \'worker\'"")\nflags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")\nflags.DEFINE_integer(""batch_size"", 100, ""Index of task within the job"")\nflags.DEFINE_integer(\'model_version\', 1, \'version number of the model.\')\nFLAGS = flags.FLAGS\n\n\ndef main(_):\n    ps_hosts = FLAGS.ps_hosts.split("","")\n\n    worker_hosts = FLAGS.worker_hosts.split("","")\n    cluster = tf.train.ClusterSpec({""ps"": ps_hosts, ""worker"": worker_hosts})\n    server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\n\n    if FLAGS.job_name == ""ps"":\n        server.join()\n    elif FLAGS.job_name == ""worker"":\n        train(server, cluster)\n\n\ndef train(server, cluster):\n    print(\'Training model...\')\n    with tf.device(tf.train.replica_device_setter(worker_device=""/job:worker/task:%d"" % FLAGS.task_index, cluster=cluster)):\n        mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n        serialized_tf_example = tf.placeholder(tf.string, name=\'tf_example\')\n        feature_configs = {\'x\': tf.FixedLenFeature(shape=[784], dtype=tf.float32), }\n        tf_example = tf.parse_example(serialized_tf_example, feature_configs)\n\n        x = tf.identity(tf_example[\'x\'], name=\'x\')  # use tf.identity() to assign name\n        y_ = tf.placeholder(\'float\', shape=[None, 10])\n\n        w = tf.Variable(tf.zeros([784, 10]))\n        b = tf.Variable(tf.zeros([10]))\n\n        y = tf.nn.softmax(tf.matmul(x, w) + b, name=\'y\')\n        cross_entropy = -tf.reduce_sum(y_ * tf.log(y))\n\n        global_step = tf.Variable(0)\n        train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy, global_step=global_step)\n        values, indices = tf.nn.top_k(y, 10)\n\n        # Use xrange in python2, range in python3\n        try:\n          xrange\n        except NameError:\n          xrange = range\n        prediction_classes = tf.contrib.lookup.index_to_string(tf.to_int64(indices), mapping=tf.constant([str(i) for i in xrange(10)]))\n        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \'float\'))\n\n        summary_op = tf.summary.merge_all()\n        init_op = tf.global_variables_initializer()\n        saver = tf.train.Saver()\n\n        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0), logdir=""train_logs"", init_op=init_op,\n                                 summary_op=summary_op, saver=saver, global_step=global_step, save_model_secs=600)\n\n        with sv.managed_session(server.target) as sess:\n            step = 0\n\n            while not sv.should_stop() and step < 1000:\n                batch_xs, batch_ys = mnist.train.next_batch(FLAGS.batch_size)\n                train_feed = {x: batch_xs, y_: batch_ys}\n\n                _, step = sess.run([train_step, global_step], feed_dict=train_feed)\n\n                if step % 1000 == 0:\n                    print(""global step: {} , accuracy:{}"".format(step, sess.run(accuracy, feed_dict=train_feed)))\n\n            print(\'training accuracy %g\' % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n            print(\'Done training!\')\n            if sv.is_chief:\n                sess.graph._unsafe_unfinalize()\n                export_path_base = str(FLAGS.model_dir)\n                export_path = os.path.join(\n                    tf.compat.as_bytes(export_path_base),\n                    tf.compat.as_bytes(str(FLAGS.model_version)))\n                print(\'Exporting trained model to\', export_path)\n                builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n\n                # Build the signature_def_map.\n                classification_inputs = tf.saved_model.utils.build_tensor_info(serialized_tf_example)\n                classification_outputs_classes = tf.saved_model.utils.build_tensor_info(prediction_classes)\n                classification_outputs_scores = tf.saved_model.utils.build_tensor_info(values)\n\n                classification_signature = tf.saved_model.signature_def_utils.build_signature_def(\n                    inputs={tf.saved_model.signature_constants.CLASSIFY_INPUTS: classification_inputs},\n                    outputs={\n                        tf.saved_model.signature_constants.CLASSIFY_OUTPUT_CLASSES: classification_outputs_classes,\n                        tf.saved_model.signature_constants.CLASSIFY_OUTPUT_SCORES: classification_outputs_scores\n                    },\n                    method_name=tf.saved_model.signature_constants.CLASSIFY_METHOD_NAME)\n\n                tensor_info_x = tf.saved_model.utils.build_tensor_info(x)\n                tensor_info_y = tf.saved_model.utils.build_tensor_info(y)\n\n                prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(\n                    inputs={\'images\': tensor_info_x},\n                    outputs={\'scores\': tensor_info_y},\n                    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n\n                legacy_init_op = tf.group(tf.tables_initializer(), name=\'legacy_init_op\')\n\n                builder.add_meta_graph_and_variables(\n                    sess, [tf.saved_model.tag_constants.SERVING],\n                    signature_def_map={\n                        \'predict_images\': prediction_signature,\n                        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: classification_signature,\n                    },\n                    legacy_init_op=legacy_init_op,\n                    clear_devices=True)\n\n                builder.save()\n                print(\'Done exporting!\')\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
etc/examples/tf-model/convolutional_network.py,0,"b'\'\'\'\nA Convolutional Network implementation example using TensorFlow library.\nThis example is using the MNIST database of handwritten digits\n(http://yann.lecun.com/exdb/mnist/)\n\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n\'\'\'\n\nimport tensorflow as tf\nimport input_data\nimport sys\nimport os\nimport itertools\nimport re\nimport time\nfrom random import randint\n\ntrain_images_file = """"\ntrain_labels_file = """"\ntest_images_file = """"\ntest_labels_file = """"\n\n# Parameters\nlearning_rate = 0.001\ntraining_iters = 200000\nbatch_size = 128\ndisplay_step = 10\n\nmodel_path = os.environ[""RESULT_DIR""]+""/model""\n\n# This helps distinguish instances when the training job is restarted.\ninstance_id = randint(0,9999)\n\ndef main(argv):\n\n    if len(argv) < 12:\n        sys.exit(""Not enough arguments provided."")\n\n    global train_images_file, train_labels_file, test_images_file, test_labels_file, learning_rate, training_iters\n\n    i = 1\n    while i <= 12:\n        arg = str(argv[i])\n        if arg == ""--trainImagesFile"":\n            train_images_file = str(argv[i+1])\n        elif arg == ""--trainLabelsFile"":\n            train_labels_file = str(argv[i+1])\n        elif arg == ""--testImagesFile"":\n            test_images_file = str(argv[i+1])\n        elif arg == ""--testLabelsFile"":\n            test_labels_file = str(argv[i+1])\n        elif arg == ""--learningRate"":\n            learning_rate = float(argv[i+1])\n        elif arg ==""--trainingIters"":\n            training_iters = int(argv[i+1])\n        i += 2\n\nif __name__ == ""__main__"":\n    main(sys.argv)\n\n# Import MNIST data\nmnist = input_data.read_data_sets(train_images_file,\n    train_labels_file, test_images_file, test_labels_file, one_hot=True)\n\n# Network Parameters\nn_input = 784 # MNIST data input (img shape: 28*28)\nn_classes = 10 # MNIST total classes (0-9 digits)\ndropout = 0.75 # Dropout, probability to keep units\n\n# tf Graph input\nx = tf.placeholder(tf.float32, [None, n_input], name=""x_input"")\ny = tf.placeholder(tf.float32, [None, n_classes])\n\n# Create some wrappers for simplicity\ndef conv2d(x, W, b, strides=1):\n    # Conv2D wrapper, with bias and relu activation\n    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=\'SAME\')\n    x = tf.nn.bias_add(x, b)\n    return tf.nn.relu(x)\n\n\ndef maxpool2d(x, k=2):\n    # MaxPool2D wrapper\n    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n                          padding=\'SAME\')\n\n\n# Create model\ndef conv_net(x, weights, biases, dropout):\n    # Reshape input picture\n    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n\n    # Convolution Layer\n    conv1 = conv2d(x, weights[\'wc1\'], biases[\'bc1\'])\n    # Max Pooling (down-sampling)\n    conv1 = maxpool2d(conv1, k=2)\n\n    # Convolution Layer\n    conv2 = conv2d(conv1, weights[\'wc2\'], biases[\'bc2\'])\n    # Max Pooling (down-sampling)\n    conv2 = maxpool2d(conv2, k=2)\n\n    # Fully connected layer\n    # Reshape conv2 output to fit fully connected layer input\n    fc1 = tf.reshape(conv2, [-1, weights[\'wd1\'].get_shape().as_list()[0]])\n    fc1 = tf.add(tf.matmul(fc1, weights[\'wd1\']), biases[\'bd1\'])\n    fc1 = tf.nn.relu(fc1)\n    # Apply Dropout\n    fc1 = tf.nn.dropout(fc1, dropout)\n\n    # Output, class prediction\n    out = tf.add(tf.matmul(fc1, weights[\'out\']), biases[\'out\'])\n    return out\n\n# Store layers weight & bias\nweights = {\n    # 5x5 conv, 1 input, 32 outputs\n    \'wc1\': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n    # 5x5 conv, 32 inputs, 64 outputs\n    \'wc2\': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n    # fully connected, 7*7*64 inputs, 1024 outputs\n    \'wd1\': tf.Variable(tf.random_normal([7*7*64, 1024])),\n    # 1024 inputs, 10 outputs (class prediction)\n    \'out\': tf.Variable(tf.random_normal([1024, n_classes]))\n}\n\nbiases = {\n    \'bc1\': tf.Variable(tf.random_normal([32])),\n    \'bc2\': tf.Variable(tf.random_normal([64])),\n    \'bd1\': tf.Variable(tf.random_normal([1024])),\n    \'out\': tf.Variable(tf.random_normal([n_classes]))\n}\n\n# Construct model\npred = conv_net(x, weights, biases, dropout)\n\nscores = tf.nn.softmax(pred, name=""y_output"")\n\n# Define loss and optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\npredictor = tf.argmax(pred, 1, name=""predictor"")\n\n# Evaluate model\ncorrect_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n\n    step = 1\n    # Keep training until reach max iterations\n    while step * batch_size < training_iters:\n        batch_x, batch_y = mnist.train.next_batch(batch_size)\n        # Run optimization op (backprop)\n        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n        if step % display_step == 0:\n\n            # Calculate batch loss and accuracy\n            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n                                                              y: batch_y})\n            print(""Time "" + ""{:.4f}"".format(time.time()) + \\\n                  "", instance "" + str(instance_id) + \\\n                  "", Iter "" + str(step * batch_size) + \\\n                  "", Minibatch Loss= "" + ""{:.6f}"".format(loss) + \\\n                  "", Training Accuracy= "" + ""{:.5f}"".format(acc))\n            sys.stdout.flush()\n        step += 1\n    print(""Optimization Finished!"")\n\n    classification_inputs = tf.saved_model.utils.build_tensor_info(x)\n    classification_outputs_classes = tf.saved_model.utils.build_tensor_info(predictor)\n    classification_outputs_scores = tf.saved_model.utils.build_tensor_info(scores)\n    \n\n    classification_signature = (\n        tf.saved_model.signature_def_utils.build_signature_def(\n            inputs={\n                tf.saved_model.signature_constants.CLASSIFY_INPUTS:\n                    classification_inputs\n            },\n            outputs={\n                tf.saved_model.signature_constants.CLASSIFY_OUTPUT_CLASSES:\n                    classification_outputs_classes,\n                tf.saved_model.signature_constants.CLASSIFY_OUTPUT_SCORES:\n                    classification_outputs_scores\n            },\n            method_name=tf.saved_model.signature_constants.CLASSIFY_METHOD_NAME))\n\n    print(""classification_signature content:"")\n    print(classification_signature)\n\n    # Calculate accuracy for 256 mnist test images\n    print(""Testing Accuracy:"", \\\n        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n                                      y: mnist.test.labels[:256]}))\n\n\n    builder = tf.saved_model.builder.SavedModelBuilder(model_path)\n    legacy_init_op = tf.group(tf.tables_initializer(), name=\'legacy_init_op\')\n    builder.add_meta_graph_and_variables(\n        sess, [tf.saved_model.tag_constants.SERVING],\n        signature_def_map={\n            \'predict_images\': classification_signature,\n        },\n        legacy_init_op=legacy_init_op)\n\n    save_path = str(builder.save())\n\n    # save_path = saver.save(sess, model_path)\n    print(""Model saved in file: %s"" % save_path)\n\n    os.system(""(cd $RESULT_DIR/model;tar cvfz ../saved_model.tar.gz .)"")\n    print(str(os.listdir(os.environ[""RESULT_DIR""])))\n    print(os.environ[""RESULT_DIR""])\n    sys.stdout.flush()\n'"
etc/examples/tf-model/input_data.py,0,"b'\n#!/usr/bin/env python\n\n""""""Functions for downloading and reading MNIST data.""""""\nimport gzip\nfrom six.moves import xrange\nfrom six.moves.urllib.request import urlretrieve\nimport numpy\nimport os\n\ndef _read32(bytestream):\n    dt = numpy.dtype(numpy.uint32).newbyteorder(\'>\')\n    return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\n\n\ndef extract_images(filename):\n    """"""Extract the images into a 4D uint8 numpy array [index, y, x, depth].""""""\n    print(\'Extracting\', filename)\n    with gzip.open(filename) as bytestream:\n        magic = _read32(bytestream)\n        if magic != 2051:\n            raise ValueError(\n                \'Invalid magic number %d in MNIST image file: %s\' %\n                (magic, filename))\n        num_images = _read32(bytestream)\n        rows = _read32(bytestream)\n        cols = _read32(bytestream)\n        buf = bytestream.read(rows * cols * num_images)\n        data = numpy.frombuffer(buf, dtype=numpy.uint8)\n        data = data.reshape(num_images, rows, cols, 1)\n        return data\n\n\ndef dense_to_one_hot(labels_dense, num_classes=10):\n    """"""Convert class labels from scalars to one-hot vectors.""""""\n    num_labels = labels_dense.shape[0]\n    index_offset = numpy.arange(num_labels) * num_classes\n    labels_one_hot = numpy.zeros((num_labels, num_classes))\n    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n    return labels_one_hot\n\n\ndef extract_labels(filename, one_hot=False):\n    """"""Extract the labels into a 1D uint8 numpy array [index].""""""\n    print(\'Extracting\', filename)\n    with gzip.open(filename) as bytestream:\n        magic = _read32(bytestream)\n        if magic != 2049:\n            raise ValueError(\n                \'Invalid magic number %d in MNIST label file: %s\' %\n                (magic, filename))\n        num_items = _read32(bytestream)\n        buf = bytestream.read(num_items)\n        labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n        if one_hot:\n            return dense_to_one_hot(labels)\n        return labels\n\n\nclass DataSet(object):\n    def __init__(self, images, labels, fake_data=False):\n        if fake_data:\n            self._num_examples = 10000\n        else:\n            assert images.shape[0] == labels.shape[0], (\n                ""images.shape: %s labels.shape: %s"" % (images.shape,\n                                                       labels.shape))\n            self._num_examples = images.shape[0]\n            # Convert shape from [num examples, rows, columns, depth]\n            # to [num examples, rows*columns] (assuming depth == 1)\n            assert images.shape[3] == 1\n            images = images.reshape(images.shape[0],\n                                    images.shape[1] * images.shape[2])\n            # Convert from [0, 255] -> [0.0, 1.0].\n            images = images.astype(numpy.float32)\n            images = numpy.multiply(images, 1.0 / 255.0)\n        self._images = images\n        self._labels = labels\n        self._epochs_completed = 0\n        self._index_in_epoch = 0\n\n    @property\n    def images(self):\n        return self._images\n\n    @property\n    def labels(self):\n        return self._labels\n\n    @property\n    def num_examples(self):\n        return self._num_examples\n\n    @property\n    def epochs_completed(self):\n        return self._epochs_completed\n\n    def next_batch(self, batch_size, fake_data=False):\n        """"""Return the next `batch_size` examples from this data set.""""""\n        if fake_data:\n            fake_image = [1.0 for _ in xrange(784)]\n            fake_label = 0\n            return [fake_image for _ in xrange(batch_size)], [\n                fake_label for _ in xrange(batch_size)]\n        start = self._index_in_epoch\n        self._index_in_epoch += batch_size\n        if self._index_in_epoch > self._num_examples:\n            # Finished epoch\n            self._epochs_completed += 1\n            # Shuffle the data\n            perm = numpy.arange(self._num_examples)\n            numpy.random.shuffle(perm)\n            self._images = self._images[perm]\n            self._labels = self._labels[perm]\n            # Start next epoch\n            start = 0\n            self._index_in_epoch = batch_size\n            assert batch_size <= self._num_examples\n        end = self._index_in_epoch\n        return self._images[start:end], self._labels[start:end]\n\n\ndef read_data_sets(train_images_file, train_labels_file, test_images_file, test_labels_file, fake_data=False, one_hot=False):\n    class DataSets(object):\n        pass\n    data_sets = DataSets()\n    if fake_data:\n        data_sets.train = DataSet([], [], fake_data=True)\n        data_sets.validation = DataSet([], [], fake_data=True)\n        data_sets.test = DataSet([], [], fake_data=True)\n        return data_sets\n    TRAIN_IMAGES = train_images_file\n    TRAIN_LABELS = train_labels_file\n    TEST_IMAGES = test_images_file\n    TEST_LABELS = test_labels_file\n    VALIDATION_SIZE = 5000\n    train_images = extract_images(TRAIN_IMAGES)\n    train_labels = extract_labels(TRAIN_LABELS, one_hot=one_hot)\n    test_images = extract_images(TEST_IMAGES)\n    test_labels = extract_labels(TEST_LABELS, one_hot=one_hot)\n    validation_images = train_images[:VALIDATION_SIZE]\n    validation_labels = train_labels[:VALIDATION_SIZE]\n    train_images = train_images[VALIDATION_SIZE:]\n    train_labels = train_labels[VALIDATION_SIZE:]\n    data_sets.train = DataSet(train_images, train_labels)\n    data_sets.validation = DataSet(validation_images, validation_labels)\n    data_sets.test = DataSet(test_images, test_labels)\n    return data_sets\n'"
etc/examples/tf-summary-model/input_data.py,0,"b'#!/usr/bin/env python\n""""""Functions for downloading and reading MNIST data.""""""\n\nimport gzip\nfrom six.moves import xrange\nfrom six.moves.urllib.request import urlretrieve\nimport numpy\nimport os\n\ndef _read32(bytestream):\n    dt = numpy.dtype(numpy.uint32).newbyteorder(\'>\')\n    return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\n\n\ndef extract_images(filename):\n    """"""Extract the images into a 4D uint8 numpy array [index, y, x, depth].""""""\n    print(\'Extracting\', filename)\n    with gzip.open(filename) as bytestream:\n        magic = _read32(bytestream)\n        if magic != 2051:\n            raise ValueError(\n                \'Invalid magic number %d in MNIST image file: %s\' %\n                (magic, filename))\n        num_images = _read32(bytestream)\n        rows = _read32(bytestream)\n        cols = _read32(bytestream)\n        buf = bytestream.read(rows * cols * num_images)\n        data = numpy.frombuffer(buf, dtype=numpy.uint8)\n        data = data.reshape(num_images, rows, cols, 1)\n        return data\n\n\ndef dense_to_one_hot(labels_dense, num_classes=10):\n    """"""Convert class labels from scalars to one-hot vectors.""""""\n    num_labels = labels_dense.shape[0]\n    index_offset = numpy.arange(num_labels) * num_classes\n    labels_one_hot = numpy.zeros((num_labels, num_classes))\n    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n    return labels_one_hot\n\n\ndef extract_labels(filename, one_hot=False):\n    """"""Extract the labels into a 1D uint8 numpy array [index].""""""\n    print(\'Extracting\', filename)\n    with gzip.open(filename) as bytestream:\n        magic = _read32(bytestream)\n        if magic != 2049:\n            raise ValueError(\n                \'Invalid magic number %d in MNIST label file: %s\' %\n                (magic, filename))\n        num_items = _read32(bytestream)\n        buf = bytestream.read(num_items)\n        labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n        if one_hot:\n            return dense_to_one_hot(labels)\n        return labels\n\n\nclass DataSet(object):\n    def __init__(self, images, labels, fake_data=False):\n        if fake_data:\n            self._num_examples = 10000\n        else:\n            assert images.shape[0] == labels.shape[0], (\n                ""images.shape: %s labels.shape: %s"" % (images.shape,\n                                                       labels.shape))\n            self._num_examples = images.shape[0]\n            # Convert shape from [num examples, rows, columns, depth]\n            # to [num examples, rows*columns] (assuming depth == 1)\n            assert images.shape[3] == 1\n            images = images.reshape(images.shape[0],\n                                    images.shape[1] * images.shape[2])\n            # Convert from [0, 255] -> [0.0, 1.0].\n            images = images.astype(numpy.float32)\n            images = numpy.multiply(images, 1.0 / 255.0)\n        self._images = images\n        self._labels = labels\n        self._epochs_completed = 0\n        self._index_in_epoch = 0\n\n    @property\n    def images(self):\n        return self._images\n\n    @property\n    def labels(self):\n        return self._labels\n\n    @property\n    def num_examples(self):\n        return self._num_examples\n\n    @property\n    def epochs_completed(self):\n        return self._epochs_completed\n\n    def next_batch(self, batch_size, fake_data=False):\n        """"""Return the next `batch_size` examples from this data set.""""""\n        if fake_data:\n            fake_image = [1.0 for _ in xrange(784)]\n            fake_label = 0\n            return [fake_image for _ in xrange(batch_size)], [\n                fake_label for _ in xrange(batch_size)]\n        start = self._index_in_epoch\n        self._index_in_epoch += batch_size\n        if self._index_in_epoch > self._num_examples:\n            # Finished epoch\n            self._epochs_completed += 1\n            # Shuffle the data\n            perm = numpy.arange(self._num_examples)\n            numpy.random.shuffle(perm)\n            self._images = self._images[perm]\n            self._labels = self._labels[perm]\n            # Start next epoch\n            start = 0\n            self._index_in_epoch = batch_size\n            assert batch_size <= self._num_examples\n        end = self._index_in_epoch\n        return self._images[start:end], self._labels[start:end]\n\n\ndef read_data_sets(train_images_file, train_labels_file, test_images_file, test_labels_file, fake_data=False, one_hot=False):\n    class DataSets(object):\n        pass\n    data_sets = DataSets()\n    if fake_data:\n        data_sets.train = DataSet([], [], fake_data=True)\n        data_sets.validation = DataSet([], [], fake_data=True)\n        data_sets.test = DataSet([], [], fake_data=True)\n        return data_sets\n    TRAIN_IMAGES = train_images_file\n    TRAIN_LABELS = train_labels_file\n    TEST_IMAGES = test_images_file\n    TEST_LABELS = test_labels_file\n    VALIDATION_SIZE = 5000\n    train_images = extract_images(TRAIN_IMAGES)\n    train_labels = extract_labels(TRAIN_LABELS, one_hot=one_hot)\n    test_images = extract_images(TEST_IMAGES)\n    test_labels = extract_labels(TEST_LABELS, one_hot=one_hot)\n    validation_images = train_images[:VALIDATION_SIZE]\n    validation_labels = train_labels[:VALIDATION_SIZE]\n    train_images = train_images[VALIDATION_SIZE:]\n    train_labels = train_labels[VALIDATION_SIZE:]\n    data_sets.train = DataSet(train_images, train_labels)\n    data_sets.validation = DataSet(validation_images, validation_labels)\n    data_sets.test = DataSet(test_images, test_labels)\n    return data_sets\n'"
etc/examples/tf-summary-model/mnist_with_summaries.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \'AS IS\' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A simple MNIST classifier which displays summaries in TensorBoard.\n\n This is an unimpressive MNIST model, but it is a good example of using\ntf.name_scope to make a graph legible in the TensorBoard graph explorer, and of\nnaming summary tags so that they are grouped meaningfully in TensorBoard.\n\nIt demonstrates the functionality of every TensorBoard dashboard.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport itertools\nimport os\nimport re\nimport sys\n\nimport tensorflow as tf\n\nimport input_data\n\n# from tensorflow.examples.tutorials.mnist import input_data\n\nFLAGS = None\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\nglobal train_images_file, train_labels_file, test_images_file\nglobal test_labels_file, learning_rate\n# global saver\n\n# Assuming the checkpoint filename is ""xyz-nnn"", return nnn (as an int).\ndef get_step_from_checkpoint_name(filename):\n    m = re.match("".*-([0-9]+)$"", filename)\n    if m is not None:\n        return int(m.group(1))\n    else:\n        return None\n\n# Restore from a checkpoint if possible, and return the step count of the checkpoint.\ndef restore_from_checkpoint(saver, sess):\n    cs = tf.train.get_checkpoint_state(os.environ[""RESULT_DIR""])\n    checkpoints = []\n    if cs is not None:\n        checkpoints = itertools.chain([cs.model_checkpoint_path], reversed(cs.all_model_checkpoint_paths))\n    step = None\n    for c in checkpoints:\n        try:\n            saver.restore(sess, c)\n            print(""Restored from checkpoint: %s"" % c)\n            step = get_step_from_checkpoint_name(c)\n            break\n        except tf.errors.NotFoundError:\n            print(""Warning: checkpoint not found: %s"" % c)\n        except tf.errors.DataLossError:\n            print(""Warning: checkpoint corrupted: %s"" % c)\n    return step\n\ndef save_checkpoint(step, saver, sess, model_path):\n    path = saver.save(sess, model_path, global_step=step)\n    print(""Saved checkpoint %s"" % path)\n\ndef train():\n\n    # Import data\n    #   mnist = input_data.read_data_sets(FLAGS.data_dir,\n    #                                     one_hot=True,\n    #                                     fake_data=FLAGS.fake_data)\n    mnist = input_data.read_data_sets(FLAGS.train_images_file,\n                                      FLAGS.train_labels_file,\n                                      FLAGS.test_images_file,\n                                      FLAGS.test_labels_file,\n                                      one_hot=True)\n\n    sess = tf.InteractiveSession()\n\n    # Create a multilayer model.\n\n    # Input placeholders\n    with tf.name_scope(\'input\'):\n        x = tf.placeholder(tf.float32, [None, 784], name=\'x-input\')\n        y_ = tf.placeholder(tf.float32, [None, 10], name=\'y-input\')\n\n    with tf.name_scope(\'input_reshape\'):\n        image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n        tf.summary.image(\'input\', image_shaped_input, 10)\n\n    # We can\'t initialize these variables to 0 - the network will get stuck.\n    def weight_variable(shape):\n        """"""Create a weight variable with appropriate initialization.""""""\n        initial = tf.truncated_normal(shape, stddev=0.1)\n        return tf.Variable(initial)\n\n    def bias_variable(shape):\n        """"""Create a bias variable with appropriate initialization.""""""\n        initial = tf.constant(0.1, shape=shape)\n        return tf.Variable(initial)\n\n    def variable_summaries(var):\n        """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""\n        with tf.name_scope(\'summaries\'):\n            mean = tf.reduce_mean(var)\n            tf.summary.scalar(\'mean\', mean)\n            with tf.name_scope(\'stddev\'):\n                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n            tf.summary.scalar(\'stddev\', stddev)\n            tf.summary.scalar(\'max\', tf.reduce_max(var))\n            tf.summary.scalar(\'min\', tf.reduce_min(var))\n            tf.summary.histogram(\'histogram\', var)\n\n    def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n        """"""Reusable code for making a simple neural net layer.\n\n        It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n        It also sets up name scoping so that the resultant graph is easy to read,\n        and adds a number of summary ops.\n        """"""\n        # Adding a name scope ensures logical grouping of the layers in the graph.\n        with tf.name_scope(layer_name):\n            # This Variable will hold the state of the weights for the layer\n            with tf.name_scope(\'weights\'):\n                weights = weight_variable([input_dim, output_dim])\n                variable_summaries(weights)\n            with tf.name_scope(\'biases\'):\n                biases = bias_variable([output_dim])\n                variable_summaries(biases)\n            with tf.name_scope(\'Wx_plus_b\'):\n                preactivate = tf.matmul(input_tensor, weights) + biases\n                tf.summary.histogram(\'pre_activations\', preactivate)\n            activations = act(preactivate, name=\'activation\')\n            tf.summary.histogram(\'activations\', activations)\n            return activations\n\n    hidden1 = nn_layer(x, 784, 500, \'layer1\')\n\n    with tf.name_scope(\'dropout\'):\n        keep_prob = tf.placeholder(tf.float32)\n        tf.summary.scalar(\'dropout_keep_probability\', keep_prob)\n        dropped = tf.nn.dropout(hidden1, keep_prob)\n\n    # Do not apply softmax activation yet, see below.\n    y = nn_layer(dropped, 500, 10, \'layer2\', act=tf.identity)\n\n    with tf.name_scope(\'cross_entropy\'):\n        # The raw formulation of cross-entropy,\n        #\n        # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),\n        #                               reduction_indices=[1]))\n        #\n        # can be numerically unstable.\n        #\n        # So here we use tf.nn.softmax_cross_entropy_with_logits on the\n        # raw outputs of the nn_layer above, and then average across\n        # the batch.\n        diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n        with tf.name_scope(\'total\'):\n            cross_entropy = tf.reduce_mean(diff)\n    tf.summary.scalar(\'cross_entropy\', cross_entropy)\n\n    with tf.name_scope(\'train\'):\n        train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\n            cross_entropy)\n\n    with tf.name_scope(\'accuracy\'):\n        with tf.name_scope(\'correct_prediction\'):\n            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n        with tf.name_scope(\'accuracy\'):\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    tf.summary.scalar(\'accuracy\', accuracy)\n\n    #     with tf.name_scope(\'loss\'):\n    #         # loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=mnist.test.images, labels=mnist.test.labels))\n    #         loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=y))\n    #         # loss = 10.5\n    #     tf.summary.scalar(\'loss\', loss)\n\n    # Merge all the summaries and write them out to /tmp/tensorflow/mnist/logs/tb (by default)\n    merged = tf.summary.merge_all()\n    train_writer = tf.summary.FileWriter(FLAGS.tb_log_dir + \'/train\', sess.graph)\n    test_writer = tf.summary.FileWriter(FLAGS.tb_log_dir + \'/test\')\n    tf.global_variables_initializer().run()\n\n    # Train the model, and also write summaries.\n    # Every 10th step, measure test-set accuracy, and write test summaries\n    # All other steps, run train_step on training data, & add training summaries\n\n    def feed_dict(train):\n        """"""Make a TensorFlow feed_dict: maps data onto Tensor placeholders.""""""\n        if train or FLAGS.fake_data:\n            xs, ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)\n            k = FLAGS.dropout\n        else:\n            xs, ys = mnist.test.images, mnist.test.labels\n            k = 1.0\n        return {x: xs, y_: ys, keep_prob: k}\n\n    model_path = os.environ[""RESULT_DIR""] + ""/model.ckpt""\n\n    print(\'Calling tf.train.Saver(...)\')\n    saver = tf.train.Saver(max_to_keep=1)\n\n    step = restore_from_checkpoint(saver, sess)\n    if step is None:\n        step = 1\n\n    for i in range(step-1, FLAGS.max_steps):\n        if i % 10 == 0:  # Record summaries and test-set accuracy\n            save_checkpoint(step, saver, sess, model_path)\n            summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n            test_writer.add_summary(summary, i)\n            print(\'Accuracy at step %s: %s\' % (i, acc))\n        else:  # Record train set summaries, and train\n            # Commnent the below code since CUPTI library is not Available on basic\n            # TensorFlow images that use CUDA 9.\n            #\n            # if i % 100 == 99:  # Record execution stats\n            #     run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n            #     run_metadata = tf.RunMetadata()\n            #     # summary, _, loss_result = sess.run([merged, train_step, loss],\n            #     summary, _ = sess.run([merged, train_step],\n            #                           feed_dict=feed_dict(True),\n            #                           options=run_options,\n            #                           run_metadata=run_metadata)\n            #     train_writer.add_run_metadata(run_metadata, \'step%03d\' % i)\n            #     train_writer.add_summary(summary, i)\n            #     print(\'Adding run metadata for\', i)\n            # else:  # Record a summary\n            summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n            train_writer.add_summary(summary, i)\n\n    save_path = saver.save(sess, model_path)\n\n    train_writer.close()\n    test_writer.close()\n\ndef main(_):\n    print(\'train_images_file: \', FLAGS.train_images_file)\n    print(\'train_labels_file: \', FLAGS.train_labels_file)\n    print(\'test_images_file: \', FLAGS.test_images_file)\n    print(\'test_labels_file: \', FLAGS.test_labels_file)\n    print(\'learning_rate: \', FLAGS.learning_rate)\n    print(\'tb_log_dir: \', FLAGS.tb_log_dir)\n    print(\'log_dir: \', FLAGS.log_dir)\n\n    # if tf.gfile.Exists(FLAGS.log_dir):\n    #    tf.gfile.DeleteRecursively(FLAGS.log_dir)\n    tf.gfile.MakeDirs(FLAGS.log_dir)\n\n    # if tf.gfile.Exists(FLAGS.tb_log_dir):\n    #    tf.gfile.DeleteRecursively(FLAGS.tb_log_dir)\n    tf.gfile.MakeDirs(FLAGS.tb_log_dir)\n\n    train()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--fake_data\', nargs=\'?\', const=True, type=bool,\n                        default=False,\n                        help=\'If true, uses fake data for unit testing.\')\n    parser.add_argument(\'--max_steps\', type=int, default=1000,\n                        help=\'Number of steps to run trainer.\')\n    parser.add_argument(\'--learning_rate\', type=float, default=0.001,\n                        help=\'Initial learning rate\')\n    parser.add_argument(\'--dropout\', type=float, default=0.9,\n                        help=\'Keep probability for training dropout.\')\n    #     parser.add_argument(\'--data_dir\', type=str, default=\'/tmp/tensorflow/mnist/input_data\',\n    #                         help=\'Directory for storing input data\')\n\n    if \'LC_TEST_ENV\' in os.environ:\n        is_test_env = os.environ[""LC_TEST_ENV""] == ""local_test""\n    else:\n        is_test_env = None\n\n    # TODO: we should be using an environment variable for job_directory\n    if is_test_env:\n        job_directory = ""./job""\n    else:\n        job_directory = os.environ[""JOB_STATE_DIR""]\n        # job_directory = ""/job""\n\n    if is_test_env:\n        if not os.path.exists(job_directory):\n            os.makedirs(job_directory)\n        logdir = job_directory + ""/logs""\n        if not os.path.exists(logdir):\n            os.makedirs(logdir)\n\n    log_directory = job_directory + ""/logs""\n\n    parser.add_argument(\'--log_dir\', type=str, default=log_directory,\n                        help=\'DLaaS log directory\')\n    parser.add_argument(\'--tb_log_dir\', type=str, default=log_directory+\'/tb\',\n                        help=\'Summaries log directory\')\n\n    parser.add_argument(\'--train_images_file\', type=str, default=\'bad\',\n                        help=\'train_images_file\')\n    parser.add_argument(\'--train_labels_file\', type=str, default=\'bad\',\n                        help=\'train_labels_file\')\n    parser.add_argument(\'--test_images_file\', type=str, default=\'bad\',\n                        help=\'test_images_file\')\n    parser.add_argument(\'--test_labels_file\', type=str, default=\'bad\',\n                        help=\'test_labels_file\')\n\n    FLAGS, unparsed = parser.parse_known_args()\n\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
metrics/log_collectors/emetrics_file/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .src import tail_em_from_emfile\n'
metrics/log_collectors/regex_extractor/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .src import extract_from_log\n'
metrics/log_collectors/simple_log_collector/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .src import tail_em_from_emfile\n'
metrics/log_collectors/tensorboard/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .src import extract_tb\n'
metrics/log_collectors/training_data_service_client/__init__.py,0,b'from .push_log_line import *\n'
metrics/log_collectors/training_data_service_client/connect.py,0,"b'#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nimport os\nimport grpc\nimport time\n\nfrom log_collectors.training_data_service_client import training_data_pb2_grpc as td\n\n\ndef get_connection()->td.TrainingDataStub:\n    with open(\'log_collectors/training_data_service_client/certs/server.crt\') as f:\n        certificate = f.read()\n\n    credentials = grpc.ssl_channel_credentials(root_certificates=certificate)\n\n    # TODO: Change these to be configurable when/if we get the viper issue straightened out.\n    isTLSEnabled = True\n    isLocal = False\n\n    if isLocal:\n        host_url = \'127.0.0.1\'\n        port = \'30015\'\n    else:\n        training_data_namespace = os.environ[""TRAINING_DATA_NAMESPACE""]\n        host_url = ""ffdl-trainingdata.%s.svc.cluster.local"" % training_data_namespace\n        port = \'80\'\n\n    host_url = \'{}:{}\'.format(host_url, port)\n\n    print(""host_url: ""+host_url)\n    sys.stdout.flush()\n    channel = None\n\n    for retryCount in range(0, 10):\n        try:\n            if isTLSEnabled:\n                channel = grpc.secure_channel(host_url, credentials,\n                                              options=((\'grpc.ssl_target_name_override\', \'dlaas.ibm.com\',),))\n            else:\n                channel = grpc.insecure_channel(host_url)\n\n            if channel is not None:\n                break\n\n        except Exception as inst:\n            print(""Exception trying to connect:"",\n                  sys.exc_info()[0])\n            print(inst)\n            sys.stdout.flush()\n\n        time.sleep(.5)\n\n    if channel is not None:\n        tdClient = td.TrainingDataStub(channel)\n    else:\n        tdClient = None\n\n    return tdClient\n\n'"
metrics/log_collectors/training_data_service_client/extract_datetime.py,0,"b'#!/usr/bin/env python\n""""""Extract datetime from text line""""""\n\n#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport re\nimport datetime\nimport time\nimport threading\n\nfrom dateutil.parser import parse as datetime_parser\n\nregExTimeGLog_str = ""^(?P<severity>[EFIW])"" \\\n                    ""(?P<month>\\d\\d)"" \\\n                    ""(?P<day>\\d\\d)\\s"" \\\n                    ""(?P<hour>\\d\\d):("" \\\n                    ""?P<minute>\\d\\d):"" \\\n                    ""(?P<second>\\d\\d)\\."" \\\n                    ""(?P<microsecond>\\d{6})\\s+"" \\\n                    ""(?P<process_id>-?\\d+)\\s"" \\\n                    ""(?P<filename>[a-zA-Z<_][\\w._<>-]+):"" \\\n                    ""(?P<line>\\d+)\\]\\s""\n\nregExTimeGLog = re.compile(regExTimeGLog_str)\n\nregex_timestamp_iso8601_str = r""\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}[.]\\d+?Z|[-+]\\d{2}:\\d{2}?""\n\nregex_timestamp_iso8601 = re.compile(regex_timestamp_iso8601_str)\n\nlast_time_lock = threading.Lock()  # type: threading.Lock\n\n\ndef now_in_milliseconds()->int:\n    return int(time.time() * 1000)\n\n\nlast_time = now_in_milliseconds()\n\n\ndef get_meta_timestamp()->int:\n    global last_time\n    # We must ensure that timestamps are always unique.  We can improve upon this\n    # a little by making a timestamp object, and then having loglines and emetrics\n    # trap separately.  But, this should be ok for now.\n    with last_time_lock:\n        this_time = now_in_milliseconds()\n        while last_time == this_time:\n            this_time = now_in_milliseconds()\n        last_time = this_time\n        timestamp = this_time\n\n    return timestamp\n\n\ndef get_log_created_year(input_file) -> int:\n    """"""Get year from log file system timestamp\n    """"""\n\n    log_created_time = os.path.getctime(input_file)\n    log_created_year = datetime.datetime.fromtimestamp(log_created_time).year\n    return log_created_year\n\n\ndef to_timestamp(dt, epoch=datetime.datetime(1970, 1, 1)):\n    td = dt - epoch\n    # return td.total_seconds()\n    return int((td.microseconds + (td.seconds + td.days * 86400) * 10**6) / 10**6)\n\n\ndef extract_datetime(line: str, year: int, existing_time: datetime = None, fuzzy: bool = True) -> (datetime, bool):\n    """"""Try to get a datetime object from the long line\n\n    Return some datetime, and a boolean value telling if the value was actually extracted from the line""""""\n\n    matches = regExTimeGLog.match(line)\n    did_get_good_time = False\n    dt = None\n    # TODO: allow type that specifies what kind of parse should be attempted\n    if matches is not None:\n        dtd = matches.groupdict()\n        # print(year)\n        # print(int(dtd[""month""]))\n        # print(int(dtd[""day""]))\n        # print(int(dtd[""hour""]))\n        # print(int(dtd[""minute""]))\n        # print(int(dtd[""second""]))\n        # print(int(dtd[""microsecond""]))\n        dt = datetime.datetime(year,\n                               int(dtd[""month""]),\n                               int(dtd[""day""]),\n                               int(dtd[""hour""]),\n                               int(dtd[""minute""]),\n                               int(dtd[""second""]),\n                               int(dtd[""microsecond""]))\n    else:\n        try:\n            # I\'d like to be less restrictive than iso8601, but, this is what we do for now.\n            matches = regex_timestamp_iso8601.match(line)\n            if matches is not None:\n                dt = datetime_parser(line, fuzzy=fuzzy)\n        except ValueError:\n            dt = None\n\n    if dt is not None:\n        did_get_good_time = True\n    elif existing_time is None:\n        dt = datetime.datetime.now()\n    else:\n        dt = existing_time\n\n    return dt, did_get_good_time\n'"
metrics/log_collectors/training_data_service_client/match_log_file.py,0,"b'#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport logging\n\nLOGFILE_DEFAULT_NAME = ""training-log.txt""\nLOGFILE_ALT1_NAME = ""user_log.txt""\n\nEMETRICS_FILE_BASE_NAME = ""evaluation-metrics.txt""\n\n# This is experimental, and for now I\'m hard coding it.  Once settled,\n# we\'ll allow this array to be set by the manifest.\npossible_log_file_names = [LOGFILE_DEFAULT_NAME, LOGFILE_ALT1_NAME]\n\n\ndef is_log_file(filename):\n    for log_file_pattern in possible_log_file_names:\n        if filename == log_file_pattern:\n            return True\n    return False\n\n\ndef is_emetrics_file(filename):\n    return EMETRICS_FILE_BASE_NAME == filename\n'"
metrics/log_collectors/training_data_service_client/print_json.py,0,"b'#!/usr/bin/env python\n""""""Print json to stdout or wherever""""""\n\n#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport re\nimport logging\nfrom google.protobuf import json_format\n\nfrom builtins import object\n\nrindex_regex = re.compile(r""\\""rindex\\"":\\s\\""([0-9]+)\\"""")\ntime_regex = re.compile(r""\\""time\\"":\\s\\""([0-9]+)\\"""")\n\n\ndef output(obj: object):\n    """"""Take a EMetrics or LogLine record and print a valid JSON object to stdout""""""\n    json_string = to_string(obj)\n    print(json_string)\n\n\ndef logging_output(obj: object):\n    """"""Take a EMetrics or LogLine record and print a valid JSON object to stdout""""""\n    json_string = to_string(obj)\n    logging.debug(""json: %s"", json_string)\n\n\ndef to_string(obj: object)->str:\n    """"""Take a EMetrics or LogLine record and return a valid JSON string""""""\n\n    # Call buggy google protobuf function.\n    json_string = json_format.MessageToJson(obj, indent=0, preserving_proto_field_name=True)\n\n    # The rest of this is a bit of a hack.  Perhaps I\'d be better off just\n    # processing the typed record and hand-printing the json.\n    json_string = json_string.replace(\'\\n\', \' \').replace(\'\\r\', \'\')\n\n    for i in range(1, 10):\n        json_string = json_string.replace(\'  \', \' \')\n        json_string = json_string.replace(\'{ ""\', \'{""\')\n        json_string = json_string.replace(\'"" }\', \'""}\')\n\n    json_string = json_string.replace(\'""type"": ""STRING""\', \'""type"": 0\')\n    json_string = json_string.replace(\'""type"": ""JSONSTRING""\', \'""type"": 1\')\n    json_string = json_string.replace(\'""type"": ""INT""\', \'""type"": 2\')\n    json_string = json_string.replace(\'""type"": ""FLOAT""\', \'""type"": 3\')\n\n    json_string = rindex_regex.sub(r\'""rindex"": \\1\', json_string)\n    json_string = time_regex.sub(r\'""time"": \\1\', json_string)\n\n    return json_string\n'"
metrics/log_collectors/training_data_service_client/push_em_line.py,0,"b'#!/usr/bin/env python\n""""""Push evaluation metrics record json line to data service or somewhere""""""\n\n#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nimport json\nimport os\nimport logging\nfrom typing import Tuple, Any\n\nfrom log_collectors.training_data_service_client import training_data_pb2 as tdp\nfrom log_collectors.training_data_service_client import print_json as print_json\n\nfrom log_collectors.training_data_service_client import extract_datetime as edt\nfrom log_collectors.training_data_service_client import training_data_buffered as tdb\n\n\ndef push(td_client: tdb.TrainingDataClientBuffered, log_file: str, log_line: str, logfile_year: str, rindex: int, rindex2: int,\n         subdir: str, extra: Any=None) -> Tuple[int, int]:\n    """"""Push the processed metrics data to the metrics service""""""\n\n    del log_file  # Ignored parameter for now\n    del logfile_year  # Ignored parameter for now\n    del extra  # Ignored parameter for now\n    try:\n        emr = json.loads(log_line+""\\n"")\n\n        logging.debug(""Processing etimes..."")\n        etimes_dict = emr[""etimes""]\n        etimes: dict = dict()\n        for key, value_any in etimes_dict.items():\n            val = value_any[""value""]\n            if ""type"" in value_any:\n                val_type = value_any[""type""]\n                etimes[key] = tdp.Any(type=val_type, value=str(val))\n            else:\n                etimes[key] = tdp.Any(value=str(val))\n\n        logging.debug(""Processing values..."")\n        values_dict = emr[""values""]\n        scalars: dict = dict()\n        for key in values_dict:\n            value_any = values_dict[key]\n            val = value_any[""value""]\n            if ""type"" in value_any:\n                val_type = value_any[""type""]\n                scalars[key] = tdp.Any(type=val_type, value=val)\n            else:\n                scalars[key] = tdp.Any(value=val)\n\n        emr_meta = emr[""meta""]\n\n        logging.debug(""Getting training_id..."")\n        training_id = emr_meta[""training_id""]\n        if training_id is None and ""TRAINING_ID"" in os.environ:\n            training_id = os.environ[""TRAINING_ID""],\n\n        if ""time"" in emr_meta and emr_meta[""time""] is not None:\n            time=emr_meta[""time""]\n        else:\n            time=edt.get_meta_timestamp()\n\n        if ""subid"" in emr_meta and emr_meta[""subid""] is not None:\n            subid=emr_meta[""subid""]\n        else:\n            subid=subdir\n\n        logging.debug(""Assembling record..."")\n        emetrics = tdp.EMetrics(\n            meta=tdp.MetaInfo(\n                training_id=training_id,\n                time=time,\n                rindex=int(emr_meta[""rindex""]),\n                subid=subid\n            ),\n            grouplabel=emr[""grouplabel""],\n            etimes=etimes,\n            values=scalars,\n        )\n\n        if td_client is not None:\n            if False:\n                print_json.logging_output(emetrics)\n            td_client.AddEMetrics(emetrics)\n\n    except Exception as inst:\n        print(""Unexpected error when attempting to process evaluation metric record:"", sys.exc_info()[0])\n        print(inst)\n        sys.stdout.flush()\n\n    return rindex+1, rindex2\n'"
metrics/log_collectors/training_data_service_client/push_log_line.py,0,"b'#!/usr/bin/env python\n""""""Push log line to data service or somewhere""""""\n\n#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport logging\n\nfrom typing import Tuple, Any\n\nfrom log_collectors.training_data_service_client import print_json\n\nfrom log_collectors.training_data_service_client import training_data_buffered as tdb\nfrom log_collectors.training_data_service_client import training_data_pb2 as tdp\nfrom log_collectors.training_data_service_client import extract_datetime as edt\n\n\ndef push(td_client: tdb.TrainingDataClientBuffered, log_file: str, log_line: str, logfile_year: str, rindex: int, rindex2: int,\n         subdir: str, extra: Any=None) -> Tuple[int, int]:\n    """"""Push the processed metrics data to the metrics service""""""\n\n    del log_file  # Ignored parameter for now\n    del logfile_year  # Ignored parameter for now\n    del extra  # Ignored parameter for now\n\n    logging.debug(""Creating logline record: %d"", rindex)\n    # logging.debug(""Sending %d (%s) %s"", rindex, subdir, log_line)\n    log_line_record = tdp.LogLine(\n        meta=tdp.MetaInfo(\n            training_id=os.environ[""TRAINING_ID""],\n            time=edt.get_meta_timestamp(),\n            rindex=rindex,\n            subid=subdir\n        ),\n        line=log_line\n    )\n\n    if td_client is not None:\n        logging.debug(""Calling AddLogLine: %d"", rindex)\n        td_client.AddLogLine(log_line_record)\n\n    return rindex+1, rindex2\n'"
metrics/log_collectors/training_data_service_client/scan_log_dirs.py,0,"b'#!/usr/bin/env python\n""""""Scan files and directories for logs and emetrics""""""\n\n#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\nimport sys\nimport logging\nimport threading\n\nfrom typing import Dict, Callable, Any, Tuple\n\nfrom log_collectors.training_data_service_client import extract_datetime as extract_datetime\nfrom log_collectors.training_data_service_client import training_data_buffered as tdb\n\nfrom . import push_log_line\nfrom . import connect\nfrom . import states\nfrom . import match_log_file\n\n\nclass LinesCounter:\n\n    def __init__(self, start=0):\n        self.lock = threading.Lock()\n        self.value = start\n        self.time_of_last_count = None\n\n    def increment(self):\n        # logging.debug(\'Waiting for lock\')\n        self.lock.acquire()\n        try:\n            # logging.debug(\'Acquired lock\')\n            self.value = self.value + 1\n            self.time_of_last_count = time.time()\n        finally:\n            self.lock.release()\n\n    def add(self, count: int):\n        # logging.debug(\'Waiting for lock\')\n        self.lock.acquire()\n        try:\n            # logging.debug(\'Acquired lock\')\n            self.value = self.value + count\n            self.time_of_last_count = time.time()\n        finally:\n            self.lock.release()\n\n    def time_since_last_entry(self):\n        if self.time_of_last_count is not None:\n            elapsed_time = time.time() - self.time_of_last_count\n            return elapsed_time\n        else:\n            return -0.0\n\n\ndef stat_nfs_safe_modification_time(file: str)->float:\n    # os.stat seems to be completely unreliable for the nfs volume,\n    # because file attributes might not have been flushed.\n    # By opening the file, it will flush the attributes.\n    # Python programming contest time:\n    #      what\'s a faster way to implement this?\n    file_fd = os.open( file, os.O_RDONLY )\n    stat = os.fstat(file_fd)\n    os.close(file_fd)\n    # stat = os.stat(self.log_file)\n\n    return stat.st_mtime\n\n\nclass GetAndPushLogLinesRunner(threading.Thread):\n\n    def __init__(self, td_client: tdb.TrainingDataClientBuffered, logfile_year, log_file: str, subdir: str,\n                 lines_counter: LinesCounter,\n                 extra: Any = None,\n                 push_function: Callable[\n                     [tdb.TrainingDataClientBuffered, str, str, str, int, int, str, Any],\n                     Tuple[int, int]]=push_log_line.push,\n                 logger: logging.Logger = logging.getLogger()):\n        super().__init__()\n        self.td_client = td_client\n        self.logfile_year = logfile_year\n        self.log_file = log_file\n        self.subdir = subdir\n        self.lines_counter = lines_counter\n        self.push_function = push_function\n        self.extra = extra\n\n        self.log_line_index = 1\n        self.record_index = 1\n        self.logger = logger\n\n        log_dir = os.environ[""LOG_DIR""]\n        path_relative_log_dir = self.log_file[len(log_dir):]\n\n        unique_id = path_relative_log_dir.replace(os.path.sep, ""_"")\n        file_pos_file_base = "".lc_pos_info_""+unique_id\n\n        job_state_dir = os.environ[""JOB_STATE_DIR""]\n        self.file_pos_file = os.path.join(job_state_dir, file_pos_file_base)\n\n        self.logger.debug(""file_pos_file: %s"", self.file_pos_file)\n\n    def processLogLines(self, file_pos: int) -> int:\n        # if os.path.exists(self.file_pos_file):\n        #     with open(self.file_pos_file, ""r"") as file_pos_file_stream:\n        #         file_pos, log_line_index, record_index = [int(x) for x in next(file_pos_file_stream).split()]\n        #         self.logger.debug(""read pos: %s %s %s"" % (file_pos, log_line_index, record_index))\n        with open(self.log_file, \'r\') as log_stream:\n            try:\n                # self.logger.debug(""GetAndPushLogLines seeking to: %d"" % file_pos)\n                log_stream.seek(file_pos)\n                for line in iter(log_stream):\n                    # self.logger.debug(""Pushing logline: %s"", line)\n                    self.log_line_index, self.record_index = \\\n                        self.push_function(self.td_client, self.log_file, line,\n                                           self.logfile_year,\n                                           self.log_line_index,\n                                           self.record_index,\n                                           self.subdir, self.extra)\n                    self.lines_counter.increment()\n                    time.sleep(0)\n            except Exception as inst:\n                self.logger.exception(""Exception"")\n                self.logger.error(""Unexpected error: %s, %s"", str(inst), self.log_file)\n            finally:\n                file_pos = log_stream.tell()\n                # self.logger.debug(""GetAndPushLogLines tell pos: %d"" % file_pos)\n                # with open(self.file_pos_file, ""w"") as file_pos_file_stream:\n                #     self.logger.debug(""write pos: %s %s %s"" % (file_pos, log_line_index, record_index))\n                #     file_pos_file_stream.write(""%s %s %s"" % (file_pos, log_line_index, record_index))\n\n        return file_pos\n\n    def run(self):\n        self.logger.info(""GetAndPushLogLinesRunner thread running: %s"" % self.log_file)\n\n        file_pos = 0\n        last_modified_time = 0.0\n        shutdown_start_time = 0.0\n\n        states.register_scanner()\n\n        try:\n            os.stat_float_times(True)\n            while True:\n                # elapsed_time_since_last_mod = time.time() - last_modified_time\n                # self.logger.debug(""time_since_last_push %r"", time_since_last_push)\n                # elapsed_time_since_last_mod > 2.0 and\n                if shutdown_start_time == 0.0 and states.is_learner_done(logger=self.logger):\n                    self.logger.info(""Learner done, begin GetAndPushLogLinesRunner shutdown: %s"", self.log_file)\n                    shutdown_start_time = time.time()\n\n                if shutdown_start_time != 0.0:\n                    elapsed_time_since_shutdown = (time.time() - shutdown_start_time)\n                    if elapsed_time_since_shutdown > states.DURATION_SHUTDOWN_DELAY:\n                        self.logger.info(""time since shutdown start: %f, Flushing GetAndPushLogLinesRunner with force: %s"",\n                                     elapsed_time_since_shutdown, self.log_file)\n\n                        self.td_client.flush_maybe(force=True)\n\n                        time.sleep(states.SLEEP_BEFORE_LC_DONE)\n\n                        break\n\n                try:\n                    latest_modified_time = stat_nfs_safe_modification_time(self.log_file)\n\n                    if last_modified_time == latest_modified_time:\n                        self.logger.debug(""file mod NOT changed: %f, %s"", latest_modified_time, self.log_file)\n                        self.td_client.flush_maybe()\n                        time.sleep(.25)  # Micro sleep\n                        continue\n                    else:\n                        self.logger.debug(""file mod changed: %f, %s"", latest_modified_time, self.log_file)\n                        last_modified_time = latest_modified_time\n                except OSError as oserr:\n                    # Doesn\'t exist?\n                    self.logger.info(""OSError: %s"", str(oserr))\n                    time.sleep(0.25)  # Micro sleep\n                    continue\n\n                file_pos = self.processLogLines(file_pos)\n                self.td_client.flush_maybe()\n\n        finally:\n            self.logger.info(""Signaling GetAndPushLogLinesRunner is done: %s"", self.log_file)\n            # if not self.running_in_foreground:\n            #     # If we\'re running in background, assume the main thread will do the signal\n            #     time.sleep(15)\n            states.signal_lc_done(logger=self.logger)\n            # I seem to have problems exiting directly, so, this sleep seems to help.\n            # My unsubstantiated theory is that gRPC needs time to flush.\n            # Note since we signaled, we won\'t actually wait n seconds, the\n            # job monitor will delete us.\n            time.sleep(states.SLEEP_BEFORE_EXIT_TIME)\n            self.logger.info(""Exiting GetAndPushLogLinesRunner: %s"", self.log_file)\n\n\nclass LogScanner:\n\n    def __init__(self, should_connect: bool=True, logger: logging.Logger = logging.getLogger()):\n\n        self.lines_counter = LinesCounter()  # type: LinesCounter\n        self.log_runners = {}                # type: Dict[str, threading.Thread]\n        self.td_client = None                # type: tdb.TrainingDataClientBuffered\n        self.logger = logger\n\n        if should_connect:\n            try:\n                self.logger.debug(""tail_to_tds: Trying to connect to Training Data Service (log lines)"")\n                self.td_client = tdb.TrainingDataClientBuffered(connect.get_connection())\n                if self.td_client.td_client is not None:\n                    self.logger.debug(""Have connection to Training Data Service (log lines)"")\n            except Exception as inst:\n                self.logger.error(""Unexpected error when attempting to connect: %r"" +\n                              sys.exc_info()[0])\n                self.logger.debug(inst)\n\n    def scan(self, log_dir: str,\n             extra: Any = None,\n             is_log: Callable[[str], bool] = match_log_file.is_log_file,\n             push_function: Callable[\n                 [tdb.TrainingDataClientBuffered, str, str, str, int, int, str, Any],\n                 Tuple[int, int]]=push_log_line.push,\n             is_emetrics: Callable[[str], bool] = None,\n             push_emetrics_function: Callable[\n                 [tdb.TrainingDataClientBuffered, str, str, str, int, int, str, Any],\n                 Tuple[int, int]]=None,\n             is_subdir_function: Callable[[str], bool]=os.path.isdir,\n             should_loop: bool=True):\n\n        while True:\n            # The design allows for there to be top-level log files, as well as subdirectory logs that\n            # contain some sub-run logs.  These are indexed in the training data service with the subdir\n            # in the MetaInfo inner struct.\n            # self.logger.debug(""tail_to_tds: does path exist? %s"" % log_dir)\n            if not os.path.exists(log_dir):\n                time.sleep(.5)\n                continue\n\n            # self.logger.debug(""tail_to_tds: path does exist: %s"" % log_dir)\n            try:\n                logfile_year = extract_datetime.get_log_created_year(log_dir)\n                for file_or_dir_name in os.listdir(log_dir):\n                    sub_dir_path = os.path.join(log_dir, file_or_dir_name)\n                    # self.logger.debug(""considering if %s is a directory"", sub_dir_path)\n                    if is_subdir_function(sub_dir_path):\n                        sub_dir_id = file_or_dir_name\n                        # Now iterate over the files in the subdirectory, hoping to find a logfile\n                        for file_in_subdir in os.listdir(sub_dir_path):\n                            log_file_path = os.path.join(sub_dir_path, file_in_subdir)\n                            if log_file_path not in self.log_runners:\n                                push_function_arg = None\n                                if is_log is not None and is_log(os.path.basename(log_file_path)):\n                                    push_function_arg = push_function\n                                elif is_emetrics is not None and is_emetrics(os.path.basename(log_file_path)):\n                                    push_function_arg = push_emetrics_function\n\n                                if push_function_arg is not None:\n                                    self.logger.info(""Creating sub-level log runner for %s"", log_file_path)\n                                    runner = GetAndPushLogLinesRunner(\n                                        td_client=tdb.TrainingDataClientBuffered(self.td_client.td_client),\n                                        logfile_year=logfile_year,\n                                        log_file=log_file_path,\n                                        subdir=sub_dir_id,\n                                        lines_counter=self.lines_counter,\n                                        extra=extra,\n                                        push_function=push_function_arg,\n                                        logger=self.logger)\n                                    self.log_runners[log_file_path] = runner\n                                    runner.start()\n                    else:\n                        log_file_path = os.path.join(log_dir, file_or_dir_name)\n\n                        if log_file_path not in self.log_runners:\n                            push_function_arg = None\n\n                            if is_log is not None and is_log(file_or_dir_name):\n                                push_function_arg = push_function\n                            elif is_emetrics is not None and is_emetrics(file_or_dir_name):\n                                push_function_arg = push_emetrics_function\n\n                            if push_function_arg is not None:\n                                self.logger.info(""Creating top-level log runner for %s"", log_file_path)\n                                runner = GetAndPushLogLinesRunner(\n                                    td_client=self.td_client,\n                                    logfile_year=logfile_year,\n                                    log_file=log_file_path,\n                                    subdir="""",\n                                    lines_counter=self.lines_counter,\n                                    extra=extra,\n                                    push_function=push_function_arg,\n                                    logger=self.logger)\n                                self.log_runners[log_file_path] = runner\n                                runner.start()\n\n                time.sleep(.5)\n                if states.is_learner_done() and states.global_scanner_count == 0:\n                    break\n            except Exception as inst:\n                self.logger.error(""Error thrown (recovering): %r"", sys.exc_info()[0])\n\n            if should_loop is False:\n                break\n'"
metrics/log_collectors/training_data_service_client/states.py,0,"b'#!/usr/bin/env python\n""""""manage state transition details""""""\n\n#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport logging\nimport threading\n\n# I seem to have problems exiting directly, so, this sleep seems to help.\n# My unsubstantiated theory is that gRPC needs time to flush.\n# Note since we signaled, we won\'t actually wait n seconds, the\n# job monitor will delete us.\nSLEEP_BEFORE_EXIT_TIME = 15\n\nSLEEP_BEFORE_LC_DONE = 4.0\n\nDURATION_SHUTDOWN_DELAY = 10.00\n\nDURATION_SHUTDOWN_DELAY_TF = 5.00\n\nglobal_state_lock = threading.Lock()\n\nglobal_scanner_count = 0\n\n\ndef register_scanner():\n    global global_scanner_count\n    with global_state_lock:\n        global_scanner_count += 1\n\n\ndef unregister_scanner():\n    global global_scanner_count\n    with global_state_lock:\n        global_scanner_count -= 1\n\n\ndef is_learner_done(logger=None):\n    logger = logger or logging.getLogger()\n    learner_exit_file_path = os.path.join(os.environ[""JOB_STATE_DIR""], ""learner.exit"")\n    halt_file_path = os.path.join(os.environ[""JOB_STATE_DIR""], ""halt"")\n    # logging.debug(""Checking is learner done: %s, %s"", learner_exit_file_path, halt_file_path)\n    learner_is_done = os.path.exists(learner_exit_file_path) or os.path.exists(halt_file_path)\n    if learner_is_done:\n        logger.debug(""Learner is done!"")\n    return learner_is_done\n\n\ndef signal_lc_done(exit_code=0, logger=logging.getLogger()):\n    global global_scanner_count\n    logger.info(""signal_lc_done, global_scanner_count (after release): %d"", global_scanner_count)\n    with global_state_lock:\n        global_scanner_count -= 1\n    pass\n'"
metrics/log_collectors/training_data_service_client/tail_to_em_from_emfile.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport time\n\nfrom . import push_em_line\nfrom . import connect\n\n\ndef collect_and_send(em_file: str, should_connect: bool=True):\n    if should_connect:\n        try:\n            print(""Trying to connect to Training Data Service (emetrics)"")\n            sys.stdout.flush()\n            tdClient = connect.get_connection()\n            if tdClient is not None:\n                print(""Have connection to Training Data Service (emetrics)"")\n                sys.stdout.flush()\n        except Exception as inst:\n            print(""Unexpected error when attempting to process evaluation metric record  (emetrics):"",\n                  sys.exc_info()[0])\n            print(inst)\n            sys.stdout.flush()\n    else:\n        print(""Not connecting to Training Data Service (emetrics)"")\n        sys.stdout.flush()\n        tdClient = None\n\n    log_line_index = 1\n    while not os.path.exists(em_file):\n        time.sleep(1)\n\n    # TODO: Keep file_pos stored in file, in case of this container\'s restart\n    file_pos = 0\n    while True:\n        with open(em_file, \'r\') as em_stream:\n            try:\n                em_stream.seek(file_pos)\n                for line in iter(em_stream):\n                    log_line_index = push_em_line.push (tdClient, line, """", log_line_index)\n            except Exception as inst:\n                print(""Unexpected error:"", str(inst))\n                sys.stdout.flush()\n\n            file_pos = em_stream.tell()\n\n\n'"
metrics/log_collectors/training_data_service_client/tail_to_tds.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport time\nimport sys\n\nfrom log_collectors.training_data_service_client import extract_datetime as extract_datetime\n\nfrom . import push_log_line\nfrom . import connect\n\n\ndef collect_and_send(log_file: str, should_connect: bool=True):\n    if should_connect:\n        try:\n            print(""Trying to connect to Training Data Service (log lines)"")\n            sys.stdout.flush()\n            tdClient = connect.get_connection()\n            if tdClient is not None:\n                print(""Have connection to Training Data Service (log lines)"")\n                sys.stdout.flush()\n        except Exception as inst:\n            print(""Unexpected error when attempting to process evaluation metric record  (log lines):"",\n                  sys.exc_info()[0])\n            print(inst)\n            sys.stdout.flush()\n\n    log_line_index = 1\n\n    while not os.path.exists(log_file):\n        time.sleep(1)\n\n    logfile_year = extract_datetime.get_log_created_year(log_file)\n\n    # TODO: Keep file_pos stored in file, in case of this container\'s restart\n    file_pos = 0\n    while True:\n        with open(log_file, \'r\') as em_stream:\n            try:\n                em_stream.seek(file_pos)\n                for line in iter(em_stream):\n                    log_line_index = push_log_line.push (tdClient, line, logfile_year, log_line_index)\n            except Exception as inst:\n                print(""Unexpected error:"", str(inst))\n                sys.stdout.flush()\n    \n            file_pos = em_stream.tell()\n'"
metrics/log_collectors/training_data_service_client/training_data_buffered.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport time\nimport threading\nimport logging\nimport grpc\nimport os\nfrom typing import List\n\nfrom log_collectors.training_data_service_client import print_json\n\nfrom log_collectors.training_data_service_client import training_data_pb2_grpc as td\nfrom log_collectors.training_data_service_client import training_data_pb2 as tdp\n\n\nclass TrainingDataClientBuffered(object):\n    """"""Buffer the TDS events, and flush them after a certain amount of time\n    has passed, or when the buffer reaches a certain size.\n\n    This class will also write the emetrics file, thus it is required to have\n    and instance specific to the job or sub-job being monitored.\n\n    The instances of this class can me accessed from multiple threads, so it\n    must be very threadsafe.  All public methods are blocking.\n    """"""\n\n    CONST_FLUSH_NUMBER_RECORDS = 10\n    CONST_FLUSH_TIME = 2.0\n\n    GRPC_RETRY_MAX = 6\n    GRPC_RETRY_SLEEP_INTERVAL = 0.5\n\n    BUFFER_MAX_TIL_DUMP = 1024\n\n    emetrics_buf = []   # type: List[tdp.EMetrics]\n    emetrics_buf_start_add_time = 0  # type: float\n\n    logline_buf = []   # type: List[tdp.LogLine]\n    logline_buf_start_add_time = 0  # type: float\n\n    buffer_lock = threading.Lock()\n    other_tdcb_instances = []   # type: List[TrainingDataClientBuffered]\n\n    def __init__(self, td_client: td.TrainingDataStub, em_file_path: str=None):\n        """"""Constructor.\n\n        Args:\n          td_client: A td.TrainingDataStub\n          em_file_path: If non-null, path where the metrics will be written\n        """"""\n\n        self.td_client = td_client\n        self.em_file_path = em_file_path\n\n        # Buffer for what should go into this instances emetrics file\n        self.emetrics_file_buf = []   # type: tdp.EMetrics\n\n        self.logger = logging.getLogger(""tds_client_buffered"")\n        self.logger.setLevel(logging.INFO)\n        self.logger.info(""Creating TrainingDataClientBuffered"")\n\n        self.last_em_file_size = 0\n\n        with TrainingDataClientBuffered.buffer_lock:\n            if em_file_path is not None:\n                for tdcb in TrainingDataClientBuffered.other_tdcb_instances:\n                    if tdcb.em_file_path is not None:\n                        self.logger.error(""only ONE EM writer per process"")\n                        raise Exception(""only ONE EM writer per process"")\n            TrainingDataClientBuffered.other_tdcb_instances.append(self)\n\n    def set_em_file_path(self, em_file_path: str):\n        # Locks are probably overkill, but ensures we\'re safe\n        with TrainingDataClientBuffered.buffer_lock:\n            self.logger.info(""Setting the em file path: %s"", em_file_path)\n            self.em_file_path = em_file_path\n\n    def __should_flush(self, buf: [], buf_start_add_time: float):\n        return len(buf) >= self.CONST_FLUSH_NUMBER_RECORDS or \\\n               (time.time() - buf_start_add_time) > self.CONST_FLUSH_TIME\n\n    def __write_em_file_path(self, force: bool=False):\n        # The force flag is sent by the caller, which is sent by someone else,\n        # which signals intent.  Even though it\'s not being used right now,\n        # I\'d rather keep the argument for the moment.\n        del force\n\n        if self.em_file_path is not None and len(self.emetrics_file_buf):\n            lines_written = 0\n            try:\n                self.logger.info(""writing %d records to %s, thread index %d"",\n                                  len(self.emetrics_file_buf), self.em_file_path, threading.get_ident())\n\n                # The lines below with ""Flush nfs buffer??"" are about trying to make sure the nfs cache is flushed.\n                # The result of a long struggle, there\'s probably a better way.\n\n                if self.last_em_file_size > 0:\n                    # Flush nfs buffer??\n                    if not os.path.exists(self.em_file_path):\n                        self.logger.error(""file was created, but now it doesn\'t exist!!! %s"", self.em_file_path)\n\n                with open(file=self.em_file_path,  mode=\'a\', buffering=-1) as em_stream:\n                    for emetrics in self.emetrics_file_buf:\n                        try:\n                            json_form = print_json.to_string(emetrics)\n                            em_stream.write(json_form)\n                            em_stream.write(""\\n"")\n                            lines_written += 1\n                        except OSError as err:\n                            self.logger.warning(""Unexpected error writing emetrics file: %s"", err)\n\n                # Please keep this in place for now for debugging.\n                # # if force:\n                # Flush nfs buffer??\n                fd = os.open(self.em_file_path, os.O_RDONLY)\n                after_stat = os.fstat(fd)\n                os.close(fd)\n\n                if after_stat.st_size <= self.last_em_file_size:\n                    self.logger.error(""what?: file grew smaller! b: %d, a: %d"",\n                                      self.last_em_file_size, after_stat.st_size)\n\n                self.last_em_file_size = after_stat.st_size\n\n            except OSError as error:  # parent of IOError, OSError *and* WindowsError where available\n                self.logger.warning(""Unexpected error opening emetrics file: %s"", error)\n            finally:\n                self.emetrics_file_buf = self.emetrics_file_buf[lines_written:]\n\n    def __add_emetrics_with_retry(self, force: bool=False)->bool:\n        success = False\n        for retryCount in range(0, self.GRPC_RETRY_MAX):\n            try:\n                self.logger.debug(""calling AddEMetricsBatch, adding %d records"",\n                                  len(TrainingDataClientBuffered.emetrics_buf))\n                self.td_client.AddEMetricsBatch(tdp.EMetricsBatch(force=force,\n                                                                  emetrics=TrainingDataClientBuffered.emetrics_buf))\n                success = True\n            except grpc.RpcError as rpc_error:\n                self.logger.warning(""RpcError error sending emetrics: %s"", rpc_error)\n            finally:\n                TrainingDataClientBuffered.emetrics_buf = []\n                TrainingDataClientBuffered.emetrics_buf_start_add_time = time.time()\n            if success:\n                break\n            else:\n                time.sleep(self.GRPC_RETRY_SLEEP_INTERVAL)\n        return success\n\n    def __add_loglines_with_retry(self, force: bool=False)->bool:\n        success = False\n        for retryCount in range(0, self.GRPC_RETRY_MAX):\n            try:\n                self.logger.debug(""calling AddLogLineBatch, adding %d lines"",\n                                  len(TrainingDataClientBuffered.logline_buf))\n\n                self.td_client.AddLogLineBatch(\n                    tdp.LogLineBatch(force=force, logLine=TrainingDataClientBuffered.logline_buf))\n                success = True\n            except grpc.RpcError as rpc_error:\n                self.logger.warning(""RpcError error sending log lines: %s"", rpc_error)\n            finally:\n                TrainingDataClientBuffered.logline_buf = []\n                TrainingDataClientBuffered.logline_buf_start_add_time = time.time()\n            if success:\n                break\n            else:\n                time.sleep(self.GRPC_RETRY_SLEEP_INTERVAL)\n        return success\n\n    def __flush_emetrics_maybe(self, force: bool=False)->bool:\n        success = False\n        if ((force and len(TrainingDataClientBuffered.emetrics_buf) > 0) or\n                (self.__should_flush(TrainingDataClientBuffered.emetrics_buf,\n                                     TrainingDataClientBuffered.emetrics_buf_start_add_time) and\n                 len(TrainingDataClientBuffered.emetrics_buf) > 0)):\n\n            self.__write_em_file_path(force=force)\n\n            success = self.__add_emetrics_with_retry(force=force)\n            if len(TrainingDataClientBuffered.emetrics_buf) > self.BUFFER_MAX_TIL_DUMP:\n                TrainingDataClientBuffered.emetrics_buf = []\n                TrainingDataClientBuffered.emetrics_buf_start_add_time = time.time()\n\n        return success\n\n    def __flush_loglines_maybe(self, force: bool=False)->bool:\n        success = False\n        if (force and len(TrainingDataClientBuffered.logline_buf) > 0) or \\\n                (self.__should_flush(TrainingDataClientBuffered.logline_buf,\n                                     TrainingDataClientBuffered.logline_buf_start_add_time) and\n                 len(TrainingDataClientBuffered.logline_buf) > 0):\n            success = self.__add_loglines_with_retry(force=force)\n            if len(TrainingDataClientBuffered.logline_buf) > self.BUFFER_MAX_TIL_DUMP:\n                TrainingDataClientBuffered.logline_buf = []\n                TrainingDataClientBuffered.logline_buf_start_add_time = time.time()\n        return success\n\n    def __flush_maybe(self, force: bool=False):\n        for tdcb in TrainingDataClientBuffered.other_tdcb_instances:\n            if tdcb is not self:\n                tdcb.__flush_loglines_maybe(force=force)\n                tdcb.__flush_emetrics_maybe(force=force)\n        self.__flush_loglines_maybe(force=force)\n        self.__flush_emetrics_maybe(force=force)\n\n    def flush_maybe(self, force: bool=False):\n        with TrainingDataClientBuffered.buffer_lock:\n            self.__flush_maybe(force)\n\n    def AddEMetrics(self, emetric_record: tdp.EMetrics):\n        with TrainingDataClientBuffered.buffer_lock:\n            if len(TrainingDataClientBuffered.emetrics_buf) == 0:\n                TrainingDataClientBuffered.emetrics_buf_start_add_time = time.time()\n            TrainingDataClientBuffered.emetrics_buf.append(emetric_record)\n            # Also append to buffer intended for emetrics file\n            if self.em_file_path is not None:\n                self.emetrics_file_buf.append(emetric_record)\n            self.logger.debug(""AddEMetrics: size after add: %d"", len(TrainingDataClientBuffered.emetrics_buf))\n            self.__flush_maybe()\n\n    def AddLogLine(self, logline_record: tdp.LogLine):\n        with TrainingDataClientBuffered.buffer_lock:\n            if len(TrainingDataClientBuffered.logline_buf) == 0:\n                TrainingDataClientBuffered.logline_buf_start_add_time = time.time()\n            TrainingDataClientBuffered.logline_buf.append(logline_record)\n            self.logger.debug(""AddLogLine: size after add: %d"", len(TrainingDataClientBuffered.logline_buf))\n            self.__flush_maybe()\n'"
metrics/log_collectors/training_data_service_client/training_data_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: training_data.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'training_data.proto\',\n  package=\'grpc.training.data.v1\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n\\x13training_data.proto\\x12\\x15grpc.training.data.v1\\""]\\n\\x08MetaInfo\\x12\\x13\\n\\x0btraining_id\\x18\\x01 \\x01(\\t\\x12\\x0f\\n\\x07user_id\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04time\\x18\\x03 \\x01(\\x03\\x12\\x0e\\n\\x06rindex\\x18\\x04 \\x01(\\x03\\x12\\r\\n\\x05subid\\x18\\x05 \\x01(\\t\\""F\\n\\x07LogLine\\x12-\\n\\x04meta\\x18\\x01 \\x01(\\x0b\\x32\\x1f.grpc.training.data.v1.MetaInfo\\x12\\x0c\\n\\x04line\\x18\\x02 \\x01(\\t\\""N\\n\\x0cLogLineBatch\\x12\\r\\n\\x05\\x66orce\\x18\\x01 \\x01(\\x08\\x12/\\n\\x07logLine\\x18\\x02 \\x03(\\x0b\\x32\\x1e.grpc.training.data.v1.LogLine\\""\\x83\\x01\\n\\x03\\x41ny\\x12\\x31\\n\\x04type\\x18\\x01 \\x01(\\x0e\\x32#.grpc.training.data.v1.Any.DataType\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\t\\"":\\n\\x08\\x44\\x61taType\\x12\\n\\n\\x06STRING\\x10\\x00\\x12\\x0e\\n\\nJSONSTRING\\x10\\x01\\x12\\x07\\n\\x03INT\\x10\\x02\\x12\\t\\n\\x05\\x46LOAT\\x10\\x03\\""\\xdd\\x02\\n\\x08\\x45Metrics\\x12-\\n\\x04meta\\x18\\x01 \\x01(\\x0b\\x32\\x1f.grpc.training.data.v1.MetaInfo\\x12;\\n\\x06\\x65times\\x18\\x02 \\x03(\\x0b\\x32+.grpc.training.data.v1.EMetrics.EtimesEntry\\x12\\x12\\n\\ngrouplabel\\x18\\x03 \\x01(\\t\\x12;\\n\\x06values\\x18\\x04 \\x03(\\x0b\\x32+.grpc.training.data.v1.EMetrics.ValuesEntry\\x1aI\\n\\x0b\\x45timesEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12)\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x1a.grpc.training.data.v1.Any:\\x02\\x38\\x01\\x1aI\\n\\x0bValuesEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12)\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x1a.grpc.training.data.v1.Any:\\x02\\x38\\x01\\""Q\\n\\rEMetricsBatch\\x12\\r\\n\\x05\\x66orce\\x18\\x01 \\x01(\\x08\\x12\\x31\\n\\x08\\x65metrics\\x18\\x02 \\x03(\\x0b\\x32\\x1f.grpc.training.data.v1.EMetrics\\""\\xd9\\x01\\n\\x05Query\\x12;\\n\\nsearchType\\x18\\x01 \\x01(\\x0e\\x32\\\'.grpc.training.data.v1.Query.SearchType\\x12-\\n\\x04meta\\x18\\x03 \\x01(\\x0b\\x32\\x1f.grpc.training.data.v1.MetaInfo\\x12\\r\\n\\x05since\\x18\\x04 \\x01(\\t\\x12\\x10\\n\\x08pagesize\\x18\\x05 \\x01(\\x05\\x12\\x0b\\n\\x03pos\\x18\\x06 \\x01(\\x03\\""6\\n\\nSearchType\\x12\\x08\\n\\x04TERM\\x10\\x00\\x12\\n\\n\\x06NESTED\\x10\\x01\\x12\\t\\n\\x05MATCH\\x10\\x02\\x12\\x07\\n\\x03\\x41LL\\x10\\x03\\""3\\n\\x0b\\x44\\x65leteQuery\\x12\\x13\\n\\x0btraining_id\\x18\\x01 \\x01(\\t\\x12\\x0f\\n\\x07user_id\\x18\\x02 \\x01(\\t\\""\\x1e\\n\\x0b\\x41\\x64\\x64Response\\x12\\x0f\\n\\x07success\\x18\\x01 \\x01(\\x08\\""!\\n\\x0e\\x44\\x65leteResponse\\x12\\x0f\\n\\x07success\\x18\\x01 \\x01(\\x08\\""\\x1c\\n\\rHelloResponse\\x12\\x0b\\n\\x03msg\\x18\\x01 \\x01(\\t\\""\\x07\\n\\x05\\x45mpty2\\xea\\x06\\n\\x0cTrainingData\\x12K\\n\\x07GetLogs\\x12\\x1c.grpc.training.data.v1.Query\\x1a\\x1e.grpc.training.data.v1.LogLine\\""\\x00\\x30\\x01\\x12P\\n\\x0bGetEMetrics\\x12\\x1c.grpc.training.data.v1.Query\\x1a\\x1f.grpc.training.data.v1.EMetrics\\""\\x00\\x30\\x01\\x12T\\n\\x0b\\x41\\x64\\x64\\x45Metrics\\x12\\x1f.grpc.training.data.v1.EMetrics\\x1a\\"".grpc.training.data.v1.AddResponse\\""\\x00\\x12R\\n\\nAddLogLine\\x12\\x1e.grpc.training.data.v1.LogLine\\x1a\\"".grpc.training.data.v1.AddResponse\\""\\x00\\x12^\\n\\x10\\x41\\x64\\x64\\x45MetricsBatch\\x12$.grpc.training.data.v1.EMetricsBatch\\x1a\\"".grpc.training.data.v1.AddResponse\\""\\x00\\x12\\\\\\n\\x0f\\x41\\x64\\x64LogLineBatch\\x12#.grpc.training.data.v1.LogLineBatch\\x1a\\"".grpc.training.data.v1.AddResponse\\""\\x00\\x12W\\n\\x0e\\x44\\x65leteEMetrics\\x12\\x1c.grpc.training.data.v1.Query\\x1a%.grpc.training.data.v1.DeleteResponse\\""\\x00\\x12W\\n\\x0e\\x44\\x65leteLogLines\\x12\\x1c.grpc.training.data.v1.Query\\x1a%.grpc.training.data.v1.DeleteResponse\\""\\x00\\x12R\\n\\tDeleteJob\\x12\\x1c.grpc.training.data.v1.Query\\x1a%.grpc.training.data.v1.DeleteResponse\\""\\x00\\x12M\\n\\x05Hello\\x12\\x1c.grpc.training.data.v1.Empty\\x1a$.grpc.training.data.v1.HelloResponse\\""\\x00\\x62\\x06proto3\')\n)\n\n\n\n_ANY_DATATYPE = _descriptor.EnumDescriptor(\n  name=\'DataType\',\n  full_name=\'grpc.training.data.v1.Any.DataType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'STRING\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'JSONSTRING\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'INT\', index=2, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'FLOAT\', index=3, number=3,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=367,\n  serialized_end=425,\n)\n_sym_db.RegisterEnumDescriptor(_ANY_DATATYPE)\n\n_QUERY_SEARCHTYPE = _descriptor.EnumDescriptor(\n  name=\'SearchType\',\n  full_name=\'grpc.training.data.v1.Query.SearchType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'TERM\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'NESTED\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'MATCH\', index=2, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ALL\', index=3, number=3,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=1026,\n  serialized_end=1080,\n)\n_sym_db.RegisterEnumDescriptor(_QUERY_SEARCHTYPE)\n\n\n_METAINFO = _descriptor.Descriptor(\n  name=\'MetaInfo\',\n  full_name=\'grpc.training.data.v1.MetaInfo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'training_id\', full_name=\'grpc.training.data.v1.MetaInfo.training_id\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'user_id\', full_name=\'grpc.training.data.v1.MetaInfo.user_id\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'time\', full_name=\'grpc.training.data.v1.MetaInfo.time\', index=2,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'rindex\', full_name=\'grpc.training.data.v1.MetaInfo.rindex\', index=3,\n      number=4, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'subid\', full_name=\'grpc.training.data.v1.MetaInfo.subid\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=46,\n  serialized_end=139,\n)\n\n\n_LOGLINE = _descriptor.Descriptor(\n  name=\'LogLine\',\n  full_name=\'grpc.training.data.v1.LogLine\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'meta\', full_name=\'grpc.training.data.v1.LogLine.meta\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'line\', full_name=\'grpc.training.data.v1.LogLine.line\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=141,\n  serialized_end=211,\n)\n\n\n_LOGLINEBATCH = _descriptor.Descriptor(\n  name=\'LogLineBatch\',\n  full_name=\'grpc.training.data.v1.LogLineBatch\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'force\', full_name=\'grpc.training.data.v1.LogLineBatch.force\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'logLine\', full_name=\'grpc.training.data.v1.LogLineBatch.logLine\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=213,\n  serialized_end=291,\n)\n\n\n_ANY = _descriptor.Descriptor(\n  name=\'Any\',\n  full_name=\'grpc.training.data.v1.Any\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'grpc.training.data.v1.Any.type\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'grpc.training.data.v1.Any.value\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _ANY_DATATYPE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=294,\n  serialized_end=425,\n)\n\n\n_EMETRICS_ETIMESENTRY = _descriptor.Descriptor(\n  name=\'EtimesEntry\',\n  full_name=\'grpc.training.data.v1.EMetrics.EtimesEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'grpc.training.data.v1.EMetrics.EtimesEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'grpc.training.data.v1.EMetrics.EtimesEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=629,\n  serialized_end=702,\n)\n\n_EMETRICS_VALUESENTRY = _descriptor.Descriptor(\n  name=\'ValuesEntry\',\n  full_name=\'grpc.training.data.v1.EMetrics.ValuesEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'grpc.training.data.v1.EMetrics.ValuesEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'grpc.training.data.v1.EMetrics.ValuesEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=704,\n  serialized_end=777,\n)\n\n_EMETRICS = _descriptor.Descriptor(\n  name=\'EMetrics\',\n  full_name=\'grpc.training.data.v1.EMetrics\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'meta\', full_name=\'grpc.training.data.v1.EMetrics.meta\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'etimes\', full_name=\'grpc.training.data.v1.EMetrics.etimes\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'grouplabel\', full_name=\'grpc.training.data.v1.EMetrics.grouplabel\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'values\', full_name=\'grpc.training.data.v1.EMetrics.values\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_EMETRICS_ETIMESENTRY, _EMETRICS_VALUESENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=428,\n  serialized_end=777,\n)\n\n\n_EMETRICSBATCH = _descriptor.Descriptor(\n  name=\'EMetricsBatch\',\n  full_name=\'grpc.training.data.v1.EMetricsBatch\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'force\', full_name=\'grpc.training.data.v1.EMetricsBatch.force\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'emetrics\', full_name=\'grpc.training.data.v1.EMetricsBatch.emetrics\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=779,\n  serialized_end=860,\n)\n\n\n_QUERY = _descriptor.Descriptor(\n  name=\'Query\',\n  full_name=\'grpc.training.data.v1.Query\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'searchType\', full_name=\'grpc.training.data.v1.Query.searchType\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'meta\', full_name=\'grpc.training.data.v1.Query.meta\', index=1,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'since\', full_name=\'grpc.training.data.v1.Query.since\', index=2,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'pagesize\', full_name=\'grpc.training.data.v1.Query.pagesize\', index=3,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'pos\', full_name=\'grpc.training.data.v1.Query.pos\', index=4,\n      number=6, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _QUERY_SEARCHTYPE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=863,\n  serialized_end=1080,\n)\n\n\n_DELETEQUERY = _descriptor.Descriptor(\n  name=\'DeleteQuery\',\n  full_name=\'grpc.training.data.v1.DeleteQuery\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'training_id\', full_name=\'grpc.training.data.v1.DeleteQuery.training_id\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'user_id\', full_name=\'grpc.training.data.v1.DeleteQuery.user_id\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1082,\n  serialized_end=1133,\n)\n\n\n_ADDRESPONSE = _descriptor.Descriptor(\n  name=\'AddResponse\',\n  full_name=\'grpc.training.data.v1.AddResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'success\', full_name=\'grpc.training.data.v1.AddResponse.success\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1135,\n  serialized_end=1165,\n)\n\n\n_DELETERESPONSE = _descriptor.Descriptor(\n  name=\'DeleteResponse\',\n  full_name=\'grpc.training.data.v1.DeleteResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'success\', full_name=\'grpc.training.data.v1.DeleteResponse.success\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1167,\n  serialized_end=1200,\n)\n\n\n_HELLORESPONSE = _descriptor.Descriptor(\n  name=\'HelloResponse\',\n  full_name=\'grpc.training.data.v1.HelloResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'msg\', full_name=\'grpc.training.data.v1.HelloResponse.msg\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1202,\n  serialized_end=1230,\n)\n\n\n_EMPTY = _descriptor.Descriptor(\n  name=\'Empty\',\n  full_name=\'grpc.training.data.v1.Empty\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1232,\n  serialized_end=1239,\n)\n\n_LOGLINE.fields_by_name[\'meta\'].message_type = _METAINFO\n_LOGLINEBATCH.fields_by_name[\'logLine\'].message_type = _LOGLINE\n_ANY.fields_by_name[\'type\'].enum_type = _ANY_DATATYPE\n_ANY_DATATYPE.containing_type = _ANY\n_EMETRICS_ETIMESENTRY.fields_by_name[\'value\'].message_type = _ANY\n_EMETRICS_ETIMESENTRY.containing_type = _EMETRICS\n_EMETRICS_VALUESENTRY.fields_by_name[\'value\'].message_type = _ANY\n_EMETRICS_VALUESENTRY.containing_type = _EMETRICS\n_EMETRICS.fields_by_name[\'meta\'].message_type = _METAINFO\n_EMETRICS.fields_by_name[\'etimes\'].message_type = _EMETRICS_ETIMESENTRY\n_EMETRICS.fields_by_name[\'values\'].message_type = _EMETRICS_VALUESENTRY\n_EMETRICSBATCH.fields_by_name[\'emetrics\'].message_type = _EMETRICS\n_QUERY.fields_by_name[\'searchType\'].enum_type = _QUERY_SEARCHTYPE\n_QUERY.fields_by_name[\'meta\'].message_type = _METAINFO\n_QUERY_SEARCHTYPE.containing_type = _QUERY\nDESCRIPTOR.message_types_by_name[\'MetaInfo\'] = _METAINFO\nDESCRIPTOR.message_types_by_name[\'LogLine\'] = _LOGLINE\nDESCRIPTOR.message_types_by_name[\'LogLineBatch\'] = _LOGLINEBATCH\nDESCRIPTOR.message_types_by_name[\'Any\'] = _ANY\nDESCRIPTOR.message_types_by_name[\'EMetrics\'] = _EMETRICS\nDESCRIPTOR.message_types_by_name[\'EMetricsBatch\'] = _EMETRICSBATCH\nDESCRIPTOR.message_types_by_name[\'Query\'] = _QUERY\nDESCRIPTOR.message_types_by_name[\'DeleteQuery\'] = _DELETEQUERY\nDESCRIPTOR.message_types_by_name[\'AddResponse\'] = _ADDRESPONSE\nDESCRIPTOR.message_types_by_name[\'DeleteResponse\'] = _DELETERESPONSE\nDESCRIPTOR.message_types_by_name[\'HelloResponse\'] = _HELLORESPONSE\nDESCRIPTOR.message_types_by_name[\'Empty\'] = _EMPTY\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nMetaInfo = _reflection.GeneratedProtocolMessageType(\'MetaInfo\', (_message.Message,), dict(\n  DESCRIPTOR = _METAINFO,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.MetaInfo)\n  ))\n_sym_db.RegisterMessage(MetaInfo)\n\nLogLine = _reflection.GeneratedProtocolMessageType(\'LogLine\', (_message.Message,), dict(\n  DESCRIPTOR = _LOGLINE,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.LogLine)\n  ))\n_sym_db.RegisterMessage(LogLine)\n\nLogLineBatch = _reflection.GeneratedProtocolMessageType(\'LogLineBatch\', (_message.Message,), dict(\n  DESCRIPTOR = _LOGLINEBATCH,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.LogLineBatch)\n  ))\n_sym_db.RegisterMessage(LogLineBatch)\n\nAny = _reflection.GeneratedProtocolMessageType(\'Any\', (_message.Message,), dict(\n  DESCRIPTOR = _ANY,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.Any)\n  ))\n_sym_db.RegisterMessage(Any)\n\nEMetrics = _reflection.GeneratedProtocolMessageType(\'EMetrics\', (_message.Message,), dict(\n\n  EtimesEntry = _reflection.GeneratedProtocolMessageType(\'EtimesEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _EMETRICS_ETIMESENTRY,\n    __module__ = \'training_data_pb2\'\n    # @@protoc_insertion_point(class_scope:grpc.training.data.v1.EMetrics.EtimesEntry)\n    ))\n  ,\n\n  ValuesEntry = _reflection.GeneratedProtocolMessageType(\'ValuesEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _EMETRICS_VALUESENTRY,\n    __module__ = \'training_data_pb2\'\n    # @@protoc_insertion_point(class_scope:grpc.training.data.v1.EMetrics.ValuesEntry)\n    ))\n  ,\n  DESCRIPTOR = _EMETRICS,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.EMetrics)\n  ))\n_sym_db.RegisterMessage(EMetrics)\n_sym_db.RegisterMessage(EMetrics.EtimesEntry)\n_sym_db.RegisterMessage(EMetrics.ValuesEntry)\n\nEMetricsBatch = _reflection.GeneratedProtocolMessageType(\'EMetricsBatch\', (_message.Message,), dict(\n  DESCRIPTOR = _EMETRICSBATCH,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.EMetricsBatch)\n  ))\n_sym_db.RegisterMessage(EMetricsBatch)\n\nQuery = _reflection.GeneratedProtocolMessageType(\'Query\', (_message.Message,), dict(\n  DESCRIPTOR = _QUERY,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.Query)\n  ))\n_sym_db.RegisterMessage(Query)\n\nDeleteQuery = _reflection.GeneratedProtocolMessageType(\'DeleteQuery\', (_message.Message,), dict(\n  DESCRIPTOR = _DELETEQUERY,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.DeleteQuery)\n  ))\n_sym_db.RegisterMessage(DeleteQuery)\n\nAddResponse = _reflection.GeneratedProtocolMessageType(\'AddResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _ADDRESPONSE,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.AddResponse)\n  ))\n_sym_db.RegisterMessage(AddResponse)\n\nDeleteResponse = _reflection.GeneratedProtocolMessageType(\'DeleteResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _DELETERESPONSE,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.DeleteResponse)\n  ))\n_sym_db.RegisterMessage(DeleteResponse)\n\nHelloResponse = _reflection.GeneratedProtocolMessageType(\'HelloResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _HELLORESPONSE,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.HelloResponse)\n  ))\n_sym_db.RegisterMessage(HelloResponse)\n\nEmpty = _reflection.GeneratedProtocolMessageType(\'Empty\', (_message.Message,), dict(\n  DESCRIPTOR = _EMPTY,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.Empty)\n  ))\n_sym_db.RegisterMessage(Empty)\n\n\n_EMETRICS_ETIMESENTRY.has_options = True\n_EMETRICS_ETIMESENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n_EMETRICS_VALUESENTRY.has_options = True\n_EMETRICS_VALUESENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n\n_TRAININGDATA = _descriptor.ServiceDescriptor(\n  name=\'TrainingData\',\n  full_name=\'grpc.training.data.v1.TrainingData\',\n  file=DESCRIPTOR,\n  index=0,\n  options=None,\n  serialized_start=1242,\n  serialized_end=2116,\n  methods=[\n  _descriptor.MethodDescriptor(\n    name=\'GetLogs\',\n    full_name=\'grpc.training.data.v1.TrainingData.GetLogs\',\n    index=0,\n    containing_service=None,\n    input_type=_QUERY,\n    output_type=_LOGLINE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'GetEMetrics\',\n    full_name=\'grpc.training.data.v1.TrainingData.GetEMetrics\',\n    index=1,\n    containing_service=None,\n    input_type=_QUERY,\n    output_type=_EMETRICS,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'AddEMetrics\',\n    full_name=\'grpc.training.data.v1.TrainingData.AddEMetrics\',\n    index=2,\n    containing_service=None,\n    input_type=_EMETRICS,\n    output_type=_ADDRESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'AddLogLine\',\n    full_name=\'grpc.training.data.v1.TrainingData.AddLogLine\',\n    index=3,\n    containing_service=None,\n    input_type=_LOGLINE,\n    output_type=_ADDRESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'AddEMetricsBatch\',\n    full_name=\'grpc.training.data.v1.TrainingData.AddEMetricsBatch\',\n    index=4,\n    containing_service=None,\n    input_type=_EMETRICSBATCH,\n    output_type=_ADDRESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'AddLogLineBatch\',\n    full_name=\'grpc.training.data.v1.TrainingData.AddLogLineBatch\',\n    index=5,\n    containing_service=None,\n    input_type=_LOGLINEBATCH,\n    output_type=_ADDRESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'DeleteEMetrics\',\n    full_name=\'grpc.training.data.v1.TrainingData.DeleteEMetrics\',\n    index=6,\n    containing_service=None,\n    input_type=_QUERY,\n    output_type=_DELETERESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'DeleteLogLines\',\n    full_name=\'grpc.training.data.v1.TrainingData.DeleteLogLines\',\n    index=7,\n    containing_service=None,\n    input_type=_QUERY,\n    output_type=_DELETERESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'DeleteJob\',\n    full_name=\'grpc.training.data.v1.TrainingData.DeleteJob\',\n    index=8,\n    containing_service=None,\n    input_type=_QUERY,\n    output_type=_DELETERESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'Hello\',\n    full_name=\'grpc.training.data.v1.TrainingData.Hello\',\n    index=9,\n    containing_service=None,\n    input_type=_EMPTY,\n    output_type=_HELLORESPONSE,\n    options=None,\n  ),\n])\n_sym_db.RegisterServiceDescriptor(_TRAININGDATA)\n\nDESCRIPTOR.services_by_name[\'TrainingData\'] = _TRAININGDATA\n\n# @@protoc_insertion_point(module_scope)\n'"
metrics/log_collectors/training_data_service_client/training_data_pb2_grpc.py,0,"b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\nfrom . import training_data_pb2 as training__data__pb2\n\n\nclass TrainingDataStub(object):\n  """"""import ""google/api/annotations.proto"";\n\n  // See https://developers.google.com/protocol-buffers/docs/proto3#any\n  // and https://github.com/google/protobuf/blob/master/src/google/protobuf/any.proto\n  import ""google/protobuf/any.proto"";\n\n  Service to store evaluation metrics\n  ===== GET ENDPOINTS, for external fetch =============\n  """"""\n\n  def __init__(self, channel):\n    """"""Constructor.\n\n    Args:\n      channel: A grpc.Channel.\n    """"""\n    self.GetLogs = channel.unary_stream(\n        \'/grpc.training.data.v1.TrainingData/GetLogs\',\n        request_serializer=training__data__pb2.Query.SerializeToString,\n        response_deserializer=training__data__pb2.LogLine.FromString,\n        )\n    self.GetEMetrics = channel.unary_stream(\n        \'/grpc.training.data.v1.TrainingData/GetEMetrics\',\n        request_serializer=training__data__pb2.Query.SerializeToString,\n        response_deserializer=training__data__pb2.EMetrics.FromString,\n        )\n    self.AddEMetrics = channel.unary_unary(\n        \'/grpc.training.data.v1.TrainingData/AddEMetrics\',\n        request_serializer=training__data__pb2.EMetrics.SerializeToString,\n        response_deserializer=training__data__pb2.AddResponse.FromString,\n        )\n    self.AddLogLine = channel.unary_unary(\n        \'/grpc.training.data.v1.TrainingData/AddLogLine\',\n        request_serializer=training__data__pb2.LogLine.SerializeToString,\n        response_deserializer=training__data__pb2.AddResponse.FromString,\n        )\n    self.AddEMetricsBatch = channel.unary_unary(\n        \'/grpc.training.data.v1.TrainingData/AddEMetricsBatch\',\n        request_serializer=training__data__pb2.EMetricsBatch.SerializeToString,\n        response_deserializer=training__data__pb2.AddResponse.FromString,\n        )\n    self.AddLogLineBatch = channel.unary_unary(\n        \'/grpc.training.data.v1.TrainingData/AddLogLineBatch\',\n        request_serializer=training__data__pb2.LogLineBatch.SerializeToString,\n        response_deserializer=training__data__pb2.AddResponse.FromString,\n        )\n    self.DeleteEMetrics = channel.unary_unary(\n        \'/grpc.training.data.v1.TrainingData/DeleteEMetrics\',\n        request_serializer=training__data__pb2.Query.SerializeToString,\n        response_deserializer=training__data__pb2.DeleteResponse.FromString,\n        )\n    self.DeleteLogLines = channel.unary_unary(\n        \'/grpc.training.data.v1.TrainingData/DeleteLogLines\',\n        request_serializer=training__data__pb2.Query.SerializeToString,\n        response_deserializer=training__data__pb2.DeleteResponse.FromString,\n        )\n    self.DeleteJob = channel.unary_unary(\n        \'/grpc.training.data.v1.TrainingData/DeleteJob\',\n        request_serializer=training__data__pb2.Query.SerializeToString,\n        response_deserializer=training__data__pb2.DeleteResponse.FromString,\n        )\n    self.Hello = channel.unary_unary(\n        \'/grpc.training.data.v1.TrainingData/Hello\',\n        request_serializer=training__data__pb2.Empty.SerializeToString,\n        response_deserializer=training__data__pb2.HelloResponse.FromString,\n        )\n\n\nclass TrainingDataServicer(object):\n  """"""import ""google/api/annotations.proto"";\n\n  // See https://developers.google.com/protocol-buffers/docs/proto3#any\n  // and https://github.com/google/protobuf/blob/master/src/google/protobuf/any.proto\n  import ""google/protobuf/any.proto"";\n\n  Service to store evaluation metrics\n  ===== GET ENDPOINTS, for external fetch =============\n  """"""\n\n  def GetLogs(self, request, context):\n    """"""Get loglines, based on query\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def GetEMetrics(self, request, context):\n    """"""Get evaluation metrics records, based on query\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def AddEMetrics(self, request, context):\n    """"""===== UPDATE ENDPOINTS, for internal use only =========\n    (Strip these from the proto for external client generation!)\n\n    Add evaluation metrics record\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def AddLogLine(self, request, context):\n    """"""Add log line record\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def AddEMetricsBatch(self, request, context):\n    """"""Add evaluation metrics record\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def AddLogLineBatch(self, request, context):\n    """"""Add log line record\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def DeleteEMetrics(self, request, context):\n    """"""Delete all evaluation metrics belonging to a training job or user id\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def DeleteLogLines(self, request, context):\n    """"""Delete all log lines belonging to a training job or user id\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def DeleteJob(self, request, context):\n    """"""Delete all log lines belonging to a training job or user id\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def Hello(self, request, context):\n    """"""===== In case you want it to say ""Hello"" =========\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n\ndef add_TrainingDataServicer_to_server(servicer, server):\n  rpc_method_handlers = {\n      \'GetLogs\': grpc.unary_stream_rpc_method_handler(\n          servicer.GetLogs,\n          request_deserializer=training__data__pb2.Query.FromString,\n          response_serializer=training__data__pb2.LogLine.SerializeToString,\n      ),\n      \'GetEMetrics\': grpc.unary_stream_rpc_method_handler(\n          servicer.GetEMetrics,\n          request_deserializer=training__data__pb2.Query.FromString,\n          response_serializer=training__data__pb2.EMetrics.SerializeToString,\n      ),\n      \'AddEMetrics\': grpc.unary_unary_rpc_method_handler(\n          servicer.AddEMetrics,\n          request_deserializer=training__data__pb2.EMetrics.FromString,\n          response_serializer=training__data__pb2.AddResponse.SerializeToString,\n      ),\n      \'AddLogLine\': grpc.unary_unary_rpc_method_handler(\n          servicer.AddLogLine,\n          request_deserializer=training__data__pb2.LogLine.FromString,\n          response_serializer=training__data__pb2.AddResponse.SerializeToString,\n      ),\n      \'AddEMetricsBatch\': grpc.unary_unary_rpc_method_handler(\n          servicer.AddEMetricsBatch,\n          request_deserializer=training__data__pb2.EMetricsBatch.FromString,\n          response_serializer=training__data__pb2.AddResponse.SerializeToString,\n      ),\n      \'AddLogLineBatch\': grpc.unary_unary_rpc_method_handler(\n          servicer.AddLogLineBatch,\n          request_deserializer=training__data__pb2.LogLineBatch.FromString,\n          response_serializer=training__data__pb2.AddResponse.SerializeToString,\n      ),\n      \'DeleteEMetrics\': grpc.unary_unary_rpc_method_handler(\n          servicer.DeleteEMetrics,\n          request_deserializer=training__data__pb2.Query.FromString,\n          response_serializer=training__data__pb2.DeleteResponse.SerializeToString,\n      ),\n      \'DeleteLogLines\': grpc.unary_unary_rpc_method_handler(\n          servicer.DeleteLogLines,\n          request_deserializer=training__data__pb2.Query.FromString,\n          response_serializer=training__data__pb2.DeleteResponse.SerializeToString,\n      ),\n      \'DeleteJob\': grpc.unary_unary_rpc_method_handler(\n          servicer.DeleteJob,\n          request_deserializer=training__data__pb2.Query.FromString,\n          response_serializer=training__data__pb2.DeleteResponse.SerializeToString,\n      ),\n      \'Hello\': grpc.unary_unary_rpc_method_handler(\n          servicer.Hello,\n          request_deserializer=training__data__pb2.Empty.FromString,\n          response_serializer=training__data__pb2.HelloResponse.SerializeToString,\n      ),\n  }\n  generic_handler = grpc.method_handlers_generic_handler(\n      \'grpc.training.data.v1.TrainingData\', rpc_method_handlers)\n  server.add_generic_rpc_handlers((generic_handler,))\n'"
demos/fashion-mnist-training/fashion-train/fashion-training/experiment.py,0,"b'\'\'\'\nClassification of Fashion MNIST images using a convolutional model written in Keras.\nThis example is using the Fashion MNIST database of clothing images provided by Zalando Research.\nhttps://github.com/zalandoresearch/fashion-mnist\n\nAuthor: IBM Watson\n\'\'\'\n\nimport argparse\nimport gzip\nimport keras\nfrom keras.callbacks import TensorBoard\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport sys\nimport time\n\nepochs = 30\nbatch_size = 128\ndropout = 0.4\n\n# Add data dir to file path\ndata_dir = os.environ[""DATA_DIR""]\ntrain_images_file = os.path.join(data_dir, \'train-images-idx3-ubyte.gz\')\ntrain_labels_file = os.path.join(data_dir, \'train-labels-idx1-ubyte.gz\')\ntest_images_file = os.path.join(data_dir, \'t10k-images-idx3-ubyte.gz\')\ntest_labels_file = os.path.join(data_dir, \'t10k-labels-idx1-ubyte.gz\')\n\n\n# Load data in MNIST format\nwith gzip.open(train_labels_file, \'rb\') as lbpath:\n    y_train = np.frombuffer(lbpath.read(), dtype=np.uint8,\n                           offset=8)\n\nwith gzip.open(train_images_file, \'rb\') as imgpath:\n    X_train = np.frombuffer(imgpath.read(), dtype=np.uint8,\n                           offset=16).reshape(len(y_train), 784)\n\nwith gzip.open(test_labels_file, \'rb\') as lbpath:\n    y_test = np.frombuffer(lbpath.read(), dtype=np.uint8,\n                           offset=8)\n\nwith gzip.open(test_images_file, \'rb\') as imgpath:\n    X_test = np.frombuffer(imgpath.read(), dtype=np.uint8,\n                           offset=16).reshape(len(y_test), 784)\n\n# Split a validation set off the train set\nsplit = int(len(y_train) * .9)-1\nX_train, X_val = X_train[:split], X_train[split:]\ny_train, y_val = y_train[:split], y_train[split:]\n\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\ny_val = to_categorical(y_val)\n\n# Reshape to correct format for conv2d input\nimg_rows, img_cols = 28, 28\ninput_shape = (img_rows, img_cols, 1)\n\nX_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\nX_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\nX_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, 1)\n\nX_train = X_train.astype(\'float32\')\nX_test = X_test.astype(\'float32\')\n\nX_train /= 255\nX_test /= 255\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation=\'relu\',\n                 kernel_initializer=\'he_normal\',\n                 input_shape=input_shape))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(dropout))\nmodel.add(Conv2D(64, (3, 3), activation=\'relu\'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(dropout))\nmodel.add(Conv2D(128, (3, 3), activation=\'relu\'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(dropout))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation=\'relu\'))\nmodel.add(Dropout(dropout))\n\nnum_classes = 10\nmodel.add(Dense(num_classes, activation=\'softmax\'))\n\nmodel.compile(loss=\'categorical_crossentropy\',\n              optimizer=""adam"",\n              metrics=[\'accuracy\'])\n\nmodel.summary()\n\nstart_time = time.time()\n\ntb_directory = os.environ[""JOB_STATE_DIR""]+""/logs/tb/test""\ntensorboard = TensorBoard(log_dir=tb_directory)\nhistory = model.fit(X_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=(X_val, y_val),\n                    callbacks=[tensorboard])\nscore = model.evaluate(X_test, y_test, verbose=0)\n\nend_time = time.time()\nminutes, seconds = divmod(end_time-start_time, 60)\nprint(""Total train time: {:0>2}:{:05.2f}"".format(int(minutes),seconds))\n\nprint(\'Final train accuracy:      %.4f\' % history.history[\'acc\'][-1])\nprint(\'Final train loss: %.4f\' % history.history[\'loss\'][-1])\nprint(\'Final validation accuracy: %.4f\' % history.history[\'val_acc\'][-1])\nprint(\'Final validation loss: %.4f\' % history.history[\'val_loss\'][-1])\nprint(\'Final test accuracy:       %.4f\' %  score[1])\nprint(\'Final test loss: %.4f\' % score[0])\n\nmodel_path = os.path.join(os.environ[""RESULT_DIR""], ""model.h5"")\nprint(""\\nSaving model to: %s"" % model_path)\nmodel.save(model_path)\n'"
etc/examples/c10d-dist-onnx/model-files/train_dist_onnx.py,20,"b'import time\nimport argparse\nimport sys\nimport os\nimport threading\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.onnx\n\nfrom math import ceil\nfrom random import Random\nfrom torch.multiprocessing import Process\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\n\n\nclass Net(nn.Module):\n    """""" Network architecture. """"""\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef get_dataset():\n    """""" Get FashionMNIST dataset """"""\n    data_path = os.environ.get(""DATA_DIR"") + \'/data\'\n    trainset = datasets.FashionMNIST(\n        data_path,\n        train=True,\n        download=False,\n        transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307, ), (0.3081, ))\n        ]))\n    testset = datasets.FashionMNIST(\n        data_path,\n        train=False,\n        download=False,\n        transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307, ), (0.3081, ))\n        ]))\n    return trainset, testset\n\n\ndef average_gradients(model):\n    """""" Gradient averaging. """"""\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM, group=dist.group.WORLD)\n        param.grad.data /= size\n\n\ndef multigpu_average_gradients(model):\n    """""" Gradient averaging. """"""\n    size = float(dist.get_world_size())\n    tensor_list = []\n    for dev_idx in range(torch.cuda.device_count()):\n        tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n    dist.all_reduce_multigpu(tensor_list, op=dist.reduce_op.SUM, group=dist.group.WORLD)\n    for tensor in tensor_list:\n        tensor /= size*len(tensor_list)\n\ndef run(rank, size, batch_size, is_gpu):\n    """""" Distributed Synchronous SGD Example """"""\n    torch.manual_seed(1234)\n    train_set, test_set = get_dataset()\n    result_dir = os.environ.get(""RESULT_DIR"") + \'/saved_model\'\n    # For GPU use\n    if is_gpu:\n        #torch.cuda.set_device(local_device)\n        model = Net().cuda()\n    else:\n        model = Net()\n    if not (size == 1):\n        model = torch.nn.parallel.DistributedDataParallel(model)\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_set,\n            num_replicas=dist.get_world_size(), rank=dist.get_rank())\n    train_set = torch.utils.data.DataLoader(\n        train_set, batch_size=batch_size, shuffle=(train_sampler is None), sampler=train_sampler,\n        pin_memory=True)\n    test_set = torch.utils.data.DataLoader(\n        test_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    # To train model\n    model.train()\n    for epoch in range(100):\n        epoch_loss = 0.0\n        for data, target in train_set:\n            # For GPU use\n            if is_gpu:\n                data, target = data.cuda(), target.cuda()\n            else:\n                data, target = Variable(data), Variable(target)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.item()\n            loss.backward()\n            if not (size == 1):\n                average_gradients(model)\n            optimizer.step()\n        print(\'Process \', dist.get_rank(),\n              \', epoch \', epoch, \'. avg_loss: \',\n              epoch_loss / len(train_set))\n\n    # Test model\n    if int(os.environ.get(""LEARNER_ID"")) == 1:\n        model.eval()\n        test_loss = 0.0\n        correct = 0\n        with torch.no_grad():\n            for data, target in test_set:\n                # For GPU use\n                if is_gpu:\n                    data, target = data.cuda(), target.cuda()\n                else:\n                    data, target = Variable(data), Variable(target)\n                output = model(data)\n                test_loss += F.nll_loss(output, target, reduction=""sum"").item()\n                pred = output.data.max(1, keepdim=True)[1]\n                correct += pred.eq(target.data.view_as(pred)).sum().item()\n            print(\'Test_set:  avg_loss: \', test_loss / len(test_set.dataset),\n                  \', accuracy: \', 100. * correct / len(test_set.dataset), \'%\')\n\n    # Save model\n    if int(os.environ.get(""LEARNER_ID"")) == 1:\n        torch.save(model, result_dir)\n        dummy_input = """"\n        if is_gpu:\n            dummy_input = Variable(torch.randn(1, 1, 28, 28)).cuda()\n        else:\n            dummy_input = Variable(torch.randn(1, 1, 28, 28))\n        model_path = os.environ.get(""RESULT_DIR"") + ""/pytorch-dist.onnx""\n        torch.onnx.export(model, dummy_input, model_path)\n\n\n# Change \'backend\' to appropriate backend identifier\ndef init_processes(rank, size, fn, path_to_file, batch_size, is_gpu, backend):\n    """""" Initialize the distributed environment. """"""\n    print(""Process "", rank, "" connected"")\n    dist.init_process_group(backend,\n                            init_method=path_to_file,\n                            world_size=size, group_name=""train_dist"",\n                            rank=rank)\n    print(""FOUND SHARED FILE"")\n    fn(rank, size, batch_size, is_gpu)\n\ndef local_process(target, args):\n    return Process(target=target, args=args)\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--batch_size\', type=int, default=1024, help=\'Specify the batch size to be used in training\')\n    args = parser.parse_args()\n\n    # Default batch size is set to 1024. When using a large numbers of learners,\n    # a larger batch size is sometimes necessary to see speed improvements.\n    batch_size = args.batch_size\n\n    start_time = time.time()\n    num_gpus = int(float(os.environ.get(""GPU_COUNT"")))\n    if num_gpus == 0:\n        world_size = int(os.environ.get(""NUM_LEARNERS""))\n    else:\n        world_size = num_gpus * int(os.environ.get(""NUM_LEARNERS""))\n    data_dir = ""file:///job/"" + os.environ.get(""TRAINING_ID"")\n    processes = []\n    print(""data_dir is "" + data_dir)\n\n    if world_size == 1:\n        run(0, 1, batch_size, (num_gpus == 1))\n        print(""COMPLETION TIME: "", time.time() - start_time)\n    else:\n        if num_gpus == 0:\n            p = local_process(init_processes, (0 , world_size, run, data_dir, batch_size, False, \'gloo\'))\n            p.start()\n            processes.append(p)\n        else:\n            for process_num in range(0, num_gpus):\n                p = local_process(init_processes, (process_num*int(os.environ.get(""NUM_LEARNERS"")) + int(os.environ.get(""LEARNER_ID"")) - 1, world_size, run, data_dir, batch_size, True, \'gloo\'))\n                p.start()\n                processes.append(p)\n\n        for p in processes:\n            p.join()\n\n        print(""COMPLETION TIME: "", time.time() - start_time)\n\n        # FfDL assume only the master learner job will terminate and store all\n        # the logging file.\n        if int(os.environ.get(""LEARNER_ID"")) != 1:\n            while True:\n                time.sleep(1000000)\n'"
etc/examples/c10d-mpi-parallelism/model-files/train_dist_parallel_mpi.py,17,"b'import time\nimport argparse\nimport sys\nimport os\nimport threading\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.onnx\nimport copy\n\nfrom math import ceil\nfrom random import Random\nfrom torch.multiprocessing import Process\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\n\nclass Net(nn.Module):\n    """""" Network architecture. """"""\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef get_dataset():\n    """""" Get FashionMNIST dataset """"""\n    data_path = datadir + \'/data\'\n    trainset = datasets.FashionMNIST(\n        data_path,\n        train=True,\n        download=False,\n        transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307, ), (0.3081, ))\n        ]))\n    testset = datasets.FashionMNIST(\n        data_path,\n        train=False,\n        download=False,\n        transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307, ), (0.3081, ))\n        ]))\n    return trainset, testset\n\n\ndef run(rank, size, batch_size, is_gpu, is_distributed):\n    """""" Distributed Synchronous SGD Example """"""\n    torch.manual_seed(1234)\n    train_set, test_set = get_dataset()\n    result_dir = resultdir + \'/saved_model\'\n    # For GPU use\n    if is_gpu:\n        model = Net().cuda()\n    else:\n        model = Net()\n    if is_distributed:\n        model = torch.nn.parallel.DistributedDataParallel(model)\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_set,\n            num_replicas=dist.get_world_size() , rank=dist.get_rank())\n    train_set = torch.utils.data.DataLoader(\n        train_set, batch_size=batch_size, shuffle=(train_sampler is None), sampler=train_sampler,\n        pin_memory=True)\n    test_set = torch.utils.data.DataLoader(\n        test_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n    # To train model\n    model.train()\n    for epoch in range(100):\n        epoch_loss = 0.0\n        if is_distributed:\n            train_sampler.set_epoch(epoch)\n        for data, target in train_set:\n            # For GPU use\n            if is_gpu:\n                data, target = data.cuda(), target.cuda()\n            else:\n                data, target = Variable(data), Variable(target)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n        print(\'Process \', dist.get_rank(),\n              \', epoch \', epoch, \'. avg_loss: \',\n              epoch_loss / len(train_set))\n\n    # Test model\n    if os.environ.get(""LEARNER_ID"") != None:\n        model.eval()\n        test_loss = 0.0\n        correct = 0\n        with torch.no_grad():\n            for data, target in test_set:\n                # For GPU use\n                if is_gpu:\n                    data, target = data.cuda(), target.cuda()\n                else:\n                    data, target = Variable(data), Variable(target)\n                output = model(data)\n                test_loss += F.nll_loss(output, target, reduction=""sum"").item()\n                pred = output.data.max(1, keepdim=True)[1]\n                correct += pred.eq(target.data.view_as(pred)).sum().item()\n            print(\'Test_set:  avg_loss: \', test_loss / len(test_set.dataset),\n                  \', accuracy: \', 100. * correct / len(test_set.dataset), \'%\')\n\n    # Save model\n    if os.environ.get(""LEARNER_ID"") != None:\n        torch.save(model.state_dict(), result_dir)\n        # NOTE: ONNX doesn\'t support scatter operation yet.\n        # dummy_input = """"\n        # if is_gpu:\n        #     dummy_input = Variable(torch.randn(1, 1, 28, 28)).cuda()\n        # else:\n        #     dummy_input = Variable(torch.randn(1, 1, 28, 28))\n        # model_path = os.environ.get(""RESULT_DIR"") + ""/pytorch-dist.onnx""\n        # torch.onnx.export(model, dummy_input, model_path)\n\n\n# Change \'backend\' to appropriate backend identifier\ndef init_processes(rank, size, fn, path_to_file, batch_size, is_gpu, backend):\n    """""" Initialize the distributed environment. """"""\n    print(""Process connected"")\n    dist.init_process_group(backend,\n                            init_method=path_to_file,\n                            world_size=size, group_name=""train_dist"",\n                            rank=rank)\n    print(""FOUND SHARED FILE"")\n    fn(rank, size, batch_size, is_gpu, True)\n\ndef local_process(target, args):\n    return Process(target=target, args=args)\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--batch_size\', type=int, default=1024, help=\'Specify the batch size to be used in training\')\n    parser.add_argument(\'--cuda\', type=str, default=""True"", help=\'Enables CUDA training\')\n    parser.add_argument(\'--nodes\', type=int, default=2, help=\'Number or nodes\')\n    parser.add_argument(\'--data_dir\', type=str, help=\'Dataset directory path\')\n    parser.add_argument(\'--result_dir\', type=str, help=\'Result directory path\')\n    parser.add_argument(\'--rank_id\', type=int, default=0, help=\'Rank of the current process\')\n    args = parser.parse_args()\n\n    # Default batch size is set to 1024. When using a large numbers of learners,\n    # a larger batch size is sometimes necessary to see speed improvements.\n    batch_size = args.batch_size\n\n    start_time = time.time()\n    data_dir = ""file:///job/""\n    datadir = args.data_dir\n    resultdir = args.result_dir\n\n    if args.nodes == 1:\n        run(0, args.nodes, batch_size, (args.cuda == ""True""), False)\n        print(""COMPLETION TIME: "", time.time() - start_time)\n    else:\n        if args.cuda == ""True"":\n            init_processes(0, args.nodes, run, data_dir, batch_size, True, \'mpi\')\n        else:\n            init_processes(0, args.nodes, run, data_dir, batch_size, False, \'mpi\')\n\n        print(""COMPLETION TIME: "", time.time() - start_time)\n        # FfDL assume only the master learner job will terminate and store all\n        # the logging file.\n        if os.environ.get(""LEARNER_ID"") == None:\n            while True:\n                time.sleep(1000000)\n'"
etc/examples/c10d-native-parallelism/model-files/train_dist_parallel.py,21,"b'import time\nimport argparse\nimport sys\nimport os\nimport threading\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.onnx\nimport copy\n\nfrom math import ceil\nfrom random import Random\nfrom torch.multiprocessing import Process\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\n\n\nclass Net(nn.Module):\n    """""" Network architecture. """"""\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        # Check input size during forward propagation\n        # print(""\\tIn Model: input size"", x.size())\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef get_dataset():\n    """""" Get FashionMNIST dataset """"""\n    data_path = os.environ.get(""DATA_DIR"") + \'/data\'\n    trainset = datasets.FashionMNIST(\n        data_path,\n        train=True,\n        download=False,\n        transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307, ), (0.3081, ))\n        ]))\n    testset = datasets.FashionMNIST(\n        data_path,\n        train=False,\n        download=False,\n        transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307, ), (0.3081, ))\n        ]))\n    return trainset, testset\n\n\ndef average_gradients(model):\n    """""" Gradient averaging. """"""\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM, group=dist.group.WORLD)\n        param.grad.data /= size\n\ndef multigpu_average_gradients(model):\n    """""" Gradient averaging. """"""\n    size = float(dist.get_world_size())\n    # for dev_idx in range(torch.cuda.device_count()):\n    #     tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n    for param in model.parameters():\n        tensor_list = []\n        for dev_idx in range(torch.cuda.device_count()):\n            tensor_list.append(param.grad.data.cuda(dev_idx))\n            dist.all_reduce_multigpu(tensor_list, op=dist.reduce_op.SUM, group=dist.group.WORLD)\n            for tensor in tensor_list:\n                tensor /= (size*len(tensor_list))\n\n\ndef run(rank, size, batch_size, is_gpu, is_distributed):\n    """""" Distributed Synchronous SGD Example """"""\n    torch.manual_seed(1234)\n    train_set, test_set = get_dataset()\n    result_dir = os.environ.get(""RESULT_DIR"") + \'/saved_model\'\n    # For GPU use\n    if is_gpu:\n        model = Net().cuda()\n    else:\n        model = Net()\n    if is_distributed:\n        if is_gpu:\n            model = torch.nn.parallel.DistributedDataParallel(model)\n        else:\n            model = torch.nn.parallel.DistributedDataParallelCPU(model)\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_set,\n            num_replicas=dist.get_world_size() , rank=dist.get_rank())\n    train_set = torch.utils.data.DataLoader(\n        train_set, batch_size=batch_size, shuffle=(train_sampler is None), sampler=train_sampler,\n        pin_memory=True)\n    test_set = torch.utils.data.DataLoader(\n        test_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    # To train model\n    model.train()\n    for epoch in range(100):\n        epoch_loss = 0.0\n        if is_distributed:\n            train_sampler.set_epoch(epoch)\n        for data, target in train_set:\n            # For GPU use\n            if is_gpu:\n                data, target = Variable(data).cuda(), Variable(target).cuda()\n            else:\n                data, target = Variable(data), Variable(target)\n            optimizer.zero_grad()\n            output = model(data)\n            # Check total batch size\n            # print(""Outside: input size"", data.size(),\n            #       ""output_size"", output.size())\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.item()\n            loss.backward()\n            # NOTE: Scatter method was used in DistributedDataParallel\n            if not (size == 1):\n                # For multi-process use case\n                # average_gradients(model)\n                multigpu_average_gradients(model)\n            optimizer.step()\n        print(\'Process \', dist.get_rank(),\n              \', epoch \', epoch + 1, \'. avg_loss: \',\n              epoch_loss / len(train_set))\n\n    # Test model\n    if dist.get_rank() == 0:\n        model.eval()\n        test_loss = 0.0\n        correct = 0\n        with torch.no_grad():\n            for data, target in test_set:\n                # For GPU use\n                if is_gpu:\n                    data, target = Variable(data).cuda(), Variable(target).cuda()\n                else:\n                    data, target = Variable(data), Variable(target)\n                output = model(data)\n                test_loss += F.nll_loss(output, target, reduction=""sum"").item()\n                pred = output.data.max(1, keepdim=True)[1]\n                correct += pred.eq(target.data.view_as(pred)).sum().item()\n            print(\'Test_set:  avg_loss: \', test_loss / len(test_set.dataset),\n                  \', accuracy: \', 100. * correct / len(test_set.dataset), \'%\')\n\n        # Save model\n        torch.save(model.state_dict(), result_dir)\n        # NOTE: ONNX doesn\'t support scatter operation yet.\n        # dummy_input = """"\n        # if is_gpu:\n        #     dummy_input = Variable(torch.randn(1, 1, 28, 28)).cuda()\n        # else:\n        #     dummy_input = Variable(torch.randn(1, 1, 28, 28))\n        # model_path = os.environ.get(""RESULT_DIR"") + ""/pytorch-dist.onnx""\n        # torch.onnx.export(model, dummy_input, model_path)\n\n\n# Change \'backend\' to appropriate backend identifier\ndef init_processes(rank, size, fn, path_to_file, batch_size, is_gpu, backend):\n    """""" Initialize the distributed environment. """"""\n    print(""Process "", rank, "" connected"")\n    dist.init_process_group(backend,\n                            init_method=path_to_file,\n                            world_size=size, group_name=""train_dist"",\n                            rank=rank)\n    print(""FOUND SHARED FILE"")\n    fn(rank, size, batch_size, is_gpu, True)\n\ndef local_process(target, args):\n    return Process(target=target, args=args)\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--batch_size\', type=int, default=1024, help=\'Specify the batch size to be used in training\')\n    args = parser.parse_args()\n\n    # Default batch size is set to 1024. When using a large numbers of learners,\n    # a larger batch size is sometimes necessary to see speed improvements.\n    batch_size = args.batch_size\n\n    start_time = time.time()\n    num_gpus = int(float(os.environ.get(""GPU_COUNT"")))\n    world_size = int(os.environ.get(""NUM_LEARNERS""))\n    # For multi-process use case\n    # if num_gpus == 0:\n    #     world_size = int(os.environ.get(""NUM_LEARNERS""))\n    # else:\n    #     world_size = num_gpus * int(os.environ.get(""NUM_LEARNERS""))\n    data_dir = ""file:///job/"" + os.environ.get(""TRAINING_ID"")\n    processes = []\n    print(""data_dir is "" + data_dir)\n\n    if world_size == 1:\n        run(0, 1, batch_size, (num_gpus == 1), False)\n        print(""COMPLETION TIME: "", time.time() - start_time)\n    else:\n        if num_gpus == 0:\n            p = local_process(init_processes, (int(os.environ.get(""LEARNER_ID"")) - 1, world_size, run, data_dir, batch_size, False, \'gloo\'))\n            p.start()\n            processes.append(p)\n        else:\n            # For multi-process use case\n            # for process_num in range(0, num_gpus):\n            #     p = local_process(init_processes, (process_num * int(os.environ.get(""NUM_LEARNERS"")) + int(os.environ.get(""LEARNER_ID"")) - 1, world_size, run, data_dir, batch_size, True, \'nccl\'))\n            #     p.start()\n            #     processes.append(p)\n            #\n            p = local_process(init_processes, (int(os.environ.get(""LEARNER_ID"")) - 1, world_size, run, data_dir, batch_size, True, \'nccl\'))\n            p.start()\n            processes.append(p)\n\n        for p in processes:\n            p.join()\n\n        print(""COMPLETION TIME: "", time.time() - start_time)\n\n        # FfDL assume only the master learner job will terminate and store all\n        # the logging file.\n        # if int(os.environ.get(""LEARNER_ID"")) != 1:\n        #     while True:\n        #         time.sleep(1000000)\n'"
etc/examples/c10d-onnx-mpi/model-files/train_dist_onnx_mpi.py,16,"b'import time\nimport argparse\nimport sys\nimport os\nimport threading\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.onnx\n\nfrom math import ceil\nfrom random import Random\nfrom torch.multiprocessing import Process\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\n\n\nclass Net(nn.Module):\n    """""" Network architecture. """"""\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef get_dataset():\n    """""" Get FashionMNIST dataset """"""\n    data_path = os.environ.get(""DATA_DIR"") + \'/data\'\n    trainset = datasets.FashionMNIST(\n        data_path,\n        train=True,\n        download=False,\n        transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307, ), (0.3081, ))\n        ]))\n    testset = datasets.FashionMNIST(\n        data_path,\n        train=False,\n        download=False,\n        transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307, ), (0.3081, ))\n        ]))\n    return trainset, testset\n\n\ndef average_gradients(model):\n    """""" Gradient averaging. """"""\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM, group=dist.group.WORLD)\n        param.grad.data /= size\n\n\ndef run(rank, size, batch_size, is_gpu):\n    """""" Distributed Synchronous SGD Example """"""\n    torch.manual_seed(1234)\n    train_set, test_set = get_dataset()\n    result_dir = resultdir + \'/saved_model\'\n    # For GPU use\n    if is_gpu:\n        model = Net().cuda()\n    else:\n        model = Net()\n        model = model\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n    if not (size == 1):\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_set,\n            num_replicas=dist.get_world_size(), rank=dist.get_rank())\n    train_set = torch.utils.data.DataLoader(\n        train_set, batch_size=batch_size, shuffle=(train_sampler is None), sampler=train_sampler,\n        pin_memory=True)\n    test_set = torch.utils.data.DataLoader(\n        test_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n\n    num_batches = ceil(len(train_set.dataset) / float(batch_size))\n    # To train model\n    model.train()\n    for epoch in range(100):\n        epoch_loss = 0.0\n        for data, target in train_set:\n            # For GPU use\n            if is_gpu:\n                data, target = data.cuda(), target.cuda()\n            else:\n                data, target = Variable(data), Variable(target)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.item()\n            loss.backward()\n            if not (size == 1):\n                average_gradients(model)\n            optimizer.step()\n        print(\'Process \', dist.get_rank(),\n              \', epoch \', epoch, \'. avg_loss: \',\n              epoch_loss / len(train_set))\n\n    # Test model\n    if dist.get_rank() == 0:\n        model.eval()\n        test_loss = 0.0\n        correct = 0\n        with torch.no_grad():\n            for data, target in test_set:\n                # For GPU use\n                if is_gpu:\n                    data, target = data.cuda(), target.cuda()\n                else:\n                    data, target = Variable(data), Variable(target)\n                output = model(data)\n                test_loss += F.nll_loss(output, target, reduction=""sum"").item()\n                pred = output.data.max(1, keepdim=True)[1]\n                correct += pred.eq(target.data.view_as(pred)).sum().item()\n            print(\'Test_set:  avg_loss: \', test_loss / len(test_set.dataset),\n                  \', accuracy: \', 100. * correct / len(test_set.dataset), \'%\')\n\n    # Save model\n    if dist.get_rank() == 0:\n        torch.save(model, result_dir)\n        dummy_input = """"\n        if is_gpu:\n            dummy_input = Variable(torch.randn(1, 1, 28, 28)).cuda()\n        else:\n            dummy_input = Variable(torch.randn(1, 1, 28, 28))\n        model_path = os.environ.get(""RESULT_DIR"") + ""/pytorch-dist.onnx""\n        torch.onnx.export(model, dummy_input, model_path)\n\n\n# Change \'backend\' to appropriate backend identifier\ndef init_processes(rank, size, fn, path_to_file, batch_size, is_gpu, backend):\n    """""" Initialize the distributed environment. """"""\n    print(""Process connected"")\n    dist.init_process_group(backend,\n                            init_method=path_to_file,\n                            world_size=size, group_name=""train_dist"",\n                            rank=rank)\n    print(""FOUND SHARED FILE"")\n    fn(rank, size, batch_size, is_gpu)\n\ndef local_process(target, args):\n    return Process(target=target, args=args)\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--batch_size\', type=int, default=1024, help=\'Specify the batch size to be used in training\')\n    parser.add_argument(\'--cuda\', type=str, default=""True"", help=\'Enables CUDA training\')\n    parser.add_argument(\'--nodes\', type=int, default=2, help=\'Number or nodes\')\n    parser.add_argument(\'--data_dir\', type=str, help=\'Dataset directory path\')\n    parser.add_argument(\'--result_dir\', type=str, help=\'Result directory path\')\n    parser.add_argument(\'--rank_id\', type=int, default=0, help=\'Rank of the current process\')\n    args = parser.parse_args()\n\n    # Default batch size is set to 1024. When using a large numbers of learners,\n    # a larger batch size is sometimes necessary to see speed improvements.\n    batch_size = args.batch_size\n\n    start_time = time.time()\n    data_dir = ""file:///job/""\n    # processes = []\n    datadir = args.data_dir\n    resultdir = args.result_dir\n\n    if args.nodes == 1:\n        run(0, args.nodes, batch_size, (args.cuda == ""True""), False)\n        print(""COMPLETION TIME: "", time.time() - start_time)\n    else:\n        if args.cuda == ""True"":\n            init_processes(0, args.nodes, run, data_dir, batch_size, True, \'mpi\')\n        else:\n            init_processes(0, args.nodes, run, data_dir, batch_size, False, \'mpi\')\n\n        print(""COMPLETION TIME: "", time.time() - start_time)\n        # FfDL assume only the master learner job will terminate and store all\n        # the logging file.\n'"
etc/examples/pytorch-dist-onnx/model-files/train_dist_onnx.py,16,"b'import time\nimport argparse\nimport sys\nimport os\nimport threading\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.onnx\n\nfrom math import ceil\nfrom random import Random\nfrom torch.multiprocessing import Process\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\n\n\nclass Partition(object):\n    """""" Dataset-like object, but only access a subset of it. """"""\n\n    def __init__(self, data, index):\n        self.data = data\n        self.index = index\n\n    def __len__(self):\n        return len(self.index)\n\n    def __getitem__(self, index):\n        data_idx = self.index[index]\n        return self.data[data_idx]\n\n\nclass DataPartitioner(object):\n    """""" Partitions a dataset into different chuncks. """"""\n\n    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n        self.data = data\n        self.partitions = []\n        rng = Random()\n        rng.seed(seed)\n        data_len = len(data)\n        indexes = [x for x in range(0, data_len)]\n        rng.shuffle(indexes)\n\n        for frac in sizes:\n            part_len = int(frac * data_len)\n            self.partitions.append(indexes[0:part_len])\n            indexes = indexes[part_len:]\n\n    def use(self, partition):\n        return Partition(self.data, self.partitions[partition])\n\n\nclass Net(nn.Module):\n    """""" Network architecture. """"""\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef partition_dataset(batch_size, is_distributed):\n    """""" Partitioning MNIST """"""\n    data_path = os.environ.get(""DATA_DIR"") + \'/data\'\n    dataset = datasets.FashionMNIST(\n        data_path,\n        train=True,\n        download=False,\n        transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307, ), (0.3081, ))\n        ]))\n    testset = datasets.FashionMNIST(\n        data_path,\n        train=False,\n        download=False,\n        transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307, ), (0.3081, ))\n        ]))\n    if is_distributed:\n        size = dist.get_world_size()\n    else:\n        size = 1\n    bsz = int(batch_size / float(size))\n    partition_sizes = [1.0 / size for _ in range(size)]\n    partition = DataPartitioner(dataset, partition_sizes)\n    partition_testset = DataPartitioner(testset, partition_sizes)\n    if is_distributed:\n        partition = partition.use(dist.get_rank())\n        partition_testset = partition_testset.use(dist.get_rank())\n    else:\n        partition = partition.use(0)\n        partition_testset = partition_testset.use(0)\n    train_set = torch.utils.data.DataLoader(\n        partition, batch_size=bsz, shuffle=True)\n    test_set = torch.utils.data.DataLoader(\n        testset, batch_size=batch_size, shuffle=True)\n    return train_set, test_set, bsz\n\n\ndef average_gradients(model):\n    """""" Gradient averaging. """"""\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM, group=0)\n        param.grad.data /= size\n\n\ndef run(rank, size, batch_size, is_gpu):\n    """""" Distributed Synchronous SGD Example """"""\n    torch.manual_seed(1234)\n    train_set, test_set, bsz = partition_dataset(batch_size, (not (size == 1)))\n    result_dir = os.environ.get(""RESULT_DIR"") + \'/saved_model\'\n    # For GPU use\n    if is_gpu:\n        device = torch.device(""cuda:{}"".format(rank))\n        model = Net().to(device)\n    else:\n        model = Net()\n        model = model\n#    model = model.cuda(rank)\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n    num_batches = ceil(len(train_set.dataset) / float(bsz))\n    # To train model\n    model.train()\n    for epoch in range(100):\n        epoch_loss = 0.0\n        for data, target in train_set:\n            # For GPU use\n            if is_gpu:\n                data, target = data.to(device), target.to(device)\n            else:\n                data, target = Variable(data), Variable(target)\n#            data, target = Variable(data.cuda(rank)), Variable(target.cuda(rank))\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.item()\n            loss.backward()\n            if not (size == 1):\n                average_gradients(model)\n            optimizer.step()\n        print(\'Process \', os.environ.get(""LEARNER_ID""),\n              \', epoch \', epoch, \'. avg_loss: \',\n              epoch_loss / len(train_set))\n\n    # Test model\n    if int(os.environ.get(""LEARNER_ID"")) == 1:\n        model.eval()\n        test_loss = 0.0\n        correct = 0\n        with torch.no_grad():\n            for data, target in test_set:\n                # For GPU use\n                if is_gpu:\n                    data, target = data.to(device), target.to(device)\n                else:\n                    data, target = Variable(data), Variable(target)\n                output = model(data)\n                test_loss += F.nll_loss(output, target, size_average=False).item()\n                pred = output.data.max(1, keepdim=True)[1]\n                correct += pred.eq(target.data.view_as(pred)).sum().item()\n            print(\'Test_set:  avg_loss: \', test_loss / len(test_set.dataset),\n                  \', accuracy: \', 100. * correct / len(test_set.dataset), \'%\')\n\n    # Save model\n    if int(os.environ.get(""LEARNER_ID"")) == 1:\n        torch.save(model, result_dir)\n        dummy_input = """"\n        if is_gpu:\n            dummy_input = Variable(torch.randn(1, 1, 28, 28)).cuda()\n        else:\n            dummy_input = Variable(torch.randn(1, 1, 28, 28))\n        model_path = os.environ.get(""RESULT_DIR"") + ""/pytorch-dist.onnx""\n        torch.onnx.export(model, dummy_input, model_path)\n\n\n# Change \'backend\' to appropriate backend identifier\ndef init_processes(rank, size, fn, path_to_file, batch_size, is_gpu, backend):\n    """""" Initialize the distributed environment. """"""\n    print(""Process "" + os.environ.get(""LEARNER_ID"") + "" connected"")\n    dist.init_process_group(backend,\n                            init_method=path_to_file,\n                            world_size=size, group_name=""train_dist"")\n    print(""FOUND SHARED FILE"")\n    fn(rank, size, batch_size, is_gpu)\n\ndef local_process(target, args):\n    return Process(target=target, args=args)\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--batch_size\', help=\'Specify the batch size to be used in training\')\n    args = parser.parse_args()\n\n    batch_size = args.batch_size\n    # Default batch size is set to 1024. When using a large numbers of learners,\n    # a larger batch size is sometimes necessary to see speed improvements.\n    if batch_size is None:\n        batch_size = 1024\n    else:\n        batch_size = int(batch_size)\n\n    start_time = time.time()\n    num_gpus = int(float(os.environ.get(""GPU_COUNT"")))\n    if num_gpus == 0:\n        world_size = int(os.environ.get(""NUM_LEARNERS""))\n    else:\n        world_size = num_gpus * int(os.environ.get(""NUM_LEARNERS""))\n    data_dir = ""file:///job/"" + os.environ.get(""TRAINING_ID"")\n    processes = []\n    print(""data_dir is "" + data_dir)\n\n    if world_size == 1:\n        run(0, 1, batch_size, (num_gpus == 1))\n        print(""COMPLETION TIME: "", time.time() - start_time)\n    else:\n        if num_gpus == 0:\n            p = local_process(init_processes, (0 , world_size, run, data_dir, batch_size, False, \'gloo\'))\n            p.start()\n            processes.append(p)\n\n        for process_num in range(0, num_gpus):\n            p = local_process(init_processes, (process_num, world_size, run, data_dir, batch_size, True, \'gloo\'))\n            p.start()\n            processes.append(p)\n\n        for p in processes:\n            p.join()\n\n        print(""COMPLETION TIME: "", time.time() - start_time)\n\n        # FfDL assume only the master learner job will terminate and store all\n        # the logging file.\n        if int(os.environ.get(""LEARNER_ID"")) != 1:\n            while True:\n                time.sleep(1000000)\n'"
metrics/log_collectors/emetrics_file/src/__init__.py,0,b'from __future__ import absolute_import\n\n'
metrics/log_collectors/emetrics_file/src/tail_em_from_emfile.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport argparse\nimport logging\n\nfrom log_collectors.training_data_service_client import scan_log_dirs\nfrom log_collectors.training_data_service_client import match_log_file\nfrom log_collectors.training_data_service_client import push_log_line\nfrom log_collectors.training_data_service_client import push_em_line\n\n\ndef main():\n    logging.basicConfig(format=\'%(filename)s %(funcName)s %(lineno)d: %(message)s\', level=logging.INFO)\n\n    log_directory = os.environ[""LOG_DIR""]\n    # log_file = log_directory + ""/latest-log""\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--log_dir\', type=str, default=log_directory,\n                        help=\'Log directory\')\n\n    parser.add_argument(\'--should_connect\', type=bool, default=True,\n                        help=\'If true send data to gRPC endpoint\')\n\n    parser.add_argument(\'--send\', dest=\'send\', action=\'store_true\')\n    parser.add_argument(\'--no-send\', dest=\'send\', action=\'store_false\')\n    parser.set_defaults(send=True)\n\n    FLAGS, _ = parser.parse_known_args()\n\n    logging.info(""Should connect: ""+str(FLAGS.should_connect))\n\n    scan_log_dirs.LogScanner(should_connect=True).scan(\n        log_dir=FLAGS.log_dir,\n        is_log=match_log_file.is_log_file,\n        push_function=push_log_line.push,\n        is_emetrics=match_log_file.is_emetrics_file,\n        push_emetrics_function=push_em_line.push)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
metrics/log_collectors/emetrics_file/src/tail_to_tds.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport argparse\n\nfrom log_collectors.training_data_service_client import tail_to_tds as tail\n\n\ndef main():\n    job_directory = os.environ[""JOB_STATE_DIR""]\n    log_directory = job_directory + ""/logs""\n    log_file = job_directory + ""/latest-log""\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--log_file\', type=str, default=log_file,\n                        help=\'Log file\')\n\n    FLAGS, unparsed = parser.parse_known_args()\n\n    tail.collect_and_send(FLAGS.log_file, True)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
metrics/log_collectors/regex_extractor/src/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .extract_from_log import extract\n'
metrics/log_collectors/regex_extractor/src/extract_from_log.py,0,"b'#!/usr/bin/env python\n""""""Default incremental log parser for Caffe deep learning jobs.""""""\n\n#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport yaml\nimport os\nimport re\nimport datetime\nimport argparse\nimport sys\nimport logging\n\nfrom typing import List, Dict, Any, Tuple\n\nfrom log_collectors.training_data_service_client import training_data_pb2 as tdp\nfrom log_collectors.training_data_service_client import training_data_buffered as tdb\n\nfrom log_collectors.training_data_service_client import extract_datetime as edt\n\nfrom log_collectors.training_data_service_client import push_log_line as push_log_line\nfrom log_collectors.training_data_service_client import match_log_file\nfrom log_collectors.training_data_service_client import scan_log_dirs\n\n\ndef read_symbol_libs(dir_path: str) -> Dict[str, str]:\n    patterns_dir = os.path.join(dir_path, ""patterns"")\n\n    symbol_dict: dict = dict()\n    pattern_lib_regex = r""(?P<name>\\w+)\\s(?P<pattern>.*)""\n    for subdir, dirs, files in os.walk(patterns_dir):\n        for file in files:\n            pattern_file_path: str = os.path.join(subdir, file)\n            with open(pattern_file_path, \'rt\') as f:\n                for line in iter(f):\n                    line = line.strip()\n                    if line.startswith(""#""):\n                        continue\n                    if line == """":\n                        continue\n                    match = re.match(pattern_lib_regex, line)\n                    if match is not None:\n                        match_dict = match.groupdict()\n                        symbol_dict[match_dict.get(""name"")] = match_dict.get(""pattern"")\n                        # print(""found: ""+match_dict.get(""name"")+"": ""+patterns_dict.get(match_dict.get(""name"")))\n    return symbol_dict\n\n\ndef replace_lib_symbols(symbol_dict: dict, regex_val: str) -> str:\n    """"""Recursively substitute library symbols\n\n    Return string that should have all it\'s symbols replaced.""""""\n\n    pattern_lib_ref_regex = r""%{(?P<type>\\w+)}""\n    pattern_lib_ref_specific_form = ""(?P<type>%s)""\n    subst_form = ""%s""\n\n    rebuilt_regex = regex_val\n    matches = re.finditer(pattern_lib_ref_regex, rebuilt_regex)\n\n    for _, match in enumerate(matches):\n        match_pairs = match.groupdict()\n        symbol_name = match_pairs[""type""]\n\n        lib_regex = symbol_dict[symbol_name]\n        pattern_lib_ref_specific = pattern_lib_ref_specific_form % symbol_name\n        pattern_lib_ref_specific = ""%{"" + pattern_lib_ref_specific + ""}""\n        subst = subst_form % lib_regex\n        rebuilt_regex = re.sub(pattern_lib_ref_specific, subst, rebuilt_regex, 1)\n\n        rebuilt_regex = replace_lib_symbols(symbol_dict, rebuilt_regex)\n\n    return rebuilt_regex\n\n\ndef read_extract_description(manifest: str, symbol_dict: dict) -> dict:\n    pattern_lib_ref_regex = r""%{(?P<type>\\w+):(?P<match_name>\\w+)}""\n    pattern_lib_ref_specific_form = ""(?P<type>%s):(?P<match_name>\\w+)""\n    subst_form = ""(?P<%s>%s)""\n\n    evaluation_metrics: dict = None\n    with open(manifest, \'r\') as stream:\n        try:\n            manifest_data: dict = yaml.load(stream)\n            evaluation_metrics = manifest_data[""evaluation_metrics""]\n\n            groups: dict = evaluation_metrics[""groups""]\n            for group_key in groups:\n                group = groups[group_key]\n                regex_val: str = group.get(""regex"")\n                rebuilt_regex = regex_val\n                matches = re.finditer(pattern_lib_ref_regex, rebuilt_regex)\n                for _, match in enumerate(matches):\n                    match_pairs = match.groupdict()\n                    symbol_name = match_pairs[""type""]\n                    match_name = match_pairs[""match_name""]\n\n                    lib_regex = symbol_dict[symbol_name]\n                    pattern_lib_ref_specific = pattern_lib_ref_specific_form % match_pairs[""type""]\n                    pattern_lib_ref_specific = ""%{"" + pattern_lib_ref_specific + ""}""\n                    subst = subst_form % (match_name, lib_regex)\n                    rebuilt_regex = re.sub(pattern_lib_ref_specific, subst, rebuilt_regex, 1)\n\n                rebuilt_regex = replace_lib_symbols(symbol_dict, rebuilt_regex)\n                group[""regex_expanded""] = re.compile(rebuilt_regex, re.MULTILINE | re.DOTALL)\n\n        except yaml.YAMLError as exc:\n            print(exc)\n\n    return evaluation_metrics\n\n\ndef type_string_to_grpc_type(value_type: str) -> str:\n    grpc_value_type = """"\n    if value_type == ""INT"":\n        grpc_value_type = tdp.Any.INT\n    elif value_type == ""FLOAT"":\n        grpc_value_type = tdp.Any.FLOAT\n    elif value_type == ""STRING"":\n        grpc_value_type = tdp.Any.STRING\n    elif value_type == ""JSONSTRING"":\n        grpc_value_type = tdp.Any.INT\n\n    return grpc_value_type\n\n\nclass ExtraPushData4RegExExtractor:\n    def __init__(self, line_lookahead: int, groups: dict):\n        self.line_lookahead = line_lookahead\n        self.groups = groups\n\n        self.line_length_stack: List[int] = []\n        self.text_window = """"\n        self.start_time: datetime = None\n        self.did_get_good_time: bool = False\n\n\ndef extract_and_push_emetrics(td_client: tdb.TrainingDataClientBuffered, log_file: str,\n                              log_line: str, logfile_year: str,\n                              line_index: int, record_index: int,\n                              subdir: str, extra_any: Any=None) -> Tuple[int, int]:\n\n    state: ExtraPushData4RegExExtractor = extra_any\n    # # Do our best to get a good start time.\n    # if not state.did_get_good_time:\n    #     # keep trying to get a good start time from the log line, until it\'s pointless\n    #     start_time, did_get_good_time = \\\n    #         edt.extract_datetime(log_line, logfile_year, state.start_time)\n\n    if td_client.em_file_path is None:\n        em_file_path = os.path.join(os.path.dirname(log_file), match_log_file.EMETRICS_FILE_BASE_NAME)\n        td_client.set_em_file_path(em_file_path)\n        logging.debug(""em_file_path: %s, subdir: %s"", td_client.em_file_path, subdir)\n\n    # logging.debug(""push_log_line.push %d (subdir: %s) %s"", line_index, subdir, log_line)\n    line_index, _ = push_log_line.push(td_client, log_file, log_line, logfile_year,\n                                       line_index, 0, subdir, state)\n\n    state.line_length_stack.append(len(log_line))\n    state.text_window += log_line\n    if len(state.line_length_stack) > state.line_lookahead:\n        length_first_line = state.line_length_stack[0]\n        state.line_length_stack = state.line_length_stack[1:]\n        state.text_window = state.text_window[length_first_line:]\n\n    logging.debug(""len groups: %d"", len(state.groups))\n    for group_key in state.groups:\n        group = state.groups[group_key]\n        name = group_key\n        regex_expanded = group[""regex_expanded""]\n        matches = regex_expanded.match(state.text_window)\n        if matches is not None:\n            values_dict = matches.groupdict()\n\n            # meta_dict_desc = group[""meta""]\n            etimes_descriptions: dict = group[""etimes""]\n            if etimes_descriptions is None:\n                logging.warning(""Did not find etimes! Found: "")\n                for axis_key in group:\n                    print(""key: ""+axis_key)\n                    sys.stdout.flush()\n                break\n\n            etimes: dict = dict()\n            for etime_key in etimes_descriptions:\n                item = etimes_descriptions[etime_key]\n                valOrRef: str = item[""value""]\n                if valOrRef.startswith(""$""):\n                    value_inner = valOrRef[1:]\n                    value_actual = values_dict[value_inner]\n                else:\n                    value_actual = valOrRef\n                grpc_value_type = type_string_to_grpc_type(item[""type""])\n                etimes[etime_key] = tdp.Any(type=grpc_value_type, value=value_actual)\n\n            if ""scalars"" in group:\n                scalars_descriptions: dict = group[""scalars""]\n            elif ""values"" in group:\n                scalars_descriptions: dict = group[""values""]\n            else:\n                scalars_descriptions = None\n\n            if scalars_descriptions is None:\n                logging.warning(""Did not find scalars! Found: "")\n                for axis_key in group:\n                    print(""key: ""+axis_key)\n                    sys.stdout.flush()\n                break\n\n            scalars: dict = dict()\n            for scalar_key in scalars_descriptions:\n                item = scalars_descriptions[scalar_key]\n                valOrRef: str = item[""value""]\n                if valOrRef.startswith(""$""):\n                    value_inner = valOrRef[1:]\n                    value_actual = values_dict[value_inner]\n                else:\n                    value_actual = valOrRef\n                value_type = item[""type""]\n                grpc_value_type = type_string_to_grpc_type(value_type)\n                scalars[scalar_key] = tdp.Any(type=grpc_value_type, value=value_actual)\n\n            # date_string: str = line\n            subid_value = subdir\n            if ""meta"" in group:\n                meta_list: dict = group[""meta""]\n                # if ""time"" in meta_list:\n                #     valOrRef: str = meta_list[""time""]\n                #     if valOrRef.startswith(""$""):\n                #         value_ref = valOrRef[1:]\n                #         # date_string = values_dict[value_ref]\n                #     else:\n                #         # date_string = valOrRef\n                #         pass\n                if ""subid"" in meta_list:\n                    valOrRef: str = meta_list[""subid""]\n                    if valOrRef.startswith(""$""):\n                        logging.debug(""resetting subdir(subid): %s, metalist: %r"",\n                                      subid_value, meta_list)\n                        value_ref = valOrRef[1:]\n                        subid_value = values_dict[value_ref]\n                    elif not valOrRef == """":\n                        logging.debug(""resetting subdir(subid): %s, metalist: %r"",\n                                      subid_value, meta_list)\n                        subid_value = valOrRef\n\n            logging.debug(""about to push evaluation metrics with subdir(subid): %s"", subid_value)\n\n            # At this point, don\'t keep trying to get a start time if we haven\'t already\n            state.did_get_good_time = True\n            emetrics = tdp.EMetrics(\n                meta=tdp.MetaInfo(\n                    training_id=os.environ[""TRAINING_ID""],\n                    time=edt.get_meta_timestamp(),\n                    rindex=record_index,\n                    subid=subid_value\n                ),\n                grouplabel=name,\n                etimes=etimes,\n                values=scalars\n            )\n            record_index += 1\n            # state.total_lines_pushed += 1\n\n            if td_client is not None:\n                td_client.AddEMetrics(emetrics)\n\n            # for now, print to stdout (support old endpoint).\n            # print(json_form)\n\n            state.text_window = """"\n            state.line_length_stack = []\n            break\n\n    return line_index, record_index\n\n\ndef extract(log_dir: str, manifest: str, should_connect: bool=True):\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    symbol_dict: Dict[str, str] = read_symbol_libs(dir_path)\n\n    evaluation_metrics_spec = read_extract_description(manifest, symbol_dict)\n\n    # Not sure why I seem to loose the under-bar somewhere along the line.\n    if ""line_lookahead"" in evaluation_metrics_spec:\n        line_lookahead: int = int(evaluation_metrics_spec[""line_lookahead""])\n    elif ""linelookahead"" in evaluation_metrics_spec:\n        line_lookahead: int = int(evaluation_metrics_spec[""linelookahead""])\n    else:\n        line_lookahead: int = 4\n\n    groups: dict = evaluation_metrics_spec[""groups""]\n\n    logging.debug(""log dir: %s"" % log_dir)\n\n    extraData = ExtraPushData4RegExExtractor(line_lookahead, groups)\n\n    scan_log_dirs.LogScanner(should_connect=should_connect).scan(\n        log_dir=log_dir,\n        extra=extraData,\n        is_log=match_log_file.is_log_file,\n        push_function=extract_and_push_emetrics)\n\n\ndef main():\n    logging.basicConfig(format=\'%(filename)s %(funcName)s %(lineno)d: %(message)s\', level=logging.INFO)\n    log_directory = os.environ[""LOG_DIR""]\n    # log_file = log_directory + ""/latest-log""\n    # em_file = log_directory + ""/evaluation-metrics.txt""\n\n    manifest = os.environ[""EM_DESCRIPTION""]\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--manifest\', type=str, default=manifest,\n                        help=\'DLaaS log directory\')\n\n    parser.add_argument(\'--log_dir\', type=str, default=log_directory,\n                        help=\'Log directory\')\n\n    FLAGS, _ = parser.parse_known_args()\n\n    extract(FLAGS.log_dir, FLAGS.manifest, True)\n\n    logging.info(""Normal exit"")\n\n\nif __name__ == \'__main__\':\n    main()\n'"
metrics/log_collectors/simple_log_collector/src/__init__.py,0,b'from __future__ import absolute_import\n\n'
metrics/log_collectors/simple_log_collector/src/tail_to_tds.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport argparse\nimport logging\n\nfrom log_collectors.training_data_service_client import match_log_file\nfrom log_collectors.training_data_service_client import push_log_line\nfrom log_collectors.training_data_service_client import scan_log_dirs\n\n\ndef main():\n    logging.basicConfig(format=\'%(filename)s %(funcName)s %(lineno)d: %(message)s\', level=logging.INFO)\n    log_directory = os.environ[""LOG_DIR""]\n    # log_file = log_directory + ""/latest-log""\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--log_dir\', type=str, default=log_directory,\n                        help=\'Log directory\')\n\n    FLAGS, unparsed = parser.parse_known_args()\n\n    scan_log_dirs.LogScanner(should_connect=True).scan(\n        log_dir=FLAGS.log_dir,\n        is_log=match_log_file.is_log_file,\n        push_function=push_log_line.push)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
metrics/log_collectors/tensorboard/src/__init__.py,0,b''
metrics/log_collectors/tensorboard/src/extract_tb.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport time\nimport threading\nfrom typing import List\n\nimport argparse\nimport sys\nimport os\nimport datetime\nimport traceback\n\nfrom log_collectors.training_data_service_client import scan_log_dirs\nfrom log_collectors.training_data_service_client import push_log_line\n\nfrom log_collectors.training_data_service_client import training_data_pb2 as tdp\nfrom log_collectors.training_data_service_client import training_data_buffered as tdb\n\nfrom log_collectors.training_data_service_client import states\n\nfrom tensorboard.backend.event_processing import event_accumulator\nfrom tensorboard.backend.event_processing import event_multiplexer\n\nfrom log_collectors.training_data_service_client import extract_datetime\n\nfrom log_collectors.training_data_service_client import match_log_file\n\netb_logger = None   # type: logging.Logger\n\n\nclass Tracker:\n    """"""Representing a group run, such as \'test\' or \'train\'""""""\n\n    def __init__(self,\n                 summary_dir: str,\n                 log_dir: str,\n                 sub_identifier: str,\n                 group: str,\n                 td_client: tdb.TrainingDataClientBuffered):\n        """"""Return tracker object (meant to be opaque) to be passed to emitEvalMetricViaTracker(...)""""""\n\n        # Create a tensorboard event accumulator.\n        self.summary_dir = summary_dir\n        etb_logger.info(""Creating ea for %s"", self.summary_dir)\n        self.ea = event_accumulator.EventAccumulator(self.summary_dir)\n\n        # ea = event_accumulator.EventAccumulator(summary_dir,\n        #     size_guidance={\n        #                    event_accumulator.COMPRESSED_HISTOGRAMS: 1,\n        #                    event_accumulator.IMAGES: 1,\n        #                    event_accumulator.AUDIO: 1,\n        #                    event_accumulator.SCALARS: 1000,\n        #                    event_accumulator.HISTOGRAMS: 1,\n        #                    })\n\n        self.log_dir = log_dir\n        self.td_client = td_client    # type: tdb.TrainingDataClientBuffered\n        self.group = group\n        self.sub_identifier = sub_identifier\n        self.step_to_event_list_dict = dict()\n        self.step_added = -1\n        self.wall_time = -1.0\n\n        etb_logger.info(""Done initializing tracker for log_dir: %s, summary_dir: %s, em_file_path: %s"",\n                        self.log_dir, self.summary_dir, self.td_client.em_file_path)\n\n    def is_queued_for_report(self)->bool:\n        return self.step_added != -1\n\n    def queued_wall_time(self)->float:\n        return self.wall_time\n\n    # Emit evaluation metrics from a tracker object, for visibility to DLaaS clients.\n    def track(self)->bool:\n\n        if self.is_queued_for_report():\n            return True\n\n        self.ea.Reload()  # loads events from file\n\n        tags = self.ea.Tags()\n        scaler_keys = tags[event_accumulator.SCALARS]\n\n        # Have to do essentially a table rotation. The event accumulator API seems\n        # only to allow retrieval per scaler, where we need all the scaler values\n        # per step.\n        # last_processed = self.last_step_processed\n        for scaler_key in scaler_keys:\n\n            scalerEvents = self.ea.Scalars(scaler_key)\n\n            did_add = False\n            for event in scalerEvents:\n                if self.step_added != -1 and event.step != self.step_added:\n                    # after the first step found, only use scaler events that match this step\n                    continue\n\n                label_event_list = self.step_to_event_list_dict.get(event.step)\n                if label_event_list is None:\n                    eventSet = EventSet(scaler_key, event)\n                    # etb_logger.debug(""   event(%s): scaler_key, %r"", scaler_key, event)\n                    self.step_to_event_list_dict[event.step] = [eventSet]\n                    self.step_added = int(event.step)\n                    did_add = True\n                    # Just track first occurrence time\n                    self.wall_time = event.wall_time\n                    eventSet.wall_time = event.wall_time\n                elif not isInList(label_event_list, scaler_key, event.step, event.wall_time):\n                    eventSet = EventSet(scaler_key, event)\n                    # etb_logger.debug(""   event(%s): scaler_key, %r"", scaler_key, event)\n                    self.step_to_event_list_dict[event.step].append(eventSet)\n                    did_add = True\n\n                if did_add:\n                    # intention: break from this scaler investigation, go to the next scaler\n                    break\n\n        return self.is_queued_for_report()\n\n    def report(self, rindex: int)->int:\n        if self.is_queued_for_report():\n            values_dict = {}\n            event_list = self.step_to_event_list_dict[self.step_added]\n            rfcRFC3339Formatted = None\n            event_wall_time = 0.0\n\n            # Transmogrify the ScalerEvent list into a values dictionary\n            for event_set in event_list:\n                label = event_set.label\n                event = event_set.event  # type: event_accumulator.ScalarEvent\n                if event_wall_time == 0.0:\n                    event_wall_time = float(event.wall_time)\n                    if rfcRFC3339Formatted is None:\n                        dt = datetime.datetime.fromtimestamp(event.wall_time)\n                        rfcRFC3339Formatted = dt.isoformat(""T"") + ""Z""\n\n                values_dict[label] = tdp.Any(type=tdp.Any.FLOAT, value=str(event.value))\n\n            # etb_logger.debug(""Calling emitEvalMetric step=%r: rindex=%r"", self.step_added, rindex)\n            emitEvalMetric(td_client=self.td_client,\n                           group_label=self.group,\n                           iterStep=self.step_added,\n                           timestamp=rfcRFC3339Formatted,\n                           values_dict=values_dict,\n                           rindex=rindex,\n                           event_wall_time=event_wall_time,\n                           sub_identifier=self.sub_identifier)\n            rindex += 1\n            self.step_added = -1\n            self.wall_time = -1.0\n\n        return rindex\n\n\nclass Run_tracker(threading.Thread):\n    """"""Representing a directory that contains a Tensorboard summary directory""""""\n\n    # See EventMultiplexer, this class is very similar, I think?  Could try to utilize that class\n    # and do away with this class.  I think.\n\n    def __init__(self, td_client: tdb.TrainingDataClientBuffered, log_dir: str, sub_identifier: str,\n                 lines_counter: scan_log_dirs.LinesCounter):\n        super().__init__()\n        global etb_logger\n        self.log_dir = log_dir\n        self.sub_identifier = sub_identifier\n        self.event_trackers = []  # type: List[Tracker]\n        self.summary_dirs = []  # list of strings representing group names\n        self.td_client = td_client\n        self.lines_counter = lines_counter\n\n        self.em_file_path = os.path.join(log_dir, match_log_file.EMETRICS_FILE_BASE_NAME)\n        self.td_client.set_em_file_path(self.em_file_path)\n\n        etb_logger.info(""new Run_tracker for %s"", log_dir)\n\n        self.record_index = 1\n\n    @staticmethod\n    def sync_event_files_for_nfs_cache(dir_name: str):\n        """"""Make sure event files are synchronized per the nfs cache""""""\n\n        for file_name in os.listdir(dir_name):\n            full_path = os.path.join(dir_name, file_name)\n            if event_accumulator.IsTensorFlowEventsFile(full_path):\n                stat = os.stat(full_path)\n                before = stat.st_mtime\n                synced_modification_time = scan_log_dirs.stat_nfs_safe_modification_time(full_path)\n                if synced_modification_time != before:\n                    etb_logger.info(""sync file: %s, b: %f, a: %f - %s"",\n                                    full_path, before, synced_modification_time, ""CHANGED"")\n\n    def build_event_trackers(self):\n\n        # Check to see if new group sub directories have been added\n        for summary_dir in event_multiplexer.GetLogdirSubdirectories(self.log_dir + \'/\'):\n            # in the case of this the top level run tracker, this will screen out\n            # sub run directories.  Otherwise log_dir will be the sub run directory itself,\n            # and this should not fire.\n            if not is_sub_run_dir(summary_dir, self.log_dir):\n                Run_tracker.sync_event_files_for_nfs_cache(summary_dir)\n                group = os.path.basename(summary_dir)\n                if summary_dir not in self.summary_dirs:\n                    self.summary_dirs.append(summary_dir)\n                    tracker = Tracker(summary_dir, self.log_dir, self.sub_identifier, group,\n                                      self.td_client)\n                    self.event_trackers.append(tracker)\n\n    def track_and_report_one(self, report_only_if_queue_full=False)->int:\n        """"""Keep the event trackers, which point to different event files, queued so the event times\n        can be examined, and then report the earliest event.""""""\n        did_report = False\n        number_queued = 0\n        for tracker in self.event_trackers:\n            if tracker.track():\n                number_queued += 1\n\n        if report_only_if_queue_full is False or number_queued == len(self.event_trackers):\n            earliest_wall_time = sys.float_info.max\n            earliest_tracker = None  # type: Tracker\n            for i, tracker in enumerate(self.event_trackers):\n                if tracker.is_queued_for_report():\n                    if tracker.wall_time < earliest_wall_time:\n                        earliest_wall_time = tracker.wall_time\n                        earliest_tracker = tracker\n\n            if earliest_tracker is not None:\n                self.record_index = earliest_tracker.report(self.record_index)\n                self.lines_counter.increment()\n                did_report = True\n\n        return did_report\n\n    def run(self):\n        states.register_scanner()\n        shutdown_start_time = 0.0\n        time_since_last_learner_done_check = time.time()\n        try:\n            while True:\n                self.build_event_trackers()\n\n                count_main_loop_report = 0\n                while self.track_and_report_one(report_only_if_queue_full=True):\n                    # loop until track_and_report_one says it didn\'t report anything\n                    count_main_loop_report += 1\n                if count_main_loop_report > 0:\n                    etb_logger.info(""Main loop report, n records: %d"", count_main_loop_report)\n\n                if shutdown_start_time == 0.0:\n                    # compare the time elapsed since the last report\n                    elapsed_since_last_check = time.time() - time_since_last_learner_done_check\n                    if elapsed_since_last_check >= 1.0:  # Check every 2.0 seconds\n                        if states.is_learner_done(logger=etb_logger):\n                            etb_logger.info(""Learner done, begin log-collector shutdown: %s"", self.log_dir)\n                            shutdown_start_time = time.time()\n                        else:\n                            time_since_last_learner_done_check = time.time()\n                elif (time.time() - shutdown_start_time) > states.DURATION_SHUTDOWN_DELAY_TF:\n\n                    # drain the event queues\n                    if os.path.exists(self.log_dir):\n                        etb_logger.info(""final drain of trackers"")\n                        count_shut_down_reports = 0\n                        # Drain the tracker queue\n                        while self.track_and_report_one(report_only_if_queue_full=False):\n                            # loop until track_and_report_one says it didn\'t report anything\n                            count_shut_down_reports += 1\n                        etb_logger.info(""Drain flush: %d"", count_shut_down_reports)\n                    else:\n                        etb_logger.info(""No log_dir, abandoning trackers"")\n                        self.td_client.set_em_file_path(str(None))\n\n                    etb_logger.info(""Flushing buffer with force"")\n                    self.td_client.flush_maybe(force=True)\n\n                    etb_logger.info(""Having a little snooze for %f seconds"", states.SLEEP_BEFORE_LC_DONE)\n                    time.sleep(states.SLEEP_BEFORE_LC_DONE)\n\n                    break  # From main loop\n\n                self.td_client.flush_maybe()\n\n                # thread yield\n                time.sleep(0)\n        finally:\n            etb_logger.info(""signaling that log-collector is done"")\n            states.signal_lc_done(logger=etb_logger)\n            # I seem to have problems exiting directly, so, this sleep seems to help.\n            # My unsubstantiated theory is that gRPC needs time to flush.\n            # Note since we signaled, we won\'t actually wait n seconds, the\n            # job monitor will delete us.\n            time.sleep(states.SLEEP_BEFORE_EXIT_TIME)\n\n\ndef isInList(label_event_list, label, step, wall_time):\n    for eventSet in label_event_list:\n        if eventSet.label == label and \\\n                eventSet.event.step == step and \\\n                eventSet.event.wall_time == wall_time:\n            return True\n\n    return False\n\n\nclass EventSet:\n\n    def __init__(self, label: str, event: event_accumulator.ScalarEvent):\n        self.label = label\n        self.event = event\n        self.wall_time = -1.0\n\n    # def emitStatus(tracker, message):\n    #     dt = datetime.datetime.now()\n    #     rfcRFC3339Formatted = dt.isoformat(""T"") + ""Z""\n    #\n    #     labelValueList = []\n    #     labelValueList.append([""Message"", message])\n    #\n    #     emitEvalMetric(tracker.log_dir, ""Status"", 0, rfcRFC3339Formatted,\n    #                    labelValueList)\n\n\n# Emit evaluation metrics for visibility to DLaaS clients\ndef emitEvalMetric(td_client: tdb.TrainingDataClientBuffered,\n                   group_label: str,  # group label, likely test or train\n                   iterStep,\n                   timestamp,\n                   values_dict,\n                   rindex: int,\n                   event_wall_time: float,\n                   sub_identifier\n                   ):\n    """"""Push the processed metrics data to the metrics service""""""\n    try:\n        etimes = dict()\n\n        etimes[\'iteration\'] = tdp.Any(type=tdp.Any.INT, value=str(iterStep))\n        etimes[\'timestamp\'] = tdp.Any(type=tdp.Any.STRING, value=timestamp)\n        etimes[\'wall_time\'] = tdp.Any(type=tdp.Any.FLOAT, value=str(event_wall_time))\n\n        # # d = datetime.datetime.utcnow() # <-- get time in UTC\n        # #     print d.isoformat(""T"") + ""Z""\n        # #     if timestamp == None:\n        # #         timestamp = start_time + datetime.timedelta(seconds=rowdict[\'Seconds\'])\n        #\n        # dict_MetricData[\'timestamp\'] = timestamp\n        # time=int(event_wall_time * 1000),\n\n        etb_logger.debug(""Creating emetrics record"")\n        emetrics = tdp.EMetrics(\n            meta=tdp.MetaInfo(\n                training_id=os.environ[""TRAINING_ID""],\n                time=extract_datetime.get_meta_timestamp(),\n                rindex=int(rindex),\n                subid=sub_identifier\n            ),\n            grouplabel=group_label,\n            etimes=etimes,\n            values=values_dict\n        )\n\n        if td_client is not None:\n            etb_logger.debug(""Calling AddEMetrics"")\n            td_client.AddEMetrics(emetrics)\n\n    except Exception as inst:\n        etb_logger.error(""Unexpected error when attempting to send emetrics: %s"", sys.exc_info()[0])\n        print(""Unexpected error when attempting to send emetrics:"", sys.exc_info()[0])\n        print(type(inst))\n        print(inst.args)\n        traceback.print_exc()\n        print(inst)\n\n        sys.stdout.flush()\n\n    return rindex+1\n\n\ndef subdir_below_logdir(summary_dir: str, log_dir)->str:\n    rel_to_logdir = str(summary_dir[len(log_dir)+1:])\n    return rel_to_logdir.split(os.path.sep)[0]\n\n\ndef is_sub_run_dir(summary_dir: str, log_dir)->bool:\n    subdir = subdir_below_logdir(summary_dir, log_dir)\n\n    return subdir.isdigit() or (subdir.startswith(""learner-"") and subdir[len(""learner-""):].isdigit())\n\n\ndef dir_below_logdir(summary_dir: str, log_dir)->str:\n    subdir = subdir_below_logdir(summary_dir, log_dir)\n    return os.path.normpath(os.path.join(log_dir, subdir))\n\n\ndef extract(log_dir: str, should_connect: bool=True):\n    global etb_logger\n\n    log_scanner = scan_log_dirs.LogScanner(should_connect=should_connect, logger=etb_logger)\n\n    log_dir = os.path.normpath(log_dir)\n\n    etb_logger.info(""looping over %s (log_dir)"", log_dir)\n\n    while True:\n        # The design allows for there to be top-level log files, as well as subdirectory logs that\n        # contain some sub-run logs.  These are indexed in the training data service with the subdir\n        # in the MetaInfo inner struct.  Because these directories can be built as we\'re processing,\n        # we check all the paths on each iteration.\n        # etb_logger.debug(""considering if %s (log_dir) exists (top_level=%r)"", log_dir, top_level)\n\n        if not os.path.exists(log_dir):\n            time.sleep(1)\n            continue\n\n        # etb_logger.debug(""log_dir DOES exist: %s"", log_dir)\n\n        # logging.debug(""calling scan for the log_scanner"")\n        log_scanner.scan(log_dir=log_dir,\n                         is_log=match_log_file.is_log_file,\n                         push_function=push_log_line.push,\n                         should_loop=False)\n\n        for summary_dir in event_multiplexer.GetLogdirSubdirectories(log_dir):\n            summary_dir = os.path.normpath(summary_dir)\n\n            run_id = """"\n            top_log_dir = log_dir\n\n            if is_sub_run_dir(summary_dir, log_dir):\n                run_id = subdir_below_logdir(summary_dir, log_dir)\n                top_log_dir = dir_below_logdir(summary_dir, log_dir)\n\n            if top_log_dir not in log_scanner.log_runners:\n                td_client = log_scanner.td_client\n                if run_id != """":\n                    # In this case, we\'ll establish a new buffered tds client, since it\n                    # has to know which directory to write to.\n                    etb_logger.info(""Creating buffered client for run_id: %s"", run_id)\n                    td_client = tdb.TrainingDataClientBuffered(td_client.td_client)\n                run_tracker = Run_tracker(\n                    td_client=td_client,\n                    log_dir=top_log_dir,\n                    sub_identifier=run_id,\n                    lines_counter=log_scanner.lines_counter)\n\n                if run_tracker is not None:\n                    log_scanner.log_runners[top_log_dir] = run_tracker\n                    run_tracker.build_event_trackers()\n                    run_tracker.start()\n\n        time.sleep(.5)\n        if states.is_learner_done() and states.global_scanner_count == 0:\n                    break\n\n# class NoEvilDirectoryWatcherMessages(logging.Filter):\n#     def filter(self, record):\n#         print(""record: %r"" % record)\n#         sys.stdout.flush()\n#\n#         if \'No path found after\' in record.getMessage():\n#             return 0\n#         else:\n#             return 1\n\ndef main():\n    global etb_logger\n\n    # Have to keep this is WARN because the tensorboard code dumps mountains of stuff with INFO\n    logging.basicConfig(format=\'%(filename)s %(funcName)s %(lineno)d: %(message)s\', level=logging.WARN)\n\n    etb_logger = logging.getLogger(""extract_tb"")\n    etb_logger.setLevel(logging.INFO)\n    # logging.getLogger().addFilter(NoEvilDirectoryWatcherMessages())\n\n    parser = argparse.ArgumentParser()\n\n    log_directory = os.environ[""LOG_DIR""]\n    # em_file = log_directory + ""/evaluation-metrics.txt""\n\n    etb_logger.info(""LOG_DIR = %s"", log_directory)\n\n    parser.add_argument(\'--log_dir\', type=str, default=log_directory,\n                        help=\'DLaaS log directory\')\n\n    # parser.add_argument(\'--em_file\', type=str, default=em_file,\n    #                     help=\'Evaluation metrics file\')\n\n    FLAGS, unparsed = parser.parse_known_args()\n\n    etb_logger.info(""FLAGS.log_dir = %s"", FLAGS.log_dir)\n\n    extract(FLAGS.log_dir)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
metrics/log_collectors/tensorboard/src/tail_to_tds.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport argparse\n\nfrom log_collectors.training_data_service_client import tail_to_tds as tail\n\n\ndef main():\n    job_directory = os.environ[""JOB_STATE_DIR""]\n    log_directory = job_directory + ""/logs""\n    log_file = job_directory + ""/latest-log""\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--log_file\', type=str, default=log_file,\n                        help=\'Log file\')\n\n    FLAGS, _ = parser.parse_known_args()\n\n    tail.collect_and_send(FLAGS.log_file, True)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
metrics/service/grpc_training_data_v1/python/__init__.py,0,b''
metrics/service/grpc_training_data_v1/python/connect.py,0,"b'#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nimport os\nimport grpc\n\nif sys.version_info[0] < 3:\n    from training_data_service_client import training_data_pb2_grpc as td\nelse:\n    from .training_data_pb2 import training_data_pb2_grpc as td\n\ndef get_connection():\n    with open(\'./certs/server.crt\') as f:\n        certificate = f.read()\n\n    credentials = grpc.ssl_channel_credentials(root_certificates=certificate)\n\n    isTLSEnabled = True\n    isLocal = False\n\n    if isLocal:\n        hosturl = \'127.0.0.1\'\n        port = \'30015\'\n    else:\n        training_data_namespace = os.environ[""TRAINING_DATA_NAMESPACE""]\n        hosturl = ""ffdl-trainingdata.%s.svc.cluster.local"" % training_data_namespace\n        port = \'80\'\n        # hosturl = ""10.177.1.186""\n        # port = \'80\'\n\n    hosturl = \'{}:{}\'.format(hosturl, port)\n\n    print(""hosturl: ""+hosturl)\n    sys.stdout.flush()\n\n    if isTLSEnabled:\n        channel = grpc.secure_channel(hosturl, credentials, options=((\'grpc.ssl_target_name_override\', \'dlaas.ibm.com\',),))\n    else:\n        channel = grpc.insecure_channel(hosturl)\n\n    tdClient = td.TrainingDataStub(channel)\n\n    return tdClient\n\n'"
metrics/service/grpc_training_data_v1/python/push_log_line.py,0,"b'#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport sys\nimport os\nimport datetime\nimport time\n\nif sys.version_info[0] < 3:\n    from training_data_service_client import training_data_pb2_grpc as td\n    from training_data_service_client import training_data_pb2 as tdp\nelse:\n    from .training_data_service_client import training_data_pb2_grpc as td\n    from .training_data_service_client import training_data_pb2 as tdp\n\ndef totimestamp(dt, epoch=datetime.datetime(1970,1,1)):\n    td = dt - epoch\n    # return td.total_seconds()\n    return int((td.microseconds + (td.seconds + td.days * 86400) * 10**6) / 10**6)\n\ndef extract_datetime_from_line(line, year):\n    # Expected format: I0210 13:39:22.381027 25210 solver.cpp:204] Iteration 100, lr = 0.00992565\n    line = line.strip().split()\n    month = int(line[0][1:3])\n    day = int(line[0][3:])\n    timestamp = line[1]\n    pos = timestamp.rfind(\'.\')\n    ts = [int(x) for x in timestamp[:pos].split(\':\')]\n    hour = ts[0]\n    minute = ts[1]\n    second = ts[2]\n    microsecond = int(timestamp[pos + 1:])\n    dt = datetime.datetime(year, month, day, hour, minute, second, microsecond)\n    return dt\n\ndef push_logline(td_client, logline, logfile_year, rindex):\n    \'\'\'Push the processed metrics data to the metrics service\'\'\'\n\n    # ms = time.time()\n    try:\n        line_time = \\\n            extract_datetime_from_line(\n                logline, logfile_year)\n        ms = totimestamp(line_time)\n    except ValueError:\n        ms = time.time()\n\n    try:\n\n        logLineRecord = tdp.LogLine(\n            meta=tdp.MetaInfo(\n                training_id=os.environ[""TRAINING_ID""],\n                time=int(ms),\n                rindex=rindex\n            ),\n            line=logline\n        )\n\n        td_client.AddLogLine(logLineRecord)\n\n    except Exception as inst:\n        print(""Unexpected error when attempting to send logline:"", sys.exc_info()[0])\n        print(type(inst))\n        print(inst.args)\n        print(inst)\n        sys.stdout.flush()\n\n    # json_string = str(logLineRecord)\n    # json_string = json_string.replace(\'\\n\', \'\').replace(\'\\r\', \'\')\n    # # json_string = json_string.replace(\'type: STRING\', \'type: 0\').replace(\'type: JSONSTRING\', \'type: 1\').replace(\'type: INT\', \'type: 2\').replace(\'type: FLOAT\', \'type: 3\')\n    # print(json_string)\n\n    return rindex+1\n'"
metrics/service/grpc_training_data_v1/python/testx.py,0,"b'#\n# Copyright 2017-2018 IBM Corporation\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport os\nimport re\nimport time\nimport datetime\nimport sys\nimport extract_seconds\n\nimport json\n\nfrom google.protobuf import json_format\n\nimport training_data_service_client.training_data_pb2 as tdp\n# from .training_data_service_client import training_data_pb2_grpc as td\n# from training_data_service_client import connect\n# from training_data_service_client import push_log_line\n\n# import parse_log_incremental\ndef totimestamp(dt, epoch=datetime.datetime(1970,1,1)):\n    td = dt - epoch\n    # return td.total_seconds()\n    return int((td.microseconds + (td.seconds + td.days * 86400) * 10**6) / 10**6)\n\ntry:\n    rowdict = dict()\n    rowdict[\'NumIters\'] = \'5\'\n    rowdict[\'Seconds\'] = 34\n\n    rowdict[\'LearningRate\'] = \'2.43\'\n    rowdict[\'loss\'] = \'7.82\'\n\n    etimes = dict()\n    # etimes[\'iteration\'] = i*10\n    etimes[\'iteration\'] = tdp.Any(type=tdp.Any.INT, value=str(int(rowdict[\'NumIters\'])))\n    d = datetime.datetime.today() + datetime.timedelta(seconds=rowdict[\'Seconds\'])\n    etimes[\'timestamp\'] = tdp.Any(type=tdp.Any.STRING, value=d.isoformat(""T"") + ""Z"")\n\n    d = datetime.datetime.utcnow()\n\n    timestamp = totimestamp(d)\n\n    valuedict = dict()\n    # for k in rowdict.keys():\n    #     valuedict[k] = tdp.Any(type=tdp.Any.FLOAT, value=str(rowdict[k]))\n\n    if \'LearningRate\' in rowdict:\n        valuedict[\'lr\'] = tdp.Any(type=tdp.Any.FLOAT, value=str(rowdict[\'LearningRate\']))\n    if \'loss\' in rowdict:\n        valuedict[\'loss\'] = tdp.Any(type=tdp.Any.FLOAT, value=str(rowdict[\'loss\']))\n    if \'accuracy\' in rowdict:\n        valuedict[\'accuracy\'] = tdp.Any(type=tdp.Any.FLOAT, value=str(rowdict[\'accuracy\']))\n\n    emetrics = tdp.EMetrics(\n        meta=tdp.MetaInfo(\n            training_id=""training-abc"",\n            time=timestamp,\n            rindex=1\n        ),\n        grouplabel=""blah"",\n        etimes= etimes,\n        values=valuedict\n    )\n    # for k in rowdict.keys():\n    #     emetrics.EtimesEntry[k] = tdp.Any(type=tdp.Any.FLOAT, value=str(rowdict[k]))\n\n    # td_client.AddEMetrics(emetrics)\n\n    # We potentially still want to output to stdout in order to get picked up by fluentd or whoever.\n    # This doesn\'t work, at least without a custom deserializer:\n    # json_string = json.dumps(emetrics, skipkeys=True, sort_keys=False,\n    #                          separators=(\',\\t\', \': \'))\n    # ...also, the gRPC generated code doesn\'t generate legal json when marshaling to string.\n    # So, hack around this for the moment.\n\n    json_string = json_format.MessageToJson(emetrics)\n    # json_string = str(emetrics)\n    # json_string = json_string.replace(\'\\n\', \'\').replace(\'\\r\', \'\')\n    # json_string = json_string.replace(\'type: STRING\', \'type: 0\').replace(\'type: JSONSTRING\', \'type: 1\').replace(\'type: INT\', \'type: 2\').replace(\'type: FLOAT\', \'type: 3\')\n    # print(json_string)\n    print(json_string)\n\nexcept Exception as inst:\n    print(""Unexpected error when attempting to send emetrics:"", sys.exc_info()[0])\n    print(type(inst))\n    print(inst.args)\n    print(inst)\n    sys.stdout.flush()\n\n\n'"
metrics/service/grpc_training_data_v1/python/training_data_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: training_data.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'training_data.proto\',\n  package=\'grpc.training.data.v1\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n\\x13training_data.proto\\x12\\x15grpc.training.data.v1\\""N\\n\\x08MetaInfo\\x12\\x13\\n\\x0btraining_id\\x18\\x01 \\x01(\\t\\x12\\x0f\\n\\x07user_id\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04time\\x18\\x03 \\x01(\\x03\\x12\\x0e\\n\\x06rindex\\x18\\x04 \\x01(\\x03\\""F\\n\\x07LogLine\\x12-\\n\\x04meta\\x18\\x01 \\x01(\\x0b\\x32\\x1f.grpc.training.data.v1.MetaInfo\\x12\\x0c\\n\\x04line\\x18\\x02 \\x01(\\t\\""\\x83\\x01\\n\\x03\\x41ny\\x12\\x31\\n\\x04type\\x18\\x01 \\x01(\\x0e\\x32#.grpc.training.data.v1.Any.DataType\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\t\\"":\\n\\x08\\x44\\x61taType\\x12\\n\\n\\x06STRING\\x10\\x00\\x12\\x0e\\n\\nJSONSTRING\\x10\\x01\\x12\\x07\\n\\x03INT\\x10\\x02\\x12\\t\\n\\x05\\x46LOAT\\x10\\x03\\""\\xdd\\x02\\n\\x08\\x45Metrics\\x12-\\n\\x04meta\\x18\\x01 \\x01(\\x0b\\x32\\x1f.grpc.training.data.v1.MetaInfo\\x12;\\n\\x06\\x65times\\x18\\x02 \\x03(\\x0b\\x32+.grpc.training.data.v1.EMetrics.EtimesEntry\\x12\\x12\\n\\ngrouplabel\\x18\\x03 \\x01(\\t\\x12;\\n\\x06values\\x18\\x04 \\x03(\\x0b\\x32+.grpc.training.data.v1.EMetrics.ValuesEntry\\x1aI\\n\\x0b\\x45timesEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12)\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x1a.grpc.training.data.v1.Any:\\x02\\x38\\x01\\x1aI\\n\\x0bValuesEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12)\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x1a.grpc.training.data.v1.Any:\\x02\\x38\\x01\\""\\xd9\\x01\\n\\x05Query\\x12;\\n\\nsearchType\\x18\\x01 \\x01(\\x0e\\x32\\\'.grpc.training.data.v1.Query.SearchType\\x12-\\n\\x04meta\\x18\\x03 \\x01(\\x0b\\x32\\x1f.grpc.training.data.v1.MetaInfo\\x12\\r\\n\\x05since\\x18\\x04 \\x01(\\t\\x12\\x10\\n\\x08pagesize\\x18\\x05 \\x01(\\x05\\x12\\x0b\\n\\x03pos\\x18\\x06 \\x01(\\x03\\""6\\n\\nSearchType\\x12\\x08\\n\\x04TERM\\x10\\x00\\x12\\n\\n\\x06NESTED\\x10\\x01\\x12\\t\\n\\x05MATCH\\x10\\x02\\x12\\x07\\n\\x03\\x41LL\\x10\\x03\\""3\\n\\x0b\\x44\\x65leteQuery\\x12\\x13\\n\\x0btraining_id\\x18\\x01 \\x01(\\t\\x12\\x0f\\n\\x07user_id\\x18\\x02 \\x01(\\t\\""\\x1e\\n\\x0b\\x41\\x64\\x64Response\\x12\\x0f\\n\\x07success\\x18\\x01 \\x01(\\x08\\""!\\n\\x0e\\x44\\x65leteResponse\\x12\\x0f\\n\\x07success\\x18\\x01 \\x01(\\x08\\""\\x1c\\n\\rHelloResponse\\x12\\x0b\\n\\x03msg\\x18\\x01 \\x01(\\t\\""\\x07\\n\\x05\\x45mpty2\\xac\\x05\\n\\x0cTrainingData\\x12K\\n\\x07GetLogs\\x12\\x1c.grpc.training.data.v1.Query\\x1a\\x1e.grpc.training.data.v1.LogLine\\""\\x00\\x30\\x01\\x12P\\n\\x0bGetEMetrics\\x12\\x1c.grpc.training.data.v1.Query\\x1a\\x1f.grpc.training.data.v1.EMetrics\\""\\x00\\x30\\x01\\x12T\\n\\x0b\\x41\\x64\\x64\\x45Metrics\\x12\\x1f.grpc.training.data.v1.EMetrics\\x1a\\"".grpc.training.data.v1.AddResponse\\""\\x00\\x12R\\n\\nAddLogLine\\x12\\x1e.grpc.training.data.v1.LogLine\\x1a\\"".grpc.training.data.v1.AddResponse\\""\\x00\\x12W\\n\\x0e\\x44\\x65leteEMetrics\\x12\\x1c.grpc.training.data.v1.Query\\x1a%.grpc.training.data.v1.DeleteResponse\\""\\x00\\x12W\\n\\x0e\\x44\\x65leteLogLines\\x12\\x1c.grpc.training.data.v1.Query\\x1a%.grpc.training.data.v1.DeleteResponse\\""\\x00\\x12R\\n\\tDeleteJob\\x12\\x1c.grpc.training.data.v1.Query\\x1a%.grpc.training.data.v1.DeleteResponse\\""\\x00\\x12M\\n\\x05Hello\\x12\\x1c.grpc.training.data.v1.Empty\\x1a$.grpc.training.data.v1.HelloResponse\\""\\x00\\x62\\x06proto3\')\n)\n\n\n\n_ANY_DATATYPE = _descriptor.EnumDescriptor(\n  name=\'DataType\',\n  full_name=\'grpc.training.data.v1.Any.DataType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'STRING\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'JSONSTRING\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'INT\', index=2, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'FLOAT\', index=3, number=3,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=272,\n  serialized_end=330,\n)\n_sym_db.RegisterEnumDescriptor(_ANY_DATATYPE)\n\n_QUERY_SEARCHTYPE = _descriptor.EnumDescriptor(\n  name=\'SearchType\',\n  full_name=\'grpc.training.data.v1.Query.SearchType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'TERM\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'NESTED\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'MATCH\', index=2, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ALL\', index=3, number=3,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=848,\n  serialized_end=902,\n)\n_sym_db.RegisterEnumDescriptor(_QUERY_SEARCHTYPE)\n\n\n_METAINFO = _descriptor.Descriptor(\n  name=\'MetaInfo\',\n  full_name=\'grpc.training.data.v1.MetaInfo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'training_id\', full_name=\'grpc.training.data.v1.MetaInfo.training_id\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'user_id\', full_name=\'grpc.training.data.v1.MetaInfo.user_id\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'time\', full_name=\'grpc.training.data.v1.MetaInfo.time\', index=2,\n      number=3, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rindex\', full_name=\'grpc.training.data.v1.MetaInfo.rindex\', index=3,\n      number=4, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=46,\n  serialized_end=124,\n)\n\n\n_LOGLINE = _descriptor.Descriptor(\n  name=\'LogLine\',\n  full_name=\'grpc.training.data.v1.LogLine\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'meta\', full_name=\'grpc.training.data.v1.LogLine.meta\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'line\', full_name=\'grpc.training.data.v1.LogLine.line\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=126,\n  serialized_end=196,\n)\n\n\n_ANY = _descriptor.Descriptor(\n  name=\'Any\',\n  full_name=\'grpc.training.data.v1.Any\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'grpc.training.data.v1.Any.type\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'grpc.training.data.v1.Any.value\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _ANY_DATATYPE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=199,\n  serialized_end=330,\n)\n\n\n_EMETRICS_ETIMESENTRY = _descriptor.Descriptor(\n  name=\'EtimesEntry\',\n  full_name=\'grpc.training.data.v1.EMetrics.EtimesEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'grpc.training.data.v1.EMetrics.EtimesEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'grpc.training.data.v1.EMetrics.EtimesEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=534,\n  serialized_end=607,\n)\n\n_EMETRICS_VALUESENTRY = _descriptor.Descriptor(\n  name=\'ValuesEntry\',\n  full_name=\'grpc.training.data.v1.EMetrics.ValuesEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'grpc.training.data.v1.EMetrics.ValuesEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'grpc.training.data.v1.EMetrics.ValuesEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=609,\n  serialized_end=682,\n)\n\n_EMETRICS = _descriptor.Descriptor(\n  name=\'EMetrics\',\n  full_name=\'grpc.training.data.v1.EMetrics\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'meta\', full_name=\'grpc.training.data.v1.EMetrics.meta\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'etimes\', full_name=\'grpc.training.data.v1.EMetrics.etimes\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'grouplabel\', full_name=\'grpc.training.data.v1.EMetrics.grouplabel\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'values\', full_name=\'grpc.training.data.v1.EMetrics.values\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_EMETRICS_ETIMESENTRY, _EMETRICS_VALUESENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=333,\n  serialized_end=682,\n)\n\n\n_QUERY = _descriptor.Descriptor(\n  name=\'Query\',\n  full_name=\'grpc.training.data.v1.Query\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'searchType\', full_name=\'grpc.training.data.v1.Query.searchType\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'meta\', full_name=\'grpc.training.data.v1.Query.meta\', index=1,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'since\', full_name=\'grpc.training.data.v1.Query.since\', index=2,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pagesize\', full_name=\'grpc.training.data.v1.Query.pagesize\', index=3,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pos\', full_name=\'grpc.training.data.v1.Query.pos\', index=4,\n      number=6, type=3, cpp_type=2, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _QUERY_SEARCHTYPE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=685,\n  serialized_end=902,\n)\n\n\n_DELETEQUERY = _descriptor.Descriptor(\n  name=\'DeleteQuery\',\n  full_name=\'grpc.training.data.v1.DeleteQuery\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'training_id\', full_name=\'grpc.training.data.v1.DeleteQuery.training_id\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'user_id\', full_name=\'grpc.training.data.v1.DeleteQuery.user_id\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=904,\n  serialized_end=955,\n)\n\n\n_ADDRESPONSE = _descriptor.Descriptor(\n  name=\'AddResponse\',\n  full_name=\'grpc.training.data.v1.AddResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'success\', full_name=\'grpc.training.data.v1.AddResponse.success\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=957,\n  serialized_end=987,\n)\n\n\n_DELETERESPONSE = _descriptor.Descriptor(\n  name=\'DeleteResponse\',\n  full_name=\'grpc.training.data.v1.DeleteResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'success\', full_name=\'grpc.training.data.v1.DeleteResponse.success\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=989,\n  serialized_end=1022,\n)\n\n\n_HELLORESPONSE = _descriptor.Descriptor(\n  name=\'HelloResponse\',\n  full_name=\'grpc.training.data.v1.HelloResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'msg\', full_name=\'grpc.training.data.v1.HelloResponse.msg\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1024,\n  serialized_end=1052,\n)\n\n\n_EMPTY = _descriptor.Descriptor(\n  name=\'Empty\',\n  full_name=\'grpc.training.data.v1.Empty\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1054,\n  serialized_end=1061,\n)\n\n_LOGLINE.fields_by_name[\'meta\'].message_type = _METAINFO\n_ANY.fields_by_name[\'type\'].enum_type = _ANY_DATATYPE\n_ANY_DATATYPE.containing_type = _ANY\n_EMETRICS_ETIMESENTRY.fields_by_name[\'value\'].message_type = _ANY\n_EMETRICS_ETIMESENTRY.containing_type = _EMETRICS\n_EMETRICS_VALUESENTRY.fields_by_name[\'value\'].message_type = _ANY\n_EMETRICS_VALUESENTRY.containing_type = _EMETRICS\n_EMETRICS.fields_by_name[\'meta\'].message_type = _METAINFO\n_EMETRICS.fields_by_name[\'etimes\'].message_type = _EMETRICS_ETIMESENTRY\n_EMETRICS.fields_by_name[\'values\'].message_type = _EMETRICS_VALUESENTRY\n_QUERY.fields_by_name[\'searchType\'].enum_type = _QUERY_SEARCHTYPE\n_QUERY.fields_by_name[\'meta\'].message_type = _METAINFO\n_QUERY_SEARCHTYPE.containing_type = _QUERY\nDESCRIPTOR.message_types_by_name[\'MetaInfo\'] = _METAINFO\nDESCRIPTOR.message_types_by_name[\'LogLine\'] = _LOGLINE\nDESCRIPTOR.message_types_by_name[\'Any\'] = _ANY\nDESCRIPTOR.message_types_by_name[\'EMetrics\'] = _EMETRICS\nDESCRIPTOR.message_types_by_name[\'Query\'] = _QUERY\nDESCRIPTOR.message_types_by_name[\'DeleteQuery\'] = _DELETEQUERY\nDESCRIPTOR.message_types_by_name[\'AddResponse\'] = _ADDRESPONSE\nDESCRIPTOR.message_types_by_name[\'DeleteResponse\'] = _DELETERESPONSE\nDESCRIPTOR.message_types_by_name[\'HelloResponse\'] = _HELLORESPONSE\nDESCRIPTOR.message_types_by_name[\'Empty\'] = _EMPTY\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nMetaInfo = _reflection.GeneratedProtocolMessageType(\'MetaInfo\', (_message.Message,), dict(\n  DESCRIPTOR = _METAINFO,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.MetaInfo)\n  ))\n_sym_db.RegisterMessage(MetaInfo)\n\nLogLine = _reflection.GeneratedProtocolMessageType(\'LogLine\', (_message.Message,), dict(\n  DESCRIPTOR = _LOGLINE,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.LogLine)\n  ))\n_sym_db.RegisterMessage(LogLine)\n\nAny = _reflection.GeneratedProtocolMessageType(\'Any\', (_message.Message,), dict(\n  DESCRIPTOR = _ANY,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.Any)\n  ))\n_sym_db.RegisterMessage(Any)\n\nEMetrics = _reflection.GeneratedProtocolMessageType(\'EMetrics\', (_message.Message,), dict(\n\n  EtimesEntry = _reflection.GeneratedProtocolMessageType(\'EtimesEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _EMETRICS_ETIMESENTRY,\n    __module__ = \'training_data_pb2\'\n    # @@protoc_insertion_point(class_scope:grpc.training.data.v1.EMetrics.EtimesEntry)\n    ))\n  ,\n\n  ValuesEntry = _reflection.GeneratedProtocolMessageType(\'ValuesEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _EMETRICS_VALUESENTRY,\n    __module__ = \'training_data_pb2\'\n    # @@protoc_insertion_point(class_scope:grpc.training.data.v1.EMetrics.ValuesEntry)\n    ))\n  ,\n  DESCRIPTOR = _EMETRICS,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.EMetrics)\n  ))\n_sym_db.RegisterMessage(EMetrics)\n_sym_db.RegisterMessage(EMetrics.EtimesEntry)\n_sym_db.RegisterMessage(EMetrics.ValuesEntry)\n\nQuery = _reflection.GeneratedProtocolMessageType(\'Query\', (_message.Message,), dict(\n  DESCRIPTOR = _QUERY,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.Query)\n  ))\n_sym_db.RegisterMessage(Query)\n\nDeleteQuery = _reflection.GeneratedProtocolMessageType(\'DeleteQuery\', (_message.Message,), dict(\n  DESCRIPTOR = _DELETEQUERY,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.DeleteQuery)\n  ))\n_sym_db.RegisterMessage(DeleteQuery)\n\nAddResponse = _reflection.GeneratedProtocolMessageType(\'AddResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _ADDRESPONSE,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.AddResponse)\n  ))\n_sym_db.RegisterMessage(AddResponse)\n\nDeleteResponse = _reflection.GeneratedProtocolMessageType(\'DeleteResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _DELETERESPONSE,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.DeleteResponse)\n  ))\n_sym_db.RegisterMessage(DeleteResponse)\n\nHelloResponse = _reflection.GeneratedProtocolMessageType(\'HelloResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _HELLORESPONSE,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.HelloResponse)\n  ))\n_sym_db.RegisterMessage(HelloResponse)\n\nEmpty = _reflection.GeneratedProtocolMessageType(\'Empty\', (_message.Message,), dict(\n  DESCRIPTOR = _EMPTY,\n  __module__ = \'training_data_pb2\'\n  # @@protoc_insertion_point(class_scope:grpc.training.data.v1.Empty)\n  ))\n_sym_db.RegisterMessage(Empty)\n\n\n_EMETRICS_ETIMESENTRY.has_options = True\n_EMETRICS_ETIMESENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n_EMETRICS_VALUESENTRY.has_options = True\n_EMETRICS_VALUESENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n\n_TRAININGDATA = _descriptor.ServiceDescriptor(\n  name=\'TrainingData\',\n  full_name=\'grpc.training.data.v1.TrainingData\',\n  file=DESCRIPTOR,\n  index=0,\n  options=None,\n  serialized_start=1064,\n  serialized_end=1748,\n  methods=[\n  _descriptor.MethodDescriptor(\n    name=\'GetLogs\',\n    full_name=\'grpc.training.data.v1.TrainingData.GetLogs\',\n    index=0,\n    containing_service=None,\n    input_type=_QUERY,\n    output_type=_LOGLINE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'GetEMetrics\',\n    full_name=\'grpc.training.data.v1.TrainingData.GetEMetrics\',\n    index=1,\n    containing_service=None,\n    input_type=_QUERY,\n    output_type=_EMETRICS,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'AddEMetrics\',\n    full_name=\'grpc.training.data.v1.TrainingData.AddEMetrics\',\n    index=2,\n    containing_service=None,\n    input_type=_EMETRICS,\n    output_type=_ADDRESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'AddLogLine\',\n    full_name=\'grpc.training.data.v1.TrainingData.AddLogLine\',\n    index=3,\n    containing_service=None,\n    input_type=_LOGLINE,\n    output_type=_ADDRESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'DeleteEMetrics\',\n    full_name=\'grpc.training.data.v1.TrainingData.DeleteEMetrics\',\n    index=4,\n    containing_service=None,\n    input_type=_QUERY,\n    output_type=_DELETERESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'DeleteLogLines\',\n    full_name=\'grpc.training.data.v1.TrainingData.DeleteLogLines\',\n    index=5,\n    containing_service=None,\n    input_type=_QUERY,\n    output_type=_DELETERESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'DeleteJob\',\n    full_name=\'grpc.training.data.v1.TrainingData.DeleteJob\',\n    index=6,\n    containing_service=None,\n    input_type=_QUERY,\n    output_type=_DELETERESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'Hello\',\n    full_name=\'grpc.training.data.v1.TrainingData.Hello\',\n    index=7,\n    containing_service=None,\n    input_type=_EMPTY,\n    output_type=_HELLORESPONSE,\n    options=None,\n  ),\n])\n_sym_db.RegisterServiceDescriptor(_TRAININGDATA)\n\nDESCRIPTOR.services_by_name[\'TrainingData\'] = _TRAININGDATA\n\ntry:\n  # THESE ELEMENTS WILL BE DEPRECATED.\n  # Please use the generated *_pb2_grpc.py files instead.\n  import grpc\n  from grpc.beta import implementations as beta_implementations\n  from grpc.beta import interfaces as beta_interfaces\n  from grpc.framework.common import cardinality\n  from grpc.framework.interfaces.face import utilities as face_utilities\n\n\n  class TrainingDataStub(object):\n    """"""import ""google/api/annotations.proto"";\n\n    // See https://developers.google.com/protocol-buffers/docs/proto3#any\n    // and https://github.com/google/protobuf/blob/master/src/google/protobuf/any.proto\n    import ""google/protobuf/any.proto"";\n\n    Service to store evaluation metrics\n    ===== GET ENDPOINTS, for external fetch =============\n    """"""\n\n    def __init__(self, channel):\n      """"""Constructor.\n\n      Args:\n        channel: A grpc.Channel.\n      """"""\n      self.GetLogs = channel.unary_stream(\n          \'/grpc.training.data.v1.TrainingData/GetLogs\',\n          request_serializer=Query.SerializeToString,\n          response_deserializer=LogLine.FromString,\n          )\n      self.GetEMetrics = channel.unary_stream(\n          \'/grpc.training.data.v1.TrainingData/GetEMetrics\',\n          request_serializer=Query.SerializeToString,\n          response_deserializer=EMetrics.FromString,\n          )\n      self.AddEMetrics = channel.unary_unary(\n          \'/grpc.training.data.v1.TrainingData/AddEMetrics\',\n          request_serializer=EMetrics.SerializeToString,\n          response_deserializer=AddResponse.FromString,\n          )\n      self.AddLogLine = channel.unary_unary(\n          \'/grpc.training.data.v1.TrainingData/AddLogLine\',\n          request_serializer=LogLine.SerializeToString,\n          response_deserializer=AddResponse.FromString,\n          )\n      self.DeleteEMetrics = channel.unary_unary(\n          \'/grpc.training.data.v1.TrainingData/DeleteEMetrics\',\n          request_serializer=Query.SerializeToString,\n          response_deserializer=DeleteResponse.FromString,\n          )\n      self.DeleteLogLines = channel.unary_unary(\n          \'/grpc.training.data.v1.TrainingData/DeleteLogLines\',\n          request_serializer=Query.SerializeToString,\n          response_deserializer=DeleteResponse.FromString,\n          )\n      self.DeleteJob = channel.unary_unary(\n          \'/grpc.training.data.v1.TrainingData/DeleteJob\',\n          request_serializer=Query.SerializeToString,\n          response_deserializer=DeleteResponse.FromString,\n          )\n      self.Hello = channel.unary_unary(\n          \'/grpc.training.data.v1.TrainingData/Hello\',\n          request_serializer=Empty.SerializeToString,\n          response_deserializer=HelloResponse.FromString,\n          )\n\n\n  class TrainingDataServicer(object):\n    """"""import ""google/api/annotations.proto"";\n\n    // See https://developers.google.com/protocol-buffers/docs/proto3#any\n    // and https://github.com/google/protobuf/blob/master/src/google/protobuf/any.proto\n    import ""google/protobuf/any.proto"";\n\n    Service to store evaluation metrics\n    ===== GET ENDPOINTS, for external fetch =============\n    """"""\n\n    def GetLogs(self, request, context):\n      """"""Get loglines, based on query\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def GetEMetrics(self, request, context):\n      """"""Get evaluation metrics records, based on query\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def AddEMetrics(self, request, context):\n      """"""===== UPDATE ENDPOINTS, for internal use only =========\n      (Strip these from the proto for external client generation!)\n\n      Add evaluation metrics record\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def AddLogLine(self, request, context):\n      """"""Add log line record\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def DeleteEMetrics(self, request, context):\n      """"""Delete all evaluation metrics belonging to a training job or user id\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def DeleteLogLines(self, request, context):\n      """"""Delete all log lines belonging to a training job or user id\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def DeleteJob(self, request, context):\n      """"""Delete all log lines belonging to a training job or user id\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n    def Hello(self, request, context):\n      """"""===== In case you want it to say ""Hello"" =========\n      """"""\n      context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n      context.set_details(\'Method not implemented!\')\n      raise NotImplementedError(\'Method not implemented!\')\n\n\n  def add_TrainingDataServicer_to_server(servicer, server):\n    rpc_method_handlers = {\n        \'GetLogs\': grpc.unary_stream_rpc_method_handler(\n            servicer.GetLogs,\n            request_deserializer=Query.FromString,\n            response_serializer=LogLine.SerializeToString,\n        ),\n        \'GetEMetrics\': grpc.unary_stream_rpc_method_handler(\n            servicer.GetEMetrics,\n            request_deserializer=Query.FromString,\n            response_serializer=EMetrics.SerializeToString,\n        ),\n        \'AddEMetrics\': grpc.unary_unary_rpc_method_handler(\n            servicer.AddEMetrics,\n            request_deserializer=EMetrics.FromString,\n            response_serializer=AddResponse.SerializeToString,\n        ),\n        \'AddLogLine\': grpc.unary_unary_rpc_method_handler(\n            servicer.AddLogLine,\n            request_deserializer=LogLine.FromString,\n            response_serializer=AddResponse.SerializeToString,\n        ),\n        \'DeleteEMetrics\': grpc.unary_unary_rpc_method_handler(\n            servicer.DeleteEMetrics,\n            request_deserializer=Query.FromString,\n            response_serializer=DeleteResponse.SerializeToString,\n        ),\n        \'DeleteLogLines\': grpc.unary_unary_rpc_method_handler(\n            servicer.DeleteLogLines,\n            request_deserializer=Query.FromString,\n            response_serializer=DeleteResponse.SerializeToString,\n        ),\n        \'DeleteJob\': grpc.unary_unary_rpc_method_handler(\n            servicer.DeleteJob,\n            request_deserializer=Query.FromString,\n            response_serializer=DeleteResponse.SerializeToString,\n        ),\n        \'Hello\': grpc.unary_unary_rpc_method_handler(\n            servicer.Hello,\n            request_deserializer=Empty.FromString,\n            response_serializer=HelloResponse.SerializeToString,\n        ),\n    }\n    generic_handler = grpc.method_handlers_generic_handler(\n        \'grpc.training.data.v1.TrainingData\', rpc_method_handlers)\n    server.add_generic_rpc_handlers((generic_handler,))\n\n\n  class BetaTrainingDataServicer(object):\n    """"""The Beta API is deprecated for 0.15.0 and later.\n\n    It is recommended to use the GA API (classes and functions in this\n    file not marked beta) for all further purposes. This class was generated\n    only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0.""""""\n    """"""import ""google/api/annotations.proto"";\n\n    // See https://developers.google.com/protocol-buffers/docs/proto3#any\n    // and https://github.com/google/protobuf/blob/master/src/google/protobuf/any.proto\n    import ""google/protobuf/any.proto"";\n\n    Service to store evaluation metrics\n    ===== GET ENDPOINTS, for external fetch =============\n    """"""\n    def GetLogs(self, request, context):\n      """"""Get loglines, based on query\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def GetEMetrics(self, request, context):\n      """"""Get evaluation metrics records, based on query\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def AddEMetrics(self, request, context):\n      """"""===== UPDATE ENDPOINTS, for internal use only =========\n      (Strip these from the proto for external client generation!)\n\n      Add evaluation metrics record\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def AddLogLine(self, request, context):\n      """"""Add log line record\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def DeleteEMetrics(self, request, context):\n      """"""Delete all evaluation metrics belonging to a training job or user id\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def DeleteLogLines(self, request, context):\n      """"""Delete all log lines belonging to a training job or user id\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def DeleteJob(self, request, context):\n      """"""Delete all log lines belonging to a training job or user id\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n    def Hello(self, request, context):\n      """"""===== In case you want it to say ""Hello"" =========\n      """"""\n      context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)\n\n\n  class BetaTrainingDataStub(object):\n    """"""The Beta API is deprecated for 0.15.0 and later.\n\n    It is recommended to use the GA API (classes and functions in this\n    file not marked beta) for all further purposes. This class was generated\n    only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0.""""""\n    """"""import ""google/api/annotations.proto"";\n\n    // See https://developers.google.com/protocol-buffers/docs/proto3#any\n    // and https://github.com/google/protobuf/blob/master/src/google/protobuf/any.proto\n    import ""google/protobuf/any.proto"";\n\n    Service to store evaluation metrics\n    ===== GET ENDPOINTS, for external fetch =============\n    """"""\n    def GetLogs(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""Get loglines, based on query\n      """"""\n      raise NotImplementedError()\n    def GetEMetrics(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""Get evaluation metrics records, based on query\n      """"""\n      raise NotImplementedError()\n    def AddEMetrics(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""===== UPDATE ENDPOINTS, for internal use only =========\n      (Strip these from the proto for external client generation!)\n\n      Add evaluation metrics record\n      """"""\n      raise NotImplementedError()\n    AddEMetrics.future = None\n    def AddLogLine(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""Add log line record\n      """"""\n      raise NotImplementedError()\n    AddLogLine.future = None\n    def DeleteEMetrics(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""Delete all evaluation metrics belonging to a training job or user id\n      """"""\n      raise NotImplementedError()\n    DeleteEMetrics.future = None\n    def DeleteLogLines(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""Delete all log lines belonging to a training job or user id\n      """"""\n      raise NotImplementedError()\n    DeleteLogLines.future = None\n    def DeleteJob(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""Delete all log lines belonging to a training job or user id\n      """"""\n      raise NotImplementedError()\n    DeleteJob.future = None\n    def Hello(self, request, timeout, metadata=None, with_call=False, protocol_options=None):\n      """"""===== In case you want it to say ""Hello"" =========\n      """"""\n      raise NotImplementedError()\n    Hello.future = None\n\n\n  def beta_create_TrainingData_server(servicer, pool=None, pool_size=None, default_timeout=None, maximum_timeout=None):\n    """"""The Beta API is deprecated for 0.15.0 and later.\n\n    It is recommended to use the GA API (classes and functions in this\n    file not marked beta) for all further purposes. This function was\n    generated only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0""""""\n    request_deserializers = {\n      (\'grpc.training.data.v1.TrainingData\', \'AddEMetrics\'): EMetrics.FromString,\n      (\'grpc.training.data.v1.TrainingData\', \'AddLogLine\'): LogLine.FromString,\n      (\'grpc.training.data.v1.TrainingData\', \'DeleteEMetrics\'): Query.FromString,\n      (\'grpc.training.data.v1.TrainingData\', \'DeleteJob\'): Query.FromString,\n      (\'grpc.training.data.v1.TrainingData\', \'DeleteLogLines\'): Query.FromString,\n      (\'grpc.training.data.v1.TrainingData\', \'GetEMetrics\'): Query.FromString,\n      (\'grpc.training.data.v1.TrainingData\', \'GetLogs\'): Query.FromString,\n      (\'grpc.training.data.v1.TrainingData\', \'Hello\'): Empty.FromString,\n    }\n    response_serializers = {\n      (\'grpc.training.data.v1.TrainingData\', \'AddEMetrics\'): AddResponse.SerializeToString,\n      (\'grpc.training.data.v1.TrainingData\', \'AddLogLine\'): AddResponse.SerializeToString,\n      (\'grpc.training.data.v1.TrainingData\', \'DeleteEMetrics\'): DeleteResponse.SerializeToString,\n      (\'grpc.training.data.v1.TrainingData\', \'DeleteJob\'): DeleteResponse.SerializeToString,\n      (\'grpc.training.data.v1.TrainingData\', \'DeleteLogLines\'): DeleteResponse.SerializeToString,\n      (\'grpc.training.data.v1.TrainingData\', \'GetEMetrics\'): EMetrics.SerializeToString,\n      (\'grpc.training.data.v1.TrainingData\', \'GetLogs\'): LogLine.SerializeToString,\n      (\'grpc.training.data.v1.TrainingData\', \'Hello\'): HelloResponse.SerializeToString,\n    }\n    method_implementations = {\n      (\'grpc.training.data.v1.TrainingData\', \'AddEMetrics\'): face_utilities.unary_unary_inline(servicer.AddEMetrics),\n      (\'grpc.training.data.v1.TrainingData\', \'AddLogLine\'): face_utilities.unary_unary_inline(servicer.AddLogLine),\n      (\'grpc.training.data.v1.TrainingData\', \'DeleteEMetrics\'): face_utilities.unary_unary_inline(servicer.DeleteEMetrics),\n      (\'grpc.training.data.v1.TrainingData\', \'DeleteJob\'): face_utilities.unary_unary_inline(servicer.DeleteJob),\n      (\'grpc.training.data.v1.TrainingData\', \'DeleteLogLines\'): face_utilities.unary_unary_inline(servicer.DeleteLogLines),\n      (\'grpc.training.data.v1.TrainingData\', \'GetEMetrics\'): face_utilities.unary_stream_inline(servicer.GetEMetrics),\n      (\'grpc.training.data.v1.TrainingData\', \'GetLogs\'): face_utilities.unary_stream_inline(servicer.GetLogs),\n      (\'grpc.training.data.v1.TrainingData\', \'Hello\'): face_utilities.unary_unary_inline(servicer.Hello),\n    }\n    server_options = beta_implementations.server_options(request_deserializers=request_deserializers, response_serializers=response_serializers, thread_pool=pool, thread_pool_size=pool_size, default_timeout=default_timeout, maximum_timeout=maximum_timeout)\n    return beta_implementations.server(method_implementations, options=server_options)\n\n\n  def beta_create_TrainingData_stub(channel, host=None, metadata_transformer=None, pool=None, pool_size=None):\n    """"""The Beta API is deprecated for 0.15.0 and later.\n\n    It is recommended to use the GA API (classes and functions in this\n    file not marked beta) for all further purposes. This function was\n    generated only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0""""""\n    request_serializers = {\n      (\'grpc.training.data.v1.TrainingData\', \'AddEMetrics\'): EMetrics.SerializeToString,\n      (\'grpc.training.data.v1.TrainingData\', \'AddLogLine\'): LogLine.SerializeToString,\n      (\'grpc.training.data.v1.TrainingData\', \'DeleteEMetrics\'): Query.SerializeToString,\n      (\'grpc.training.data.v1.TrainingData\', \'DeleteJob\'): Query.SerializeToString,\n      (\'grpc.training.data.v1.TrainingData\', \'DeleteLogLines\'): Query.SerializeToString,\n      (\'grpc.training.data.v1.TrainingData\', \'GetEMetrics\'): Query.SerializeToString,\n      (\'grpc.training.data.v1.TrainingData\', \'GetLogs\'): Query.SerializeToString,\n      (\'grpc.training.data.v1.TrainingData\', \'Hello\'): Empty.SerializeToString,\n    }\n    response_deserializers = {\n      (\'grpc.training.data.v1.TrainingData\', \'AddEMetrics\'): AddResponse.FromString,\n      (\'grpc.training.data.v1.TrainingData\', \'AddLogLine\'): AddResponse.FromString,\n      (\'grpc.training.data.v1.TrainingData\', \'DeleteEMetrics\'): DeleteResponse.FromString,\n      (\'grpc.training.data.v1.TrainingData\', \'DeleteJob\'): DeleteResponse.FromString,\n      (\'grpc.training.data.v1.TrainingData\', \'DeleteLogLines\'): DeleteResponse.FromString,\n      (\'grpc.training.data.v1.TrainingData\', \'GetEMetrics\'): EMetrics.FromString,\n      (\'grpc.training.data.v1.TrainingData\', \'GetLogs\'): LogLine.FromString,\n      (\'grpc.training.data.v1.TrainingData\', \'Hello\'): HelloResponse.FromString,\n    }\n    cardinalities = {\n      \'AddEMetrics\': cardinality.Cardinality.UNARY_UNARY,\n      \'AddLogLine\': cardinality.Cardinality.UNARY_UNARY,\n      \'DeleteEMetrics\': cardinality.Cardinality.UNARY_UNARY,\n      \'DeleteJob\': cardinality.Cardinality.UNARY_UNARY,\n      \'DeleteLogLines\': cardinality.Cardinality.UNARY_UNARY,\n      \'GetEMetrics\': cardinality.Cardinality.UNARY_STREAM,\n      \'GetLogs\': cardinality.Cardinality.UNARY_STREAM,\n      \'Hello\': cardinality.Cardinality.UNARY_UNARY,\n    }\n    stub_options = beta_implementations.stub_options(host=host, metadata_transformer=metadata_transformer, request_serializers=request_serializers, response_deserializers=response_deserializers, thread_pool=pool, thread_pool_size=pool_size)\n    return beta_implementations.dynamic_stub(channel, \'grpc.training.data.v1.TrainingData\', cardinalities, options=stub_options)\nexcept ImportError:\n  pass\n# @@protoc_insertion_point(module_scope)\n'"
metrics/service/grpc_training_data_v1/python/training_data_pb2_grpc.py,0,"b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\nimport training_data_pb2 as training__data__pb2\n\n\nclass TrainingDataStub(object):\n  """"""import ""google/api/annotations.proto"";\n\n  // See https://developers.google.com/protocol-buffers/docs/proto3#any\n  // and https://github.com/google/protobuf/blob/master/src/google/protobuf/any.proto\n  import ""google/protobuf/any.proto"";\n\n  Service to store evaluation metrics\n  ===== GET ENDPOINTS, for external fetch =============\n  """"""\n\n  def __init__(self, channel):\n    """"""Constructor.\n\n    Args:\n      channel: A grpc.Channel.\n    """"""\n    self.GetLogs = channel.unary_stream(\n        \'/grpc.training.data.v1.TrainingData/GetLogs\',\n        request_serializer=training__data__pb2.Query.SerializeToString,\n        response_deserializer=training__data__pb2.LogLine.FromString,\n        )\n    self.GetEMetrics = channel.unary_stream(\n        \'/grpc.training.data.v1.TrainingData/GetEMetrics\',\n        request_serializer=training__data__pb2.Query.SerializeToString,\n        response_deserializer=training__data__pb2.EMetrics.FromString,\n        )\n    self.AddEMetrics = channel.unary_unary(\n        \'/grpc.training.data.v1.TrainingData/AddEMetrics\',\n        request_serializer=training__data__pb2.EMetrics.SerializeToString,\n        response_deserializer=training__data__pb2.AddResponse.FromString,\n        )\n    self.AddLogLine = channel.unary_unary(\n        \'/grpc.training.data.v1.TrainingData/AddLogLine\',\n        request_serializer=training__data__pb2.LogLine.SerializeToString,\n        response_deserializer=training__data__pb2.AddResponse.FromString,\n        )\n    self.DeleteEMetrics = channel.unary_unary(\n        \'/grpc.training.data.v1.TrainingData/DeleteEMetrics\',\n        request_serializer=training__data__pb2.Query.SerializeToString,\n        response_deserializer=training__data__pb2.DeleteResponse.FromString,\n        )\n    self.DeleteLogLines = channel.unary_unary(\n        \'/grpc.training.data.v1.TrainingData/DeleteLogLines\',\n        request_serializer=training__data__pb2.Query.SerializeToString,\n        response_deserializer=training__data__pb2.DeleteResponse.FromString,\n        )\n    self.DeleteJob = channel.unary_unary(\n        \'/grpc.training.data.v1.TrainingData/DeleteJob\',\n        request_serializer=training__data__pb2.Query.SerializeToString,\n        response_deserializer=training__data__pb2.DeleteResponse.FromString,\n        )\n    self.Hello = channel.unary_unary(\n        \'/grpc.training.data.v1.TrainingData/Hello\',\n        request_serializer=training__data__pb2.Empty.SerializeToString,\n        response_deserializer=training__data__pb2.HelloResponse.FromString,\n        )\n\n\nclass TrainingDataServicer(object):\n  """"""import ""google/api/annotations.proto"";\n\n  // See https://developers.google.com/protocol-buffers/docs/proto3#any\n  // and https://github.com/google/protobuf/blob/master/src/google/protobuf/any.proto\n  import ""google/protobuf/any.proto"";\n\n  Service to store evaluation metrics\n  ===== GET ENDPOINTS, for external fetch =============\n  """"""\n\n  def GetLogs(self, request, context):\n    """"""Get loglines, based on query\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def GetEMetrics(self, request, context):\n    """"""Get evaluation metrics records, based on query\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def AddEMetrics(self, request, context):\n    """"""===== UPDATE ENDPOINTS, for internal use only =========\n    (Strip these from the proto for external client generation!)\n\n    Add evaluation metrics record\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def AddLogLine(self, request, context):\n    """"""Add log line record\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def DeleteEMetrics(self, request, context):\n    """"""Delete all evaluation metrics belonging to a training job or user id\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def DeleteLogLines(self, request, context):\n    """"""Delete all log lines belonging to a training job or user id\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def DeleteJob(self, request, context):\n    """"""Delete all log lines belonging to a training job or user id\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def Hello(self, request, context):\n    """"""===== In case you want it to say ""Hello"" =========\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n\ndef add_TrainingDataServicer_to_server(servicer, server):\n  rpc_method_handlers = {\n      \'GetLogs\': grpc.unary_stream_rpc_method_handler(\n          servicer.GetLogs,\n          request_deserializer=training__data__pb2.Query.FromString,\n          response_serializer=training__data__pb2.LogLine.SerializeToString,\n      ),\n      \'GetEMetrics\': grpc.unary_stream_rpc_method_handler(\n          servicer.GetEMetrics,\n          request_deserializer=training__data__pb2.Query.FromString,\n          response_serializer=training__data__pb2.EMetrics.SerializeToString,\n      ),\n      \'AddEMetrics\': grpc.unary_unary_rpc_method_handler(\n          servicer.AddEMetrics,\n          request_deserializer=training__data__pb2.EMetrics.FromString,\n          response_serializer=training__data__pb2.AddResponse.SerializeToString,\n      ),\n      \'AddLogLine\': grpc.unary_unary_rpc_method_handler(\n          servicer.AddLogLine,\n          request_deserializer=training__data__pb2.LogLine.FromString,\n          response_serializer=training__data__pb2.AddResponse.SerializeToString,\n      ),\n      \'DeleteEMetrics\': grpc.unary_unary_rpc_method_handler(\n          servicer.DeleteEMetrics,\n          request_deserializer=training__data__pb2.Query.FromString,\n          response_serializer=training__data__pb2.DeleteResponse.SerializeToString,\n      ),\n      \'DeleteLogLines\': grpc.unary_unary_rpc_method_handler(\n          servicer.DeleteLogLines,\n          request_deserializer=training__data__pb2.Query.FromString,\n          response_serializer=training__data__pb2.DeleteResponse.SerializeToString,\n      ),\n      \'DeleteJob\': grpc.unary_unary_rpc_method_handler(\n          servicer.DeleteJob,\n          request_deserializer=training__data__pb2.Query.FromString,\n          response_serializer=training__data__pb2.DeleteResponse.SerializeToString,\n      ),\n      \'Hello\': grpc.unary_unary_rpc_method_handler(\n          servicer.Hello,\n          request_deserializer=training__data__pb2.Empty.FromString,\n          response_serializer=training__data__pb2.HelloResponse.SerializeToString,\n      ),\n  }\n  generic_handler = grpc.method_handlers_generic_handler(\n      \'grpc.training.data.v1.TrainingData\', rpc_method_handlers)\n  server.add_generic_rpc_handlers((generic_handler,))\n'"
