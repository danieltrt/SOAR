file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n# To use a consistent encoding\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nsetup(\n    name=\'seq2seq\',\n\n    # Versions should comply with PEP440.  For a discussion on single-sourcing\n    # the version across setup.py and the project code, see\n    # https://packaging.python.org/en/latest/single_source_version.html\n    version=\'0.1.6\',\n\n    description=\'A framework for sequence-to-sequence (seq2seq) models implemented in PyTorch.\',\n    long_description=long_description,\n\n    # The project\'s main homepage.\n    url=\'https://github.com/IBM/pytorch-seq2seq\',\n\n    # Choose your license\n    license=\'Apache License 2.0\',\n\n    # See https://pypi.python.org/pypi?%3Aaction=list_classifiers\n    classifiers=[\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        \'Development Status :: 3 - Alpha\',\n\n        # Indicate who your project is intended for\n        \'Intended Audience :: Research\',\n        \'Topic :: Software Development\',\n\n        # Pick your license as you wish (should match ""license"" above)\n        \'License :: Apache License 2.0\',\n\n        # Specify the Python versions you support here. In particular, ensure\n        # that you indicate whether you support Python 2, Python 3 or both.\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3.6\'\n    ],\n\n    # What does your project relate to?\n    keywords=\'seq2seq py-torch development\',\n\n    # You can just specify the packages manually here if your project is\n    # simple. Or you can use find_packages().\n    packages=find_packages(exclude=[\'contrib\', \'docs\', \'tests\']),\n\n    # Alternatively, if you want to distribute just a my_module.py, uncomment\n    # this:\n    #   py_modules=[""my_module""],\n\n    # List run-time dependencies here.  These will be installed by pip when\n    # your project is installed. For an analysis of ""install_requires"" vs pip\'s\n    # requirements files see:\n    # https://packaging.python.org/en/latest/requirements.html\n    install_requires=[\'numpy\', \'torch\', \'torchtext\'],\n\n    # List additional groups of dependencies here (e.g. development\n    # dependencies). You can install these using the following syntax,\n    # for example:\n    # $ pip install -e .[dev,test]\n    extras_require={\n        \'dev\': [\'check-manifest\'],\n        \'test\': [\'coverage\'],\n    }\n)\n'"
examples/sample.py,5,"b'import os\nimport argparse\nimport logging\n\nimport torch\nfrom torch.optim.lr_scheduler import StepLR\nimport torchtext\n\nimport seq2seq\nfrom seq2seq.trainer import SupervisedTrainer\nfrom seq2seq.models import EncoderRNN, DecoderRNN, Seq2seq\nfrom seq2seq.loss import Perplexity\nfrom seq2seq.optim import Optimizer\nfrom seq2seq.dataset import SourceField, TargetField\nfrom seq2seq.evaluator import Predictor\nfrom seq2seq.util.checkpoint import Checkpoint\n\ntry:\n    raw_input          # Python 2\nexcept NameError:\n    raw_input = input  # Python 3\n\n# Sample usage:\n#     # training\n#     python examples/sample.py --train_path $TRAIN_PATH --dev_path $DEV_PATH --expt_dir $EXPT_PATH\n#     # resuming from the latest checkpoint of the experiment\n#      python examples/sample.py --train_path $TRAIN_PATH --dev_path $DEV_PATH --expt_dir $EXPT_PATH --resume\n#      # resuming from a specific checkpoint\n#      python examples/sample.py --train_path $TRAIN_PATH --dev_path $DEV_PATH --expt_dir $EXPT_PATH --load_checkpoint $CHECKPOINT_DIR\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--train_path\', action=\'store\', dest=\'train_path\',\n                    help=\'Path to train data\')\nparser.add_argument(\'--dev_path\', action=\'store\', dest=\'dev_path\',\n                    help=\'Path to dev data\')\nparser.add_argument(\'--expt_dir\', action=\'store\', dest=\'expt_dir\', default=\'./experiment\',\n                    help=\'Path to experiment directory. If load_checkpoint is True, then path to checkpoint directory has to be provided\')\nparser.add_argument(\'--load_checkpoint\', action=\'store\', dest=\'load_checkpoint\',\n                    help=\'The name of the checkpoint to load, usually an encoded time string\')\nparser.add_argument(\'--resume\', action=\'store_true\', dest=\'resume\',\n                    default=False,\n                    help=\'Indicates if training has to be resumed from the latest checkpoint\')\nparser.add_argument(\'--log-level\', dest=\'log_level\',\n                    default=\'info\',\n                    help=\'Logging level.\')\n\nopt = parser.parse_args()\n\nLOG_FORMAT = \'%(asctime)s %(name)-12s %(levelname)-8s %(message)s\'\nlogging.basicConfig(format=LOG_FORMAT, level=getattr(logging, opt.log_level.upper()))\nlogging.info(opt)\n\nif opt.load_checkpoint is not None:\n    logging.info(""loading checkpoint from {}"".format(os.path.join(opt.expt_dir, Checkpoint.CHECKPOINT_DIR_NAME, opt.load_checkpoint)))\n    checkpoint_path = os.path.join(opt.expt_dir, Checkpoint.CHECKPOINT_DIR_NAME, opt.load_checkpoint)\n    checkpoint = Checkpoint.load(checkpoint_path)\n    seq2seq = checkpoint.model\n    input_vocab = checkpoint.input_vocab\n    output_vocab = checkpoint.output_vocab\nelse:\n    # Prepare dataset\n    src = SourceField()\n    tgt = TargetField()\n    max_len = 50\n    def len_filter(example):\n        return len(example.src) <= max_len and len(example.tgt) <= max_len\n    train = torchtext.data.TabularDataset(\n        path=opt.train_path, format=\'tsv\',\n        fields=[(\'src\', src), (\'tgt\', tgt)],\n        filter_pred=len_filter\n    )\n    dev = torchtext.data.TabularDataset(\n        path=opt.dev_path, format=\'tsv\',\n        fields=[(\'src\', src), (\'tgt\', tgt)],\n        filter_pred=len_filter\n    )\n    src.build_vocab(train, max_size=50000)\n    tgt.build_vocab(train, max_size=50000)\n    input_vocab = src.vocab\n    output_vocab = tgt.vocab\n\n    # NOTE: If the source field name and the target field name\n    # are different from \'src\' and \'tgt\' respectively, they have\n    # to be set explicitly before any training or inference\n    # seq2seq.src_field_name = \'src\'\n    # seq2seq.tgt_field_name = \'tgt\'\n\n    # Prepare loss\n    weight = torch.ones(len(tgt.vocab))\n    pad = tgt.vocab.stoi[tgt.pad_token]\n    loss = Perplexity(weight, pad)\n    if torch.cuda.is_available():\n        loss.cuda()\n\n    seq2seq = None\n    optimizer = None\n    if not opt.resume:\n        # Initialize model\n        hidden_size=128\n        bidirectional = True\n        encoder = EncoderRNN(len(src.vocab), max_len, hidden_size,\n                             bidirectional=bidirectional, variable_lengths=True)\n        decoder = DecoderRNN(len(tgt.vocab), max_len, hidden_size * 2 if bidirectional else hidden_size,\n                             dropout_p=0.2, use_attention=True, bidirectional=bidirectional,\n                             eos_id=tgt.eos_id, sos_id=tgt.sos_id)\n        seq2seq = Seq2seq(encoder, decoder)\n        if torch.cuda.is_available():\n            seq2seq.cuda()\n\n        for param in seq2seq.parameters():\n            param.data.uniform_(-0.08, 0.08)\n\n        # Optimizer and learning rate scheduler can be customized by\n        # explicitly constructing the objects and pass to the trainer.\n        #\n        # optimizer = Optimizer(torch.optim.Adam(seq2seq.parameters()), max_grad_norm=5)\n        # scheduler = StepLR(optimizer.optimizer, 1)\n        # optimizer.set_scheduler(scheduler)\n\n    # train\n    t = SupervisedTrainer(loss=loss, batch_size=32,\n                          checkpoint_every=50,\n                          print_every=10, expt_dir=opt.expt_dir)\n\n    seq2seq = t.train(seq2seq, train,\n                      num_epochs=6, dev_data=dev,\n                      optimizer=optimizer,\n                      teacher_forcing_ratio=0.5,\n                      resume=opt.resume)\n\npredictor = Predictor(seq2seq, input_vocab, output_vocab)\n\nwhile True:\n    seq_str = raw_input(""Type in a source sequence:"")\n    seq = seq_str.strip().split()\n    print(predictor.predict(seq))\n'"
scripts/generate_toy_data.py,0,"b'from __future__ import print_function\nimport argparse\nimport os\nimport shutil\nimport random\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--dir\', help=""data directory"", default=""../data"")\nparser.add_argument(\'--max-len\', help=""max sequence length"", default=10)\nargs = parser.parse_args()\n\ndef generate_dataset(root, name, size):\n    path = os.path.join(root, name)\n    if not os.path.exists(path):\n        os.mkdir(path)\n\n    # generate data file\n    data_path = os.path.join(path, \'data.txt\')\n    with open(data_path, \'w\') as fout:\n        for _ in range(size):\n            length = random.randint(1, args.max_len)\n            seq = []\n            for _ in range(length):\n                seq.append(str(random.randint(0, 9)))\n            fout.write(""\\t"".join(["" "".join(seq), "" "".join(reversed(seq))]))\n            fout.write(\'\\n\')\n\n    # generate vocabulary\n    src_vocab = os.path.join(path, \'vocab.source\')\n    with open(src_vocab, \'w\') as fout:\n        fout.write(""\\n"".join([str(i) for i in range(10)]))\n    tgt_vocab = os.path.join(path, \'vocab.target\')\n    shutil.copy(src_vocab, tgt_vocab)\n\nif __name__ == \'__main__\':\n    data_dir = args.dir\n    if not os.path.exists(data_dir):\n        os.mkdir(data_dir)\n\n    toy_dir = os.path.join(data_dir, \'toy_reverse\')\n    if not os.path.exists(toy_dir):\n        os.mkdir(toy_dir)\n\n    generate_dataset(toy_dir, \'train\', 10000)\n    generate_dataset(toy_dir, \'dev\', 1000)\n    generate_dataset(toy_dir, \'test\', 1000)\n'"
scripts/integration_test.py,3,"b'import os\nimport argparse\nimport logging\n\nimport torch\nimport torchtext\n\nimport seq2seq\nfrom seq2seq.trainer import SupervisedTrainer\nfrom seq2seq.models import EncoderRNN, DecoderRNN, TopKDecoder, Seq2seq\nfrom seq2seq.loss import Perplexity\nfrom seq2seq.dataset import SourceField, TargetField\nfrom seq2seq.evaluator import Predictor, Evaluator\nfrom seq2seq.util.checkpoint import Checkpoint\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--train_path\', action=\'store\', dest=\'train_path\',\n                    help=\'Path to train data\')\nparser.add_argument(\'--dev_path\', action=\'store\', dest=\'dev_path\',\n                    help=\'Path to dev data\')\nparser.add_argument(\'--expt_dir\', action=\'store\', dest=\'expt_dir\', default=\'./experiment\',\n                    help=\'Path to experiment directory. If load_checkpoint is True, then path to checkpoint directory has to be provided\')\nparser.add_argument(\'--load_checkpoint\', action=\'store\', dest=\'load_checkpoint\',\n                    help=\'The name of the checkpoint to load, usually an encoded time string\')\nparser.add_argument(\'--resume\', action=\'store_true\', dest=\'resume\',\n                    default=False,\n                    help=\'Indicates if training has to be resumed from the latest checkpoint\')\nparser.add_argument(\'--log-level\', dest=\'log_level\',\n                    default=\'info\',\n                    help=\'Logging level.\')\n\nopt = parser.parse_args()\n\nLOG_FORMAT = \'%(asctime)s %(name)-12s %(levelname)-8s %(message)s\'\nlogging.basicConfig(format=LOG_FORMAT, level=getattr(logging, opt.log_level.upper()))\nlogging.info(opt)\n\n# Prepare dataset\nsrc = SourceField()\ntgt = TargetField()\nmax_len = 50\ndef len_filter(example):\n    return len(example.src) <= max_len and len(example.tgt) <= max_len\ntrain = torchtext.data.TabularDataset(\n    path=opt.train_path, format=\'tsv\',\n    fields=[(\'src\', src), (\'tgt\', tgt)],\n    filter_pred=len_filter\n)\ndev = torchtext.data.TabularDataset(\n    path=opt.dev_path, format=\'tsv\',\n    fields=[(\'src\', src), (\'tgt\', tgt)],\n    filter_pred=len_filter\n)\nsrc.build_vocab(train, max_size=50000)\ntgt.build_vocab(train, max_size=50000)\ninput_vocab = src.vocab\noutput_vocab = tgt.vocab\n\n# Prepare loss\nweight = torch.ones(len(tgt.vocab))\npad = tgt.vocab.stoi[tgt.pad_token]\nloss = Perplexity(weight, pad)\nif torch.cuda.is_available():\n    loss.cuda()\n\nif opt.load_checkpoint is not None:\n    logging.info(""loading checkpoint from {}"".format(os.path.join(opt.expt_dir, Checkpoint.CHECKPOINT_DIR_NAME, opt.load_checkpoint)))\n    checkpoint_path = os.path.join(opt.expt_dir, Checkpoint.CHECKPOINT_DIR_NAME, opt.load_checkpoint)\n    checkpoint = Checkpoint.load(checkpoint_path)\n    seq2seq = checkpoint.model\n    input_vocab = checkpoint.input_vocab\n    output_vocab = checkpoint.output_vocab\nelse:\n    seq2seq = None\n    optimizer = None\n    if not opt.resume:\n        # Initialize model\n        hidden_size=128\n        bidirectional = True\n        encoder = EncoderRNN(len(src.vocab), max_len, hidden_size,\n                             bidirectional=bidirectional,\n                             rnn_cell=\'lstm\',\n                             variable_lengths=True)\n        decoder = DecoderRNN(len(tgt.vocab), max_len, hidden_size * 2,\n                             dropout_p=0.2, use_attention=True,\n                             bidirectional=bidirectional,\n                             rnn_cell=\'lstm\',\n                             eos_id=tgt.eos_id, sos_id=tgt.sos_id)\n        seq2seq = Seq2seq(encoder, decoder)\n        if torch.cuda.is_available():\n            seq2seq.cuda()\n\n        for param in seq2seq.parameters():\n            param.data.uniform_(-0.08, 0.08)\n\n    # train\n    t = SupervisedTrainer(loss=loss, batch_size=32,\n                          checkpoint_every=50,\n                          print_every=10, expt_dir=opt.expt_dir)\n\n    seq2seq = t.train(seq2seq, train,\n                      num_epochs=6, dev_data=dev,\n                      optimizer=optimizer,\n                      teacher_forcing_ratio=0.5,\n                      resume=opt.resume)\n\nevaluator = Evaluator(loss=loss, batch_size=32)\ndev_loss, accuracy = evaluator.evaluate(seq2seq, dev)\nassert dev_loss < 1.5\n\nbeam_search = Seq2seq(seq2seq.encoder, TopKDecoder(seq2seq.decoder, 3))\n\npredictor = Predictor(beam_search, input_vocab, output_vocab)\ninp_seq = ""1 3 5 7 9""\nseq = predictor.predict(inp_seq.split())\nassert "" "".join(seq[:-1]) == inp_seq[::-1]\n'"
seq2seq/__init__.py,0,"b""src_field_name = 'src'\ntgt_field_name = 'tgt'\n"""
tests/__init__.py,0,b''
tests/test_checkpoint.py,6,"b'import unittest\nimport os\nimport shutil\n\nimport mock\nfrom mock import ANY\n\nfrom seq2seq.util.checkpoint import Checkpoint\n\n\nclass TestCheckpoint(unittest.TestCase):\n\n    EXP_DIR = ""test_experiment""\n\n    def tearDown(self):\n        path = self._get_experiment_dir()\n        if os.path.exists(path):\n            shutil.rmtree(path)\n\n    def test_path_error(self):\n        ckpt = Checkpoint(None, None, None, None, None, None)\n        self.assertRaises(LookupError, lambda: ckpt.path)\n\n    @mock.patch(\'seq2seq.util.checkpoint.os.listdir\')\n    def test_get_latest_checkpoint(self, mock_listdir):\n        mock_listdir.return_value = [\'2017_05_22_09_47_26\',\n                                     \'2017_05_22_09_47_31\',\n                                     \'2017_05_23_10_47_29\']\n        latest_checkpoint = Checkpoint.get_latest_checkpoint(self.EXP_DIR)\n        self.assertEquals(latest_checkpoint,\n                          os.path.join(self.EXP_DIR,\n                                       \'checkpoints/2017_05_23_10_47_29\'))\n\n    @mock.patch(\'seq2seq.util.checkpoint.torch\')\n    @mock.patch(\'seq2seq.util.checkpoint.dill\')\n    @mock.patch(\'seq2seq.util.checkpoint.open\')\n    def test_save_checkpoint_calls_torch_save(self, mock_open, mock_dill, mock_torch):\n        epoch = 5\n        step = 10\n        optim = mock.Mock()\n        state_dict = {\'epoch\': epoch, \'step\': step, \'optimizer\': optim}\n\n        mock_model = mock.Mock()\n        mock_vocab = mock.Mock()\n        mock_open.return_value = mock.MagicMock()\n\n        chk_point = Checkpoint(model=mock_model, optimizer=optim,\n                               epoch=epoch, step=step,\n                               input_vocab=mock_vocab, output_vocab=mock_vocab)\n\n        path = chk_point.save(self._get_experiment_dir())\n\n        self.assertEquals(2, mock_torch.save.call_count)\n        mock_torch.save.assert_any_call(state_dict,\n                                        os.path.join(chk_point.path, Checkpoint.TRAINER_STATE_NAME))\n        mock_torch.save.assert_any_call(mock_model,\n                                        os.path.join(chk_point.path, Checkpoint.MODEL_NAME))\n        self.assertEquals(2, mock_open.call_count)\n        mock_open.assert_any_call(os.path.join(path, Checkpoint.INPUT_VOCAB_FILE), ANY)\n        mock_open.assert_any_call(os.path.join(path, Checkpoint.OUTPUT_VOCAB_FILE), ANY)\n        self.assertEquals(2, mock_dill.dump.call_count)\n        mock_dill.dump.assert_any_call(mock_vocab,\n                                       mock_open.return_value.__enter__.return_value)\n\n    @mock.patch(\'seq2seq.util.checkpoint.torch\')\n    @mock.patch(\'seq2seq.util.checkpoint.dill\')\n    @mock.patch(\'seq2seq.util.checkpoint.open\')\n    def test_load(self, mock_open, mock_dill, mock_torch):\n        dummy_vocabulary = mock.Mock()\n        mock_optimizer = mock.Mock()\n        torch_dict = {""optimizer"": mock_optimizer, ""epoch"": 5, ""step"": 10}\n        mock_open.return_value = mock.MagicMock()\n        mock_torch.load.side_effect = [torch_dict, mock.MagicMock()]\n        mock_dill.load.return_value = dummy_vocabulary\n\n        loaded_chk_point = Checkpoint.load(""mock_checkpoint_path"")\n\n        mock_torch.load.assert_any_call(\n            os.path.join(\'mock_checkpoint_path\', Checkpoint.TRAINER_STATE_NAME))\n        mock_torch.load.assert_any_call(\n            os.path.join(""mock_checkpoint_path"", Checkpoint.MODEL_NAME))\n\n        self.assertEquals(loaded_chk_point.epoch, torch_dict[\'epoch\'])\n        self.assertEquals(loaded_chk_point.optimizer, torch_dict[\'optimizer\'])\n        self.assertEquals(loaded_chk_point.step, torch_dict[\'step\'])\n        self.assertEquals(loaded_chk_point.input_vocab, dummy_vocabulary)\n        self.assertEquals(loaded_chk_point.output_vocab, dummy_vocabulary)\n\n    def _get_experiment_dir(self):\n        root_dir = os.path.dirname(os.path.realpath(__file__))\n        experiment_dir = os.path.join(root_dir, self.EXP_DIR)\n        return experiment_dir\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/test_decoder_rnn.py,4,"b'import os\nimport unittest\n\nimport torch\n\nfrom seq2seq.models import DecoderRNN\n\nclass TestDecoderRNN(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(self):\n        self.vocab_size = 100\n\n    def test_input_dropout_WITH_PROB_ZERO(self):\n        rnn = DecoderRNN(self.vocab_size, 50, 16, 0, 1, input_dropout_p=0)\n        for param in rnn.parameters():\n            param.data.uniform_(-1, 1)\n        output1, _, _ = rnn()\n        output2, _, _ = rnn()\n        for prob1, prob2 in zip(output1, output2):\n            self.assertTrue(torch.equal(prob1.data, prob2.data))\n\n    def test_input_dropout_WITH_NON_ZERO_PROB(self):\n        rnn = DecoderRNN(self.vocab_size, 50, 16, 0, 1, input_dropout_p=0.5)\n        for param in rnn.parameters():\n            param.data.uniform_(-1, 1)\n\n        equal = True\n        for _ in range(50):\n            output1, _, _ = rnn()\n            output2, _, _ = rnn()\n            if not torch.equal(output1[0].data, output2[0].data):\n                equal = False\n                break\n        self.assertFalse(equal)\n\n    def test_dropout_WITH_PROB_ZERO(self):\n        rnn = DecoderRNN(self.vocab_size, 50, 16, 0, 1, dropout_p=0)\n        for param in rnn.parameters():\n            param.data.uniform_(-1, 1)\n        output1, _, _ = rnn()\n        output2, _, _ = rnn()\n        for prob1, prob2 in zip(output1, output2):\n            self.assertTrue(torch.equal(prob1.data, prob2.data))\n\n    def test_dropout_WITH_NON_ZERO_PROB(self):\n        rnn = DecoderRNN(self.vocab_size, 50, 16, 0, 1, n_layers=2, dropout_p=0.5)\n        for param in rnn.parameters():\n            param.data.uniform_(-1, 1)\n\n        equal = True\n        for _ in range(50):\n            output1, _, _ = rnn()\n            output2, _, _ = rnn()\n            if not torch.equal(output1[0].data, output2[0].data):\n                equal = False\n                break\n        self.assertFalse(equal)\n'"
tests/test_encoder_rnn.py,8,"b""import os\nimport unittest\n\nimport torch\nfrom torch.autograd import Variable\nfrom seq2seq.models import EncoderRNN\n\nclass TestEncoderRNN(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(self):\n        self.vocab_size = 100\n        self.input_var = Variable(torch.randperm(self.vocab_size).view(10, 10))\n        self.lengths = [10] * 10\n\n    def test_input_dropout_WITH_PROB_ZERO(self):\n        rnn = EncoderRNN(self.vocab_size, 50, 16, input_dropout_p=0)\n        for param in rnn.parameters():\n            param.data.uniform_(-1, 1)\n        output1, _ = rnn(self.input_var, self.lengths)\n        output2, _ = rnn(self.input_var, self.lengths)\n        self.assertTrue(torch.equal(output1.data, output2.data))\n\n    def test_input_dropout_WITH_NON_ZERO_PROB(self):\n        rnn = EncoderRNN(self.vocab_size, 50, 16, input_dropout_p=0.5)\n        for param in rnn.parameters():\n            param.data.uniform_(-1, 1)\n\n        equal = True\n        for _ in range(50):\n            output1, _ = rnn(self.input_var, self.lengths)\n            output2, _ = rnn(self.input_var, self.lengths)\n            if not torch.equal(output1.data, output2.data):\n                equal = False\n                break\n        self.assertFalse(equal)\n\n    def test_dropout_WITH_PROB_ZERO(self):\n        rnn = EncoderRNN(self.vocab_size, 50, 16, dropout_p=0)\n        for param in rnn.parameters():\n            param.data.uniform_(-1, 1)\n        output1, _ = rnn(self.input_var, self.lengths)\n        output2, _ = rnn(self.input_var, self.lengths)\n        self.assertTrue(torch.equal(output1.data, output2.data))\n\n    def test_dropout_WITH_NON_ZERO_PROB(self):\n        # It's critical to set n_layer=2 here since dropout won't work\n        # when the RNN only has one layer according to pytorch's doc\n        rnn = EncoderRNN(self.vocab_size, 50, 16, n_layers=2, dropout_p=0.5)\n        for param in rnn.parameters():\n            param.data.uniform_(-1, 1)\n\n        equal = True\n        for _ in range(50):\n            output1, _ = rnn(self.input_var, self.lengths)\n            output2, _ = rnn(self.input_var, self.lengths)\n            if not torch.equal(output1.data, output2.data):\n                equal = False\n                break\n        self.assertFalse(equal)\n\n    def test_pretrained_embedding(self):\n        hidden_size = 16\n        pretrained_embedding = torch.randn(self.vocab_size, hidden_size)\n        rnn = EncoderRNN(self.vocab_size, 50, hidden_size,\n                         embedding=pretrained_embedding,\n                         update_embedding=False)\n        self.assertTrue(torch.equal(pretrained_embedding, rnn.embedding.weight.data))\n        self.assertFalse(rnn.embedding.weight.requires_grad)\n"""
tests/test_evaluator.py,1,"b'from __future__ import division\nimport os\nimport math\nimport unittest\n\nfrom mock import MagicMock, patch, call, ANY\nimport torchtext\n\nfrom seq2seq.dataset import SourceField, TargetField\nfrom seq2seq.evaluator import Evaluator\nfrom seq2seq.models import Seq2seq, EncoderRNN, DecoderRNN\n\nclass TestPredictor(unittest.TestCase):\n\n    def setUp(self):\n        test_path = os.path.dirname(os.path.realpath(__file__))\n        src = SourceField()\n        tgt = TargetField()\n        self.dataset = torchtext.data.TabularDataset(\n            path=os.path.join(test_path, \'data/eng-fra.txt\'), format=\'tsv\',\n            fields=[(\'src\', src), (\'tgt\', tgt)],\n        )\n        src.build_vocab(self.dataset)\n        tgt.build_vocab(self.dataset)\n\n        encoder = EncoderRNN(len(src.vocab), 10, 10, rnn_cell=\'lstm\')\n        decoder = DecoderRNN(len(tgt.vocab), 10, 10, tgt.sos_id, tgt.eos_id, rnn_cell=\'lstm\')\n        self.seq2seq = Seq2seq(encoder, decoder)\n\n        for param in self.seq2seq.parameters():\n            param.data.uniform_(-0.08, 0.08)\n\n    @patch.object(Seq2seq, \'__call__\', return_value=([], None, dict(inputs=[], length=[10]*64, sequence=MagicMock())))\n    @patch.object(Seq2seq, \'eval\')\n    def test_set_eval_mode(self, mock_eval, mock_call):\n        """""" Make sure that evaluation is done in evaluation mode. """"""\n        mock_mgr = MagicMock()\n        mock_mgr.attach_mock(mock_eval, \'eval\')\n        mock_mgr.attach_mock(mock_call, \'call\')\n\n        evaluator = Evaluator(batch_size=64)\n        with patch(\'seq2seq.evaluator.evaluator.torch.stack\', return_value=None), \\\n                patch(\'seq2seq.loss.NLLLoss.eval_batch\', return_value=None):\n            evaluator.evaluate(self.seq2seq, self.dataset)\n\n        num_batches = int(math.ceil(len(self.dataset) / evaluator.batch_size))\n        expected_calls = [call.eval()] + num_batches * [call.call(ANY, ANY, ANY)]\n        self.assertEquals(expected_calls, mock_mgr.mock_calls)\n'"
tests/test_fields.py,0,"b""import os\nimport unittest\n\nimport torchtext\n\nfrom seq2seq.dataset import SourceField, TargetField\n\nclass TestField(unittest.TestCase):\n\n    def test_sourcefield(self):\n        field = SourceField()\n        self.assertTrue(isinstance(field, torchtext.data.Field))\n        self.assertTrue(field.batch_first)\n        self.assertTrue(field.include_lengths)\n\n    def test_sourcefield_with_wrong_setting(self):\n        field = SourceField(batch_first=False, include_lengths=False)\n        self.assertTrue(isinstance(field, torchtext.data.Field))\n        self.assertTrue(field.batch_first)\n        self.assertTrue(field.include_lengths)\n\n    def test_targetfield(self):\n        field = TargetField()\n        self.assertTrue(isinstance(field, torchtext.data.Field))\n        self.assertTrue(field.batch_first)\n\n        processed = field.preprocessing([None])\n        self.assertEqual(processed, ['<sos>', None, '<eos>'])\n\n    def test_targetfield_with_other_setting(self):\n        field = TargetField(batch_first=False, preprocessing=lambda seq: seq + seq)\n        self.assertTrue(isinstance(field, torchtext.data.Field))\n        self.assertTrue(field.batch_first)\n\n        processed = field.preprocessing([None])\n        self.assertEqual(processed, ['<sos>', None, None, '<eos>'])\n\n    def test_targetfield_specials(self):\n        test_path = os.path.dirname(os.path.realpath(__file__))\n        data_path = os.path.join(test_path, 'data/eng-fra.txt')\n        field = TargetField()\n        train = torchtext.data.TabularDataset(\n            path=data_path, format='tsv',\n            fields=[('src', torchtext.data.Field()), ('trg', field)]\n        )\n        self.assertTrue(field.sos_id is None)\n        self.assertTrue(field.eos_id is None)\n        field.build_vocab(train)\n        self.assertFalse(field.sos_id is None)\n        self.assertFalse(field.eos_id is None)\n"""
tests/test_loss_loss.py,9,"b'import math\nimport random\nimport unittest\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom seq2seq.loss.loss import Loss\nfrom seq2seq.loss import NLLLoss, Perplexity\n\nclass TestLoss(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        num_class = 5\n        batch_size = 5\n        num_batch = 10\n        cls.num_batch = num_batch\n        cls.outputs = [F.softmax(Variable(torch.randn(batch_size, num_class)), dim=1)\n                   for _ in range(num_batch)]\n        cls.targets = [Variable(torch.LongTensor([random.randint(0, num_class - 1)\n                                              for _ in range(batch_size)]))\n                   for _ in range(num_batch)]\n\n    def test_loss_init(self):\n        name = ""name""\n        loss = Loss(name, torch.nn.NLLLoss())\n        self.assertEqual(loss.name, name)\n\n    def test_loss_init_WITH_NON_LOSS(self):\n        self.assertRaises(ValueError, lambda: Loss(""name"", ""loss""))\n\n    def test_loss_backward_WITH_NO_LOSS(self):\n        loss = Loss(""name"", torch.nn.NLLLoss())\n        self.assertRaises(ValueError, lambda: loss.backward())\n\n    def test_nllloss_init(self):\n        loss = NLLLoss()\n        self.assertEqual(loss.name, NLLLoss._NAME)\n        self.assertTrue(type(loss.criterion) is torch.nn.NLLLoss)\n\n    def test_nllloss_init_WITH_MASK_BUT_NO_WEIGHT(self):\n        mask = 1\n        self.assertRaises(ValueError, lambda: NLLLoss(mask=mask))\n\n    def test_nllloss(self):\n        loss = NLLLoss()\n        pytorch_loss = 0\n        pytorch_criterion = torch.nn.NLLLoss()\n        for output, target in zip(self.outputs, self.targets):\n            loss.eval_batch(output, target)\n            pytorch_loss += pytorch_criterion(output, target)\n\n        loss_val = loss.get_loss()\n        pytorch_loss /= self.num_batch\n\n        self.assertAlmostEqual(loss_val, pytorch_loss.item())\n\n    def test_nllloss_WITH_OUT_SIZE_AVERAGE(self):\n        loss = NLLLoss(size_average=False)\n        pytorch_loss = 0\n        pytorch_criterion = torch.nn.NLLLoss(size_average=False)\n        for output, target in zip(self.outputs, self.targets):\n            loss.eval_batch(output, target)\n            pytorch_loss += pytorch_criterion(output, target)\n\n        loss_val = loss.get_loss()\n\n        self.assertAlmostEqual(loss_val, pytorch_loss.item())\n\n    def test_perplexity_init(self):\n        loss = Perplexity()\n        self.assertEqual(loss.name, Perplexity._NAME)\n\n    def test_perplexity(self):\n        nll = NLLLoss()\n        ppl = Perplexity()\n        for output, target in zip(self.outputs, self.targets):\n            nll.eval_batch(output, target)\n            ppl.eval_batch(output, target)\n\n        nll_loss = nll.get_loss()\n        ppl_loss = ppl.get_loss()\n\n        self.assertAlmostEqual(ppl_loss, math.exp(nll_loss))\n'"
tests/test_optim_optim.py,8,"b'import unittest\n\nimport torch\nfrom torch.optim.lr_scheduler import StepLR\nimport mock\n\nfrom seq2seq.optim import Optimizer\n\nclass TestOptimizer(unittest.TestCase):\n\n    def test_init(self):\n        params = [torch.nn.Parameter(torch.randn(2,3,4))]\n        try:\n            optimizer = Optimizer(torch.optim.Adam(params))\n        except:\n            self.fail(""__init__ failed."")\n\n        self.assertEquals(optimizer.max_grad_norm, 0)\n\n    def test_update(self):\n        params = [torch.nn.Parameter(torch.randn(2,3,4))]\n        optimizer = Optimizer(torch.optim.Adam(params, lr=1), max_grad_norm=5)\n        scheduler = StepLR(optimizer.optimizer, 1, gamma=0.1)\n        optimizer.set_scheduler(scheduler)\n        optimizer.update(10, 0)\n        optimizer.update(10, 1)\n        self.assertEquals(optimizer.optimizer.param_groups[0][\'lr\'], 0.1)\n\n    @mock.patch(""torch.nn.utils.clip_grad_norm_"")\n    def test_step(self, mock_clip_grad_norm):\n        params = [torch.nn.Parameter(torch.randn(2,3,4))]\n        optim = Optimizer(torch.optim.Adam(params),\n                          max_grad_norm=5)\n        optim.step()\n        mock_clip_grad_norm.assert_called_once()\n'"
tests/test_predictor.py,0,"b'import os\nimport unittest\n\nimport torchtext\n\nfrom seq2seq.evaluator import Predictor\nfrom seq2seq.dataset import SourceField, TargetField\nfrom seq2seq.models import Seq2seq, EncoderRNN, DecoderRNN\n\nclass TestPredictor(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(self):\n        test_path = os.path.dirname(os.path.realpath(__file__))\n        src = SourceField()\n        trg = TargetField()\n        dataset = torchtext.data.TabularDataset(\n            path=os.path.join(test_path, \'data/eng-fra.txt\'), format=\'tsv\',\n            fields=[(\'src\', src), (\'trg\', trg)],\n        )\n        src.build_vocab(dataset)\n        trg.build_vocab(dataset)\n\n        encoder = EncoderRNN(len(src.vocab), 10, 10, rnn_cell=\'lstm\')\n        decoder = DecoderRNN(len(trg.vocab), 10, 10, trg.sos_id, trg.eos_id, rnn_cell=\'lstm\')\n        seq2seq = Seq2seq(encoder, decoder)\n        self.predictor = Predictor(seq2seq, src.vocab, trg.vocab)\n\n    def test_predict(self):\n        src_seq = [""I"", ""am"", ""fat""]\n        tgt_seq = self.predictor.predict(src_seq)\n        for tok in tgt_seq:\n            self.assertTrue(tok in self.predictor.tgt_vocab.stoi)\n'"
tests/test_seq2seq.py,0,b'import unittest\n\nclass TestSeq2seq(unittest.TestCase):\n    pass\n'
tests/test_supervised_trainer.py,0,"b""import unittest\nimport os\n\nimport mock\nimport torchtext\n\nfrom seq2seq.dataset import SourceField, TargetField\nfrom seq2seq.trainer import SupervisedTrainer\n\nclass TestSupervisedTrainer(unittest.TestCase):\n\n    def setUp(self):\n        test_path = os.path.dirname(os.path.realpath(__file__))\n        src = SourceField()\n        tgt = TargetField()\n        self.dataset = torchtext.data.TabularDataset(\n            path=os.path.join(test_path, 'data/eng-fra.txt'), format='tsv',\n            fields=[('src', src), ('tgt', tgt)],\n        )\n        src.build_vocab(self.dataset)\n        tgt.build_vocab(self.dataset)\n\n    @mock.patch('seq2seq.trainer.SupervisedTrainer._train_batch', return_value=0)\n    @mock.patch('seq2seq.util.checkpoint.Checkpoint.save')\n    def test_batch_num_when_resuming(self, mock_checkpoint, mock_func):\n        mock_model = mock.Mock()\n        mock_optim = mock.Mock()\n\n        trainer = SupervisedTrainer(batch_size=16)\n        trainer.optimizer = mock_optim\n        n_epoches = 1\n        start_epoch = 1\n        steps_per_epoch = 7\n        step = 3\n        trainer._train_epoches(self.dataset, mock_model, n_epoches, start_epoch, step)\n\n        self.assertEqual(steps_per_epoch - step, mock_func.call_count)\n\n    @mock.patch('seq2seq.trainer.SupervisedTrainer._train_batch', return_value=0)\n    @mock.patch('seq2seq.util.checkpoint.Checkpoint.save')\n    def test_resume_from_multiple_of_epoches(self, mock_checkpoint, mock_func):\n        mock_model = mock.Mock()\n        mock_optim = mock.Mock()\n\n        trainer = SupervisedTrainer(batch_size=16)\n        trainer.optimizer = mock_optim\n        n_epoches = 1\n        start_epoch = 1\n        step = 7\n        trainer._train_epoches(self.dataset, mock_model, n_epoches, start_epoch, step)\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/test_topkdecoder.py,4,"b'import unittest\n\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom seq2seq.models import DecoderRNN, TopKDecoder\n\nclass TestDecoderRNN(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(self):\n        self.vocab_size = 3\n\n    def test_init(self):\n        decoder = DecoderRNN(self.vocab_size, 50, 16, 0, 1, input_dropout_p=0)\n        TopKDecoder(decoder, 3)\n\n    def test_k_1(self):\n        """""" When k=1, the output of topk decoder should be the same as a normal decoder. """"""\n        batch_size = 1\n        eos = 1\n\n        for _ in range(10):\n            # Repeat the randomized test multiple times\n            decoder = DecoderRNN(self.vocab_size, 50, 16, 0, eos)\n            for param in decoder.parameters():\n                param.data.uniform_(-1, 1)\n            topk_decoder = TopKDecoder(decoder, 1)\n\n            output, _, other = decoder()\n            output_topk, _, other_topk = topk_decoder()\n\n            self.assertEqual(len(output), len(output_topk))\n\n            finished = [False] * batch_size\n            seq_scores = [0] * batch_size\n\n            for t_step, t_output in enumerate(output):\n                score, _ = t_output.topk(1)\n                symbols = other[\'sequence\'][t_step]\n                for b in range(batch_size):\n                    seq_scores[b] += score[b].data[0]\n                    symbol = symbols[b].data[0]\n                    if not finished[b] and symbol == eos:\n                        finished[b] = True\n                        self.assertEqual(other_topk[\'length\'][b], t_step + 1)\n                        self.assertTrue(np.isclose(seq_scores[b], other_topk[\'score\'][b][0]))\n                    if not finished[b]:\n                        symbol_topk = other_topk[\'topk_sequence\'][t_step][b].data[0][0]\n                        self.assertEqual(symbol, symbol_topk)\n                        self.assertTrue(torch.equal(t_output.data, output_topk[t_step].data))\n                if sum(finished) == batch_size:\n                    break\n\n    def test_k_greater_then_1(self):\n        """""" Implement beam search manually and compare results from topk decoder. """"""\n        max_len = 50\n        beam_size = 3\n        batch_size = 1\n        hidden_size = 8\n        sos = 0\n        eos = 1\n\n        for _ in range(10):\n            decoder = DecoderRNN(self.vocab_size, max_len, hidden_size, sos, eos)\n            for param in decoder.parameters():\n                param.data.uniform_(-1, 1)\n            topk_decoder = TopKDecoder(decoder, beam_size)\n\n            encoder_hidden = torch.autograd.Variable(torch.randn(1, batch_size, hidden_size))\n            _, _, other_topk = topk_decoder(encoder_hidden=encoder_hidden)\n\n            # Queue state:\n            #   1. time step\n            #   2. symbol\n            #   3. hidden state\n            #   4. accumulated log likelihood\n            #   5. beam number\n            batch_queue = [[(-1, sos, encoder_hidden[:,b,:].unsqueeze(1), 0, None)] for b in range(batch_size)]\n            time_batch_queue = [batch_queue]\n            batch_finished_seqs = [list() for _ in range(batch_size)]\n            for t in range(max_len):\n                new_batch_queue = []\n                for b in range(batch_size):\n                    new_queue = []\n                    for k in range(min(len(time_batch_queue[t][b]), beam_size)):\n                        _, inputs, hidden, seq_score, _ = time_batch_queue[t][b][k]\n                        if inputs == eos:\n                            batch_finished_seqs[b].append(time_batch_queue[t][b][k])\n                            continue\n                        inputs = torch.autograd.Variable(torch.LongTensor([[inputs]]))\n                        decoder_outputs, hidden, _ = decoder.forward_step(inputs, hidden, None, F.log_softmax)\n                        topk_score, topk = decoder_outputs[0].data.topk(beam_size)\n                        for score, sym in zip(topk_score.tolist()[0], topk.tolist()[0]):\n                            new_queue.append((t, sym, hidden, score + seq_score, k))\n                    new_queue = sorted(new_queue, key=lambda x: x[3], reverse=True)[:beam_size]\n                    new_batch_queue.append(new_queue)\n                time_batch_queue.append(new_batch_queue)\n\n            # finished beams\n            finalist = [l[:beam_size] for l in batch_finished_seqs]\n            # unfinished beams\n            for b in range(batch_size):\n                if len(finalist[b]) < beam_size:\n                    last_step = sorted(time_batch_queue[-1][b], key=lambda x: x[3], reverse=True)\n                    finalist[b] += last_step[:beam_size - len(finalist[b])]\n\n            # back track\n            topk = []\n            for b in range(batch_size):\n                batch_topk = []\n                for k in range(beam_size):\n                    seq = [finalist[b][k]]\n                    prev_k = seq[-1][4]\n                    prev_t = seq[-1][0]\n                    while prev_k is not None:\n                        seq.append(time_batch_queue[prev_t][b][prev_k])\n                        prev_k = seq[-1][4]\n                        prev_t = seq[-1][0]\n                    batch_topk.append([s for s in reversed(seq)])\n                topk.append(batch_topk)\n\n            for b in range(batch_size):\n                topk[b] = sorted(topk[b], key=lambda s: s[-1][3], reverse=True)\n\n            topk_scores = other_topk[\'score\']\n            topk_lengths = other_topk[\'topk_length\']\n            topk_pred_symbols = other_topk[\'topk_sequence\']\n            for b in range(batch_size):\n                precision_error = False\n                for k in range(beam_size - 1):\n                    if np.isclose(topk_scores[b][k], topk_scores[b][k+1]):\n                        precision_error = True\n                        break\n                if precision_error:\n                    break\n                for k in range(beam_size):\n                    self.assertEqual(topk_lengths[b][k], len(topk[b][k]) - 1)\n                    self.assertTrue(np.isclose(topk_scores[b][k], topk[b][k][-1][3]))\n                    total_steps = topk_lengths[b][k]\n                    for t in range(total_steps):\n                        self.assertEqual(topk_pred_symbols[t][b, k].data[0], topk[b][k][t+1][1]) # topk includes SOS\n'"
docs/source/conf.py,1,"b'# -*- coding: utf-8 -*-\n#\n# pytorch-seq2seq documentation build configuration file, created by\n# sphinx-quickstart on Tue Jun 27 14:35:26 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'../../seq2seq/\'))\nimport seq2seq\nimport sphinx_rtd_theme\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.viewcode\']\n\nnapoleon_use_ivar = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# Source parsers\nsource_parsers = {\n    \'.md\': \'recommonmark.parser.CommonMarkParser\'\n}\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = [\'.rst\', \'.md\']\n# source_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'pytorch-seq2seq\'\ncopyright = u\'2017, pytorch-seq2seq Contritors\'\nauthor = u\'pytorch-seq2seq Contributors\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = u\'0.1.6\'\n# The full version, including alpha/beta/rc tags.\nrelease = u\'0.1.6\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\n# html_static_path = [\'_static\']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'PyTorch-seq2seqdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'pytorch-seq2seq.tex\', u\'pytorch-seq2seq Documentation\',\n     u\'Kyle Gao, Kshitij Fadnis, Avinash Balakrishnan, Ryan Musa, Tejaswini Pedapati\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'pytorch-seq2seq\', u\'pytorch-seq2seq Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'pytorch-seq2seq\', u\'pytorch-seq2seq Documentation\',\n     author, \'pytorch-seq2seq\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/\', None),\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy/\', None),\n    \'PyTorch\': (\'http://pytorch.org/docs/master/\', None),\n}\n'"
seq2seq/dataset/__init__.py,0,"b'from .fields import SourceField, TargetField\n'"
seq2seq/dataset/fields.py,0,"b'import logging\n\nimport torchtext\n\nclass SourceField(torchtext.data.Field):\n    """""" Wrapper class of torchtext.data.Field that forces batch_first and include_lengths to be True. """"""\n\n    def __init__(self, **kwargs):\n        logger = logging.getLogger(__name__)\n\n        if kwargs.get(\'batch_first\') is False:\n            logger.warning(""Option batch_first has to be set to use pytorch-seq2seq.  Changed to True."")\n        kwargs[\'batch_first\'] = True\n        if kwargs.get(\'include_lengths\') is False:\n            logger.warning(""Option include_lengths has to be set to use pytorch-seq2seq.  Changed to True."")\n        kwargs[\'include_lengths\'] = True\n\n        super(SourceField, self).__init__(**kwargs)\n\nclass TargetField(torchtext.data.Field):\n    """""" Wrapper class of torchtext.data.Field that forces batch_first to be True and prepend <sos> and append <eos> to sequences in preprocessing step.\n\n    Attributes:\n        sos_id: index of the start of sentence symbol\n        eos_id: index of the end of sentence symbol\n    """"""\n\n    SYM_SOS = \'<sos>\'\n    SYM_EOS = \'<eos>\'\n\n    def __init__(self, **kwargs):\n        logger = logging.getLogger(__name__)\n\n        if kwargs.get(\'batch_first\') == False:\n            logger.warning(""Option batch_first has to be set to use pytorch-seq2seq.  Changed to True."")\n        kwargs[\'batch_first\'] = True\n        if kwargs.get(\'preprocessing\') is None:\n            kwargs[\'preprocessing\'] = lambda seq: [self.SYM_SOS] + seq + [self.SYM_EOS]\n        else:\n            func = kwargs[\'preprocessing\']\n            kwargs[\'preprocessing\'] = lambda seq: [self.SYM_SOS] + func(seq) + [self.SYM_EOS]\n\n        self.sos_id = None\n        self.eos_id = None\n        super(TargetField, self).__init__(**kwargs)\n\n    def build_vocab(self, *args, **kwargs):\n        super(TargetField, self).build_vocab(*args, **kwargs)\n        self.sos_id = self.vocab.stoi[self.SYM_SOS]\n        self.eos_id = self.vocab.stoi[self.SYM_EOS]\n'"
seq2seq/evaluator/__init__.py,0,b'from .evaluator import Evaluator\nfrom .predictor import Predictor\n'
seq2seq/evaluator/evaluator.py,2,"b'from __future__ import print_function, division\n\nimport torch\nimport torchtext\n\nimport seq2seq\nfrom seq2seq.loss import NLLLoss\n\nclass Evaluator(object):\n    """""" Class to evaluate models with given datasets.\n\n    Args:\n        loss (seq2seq.loss, optional): loss for evaluator (default: seq2seq.loss.NLLLoss)\n        batch_size (int, optional): batch size for evaluator (default: 64)\n    """"""\n\n    def __init__(self, loss=NLLLoss(), batch_size=64):\n        self.loss = loss\n        self.batch_size = batch_size\n\n    def evaluate(self, model, data):\n        """""" Evaluate a model on given dataset and return performance.\n\n        Args:\n            model (seq2seq.models): model to evaluate\n            data (seq2seq.dataset.dataset.Dataset): dataset to evaluate against\n\n        Returns:\n            loss (float): loss of the given model on the given dataset\n        """"""\n        model.eval()\n\n        loss = self.loss\n        loss.reset()\n        match = 0\n        total = 0\n\n        device = None if torch.cuda.is_available() else -1\n        batch_iterator = torchtext.data.BucketIterator(\n            dataset=data, batch_size=self.batch_size,\n            sort=True, sort_key=lambda x: len(x.src),\n            device=device, train=False)\n        tgt_vocab = data.fields[seq2seq.tgt_field_name].vocab\n        pad = tgt_vocab.stoi[data.fields[seq2seq.tgt_field_name].pad_token]\n\n        with torch.no_grad():\n            for batch in batch_iterator:\n                input_variables, input_lengths  = getattr(batch, seq2seq.src_field_name)\n                target_variables = getattr(batch, seq2seq.tgt_field_name)\n\n                decoder_outputs, decoder_hidden, other = model(input_variables, input_lengths.tolist(), target_variables)\n\n                # Evaluation\n                seqlist = other[\'sequence\']\n                for step, step_output in enumerate(decoder_outputs):\n                    target = target_variables[:, step + 1]\n                    loss.eval_batch(step_output.view(target_variables.size(0), -1), target)\n\n                    non_padding = target.ne(pad)\n                    correct = seqlist[step].view(-1).eq(target).masked_select(non_padding).sum().item()\n                    match += correct\n                    total += non_padding.sum().item()\n\n        if total == 0:\n            accuracy = float(\'nan\')\n        else:\n            accuracy = match / total\n\n        return loss.get_loss(), accuracy\n'"
seq2seq/evaluator/predictor.py,5,"b'import torch\nfrom torch.autograd import Variable\n\n\nclass Predictor(object):\n\n    def __init__(self, model, src_vocab, tgt_vocab):\n        """"""\n        Predictor class to evaluate for a given model.\n        Args:\n            model (seq2seq.models): trained model. This can be loaded from a checkpoint\n                using `seq2seq.util.checkpoint.load`\n            src_vocab (seq2seq.dataset.vocabulary.Vocabulary): source sequence vocabulary\n            tgt_vocab (seq2seq.dataset.vocabulary.Vocabulary): target sequence vocabulary\n        """"""\n        if torch.cuda.is_available():\n            self.model = model.cuda()\n        else:\n            self.model = model.cpu()\n        self.model.eval()\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n\n    def get_decoder_features(self, src_seq):\n        src_id_seq = torch.LongTensor([self.src_vocab.stoi[tok] for tok in src_seq]).view(1, -1)\n        if torch.cuda.is_available():\n            src_id_seq = src_id_seq.cuda()\n\n        with torch.no_grad():\n            softmax_list, _, other = self.model(src_id_seq, [len(src_seq)])\n\n        return other\n\n    def predict(self, src_seq):\n        """""" Make prediction given `src_seq` as input.\n\n        Args:\n            src_seq (list): list of tokens in source language\n\n        Returns:\n            tgt_seq (list): list of tokens in target language as predicted\n            by the pre-trained model\n        """"""\n        other = self.get_decoder_features(src_seq)\n\n        length = other[\'length\'][0]\n\n        tgt_id_seq = [other[\'sequence\'][di][0].data[0] for di in range(length)]\n        tgt_seq = [self.tgt_vocab.itos[tok] for tok in tgt_id_seq]\n        return tgt_seq\n\n    def predict_n(self, src_seq, n=1):\n        """""" Make \'n\' predictions given `src_seq` as input.\n\n        Args:\n            src_seq (list): list of tokens in source language\n            n (int): number of predicted seqs to return. If None,\n                     it will return just one seq.\n\n        Returns:\n            tgt_seq (list): list of tokens in target language as predicted\n                            by the pre-trained model\n        """"""\n        other = self.get_decoder_features(src_seq)\n\n        result = []\n        for x in range(0, int(n)):\n            length = other[\'topk_length\'][0][x]\n            tgt_id_seq = [other[\'topk_sequence\'][di][0, x, 0].data[0] for di in range(length)]\n            tgt_seq = [self.tgt_vocab.itos[tok] for tok in tgt_id_seq]\n            result.append(tgt_seq)\n\n        return result\n'"
seq2seq/loss/__init__.py,0,"b'from .loss import NLLLoss, Perplexity\n'"
seq2seq/loss/loss.py,12,"b'from __future__ import print_function\nimport math\nimport torch.nn as nn\nimport numpy as np\n\nclass Loss(object):\n    """""" Base class for encapsulation of the loss functions.\n\n    This class defines interfaces that are commonly used with loss functions\n    in training and inferencing.  For information regarding individual loss\n    functions, please refer to http://pytorch.org/docs/master/nn.html#loss-functions\n\n    Note:\n        Do not use this class directly, use one of the sub classes.\n\n    Args:\n        name (str): name of the loss function used by logging messages.\n        criterion (torch.nn._Loss): one of PyTorch\'s loss function.  Refer\n            to http://pytorch.org/docs/master/nn.html#loss-functions for\n            a list of them.\n\n    Attributes:\n        name (str): name of the loss function used by logging messages.\n        criterion (torch.nn._Loss): one of PyTorch\'s loss function.  Refer\n            to http://pytorch.org/docs/master/nn.html#loss-functions for\n            a list of them.  Implementation depends on individual\n            sub-classes.\n        acc_loss (int or torcn.nn.Tensor): variable that stores accumulated loss.\n        norm_term (float): normalization term that can be used to calculate\n            the loss of multiple batches.  Implementation depends on individual\n            sub-classes.\n    """"""\n\n    def __init__(self, name, criterion):\n        self.name = name\n        self.criterion = criterion\n        if not issubclass(type(self.criterion), nn.modules.loss._Loss):\n            raise ValueError(""Criterion has to be a subclass of torch.nn._Loss"")\n        # accumulated loss\n        self.acc_loss = 0\n        # normalization term\n        self.norm_term = 0\n\n    def reset(self):\n        """""" Reset the accumulated loss. """"""\n        self.acc_loss = 0\n        self.norm_term = 0\n\n    def get_loss(self):\n        """""" Get the loss.\n\n        This method defines how to calculate the averaged loss given the\n        accumulated loss and the normalization term.  Override to define your\n        own logic.\n\n        Returns:\n            loss (float): value of the loss.\n        """"""\n        raise NotImplementedError\n\n    def eval_batch(self, outputs, target):\n        """""" Evaluate and accumulate loss given outputs and expected results.\n\n        This method is called after each batch with the batch outputs and\n        the target (expected) results.  The loss and normalization term are\n        accumulated in this method.  Override it to define your own accumulation\n        method.\n\n        Args:\n            outputs (torch.Tensor): outputs of a batch.\n            target (torch.Tensor): expected output of a batch.\n        """"""\n        raise NotImplementedError\n\n    def cuda(self):\n        self.criterion.cuda()\n\n    def backward(self):\n        if type(self.acc_loss) is int:\n            raise ValueError(""No loss to back propagate."")\n        self.acc_loss.backward()\n\nclass NLLLoss(Loss):\n    """""" Batch averaged negative log-likelihood loss.\n\n    Args:\n        weight (torch.Tensor, optional): refer to http://pytorch.org/docs/master/nn.html#nllloss\n        mask (int, optional): index of masked token, i.e. weight[mask] = 0.\n        size_average (bool, optional): refer to http://pytorch.org/docs/master/nn.html#nllloss\n    """"""\n\n    _NAME = ""Avg NLLLoss""\n\n    def __init__(self, weight=None, mask=None, size_average=True):\n        self.mask = mask\n        self.size_average = size_average\n        if mask is not None:\n            if weight is None:\n                raise ValueError(""Must provide weight with a mask."")\n            weight[mask] = 0\n\n        super(NLLLoss, self).__init__(\n            self._NAME,\n            nn.NLLLoss(weight=weight, size_average=size_average))\n\n    def get_loss(self):\n        if isinstance(self.acc_loss, int):\n            return 0\n        # total loss for all batches\n        loss = self.acc_loss.data.item()\n        if self.size_average:\n            # average loss per batch\n            loss /= self.norm_term\n        return loss\n\n    def eval_batch(self, outputs, target):\n        self.acc_loss += self.criterion(outputs, target)\n        self.norm_term += 1\n\nclass Perplexity(NLLLoss):\n    """""" Language model perplexity loss.\n\n    Perplexity is the token averaged likelihood.  When the averaging options are the\n    same, it is the exponential of negative log-likelihood.\n\n    Args:\n        weight (torch.Tensor, optional): refer to http://pytorch.org/docs/master/nn.html#nllloss\n        mask (int, optional): index of masked token, i.e. weight[mask] = 0.\n    """"""\n\n    _NAME = ""Perplexity""\n    _MAX_EXP = 100\n\n    def __init__(self, weight=None, mask=None):\n        super(Perplexity, self).__init__(weight=weight, mask=mask, size_average=False)\n\n    def eval_batch(self, outputs, target):\n        self.acc_loss += self.criterion(outputs, target)\n        if self.mask is None:\n            self.norm_term += np.prod(target.size())\n        else:\n            self.norm_term += target.data.ne(self.mask).sum()\n\n    def get_loss(self):\n        nll = super(Perplexity, self).get_loss()\n        nll /= self.norm_term.item()\n        if nll > Perplexity._MAX_EXP:\n            print(""WARNING: Loss exceeded maximum value, capping to e^100"")\n            return math.exp(Perplexity._MAX_EXP)\n        return math.exp(nll)\n'"
seq2seq/models/DecoderRNN.py,10,"b'import random\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nfrom .attention import Attention\nfrom .baseRNN import BaseRNN\n\nif torch.cuda.is_available():\n    import torch.cuda as device\nelse:\n    import torch as device\n\n\nclass DecoderRNN(BaseRNN):\n    r""""""\n    Provides functionality for decoding in a seq2seq framework, with an option for attention.\n\n    Args:\n        vocab_size (int): size of the vocabulary\n        max_len (int): a maximum allowed length for the sequence to be processed\n        hidden_size (int): the number of features in the hidden state `h`\n        sos_id (int): index of the start of sentence symbol\n        eos_id (int): index of the end of sentence symbol\n        n_layers (int, optional): number of recurrent layers (default: 1)\n        rnn_cell (str, optional): type of RNN cell (default: gru)\n        bidirectional (bool, optional): if the encoder is bidirectional (default False)\n        input_dropout_p (float, optional): dropout probability for the input sequence (default: 0)\n        dropout_p (float, optional): dropout probability for the output sequence (default: 0)\n        use_attention(bool, optional): flag indication whether to use attention mechanism or not (default: false)\n\n    Attributes:\n        KEY_ATTN_SCORE (str): key used to indicate attention weights in `ret_dict`\n        KEY_LENGTH (str): key used to indicate a list representing lengths of output sequences in `ret_dict`\n        KEY_SEQUENCE (str): key used to indicate a list of sequences in `ret_dict`\n\n    Inputs: inputs, encoder_hidden, encoder_outputs, function, teacher_forcing_ratio\n        - **inputs** (batch, seq_len, input_size): list of sequences, whose length is the batch size and within which\n          each sequence is a list of token IDs.  It is used for teacher forcing when provided. (default `None`)\n        - **encoder_hidden** (num_layers * num_directions, batch_size, hidden_size): tensor containing the features in the\n          hidden state `h` of encoder. Used as the initial hidden state of the decoder. (default `None`)\n        - **encoder_outputs** (batch, seq_len, hidden_size): tensor with containing the outputs of the encoder.\n          Used for attention mechanism (default is `None`).\n        - **function** (torch.nn.Module): A function used to generate symbols from RNN hidden state\n          (default is `torch.nn.functional.log_softmax`).\n        - **teacher_forcing_ratio** (float): The probability that teacher forcing will be used. A random number is\n          drawn uniformly from 0-1 for every decoding token, and if the sample is smaller than the given value,\n          teacher forcing would be used (default is 0).\n\n    Outputs: decoder_outputs, decoder_hidden, ret_dict\n        - **decoder_outputs** (seq_len, batch, vocab_size): list of tensors with size (batch_size, vocab_size) containing\n          the outputs of the decoding function.\n        - **decoder_hidden** (num_layers * num_directions, batch, hidden_size): tensor containing the last hidden\n          state of the decoder.\n        - **ret_dict**: dictionary containing additional information as follows {*KEY_LENGTH* : list of integers\n          representing lengths of output sequences, *KEY_SEQUENCE* : list of sequences, where each sequence is a list of\n          predicted token IDs }.\n    """"""\n\n    KEY_ATTN_SCORE = \'attention_score\'\n    KEY_LENGTH = \'length\'\n    KEY_SEQUENCE = \'sequence\'\n\n    def __init__(self, vocab_size, max_len, hidden_size,\n            sos_id, eos_id,\n            n_layers=1, rnn_cell=\'gru\', bidirectional=False,\n            input_dropout_p=0, dropout_p=0, use_attention=False):\n        super(DecoderRNN, self).__init__(vocab_size, max_len, hidden_size,\n                input_dropout_p, dropout_p,\n                n_layers, rnn_cell)\n\n        self.bidirectional_encoder = bidirectional\n        self.rnn = self.rnn_cell(hidden_size, hidden_size, n_layers, batch_first=True, dropout=dropout_p)\n\n        self.output_size = vocab_size\n        self.max_length = max_len\n        self.use_attention = use_attention\n        self.eos_id = eos_id\n        self.sos_id = sos_id\n\n        self.init_input = None\n\n        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n        if use_attention:\n            self.attention = Attention(self.hidden_size)\n\n        self.out = nn.Linear(self.hidden_size, self.output_size)\n\n    def forward_step(self, input_var, hidden, encoder_outputs, function):\n        batch_size = input_var.size(0)\n        output_size = input_var.size(1)\n        embedded = self.embedding(input_var)\n        embedded = self.input_dropout(embedded)\n\n        output, hidden = self.rnn(embedded, hidden)\n\n        attn = None\n        if self.use_attention:\n            output, attn = self.attention(output, encoder_outputs)\n\n        predicted_softmax = function(self.out(output.contiguous().view(-1, self.hidden_size)), dim=1).view(batch_size, output_size, -1)\n        return predicted_softmax, hidden, attn\n\n    def forward(self, inputs=None, encoder_hidden=None, encoder_outputs=None,\n                    function=F.log_softmax, teacher_forcing_ratio=0):\n        ret_dict = dict()\n        if self.use_attention:\n            ret_dict[DecoderRNN.KEY_ATTN_SCORE] = list()\n\n        inputs, batch_size, max_length = self._validate_args(inputs, encoder_hidden, encoder_outputs,\n                                                             function, teacher_forcing_ratio)\n        decoder_hidden = self._init_state(encoder_hidden)\n\n        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n        decoder_outputs = []\n        sequence_symbols = []\n        lengths = np.array([max_length] * batch_size)\n\n        def decode(step, step_output, step_attn):\n            decoder_outputs.append(step_output)\n            if self.use_attention:\n                ret_dict[DecoderRNN.KEY_ATTN_SCORE].append(step_attn)\n            symbols = decoder_outputs[-1].topk(1)[1]\n            sequence_symbols.append(symbols)\n\n            eos_batches = symbols.data.eq(self.eos_id)\n            if eos_batches.dim() > 0:\n                eos_batches = eos_batches.cpu().view(-1).numpy()\n                update_idx = ((lengths > step) & eos_batches) != 0\n                lengths[update_idx] = len(sequence_symbols)\n            return symbols\n\n        # Manual unrolling is used to support random teacher forcing.\n        # If teacher_forcing_ratio is True or False instead of a probability, the unrolling can be done in graph\n        if use_teacher_forcing:\n            decoder_input = inputs[:, :-1]\n            decoder_output, decoder_hidden, attn = self.forward_step(decoder_input, decoder_hidden, encoder_outputs,\n                                                                     function=function)\n\n            for di in range(decoder_output.size(1)):\n                step_output = decoder_output[:, di, :]\n                if attn is not None:\n                    step_attn = attn[:, di, :]\n                else:\n                    step_attn = None\n                decode(di, step_output, step_attn)\n        else:\n            decoder_input = inputs[:, 0].unsqueeze(1)\n            for di in range(max_length):\n                decoder_output, decoder_hidden, step_attn = self.forward_step(decoder_input, decoder_hidden, encoder_outputs,\n                                                                         function=function)\n                step_output = decoder_output.squeeze(1)\n                symbols = decode(di, step_output, step_attn)\n                decoder_input = symbols\n\n        ret_dict[DecoderRNN.KEY_SEQUENCE] = sequence_symbols\n        ret_dict[DecoderRNN.KEY_LENGTH] = lengths.tolist()\n\n        return decoder_outputs, decoder_hidden, ret_dict\n\n    def _init_state(self, encoder_hidden):\n        """""" Initialize the encoder hidden state. """"""\n        if encoder_hidden is None:\n            return None\n        if isinstance(encoder_hidden, tuple):\n            encoder_hidden = tuple([self._cat_directions(h) for h in encoder_hidden])\n        else:\n            encoder_hidden = self._cat_directions(encoder_hidden)\n        return encoder_hidden\n\n    def _cat_directions(self, h):\n        """""" If the encoder is bidirectional, do the following transformation.\n            (#directions * #layers, #batch, hidden_size) -> (#layers, #batch, #directions * hidden_size)\n        """"""\n        if self.bidirectional_encoder:\n            h = torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)\n        return h\n\n    def _validate_args(self, inputs, encoder_hidden, encoder_outputs, function, teacher_forcing_ratio):\n        if self.use_attention:\n            if encoder_outputs is None:\n                raise ValueError(""Argument encoder_outputs cannot be None when attention is used."")\n\n        # inference batch size\n        if inputs is None and encoder_hidden is None:\n            batch_size = 1\n        else:\n            if inputs is not None:\n                batch_size = inputs.size(0)\n            else:\n                if self.rnn_cell is nn.LSTM:\n                    batch_size = encoder_hidden[0].size(1)\n                elif self.rnn_cell is nn.GRU:\n                    batch_size = encoder_hidden.size(1)\n\n        # set default input and max decoding length\n        if inputs is None:\n            if teacher_forcing_ratio > 0:\n                raise ValueError(""Teacher forcing has to be disabled (set 0) when no inputs is provided."")\n            inputs = torch.LongTensor([self.sos_id] * batch_size).view(batch_size, 1)\n            if torch.cuda.is_available():\n                inputs = inputs.cuda()\n            max_length = self.max_length\n        else:\n            max_length = inputs.size(1) - 1 # minus the start of sequence symbol\n\n        return inputs, batch_size, max_length\n'"
seq2seq/models/EncoderRNN.py,2,"b'import torch.nn as nn\n\nfrom .baseRNN import BaseRNN\n\nclass EncoderRNN(BaseRNN):\n    r""""""\n    Applies a multi-layer RNN to an input sequence.\n\n    Args:\n        vocab_size (int): size of the vocabulary\n        max_len (int): a maximum allowed length for the sequence to be processed\n        hidden_size (int): the number of features in the hidden state `h`\n        input_dropout_p (float, optional): dropout probability for the input sequence (default: 0)\n        dropout_p (float, optional): dropout probability for the output sequence (default: 0)\n        n_layers (int, optional): number of recurrent layers (default: 1)\n        bidirectional (bool, optional): if True, becomes a bidirectional encodr (defulat False)\n        rnn_cell (str, optional): type of RNN cell (default: gru)\n        variable_lengths (bool, optional): if use variable length RNN (default: False)\n        embedding (torch.Tensor, optional): Pre-trained embedding.  The size of the tensor has to match\n            the size of the embedding parameter: (vocab_size, hidden_size).  The embedding layer would be initialized\n            with the tensor if provided (default: None).\n        update_embedding (bool, optional): If the embedding should be updated during training (default: False).\n\n    Inputs: inputs, input_lengths\n        - **inputs**: list of sequences, whose length is the batch size and within which each sequence is a list of token IDs.\n        - **input_lengths** (list of int, optional): list that contains the lengths of sequences\n            in the mini-batch, it must be provided when using variable length RNN (default: `None`)\n\n    Outputs: output, hidden\n        - **output** (batch, seq_len, hidden_size): tensor containing the encoded features of the input sequence\n        - **hidden** (num_layers * num_directions, batch, hidden_size): tensor containing the features in the hidden state `h`\n\n    Examples::\n\n         >>> encoder = EncoderRNN(input_vocab, max_seq_length, hidden_size)\n         >>> output, hidden = encoder(input)\n\n    """"""\n\n    def __init__(self, vocab_size, max_len, hidden_size,\n                 input_dropout_p=0, dropout_p=0,\n                 n_layers=1, bidirectional=False, rnn_cell=\'gru\', variable_lengths=False,\n                 embedding=None, update_embedding=True):\n        super(EncoderRNN, self).__init__(vocab_size, max_len, hidden_size,\n                input_dropout_p, dropout_p, n_layers, rnn_cell)\n\n        self.variable_lengths = variable_lengths\n        self.embedding = nn.Embedding(vocab_size, hidden_size)\n        if embedding is not None:\n            self.embedding.weight = nn.Parameter(embedding)\n        self.embedding.weight.requires_grad = update_embedding\n        self.rnn = self.rnn_cell(hidden_size, hidden_size, n_layers,\n                                 batch_first=True, bidirectional=bidirectional, dropout=dropout_p)\n\n    def forward(self, input_var, input_lengths=None):\n        """"""\n        Applies a multi-layer RNN to an input sequence.\n\n        Args:\n            input_var (batch, seq_len): tensor containing the features of the input sequence.\n            input_lengths (list of int, optional): A list that contains the lengths of sequences\n              in the mini-batch\n\n        Returns: output, hidden\n            - **output** (batch, seq_len, hidden_size): variable containing the encoded features of the input sequence\n            - **hidden** (num_layers * num_directions, batch, hidden_size): variable containing the features in the hidden state h\n        """"""\n        embedded = self.embedding(input_var)\n        embedded = self.input_dropout(embedded)\n        if self.variable_lengths:\n            embedded = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, batch_first=True)\n        output, hidden = self.rnn(embedded)\n        if self.variable_lengths:\n            output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n        return output, hidden\n'"
seq2seq/models/TopKDecoder.py,15,"b'import torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\ndef _inflate(tensor, times, dim):\n        """"""\n        Given a tensor, \'inflates\' it along the given dimension by replicating each slice specified number of times (in-place)\n\n        Args:\n            tensor: A :class:`Tensor` to inflate\n            times: number of repetitions\n            dim: axis for inflation (default=0)\n\n        Returns:\n            A :class:`Tensor`\n\n        Examples::\n            >> a = torch.LongTensor([[1, 2], [3, 4]])\n            >> a\n            1   2\n            3   4\n            [torch.LongTensor of size 2x2]\n            >> b = ._inflate(a, 2, dim=1)\n            >> b\n            1   2   1   2\n            3   4   3   4\n            [torch.LongTensor of size 2x4]\n            >> c = _inflate(a, 2, dim=0)\n            >> c\n            1   2\n            3   4\n            1   2\n            3   4\n            [torch.LongTensor of size 4x2]\n\n        """"""\n        repeat_dims = [1] * tensor.dim()\n        repeat_dims[dim] = times\n        return tensor.repeat(*repeat_dims)\n\nclass TopKDecoder(torch.nn.Module):\n    r""""""\n    Top-K decoding with beam search.\n\n    Args:\n        decoder_rnn (DecoderRNN): An object of DecoderRNN used for decoding.\n        k (int): Size of the beam.\n\n    Inputs: inputs, encoder_hidden, encoder_outputs, function, teacher_forcing_ratio\n        - **inputs** (seq_len, batch, input_size): list of sequences, whose length is the batch size and within which\n          each sequence is a list of token IDs.  It is used for teacher forcing when provided. (default is `None`)\n        - **encoder_hidden** (num_layers * num_directions, batch_size, hidden_size): tensor containing the features\n          in the hidden state `h` of encoder. Used as the initial hidden state of the decoder.\n        - **encoder_outputs** (batch, seq_len, hidden_size): tensor with containing the outputs of the encoder.\n          Used for attention mechanism (default is `None`).\n        - **function** (torch.nn.Module): A function used to generate symbols from RNN hidden state\n          (default is `torch.nn.functional.log_softmax`).\n        - **teacher_forcing_ratio** (float): The probability that teacher forcing will be used. A random number is\n          drawn uniformly from 0-1 for every decoding token, and if the sample is smaller than the given value,\n          teacher forcing would be used (default is 0).\n\n    Outputs: decoder_outputs, decoder_hidden, ret_dict\n        - **decoder_outputs** (batch): batch-length list of tensors with size (max_length, hidden_size) containing the\n          outputs of the decoder.\n        - **decoder_hidden** (num_layers * num_directions, batch, hidden_size): tensor containing the last hidden\n          state of the decoder.\n        - **ret_dict**: dictionary containing additional information as follows {*length* : list of integers\n          representing lengths of output sequences, *topk_length*: list of integers representing lengths of beam search\n          sequences, *sequence* : list of sequences, where each sequence is a list of predicted token IDs,\n          *topk_sequence* : list of beam search sequences, each beam is a list of token IDs, *inputs* : target\n          outputs if provided for decoding}.\n    """"""\n\n    def __init__(self, decoder_rnn, k):\n        super(TopKDecoder, self).__init__()\n        self.rnn = decoder_rnn\n        self.k = k\n        self.hidden_size = self.rnn.hidden_size\n        self.V = self.rnn.output_size\n        self.SOS = self.rnn.sos_id\n        self.EOS = self.rnn.eos_id\n\n    def forward(self, inputs=None, encoder_hidden=None, encoder_outputs=None, function=F.log_softmax,\n                    teacher_forcing_ratio=0, retain_output_probs=True):\n        """"""\n        Forward rnn for MAX_LENGTH steps.  Look at :func:`seq2seq.models.DecoderRNN.DecoderRNN.forward_rnn` for details.\n        """"""\n\n        inputs, batch_size, max_length = self.rnn._validate_args(inputs, encoder_hidden, encoder_outputs,\n                                                                 function, teacher_forcing_ratio)\n\n        self.pos_index = Variable(torch.LongTensor(range(batch_size)) * self.k).view(-1, 1)\n\n        # Inflate the initial hidden states to be of size: b*k x h\n        encoder_hidden = self.rnn._init_state(encoder_hidden)\n        if encoder_hidden is None:\n            hidden = None\n        else:\n            if isinstance(encoder_hidden, tuple):\n                hidden = tuple([_inflate(h, self.k, 1) for h in encoder_hidden])\n            else:\n                hidden = _inflate(encoder_hidden, self.k, 1)\n\n        # ... same idea for encoder_outputs and decoder_outputs\n        if self.rnn.use_attention:\n            inflated_encoder_outputs = _inflate(encoder_outputs, self.k, 0)\n        else:\n            inflated_encoder_outputs = None\n\n        # Initialize the scores; for the first step,\n        # ignore the inflated copies to avoid duplicate entries in the top k\n        sequence_scores = torch.Tensor(batch_size * self.k, 1)\n        sequence_scores.fill_(-float(\'Inf\'))\n        sequence_scores.index_fill_(0, torch.LongTensor([i * self.k for i in range(0, batch_size)]), 0.0)\n        sequence_scores = Variable(sequence_scores)\n\n        # Initialize the input vector\n        input_var = Variable(torch.transpose(torch.LongTensor([[self.SOS] * batch_size * self.k]), 0, 1))\n\n        # Store decisions for backtracking\n        stored_outputs = list()\n        stored_scores = list()\n        stored_predecessors = list()\n        stored_emitted_symbols = list()\n        stored_hidden = list()\n\n        for _ in range(0, max_length):\n\n            # Run the RNN one step forward\n            log_softmax_output, hidden, _ = self.rnn.forward_step(input_var, hidden,\n                                                                  inflated_encoder_outputs, function=function)\n\n            # If doing local backprop (e.g. supervised training), retain the output layer\n            if retain_output_probs:\n                stored_outputs.append(log_softmax_output)\n\n            # To get the full sequence scores for the new candidates, add the local scores for t_i to the predecessor scores for t_(i-1)\n            sequence_scores = _inflate(sequence_scores, self.V, 1)\n            sequence_scores += log_softmax_output.squeeze(1)\n            scores, candidates = sequence_scores.view(batch_size, -1).topk(self.k, dim=1)\n\n            # Reshape input = (bk, 1) and sequence_scores = (bk, 1)\n            input_var = (candidates % self.V).view(batch_size * self.k, 1)\n            sequence_scores = scores.view(batch_size * self.k, 1)\n\n            # Update fields for next timestep\n            predecessors = (candidates / self.V + self.pos_index.expand_as(candidates)).view(batch_size * self.k, 1)\n            if isinstance(hidden, tuple):\n                hidden = tuple([h.index_select(1, predecessors.squeeze()) for h in hidden])\n            else:\n                hidden = hidden.index_select(1, predecessors.squeeze())\n\n            # Update sequence scores and erase scores for end-of-sentence symbol so that they aren\'t expanded\n            stored_scores.append(sequence_scores.clone())\n            eos_indices = input_var.data.eq(self.EOS)\n            if eos_indices.nonzero().dim() > 0:\n                sequence_scores.data.masked_fill_(eos_indices, -float(\'inf\'))\n\n            # Cache results for backtracking\n            stored_predecessors.append(predecessors)\n            stored_emitted_symbols.append(input_var)\n            stored_hidden.append(hidden)\n\n        # Do backtracking to return the optimal values\n        output, h_t, h_n, s, l, p = self._backtrack(stored_outputs, stored_hidden,\n                                                    stored_predecessors, stored_emitted_symbols,\n                                                    stored_scores, batch_size, self.hidden_size)\n\n        # Build return objects\n        decoder_outputs = [step[:, 0, :] for step in output]\n        if isinstance(h_n, tuple):\n            decoder_hidden = tuple([h[:, :, 0, :] for h in h_n])\n        else:\n            decoder_hidden = h_n[:, :, 0, :]\n        metadata = {}\n        metadata[\'inputs\'] = inputs\n        metadata[\'output\'] = output\n        metadata[\'h_t\'] = h_t\n        metadata[\'score\'] = s\n        metadata[\'topk_length\'] = l\n        metadata[\'topk_sequence\'] = p\n        metadata[\'length\'] = [seq_len[0] for seq_len in l]\n        metadata[\'sequence\'] = [seq[0] for seq in p]\n        return decoder_outputs, decoder_hidden, metadata\n\n    def _backtrack(self, nw_output, nw_hidden, predecessors, symbols, scores, b, hidden_size):\n        """"""Backtracks over batch to generate optimal k-sequences.\n\n        Args:\n            nw_output [(batch*k, vocab_size)] * sequence_length: A Tensor of outputs from network\n            nw_hidden [(num_layers, batch*k, hidden_size)] * sequence_length: A Tensor of hidden states from network\n            predecessors [(batch*k)] * sequence_length: A Tensor of predecessors\n            symbols [(batch*k)] * sequence_length: A Tensor of predicted tokens\n            scores [(batch*k)] * sequence_length: A Tensor containing sequence scores for every token t = [0, ... , seq_len - 1]\n            b: Size of the batch\n            hidden_size: Size of the hidden state\n\n        Returns:\n            output [(batch, k, vocab_size)] * sequence_length: A list of the output probabilities (p_n)\n            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n\n            h_t [(batch, k, hidden_size)] * sequence_length: A list containing the output features (h_n)\n            from the last layer of the RNN, for every n = [0, ... , seq_len - 1]\n\n            h_n(batch, k, hidden_size): A Tensor containing the last hidden state for all top-k sequences.\n\n            score [batch, k]: A list containing the final scores for all top-k sequences\n\n            length [batch, k]: A list specifying the length of each sequence in the top-k candidates\n\n            p (batch, k, sequence_len): A Tensor containing predicted sequence\n        """"""\n\n        lstm = isinstance(nw_hidden[0], tuple)\n\n        # initialize return variables given different types\n        output = list()\n        h_t = list()\n        p = list()\n        # Placeholder for last hidden state of top-k sequences.\n        # If a (top-k) sequence ends early in decoding, `h_n` contains\n        # its hidden state when it sees EOS.  Otherwise, `h_n` contains\n        # the last hidden state of decoding.\n        if lstm:\n            state_size = nw_hidden[0][0].size()\n            h_n = tuple([torch.zeros(state_size), torch.zeros(state_size)])\n        else:\n            h_n = torch.zeros(nw_hidden[0].size())\n        l = [[self.rnn.max_length] * self.k for _ in range(b)]  # Placeholder for lengths of top-k sequences\n                                                                # Similar to `h_n`\n\n        # the last step output of the beams are not sorted\n        # thus they are sorted here\n        sorted_score, sorted_idx = scores[-1].view(b, self.k).topk(self.k)\n        # initialize the sequence scores with the sorted last step beam scores\n        s = sorted_score.clone()\n\n        batch_eos_found = [0] * b   # the number of EOS found\n                                    # in the backward loop below for each batch\n\n        t = self.rnn.max_length - 1\n        # initialize the back pointer with the sorted order of the last step beams.\n        # add self.pos_index for indexing variable with b*k as the first dimension.\n        t_predecessors = (sorted_idx + self.pos_index.expand_as(sorted_idx)).view(b * self.k)\n        while t >= 0:\n            # Re-order the variables with the back pointer\n            current_output = nw_output[t].index_select(0, t_predecessors)\n            if lstm:\n                current_hidden = tuple([h.index_select(1, t_predecessors) for h in nw_hidden[t]])\n            else:\n                current_hidden = nw_hidden[t].index_select(1, t_predecessors)\n            current_symbol = symbols[t].index_select(0, t_predecessors)\n            # Re-order the back pointer of the previous step with the back pointer of\n            # the current step\n            t_predecessors = predecessors[t].index_select(0, t_predecessors).squeeze()\n\n            # This tricky block handles dropped sequences that see EOS earlier.\n            # The basic idea is summarized below:\n            #\n            #   Terms:\n            #       Ended sequences = sequences that see EOS early and dropped\n            #       Survived sequences = sequences in the last step of the beams\n            #\n            #       Although the ended sequences are dropped during decoding,\n            #   their generated symbols and complete backtracking information are still\n            #   in the backtracking variables.\n            #   For each batch, everytime we see an EOS in the backtracking process,\n            #       1. If there is survived sequences in the return variables, replace\n            #       the one with the lowest survived sequence score with the new ended\n            #       sequences\n            #       2. Otherwise, replace the ended sequence with the lowest sequence\n            #       score with the new ended sequence\n            #\n            eos_indices = symbols[t].data.squeeze(1).eq(self.EOS).nonzero()\n            if eos_indices.dim() > 0:\n                for i in range(eos_indices.size(0)-1, -1, -1):\n                    # Indices of the EOS symbol for both variables\n                    # with b*k as the first dimension, and b, k for\n                    # the first two dimensions\n                    idx = eos_indices[i]\n                    b_idx = int(idx[0] / self.k)\n                    # The indices of the replacing position\n                    # according to the replacement strategy noted above\n                    res_k_idx = self.k - (batch_eos_found[b_idx] % self.k) - 1\n                    batch_eos_found[b_idx] += 1\n                    res_idx = b_idx * self.k + res_k_idx\n\n                    # Replace the old information in return variables\n                    # with the new ended sequence information\n                    t_predecessors[res_idx] = predecessors[t][idx[0]]\n                    current_output[res_idx, :] = nw_output[t][idx[0], :]\n                    if lstm:\n                        current_hidden[0][:, res_idx, :] = nw_hidden[t][0][:, idx[0], :]\n                        current_hidden[1][:, res_idx, :] = nw_hidden[t][1][:, idx[0], :]\n                        h_n[0][:, res_idx, :] = nw_hidden[t][0][:, idx[0], :].data\n                        h_n[1][:, res_idx, :] = nw_hidden[t][1][:, idx[0], :].data\n                    else:\n                        current_hidden[:, res_idx, :] = nw_hidden[t][:, idx[0], :]\n                        h_n[:, res_idx, :] = nw_hidden[t][:, idx[0], :].data\n                    current_symbol[res_idx, :] = symbols[t][idx[0]]\n                    s[b_idx, res_k_idx] = scores[t][idx[0]].data[0]\n                    l[b_idx][res_k_idx] = t + 1\n\n            # record the back tracked results\n            output.append(current_output)\n            h_t.append(current_hidden)\n            p.append(current_symbol)\n\n            t -= 1\n\n        # Sort and re-order again as the added ended sequences may change\n        # the order (very unlikely)\n        s, re_sorted_idx = s.topk(self.k)\n        for b_idx in range(b):\n            l[b_idx] = [l[b_idx][k_idx.item()] for k_idx in re_sorted_idx[b_idx,:]]\n\n        re_sorted_idx = (re_sorted_idx + self.pos_index.expand_as(re_sorted_idx)).view(b * self.k)\n\n        # Reverse the sequences and re-order at the same time\n        # It is reversed because the backtracking happens in reverse time order\n        output = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(output)]\n        p = [step.index_select(0, re_sorted_idx).view(b, self.k, -1) for step in reversed(p)]\n        if lstm:\n            h_t = [tuple([h.index_select(1, re_sorted_idx).view(-1, b, self.k, hidden_size) for h in step]) for step in reversed(h_t)]\n            h_n = tuple([h.index_select(1, re_sorted_idx.data).view(-1, b, self.k, hidden_size) for h in h_n])\n        else:\n            h_t = [step.index_select(1, re_sorted_idx).view(-1, b, self.k, hidden_size) for step in reversed(h_t)]\n            h_n = h_n.index_select(1, re_sorted_idx.data).view(-1, b, self.k, hidden_size)\n        s = s.data\n\n        return output, h_t, h_n, s, l, p\n\n    def _mask_symbol_scores(self, score, idx, masking_score=-float(\'inf\')):\n            score[idx] = masking_score\n\n    def _mask(self, tensor, idx, dim=0, masking_score=-float(\'inf\')):\n        if len(idx.size()) > 0:\n            indices = idx[:, 0]\n            tensor.index_fill_(dim, indices, masking_score)\n\n\n'"
seq2seq/models/__init__.py,0,b'from .EncoderRNN import EncoderRNN\nfrom .DecoderRNN import DecoderRNN\nfrom .TopKDecoder import TopKDecoder\nfrom .seq2seq import Seq2seq\n'
seq2seq/models/attention.py,10,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Attention(nn.Module):\n    r""""""\n    Applies an attention mechanism on the output features from the decoder.\n\n    .. math::\n            \\begin{array}{ll}\n            x = context*output \\\\\n            attn = exp(x_i) / sum_j exp(x_j) \\\\\n            output = \\tanh(w * (attn * context) + b * output)\n            \\end{array}\n\n    Args:\n        dim(int): The number of expected features in the output\n\n    Inputs: output, context\n        - **output** (batch, output_len, dimensions): tensor containing the output features from the decoder.\n        - **context** (batch, input_len, dimensions): tensor containing features of the encoded input sequence.\n\n    Outputs: output, attn\n        - **output** (batch, output_len, dimensions): tensor containing the attended output features from the decoder.\n        - **attn** (batch, output_len, input_len): tensor containing attention weights.\n\n    Attributes:\n        linear_out (torch.nn.Linear): applies a linear transformation to the incoming data: :math:`y = Ax + b`.\n        mask (torch.Tensor, optional): applies a :math:`-inf` to the indices specified in the `Tensor`.\n\n    Examples::\n\n         >>> attention = seq2seq.models.Attention(256)\n         >>> context = Variable(torch.randn(5, 3, 256))\n         >>> output = Variable(torch.randn(5, 5, 256))\n         >>> output, attn = attention(output, context)\n\n    """"""\n    def __init__(self, dim):\n        super(Attention, self).__init__()\n        self.linear_out = nn.Linear(dim*2, dim)\n        self.mask = None\n\n    def set_mask(self, mask):\n        """"""\n        Sets indices to be masked\n\n        Args:\n            mask (torch.Tensor): tensor containing indices to be masked\n        """"""\n        self.mask = mask\n\n    def forward(self, output, context):\n        batch_size = output.size(0)\n        hidden_size = output.size(2)\n        input_size = context.size(1)\n        # (batch, out_len, dim) * (batch, in_len, dim) -> (batch, out_len, in_len)\n        attn = torch.bmm(output, context.transpose(1, 2))\n        if self.mask is not None:\n            attn.data.masked_fill_(self.mask, -float(\'inf\'))\n        attn = F.softmax(attn.view(-1, input_size), dim=1).view(batch_size, -1, input_size)\n\n        # (batch, out_len, in_len) * (batch, in_len, dim) -> (batch, out_len, dim)\n        mix = torch.bmm(attn, context)\n\n        # concat -> (batch, out_len, 2*dim)\n        combined = torch.cat((mix, output), dim=2)\n        # output -> (batch, out_len, dim)\n        output = F.tanh(self.linear_out(combined.view(-1, 2 * hidden_size))).view(batch_size, -1, hidden_size)\n\n        return output, attn\n'"
seq2seq/models/baseRNN.py,1,"b'"""""" A base class for RNN. """"""\nimport torch.nn as nn\n\n\nclass BaseRNN(nn.Module):\n    r""""""\n    Applies a multi-layer RNN to an input sequence.\n    Note:\n        Do not use this class directly, use one of the sub classes.\n    Args:\n        vocab_size (int): size of the vocabulary\n        max_len (int): maximum allowed length for the sequence to be processed\n        hidden_size (int): number of features in the hidden state `h`\n        input_dropout_p (float): dropout probability for the input sequence\n        dropout_p (float): dropout probability for the output sequence\n        n_layers (int): number of recurrent layers\n        rnn_cell (str): type of RNN cell (Eg. \'LSTM\' , \'GRU\')\n\n    Inputs: ``*args``, ``**kwargs``\n        - ``*args``: variable length argument list.\n        - ``**kwargs``: arbitrary keyword arguments.\n\n    Attributes:\n        SYM_MASK: masking symbol\n        SYM_EOS: end-of-sequence symbol\n    """"""\n    SYM_MASK = ""MASK""\n    SYM_EOS = ""EOS""\n\n    def __init__(self, vocab_size, max_len, hidden_size, input_dropout_p, dropout_p, n_layers, rnn_cell):\n        super(BaseRNN, self).__init__()\n        self.vocab_size = vocab_size\n        self.max_len = max_len\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.input_dropout_p = input_dropout_p\n        self.input_dropout = nn.Dropout(p=input_dropout_p)\n        if rnn_cell.lower() == \'lstm\':\n            self.rnn_cell = nn.LSTM\n        elif rnn_cell.lower() == \'gru\':\n            self.rnn_cell = nn.GRU\n        else:\n            raise ValueError(""Unsupported RNN Cell: {0}"".format(rnn_cell))\n\n        self.dropout_p = dropout_p\n\n    def forward(self, *args, **kwargs):\n        raise NotImplementedError()\n'"
seq2seq/models/seq2seq.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Seq2seq(nn.Module):\n    """""" Standard sequence-to-sequence architecture with configurable encoder\n    and decoder.\n\n    Args:\n        encoder (EncoderRNN): object of EncoderRNN\n        decoder (DecoderRNN): object of DecoderRNN\n        decode_function (func, optional): function to generate symbols from output hidden states (default: F.log_softmax)\n\n    Inputs: input_variable, input_lengths, target_variable, teacher_forcing_ratio\n        - **input_variable** (list, option): list of sequences, whose length is the batch size and within which\n          each sequence is a list of token IDs. This information is forwarded to the encoder.\n        - **input_lengths** (list of int, optional): A list that contains the lengths of sequences\n            in the mini-batch, it must be provided when using variable length RNN (default: `None`)\n        - **target_variable** (list, optional): list of sequences, whose length is the batch size and within which\n          each sequence is a list of token IDs. This information is forwarded to the decoder.\n        - **teacher_forcing_ratio** (int, optional): The probability that teacher forcing will be used. A random number\n          is drawn uniformly from 0-1 for every decoding token, and if the sample is smaller than the given value,\n          teacher forcing would be used (default is 0)\n\n    Outputs: decoder_outputs, decoder_hidden, ret_dict\n        - **decoder_outputs** (batch): batch-length list of tensors with size (max_length, hidden_size) containing the\n          outputs of the decoder.\n        - **decoder_hidden** (num_layers * num_directions, batch, hidden_size): tensor containing the last hidden\n          state of the decoder.\n        - **ret_dict**: dictionary containing additional information as follows {*KEY_LENGTH* : list of integers\n          representing lengths of output sequences, *KEY_SEQUENCE* : list of sequences, where each sequence is a list of\n          predicted token IDs, *KEY_INPUT* : target outputs if provided for decoding, *KEY_ATTN_SCORE* : list of\n          sequences, where each list is of attention weights }.\n\n    """"""\n\n    def __init__(self, encoder, decoder, decode_function=F.log_softmax):\n        super(Seq2seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.decode_function = decode_function\n\n    def flatten_parameters(self):\n        self.encoder.rnn.flatten_parameters()\n        self.decoder.rnn.flatten_parameters()\n\n    def forward(self, input_variable, input_lengths=None, target_variable=None,\n                teacher_forcing_ratio=0):\n        encoder_outputs, encoder_hidden = self.encoder(input_variable, input_lengths)\n        result = self.decoder(inputs=target_variable,\n                              encoder_hidden=encoder_hidden,\n                              encoder_outputs=encoder_outputs,\n                              function=self.decode_function,\n                              teacher_forcing_ratio=teacher_forcing_ratio)\n        return result\n'"
seq2seq/optim/__init__.py,0,b'from .optim import Optimizer\n'
seq2seq/optim/optim.py,7,"b'import itertools\n\nimport torch\n\nclass Optimizer(object):\n    """""" The Optimizer class encapsulates torch.optim package and provides functionalities\n    for learning rate scheduling and gradient norm clipping.\n\n    Args:\n        optim (torch.optim.Optimizer): optimizer object, the parameters to be optimized\n            should be given when instantiating the object, e.g. torch.optim.SGD(params)\n        max_grad_norm (float, optional): value used for gradient norm clipping,\n            set 0 to disable (default 0)\n    """"""\n\n    _ARG_MAX_GRAD_NORM = \'max_grad_norm\'\n\n    def __init__(self, optim, max_grad_norm=0):\n        self.optimizer = optim\n        self.scheduler = None\n        self.max_grad_norm = max_grad_norm\n\n    def set_scheduler(self, scheduler):\n        """""" Set the learning rate scheduler.\n\n        Args:\n            scheduler (torch.optim.lr_scheduler.*): object of learning rate scheduler,\n               e.g. torch.optim.lr_scheduler.StepLR\n        """"""\n        self.scheduler = scheduler\n\n    def step(self):\n        """""" Performs a single optimization step, including gradient norm clipping if necessary. """"""\n        if self.max_grad_norm > 0:\n            params = itertools.chain.from_iterable([group[\'params\'] for group in self.optimizer.param_groups])\n            torch.nn.utils.clip_grad_norm_(params, self.max_grad_norm)\n        self.optimizer.step()\n\n    def update(self, loss, epoch):\n        """""" Update the learning rate if the criteria of the scheduler are met.\n\n        Args:\n            loss (float): The current loss.  It could be training loss or developing loss\n                depending on the caller.  By default the supervised trainer uses developing\n                loss.\n            epoch (int): The current epoch number.\n        """"""\n        if self.scheduler is None:\n            pass\n        elif isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            self.scheduler.step(loss)\n        else:\n            self.scheduler.step()\n'"
seq2seq/trainer/__init__.py,0,b'from .supervised_trainer import SupervisedTrainer\n'
seq2seq/trainer/supervised_trainer.py,3,"b'from __future__ import division\nimport logging\nimport os\nimport random\nimport time\n\nimport torch\nimport torchtext\nfrom torch import optim\n\nimport seq2seq\nfrom seq2seq.evaluator import Evaluator\nfrom seq2seq.loss import NLLLoss\nfrom seq2seq.optim import Optimizer\nfrom seq2seq.util.checkpoint import Checkpoint\n\nclass SupervisedTrainer(object):\n    """""" The SupervisedTrainer class helps in setting up a training framework in a\n    supervised setting.\n\n    Args:\n        expt_dir (optional, str): experiment Directory to store details of the experiment,\n            by default it makes a folder in the current directory to store the details (default: `experiment`).\n        loss (seq2seq.loss.loss.Loss, optional): loss for training, (default: seq2seq.loss.NLLLoss)\n        batch_size (int, optional): batch size for experiment, (default: 64)\n        checkpoint_every (int, optional): number of batches to checkpoint after, (default: 100)\n    """"""\n    def __init__(self, expt_dir=\'experiment\', loss=NLLLoss(), batch_size=64,\n                 random_seed=None,\n                 checkpoint_every=100, print_every=100):\n        self._trainer = ""Simple Trainer""\n        self.random_seed = random_seed\n        if random_seed is not None:\n            random.seed(random_seed)\n            torch.manual_seed(random_seed)\n        self.loss = loss\n        self.evaluator = Evaluator(loss=self.loss, batch_size=batch_size)\n        self.optimizer = None\n        self.checkpoint_every = checkpoint_every\n        self.print_every = print_every\n\n        if not os.path.isabs(expt_dir):\n            expt_dir = os.path.join(os.getcwd(), expt_dir)\n        self.expt_dir = expt_dir\n        if not os.path.exists(self.expt_dir):\n            os.makedirs(self.expt_dir)\n        self.batch_size = batch_size\n\n        self.logger = logging.getLogger(__name__)\n\n    def _train_batch(self, input_variable, input_lengths, target_variable, model, teacher_forcing_ratio):\n        loss = self.loss\n        # Forward propagation\n        decoder_outputs, decoder_hidden, other = model(input_variable, input_lengths, target_variable,\n                                                       teacher_forcing_ratio=teacher_forcing_ratio)\n        # Get loss\n        loss.reset()\n        for step, step_output in enumerate(decoder_outputs):\n            batch_size = target_variable.size(0)\n            loss.eval_batch(step_output.contiguous().view(batch_size, -1), target_variable[:, step + 1])\n        # Backward propagation\n        model.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        return loss.get_loss()\n\n    def _train_epoches(self, data, model, n_epochs, start_epoch, start_step,\n                       dev_data=None, teacher_forcing_ratio=0):\n        log = self.logger\n\n        print_loss_total = 0  # Reset every print_every\n        epoch_loss_total = 0  # Reset every epoch\n\n        device = None if torch.cuda.is_available() else -1\n        batch_iterator = torchtext.data.BucketIterator(\n            dataset=data, batch_size=self.batch_size,\n            sort=False, sort_within_batch=True,\n            sort_key=lambda x: len(x.src),\n            device=device, repeat=False)\n\n        steps_per_epoch = len(batch_iterator)\n        total_steps = steps_per_epoch * n_epochs\n\n        step = start_step\n        step_elapsed = 0\n        for epoch in range(start_epoch, n_epochs + 1):\n            log.debug(""Epoch: %d, Step: %d"" % (epoch, step))\n\n            batch_generator = batch_iterator.__iter__()\n            # consuming seen batches from previous training\n            for _ in range((epoch - 1) * steps_per_epoch, step):\n                next(batch_generator)\n\n            model.train(True)\n            for batch in batch_generator:\n                step += 1\n                step_elapsed += 1\n\n                input_variables, input_lengths = getattr(batch, seq2seq.src_field_name)\n                target_variables = getattr(batch, seq2seq.tgt_field_name)\n\n                loss = self._train_batch(input_variables, input_lengths.tolist(), target_variables, model, teacher_forcing_ratio)\n\n                # Record average loss\n                print_loss_total += loss\n                epoch_loss_total += loss\n\n                if step % self.print_every == 0 and step_elapsed > self.print_every:\n                    print_loss_avg = print_loss_total / self.print_every\n                    print_loss_total = 0\n                    log_msg = \'Progress: %d%%, Train %s: %.4f\' % (\n                        step / total_steps * 100,\n                        self.loss.name,\n                        print_loss_avg)\n                    log.info(log_msg)\n\n                # Checkpoint\n                if step % self.checkpoint_every == 0 or step == total_steps:\n                    Checkpoint(model=model,\n                               optimizer=self.optimizer,\n                               epoch=epoch, step=step,\n                               input_vocab=data.fields[seq2seq.src_field_name].vocab,\n                               output_vocab=data.fields[seq2seq.tgt_field_name].vocab).save(self.expt_dir)\n\n            if step_elapsed == 0: continue\n\n            epoch_loss_avg = epoch_loss_total / min(steps_per_epoch, step - start_step)\n            epoch_loss_total = 0\n            log_msg = ""Finished epoch %d: Train %s: %.4f"" % (epoch, self.loss.name, epoch_loss_avg)\n            if dev_data is not None:\n                dev_loss, accuracy = self.evaluator.evaluate(model, dev_data)\n                self.optimizer.update(dev_loss, epoch)\n                log_msg += "", Dev %s: %.4f, Accuracy: %.4f"" % (self.loss.name, dev_loss, accuracy)\n                model.train(mode=True)\n            else:\n                self.optimizer.update(epoch_loss_avg, epoch)\n\n            log.info(log_msg)\n\n    def train(self, model, data, num_epochs=5,\n              resume=False, dev_data=None,\n              optimizer=None, teacher_forcing_ratio=0):\n        """""" Run training for a given model.\n\n        Args:\n            model (seq2seq.models): model to run training on, if `resume=True`, it would be\n               overwritten by the model loaded from the latest checkpoint.\n            data (seq2seq.dataset.dataset.Dataset): dataset object to train on\n            num_epochs (int, optional): number of epochs to run (default 5)\n            resume(bool, optional): resume training with the latest checkpoint, (default False)\n            dev_data (seq2seq.dataset.dataset.Dataset, optional): dev Dataset (default None)\n            optimizer (seq2seq.optim.Optimizer, optional): optimizer for training\n               (default: Optimizer(pytorch.optim.Adam, max_grad_norm=5))\n            teacher_forcing_ratio (float, optional): teaching forcing ratio (default 0)\n        Returns:\n            model (seq2seq.models): trained model.\n        """"""\n        # If training is set to resume\n        if resume:\n            latest_checkpoint_path = Checkpoint.get_latest_checkpoint(self.expt_dir)\n            resume_checkpoint = Checkpoint.load(latest_checkpoint_path)\n            model = resume_checkpoint.model\n            self.optimizer = resume_checkpoint.optimizer\n\n            # A walk around to set optimizing parameters properly\n            resume_optim = self.optimizer.optimizer\n            defaults = resume_optim.param_groups[0]\n            defaults.pop(\'params\', None)\n            defaults.pop(\'initial_lr\', None)\n            self.optimizer.optimizer = resume_optim.__class__(model.parameters(), **defaults)\n\n            start_epoch = resume_checkpoint.epoch\n            step = resume_checkpoint.step\n        else:\n            start_epoch = 1\n            step = 0\n            if optimizer is None:\n                optimizer = Optimizer(optim.Adam(model.parameters()), max_grad_norm=5)\n            self.optimizer = optimizer\n\n        self.logger.info(""Optimizer: %s, Scheduler: %s"" % (self.optimizer.optimizer, self.optimizer.scheduler))\n\n        self._train_epoches(data, model, num_epochs,\n                            start_epoch, step, dev_data=dev_data,\n                            teacher_forcing_ratio=teacher_forcing_ratio)\n        return model\n'"
seq2seq/util/__init__.py,0,b''
seq2seq/util/checkpoint.py,7,"b'from __future__ import print_function\nimport os\nimport time\nimport shutil\n\nimport torch\nimport dill\n\nclass Checkpoint(object):\n    """"""\n    The Checkpoint class manages the saving and loading of a model during training. It allows training to be suspended\n    and resumed at a later time (e.g. when running on a cluster using sequential jobs).\n\n    To make a checkpoint, initialize a Checkpoint object with the following args; then call that object\'s save() method\n    to write parameters to disk.\n\n    Args:\n        model (seq2seq): seq2seq model being trained\n        optimizer (Optimizer): stores the state of the optimizer\n        epoch (int): current epoch (an epoch is a loop through the full training data)\n        step (int): number of examples seen within the current epoch\n        input_vocab (Vocabulary): vocabulary for the input language\n        output_vocab (Vocabulary): vocabulary for the output language\n\n    Attributes:\n        CHECKPOINT_DIR_NAME (str): name of the checkpoint directory\n        TRAINER_STATE_NAME (str): name of the file storing trainer states\n        MODEL_NAME (str): name of the file storing model\n        INPUT_VOCAB_FILE (str): name of the input vocab file\n        OUTPUT_VOCAB_FILE (str): name of the output vocab file\n    """"""\n\n    CHECKPOINT_DIR_NAME = \'checkpoints\'\n    TRAINER_STATE_NAME = \'trainer_states.pt\'\n    MODEL_NAME = \'model.pt\'\n    INPUT_VOCAB_FILE = \'input_vocab.pt\'\n    OUTPUT_VOCAB_FILE = \'output_vocab.pt\'\n\n    def __init__(self, model, optimizer, epoch, step, input_vocab, output_vocab, path=None):\n        self.model = model\n        self.optimizer = optimizer\n        self.input_vocab = input_vocab\n        self.output_vocab = output_vocab\n        self.epoch = epoch\n        self.step = step\n        self._path = path\n\n    @property\n    def path(self):\n        if self._path is None:\n            raise LookupError(""The checkpoint has not been saved."")\n        return self._path\n\n    def save(self, experiment_dir):\n        """"""\n        Saves the current model and related training parameters into a subdirectory of the checkpoint directory.\n        The name of the subdirectory is the current local time in Y_M_D_H_M_S format.\n        Args:\n            experiment_dir (str): path to the experiment root directory\n        Returns:\n             str: path to the saved checkpoint subdirectory\n        """"""\n        date_time = time.strftime(\'%Y_%m_%d_%H_%M_%S\', time.localtime())\n\n        self._path = os.path.join(experiment_dir, self.CHECKPOINT_DIR_NAME, date_time)\n        path = self._path\n\n        if os.path.exists(path):\n            shutil.rmtree(path)\n        os.makedirs(path)\n        torch.save({\'epoch\': self.epoch,\n                    \'step\': self.step,\n                    \'optimizer\': self.optimizer\n                   },\n                   os.path.join(path, self.TRAINER_STATE_NAME))\n        torch.save(self.model, os.path.join(path, self.MODEL_NAME))\n\n        with open(os.path.join(path, self.INPUT_VOCAB_FILE), \'wb\') as fout:\n            dill.dump(self.input_vocab, fout)\n        with open(os.path.join(path, self.OUTPUT_VOCAB_FILE), \'wb\') as fout:\n            dill.dump(self.output_vocab, fout)\n\n        return path\n\n    @classmethod\n    def load(cls, path):\n        """"""\n        Loads a Checkpoint object that was previously saved to disk.\n        Args:\n            path (str): path to the checkpoint subdirectory\n        Returns:\n            checkpoint (Checkpoint): checkpoint object with fields copied from those stored on disk\n        """"""\n        if torch.cuda.is_available():\n            resume_checkpoint = torch.load(os.path.join(path, cls.TRAINER_STATE_NAME))\n            model = torch.load(os.path.join(path, cls.MODEL_NAME))\n        else:\n            resume_checkpoint = torch.load(os.path.join(path, cls.TRAINER_STATE_NAME), map_location=lambda storage, loc: storage)\n            model = torch.load(os.path.join(path, cls.MODEL_NAME), map_location=lambda storage, loc: storage)\n\n        model.flatten_parameters() # make RNN parameters contiguous\n        with open(os.path.join(path, cls.INPUT_VOCAB_FILE), \'rb\') as fin:\n            input_vocab = dill.load(fin)\n        with open(os.path.join(path, cls.OUTPUT_VOCAB_FILE), \'rb\') as fin:\n            output_vocab = dill.load(fin)\n        optimizer = resume_checkpoint[\'optimizer\']\n        return Checkpoint(model=model, input_vocab=input_vocab,\n                          output_vocab=output_vocab,\n                          optimizer=optimizer,\n                          epoch=resume_checkpoint[\'epoch\'],\n                          step=resume_checkpoint[\'step\'],\n                          path=path)\n\n    @classmethod\n    def get_latest_checkpoint(cls, experiment_path):\n        """"""\n        Given the path to an experiment directory, returns the path to the last saved checkpoint\'s subdirectory.\n\n        Precondition: at least one checkpoint has been made (i.e., latest checkpoint subdirectory exists).\n        Args:\n            experiment_path (str): path to the experiment directory\n        Returns:\n             str: path to the last saved checkpoint\'s subdirectory\n        """"""\n        checkpoints_path = os.path.join(experiment_path, cls.CHECKPOINT_DIR_NAME)\n        all_times = sorted(os.listdir(checkpoints_path), reverse=True)\n        return os.path.join(checkpoints_path, all_times[0])\n'"
