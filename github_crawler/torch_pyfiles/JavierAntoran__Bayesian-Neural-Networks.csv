file_path,api_count,code
train_BayesByBackprop_MNIST.py,6,"b'from __future__ import division, print_function\nimport time\nimport torch.utils.data\nfrom torchvision import transforms, datasets\nimport argparse\nimport matplotlib\nfrom src.Bayes_By_Backprop.model import *\nfrom src.Bayes_By_Backprop_Local_Reparametrization.model import *\n\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\n\nparser = argparse.ArgumentParser(description=\'Train Bayesian Neural Net on MNIST with Variational Inference\')\nparser.add_argument(\'--model\', type=str, nargs=\'?\', action=\'store\', default=\'Local_Reparam\',\n                    help=\'Model to run. Options are \\\'Gaussian_prior\\\', \\\'Laplace_prior\\\', \\\'GMM_prior\\\',\'\n                         \' \\\'Local_Reparam\\\'. Default: \\\'Local_Reparam\\\'.\')\nparser.add_argument(\'--prior_sig\', type=float, nargs=\'?\', action=\'store\', default=0.1,\n                    help=\'Standard deviation of prior. Default: 0.1.\')\nparser.add_argument(\'--epochs\', type=int, nargs=\'?\', action=\'store\', default=200,\n                    help=\'How many epochs to train. Default: 200.\')\nparser.add_argument(\'--lr\', type=float, nargs=\'?\', action=\'store\', default=1e-3,\n                    help=\'learning rate. Default: 1e-3.\')\nparser.add_argument(\'--n_samples\', type=float, nargs=\'?\', action=\'store\', default=3,\n                    help=\'How many MC samples to take when approximating the ELBO. Default: 3.\')\nparser.add_argument(\'--models_dir\', type=str, nargs=\'?\', action=\'store\', default=\'BBP_models\',\n                    help=\'Where to save learnt weights and train vectors. Default: \\\'BBP_models\\\'.\')\nparser.add_argument(\'--results_dir\', type=str, nargs=\'?\', action=\'store\', default=\'BBP_results\',\n                    help=\'Where to save learnt training plots. Default: \\\'BBP_results\\\'.\')\nargs = parser.parse_args()\n\n\n\n# Where to save models weights\nmodels_dir = args.models_dir\n# Where to save plots and error, accuracy vectors\nresults_dir = args.results_dir\n\nmkdir(models_dir)\nmkdir(results_dir)\n# ------------------------------------------------------------------------------------------------------\n# train config\nNTrainPointsMNIST = 60000\nbatch_size = 100\nnb_epochs = args.epochs\nlog_interval = 1\n\n\n# ------------------------------------------------------------------------------------------------------\n# dataset\ncprint(\'c\', \'\\nData:\')\n\n# load data\n\n# data augmentation\ntransform_train = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n])\n\nuse_cuda = torch.cuda.is_available()\n\ntrainset = datasets.MNIST(root=\'../data\', train=True, download=True, transform=transform_train)\nvalset = datasets.MNIST(root=\'../data\', train=False, download=True, transform=transform_test)\n\nif use_cuda:\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n                                              num_workers=3)\n    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n                                            num_workers=3)\n\nelse:\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n                                              num_workers=3)\n    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n                                            num_workers=3)\n\n## ---------------------------------------------------------------------------------------------------------------------\n# net dims\ncprint(\'c\', \'\\nNetwork:\')\n\nlr = args.lr\nnsamples = int(args.n_samples)  # How many samples to estimate ELBO with at each iteration\n########################################################################################\n\nif args.model == \'Local_Reparam\':\n    net = BBP_Bayes_Net_LR(lr=lr, channels_in=1, side_in=28, cuda=use_cuda, classes=10, batch_size=batch_size,\n                     Nbatches=(NTrainPointsMNIST / batch_size), nhid=1200, prior_sig=args.prior_sig)\nelif args.model == \'Laplace_prior\':\n    net = BBP_Bayes_Net(lr=lr, channels_in=1, side_in=28, cuda=use_cuda, classes=10, batch_size=batch_size,\n                        Nbatches=(NTrainPointsMNIST / batch_size), nhid=1200,\n                        prior_instance=laplace_prior(mu=0, b=args.prior_sig))\nelif args.model == \'Gaussian_prior\':\n    net = BBP_Bayes_Net(lr=lr, channels_in=1, side_in=28, cuda=use_cuda, classes=10, batch_size=batch_size,\n                        Nbatches=(NTrainPointsMNIST / batch_size), nhid=1200,\n                        prior_instance=isotropic_gauss_prior(mu=0, sigma=args.prior_sig))\nelif args.model == \'GMM_prior\':\n    net = BBP_Bayes_Net(lr=lr, channels_in=1, side_in=28, cuda=use_cuda, classes=10, batch_size=batch_size,\n                        Nbatches=(NTrainPointsMNIST / batch_size), nhid=1200,\n                        prior_instance=spike_slab_2GMM(mu1=0, mu2=0, sigma1=args.prior_sig, sigma2=0.0005, pi=0.75))\nelse:\n    print(\'Invalid model type\')\n    exit(1)\n\n## ---------------------------------------------------------------------------------------------------------------------\n# train\nepoch = 0\ncprint(\'c\', \'\\nTrain:\')\n\nprint(\'  init cost variables:\')\nkl_cost_train = np.zeros(nb_epochs)\npred_cost_train = np.zeros(nb_epochs)\nerr_train = np.zeros(nb_epochs)\n\ncost_dev = np.zeros(nb_epochs)\nerr_dev = np.zeros(nb_epochs)\nbest_err = np.inf\n\nnb_its_dev = 1\n\ntic0 = time.time()\nfor i in range(epoch, nb_epochs):\n    # We draw more samples on the first epoch in order to ensure convergence\n    if i == 0:\n        ELBO_samples = 10\n    else:\n        ELBO_samples = nsamples\n\n    net.set_mode_train(True)\n    tic = time.time()\n    nb_samples = 0\n\n    for x, y in trainloader:\n        cost_dkl, cost_pred, err = net.fit(x, y, samples=ELBO_samples)\n\n        err_train[i] += err\n        kl_cost_train[i] += cost_dkl\n        pred_cost_train[i] += cost_pred\n        nb_samples += len(x)\n\n    kl_cost_train[i] /= nb_samples  # Normalise by number of samples in order to get comparable number to the -log like\n    pred_cost_train[i] /= nb_samples\n    err_train[i] /= nb_samples\n\n    toc = time.time()\n    net.epoch = i\n    # ---- print\n    print(""it %d/%d, Jtr_KL = %f, Jtr_pred = %f, err = %f, "" % (\n    i, nb_epochs, kl_cost_train[i], pred_cost_train[i], err_train[i]), end="""")\n    cprint(\'r\', \'   time: %f seconds\\n\' % (toc - tic))\n\n    # ---- dev\n    if i % nb_its_dev == 0:\n        net.set_mode_train(False)\n        nb_samples = 0\n        for j, (x, y) in enumerate(valloader):\n            cost, err, probs = net.eval(x, y)  # This takes the expected weights to save time, not proper inference\n\n            cost_dev[i] += cost\n            err_dev[i] += err\n            nb_samples += len(x)\n\n        cost_dev[i] /= nb_samples\n        err_dev[i] /= nb_samples\n\n        cprint(\'g\', \'    Jdev = %f, err = %f\\n\' % (cost_dev[i], err_dev[i]))\n\n        if err_dev[i] < best_err:\n            best_err = err_dev[i]\n            cprint(\'b\', \'best test error\')\n            net.save(models_dir + \'/theta_best.dat\')\n\ntoc0 = time.time()\nruntime_per_it = (toc0 - tic0) / float(nb_epochs)\ncprint(\'r\', \'   average time: %f seconds\\n\' % runtime_per_it)\n\nnet.save(models_dir + \'/theta_last.dat\')\n\n## ---------------------------------------------------------------------------------------------------------------------\n# results\ncprint(\'c\', \'\\nRESULTS:\')\nnb_parameters = net.get_nb_parameters()\nbest_cost_dev = np.min(cost_dev)\nbest_cost_train = np.min(pred_cost_train)\nerr_dev_min = err_dev[::nb_its_dev].min()\n\nprint(\'  cost_dev: %f (cost_train %f)\' % (best_cost_dev, best_cost_train))\nprint(\'  err_dev: %f\' % (err_dev_min))\nprint(\'  nb_parameters: %d (%s)\' % (nb_parameters, humansize(nb_parameters)))\nprint(\'  time_per_it: %fs\\n\' % (runtime_per_it))\n\n## Save results for plots\n# np.save(\'results/test_predictions.npy\', test_predictions)\nnp.save(results_dir + \'/KL_cost_train.npy\', kl_cost_train)\nnp.save(results_dir + \'/pred_cost_train.npy\', pred_cost_train)\nnp.save(results_dir + \'/cost_dev.npy\', cost_dev)\nnp.save(results_dir + \'/err_train.npy\', err_train)\nnp.save(results_dir + \'/err_dev.npy\', err_dev)\n\n## ---------------------------------------------------------------------------------------------------------------------\n# fig cost vs its\n\ntextsize = 15\nmarker = 5\n\nplt.figure(dpi=100)\nfig, ax1 = plt.subplots()\nax1.plot(pred_cost_train, \'r--\')\nax1.plot(range(0, nb_epochs, nb_its_dev), cost_dev[::nb_its_dev], \'b-\')\nax1.set_ylabel(\'Cross Entropy\')\nplt.xlabel(\'epoch\')\nplt.grid(b=True, which=\'major\', color=\'k\', linestyle=\'-\')\nplt.grid(b=True, which=\'minor\', color=\'k\', linestyle=\'--\')\nlgd = plt.legend([\'train error\', \'test error\'], markerscale=marker, prop={\'size\': textsize, \'weight\': \'normal\'})\nax = plt.gca()\nplt.title(\'classification costs\')\nfor item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n             ax.get_xticklabels() + ax.get_yticklabels()):\n    item.set_fontsize(textsize)\n    item.set_weight(\'normal\')\nplt.savefig(results_dir + \'/pred_cost.png\', bbox_extra_artists=(lgd,), bbox_inches=\'tight\')\n\nplt.figure()\nfig, ax1 = plt.subplots()\nax1.plot(kl_cost_train, \'r\')\nax1.set_ylabel(\'nats?\')\nplt.xlabel(\'epoch\')\nplt.grid(b=True, which=\'major\', color=\'k\', linestyle=\'-\')\nplt.grid(b=True, which=\'minor\', color=\'k\', linestyle=\'--\')\nax = plt.gca()\nplt.title(\'DKL (per sample)\')\nfor item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n             ax.get_xticklabels() + ax.get_yticklabels()):\n    item.set_fontsize(textsize)\n    item.set_weight(\'normal\')\nplt.savefig(results_dir + \'/KL_cost.png\', bbox_extra_artists=(lgd,), bbox_inches=\'tight\')\n\nplt.figure(dpi=100)\nfig2, ax2 = plt.subplots()\nax2.set_ylabel(\'% error\')\nax2.semilogy(range(0, nb_epochs, nb_its_dev), 100 * err_dev[::nb_its_dev], \'b-\')\nax2.semilogy(100 * err_train, \'r--\')\nplt.xlabel(\'epoch\')\nplt.grid(b=True, which=\'major\', color=\'k\', linestyle=\'-\')\nplt.grid(b=True, which=\'minor\', color=\'k\', linestyle=\'--\')\nax2.get_yaxis().set_minor_formatter(matplotlib.ticker.ScalarFormatter())\nax2.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\nlgd = plt.legend([\'test error\', \'train error\'], markerscale=marker, prop={\'size\': textsize, \'weight\': \'normal\'})\nax = plt.gca()\nfor item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n             ax.get_xticklabels() + ax.get_yticklabels()):\n    item.set_fontsize(textsize)\n    item.set_weight(\'normal\')\nplt.savefig(results_dir + \'/err.png\', bbox_extra_artists=(lgd,), box_inches=\'tight\')\n'"
train_BootrapEnsemble_MNIST.py,7,"b'from __future__ import division, print_function\nimport time\nimport torch.utils.data\nfrom torchvision import transforms, datasets\nimport argparse\nimport matplotlib\nfrom src.Bootstrap_Ensemble.model import *\nimport copy\n\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\n\nparser = argparse.ArgumentParser(description=\'Train Ensemble of MAP nets using bootstrapping\')\n\nparser.add_argument(\'--weight_decay\', type=float, nargs=\'?\', action=\'store\', default=0,\n                    help=\'Specify the precision of an isotropic Gaussian prior. Default: 0.\')\nparser.add_argument(\'--subsample\', type=float, nargs=\'?\', action=\'store\', default=0.8,\n                    help=\'Rate at which to subsample the dataset to train each net in the ensemble. Default: 0.8.\')\nparser.add_argument(\'--n_nets\', type=int, nargs=\'?\', action=\'store\', default=100,\n                    help=\'Number of nets in ensemble. Default: 100.\')\nparser.add_argument(\'--epochs\', type=int, nargs=\'?\', action=\'store\', default=10,\n                    help=\'How many epochs to train each net. Default: 10.\')\nparser.add_argument(\'--lr\', type=float, nargs=\'?\', action=\'store\', default=1e-3,\n                    help=\'learning rate. Default: 1e-3.\')\nparser.add_argument(\'--models_dir\', type=str, nargs=\'?\', action=\'store\', default=\'Ensemble_models\',\n                    help=\'Where to save learnt weights and train vectors. Default: \\\'Ensemble_models\\\'.\')\nparser.add_argument(\'--results_dir\', type=str, nargs=\'?\', action=\'store\', default=\'Ensemble_results\',\n                    help=\'Where to save learnt training plots. Default: \\\'Ensemble_results\\\'.\')\nargs = parser.parse_args()\n\n\n\n# Where to save models weights\nmodels_dir = args.models_dir\n# Where to save plots and error, accuracy vectors\nresults_dir = args.results_dir\n\nmkdir(models_dir)\nmkdir(results_dir)\n# ------------------------------------------------------------------------------------------------------\n# train config\nNTrainPointsMNIST = 60000\nbatch_size = 128\nnb_epochs = args.epochs\nlog_interval = 1\n\n# ------------------------------------------------------------------------------------------------------\n# dataset\ncprint(\'c\', \'\\nData:\')\n\n# load data\n\n# data augmentation\ntransform_train = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n])\n\nuse_cuda = torch.cuda.is_available()\n\n\ntrainset = datasets.MNIST(root=\'../data\', train=True, download=True, transform=transform_train)\nvalset = datasets.MNIST(root=\'../data\', train=False, download=True, transform=transform_test)\n\n## ---------------------------------------------------------------------------------------------------------------------\n# net dims\ncprint(\'c\', \'\\nNetwork:\')\n\nlr = args.lr\nweight_decay = args.weight_decay\n########################################################################################\n# This is The Bootstrapy part\n\nNruns = args.n_nets\n\nweight_set_samples = []\n\np_subsample = args.subsample\n\nfor iii in range(Nruns):\n    keep_idx = []\n    for idx in range(len(trainset)):\n\n        if np.random.binomial(1, p_subsample, size=1) == 1:\n            keep_idx.append(idx)\n\n    keep_idx = np.array(keep_idx)\n\n    from torch.utils.data.sampler import SubsetRandomSampler\n\n    sampler = SubsetRandomSampler(keep_idx)\n\n    if use_cuda:\n        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False, pin_memory=True,\n                                                  num_workers=3, sampler=sampler)\n        valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n                                                num_workers=3)\n\n    else:\n        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False, pin_memory=False,\n                                                  num_workers=3, sampler=sampler)\n        valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n                                                num_workers=3)\n\n    ###############################################################\n    net = Bootstrap_Net(lr=lr, channels_in=1, side_in=28, cuda=use_cuda, classes=10, batch_size=batch_size,\n                        weight_decay=weight_decay, n_hid=1200)\n\n    epoch = 0\n\n    ## ---------------------------------------------------------------------------------------------------------------------\n    # train\n    cprint(\'c\', \'\\nTrain:\')\n\n    print(\'  init cost variables:\')\n    pred_cost_train = np.zeros(nb_epochs)\n    err_train = np.zeros(nb_epochs)\n\n    cost_dev = np.zeros(nb_epochs)\n    err_dev = np.zeros(nb_epochs)\n    # best_cost = np.inf\n    best_err = np.inf\n\n    nb_its_dev = 1\n\n    tic0 = time.time()\n    for i in range(epoch, nb_epochs):\n\n        net.set_mode_train(True)\n\n        tic = time.time()\n        nb_samples = 0\n\n        for x, y in trainloader:\n            cost_pred, err = net.fit(x, y)\n\n            err_train[i] += err\n            pred_cost_train[i] += cost_pred\n            nb_samples += len(x)\n\n        pred_cost_train[i] /= nb_samples\n        err_train[i] /= nb_samples\n\n        toc = time.time()\n        net.epoch = i\n        # ---- print\n        print(""it %d/%d, Jtr_pred = %f, err = %f, "" % (i, nb_epochs, pred_cost_train[i], err_train[i]), end="""")\n        cprint(\'r\', \'   time: %f seconds\\n\' % (toc - tic))\n\n        # ---- dev\n        if i % nb_its_dev == 0:\n            net.set_mode_train(False)\n            nb_samples = 0\n            for j, (x, y) in enumerate(valloader):\n                cost, err, probs = net.eval(x, y)\n\n                cost_dev[i] += cost\n                err_dev[i] += err\n                nb_samples += len(x)\n\n            cost_dev[i] /= nb_samples\n            err_dev[i] /= nb_samples\n\n            cprint(\'g\', \'    Jdev = %f, err = %f\\n\' % (cost_dev[i], err_dev[i]))\n\n            if err_dev[i] < best_err:\n                best_err = err_dev[i]\n\n    toc0 = time.time()\n    runtime_per_it = (toc0 - tic0) / float(nb_epochs)\n    cprint(\'r\', \'   average time: %f seconds\\n\' % runtime_per_it)\n\n    ## ---------------------------------------------------------------------------------------------------------------------\n    # results\n    cprint(\'c\', \'\\nRESULTS:\')\n    nb_parameters = net.get_nb_parameters()\n    best_cost_dev = np.min(cost_dev)\n    best_cost_train = np.min(pred_cost_train)\n    err_dev_min = err_dev[::nb_its_dev].min()\n\n    print(\'  cost_dev: %f (cost_train %f)\' % (best_cost_dev, best_cost_train))\n    print(\'  err_dev: %f\' % (err_dev_min))\n    print(\'  nb_parameters: %d (%s)\' % (nb_parameters, humansize(nb_parameters)))\n    print(\'  time_per_it: %fs\\n\' % (runtime_per_it))\n\n    ########\n    weight_set_samples.append(copy.deepcopy(net.model.state_dict()))\n\n\n    ## ---------------------------------------------------------------------------------------------------------------------\n    # fig cost vs its\n\n    textsize = 15\n    marker = 5\n\n    plt.figure(dpi=100)\n    fig, ax1 = plt.subplots()\n    ax1.plot(pred_cost_train, \'r--\')\n    ax1.plot(range(0, nb_epochs, nb_its_dev), cost_dev[::nb_its_dev], \'b-\')\n    ax1.set_ylabel(\'Cross Entropy\')\n    plt.xlabel(\'epoch\')\n    plt.grid(b=True, which=\'major\', color=\'k\', linestyle=\'-\')\n    plt.grid(b=True, which=\'minor\', color=\'k\', linestyle=\'--\')\n    lgd = plt.legend([\'train error\', \'test error\'], markerscale=marker, prop={\'size\': textsize, \'weight\': \'normal\'})\n    ax = plt.gca()\n    plt.title(\'classification costs\')\n    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n                 ax.get_xticklabels() + ax.get_yticklabels()):\n        item.set_fontsize(textsize)\n        item.set_weight(\'normal\')\n    plt.savefig(results_dir + \'/cost%d.png\' % iii, bbox_extra_artists=(lgd,), bbox_inches=\'tight\')\n\n    plt.figure(dpi=100)\n    fig2, ax2 = plt.subplots()\n    ax2.set_ylabel(\'% error\')\n    ax2.semilogy(range(0, nb_epochs, nb_its_dev), 100 * err_dev[::nb_its_dev], \'b-\')\n    ax2.semilogy(100 * err_train, \'r--\')\n    plt.xlabel(\'epoch\')\n    plt.grid(b=True, which=\'major\', color=\'k\', linestyle=\'-\')\n    plt.grid(b=True, which=\'minor\', color=\'k\', linestyle=\'--\')\n    ax2.get_yaxis().set_minor_formatter(matplotlib.ticker.ScalarFormatter())\n    ax2.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n    lgd = plt.legend([\'test error\', \'train error\'], markerscale=marker, prop={\'size\': textsize, \'weight\': \'normal\'})\n    ax = plt.gca()\n    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n                 ax.get_xticklabels() + ax.get_yticklabels()):\n        item.set_fontsize(textsize)\n        item.set_weight(\'normal\')\n    plt.savefig(results_dir + \'/err%d.png\' % iii, bbox_extra_artists=(lgd,), box_inches=\'tight\')\n\n\nsave_object(weight_set_samples, models_dir+\'/state_dicts.pkl\')\n'"
train_KFLaplace_MNIST.py,6,"b'from __future__ import division, print_function\nimport time\nimport torch.utils.data\nfrom torchvision import transforms, datasets\nimport argparse\nimport matplotlib\nfrom src.KF_Laplace.model import *\nfrom src.KF_Laplace.hessian_operations import chol_scale_invert_kron_factor\n\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\n\nparser = argparse.ArgumentParser(description=\'Train Bayesian Neural Net on MNIST with MC Dropout Variational Inference\')\n\nparser.add_argument(\'--weight_decay\', type=float, nargs=\'?\', action=\'store\', default=0.01,\n                    help=\'Specify the precision of an isotropic Gaussian prior. Default: 0.01\')\nparser.add_argument(\'--hessian_diag_sig\', type=float, nargs=\'?\', action=\'store\', default=0.15,\n                    help=\'Specify Gaussian prior std for a diagonal term that is added to the approximate hessian. Default: 0.15.\')\nparser.add_argument(\'--epochs\', type=int, nargs=\'?\', action=\'store\', default=10,\n                    help=\'How many epochs to train. Default: 10.\')\nparser.add_argument(\'--lr\', type=float, nargs=\'?\', action=\'store\', default=1e-3,\n                    help=\'learning rate. Default: 1e-3.\')\nparser.add_argument(\'--models_dir\', type=str, nargs=\'?\', action=\'store\', default=\'KFLaplace_models\',\n                    help=\'Where to save learnt weights, train vectors and Hessian params. Default: \\\'KFLaplace_models\\\'.\')\nparser.add_argument(\'--results_dir\', type=str, nargs=\'?\', action=\'store\', default=\'KFLaplace_results\',\n                    help=\'Where to save learnt training plots. Default: \\\'KFLaplace_results\\\'.\')\nargs = parser.parse_args()\n\n\n\n# Where to save models weights\nmodels_dir = args.models_dir\n# Where to save plots and error, accuracy vectors\nresults_dir = args.results_dir\n\nmkdir(models_dir)\nmkdir(results_dir)\n# ------------------------------------------------------------------------------------------------------\n# train config\nNTrainPointsMNIST = 60000\nbatch_size = 128\nnb_epochs = args.epochs\nlog_interval = 1\n\n\n# ------------------------------------------------------------------------------------------------------\n# dataset\ncprint(\'c\', \'\\nData:\')\n\n# load data\n\n# data augmentation\ntransform_train = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n])\n\nuse_cuda = torch.cuda.is_available()\n\ntrainset = datasets.MNIST(root=\'../data\', train=True, download=True, transform=transform_train)\nvalset = datasets.MNIST(root=\'../data\', train=False, download=True, transform=transform_test)\n\nif use_cuda:\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n                                              num_workers=3)\n    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n                                            num_workers=3)\n\nelse:\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n                                              num_workers=3)\n    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n                                            num_workers=3)\n\n## ---------------------------------------------------------------------------------------------------------------------\n# net dims\ncprint(\'c\', \'\\nNetwork:\')\n\nlr = args.lr\nprior_sig = np.sqrt(1/args.weight_decay)\n########################################################################################\n\n\n\nnet = KBayes_Net(lr=lr, channels_in=1, side_in=28, cuda=use_cuda, classes=10, n_hid=1200, batch_size=batch_size, prior_sig=prior_sig)\n\n## ---------------------------------------------------------------------------------------------------------------------\n# train\nepoch = 0\ncprint(\'c\', \'\\nTrain:\')\n\nprint(\'  init cost variables:\')\npred_cost_train = np.zeros(nb_epochs)\nerr_train = np.zeros(nb_epochs)\n\ncost_dev = np.zeros(nb_epochs)\nerr_dev = np.zeros(nb_epochs)\nbest_err = np.inf\n\nnb_its_dev = 1\n\ntic0 = time.time()\nfor i in range(epoch, nb_epochs):\n\n    net.set_mode_train(True)\n    tic = time.time()\n    nb_samples = 0\n\n    for x, y in trainloader:\n        cost_pred, err = net.fit(x, y)\n\n        err_train[i] += err\n        pred_cost_train[i] += cost_pred\n        nb_samples += len(x)\n\n    pred_cost_train[i] /= nb_samples\n    err_train[i] /= nb_samples\n\n    toc = time.time()\n    net.epoch = i\n    # ---- print\n    print(""it %d/%d, Jtr_pred = %f, err = %f, "" % (i, nb_epochs, pred_cost_train[i], err_train[i]), end="""")\n    cprint(\'r\', \'   time: %f seconds\\n\' % (toc - tic))\n\n\n    # ---- dev\n    if i % nb_its_dev == 0:\n        net.set_mode_train(False)\n        nb_samples = 0\n        for j, (x, y) in enumerate(valloader):\n            cost, err, probs = net.eval(x, y)\n\n            cost_dev[i] += cost\n            err_dev[i] += err\n            nb_samples += len(x)\n\n        cost_dev[i] /= nb_samples\n        err_dev[i] /= nb_samples\n\n        cprint(\'g\', \'    Jdev = %f, err = %f\\n\' % (cost_dev[i], err_dev[i]))\n\n        if err_dev[i] < best_err:\n            best_err = err_dev[i]\n            cprint(\'b\', \'best test error\')\n            net.save(models_dir+\'/theta_best.dat\')\n\ntoc0 = time.time()\nruntime_per_it = (toc0 - tic0) / float(nb_epochs)\ncprint(\'r\', \'   average time: %f seconds\\n\' % runtime_per_it)\n\nnet.save(models_dir+\'/theta_last.dat\')\n\n## ---------------------------------------------------------------------------------------------------------------------\n# results\ncprint(\'c\', \'\\nRESULTS:\')\nnb_parameters = net.get_nb_parameters()\nbest_cost_dev = np.min(cost_dev)\nbest_cost_train = np.min(pred_cost_train)\nerr_dev_min = err_dev[::nb_its_dev].min()\n\nprint(\'  cost_dev: %f (cost_train %f)\' % (best_cost_dev, best_cost_train))\nprint(\'  err_dev: %f\' % (err_dev_min))\nprint(\'  nb_parameters: %d (%s)\' % (nb_parameters, humansize(nb_parameters)))\nprint(\'  time_per_it: %fs\\n\' % (runtime_per_it))\n\n## Save results for plots\nnp.save(results_dir + \'/cost_train.npy\', pred_cost_train)\nnp.save(results_dir + \'/cost_dev.npy\', cost_dev)\nnp.save(results_dir + \'/err_train.npy\', err_train)\nnp.save(results_dir + \'/err_dev.npy\', err_dev)\n\n## Time to do Laplace Approximation.\nprint(\'MAP configuration reached: Calculating block diagonal Hessian.\')\n\n# Get hessian factors\nEQ1, EHH1, MAP1, EQ2, EHH2, MAP2, EQ3, EHH3, MAP3 = net.get_K_laplace_params(trainloader)\n\nh_params = [EQ1, EHH1, MAP1, EQ2, EHH2, MAP2, EQ3, EHH3, MAP3]\nsave_object(h_params, models_dir+\'/block_hessian_params.pkl\')\nprint(\'Hessian Parameters Saved\')\n\ndata_scale = np.sqrt(len(trainset))\nprior_sig = args.hessian_diag_sig\nprior_prec = 1/prior_sig**2\nprior_scale = np.sqrt(prior_prec)\n\n# Scale and invert factors\n# upper_Qinv, lower_HHinv\nprint(\'Scaling and inverting Hessian factors\')\nscale_inv_EQ1 = chol_scale_invert_kron_factor(EQ1, prior_scale, data_scale, upper=True)\nscale_inv_EHH1 = chol_scale_invert_kron_factor(EHH1, prior_scale, data_scale, upper=False)\n\nscale_inv_EQ2 = chol_scale_invert_kron_factor(EQ2, prior_scale, data_scale, upper=True)\nscale_inv_EHH2 = chol_scale_invert_kron_factor(EHH2, prior_scale, data_scale, upper=False)\n\nscale_inv_EQ3 = chol_scale_invert_kron_factor(EQ3, prior_scale, data_scale, upper=True)\nscale_inv_EHH3 = chol_scale_invert_kron_factor(EHH3, prior_scale, data_scale, upper=False)\n\n\n\n## ---------------------------------------------------------------------------------------------------------------------\n# fig cost vs its\n\ntextsize = 15\nmarker = 5\n\nplt.figure(dpi=100)\nfig, ax1 = plt.subplots()\nax1.plot(range(0, nb_epochs, nb_its_dev), cost_dev[::nb_its_dev], \'b-\')\nax1.plot(pred_cost_train, \'r--\')\nax1.set_ylabel(\'Cross Entropy\')\nplt.xlabel(\'epoch\')\nplt.grid(b=True, which=\'major\', color=\'k\', linestyle=\'-\')\nplt.grid(b=True, which=\'minor\', color=\'k\', linestyle=\'--\')\nlgd = plt.legend([\'test error\', \'train error\'], markerscale=marker, prop={\'size\': textsize, \'weight\': \'normal\'})\nax = plt.gca()\nplt.title(\'classification costs\')\nfor item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n             ax.get_xticklabels() + ax.get_yticklabels()):\n    item.set_fontsize(textsize)\n    item.set_weight(\'normal\')\nplt.savefig(results_dir + \'/cost.png\', bbox_extra_artists=(lgd,), bbox_inches=\'tight\')\n\nplt.figure(dpi=100)\nfig2, ax2 = plt.subplots()\nax2.set_ylabel(\'% error\')\nax2.semilogy(range(0, nb_epochs, nb_its_dev), 100 * err_dev[::nb_its_dev], \'b-\')\nax2.semilogy(100 * err_train, \'r--\')\nplt.xlabel(\'epoch\')\nplt.grid(b=True, which=\'major\', color=\'k\', linestyle=\'-\')\nplt.grid(b=True, which=\'minor\', color=\'k\', linestyle=\'--\')\nax2.get_yaxis().set_minor_formatter(matplotlib.ticker.ScalarFormatter())\nax2.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\nlgd = plt.legend([\'test error\', \'train error\'], markerscale=marker, prop={\'size\': textsize, \'weight\': \'normal\'})\nax = plt.gca()\nfor item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n             ax.get_xticklabels() + ax.get_yticklabels()):\n    item.set_fontsize(textsize)\n    item.set_weight(\'normal\')\nplt.savefig(results_dir + \'/err.png\', bbox_extra_artists=(lgd,), box_inches=\'tight\')'"
train_MCDropout_MNIST.py,6,"b'from __future__ import division, print_function\nimport time\nimport torch.utils.data\nfrom torchvision import transforms, datasets\nimport argparse\nimport matplotlib\nfrom src.MC_dropout.model import *\n\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\n\nparser = argparse.ArgumentParser(description=\'Train Bayesian Neural Net on MNIST with MC Dropout Variational Inference\')\n\nparser.add_argument(\'--weight_decay\', type=float, nargs=\'?\', action=\'store\', default=1,\n                    help=\'Specify the precision of an isotropic Gaussian prior. Default: 1.\')\nparser.add_argument(\'--epochs\', type=int, nargs=\'?\', action=\'store\', default=60,\n                    help=\'How many epochs to train. Default: 60.\')\nparser.add_argument(\'--lr\', type=float, nargs=\'?\', action=\'store\', default=1e-3,\n                    help=\'learning rate. Default: 1e-3.\')\nparser.add_argument(\'--models_dir\', type=str, nargs=\'?\', action=\'store\', default=\'MCdrop_models\',\n                    help=\'Where to save learnt weights and train vectors. Default: \\\'MCdrop_models\\\'.\')\nparser.add_argument(\'--results_dir\', type=str, nargs=\'?\', action=\'store\', default=\'MCdrop_results\',\n                    help=\'Where to save learnt training plots. Default: \\\'MCdrop_results\\\'.\')\nargs = parser.parse_args()\n\n\n\n# Where to save models weights\nmodels_dir = args.models_dir\n# Where to save plots and error, accuracy vectors\nresults_dir = args.results_dir\n\nmkdir(models_dir)\nmkdir(results_dir)\n# ------------------------------------------------------------------------------------------------------\n# train config\nNTrainPointsMNIST = 60000\nbatch_size = 128\nnb_epochs = args.epochs\nlog_interval = 1\n\n\n# ------------------------------------------------------------------------------------------------------\n# dataset\ncprint(\'c\', \'\\nData:\')\n\n# load data\n\n# data augmentation\ntransform_train = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n])\n\nuse_cuda = torch.cuda.is_available()\n\ntrainset = datasets.MNIST(root=\'../data\', train=True, download=True, transform=transform_train)\nvalset = datasets.MNIST(root=\'../data\', train=False, download=True, transform=transform_test)\n\nif use_cuda:\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n                                              num_workers=3)\n    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n                                            num_workers=3)\n\nelse:\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n                                              num_workers=3)\n    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n                                            num_workers=3)\n\n## ---------------------------------------------------------------------------------------------------------------------\n# net dims\ncprint(\'c\', \'\\nNetwork:\')\n\nlr = args.lr\n########################################################################################\n\nnet = MC_drop_net(lr=lr, channels_in=1, side_in=28, cuda=use_cuda, classes=10, batch_size=batch_size,\n                  weight_decay=args.weight_decay, n_hid=1200)\n\n## ---------------------------------------------------------------------------------------------------------------------\n# train\nepoch = 0\ncprint(\'c\', \'\\nTrain:\')\n\nprint(\'  init cost variables:\')\nkl_cost_train = np.zeros(nb_epochs)\npred_cost_train = np.zeros(nb_epochs)\nerr_train = np.zeros(nb_epochs)\n\ncost_dev = np.zeros(nb_epochs)\nerr_dev = np.zeros(nb_epochs)\nbest_err = np.inf\n\nnb_its_dev = 1\n\ntic0 = time.time()\nfor i in range(epoch, nb_epochs):\n\n    net.set_mode_train(True)\n    tic = time.time()\n    nb_samples = 0\n\n    for x, y in trainloader:\n        cost_pred, err = net.fit(x, y)\n\n        err_train[i] += err\n        pred_cost_train[i] += cost_pred\n        nb_samples += len(x)\n\n    pred_cost_train[i] /= nb_samples\n    err_train[i] /= nb_samples\n\n    toc = time.time()\n    net.epoch = i\n    # ---- print\n    print(""it %d/%d, Jtr_pred = %f, err = %f, "" % (i, nb_epochs, pred_cost_train[i], err_train[i]), end="""")\n    cprint(\'r\', \'   time: %f seconds\\n\' % (toc - tic))\n\n\n    # ---- dev\n    if i % nb_its_dev == 0:\n        net.set_mode_train(False)\n        nb_samples = 0\n        for j, (x, y) in enumerate(valloader):\n            cost, err, probs = net.eval(x, y)\n\n            cost_dev[i] += cost\n            err_dev[i] += err\n            nb_samples += len(x)\n\n        cost_dev[i] /= nb_samples\n        err_dev[i] /= nb_samples\n\n        cprint(\'g\', \'    Jdev = %f, err = %f\\n\' % (cost_dev[i], err_dev[i]))\n\n        if err_dev[i] < best_err:\n            best_err = err_dev[i]\n            cprint(\'b\', \'best test error\')\n            net.save(models_dir+\'/theta_best.dat\')\n\ntoc0 = time.time()\nruntime_per_it = (toc0 - tic0) / float(nb_epochs)\ncprint(\'r\', \'   average time: %f seconds\\n\' % runtime_per_it)\n\nnet.save(models_dir+\'/theta_last.dat\')\n\n## ---------------------------------------------------------------------------------------------------------------------\n# results\ncprint(\'c\', \'\\nRESULTS:\')\nnb_parameters = net.get_nb_parameters()\nbest_cost_dev = np.min(cost_dev)\nbest_cost_train = np.min(pred_cost_train)\nerr_dev_min = err_dev[::nb_its_dev].min()\n\nprint(\'  cost_dev: %f (cost_train %f)\' % (best_cost_dev, best_cost_train))\nprint(\'  err_dev: %f\' % (err_dev_min))\nprint(\'  nb_parameters: %d (%s)\' % (nb_parameters, humansize(nb_parameters)))\nprint(\'  time_per_it: %fs\\n\' % (runtime_per_it))\n\n## Save results for plots\nnp.save(results_dir + \'/cost_train.npy\', pred_cost_train)\nnp.save(results_dir + \'/cost_dev.npy\', cost_dev)\nnp.save(results_dir + \'/err_train.npy\', err_train)\nnp.save(results_dir + \'/err_dev.npy\', err_dev)\n\n## ---------------------------------------------------------------------------------------------------------------------\n# fig cost vs its\n\ntextsize = 15\nmarker = 5\n\nplt.figure(dpi=100)\nfig, ax1 = plt.subplots()\nax1.plot(range(0, nb_epochs, nb_its_dev), cost_dev[::nb_its_dev], \'b-\')\nax1.plot(pred_cost_train, \'r--\')\nax1.set_ylabel(\'Cross Entropy\')\nplt.xlabel(\'epoch\')\nplt.grid(b=True, which=\'major\', color=\'k\', linestyle=\'-\')\nplt.grid(b=True, which=\'minor\', color=\'k\', linestyle=\'--\')\nlgd = plt.legend([\'test error\', \'train error\'], markerscale=marker, prop={\'size\': textsize, \'weight\': \'normal\'})\nax = plt.gca()\nplt.title(\'classification costs\')\nfor item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n             ax.get_xticklabels() + ax.get_yticklabels()):\n    item.set_fontsize(textsize)\n    item.set_weight(\'normal\')\nplt.savefig(results_dir + \'/cost.png\', bbox_extra_artists=(lgd,), bbox_inches=\'tight\')\n\nplt.figure(dpi=100)\nfig2, ax2 = plt.subplots()\nax2.set_ylabel(\'% error\')\nax2.semilogy(range(0, nb_epochs, nb_its_dev), 100 * err_dev[::nb_its_dev], \'b-\')\nax2.semilogy(100 * err_train, \'r--\')\nplt.xlabel(\'epoch\')\nplt.grid(b=True, which=\'major\', color=\'k\', linestyle=\'-\')\nplt.grid(b=True, which=\'minor\', color=\'k\', linestyle=\'--\')\nax2.get_yaxis().set_minor_formatter(matplotlib.ticker.ScalarFormatter())\nax2.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\nlgd = plt.legend([\'test error\', \'train error\'], markerscale=marker, prop={\'size\': textsize, \'weight\': \'normal\'})\nax = plt.gca()\nfor item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n             ax.get_xticklabels() + ax.get_yticklabels()):\n    item.set_fontsize(textsize)\n    item.set_weight(\'normal\')\nplt.savefig(results_dir + \'/err.png\', bbox_extra_artists=(lgd,), box_inches=\'tight\')'"
train_SGHMC_MNIST.py,6,"b'from __future__ import division, print_function\nimport time\nimport torch.utils.data\nfrom torchvision import transforms, datasets\nimport argparse\nimport matplotlib\nfrom src.Stochastic_Gradient_HMC_SA.model import BNN_cat\nfrom src.utils import *\n\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\n\nparser = argparse.ArgumentParser(description=\'Train Bayesian Neural Net on MNIST with Scale-adapted Stochastic Gradient HMC\')\nparser.add_argument(\'--epochs\', type=int, nargs=\'?\', action=\'store\', default=250,\n                    help=\'How many epochs to train. Default: 250.\')\nparser.add_argument(\'--sample_freq\', type=int, nargs=\'?\', action=\'store\', default=2,\n                    help=\'How many epochs pass between saving samples. Default: 2.\')\nparser.add_argument(\'--burn_in\', type=int, nargs=\'?\', action=\'store\', default=20,\n                    help=\'How many epochs to burn in for?. Default: 20.\')\nparser.add_argument(\'--lr\', type=float, nargs=\'?\', action=\'store\', default=1e-2,\n                    help=\'learning rate. I recommend 1e-2. Default: 1e-2.\')\nparser.add_argument(\'--models_dir\', type=str, nargs=\'?\', action=\'store\', default=\'SGHMC_models\',\n                    help=\'Where to save learnt weights and train vectors. Default: \\\'SGHMC_models\\\'.\')\nparser.add_argument(\'--results_dir\', type=str, nargs=\'?\', action=\'store\', default=\'SGHMC_results\',\n                    help=\'Where to save learnt training plots. Default: \\\'SGHMC_results\\\'.\')\nargs = parser.parse_args()\n\n\n\n# Where to save models weights\nmodels_dir = args.models_dir\n# Where to save plots and error, accuracy vectors\nresults_dir = args.results_dir\n\nmkdir(models_dir)\nmkdir(results_dir)\n# ------------------------------------------------------------------------------------------------------\n# train config\nNTrainPointsMNIST = 60000\nbatch_size = 256\nnb_epochs = args.epochs\nlog_interval = 1\nnb_its_dev = log_interval\nflat_ims=True\n# ------------------------------------------------------------------------------------------------------\n# dataset\ncprint(\'c\', \'\\nData:\')\n\n# load data\n\n# data augmentation\ntransform_train = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n])\n\nuse_cuda = torch.cuda.is_available()\n\ntrainset = datasets.MNIST(root=\'../data\', train=True, download=True, transform=transform_train)\nvalset = datasets.MNIST(root=\'../data\', train=False, download=True, transform=transform_test)\n\nif use_cuda:\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n                                              num_workers=3)\n    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n                                            num_workers=3)\n\nelse:\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n                                              num_workers=3)\n    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n                                            num_workers=3)\n\n## ---------------------------------------------------------------------------------------------------------------------\n# net dims\ncprint(\'c\', \'\\nNetwork:\')\n\nlr = args.lr\n########################################################################################\n\n\nnet = BNN_cat(NTrainPointsMNIST, lr=lr, cuda=use_cuda, grad_std_mul=20)\n\n\n## weight saving parameters #######\nburn_in = args.burn_in\nsim_steps = args.sample_freq\nN_saves=100\nresample_its = 50\nresample_prior_its = 15\nre_burn = 1e8\n###################################\n\n## ---------------------------------------------------------------------------------------------------------------------\n\n\n\n# net dims\nepoch = 0\nit_count = 0\n## ---------------------------------------------------------------------------------------------------------------------\n# train\ncprint(\'c\', \'\\nTrain:\')\n\nprint(\'  init cost variables:\')\ncost_train = np.zeros(nb_epochs)\nerr_train = np.zeros(nb_epochs)\ncost_dev = np.zeros(nb_epochs)\nerr_dev = np.zeros(nb_epochs)\nbest_cost = np.inf\nbest_err = np.inf\n\ntic0 = time.time()\nfor i in range(epoch, nb_epochs):\n    net.set_mode_train(True)\n    tic = time.time()\n    nb_samples = 0\n    for x, y in trainloader:\n\n        if flat_ims:\n            x = x.view(x.shape[0], -1)\n\n        cost_pred, err = net.fit(x, y, burn_in=(i % re_burn < burn_in),\n                                 resample_momentum=(it_count % resample_its == 0),\n                                 resample_prior=(it_count % resample_prior_its == 0))\n        it_count += 1\n        err_train[i] += err\n        cost_train[i] += cost_pred\n        nb_samples += len(x)\n\n    cost_train[i] /= nb_samples\n    err_train[i] /= nb_samples\n    toc = time.time()\n\n    # ---- print\n    print(""it %d/%d, Jtr_pred = %f, err = %f, "" % (i, nb_epochs, cost_train[i], err_train[i]), end="""")\n    cprint(\'r\', \'   time: %f seconds\\n\' % (toc - tic))\n    net.update_lr(i)\n\n    # ---- save weights\n    if i % re_burn >= burn_in and i % sim_steps == 0:\n        net.save_sampled_net(max_samples=N_saves)\n\n    # ---- dev\n    if i % nb_its_dev == 0:\n        nb_samples = 0\n        for j, (x, y) in enumerate(valloader):\n            if flat_ims:\n                x = x.view(x.shape[0], -1)\n\n            cost, err, probs = net.eval(x, y)\n\n            cost_dev[i] += cost\n            err_dev[i] += err\n            nb_samples += len(x)\n\n        cost_dev[i] /= nb_samples\n        err_dev[i] /= nb_samples\n\n        cprint(\'g\', \'    Jdev = %f, err = %f\\n\' % (cost_dev[i], err_dev[i]))\n        if err_dev[i] < best_err:\n            best_err = err_dev[i]\n            cprint(\'b\', \'best test error\')\n\ntoc0 = time.time()\nruntime_per_it = (toc0 - tic0) / float(nb_epochs)\ncprint(\'r\', \'   average time: %f seconds\\n\' % runtime_per_it)\n\n## SAVE WEIGHTS\nnet.save_weights(models_dir + \'/state_dicts.pkl\')\n\n\n## ---------------------------------------------------------------------------------------------------------------------\n# fig cost vs its\n\n# fig cost vs its\ntextsize = 15\nmarker = 5\n\nplt.figure(dpi=100)\nfig, ax1 = plt.subplots()\nax1.plot(range(0, nb_epochs, nb_its_dev), np.clip(cost_dev[::nb_its_dev], a_min=-5, a_max=5), \'b-\')\nax1.plot(np.clip(cost_train, a_min=-5, a_max=5), \'r--\')\nax1.set_ylabel(\'Cross Entropy\')\nplt.xlabel(\'epoch\')\nplt.grid(b=True, which=\'major\', color=\'k\', linestyle=\'-\')\nplt.grid(b=True, which=\'minor\', color=\'k\', linestyle=\'--\')\nlgd = plt.legend([\'test error\', \'train error\'], markerscale=marker, prop={\'size\': textsize, \'weight\': \'normal\'})\nax = plt.gca()\nplt.title(\'classification costs\')\nfor item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n             ax.get_xticklabels() + ax.get_yticklabels()):\n    item.set_fontsize(textsize)\n    item.set_weight(\'normal\')\nplt.savefig(results_dir + \'/cost.png\', bbox_extra_artists=(lgd,), bbox_inches=\'tight\')\n\nplt.figure(dpi=100)\nfig2, ax2 = plt.subplots()\nax2.set_ylabel(\'% error\')\nax2.semilogy(range(0, nb_epochs, nb_its_dev), err_dev[::nb_its_dev], \'b-\')\nax2.semilogy(err_train, \'r--\')\nax2.set_ylim(top=1, bottom=1e-3)\nplt.xlabel(\'epoch\')\nplt.grid(b=True, which=\'major\', color=\'k\', linestyle=\'-\')\nplt.grid(b=True, which=\'minor\', color=\'k\', linestyle=\'--\')\nax2.get_yaxis().set_minor_formatter(matplotlib.ticker.ScalarFormatter())\nax2.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\nlgd = plt.legend([\'test error\', \'train error\'], markerscale=marker, prop={\'size\': textsize, \'weight\': \'normal\'})\nax = plt.gca()\nfor item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n             ax.get_xticklabels() + ax.get_yticklabels()):\n    item.set_fontsize(textsize)\n    item.set_weight(\'normal\')\nplt.savefig(results_dir + \'/err.png\', bbox_extra_artists=(lgd,), box_inches=\'tight\')\nplt.show()\n'"
train_SGLD_MNIST.py,6,"b'from __future__ import division, print_function\nimport time\nimport torch.utils.data\nfrom torchvision import transforms, datasets\nimport argparse\nimport matplotlib\nfrom src.Stochastic_Gradient_Langevin_Dynamics.model import *\n\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\n\nparser = argparse.ArgumentParser(description=\'Train Bayesian Neural Net on MNIST with Stochastic Gradient Langevin Dynamics\')\nparser.add_argument(\'--use_preconditioning\', type=bool, nargs=\'?\', action=\'store\', default=True,\n                    help=\'Use RMSprop preconditioning. Default: True.\')\nparser.add_argument(\'--prior_sig\', type=float, nargs=\'?\', action=\'store\', default=0.1,\n                    help=\'Standard deviation of prior. Default: 0.1.\')\nparser.add_argument(\'--epochs\', type=int, nargs=\'?\', action=\'store\', default=200,\n                    help=\'How many epochs to train. Default: 200.\')\nparser.add_argument(\'--lr\', type=float, nargs=\'?\', action=\'store\', default=1e-3,\n                    help=\'learning rate. I recommend 1e-3 if preconditioning, else 1e-4. Default: 1e-3.\')\nparser.add_argument(\'--models_dir\', type=str, nargs=\'?\', action=\'store\', default=\'SGLD_models\',\n                    help=\'Where to save learnt weights and train vectors. Default: \\\'SGLD_models\\\'.\')\nparser.add_argument(\'--results_dir\', type=str, nargs=\'?\', action=\'store\', default=\'SGLD_results\',\n                    help=\'Where to save learnt training plots. Default: \\\'SGLD_results\\\'.\')\nargs = parser.parse_args()\n\n\n\n# Where to save models weights\nmodels_dir = args.models_dir\n# Where to save plots and error, accuracy vectors\nresults_dir = args.results_dir\n\nmkdir(models_dir)\nmkdir(results_dir)\n# ------------------------------------------------------------------------------------------------------\n# train config\nNTrainPointsMNIST = 60000\nbatch_size = 128\nnb_epochs = args.epochs\nlog_interval = 1\n\n\n# ------------------------------------------------------------------------------------------------------\n# dataset\ncprint(\'c\', \'\\nData:\')\n\n# load data\n\n# data augmentation\ntransform_train = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n])\n\nuse_cuda = torch.cuda.is_available()\n\ntrainset = datasets.MNIST(root=\'../data\', train=True, download=True, transform=transform_train)\nvalset = datasets.MNIST(root=\'../data\', train=False, download=True, transform=transform_test)\n\nif use_cuda:\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=True,\n                                              num_workers=3)\n    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=True,\n                                            num_workers=3)\n\nelse:\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, pin_memory=False,\n                                              num_workers=3)\n    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, pin_memory=False,\n                                            num_workers=3)\n\n## ---------------------------------------------------------------------------------------------------------------------\n# net dims\ncprint(\'c\', \'\\nNetwork:\')\n\nlr = args.lr\n########################################################################################\n\nif args.use_preconditioning:\n    net = Net_langevin(lr=lr, channels_in=1, side_in=28, cuda=use_cuda, classes=10,\n                       prior_sig=args.prior_sig, N_train=NTrainPointsMNIST, nhid=1200, use_p=True)\nelse:\n    net = Net_langevin(lr=lr, channels_in=1, side_in=28, cuda=use_cuda, classes=10,\n                       prior_sig=args.prior_sig, N_train=NTrainPointsMNIST, nhid=1200, use_p=False)\n\n## ---------------------------------------------------------------------------------------------------------------------\n# train\nepoch = 0\ncprint(\'c\', \'\\nTrain:\')\n\n## weight saving parameters #######\nstart_save = 15\nsave_every = 2  # We sample every 2 epochs as I have found samples to be correlated after only 1\nN_saves = 100  # Max number of saves\n###################################\n\nprint(\'  init cost variables:\')\nkl_cost_train = np.zeros(nb_epochs)\npred_cost_train = np.zeros(nb_epochs)\nerr_train = np.zeros(nb_epochs)\n\ncost_dev = np.zeros(nb_epochs)\nerr_dev = np.zeros(nb_epochs)\nbest_err = np.inf\n\nnb_its_dev = 1\n\ntic0 = time.time()\nfor i in range(epoch, nb_epochs):\n\n    net.set_mode_train(True)\n    tic = time.time()\n    nb_samples = 0\n\n    for x, y in trainloader:\n        cost_pred, err = net.fit(x, y)\n\n        err_train[i] += err\n        pred_cost_train[i] += cost_pred\n        nb_samples += len(x)\n\n    pred_cost_train[i] /= nb_samples\n    err_train[i] /= nb_samples\n\n    toc = time.time()\n    net.epoch = i\n    # ---- print\n    print(""it %d/%d, Jtr_pred = %f, err = %f, "" % (i, nb_epochs, pred_cost_train[i], err_train[i]), end="""")\n    cprint(\'r\', \'   time: %f seconds\\n\' % (toc - tic))\n\n    # ---- save weights\n    if i >= start_save and i % save_every == 0:\n        net.save_sampled_net(max_samples=N_saves)\n\n    # ---- dev\n    if i % nb_its_dev == 0:\n        net.set_mode_train(False)\n        nb_samples = 0\n        for j, (x, y) in enumerate(valloader):\n            cost, err, probs = net.eval(x, y)\n\n            cost_dev[i] += cost\n            err_dev[i] += err\n            nb_samples += len(x)\n\n        cost_dev[i] /= nb_samples\n        err_dev[i] /= nb_samples\n\n        cprint(\'g\', \'    Jdev = %f, err = %f\\n\' % (cost_dev[i], err_dev[i]))\n\n        if err_dev[i] < best_err:\n            best_err = err_dev[i]\n            cprint(\'b\', \'best test error\')\n            net.save(models_dir+\'/theta_best.dat\')\n\ntoc0 = time.time()\nruntime_per_it = (toc0 - tic0) / float(nb_epochs)\ncprint(\'r\', \'   average time: %f seconds\\n\' % runtime_per_it)\n\n# Save weight samples from the posterior\nsave_object(net.weight_set_samples, models_dir+\'/state_dicts.pkl\')\n\n## ---------------------------------------------------------------------------------------------------------------------\n# results\ncprint(\'c\', \'\\nRESULTS:\')\nnb_parameters = net.get_nb_parameters()\nbest_cost_dev = np.min(cost_dev)\nbest_cost_train = np.min(pred_cost_train)\nerr_dev_min = err_dev[::nb_its_dev].min()\n\nprint(\'  cost_dev: %f (cost_train %f)\' % (best_cost_dev, best_cost_train))\nprint(\'  err_dev: %f\' % (err_dev_min))\nprint(\'  nb_parameters: %d (%s)\' % (nb_parameters, humansize(nb_parameters)))\nprint(\'  time_per_it: %fs\\n\' % (runtime_per_it))\n\n## Save results for plots\nnp.save(results_dir + \'/cost_train.npy\', pred_cost_train)\nnp.save(results_dir + \'/cost_dev.npy\', cost_dev)\nnp.save(results_dir + \'/err_train.npy\', err_train)\nnp.save(results_dir + \'/err_dev.npy\', err_dev)\n\n## ---------------------------------------------------------------------------------------------------------------------\n# fig cost vs its\n\ntextsize = 15\nmarker = 5\n\nplt.figure(dpi=100)\nfig, ax1 = plt.subplots()\nax1.plot(range(0, nb_epochs, nb_its_dev), cost_dev[::nb_its_dev], \'b-\')\nax1.plot(pred_cost_train, \'r--\')\nax1.set_ylabel(\'Cross Entropy\')\nplt.xlabel(\'epoch\')\nplt.grid(b=True, which=\'major\', color=\'k\', linestyle=\'-\')\nplt.grid(b=True, which=\'minor\', color=\'k\', linestyle=\'--\')\nlgd = plt.legend([\'test error\', \'train error\'], markerscale=marker, prop={\'size\': textsize, \'weight\': \'normal\'})\nax = plt.gca()\nplt.title(\'classification costs\')\nfor item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n             ax.get_xticklabels() + ax.get_yticklabels()):\n    item.set_fontsize(textsize)\n    item.set_weight(\'normal\')\nplt.savefig(results_dir + \'/cost.png\', bbox_extra_artists=(lgd,), bbox_inches=\'tight\')\n\nplt.figure(dpi=100)\nfig2, ax2 = plt.subplots()\nax2.set_ylabel(\'% error\')\nax2.semilogy(range(0, nb_epochs, nb_its_dev), 100 * err_dev[::nb_its_dev], \'b-\')\nax2.semilogy(100 * err_train, \'r--\')\nplt.xlabel(\'epoch\')\nplt.grid(b=True, which=\'major\', color=\'k\', linestyle=\'-\')\nplt.grid(b=True, which=\'minor\', color=\'k\', linestyle=\'--\')\nax2.get_yaxis().set_minor_formatter(matplotlib.ticker.ScalarFormatter())\nax2.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\nlgd = plt.legend([\'test error\', \'train error\'], markerscale=marker, prop={\'size\': textsize, \'weight\': \'normal\'})\nax = plt.gca()\nfor item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n             ax.get_xticklabels() + ax.get_yticklabels()):\n    item.set_fontsize(textsize)\n    item.set_weight(\'normal\')\nplt.savefig(results_dir + \'/err.png\', bbox_extra_artists=(lgd,), box_inches=\'tight\')'"
src/__init__.py,0,b''
src/base_net.py,2,"b""from src.utils import *\n\n\nclass BaseNet(object):\n    def __init__(self):\n        cprint('c', '\\nNet:')\n\n    def get_nb_parameters(self):\n        return np.sum(p.numel() for p in self.model.parameters())\n\n    def set_mode_train(self, train=True):\n        if train:\n            self.model.train()\n        else:\n            self.model.eval()\n\n    def update_lr(self, epoch, gamma=0.99):\n        self.epoch += 1\n        if self.schedule is not None:\n            if len(self.schedule) == 0 or epoch in self.schedule:\n                self.lr *= gamma\n                print('learning rate: %f  (%d)\\n' % self.lr, epoch)\n                for param_group in self.optimizer.param_groups:\n                    param_group['lr'] = self.lr\n\n    def save(self, filename):\n        cprint('c', 'Writting %s\\n' % filename)\n        torch.save({\n            'epoch': self.epoch,\n            'lr': self.lr,\n            'model': self.model,\n            'optimizer': self.optimizer}, filename)\n\n    def load(self, filename):\n        cprint('c', 'Reading %s\\n' % filename)\n        state_dict = torch.load(filename)\n        self.epoch = state_dict['epoch']\n        self.lr = state_dict['lr']\n        self.model = state_dict['model']\n        self.optimizer = state_dict['optimizer']\n        print('  restoring epoch: %d, lr: %f' % (self.epoch, self.lr))\n        return self.epoch"""
src/priors.py,6,"b'from __future__ import division\nimport numpy as np\nimport torch\n\n\ndef isotropic_gauss_loglike(x, mu, sigma, do_sum=True):\n    cte_term = -(0.5) * np.log(2 * np.pi)\n    det_sig_term = -torch.log(sigma)\n    inner = (x - mu) / sigma\n    dist_term = -(0.5) * (inner ** 2)\n\n    if do_sum:\n        out = (cte_term + det_sig_term + dist_term).sum()  # sum over all weights\n    else:\n        out = (cte_term + det_sig_term + dist_term)\n    return out\n\n\nclass laplace_prior(object):\n    def __init__(self, mu, b):\n        self.mu = mu\n        self.b = b\n\n    def loglike(self, x, do_sum=True):\n        if do_sum:\n            return (-np.log(2 * self.b) - torch.abs(x - self.mu) / self.b).sum()\n        else:\n            return (-np.log(2 * self.b) - torch.abs(x - self.mu) / self.b)\n\n\nclass isotropic_gauss_prior(object):\n    def __init__(self, mu, sigma):\n        self.mu = mu\n        self.sigma = sigma\n\n        self.cte_term = -(0.5) * np.log(2 * np.pi)\n        self.det_sig_term = -np.log(self.sigma)\n\n    def loglike(self, x, do_sum=True):\n\n        dist_term = -(0.5) * ((x - self.mu) / self.sigma) ** 2\n        if do_sum:\n            return (self.cte_term + self.det_sig_term + dist_term).sum()\n        else:\n            return (self.cte_term + self.det_sig_term + dist_term)\n\n\nclass spike_slab_2GMM(object):\n    def __init__(self, mu1, mu2, sigma1, sigma2, pi):\n        self.N1 = isotropic_gauss_prior(mu1, sigma1)\n        self.N2 = isotropic_gauss_prior(mu2, sigma2)\n\n        self.pi1 = pi\n        self.pi2 = (1 - pi)\n\n    def loglike(self, x):\n        N1_ll = self.N1.loglike(x)\n        N2_ll = self.N2.loglike(x)\n\n        # Numerical stability trick -> unnormalising logprobs will underflow otherwise\n        max_loglike = torch.max(N1_ll, N2_ll)\n        normalised_like = self.pi1 * torch.exp(N1_ll - max_loglike) + self.pi2 * torch.exp(N2_ll - max_loglike)\n        loglike = torch.log(normalised_like) + max_loglike\n\n        return loglike'"
src/utils.py,5,"b'from __future__ import print_function, division\nimport torch\nfrom torch.autograd import Variable\nfrom PIL import Image\nimport torch.utils.data as data\nimport numpy as np\nimport os\nimport sys\ntry:\n    import cPickle as pickle\nexcept:\n    import pickle\n\n\ndef load_object(filename):\n    with open(filename, \'rb\') as input:\n        return pickle.load(input)\n\n\ndef save_object(obj, filename):\n    with open(filename, \'wb\') as output:  # Overwrites any existing file.\n        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n\n\ndef mkdir(paths):\n    if not isinstance(paths, (list, tuple)):\n        paths = [paths]\n    for path in paths:\n        if not os.path.isdir(path):\n            os.makedirs(path)\n\n\nsuffixes = [\'B\', \'KB\', \'MB\', \'GB\', \'TB\', \'PB\']\n\n\ndef humansize(nbytes):\n    i = 0\n    while nbytes >= 1024 and i < len(suffixes) - 1:\n        nbytes /= 1024.\n        i += 1\n    f = (\'%.2f\' % nbytes)\n    return \'%s%s\' % (f, suffixes[i])\n\n\ndef get_num_batches(nb_samples, batch_size, roundup=True):\n    if roundup:\n        return ((nb_samples + (-nb_samples % batch_size)) / batch_size)  # roundup division\n    else:\n        return nb_samples / batch_size\n\n\ndef generate_ind_batch(nb_samples, batch_size, random=True, roundup=True):\n    if random:\n        ind = np.random.permutation(nb_samples)\n    else:\n        ind = range(int(nb_samples))\n    for i in range(int(get_num_batches(nb_samples, batch_size, roundup))):\n        yield ind[i * batch_size: (i + 1) * batch_size]\n\n\ndef to_variable(var=(), cuda=True, volatile=False):\n    out = []\n    for v in var:\n        if isinstance(v, np.ndarray):\n            v = torch.from_numpy(v).type(torch.FloatTensor)\n\n        if not v.is_cuda and cuda:\n            v = v.cuda()\n\n        if not isinstance(v, Variable):\n            v = Variable(v, volatile=volatile)\n\n        out.append(v)\n    return out\n\n\ndef cprint(color, text, **kwargs):\n    if color[0] == \'*\':\n        pre_code = \'1;\'\n        color = color[1:]\n    else:\n        pre_code = \'\'\n    code = {\n        \'a\': \'30\',\n        \'r\': \'31\',\n        \'g\': \'32\',\n        \'y\': \'33\',\n        \'b\': \'34\',\n        \'p\': \'35\',\n        \'c\': \'36\',\n        \'w\': \'37\'\n    }\n    print(""\\x1b[%s%sm%s\\x1b[0m"" % (pre_code, code[color], text), **kwargs)\n    sys.stdout.flush()\n\n\ndef shuffle_in_unison_scary(a, b):\n    rng_state = np.random.get_state()\n    np.random.shuffle(a)\n    np.random.set_state(rng_state)\n    np.random.shuffle(b)\n\n\nclass Datafeed(data.Dataset):\n\n    def __init__(self, x_train, y_train, transform=None):\n        self.x_train = x_train\n        self.y_train = y_train\n        self.transform = transform\n\n    def __getitem__(self, index):\n        img = self.x_train[index]\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, self.y_train[index]\n\n    def __len__(self):\n        return len(self.x_train)\n\nclass DatafeedImage(data.Dataset):\n    def __init__(self, x_train, y_train, transform=None):\n        self.x_train = x_train\n        self.y_train = y_train\n        self.transform = transform\n\n    def __getitem__(self, index):\n        img = self.x_train[index]\n        img = Image.fromarray(np.uint8(img))\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, self.y_train[index]\n\n    def __len__(self):\n        return len(self.x_train)\n\n\n### functions for BNN with gauss output: ###\n\ndef diagonal_gauss_loglike(x, mu, sigma):\n    # note that we can just treat each dim as isotropic and then do sum\n    cte_term = -(0.5)*np.log(2*np.pi)\n    det_sig_term = -torch.log(sigma)\n    inner = (x - mu)/sigma\n    dist_term = -(0.5)*(inner**2)\n    log_px = (cte_term + det_sig_term + dist_term).sum(dim=1, keepdim=False)\n    return log_px\n\ndef get_rms(mu, y, y_means, y_stds):\n    x_un = mu * y_stds + y_means\n    y_un = y * y_stds + y_means\n    return torch.sqrt(((x_un - y_un)**2).sum() / y.shape[0])\n\n\ndef get_loglike(mu, sigma, y, y_means, y_stds):\n    mu_un = mu * y_stds + y_means\n    y_un = y * y_stds + y_means\n    sigma_un = sigma * y_stds\n    ll = diagonal_gauss_loglike(y_un, mu_un, sigma_un)\n    return ll.mean(dim=0)\n'"
src/Bayes_By_Backprop/__init__.py,0,b''
src/Bayes_By_Backprop/model.py,17,"b'from src.priors import *\n\nfrom src.base_net import *\n\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport copy\n\ndef sample_weights(W_mu, b_mu, W_p, b_p):\n    """"""Quick method for sampling weights and exporting weights""""""\n    eps_W = W_mu.data.new(W_mu.size()).normal_()\n    # sample parameters\n    std_w = 1e-6 + F.softplus(W_p, beta=1, threshold=20)\n    W = W_mu + 1 * std_w * eps_W\n\n    if b_mu is not None:\n        std_b = 1e-6 + F.softplus(b_p, beta=1, threshold=20)\n        eps_b = b_mu.data.new(b_mu.size()).normal_()\n        b = b_mu + 1 * std_b * eps_b\n    else:\n        b = None\n\n    return W, b\n\n\nclass BayesLinear_Normalq(nn.Module):\n    """"""Linear Layer where weights are sampled from a fully factorised Normal with learnable parameters. The likelihood\n     of the weight samples under the prior and the approximate posterior are returned with each forward pass in order\n     to estimate the KL term in the ELBO.\n    """"""\n    def __init__(self, n_in, n_out, prior_class):\n        super(BayesLinear_Normalq, self).__init__()\n        self.n_in = n_in\n        self.n_out = n_out\n        self.prior = prior_class\n\n        # Learnable parameters -> Initialisation is set empirically.\n        self.W_mu = nn.Parameter(torch.Tensor(self.n_in, self.n_out).uniform_(-0.1, 0.1))\n        self.W_p = nn.Parameter(torch.Tensor(self.n_in, self.n_out).uniform_(-3, -2))\n\n        self.b_mu = nn.Parameter(torch.Tensor(self.n_out).uniform_(-0.1, 0.1))\n        self.b_p = nn.Parameter(torch.Tensor(self.n_out).uniform_(-3, -2))\n\n        self.lpw = 0\n        self.lqw = 0\n\n    def forward(self, X, sample=False):\n        #         print(self.training)\n\n        if not self.training and not sample:  # When training return MLE of w for quick validation\n            output = torch.mm(X, self.W_mu) + self.b_mu.expand(X.size()[0], self.n_out)\n            return output, 0, 0\n\n        else:\n\n            # Tensor.new()  Constructs a new tensor of the same data type as self tensor.\n            # the same random sample is used for every element in the minibatch\n            eps_W = Variable(self.W_mu.data.new(self.W_mu.size()).normal_())\n            eps_b = Variable(self.b_mu.data.new(self.b_mu.size()).normal_())\n\n            # sample parameters\n            std_w = 1e-6 + F.softplus(self.W_p, beta=1, threshold=20)\n            std_b = 1e-6 + F.softplus(self.b_p, beta=1, threshold=20)\n\n            W = self.W_mu + 1 * std_w * eps_W\n            b = self.b_mu + 1 * std_b * eps_b\n\n            output = torch.mm(X, W) + b.unsqueeze(0).expand(X.shape[0], -1)  # (batch_size, n_output)\n\n            lqw = isotropic_gauss_loglike(W, self.W_mu, std_w) + isotropic_gauss_loglike(b, self.b_mu, std_b)\n            lpw = self.prior.loglike(W) + self.prior.loglike(b)\n            return output, lqw, lpw\n\n\n\nclass bayes_linear_2L(nn.Module):\n    """"""2 hidden layer Bayes By Backprop (VI) Network""""""\n    def __init__(self, input_dim, output_dim, n_hid, prior_instance):\n        super(bayes_linear_2L, self).__init__()\n\n        # prior_instance = isotropic_gauss_prior(mu=0, sigma=0.1)\n        # prior_instance = spike_slab_2GMM(mu1=0, mu2=0, sigma1=0.135, sigma2=0.001, pi=0.5)\n        # prior_instance = isotropic_gauss_prior(mu=0, sigma=0.1)\n        self.prior_instance = prior_instance\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        self.bfc1 = BayesLinear_Normalq(input_dim, n_hid, self.prior_instance)\n        self.bfc2 = BayesLinear_Normalq(n_hid, n_hid, self.prior_instance)\n        self.bfc3 = BayesLinear_Normalq(n_hid, output_dim, self.prior_instance)\n\n        # choose your non linearity\n        # self.act = nn.Tanh()\n        # self.act = nn.Sigmoid()\n        self.act = nn.ReLU(inplace=True)\n        # self.act = nn.ELU(inplace=True)\n        # self.act = nn.SELU(inplace=True)\n\n    def forward(self, x, sample=False):\n        tlqw = 0\n        tlpw = 0\n\n        x = x.view(-1, self.input_dim)  # view(batch_size, input_dim)\n        # -----------------\n        x, lqw, lpw = self.bfc1(x, sample)\n        tlqw = tlqw + lqw\n        tlpw = tlpw + lpw\n        # -----------------\n        x = self.act(x)\n        # -----------------\n        x, lqw, lpw = self.bfc2(x, sample)\n        tlqw = tlqw + lqw\n        tlpw = tlpw + lpw\n        # -----------------\n        x = self.act(x)\n        # -----------------\n        y, lqw, lpw = self.bfc3(x, sample)\n        tlqw = tlqw + lqw\n        tlpw = tlpw + lpw\n\n        return y, tlqw, tlpw\n\n    def sample_predict(self, x, Nsamples):\n        """"""Used for estimating the data\'s likelihood by approximately marginalising the weights with MC""""""\n        # Just copies type from x, initializes new vector\n        predictions = x.data.new(Nsamples, x.shape[0], self.output_dim)\n        tlqw_vec = np.zeros(Nsamples)\n        tlpw_vec = np.zeros(Nsamples)\n\n        for i in range(Nsamples):\n            y, tlqw, tlpw = self.forward(x, sample=True)\n            predictions[i] = y\n            tlqw_vec[i] = tlqw\n            tlpw_vec[i] = tlpw\n\n        return predictions, tlqw_vec, tlpw_vec\n\nclass BBP_Bayes_Net(BaseNet):\n    """"""Full network wrapper for Bayes By Backprop nets with methods for training, prediction and weight prunning""""""\n    eps = 1e-6\n\n    def __init__(self, lr=1e-3, channels_in=3, side_in=28, cuda=True, classes=10, batch_size=128, Nbatches=0,\n                 nhid=1200, prior_instance=laplace_prior(mu=0, b=0.1)):\n        super(BBP_Bayes_Net, self).__init__()\n        cprint(\'y\', \' Creating Net!! \')\n        self.lr = lr\n        self.schedule = None  # [] #[50,200,400,600]\n        self.cuda = cuda\n        self.channels_in = channels_in\n        self.classes = classes\n        self.batch_size = batch_size\n        self.Nbatches = Nbatches\n        self.prior_instance = prior_instance\n        self.nhid = nhid\n        self.side_in = side_in\n        self.create_net()\n        self.create_opt()\n        self.epoch = 0\n\n        self.test = False\n\n    def create_net(self):\n        torch.manual_seed(42)\n        if self.cuda:\n            torch.cuda.manual_seed(42)\n\n        self.model = bayes_linear_2L(input_dim=self.channels_in * self.side_in * self.side_in,\n                                     output_dim=self.classes, n_hid=self.nhid, prior_instance=self.prior_instance)\n        if self.cuda:\n            self.model.cuda()\n        #             cudnn.benchmark = True\n\n        print(\'    Total params: %.2fM\' % (self.get_nb_parameters() / 1000000.0))\n\n    def create_opt(self):\n        #         self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(0.9, 0.999), eps=1e-08,\n        #                                           weight_decay=0)\n        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0)\n\n    #         self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9)\n    #         self.sched = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=10, last_epoch=-1)\n\n    def fit(self, x, y, samples=1):\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        self.optimizer.zero_grad()\n\n        if samples == 1:\n            out, tlqw, tlpw = self.model(x)\n            mlpdw = F.cross_entropy(out, y, reduction=\'sum\')\n            Edkl = (tlqw - tlpw) / self.Nbatches\n\n        elif samples > 1:\n            mlpdw_cum = 0\n            Edkl_cum = 0\n\n            for i in range(samples):\n                out, tlqw, tlpw = self.model(x, sample=True)\n                mlpdw_i = F.cross_entropy(out, y, reduction=\'sum\')\n                Edkl_i = (tlqw - tlpw) / self.Nbatches\n                mlpdw_cum = mlpdw_cum + mlpdw_i\n                Edkl_cum = Edkl_cum + Edkl_i\n\n            mlpdw = mlpdw_cum / samples\n            Edkl = Edkl_cum / samples\n\n        loss = Edkl + mlpdw\n        loss.backward()\n        self.optimizer.step()\n\n        # out: (batch_size, out_channels, out_caps_dims)\n        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return Edkl.data, mlpdw.data, err\n\n    def eval(self, x, y, train=False):\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out, _, _ = self.model(x)\n\n        loss = F.cross_entropy(out, y, reduction=\'sum\')\n\n        probs = F.softmax(out, dim=1).data.cpu()\n\n        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return loss.data, err, probs\n\n    def sample_eval(self, x, y, Nsamples, logits=True, train=False):\n        """"""Prediction, only returining result with weights marginalised""""""\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out, _, _ = self.model.sample_predict(x, Nsamples)\n\n        if logits:\n            mean_out = out.mean(dim=0, keepdim=False)\n            loss = F.cross_entropy(mean_out, y, reduction=\'sum\')\n            probs = F.softmax(mean_out, dim=1).data.cpu()\n\n        else:\n            mean_out = F.softmax(out, dim=2).mean(dim=0, keepdim=False)\n            probs = mean_out.data.cpu()\n\n            log_mean_probs_out = torch.log(mean_out)\n            loss = F.nll_loss(log_mean_probs_out, y, reduction=\'sum\')\n\n        pred = mean_out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return loss.data, err, probs\n\n    def all_sample_eval(self, x, y, Nsamples):\n        """"""Returns predictions for each MC sample""""""\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out, _, _ = self.model.sample_predict(x, Nsamples)\n\n        prob_out = F.softmax(out, dim=2)\n        prob_out = prob_out.data\n\n        return prob_out\n\n    def get_weight_samples(self, Nsamples=10):\n        state_dict = self.model.state_dict()\n        weight_vec = []\n\n        for i in range(Nsamples):\n            previous_layer_name = \'\'\n            for key in state_dict.keys():\n                layer_name = key.split(\'.\')[0]\n                if layer_name != previous_layer_name:\n                    previous_layer_name = layer_name\n\n                    W_mu = state_dict[layer_name + \'.W_mu\'].data\n                    W_p = state_dict[layer_name + \'.W_p\'].data\n\n                    #                 b_mu = state_dict[layer_name+\'.b_mu\'].cpu().data\n                    #                 b_p = state_dict[layer_name+\'.b_p\'].cpu().data\n\n                    W, b = sample_weights(W_mu=W_mu, b_mu=None, W_p=W_p, b_p=None)\n\n                    for weight in W.cpu().view(-1):\n                        weight_vec.append(weight)\n\n        return np.array(weight_vec)\n\n    def get_weight_SNR(self, thresh=None):\n        state_dict = self.model.state_dict()\n        weight_SNR_vec = []\n\n        if thresh is not None:\n            mask_dict = {}\n\n        previous_layer_name = \'\'\n        for key in state_dict.keys():\n            layer_name = key.split(\'.\')[0]\n            if layer_name != previous_layer_name:\n                previous_layer_name = layer_name\n\n                W_mu = state_dict[layer_name + \'.W_mu\'].data\n                W_p = state_dict[layer_name + \'.W_p\'].data\n                sig_W = 1e-6 + F.softplus(W_p, beta=1, threshold=20)\n\n                b_mu = state_dict[layer_name + \'.b_mu\'].data\n                b_p = state_dict[layer_name + \'.b_p\'].data\n                sig_b = 1e-6 + F.softplus(b_p, beta=1, threshold=20)\n\n                W_snr = (torch.abs(W_mu) / sig_W)\n                b_snr = (torch.abs(b_mu) / sig_b)\n\n                if thresh is not None:\n                    mask_dict[layer_name + \'.W\'] = W_snr > thresh\n                    mask_dict[layer_name + \'.b\'] = b_snr > thresh\n\n                else:\n\n                    for weight_SNR in W_snr.cpu().view(-1):\n                        weight_SNR_vec.append(weight_SNR)\n\n                    for weight_SNR in b_snr.cpu().view(-1):\n                        weight_SNR_vec.append(weight_SNR)\n\n        if thresh is not None:\n            return mask_dict\n        else:\n            return np.array(weight_SNR_vec)\n\n    def get_weight_KLD(self, Nsamples=20, thresh=None):\n        state_dict = self.model.state_dict()\n        weight_KLD_vec = []\n\n        if thresh is not None:\n            mask_dict = {}\n\n        previous_layer_name = \'\'\n        for key in state_dict.keys():\n            layer_name = key.split(\'.\')[0]\n            if layer_name != previous_layer_name:\n                previous_layer_name = layer_name\n\n                W_mu = state_dict[layer_name + \'.W_mu\'].data\n                W_p = state_dict[layer_name + \'.W_p\'].data\n                b_mu = state_dict[layer_name + \'.b_mu\'].data\n                b_p = state_dict[layer_name + \'.b_p\'].data\n\n                std_w = 1e-6 + F.softplus(W_p, beta=1, threshold=20)\n                std_b = 1e-6 + F.softplus(b_p, beta=1, threshold=20)\n\n                KL_W = W_mu.new(W_mu.size()).zero_()\n                KL_b = b_mu.new(b_mu.size()).zero_()\n                for i in range(Nsamples):\n                    W, b = sample_weights(W_mu=W_mu, b_mu=b_mu, W_p=W_p, b_p=b_p)\n                    # Note that this will currently not work with slab and spike prior\n                    KL_W += isotropic_gauss_loglike(W, W_mu, std_w,\n                                                    do_sum=False) - self.model.prior_instance.loglike(W,\n                                                                                                      do_sum=False)\n                    KL_b += isotropic_gauss_loglike(b, b_mu, std_b,\n                                                    do_sum=False) - self.model.prior_instance.loglike(b,\n                                                                                                      do_sum=False)\n\n                KL_W /= Nsamples\n                KL_b /= Nsamples\n\n                if thresh is not None:\n                    mask_dict[layer_name + \'.W\'] = KL_W > thresh\n                    mask_dict[layer_name + \'.b\'] = KL_b > thresh\n\n                else:\n\n                    for weight_KLD in KL_W.cpu().view(-1):\n                        weight_KLD_vec.append(weight_KLD)\n\n                    for weight_KLD in KL_b.cpu().view(-1):\n                        weight_KLD_vec.append(weight_KLD)\n\n        if thresh is not None:\n            return mask_dict\n        else:\n            return np.array(weight_KLD_vec)\n\n    def mask_model(self, Nsamples=0, thresh=0):\n        \'\'\'\n        Nsamples is used to select SNR (0) or KLD (>0) based masking\n        \'\'\'\n        original_state_dict = copy.deepcopy(self.model.state_dict())\n        state_dict = self.model.state_dict()\n\n        if Nsamples == 0:\n            mask_dict = self.get_weight_SNR(thresh=thresh)\n        else:\n            mask_dict = self.get_weight_KLD(Nsamples=Nsamples, thresh=thresh)\n\n        n_unmasked = 0\n\n        previous_layer_name = \'\'\n        for key in state_dict.keys():\n            layer_name = key.split(\'.\')[0]\n            if layer_name != previous_layer_name:\n                previous_layer_name = layer_name\n\n                state_dict[layer_name + \'.W_mu\'][1 - mask_dict[layer_name + \'.W\']] = 0\n                state_dict[layer_name + \'.W_p\'][1 - mask_dict[layer_name + \'.W\']] = -1000\n\n                state_dict[layer_name + \'.b_mu\'][1 - mask_dict[layer_name + \'.b\']] = 0\n                state_dict[layer_name + \'.b_p\'][1 - mask_dict[layer_name + \'.b\']] = -1000\n\n                n_unmasked += mask_dict[layer_name + \'.W\'].sum()\n                n_unmasked += mask_dict[layer_name + \'.b\'].sum()\n\n        return original_state_dict, n_unmasked\n\n\n'"
src/Bayes_By_Backprop_Local_Reparametrization/__init__.py,0,b''
src/Bayes_By_Backprop_Local_Reparametrization/model.py,17,"b'from src.priors import *\nfrom src.base_net import *\n\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport copy\n\ndef sample_weights(W_mu, b_mu, W_p, b_p):\n    """"""Quick method for sampling weights and exporting weights""""""\n    eps_W = W_mu.data.new(W_mu.size()).normal_()\n    # sample parameters\n    std_w = 1e-6 + F.softplus(W_p, beta=1, threshold=20)\n    W = W_mu + 1 * std_w * eps_W\n\n    if b_mu is not None:\n        std_b = 1e-6 + F.softplus(b_p, beta=1, threshold=20)\n        eps_b = b_mu.data.new(b_mu.size()).normal_()\n        b = b_mu + 1 * std_b * eps_b\n    else:\n        b = None\n\n    return W, b\n\n\ndef KLD_cost(mu_p, sig_p, mu_q, sig_q):\n    KLD = 0.5 * (2 * torch.log(sig_p / sig_q) - 1 + (sig_q / sig_p).pow(2) + ((mu_p - mu_q) / sig_p).pow(2)).sum()\n    # https://arxiv.org/abs/1312.6114 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n    return KLD\n\n\nclass BayesLinear_local_reparam(nn.Module):\n    """"""Linear Layer where activations are sampled from a fully factorised normal which is given by aggregating\n     the moments of each weight\'s normal distribution. The KL divergence is obtained in closed form. Only works\n      with gaussian priors.\n    """"""\n    def __init__(self, n_in, n_out, prior_sig):\n        super(BayesLinear_local_reparam, self).__init__()\n        self.n_in = n_in\n        self.n_out = n_out\n        self.prior_sig = prior_sig\n\n        # Learnable parameters\n        self.W_mu = nn.Parameter(torch.Tensor(self.n_in, self.n_out).uniform_(-0.1, 0.1))\n        self.W_p = nn.Parameter(\n            torch.Tensor(self.n_in, self.n_out).uniform_(-3, -2))\n\n        self.b_mu = nn.Parameter(torch.Tensor(self.n_out).uniform_(-0.1, 0.1))\n        self.b_p = nn.Parameter(torch.Tensor(self.n_out).uniform_(-3, -2))\n\n    def forward(self, X, sample=False):\n        #         print(self.training)\n\n        if not self.training and not sample:  # This is just a placeholder function\n            output = torch.mm(X, self.W_mu) + self.b_mu.expand(X.size()[0], self.n_out)\n            return output, 0, 0\n\n        else:\n\n            # calculate std\n            std_w = 1e-6 + F.softplus(self.W_p, beta=1, threshold=20)\n            std_b = 1e-6 + F.softplus(self.b_p, beta=1, threshold=20)\n\n            act_W_mu = torch.mm(X, self.W_mu)  # self.W_mu + std_w * eps_W\n            act_W_std = torch.sqrt(torch.mm(X.pow(2), std_w.pow(2)))\n\n            # Tensor.new()  Constructs a new tensor of the same data type as self tensor.\n            # the same random sample is used for every element in the minibatch output\n            eps_W = Variable(self.W_mu.data.new(act_W_std.size()).normal_(mean=0, std=1))\n            eps_b = Variable(self.b_mu.data.new(std_b.size()).normal_(mean=0, std=1))\n\n            act_W_out = act_W_mu + act_W_std * eps_W  # (batch_size, n_output)\n            act_b_out = self.b_mu + std_b * eps_b\n\n            output = act_W_out + act_b_out.unsqueeze(0).expand(X.shape[0], -1)\n\n            kld = KLD_cost(mu_p=0, sig_p=self.prior_sig, mu_q=self.W_mu, sig_q=std_w) + KLD_cost(mu_p=0, sig_p=0.1, mu_q=self.b_mu,\n                                                                                      sig_q=std_b)\n            return output, kld, 0\n\n\nclass bayes_linear_LR_2L(nn.Module):\n    def __init__(self, input_dim, output_dim, nhid, prior_sig):\n        super(bayes_linear_LR_2L, self).__init__()\n\n        n_hid = nhid\n        self.prior_sig = prior_sig\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        self.bfc1 = BayesLinear_local_reparam(input_dim, n_hid, self.prior_sig)\n        self.bfc2 = BayesLinear_local_reparam(n_hid, n_hid, self.prior_sig)\n        self.bfc3 = BayesLinear_local_reparam(n_hid, output_dim, self.prior_sig)\n\n        # choose your non linearity\n        # self.act = nn.Tanh()\n        # self.act = nn.Sigmoid()\n        self.act = nn.ReLU(inplace=True)\n        # self.act = nn.ELU(inplace=True)\n        # self.act = nn.SELU(inplace=True)\n\n    def forward(self, x, sample=False):\n        tlqw = 0\n        tlpw = 0\n\n        x = x.view(-1, self.input_dim)  # view(batch_size, input_dim)\n        # -----------------\n        x, lqw, lpw = self.bfc1(x, sample)\n        tlqw = tlqw + lqw\n        tlpw = tlpw + lpw\n        # -----------------\n        x = self.act(x)\n        # -----------------\n        x, lqw, lpw = self.bfc2(x, sample)\n        tlqw = tlqw + lqw\n        tlpw = tlpw + lpw\n        # -----------------\n        x = self.act(x)\n        # -----------------\n        y, lqw, lpw = self.bfc3(x, sample)\n        tlqw = tlqw + lqw\n        tlpw = tlpw + lpw\n\n        return y, tlqw, tlpw\n\n    def sample_predict(self, x, Nsamples):\n        # Just copies type from x, initializes new vector\n        predictions = x.data.new(Nsamples, x.shape[0], self.output_dim)\n        tlqw_vec = np.zeros(Nsamples)\n        tlpw_vec = np.zeros(Nsamples)\n\n        for i in range(Nsamples):\n            y, tlqw, tlpw = self.forward(x, sample=True)\n            predictions[i] = y\n            tlqw_vec[i] = tlqw\n            tlpw_vec[i] = tlpw\n\n        return predictions, tlqw_vec, tlpw_vec\n\n\nclass BBP_Bayes_Net_LR(BaseNet):\n    """"""Full network wrapper for Bayes By Backprop nets with methods for training, prediction and weight prunning""""""\n    def __init__(self, lr=1e-3, channels_in=3, side_in=28, cuda=True, classes=10, batch_size=128, Nbatches=0,\n                 nhid=1200, prior_sig=0.1):\n        super(BBP_Bayes_Net_LR, self).__init__()\n        cprint(\'y\', \' Creating Net!! \')\n        self.lr = lr\n        self.schedule = None  # [] #[50,200,400,600]\n        self.cuda = cuda\n        self.channels_in = channels_in\n        self.classes = classes\n        self.nhid = nhid\n        self.prior_sig = prior_sig\n        self.batch_size = batch_size\n        self.Nbatches = Nbatches\n        self.side_in = side_in\n        self.create_net()\n        self.create_opt()\n        self.epoch = 0\n\n        self.test = False\n\n    def create_net(self):\n        torch.manual_seed(42)\n        if self.cuda:\n            torch.cuda.manual_seed(42)\n\n        self.model = bayes_linear_LR_2L(input_dim=self.channels_in * self.side_in * self.side_in, output_dim=self.classes,\n                                     nhid=self.nhid, prior_sig=self.prior_sig)\n        if self.cuda:\n            self.model = self.model.cuda()\n\n        print(\'    Total params: %.2fM\' % (self.get_nb_parameters() / 1000000.0))\n\n    def create_opt(self):\n        #         self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(0.9, 0.999), eps=1e-08,\n        #                                           weight_decay=0)\n        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0)\n\n    def fit(self, x, y, samples=1):\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        self.optimizer.zero_grad()\n\n        if samples == 1:\n            out, tlqw, tlpw = self.model(x)\n            mlpdw = F.cross_entropy(out, y, reduction=\'sum\')\n            Edkl = (tlqw - tlpw) / self.Nbatches\n\n        elif samples > 1:\n            mlpdw_cum = 0\n            Edkl_cum = 0\n\n            for i in range(samples):\n                out, tlqw, tlpw = self.model(x, sample=True)\n                mlpdw_i = F.cross_entropy(out, y, reduction=\'sum\')\n                Edkl_i = (tlqw - tlpw) / self.Nbatches\n                mlpdw_cum = mlpdw_cum + mlpdw_i\n                Edkl_cum = Edkl_cum + Edkl_i\n\n            mlpdw = mlpdw_cum / samples\n            Edkl = Edkl_cum / samples\n\n        loss = Edkl + mlpdw\n        loss.backward()\n        self.optimizer.step()\n\n        # out: (batch_size, out_channels, out_caps_dims)\n        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return Edkl.data, mlpdw.data, err\n\n    def eval(self, x, y, train=False):\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out, _, _ = self.model(x)\n\n        loss = F.cross_entropy(out, y, reduction=\'sum\')\n\n        probs = F.softmax(out, dim=1).data.cpu()\n\n        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return loss.data, err, probs\n\n    def sample_eval(self, x, y, Nsamples, logits=True, train=False):\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out, _, _ = self.model.sample_predict(x, Nsamples)\n\n        if logits:\n            mean_out = out.mean(dim=0, keepdim=False)\n            loss = F.cross_entropy(mean_out, y, reduction=\'sum\')\n            probs = F.softmax(mean_out, dim=1).data.cpu()\n\n        else:\n            mean_out = F.softmax(out, dim=2).mean(dim=0, keepdim=False)\n            probs = mean_out.data.cpu()\n\n            log_mean_probs_out = torch.log(mean_out)\n            loss = F.nll_loss(log_mean_probs_out, y, reduction=\'sum\')\n\n        pred = mean_out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return loss.data, err, probs\n\n    def all_sample_eval(self, x, y, Nsamples):\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out, _, _ = self.model.sample_predict(x, Nsamples)\n\n        prob_out = F.softmax(out, dim=2)\n        prob_out = prob_out.data\n\n        return prob_out\n\n    def get_weight_samples(self, Nsamples=10):\n        state_dict = self.model.state_dict()\n        weight_vec = []\n\n        for i in range(Nsamples):\n            previous_layer_name = \'\'\n            for key in state_dict.keys():\n                layer_name = key.split(\'.\')[0]\n                if layer_name != previous_layer_name:\n                    previous_layer_name = layer_name\n\n                    W_mu = state_dict[layer_name + \'.W_mu\'].data\n                    W_p = state_dict[layer_name + \'.W_p\'].data\n\n                    #                 b_mu = state_dict[layer_name+\'.b_mu\'].cpu().data\n                    #                 b_p = state_dict[layer_name+\'.b_p\'].cpu().data\n\n                    W, b = sample_weights(W_mu=W_mu, b_mu=None, W_p=W_p, b_p=None)\n\n                    for weight in W.cpu().view(-1):\n                        weight_vec.append(weight)\n\n        return np.array(weight_vec)\n\n    def get_weight_SNR(self, thresh=None):\n        state_dict = self.model.state_dict()\n        weight_SNR_vec = []\n\n        if thresh is not None:\n            mask_dict = {}\n\n        previous_layer_name = \'\'\n        for key in state_dict.keys():\n            layer_name = key.split(\'.\')[0]\n            if layer_name != previous_layer_name:\n                previous_layer_name = layer_name\n\n                W_mu = state_dict[layer_name + \'.W_mu\'].data\n                W_p = state_dict[layer_name + \'.W_p\'].data\n                sig_W = 1e-6 + F.softplus(W_p, beta=1, threshold=20)\n\n                b_mu = state_dict[layer_name + \'.b_mu\'].data\n                b_p = state_dict[layer_name + \'.b_p\'].data\n                sig_b = 1e-6 + F.softplus(b_p, beta=1, threshold=20)\n\n                W_snr = (torch.abs(W_mu) / sig_W)\n                b_snr = (torch.abs(b_mu) / sig_b)\n\n                if thresh is not None:\n                    mask_dict[layer_name + \'.W\'] = W_snr > thresh\n                    mask_dict[layer_name + \'.b\'] = b_snr > thresh\n\n                else:\n\n                    for weight_SNR in W_snr.cpu().view(-1):\n                        weight_SNR_vec.append(weight_SNR)\n\n                    for weight_SNR in b_snr.cpu().view(-1):\n                        weight_SNR_vec.append(weight_SNR)\n\n        if thresh is not None:\n            return mask_dict\n        else:\n            return np.array(weight_SNR_vec)\n\n    def get_weight_KLD(self, Nsamples=20, thresh=None):\n        state_dict = self.model.state_dict()\n        weight_KLD_vec = []\n\n        if thresh is not None:\n            mask_dict = {}\n\n        previous_layer_name = \'\'\n        for key in state_dict.keys():\n            layer_name = key.split(\'.\')[0]\n            if layer_name != previous_layer_name:\n                previous_layer_name = layer_name\n\n                W_mu = state_dict[layer_name + \'.W_mu\'].data\n                W_p = state_dict[layer_name + \'.W_p\'].data\n                b_mu = state_dict[layer_name + \'.b_mu\'].data\n                b_p = state_dict[layer_name + \'.b_p\'].data\n\n                std_w = 1e-6 + F.softplus(W_p, beta=1, threshold=20)\n                std_b = 1e-6 + F.softplus(b_p, beta=1, threshold=20)\n\n                KL_W = W_mu.new(W_mu.size()).zero_()\n                KL_b = b_mu.new(b_mu.size()).zero_()\n                for i in range(Nsamples):\n                    W, b = sample_weights(W_mu=W_mu, b_mu=b_mu, W_p=W_p, b_p=b_p)\n                    # Note that this will currently not work with slab and spike prior\n                    KL_W += isotropic_gauss_loglike(W, W_mu, std_w, do_sum=False) - self.model.prior_instance.loglike(W,\n                                                                                                                      do_sum=False)\n                    KL_b += isotropic_gauss_loglike(b, b_mu, std_b, do_sum=False) - self.model.prior_instance.loglike(b,\n                                                                                                                      do_sum=False)\n\n                KL_W /= Nsamples\n                KL_b /= Nsamples\n\n                if thresh is not None:\n                    mask_dict[layer_name + \'.W\'] = KL_W > thresh\n                    mask_dict[layer_name + \'.b\'] = KL_b > thresh\n\n                else:\n\n                    for weight_KLD in KL_W.cpu().view(-1):\n                        weight_KLD_vec.append(weight_KLD)\n\n                    for weight_KLD in KL_b.cpu().view(-1):\n                        weight_KLD_vec.append(weight_KLD)\n\n        if thresh is not None:\n            return mask_dict\n        else:\n            return np.array(weight_KLD_vec)\n\n    def mask_model(self, Nsamples=0, thresh=0):\n        \'\'\'\n        Nsamples is used to select SNR (0) or KLD (>0) based masking\n        \'\'\'\n        original_state_dict = copy.deepcopy(self.model.state_dict())\n        state_dict = self.model.state_dict()\n\n        if Nsamples == 0:\n            mask_dict = self.get_weight_SNR(thresh=thresh)\n        else:\n            mask_dict = self.get_weight_KLD(Nsamples=Nsamples, thresh=thresh)\n\n        n_unmasked = 0\n\n        previous_layer_name = \'\'\n        for key in state_dict.keys():\n            layer_name = key.split(\'.\')[0]\n            if layer_name != previous_layer_name:\n                previous_layer_name = layer_name\n\n                state_dict[layer_name + \'.W_mu\'][1 - mask_dict[layer_name + \'.W\']] = 0\n                state_dict[layer_name + \'.W_p\'][1 - mask_dict[layer_name + \'.W\']] = -1000\n\n                state_dict[layer_name + \'.b_mu\'][1 - mask_dict[layer_name + \'.b\']] = 0\n                state_dict[layer_name + \'.b_p\'][1 - mask_dict[layer_name + \'.b\']] = -1000\n\n                n_unmasked += mask_dict[layer_name + \'.W\'].sum()\n                n_unmasked += mask_dict[layer_name + \'.b\'].sum()\n\n        return original_state_dict, n_unmasked\n\n'"
src/Bootstrap_Ensemble/__init__.py,0,b''
src/Bootstrap_Ensemble/model.py,9,"b""from src.priors import *\nfrom src.base_net import *\n\nimport torch.nn.functional as F\nimport torch.nn as nn\n\nclass Linear_2L(nn.Module):\n    def __init__(self, input_dim, output_dim, n_hid):\n        super(Linear_2L, self).__init__()\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        self.fc1 = nn.Linear(input_dim, n_hid)\n        self.fc2 = nn.Linear(n_hid, n_hid)\n        self.fc3 = nn.Linear(n_hid, output_dim)\n\n        # choose your non linearity\n        # self.act = nn.Tanh()\n        # self.act = nn.Sigmoid()\n        self.act = nn.ReLU(inplace=True)\n        # self.act = nn.ELU(inplace=True)\n        # self.act = nn.SELU(inplace=True)\n\n    def forward(self, x):\n        x = x.view(-1, self.input_dim)  # view(batch_size, input_dim)\n        # -----------------\n        x = self.fc1(x)\n        # -----------------\n        x = self.act(x)\n        # -----------------\n        x = self.fc2(x)\n        # -----------------\n        x = self.act(x)\n        # -----------------\n        y = self.fc3(x)\n\n        return y\n\n\nclass Bootstrap_Net(BaseNet):\n    eps = 1e-6\n\n    def __init__(self, lr=1e-3, channels_in=3, side_in=28, cuda=True, classes=10, batch_size=128, weight_decay=0, n_hid=1200):\n        super(Bootstrap_Net, self).__init__()\n        cprint('y', ' Creating Net!! ')\n        self.lr = lr\n        self.schedule = None  # [] #[50,200,400,600]\n        self.cuda = cuda\n        self.channels_in = channels_in\n        self.weight_decay = weight_decay\n        self.classes = classes\n        self.n_hid = n_hid\n        self.batch_size = batch_size\n        self.side_in = side_in\n        self.create_net()\n        self.create_opt()\n        self.epoch = 0\n\n        self.test = False\n\n    def create_net(self):\n        #         torch.manual_seed(42)\n        #         if self.cuda:\n        #             torch.cuda.manual_seed(42)\n\n        self.model = Linear_2L(input_dim=self.channels_in * self.side_in * self.side_in, output_dim=self.classes,\n                               n_hid=self.n_hid)\n        if self.cuda:\n            self.model.cuda()\n        #             cudnn.benchmark = True\n\n        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n\n    def create_opt(self):\n        #         self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(0.9, 0.999), eps=1e-08,\n        #                                           weight_decay=0)\n        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.5,\n                                         weight_decay=self.weight_decay)\n\n    #         self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9)\n    #         self.sched = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=10, last_epoch=-1)\n\n    def fit(self, x, y):\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        self.optimizer.zero_grad()\n\n        out = self.model(x)\n        loss = F.cross_entropy(out, y, reduction='sum')\n\n        loss.backward()\n        self.optimizer.step()\n\n        # out: (batch_size, out_channels, out_caps_dims)\n        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return loss.data, err\n\n    def eval(self, x, y, train=False):\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out = self.model(x)\n\n        loss = F.cross_entropy(out, y, reduction='sum')\n\n        probs = F.softmax(out, dim=1).data.cpu()\n\n        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return loss.data, err, probs\n\n    def get_weight_samples(self):\n        weight_vec = []\n\n        state_dict = self.model.state_dict()\n\n        for key in state_dict.keys():\n\n            if 'weight' in key:\n                weight_mtx = state_dict[key].cpu().data\n                for weight in weight_mtx.view(-1):\n                    weight_vec.append(weight)\n\n        return np.array(weight_vec)\n\n    def sample_eval(self, x, y, weight_set_samples, logits=True, train=False):\n\n        Nsamples = len(weight_set_samples)\n\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out = x.data.new(Nsamples, x.shape[0], self.classes)\n\n        # iterate over all saved weight configuration samples\n        for idx, weight_dict in enumerate(weight_set_samples):\n            if idx == Nsamples:\n                break\n            self.model.load_state_dict(weight_dict)\n            out[idx] = self.model(x)\n\n        if logits:\n            mean_out = out.mean(dim=0, keepdim=False)\n            loss = F.cross_entropy(mean_out, y, reduction='sum')\n            probs = F.softmax(mean_out, dim=1).data.cpu()\n\n        else:\n            mean_out = F.softmax(out, dim=2).mean(dim=0, keepdim=False)\n            probs = mean_out.data.cpu()\n\n            log_mean_probs_out = torch.log(mean_out)\n            loss = F.nll_loss(log_mean_probs_out, y, reduction='sum')\n\n        pred = mean_out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return loss.data, err, probs\n\n    def all_sample_eval(self, x, y, weight_set_samples):\n\n        Nsamples = len(weight_set_samples)\n\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out = x.data.new(Nsamples, x.shape[0], self.classes)\n\n        # iterate over all saved weight configuration samples\n        for idx, weight_dict in enumerate(weight_set_samples):\n            if idx == Nsamples:\n                break\n            self.model.load_state_dict(weight_dict)\n            out[idx] = self.model(x)\n\n        prob_out = F.softmax(out, dim=2)\n        prob_out = prob_out.data\n\n        return prob_out"""
src/KF_Laplace/__init__.py,0,b''
src/KF_Laplace/hessian_operations.py,12,"b'from __future__ import division\n\nimport torch\n# from src.base_net import *\n\ndef softmax_CE_preact_hessian(last_layer_acts):\n    side = last_layer_acts.shape[1]\n    I = torch.eye(side).type(torch.ByteTensor)\n    # for i != j    H = -ai * aj -- Note that these are activations not pre-activations\n    Hl = - last_layer_acts.unsqueeze(1) * last_layer_acts.unsqueeze(2)\n    # for i == j    H = ai * (1 - ai)\n    Hl[:, I] = last_layer_acts * (1 - last_layer_acts)\n    return Hl\n\n\ndef layer_act_hessian_recurse(prev_hessian, prev_weights, layer_pre_acts):\n    newside = layer_pre_acts.shape[1]\n    batch_size = layer_pre_acts.shape[0]\n    I = torch.eye(newside).type(torch.ByteTensor)  # .unsqueeze(0).expand([batch_size, -1, -1])\n\n    #     print(d_act(layer_pre_acts).unsqueeze(1).shape, I.shape)\n    B = prev_weights.data.new(batch_size, newside, newside).fill_(0)\n    B[:, I] = (layer_pre_acts > 0).type(B.type())  # d_act(layer_pre_acts)\n    D = prev_weights.data.new(batch_size, newside, newside).fill_(0)  # is just 0 for a piecewise linear\n    #     D[:, I] = dd_act(layer_pre_acts) * act_grads\n\n    Hl = torch.bmm(torch.t(prev_weights).unsqueeze(0).expand([batch_size, -1, -1]), prev_hessian)\n    Hl = torch.bmm(Hl, prev_weights.unsqueeze(0).expand([batch_size, -1, -1]))\n    Hl = torch.bmm(B, Hl)\n    Hl = torch.matmul(Hl, B)\n    Hl = Hl + D\n\n    return Hl\n\n\ndef chol_scale_invert_kron_factor(factor, prior_scale, data_scale, upper=False):\n    scaled_factor = data_scale * factor + prior_scale * torch.eye(factor.shape[0]).type(factor.type())\n    inv_factor = torch.inverse(scaled_factor)\n    chol_inv_factor = torch.cholesky(inv_factor, upper=upper)\n    return chol_inv_factor\n\n\ndef sample_K_laplace_MN(MAP, upper_Qinv, lower_HHinv):\n    # H = Qi (kron) HHi\n    # sample isotropic unit variance mtrix normal\n    Z = MAP.data.new(MAP.size()).normal_(mean=0, std=1)\n    # AAT = HHi\n    #     A = torch.cholesky(HHinv, upper=False)\n    # BTB = Qi\n    #     B = torch.cholesky(Qinv, upper=True)\n    all_mtx_sample = MAP + torch.matmul(torch.matmul(lower_HHinv, Z), upper_Qinv)\n\n    weight_mtx_sample = all_mtx_sample[:, :-1]\n    bias_mtx_sample = all_mtx_sample[:, -1]\n\n    return weight_mtx_sample, bias_mtx_sample'"
src/KF_Laplace/model.py,23,"b'from src.base_net import *\nfrom hessian_operations import sample_K_laplace_MN, softmax_CE_preact_hessian, layer_act_hessian_recurse\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Linear_2L_KFRA(nn.Module):\n    def __init__(self, input_dim, output_dim, n_hid):\n        super(Linear_2L_KFRA, self).__init__()\n\n        self.n_hid = n_hid\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        self.fc1 = nn.Linear(input_dim, self.n_hid)\n        self.fc2 = nn.Linear(self.n_hid, self.n_hid)\n        self.fc3 = nn.Linear(self.n_hid, output_dim)\n\n        # choose your non linearity\n        self.act = nn.ReLU(inplace=True)\n\n        self.one = None\n        self.a2 = None\n        self.h2 = None\n        self.a1 = None\n        self.h1 = None\n        self.a0 = None\n\n    def forward(self, x):\n        self.one = x.new(x.shape[0], 1).fill_(1)\n\n        a0 = x.view(-1, self.input_dim)  # view(batch_size, input_dim)\n        self.a0 = torch.cat((a0.data, self.one), dim=1)\n        # -----------------\n        h1 = self.fc1(a0)\n        self.h1 = h1.data  # torch.cat((h1, self.one), dim=1)\n        # -----------------\n        a1 = self.act(h1)\n        #         a1.retain_grad()\n        self.a1 = torch.cat((a1.data, self.one), dim=1)\n        # -----------------\n        h2 = self.fc2(a1)\n        self.h2 = h2.data  # torch.cat((h2, self.one), dim=1)\n        # -----------------\n        a2 = self.act(h2)\n        #         a2.retain_grad()\n        self.a2 = torch.cat((a2.data, self.one), dim=1)\n        # -----------------\n        h3 = self.fc3(a2)\n\n        return h3\n\n    def sample_predict(self, x, Nsamples, Qinv1, HHinv1, MAP1, Qinv2, HHinv2, MAP2, Qinv3, HHinv3, MAP3):\n        # Just copies type from x, initializes new vector\n        predictions = x.data.new(Nsamples, x.shape[0], self.output_dim)\n        x = x.view(-1, self.input_dim)\n        for i in range(Nsamples):\n            # -----------------\n            w1, b1 = sample_K_laplace_MN(MAP1, Qinv1, HHinv1)\n            a = torch.matmul(x, torch.t(w1)) + b1.unsqueeze(0)\n            a = self.act(a)\n            # -----------------\n            w2, b2 = sample_K_laplace_MN(MAP2, Qinv2, HHinv2)\n            a = torch.matmul(a, torch.t(w2)) + b2.unsqueeze(0)\n            a = self.act(a)\n            # -----------------\n            w3, b3 = sample_K_laplace_MN(MAP3, Qinv3, HHinv3)\n            y = torch.matmul(a, torch.t(w3)) + b3.unsqueeze(0)\n            predictions[i] = y\n\n        return predictions\n\n\nclass KBayes_Net(BaseNet):\n    eps = 1e-6\n\n    def __init__(self, lr=1e-3, channels_in=3, side_in=28, cuda=True, classes=10, n_hid=1200, batch_size=128, prior_sig=0):\n        super(KBayes_Net, self).__init__()\n        cprint(\'y\', \' Creating Net!! \')\n        self.lr = lr\n        self.schedule = None  # [] #[50,200,400,600]\n        self.cuda = cuda\n        self.channels_in = channels_in\n        self.n_hid = n_hid\n        self.prior_sig = prior_sig\n        self.classes = classes\n        self.batch_size = batch_size\n        self.side_in = side_in\n        self.create_net()\n        self.create_opt()\n        self.epoch = 0\n\n        self.test = False\n\n    def create_net(self):\n        torch.manual_seed(42)\n        if self.cuda:\n            torch.cuda.manual_seed(42)\n\n        self.model = Linear_2L_KFRA(input_dim=self.channels_in * self.side_in * self.side_in, output_dim=self.classes,\n                                    n_hid=self.n_hid)\n        if self.cuda:\n            self.model.cuda()\n        #             cudnn.benchmark = True\n\n        print(\'    Total params: %.2fM\' % (self.get_nb_parameters() / 1000000.0))\n\n    def create_opt(self):\n        #         self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(0.9, 0.999), eps=1e-08,\n        #                                           weight_decay=0)\n        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.5,\n                                         weight_decay=(1 / self.prior_sig ** 2))\n\n    #         self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9)\n    #         self.sched = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=10, last_epoch=-1)\n\n    def fit(self, x, y):\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        self.optimizer.zero_grad()\n\n        out = self.model(x)\n        loss = F.cross_entropy(out, y, reduction=\'sum\')\n\n        loss.backward()\n        self.optimizer.step()\n\n        # out: (batch_size, out_channels, out_caps_dims)\n        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return loss.data, err\n\n    def eval(self, x, y, train=False):\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out = self.model(x)\n\n        loss = F.cross_entropy(out, y, reduction=\'sum\')\n\n        probs = F.softmax(out, dim=1).data.cpu()\n\n        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return loss.data, err, probs\n\n    def get_K_laplace_params(self, trainloader):\n        """"""Do pass through full training set in order to get the expected layer-wise hessian product factors.\n        This is done by calculating the hessian for the first layer\'s activations and propagating them backward\n        recursively.""""""\n        self.model.eval()\n\n        it_counter = 0\n        cum_HH1 = self.model.fc1.weight.data.new(self.model.n_hid, self.model.n_hid).fill_(0)\n        cum_HH2 = self.model.fc1.weight.data.new(self.model.n_hid, self.model.n_hid).fill_(0)\n        cum_HH3 = self.model.fc1.weight.data.new(self.model.output_dim, self.model.output_dim).fill_(0)\n\n        cum_Q1 = self.model.fc1.weight.data.new(self.model.input_dim + 1, self.model.input_dim + 1).fill_(0)\n        cum_Q2 = self.model.fc1.weight.data.new(self.model.n_hid + 1, self.model.n_hid + 1).fill_(0)\n        cum_Q3 = self.model.fc1.weight.data.new(self.model.n_hid + 1, self.model.n_hid + 1).fill_(0)\n\n        # Forward pass\n\n        for x, y in trainloader:\n            x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n            self.optimizer.zero_grad()\n\n            out = self.model(x)\n            out_act = F.softmax(out, dim=1)\n            loss = F.cross_entropy(out, y, reduction=\'sum\')\n\n            loss.backward()\n\n            #     ------------------------------------------------------------------\n            HH3 = softmax_CE_preact_hessian(out_act.data)\n            cum_HH3 += HH3.sum(dim=0)\n            #     print(model.a2.data.shape)\n            Q3 = torch.bmm(self.model.a2.data.unsqueeze(2), self.model.a2.data.unsqueeze(1))\n            cum_Q3 += Q3.sum(dim=0)\n            #     ------------------------------------------------------------------\n            HH2 = layer_act_hessian_recurse(prev_hessian=HH3, prev_weights=self.model.fc3.weight.data,\n                                            layer_pre_acts=self.model.h2.data)\n            cum_HH2 += HH2.sum(dim=0)\n            Q2 = torch.bmm(self.model.a1.data.unsqueeze(2), self.model.a1.data.unsqueeze(1))\n            cum_Q2 += Q2.sum(dim=0)\n            #     ------------------------------------------------------------------\n            HH1 = layer_act_hessian_recurse(prev_hessian=HH2, prev_weights=self.model.fc2.weight.data,\n                                            layer_pre_acts=self.model.h1.data)\n            cum_HH1 += HH1.sum(dim=0)\n            Q1 = torch.bmm(self.model.a0.data.unsqueeze(2), self.model.a0.data.unsqueeze(1))\n            cum_Q1 += Q1.sum(dim=0)\n            #     ------------------------------------------------------------------\n            it_counter += x.shape[0]\n            # print(it_counter)\n\n        EHH3 = cum_HH3 / it_counter\n        EHH2 = cum_HH2 / it_counter\n        EHH1 = cum_HH1 / it_counter\n\n        EQ3 = cum_Q3 / it_counter\n        EQ2 = cum_Q2 / it_counter\n        EQ1 = cum_Q1 / it_counter\n\n        MAP3 = torch.cat((self.model.fc3.weight.data, self.model.fc3.bias.data.unsqueeze(1)), dim=1)\n        MAP2 = torch.cat((self.model.fc2.weight.data, self.model.fc2.bias.data.unsqueeze(1)), dim=1)\n        MAP1 = torch.cat((self.model.fc1.weight.data, self.model.fc1.bias.data.unsqueeze(1)), dim=1)\n\n        return EQ1, EHH1, MAP1, EQ2, EHH2, MAP2, EQ3, EHH3, MAP3\n\n    def sample_eval(self, x, y, Nsamples, scale_inv_EQ1, scale_inv_EHH1, MAP1, scale_inv_EQ2, scale_inv_EHH2, MAP2,\n                    scale_inv_EQ3, scale_inv_EHH3, MAP3, logits=False):\n        """"""Prediction, only returining result with weights marginalised""""""\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out = self.model.sample_predict(x, Nsamples, scale_inv_EQ1, scale_inv_EHH1, MAP1, scale_inv_EQ2, scale_inv_EHH2,\n                                        MAP2, scale_inv_EQ3, scale_inv_EHH3, MAP3)\n\n        if logits:\n            mean_out = out.mean(dim=0, keepdim=False)\n            loss = F.cross_entropy(mean_out, y, reduction=\'sum\')\n            probs = F.softmax(mean_out, dim=1).data.cpu()\n\n        else:\n            mean_out = F.softmax(out, dim=2).mean(dim=0, keepdim=False)\n            probs = mean_out.data.cpu()\n\n            log_mean_probs_out = torch.log(mean_out)\n            loss = F.nll_loss(log_mean_probs_out, y, reduction=\'sum\')\n\n        pred = mean_out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return loss.data, err, probs\n\n    def all_sample_eval(self, x, y, Nsamples, scale_inv_EQ1, scale_inv_EHH1, MAP1, scale_inv_EQ2, scale_inv_EHH2, MAP2,\n                        scale_inv_EQ3, scale_inv_EHH3, MAP3):\n        """"""Returns predictions for each MC sample""""""\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out = self.model.sample_predict(x, Nsamples, scale_inv_EQ1, scale_inv_EHH1, MAP1, scale_inv_EQ2, scale_inv_EHH2,\n                                        MAP2, scale_inv_EQ3, scale_inv_EHH3, MAP3)\n\n        prob_out = F.softmax(out, dim=2)\n        prob_out = prob_out.data\n\n        return prob_out\n\n    def get_weight_samples(self, Nsamples, scale_inv_EQ1, scale_inv_EHH1, MAP1, scale_inv_EQ2, scale_inv_EHH2, MAP2,\n                           scale_inv_EQ3, scale_inv_EHH3, MAP3):\n        weight_vec = []\n\n        for i in range(Nsamples):\n\n            w1, b1 = sample_K_laplace_MN(MAP1, scale_inv_EQ1, scale_inv_EHH1)\n            w2, b2 = sample_K_laplace_MN(MAP2, scale_inv_EQ2, scale_inv_EHH2)\n            w3, b3 = sample_K_laplace_MN(MAP3, scale_inv_EQ3, scale_inv_EHH3)\n\n            for weight in w1.cpu().numpy().flatten():\n                weight_vec.append(weight)\n            for weight in w2.cpu().numpy().flatten():\n                weight_vec.append(weight)\n            for weight in w3.cpu().numpy().flatten():\n                weight_vec.append(weight)\n\n        return np.array(weight_vec)'"
src/MC_dropout/__init__.py,0,b''
src/MC_dropout/model.py,9,"b""from src.priors import *\nfrom src.base_net import *\n\nimport torch.nn.functional as F\nimport torch.nn as nn\n\n\ndef MC_dropout(act_vec, p=0.5, mask=True):\n    return F.dropout(act_vec, p=p, training=mask, inplace=True)\n\n\nclass Linear_2L(nn.Module):\n    def __init__(self, input_dim, output_dim, n_hid):\n        super(Linear_2L, self).__init__()\n\n        self.pdrop = 0.5\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        self.fc1 = nn.Linear(input_dim, n_hid)\n        self.fc2 = nn.Linear(n_hid, n_hid)\n        self.fc3 = nn.Linear(n_hid, output_dim)\n\n        # choose your non linearity\n        # self.act = nn.Tanh()\n        # self.act = nn.Sigmoid()\n        self.act = nn.ReLU(inplace=True)\n        # self.act = nn.ELU(inplace=True)\n        # self.act = nn.SELU(inplace=True)\n\n    def forward(self, x, sample=True):\n        mask = self.training or sample  # if training or sampling, mc dropout will apply random binary mask\n        # Otherwise, for regular test set evaluation, we can just scale activations\n\n        x = x.view(-1, self.input_dim)  # view(batch_size, input_dim)\n        # -----------------\n        x = self.fc1(x)\n        x = MC_dropout(x, p=self.pdrop, mask=mask)\n        # -----------------\n        x = self.act(x)\n        # -----------------\n        x = self.fc2(x)\n        x = MC_dropout(x, p=self.pdrop, mask=mask)\n        # -----------------\n        x = self.act(x)\n        # -----------------\n        y = self.fc3(x)\n\n        return y\n\n    def sample_predict(self, x, Nsamples):\n        # Just copies type from x, initializes new vector\n        predictions = x.data.new(Nsamples, x.shape[0], self.output_dim)\n\n        for i in range(Nsamples):\n            y = self.forward(x, sample=True)\n            predictions[i] = y\n\n        return predictions\n\n\nclass MC_drop_net(BaseNet):\n    eps = 1e-6\n\n    def __init__(self, lr=1e-3, channels_in=3, side_in=28, cuda=True, classes=10, batch_size=128, weight_decay=0, n_hid=1200):\n        super(MC_drop_net, self).__init__()\n        cprint('y', ' Creating Net!! ')\n        self.lr = lr\n        self.schedule = None  # [] #[50,200,400,600]\n        self.cuda = cuda\n        self.channels_in = channels_in\n        self.weight_decay = weight_decay\n        self.classes = classes\n        self.n_hid = n_hid\n        self.batch_size = batch_size\n        self.side_in = side_in\n        self.create_net()\n        self.create_opt()\n        self.epoch = 0\n\n        self.test = False\n\n    def create_net(self):\n        torch.manual_seed(42)\n        if self.cuda:\n            torch.cuda.manual_seed(42)\n\n        self.model = Linear_2L(input_dim=self.channels_in * self.side_in * self.side_in, output_dim=self.classes,\n                               n_hid=self.n_hid)\n        if self.cuda:\n            self.model.cuda()\n        #             cudnn.benchmark = True\n\n        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n\n    def create_opt(self):\n        #         self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(0.9, 0.999), eps=1e-08,\n        #                                           weight_decay=0)\n        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.5,\n                                         weight_decay=self.weight_decay)\n\n    #         self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9)\n    #         self.sched = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=10, last_epoch=-1)\n\n    def fit(self, x, y):\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        self.optimizer.zero_grad()\n\n        out = self.model(x)\n        loss = F.cross_entropy(out, y, reduction='sum')\n\n        loss.backward()\n        self.optimizer.step()\n\n        # out: (batch_size, out_channels, out_caps_dims)\n        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return loss.data, err\n\n    def eval(self, x, y, train=False):\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out = self.model(x)\n\n        loss = F.cross_entropy(out, y, reduction='sum')\n\n        probs = F.softmax(out, dim=1).data.cpu()\n\n        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return loss.data, err, probs\n\n    def sample_eval(self, x, y, Nsamples, logits=True, train=False):\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out = self.model.sample_predict(x, Nsamples)\n\n        if logits:\n            mean_out = out.mean(dim=0, keepdim=False)\n            loss = F.cross_entropy(mean_out, y, reduction='sum')\n            probs = F.softmax(mean_out, dim=1).data.cpu()\n\n        else:\n            mean_out = F.softmax(out, dim=2).mean(dim=0, keepdim=False)\n            probs = mean_out.data.cpu()\n\n            log_mean_probs_out = torch.log(mean_out)\n            loss = F.nll_loss(log_mean_probs_out, y, reduction='sum')\n\n        pred = mean_out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return loss.data, err, probs\n\n    def all_sample_eval(self, x, y, Nsamples):\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out = self.model.sample_predict(x, Nsamples)\n\n        prob_out = F.softmax(out, dim=2)\n        prob_out = prob_out.data\n\n        return prob_out\n\n    def get_weight_samples(self):\n        weight_vec = []\n\n        state_dict = self.model.state_dict()\n\n        for key in state_dict.keys():\n\n            if 'weight' in key:\n                weight_mtx = state_dict[key].cpu().data\n                for weight in weight_mtx.view(-1):\n                    weight_vec.append(weight)\n\n        return np.array(weight_vec)\n"""
src/Stochastic_Gradient_HMC_SA/__init__.py,0,b''
src/Stochastic_Gradient_HMC_SA/model.py,4,"b'from src.base_net import *\nfrom .optimizers import *\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport copy\n\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim, width, depth, output_dim):\n        super(MLP, self).__init__()\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.width = width\n        self.depth = depth\n\n        layers = [nn.Linear(input_dim, width), nn.ReLU()]\n        for i in range(depth - 1):\n            layers.append(nn.Linear(width, width))\n            layers.append(nn.ReLU())\n        layers.append(nn.Linear(width, output_dim))\n\n        self.block = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass BNN_cat(BaseNet):  # for categorical distributions\n    def __init__(self, N_train, lr=1e-2, cuda=True, grad_std_mul=30):\n        super(BNN_cat, self).__init__()\n\n        cprint(\'y\', \'BNN categorical output\')\n        self.lr = lr\n        self.model = MLP(input_dim=784, width=1200, depth=2, output_dim=10)\n        self.cuda = cuda\n\n        self.N_train = N_train\n        self.create_net()\n        self.create_opt()\n        self.schedule = None  # [] #[50,200,400,600]\n        self.epoch = 0\n\n        self.grad_buff = []\n        self.max_grad = 1e20\n        self.grad_std_mul = grad_std_mul\n\n        self.weight_set_samples = []\n\n    def create_net(self):\n        torch.manual_seed(42)\n        if self.cuda:\n            torch.cuda.manual_seed(42)\n        if self.cuda:\n            self.model.cuda()\n\n        print(\'    Total params: %.2fM\' % (self.get_nb_parameters() / 1000000.0))\n\n    def create_opt(self):\n        """"""This optimiser incorporates the gaussian prior term automatically. The prior variance is gibbs sampled from\n        its posterior using a gamma hyper-prior.""""""\n        self.optimizer = H_SA_SGHMC(params=self.model.parameters(), lr=self.lr, base_C=0.05, gauss_sig=0.1)  # this last parameter does nothing\n\n    def fit(self, x, y, burn_in=False, resample_momentum=False, resample_prior=False):\n        self.set_mode_train(train=True)\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n        self.optimizer.zero_grad()\n        out = self.model(x)\n        loss = F.cross_entropy(out, y, reduction=\'mean\')\n        loss = loss * self.N_train  # We use mean because we treat as an estimation of whole dataset\n        loss.backward()\n\n        # Gradient buffer to allow for dynamic clipping and prevent explosions\n        if len(self.grad_buff) > 1000:\n            self.max_grad = np.mean(self.grad_buff) + self.grad_std_mul * np.std(self.grad_buff)\n            self.grad_buff.pop(0)\n        # Clipping to prevent explosions\n        self.grad_buff.append(nn.utils.clip_grad_norm_(parameters=self.model.parameters(),\n                                                       max_norm=self.max_grad, norm_type=2))\n        if self.grad_buff[-1] >= self.max_grad:\n            print(self.max_grad, self.grad_buff[-1])\n            self.grad_buff.pop()\n        self.optimizer.step(burn_in=burn_in, resample_momentum=resample_momentum, resample_prior=resample_prior)\n\n        # out: (batch_size, out_channels, out_caps_dims)\n        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return loss.data * x.shape[0] / self.N_train, err\n\n    def eval(self, x, y, train=False):\n        self.set_mode_train(train=False)\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out = self.model(x)\n        loss = F.cross_entropy(out, y, reduction=\'sum\')\n        probs = F.softmax(out, dim=1).data.cpu()\n\n        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return loss.data, err, probs\n\n    def save_sampled_net(self, max_samples):\n\n        if len(self.weight_set_samples) >= max_samples:\n            self.weight_set_samples.pop(0)\n\n        self.weight_set_samples.append(copy.deepcopy(self.model.state_dict()))\n\n        cprint(\'c\', \' saving weight samples %d/%d\' % (len(self.weight_set_samples), max_samples))\n        return None\n\n    def predict(self, x):\n        self.set_mode_train(train=False)\n        x, = to_variable(var=(x, ), cuda=self.cuda)\n        out = self.model(x)\n        probs = F.softmax(out, dim=1).data.cpu()\n        return probs.data\n\n    def sample_predict(self, x, Nsamples=0, grad=False):\n        """"""return predictions using multiple samples from posterior""""""\n        self.set_mode_train(train=False)\n        if Nsamples == 0:\n            Nsamples = len(self.weight_set_samples)\n        x, = to_variable(var=(x, ), cuda=self.cuda)\n\n        if grad:\n            self.optimizer.zero_grad()\n            if not x.requires_grad:\n                x.requires_grad = True\n\n        out = x.data.new(Nsamples, x.shape[0], self.model.output_dim)\n\n        # iterate over all saved weight configuration samples\n        for idx, weight_dict in enumerate(self.weight_set_samples):\n            if idx == Nsamples:\n                break\n            self.model.load_state_dict(weight_dict)\n            out[idx] = self.model(x)\n\n        out = out[:idx]\n        prob_out = F.softmax(out, dim=2)\n\n        if grad:\n            return prob_out\n        else:\n            return prob_out.data\n\n    def get_weight_samples(self, Nsamples=0):\n        """"""return weight samples from posterior in a single-column array""""""\n        weight_vec = []\n\n        if Nsamples == 0 or Nsamples > len(self.weight_set_samples):\n            Nsamples = len(self.weight_set_samples)\n\n        for idx, state_dict in enumerate(self.weight_set_samples):\n            if idx == Nsamples:\n                break\n\n            for key in state_dict.keys():\n                if \'weight\' in key:\n                    weight_mtx = state_dict[key].cpu().data\n                    for weight in weight_mtx.view(-1):\n                        weight_vec.append(weight)\n\n        return np.array(weight_vec)\n\n    def save_weights(self, filename):\n        save_object(self.weight_set_samples, filename)\n\n    def load_weights(self, filename, subsample=1):\n        self.weight_set_samples = load_object(filename)\n        self.weight_set_samples = self.weight_set_samples[::subsample]'"
src/Stochastic_Gradient_HMC_SA/optimizers.py,10,"b'from __future__ import division\nimport torch\nfrom numpy.random import gamma\nfrom torch.optim import Optimizer\n\n\nclass H_SA_SGHMC(Optimizer):\n    """""" Stochastic Gradient Hamiltonian Monte-Carlo Sampler that uses scale adaption during burn-in\n        procedure to find some hyperparamters. A gaussian prior is placed over parameters and a Gamma\n        Hyperprior is placed over the prior\'s standard deviation""""""\n\n    def __init__(self, params, lr=1e-2, base_C=0.05, gauss_sig=0.1, alpha0=10, beta0=10):\n\n        self.eps = 1e-6\n        self.alpha0 = alpha0\n        self.beta0 = beta0\n\n        if gauss_sig == 0:\n            self.weight_decay = 0\n        else:\n            self.weight_decay = 1 / (gauss_sig ** 2)\n\n        if self.weight_decay <= 0.0:\n            raise ValueError(""Invalid weight_decay value: {}"".format(weight_decay))\n        if lr < 0.0:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if base_C < 0:\n            raise ValueError(""Invalid friction term: {}"".format(base_C))\n\n        defaults = dict(\n            lr=lr,\n            base_C=base_C,\n        )\n        super(H_SA_SGHMC, self).__init__(params, defaults)\n\n    def step(self, burn_in=False, resample_momentum=False, resample_prior=False):\n        """"""Simulate discretized Hamiltonian dynamics for one step""""""\n        loss = None\n\n        for group in self.param_groups:  # iterate over blocks -> the ones defined in defaults. We dont use groups.\n            for p in group[""params""]:  # these are weight and bias matrices\n                if p.grad is None:\n                    continue\n                state = self.state[p]  # define dict for each individual param\n                if len(state) == 0:\n                    state[""iteration""] = 0\n                    state[""tau""] = torch.ones_like(p)\n                    state[""g""] = torch.ones_like(p)\n                    state[""V_hat""] = torch.ones_like(p)\n                    state[""v_momentum""] = torch.zeros_like(\n                        p)  # p.data.new(p.data.size()).normal_(mean=0, std=np.sqrt(group[""lr""])) #\n                    state[\'weight_decay\'] = self.weight_decay\n\n                state[""iteration""] += 1  # this is kind of useless now but lets keep it provisionally\n\n                if resample_prior:\n                    alpha = self.alpha0 + p.data.nelement() / 2\n                    beta = self.beta0 + (p.data ** 2).sum().item() / 2\n                    gamma_sample = gamma(shape=alpha, scale=1 / (beta), size=None)\n                    #                     print(\'std\', 1/np.sqrt(gamma_sample))\n                    state[\'weight_decay\'] = gamma_sample\n\n                base_C, lr = group[""base_C""], group[""lr""]\n                weight_decay = state[""weight_decay""]\n                tau, g, V_hat = state[""tau""], state[""g""], state[""V_hat""]\n\n                d_p = p.grad.data\n                if weight_decay != 0:\n                    d_p.add_(weight_decay, p.data)\n\n                # update parameters during burn-in\n                if burn_in:  # We update g first as it makes most sense\n                    tau.add_(-tau * (g ** 2) / (\n                                V_hat + self.eps) + 1)  # specifies the moving average window, see Eq 9 in [1] left\n                    tau_inv = 1. / (tau + self.eps)\n                    g.add_(-tau_inv * g + tau_inv * d_p)  # average gradient see Eq 9 in [1] right\n                    V_hat.add_(-tau_inv * V_hat + tau_inv * (d_p ** 2))  # gradient variance see Eq 8 in [1]\n\n                V_sqrt = torch.sqrt(V_hat)\n                V_inv_sqrt = 1. / (V_sqrt + self.eps)  # preconditioner\n\n                if resample_momentum:  # equivalent to var = M under momentum reparametrisation\n                    state[""v_momentum""] = torch.normal(mean=torch.zeros_like(d_p),\n                                                       std=torch.sqrt((lr ** 2) * V_inv_sqrt))\n                v_momentum = state[""v_momentum""]\n\n                noise_var = (2. * (lr ** 2) * V_inv_sqrt * base_C - (lr ** 4))\n                noise_std = torch.sqrt(torch.clamp(noise_var, min=1e-16))\n                # sample random epsilon\n                noise_sample = torch.normal(mean=torch.zeros_like(d_p), std=torch.ones_like(d_p) * noise_std)\n\n                # update momentum (Eq 10 right in [1])\n                v_momentum.add_(- (lr ** 2) * V_inv_sqrt * d_p - base_C * v_momentum + noise_sample)\n\n                # update theta (Eq 10 left in [1])\n                p.data.add_(v_momentum)\n\n        return loss'"
src/Stochastic_Gradient_Langevin_Dynamics/__init__.py,0,b''
src/Stochastic_Gradient_Langevin_Dynamics/model.py,6,"b""from src.base_net import *\nfrom optimizers import *\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport copy\n\n\nclass Linear_2L(nn.Module):\n    def __init__(self, input_dim, output_dim, n_hid):\n        super(Linear_2L, self).__init__()\n\n        self.n_hid = n_hid\n\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        self.fc1 = nn.Linear(input_dim, self.n_hid)\n        self.fc2 = nn.Linear(self.n_hid, self.n_hid)\n        self.fc3 = nn.Linear(self.n_hid, output_dim)\n\n        # choose your non linearity\n        # self.act = nn.Tanh()\n        # self.act = nn.Sigmoid()\n        self.act = nn.ReLU(inplace=True)\n        # self.act = nn.ELU(inplace=True)\n        # self.act = nn.SELU(inplace=True)\n\n    def forward(self, x):\n        x = x.view(-1, self.input_dim)  # view(batch_size, input_dim)\n        # -----------------\n        x = self.fc1(x)\n        # -----------------\n        x = self.act(x)\n        # -----------------\n        x = self.fc2(x)\n        # -----------------\n        x = self.act(x)\n        # -----------------\n        y = self.fc3(x)\n\n        return y\n\n\nclass Net_langevin(BaseNet):\n    eps = 1e-6\n\n    def __init__(self, lr=1e-3, channels_in=3, side_in=28, cuda=True, classes=10, N_train=60000, prior_sig=0,\n                 nhid=1200, use_p=False):\n        super(Net_langevin, self).__init__()\n        cprint('y', ' Creating Net!! ')\n        self.lr = lr\n        self.schedule = None  # [] #[50,200,400,600]\n        self.cuda = cuda\n        self.channels_in = channels_in\n        self.prior_sig = prior_sig\n        self.classes = classes\n        self.N_train = N_train\n        self.side_in = side_in\n        self.nhid = nhid\n        self.use_p = use_p\n        self.create_net()\n        self.create_opt()\n        self.epoch = 0\n\n        self.weight_set_samples = []\n        self.test = False\n\n    def create_net(self):\n        torch.manual_seed(42)\n        if self.cuda:\n            torch.cuda.manual_seed(42)\n\n        self.model = Linear_2L(input_dim=self.channels_in * self.side_in * self.side_in, output_dim=self.classes,\n                               n_hid=self.nhid)\n        if self.cuda:\n            self.model.cuda()\n        #             cudnn.benchmark = True\n\n        print('    Total params: %.2fM' % (self.get_nb_parameters() / 1000000.0))\n\n    def create_opt(self):\n\n        if self.use_p:\n            self.optimizer = pSGLD(params=self.model.parameters(), lr=self.lr, norm_sigma=self.prior_sig, addnoise=True)\n        else:\n            self.optimizer = SGLD(params=self.model.parameters(), lr=self.lr, norm_sigma=self.prior_sig, addnoise=True)\n\n    #         self.sched = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=10, last_epoch=-1)\n\n    def fit(self, x, y):\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        self.optimizer.zero_grad()\n\n        out = self.model(x)\n        # We use mean because we treat the loss as an estimation of whole dataset's likelihood\n        loss = F.cross_entropy(out, y, reduction='mean')\n        loss = loss * self.N_train  # We scale the loss to represent the whole dataset\n\n        loss.backward()\n        self.optimizer.step()\n\n        # out: (batch_size, out_channels, out_caps_dims)\n        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return loss.data * x.shape[0] / self.N_train, err\n\n    def eval(self, x, y, train=False):\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out = self.model(x)\n\n        loss = F.cross_entropy(out, y, reduction='sum')\n\n        probs = F.softmax(out, dim=1).data.cpu()\n\n        pred = out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return loss.data, err, probs\n\n    def save_sampled_net(self, max_samples):\n\n        if len(self.weight_set_samples) >= max_samples:\n            self.weight_set_samples.pop(0)\n\n        self.weight_set_samples.append(copy.deepcopy(self.model.state_dict()))\n\n        cprint('c', ' saving weight samples %d/%d' % (len(self.weight_set_samples), max_samples))\n\n        return None\n\n    def sample_eval(self, x, y, Nsamples=0, logits=True, train=False):\n        if Nsamples == 0:\n            Nsamples = len(self.weight_set_samples)\n\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out = x.data.new(Nsamples, x.shape[0], self.classes)\n\n        # iterate over all saved weight configuration samples\n        for idx, weight_dict in enumerate(self.weight_set_samples):\n            if idx == Nsamples:\n                break\n            self.model.load_state_dict(weight_dict)\n            out[idx] = self.model(x)\n\n        if logits:\n            mean_out = out.mean(dim=0, keepdim=False)\n            loss = F.cross_entropy(mean_out, y, reduction='sum')\n            probs = F.softmax(mean_out, dim=1).data.cpu()\n\n        else:\n            mean_out = F.softmax(out, dim=2).mean(dim=0, keepdim=False)\n            probs = mean_out.data.cpu()\n\n            log_mean_probs_out = torch.log(mean_out)\n            loss = F.nll_loss(log_mean_probs_out, y, reduction='sum')\n\n        pred = mean_out.data.max(dim=1, keepdim=False)[1]  # get the index of the max log-probability\n        err = pred.ne(y.data).sum()\n\n        return loss.data, err, probs\n\n    def all_sample_eval(self, x, y, Nsamples):\n        if Nsamples == 0:\n            Nsamples = len(self.weight_set_samples)\n\n        x, y = to_variable(var=(x, y.long()), cuda=self.cuda)\n\n        out = x.data.new(Nsamples, x.shape[0], self.classes)\n\n        # iterate over all saved weight configuration samples\n        for idx, weight_dict in enumerate(self.weight_set_samples):\n            if idx == Nsamples:\n                break\n            self.model.load_state_dict(weight_dict)\n            out[idx] = self.model(x)\n\n        prob_out = F.softmax(out, dim=2)\n        prob_out = prob_out.data\n\n        return prob_out\n\n    def get_weight_samples(self, Nsamples=0):\n        weight_vec = []\n\n        if Nsamples == 0 or Nsamples > len(self.weight_set_samples):\n            Nsamples = len(self.weight_set_samples)\n\n        for idx, state_dict in enumerate(self.weight_set_samples):\n            if idx == Nsamples:\n                break\n\n            for key in state_dict.keys():\n                if 'weight' in key:\n                    weight_mtx = state_dict[key].cpu()\n                    for weight in weight_mtx.view(-1):\n                        weight_vec.append(weight)\n\n        return np.array(weight_vec)"""
src/Stochastic_Gradient_Langevin_Dynamics/optimizers.py,4,"b'from torch.optim.optimizer import Optimizer, required\nimport numpy as np\nimport torch\n\nclass SGLD(Optimizer):\n    """"""\n    SGLD optimiser based on pytorch\'s SGD.\n    Note that the weight decay is specified in terms of the gaussian prior sigma.\n    """"""\n\n    def __init__(self, params, lr=required, norm_sigma=0, addnoise=True):\n\n        weight_decay = 1 / (norm_sigma ** 2)\n\n        if weight_decay < 0.0:\n            raise ValueError(""Invalid weight_decay value: {}"".format(weight_decay))\n        if lr is not required and lr < 0.0:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n\n        defaults = dict(lr=lr, weight_decay=weight_decay, addnoise=addnoise)\n\n        super(SGLD, self).__init__(params, defaults)\n\n    def step(self):\n        """"""\n        Performs a single optimization step.\n        """"""\n        loss = None\n\n        for group in self.param_groups:\n\n            weight_decay = group[\'weight_decay\']\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                d_p = p.grad.data\n                if weight_decay != 0:\n                    d_p.add_(weight_decay, p.data)\n\n                if group[\'addnoise\']:\n\n                    langevin_noise = p.data.new(p.data.size()).normal_(mean=0, std=1) / np.sqrt(group[\'lr\'])\n                    p.data.add_(-group[\'lr\'],\n                                0.5 * d_p + langevin_noise)\n                else:\n                    p.data.add_(-group[\'lr\'], 0.5 * d_p)\n\n        return loss\n\n\nclass pSGLD(Optimizer):\n    """"""\n    RMSprop preconditioned SGLD using pytorch rmsprop implementation.\n    """"""\n\n    def __init__(self, params, lr=required, norm_sigma=0, alpha=0.99, eps=1e-8, centered=False, addnoise=True):\n\n        weight_decay = 1 / (norm_sigma ** 2)\n\n        if weight_decay < 0.0:\n            raise ValueError(""Invalid weight_decay value: {}"".format(weight_decay))\n        if lr is not required and lr < 0.0:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        defaults = dict(lr=lr, weight_decay=weight_decay, alpha=alpha, eps=eps, centered=centered, addnoise=addnoise)\n        super(pSGLD, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(pSGLD, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'centered\', False)\n\n    def step(self):\n        """"""\n        Performs a single optimization step.\n        """"""\n        loss = None\n\n        for group in self.param_groups:\n\n            weight_decay = group[\'weight_decay\']\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                d_p = p.grad.data\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    state[\'square_avg\'] = torch.zeros_like(p.data)\n                    if group[\'centered\']:\n                        state[\'grad_avg\'] = torch.zeros_like(p.data)\n\n                square_avg = state[\'square_avg\']\n                alpha = group[\'alpha\']\n                state[\'step\'] += 1\n\n                if weight_decay != 0:\n                    d_p.add_(weight_decay, p.data)\n\n                # sqavg x alpha + (1-alph) sqavg *(elemwise) sqavg\n                square_avg.mul_(alpha).addcmul_(1 - alpha, d_p, d_p)\n\n                if group[\'centered\']:\n                    grad_avg = state[\'grad_avg\']\n                    grad_avg.mul_(alpha).add_(1 - alpha, d_p)\n                    avg = square_avg.cmul(-1, grad_avg, grad_avg).sqrt().add_(group[\'eps\'])\n                else:\n                    avg = square_avg.sqrt().add_(group[\'eps\'])\n\n                #                 print(avg.shape)\n                if group[\'addnoise\']:\n                    langevin_noise = p.data.new(p.data.size()).normal_(mean=0, std=1) / np.sqrt(group[\'lr\'])\n                    p.data.add_(-group[\'lr\'],\n                                0.5 * d_p.div_(avg) + langevin_noise / torch.sqrt(avg))\n\n                else:\n                    p.data.addcdiv_(-group[\'lr\'], 0.5 * d_p, avg)\n        return loss\n'"
