file_path,api_count,code
global_vars.py,0,"b""model_urls = {\n    'older': 'https://www.dropbox.com/s/8irg2hguatwdm6v/older.pth?dl=1',\n    'younger': 'https://www.dropbox.com/s/drsx7slvmjdpwuq/younger.pth?dl=1',\n    'facehair': 'https://www.dropbox.com/s/xjd2xh53vw82ces/facehair.pth?dl=1',\n    'masculinization': 'https://www.dropbox.com/s/20q30jnn02qn7n0/masculinization.pth?dl=1',\n    'feminization': 'https://www.dropbox.com/s/5timkh7fuclwk9m/feminization.pth?dl=1',\n    'vgg19g': 'https://www.dropbox.com/s/4lbt58k10o84l5h/vgg19g-4aff041b.pth?dl=1',\n    'vgg_decoder_res': 'https://www.dropbox.com/s/t8vsobxz8avsmj0/decoder_res-9d1e0fe5.pth?dl=1',\n}\n\nfacelet_path = 'facelet_bank'\n"""
test_facelet_net.py,1,"b""'''\nThis script is the testing interface. Please see readme.md for details.\n\nIf you find this project useful for your research, please kindly cite our paper:\n\n@inproceedings{Chen2018Facelet,\n  title={Facelet-Bank for Fast Portrait Manipulation},\n  author={Chen, Ying-Cong and Lin, Huaijia and Shu, Michelle and Li, Ruiyu and Tao, Xin and Ye, Yangang and Shen, Xiaoyong and Jia, Jiaya},\n  booktitle={CVPR},\n  year={2018}\n}\n'''\n\nfrom network.facelet_net import *\nfrom util import test_parse as argparse\nfrom data.testData import Dataset, VideoDataset, untransform\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom network.decoder import vgg_decoder\nfrom global_vars import *\nimport imageio\nfrom util import framework\nfrom network.base_network import VGG\nimport glob\nimport os\n\n\ndef forward(image, vgg, facelet, decoder, weight):\n    vgg_feat = vgg.forward(image)\n    w = facelet.forward(vgg_feat)\n    vgg_feat_transformed = [vgg_feat_ + weight * w_ for vgg_feat_, w_ in zip(vgg_feat, w)]\n    return decoder.forward(vgg_feat_transformed, image)\n\n\ndef test_image():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('command', help='test_image')\n    parser.add_argument('-ip', '--input_path', default='examples/input.png',\n                        help='path of the testing image. Use comma to separate each path. If this argument is a directory, then it will test all images in this directory.')\n    args = parser.parse_args()\n    vgg = VGG()\n    args.pretrained = not args.local_model\n    facelet = Facelet(args)\n    if args.local_model:\n        facelet.load(args.effect, pretrain_path='checkpoints')\n    decoder = vgg_decoder()\n    if not args.cpu:\n        vgg = vgg.cuda()\n        facelet = facelet.cuda()\n        decoder = decoder.cuda()\n    if os.path.isdir(args.input_path):\n        print('input path is a directory. All images in this folder will be tested.')\n        image_list = glob.glob(args.input_path + '/*')\n    else:\n        image_list = args.input_path.split(',')\n    dataset = Dataset(image_list, scale=util.str2numlist(args.size))\n    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n    for idx, data in enumerate(tqdm(dataloader), 0):\n        image, filename, image_shape = data\n        filename = filename[0]\n        image = util.toVariable(image)\n        if not args.cpu:\n            image = image.cuda()\n        output = forward(image, vgg, facelet, decoder, args.strength)\n        output = untransform(output.data[0].cpu())\n        output = util.center_crop(output, (image_shape[0][0], image_shape[1][0]))\n        imageio.imwrite('%s-%s-s-%d.%s' % (util.remove_format_name(filename), args.effect, args.strength, filename[-3:]),\n                        output)\n\n\ndef test_video():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('command', help='test_video')\n    parser.add_argument('-ip', '--input_path', default='examples/input.mp4', help='the path to a video file')\n    args = parser.parse_args()\n    vgg = VGG()\n    args.pretrained = not args.local_model\n    facelet = Facelet(args)\n    if args.local_model:\n        facelet.load(args.effect, pretrain_path='checkpoints')\n    decoder = vgg_decoder()\n    if not args.cpu:\n        vgg = vgg.cuda()\n        facelet = facelet.cuda()\n        decoder = decoder.cuda()\n    reader = imageio.get_reader(args.input_path)\n    fps = reader.get_meta_data()['fps']\n    savepath = '%s-%s-s-%d.%s' % (util.remove_format_name(args.input_path), args.effect, args.strength, args.input_path[-3:])\n    print('saving to %s' % savepath)\n    writer = imageio.get_writer(savepath, fps=fps)\n\n    dataset = VideoDataset(reader, scale=util.str2numlist(args.size))\n    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n    for idx, data in enumerate(tqdm(dataloader), 0):\n        image, image_shape = data\n        image = util.toVariable(image)\n        if not args.cpu:\n            image = image.cuda()\n        output = forward(image, vgg, facelet, decoder, args.strength)\n        output = untransform(output.data[0].cpu())\n        output = util.center_crop(output, (image_shape[0][0], image_shape[1][0]))\n        writer.append_data(output)\n    writer.close()\n\n\nif __name__ == '__main__':\n    Framework = framework.CommandCall()\n    Framework.add(test_image)\n    Framework.add(test_video)\n    Framework.run()\n"""
train_facelet_net.py,2,"b'\'\'\'\nThis script provides the training interface of the Facelet model. Generated attribute vectors are required for training.\nPlease setup DFI project (https://github.com/paulu/deepfeatinterp), and use ""DFI/demo2_facelet.py"" to extract attribute vectors.\nMore details can be found on readme.md.\n\nIf you find this project useful for your research, please kindly cite our paper:\n\n@inproceedings{Chen2018Facelet,\n  title={Facelet-Bank for Fast Portrait Manipulation},\n  author={Chen, Ying-Cong and Lin, Huaijia and Shu, Michelle and Li, Ruiyu and Tao, Xin and Ye, Yangang and Shen, Xiaoyong and Jia, Jiaya},\n  booktitle={CVPR},\n  year={2018}\n}\n\'\'\'\nimport os\nfrom data.trainDataset import Dataset\nfrom data import walk_data\nfrom torch.utils.data import DataLoader\nfrom network.facelet_net import Facelet\nimport util.train_parse as argparse\nfrom network.base_network import VGG\nfrom util import util\nimport torch\nfrom tqdm import tqdm\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-e\', \'--effect\', default=\'facehair\',\n                    help=\'What kind of effect to train. Please note that the spefify\')\nparser.add_argument(\'--pretrain_path\', help=\'pretrain model path\')\nparser.add_argument(\'--pretrain_label\', default=\'latest\', help=\'pretrain model label\')\nparser.add_argument(\'--npz_path\', type=str,\n                    default=\'../face_edit/datasets/training/proj23_ycchen2/deepfeatinterp/attribute_vector\',\n                    help=\'the path of attribute vector\')\nparser.add_argument(\'-ip\', \'--input_path\', type=str,\n                    default=\'../face_edit/datasets/training/proj23/houyang/images_aligned/facemodel_four/celeba\',\n                    help=\'the path of training image\')\nargs = parser.parse_args()\n\n\ndef train():\n    image_path = args.input_path\n    gt_path = args.npz_path\n    npz_list, image_list = walk_data.glob_image_from_npz(gt_path, image_path, \'*_%s.npz\' % args.effect)\n    trainingSet = Dataset(image_list=image_list, npz_list=npz_list)\n    dataloader = DataLoader(trainingSet, batch_size=args.batch_size, shuffle=True, num_workers=4)\n    vgg = torch.nn.DataParallel(VGG()).cuda()\n    args.pretrained = False\n    facelet = Facelet(args)\n    facelet = facelet.cuda()\n    global_step = 0\n    if args.pretrain_path is not None:\n        facelet.load(args.pretrain_path, args.pretrain_label)\n    for epoch in range(args.epoch):\n        for idx, data in enumerate(tqdm(dataloader), 0):\n            image, gt = data\n            vgg_feat = vgg.forward(util.toVariable(image).cuda())\n            _ = facelet.optimize_parameters(vgg_feat, gt)\n            if global_step % 10 == 0:\n                facelet.print_current_errors(epoch=epoch, i=idx)\n            if global_step > 0 and global_step % args.snapshot == 0:\n                facelet.save(label=args.effect)\n            global_step += 1\n        facelet.save(label=args.effect)\n    facelet.save(label=args.effect)\n\n\nif __name__ == \'__main__\':\n    train()\n'"
DFI/demo2_facelet.py,1,"b""'''\nThis script aims to extract attribute vectors from images. It is modified by the demo2.py.\n\nNOTE: it works with the DFI project (https://github.com/paulu/deepfeatinterp) rather than this project.\nPlease copy this script to the root folder of DFI project before runing it.\n\nPlease use\npython demo2_facelet.py -h\nto see the help of options.\n\nFor more details, please see readme.md.\n'''\nfrom __future__ import division\nfrom __future__ import with_statement\nfrom __future__ import print_function\n\nimport time\n\ntimestamp = int(round(time.time()))\nimport numpy\nimport deepmodels\nimport json\nimport os.path\nimport argparse\nimport alignface\nimport imageutils\nimport utils\nfrom tqdm import tqdm\nimport glob\ndef fit_submanifold_landmarks_to_image(template, original, Xlm, face_d, face_p, landmarks=list(range(68))):\n    '''\n    Fit the submanifold to the template and take the top-K.\n\n    Xlm is a N x 68 x 2 list of landmarks.\n    '''\n    lossX = numpy.empty((len(Xlm),), dtype=numpy.float64)\n    MX = numpy.empty((len(Xlm), 2, 3), dtype=numpy.float64)\n    nfail = 0\n    for i in range(len(Xlm)):\n        lm = Xlm[i]\n        try:\n            M, loss = alignface.fit_face_landmarks(Xlm[i], template, landmarks=landmarks, image_dims=original.shape[:2])\n            lossX[i] = loss\n            MX[i] = M\n        except alignface.FitError:\n            lossX[i] = float('inf')\n            MX[i] = 0\n            nfail += 1\n    if nfail > 1:\n        print('fit submanifold, {} errors.'.format(nfail))\n    a = numpy.argsort(lossX)\n    return a, lossX, MX\n\n\nif __name__ == '__main__':\n    # configure by command-line arguments\n    parser = argparse.ArgumentParser(description='Extracting attribute vector for facelet training',\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument('-e', '--effect', type=str, default='facehair', help='desired transformation')\n    parser.add_argument('-ip', '--input_path', type=str,default='images/celeba', help='the training image folder')\n    parser.add_argument('-gpu', type=str, default='0', help='the gpu id to use')\n    parser.add_argument('--backend', type=str, default='torch', choices=['torch', 'caffe+scipy'],\n                        help='reconstruction implementation')\n    parser.add_argument('--K', type=int, default=100, help='number of nearest neighbors')\n    parser.add_argument('--delta', type=str, default='3.5', help='comma-separated list of interpolation steps')\n    parser.add_argument('--npz_path', type=str, default='attribute_vector', help='the path to store npz data')\n    config = parser.parse_args()\n    # print(json.dumps(config.__dict__))\n    os.environ['CUDA_VISIBLE_DEVICES'] = config.gpu\n\n    # load models\n    if config.backend == 'torch':\n        import deepmodels_torch\n\n        model = deepmodels_torch.vgg19g_torch(device_id=0)\n    elif config.backend == 'caffe+scipy':\n        model = deepmodels.vgg19g(device_id=0)\n    else:\n        raise ValueError('Unknown backend')\n    classifier = deepmodels.facemodel_attributes()\n    fields = classifier.fields()\n    gender = fields.index('Male')\n    smile = fields.index('Smiling')\n    face_d, face_p = alignface.load_face_detector()\n    # Set the free parameters\n    K = config.K\n    delta_params = [float(x.strip()) for x in config.delta.split(',')]\n    image_list = glob.glob(config.input_path+'/*')\n    X = image_list\n    t0 = time.time()\n    opathlist = []\n    # for each test image\n    for i in tqdm(range(len(X))):\n        xX = X[i]\n        prefix_path = os.path.splitext(xX)[0]\n        try:\n            template, original = alignface.detect_landmarks(xX, face_d, face_p)\n        except:\n            print('%s face landmark detection error'% xX)\n            continue\n        image_dims = original.shape[:2]\n        XF = model.mean_F([original])\n        XA = classifier.score([xX])[0]\n\n        # select positive and negative sets based on gender and mouth\n        # You can add other attributes here.\n        if config.effect == 'older':\n            cP = [(gender, XA[gender] >= 0), (fields.index('Young'), True)]\n            cQ = [(gender, XA[gender] >= 0), (fields.index('Young'), False)]\n        elif config.effect == 'younger':\n            cP = [(gender, XA[gender] >= 0), (fields.index('Young'), False)]\n            cQ = [(gender, XA[gender] >= 0), (fields.index('Young'), True)]\n        elif config.effect == 'facehair':\n            cP = [(gender, XA[gender] >= 0), (fields.index('No_Beard'), True), (fields.index('Mustache'), False)]\n            cQ = [(gender, XA[gender] >= 0), (fields.index('No_Beard'), False), (fields.index('Mustache'), True)]\n        else:\n            raise ValueError('Unknown method')\n        P = classifier.select(cP, XA)\n        Q = classifier.select(cQ, XA)\n        if len(P) < 4 * K or len(Q) < 4 * K:\n            print('{}: Not enough images in database (|P|={}, |Q|={}).'.format(xX, len(P), len(Q)))\n            continue\n\n        # fit the best 4K database images to input image\n        Plm = classifier.lookup_landmarks(P[:4 * K])\n        Qlm = classifier.lookup_landmarks(Q[:4 * K])\n        idxP, lossP, MP = fit_submanifold_landmarks_to_image(template, original, Plm, face_d, face_p)\n        idxQ, lossQ, MQ = fit_submanifold_landmarks_to_image(template, original, Qlm, face_d, face_p)\n        # Use the K best fitted images\n        xP = [P[i] for i in idxP[:K]]\n        xQ = [Q[i] for i in idxQ[:K]]\n        PF = model.mean_F(utils.warped_image_feed(xP, MP[idxP[:K]], image_dims))\n        QF = model.mean_F(utils.warped_image_feed(xQ, MQ[idxQ[:K]], image_dims))\n        WF = (QF - PF)\n        if not os.path.exists(config.npz_path):\n            os.makedirs(config.npz_path)\n        file_name = os.path.basename(xX)\n        file_name = file_name.split('.')[:-1]\n        file_name = '.'.join(file_name)\n        numpy.savez('{}/{}_{}.npz'.format(config.npz_path, file_name, config.effect), WF=WF)\n"""
data/__init__.py,0,b''
data/base_dataset.py,2,"b""import torch.utils.data as data\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport math\nimport torch\n\nclass BaseDataset(data.Dataset):\n    def __init__(self):\n        super(BaseDataset, self).__init__()\n\n    def name(self):\n        return 'BaseDataset'\n\n    def initialize(self, opt):\n        pass\n\n\nclass FitToQuantum():\n    def __init__(self, quantum=112):\n        self.quantum = float(quantum)\n\n    def __call__(self, img):\n        quantum = self.quantum\n        size = img.size()\n\n        if img.size(1) % int(quantum) == 0:\n            pad_w = 0\n        else:\n            pad_w = int((quantum - img.size(1) % int(quantum)) / 2)\n\n        if img.size(2) % int(quantum) == 0:\n            pad_h = 0\n        else:\n            pad_h = int((quantum - img.size(2) % int(quantum)) / 2)\n\n        res = torch.zeros(size[0],\n                          int(math.ceil(size[1] / quantum) * quantum),\n                          int(math.ceil(size[2] / quantum) * quantum))\n        res[:, pad_w:(pad_w + size[1]), pad_h:(pad_h + size[2])].copy_(img)\n        return res\n\n\ndef get_transform(opt):\n    transform_list = []\n    if opt.resize_or_crop == 'resize_and_crop':\n        osize = [opt.loadSize, opt.loadSize]\n        transform_list.append(transforms.Scale(osize, Image.BICUBIC))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == 'crop':\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == 'scale_width':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.fineSize)))\n    elif opt.resize_or_crop == 'scale_width_and_crop':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.loadSize)))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n\n    if opt.isTrain and not opt.no_flip:\n        transform_list.append(transforms.RandomHorizontalFlip())\n\n    transform_list += [transforms.ToTensor(),\n                       transforms.Normalize((0.5, 0.5, 0.5),\n                                            (0.5, 0.5, 0.5))]\n    return transforms.Compose(transform_list)\n\n\ndef __scale_width(img, target_width):\n    ow, oh = img.size\n    if (ow == target_width):\n        return img\n    w = target_width\n    h = int(target_width * oh / ow)\n    return img.resize((w, h), Image.BICUBIC)\n"""
data/testData.py,2,"b""'''\nonly loop the images, used for testing\n'''\nfrom . import base_dataset\nimport scipy.misc\nimport imageio\nimport numpy as np\nimport torch\nfrom .trainDataset import forward_transform, untransform\n\nmean = torch.Tensor((0.485, 0.456, 0.406))\nstdv = torch.Tensor((0.229, 0.224, 0.225))\n\n\nclass Dataset(base_dataset.BaseDataset):\n    def __init__(self, image_list, transform=forward_transform, scale=(448, 448)):\n        super(Dataset, self).__init__()\n        self.files = []\n        supported_format = ['jpg', 'png', 'jpeg']\n        for image_now in image_list:  # filter out files that are not image\n            format = image_now.split('.')[-1]\n            format = format.lower()\n            is_image = False\n            for sf in supported_format:\n                if format == sf:\n                    is_image = True\n                    break\n            if is_image:\n                self.files += [image_now]\n\n        print('* Total Images: {}'.format(len(self.files)))\n        self.transform = transform\n        self.scale = scale\n\n    def __getitem__(self, index):\n        img = imageio.imread(self.files[index]).astype(np.float32)\n        if self.scale[0] > 0 and img.shape[1] != self.scale:\n            img = scipy.misc.imresize(img, [self.scale[0], self.scale[1]])\n        shape = img.shape\n        img = self.transform(img)\n        return img, self.files[index], shape\n\n    def __len__(self):\n        return len(self.files)\n\n\nclass VideoDataset(base_dataset.BaseDataset):\n    def __init__(self, reader, transform=forward_transform, scale=(448, 448)):\n        super(VideoDataset, self).__init__()\n        self.reader = reader\n        print('* Total Frames: {}'.format(len(self.reader)))\n        self.transform = transform\n        self.scale = scale\n\n    def __getitem__(self, index):\n        img = self.reader.get_data(index).astype(np.float32)\n        if self.scale[0] > 0 and img.shape[1] != self.scale:\n            img = scipy.misc.imresize(img, [self.scale[0], self.scale[1]])\n        shape = img.shape\n        img = self.transform(img)\n        return img, shape\n\n    def __len__(self):\n        return len(self.reader)\n"""
data/trainDataset.py,12,"b""'''\nload paired image and npz data\n'''\nfrom . import base_dataset\nimport imageio\nimport scipy.misc\nimport numpy as np\nimport torchvision as tv\nimport torchvision.transforms as transforms\nimport torch\n\nmean = torch.Tensor((0.485, 0.456, 0.406))\nstdv = torch.Tensor((0.229, 0.224, 0.225))\n\nforward_transform = tv.transforms.Compose(\n    [transforms.ToTensor(), tv.transforms.Normalize(mean=mean, std=stdv), base_dataset.FitToQuantum()])\n\n\ndef vec2featmap(cur_npz, scale):\n    scale1 = int(scale / 4)\n    scale2 = int(scale1 / 2)\n    scale3 = int(scale2 / 2)\n    npz = [torch.zeros(0) for i in range(3)]\n    cur_npz = torch.from_numpy(cur_npz)\n    npz[0] = torch.cat((npz[0], cur_npz[: 256 * scale1 * scale1].resize_(256, scale1, scale1)), 0)\n    npz[1] = torch.cat((npz[1], cur_npz[\n                                256 * scale1 * scale1: 256 * scale1 * scale1 + 512 * scale2 * scale2].resize_(\n        512, scale2, scale2)), 0)\n    npz[2] = torch.cat((npz[2], cur_npz[\n                                256 * scale1 * scale1 + 512 * scale2 * scale2: 256 * scale1 * scale1 + 512 * scale2 * scale2 + 512 * scale3 * scale3].resize_(\n        512, scale3, scale3)), 0)\n    return npz\n\n\nclass Dataset(base_dataset.BaseDataset):\n    def __init__(self, image_list, npz_list, transform=forward_transform, scale=448):\n        super(Dataset, self).__init__()\n        self.files = image_list\n        self.files_npz = npz_list\n        print('* Total Images: {}'.format(len(self.files)))\n        self.transform = transform\n        self.scale = scale\n\n    def __getitem__(self, index):\n        try:\n            img = imageio.imread(self.files[index]).astype(np.float32)\n            # print('scale=',self.scale,self.files_npz[index])\n            if img.shape[1] != self.scale:\n                img = scipy.misc.imresize(img, [self.scale, self.scale])\n            # print('img',img.shape)\n            img = self.transform(img)\n            # print('img_trans',img.shape)\n            cur_npz = np.load(self.files_npz[index])['WF']\n            scale1 = int(self.scale / 4)\n            scale2 = int(scale1 / 2)\n            scale3 = int(scale2 / 2)\n            npz = [torch.zeros(0) for i in range(3)]\n            cur_npz = torch.from_numpy(cur_npz)\n\n            # print('npz', npz[0].shape, npz[1].shape, npz[2].shape)\n            # print('cur_npz', cur_npz.shape)\n            npz[0] = torch.cat((npz[0], cur_npz[: 256 * scale1 * scale1].view(256, scale1, scale1)), 0)\n            npz[1] = torch.cat((npz[1], cur_npz[\n                                        256 * scale1 * scale1: 256 * scale1 * scale1 + 512 * scale2 * scale2].view(\n                512, scale2, scale2)), 0)\n            npz[2] = torch.cat((npz[2], cur_npz[\n                                        256 * scale1 * scale1 + 512 * scale2 * scale2: 256 * scale1 * scale1 + 512 * scale2 * scale2 + 512 * scale3 * scale3].view(\n                512, scale3, scale3)), 0)\n            # print(npz[0].size())\n            return img, npz\n        except:\n            print('load current data error, randomly sample another one')\n            return self.__getitem__(np.random.randint(self.__len__()))\n\n    def __len__(self):\n        return len(self.files)\n\n\ndef untransform(img, mean=mean, stdv=stdv):\n    scale = list(img.size())\n    img *= stdv.view(3, 1, 1).expand(3, scale[-2], scale[-1])\n    img += mean.view(3, 1, 1).expand(3, scale[-2], scale[-1])\n    img = img.numpy()\n    img[img > 1.] = 1.\n    img[img < 0.] = 0.\n    img = img * 255\n    img = img.transpose(1, 2, 0).astype(np.uint8)\n    return img\n\n\n"""
data/walk_data.py,0,"b""from util import util\nimport glob\nimport os\n\n\ndef filter_not_exist(npz_path, img_path):\n    '''\n    given a tuple of path list, filter those non-exist items\n    :param tuple_list:\n    :return:\n    '''\n    out_npz = []\n    out_img = []\n    for npz, img in zip(npz_path, img_path):\n        if os.path.exists(npz) and os.path.exists(img):\n            out_npz += [npz]\n            out_img += [img]\n    return out_npz, out_img\n\ndef glob_image_from_npz(npz_path, img_path, npz_pattern):\n    '''\n    glob images and npz based on the npz files\n    :param npz_path:\n    :param img_path:\n    :param npz_pattern:\n    :param method:\n    :return:\n    '''\n\n    def npz_name_to_image_name(npz_name):\n        img_name = npz_name.split('_')[0]\n        # img_name = npz_name[:-13]  # 'facehair.npz'\n        return img_name\n\n    npz_list = util.globall(npz_path, npz_pattern)\n    assert util.check_exist(npz_list), 'file does not exist'\n    print('length of file:%d'%len(npz_list))\n    non_root_name = [name[len(npz_path):] for name in npz_list]\n    img_list = []\n    for name in non_root_name:\n        img_now = img_path + '/' + npz_name_to_image_name(name) + '.jpg'\n        if not os.path.exists(img_now):\n            img_now = img_path + '/' + npz_name_to_image_name(name) + '.png'\n        # if not os.path.exists(img_now):\n        #     img_now = img_path + '/' + npz_name_to_image_name(name) + '.jpeg'\n        # if not os.path.exists(img_now):\n        #     img_now = img_path + '/' + npz_name_to_image_name(name) + '.JPG'\n        # if not os.path.exists(img_now):\n        #     img_now = img_path + '/' + npz_name_to_image_name(name) + '.PNG'\n        # if not os.path.exists(img_now):\n        #     img_now = img_path + '/' + npz_name_to_image_name(name) + '.JPEG'\n        img_list += [img_now]\n    npz_list, img_list = filter_not_exist(npz_list, img_list)\n    assert util.check_exist(img_list), 'image not exist'\n    return npz_list, img_list\n"""
network/__init__.py,0,b''
network/base_network.py,5,"b""#!/usr/bin/env python2\n\nfrom __future__ import division\nfrom __future__ import with_statement\nfrom __future__ import print_function\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nimport os\nfrom global_vars import *\n\n\nclass BaseModel(object):\n    def name(self):\n        return 'BaseModel'\n\n    def set_input(self, input):\n        self.input = input\n\n    def forward(self, x):\n        pass\n\n    # used in test time, no backprop\n    def test(self):\n        pass\n\n    def get_image_paths(self):\n        pass\n\n    def optimize_parameters(self):\n        pass\n\n    def get_current_visuals(self):\n        return self.input\n\n    def print_current_errors(self, epoch, i, record_file=None):\n        errors = self.get_current_errors()\n        message = '(epoch: %d, iters: %d) ' % (epoch, i)\n        for k, v in errors.items():\n            message += '%s: %.3f ' % (k, v)\n        print(message)\n        if record_file is not None:\n            with open(record_file + '/loss.txt', 'w') as f:\n                print(message, file=f)\n\n    def save(self, label):\n        pass\n\n    def load(self, pretrain_path, label):\n        pass\n\n    # helper saving function that can be used by subclasses\n    def save_network(self, network, save_dir, label):\n        save_filename = '%s.pth' % label\n        save_path = os.path.join(save_dir, save_filename)\n        print('saving %s in %s' % (save_filename, save_path))\n        torch.save(network.cpu().state_dict(), save_path)\n        network.cuda()\n\n    # helper resuming function that can be used by subclasses\n    def resume_network(self, network, network_label, epoch_label):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(self.opt.save_dir, save_filename)\n        print('loading %s from %s' % (save_filename, save_path))\n        network.load_state_dict(torch.load(save_path))\n\n    # helper loading function that can be used by subclasses\n    def load_network(self,network, pretrain_path, label):\n        filename = '%s.pth' % label\n        save_path = os.path.join(pretrain_path, filename)\n        print('loading %s from %s' % (filename, pretrain_path))\n        network.load_state_dict(torch.load(save_path))\n\n\n\n    # update learning rate (called once every epoch)\n    def update_learning_rate(self):\n        for scheduler in self.schedulers:\n            scheduler.step()\n        lr = self.optimizers[0].param_groups[0]['lr']\n        print('learning rate = %.7f' % lr)\n\n\nclass VGG(nn.Module, BaseModel):\n    def __init__(self, pretrained=True):\n        super(VGG, self).__init__()\n        self.features_1 = nn.Sequential(OrderedDict([\n            ('conv1_1', nn.Conv2d(3, 64, kernel_size=3, padding=1)),\n            ('relu1_1', nn.ReLU(inplace=True)),\n            ('conv1_2', nn.Conv2d(64, 64, kernel_size=3, padding=1)),\n            ('relu1_2', nn.ReLU(inplace=True)),\n            ('pool1', nn.MaxPool2d(2, 2)),\n            ('conv2_1', nn.Conv2d(64, 128, kernel_size=3, padding=1)),\n            ('relu2_1', nn.ReLU(inplace=True)),\n            ('conv2_2', nn.Conv2d(128, 128, kernel_size=3, padding=1)),\n            ('relu2_2', nn.ReLU(inplace=True)),\n            ('pool2', nn.MaxPool2d(2, 2)),\n            ('conv3_1', nn.Conv2d(128, 256, kernel_size=3, padding=1)),\n            ('relu3_1', nn.ReLU(inplace=True)),\n        ]))\n        self.features_2 = nn.Sequential(OrderedDict([\n            ('conv3_2', nn.Conv2d(256, 256, kernel_size=3, padding=1)),\n            ('relu3_2', nn.ReLU(inplace=True)),\n            ('conv3_3', nn.Conv2d(256, 256, kernel_size=3, padding=1)),\n            ('relu3_3', nn.ReLU(inplace=True)),\n            ('conv3_4', nn.Conv2d(256, 256, kernel_size=3, padding=1)),\n            ('relu3_5', nn.ReLU(inplace=True)),\n            ('pool3', nn.MaxPool2d(2, 2)),\n            ('conv4_1', nn.Conv2d(256, 512, kernel_size=3, padding=1)),\n            ('relu4_1', nn.ReLU(inplace=True)),\n        ]))\n        self.features_3 = nn.Sequential(OrderedDict([\n            ('conv4_2', nn.Conv2d(512, 512, kernel_size=3, padding=1)),\n            ('relu4_2', nn.ReLU(inplace=True)),\n            ('conv4_3', nn.Conv2d(512, 512, kernel_size=3, padding=1)),\n            ('relu4_3', nn.ReLU(inplace=True)),\n            ('conv4_4', nn.Conv2d(512, 512, kernel_size=3, padding=1)),\n            ('relu4_4', nn.ReLU(inplace=True)),\n            ('pool4', nn.MaxPool2d(2, 2)),\n            ('conv5_1', nn.Conv2d(512, 512, kernel_size=3, padding=1)),\n            ('relu5_1', nn.ReLU(inplace=True)),\n        ]))\n        if pretrained:\n            print('loading pretrained weights of VGG encoder')\n            state_dict = torch.utils.model_zoo.load_url(model_urls['vgg19g'], 'facelet_bank')\n            model_dict = self.state_dict()\n            model_dict.update(state_dict)\n            self.load_state_dict(model_dict)\n\n    def forward(self, x):\n        features_1 = self.features_1(x)\n        features_2 = self.features_2(features_1)\n        features_3 = self.features_3(features_2)\n        return features_1, features_2, features_3\n\n\nclass Vgg_recon(nn.Module):\n    def __init__(self, drop_rate=0):\n        super(Vgg_recon, self).__init__()\n\n        self.recon5 = _PoolingBlock(3, 512, 512, drop_rate=drop_rate)\n        self.upool4 = _TransitionUp(512, 512)\n        self.upsample4 = _Upsample(512, 512)\n        # self.recon4 = _PoolingBlock(3, 1024, 512, drop_rate = drop_rate)\n        self.recon4 = _PoolingBlock(3, 512, 512, drop_rate=drop_rate)\n        self.upool3 = _TransitionUp(512, 256)\n        self.upsample3 = _Upsample(512, 256)\n        self.recon3 = _PoolingBlock(3, 256, 256, drop_rate=drop_rate)\n        self.upool2 = _TransitionUp(256, 128)\n        self.upsample2 = _Upsample(256, 128)\n        self.recon2 = _PoolingBlock(2, 128, 128, drop_rate=drop_rate)\n        self.upool1 = _TransitionUp(128, 64)\n        self.upsample1 = _Upsample(128, 64)\n        self.recon1 = _PoolingBlock(1, 64, 64, drop_rate=drop_rate)\n        self.recon0 = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n\n    def forward(self, fy):\n        # print('fy', len(fy))\n        features_1, features_2, features_3 = fy\n\n        recon5 = self.recon5(features_3)\n        recon5 = nn.functional.upsample(recon5, scale_factor=2, mode='bilinear')\n        upool4 = self.upsample4(recon5)\n\n        recon4 = self.recon4(upool4 + features_2)\n        recon4 = nn.functional.upsample(recon4, scale_factor=2, mode='bilinear')\n        upool3 = self.upsample3(recon4)\n\n        recon3 = self.recon3(upool3 + features_1)\n        recon3 = nn.functional.upsample(recon3, scale_factor=2, mode='bilinear')\n        upool2 = self.upsample2(recon3)\n\n        recon2 = self.recon2(upool2)\n        recon2 = nn.functional.upsample(recon2, scale_factor=2, mode='bilinear')\n        upool1 = self.upsample1(recon2)\n\n        recon1 = self.recon1(upool1)\n        recon0 = self.recon0(recon1)\n        return recon0\n\n\n\nclass _PoolingBlock(nn.Sequential):\n    def __init__(self, n_convs, n_input_filters, n_output_filters, drop_rate):\n        super(_PoolingBlock, self).__init__()\n        for i in range(n_convs):\n            self.add_module('conv.%d' % (i + 1),\n                            nn.Conv2d(n_input_filters if i == 0 else n_output_filters, n_output_filters, kernel_size=3,\n                                      padding=1))\n            # self.add_module('norm.%d' % (i+1), nn.BatchNorm2d(n_output_filters)) # xtao\n            self.add_module('norm.%d' % (i + 1), nn.BatchNorm2d(n_output_filters))\n            self.add_module('relu.%d' % (i + 1), nn.ReLU(inplace=True))\n            if drop_rate > 0:\n                self.add_module('drop.%d' % (i + 1), nn.Dropout(p=drop_rate))\n\n\nclass _TransitionUp(nn.Sequential):\n    def __init__(self, n_input_filters, n_output_filters):\n        super(_TransitionUp, self).__init__()\n        self.add_module('unpool.conv',\n                        nn.ConvTranspose2d(n_input_filters, n_output_filters, kernel_size=4, stride=2, padding=1))\n        # self.add_module('interp.conv', nn.Conv2d(n_input_filters, n_output_filters, kernel_size=3, padding=1))\n        self.add_module('unpool.norm', nn.BatchNorm2d(n_output_filters))\n\n\nclass _Upsample(nn.Sequential):\n    def __init__(self, n_input_filters, n_output_filters):\n        super(_Upsample, self).__init__()\n        # self.add_module('unpool.conv', nn.ConvTranspose2d(n_input_filters, n_output_filters, kernel_size=4, stride=2, padding=1))\n        self.add_module('interp.conv', nn.Conv2d(n_input_filters, n_output_filters, kernel_size=3, padding=1))\n        self.add_module('interp.norm', nn.BatchNorm2d(n_output_filters))\n\n\n"""
network/decoder.py,1,"b""'''\nNote: The decoder is slightly different from the original paper. In this implementation, the decoder only learns the\ndifference between the original image and the edited image. In practice it works better and easier to train.\n'''\n\nfrom torch import nn\nimport torch\nfrom util import util\nfrom global_vars import *\nfrom . import base_network\n\n\nclass vgg_decoder(base_network.BaseModel, nn.Module):\n    def __init__(self, pretrained=True):\n        super(vgg_decoder, self).__init__()\n        self._define_model()\n        if pretrained:\n            print('loading pretrained weights of VGG decoder')\n            state_dict = torch.utils.model_zoo.load_url(model_urls['vgg_decoder_res'], model_dir='facelet_bank')\n            model_dict = self.model.state_dict()\n            model_dict.update(state_dict)\n            self.model.load_state_dict(model_dict)\n\n    def _define_model(self):\n        self.model = base_network.Vgg_recon()\n\n    def forward(self, fy, img=None):\n        fy = [util.toVariable(f) for f in fy]\n        y = self.model.forward(fy)\n        y = y + img\n        return y\n\n    def load(self, pretrain_path, epoch_label='latest'):\n        self.load_network(pretrain_path, self.model, 'recon', epoch_label)\n"""
network/facelet_net.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import OrderedDict\nfrom util import util\nfrom util import opt\nimport functools\nfrom . import base_network\nfrom global_vars import *\n\n\nclass Facelet(base_network.BaseModel, nn.Module):\n    def __init__(self, opt=opt.opt()):\n        super(Facelet, self).__init__()\n        self._default_opt()\n        self.opt = self.opt.merge_opt(opt)\n        self._define_model()\n        self._define_optimizer()\n        if self.opt.pretrained:\n            state_dict = util.load_from_url(model_urls[self.opt.effect], save_dir=facelet_path)\n            model_dict = self.model.state_dict()\n            model_dict.update(state_dict)\n            self.model.load_state_dict(model_dict)\n\n    def _default_opt(self):\n        self.opt = opt.opt()\n        self.opt.save_dir = 'checkpoints'\n        self.opt.name = 'Facelet'\n        self.opt.lr = 1e-4\n        self.opt.pretrained = True\n        self.opt.effect = 'facehair'\n\n    def _define_model(self):\n        self.model = nn.Module()\n        self.model.w_1 = simpleCNNGenerator(256, 256)\n        self.model.w_2 = simpleCNNGenerator(512, 512)\n        self.model.w_3 = simpleCNNGenerator(512, 512)\n\n    def _define_optimizer(self):\n        self.schedulers = []\n        self.optimizer = optim.Adam([{'params': self.model.parameters(), 'lr': self.opt.lr}])\n\n    def forward(self, vgg_feat):\n        f1, f2, f3 = vgg_feat\n        w1 = self.model.w_1(f1)\n        w2 = self.model.w_2(f2)\n        w3 = self.model.w_3(f3)\n        return w1, w2, w3\n\n    def backward_G(self, vgg_feat, gt):\n        self.mseloss = []\n\n        # it sets the loss\n        for i in range(3):\n            criterionL2 = torch.nn.MSELoss(size_average=True)\n            self.mseloss += [criterionL2(vgg_feat[i], gt[i])]\n        self.loss_all = sum(self.mseloss)\n        self.loss_all.backward()\n\n    def optimize_parameters(self, vgg_feat, gt):\n        self.model.train()\n        self.optimizer.zero_grad()\n        vgg_feat = [util.toVariable(vgg_feat_, requires_grad=False).cuda() for vgg_feat_ in vgg_feat]\n        gt = [util.toVariable(gt_, requires_grad=False).cuda() for gt_ in gt]\n        w = self.forward(vgg_feat)\n        self.backward_G(w, gt)\n        self.optimizer.step()\n        return w\n\n    def get_current_errors(self):\n        return OrderedDict([('loss1', self.mseloss[0].data[0]),\n                            ('loss2', self.mseloss[1].data[0]),\n                            ('loss3', self.mseloss[2].data[0]),\n                            ('loss_all', self.loss_all.data[0]),\n                            ])\n\n    def print_current_errors(self, epoch, i):\n        errors = self.get_current_errors()\n        message = '(epoch: %d, iters: %d) ' % (epoch, i)\n        for k, v in errors.items():\n            message += '%s: %.3f ' % (k, v)\n        print(message)\n\n    def save(self, label, save_dir='checkpoints'):\n        self.save_network(self.model, save_dir, label)\n\n    def load(self, label, pretrain_path='checkpoints'):\n        print('loading facelet model')\n        self.load_network(self.model, pretrain_path, label)\n        print('facelet model loaded successfully')\n\n\nclass simpleCNNGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, norm_layer=None, n_blocks=3):\n        assert (n_blocks >= 0)\n        super(simpleCNNGenerator, self).__init__()\n        ngf = input_nc\n        self.input_nc = input_nc\n        self.output_nc = output_nc\n        self.ngf = ngf\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n        if norm_layer is not None:\n            model = [nn.ReflectionPad2d(1),\n                     nn.Conv2d(input_nc, ngf, kernel_size=3, padding=0,\n                               bias=use_bias),\n                     norm_layer(ngf),\n                     nn.ReLU(inplace=True)]\n        else:\n            model = [nn.ReflectionPad2d(1),\n                     nn.Conv2d(input_nc, ngf, kernel_size=3, padding=0),\n                     nn.ReLU(inplace=True)]\n        for i in range(n_blocks - 2):\n            if norm_layer is not None:\n                model += [nn.Conv2d(ngf, ngf, kernel_size=3, padding=1,\n                                    bias=use_bias),\n                          norm_layer(ngf),\n                          nn.ReLU(inplace=True)]\n            else:\n                model += [nn.Conv2d(ngf, ngf, kernel_size=3, padding=1),\n                          nn.ReLU(inplace=True)]\n\n        if norm_layer is not None:\n            model += [nn.ReflectionPad2d(1),\n                      nn.Conv2d(ngf, output_nc, kernel_size=3, padding=0,\n                                bias=use_bias)]\n        else:\n            model += [nn.ReflectionPad2d(1),\n                      nn.Conv2d(ngf, output_nc, kernel_size=3, padding=0)]\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        return self.model(input)\n"""
util/__init__.py,0,b''
util/framework.py,0,"b""'''\nthe framework that are used to execute the code with argparse\n'''\nimport sys\n\n\nclass CommandCall(object):\n    '''\n    This framework fakes Git that use sub command to call.\n    '''\n\n    def __init__(self):\n        self.func_dict = {}\n        # use dispatch pattern to invoke method with same name\n\n    def add(self, func):\n        self.func_dict[func.__name__] = func\n\n    def run(self):\n        import argparse as parser_all\n        parser = parser_all.ArgumentParser(\n            description='Pretends to be git',\n            usage='The usage of this script is similar to git, e.g., git [command] [--params]. Please see readme more details. ')\n        parser.add_argument('command', help='Subcommand to run')\n        # parse_args defaults to [1:] for args, but you need to\n        # exclude the rest of the args too, or validation will fail\n        args = parser.parse_args(sys.argv[1:2])\n        if args.command not in self.func_dict:\n            print('Unrecognized command')\n            parser.print_help()\n            exit(1)\n        self.func_dict[args.command]()\n"""
util/opt.py,0,"b'\'\'\'\nset the default option.\n\'\'\'\nimport collections\n\n\nclass opt(object):\n    def __init__(self, **entries):\n        self.__dict__.update(entries)\n\n    def merge_dict(self, d):\n        # update the option attribute with dict.\n        self.__dict__.update(d)\n        return self\n\n    def merge_opt(self, o):\n        # update the option attribute with another class\n        d = vars(o)\n        self.__dict__.update(d)\n        return self\n\n\ndef merge_dict(dct, merge_dct):\n    """""" Recursive dict merge. Inspired by :meth:``dict.update()``, instead of\n    updating only top-level keys, dict_merge recurses down into dicts nested\n    to an arbitrary depth, updating keys. The ``merge_dct`` is merged into\n    ``dct``.\n    :param dct: dict onto which the merge is executed\n    :param merge_dct: dct merged into dct\n    :return: None\n    """"""\n    for k, v in merge_dct.iteritems():\n        if (k in dct and isinstance(dct[k], dict)\n            and isinstance(merge_dct[k], collections.Mapping)):\n            merge_dict(dct[k], merge_dct[k])\n        else:\n            dct[k] = merge_dct[k]\n\n\nif __name__ == \'__main__\':\n    a = opt()\n    b = opt()\n    a.a = 1\n    a.b = 1\n    b.b = 2\n    b.c = 2\n    import util\n\n    util.print_args_to_screen(a)\n    util.print_args_to_screen(b)\n    a.merge_opt(b)\n    util.print_args_to_screen(a)\n'"
util/sha256.py,0,"b""import hashlib\nimport sys\n\ndef sha256_checksum(filename, block_size=65536):\n    sha256 = hashlib.sha256()\n    with open(filename, 'rb') as f:\n        for block in iter(lambda: f.read(block_size), b''):\n            sha256.update(block)\n    return sha256.hexdigest()\n\ndef main():\n    for f in sys.argv[1:]:\n        checksum = sha256_checksum(f)\n        print(f + '\\t' + checksum)\n\nif __name__ == '__main__':\n    main()"""
util/test_parse.py,0,"b'from argparse import *\nfrom . import util\nimport os\n\n\nclass ArgumentParser(ArgumentParser):\n    def __init__(self, formatter_class=ArgumentDefaultsHelpFormatter, **kwargs):\n        super(ArgumentParser, self).__init__(formatter_class=formatter_class, **kwargs)\n        self.add_argument(\'-gpu\', \'--gpu_id\', default=\'0\', help=\'GPU ID to use.\')\n        self.add_argument(\'-s\', \'--strength\', type=float, default=5, help=\'The edit strength\')\n        self.add_argument(\'-e\', \'--effect\', default=\'facehair\',\n                          help=\'Which kind of effect to use. Current version supports facehair, older, younger.\')\n        self.add_argument(\'--size\', default=\'0,0\',\n                          help=\'Resize the input image specific size. 0,0 indicates keep the original shape.\')\n        self.add_argument(\'-cpu\', action=\'store_true\', help=\'include \\""-cpu\\"" if you want to work on CPU.\')\n        self.add_argument(\'--local_model\', action=\'store_true\', help=\'include \\""--local_model\\"" if you want to test your own model. Those models are stored in \\""checkpoints\\""\')\n\n    def parse_args(self, args=None, namespace=None):\n        args = super(ArgumentParser, self).parse_args(args=args, namespace=namespace)\n        os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu_id\n        return args\n'"
util/train_parse.py,0,"b'from argparse import *\nfrom . import util\nimport os\n\n\nclass ArgumentParser(ArgumentParser):\n    def __init__(self, formatter_class=ArgumentDefaultsHelpFormatter, **kwargs):\n        super(ArgumentParser, self).__init__(formatter_class=formatter_class, **kwargs)\n        self.add_argument(\'-gpu\', \'--gpu_id\', default=\'7\', help=\'gpu id\')\n        self.add_argument(\'-lr\', \'--lr\', type=float, default=1e-4, help=\'learning rate\')\n        self.add_argument(\'-ss\', \'--snapshot\', type=int, default=10000, help=\'number of steps to save each snapshot\')\n        self.add_argument(\'-sp\', \'--save_path\', default=\'\',\n                          help=\'the save path. it will automatically stored in checkpoints/SCRIPT_NAME/SAVEPATH\')\n        self.add_argument(\'-bs\', \'--batch_size\', type=int, default=16, help=\'batch_size\')\n        self.add_argument(\'--epoch\', type=int, default=30, help=\'the number of epoches\')\n\n    def parse_args(self, args=None, namespace=None):\n        args = super(ArgumentParser, self).parse_args(args=args, namespace=namespace)\n        util.print_args_to_screen(args)\n        os.environ[""CUDA_VISIBLE_DEVICES""] = args.gpu_id\n        return args\n'"
util/util.py,8,"b'from __future__ import print_function\nimport os\nimport glob\nimport sys\nimport fnmatch\nimport shutil\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom tqdm import tqdm\nimport tempfile\n\ntry:\n    from requests.utils import urlparse\n    import requests.get as urlopen\n\n    requests_available = True\nexcept ImportError:\n    requests_available = False\n    if sys.version_info[0] == 2:\n        from urlparse import urlparse  # noqa f811\n        from urllib2 import urlopen  # noqa f811\n    else:\n        from urllib.request import urlopen\n        from urllib.parse import urlparse\n\n\n# Converts a Tensor into a Numpy array\n# |imtype|: the desired type of the converted numpy array\ndef tensor2im(image_tensor, imtype=np.uint8):\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    if image_numpy.shape[0] == 1:\n        image_numpy = np.tile(image_numpy, (3, 1, 1))\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n    return image_numpy.astype(imtype)\n\n\ndef diagnose_network(net, name=\'network\'):\n    mean = 0.0\n    count = 0\n    for param in net.parameters():\n        if param.grad is not None:\n            mean += torch.mean(torch.abs(param.grad.data))\n            count += 1\n    if count > 0:\n        mean = mean / count\n    print(name)\n    print(mean)\n\n\n# implement expand_dims for pytorch tensor x\ndef expand_dims(x, axis):\n    shape = list(x.size())\n    assert len(shape) >= axis, \'expand_dims error\'\n    shape.insert(axis, 1)\n    y = x.view(shape)\n    return y\n\n\n# convert a unknown object (could be variable) to tensor\ndef toTensor(obj):\n    if type(obj) == Variable:\n        y = obj.data\n    elif type(obj) == np.ndarray:\n        y = torch.from_numpy(obj)\n    elif type(obj) == torch.FloatTensor or type(obj) == torch.cuda.FloatTensor:\n        y = obj\n    elif type(obj) == torch.nn.Parameter:\n        y = obj.data\n    else:\n        assert 0, \'type: %s is not supported yet\' % type(obj)\n    return y\n\n\n# convert a unknown object (could be variable) to tensor\ndef toVariable(obj, requires_grad=False):\n    if type(obj) == Variable:\n        y = Variable(obj.data, requires_grad=requires_grad)\n    elif type(obj) == np.ndarray:\n        y = torch.from_numpy(obj.astype(np.float32))\n        y = Variable(y, requires_grad=requires_grad)\n    elif type(obj) == torch.FloatTensor or type(obj) == torch.cuda.FloatTensor:\n        y = Variable(obj, requires_grad=requires_grad)\n    else:\n        assert 0, \'type: %s is not supported yet\' % type(obj)\n    return y\n\n\ndef print_network(net, filepath=None):\n    if filepath is None:\n        num_params = 0\n        for param in net.parameters():\n            num_params += param.numel()\n        print(net)\n        print(\'Total number of parameters: %d\' % num_params)\n    else:\n        num_params = 0\n        with open(filepath + \'/network.txt\', \'w\') as f:\n            for param in net.parameters():\n                num_params += param.numel()\n            print(net, file=f)\n            f.write(\'Total number of parameters: %d\' % num_params)\n\n\n# -------------------------- General ---------------------------------#\n\n\ndef _download_url_to_file(url, dst):\n    u = urlopen(url)\n    if requests_available:\n        file_size = int(u.headers[""Content-Length""])\n        u = u.raw\n    else:\n        meta = u.info()\n        if hasattr(meta, \'getheaders\'):\n            file_size = int(meta.getheaders(""Content-Length"")[0])\n        else:\n            file_size = int(meta.get_all(""Content-Length"")[0])\n\n    f = tempfile.NamedTemporaryFile(delete=False)\n    with tqdm(total=file_size) as pbar:\n        while True:\n            buffer = u.read(8192)\n            if len(buffer) == 0:\n                break\n            f.write(buffer)\n            pbar.update(len(buffer))\n    f.close()\n    shutil.move(f.name, dst)\n\n\ndef load_from_url(url, save_dir=\'facelet_bank\'):\n    parts = urlparse(url)\n    filename = os.path.basename(parts.path)\n    cached_file = os.path.join(save_dir, filename)\n    if not os.path.exists(cached_file):\n        sys.stderr.write(\'Downloading: ""{}"" to {}\\n\'.format(url, cached_file))\n        _download_url_to_file(url, cached_file)\n    return torch.load(cached_file)\n\n\ndef center_crop(img, target_size):\n    \'\'\'\n    center crop on numpy data.\n    :param img: H x W x C\n    :param target_size: h x w\n    :return: h x w x C\n    \'\'\'\n    diff_x = img.shape[0] - target_size[0]\n    diff_y = img.shape[1] - target_size[1]\n    start_x = int(diff_x // 2)\n    start_y = int(diff_y // 2)\n    if len(img.shape) > 2:\n        img2 = img[start_x:start_x + target_size[0], start_y:start_y + target_size[1], :]\n    else:\n        img2 = img[start_x:start_x + target_size[0], start_y:start_y + target_size[1]]\n    return img2\n\n\ndef remove_format_name(filename):\n    filename = filename.split(\'.\')\n    filename = \'.\'.join(filename[:-1])\n    return filename\n\ndef check_exist(file_list):\n    for file in file_list:\n        if not os.path.exists(file):\n            print(\'file not exit: \' + file)\n            return False\n    return True\n\n\ndef str2numlist(str_in, type=int):\n    dc = []\n    dc_str = str_in.split(\',\')\n    for d in dc_str:\n        dc += [type(d)]\n    return dc\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        print(\'mkdir %s\' % path)\n        os.makedirs(path)\n\n\ndef globall(path, pattern):\n    \'\'\'\n    glob all data based on the pattern\n    :param path: the root path\n    :param pattern: the pattern to filter\n    :return: all files that matches the pattern\n    \'\'\'\n    matches = []\n    for root, dirnames, filenames in os.walk(path):\n        for filename in fnmatch.filter(filenames, pattern):\n            matches.append(os.path.join(root, filename))\n    return matches\n\n\ndef script_name():\n    name = os.path.basename(sys.argv[0])\n    name = name[:-3]\n    return name\n\n\ndef print_args(ckpt_dir, args):\n    \'\'\'\n    print all args generated from the argparse\n    :param ckpt_dir: the save dir\n    :param args: the args\n    :return:\n    \'\'\'\n    args_dict = vars(args)\n    with open(ckpt_dir + \'/options.txt\', \'w\') as f:\n        for k, v in args_dict.items():\n            f.write(\'%s: %s\\n\' % (k, v))\n\n\ndef print_args_to_screen(args):\n    \'\'\'\n    print all args generated from the argparse\n    :param ckpt_dir: the save dir\n    :param args: the args\n    :return:\n    \'\'\'\n    args_dict = vars(args)\n    for k, v in args_dict.items():\n        print(\'%s: %s\' % (k, v))\n    print(\'\\n\')\n\n\n'"
