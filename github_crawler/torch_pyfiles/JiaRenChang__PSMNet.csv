file_path,api_count,code
Test_img.py,8,"b""from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport numpy as np\nimport time\nimport math\nfrom models import *\nimport cv2\nfrom PIL import Image\n\n# 2012 data /media/jiaren/ImageNet/data_scene_flow_2012/testing/\n\nparser = argparse.ArgumentParser(description='PSMNet')\nparser.add_argument('--KITTI', default='2015',\n                    help='KITTI version')\nparser.add_argument('--datapath', default='/media/jiaren/ImageNet/data_scene_flow_2015/testing/',\n                    help='select model')\nparser.add_argument('--loadmodel', default='./trained/pretrained_model_KITTI2015.tar',\n                    help='loading model')\nparser.add_argument('--leftimg', default= './VO04_L.png',\n                    help='load model')\nparser.add_argument('--rightimg', default= './VO04_R.png',\n                    help='load model')                                      \nparser.add_argument('--model', default='stackhourglass',\n                    help='select model')\nparser.add_argument('--maxdisp', type=int, default=192,\n                    help='maxium disparity')\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='enables CUDA training')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed (default: 1)')\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\nif args.model == 'stackhourglass':\n    model = stackhourglass(args.maxdisp)\nelif args.model == 'basic':\n    model = basic(args.maxdisp)\nelse:\n    print('no model')\n\nmodel = nn.DataParallel(model, device_ids=[0])\nmodel.cuda()\n\nif args.loadmodel is not None:\n    print('load PSMNet')\n    state_dict = torch.load(args.loadmodel)\n    model.load_state_dict(state_dict['state_dict'])\n\nprint('Number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n\ndef test(imgL,imgR):\n        model.eval()\n\n        if args.cuda:\n           imgL = imgL.cuda()\n           imgR = imgR.cuda()     \n\n        with torch.no_grad():\n            disp = model(imgL,imgR)\n\n        disp = torch.squeeze(disp)\n        pred_disp = disp.data.cpu().numpy()\n\n        return pred_disp\n\n\ndef main():\n\n        normal_mean_var = {'mean': [0.485, 0.456, 0.406],\n                            'std': [0.229, 0.224, 0.225]}\n        infer_transform = transforms.Compose([transforms.ToTensor(),\n                                              transforms.Normalize(**normal_mean_var)])    \n\n        imgL_o = Image.open(args.leftimg).convert('RGB')\n        imgR_o = Image.open(args.rightimg).convert('RGB')\n\n        imgL = infer_transform(imgL_o)\n        imgR = infer_transform(imgR_o) \n       \n\n        # pad to width and hight to 16 times\n        if imgL.shape[1] % 16 != 0:\n            times = imgL.shape[1]//16       \n            top_pad = (times+1)*16 -imgL.shape[1]\n        else:\n            top_pad = 0\n\n        if imgL.shape[2] % 16 != 0:\n            times = imgL.shape[2]//16                       \n            right_pad = (times+1)*16-imgL.shape[2]\n        else:\n            right_pad = 0    \n\n        imgL = F.pad(imgL,(0,right_pad, top_pad,0)).unsqueeze(0)\n        imgR = F.pad(imgR,(0,right_pad, top_pad,0)).unsqueeze(0)\n\n        start_time = time.time()\n        pred_disp = test(imgL,imgR)\n        print('time = %.2f' %(time.time() - start_time))\n\n        \n        if top_pad !=0 or right_pad != 0:\n            img = pred_disp[top_pad:,:-right_pad]\n        else:\n            img = pred_disp\n        \n        img = (img*256).astype('uint16')\n        img = Image.fromarray(img)\n        img.save('Test_disparity.png')\n\nif __name__ == '__main__':\n   main()\n\n\n\n\n\n\n"""
finetune.py,26,"b""from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport skimage\nimport skimage.io\nimport skimage.transform\nimport numpy as np\nimport time\nimport math\nfrom dataloader import KITTIloader2015 as ls\nfrom dataloader import KITTILoader as DA\n\nfrom models import *\n\nparser = argparse.ArgumentParser(description='PSMNet')\nparser.add_argument('--maxdisp', type=int ,default=192,\n                    help='maxium disparity')\nparser.add_argument('--model', default='stackhourglass',\n                    help='select model')\nparser.add_argument('--datatype', default='2015',\n                    help='datapath')\nparser.add_argument('--datapath', default='/media/jiaren/ImageNet/data_scene_flow_2015/training/',\n                    help='datapath')\nparser.add_argument('--epochs', type=int, default=300,\n                    help='number of epochs to train')\nparser.add_argument('--loadmodel', default='./trained/submission_model.tar',\n                    help='load model')\nparser.add_argument('--savemodel', default='./',\n                    help='save model')\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='enables CUDA training')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed (default: 1)')\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\nif args.datatype == '2015':\n   from dataloader import KITTIloader2015 as ls\nelif args.datatype == '2012':\n   from dataloader import KITTIloader2012 as ls\n\nall_left_img, all_right_img, all_left_disp, test_left_img, test_right_img, test_left_disp = ls.dataloader(args.datapath)\n\nTrainImgLoader = torch.utils.data.DataLoader(\n         DA.myImageFloder(all_left_img,all_right_img,all_left_disp, True), \n         batch_size= 12, shuffle= True, num_workers= 8, drop_last=False)\n\nTestImgLoader = torch.utils.data.DataLoader(\n         DA.myImageFloder(test_left_img,test_right_img,test_left_disp, False), \n         batch_size= 8, shuffle= False, num_workers= 4, drop_last=False)\n\nif args.model == 'stackhourglass':\n    model = stackhourglass(args.maxdisp)\nelif args.model == 'basic':\n    model = basic(args.maxdisp)\nelse:\n    print('no model')\n\nif args.cuda:\n    model = nn.DataParallel(model)\n    model.cuda()\n\nif args.loadmodel is not None:\n    state_dict = torch.load(args.loadmodel)\n    model.load_state_dict(state_dict['state_dict'])\n\nprint('Number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n\noptimizer = optim.Adam(model.parameters(), lr=0.1, betas=(0.9, 0.999))\n\ndef train(imgL,imgR,disp_L):\n        model.train()\n        imgL   = Variable(torch.FloatTensor(imgL))\n        imgR   = Variable(torch.FloatTensor(imgR))   \n        disp_L = Variable(torch.FloatTensor(disp_L))\n\n        if args.cuda:\n            imgL, imgR, disp_true = imgL.cuda(), imgR.cuda(), disp_L.cuda()\n\n        #---------\n        mask = (disp_true > 0)\n        mask.detach_()\n        #----\n\n        optimizer.zero_grad()\n        \n        if args.model == 'stackhourglass':\n            output1, output2, output3 = model(imgL,imgR)\n            output1 = torch.squeeze(output1,1)\n            output2 = torch.squeeze(output2,1)\n            output3 = torch.squeeze(output3,1)\n            loss = 0.5*F.smooth_l1_loss(output1[mask], disp_true[mask], size_average=True) + 0.7*F.smooth_l1_loss(output2[mask], disp_true[mask], size_average=True) + F.smooth_l1_loss(output3[mask], disp_true[mask], size_average=True) \n        elif args.model == 'basic':\n            output = model(imgL,imgR)\n            output = torch.squeeze(output3,1)\n            loss = F.smooth_l1_loss(output3[mask], disp_true[mask], size_average=True)\n\n        loss.backward()\n        optimizer.step()\n\n        return loss.data[0]\n\ndef test(imgL,imgR,disp_true):\n        model.eval()\n        imgL   = Variable(torch.FloatTensor(imgL))\n        imgR   = Variable(torch.FloatTensor(imgR))   \n        if args.cuda:\n            imgL, imgR = imgL.cuda(), imgR.cuda()\n\n        with torch.no_grad():\n            output3 = model(imgL,imgR)\n\n        pred_disp = output3.data.cpu()\n\n        #computing 3-px error#\n        true_disp = disp_true\n        index = np.argwhere(true_disp>0)\n        disp_true[index[0][:], index[1][:], index[2][:]] = np.abs(true_disp[index[0][:], index[1][:], index[2][:]]-pred_disp[index[0][:], index[1][:], index[2][:]])\n        correct = (disp_true[index[0][:], index[1][:], index[2][:]] < 3)|(disp_true[index[0][:], index[1][:], index[2][:]] < true_disp[index[0][:], index[1][:], index[2][:]]*0.05)      \n        torch.cuda.empty_cache()\n\n        return 1-(float(torch.sum(correct))/float(len(index[0])))\n\ndef adjust_learning_rate(optimizer, epoch):\n    if epoch <= 200:\n       lr = 0.001\n    else:\n       lr = 0.0001\n    print(lr)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\ndef main():\n\tmax_acc=0\n\tmax_epo=0\n\tstart_full_time = time.time()\n\n\tfor epoch in range(1, args.epochs+1):\n\t   total_train_loss = 0\n\t   total_test_loss = 0\n\t   adjust_learning_rate(optimizer,epoch)\n           \n               ## training ##\n           for batch_idx, (imgL_crop, imgR_crop, disp_crop_L) in enumerate(TrainImgLoader):\n               start_time = time.time() \n\n               loss = train(imgL_crop,imgR_crop, disp_crop_L)\n\t       print('Iter %d training loss = %.3f , time = %.2f' %(batch_idx, loss, time.time() - start_time))\n\t       total_train_loss += loss\n\t   print('epoch %d total training loss = %.3f' %(epoch, total_train_loss/len(TrainImgLoader)))\n\t   \n               ## Test ##\n\n           for batch_idx, (imgL, imgR, disp_L) in enumerate(TestImgLoader):\n               test_loss = test(imgL,imgR, disp_L)\n               print('Iter %d 3-px error in val = %.3f' %(batch_idx, test_loss*100))\n               total_test_loss += test_loss\n\n\n\t   print('epoch %d total 3-px error in val = %.3f' %(epoch, total_test_loss/len(TestImgLoader)*100))\n\t   if total_test_loss/len(TestImgLoader)*100 > max_acc:\n\t\tmax_acc = total_test_loss/len(TestImgLoader)*100\n\t\tmax_epo = epoch\n\t   print('MAX epoch %d total test error = %.3f' %(max_epo, max_acc))\n\n\t   #SAVE\n\t   savefilename = args.savemodel+'finetune_'+str(epoch)+'.tar'\n\t   torch.save({\n\t\t    'epoch': epoch,\n\t\t    'state_dict': model.state_dict(),\n\t\t    'train_loss': total_train_loss/len(TrainImgLoader),\n\t\t    'test_loss': total_test_loss/len(TestImgLoader)*100,\n\t\t}, savefilename)\n\t\n        print('full finetune time = %.2f HR' %((time.time() - start_full_time)/3600))\n\tprint(max_epo)\n\tprint(max_acc)\n\n\nif __name__ == '__main__':\n   main()\n"""
main.py,19,"b""from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport numpy as np\nimport time\nimport math\nfrom dataloader import listflowfile as lt\nfrom dataloader import SecenFlowLoader as DA\nfrom models import *\n\nparser = argparse.ArgumentParser(description='PSMNet')\nparser.add_argument('--maxdisp', type=int ,default=192,\n                    help='maxium disparity')\nparser.add_argument('--model', default='stackhourglass',\n                    help='select model')\nparser.add_argument('--datapath', default='/media/jiaren/ImageNet/SceneFlowData/',\n                    help='datapath')\nparser.add_argument('--epochs', type=int, default=0,\n                    help='number of epochs to train')\nparser.add_argument('--loadmodel', default= './trained/pretrained_sceneflow.tar',\n                    help='load model')\nparser.add_argument('--savemodel', default='./',\n                    help='save model')\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='enables CUDA training')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed (default: 1)')\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\nall_left_img, all_right_img, all_left_disp, test_left_img, test_right_img, test_left_disp = lt.dataloader(args.datapath)\n\nTrainImgLoader = torch.utils.data.DataLoader(\n         DA.myImageFloder(all_left_img,all_right_img,all_left_disp, True), \n         batch_size= 12, shuffle= True, num_workers= 8, drop_last=False)\n\nTestImgLoader = torch.utils.data.DataLoader(\n         DA.myImageFloder(test_left_img,test_right_img,test_left_disp, False), \n         batch_size= 8, shuffle= False, num_workers= 4, drop_last=False)\n\n\nif args.model == 'stackhourglass':\n    model = stackhourglass(args.maxdisp)\nelif args.model == 'basic':\n    model = basic(args.maxdisp)\nelse:\n    print('no model')\n\nif args.cuda:\n    model = nn.DataParallel(model)\n    model.cuda()\n\nif args.loadmodel is not None:\n    print('Load pretrained model')\n    pretrain_dict = torch.load(args.loadmodel)\n    model.load_state_dict(pretrain_dict['state_dict'])\n\nprint('Number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n\noptimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n\ndef train(imgL,imgR, disp_L):\n        model.train()\n\n        if args.cuda:\n            imgL, imgR, disp_true = imgL.cuda(), imgR.cuda(), disp_L.cuda()\n\n       #---------\n        mask = disp_true < args.maxdisp\n        mask.detach_()\n        #----\n        optimizer.zero_grad()\n        \n        if args.model == 'stackhourglass':\n            output1, output2, output3 = model(imgL,imgR)\n            output1 = torch.squeeze(output1,1)\n            output2 = torch.squeeze(output2,1)\n            output3 = torch.squeeze(output3,1)\n            loss = 0.5*F.smooth_l1_loss(output1[mask], disp_true[mask], size_average=True) + 0.7*F.smooth_l1_loss(output2[mask], disp_true[mask], size_average=True) + F.smooth_l1_loss(output3[mask], disp_true[mask], size_average=True) \n        elif args.model == 'basic':\n            output = model(imgL,imgR)\n            output = torch.squeeze(output,1)\n            loss = F.smooth_l1_loss(output[mask], disp_true[mask], size_average=True)\n\n        loss.backward()\n        optimizer.step()\n\n        return loss.data\n\ndef test(imgL,imgR,disp_true):\n\n        model.eval()\n  \n        if args.cuda:\n            imgL, imgR, disp_true = imgL.cuda(), imgR.cuda(), disp_true.cuda()\n        #---------\n        mask = disp_true < 192\n        #----\n\n        if imgL.shape[2] % 16 != 0:\n            times = imgL.shape[2]//16       \n            top_pad = (times+1)*16 -imgL.shape[2]\n        else:\n            top_pad = 0\n\n        if imgL.shape[3] % 16 != 0:\n            times = imgL.shape[3]//16                       \n            right_pad = (times+1)*16-imgL.shape[3]\n        else:\n            right_pad = 0  \n\n        imgL = F.pad(imgL,(0,right_pad, top_pad,0))\n        imgR = F.pad(imgR,(0,right_pad, top_pad,0))\n\n        with torch.no_grad():\n            output3 = model(imgL,imgR)\n            output3 = torch.squeeze(output3)\n        \n        if top_pad !=0:\n            img = output3[:,top_pad:,:]\n        else:\n            img = output3\n\n        if len(disp_true[mask])==0:\n           loss = 0\n        else:\n           loss = F.l1_loss(img[mask],disp_true[mask]) #torch.mean(torch.abs(img[mask]-disp_true[mask]))  # end-point-error\n\n        return loss.data.cpu()\n\ndef adjust_learning_rate(optimizer, epoch):\n    lr = 0.001\n    print(lr)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\ndef main():\n\n\tstart_full_time = time.time()\n\tfor epoch in range(0, args.epochs):\n\t   print('This is %d-th epoch' %(epoch))\n\t   total_train_loss = 0\n\t   adjust_learning_rate(optimizer,epoch)\n\n\t   ## training ##\n\t   for batch_idx, (imgL_crop, imgR_crop, disp_crop_L) in enumerate(TrainImgLoader):\n\t     start_time = time.time()\n\n\t     loss = train(imgL_crop,imgR_crop, disp_crop_L)\n\t     print('Iter %d training loss = %.3f , time = %.2f' %(batch_idx, loss, time.time() - start_time))\n\t     total_train_loss += loss\n\t   print('epoch %d total training loss = %.3f' %(epoch, total_train_loss/len(TrainImgLoader)))\n\n\t   #SAVE\n\t   savefilename = args.savemodel+'/checkpoint_'+str(epoch)+'.tar'\n\t   torch.save({\n\t\t    'epoch': epoch,\n\t\t    'state_dict': model.state_dict(),\n                    'train_loss': total_train_loss/len(TrainImgLoader),\n\t\t}, savefilename)\n\n\tprint('full training time = %.2f HR' %((time.time() - start_full_time)/3600))\n\n\t#------------- TEST ------------------------------------------------------------\n\ttotal_test_loss = 0\n\tfor batch_idx, (imgL, imgR, disp_L) in enumerate(TestImgLoader):\n\t       test_loss = test(imgL,imgR, disp_L)\n\t       print('Iter %d test loss = %.3f' %(batch_idx, test_loss))\n\t       total_test_loss += test_loss\n\n\tprint('total test loss = %.3f' %(total_test_loss/len(TestImgLoader)))\n\t#----------------------------------------------------------------------------------\n\t#SAVE test information\n\tsavefilename = args.savemodel+'testinformation.tar'\n\ttorch.save({\n\t\t    'test_loss': total_test_loss/len(TestImgLoader),\n\t\t}, savefilename)\n\n\nif __name__ == '__main__':\n   main()\n    \n"""
submission.py,8,"b""from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport numpy as np\nimport time\nimport math\nfrom models import *\nfrom PIL import Image\n\nparser = argparse.ArgumentParser(description='PSMNet')\nparser.add_argument('--KITTI', default='2015',\n                    help='KITTI version')\nparser.add_argument('--datapath', default='/media/jiaren/ImageNet/data_scene_flow_2015/testing/',\n                    help='select model')\nparser.add_argument('--loadmodel', default='./trained/pretrained_model_KITTI2015.tar',\n                    help='loading model')\nparser.add_argument('--model', default='stackhourglass',\n                    help='select model')\nparser.add_argument('--maxdisp', default=192,\n                    help='maxium disparity')\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='enables CUDA training')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed (default: 1)')\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\nif args.KITTI == '2015':\n   from dataloader import KITTI_submission_loader as DA\nelse:\n   from dataloader import KITTI_submission_loader2012 as DA  \n\ntest_left_img, test_right_img = DA.dataloader(args.datapath)\n\nif args.model == 'stackhourglass':\n    model = stackhourglass(args.maxdisp)\nelif args.model == 'basic':\n    model = basic(args.maxdisp)\nelse:\n    print('no model')\n\nmodel = nn.DataParallel(model, device_ids=[0])\nmodel.cuda()\n\nif args.loadmodel is not None:\n    state_dict = torch.load(args.loadmodel)\n    model.load_state_dict(state_dict['state_dict'])\n\nprint('Number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))\n\ndef test(imgL,imgR):\n    model.eval()\n\n    if args.cuda:\n        imgL = imgL.cuda()\n        imgR = imgR.cuda()     \n\n    with torch.no_grad():\n        output = model(imgL,imgR)\n    output = torch.squeeze(output).data.cpu().numpy()\n    return output\n\ndef main():\n    normal_mean_var = {'mean': [0.485, 0.456, 0.406],\n                        'std': [0.229, 0.224, 0.225]}\n    infer_transform = transforms.Compose([transforms.ToTensor(),\n                                            transforms.Normalize(**normal_mean_var)])    \n\n    for inx in range(len(test_left_img)):\n\n        imgL_o = Image.open(test_left_img[inx]).convert('RGB')\n        imgR_o = Image.open(test_right_img[inx]).convert('RGB')\n\n        imgL = infer_transform(imgL_o)\n        imgR = infer_transform(imgR_o)         \n\n        # pad to width and hight to 16 times\n        if imgL.shape[1] % 16 != 0:\n            times = imgL.shape[1]//16       \n            top_pad = (times+1)*16 -imgL.shape[1]\n        else:\n            top_pad = 0\n\n        if imgL.shape[2] % 16 != 0:\n            times = imgL.shape[2]//16                       \n            right_pad = (times+1)*16-imgL.shape[2]\n        else:\n            right_pad = 0    \n\n        imgL = F.pad(imgL,(0,right_pad, top_pad,0)).unsqueeze(0)\n        imgR = F.pad(imgR,(0,right_pad, top_pad,0)).unsqueeze(0)\n\n        start_time = time.time()\n        pred_disp = test(imgL,imgR)\n        print('time = %.2f' %(time.time() - start_time))\n\n        if top_pad !=0 or right_pad != 0:\n            img = pred_disp[top_pad:,:-right_pad]\n        else:\n            img = pred_disp\n\n        img = (img*256).astype('uint16')\n        img = Image.fromarray(img)\n        img.save(test_left_img[inx].split('/')[-1])\n\n\nif __name__ == '__main__':\n    main()\n\n\n\n\n\n\n"""
dataloader/KITTILoader.py,1,"b""import os\nimport torch\nimport torch.utils.data as data\nimport torch\nimport torchvision.transforms as transforms\nimport random\nfrom PIL import Image, ImageOps\nimport numpy as np\nimport preprocess \n\nIMG_EXTENSIONS = [\n    '.jpg', '.JPG', '.jpeg', '.JPEG',\n    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n]\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\ndef default_loader(path):\n    return Image.open(path).convert('RGB')\n\ndef disparity_loader(path):\n    return Image.open(path)\n\n\nclass myImageFloder(data.Dataset):\n    def __init__(self, left, right, left_disparity, training, loader=default_loader, dploader= disparity_loader):\n \n        self.left = left\n        self.right = right\n        self.disp_L = left_disparity\n        self.loader = loader\n        self.dploader = dploader\n        self.training = training\n\n    def __getitem__(self, index):\n        left  = self.left[index]\n        right = self.right[index]\n        disp_L= self.disp_L[index]\n\n        left_img = self.loader(left)\n        right_img = self.loader(right)\n        dataL = self.dploader(disp_L)\n\n\n        if self.training:  \n           w, h = left_img.size\n           th, tw = 256, 512\n \n           x1 = random.randint(0, w - tw)\n           y1 = random.randint(0, h - th)\n\n           left_img = left_img.crop((x1, y1, x1 + tw, y1 + th))\n           right_img = right_img.crop((x1, y1, x1 + tw, y1 + th))\n\n           dataL = np.ascontiguousarray(dataL,dtype=np.float32)/256\n           dataL = dataL[y1:y1 + th, x1:x1 + tw]\n\n           processed = preprocess.get_transform(augment=False)  \n           left_img   = processed(left_img)\n           right_img  = processed(right_img)\n\n           return left_img, right_img, dataL\n        else:\n           w, h = left_img.size\n\n           left_img = left_img.crop((w-1232, h-368, w, h))\n           right_img = right_img.crop((w-1232, h-368, w, h))\n           w1, h1 = left_img.size\n\n           dataL = dataL.crop((w-1232, h-368, w, h))\n           dataL = np.ascontiguousarray(dataL,dtype=np.float32)/256\n\n           processed = preprocess.get_transform(augment=False)  \n           left_img       = processed(left_img)\n           right_img      = processed(right_img)\n\n           return left_img, right_img, dataL\n\n    def __len__(self):\n        return len(self.left)\n"""
dataloader/KITTI_submission_loader.py,1,"b""import torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\nimport numpy as np\n\nIMG_EXTENSIONS = [\n    '.jpg', '.JPG', '.jpeg', '.JPEG',\n    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\ndef dataloader(filepath):\n\n  left_fold  = 'image_2/'\n  right_fold = 'image_3/'\n\n\n  image = [img for img in os.listdir(filepath+left_fold) if img.find('_10') > -1]\n\n\n  left_test  = [filepath+left_fold+img for img in image]\n  right_test = [filepath+right_fold+img for img in image]\n\n  return left_test, right_test\n"""
dataloader/KITTI_submission_loader2012.py,1,"b""import torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\nimport numpy as np\n\nIMG_EXTENSIONS = [\n    '.jpg', '.JPG', '.jpeg', '.JPEG',\n    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\ndef dataloader(filepath):\n\n  left_fold  = 'colored_0/'\n  right_fold = 'colored_1/'\n\n\n  image = [img for img in os.listdir(filepath+left_fold) if img.find('_10') > -1]\n\n\n  left_test  = [filepath+left_fold+img for img in image]\n  right_test = [filepath+right_fold+img for img in image]\n\n  return left_test, right_test\n"""
dataloader/KITTIloader2012.py,1,"b""import torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\nimport numpy as np\n\nIMG_EXTENSIONS = [\n    '.jpg', '.JPG', '.jpeg', '.JPEG',\n    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\ndef dataloader(filepath):\n\n  left_fold  = 'colored_0/'\n  right_fold = 'colored_1/'\n  disp_noc   = 'disp_occ/'\n\n  image = [img for img in os.listdir(filepath+left_fold) if img.find('_10') > -1]\n\n  train = image[:]\n  val   = image[160:]\n\n  left_train  = [filepath+left_fold+img for img in train]\n  right_train = [filepath+right_fold+img for img in train]\n  disp_train = [filepath+disp_noc+img for img in train]\n\n\n  left_val  = [filepath+left_fold+img for img in val]\n  right_val = [filepath+right_fold+img for img in val]\n  disp_val = [filepath+disp_noc+img for img in val]\n\n  return left_train, right_train, disp_train, left_val, right_val, disp_val\n"""
dataloader/KITTIloader2015.py,1,"b""import torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\nimport numpy as np\n\nIMG_EXTENSIONS = [\n    '.jpg', '.JPG', '.jpeg', '.JPEG',\n    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\ndef dataloader(filepath):\n\n  left_fold  = 'image_2/'\n  right_fold = 'image_3/'\n  disp_L = 'disp_occ_0/'\n  disp_R = 'disp_occ_1/'\n\n  image = [img for img in os.listdir(filepath+left_fold) if img.find('_10') > -1]\n\n  train = image[:160]\n  val   = image[160:]\n\n  left_train  = [filepath+left_fold+img for img in train]\n  right_train = [filepath+right_fold+img for img in train]\n  disp_train_L = [filepath+disp_L+img for img in train]\n  #disp_train_R = [filepath+disp_R+img for img in train]\n\n  left_val  = [filepath+left_fold+img for img in val]\n  right_val = [filepath+right_fold+img for img in val]\n  disp_val_L = [filepath+disp_L+img for img in val]\n  #disp_val_R = [filepath+disp_R+img for img in val]\n\n  return left_train, right_train, disp_train_L, left_val, right_val, disp_val_L\n"""
dataloader/SecenFlowLoader.py,1,"b""import os\nimport torch\nimport torch.utils.data as data\nimport torch\nimport torchvision.transforms as transforms\nimport random\nfrom PIL import Image, ImageOps\nfrom . import preprocess \nfrom . import listflowfile as lt\nfrom . import readpfm as rp\nimport numpy as np\n\nIMG_EXTENSIONS = [\n    '.jpg', '.JPG', '.jpeg', '.JPEG',\n    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n]\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\ndef default_loader(path):\n    return Image.open(path).convert('RGB')\n\ndef disparity_loader(path):\n    return rp.readPFM(path)\n\nclass myImageFloder(data.Dataset):\n    def __init__(self, left, right, left_disparity, training, loader=default_loader, dploader= disparity_loader):\n \n        self.left = left\n        self.right = right\n        self.disp_L = left_disparity\n        self.loader = loader\n        self.dploader = dploader\n        self.training = training\n\n    def __getitem__(self, index):\n        left  = self.left[index]\n        right = self.right[index]\n        disp_L= self.disp_L[index]\n\n\n        left_img = self.loader(left)\n        right_img = self.loader(right)\n        dataL, scaleL = self.dploader(disp_L)\n        dataL = np.ascontiguousarray(dataL,dtype=np.float32)\n\n        if self.training:  \n            w, h = left_img.size\n            th, tw = 256, 512\n\n            x1 = random.randint(0, w - tw)\n            y1 = random.randint(0, h - th)\n\n            left_img = left_img.crop((x1, y1, x1 + tw, y1 + th))\n            right_img = right_img.crop((x1, y1, x1 + tw, y1 + th))\n\n            dataL = dataL[y1:y1 + th, x1:x1 + tw]\n\n            processed = preprocess.get_transform(augment=False)  \n            left_img   = processed(left_img)\n            right_img  = processed(right_img)\n\n            return left_img, right_img, dataL\n        else:\n            processed = preprocess.get_transform(augment=False)  \n            left_img       = processed(left_img)\n            right_img      = processed(right_img) \n            return left_img, right_img, dataL\n\n    def __len__(self):\n        return len(self.left)\n"""
dataloader/__init__.py,0,b''
dataloader/listflowfile.py,1,"b'import torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\ndef dataloader(filepath):\n\n classes = [d for d in os.listdir(filepath) if os.path.isdir(os.path.join(filepath, d))]\n image = [img for img in classes if img.find(\'frames_cleanpass\') > -1]\n disp  = [dsp for dsp in classes if dsp.find(\'disparity\') > -1]\n\n monkaa_path = filepath + [x for x in image if \'monkaa\' in x][0]\n monkaa_disp = filepath + [x for x in disp if \'monkaa\' in x][0]\n\n \n monkaa_dir  = os.listdir(monkaa_path)\n\n all_left_img=[]\n all_right_img=[]\n all_left_disp = []\n test_left_img=[]\n test_right_img=[]\n test_left_disp = []\n\n\n for dd in monkaa_dir:\n   for im in os.listdir(monkaa_path+\'/\'+dd+\'/left/\'):\n    if is_image_file(monkaa_path+\'/\'+dd+\'/left/\'+im):\n     all_left_img.append(monkaa_path+\'/\'+dd+\'/left/\'+im)\n     all_left_disp.append(monkaa_disp+\'/\'+dd+\'/left/\'+im.split(""."")[0]+\'.pfm\')\n\n   for im in os.listdir(monkaa_path+\'/\'+dd+\'/right/\'):\n    if is_image_file(monkaa_path+\'/\'+dd+\'/right/\'+im):\n     all_right_img.append(monkaa_path+\'/\'+dd+\'/right/\'+im)\n\n flying_path = filepath + [x for x in image if x == \'frames_cleanpass\'][0]\n flying_disp = filepath + [x for x in disp if x == \'frames_disparity\'][0]\n flying_dir = flying_path+\'/TRAIN/\'\n subdir = [\'A\',\'B\',\'C\']\n\n for ss in subdir:\n    flying = os.listdir(flying_dir+ss)\n\n    for ff in flying:\n      imm_l = os.listdir(flying_dir+ss+\'/\'+ff+\'/left/\')\n      for im in imm_l:\n       if is_image_file(flying_dir+ss+\'/\'+ff+\'/left/\'+im):\n         all_left_img.append(flying_dir+ss+\'/\'+ff+\'/left/\'+im)\n\n       all_left_disp.append(flying_disp+\'/TRAIN/\'+ss+\'/\'+ff+\'/left/\'+im.split(""."")[0]+\'.pfm\')\n\n       if is_image_file(flying_dir+ss+\'/\'+ff+\'/right/\'+im):\n         all_right_img.append(flying_dir+ss+\'/\'+ff+\'/right/\'+im)\n\n flying_dir = flying_path+\'/TEST/\'\n\n subdir = [\'A\',\'B\',\'C\']\n\n for ss in subdir:\n    flying = os.listdir(flying_dir+ss)\n\n    for ff in flying:\n      imm_l = os.listdir(flying_dir+ss+\'/\'+ff+\'/left/\')\n      for im in imm_l:\n       if is_image_file(flying_dir+ss+\'/\'+ff+\'/left/\'+im):\n         test_left_img.append(flying_dir+ss+\'/\'+ff+\'/left/\'+im)\n\n       test_left_disp.append(flying_disp+\'/TEST/\'+ss+\'/\'+ff+\'/left/\'+im.split(""."")[0]+\'.pfm\')\n\n       if is_image_file(flying_dir+ss+\'/\'+ff+\'/right/\'+im):\n         test_right_img.append(flying_dir+ss+\'/\'+ff+\'/right/\'+im)\n\n\n\n driving_dir = filepath + [x for x in image if \'driving\' in x][0] + \'/\'\n driving_disp = filepath + [x for x in disp if \'driving\' in x][0]\n\n subdir1 = [\'35mm_focallength\',\'15mm_focallength\']\n subdir2 = [\'scene_backwards\',\'scene_forwards\']\n subdir3 = [\'fast\',\'slow\']\n\n for i in subdir1:\n   for j in subdir2:\n    for k in subdir3:\n        imm_l = os.listdir(driving_dir+i+\'/\'+j+\'/\'+k+\'/left/\')    \n        for im in imm_l:\n          if is_image_file(driving_dir+i+\'/\'+j+\'/\'+k+\'/left/\'+im):\n            all_left_img.append(driving_dir+i+\'/\'+j+\'/\'+k+\'/left/\'+im)\n          all_left_disp.append(driving_disp+\'/\'+i+\'/\'+j+\'/\'+k+\'/left/\'+im.split(""."")[0]+\'.pfm\')\n\n          if is_image_file(driving_dir+i+\'/\'+j+\'/\'+k+\'/right/\'+im):\n            all_right_img.append(driving_dir+i+\'/\'+j+\'/\'+k+\'/right/\'+im)\n\n\n return all_left_img, all_right_img, all_left_disp, test_left_img, test_right_img, test_left_disp\n\n\n'"
dataloader/preprocess.py,3,"b'import torch\nimport torchvision.transforms as transforms\nimport random\n\n__imagenet_stats = {\'mean\': [0.485, 0.456, 0.406],\n                   \'std\': [0.229, 0.224, 0.225]}\n\n#__imagenet_stats = {\'mean\': [0.5, 0.5, 0.5],\n#                   \'std\': [0.5, 0.5, 0.5]}\n\n__imagenet_pca = {\n    \'eigval\': torch.Tensor([0.2175, 0.0188, 0.0045]),\n    \'eigvec\': torch.Tensor([\n        [-0.5675,  0.7192,  0.4009],\n        [-0.5808, -0.0045, -0.8140],\n        [-0.5836, -0.6948,  0.4203],\n    ])\n}\n\n\ndef scale_crop(input_size, scale_size=None, normalize=__imagenet_stats):\n    t_list = [\n        transforms.ToTensor(),\n        transforms.Normalize(**normalize),\n    ]\n    #if scale_size != input_size:\n    #t_list = [transforms.Scale((960,540))] + t_list\n\n    return transforms.Compose(t_list)\n\n\ndef scale_random_crop(input_size, scale_size=None, normalize=__imagenet_stats):\n    t_list = [\n        transforms.RandomCrop(input_size),\n        transforms.ToTensor(),\n        transforms.Normalize(**normalize),\n    ]\n    if scale_size != input_size:\n        t_list = [transforms.Scale(scale_size)] + t_list\n\n    transforms.Compose(t_list)\n\n\ndef pad_random_crop(input_size, scale_size=None, normalize=__imagenet_stats):\n    padding = int((scale_size - input_size) / 2)\n    return transforms.Compose([\n        transforms.RandomCrop(input_size, padding=padding),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(**normalize),\n    ])\n\n\ndef inception_preproccess(input_size, normalize=__imagenet_stats):\n    return transforms.Compose([\n        transforms.RandomSizedCrop(input_size),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(**normalize)\n    ])\ndef inception_color_preproccess(input_size, normalize=__imagenet_stats):\n    return transforms.Compose([\n        #transforms.RandomSizedCrop(input_size),\n        #transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ColorJitter(\n            brightness=0.4,\n            contrast=0.4,\n            saturation=0.4,\n        ),\n        Lighting(0.1, __imagenet_pca[\'eigval\'], __imagenet_pca[\'eigvec\']),\n        transforms.Normalize(**normalize)\n    ])\n\n\ndef get_transform(name=\'imagenet\', input_size=None,\n                  scale_size=None, normalize=None, augment=True):\n    normalize = __imagenet_stats\n    input_size = 256\n    if augment:\n            return inception_color_preproccess(input_size, normalize=normalize)\n    else:\n            return scale_crop(input_size=input_size,\n                              scale_size=scale_size, normalize=normalize)\n\n\n\n\nclass Lighting(object):\n    """"""Lighting noise(AlexNet - style PCA - based noise)""""""\n\n    def __init__(self, alphastd, eigval, eigvec):\n        self.alphastd = alphastd\n        self.eigval = eigval\n        self.eigvec = eigvec\n\n    def __call__(self, img):\n        if self.alphastd == 0:\n            return img\n\n        alpha = img.new().resize_(3).normal_(0, self.alphastd)\n        rgb = self.eigvec.type_as(img).clone()\\\n            .mul(alpha.view(1, 3).expand(3, 3))\\\n            .mul(self.eigval.view(1, 3).expand(3, 3))\\\n            .sum(1).squeeze()\n\n        return img.add(rgb.view(3, 1, 1).expand_as(img))\n\n\nclass Grayscale(object):\n\n    def __call__(self, img):\n        gs = img.clone()\n        gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n        gs[1].copy_(gs[0])\n        gs[2].copy_(gs[0])\n        return gs\n\n\nclass Saturation(object):\n\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        gs = Grayscale()(img)\n        alpha = random.uniform(0, self.var)\n        return img.lerp(gs, alpha)\n\n\nclass Brightness(object):\n\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        gs = img.new().resize_as_(img).zero_()\n        alpha = random.uniform(0, self.var)\n        return img.lerp(gs, alpha)\n\n\nclass Contrast(object):\n\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        gs = Grayscale()(img)\n        gs.fill_(gs.mean())\n        alpha = random.uniform(0, self.var)\n        return img.lerp(gs, alpha)\n\n\nclass RandomOrder(object):\n    """""" Composes several transforms together in random order.\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        if self.transforms is None:\n            return img\n        order = torch.randperm(len(self.transforms))\n        for i in order:\n            img = self.transforms[i](img)\n        return img\n\n\nclass ColorJitter(RandomOrder):\n\n    def __init__(self, brightness=0.4, contrast=0.4, saturation=0.4):\n        self.transforms = []\n        if brightness != 0:\n            self.transforms.append(Brightness(brightness))\n        if contrast != 0:\n            self.transforms.append(Contrast(contrast))\n        if saturation != 0:\n            self.transforms.append(Saturation(saturation))\n'"
dataloader/readpfm.py,0,"b""import re\nimport numpy as np\nimport sys\nimport chardet \n\ndef readPFM(file):\n    file = open(file, 'rb')\n\n    color = None\n    width = None\n    height = None\n    scale = None\n    endian = None\n\n    header = file.readline().rstrip()\n    encode_type = chardet.detect(header)  \n    header = header.decode(encode_type['encoding'])\n    if header == 'PF':\n        color = True\n    elif header == 'Pf':\n        color = False\n    else:\n        raise Exception('Not a PFM file.')\n\n    dim_match = re.match(r'^(\\d+)\\s(\\d+)\\s$', file.readline().decode(encode_type['encoding']))\n    if dim_match:\n        width, height = map(int, dim_match.groups())\n    else:\n        raise Exception('Malformed PFM header.')\n\n    scale = float(file.readline().rstrip().decode(encode_type['encoding']))\n    if scale < 0: # little-endian\n        endian = '<'\n        scale = -scale\n    else:\n        endian = '>' # big-endian\n\n    data = np.fromfile(file, endian + 'f')\n    shape = (height, width, 3) if color else (height, width)\n\n    data = np.reshape(data, shape)\n    data = np.flipud(data)\n    return data, scale\n\n\n"""
models/__init__.py,0,b'from .basic import PSMNet as basic\nfrom .stackhourglass import PSMNet as stackhourglass\n\n'
models/basic.py,6,"b""from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport math\nfrom submodule import *\n\nclass PSMNet(nn.Module):\n    def __init__(self, maxdisp):\n        super(PSMNet, self).__init__()\n        self.maxdisp = maxdisp\n        self.feature_extraction = feature_extraction()\n\n########\n        self.dres0 = nn.Sequential(convbn_3d(64, 32, 3, 1, 1),\n                                     nn.ReLU(inplace=True),\n                                     convbn_3d(32, 32, 3, 1, 1),\n                                     nn.ReLU(inplace=True))\n\n        self.dres1 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),\n                                   nn.ReLU(inplace=True),\n                                   convbn_3d(32, 32, 3, 1, 1)) \n\n        self.dres2 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),\n                                   nn.ReLU(inplace=True),\n                                   convbn_3d(32, 32, 3, 1, 1))\n \n        self.dres3 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),\n                                   nn.ReLU(inplace=True),\n                                   convbn_3d(32, 32, 3, 1, 1)) \n\n        self.dres4 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),\n                                   nn.ReLU(inplace=True),\n                                   convbn_3d(32, 32, 3, 1, 1)) \n \n        self.classify = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),\n                                      nn.ReLU(inplace=True),\n                                      nn.Conv3d(32, 1, kernel_size=3, padding=1, stride=1,bias=False))\n\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.Conv3d):\n                n = m.kernel_size[0] * m.kernel_size[1]*m.kernel_size[2] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n\n    def forward(self, left, right):\n\n        refimg_fea     = self.feature_extraction(left)\n        targetimg_fea  = self.feature_extraction(right)\n \n        #matching\n        cost = Variable(torch.FloatTensor(refimg_fea.size()[0], refimg_fea.size()[1]*2, self.maxdisp/4,  refimg_fea.size()[2],  refimg_fea.size()[3]).zero_(), volatile= not self.training).cuda()\n\n        for i in range(self.maxdisp/4):\n            if i > 0 :\n             cost[:, :refimg_fea.size()[1], i, :,i:]   = refimg_fea[:,:,:,i:]\n             cost[:, refimg_fea.size()[1]:, i, :,i:] = targetimg_fea[:,:,:,:-i]\n            else:\n             cost[:, :refimg_fea.size()[1], i, :,:]   = refimg_fea\n             cost[:, refimg_fea.size()[1]:, i, :,:]   = targetimg_fea\n        cost = cost.contiguous()\n\n        cost0 = self.dres0(cost)\n        cost0 = self.dres1(cost0) + cost0\n        cost0 = self.dres2(cost0) + cost0 \n        cost0 = self.dres3(cost0) + cost0 \n        cost0 = self.dres4(cost0) + cost0\n\n        cost = self.classify(cost0)\n        cost = F.upsample(cost, [self.maxdisp,left.size()[2],left.size()[3]], mode='trilinear')\n        cost = torch.squeeze(cost,1)\n        pred = F.softmax(cost)\n        pred = disparityregression(self.maxdisp)(pred)\n\n        return pred\n"""
models/stackhourglass.py,8,"b'from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport math\nfrom submodule import *\n\nclass hourglass(nn.Module):\n    def __init__(self, inplanes):\n        super(hourglass, self).__init__()\n\n        self.conv1 = nn.Sequential(convbn_3d(inplanes, inplanes*2, kernel_size=3, stride=2, pad=1),\n                                   nn.ReLU(inplace=True))\n\n        self.conv2 = convbn_3d(inplanes*2, inplanes*2, kernel_size=3, stride=1, pad=1)\n\n        self.conv3 = nn.Sequential(convbn_3d(inplanes*2, inplanes*2, kernel_size=3, stride=2, pad=1),\n                                   nn.ReLU(inplace=True))\n\n        self.conv4 = nn.Sequential(convbn_3d(inplanes*2, inplanes*2, kernel_size=3, stride=1, pad=1),\n                                   nn.ReLU(inplace=True))\n\n        self.conv5 = nn.Sequential(nn.ConvTranspose3d(inplanes*2, inplanes*2, kernel_size=3, padding=1, output_padding=1, stride=2,bias=False),\n                                   nn.BatchNorm3d(inplanes*2)) #+conv2\n\n        self.conv6 = nn.Sequential(nn.ConvTranspose3d(inplanes*2, inplanes, kernel_size=3, padding=1, output_padding=1, stride=2,bias=False),\n                                   nn.BatchNorm3d(inplanes)) #+x\n\n    def forward(self, x ,presqu, postsqu):\n        \n        out  = self.conv1(x) #in:1/4 out:1/8\n        pre  = self.conv2(out) #in:1/8 out:1/8\n        if postsqu is not None:\n           pre = F.relu(pre + postsqu, inplace=True)\n        else:\n           pre = F.relu(pre, inplace=True)\n\n        out  = self.conv3(pre) #in:1/8 out:1/16\n        out  = self.conv4(out) #in:1/16 out:1/16\n\n        if presqu is not None:\n           post = F.relu(self.conv5(out)+presqu, inplace=True) #in:1/16 out:1/8\n        else:\n           post = F.relu(self.conv5(out)+pre, inplace=True) \n\n        out  = self.conv6(post)  #in:1/8 out:1/4\n\n        return out, pre, post\n\nclass PSMNet(nn.Module):\n    def __init__(self, maxdisp):\n        super(PSMNet, self).__init__()\n        self.maxdisp = maxdisp\n\n        self.feature_extraction = feature_extraction()\n\n        self.dres0 = nn.Sequential(convbn_3d(64, 32, 3, 1, 1),\n                                     nn.ReLU(inplace=True),\n                                     convbn_3d(32, 32, 3, 1, 1),\n                                     nn.ReLU(inplace=True))\n\n        self.dres1 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),\n                                   nn.ReLU(inplace=True),\n                                   convbn_3d(32, 32, 3, 1, 1)) \n\n        self.dres2 = hourglass(32)\n\n        self.dres3 = hourglass(32)\n\n        self.dres4 = hourglass(32)\n\n        self.classif1 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),\n                                      nn.ReLU(inplace=True),\n                                      nn.Conv3d(32, 1, kernel_size=3, padding=1, stride=1,bias=False))\n\n        self.classif2 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),\n                                      nn.ReLU(inplace=True),\n                                      nn.Conv3d(32, 1, kernel_size=3, padding=1, stride=1,bias=False))\n\n        self.classif3 = nn.Sequential(convbn_3d(32, 32, 3, 1, 1),\n                                      nn.ReLU(inplace=True),\n                                      nn.Conv3d(32, 1, kernel_size=3, padding=1, stride=1,bias=False))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.Conv3d):\n                n = m.kernel_size[0] * m.kernel_size[1]*m.kernel_size[2] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n\n    def forward(self, left, right):\n\n        refimg_fea     = self.feature_extraction(left)\n        targetimg_fea  = self.feature_extraction(right)\n\n\n        #matching\n        cost = Variable(torch.FloatTensor(refimg_fea.size()[0], refimg_fea.size()[1]*2, self.maxdisp//4,  refimg_fea.size()[2],  refimg_fea.size()[3]).zero_()).cuda()\n\n        for i in range(self.maxdisp//4):\n            if i > 0 :\n             cost[:, :refimg_fea.size()[1], i, :,i:]   = refimg_fea[:,:,:,i:]\n             cost[:, refimg_fea.size()[1]:, i, :,i:] = targetimg_fea[:,:,:,:-i]\n            else:\n             cost[:, :refimg_fea.size()[1], i, :,:]   = refimg_fea\n             cost[:, refimg_fea.size()[1]:, i, :,:]   = targetimg_fea\n        cost = cost.contiguous()\n\n        cost0 = self.dres0(cost)\n        cost0 = self.dres1(cost0) + cost0\n\n        out1, pre1, post1 = self.dres2(cost0, None, None) \n        out1 = out1+cost0\n\n        out2, pre2, post2 = self.dres3(out1, pre1, post1) \n        out2 = out2+cost0\n\n        out3, pre3, post3 = self.dres4(out2, pre1, post2) \n        out3 = out3+cost0\n\n        cost1 = self.classif1(out1)\n        cost2 = self.classif2(out2) + cost1\n        cost3 = self.classif3(out3) + cost2\n\n        if self.training:\n\t\tcost1 = F.upsample(cost1, [self.maxdisp,left.size()[2],left.size()[3]], mode=\'trilinear\')\n\t\tcost2 = F.upsample(cost2, [self.maxdisp,left.size()[2],left.size()[3]], mode=\'trilinear\')\n\n\t\tcost1 = torch.squeeze(cost1,1)\n\t\tpred1 = F.softmax(cost1,dim=1)\n\t\tpred1 = disparityregression(self.maxdisp)(pred1)\n\n\t\tcost2 = torch.squeeze(cost2,1)\n\t\tpred2 = F.softmax(cost2,dim=1)\n\t\tpred2 = disparityregression(self.maxdisp)(pred2)\n\n        cost3 = F.upsample(cost3, [self.maxdisp,left.size()[2],left.size()[3]], mode=\'trilinear\')\n        cost3 = torch.squeeze(cost3,1)\n        pred3 = F.softmax(cost3,dim=1)\n\t#For your information: This formulation \'softmax(c)\' learned ""similarity"" \n\t#while \'softmax(-c)\' learned \'matching cost\' as mentioned in the paper.\n\t#However, \'c\' or \'-c\' do not affect the performance because feature-based cost volume provided flexibility.\n        pred3 = disparityregression(self.maxdisp)(pred3)\n\n        if self.training:\n            return pred1, pred2, pred3\n        else:\n            return pred3\n'"
models/submodule.py,10,"b""from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport math\nimport numpy as np\n\ndef convbn(in_planes, out_planes, kernel_size, stride, pad, dilation):\n\n    return nn.Sequential(nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=dilation if dilation > 1 else pad, dilation = dilation, bias=False),\n                         nn.BatchNorm2d(out_planes))\n\n\ndef convbn_3d(in_planes, out_planes, kernel_size, stride, pad):\n\n    return nn.Sequential(nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, padding=pad, stride=stride,bias=False),\n                         nn.BatchNorm3d(out_planes))\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(BasicBlock, self).__init__()\n\n        self.conv1 = nn.Sequential(convbn(inplanes, planes, 3, stride, pad, dilation),\n                                   nn.ReLU(inplace=True))\n\n        self.conv2 = convbn(planes, planes, 3, 1, pad, dilation)\n\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.conv2(out)\n\n        if self.downsample is not None:\n            x = self.downsample(x)\n\n        out += x\n\n        return out\n\nclass matchshifted(nn.Module):\n    def __init__(self):\n        super(matchshifted, self).__init__()\n\n    def forward(self, left, right, shift):\n        batch, filters, height, width = left.size()\n        shifted_left  = F.pad(torch.index_select(left,  3, Variable(torch.LongTensor([i for i in range(shift,width)])).cuda()),(shift,0,0,0))\n        shifted_right = F.pad(torch.index_select(right, 3, Variable(torch.LongTensor([i for i in range(width-shift)])).cuda()),(shift,0,0,0))\n        out = torch.cat((shifted_left,shifted_right),1).view(batch,filters*2,1,height,width)\n        return out\n\nclass disparityregression(nn.Module):\n    def __init__(self, maxdisp):\n        super(disparityregression, self).__init__()\n        self.disp = Variable(torch.Tensor(np.reshape(np.array(range(maxdisp)),[1,maxdisp,1,1])).cuda(), requires_grad=False)\n\n    def forward(self, x):\n        disp = self.disp.repeat(x.size()[0],1,x.size()[2],x.size()[3])\n        out = torch.sum(x*disp,1)\n        return out\n\nclass feature_extraction(nn.Module):\n    def __init__(self):\n        super(feature_extraction, self).__init__()\n        self.inplanes = 32\n        self.firstconv = nn.Sequential(convbn(3, 32, 3, 2, 1, 1),\n                                       nn.ReLU(inplace=True),\n                                       convbn(32, 32, 3, 1, 1, 1),\n                                       nn.ReLU(inplace=True),\n                                       convbn(32, 32, 3, 1, 1, 1),\n                                       nn.ReLU(inplace=True))\n\n        self.layer1 = self._make_layer(BasicBlock, 32, 3, 1,1,1)\n        self.layer2 = self._make_layer(BasicBlock, 64, 16, 2,1,1) \n        self.layer3 = self._make_layer(BasicBlock, 128, 3, 1,1,1)\n        self.layer4 = self._make_layer(BasicBlock, 128, 3, 1,1,2)\n\n        self.branch1 = nn.Sequential(nn.AvgPool2d((64, 64), stride=(64,64)),\n                                     convbn(128, 32, 1, 1, 0, 1),\n                                     nn.ReLU(inplace=True))\n\n        self.branch2 = nn.Sequential(nn.AvgPool2d((32, 32), stride=(32,32)),\n                                     convbn(128, 32, 1, 1, 0, 1),\n                                     nn.ReLU(inplace=True))\n\n        self.branch3 = nn.Sequential(nn.AvgPool2d((16, 16), stride=(16,16)),\n                                     convbn(128, 32, 1, 1, 0, 1),\n                                     nn.ReLU(inplace=True))\n\n        self.branch4 = nn.Sequential(nn.AvgPool2d((8, 8), stride=(8,8)),\n                                     convbn(128, 32, 1, 1, 0, 1),\n                                     nn.ReLU(inplace=True))\n\n        self.lastconv = nn.Sequential(convbn(320, 128, 3, 1, 1, 1),\n                                      nn.ReLU(inplace=True),\n                                      nn.Conv2d(128, 32, kernel_size=1, padding=0, stride = 1, bias=False))\n\n    def _make_layer(self, block, planes, blocks, stride, pad, dilation):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n           downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),)\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, pad, dilation))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes,1,None,pad,dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        output      = self.firstconv(x)\n        output      = self.layer1(output)\n        output_raw  = self.layer2(output)\n        output      = self.layer3(output_raw)\n        output_skip = self.layer4(output)\n\n\n        output_branch1 = self.branch1(output_skip)\n        output_branch1 = F.upsample(output_branch1, (output_skip.size()[2],output_skip.size()[3]),mode='bilinear')\n\n        output_branch2 = self.branch2(output_skip)\n        output_branch2 = F.upsample(output_branch2, (output_skip.size()[2],output_skip.size()[3]),mode='bilinear')\n\n        output_branch3 = self.branch3(output_skip)\n        output_branch3 = F.upsample(output_branch3, (output_skip.size()[2],output_skip.size()[3]),mode='bilinear')\n\n        output_branch4 = self.branch4(output_skip)\n        output_branch4 = F.upsample(output_branch4, (output_skip.size()[2],output_skip.size()[3]),mode='bilinear')\n\n        output_feature = torch.cat((output_raw, output_skip, output_branch4, output_branch3, output_branch2, output_branch1), 1)\n        output_feature = self.lastconv(output_feature)\n\n        return output_feature\n\n\n\n"""
utils/__init__.py,0,b''
utils/preprocess.py,3,"b'import torch\nimport torchvision.transforms as transforms\nimport random\n\n__imagenet_stats = {\'mean\': [0.485, 0.456, 0.406],\n                   \'std\': [0.229, 0.224, 0.225]}\n\n#__imagenet_stats = {\'mean\': [0.5, 0.5, 0.5],\n#                   \'std\': [0.5, 0.5, 0.5]}\n\n__imagenet_pca = {\n    \'eigval\': torch.Tensor([0.2175, 0.0188, 0.0045]),\n    \'eigvec\': torch.Tensor([\n        [-0.5675,  0.7192,  0.4009],\n        [-0.5808, -0.0045, -0.8140],\n        [-0.5836, -0.6948,  0.4203],\n    ])\n}\n\n\ndef scale_crop(input_size, scale_size=None, normalize=__imagenet_stats):\n    t_list = [\n        transforms.ToTensor(),\n        transforms.Normalize(**normalize),\n    ]\n    #if scale_size != input_size:\n    #t_list = [transforms.Scale((960,540))] + t_list\n\n    return transforms.Compose(t_list)\n\n\ndef scale_random_crop(input_size, scale_size=None, normalize=__imagenet_stats):\n    t_list = [\n        transforms.RandomCrop(input_size),\n        transforms.ToTensor(),\n        transforms.Normalize(**normalize),\n    ]\n    if scale_size != input_size:\n        t_list = [transforms.Scale(scale_size)] + t_list\n\n    transforms.Compose(t_list)\n\n\ndef pad_random_crop(input_size, scale_size=None, normalize=__imagenet_stats):\n    padding = int((scale_size - input_size) / 2)\n    return transforms.Compose([\n        transforms.RandomCrop(input_size, padding=padding),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(**normalize),\n    ])\n\n\ndef inception_preproccess(input_size, normalize=__imagenet_stats):\n    return transforms.Compose([\n        transforms.RandomSizedCrop(input_size),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(**normalize)\n    ])\ndef inception_color_preproccess(input_size, normalize=__imagenet_stats):\n    return transforms.Compose([\n        #transforms.RandomSizedCrop(input_size),\n        #transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        ColorJitter(\n            brightness=0.4,\n            contrast=0.4,\n            saturation=0.4,\n        ),\n        Lighting(0.1, __imagenet_pca[\'eigval\'], __imagenet_pca[\'eigvec\']),\n        transforms.Normalize(**normalize)\n    ])\n\n\ndef get_transform(name=\'imagenet\', input_size=None,\n                  scale_size=None, normalize=None, augment=True):\n    normalize = __imagenet_stats\n    input_size = 256\n    if augment:\n            return inception_color_preproccess(input_size, normalize=normalize)\n    else:\n            return scale_crop(input_size=input_size,\n                              scale_size=scale_size, normalize=normalize)\n\n\n\n\nclass Lighting(object):\n    """"""Lighting noise(AlexNet - style PCA - based noise)""""""\n\n    def __init__(self, alphastd, eigval, eigvec):\n        self.alphastd = alphastd\n        self.eigval = eigval\n        self.eigvec = eigvec\n\n    def __call__(self, img):\n        if self.alphastd == 0:\n            return img\n\n        alpha = img.new().resize_(3).normal_(0, self.alphastd)\n        rgb = self.eigvec.type_as(img).clone()\\\n            .mul(alpha.view(1, 3).expand(3, 3))\\\n            .mul(self.eigval.view(1, 3).expand(3, 3))\\\n            .sum(1).squeeze()\n\n        return img.add(rgb.view(3, 1, 1).expand_as(img))\n\n\nclass Grayscale(object):\n\n    def __call__(self, img):\n        gs = img.clone()\n        gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n        gs[1].copy_(gs[0])\n        gs[2].copy_(gs[0])\n        return gs\n\n\nclass Saturation(object):\n\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        gs = Grayscale()(img)\n        alpha = random.uniform(0, self.var)\n        return img.lerp(gs, alpha)\n\n\nclass Brightness(object):\n\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        gs = img.new().resize_as_(img).zero_()\n        alpha = random.uniform(0, self.var)\n        return img.lerp(gs, alpha)\n\n\nclass Contrast(object):\n\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        gs = Grayscale()(img)\n        gs.fill_(gs.mean())\n        alpha = random.uniform(0, self.var)\n        return img.lerp(gs, alpha)\n\n\nclass RandomOrder(object):\n    """""" Composes several transforms together in random order.\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        if self.transforms is None:\n            return img\n        order = torch.randperm(len(self.transforms))\n        for i in order:\n            img = self.transforms[i](img)\n        return img\n\n\nclass ColorJitter(RandomOrder):\n\n    def __init__(self, brightness=0.4, contrast=0.4, saturation=0.4):\n        self.transforms = []\n        if brightness != 0:\n            self.transforms.append(Brightness(brightness))\n        if contrast != 0:\n            self.transforms.append(Contrast(contrast))\n        if saturation != 0:\n            self.transforms.append(Saturation(saturation))\n'"
utils/readpfm.py,0,"b""import re\nimport numpy as np\nimport sys\n \n\ndef readPFM(file):\n    file = open(file, 'rb')\n\n    color = None\n    width = None\n    height = None\n    scale = None\n    endian = None\n\n    header = file.readline().rstrip()\n    if header == 'PF':\n        color = True\n    elif header == 'Pf':\n        color = False\n    else:\n        raise Exception('Not a PFM file.')\n\n    dim_match = re.match(r'^(\\d+)\\s(\\d+)\\s$', file.readline())\n    if dim_match:\n        width, height = map(int, dim_match.groups())\n    else:\n        raise Exception('Malformed PFM header.')\n\n    scale = float(file.readline().rstrip())\n    if scale < 0: # little-endian\n        endian = '<'\n        scale = -scale\n    else:\n        endian = '>' # big-endian\n\n    data = np.fromfile(file, endian + 'f')\n    shape = (height, width, 3) if color else (height, width)\n\n    data = np.reshape(data, shape)\n    data = np.flipud(data)\n    return data, scale\n\n"""
