file_path,api_count,code
helpers/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# EOF\n'
helpers/io_methods.py,0,"b'# -*- coding: utf-8 -*-\r\n__author__ = \'S.I. Mimilakis\'\r\n__copyright__ = \'MacSeNet\'\r\n\r\nimport os, subprocess\r\nimport numpy as np\r\nimport wave as _wave\r\nfrom scipy.io.wavfile import write, read\r\nfrom sys import platform\r\n\r\nclass AudioIO:\r\n\t"""""" Class for handling audio input/output operations.\r\n\t    It supports reading and writing of various audio formats\r\n\t    via \'audioRead\' & \'audioWrite\' methods. Moreover playback\r\n\t    can be performed by using \'sound\' method. For formats\r\n\t    different than \'.wav\' a coder is needed. In this case\r\n\t    libffmpeg is being used, where the absolute path of\r\n\t    the static build should be given to the class variable.\r\n\t    Finally, energy normalisation and anti-clipping methods\r\n\t    are also covered in the last two methods.\r\n\r\n\t\tBasic Usage examples:\r\n\t\tImport the class :\r\n\t\timport IOMethods as IO\r\n\t\t-For loading wav files:\r\n\t\t\tx, fs = IO.AudioIO.wavRead(\'myWavFile.wav\', mono = True)\r\n\t\t-In case that compressed files are about to be read specify\r\n\t\t\tthe path to the libffmpeg library by changing the \'pathToffmpeg\'\r\n\t\t\tvariable and then type:\r\n\t\t\tx, fs = IO.AudioIO.audioRead()\r\n\t\t-For writing wav files:\r\n\t\t\tIO.AudioIO.audioWrite(x, fs, 16, \'myNewWavFile.wav\', \'wav\')\r\n\r\n\t\t-For listening wav files:\r\n\t\t\tIO.AudioIO.sound(x,fs)\r\n\r\n\t""""""\r\n\t# Normalisation parameters for wavreading and writing\r\n\tnormFact = {\'int8\' : (2**7) -1,\r\n\t\t\t\t\'int16\': (2**15)-1,\r\n\t\t\t\t\'int24\': (2**23)-1,\r\n\t\t\t\t\'int32\': (2**31)-1,\r\n\t\t\t\t\'int64\': (2**63)-1,\r\n\t\t\t\t\'float32\': 1.0,\r\n\t\t\t\t\'float64\': 1.0}\r\n\r\n\t# \'Silence\' the bash output\r\n\tFNULL = open(os.devnull, \'w\')\r\n\r\n\t# Absolute path needed here\r\n\tpathToffmpeg = \'/home/mis/Documents/Python/Projects/SourceSeparation/MiscFiles\'\r\n\r\n\r\n\tdef __init__(self):\r\n\t\tpass\r\n\r\n\t@staticmethod\r\n\tdef audioRead(fileName, mono=False):\r\n\t\t"""""" Function to load audio files such as *.mp3, *.au, *.wma, *.m4a, *.x-wav & *.aiff.\r\n\t\t\tIt first converts them to .wav and reads them with the methods below.\r\n\t\t\tCurrently, it uses a static build of ffmpeg.\r\n\r\n        Args:\r\n            fileName:       (str)       Absolute filename of WAV file\r\n            mono:           (bool)      Switch if samples should be converted to mono\r\n        Returns:\r\n            samples:        (np array)  Audio samples (between [-1,1]\r\n                                        (if stereo: numSamples x numChannels,\r\n                                        if mono: numSamples)\r\n            sampleRate:     (float):    Sampling frequency [Hz]\r\n        """"""\r\n\r\n\t\t# Get the absolute path\r\n\t\tfileName = os.path.abspath(fileName)\r\n\r\n\t\t# Linux\r\n\t\tif (platform == ""linux"") or (platform == ""linux2""):\r\n\t\t\tconvDict = {\r\n\t\t\t\t\'m4a\':[os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_linux\')\r\n\t\t\t\t\t+ \' -i \' + fileName + \' \', -3],\r\n\t\t\t\t\'mp3\':[os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_linux\')\r\n\t\t\t\t\t+ \' -i \' + fileName + \' \', -3],\r\n\t\t\t\t\'au\': [os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_linux\')\r\n\t\t\t\t\t + \' -i \' + fileName + \' \', -2],\r\n\t\t\t\t\'wma\':[os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_linux\')\r\n\t\t\t\t\t + \' -i \' + fileName + \' \', -3],\r\n\t\t\t\t\'aiff\':[os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_linux\')\r\n\t\t\t\t\t + \' -i \' + fileName + \' \', -4],\r\n\t\t\t\t\'wav\':[os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_linux\')\r\n\t\t\t\t\t + \' -i \' + fileName + \' \', -3]\r\n\t\t\t\t\t\t}\r\n\r\n\t\t# MacOSX\r\n\t\telif (platform == ""darwin""):\r\n\t\t\tconvDict = {\r\n\t\t\t\t\'m4a\':[os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_osx\')\r\n\t\t\t\t\t+ \' -i \' + fileName + \' \', -3],\r\n\t\t\t\t\'mp3\':[os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_osx\')\r\n\t\t\t\t\t+ \' -i \' + fileName + \' \', -3],\r\n\t\t\t\t\'au\': [os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_osx\')\r\n\t\t\t\t\t + \' -i \' + fileName + \' \', -2],\r\n\t\t\t\t\'wma\':[os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_osx\')\r\n\t\t\t\t\t + \' -i \' + fileName + \' \', -3],\r\n\t\t\t\t\'aiff\': [os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_osx\')\r\n\t\t\t\t\t + \' -i \' + fileName + \' \', -4],\r\n\t\t\t\t\'wav\': [os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_osx\')\r\n\t\t\t\t\t\t+ \' -i \' + fileName + \' \', -3]\r\n\t\t\t\t\t\t}\r\n\t\t# Add windows support!\r\n\t\telse :\r\n\t\t\traise Exception(\'This OS is not supported.\')\r\n\r\n\t\t# Construct\r\n\r\n\t\tif fileName[convDict[\'mp3\'][1]:] == \'mp3\':\r\n\t\t\tprint(fileName[convDict[\'mp3\'][1]:])\r\n\t\t\tmodfileName = os.path.join(os.path.abspath(fileName[:convDict[\'mp3\'][1]] + \'wav\'))\r\n\t\t\tsubprocess.call(convDict[\'mp3\'][0]+modfileName, shell = True,  stdout=AudioIO.FNULL, stderr=subprocess.STDOUT)\r\n\t\t\tsamples, sampleRate = AudioIO.wavRead(modfileName, mono)\r\n\t\t\tos.remove(modfileName)\r\n\r\n\t\telif fileName[convDict[\'au\'][1]:] == \'au\':\r\n\t\t\tprint(fileName[convDict[\'au\'][1]:])\r\n\t\t\tmodfileName = os.path.join(os.path.abspath(fileName[:convDict[\'au\'][1]] + \'wav\'))\r\n\t\t\tsubprocess.call(convDict[\'au\'][0]+modfileName, shell = True,  stdout=AudioIO.FNULL, stderr=subprocess.STDOUT)\r\n\t\t\tsamples, sampleRate = AudioIO.wavRead(modfileName, mono)\r\n\t\t\tos.remove(modfileName)\r\n\r\n\t\telif fileName[convDict[\'wma\'][1]:] == \'wma\':\r\n\t\t\tprint(fileName[convDict[\'wma\'][1]:])\r\n\t\t\tmodfileName = os.path.join(os.path.abspath(fileName[:convDict[\'wma\'][1]] + \'wav\'))\r\n\t\t\tsubprocess.call(convDict[\'wma\'][0]+modfileName, shell = True,  stdout=AudioIO.FNULL, stderr=subprocess.STDOUT)\r\n\t\t\tsamples, sampleRate = AudioIO.wavRead(modfileName, mono)\r\n\t\t\tos.remove(modfileName)\r\n\r\n\t\telif fileName[convDict[\'aiff\'][1]:] == \'aiff\':\r\n\t\t\tprint(fileName[convDict[\'aiff\'][1]:])\r\n\t\t\tmodfileName = os.path.join(os.path.abspath(fileName[:convDict[\'aiff\'][1]] + \'wav\'))\r\n\t\t\tsubprocess.call(convDict[\'aiff\'][0]+modfileName, shell = True,  stdout=AudioIO.FNULL, stderr=subprocess.STDOUT)\r\n\t\t\tsamples, sampleRate = AudioIO.wavRead(modfileName, mono)\r\n\t\t\tos.remove(modfileName)\r\n\r\n\t\telif fileName[convDict[\'wav\'][1]:] == \'wav\':\r\n\t\t\t""""""\r\n\t\t\t\tGeneral purpose reading of wav files that do not contain the RIFF header.\r\n\t\t\t""""""\r\n\t\t\tprint(\'x-wav\')\r\n\t\t\tmodfileName = os.path.join(os.path.abspath(fileName[:-4] + \'_temp.wav\'))\r\n\t\t\tsubprocess.call(convDict[\'wav\'][0] + modfileName, shell=True, stdout=AudioIO.FNULL,\r\n\t\t\t\t\t\t\tstderr=subprocess.STDOUT)\r\n\t\t\tsamples, sampleRate = AudioIO.wavRead(modfileName, mono)\r\n\t\t\tos.remove(modfileName)\r\n\r\n\t\telif fileName[convDict[\'m4a\'][1]:] == \'m4a\':\r\n\t\t\tprint(fileName[convDict[\'m4a\'][1]:])\r\n\t\t\tmodfileName = os.path.join(os.path.abspath(fileName[:-4] + \'_temp.wav\'))\r\n\t\t\tsubprocess.call(convDict[\'m4a\'][0] + modfileName, shell=True, stdout=AudioIO.FNULL,\r\n\t\t\t\t\t\t\tstderr=subprocess.STDOUT)\r\n\t\t\tsamples, sampleRate = AudioIO.wavRead(modfileName, mono)\r\n\t\t\tos.remove(modfileName)\r\n\r\n\t\telse :\r\n\t\t\traise Exception(\'This format is not supported.\')\r\n\r\n\t\treturn samples, sampleRate\r\n\r\n\t@staticmethod\r\n\tdef audioWrite(y, fs, nbits, audioFile, format):\r\n\t\t"""""" Write samples to WAV file and then converts to selected\r\n\t\tformat using ffmpeg.\r\n        Args:\r\n            samples: \t(ndarray / 2D ndarray) (floating point) sample vector\r\n                    \t\tmono:   DIM: nSamples\r\n                    \t\tstereo: DIM: nSamples x nChannels\r\n\r\n            fs: \t\t(int) Sample rate in Hz\r\n            nBits: \t\t(int) Number of bits\r\n            audioFile: \t(string) File name to write\r\n            format:\t\t(string) Selected format\r\n            \t\t\t\t\'mp3\' \t: Writes to .mp3\r\n            \t\t\t\t\'wma\' \t: Writes to .wma\r\n            \t\t\t\t\'wav\' \t: Writes to .wav\r\n            \t\t\t\t\'aiff\'\t: Writes to .aiff\r\n            \t\t\t\t\'au\'\t: Writes to .au\r\n            \t\t\t\t\'m4a\'   : Writes to .m4a\r\n\t\t""""""\r\n\r\n\t\t# Linux\r\n\t\tif (platform == ""linux"") or (platform == ""linux2""):\r\n\t\t\tconvDict = {\r\n\t\t\t\t\'m4a\':  [os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_linux\') + \' -i \', -3],\r\n\t\t\t\t\'mp3\':  [os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_linux\') + \' -i \', -3],\r\n\t\t\t\t\'au\':   [os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_linux\') + \' -i \', -2],\r\n\t\t\t\t\'wma\':  [os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_linux\') + \' -i \', -3],\r\n\t\t\t\t\'aiff\': [os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_linux\') + \' -i \', -4]\r\n\t\t\t\t\t\t}\r\n\r\n\t\t# MacOSX\r\n\t\telif (platform == ""darwin""):\r\n\t\t\tconvDict = {\r\n\t\t\t\t\'m4a\':  [os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_osx\') + \' -i \', -3],\r\n\t\t\t\t\'mp3\':  [os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_osx\') + \' -i \', -3],\r\n\t\t\t\t\'au\':   [os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_osx\') + \' -i \', -2],\r\n\t\t\t\t\'wma\':  [os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_osx\') + \' -i \', -3],\r\n\t\t\t\t\'aiff\': [os.path.join(AudioIO.pathToffmpeg, \'ffmpeg_osx\') + \' -i \', -4]\r\n\t\t\t\t\t\t}\r\n\r\n\t\telse :\r\n\t\t\traise Exception(\'This OS is not supported.\')\r\n\r\n\t\tif (format == \'mp3\'):\r\n\t\t\tprmfileName = os.path.join(os.path.abspath(audioFile[:convDict[\'mp3\'][1]] + \'wav\'))\r\n\t\t\tAudioIO.wavWrite(y, fs, nbits, prmfileName)\r\n\t\t\tsubprocess.call(convDict[\'mp3\'][0] + prmfileName + \' \' + audioFile,\r\n\t\t\t\t\t\t\tshell = True,  stdout=AudioIO.FNULL, stderr=subprocess.STDOUT)\r\n\t\t\tos.remove(prmfileName)\r\n\r\n\t\telif (format == \'wav\'):\r\n\t\t\tAudioIO.wavWrite(y, fs, nbits, audioFile)\r\n\r\n\t\telif (format == \'wma\'):\r\n\t\t\tprmfileName = os.path.join(os.path.abspath(audioFile[:convDict[\'wma\'][1]] + \'wav\'))\r\n\t\t\tAudioIO.wavWrite(y, fs, nbits, prmfileName)\r\n\t\t\tsubprocess.call(convDict[\'wma\'][0] + prmfileName + \' \' + audioFile,\r\n\t\t\t\t\t\t\tshell = True,  stdout=AudioIO.FNULL, stderr=subprocess.STDOUT)\r\n\t\t\tos.remove(prmfileName)\r\n\r\n\t\telif (format == \'aiff\'):\r\n\t\t\tprmfileName = os.path.join(os.path.abspath(audioFile[:convDict[\'aiff\'][1]] + \'wav\'))\r\n\t\t\tAudioIO.wavWrite(y, fs, nbits, prmfileName)\r\n\t\t\tsubprocess.call(convDict[\'aiff\'][0] + prmfileName + \' \' + audioFile,\r\n\t\t\t\t\t\t\tshell = True,  stdout=AudioIO.FNULL, stderr=subprocess.STDOUT)\r\n\t\t\tos.remove(prmfileName)\r\n\r\n\t\telif (format == \'au\'):\r\n\t\t\tprmfileName = os.path.join(os.path.abspath(audioFile[:convDict[\'au\'][1]] + \'wav\'))\r\n\t\t\tAudioIO.wavWrite(y, fs, nbits, prmfileName)\r\n\t\t\tsubprocess.call(convDict[\'au\'][0] + prmfileName + \' \' + audioFile,\r\n\t\t\t\t\t\t\tshell = True,  stdout=AudioIO.FNULL, stderr=subprocess.STDOUT)\r\n\t\t\tos.remove(prmfileName)\r\n\r\n\t\telif (format == \'m4a\'):\r\n\t\t\tprmfileName = os.path.join(os.path.abspath(audioFile[:convDict[\'m4a\'][1]] + \'wav\'))\r\n\t\t\tAudioIO.wavWrite(y, fs, nbits, prmfileName)\r\n\t\t\tsubprocess.call(convDict[\'m4a\'][0] + prmfileName + \' -b:a 320k \' + audioFile,\r\n\t\t\t\t\t\t\tshell = True, stdout=AudioIO.FNULL, stderr=subprocess.STDOUT)\r\n\t\t\tos.remove(prmfileName)\r\n\t\telse :\r\n\t\t\traise Exception(\'This format is not supported.\')\r\n\r\n\t@staticmethod\r\n\tdef wavRead(fileName, mono=False):\r\n\t\t"""""" Function to load WAV file.\r\n\r\n        Args:\r\n            fileName:       (str)       Absolute filename of WAV file\r\n            mono:           (bool)      Switch if samples should be converted to mono\r\n        Returns:\r\n            samples:        (np array)  Audio samples (between [-1,1]\r\n                                        (if stereo: numSamples x numChannels,\r\n                                        if mono: numSamples)\r\n            sampleRate:     (float):    Sampling frequency [Hz]\r\n        """"""\r\n\t\ttry:\r\n\t\t\tsamples, sampleRate = AudioIO._loadWAVWithWave(fileName)\r\n\t\t\tsWidth = _wave.open(fileName).getsampwidth()\r\n\t\t\tif sWidth == 1:\r\n\t\t\t\t#print(\'8bit case\')\r\n\t\t\t\tsamples = samples.astype(float) / AudioIO.normFact[\'int8\'] - 1.0\r\n\t\t\telif sWidth == 2:\r\n\t\t\t\t#print(\'16bit case\')\r\n\t\t\t\tsamples = samples.astype(float) / AudioIO.normFact[\'int16\']\r\n\t\t\telif sWidth == 3:\r\n\t\t\t\t#print(\'24bit case\')\r\n\t\t\t\tsamples = samples.astype(float) / AudioIO.normFact[\'int24\']\r\n\t\texcept:\r\n\t\t\t#print(\'32bit case\')\r\n\t\t\tsamples, sampleRate = AudioIO._loadWAVWithScipy(fileName)\r\n\r\n\t\t# mono conversion\r\n\t\tif mono:\r\n\t\t\tif samples.ndim == 2 and samples.shape[1] > 1:\r\n\t\t\t\tsamples = (samples[:, 0] + samples[:, 1])*0.5\r\n\r\n\t\treturn samples, sampleRate\r\n\r\n\t@staticmethod\r\n\tdef _loadWAVWithWave(fileName):\r\n\t\t"""""" Load samples & sample rate from 24 bit WAV file """"""\r\n\t\twav = _wave.open(fileName)\r\n\t\trate = wav.getframerate()\r\n\t\tnchannels = wav.getnchannels()\r\n\t\tsampwidth = wav.getsampwidth()\r\n\t\tnframes = wav.getnframes()\r\n\t\tdata = wav.readframes(nframes)\r\n\t\twav.close()\r\n\t\tarray = AudioIO._wav2array(nchannels, sampwidth, data)\r\n\r\n\t\treturn array, rate\r\n\r\n\t@staticmethod\r\n\tdef _loadWAVWithScipy(fileName):\r\n\t\t"""""" Load samples & sample rate from WAV file """"""\r\n\t\tinputData = read(fileName)\r\n\t\tsamples = inputData[1]\r\n\t\tsampleRate = inputData[0]\r\n\r\n\t\treturn samples, sampleRate\r\n\r\n\t@staticmethod\r\n\tdef _wav2array(nchannels, sampwidth, data):\r\n\t\t""""""data must be the string containing the bytes from the wav file.""""""\r\n\t\tnum_samples, remainder = divmod(len(data), sampwidth * nchannels)\r\n\t\tif remainder > 0:\r\n\t\t\traise ValueError(\'The length of data is not a multiple of \'\r\n                             \'sampwidth * num_channels.\')\r\n\t\tif sampwidth > 4:\r\n\t\t\traise ValueError(""sampwidth must not be greater than 4."")\r\n\r\n\t\tif sampwidth == 3:\r\n\t\t\ta = np.empty((num_samples, nchannels, 4), dtype = np.uint8)\r\n\t\t\traw_bytes = np.fromstring(data, dtype = np.uint8)\r\n\t\t\ta[:, :, :sampwidth] = raw_bytes.reshape(-1, nchannels, sampwidth)\r\n\t\t\ta[:, :, sampwidth:] = (a[:, :, sampwidth - 1:sampwidth] >> 7) * 255\r\n\t\t\tresult = a.view(\'<i4\').reshape(a.shape[:-1])\r\n\t\telse:\r\n\t\t\t# 8 bit samples are stored as unsigned ints; others as signed ints.\r\n\t\t\tdt_char = \'u\' if sampwidth == 1 else \'i\'\r\n\t\t\ta = np.fromstring(data, dtype=\'<%s%d\' % (dt_char, sampwidth))\r\n\t\t\tresult = a.reshape(-1, nchannels)\r\n\t\treturn result\r\n\r\n\t@staticmethod\r\n\tdef wavWrite(y, fs, nbits, audioFile):\r\n\t\t"""""" Write samples to WAV file\r\n        Args:\r\n            samples: (ndarray / 2D ndarray) (floating point) sample vector\r\n                    \tmono: DIM: nSamples\r\n                    \tstereo: DIM: nSamples x nChannels\r\n\r\n            fs: \t(int) Sample rate in Hz\r\n            nBits: \t(int) Number of bits\r\n            fnWAV: \t(string) WAV file name to write\r\n\t\t""""""\r\n\t\tif nbits == 8:\r\n\t\t\tintsamples = (y+1.0) * AudioIO.normFact[\'int\' + str(nbits)]\r\n\t\t\tfX = np.int8(intsamples)\r\n\t\telif nbits == 16:\r\n\t\t\tintsamples = y * AudioIO.normFact[\'int\' + str(nbits)]\r\n\t\t\tfX = np.int16(intsamples)\r\n\t\telif nbits > 16:\r\n\t\t\tfX = y\r\n\r\n\t\twrite(audioFile, fs, fX)\r\n\r\n\t@staticmethod\r\n\tdef sound(x,fs):\r\n\t\t"""""" Plays a wave file using the pyglet library. But first, it has to be written.\r\n\t\t\tTermination of the playback is being performed by any keyboard input and Enter.\r\n\t\t\tArgs:\r\n\t\t\tx: \t\t   (array) Floating point samples\r\n\t\t\tfs:\t\t   (int) The sampling rate\r\n\t\t""""""\r\n\t\timport pyglet as pg\r\n\t\tglobal player\r\n\t\t# Call the writing function\r\n\t\tAudioIO.wavWrite(x, fs, 16, \'testPlayback.wav\')\r\n\t\t# Initialize playback engine\r\n\t\tplayer = pg.media.Player()\r\n\t\t# Initialize the object with the audio file\r\n\t\tplayback = pg.media.load(\'testPlayback.wav\')\r\n\t\t# Set it to player\r\n\t\tplayer.queue(playback)\r\n\t\t# Sound call\r\n\t\tplayer.play()\r\n\t\t# Killed by ""keyboard""\r\n\t\tkill = raw_input()\r\n\t\tif kill or kill == \'\':\r\n\t\t\tAudioIO.stop()\r\n\t\t# Remove the dummy wave write\r\n\t\tos.remove(\'testPlayback.wav\')\r\n\r\n\t@staticmethod\r\n\tdef stop():\r\n\t\t"""""" Stops a playback object of the pyglet library.\r\n\t\t\tIt does not accept arguments, but a player has to be\r\n\t\t\talready initialized by the above ""sound"" method.\r\n\t\t""""""\r\n\t\tglobal player\r\n\t\t# Just Pause & Destruct\r\n\t\tplayer.pause()\r\n\t\tplayer = None\r\n\t\treturn None\r\n\r\nif __name__ == ""__main__"":\r\n\t# Define File\r\n\tmyReadFile = \'EnterYourWavFile.wav\'\r\n\r\n\t# Read the file\r\n\tx, fs = AudioIO.wavRead(myReadFile, mono = True)\r\n\r\n\t# Gain parameter\r\n\tg = 0.2\r\n\r\n\t# Listen to it\r\n\tAudioIO.sound(x*g,fs)\r\n\r\n\t# Make it better and write it to disk\r\n\tx2 = np.empty((len(x),2), dtype = np.float32)\r\n\ttry :\r\n\t\tx2[:,0] = x * g\r\n\t\tx2[:,1] = np.roll(x*g, 512)\r\n\texcept ValueError:\r\n\t\tx2[:,0] = x[:,0] * g\r\n\t\tx2[:,1] = np.roll(x[:,0] * g, 256)\r\n\r\n\t# Listen to stereo processed\r\n\tAudioIO.sound(x2*g,fs)\r\n\tAudioIO.audioWrite(x2, fs, 16, \'myNewWavFile.wav\', \'wav\')\r\n\r\n# EOF'"
helpers/iterative_inference.py,5,"b""# -*- coding: utf-8 -*-\n__author__ = 'J.F. Santos, K. Drossos'\n\nimport torch\nfrom losses import loss_functions\n\n\ndef iterative_inference(module, x, criterion=None, tol=1e-9, max_iter=10):\n    if criterion is None:\n        criterion = loss_functions.mse\n\n    y0 = module(x)\n    for k in range(max_iter):\n        y = module(y0)\n        if criterion(y0, y).data[0] < tol:\n            break\n        else:\n            y0 = y\n    return y, k\n\n\ndef iterative_recurrent_inference(module, H_enc, criterion=None, tol=1e-9, max_iter=10):\n    if criterion is None:\n        criterion = loss_functions.mse\n\n    y0 = module(H_enc)\n    for iter in range(max_iter):\n        y = module(y0)\n        if criterion(y0, y).data[0] < tol:\n            break\n        else:\n            y0 = y\n    return y\n\n\nif __name__ == '__main__':\n    model = torch.nn.Linear(10, 10)\n    x = torch.autograd.Variable(torch.rand(4, 10))\n    ground_truth = torch.autograd.Variable(x.data + 0.1 * torch.rand(4, 10))\n    criterion = torch.nn.MSELoss()\n    opt = torch.optim.Adam(model.parameters())\n\n    for k in range(10000):\n        y, n_iter = iterative_inference(model, x)\n        loss = criterion(y, ground_truth)\n        loss.backward()\n        opt.step()\n        print('Iter. {} - loss = {}'.format(k, loss))\n\n"""
helpers/masking_methods.py,0,"b'# -*- coding: utf-8 -*-\n__author__ = \'S.I. Mimilakis\'\n__copyright__ = \'MacSeNet\'\n\n""""""\n    Following masking methods of https://github.com/Js-Mim/ASP.\n""""""\n\n# imports\nimport numpy as np\nfrom scipy.fftpack import fft\n\n\nclass FrequencyMasking:\n\t""""""Class containing various time-frequency masking methods,\n\t   for processing Time-Frequency representations.\n\t""""""\n\n\tdef __init__(self, mX, sTarget, nResidual, psTarget=[], pnResidual=[], alpha=1.2, method=\'Wiener\'):\n\t\tself._mX = mX\n\t\tself._eps = np.finfo(np.float).eps\n\t\tself._sTarget = sTarget\n\t\tself._nResidual = nResidual\n\t\tself._pTarget = psTarget\n\t\tself._pY = pnResidual\n\t\tself._mask = []\n\t\tself._Out = []\n\t\tself._alpha = alpha\n\t\tself._method = method\n\t\tself._iterations = 200\n\t\tself._lr = 1.5e-3\n\t\tself._hetaplus = 1.1\n\t\tself._hetaminus = 0.1\n\t\tself._amountiter = 0\n\n\tdef __call__(self, reverse=False):\n\t\tif self._method == \'Phase\':\n\t\t\tif not self._pTarget.size or not self._pTarget.size:\n\t\t\t\traise ValueError(\'Phase-sensitive masking cannot be performed without phase information.\')\n\t\t\telse:\n\t\t\t\tFrequencyMasking.phaseSensitive(self)\n\t\t\t\tif not reverse:\n\t\t\t\t\tFrequencyMasking.applyMask(self)\n\t\t\t\telse:\n\t\t\t\t\tFrequencyMasking.applyReverseMask(self)\n\n\t\telif self._method == \'IRM\':\n\t\t\tFrequencyMasking.IRM(self)\n\t\t\tif not reverse:\n\t\t\t\tFrequencyMasking.applyMask(self)\n\t\t\telse:\n\t\t\t\tFrequencyMasking.applyReverseMask(self)\n\n\t\telif self._method == \'IAM\':\n\t\t\tFrequencyMasking.IAM(self)\n\t\t\tif not reverse:\n\t\t\t\tFrequencyMasking.applyMask(self)\n\t\t\telse:\n\t\t\t\tFrequencyMasking.applyReverseMask(self)\n\n\t\telif self._method == \'IBM\':\n\t\t\tFrequencyMasking.IBM(self)\n\t\t\tif not reverse:\n\t\t\t\tFrequencyMasking.applyMask(self)\n\t\t\telse:\n\t\t\t\tFrequencyMasking.applyReverseMask(self)\n\n\t\telif self._method == \'UBBM\':\n\t\t\tFrequencyMasking.UBBM(self)\n\t\t\tif not reverse:\n\t\t\t\tFrequencyMasking.applyMask(self)\n\t\t\telse:\n\t\t\t\tFrequencyMasking.applyReverseMask(self)\n\n\t\telif self._method == \'Wiener\':\n\t\t\tFrequencyMasking.Wiener(self)\n\t\t\tif not reverse:\n\t\t\t\tFrequencyMasking.applyMask(self)\n\t\t\telse:\n\t\t\t\tFrequencyMasking.applyReverseMask(self)\n\n\t\telif self._method == \'alphaWiener\':\n\t\t\tFrequencyMasking.alphaHarmonizableProcess(self)\n\t\t\tif not reverse:\n\t\t\t\tFrequencyMasking.applyMask(self)\n\t\t\telse:\n\t\t\t\tFrequencyMasking.applyReverseMask(self)\n\n\t\telif self._method == \'expMask\':\n\t\t\tFrequencyMasking.ExpM(self)\n\t\t\tif not reverse:\n\t\t\t\tFrequencyMasking.applyMask(self)\n\t\t\telse:\n\t\t\t\tFrequencyMasking.applyReverseMask(self)\n\n\t\telif self._method == \'MWF\':\n\t\t\tprint(\'Multichannel Wiener Filtering\')\n\t\t\tFrequencyMasking.MWF(self)\n\n\t\treturn self._Out\n\n\tdef IRM(self):\n\t\t""""""\n\t\t\tComputation of Ideal Amplitude Ratio Mask. As appears in :\n\t\t\tH Erdogan, John R. Hershey, Shinji Watanabe, and Jonathan Le Roux,\n\t   \t\t""Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks,""\n\t   \t\tin ICASSP 2015, Brisbane, April, 2015.\n\t\tArgs:\n\t\t\tsTarget:   (2D ndarray) Magnitude Spectrogram of the target component\n\t\t\tnResidual: (2D ndarray) Magnitude Spectrogram of the residual component\n\t\tReturns:\n\t\t\tmask:      (2D ndarray) Array that contains time frequency gain values\n\n\t\t""""""\n\t\tprint(\'Ideal Amplitude Ratio Mask\')\n\t\tself._mask = np.divide(self._sTarget, (self._sTarget + self._nResidual + self._eps))\n\n\tdef IAM(self):\n\t\t""""""\n\t\t\tComputation of Ideal Amplitude Mask. As appears in :\n\t\t\tH. Erdogan, J. R. Hershey, S. Watanabe, and J. Le Roux,\n\t   \t\t""Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks,""\n\t   \t\tin ICASSP 2015, Brisbane, April, 2015.\n\t\tArgs:\n\t\t\tsTarget:   (2D ndarray) Magnitude Spectrogram of the target component\n\t\t\tnResidual: (2D ndarray) Magnitude Spectrogram of the residual component\n\t\t\t\t\t\t\t\t\t(In this case the observed mixture should be placed)\n\t\tReturns:\n\t\t\tmask:      (2D ndarray) Array that contains time frequency gain values\n\n\t\t""""""\n\t\tprint(\'Ideal Amplitude Mask\')\n\t\tself._mask = np.divide(self._sTarget, (self._nResidual + self._eps))\n\n\tdef ExpM(self):\n\t\t""""""\n\t\t\tApproximate a signal via element-wise exponentiation. As appears in :\n\t\t\tS.I. Mimilakis, K. Drossos, T. Virtanen, and G. Schuller,\n\t\t\t""Deep Neural Networks for Dynamic Range Compression in Mastering Applications,""\n\t\t\tin proc. of the 140th Audio Engineering Society Convention, Paris, 2016.\n\t\tArgs:\n\t\t\tsTarget:   (2D ndarray) Magnitude Spectrogram of the target component\n\t\t\tnResidual: (2D ndarray) Magnitude Spectrogram of the residual component\n\t\tReturns:\n\t\t\tmask:      (2D ndarray) Array that contains time frequency gain values\n\n\t\t""""""\n\t\tprint(\'Exponential mask\')\n\t\tself._mask = np.divide(np.log(self._sTarget.clip(self._eps, np.inf) ** self._alpha), \\\n\t\t\t\t\t\t\t   np.log(self._nResidual.clip(self._eps, np.inf) ** self._alpha))\n\n\tdef IBM(self):\n\t\t""""""\n\t\t\tComputation of Ideal Binary Mask.\n\t\tArgs:\n\t\t\tsTarget:   (2D ndarray) Magnitude Spectrogram of the target component\n\t\t\tnResidual: (2D ndarray) Magnitude Spectrogram of the residual component\n\t\tReturns:\n\t\t\tmask:      (2D ndarray) Array that contains time frequency gain values\n\n\t\t""""""\n\t\tprint(\'Ideal Binary Mask\')\n\t\ttheta = 0.5\n\t\tmask = np.divide(self._sTarget ** self._alpha, (self._eps + self._nResidual ** self._alpha))\n\t\tbg = np.where(mask >= theta)\n\t\tsm = np.where(mask < theta)\n\t\tmask[bg[0], bg[1]] = 1.\n\t\tmask[sm[0], sm[1]] = 0.\n\t\tself._mask = mask\n\n\tdef UBBM(self):\n\t\t""""""\n\t\t\tComputation of Upper Bound Binary Mask. As appears in :\n\t\t\t- J.J. Burred, ""From Sparse Models to Timbre Learning: New Methods for Musical Source Separation"", PhD Thesis,\n\t\t\tTU Berlin, 2009.\n\n\t\tArgs:\n\t\t\tsTarget:   (2D ndarray) Magnitude Spectrogram of the target component\n\t\t\tnResidual: (2D ndarray) Magnitude Spectrogram of the residual component (Should not contain target source!)\n\t\tReturns:\n\t\t\tmask:      (2D ndarray) Array that contains time frequency gain values\n\t\t""""""\n\t\tprint(\'Upper Bound Binary Mask\')\n\t\tmask = 20. * np.log(self._eps + np.divide((self._eps + (self._sTarget ** self._alpha)),\n\t\t\t\t\t\t\t\t\t\t\t\t  ((self._eps + (self._nResidual ** self._alpha)))))\n\t\tbg = np.where(mask >= 0)\n\t\tsm = np.where(mask < 0)\n\t\tmask[bg[0], bg[1]] = 1.\n\t\tmask[sm[0], sm[1]] = 0.\n\t\tself._mask = mask\n\n\tdef Wiener(self):\n\t\t""""""\n\t\t\tComputation of Wiener-like Mask. As appears in :\n\t\t\tH Erdogan, John R. Hershey, Shinji Watanabe, and Jonathan Le Roux,\n\t   \t\t""Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks,""\n\t   \t\tin ICASSP 2015, Brisbane, April, 2015.\n\t\tArgs:\n\t\t\t\tsTarget:   (2D ndarray) Magnitude Spectrogram of the target component\n\t\t\t\tnResidual: (2D ndarray) Magnitude Spectrogram of the residual component\n\t\tReturns:\n\t\t\t\tmask:      (2D ndarray) Array that contains time frequency gain values\n\t\t""""""\n\t\tprint(\'Wiener-like Mask\')\n\t\tlocalsTarget = self._sTarget ** 2.\n\t\tnumElements = len(self._nResidual)\n\t\tif numElements > 1:\n\t\t\tlocalnResidual = self._nResidual[0] ** 2. + localsTarget\n\t\t\tfor indx in range(1, numElements):\n\t\t\t\tlocalnResidual += self._nResidual[indx] ** 2.\n\t\telse:\n\t\t\tlocalnResidual = self._nResidual[0] ** 2. + localsTarget\n\n\t\tself._mask = np.divide((localsTarget + self._eps), (self._eps + localnResidual))\n\n\tdef alphaHarmonizableProcess(self):\n\t\t""""""\n\t\t\tComputation of Wiener like mask using fractional power spectrograms. As appears in :\n\t\t\tA. Liutkus, R. Badeau, ""Generalized Wiener filtering with fractional power spectrograms"",\n    \t\t40th International Conference on Acoustics, Speech and Signal Processing (ICASSP),\n    \t\tApr 2015, Brisbane, Australia.\n\t\tArgs:\n\t\t\tsTarget:   (2D ndarray) Magnitude Spectrogram of the target component\n\t\t    nResidual: (2D ndarray) Magnitude Spectrogram of the residual component or a list\n\t\t\t\t\t\t\t\t\tof 2D ndarrays which will be added together\n\t\tReturns:\n\t\t\tmask:      (2D ndarray) Array that contains time frequency gain values\n\n\t\t""""""\n\t\tprint(\'Harmonizable Process with alpha:\', str(self._alpha))\n\t\tlocalsTarget = self._sTarget ** self._alpha\n\t\tnumElements = len(self._nResidual)\n\t\tif numElements > 1:\n\t\t\tlocalnResidual = self._nResidual[0] ** self._alpha + localsTarget\n\t\t\tfor indx in range(1, numElements):\n\t\t\t\tlocalnResidual += self._nResidual[indx] ** self._alpha\n\t\telse:\n\t\t\tlocalnResidual = self._nResidual[0] ** self._alpha + localsTarget\n\n\t\tself._mask = np.divide((localsTarget + self._eps), (self._eps + localnResidual))\n\n\tdef phaseSensitive(self):\n\t\t""""""\n\t\t\tComputation of Phase Sensitive Mask. As appears in :\n\t\t\tH Erdogan, John R. Hershey, Shinji Watanabe, and Jonathan Le Roux,\n\t   \t\t""Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks,""\n\t   \t\tin ICASSP 2015, Brisbane, April, 2015.\n\n\t\tArgs:\n\t\t\tmTarget:   (2D ndarray) Magnitude Spectrogram of the target component\n\t\t\tpTarget:   (2D ndarray) Phase Spectrogram of the output component\n\t\t\tmY:        (2D ndarray) Magnitude Spectrogram of the residual component\n\t\t\tpY:        (2D ndarray) Phase Spectrogram of the residual component\n\t\tReturns:\n\t\t\tmask:      (2D ndarray) Array that contains time frequency gain values\n\n\t\t""""""\n\t\tprint(\'Truncated Phase Sensitive Masking.\')\n\t\t# Compute Phase Difference\n\t\tTheta = (self._pTarget - self._pY)\n\t\tTheta = np.clip(np.cos(Theta), a_min=0., a_max=1.)\n\t\tself._mask = np.divide(self._sTarget, self._eps + self._nResidual) * Theta\n\n\tdef optAlpha(self, initloss):\n\t\t""""""\n\t\t\tA simple gradiend descent method using the RProp algorithm,\n\t\t\tfor finding optimum power-spectral density exponents (alpha) for generalized Wiener filtering.\n\t\tArgs:\n\t\t\tsTarget  : (2D ndarray) Magnitude Spectrogram of the target component\n\t\t\tnResidual: (2D ndarray) Magnitude Spectrogram of the residual component or a list\n\t\t\t\t\t\t\t\t\tof 2D ndarrays which will be added together\n\t\t\tinitloss : (float)\t\tInitial loss, for comparisson\n\t\tReturns:\n\t\t\tmask:      (2D ndarray) Array that contains time frequency gain values\n\n\t\t""""""\n\t\t# Initialization of the parameters\n\t\t# Put every source spectrogram into an array, given an input list.\n\t\tslist = list(self._nResidual)\n\t\tslist.insert(0, self._sTarget)\n\t\tnumElements = len(slist)\n\t\tslist = np.asarray(slist)\n\n\t\talpha = np.array([1.15] * (numElements))  # Initialize an array of alpha values to be found.\n\t\tdloss = np.array([0.] * (numElements))  # Initialize an array of loss functions to be used.\n\t\tlrs = np.array(\n\t\t\t[self._lr] * (numElements))  # Initialize an array of learning rates to be applied to each source.\n\n\t\t# Begin of otpimization\n\t\tisloss = []\n\t\talphalog = []\n\t\tfor iter in xrange(self._iterations):\n\t\t\t# The actual function of additive power spectrograms\n\t\t\tXhat = np.sum(np.power(slist, np.reshape(alpha, (numElements, 1, 1))), axis=0)\n\t\t\tfor source in xrange(numElements):\n\t\t\t\t# Derivative with respect to the function of additive power spectrograms\n\t\t\t\tdX = (slist[source, :, :] ** alpha[source]) * np.log(slist[source, :, :] + self._eps)\n\n\t\t\t\t# Chain rule between the above derivative and the IS derivative\n\t\t\t\tdloss[source] = self._dIS(Xhat) * np.mean(dX)\n\n\t\t\talpha -= (lrs * dloss)\n\n\t\t\t# Make sure the initial alpha are inside reasonable and comparable values\n\t\t\talpha = np.clip(alpha, a_min=0.5, a_max=2.)\n\t\t\talpha = np.round(alpha * 100.) / 100.\n\n\t\t\t# Store the evolution of alphas\n\t\t\talphalog.append(alpha)\n\n\t\t\t# Check IS Loss by computing Xhat\n\t\t\tXhat = 0\n\t\t\tfor source in xrange(numElements):\n\t\t\t\tXhat += slist[source, :, :] ** alpha[source]\n\n\t\t\tisloss.append(self._IS(Xhat))\n\n\t\t\t# Apply RProp\n\t\t\tif (iter > 2):\n\t\t\t\tif (isloss[-2] - isloss[-1] > 0):\n\t\t\t\t\tlrs *= self._hetaplus\n\n\t\t\t\tif (isloss[-2] - isloss[-1] < 0):\n\t\t\t\t\tlrs *= self._hetaminus\n\n\t\t\t\tif (iter > 4):\n\t\t\t\t\tif (np.abs(isloss[-2] - isloss[-1]) < 1e-4 and np.abs(isloss[-3] - isloss[-2]) < 1e-4):\n\t\t\t\t\t\tif (isloss[-1] > 3e-1):\n\t\t\t\t\t\t\tprint(\'Stuck...\')\n\t\t\t\t\t\t\talpha = alphalog[np.argmin(isloss) - 1]\n\t\t\t\t\t\t\tisloss[-1] = isloss[np.argmin(isloss)]\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tprint(\'Local Minimum Found\')\n\n\t\t\t\t\t\tprint(\'Final Loss: \' + str(isloss[-1]) + \' with characteristic exponent(s): \' + str(alpha))\n\t\t\t\t\t\tbreak\n\n\t\t\tprint(\'Loss: \' + str(isloss[-1]) + \' with characteristic exponent(s): \' + str(alpha))\n\n\t\t# If the operation was terminated by the end of iterations pick the minimum\n\t\tif iter == self._iterations:\n\t\t\tself._alpha = alphalog[np.argmin(isloss)]\n\t\t\tself._closs = isloss[np.argmin(isloss)]\n\t\telse:\n\t\t\tself._closs = isloss[-1]\n\t\t\tself._alpha = alpha\n\n\t\t# Export the amount of iterations\n\t\tself._amountiter = iter\n\n\t\t# Evaluate Xhat for the mask update\n\t\tXhat = 0\n\t\tfor source in xrange(numElements):\n\t\t\tXhat += slist[source, :, :] ** alpha[source]\n\n\t\tself._mask = np.divide((slist[0, :, :] ** alpha[0] + self._eps), (Xhat + self._eps))\n\n\tdef MWF(self):\n\t\t"""""" Multi-channel Wiener filtering as appears in:\n\t\tI. Cohen, J. Benesty, and S. Gannot, Speech Processing in Modern\n\t\tCommunication, Springer, Berlin, Heidelberg, 2010, Chapter 9.\n\t\tArgs:\n\t\t\tmTarget:   (3D ndarray) Magnitude Spectrogram of the target component\n\t\t\tmY:        (3D ndarray) Magnitude Spectrogram of the output component\n\t\t\t\t\t\t\t\t    (M channels x F frequency samples x T time-frames).\n\t\tReturns:\n\t\t\t_Out:      (3D ndarray) Array that contains the estimated source.\n\t\t""""""\n\t\t# Parameter for the update\n\t\tflambda = 0.99  # Forgetting Factor\n\n\t\tcX = self._sTarget ** self._alpha\n\t\tcN = self._nResidual ** self._alpha\n\n\t\tM = self._mX.shape[0]  # Number of channels\n\t\tgF = 1. / M  # Gain factor\n\t\teM = cX.shape[0]  # Number of estimated channels\n\t\tF = cX.shape[1]  # Number of frequency samples\n\t\tT = cX.shape[2]  # Number of time-frames\n\t\tfout = np.zeros((M, F, T), dtype=np.float32)  # Initializing output\n\t\tI = np.eye(M)  # Identity matrix\n\n\t\t# Initialization of covariance matrices\n\t\tRxx = np.repeat(np.reshape(I, (M, M, 1)), F, axis=-1)\n\t\tRnn = np.repeat(np.reshape(I, (M, M, 1)), F, axis=-1)\n\n\t\t# Recursive updates\n\t\tfor t in xrange(T):\n\t\t\tfor f in xrange(F):\n\t\t\t\tif eM == 1:\n\t\t\t\t\tRxx[:, :, f] = flambda * Rxx[:, :, f] + (1. - flambda) * (cX[:, f, t])\n\t\t\t\t\tRnn[:, :, f] = flambda * Rnn[:, :, f] + (1. - flambda) * (cN[:, f, t])\n\t\t\t\telse:\n\t\t\t\t\tRxx[:, :, f] = (np.dot(cX[:, f:f + 1, t], cX[:, f:f + 1, t].T)) / np.sum(cX[:, f, t], axis=0)\n\t\t\t\t\tRnn[:, :, f] = (np.dot(cN[:, f:f + 1, t], cN[:, f:f + 1, t].T)) / np.sum(cN[:, f, t], axis=0)\n\n\t\t\t\tinv = np.dot(np.linalg.pinv(Rnn[:, :, f]), (Rnn[:, :, f] + Rxx[:, :, f]))\n\t\t\t\tif eM == 1:\n\t\t\t\t\tWf = ((inv - I) / ((cN[:, f, t] + cX[:, f, t] + 1e-6) / (cX[:, f, t] + 1e-6) + np.trace(inv) * gF))\n\t\t\t\telse:\n\t\t\t\t\tWf = ((inv - I) / (gF * np.trace(inv)))\n\n\t\t\t\tfout[:, f, t] = np.dot(Wf.T, self._mX[:, f, t])\n\n\t\tself._Out = np.abs(fout)\n\n\tdef applyMask(self):\n\t\t"""""" Compute the filtered output spectrogram.\n\t\tArgs:\n\t\t\tmask:   (2D ndarray) Array that contains time frequency gain values\n\t\t\tmX:     (2D ndarray) Input Magnitude Spectrogram\n\t\tReturns:\n\t\t\tY:      (2D ndarray) Filtered version of the Magnitude Spectrogram\n\t\t""""""\n\t\tif self._method == \'expMask\':\n\t\t\tself._Out = (self._mX ** self._alpha) ** self._mask\n\t\telse:\n\t\t\tself._Out = np.multiply(self._mask, self._mX)\n\n\tdef applyReverseMask(self):\n\t\t"""""" Compute the filtered output spectrogram, reversing the gain values.\n\t\tArgs:\n\t\t\tmask:   (2D ndarray) Array that contains time frequency gain values\n\t\t\tmX:     (2D ndarray) Input Magnitude Spectrogram\n\t\tReturns:\n\t\t\tY:      (2D ndarray) Filtered version of the Magnitude Spectrogram\n\t\t""""""\n\t\tif self._method == \'expMask\':\n\t\t\traise ValueError(\'Cannot compute that using such masking method.\')\n\t\telse:\n\t\t\tself._Out = np.multiply((1. - self._mask), self._mX)\n\n\tdef _IS(self, Xhat):\n\t\t"""""" Compute the Itakura-Saito distance between the observed magnitude spectrum\n\t\t\tand the estimated one.\n\t\tArgs:\n\t\t\tmX    :   \t(2D ndarray) Input Magnitude Spectrogram\n\t\t\tXhat  :     (2D ndarray) Estimated Magnitude Spectrogram\n\t\tReturns:\n\t\t\tdis   :     (float) Average Itakura-Saito distance\n\t\t""""""\n\t\tr1 = (np.abs(self._mX) ** self._alpha + self._eps) / (np.abs(Xhat) + self._eps)\n\t\tlg = np.log((np.abs(self._mX) ** self._alpha + self._eps)) - np.log((np.abs(Xhat) + self._eps))\n\t\treturn np.mean(r1 - lg - 1.)\n\n\tdef _dIS(self, Xhat):\n\t\t"""""" Computation of the first derivative of Itakura-Saito function. As appears in :\n\t\t\tCedric Fevotte and Jerome Idier, ""Algorithms for nonnegative matrix factorization\n\t\t\twith the beta-divergence"", in CoRR, vol. abs/1010.1763, 2010.\n\t\tArgs:\n\t\t\tmX    :   \t(2D ndarray) Input Magnitude Spectrogram\n\t\t\tXhat  :     (2D ndarray) Estimated Magnitude Spectrogram\n\t\tReturns:\n\t\t\tdis\'  :     (float) Average of first derivative of Itakura-Saito distance.\n\t\t""""""\n\t\tdis = (np.abs(Xhat + self._eps) ** (-2.)) * (np.abs(Xhat) - np.abs(self._mX) ** self._alpha)\n\t\treturn (np.mean(dis))\n\n\nif __name__ == ""__main__"":\n\n\t# Small test\n\tkSin = (0.5 * np.cos(np.arange(4096) * (1000.0 * (3.1415926 * 2.0) / 44100)))\n\tnoise = (np.random.uniform(-0.25,0.25,4096))\n\t# Noisy observation\n\tobs = (kSin + noise)\n\n\tkSinX = fft(kSin, 4096)\n\tnoisX = fft(noise, 4096)\n\tobsX  = fft(obs, 4096)\n\n\t# Wiener Case\n\tmask = FrequencyMasking(np.abs(obsX), np.abs(kSinX), [np.abs(noisX)], [], [], alpha = 2., method = \'alphaWiener\')\n\tsinhat = mask()\n\tnoisehat = mask(reverse = True)\n\t# Access the mask if needed\n\tndmask = mask._mask'"
helpers/nnet_helpers.py,0,"b'# -*- coding: utf-8 -*-\n__author__ = \'S.I. Mimilakis\'\n__copyright__ = \'MacSeNet\'\n\n# imports\nfrom helpers.io_methods import AudioIO as Io\nfrom helpers.masking_methods import FrequencyMasking as Fm\nfrom mir_eval import separation as bss_eval\nfrom numpy.lib import stride_tricks\nfrom helpers import iterative_inference as it_infer\nfrom helpers import tf_methods as tf\nimport pickle as pickle\nimport numpy as np\nimport os\n\n# definitions\nmixtures_path = \'DSD100/Mixtures/\'\nsources_path = \'DSD100/Sources/\'\nkeywords = [\'bass.wav\', \'drums.wav\', \'other.wav\', \'vocals.wav\', \'mixture.wav\']\nfoldersList = [\'Dev\', \'Test\']\nsave_path = \'results/GRU_sskip_filt/inference_m3_i10plus/\'\n\n__all__ = [\n    \'prepare_overlap_sequences\',\n    \'get_data\',\n    \'test_eval\',\n    \'test_nnet\'\n]\n\n\ndef prepare_overlap_sequences(ms, vs, bk, l_size, o_lap, bsize):\n    """"""\n        Method to prepare overlapping sequences of the given magnitude spectra.\n        Args:\n            ms               : (2D Array)  Mixture magnitude spectra (Time frames times Frequency sub-bands).\n            vs               : (2D Array)  Singing voice magnitude spectra (Time frames times Frequency sub-bands).\n            bk               : (2D Array)  Background magnitude spectra (Time frames times Frequency sub-bands).\n            l_size           : (int)       Length of the time-sequence.\n            o_lap            : (int)       Overlap between spectrogram time-sequences\n                                           (to recover the missing information from the context information).\n            bsize            : (int)       Batch size.\n\n        Returns:\n            ms               : (3D Array)  Mixture magnitude spectra training data\n                                           reshaped into overlapping sequences.\n            vs               : (3D Array)  Singing voice magnitude spectra training data\n                                           reshaped into overlapping sequences.\n            bk               : (3D Array)  Background magnitude spectra training data\n                                           reshaped into overlapping sequences.\n\n    """"""\n    trim_frame = ms.shape[0] % (l_size - o_lap)\n    trim_frame -= (l_size - o_lap)\n    trim_frame = np.abs(trim_frame)\n    # Zero-padding\n    if trim_frame != 0:\n        ms = np.pad(ms, ((0, trim_frame), (0, 0)), \'constant\', constant_values=(0, 0))\n        vs = np.pad(vs, ((0, trim_frame), (0, 0)), \'constant\', constant_values=(0, 0))\n        bk = np.pad(bk, ((0, trim_frame), (0, 0)), \'constant\', constant_values=(0, 0))\n\n    # Reshaping with overlap\n    ms = stride_tricks.as_strided(ms, shape=(ms.shape[0] / (l_size - o_lap), l_size, ms.shape[1]),\n                                  strides=(ms.strides[0] * (l_size - o_lap), ms.strides[0], ms.strides[1]))\n    ms = ms[:-1, :, :]\n\n    vs = stride_tricks.as_strided(vs, shape=(vs.shape[0] / (l_size - o_lap), l_size, vs.shape[1]),\n                                  strides=(vs.strides[0] * (l_size - o_lap), vs.strides[0], vs.strides[1]))\n    vs = vs[:-1, :, :]\n\n    bk = stride_tricks.as_strided(bk, shape=(bk.shape[0] / (l_size - o_lap), l_size, bk.shape[1]),\n                                  strides=(bk.strides[0] * (l_size - o_lap), bk.strides[0], bk.strides[1]))\n    bk = bk[:-1, :, :]\n\n    b_trim_frame = (ms.shape[0] % bsize)\n    if b_trim_frame != 0:\n        ms = ms[:-b_trim_frame, :, :]\n        vs = vs[:-b_trim_frame, :, :]\n        bk = bk[:-b_trim_frame, :, :]\n\n    return ms, vs, bk\n\n\ndef get_data(current_set, set_size, wsz=2049, N=4096, hop=384, T=100, L=20, B=16):\n    """"""\n        Method to acquire training data. The STFT analysis is included.\n        Args:\n            current_set      : (int)       An integer denoting the current training set.\n            set_size         : (int)       The amount of files a set has.\n            wsz              : (int)       Window size in samples.\n            N                : (int)       The FFT size.\n            hop              : (int)       Hop size in samples.\n            T                : (int)       Length of the time-sequence.\n            L                : (int)       Number of context frames from the time-sequence.\n            B                : (int)       Batch size.\n\n        Returns:\n            ms_train        :  (3D Array)  Mixture magnitude training data, for the current set.\n            vs_train        :  (3D Array)  Singing voice magnitude training data, for the current set.\n\n    """"""\n\n    # Generate full paths for dev and test\n    dev_mixtures_list = sorted(os.listdir(mixtures_path + foldersList[0]))\n    dev_mixtures_list = [mixtures_path + foldersList[0] + \'/\' + i for i in dev_mixtures_list]\n    dev_sources_list = sorted(os.listdir(sources_path + foldersList[0]))\n    dev_sources_list = [sources_path + foldersList[0] + \'/\' + i for i in dev_sources_list]\n\n    # Current lists for training\n    c_train_slist = dev_sources_list[(current_set - 1) * set_size: current_set * set_size]\n    c_train_mlist = dev_mixtures_list[(current_set - 1) * set_size: current_set * set_size]\n\n    for index in range(len(c_train_mlist)):\n\n        # print(\'Reading:\' + c_train_mlist[index])\n\n        # Reading\n        vox, _ = Io.wavRead(os.path.join(c_train_slist[index], keywords[3]), mono=False)\n        mix, _ = Io.wavRead(os.path.join(c_train_mlist[index], keywords[4]), mono=False)\n\n        # STFT Analysing\n        ms_seg, _ = tf.TimeFrequencyDecomposition.STFT(0.5*np.sum(mix, axis=-1), tf.hamming(wsz, True), N, hop)\n        vs_seg, _ = tf.TimeFrequencyDecomposition.STFT(0.5*np.sum(vox, axis=-1), tf.hamming(wsz, True), N, hop)\n\n        # Remove null frames\n        ms_seg = ms_seg[3:-3, :]\n        vs_seg = vs_seg[3:-3, :]\n\n        # Stack some spectrograms and fit\n        if index == 0:\n            ms_train = ms_seg\n            vs_train = vs_seg\n        else:\n            ms_train = np.vstack((ms_train, ms_seg))\n            vs_train = np.vstack((vs_train, vs_seg))\n\n    # Data preprocessing\n    # Freeing up some memory\n    ms_seg = None\n    vs_seg = None\n\n    # Learning the filtering process\n    mask = Fm(ms_train, vs_train, ms_train, [], [], alpha=1., method=\'IRM\')\n    vs_train = mask()\n    vs_train *= 2.\n    vs_train = np.clip(vs_train, a_min=0., a_max=1.)\n    ms_train = np.clip(ms_train, a_min=0., a_max=1.)\n    mask = None\n    ms_train, vs_train, _ = prepare_overlap_sequences(ms_train, vs_train, ms_train, T, L*2, B)\n\n    return ms_train, vs_train\n\n\ndef test_eval(nnet, B, T, N, L, wsz, hop):\n    """"""\n        Method to test the model on the test data. Writes the outcomes in "".wav"" format and.\n        stores them under the defined results path. Optionally, it performs BSS-Eval using\n        MIREval python toolbox (Used only for comparison to BSSEval Matlab implementation).\n        The evaluation results are stored under the defined save path.\n        Args:\n            nnet             : (List)      A list containing the Pytorch modules of the skip-filtering model.\n            B                : (int)       Batch size.\n            T                : (int)       Length of the time-sequence.\n            N                : (int)       The FFT size.\n            L                : (int)       Number of context frames from the time-sequence.\n            wsz              : (int)       Window size in samples.\n            hop              : (int)       Hop size in samples.\n    """"""\n    nnet[0].eval()\n    nnet[1].eval()\n    nnet[2].eval()\n    nnet[3].eval()\n\n    def my_res(mx, vx, L, wsz):\n        """"""\n            A helper function to reshape data according\n            to the context frame.\n        """"""\n        mx = np.ascontiguousarray(mx[:, L:-L, :], dtype=np.float32)\n        mx.shape = (mx.shape[0]*mx.shape[1], wsz)\n        vx = np.ascontiguousarray(vx[:, L:-L, :], dtype=np.float32)\n        vx.shape = (vx.shape[0]*vx.shape[1], wsz)\n\n        return mx, vx\n\n    # Paths for loading and storing the test-set\n    # Generate full paths for test\n    test_sources_list = sorted(os.listdir(sources_path + foldersList[1]))\n    test_sources_list = [sources_path + foldersList[1] + \'/\' + i for i in test_sources_list]\n\n    # Initializing the containers of the metrics\n    sdr = []\n    sir = []\n    sar = []\n\n    for indx in xrange(len(test_sources_list)):\n        print(\'Reading:\' + test_sources_list[indx])\n        # Reading\n        bass, _ = Io.wavRead(os.path.join(test_sources_list[indx], keywords[0]), mono=False)\n        drums, _ = Io.wavRead(os.path.join(test_sources_list[indx], keywords[1]), mono=False)\n        oth, _ = Io.wavRead(os.path.join(test_sources_list[indx], keywords[2]), mono=False)\n        vox, _ = Io.wavRead(os.path.join(test_sources_list[indx], keywords[3]), mono=False)\n\n        bk_true = np.sum(bass + drums + oth, axis=-1) * 0.5\n        mix = np.sum(bass + drums + oth + vox, axis=-1) * 0.5\n        sv_true = np.sum(vox, axis=-1) * 0.5\n\n        # STFT Analysing\n        mx, px = tf.TimeFrequencyDecomposition.STFT(mix, tf.hamming(wsz, True), N, hop)\n\n        # Data reshaping (magnitude and phase)\n        mx, px, _ = prepare_overlap_sequences(mx, px, px, T, 2*L, B)\n\n        # The actual ""denoising"" part\n        vx_hat = np.zeros((mx.shape[0], T-L*2, wsz), dtype=np.float32)\n\n        for batch in xrange(mx.shape[0]/B):\n            H_enc = nnet[0](mx[batch * B: (batch+1)*B, :, :])\n\n            H_j_dec = it_infer.iterative_recurrent_inference(nnet[1], H_enc,\n                                                             criterion=None, tol=1e-3, max_iter=10)\n\n            vs_hat, mask = nnet[2](H_j_dec, mx[batch * B: (batch+1)*B, :, :])\n            y_out = nnet[3](vs_hat)\n            vx_hat[batch * B: (batch+1)*B, :, :] = y_out.data.cpu().numpy()\n\n        # Final reshaping\n        vx_hat.shape = (vx_hat.shape[0]*vx_hat.shape[1], wsz)\n        mx, px = my_res(mx, px, L, wsz)\n\n        # Time-domain recovery\n        # Iterative G-L algorithm\n        for GLiter in range(10):\n            sv_hat = tf.TimeFrequencyDecomposition.iSTFT(vx_hat, px, wsz, hop, True)\n            _, px = tf.TimeFrequencyDecomposition.STFT(sv_hat, tf.hamming(wsz, True), N, hop)\n\n        # Removing the samples that no estimation exists\n        mix = mix[L*hop:]\n        sv_true = sv_true[L*hop:]\n        bk_true = bk_true[L*hop:]\n\n        # Background music estimation\n        if len(sv_true) > len(sv_hat):\n            bk_hat = mix[:len(sv_hat)] - sv_hat\n        else:\n            bk_hat = mix - sv_hat[:len(mix)]\n\n        # Disk writing for external BSS_eval using DSD100-tools (used in our paper)\n        Io.wavWrite(sv_true, 44100, 16, os.path.join(save_path, \'tf_true_sv_\' + str(indx) + \'.wav\'))\n        Io.wavWrite(bk_true, 44100, 16, os.path.join(save_path, \'tf_true_bk_\' + str(indx) + \'.wav\'))\n        Io.wavWrite(sv_hat, 44100, 16, os.path.join(save_path, \'tf_hat_sv_\' + str(indx) + \'.wav\'))\n        Io.wavWrite(bk_hat, 44100, 16, os.path.join(save_path, \'tf_hat_bk_\' + str(indx) + \'.wav\'))\n        Io.wavWrite(mix, 44100, 16, os.path.join(save_path, \'tf_mix_\' + str(indx) + \'.wav\'))\n\n        # Internal BSSEval using librosa (just for comparison)\n        if len(sv_true) > len(sv_hat):\n            c_sdr, _, c_sir, c_sar, _ = bss_eval.bss_eval_images_framewise([sv_true[:len(sv_hat)], bk_true[:len(sv_hat)]],\n                                                                           [sv_hat, bk_hat])\n        else:\n            c_sdr, _, c_sir, c_sar, _ = bss_eval.bss_eval_images_framewise([sv_true, bk_true],\n                                                                           [sv_hat[:len(sv_true)], bk_hat[:len(sv_true)]])\n\n        sdr.append(c_sdr)\n        sir.append(c_sir)\n        sar.append(c_sar)\n\n        # Storing the results iteratively\n        pickle.dump(sdr, open(os.path.join(save_path, \'SDR.p\'), \'wb\'))\n        pickle.dump(sir, open(os.path.join(save_path, \'SIR.p\'), \'wb\'))\n        pickle.dump(sar, open(os.path.join(save_path, \'SAR.p\'), \'wb\'))\n\n    return None\n\n\ndef test_nnet(nnet, seqlen=100, olap=40, wsz=2049, N=4096, hop=384, B=16):\n    """"""\n        Method to test the model on some data. Writes the outcomes in "".wav"" format and.\n        stores them under the defined results path.\n        Args:\n            nnet             : (List)      A list containing the Pytorch modules of the skip-filtering model.\n            seqlen           : (int)       Length of the time-sequence.\n            olap             : (int)       Overlap between spectrogram time-sequences\n                                           (to recover the missing information from the context information).\n            wsz              : (int)       Window size in samples.\n            N                : (int)       The FFT size.\n            hop              : (int)       Hop size in samples.\n            B                : (int)       Batch size.\n    """"""\n    nnet[0].eval()\n    nnet[1].eval()\n    nnet[2].eval()\n    nnet[3].eval()\n    L = olap/2\n    w = tf.hamming(wsz, True)\n    x, fs = Io.wavRead(\'results/test_files/test.wav\', mono=True)\n\n    mx, px = tf.TimeFrequencyDecomposition.STFT(x, w, N, hop)\n    mx, px, _ = prepare_overlap_sequences(mx, px, mx, seqlen, olap, B)\n    vs_out = np.zeros((mx.shape[0], seqlen-olap, wsz), dtype=np.float32)\n\n    for batch in xrange(mx.shape[0]/B):\n        # Mixture to Singing voice\n        H_enc = nnet[0](mx[batch * B: (batch+1)*B, :, :])\n        H_j_dec = it_infer.iterative_recurrent_inference(nnet[1], H_enc,\n                                                         criterion=None, tol=1e-3, max_iter=10)\n\n        vs_hat, mask = nnet[2](H_j_dec, mx[batch * B: (batch+1)*B, :, :])\n        y_out = nnet[3](vs_hat)\n        vs_out[batch * B: (batch+1)*B, :, :] = y_out.data.cpu().numpy()\n\n    vs_out.shape = (vs_out.shape[0]*vs_out.shape[1], wsz)\n\n    if olap == 1:\n        mx = np.ascontiguousarray(mx, dtype=np.float32)\n        px = np.ascontiguousarray(px, dtype=np.float32)\n    else:\n        mx = np.ascontiguousarray(mx[:, olap/2:-olap/2, :], dtype=np.float32)\n        px = np.ascontiguousarray(px[:, olap/2:-olap/2, :], dtype=np.float32)\n\n    mx.shape = (mx.shape[0]*mx.shape[1], wsz)\n    px.shape = (px.shape[0]*px.shape[1], wsz)\n\n    # Approximated sources\n    # Iterative G-L algorithm\n    for GLiter in range(10):\n        y_recb = tf.TimeFrequencyDecomposition.iSTFT(vs_out, px, wsz, hop, True)\n        _, px = tf.TimeFrequencyDecomposition.STFT(y_recb, tf.hamming(wsz, True), N, hop)\n\n    x = x[olap/2 * hop:]\n\n    Io.wavWrite(y_recb, fs, 16, \'results/test_files/test_sv.wav\')\n    Io.wavWrite(x[:len(y_recb)], fs, 16, \'results/test_files/test_mix.wav\')\n\n    return None\n\n# EOF\n'"
helpers/tf_methods.py,0,"b'# -*- coding: utf-8 -*-\n__author__ = \'S.I. Mimilakis\'\n__copyright__ = \'MacSeNet\'\n\n# imports\nimport math\nimport numpy as np\nfrom scipy.fftpack import fft, ifft\nfrom scipy.signal import hamming\n\n# definition\neps = np.finfo(np.float32).tiny\n\n\nclass TimeFrequencyDecomposition:\n    """""" A Class that performs time-frequency decompositions by means of a\n        Discrete Fourier Transform, using Fast Fourier Transform algorithm\n        by SciPy, MDCT with modified type IV bases, PQMF,\n        and Fractional Fast Fourier Transform.\n    """"""\n\n    @staticmethod\n    def DFT(x, w, N):\n        """""" Discrete Fourier Transformation(Analysis) of a given real input signal\n        via an FFT implementation from scipy. Single channel is being supported.\n        Args:\n            x       : (array) Real time domain input signal\n            w       : (array) Desired windowing function\n            N       : (int)   FFT size\n        Returns:\n            magX    : (2D ndarray) Magnitude Spectrum\n            phsX    : (2D ndarray) Phase Spectrum\n        """"""\n\n        # Half spectrum size containing DC component\n        hlfN = (N/2)+1\n\n        # Half window size. Two parameters to perform zero-phase windowing technique\n        hw1 = int(math.floor((w.size+1)/2))\n        hw2 = int(math.floor(w.size/2))\n\n        # Window the input signal\n        winx = x*w\n\n        # Initialize FFT buffer with zeros and perform zero-phase windowing\n        fftbuffer = np.zeros(N)\n        fftbuffer[:hw1] = winx[hw2:]\n        fftbuffer[-hw2:] = winx[:hw2]\n\n        # Compute DFT via scipy\'s FFT implementation\n        X = fft(fftbuffer)\n\n        # Acquire magnitude and phase spectrum\n        magX = (np.abs(X[:hlfN]))\n        phsX = (np.angle(X[:hlfN]))\n\n        return magX, phsX\n\n    @staticmethod\n    def iDFT(magX, phsX, wsz):\n        """""" Discrete Fourier Transformation(Synthesis) of a given spectral analysis\n        via an inverse FFT implementation from scipy.\n        Args:\n            magX    : (2D ndarray) Magnitude Spectrum\n            phsX    : (2D ndarray) Phase Spectrum\n            wsz     :  (int)   Synthesis window size\n        Returns:\n            y       : (array) Real time domain output signal\n        """"""\n\n        # Get FFT Size\n        hlfN = magX.size\n        N = (hlfN-1)*2\n\n        # Half of window size parameters\n        hw1 = int(math.floor((wsz+1)/2))\n        hw2 = int(math.floor(wsz/2))\n\n        # Initialise output spectrum with zeros\n        Y = np.zeros(N, dtype=complex)\n        # Initialise output array with zeros\n        y = np.zeros(wsz)\n\n        # Compute complex spectrum(both sides) in two steps\n        Y[0:hlfN] = magX * np.exp(1j*phsX)\n        Y[hlfN:] = magX[-2:0:-1] * np.exp(-1j*phsX[-2:0:-1])\n\n        # Perform the iDFT\n        fftbuffer = np.real(ifft(Y))\n\n        # Roll-back the zero-phase windowing technique\n        y[:hw2] = fftbuffer[-hw2:]\n        y[hw2:] = fftbuffer[:hw1]\n\n        return y\n\n    @staticmethod\n    def STFT(x, w, N, hop):\n        """""" Short Time Fourier Transform analysis of a given real input signal,\n        via the above DFT method.\n        Args:\n            x   : \t(array)  Time-domain signal\n            w   :   (array)  Desired windowing function\n            N   :   (int)    FFT size\n            hop :   (int)    Hop size\n        Returns:\n            sMx :   (2D ndarray) Stacked arrays of magnitude spectra\n            sPx :   (2D ndarray) Stacked arrays of phase spectra\n        """"""\n\n        # Analysis Parameters\n        wsz = w.size\n\n        # Add some zeros at the start and end of the signal to avoid window smearing\n        x = np.append(np.zeros(3*hop),x)\n        x = np.append(x, np.zeros(3*hop))\n\n        # Initialize sound pointers\n        pin = 0\n        pend = x.size - wsz\n        indx = 0\n\n        # Normalise windowing function\n        if np.sum(w) != 0.:\n            w = w / np.sqrt(N)\n\n        # Initialize storing matrix\n        xmX = np.zeros((len(x)/hop, N/2 + 1), dtype = np.float32)\n        xpX = np.zeros((len(x)/hop, N/2 + 1), dtype = np.float32)\n\n        # Analysis Loop\n        while pin <= pend:\n            # Acquire Segment\n            xSeg = x[pin:pin+wsz]\n\n            # Perform DFT on segment\n            mcX, pcX = TimeFrequencyDecomposition.DFT(xSeg, w, N)\n\n            xmX[indx, :] = mcX\n            xpX[indx, :] = pcX\n\n            # Update pointers and indices\n            pin += hop\n            indx += 1\n\n        return xmX, xpX\n\n    @staticmethod\n    def GLA(wsz, hop, N=4096):\n        """""" LSEE-MSTFT algorithm for computing the synthesis window used in\n        inverse STFT method below.\n        Args:\n            wsz :   (int)    Synthesis window size\n            hop :   (int)    Hop size\n            N   :   (int)    DFT Size\n        Returns :\n            symw:   (array)  Synthesised windowing function\n\n        References :\n            [1] Daniel W. Griffin and Jae S. Lim, ``Signal estimation from modified short-time\n            Fourier transform,\'\' IEEE Transactions on Acoustics, Speech and Signal Processing,\n            vol. 32, no. 2, pp. 236-243, Apr 1984.\n        """"""\n        synw = hamming(wsz)/np.sqrt(N)\n        synwProd = synw ** 2.\n        synwProd.shape = (wsz, 1)\n        redundancy = wsz/hop\n        env = np.zeros((wsz, 1))\n        for k in xrange(-redundancy, redundancy + 1):\n            envInd = (hop*k)\n            winInd = np.arange(1, wsz+1)\n            envInd += winInd\n\n            valid = np.where((envInd > 0) & (envInd <= wsz))\n            envInd = envInd[valid] - 1\n            winInd = winInd[valid] - 1\n            env[envInd] += synwProd[winInd]\n\n        synw = synw/env[:, 0]\n        return synw\n\n    @staticmethod\n    def iSTFT(xmX, xpX, wsz, hop, smt=False) :\n        """""" Short Time Fourier Transform synthesis of given magnitude and phase spectra,\n        via the above iDFT method.\n        Args:\n            xmX :   (2D ndarray)  Magnitude spectrum\n            xpX :   (2D ndarray)  Phase spectrum\n            wsz :   (int)         Synthesis window size\n            hop :   (int)         Hop size\n            smt :   (bool)        Whether or not use a post-processing step in time domain\n                                  signal recovery, using synthesis windows.\n        Returns :\n            y   :   (array)       Synthesised time-domain real signal.\n        """"""\n\n        # GL-Algorithm or simple OLA\n        if smt == True:\n            rs = TimeFrequencyDecomposition.GLA(wsz, hop, (wsz - 1)*2.)\n        else:\n            rs = np.sqrt(2. * (wsz-1))\n\n        # Acquire half window sizes\n        hw1 = int(math.floor((wsz+1)/2))\n        hw2 = int(math.floor(wsz/2))\n\n        # Acquire the number of STFT frames\n        numFr = xmX.shape[0]\n\n        # Initialise output array with zeros\n        y = np.zeros(numFr * hop + hw1 + hw2)\n\n        # Initialise sound pointer\n        pin = 0\n\n        # Main Synthesis Loop\n        for indx in range(numFr):\n            # Inverse Discrete Fourier Transform\n            ybuffer = TimeFrequencyDecomposition.iDFT(xmX[indx, :], xpX[indx, :], wsz)\n\n            # Overlap and Add\n            y[pin:pin+wsz] += ybuffer*rs\n\n            # Advance pointer\n            pin += hop\n\n        # Delete the extra zeros that the analysis had placed\n        y = np.delete(y, range(3*hop))\n        y = np.delete(y, range(y.size-(3*hop + 1), y.size))\n\n        return y\n\n    @staticmethod\n    def MCSTFT(x, w, N, hop):\n        """""" Short Time Fourier Transform analysis of a given real input signal,\n        over multiple channels.\n        Args:\n            x   : \t(2D array)  Multichannel time-domain signal (nsamples x nchannels)\n            w   :   (array)     Desired windowing function\n            N   :   (int)       FFT size\n            hop :   (int)       Hop size\n        Returns:\n            sMx :   (3D ndarray) Stacked arrays of magnitude spectra\n            sPx :   (3D ndarray) Stacked arrays of phase spectra\n                                 Of the shape (Channels x Frequency-samples x Time-frames)\n        """"""\n        M = x.shape[1]      # Number of channels\n\n        # Analyse the first incoming channel to acquire the dimensions\n        mX, pX = TimeFrequencyDecomposition.STFT(x[:, 0], w, N, hop)\n        smX = np.zeros((M, mX.shape[1], mX.shape[0]), dtype = np.float32)\n        spX = np.zeros((M, pX.shape[1], pX.shape[0]), dtype = np.float32)\n        # Storing it to the actual return and free up some memory\n        smX[0, :, :] = mX.T\n        spX[0, :, :] = pX.T\n        del mX, pX\n\n        for channel in xrange(1, M):\n            mX, pX = TimeFrequencyDecomposition.STFT(x[:, channel], w, N, hop)\n            smX[channel, :, :] = mX.T\n            spX[channel, :, :] = pX.T\n\n        del mX, pX\n\n        return smX, spX\n\n    @staticmethod\n    def MCiSTFT(xmX, xpX, wsz, hop, smt=False):\n        """""" Short Time Fourier Transform synthesis of given magnitude and phase spectra\n        over multiple channels.\n        Args:\n            xMx :   (3D ndarray) Stacked arrays of magnitude spectra\n            xPx :   (3D ndarray) Stacked arrays of phase spectra\n                                 Of the shape (Channels x Frequency samples x Time-frames)\n            wsz :   (int)        Synthesis Window size\n            hop :   (int)        Hop size\n            smt :   (bool)       Whether or not use a post-processing step in time domain\n                                 signal recovery, using synthesis windows\n        Returns :\n            y   :   (2D array)   Synthesised time-domain real signal of the shape (nsamples x nchannels)\n        """"""\n        M = xmX.shape[0]      # Number of channels\n        F = xmX.shape[1]      # Number of frequency samples\n        T = xmX.shape[2]      # Number of time-frames\n\n        # Synthesize the first incoming channel to acquire the dimensions\n        y = TimeFrequencyDecomposition.iSTFT(xmX[0, :, :].T, xpX[0, :, :].T, wsz, hop, smt)\n        yout = np.zeros((len(y), M), dtype = np.float32)\n        # Storing it to the actual return and free up some memory\n        yout[:, 0] = y\n        del y\n\n        for channel in xrange(1, M):\n            y = TimeFrequencyDecomposition.iSTFT(xmX[channel, :, :].T, xpX[channel, :, :].T, wsz, hop, smt)\n            yout[:, channel] = y\n\n        del y\n\n        return yout'"
helpers/visualize.py,0,"b""# -*- coding: utf-8 -*-\n__author__ = 'S.I. Mimilakis'\n__copyright__ = 'MacSeNet'\n\n# imports\nfrom visdom import Visdom\nimport numpy as np\n\nviz = Visdom()\n\n\ndef init_visdom():\n    viz.close()\n    window = viz.line(X=np.arange(0, 1),\n                      Y=np.reshape(0, 1),\n                      opts=dict(\n                      fillarea=True,\n                      legend=False,\n                      width=660,\n                      height=660,\n                      xlabel='Number of weight updates',\n                      ylabel='Divergence Metric',\n                      ytype='lin',\n                      title='Kullback-Leibler Loss',\n                      marginleft=0,\n                      marginright=0,\n                      marginbottom=0,\n                      margintop=0,)\n                      )\n\n    windowB = viz.line(X=np.arange(0, 1),\n                  Y=np.reshape(0, 1),\n                  opts=dict(\n                  fillarea=True,\n                  legend=False,\n                  width=660,\n                  height=660,\n                  xlabel='Number of weight updates',\n                  ytype='lin',\n                  title='Sparsity Term Monitoring',\n                  marginleft=0,\n                  marginright=0,\n                  marginbottom=0,\n                  margintop=0,)\n                  )\n\n    return window, windowB\n\n# EOF\n"""
losses/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# EOF\n'
losses/loss_functions.py,3,"b""# -*- coding: utf-8 -*-\n__author__ = 'S.I. Mimilakis'\n__copyright__ = 'MacSeNet'\n\n# imports\nimport torch\n\n\ndef kullback_leibler(x, x_hat):\n    # Generalized KL\n    rec = torch.sum(x * (torch.log(x + 1e-6) - torch.log(x_hat + 1e-6)) + (x_hat - x), dim=-1)\n    return torch.mean(rec)\n\n\ndef mse(x, x_hat):\n    return torch.mean(torch.pow(x - x_hat, 2.))\n\n# EOF\n"""
modules/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# EOF\n'
modules/cls_mlp.py,4,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# imports\nfrom torch import nn\n\n__docformat__ = \'reStructuredText\'\n\n__all__ = [\n    \'MLP\'\n]\n\n\ndef make_to_list(x, length=1):\n    """"""\n\n    Makes a list of `x` argument.\n\n    :param x: The argument to make list of.\n    :type x: list|int|str|float|double\n    :param length: How long will be the list.\n    :type length: int\n    :return: A list from `x` if `x` was not a list in the first place.\n    :rtype: list[int|str|float|double]\n    """"""\n    to_return = [x] if type(x) not in [list, tuple] else x\n    if len(to_return) == 1 and len(to_return) < length:\n        to_return = to_return * length\n    return to_return\n\n\nclass MLP(nn.Module):\n\n    def __init__(self, initial_input_dim, output_dims, activations, dropouts, use_dropout=True,\n                 use_bias=True, bias_value=0, weight_init_function=nn.init.xavier_normal,\n                 my_name=\'linear_layer\'):\n        """"""\n\n        A class for making an MLP.\n\n        :param initial_input_dim: The initial input dimensionality to the MLP\n        :type initial_input_dim: int\n        :param output_dims: The output dimensionalities for the MLP\n        :type output_dims: int | list[int]\n        :param activations: The activations to be used for each layer of the\\\n                            MLP. Must be the function or a list of functions. \\\n                            If it is a list, then the length of the list must be\\\n                            equal to the length of the output dimensionalities `output_dims`.\n        :type activations: callable | list[callable]\n        :param dropouts: The dropouts to be used. Can be one dropout (same for all layers) \\\n                         or a list of dropouts, specifying the dropout for each layer. It if\\\n                         is a list, then the length must be equal to the output dimensionalities\\\n                         `output_dims`.\n        :type dropouts: float | list[float]\n        :param use_dropout: A flag to indicate the usage of dropout. Can be a single value \\\n                            (applied to all layers) or a list of values, for each layer \\\n                            specifically. If it is a list, then the length must be equal \\\n                            to the output dimensionalities `output_dims`.\n        :type use_dropout: bool | list[bool]\n        :param use_bias: A flag to indicate the usage of bias. Can be a single bool value or a\\\n                         list. If it is a single value, then this value is used for all layers. \\\n                         If it is a list, then each value is used for the corresponding layer.\n        :type use_bias: bool | list[bool]\n        :param bias_value: The value to be used for bias initialization.\n        :type bias_value: int | float | list[int] | list[float]\n        :param weight_init_function: The function to be used for weight initialization.\n        :type weight_init_function: callable | list[callable]\n        :param my_name: A string to identify the name of each layer. An index will be appended\\\n                        after the name for each layer.\n        :type my_name: str\n        """"""\n        super(MLP, self).__init__()\n\n        self.my_name = my_name\n\n        self.initial_input_dim = initial_input_dim\n\n        if type(output_dims) == int:\n            output_dims = [output_dims]\n\n        if type(output_dims) == tuple:\n            output_dims = list(output_dims)\n\n        self.dims = [self.initial_input_dim] + output_dims\n\n        self.activations = make_to_list(activations, len(self.dims) - 1)\n        self.dropout_values = make_to_list(dropouts, len(self.dims) - 1)\n        self.use_dropout = make_to_list(use_dropout, len(self.dims) - 1)\n        self.use_bias = make_to_list(use_bias, len(self.dims) - 1)\n        self.bias_values = make_to_list(bias_value, len(self.dims) - 1)\n        self.weight_init_functions = make_to_list(weight_init_function, len(self.dims) - 1)\n\n        self.layers = []\n        self.dropouts = []\n\n        for i_dim in range(len(self.dims) - 1):\n            self.layers.append(nn.Linear(\n                in_features=self.dims[i_dim],\n                out_features=self.dims[i_dim + 1],\n                bias=self.use_bias[i_dim]\n            ))\n            if self.use_dropout[i_dim]:\n                self.dropouts.append(nn.Dropout(\n                    p=self.dropout_values[i_dim]\n                ))\n                setattr(\n                    self,\n                    \'{the_name}_dropout_{the_index}\'.format(\n                        the_name=self.my_name,\n                        the_index=i_dim\n                    ),\n                    self.dropouts[-1]\n                )\n            else:\n                self.dropouts.append(None)\n\n            setattr(\n                self,\n                \'{the_name}_{the_index}\'.format(\n                    the_name=self.my_name,\n                    the_index=i_dim\n                ),\n                self.layers[-1]\n            )\n\n        self.init_weights_and_biases()\n\n    def init_weights_and_biases(self):\n        for layer, init_function, bias_value, use_bias in zip(\n            self.layers, self.weight_init_functions,\n                self.bias_values, self.use_bias\n        ):\n            init_function(layer.weight.data)\n            if use_bias:\n                nn.init.constant(layer.bias.data, bias_value)\n\n    def forward(self, x):\n        output = self.activations[0](self.layers[0](self.dropouts[0](x)))\n\n        for activation, layer, dropout in zip(\n                self.activations[1:], self.layers[1:], self.dropouts[1:]\n        ):\n            if dropout is not None:\n                output = dropout(output)\n            output = activation(layer(output))\n\n        return output\n\n\ndef main():\n    # Testing of the MLP class\n    import torch\n    from torch.nn import functional\n    from torch import optim\n    from torch.autograd import Variable\n    import time\n\n    total_examples = 10\n    initial_dim = 64\n    output_dims = [64, 128, 256, 128, 64]\n    epochs = 1000\n\n    init_function = nn.init.xavier_uniform\n    bias_values = [0, 1, -1, 0, 0]  # or bias_values = 0\n    use_biases = [True, False, True, True, True]  # or use_biases = True\n    activations = functional.sigmoid\n    the_name = \'Testing MLP\'\n    dropouts = [.4, .6, .1, .4, .5]  # or dropouts = .5\n    use_dropout = [True, True, False, True, True]  # or use_dropout = True\n\n    x = Variable(torch.rand((total_examples, initial_dim)))\n    y = Variable(torch.rand((total_examples, output_dims[-1])))\n\n    mlp = MLP(\n        initial_input_dim=initial_dim,\n        output_dims=output_dims,\n        activations=activations,\n        dropouts=dropouts,\n        use_dropout=use_dropout,\n        use_bias=use_biases,\n        bias_value=bias_values,\n        weight_init_function=init_function,\n        my_name=the_name\n    )\n\n    optimizer = optim.Adam(mlp.parameters())\n\n    for epoch in range(epochs):\n        start_time = time.time()\n        y_hat = mlp(x)\n        optimizer.zero_grad()\n        loss = functional.l1_loss(y_hat, y)\n        loss.backward()\n        optimizer.step()\n\n        print(\'Epoch {:5d} | Loss: {:.6f} | Elapsed time: {:.6f} sec(s)\'.format(\n            epoch,\n            loss.data[0],\n            time.time() - start_time\n        ))\n\nif __name__ == \'__main__\':\n    main()\n\n# EOF\n'"
modules/cls_sparse_skip_filt.py,22,"b'# -*- coding: utf-8 -*-\n__author__ = \'S.I. Mimilakis\'\n__copyright__ = \'MacSeNet\'\n\n# imports\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n\nclass BiGRUEncoder(nn.Module):\n\n    """""" Class that builds skip-filtering\n        connections neural network.\n        Encoder part.\n    """"""\n\n    def __init__(self, B, T, N, F, L):\n        """"""\n        Constructing blocks of the model based\n        on the sparse skip-filtering connections.\n        Args :\n            B      : (int) Batch size\n            T      : (int) Length of the time-sequence.\n            N      : (int) Original dimensionallity of the input.\n            F      : (int) Dimensionallity of the input\n                               (Amount of frequency sub-bands).\n            L      : (int) Length of the half context time-sequence.\n        """"""\n        super(BiGRUEncoder, self).__init__()\n        self._B = B\n        self._T = T\n        self._N = N\n        self._F = F\n        self._L = L\n        self._alpha = 1.\n\n        # Bi-GRU Encoder\n        self.gruEncF = nn.GRUCell(self._F, self._F)\n        self.gruEncB = nn.GRUCell(self._F, self._F)\n\n        # Initialize the weights\n        self.initialize_encoder()\n\n    def initialize_encoder(self):\n        """"""\n            Manual weight/bias initialization.\n        """"""\n        nn.init.orthogonal(self.gruEncF.weight_hh)\n        nn.init.xavier_normal(self.gruEncF.weight_ih)\n        self.gruEncF.bias_hh.data.zero_()\n        self.gruEncF.bias_ih.data.zero_()\n\n        nn.init.orthogonal(self.gruEncB.weight_hh)\n        nn.init.xavier_normal(self.gruEncB.weight_ih)\n        self.gruEncB.bias_hh.data.zero_()\n        self.gruEncB.bias_ih.data.zero_()\n        print(\'Initialization of the encoder done...\')\n\n        return None\n\n    def forward(self, input_x):\n\n        if torch.has_cudnn:\n            # Initialization of the hidden states\n            h_t_fr = Variable(torch.zeros(self._B, self._F).cuda(), requires_grad=False)\n            h_t_bk = Variable(torch.zeros(self._B, self._F).cuda(), requires_grad=False)\n            H_enc = Variable(torch.zeros(self._B, self._T - (2 * self._L), 2 * self._F).cuda(), requires_grad=False)\n\n            # Input is of the shape : (B (batches), T (time-sequence), N(frequency sub-bands))\n            # Cropping some ""un-necessary"" frequency sub-bands\n            cxin = Variable(torch.pow(torch.from_numpy(input_x[:, :, :self._F]).cuda(), self._alpha))\n\n        else:\n            # Initialization of the hidden states\n            h_t_fr = Variable(torch.zeros(self._B, self._F), requires_grad=False)\n            h_t_bk = Variable(torch.zeros(self._B, self._F), requires_grad=False)\n            H_enc = Variable(torch.zeros(self._B, self._T - (2 * self._L), 2 * self._F), requires_grad=False)\n\n            # Input is of the shape : (B (batches), T (time-sequence), N(frequency sub-bands))\n            # Cropping some ""un-necessary"" frequency sub-bands\n            cxin = Variable(torch.pow(torch.from_numpy(input_x[:, :, :self._F]), self._alpha))\n\n        for t in range(self._T):\n            # Bi-GRU Encoding\n            h_t_fr = self.gruEncF((cxin[:, t, :]), h_t_fr)\n            h_t_bk = self.gruEncB((cxin[:, self._T - t - 1, :]), h_t_bk)\n            # Residual connections\n            h_t_fr += cxin[:, t, :]\n            h_t_bk += cxin[:, self._T - t - 1, :]\n\n            # Remove context and concatenate\n            if (t >= self._L) and (t < self._T - self._L):\n                h_t = torch.cat((h_t_fr, h_t_bk), dim=1)\n                H_enc[:, t - self._L, :] = h_t\n\n        return H_enc\n\n\nclass Decoder(nn.Module):\n\n    """""" Class that builds skip-filtering\n        connections neural network.\n        Decoder part.\n    """"""\n\n    def __init__(self, B, T, N, F, L, infr):\n        """"""\n        Constructing blocks of the model based\n        on the sparse skip-filtering connections.\n        Args :\n            B      : (int) Batch size\n            T      : (int) Length of the time-sequence.\n            N      : (int) Original dimensionallity of the input.\n            F      : (int) Dimensionallity of the input\n                           (Amount of frequency sub-bands).\n            L      : (int) Length of the half context time-sequence.\n            infr   : (bool)If the decoder uses recurrent inference or not.\n        """"""\n        super(Decoder, self).__init__()\n        self._B = B\n        self._T = T\n        self._N = N\n        self._F = F\n        self._L = L\n        if infr:\n            self._gruout = 2*self._F\n        else:\n            self._gruout = self._F\n\n        # GRU Decoder\n        self.gruDec = nn.GRUCell(2*self._F, self._gruout)\n\n        # Initialize the weights\n        self.initialize_decoder()\n\n    def initialize_decoder(self):\n        """"""\n            Manual weight/bias initialization.\n        """"""\n        nn.init.orthogonal(self.gruDec.weight_hh)\n        nn.init.xavier_normal(self.gruDec.weight_ih)\n        self.gruDec.bias_hh.data.zero_()\n        self.gruDec.bias_ih.data.zero_()\n\n        print(\'Initialization of the decoder done...\')\n        return None\n\n    def forward(self, H_enc):\n        if torch.has_cudnn:\n            # Initialization of the hidden states\n            h_t_dec = Variable(torch.zeros(self._B, self._gruout).cuda(), requires_grad=False)\n\n            # Initialization of the decoder output\n            H_j_dec = Variable(torch.zeros(self._B, self._T - (self._L * 2), self._gruout).cuda(), requires_grad=False)\n\n        else:\n            # Initialization of the hidden states\n            h_t_dec = Variable(torch.zeros(self._B, self._gruout), requires_grad=False)\n\n            # Initialization of the decoder output\n            H_j_dec = Variable(torch.zeros(self._B, self._T - (self._L * 2), self._gruout), requires_grad=False)\n\n        for ts in range(self._T - (self._L * 2)):\n            # GRU Decoding\n            h_t_dec = self.gruDec(H_enc[:, ts, :], h_t_dec)\n            H_j_dec[:, ts, :] = h_t_dec\n\n        return H_j_dec\n\n\nclass SparseDecoder(nn.Module):\n\n    """""" Class that builds skip-filtering\n        connections neural network.\n        Decoder part.\n    """"""\n\n    def __init__(self, B, T, N, F, L, ifnr=True):\n        """"""\n        Constructing blocks of the model based\n        on the sparse skip-filtering connections.\n        Args :\n            B      : (int) Batch size\n            T      : (int) Length of the time-sequence.\n            N      : (int) Original dimensionallity of the input.\n            F      : (int) Dimensionallity of the input\n                           (Amount of frequency sub-bands).\n            L      : (int) Length of the half context time-sequence.\n            infr   : (bool)If the GRU decoder used recurrent inference or not.\n        """"""\n        super(SparseDecoder, self).__init__()\n        self._B = B\n        self._T = T\n        self._N = N\n        self._F = F\n        self._L = L\n\n        # FF Sparse Decoder\n        if ifnr:\n            self.ffDec = nn.Linear(2*self._F, self._N)\n        else:\n            self.ffDec = nn.Linear(self._F, self._N)\n\n        # Initialize the weights\n        self.initialize_decoder()\n\n        # Additional functions\n        self.relu = nn.ReLU()\n\n    def initialize_decoder(self):\n        """"""\n            Manual weight/bias initialization.\n        """"""\n        nn.init.xavier_normal(self.ffDec.weight)\n        self.ffDec.bias.data.zero_()\n\n        print(\'Initialization of the sparse decoder done...\')\n        return None\n\n    def forward(self, H_j_dec, input_x):\n        if torch.has_cudnn:\n            # Input is of the shape : (B, T, N)\n            input_x = Variable(torch.from_numpy(input_x[:, self._L:-self._L, :]).cuda(), requires_grad=True)\n\n        else:\n            # Input is of the shape : (B, T, N)\n            # Cropping some ""un-necessary"" frequency sub-bands\n            input_x = Variable(torch.from_numpy(input_x[:, self._L:-self._L, :]), requires_grad=True)\n\n        # Decode/Sparsify mask\n        mask_t1 = self.relu(self.ffDec(H_j_dec))\n        # Apply skip-filtering connections\n        Y_j = torch.mul(mask_t1, input_x)\n\n        return Y_j, mask_t1\n\n\nclass SourceEnhancement(nn.Module):\n\n    """""" Class that builds the source enhancement\n        module of the skip-filtering connections\n        neural network. This could be used for\n        recursive inference.\n    """"""\n\n    def __init__(self, B, T, N, F, L):\n        """"""\n        Constructing blocks of the model based\n        on the sparse skip-filtering connections.\n        Args :\n            B            : (int) Batch size\n            T            : (int) Length of the time-sequence.\n            N            : (int) Original dimensionallity of the input.\n            F            : (int) Dimensionallity of the input\n                                 (Amount of frequency sub-bands).\n            L            : (int) Length of the half context time-sequence.\n        """"""\n        super(SourceEnhancement, self).__init__()\n        self._B = B\n        self._T = T\n        self._N = N\n        self._F = F\n        self._L = L\n\n        # FF Source Enhancement Layer\n        self.ffSe_enc = nn.Linear(self._N, self._N/2)\n        self.ffSe_dec = nn.Linear(self._N/2, self._N)\n\n        # Initialize the weights\n        self.initialize_module()\n\n        # Additional functions\n        self.relu = nn.ReLU()\n\n    def initialize_module(self):\n        """"""\n            Manual weight/bias initialization.\n        """"""\n        nn.init.xavier_normal(self.ffSe_dec.weight)\n        self.ffSe_dec.bias.data.zero_()\n        nn.init.xavier_normal(self.ffSe_enc.weight)\n        self.ffSe_enc.bias.data.zero_()\n        print(\'Initialization of the source enhancement module done...\')\n\n        return None\n\n    def forward(self, Y_hat):\n        # Enhance Source\n        mask_enc_hl = self.relu(self.ffSe_enc(Y_hat))\n        mask_t2 = self.relu(self.ffSe_dec(mask_enc_hl))\n        # Apply skip-filtering connections\n        Y_hat_filt = torch.mul(mask_t2, Y_hat)\n\n        return Y_hat_filt\n\n\n# EOF\n'"
processes_scripts/__init__.py,0,b'# -*- coding: utf-8 -*-\n\n# EOF\n'
processes_scripts/main_script.py,23,"b'# -*- coding: utf-8 -*-\n__author__ = \'S.I. Mimilakis\'\n__copyright__ = \'MacSeNet\'\n\n# imports\nimport torch\nimport torch.optim as optim\nimport numpy as np\nfrom tqdm import tqdm\nfrom helpers import visualize, nnet_helpers\nfrom torch.autograd import Variable\nfrom modules import cls_sparse_skip_filt as s_s_net\nfrom losses import loss_functions\nfrom helpers import iterative_inference as it_infer\n\n\ndef main(training, apply_sparsity):\n    """"""\n        The main function to train and test.\n    """"""\n    # Reproducible results\n    np.random.seed(218)\n    torch.manual_seed(218)\n    if torch.has_cudnn:\n        torch.cuda.manual_seed(218)\n        torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\n\n    # Analysis\n    wsz = 2049   # Window-size\n    Ns = 4096    # FFT size\n    hop = 384    # Hop size\n    fs = 44100   # Sampling frequency\n\n    # Parameters\n    B = 16                      # Batch-size\n    T = 60                      # Length of the sequence\n    N = 2049                    # Frequency sub-bands to be processed\n    F = 744                     # Frequency sub-bands for encoding\n    L = 10                      # Context parameter (2*L frames will be removed)\n    epochs = 100                # Epochs\n    init_lr = 1e-4              # Initial learning rate\n    mnorm = 0.5\t                # L2-based norm clipping\n    mask_loss_threshold = 1.5   # Scalar indicating the threshold for the time-frequency masking module\n    good_loss_threshold = 0.25  # Scalar indicating the threshold for the source enhancment module\n\n    # Data (Predifined by the DSD100 dataset and the non-instumental/non-bleeding stems of MedleydB)\n    totTrainFiles = 116\n    numFilesPerTr = 4\n\n    print(\'------------   Building model   ------------\')\n    encoder = s_s_net.BiGRUEncoder(B, T, N, F, L)\n    decoder = s_s_net.Decoder(B, T, N, F, L, infr=True)\n    sp_decoder = s_s_net.SparseDecoder(B, T, N, F, L)\n    source_enhancement = s_s_net.SourceEnhancement(B, T, N, F, L)\n\n    encoder.train(mode=True)\n    decoder.train(mode=True)\n    sp_decoder.train(mode=True)\n    source_enhancement.train(mode=True)\n\n    if torch.has_cudnn:\n        print(\'------------   CUDA Enabled   --------------\')\n        encoder.cuda()\n        decoder.cuda()\n        sp_decoder.cuda()\n        source_enhancement.cuda()\n\n    # Defining objectives\n    rec_criterion = loss_functions.kullback_leibler                 # Reconstruction criterion\n\n    optimizer = optim.Adam(list(encoder.parameters()) +\n                           list(decoder.parameters()) +\n                           list(sp_decoder.parameters()) +\n                           list(source_enhancement.parameters()),\n                           lr=init_lr\n                           )\n\n    if training:\n        win_viz, winb_viz = visualize.init_visdom()\n        batch_loss = []\n        # Over epochs\n        batch_index = 0\n        for epoch in range(epochs):\n            print(\'Epoch: \' + str(epoch+1))\n            epoch_loss = []\n            # Over the set of files\n            for index in range(totTrainFiles/numFilesPerTr):\n                # Get Data\n                ms, vs = nnet_helpers.get_data(index+1, numFilesPerTr, wsz, Ns, hop, T, L, B)\n\n                # Shuffle data\n                shf_indices = np.random.permutation(ms.shape[0])\n                ms = ms[shf_indices]\n                vs = vs[shf_indices]\n\n                # Over batches\n                for batch in tqdm(range(ms.shape[0]/B)):\n                    # Mixture to Singing voice\n                    H_enc = encoder(ms[batch * B: (batch+1)*B, :, :])\n                    # Iterative inference\n                    H_j_dec = it_infer.iterative_recurrent_inference(decoder, H_enc,\n                                                                     criterion=None, tol=1e-3, max_iter=10)\n                    vs_hat_b = sp_decoder(H_j_dec, ms[batch * B: (batch+1)*B, :, :])[0]\n                    vs_hat_b_filt = source_enhancement(vs_hat_b)\n\n                    # Loss\n                    if torch.has_cudnn:\n                        loss = rec_criterion(Variable(torch.from_numpy(vs[batch * B: (batch+1)*B, L:-L, :]).cuda()),\n                                         vs_hat_b_filt)\n\n                        loss_mask = rec_criterion(Variable(torch.from_numpy(vs[batch * B: (batch+1)*B, L:-L, :]).cuda()),\n                                         vs_hat_b)\n\n                        if loss_mask.data[0] >= mask_loss_threshold and loss.data[0] >= good_loss_threshold:\n                            loss += loss_mask\n\n                    else:\n                        loss = rec_criterion(Variable(torch.from_numpy(vs[batch * B: (batch+1)*B, L:-L, :])),\n                                         vs_hat_b_filt)\n\n                        loss_mask = rec_criterion(Variable(torch.from_numpy(vs[batch * B: (batch+1)*B, L:-L, :])),\n                                         vs_hat_b)\n\n                        if loss_mask.data[0] >= mask_loss_threshold and loss.data[0] >= good_loss_threshold:\n                            loss += loss_mask\n\n                    # Store loss for display and scheduler\n                    batch_loss += [loss.data[0]]\n                    epoch_loss += [loss.data[0]]\n\n                    # Sparsity term\n                    if apply_sparsity:\n                        sparsity_penalty = torch.sum(torch.abs(torch.diag(sp_decoder.ffDec.weight.data))) * 1e-2 +\\\n                                           torch.sum(torch.pow(source_enhancement.ffSe_dec.weight, 2.)) * 1e-4\n\n                        loss += sparsity_penalty\n\n                        winb_viz = visualize.viz.line(X=np.arange(batch_index, batch_index+1),\n                             Y=np.reshape(sparsity_penalty.data[0], (1,)),\n                             win=winb_viz, update=\'append\')\n\n                    optimizer.zero_grad()\n\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm(list(encoder.parameters()) +\n                                                  list(decoder.parameters()) +\n                                                  list(sp_decoder.parameters()) +\n                                                  list(source_enhancement.parameters()),\n                                                  max_norm=mnorm, norm_type=2)\n                    optimizer.step()\n                    # Update graphs\n                    win_viz = visualize.viz.line(X=np.arange(batch_index, batch_index+1),\n                                                 Y=np.reshape(batch_loss[batch_index], (1,)),\n                                                 win=win_viz, update=\'append\')\n                    batch_index += 1\n\n            if (epoch+1) % 40 == 0:\n                print(\'------------   Saving model   ------------\')\n                torch.save(encoder.state_dict(), \'results/torch_sps_encoder_\' + str(epoch+1)+\'.pytorch\')\n                torch.save(decoder.state_dict(), \'results/torch_sps_decoder_\' + str(epoch+1)+\'.pytorch\')\n                torch.save(sp_decoder.state_dict(), \'results/torch_sps_sp_decoder_\' + str(epoch+1)+\'.pytorch\')\n                torch.save(source_enhancement.state_dict(), \'results/torch_sps_se_\' + str(epoch+1)+\'.pytorch\')\n                print(\'------------       Done       ------------\')\n    else:\n        print(\'-------  Loading pre-trained model   -------\')\n        print(\'-------  Loading inference weights  -------\')\n        encoder.load_state_dict(torch.load(\'results/results_inference/torch_sps_encoder.pytorch\', map_location={key:\'cpu\' for key in [\'gpu:0\'] if not torch.has_cudnn}))\n        decoder.load_state_dict(torch.load(\'results/results_inference/torch_sps_decoder.pytorch\', map_location={key:\'cpu\' for key in [\'gpu:0\'] if not torch.has_cudnn}))\n        sp_decoder.load_state_dict(torch.load(\'results/results_inference/torch_sps_sp_decoder.pytorch\', map_location={key:\'cpu\' for key in [\'gpu:0\'] if not torch.has_cudnn}))\n        source_enhancement.load_state_dict(torch.load(\'results/results_inference/torch_sps_se.pytorch\', map_location={key:\'cpu\' for key in [\'gpu:0\'] if not torch.has_cudnn}))\n        print(\'-------------      Done        -------------\')\n\n    return encoder, decoder, sp_decoder, source_enhancement\n\n\nif __name__ == \'__main__\':\n    training = False         # Whether to train or test the trained model (requires the optimized parameters)\n    apply_sparsity = True    # Whether to apply a sparse penalty or not\n\n    sfiltnet = main(training, apply_sparsity)\n\n    #print(\'-------------     BSS-Eval     -------------\')\n    #nnet_helpers.test_eval(sfiltnet, 16, 60, 4096, 10, 2049, 384)\n    #print(\'-------------       Done       -------------\')\n    print(\'-------------     DNN-Test     -------------\')\n    nnet_helpers.test_nnet(sfiltnet, 60, 10*2, 2049, 4096, 384, 16)\n    print(\'-------------       Done       -------------\')\n\n# EOF\n'"
results/results_inference/__init__.py,0,"b""# -*- coding: utf-8 -*-\n__author__ = 'S.I. Mimilakis'\n__copyright__ = 'MacSeNet'\n\n\n# EOF\n"""
