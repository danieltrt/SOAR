file_path,api_count,code
demo.py,4,"b'import argparse\nimport torch\nimport torch.nn.parallel\n\nfrom models import modules, net, resnet, densenet, senet\nimport numpy as np\nimport loaddata_demo as loaddata\nimport pdb\n\nimport matplotlib.image\nimport matplotlib.pyplot as plt\nplt.set_cmap(""jet"")\n\n\ndef define_model(is_resnet, is_densenet, is_senet):\n    if is_resnet:\n        original_model = resnet.resnet50(pretrained = True)\n        Encoder = modules.E_resnet(original_model) \n        model = net.model(Encoder, num_features=2048, block_channel = [256, 512, 1024, 2048])\n    if is_densenet:\n        original_model = densenet.densenet161(pretrained=True)\n        Encoder = modules.E_densenet(original_model)\n        model = net.model(Encoder, num_features=2208, block_channel = [192, 384, 1056, 2208])\n    if is_senet:\n        original_model = senet.senet154(pretrained=\'imagenet\')\n        Encoder = modules.E_senet(original_model)\n        model = net.model(Encoder, num_features=2048, block_channel = [256, 512, 1024, 2048])\n\n    return model\n   \n\ndef main():\n    model = define_model(is_resnet=False, is_densenet=False, is_senet=True)\n    model = torch.nn.DataParallel(model).cuda()\n    model.load_state_dict(torch.load(\'./pretrained_model/model_senet\'))\n    model.eval()\n\n    nyu2_loader = loaddata.readNyu2(\'data/demo/img_nyu2.png\')\n  \n    test(nyu2_loader, model)\n\n\ndef test(nyu2_loader, model):\n    for i, image in enumerate(nyu2_loader):     \n        image = torch.autograd.Variable(image, volatile=True).cuda()\n        out = model(image)\n        \n        matplotlib.image.imsave(\'data/demo/out.png\', out.view(out.size(2),out.size(3)).data.cpu().numpy())\n\nif __name__ == \'__main__\':\n    main()\n'"
demo_transform.py,7,"b'\nimport torch\nfrom PIL import Image\nimport numpy as np\n\ntry:\n    import accimage\nexcept ImportError:\n    accimage = None\n\n\n\n\ndef _is_pil_image(img):\n    if accimage is not None:\n        return isinstance(img, (Image.Image, accimage.Image))\n    else:\n        return isinstance(img, Image.Image)\n\ndef _is_numpy_image(img):\n    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n\nclass Scale(object):\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, image):\n        image = self.changeScale(image,self.size)\n      \n        return image\n\n    def changeScale(self, img, size, interpolation=Image.BILINEAR):\n        ow, oh = size     \n\n        return img.resize((ow, oh), interpolation)\n  \nclass CenterCrop(object):\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, image):\n        image = self.centerCrop(image,self.size)\n\n        return image\n\n    def centerCrop(self,image, size):       \n        w1, h1 = image.size\n        tw, th = size\n\n        if w1 == tw and h1 == th:\n            return image\n\n        x1 = int(round((w1 - tw) / 2.))\n        y1 = int(round((h1 - th) / 2.))\n       \n        image = image.crop((x1, y1, tw+x1, th+y1))\n\n        return image\n\n\nclass ToTensor(object):\n    """"""Convert a ``PIL.Image`` or ``numpy.ndarray`` to tensor.\n    Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n    """"""\n\n    def __call__(self, image):       \n        image = self.to_tensor(image)\n       \n        return image\n     \n\n    def to_tensor(self,pic):\n        if not(_is_pil_image(pic) or _is_numpy_image(pic)):\n            raise TypeError(\'pic should be PIL Image or ndarray. Got {}\'.format(type(pic)))\n        \n        if isinstance(pic, np.ndarray):\n           \n            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n            return img.float().div(255)\n            \n\n        if accimage is not None and isinstance(pic, accimage.Image):\n            nppic = np.zeros([pic.channels, pic.height, pic.width], dtype=np.float32)\n            pic.copyto(nppic)\n            return torch.from_numpy(nppic)\n\n        # handle PIL Image\n        if pic.mode == \'I\':\n            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n        elif pic.mode == \'I;16\':\n            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n        else:\n            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n        if pic.mode == \'YCbCr\':\n            nchannel = 3\n        elif pic.mode == \'I;16\':\n            nchannel = 1\n        else:\n            nchannel = len(pic.mode)\n        img = img.view(pic.size[1], pic.size[0], nchannel)\n        # put it from HWC to CHW format\n        # yikes, this transpose takes 80% of the loading time/CPU\n        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n        if isinstance(img, torch.ByteTensor):\n            return img.float().div(255)\n        else:\n            return img\n\n\n\nclass Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, image):\n        image = self.normalize(image, self.mean, self.std)\n\n        return image\n\n    def normalize(self, tensor, mean, std):\n        for t, m, s in zip(tensor, mean, std):\n            t.sub_(m).div_(s)\n            \n        return tensor\n'"
loaddata.py,3,"b'import pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nfrom PIL import Image\nimport random\nfrom nyu_transform import *\n\n\nclass depthDataset(Dataset):\n    """"""Face Landmarks dataset.""""""\n\n    def __init__(self, csv_file, transform=None):\n        self.frame = pd.read_csv(csv_file, header=None)\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        image_name = self.frame.ix[idx, 0]\n        depth_name = self.frame.ix[idx, 1]\n\n        image = Image.open(image_name)\n        depth = Image.open(depth_name)\n\n        sample = {\'image\': image, \'depth\': depth}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n    def __len__(self):\n        return len(self.frame)\n\n\ndef getTrainingData(batch_size=64):\n    __imagenet_pca = {\n        \'eigval\': torch.Tensor([0.2175, 0.0188, 0.0045]),\n        \'eigvec\': torch.Tensor([\n            [-0.5675,  0.7192,  0.4009],\n            [-0.5808, -0.0045, -0.8140],\n            [-0.5836, -0.6948,  0.4203],\n        ])\n    }\n    __imagenet_stats = {\'mean\': [0.485, 0.456, 0.406],\n                        \'std\': [0.229, 0.224, 0.225]}\n\n    transformed_training = depthDataset(csv_file=\'./data/nyu2_train.csv\',\n                                        transform=transforms.Compose([\n                                            Scale(240),\n                                            RandomHorizontalFlip(),\n                                            RandomRotate(5),\n                                            CenterCrop([304, 228], [152, 114]),\n                                            ToTensor(),\n                                            Lighting(0.1, __imagenet_pca[\n                                                \'eigval\'], __imagenet_pca[\'eigvec\']),\n                                            ColorJitter(\n                                                brightness=0.4,\n                                                contrast=0.4,\n                                                saturation=0.4,\n                                            ),\n                                            Normalize(__imagenet_stats[\'mean\'],\n                                                      __imagenet_stats[\'std\'])\n                                        ]))\n\n    dataloader_training = DataLoader(transformed_training, batch_size,\n                                     shuffle=True, num_workers=4, pin_memory=False)\n\n    return dataloader_training\n\n\ndef getTestingData(batch_size=64):\n\n    __imagenet_stats = {\'mean\': [0.485, 0.456, 0.406],\n                        \'std\': [0.229, 0.224, 0.225]}\n    # scale = random.uniform(1, 1.5)\n    transformed_testing = depthDataset(csv_file=\'./data/nyu2_test.csv\',\n                                       transform=transforms.Compose([\n                                           Scale(240),\n                                           CenterCrop([304, 228], [304, 228]),\n                                           ToTensor(is_test=True),\n                                           Normalize(__imagenet_stats[\'mean\'],\n                                                     __imagenet_stats[\'std\'])\n                                       ]))\n\n    dataloader_testing = DataLoader(transformed_testing, batch_size,\n                                    shuffle=False, num_workers=0, pin_memory=False)\n\n    return dataloader_testing\n'"
loaddata_demo.py,1,"b""import pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nfrom PIL import Image\nimport random\nfrom demo_transform import *\n\n\n\nclass depthDataset(Dataset):\n    def __init__(self, filename, transform=None):\n        self.frame = filename\n        self.transform = transform\n\n    def __getitem__(self,idx):       \n        image = Image.open(self.frame)\n        \n        if self.transform:\n            image = self.transform(image)\n\n        return image\n\n    def __len__(self):\n        return int(1)\n     \n\n\ndef readNyu2(filename):\n    __imagenet_stats = {'mean': [0.485, 0.456, 0.406],\n                        'std': [0.229, 0.224, 0.225]}\n\n    image_trans = depthDataset(filename,\n                        transform=transforms.Compose([\n                        Scale([320, 240]),\n                        CenterCrop([304, 228]),\n                        ToTensor(),                                \n                        Normalize(__imagenet_stats['mean'],\n                                 __imagenet_stats['std'])\n                       ]))\n\n    image = DataLoader(image_trans, batch_size=1, shuffle=False, num_workers=0, pin_memory=False)\n\n\n    return image\n"""
nyu_transform.py,9,"b'\nimport torch\nimport numpy as np\nfrom PIL import Image, ImageOps\nimport collections\ntry:\n    import accimage\nexcept ImportError:\n    accimage = None\nimport random\nimport scipy.ndimage as ndimage\n\nimport pdb\n\n\ndef _is_pil_image(img):\n    if accimage is not None:\n        return isinstance(img, (Image.Image, accimage.Image))\n    else:\n        return isinstance(img, Image.Image)\n\n\ndef _is_numpy_image(img):\n    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n\n\nclass RandomRotate(object):\n    """"""Random rotation of the image from -angle to angle (in degrees)\n    This is useful for dataAugmentation, especially for geometric problems such as FlowEstimation\n    angle: max angle of the rotation\n    interpolation order: Default: 2 (bilinear)\n    reshape: Default: false. If set to true, image size will be set to keep every pixel in the image.\n    diff_angle: Default: 0. Must stay less than 10 degrees, or linear approximation of flowmap will be off.\n    """"""\n\n    def __init__(self, angle, diff_angle=0, order=2, reshape=False):\n        self.angle = angle\n        self.reshape = reshape\n        self.order = order\n\n    def __call__(self, sample):\n        image, depth = sample[\'image\'], sample[\'depth\']\n\n        applied_angle = random.uniform(-self.angle, self.angle)\n        angle1 = applied_angle\n        angle1_rad = angle1 * np.pi / 180\n\n        image = ndimage.interpolation.rotate(\n            image, angle1, reshape=self.reshape, order=self.order)\n        depth = ndimage.interpolation.rotate(\n            depth, angle1, reshape=self.reshape, order=self.order)\n\n        image = Image.fromarray(image)\n        depth = Image.fromarray(depth)\n\n        return {\'image\': image, \'depth\': depth}\n\nclass RandomHorizontalFlip(object):\n\n    def __call__(self, sample):\n        image, depth = sample[\'image\'], sample[\'depth\']\n\n        if not _is_pil_image(image):\n            raise TypeError(\n                \'img should be PIL Image. Got {}\'.format(type(img)))\n        if not _is_pil_image(depth):\n            raise TypeError(\n                \'img should be PIL Image. Got {}\'.format(type(depth)))\n\n        if random.random() < 0.5:\n            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n            depth = depth.transpose(Image.FLIP_LEFT_RIGHT)\n\n        return {\'image\': image, \'depth\': depth}\n\n\nclass Scale(object):\n    """""" Rescales the inputs and target arrays to the given \'size\'.\n    \'size\' will be the size of the smaller edge.\n    For example, if height > width, then image will be\n    rescaled to (size * height / width, size)\n    size: size of the smaller edge\n    interpolation order: Default: 2 (bilinear)\n    """"""\n\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, sample):\n        image, depth = sample[\'image\'], sample[\'depth\']\n\n        image = self.changeScale(image, self.size)\n        depth = self.changeScale(depth, self.size,Image.NEAREST)\n \n        return {\'image\': image, \'depth\': depth}\n\n    def changeScale(self, img, size, interpolation=Image.BILINEAR):\n\n        if not _is_pil_image(img):\n            raise TypeError(\n                \'img should be PIL Image. Got {}\'.format(type(img)))\n        if not (isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)):\n            raise TypeError(\'Got inappropriate size arg: {}\'.format(size))\n\n        if isinstance(size, int):\n            w, h = img.size\n            if (w <= h and w == size) or (h <= w and h == size):\n                return img\n            if w < h:\n                ow = size\n                oh = int(size * h / w)\n                return img.resize((ow, oh), interpolation)\n            else:\n                oh = size\n                ow = int(size * w / h)\n                return img.resize((ow, oh), interpolation)\n        else:\n            return img.resize(size[::-1], interpolation)\n\n\nclass CenterCrop(object):\n    def __init__(self, size_image, size_depth):\n        self.size_image = size_image\n        self.size_depth = size_depth\n\n    def __call__(self, sample):\n        image, depth = sample[\'image\'], sample[\'depth\']\n\n        image = self.centerCrop(image, self.size_image)\n        depth = self.centerCrop(depth, self.size_image)\n\n        ow, oh = self.size_depth\n        depth = depth.resize((ow, oh))\n\n        return {\'image\': image, \'depth\': depth}\n\n    def centerCrop(self, image, size):\n\n        w1, h1 = image.size\n\n        tw, th = size\n\n        if w1 == tw and h1 == th:\n            return image\n\n        x1 = int(round((w1 - tw) / 2.))\n        y1 = int(round((h1 - th) / 2.))\n\n        image = image.crop((x1, y1, tw + x1, th + y1))\n\n        return image\n\n\nclass ToTensor(object):\n    """"""Convert a ``PIL.Image`` or ``numpy.ndarray`` to tensor.\n    Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n    """"""\n    def __init__(self,is_test=False):\n        self.is_test = is_test\n\n    def __call__(self, sample):\n        image, depth = sample[\'image\'], sample[\'depth\']\n        """"""\n        Args:\n            pic (PIL.Image or numpy.ndarray): Image to be converted to tensor.\n        Returns:\n            Tensor: Converted image.\n        """"""\n        # ground truth depth of training samples is stored in 8-bit while test samples are saved in 16 bit\n        image = self.to_tensor(image)\n        if self.is_test:\n            depth = self.to_tensor(depth).float()/1000\n        else:            \n            depth = self.to_tensor(depth).float()*10\n        return {\'image\': image, \'depth\': depth}\n\n    def to_tensor(self, pic):\n        if not(_is_pil_image(pic) or _is_numpy_image(pic)):\n            raise TypeError(\n                \'pic should be PIL Image or ndarray. Got {}\'.format(type(pic)))\n\n        if isinstance(pic, np.ndarray):\n            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n\n            return img.float().div(255)\n\n        if accimage is not None and isinstance(pic, accimage.Image):\n            nppic = np.zeros(\n                [pic.channels, pic.height, pic.width], dtype=np.float32)\n            pic.copyto(nppic)\n            return torch.from_numpy(nppic)\n\n        # handle PIL Image\n        if pic.mode == \'I\':\n            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n        elif pic.mode == \'I;16\':\n            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n        else:\n            img = torch.ByteTensor(\n                torch.ByteStorage.from_buffer(pic.tobytes()))\n        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n        if pic.mode == \'YCbCr\':\n            nchannel = 3\n        elif pic.mode == \'I;16\':\n            nchannel = 1\n        else:\n            nchannel = len(pic.mode)\n        img = img.view(pic.size[1], pic.size[0], nchannel)\n        # put it from HWC to CHW format\n        # yikes, this transpose takes 80% of the loading time/CPU\n        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n        if isinstance(img, torch.ByteTensor):\n            return img.float().div(255)\n        else:\n            return img\n\n\nclass Lighting(object):\n\n    def __init__(self, alphastd, eigval, eigvec):\n        self.alphastd = alphastd\n        self.eigval = eigval\n        self.eigvec = eigvec\n\n    def __call__(self, sample):\n        image, depth = sample[\'image\'], sample[\'depth\']\n        if self.alphastd == 0:\n            return image\n\n        alpha = image.new().resize_(3).normal_(0, self.alphastd)\n        rgb = self.eigvec.type_as(image).clone()\\\n            .mul(alpha.view(1, 3).expand(3, 3))\\\n            .mul(self.eigval.view(1, 3).expand(3, 3))\\\n            .sum(1).squeeze()\n\n        image = image.add(rgb.view(3, 1, 1).expand_as(image))\n\n        return {\'image\': image, \'depth\': depth}\n\n\nclass Grayscale(object):\n\n    def __call__(self, img):\n        gs = img.clone()\n        gs[0].mul_(0.299).add_(0.587, gs[1]).add_(0.114, gs[2])\n        gs[1].copy_(gs[0])\n        gs[2].copy_(gs[0])\n        return gs\n\n\nclass Saturation(object):\n\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        gs = Grayscale()(img)\n        alpha = random.uniform(-self.var, self.var)\n        return img.lerp(gs, alpha)\n\n\nclass Brightness(object):\n\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        gs = img.new().resize_as_(img).zero_()\n        alpha = random.uniform(-self.var, self.var)\n\n        return img.lerp(gs, alpha)\n\n\nclass Contrast(object):\n\n    def __init__(self, var):\n        self.var = var\n\n    def __call__(self, img):\n        gs = Grayscale()(img)\n        gs.fill_(gs.mean())\n        alpha = random.uniform(-self.var, self.var)\n        return img.lerp(gs, alpha)\n\n\nclass RandomOrder(object):\n    """""" Composes several transforms together in random order.\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, sample):\n        image, depth = sample[\'image\'], sample[\'depth\']\n\n        if self.transforms is None:\n            return {\'image\': image, \'depth\': depth}\n        order = torch.randperm(len(self.transforms))\n        for i in order:\n            image = self.transforms[i](image)\n\n        return {\'image\': image, \'depth\': depth}\n\n\nclass ColorJitter(RandomOrder):\n\n    def __init__(self, brightness=0.4, contrast=0.4, saturation=0.4):\n        self.transforms = []\n        if brightness != 0:\n            self.transforms.append(Brightness(brightness))\n        if contrast != 0:\n            self.transforms.append(Contrast(contrast))\n        if saturation != 0:\n            self.transforms.append(Saturation(saturation))\n\n\nclass Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, sample):\n        """"""\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        Returns:\n            Tensor: Normalized image.\n        """"""\n        image, depth = sample[\'image\'], sample[\'depth\']\n\n        image = self.normalize(image, self.mean, self.std)\n\n        return {\'image\': image, \'depth\': depth}\n\n    def normalize(self, tensor, mean, std):\n        """"""Normalize a tensor image with mean and standard deviation.\n        See ``Normalize`` for more details.\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n            mean (sequence): Sequence of means for R, G, B channels respecitvely.\n            std (sequence): Sequence of standard deviations for R, G, B channels\n                respecitvely.\n        Returns:\n            Tensor: Normalized image.\n        """"""\n\n        # TODO: make efficient\n        for t, m, s in zip(tensor, mean, std):\n            t.sub_(m).div_(s)\n        return tensor\n'"
sobel.py,2,"b'import torch\nimport torch.nn as nn\nimport numpy as np\n\nclass Sobel(nn.Module):\n    def __init__(self):\n        super(Sobel, self).__init__()\n        self.edge_conv = nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=1, bias=False)\n        edge_kx = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n        edge_ky = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n        edge_k = np.stack((edge_kx, edge_ky))\n\n        edge_k = torch.from_numpy(edge_k).float().view(2, 1, 3, 3)\n        self.edge_conv.weight = nn.Parameter(edge_k)\n        \n        for param in self.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        out = self.edge_conv(x) \n        out = out.contiguous().view(-1, 2, x.size(2), x.size(3))\n  \n        return out\n\n'"
test.py,11,"b""import argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\n\nfrom models import modules, net, resnet, densenet, senet\nimport loaddata\nimport util\nimport numpy as np\nimport sobel\n\n\ndef main():\n    model = define_model(is_resnet=False, is_densenet=False, is_senet=True)\n    model = torch.nn.DataParallel(model).cuda()\n    model.load_state_dict(torch.load('./pretrained_model/model_senet'))\n\n    test_loader = loaddata.getTestingData(1)\n    test(test_loader, model, 0.25)\n\n\ndef test(test_loader, model, thre):\n    model.eval()\n\n    totalNumber = 0\n\n    Ae = 0\n    Pe = 0\n    Re = 0\n    Fe = 0\n\n    errorSum = {'MSE': 0, 'RMSE': 0, 'ABS_REL': 0, 'LG10': 0,\n                'MAE': 0,  'DELTA1': 0, 'DELTA2': 0, 'DELTA3': 0}\n\n    for i, sample_batched in enumerate(test_loader):\n        image, depth = sample_batched['image'], sample_batched['depth']\n\n        depth = depth.cuda(async=True)\n        image = image.cuda()\n\n        image = torch.autograd.Variable(image, volatile=True)\n        depth = torch.autograd.Variable(depth, volatile=True)\n \n        output = model(image)\n        output = torch.nn.functional.upsample(output, size=[depth.size(2),depth.size(3)], mode='bilinear')\n\n        depth_edge = edge_detection(depth)\n        output_edge = edge_detection(output)\n\n        batchSize = depth.size(0)\n        totalNumber = totalNumber + batchSize\n        errors = util.evaluateError(output, depth)\n        errorSum = util.addErrors(errorSum, errors, batchSize)\n        averageError = util.averageErrors(errorSum, totalNumber)\n\n        edge1_valid = (depth_edge > thre)\n        edge2_valid = (output_edge > thre)\n\n        nvalid = np.sum(torch.eq(edge1_valid, edge2_valid).float().data.cpu().numpy())\n        A = nvalid / (depth.size(2)*depth.size(3))\n\n        nvalid2 = np.sum(((edge1_valid + edge2_valid) ==2).float().data.cpu().numpy())\n        P = nvalid2 / (np.sum(edge2_valid.data.cpu().numpy()))\n        R = nvalid2 / (np.sum(edge1_valid.data.cpu().numpy()))\n\n        F = (2 * P * R) / (P + R)\n\n        Ae += A\n        Pe += P\n        Re += R\n        Fe += F\n\n    Av = Ae / totalNumber\n    Pv = Pe / totalNumber\n    Rv = Re / totalNumber\n    Fv = Fe / totalNumber\n    print('PV', Pv)\n    print('RV', Rv)\n    print('FV', Fv)\n\n    averageError['RMSE'] = np.sqrt(averageError['MSE'])\n    print(averageError)\n\ndef define_model(is_resnet, is_densenet, is_senet):\n    if is_resnet:\n        original_model = resnet.resnet50(pretrained = True)\n        Encoder = modules.E_resnet(original_model) \n        model = net.model(Encoder, num_features=2048, block_channel = [256, 512, 1024, 2048])\n    if is_densenet:\n        original_model = densenet.densenet161(pretrained=True)\n        Encoder = modules.E_densenet(original_model)\n        model = net.model(Encoder, num_features=2208, block_channel = [192, 384, 1056, 2208])\n    if is_senet:\n        original_model = senet.senet154(pretrained='imagenet')\n        Encoder = modules.E_senet(original_model)\n        model = net.model(Encoder, num_features=2048, block_channel = [256, 512, 1024, 2048])\n\n    return model\n   \n\ndef edge_detection(depth):\n    get_edge = sobel.Sobel().cuda()\n\n    edge_xy = get_edge(depth)\n    edge_sobel = torch.pow(edge_xy[:, 0, :, :], 2) + \\\n        torch.pow(edge_xy[:, 1, :, :], 2)\n    edge_sobel = torch.sqrt(edge_sobel)\n\n    return edge_sobel\n\n\nif __name__ == '__main__':\n    main()\n"""
train.py,20,"b""import argparse\n\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport loaddata\nimport util\nimport numpy as np\nimport sobel\nfrom models import modules, net, resnet, densenet, senet\n\nparser = argparse.ArgumentParser(description='PyTorch DenseNet Training')\nparser.add_argument('--epochs', default=20, type=int,\n                    help='number of total epochs to run')\nparser.add_argument('--start-epoch', default=0, type=int,\n                    help='manual epoch number (useful on restarts)')\nparser.add_argument('--lr', '--learning-rate', default=0.0001, type=float,\n                    help='initial learning rate')\nparser.add_argument('--momentum', default=0.9, type=float, help='momentum')\nparser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n                    help='weight decay (default: 1e-4)')\n\n\ndef define_model(is_resnet, is_densenet, is_senet):\n    if is_resnet:\n        original_model = resnet.resnet50(pretrained = True)\n        Encoder = modules.E_resnet(original_model) \n        model = net.model(Encoder, num_features=2048, block_channel = [256, 512, 1024, 2048])\n    if is_densenet:\n        original_model = densenet.densenet161(pretrained=True)\n        Encoder = modules.E_densenet(original_model)\n        model = net.model(Encoder, num_features=2208, block_channel = [192, 384, 1056, 2208])\n    if is_senet:\n        original_model = senet.senet154(pretrained='imagenet')\n        Encoder = modules.E_senet(original_model)\n        model = net.model(Encoder, num_features=2048, block_channel = [256, 512, 1024, 2048])\n\n    return model\n   \n\ndef main():\n    global args\n    args = parser.parse_args()\n    model = define_model(is_resnet=False, is_densenet=False, is_senet=True)\n \n    if torch.cuda.device_count() == 8:\n        model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3, 4, 5, 6, 7]).cuda()\n        batch_size = 64\n    elif torch.cuda.device_count() == 4:\n        model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3]).cuda()\n        batch_size = 32\n    else:\n        model = model.cuda()\n        batch_size = 8\n\n    cudnn.benchmark = True\n    optimizer = torch.optim.Adam(model.parameters(), args.lr, weight_decay=args.weight_decay)\n\n    train_loader = loaddata.getTrainingData(batch_size)\n\n    for epoch in range(args.start_epoch, args.epochs):\n        adjust_learning_rate(optimizer, epoch)\n        train(train_loader, model, optimizer, epoch)\n    \n    save_checkpoint({'state_dict': model.state_dict()})\n\n\ndef train(train_loader, model, optimizer, epoch):\n    criterion = nn.L1Loss()\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n\n    model.train()\n\n    cos = nn.CosineSimilarity(dim=1, eps=0)\n    get_gradient = sobel.Sobel().cuda()\n\n    end = time.time()\n    for i, sample_batched in enumerate(train_loader):\n        image, depth = sample_batched['image'], sample_batched['depth']\n\n        depth = depth.cuda(async=True)\n        image = image.cuda()\n        image = torch.autograd.Variable(image)\n        depth = torch.autograd.Variable(depth)\n\n        ones = torch.ones(depth.size(0), 1, depth.size(2),depth.size(3)).float().cuda()\n        ones = torch.autograd.Variable(ones)\n        optimizer.zero_grad()\n\n        output = model(image)\n\n        depth_grad = get_gradient(depth)\n        output_grad = get_gradient(output)\n        depth_grad_dx = depth_grad[:, 0, :, :].contiguous().view_as(depth)\n        depth_grad_dy = depth_grad[:, 1, :, :].contiguous().view_as(depth)\n        output_grad_dx = output_grad[:, 0, :, :].contiguous().view_as(depth)\n        output_grad_dy = output_grad[:, 1, :, :].contiguous().view_as(depth)\n\n        depth_normal = torch.cat((-depth_grad_dx, -depth_grad_dy, ones), 1)\n        output_normal = torch.cat((-output_grad_dx, -output_grad_dy, ones), 1)\n\n        # depth_normal = F.normalize(depth_normal, p=2, dim=1)\n        # output_normal = F.normalize(output_normal, p=2, dim=1)\n\n        loss_depth = torch.log(torch.abs(output - depth) + 0.5).mean()\n        loss_dx = torch.log(torch.abs(output_grad_dx - depth_grad_dx) + 0.5).mean()\n        loss_dy = torch.log(torch.abs(output_grad_dy - depth_grad_dy) + 0.5).mean()\n        loss_normal = torch.abs(1 - cos(output_normal, depth_normal)).mean()\n\n        loss = loss_depth + loss_normal + (loss_dx + loss_dy)\n\n        losses.update(loss.data[0], image.size(0))\n        loss.backward()\n        optimizer.step()\n\n        batch_time.update(time.time() - end)\n        end = time.time()\n   \n        batchSize = depth.size(0)\n\n        print('Epoch: [{0}][{1}/{2}]\\t'\n          'Time {batch_time.val:.3f} ({batch_time.sum:.3f})\\t'\n          'Loss {loss.val:.4f} ({loss.avg:.4f})'\n          .format(epoch, i, len(train_loader), batch_time=batch_time, loss=losses))\n \n\ndef adjust_learning_rate(optimizer, epoch):\n    lr = args.lr * (0.1 ** (epoch // 5))\n\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef save_checkpoint(state, filename='checkpoint.pth.tar'):\n    torch.save(state, filename)\n\n\nif __name__ == '__main__':\n    main()\n"""
util.py,20,"b""#original script: https://github.com/fangchangma/sparse-to-dense/blob/master/utils.lua\n\t\nimport torch\nimport math\nimport numpy as np\n\ndef lg10(x):\n    return torch.div(torch.log(x), math.log(10))\n\ndef maxOfTwo(x, y):\n    z = x.clone()\n    maskYLarger = torch.lt(x, y)\n    z[maskYLarger.detach()] = y[maskYLarger.detach()]\n    return z\n\ndef nValid(x):\n    return torch.sum(torch.eq(x, x).float())\n\ndef nNanElement(x):\n    return torch.sum(torch.ne(x, x).float())\n\ndef getNanMask(x):\n    return torch.ne(x, x)\n\ndef setNanToZero(input, target):\n    nanMask = getNanMask(target)\n    nValidElement = nValid(target)\n\n    _input = input.clone()\n    _target = target.clone()\n\n    _input[nanMask] = 0\n    _target[nanMask] = 0\n\n    return _input, _target, nanMask, nValidElement\n\n\ndef evaluateError(output, target):\n    errors = {'MSE': 0, 'RMSE': 0, 'ABS_REL': 0, 'LG10': 0,\n              'MAE': 0,  'DELTA1': 0, 'DELTA2': 0, 'DELTA3': 0}\n\n    _output, _target, nanMask, nValidElement = setNanToZero(output, target)\n\n    if (nValidElement.data.cpu().numpy() > 0):\n        diffMatrix = torch.abs(_output - _target)\n\n        errors['MSE'] = torch.sum(torch.pow(diffMatrix, 2)) / nValidElement\n\n        errors['MAE'] = torch.sum(diffMatrix) / nValidElement\n\n        realMatrix = torch.div(diffMatrix, _target)\n        realMatrix[nanMask] = 0\n        errors['ABS_REL'] = torch.sum(realMatrix) / nValidElement\n\n        LG10Matrix = torch.abs(lg10(_output) - lg10(_target))\n        LG10Matrix[nanMask] = 0\n        errors['LG10'] = torch.sum(LG10Matrix) / nValidElement\n\n        yOverZ = torch.div(_output, _target)\n        zOverY = torch.div(_target, _output)\n\n        maxRatio = maxOfTwo(yOverZ, zOverY)\n\n        errors['DELTA1'] = torch.sum(\n            torch.le(maxRatio, 1.25).float()) / nValidElement\n        errors['DELTA2'] = torch.sum(\n            torch.le(maxRatio, math.pow(1.25, 2)).float()) / nValidElement\n        errors['DELTA3'] = torch.sum(\n            torch.le(maxRatio, math.pow(1.25, 3)).float()) / nValidElement\n\n        errors['MSE'] = float(errors['MSE'].data.cpu().numpy())\n        errors['ABS_REL'] = float(errors['ABS_REL'].data.cpu().numpy())\n        errors['LG10'] = float(errors['LG10'].data.cpu().numpy())\n        errors['MAE'] = float(errors['MAE'].data.cpu().numpy())\n        errors['DELTA1'] = float(errors['DELTA1'].data.cpu().numpy())\n        errors['DELTA2'] = float(errors['DELTA2'].data.cpu().numpy())\n        errors['DELTA3'] = float(errors['DELTA3'].data.cpu().numpy())\n\n    return errors\n\n\ndef addErrors(errorSum, errors, batchSize):\n    errorSum['MSE']=errorSum['MSE'] + errors['MSE'] * batchSize\n    errorSum['ABS_REL']=errorSum['ABS_REL'] + errors['ABS_REL'] * batchSize\n    errorSum['LG10']=errorSum['LG10'] + errors['LG10'] * batchSize\n    errorSum['MAE']=errorSum['MAE'] + errors['MAE'] * batchSize\n\n    errorSum['DELTA1']=errorSum['DELTA1'] + errors['DELTA1'] * batchSize\n    errorSum['DELTA2']=errorSum['DELTA2'] + errors['DELTA2'] * batchSize\n    errorSum['DELTA3']=errorSum['DELTA3'] + errors['DELTA3'] * batchSize\n\n    return errorSum\n\n\ndef averageErrors(errorSum, N):\n    averageError={'MSE': 0, 'RMSE': 0, 'ABS_REL': 0, 'LG10': 0,\n                    'MAE': 0,  'DELTA1': 0, 'DELTA2': 0, 'DELTA3': 0}\n\n    averageError['MSE'] = errorSum['MSE'] / N\n    averageError['ABS_REL'] = errorSum['ABS_REL'] / N\n    averageError['LG10'] = errorSum['LG10'] / N\n    averageError['MAE'] = errorSum['MAE'] / N\n\n    averageError['DELTA1'] = errorSum['DELTA1'] / N\n    averageError['DELTA2'] = errorSum['DELTA2'] / N\n    averageError['DELTA3'] = errorSum['DELTA3'] / N\n\n    return averageError\n\n\n\n\n\n\t\n"""
models/densenet.py,9,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom collections import OrderedDict\nimport pdb\nimport copy\nfrom torchvision import utils\nimport numpy as np\n\n__all__ = [\'DenseNet\', \'densenet121\',\n           \'densenet169\', \'densenet201\', \'densenet161\']\n\n\nmodel_urls = {\n    \'densenet121\': \'https://download.pytorch.org/models/densenet121-a639ec97.pth\',\n    \'densenet169\': \'https://download.pytorch.org/models/densenet169-b2777c0a.pth\',\n    \'densenet201\': \'https://download.pytorch.org/models/densenet201-c1103571.pth\',\n    \'densenet161\': \'https://download.pytorch.org/models/densenet161-8d451a50.pth\',\n}\n\n\n\ndef densenet161(pretrained=False, **kwargs):\n    r""""""Densenet-161 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = DenseNet(num_init_features=96, growth_rate=48, block_config=(6, 12, 36, 24),\n                     **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'densenet161\'], \'pretrained_model/encoder\'))\n    return model\n\n\nclass _DenseLayer(nn.Sequential):\n\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module(\'norm.1\', nn.BatchNorm2d(num_input_features)),\n        self.add_module(\'relu.1\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv.1\', nn.Conv2d(num_input_features, bn_size *\n                                            growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module(\'norm.2\', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module(\'relu.2\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv.2\', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                                            kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(\n                new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i *\n                                growth_rate, growth_rate, bn_size, drop_rate)\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n\nclass _Transition(nn.Sequential):\n\n    def __init__(self, num_input_features, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module(\'norm\', nn.BatchNorm2d(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\'conv\', nn.Conv2d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        self.add_module(\'pool\', nn.AvgPool2d(kernel_size=2, stride=2))\n\n\n\nclass _DenseLayer(nn.Sequential):\n\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module(\'norm.1\', nn.BatchNorm2d(num_input_features)),\n        self.add_module(\'relu.1\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv.1\', nn.Conv2d(num_input_features, bn_size *\n                                            growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module(\'norm.2\', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module(\'relu.2\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv.2\', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                                            kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(\n                new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i *\n                                growth_rate, growth_rate, bn_size, drop_rate)\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n\nclass DenseNet(nn.Module):\n\n    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000):\n\n        super(DenseNet, self).__init__()\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv0\', nn.Conv2d(3, num_init_features,\n                                kernel_size=7, stride=2, padding=3, bias=False)),\n            (\'norm0\', nn.BatchNorm2d(num_init_features)),\n            (\'relu0\', nn.ReLU(inplace=True)),\n            (\'pool0\', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n        ]))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(\n                    num_input_features=num_features, num_output_features=num_features // 2)\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = num_features // 2\n            # print(str(i), num_features)\n\n        # Final batch norm\n        self.features.add_module(\'norm5\', nn.BatchNorm2d(num_features))\n        self.num_features = num_features\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n      \n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, inplace=True)\n        out = F.avg_pool2d(out, kernel_size=7, stride=1).view(\n            features.size(0), -1)\n        out = self.classifier(out)\n        return out, self.num_features\n\n'"
models/modules.py,4,"b""from collections import OrderedDict\nimport math\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.utils import model_zoo\nimport copy\nimport numpy as np\nimport senet\nimport resnet\nimport densenet\n\nclass _UpProjection(nn.Sequential):\n\n    def __init__(self, num_input_features, num_output_features):\n        super(_UpProjection, self).__init__()\n\n        self.conv1 = nn.Conv2d(num_input_features, num_output_features,\n                               kernel_size=5, stride=1, padding=2, bias=False)\n        self.bn1 = nn.BatchNorm2d(num_output_features)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1_2 = nn.Conv2d(num_output_features, num_output_features,\n                                 kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1_2 = nn.BatchNorm2d(num_output_features)\n\n        self.conv2 = nn.Conv2d(num_input_features, num_output_features,\n                               kernel_size=5, stride=1, padding=2, bias=False)\n        self.bn2 = nn.BatchNorm2d(num_output_features)\n\n    def forward(self, x, size):\n        x = F.upsample(x, size=size, mode='bilinear')\n        x_conv1 = self.relu(self.bn1(self.conv1(x)))\n        bran1 = self.bn1_2(self.conv1_2(x_conv1))\n        bran2 = self.bn2(self.conv2(x))\n\n        out = self.relu(bran1 + bran2)\n\n        return out\n\nclass E_resnet(nn.Module):\n\n    def __init__(self, original_model, num_features = 2048):\n        super(E_resnet, self).__init__()        \n        self.conv1 = original_model.conv1\n        self.bn1 = original_model.bn1\n        self.relu = original_model.relu\n        self.maxpool = original_model.maxpool\n\n        self.layer1 = original_model.layer1\n        self.layer2 = original_model.layer2\n        self.layer3 = original_model.layer3\n        self.layer4 = original_model.layer4\n       \n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x_block1 = self.layer1(x)\n        x_block2 = self.layer2(x_block1)\n        x_block3 = self.layer3(x_block2)\n        x_block4 = self.layer4(x_block3)\n\n        return x_block1, x_block2, x_block3, x_block4\n\nclass E_densenet(nn.Module):\n\n    def __init__(self, original_model, num_features = 2208):\n        super(E_densenet, self).__init__()        \n        self.features = original_model.features\n\n    def forward(self, x):\n        x01 = self.features[0](x)\n        x02 = self.features[1](x01)\n        x03 = self.features[2](x02)\n        x04 = self.features[3](x03)\n\n        x_block1 = self.features[4](x04)\n        x_block1 = self.features[5][0](x_block1)\n        x_block1 = self.features[5][1](x_block1)\n        x_block1 = self.features[5][2](x_block1)\n        x_tran1 = self.features[5][3](x_block1)\n\n        x_block2 = self.features[6](x_tran1)\n        x_block2 = self.features[7][0](x_block2)\n        x_block2 = self.features[7][1](x_block2)\n        x_block2 = self.features[7][2](x_block2)\n        x_tran2 = self.features[7][3](x_block2)\n\n        x_block3 = self.features[8](x_tran2)\n        x_block3 = self.features[9][0](x_block3)\n        x_block3 = self.features[9][1](x_block3)\n        x_block3 = self.features[9][2](x_block3)\n        x_tran3 = self.features[9][3](x_block3)\n\n        x_block4 = self.features[10](x_tran3)\n        x_block4 = F.relu(self.features[11](x_block4))\n\n        return x_block1, x_block2, x_block3, x_block4\n\nclass E_senet(nn.Module):\n\n    def __init__(self, original_model, num_features = 2048):\n        super(E_senet, self).__init__()        \n        self.base = nn.Sequential(*list(original_model.children())[:-3])\n\n    def forward(self, x):\n        x = self.base[0](x)\n        x_block1 = self.base[1](x)\n        x_block2 = self.base[2](x_block1)\n        x_block3 = self.base[3](x_block2)\n        x_block4 = self.base[4](x_block3)\n\n        return x_block1, x_block2, x_block3, x_block4\n\nclass D(nn.Module):\n\n    def __init__(self, num_features = 2048):\n        super(D, self).__init__()\n        self.conv = nn.Conv2d(num_features, num_features //\n                               2, kernel_size=1, stride=1, bias=False)\n        num_features = num_features // 2\n        self.bn = nn.BatchNorm2d(num_features)\n\n        self.up1 = _UpProjection(\n            num_input_features=num_features, num_output_features=num_features // 2)\n        num_features = num_features // 2\n\n        self.up2 = _UpProjection(\n            num_input_features=num_features, num_output_features=num_features // 2)\n        num_features = num_features // 2\n\n        self.up3 = _UpProjection(\n            num_input_features=num_features, num_output_features=num_features // 2)\n        num_features = num_features // 2\n\n        self.up4 = _UpProjection(\n            num_input_features=num_features, num_output_features=num_features // 2)\n        num_features = num_features // 2\n\n\n    def forward(self, x_block1, x_block2, x_block3, x_block4):\n        x_d0 = F.relu(self.bn(self.conv(x_block4)))\n        x_d1 = self.up1(x_d0, [x_block3.size(2), x_block3.size(3)])\n        x_d2 = self.up2(x_d1, [x_block2.size(2), x_block2.size(3)])\n        x_d3 = self.up3(x_d2, [x_block1.size(2), x_block1.size(3)])\n        x_d4 = self.up4(x_d3, [x_block1.size(2)*2, x_block1.size(3)*2])\n\n        return x_d4\n\nclass MFF(nn.Module):\n\n    def __init__(self, block_channel, num_features=64):\n\n        super(MFF, self).__init__()\n        \n        self.up1 = _UpProjection(\n            num_input_features=block_channel[0], num_output_features=16)\n        \n        self.up2 = _UpProjection(\n            num_input_features=block_channel[1], num_output_features=16)\n       \n        self.up3 = _UpProjection(\n            num_input_features=block_channel[2], num_output_features=16)\n       \n        self.up4 = _UpProjection(\n            num_input_features=block_channel[3], num_output_features=16)\n\n        self.conv = nn.Conv2d(\n            num_features, num_features, kernel_size=5, stride=1, padding=2, bias=False)\n        self.bn = nn.BatchNorm2d(num_features)\n        \n\n    def forward(self, x_block1, x_block2, x_block3, x_block4, size):\n        x_m1 = self.up1(x_block1, size)\n        x_m2 = self.up2(x_block2, size)\n        x_m3 = self.up3(x_block3, size)\n        x_m4 = self.up4(x_block4, size)\n\n        x = self.bn(self.conv(torch.cat((x_m1, x_m2, x_m3, x_m4), 1)))\n        x = F.relu(x)\n\n        return x\n\n\nclass R(nn.Module):\n    def __init__(self, block_channel):\n\n        super(R, self).__init__()\n        \n        num_features = 64 + block_channel[3]//32\n        self.conv0 = nn.Conv2d(num_features, num_features,\n                               kernel_size=5, stride=1, padding=2, bias=False)\n        self.bn0 = nn.BatchNorm2d(num_features)\n\n        self.conv1 = nn.Conv2d(num_features, num_features,\n                               kernel_size=5, stride=1, padding=2, bias=False)\n        self.bn1 = nn.BatchNorm2d(num_features)\n\n        self.conv2 = nn.Conv2d(\n            num_features, 1, kernel_size=5, stride=1, padding=2, bias=True)\n\n    def forward(self, x):\n        x0 = self.conv0(x)\n        x0 = self.bn0(x0)\n        x0 = F.relu(x0)\n\n        x1 = self.conv1(x0)\n        x1 = self.bn1(x1)\n        x1 = F.relu(x1)\n\n        x2 = self.conv2(x1)\n\n        return x2"""
models/net.py,4,"b'from collections import OrderedDict\nimport math\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.utils import model_zoo\nimport copy\nimport numpy as np\nimport modules\nfrom torchvision import utils\n\nimport senet\nimport resnet\nimport densenet\n\nclass model(nn.Module):\n    def __init__(self, Encoder, num_features, block_channel):\n\n        super(model, self).__init__()\n\n        self.E = Encoder\n        self.D = modules.D(num_features)\n        self.MFF = modules.MFF(block_channel)\n        self.R = modules.R(block_channel)\n\n\n    def forward(self, x):\n        x_block1, x_block2, x_block3, x_block4 = self.E(x)\n        x_decoder = self.D(x_block1, x_block2, x_block3, x_block4)\n        x_mff = self.MFF(x_block1, x_block2, x_block3, x_block4,[x_decoder.size(2),x_decoder.size(3)])\n        out = self.R(torch.cat((x_decoder, x_mff), 1))\n\n        return out\n'"
models/resnet.py,8,"b'import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch.nn.functional as F\nimport torch\nimport numpy as np\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\'], \'pretrained_model/encoder\'))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n'"
models/senet.py,3,"b'""""""\nResNet code gently borrowed from\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n""""""\n\nfrom collections import OrderedDict\nimport math\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.utils import model_zoo\nimport copy\nimport numpy as np\n\n__all__ = [\'SENet\', \'senet154\', \'se_resnet50\', \'se_resnet101\', \'se_resnet152\',\n           \'se_resnext50_32x4d\', \'se_resnext101_32x4d\']\n\npretrained_settings = {\n    \'senet154\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet50\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet101\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet152\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnext50_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnext101_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    """"""\n    Base class for bottlenecks that implements `forward()` method.\n    """"""\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    """"""\n    Bottleneck for SENet154.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    """"""\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n                               stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    """"""\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        # width = math.floor(planes * (base_width / 64)) * groups\n        # pdb.set_trace()\n        width = int(planes * base_width / 64) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n                               stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        """"""\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        """"""\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                (\'bn1\', nn.BatchNorm2d(64)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n                (\'conv2\', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                (\'bn2\', nn.BatchNorm2d(64)),\n                (\'relu2\', nn.ReLU(inplace=True)),\n                (\'conv3\', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                (\'bn3\', nn.BatchNorm2d(inplanes)),\n                (\'relu3\', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                (\'bn1\', nn.BatchNorm2d(inplanes)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append((\'pool\', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n \n        return x\n\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x,x_):      \n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings[\'num_classes\'], \\\n        \'num_classes should be {}, but is {}\'.format(\n            settings[\'num_classes\'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings[\'url\'], \'pretrained_model/encoder\'))\n    model.input_space = settings[\'input_space\']\n    model.input_size = settings[\'input_size\']\n    model.input_range = settings[\'input_range\']\n    model.mean = settings[\'mean\']\n    model.std = settings[\'std\']\n\n\ndef senet154(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n                  dropout_p=0.2, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'senet154\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet50(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet50\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet101(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet101\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet152(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet152\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext50_32x4d(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnext50_32x4d\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext101_32x4d(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnext101_32x4d\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n'"
