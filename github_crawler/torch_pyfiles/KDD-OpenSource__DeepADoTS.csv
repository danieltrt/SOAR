file_path,api_count,code
experiments.py,0,"b'import numpy as np\nimport pandas as pd\nfrom itertools import product\n\nfrom src.datasets import SyntheticDataGenerator, MultivariateAnomalyFunction\nfrom src.evaluation.evaluator import Evaluator\n\n\n# Validates all algorithms regarding polluted data based on a given outlier type.\n# The pollution of the training data is tested from 0 to 100% (with default steps=5).\ndef run_pollution_experiment(detectors, seeds, runs, outlier_type=\'extreme_1\', output_dir=None, steps=5,\n                             store_results=True):\n    return run_experiment_evaluation(detectors, seeds, runs, output_dir, \'polluted\', steps, outlier_type,\n                                     store_results=store_results)\n\n\n# Validates all algorithms regarding missing data based on a given outlier type.\n# The percentage of missing values within the training data is tested from 0 to 100% (with default\n# steps=5). By default the missing values are represented as zeros since no algorithm can\'t handle\n# nan values.\ndef run_missing_experiment(detectors, seeds, runs, outlier_type=\'extreme_1\', output_dir=None, steps=5,\n                           store_results=True):\n    return run_experiment_evaluation(detectors, seeds, runs, output_dir, \'missing\', steps, outlier_type,\n                                     store_results=store_results)\n\n\n# high-dimensional experiment on normal outlier types\ndef run_multi_dim_experiment(detectors, seeds, runs, outlier_type=\'extreme_1\', output_dir=None, steps=5,\n                             store_results=True):\n    return run_experiment_evaluation(detectors, seeds, runs, output_dir, \'multi_dim\', steps, outlier_type,\n                                     store_results=store_results)\n\n\n# Validates all algorithms regarding different heights of extreme outliers\n# The extreme values are added to the outlier timestamps everywhere in the dataset distribution.\ndef run_extremes_experiment(detectors, seeds, runs, outlier_type=\'extreme_1\', output_dir=None, steps=10,\n                            store_results=True):\n    return run_experiment_evaluation(detectors, seeds, runs, output_dir, \'extreme\', steps, outlier_type,\n                                     store_results=store_results)\n\n\ndef run_multivariate_experiment(detectors, seeds, runs, output_dir=None, store_results=True):\n    return run_experiment_evaluation(detectors, seeds, runs, output_dir, \'multivariate\', store_results=store_results)\n\n\ndef run_multivariate_polluted_experiment(detectors, seeds, runs, outlier_type, output_dir=None, store_results=True):\n    return run_experiment_evaluation(detectors, seeds, runs, output_dir, \'mv_polluted\',\n                                     outlier_type=outlier_type, store_results=store_results)\n\n\ndef run_multi_dim_multivariate_experiment(detectors, seeds, runs, outlier_type, output_dir=None,\n                                          steps=2, store_results=True):\n    return run_experiment_evaluation(detectors, seeds, runs, output_dir, \'multi_dim_multivariate\',\n                                     steps, outlier_type=outlier_type, store_results=store_results)\n\n\ndef run_different_window_sizes_evaluator(detectors, seeds, runs):\n    results = pd.DataFrame()\n    for seed in seeds:\n        datasets = [SyntheticDataGenerator.long_term_dependencies_width(seed),\n                    SyntheticDataGenerator.long_term_dependencies_height(seed),\n                    SyntheticDataGenerator.long_term_dependencies_missing(seed)]\n        evaluator = Evaluator(datasets, detectors, seed=seed)\n        evaluator.evaluate()\n        evaluator.plot_scores()\n        result = evaluator.benchmarks()\n        results = results.append(result, ignore_index=True)\n    evaluator.set_benchmark_results(results)\n    evaluator.export_results(\'run_different_windows\')\n    evaluator.create_boxplots(runs=runs, data=results, detectorwise=False)\n    evaluator.create_boxplots(runs=runs, data=results, detectorwise=True)\n    return evaluator\n\n\n# outlier type means agots types for the univariate experiments, the multivariate types for the multivariate experiments\ndef get_datasets_for_multiple_runs(anomaly_type, seeds, steps, outlier_type):\n    for seed in seeds:\n        if anomaly_type == \'extreme\':\n            yield [SyntheticDataGenerator.get(f\'{outlier_type}_extremeness\', seed, extreme)\n                   for extreme in np.logspace(4, -5, num=steps, base=2)]\n        elif anomaly_type == \'missing\':\n            yield [SyntheticDataGenerator.get(f\'{outlier_type}_missing\', seed, missing)\n                   for missing in np.logspace(-6.5, -0.15, num=steps, base=2)]\n        elif anomaly_type == \'polluted\':\n            yield [SyntheticDataGenerator.get(f\'{outlier_type}_polluted\', seed, pollution_percentage=pollution)\n                   for pollution in [0.01, 0.05, 0.1, 0.2, 0.5]]\n        elif anomaly_type == \'mv_polluted\':\n            yield [MultivariateAnomalyFunction.get_multivariate_dataset(\n                outlier_type, random_seed=seed, train_pollution=pollution)\n                for pollution in [0.01, 0.05, 0.1, 0.2, 0.5]]\n        elif anomaly_type == \'multivariate\':\n            multivariate_anomaly_functions = [\'doubled\', \'inversed\', \'shrinked\', \'delayed\', \'xor\', \'delayed_missing\']\n            yield [MultivariateAnomalyFunction.get_multivariate_dataset(dim_func, random_seed=seed)\n                   for dim_func in multivariate_anomaly_functions]\n        elif anomaly_type == \'multi_dim_multivariate\':\n            group_sizes = [None, 20]\n            num_dims = [25, 75, 125, 250]\n            yield [MultivariateAnomalyFunction.get_multivariate_dataset(\n                outlier_type, random_seed=seed, features=dim, group_size=gsize,\n                name=f\'Synthetic Multivariate {dim}-dimensional {outlier_type} \'\n                     f\'Curve Outliers with {gsize or dim} per group\'\n            )\n                for dim, gsize in product(num_dims, group_sizes)]\n        elif anomaly_type == \'multi_dim\':\n            yield [SyntheticDataGenerator.get(f\'{outlier_type}\', seed, num_dim)\n                   for num_dim in np.linspace(100, 1500, steps, dtype=int)]\n\n\ndef run_experiment_evaluation(detectors, seeds, runs, output_dir, anomaly_type, steps=5, outlier_type=\'extreme_1\',\n                              store_results=True):\n    datasets = list(get_datasets_for_multiple_runs(anomaly_type, seeds, steps, outlier_type))\n    results = pd.DataFrame()\n    evaluator = None\n\n    for index, seed in enumerate(seeds):\n        evaluator = Evaluator(datasets[index], detectors, output_dir, seed=seed)\n        evaluator.evaluate()\n        result = evaluator.benchmarks()\n        evaluator.plot_roc_curves(store=store_results)\n        evaluator.plot_threshold_comparison(store=store_results)\n        evaluator.plot_scores(store=store_results)\n        evaluator.set_benchmark_results(result)\n        evaluator.export_results(f\'experiment-run-{index}-{seed}\')\n        results = results.append(result, ignore_index=True)\n\n    if not store_results:\n        return\n\n    # set average results from multiple pipeline runs for evaluation\n    avg_results = results.groupby([\'dataset\', \'algorithm\'], as_index=False).mean()\n    evaluator.set_benchmark_results(avg_results)\n    evaluator.export_results(f\'experiment-{anomaly_type}\')\n\n    # Plots which need the whole data (not averaged)\n    evaluator.create_boxplots(runs=runs, data=results, detectorwise=True, store=store_results)\n    evaluator.create_boxplots(runs=runs, data=results, detectorwise=False, store=store_results)\n    evaluator.gen_merged_tables(results, f\'for_{anomaly_type}\', store=store_results)\n\n    # Plots using \'self.benchmark_results\' -> using the averaged results\n    evaluator.create_bar_charts(runs=runs, detectorwise=True, store=store_results)\n    evaluator.create_bar_charts(runs=runs, detectorwise=False, store=store_results)\n    evaluator.plot_auroc(title=f\'Area under the curve for differing {anomaly_type} anomalies\', store=store_results)\n\n    # Plots using \'self.results\' (need the score) -> only from the last run\n    evaluator.plot_threshold_comparison(store=store_results)\n    evaluator.plot_scores(store=store_results)\n    evaluator.plot_roc_curves(store=store_results)\n\n    return evaluator\n\n\ndef announce_experiment(title: str, dashes: int = 70):\n    print(f\'\\n###{""-""*dashes}###\')\n    message = f\'Experiment: {title}\'\n    before = (dashes - len(message)) // 2\n    after = dashes - len(message) - before\n    print(f\'###{""-""*before}{message}{""-""*after}###\')\n    print(f\'###{""-""*dashes}###\\n\')\n'"
main.py,0,"b""import glob\nimport os\n\nimport numpy as np\nimport pandas as pd\n\nfrom experiments import run_extremes_experiment, run_multivariate_experiment, run_multi_dim_multivariate_experiment, \\\n    announce_experiment, run_multivariate_polluted_experiment, run_different_window_sizes_evaluator\nfrom src.algorithms import AutoEncoder, DAGMM, RecurrentEBM, LSTMAD, LSTMED\nfrom src.datasets import KDDCup, RealPickledDataset\nfrom src.evaluation import Evaluator\n\nRUNS = 1\n\n\ndef main():\n    run_experiments()\n\n\ndef detectors(seed):\n    standard_epochs = 40\n    dets = [AutoEncoder(num_epochs=standard_epochs, seed=seed),\n            DAGMM(num_epochs=standard_epochs, seed=seed, lr=1e-4),\n            DAGMM(num_epochs=standard_epochs, autoencoder_type=DAGMM.AutoEncoder.LSTM, seed=seed),\n            LSTMAD(num_epochs=standard_epochs, seed=seed), LSTMED(num_epochs=standard_epochs, seed=seed),\n            RecurrentEBM(num_epochs=standard_epochs, seed=seed)]\n\n    return sorted(dets, key=lambda x: x.framework)\n\n\ndef run_experiments():\n    # Set the seed manually for reproducibility.\n    seeds = np.random.randint(np.iinfo(np.uint32).max, size=RUNS, dtype=np.uint32)\n    output_dir = 'reports/experiments'\n    evaluators = []\n    outlier_height_steps = 10\n\n    for outlier_type in ['extreme_1', 'shift_1', 'variance_1', 'trend_1']:\n        announce_experiment('Outlier Height')\n        ev_extr = run_extremes_experiment(\n            detectors, seeds, RUNS, outlier_type, steps=outlier_height_steps,\n            output_dir=os.path.join(output_dir, outlier_type, 'intensity'))\n        evaluators.append(ev_extr)\n\n    announce_experiment('Multivariate Datasets')\n    ev_mv = run_multivariate_experiment(\n        detectors, seeds, RUNS,\n        output_dir=os.path.join(output_dir, 'multivariate'))\n    evaluators.append(ev_mv)\n\n    for mv_anomaly in ['doubled', 'inversed', 'shrinked', 'delayed', 'xor', 'delayed_missing']:\n        announce_experiment(f'Multivariate Polluted {mv_anomaly} Datasets')\n        ev_mv = run_multivariate_polluted_experiment(\n            detectors, seeds, RUNS, mv_anomaly,\n            output_dir=os.path.join(output_dir, 'mv_polluted'))\n        evaluators.append(ev_mv)\n\n        announce_experiment(f'High-dimensional multivariate {mv_anomaly} outliers')\n        ev_mv_dim = run_multi_dim_multivariate_experiment(\n            detectors, seeds, RUNS, mv_anomaly, steps=20,\n            output_dir=os.path.join(output_dir, 'multi_dim_mv'))\n        evaluators.append(ev_mv_dim)\n\n    announce_experiment('Long-Term Experiments')\n    ev_different_windows = run_different_window_sizes_evaluator(different_window_detectors, seeds, RUNS)\n    evaluators.append(ev_different_windows)\n\n    for ev in evaluators:\n        ev.plot_single_heatmap()\n\n\ndef evaluate_real_datasets():\n    REAL_DATASET_GROUP_PATH = 'data/raw/'\n    real_dataset_groups = glob.glob(REAL_DATASET_GROUP_PATH + '*')\n    seeds = np.random.randint(np.iinfo(np.uint32).max, size=RUNS, dtype=np.uint32)\n    results = pd.DataFrame()\n    datasets = [KDDCup(seed=1)]\n    for real_dataset_group in real_dataset_groups:\n        for data_set_path in glob.glob(real_dataset_group + '/labeled/train/*'):\n            data_set_name = data_set_path.split('/')[-1].replace('.pkl', '')\n            dataset = RealPickledDataset(data_set_name, data_set_path)\n            datasets.append(dataset)\n\n    for seed in seeds:\n        datasets[0] = KDDCup(seed)\n        evaluator = Evaluator(datasets, detectors, seed=seed)\n        evaluator.evaluate()\n        result = evaluator.benchmarks()\n        evaluator.plot_roc_curves()\n        evaluator.plot_threshold_comparison()\n        evaluator.plot_scores()\n        results = results.append(result, ignore_index=True)\n\n    avg_results = results.groupby(['dataset', 'algorithm'], as_index=False).mean()\n    evaluator.set_benchmark_results(avg_results)\n    evaluator.export_results('run_real_datasets')\n    evaluator.create_boxplots(runs=RUNS, data=results, detectorwise=False)\n    evaluator.create_boxplots(runs=RUNS, data=results, detectorwise=True)\n\n\ndef different_window_detectors(seed):\n    standard_epochs = 40\n    dets = [LSTMAD(num_epochs=standard_epochs)]\n    for window_size in [13, 25, 50, 100]:\n        dets.extend([LSTMED(name='LSTMED Window: ' + str(window_size), num_epochs=standard_epochs, seed=seed,\n                            sequence_length=window_size), AutoEncoder(name='AE Window: ' + str(window_size),\n                                                                      num_epochs=standard_epochs, seed=seed,\n                                                                      sequence_length=window_size)])\n    return dets\n\n\nif __name__ == '__main__':\n    main()\n"""
setup.py,0,"b'from setuptools import setup\n\n\ndef parse_requirements(filename):\n    """""" load requirements from a pip requirements file """"""\n    lineiter = (line.strip() for line in open(filename))\n    return [line for line in lineiter if line and not line.startswith(""#"")]\n\n\nsetup(\n    name=\'deep-adots\',\n    author=\'Maxi Fischer, Willi Gierke, Ajay Kesar, Thomas Kellermeier, Axel Stebner, Daniel Thevessen\',\n    description=\'Unsupervised Anomaly Detection: Representation Learning for Predictive Maintenance over Time\',\n    long_description=open(\'README.md\').read(),\n    version=\'0.0\',\n    packages=[],\n    scripts=[],\n    # Requirements for executing the project (not development)\n    install_requires=parse_requirements(\'requirements.txt\'),\n    url=\'github.com/KDD-OpenSource/DeepADoTS\',\n    license=\'MIT License\',\n)\n'"
src/__init__.py,0,b''
src/data_loader.py,0,"b""import numpy as np\nfrom sklearn.model_selection import train_test_split\n\n\ndef get_train_test_split(data_path, data_type='npz', test_size=0.2, shuffle=False):\n    data = np.load(data_path)\n    if data_type == 'npz':\n        data = data[data.files[0]]\n    labels = data[:, -1]\n    features = data[:, :-1]\n    return train_test_split(features, labels, test_size=test_size, shuffle=shuffle)\n"""
tests/__init__.py,0,b''
tests/test_initialization.py,0,"b'""""""Test each detector on each synthetic dataset""""""\n\nimport os\nimport unittest\n\nimport numpy as np\n\nfrom experiments import run_extremes_experiment, announce_experiment\nfrom src.algorithms import AutoEncoder, DAGMM, RecurrentEBM, LSTMAD, LSTMED\n\n\nclass InitializationTestCase(unittest.TestCase):\n\n    @staticmethod\n    def test_algorithm_initializations():\n        def detectors(seed):\n            dets = [AutoEncoder(num_epochs=1, seed=seed), DAGMM(num_epochs=1, seed=seed),\n                    DAGMM(num_epochs=1, autoencoder_type=DAGMM.AutoEncoder.LSTM, seed=seed),\n                    LSTMAD(num_epochs=1, seed=seed), LSTMED(num_epochs=1, seed=seed),\n                    RecurrentEBM(num_epochs=1, seed=seed)]\n            return sorted(dets, key=lambda x: x.framework)\n\n        RUNS = 1\n        seeds = np.random.randint(np.iinfo(np.uint32).max, size=RUNS, dtype=np.uint32)\n        output_dir = \'reports/experiments\'\n        evaluators = []\n        outlier_height_steps = 1\n\n        for outlier_type in [\'extreme_1\', \'shift_1\', \'variance_1\', \'trend_1\']:\n            announce_experiment(\'Outlier Height\')\n            ev_extr = run_extremes_experiment(\n                detectors, seeds, RUNS, outlier_type, steps=outlier_height_steps,\n                output_dir=os.path.join(output_dir, outlier_type, \'intensity\'))\n            evaluators.append(ev_extr)\n\n        ev_extr.plot_single_heatmap()\n'"
src/algorithms/__init__.py,0,"b""from .dagmm import DAGMM\nfrom .donut import Donut\nfrom .autoencoder import AutoEncoder\nfrom .lstm_ad import LSTMAD\nfrom .lstm_enc_dec_axl import LSTMED\nfrom .rnn_ebm import RecurrentEBM\n\n__all__ = [\n    'AutoEncoder',\n    'DAGMM',\n    'Donut',\n    'LSTMAD',\n    'LSTMED',\n    'RecurrentEBM'\n]\n"""
src/algorithms/algorithm_utils.py,4,"b'import abc\nimport logging\nimport random\n\nimport numpy as np\nimport torch\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\nfrom torch.autograd import Variable\n\n\nclass Algorithm(metaclass=abc.ABCMeta):\n    def __init__(self, module_name, name, seed, details=False):\n        self.logger = logging.getLogger(module_name)\n        self.name = name\n        self.seed = seed\n        self.details = details\n        self.prediction_details = {}\n\n        if self.seed is not None:\n            random.seed(seed)\n            np.random.seed(seed)\n\n    def __str__(self):\n        return self.name\n\n    @abc.abstractmethod\n    def fit(self, X):\n        """"""\n        Train the algorithm on the given dataset\n        """"""\n\n    @abc.abstractmethod\n    def predict(self, X):\n        """"""\n        :return anomaly score\n        """"""\n\n\nclass PyTorchUtils(metaclass=abc.ABCMeta):\n    def __init__(self, seed, gpu):\n        self.gpu = gpu\n        self.seed = seed\n        if self.seed is not None:\n            torch.manual_seed(self.seed)\n            torch.cuda.manual_seed(self.seed)\n        self.framework = 0\n\n    @property\n    def device(self):\n        return torch.device(f\'cuda:{self.gpu}\' if torch.cuda.is_available() and self.gpu is not None else \'cpu\')\n\n    def to_var(self, t, **kwargs):\n        # ToDo: check whether cuda Variable.\n        t = t.to(self.device)\n        return Variable(t, **kwargs)\n\n    def to_device(self, model):\n        model.to(self.device)\n\n\nclass TensorflowUtils(metaclass=abc.ABCMeta):\n    def __init__(self, seed, gpu):\n        self.gpu = gpu\n        self.seed = seed\n        if self.seed is not None:\n            tf.set_random_seed(seed)\n        self.framework = 1\n\n    @property\n    def device(self):\n        local_device_protos = device_lib.list_local_devices()\n        gpus = [x.name for x in local_device_protos if x.device_type == \'GPU\']\n        return tf.device(gpus[self.gpu] if gpus and self.gpu is not None else \'/cpu:0\')\n'"
src/algorithms/autoencoder.py,4,"b""import logging\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom scipy.stats import multivariate_normal\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom tqdm import trange\n\nfrom .algorithm_utils import Algorithm, PyTorchUtils\n\n\nclass AutoEncoder(Algorithm, PyTorchUtils):\n    def __init__(self, name: str = 'AutoEncoder', num_epochs: int = 10, batch_size: int = 20, lr: float = 1e-3,\n                 hidden_size: int = 5, sequence_length: int = 30, train_gaussian_percentage: float = 0.25,\n                 seed: int = None, gpu: int = None, details=True):\n        Algorithm.__init__(self, __name__, name, seed, details=details)\n        PyTorchUtils.__init__(self, seed, gpu)\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.lr = lr\n\n        self.hidden_size = hidden_size\n        self.sequence_length = sequence_length\n        self.train_gaussian_percentage = train_gaussian_percentage\n\n        self.aed = None\n        self.mean, self.cov = None, None\n\n    def fit(self, X: pd.DataFrame):\n        X.interpolate(inplace=True)\n        X.bfill(inplace=True)\n        data = X.values\n        sequences = [data[i:i + self.sequence_length] for i in range(data.shape[0] - self.sequence_length + 1)]\n        indices = np.random.permutation(len(sequences))\n        split_point = int(self.train_gaussian_percentage * len(sequences))\n        train_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n                                  sampler=SubsetRandomSampler(indices[:-split_point]), pin_memory=True)\n        train_gaussian_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n                                           sampler=SubsetRandomSampler(indices[-split_point:]), pin_memory=True)\n\n        self.aed = AutoEncoderModule(X.shape[1], self.sequence_length, self.hidden_size, seed=self.seed, gpu=self.gpu)\n        self.to_device(self.aed)  # .double()\n        optimizer = torch.optim.Adam(self.aed.parameters(), lr=self.lr)\n\n        self.aed.train()\n        for epoch in trange(self.num_epochs):\n            logging.debug(f'Epoch {epoch+1}/{self.num_epochs}.')\n            for ts_batch in train_loader:\n                output = self.aed(self.to_var(ts_batch))\n                loss = nn.MSELoss(size_average=False)(output, self.to_var(ts_batch.float()))\n                self.aed.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n        self.aed.eval()\n        error_vectors = []\n        for ts_batch in train_gaussian_loader:\n            output = self.aed(self.to_var(ts_batch))\n            error = nn.L1Loss(reduce=False)(output, self.to_var(ts_batch.float()))\n            error_vectors += list(error.view(-1, X.shape[1]).data.cpu().numpy())\n\n        self.mean = np.mean(error_vectors, axis=0)\n        self.cov = np.cov(error_vectors, rowvar=False)\n\n    def predict(self, X: pd.DataFrame) -> np.array:\n        X.interpolate(inplace=True)\n        X.bfill(inplace=True)\n        data = X.values\n        sequences = [data[i:i + self.sequence_length] for i in range(data.shape[0] - self.sequence_length + 1)]\n        data_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, shuffle=False, drop_last=False)\n\n        self.aed.eval()\n        mvnormal = multivariate_normal(self.mean, self.cov, allow_singular=True)\n        scores = []\n        outputs = []\n        errors = []\n        for idx, ts in enumerate(data_loader):\n            output = self.aed(self.to_var(ts))\n            error = nn.L1Loss(reduce=False)(output, self.to_var(ts.float()))\n            score = -mvnormal.logpdf(error.view(-1, X.shape[1]).data.cpu().numpy())\n            scores.append(score.reshape(ts.size(0), self.sequence_length))\n            if self.details:\n                outputs.append(output.cpu().data.numpy())\n                errors.append(error.cpu().data.numpy())\n\n        # stores seq_len-many scores per timestamp and averages them\n        scores = np.concatenate(scores)\n        lattice = np.full((self.sequence_length, X.shape[0]), np.nan)\n        for i, score in enumerate(scores):\n            lattice[i % self.sequence_length, i:i + self.sequence_length] = score\n        scores = np.nanmean(lattice, axis=0)\n\n        if self.details:\n            outputs = np.concatenate(outputs)\n            lattice = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n            for i, output in enumerate(outputs):\n                lattice[i % self.sequence_length, i:i + self.sequence_length, :] = output\n            self.prediction_details.update({'reconstructions_mean': np.nanmean(lattice, axis=0).T})\n\n            errors = np.concatenate(errors)\n            lattice = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n            for i, error in enumerate(errors):\n                lattice[i % self.sequence_length, i:i + self.sequence_length, :] = error\n            self.prediction_details.update({'errors_mean': np.nanmean(lattice, axis=0).T})\n\n        return scores\n\n\nclass AutoEncoderModule(nn.Module, PyTorchUtils):\n    def __init__(self, n_features: int, sequence_length: int, hidden_size: int, seed: int, gpu: int):\n        # Each point is a flattened window and thus has as many features as sequence_length * features\n        super().__init__()\n        PyTorchUtils.__init__(self, seed, gpu)\n        input_length = n_features * sequence_length\n\n        # creates powers of two between eight and the next smaller power from the input_length\n        dec_steps = 2 ** np.arange(max(np.ceil(np.log2(hidden_size)), 2), np.log2(input_length))[1:]\n        dec_setup = np.concatenate([[hidden_size], dec_steps.repeat(2), [input_length]])\n        enc_setup = dec_setup[::-1]\n\n        layers = np.array([[nn.Linear(int(a), int(b)), nn.Tanh()] for a, b in enc_setup.reshape(-1, 2)]).flatten()[:-1]\n        self._encoder = nn.Sequential(*layers)\n        self.to_device(self._encoder)\n\n        layers = np.array([[nn.Linear(int(a), int(b)), nn.Tanh()] for a, b in dec_setup.reshape(-1, 2)]).flatten()[:-1]\n        self._decoder = nn.Sequential(*layers)\n        self.to_device(self._decoder)\n\n    def forward(self, ts_batch, return_latent: bool = False):\n        flattened_sequence = ts_batch.view(ts_batch.size(0), -1)\n        enc = self._encoder(flattened_sequence.float())\n        dec = self._decoder(enc)\n        reconstructed_sequence = dec.view(ts_batch.size())\n        return (reconstructed_sequence, enc) if return_latent else reconstructed_sequence\n"""
src/algorithms/dagmm.py,28,"b'""""""Adapted from Daniel Stanley Tan (https://github.com/danieltan07/dagmm)""""""\nimport logging\nimport sys\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom tqdm import trange\n\nfrom .algorithm_utils import Algorithm, PyTorchUtils\nfrom .autoencoder import AutoEncoderModule\nfrom .lstm_enc_dec_axl import LSTMEDModule\n\n\nclass DAGMM(Algorithm, PyTorchUtils):\n    class AutoEncoder:\n        NN = AutoEncoderModule\n        LSTM = LSTMEDModule\n\n    def __init__(self, num_epochs=10, lambda_energy=0.1, lambda_cov_diag=0.005, lr=1e-3, batch_size=50, gmm_k=3,\n                 normal_percentile=80, sequence_length=30, autoencoder_type=AutoEncoderModule, autoencoder_args=None,\n                 hidden_size: int = 5, seed: int = None, gpu: int = None, details=True):\n        _name = \'LSTM-DAGMM\' if autoencoder_type == LSTMEDModule else \'DAGMM\'\n        Algorithm.__init__(self, __name__, _name, seed, details=details)\n        PyTorchUtils.__init__(self, seed, gpu)\n        self.num_epochs = num_epochs\n        self.lambda_energy = lambda_energy\n        self.lambda_cov_diag = lambda_cov_diag\n        self.lr = lr\n        self.batch_size = batch_size\n        self.sequence_length = sequence_length\n        self.gmm_k = gmm_k  # Number of Gaussian mixtures\n        self.normal_percentile = normal_percentile  # Up to which percentile data should be considered normal\n        self.autoencoder_type = autoencoder_type\n        if autoencoder_type == AutoEncoderModule:\n            self.autoencoder_args = ({\'sequence_length\': self.sequence_length})\n        elif autoencoder_type == LSTMEDModule:\n            self.autoencoder_args = ({\'n_layers\': (1, 1), \'use_bias\': (True, True), \'dropout\': (0.0, 0.0)})\n        self.autoencoder_args.update({\'seed\': seed, \'gpu\': gpu})\n        if autoencoder_args is not None:\n            self.autoencoder_args.update(autoencoder_args)\n        self.hidden_size = hidden_size\n\n        self.dagmm, self.optimizer, self.train_energy, self._threshold = None, None, None, None\n\n    def reset_grad(self):\n        self.dagmm.zero_grad()\n\n    def dagmm_step(self, input_data):\n        self.dagmm.train()\n        enc, dec, z, gamma = self.dagmm(input_data)\n        total_loss, sample_energy, recon_error, cov_diag = self.dagmm.loss_function(input_data, dec, z, gamma,\n                                                                                    self.lambda_energy,\n                                                                                    self.lambda_cov_diag)\n        self.reset_grad()\n        total_loss = torch.clamp(total_loss, max=1e7)  # Extremely high loss can cause NaN gradients\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.dagmm.parameters(), 5)\n        # if np.array([np.isnan(p.grad.detach().numpy()).any() for p in self.dagmm.parameters()]).any():\n        #     import IPython; IPython.embed()\n        self.optimizer.step()\n        return total_loss, sample_energy, recon_error, cov_diag\n\n    def fit(self, X: pd.DataFrame):\n        """"""Learn the mixture probability, mean and covariance for each component k.\n        Store the computed energy based on the training data and the aforementioned parameters.""""""\n        X.interpolate(inplace=True)\n        X.bfill(inplace=True)\n        data = X.values\n        sequences = [data[i:i + self.sequence_length] for i in range(X.shape[0] - self.sequence_length + 1)]\n        data_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, shuffle=True, drop_last=True)\n        self.hidden_size = 5 + int(X.shape[1] / 20)\n        autoencoder = self.autoencoder_type(X.shape[1], hidden_size=self.hidden_size, **self.autoencoder_args)\n        self.dagmm = DAGMMModule(autoencoder, n_gmm=self.gmm_k, latent_dim=self.hidden_size + 2,\n                                 seed=self.seed, gpu=self.gpu)\n        self.to_device(self.dagmm)\n        self.optimizer = torch.optim.Adam(self.dagmm.parameters(), lr=self.lr)\n\n        for _ in trange(self.num_epochs):\n            for input_data in data_loader:\n                input_data = self.to_var(input_data)\n                self.dagmm_step(input_data.float())\n\n        self.dagmm.eval()\n        n = 0\n        mu_sum = 0\n        cov_sum = 0\n        gamma_sum = 0\n        for input_data in data_loader:\n            input_data = self.to_var(input_data)\n            _, _, z, gamma = self.dagmm(input_data.float())\n            phi, mu, cov = self.dagmm.compute_gmm_params(z, gamma)\n\n            batch_gamma_sum = torch.sum(gamma, dim=0)\n\n            gamma_sum += batch_gamma_sum\n            mu_sum += mu * batch_gamma_sum.unsqueeze(-1)  # keep sums of the numerator only\n            cov_sum += cov * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1)  # keep sums of the numerator only\n\n            n += input_data.size(0)\n\n    def predict(self, X: pd.DataFrame):\n        """"""Using the learned mixture probability, mean and covariance for each component k, compute the energy on the\n        given data.""""""\n        self.dagmm.eval()\n        X.interpolate(inplace=True)\n        X.bfill(inplace=True)\n        data = X.values\n        sequences = [data[i:i + self.sequence_length] for i in range(len(data) - self.sequence_length + 1)]\n        data_loader = DataLoader(dataset=sequences, batch_size=1, shuffle=False)\n        test_energy = np.full((self.sequence_length, X.shape[0]), np.nan)\n\n        encodings = np.full((self.sequence_length, X.shape[0], self.hidden_size), np.nan)\n        decodings = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n        euc_errors = np.full((self.sequence_length, X.shape[0]), np.nan)\n        csn_errors = np.full((self.sequence_length, X.shape[0]), np.nan)\n\n        for i, sequence in enumerate(data_loader):\n            enc, dec, z, gamma = self.dagmm(self.to_var(sequence).float())\n            sample_energy, _ = self.dagmm.compute_energy(z, size_average=False)\n            idx = (i % self.sequence_length, np.arange(i, i + self.sequence_length))\n            test_energy[idx] = sample_energy.data.numpy()\n\n            if self.details:\n                encodings[idx] = enc.data.numpy()\n                decodings[idx] = dec.data.numpy()\n                euc_errors[idx] = z[:, 1].data.numpy()\n                csn_errors[idx] = z[:, 2].data.numpy()\n\n        test_energy = np.nanmean(test_energy, axis=0)\n\n        if self.details:\n            self.prediction_details.update({\'latent_representations\': np.nanmean(encodings, axis=0).T})\n            self.prediction_details.update({\'reconstructions_mean\': np.nanmean(decodings, axis=0).T})\n            self.prediction_details.update({\'euclidean_errors_mean\': np.nanmean(euc_errors, axis=0)})\n            self.prediction_details.update({\'cosine_errors_mean\': np.nanmean(csn_errors, axis=0)})\n\n        return test_energy\n\n\nclass DAGMMModule(nn.Module, PyTorchUtils):\n    """"""Residual Block.""""""\n\n    def __init__(self, autoencoder, n_gmm, latent_dim, seed: int, gpu: int):\n        super(DAGMMModule, self).__init__()\n        PyTorchUtils.__init__(self, seed, gpu)\n\n        self.add_module(\'autoencoder\', autoencoder)\n\n        layers = [\n            nn.Linear(latent_dim, 10),\n            nn.Tanh(),\n            nn.Dropout(p=0.5),\n            nn.Linear(10, n_gmm),\n            nn.Softmax(dim=1)\n        ]\n        self.estimation = nn.Sequential(*layers)\n        self.to_device(self.estimation)\n\n        self.register_buffer(\'phi\', self.to_var(torch.zeros(n_gmm)))\n        self.register_buffer(\'mu\', self.to_var(torch.zeros(n_gmm, latent_dim)))\n        self.register_buffer(\'cov\', self.to_var(torch.zeros(n_gmm, latent_dim, latent_dim)))\n\n    def relative_euclidean_distance(self, a, b, dim=1):\n        return (a - b).norm(2, dim=dim) / torch.clamp(a.norm(2, dim=dim), min=1e-10)\n\n    def forward(self, x):\n        dec, enc = self.autoencoder(x, return_latent=True)\n\n        rec_cosine = F.cosine_similarity(x.view(x.shape[0], -1), dec.view(dec.shape[0], -1), dim=1)\n        rec_euclidean = self.relative_euclidean_distance(x.view(x.shape[0], -1), dec.view(dec.shape[0], -1), dim=1)\n\n        # Concatenate latent representation, cosine similarity and relative Euclidean distance between x and dec(enc(x))\n        z = torch.cat([enc, rec_euclidean.unsqueeze(-1), rec_cosine.unsqueeze(-1)], dim=1)\n        gamma = self.estimation(z)\n\n        return enc, dec, z, gamma\n\n    def compute_gmm_params(self, z, gamma):\n        N = gamma.size(0)\n        # K\n        sum_gamma = torch.sum(gamma, dim=0)\n\n        # K\n        phi = (sum_gamma / N)\n\n        self.phi = phi.data\n\n        # K x D\n        mu = torch.sum(gamma.unsqueeze(-1) * z.unsqueeze(1), dim=0) / sum_gamma.unsqueeze(-1)\n        self.mu = mu.data\n        # z = N x D\n        # mu = K x D\n        # gamma N x K\n\n        # z_mu = N x K x D\n        z_mu = (z.unsqueeze(1) - mu.unsqueeze(0))\n\n        # z_mu_outer = N x K x D x D\n        z_mu_outer = z_mu.unsqueeze(-1) * z_mu.unsqueeze(-2)\n\n        # K x D x D\n        cov = torch.sum(gamma.unsqueeze(-1).unsqueeze(-1) * z_mu_outer, dim=0) / sum_gamma.unsqueeze(-1).unsqueeze(-1)\n        self.cov = cov.data\n\n        return phi, mu, cov\n\n    def compute_energy(self, z, phi=None, mu=None, cov=None, size_average=True):\n        if phi is None:\n            phi = Variable(self.phi)\n        if mu is None:\n            mu = Variable(self.mu)\n        if cov is None:\n            cov = Variable(self.cov)\n\n        k, d, _ = cov.size()\n\n        z_mu = (z.unsqueeze(1) - mu.unsqueeze(0))\n\n        cov_inverse = []\n        det_cov = []\n        cov_diag = 0\n        eps = 1e-12\n        for i in range(k):\n            # K x D x D\n            cov_k = cov[i] + self.to_var(torch.eye(d) * eps)\n            pinv = np.linalg.pinv(cov_k.data.numpy())\n            cov_inverse.append(Variable(torch.from_numpy(pinv)).unsqueeze(0))\n\n            eigvals = np.linalg.eigvals(cov_k.data.cpu().numpy() * (2 * np.pi))\n            if np.min(eigvals) < 0:\n                logging.warning(f\'Determinant was negative! Clipping Eigenvalues to 0+epsilon from {np.min(eigvals)}\')\n            determinant = np.prod(np.clip(eigvals, a_min=sys.float_info.epsilon, a_max=None))\n            det_cov.append(determinant)\n\n            cov_diag = cov_diag + torch.sum(1 / cov_k.diag())\n\n        # K x D x D\n        cov_inverse = torch.cat(cov_inverse, dim=0)\n        # K\n        det_cov = Variable(torch.from_numpy(np.float32(np.array(det_cov))))\n\n        # N x K\n        exp_term_tmp = -0.5 * torch.sum(torch.sum(z_mu.unsqueeze(-1) * cov_inverse.unsqueeze(0), dim=-2) * z_mu, dim=-1)\n        # for stability (logsumexp)\n        max_val = torch.max((exp_term_tmp).clamp(min=0), dim=1, keepdim=True)[0]\n\n        exp_term = torch.exp(exp_term_tmp - max_val)\n\n        sample_energy = -max_val.squeeze() - torch.log(\n            torch.sum(self.to_var(phi.unsqueeze(0)) * exp_term / (torch.sqrt(self.to_var(det_cov)) + eps).unsqueeze(0),\n                      dim=1) + eps)\n\n        if size_average:\n            sample_energy = torch.mean(sample_energy)\n\n        return sample_energy, cov_diag\n\n    def loss_function(self, x, x_hat, z, gamma, lambda_energy, lambda_cov_diag):\n        recon_error = torch.mean((x.view(*x_hat.shape) - x_hat) ** 2)\n        phi, mu, cov = self.compute_gmm_params(z, gamma)\n        sample_energy, cov_diag = self.compute_energy(z, phi, mu, cov)\n        loss = recon_error + lambda_energy * sample_energy + lambda_cov_diag * cov_diag\n        return loss, sample_energy, recon_error, cov_diag\n'"
src/algorithms/donut.py,0,"b'import sys\n\nimport numpy as np\nimport pandas as pd\nimport six\nimport tensorflow as tf\nfrom donut import DonutTrainer, DonutPredictor, Donut as DonutModel, complete_timestamp, standardize_kpi\nfrom donut.augmentation import MissingDataInjection\nfrom donut.utils import BatchSlidingWindow\nfrom tensorflow import keras as K\nfrom tfsnippet.modules import Sequential\nfrom tfsnippet.scaffold import TrainLoop\nfrom tfsnippet.utils import (get_default_session_or_error,\n                             ensure_variables_initialized)\nfrom tqdm import trange\n\nfrom .algorithm_utils import Algorithm, TensorflowUtils\n\n\nclass QuietDonutTrainer(DonutTrainer):\n    def fit(self, values, labels, missing, mean, std, excludes=None,\n            valid_portion=0.3, summary_dir=None):\n        """"""\n        Train the :class:`Donut` model with given data.\n        From https://github.com/haowen-xu/donut/blob/master/donut/training.py but without prints.\n\n        Args:\n            values (np.ndarray): 1-D `float32` array, the standardized\n                KPI observations.\n            labels (np.ndarray): 1-D `int32` array, the anomaly labels.\n            missing (np.ndarray): 1-D `int32` array, the indicator of\n                missing points.\n            mean (float): The mean of KPI observations before standardization.\n            std (float): The standard deviation of KPI observations before\n                standardization.\n            excludes (np.ndarray): 1-D `bool` array, indicators of whether\n                or not to totally exclude a point.  If a point is excluded,\n                any window which contains that point is excluded.\n                (default :obj:`None`, no point is totally excluded)\n            valid_portion (float): Ratio of validation data out of all the\n                specified training data. (default 0.3)\n            summary_dir (str): Optional summary directory for\n                :class:`tf.summary.FileWriter`. (default :obj:`None`,\n                summary is disabled)\n        """"""\n        sess = get_default_session_or_error()\n\n        # split the training & validation set\n        values = np.asarray(values, dtype=np.float32)\n        labels = np.asarray(labels, dtype=np.int32)\n        missing = np.asarray(missing, dtype=np.int32)\n        if len(values.shape) != 1:\n            raise ValueError(\'`values` must be a 1-D array\')\n        if labels.shape != values.shape:\n            raise ValueError(\'The shape of `labels` does not agree with \'\n                             \'the shape of `values` ({} vs {})\'.\n                             format(labels.shape, values.shape))\n        if missing.shape != values.shape:\n            raise ValueError(\'The shape of `missing` does not agree with \'\n                             \'the shape of `values` ({} vs {})\'.\n                             format(missing.shape, values.shape))\n\n        n = int(len(values) * valid_portion)\n        train_values, v_x = values[:-n], values[-n:]\n        train_labels, valid_labels = labels[:-n], labels[-n:]\n        train_missing, valid_missing = missing[:-n], missing[-n:]\n        v_y = np.logical_or(valid_labels, valid_missing).astype(np.int32)\n        if excludes is None:\n            train_excludes, valid_excludes = None, None\n        else:\n            train_excludes, valid_excludes = excludes[:-n], excludes[-n:]\n\n        # data augmentation object and the sliding window iterator\n        # If std is zero choose a number close to zero\n        aug = MissingDataInjection(mean, std, self._missing_data_injection_rate)\n        train_sliding_window = BatchSlidingWindow(\n            array_size=len(train_values),\n            window_size=self.model.x_dims,\n            batch_size=self._batch_size,\n            excludes=train_excludes,\n            shuffle=True,\n            ignore_incomplete_batch=True,\n        )\n        valid_sliding_window = BatchSlidingWindow(\n            array_size=len(v_x),\n            window_size=self.model.x_dims,\n            batch_size=self._valid_batch_size,\n            excludes=valid_excludes,\n        )\n\n        # initialize the variables of the trainer, and the model\n        sess.run(self._trainer_initializer)\n        ensure_variables_initialized(self._train_params)\n\n        # training loop\n        lr = self._initial_lr\n        # Side effect. EarlyStopping stores variables temporarely in a Temp dir\n        with TrainLoop(\n                param_vars=self._train_params,\n                early_stopping=True,\n                summary_dir=summary_dir,\n                max_epoch=self._max_epoch,\n                max_step=self._max_step) as loop:  # type: TrainLoop\n\n            for epoch in loop.iter_epochs():\n                x, y1, y2 = aug.augment(\n                    train_values, train_labels, train_missing)\n                y = np.logical_or(y1, y2).astype(np.int32)\n\n                train_iterator = train_sliding_window.get_iterator([x, y])\n                for step, (batch_x, batch_y) in loop.iter_steps(train_iterator):\n                    # run a training step\n                    feed_dict = dict(six.iteritems(self._feed_dict))\n                    feed_dict[self._learning_rate] = lr\n                    feed_dict[self._input_x] = batch_x\n                    feed_dict[self._input_y] = batch_y\n                    loss, _ = sess.run(\n                        [self._loss, self._train_op], feed_dict=feed_dict)\n                    loop.collect_metrics({\'loss\': loss})\n\n                    if step % self._valid_step_freq == 0:\n                        # collect variable summaries\n                        if summary_dir is not None:\n                            loop.add_summary(sess.run(self._summary_op))\n\n                        # do validation in batches\n                        with loop.timeit(\'valid_time\'), loop.metric_collector(\'valid_loss\') as mc:\n                            v_it = valid_sliding_window.get_iterator([v_x, v_y])\n                            for b_v_x, b_v_y in v_it:\n                                feed_dict = dict(\n                                    six.iteritems(self._valid_feed_dict))\n                                feed_dict[self._input_x] = b_v_x\n                                feed_dict[self._input_y] = b_v_y\n                                loss = sess.run(self._loss, feed_dict=feed_dict)\n                                mc.collect(loss, weight=len(b_v_x))\n\n                # anneal the learning rate\n                if self._lr_anneal_epochs and epoch % self._lr_anneal_epochs == 0:\n                    lr *= self._lr_anneal_factor\n\n\nclass Donut(Algorithm, TensorflowUtils):\n    """"""For each feature, the anomaly score is set to 1 for a point if its reconstruction probability\n    is smaller than mean - std of the reconstruction probabilities for that feature. For each point\n    in time, the maximum of the scores of the features is taken to support multivariate time series as well.""""""\n\n    def __init__(self, num_epochs=256, batch_size=32, x_dims=120,\n                 seed: int = None, gpu: int = None):\n        Algorithm.__init__(self, __name__, \'Donut\', seed)\n        TensorflowUtils.__init__(self, seed, gpu)\n        self.max_epoch = num_epochs\n        self.x_dims = x_dims\n        self.batch_size = batch_size\n        self.means, self.stds, self.tf_sessions, self.models = [], [], [], []\n\n    def fit(self, X: pd.DataFrame):\n        with self.device:\n            # Reset all results from last run to avoid reusing variables\n            self.means, self.stds, self.tf_sessions, self.models = [], [], [], []\n            for col_idx in trange(len(X.columns)):\n                col = X.columns[col_idx]\n                tf_session = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n                timestamps = X.index\n                features = X.loc[:, col].interpolate().bfill().values\n                labels = pd.Series(0, X.index)\n                timestamps, _, (features, labels) = complete_timestamp(timestamps, (features, labels))\n                missing = np.isnan(X.loc[:, col].values)\n                _, mean, std = standardize_kpi(features, excludes=np.logical_or(labels, missing))\n\n                with tf.variable_scope(\'model\') as model_vs:\n                    model = DonutModel(\n                        h_for_p_x=Sequential([\n                            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n                                           activation=tf.nn.relu),\n                            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n                                           activation=tf.nn.relu),\n                        ]),\n                        h_for_q_z=Sequential([\n                            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n                                           activation=tf.nn.relu),\n                            K.layers.Dense(100, kernel_regularizer=K.regularizers.l2(0.001),\n                                           activation=tf.nn.relu),\n                        ]),\n                        x_dims=self.x_dims,\n                        z_dims=5,\n                    )\n\n                trainer = QuietDonutTrainer(model=model, model_vs=model_vs, max_epoch=self.max_epoch,\n                                            batch_size=self.batch_size, valid_batch_size=self.batch_size,\n                                            missing_data_injection_rate=0.0, lr_anneal_factor=1.0)\n                with tf_session.as_default():\n                    trainer.fit(features, labels, missing, mean, std, valid_portion=0.25)\n                self.means.append(mean)\n                self.stds.append(std)\n                self.tf_sessions.append(tf_session)\n                self.models.append(model)\n\n    def predict(self, X: pd.DataFrame):\n        """"""Since we predict the anomaly scores for each feature independently, we already return a binarized one-\n        dimensional anomaly score array.""""""\n        with self.device:\n            test_scores = np.zeros_like(X)\n            for col_idx, col in enumerate(X.columns):\n                mean, std, tf_session, model = \\\n                    self.means[col_idx], self.stds[col_idx], self.tf_sessions[col_idx], self.models[col_idx]\n                test_values, _, _ = standardize_kpi(X.loc[:, col], mean=mean, std=std)\n                test_missing = np.zeros_like(test_values)\n                predictor = DonutPredictor(model)\n                with tf_session.as_default():\n                    test_score = predictor.get_score(test_values, test_missing)\n                # Convert to negative reconstruction probability so score is in accordance with other detectors\n                test_score = -np.power(np.e, test_score)\n                test_scores[self.x_dims - 1:, col_idx] = test_score\n            aggregated_test_scores = np.amax(test_scores, axis=1)\n            aggregated_test_scores[:self.x_dims - 1] = np.nanmin(aggregated_test_scores) - sys.float_info.epsilon\n            return aggregated_test_scores\n'"
src/algorithms/lstm_ad.py,16,"b'import numpy as np\nimport pandas as pd\nimport torch\nfrom scipy.stats import multivariate_normal\nfrom torch.autograd import Variable\nfrom tqdm import trange\n\nfrom .algorithm_utils import Algorithm, PyTorchUtils\n\n\nclass LSTMAD(Algorithm, PyTorchUtils):\n    """""" LSTM-AD implementation using PyTorch.\n    The interface of the class is sklearn-like.\n    """"""\n\n    def __init__(self, len_in=1, len_out=10, num_epochs=100, lr=1e-3, batch_size=1,\n                 seed: int = None, gpu: int = None, details=True):\n        Algorithm.__init__(self, __name__, \'LSTM-AD\', seed, details=details)\n        PyTorchUtils.__init__(self, seed, gpu)\n        self.num_epochs = num_epochs\n        self.lr = lr\n        self.batch_size = batch_size\n\n        self.len_in = len_in\n        self.len_out = len_out\n\n        self.mean, self.cov = None, None\n\n    def fit(self, X):\n        X.interpolate(inplace=True)\n        X.bfill(inplace=True)\n        self.batch_size = 1\n        self._build_model(X.shape[-1], self.batch_size)\n\n        self.model.train()\n        split_point = int(0.75 * len(X))\n        X_train = X.loc[:split_point, :]\n        X_train_gaussian = X.loc[split_point:, :]\n\n        input_data_train, target_data_train = self._input_and_target_data(X_train)\n        self._train_model(input_data_train, target_data_train)\n\n        self.model.eval()\n        input_data_gaussian, target_data_gaussian = self._input_and_target_data_eval(X_train_gaussian)\n        predictions_gaussian = self.model(input_data_gaussian)\n        errors = self._calc_errors(predictions_gaussian, target_data_gaussian)\n\n        # fit multivariate Gaussian on (validation set) error distribution (via maximum likelihood estimation)\n        norm = errors.reshape(errors.shape[0] * errors.shape[1], X.shape[-1] * self.len_out)\n        self.mean = np.mean(norm, axis=0)\n        self.cov = np.cov(norm.T)\n\n    def predict(self, X):\n        X.interpolate(inplace=True)\n        X.bfill(inplace=True)\n        self.model.eval()\n        input_data, target_data = self._input_and_target_data_eval(X)\n\n        predictions = self.model(input_data)\n        errors, stacked_preds = self._calc_errors(predictions, target_data, return_stacked_predictions=True)\n\n        if self.details:\n            self.prediction_details.update({\'predictions_mean\': np.pad(\n                stacked_preds.mean(axis=3).squeeze(0).T, ((0, 0), (self.len_in + self.len_out - 1, 0)),\n                \'constant\', constant_values=np.nan)})\n            self.prediction_details.update({\'errors_mean\': np.pad(\n                errors.mean(axis=3).reshape(-1), (self.len_in + self.len_out - 1, 0),\n                \'constant\', constant_values=np.nan)})\n\n        norm = errors.reshape(errors.shape[0] * errors.shape[1], X.shape[-1] * self.len_out)\n        scores = -multivariate_normal.logpdf(norm, mean=self.mean, cov=self.cov, allow_singular=True)\n        scores = np.pad(scores, (self.len_in + self.len_out - 1, 0), \'constant\', constant_values=np.nan)\n        return scores\n\n    def _input_and_target_data(self, X: pd.DataFrame):\n        X = np.expand_dims(X, axis=0)\n        input_data = self.to_var(torch.from_numpy(X[:, :-self.len_out, :]), requires_grad=False)\n        target_data = []\n        for l in range(self.len_out - 1):\n            target_data += [X[:, 1 + l:-self.len_out + 1 + l, :]]\n        target_data += [X[:, self.len_out:, :]]\n        target_data = self.to_var(torch.from_numpy(np.stack(target_data, axis=3)), requires_grad=False)\n\n        return input_data, target_data\n\n    def _input_and_target_data_eval(self, X: pd.DataFrame):\n        X = np.expand_dims(X, axis=0)\n        input_data = self.to_var(torch.from_numpy(X), requires_grad=False)\n        target_data = self.to_var(torch.from_numpy(X[:, self.len_in + self.len_out - 1:, :]), requires_grad=False)\n        return input_data, target_data\n\n    def _calc_errors(self, predictions, target_data, return_stacked_predictions=False):\n        errors = [predictions.data.numpy()[:, self.len_out - 1:-self.len_in, :, 0]]\n        for l in range(1, self.len_out):\n            errors += [predictions.data.numpy()[:, self.len_out - 1 - l:-self.len_in - l, :, l]]\n        errors = np.stack(errors, axis=3)\n        stacked_predictions = errors\n        errors = target_data.data.numpy()[..., np.newaxis] - errors\n        return errors if return_stacked_predictions is False else (errors, stacked_predictions)\n\n    def _build_model(self, d, batch_size):\n        self.model = LSTMSequence(d, batch_size, len_in=self.len_in, len_out=self.len_out)\n        self.to_device(self.model)\n        self.model.double()\n\n        self.loss = torch.nn.MSELoss()\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n\n    def _train_model(self, input_data, target_data):\n        def closure():\n            return self._train(input_data, target_data)\n\n        for epoch in trange(self.num_epochs):\n            self.optimizer.step(closure)\n\n    def _train(self, input_data, target_data):\n        self.optimizer.zero_grad()\n        output_data = self.model(input_data)\n        loss_train = self.loss(output_data, target_data)\n        loss_train.backward()\n        return loss_train\n\n\nclass LSTMSequence(torch.nn.Module):\n    def __init__(self, d, batch_size: int, len_in=1, len_out=10):\n        super().__init__()\n        self.d = d  # input and output feature dimensionality\n        self.batch_size = batch_size\n        self.len_in = len_in\n        self.len_out = len_out\n        self.hidden_size1 = 32\n        self.hidden_size2 = 32\n        self.lstm1 = torch.nn.LSTMCell(d * len_in, self.hidden_size1)\n        self.lstm2 = torch.nn.LSTMCell(self.hidden_size1, self.hidden_size2)\n        self.linear = torch.nn.Linear(self.hidden_size2, d * len_out)\n\n        self.register_buffer(\'h_t\', torch.zeros(self.batch_size, self.hidden_size1))\n        self.register_buffer(\'c_t\', torch.zeros(self.batch_size, self.hidden_size1))\n        self.register_buffer(\'h_t2\', torch.zeros(self.batch_size, self.hidden_size1))\n        self.register_buffer(\'c_t2\', torch.zeros(self.batch_size, self.hidden_size1))\n\n    def forward(self, input):\n        outputs = []\n        h_t = Variable(self.h_t.double(), requires_grad=False)\n        c_t = Variable(self.c_t.double(), requires_grad=False)\n        h_t2 = Variable(self.h_t2.double(), requires_grad=False)\n        c_t2 = Variable(self.c_t2.double(), requires_grad=False)\n\n        for input_t in input.chunk(input.size(1), dim=1):\n            h_t, c_t = self.lstm1(input_t.squeeze(dim=1), (h_t, c_t))\n            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n            output = self.linear(h_t2)\n            outputs += [output]\n        outputs = torch.stack(outputs, 1).squeeze()  # stack (n, d * len_out) outputs in time dimensionality (dim=1)\n\n        return outputs.view(input.size(0), input.size(1), self.d, self.len_out)\n'"
src/algorithms/lstm_enc_dec_axl.py,7,"b""import logging\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom scipy.stats import multivariate_normal\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom tqdm import trange\n\nfrom .algorithm_utils import Algorithm, PyTorchUtils\n\n\nclass LSTMED(Algorithm, PyTorchUtils):\n    def __init__(self, name: str = 'LSTM-ED', num_epochs: int = 10, batch_size: int = 20, lr: float = 1e-3,\n                 hidden_size: int = 5, sequence_length: int = 30, train_gaussian_percentage: float = 0.25,\n                 n_layers: tuple = (1, 1), use_bias: tuple = (True, True), dropout: tuple = (0, 0),\n                 seed: int = None, gpu: int = None, details=True):\n        Algorithm.__init__(self, __name__, name, seed, details=details)\n        PyTorchUtils.__init__(self, seed, gpu)\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.lr = lr\n\n        self.hidden_size = hidden_size\n        self.sequence_length = sequence_length\n        self.train_gaussian_percentage = train_gaussian_percentage\n\n        self.n_layers = n_layers\n        self.use_bias = use_bias\n        self.dropout = dropout\n\n        self.lstmed = None\n        self.mean, self.cov = None, None\n\n    def fit(self, X: pd.DataFrame):\n        X.interpolate(inplace=True)\n        X.bfill(inplace=True)\n        data = X.values\n        sequences = [data[i:i + self.sequence_length] for i in range(data.shape[0] - self.sequence_length + 1)]\n        indices = np.random.permutation(len(sequences))\n        split_point = int(self.train_gaussian_percentage * len(sequences))\n        train_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n                                  sampler=SubsetRandomSampler(indices[:-split_point]), pin_memory=True)\n        train_gaussian_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n                                           sampler=SubsetRandomSampler(indices[-split_point:]), pin_memory=True)\n\n        self.lstmed = LSTMEDModule(X.shape[1], self.hidden_size,\n                                   self.n_layers, self.use_bias, self.dropout,\n                                   seed=self.seed, gpu=self.gpu)\n        self.to_device(self.lstmed)\n        optimizer = torch.optim.Adam(self.lstmed.parameters(), lr=self.lr)\n\n        self.lstmed.train()\n        for epoch in trange(self.num_epochs):\n            logging.debug(f'Epoch {epoch+1}/{self.num_epochs}.')\n            for ts_batch in train_loader:\n                output = self.lstmed(self.to_var(ts_batch))\n                loss = nn.MSELoss(size_average=False)(output, self.to_var(ts_batch.float()))\n                self.lstmed.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n        self.lstmed.eval()\n        error_vectors = []\n        for ts_batch in train_gaussian_loader:\n            output = self.lstmed(self.to_var(ts_batch))\n            error = nn.L1Loss(reduce=False)(output, self.to_var(ts_batch.float()))\n            error_vectors += list(error.view(-1, X.shape[1]).data.cpu().numpy())\n\n        self.mean = np.mean(error_vectors, axis=0)\n        self.cov = np.cov(error_vectors, rowvar=False)\n\n    def predict(self, X: pd.DataFrame):\n        X.interpolate(inplace=True)\n        X.bfill(inplace=True)\n        data = X.values\n        sequences = [data[i:i + self.sequence_length] for i in range(data.shape[0] - self.sequence_length + 1)]\n        data_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, shuffle=False, drop_last=False)\n\n        self.lstmed.eval()\n        mvnormal = multivariate_normal(self.mean, self.cov, allow_singular=True)\n        scores = []\n        outputs = []\n        errors = []\n        for idx, ts in enumerate(data_loader):\n            output = self.lstmed(self.to_var(ts))\n            error = nn.L1Loss(reduce=False)(output, self.to_var(ts.float()))\n            score = -mvnormal.logpdf(error.view(-1, X.shape[1]).data.cpu().numpy())\n            scores.append(score.reshape(ts.size(0), self.sequence_length))\n            if self.details:\n                outputs.append(output.data.numpy())\n                errors.append(error.data.numpy())\n\n        # stores seq_len-many scores per timestamp and averages them\n        scores = np.concatenate(scores)\n        lattice = np.full((self.sequence_length, data.shape[0]), np.nan)\n        for i, score in enumerate(scores):\n            lattice[i % self.sequence_length, i:i + self.sequence_length] = score\n        scores = np.nanmean(lattice, axis=0)\n\n        if self.details:\n            outputs = np.concatenate(outputs)\n            lattice = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n            for i, output in enumerate(outputs):\n                lattice[i % self.sequence_length, i:i + self.sequence_length, :] = output\n            self.prediction_details.update({'reconstructions_mean': np.nanmean(lattice, axis=0).T})\n\n            errors = np.concatenate(errors)\n            lattice = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n            for i, error in enumerate(errors):\n                lattice[i % self.sequence_length, i:i + self.sequence_length, :] = error\n            self.prediction_details.update({'errors_mean': np.nanmean(lattice, axis=0).T})\n\n        return scores\n\n\nclass LSTMEDModule(nn.Module, PyTorchUtils):\n    def __init__(self, n_features: int, hidden_size: int,\n                 n_layers: tuple, use_bias: tuple, dropout: tuple,\n                 seed: int, gpu: int):\n        super().__init__()\n        PyTorchUtils.__init__(self, seed, gpu)\n        self.n_features = n_features\n        self.hidden_size = hidden_size\n\n        self.n_layers = n_layers\n        self.use_bias = use_bias\n        self.dropout = dropout\n\n        self.encoder = nn.LSTM(self.n_features, self.hidden_size, batch_first=True,\n                               num_layers=self.n_layers[0], bias=self.use_bias[0], dropout=self.dropout[0])\n        self.to_device(self.encoder)\n        self.decoder = nn.LSTM(self.n_features, self.hidden_size, batch_first=True,\n                               num_layers=self.n_layers[1], bias=self.use_bias[1], dropout=self.dropout[1])\n        self.to_device(self.decoder)\n        self.hidden2output = nn.Linear(self.hidden_size, self.n_features)\n        self.to_device(self.hidden2output)\n\n    def _init_hidden(self, batch_size):\n        return (self.to_var(torch.Tensor(self.n_layers[0], batch_size, self.hidden_size).zero_()),\n                self.to_var(torch.Tensor(self.n_layers[0], batch_size, self.hidden_size).zero_()))\n\n    def forward(self, ts_batch, return_latent: bool = False):\n        batch_size = ts_batch.shape[0]\n\n        # 1. Encode the timeseries to make use of the last hidden state.\n        enc_hidden = self._init_hidden(batch_size)  # initialization with zero\n        _, enc_hidden = self.encoder(ts_batch.float(), enc_hidden)  # .float() here or .double() for the model\n\n        # 2. Use hidden state as initialization for our Decoder-LSTM\n        dec_hidden = enc_hidden\n\n        # 3. Also, use this hidden state to get the first output aka the last point of the reconstructed timeseries\n        # 4. Reconstruct timeseries backwards\n        #    * Use true data for training decoder\n        #    * Use hidden2output for prediction\n        output = self.to_var(torch.Tensor(ts_batch.size()).zero_())\n        for i in reversed(range(ts_batch.shape[1])):\n            output[:, i, :] = self.hidden2output(dec_hidden[0][0, :])\n\n            if self.training:\n                _, dec_hidden = self.decoder(ts_batch[:, i].unsqueeze(1).float(), dec_hidden)\n            else:\n                _, dec_hidden = self.decoder(output[:, i].unsqueeze(1), dec_hidden)\n\n        return (output, enc_hidden[1][-1]) if return_latent else output\n"""
src/algorithms/rnn_ebm.py,0,"b'import numpy as np\nimport tensorflow as tf\nfrom tqdm import trange\n\nfrom .algorithm_utils import Algorithm, TensorflowUtils\n\n\nclass RecurrentEBM(Algorithm, TensorflowUtils):\n    """""" Recurrent Energy-Based Model implementation using TensorFlow.\n    The interface of the class is sklearn-like.\n    """"""\n\n    def __init__(self, num_epochs=100, n_hidden=50, n_hidden_recurrent=100,\n                 min_lr=1e-3, min_energy=None, batch_size=10,\n                 seed: int = None, gpu: int = None):\n        Algorithm.__init__(self, __name__, \'Recurrent EBM\', seed)\n        TensorflowUtils.__init__(self, seed, gpu)\n        self.num_epochs = num_epochs\n        self.n_hidden = n_hidden  # Size of RBM\'s hidden layer\n        self.n_hidden_recurrent = n_hidden_recurrent  # Size of RNN\'s hidden layer\n        self.min_lr = min_lr\n        self.min_energy = min_energy  # Threshold for anomaly\n        self.batch_size = batch_size\n\n        # Placeholders\n        self.input_data = None\n        self.lr = None\n        self._batch_size = None\n\n        # Variables\n        self.W, self.Wuh, self.Wux, self.Wxu, self.Wuu, self.bu, self.u0, self.bh, self.bx, self.BH_t, self.BX_t = [None] * 11\n        self.tvars = []\n\n        self.update = None\n        self.cost = None\n\n        self.tf_session = None\n\n    def fit(self, X):\n        X.interpolate(inplace=True)\n        X.bfill(inplace=True)\n        with self.device:\n            self._build_model(X.shape[1])\n            self.tf_session = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n            self._initialize_tf()\n            self._train_model(X, self.batch_size)\n\n    def predict(self, X):\n        X.interpolate(inplace=True)\n        X.bfill(inplace=True)\n        with self.device:\n            scores = []\n            labels = []\n\n            for i in range(len(X)):\n                reconstruction_err = self.tf_session.run([self.cost],\n                                                         feed_dict={self.input_data: X[i:i + 1],\n                                                                    self._batch_size: 1})\n                scores.append(reconstruction_err[0])\n            if self.min_energy is not None:\n                labels = np.where(scores >= self.min_energy)\n\n            scores = np.array(scores)\n\n            return (labels, scores) if self.min_energy is not None else scores\n\n    def _train_model(self, train_set, batch_size):\n        for epoch in trange(self.num_epochs):\n            costs = []\n            for i in range(0, len(train_set), batch_size):\n                x = train_set[i:i + batch_size]\n                if len(x) == batch_size:\n                    alpha = self.min_lr  # min(self.min_lr, 0.1 / float(i + 1))\n                    _, C = self.tf_session.run([self.update, self.cost],\n                                               feed_dict={self.input_data: x, self.lr: alpha,\n                                                          self._batch_size: batch_size})\n                    costs.append(C)\n            self.logger.debug(f\'Epoch: {epoch+1} Cost: {np.mean(costs)}\')\n\n    def _initialize_tf(self):\n        init = tf.global_variables_initializer()\n        self.tf_session.run(init)\n\n    def _build_model(self, n_visible):\n        self.input_data, self.lr, self._batch_size = self._create_placeholders(n_visible)\n        self.W, self.Wuh, self.Wux, self.Wxu, self.Wuu, self.bu, \\\n        self.u0, self.bh, self.bx, self.BH_t, self.BX_t = self._create_variables(n_visible)\n\n        def rnn_recurrence(u_tmin1, sl):\n            # Iterate through the data in the batch and generate the values of the RNN hidden nodes\n            sl = tf.reshape(sl, [1, n_visible])\n            u_t = tf.nn.softplus(self.bu + tf.matmul(sl, self.Wxu) + tf.matmul(u_tmin1, self.Wuu))\n            return u_t\n\n        def visible_bias_recurrence(bx_t, u_tmin1):\n            # Iterate through the values of the RNN hidden nodes and generate the values of the visible bias vectors\n            bx_t = tf.add(self.bx, tf.matmul(u_tmin1, self.Wux))\n            return bx_t\n\n        def hidden_bias_recurrence(bh_t, u_tmin1):\n            # Iterate through the values of the RNN hidden nodes and generate the values of the hidden bias vectors\n            bh_t = tf.add(self.bh, tf.matmul(u_tmin1, self.Wuh))\n            return bh_t\n\n        self.BH_t = tf.tile(self.BH_t, [self._batch_size, 1])\n        self.BX_t = tf.tile(self.BX_t, [self._batch_size, 1])\n\n        # Scan through the rnn and generate the value for each hidden node in the batch\n        u_t = tf.scan(rnn_recurrence, self.input_data, initializer=self.u0)\n        # Scan through the rnn and generate the visible and hidden biases for each RBM in the batch\n        self.BX_t = tf.reshape(tf.scan(visible_bias_recurrence, u_t, tf.zeros([1, n_visible], tf.float32)),\n                               [n_visible, self._batch_size])\n        self.BH_t = tf.reshape(tf.scan(hidden_bias_recurrence, u_t, tf.zeros([1, self.n_hidden], tf.float32)),\n                               [self.n_hidden, self._batch_size])\n\n        self.cost = self._run_ebm(self.input_data, self.W, self.BX_t, self.BH_t)\n\n        self.tvars = [self.W, self.Wuh, self.Wux, self.Wxu, self.Wuu, self.bu, self.u0, self.bh, self.bx]\n        opt_func = tf.train.AdamOptimizer(learning_rate=self.lr)\n        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, self.tvars), 1)\n        self.update = opt_func.apply_gradients(zip(grads, self.tvars))\n\n    def _run_ebm(self, x, W, b_prime, b):\n        """""" Runs EBM for time step and returns reconstruction error.\n        1-layer implementation, TODO: implement and test deep structure\n        """"""\n        x = tf.transpose(x)  # For batch processing\n\n        forward = tf.matmul(tf.transpose(W), x) + b\n        reconstruction = tf.matmul(W, tf.sigmoid(forward)) + b_prime\n        loss = tf.reduce_sum(tf.square(x - reconstruction))\n        return loss\n\n    def _create_placeholders(self, n_visible):\n        x = tf.placeholder(tf.float32, [None, n_visible], name=\'x_input\')\n        lr = tf.placeholder(tf.float32)\n        batch_size = tf.placeholder(tf.int32)\n        return x, lr, batch_size\n\n    def _create_variables(self, n_visible):\n        W = tf.Variable(tf.random_normal([n_visible, self.n_hidden], stddev=0.01), name=\'W\')\n        Wuh = tf.Variable(tf.random_normal([self.n_hidden_recurrent, self.n_hidden], stddev=0.01), name=\'Wuh\')\n        Wux = tf.Variable(tf.random_normal([self.n_hidden_recurrent, n_visible], stddev=0.01), name=\'Wux\')\n        Wxu = tf.Variable(tf.random_normal([n_visible, self.n_hidden_recurrent], stddev=0.01), name=\'Wxu\')\n        Wuu = tf.Variable(tf.random_normal([self.n_hidden_recurrent, self.n_hidden_recurrent], stddev=0.01), name=\'Wuu\')\n        bu = tf.Variable(tf.zeros([1, self.n_hidden_recurrent]), name=\'bu\')\n        u0 = tf.Variable(tf.zeros([1, self.n_hidden_recurrent]), name=\'u0\')\n        bh = tf.Variable(tf.zeros([1, self.n_hidden]), name=\'bh\')\n        bx = tf.Variable(tf.zeros([1, n_visible]), name=\'bx\')\n        BH_t = tf.Variable(tf.zeros([1, self.n_hidden]), name=\'BH_t\')\n        BX_t = tf.Variable(tf.zeros([1, n_visible]), name=\'BX_t\')\n\n        return W, Wuh, Wux, Wxu, Wuu, bu, u0, bh, bx, BH_t, BX_t\n'"
src/datasets/__init__.py,0,"b""from .dataset import Dataset\nfrom .kdd_cup import KDDCup\nfrom .multivariate_anomaly_function import MultivariateAnomalyFunction\nfrom .real_datasets import RealDataset, RealPickledDataset\nfrom .synthetic_data_generator import SyntheticDataGenerator\nfrom .synthetic_dataset import SyntheticDataset\n\n__all__ = [\n    'Dataset',\n    'SyntheticDataset',\n    'RealDataset',\n    'RealPickledDataset',\n    'KDDCup',\n    'SyntheticDataGenerator',\n    'MultivariateAnomalyFunction'\n]\n"""
src/datasets/dataset.py,0,"b'import abc\nimport logging\nimport os\nimport pickle\n\nimport pandas as pd\n\n\nclass Dataset:\n\n    def __init__(self, name: str, file_name: str):\n        self.name = name\n        self.processed_path = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                                                           \'../../data/processed/\', file_name))\n\n        self._data = None\n        self.logger = logging.getLogger(__name__)\n\n    def __str__(self) -> str:\n        return self.name\n\n    @abc.abstractmethod\n    def load(self):\n        """"""Load data""""""\n\n    def data(self) -> (pd.DataFrame, pd.Series, pd.DataFrame, pd.Series):\n        """"""Return data, load if necessary""""""\n        if self._data is None:\n            self.load()\n        return self._data\n\n    def save(self):\n        pickle.dump(self._data, open(self.processed_path, \'wb\'))\n'"
src/datasets/kdd_cup.py,0,"b'import numpy as np\nimport pandas as pd\n\nfrom .real_datasets import RealDataset\n\n\nclass KDDCup(RealDataset):\n    def __init__(self, seed):\n        super().__init__(\n            name=""KDD Cup \'99"", raw_path=\'kddcup-data_10_percent_corrected.txt\', file_name=\'kdd_cup.npz\'\n        )\n        self.seed = seed\n\n    def load(self):\n        (a, b), (c, d) = self.get_data_dagmm()\n        self._data = (a, b, c, d)\n\n    def get_data_dagmm(self):\n        """"""\n        This approach is used by the DAGMM paper (Zong et al., 2018) and was first described in Zhai et al.,\n        Deep structured energy based models for anomaly detection:\n        ""As 20% of data samples are labeled as \xe2\x80\x9cnormal\xe2\x80\x9d and the rest are labeled as \xe2\x80\x9cattack\xe2\x80\x9d, \xe2\x80\x9cnormal\xe2\x80\x9d samples are in a\n        minority group; therefore, \xe2\x80\x9cnormal\xe2\x80\x9d ones are treated as anomalies in this task"" - Zong et al., 2018\n        ""[...]in each run, we take 50% of data by random sampling for training with the rest 50% reserved for testing,\n        and only data samples from the normal class are used for training models.[...] - Zong et al., 2018""\n        :return: (X_train, y_train), (X_test, y_test)\n        """"""\n        data = np.load(self.processed_path)\n        np.random.seed(self.seed)\n\n        labels = data[\'kdd\'][:, -1]\n        features = data[\'kdd\'][:, :-1]\n\n        normal_data = features[labels == 1]\n        normal_labels = labels[labels == 1]\n\n        attack_data = features[labels == 0]\n        attack_labels = labels[labels == 0]\n\n        n_attack = attack_data.shape[0]\n\n        rand_idx = np.arange(n_attack)\n        np.random.shuffle(rand_idx)\n        n_train = n_attack // 2\n\n        train = attack_data[rand_idx[:n_train]]\n        train_labels = attack_labels[rand_idx[:n_train]]\n\n        test = attack_data[rand_idx[n_train:]]\n        test_labels = attack_labels[rand_idx[n_train:]]\n\n        test = np.concatenate((test, normal_data), axis=0)\n        test_labels = np.concatenate((test_labels, normal_labels), axis=0)\n\n        return (pd.DataFrame(data=train), pd.DataFrame(data=train_labels)), (\n            pd.DataFrame(data=test), pd.DataFrame(data=test_labels))\n'"
src/datasets/multivariate_anomaly_function.py,0,"b'import numpy as np\n\nfrom .synthetic_multivariate_dataset import SyntheticMultivariateDataset\n\n\nclass MultivariateAnomalyFunction:\n    # ----- Functions generating the anomalous dimension --------- #\n    # A MultivariateAnomalyFunction should return a tuple containing the following three values:\n    # * The values of the second dimension (array of max `interval_length` numbers)\n    # * Starting point for the anomaly\n    # * End point for the anomaly section\n    # The last two values are ignored for generation of not anomalous data\n\n    # Get a dataset by passing the method name as string. All following parameters\n    # are passed through. Throws AttributeError if attribute was not found.\n    @staticmethod\n    def get_multivariate_dataset(method, name=None, group_size=None, *args, **kwargs):\n        name = name or f\'Synthetic Multivariate {method} Curve Outliers\'\n        func = getattr(MultivariateAnomalyFunction, method)\n        return SyntheticMultivariateDataset(anomaly_func=func,\n                                            name=name,\n                                            group_size=group_size,\n                                            *args,\n                                            **kwargs)\n\n    @staticmethod\n    def doubled(curve_values, anomalous, _):\n        factor = 4 if anomalous else 2\n        return curve_values * factor, 0, len(curve_values)\n\n    @staticmethod\n    def inversed(curve_values, anomalous, _):\n        factor = -2 if anomalous else 2\n        return curve_values * factor, 0, len(curve_values)\n\n    @staticmethod\n    def shrinked(curve_values, anomalous, _):\n        if not anomalous:\n            return curve_values, -1, -1\n        else:\n            new_curve = curve_values[::2]\n            nonce = np.zeros(len(curve_values) - len(new_curve))\n            values = np.concatenate([nonce, new_curve])\n            return values, 0, len(values)\n\n    @staticmethod\n    def xor(curve_values, anomalous, interval_length):\n        pause_length = interval_length - len(curve_values)\n        if not anomalous:\n            # No curve during the other curve in the 1st dimension\n            nonce = np.zeros(len(curve_values))\n            # Insert a curve with the same amplitude during the pause of the 1st dimension\n            new_curve = MultivariateAnomalyFunction.shrink_curve(curve_values, pause_length)\n            return np.concatenate([nonce, new_curve]), -1, -1\n        else:\n            # Anomaly: curves overlap (at the same time or at least half overlapping)\n            max_pause = min(len(curve_values) // 2, pause_length)\n            nonce = np.zeros(max_pause)\n            return np.concatenate([nonce, curve_values]), len(nonce), len(curve_values)\n\n    @staticmethod\n    def delayed(curve_values, anomalous, interval_length):\n        if not anomalous:\n            return curve_values, -1, -1\n        else:\n            # The curve in the second dimension occurs a few timestamps later\n            left_space = interval_length - len(curve_values)\n            delay = min(len(curve_values) // 2, left_space)\n            nonce = np.zeros(delay)\n            values = np.concatenate([nonce, curve_values])\n            return values, 0, len(values)\n\n    @staticmethod\n    def delayed_missing(curve_values, anomalous, interval_length):\n        starting_point = len(curve_values) // 5\n        # If the space is too small for the normal curve we\'re shrinking it (which is not anomalous)\n        left_space = interval_length - starting_point\n        new_curve_length = min(left_space, len(curve_values))\n        if not anomalous:\n            # The curve in the second dimension occurs a few timestamps later\n            nonce = np.zeros(starting_point)\n            new_curve = MultivariateAnomalyFunction.shrink_curve(curve_values, new_curve_length)\n            values = np.concatenate([nonce, new_curve])\n            return values, -1, -1\n        else:\n            end_point = starting_point + new_curve_length\n            nonce = np.zeros(end_point)\n            return nonce, starting_point, end_point\n\n    """"""\n        This is a helper function for shrinking an already generated curve.\n    """"""\n\n    @staticmethod\n    def shrink_curve(curve_values, new_length):\n        if new_length == len(curve_values):\n            return curve_values\n        orig_amplitude = max(abs(curve_values))\n        orig_amplitude *= np.sign(curve_values.mean())\n        return SyntheticMultivariateDataset.get_curve(new_length, orig_amplitude)\n'"
src/datasets/real_datasets.py,0,"b'import os\nimport pickle\n\nimport numpy as np\nimport pandas as pd\n\nfrom .dataset import Dataset\n\n\nclass RealDataset(Dataset):\n    def __init__(self, raw_path, **kwargs):\n        super().__init__(**kwargs)\n        self.raw_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), ""../../data/raw/"", raw_path)\n\n\nclass RealPickledDataset(Dataset):\n    """"""Class for pickled datasets from https://github.com/chickenbestlover/RNN-Time-series-Anomaly-Detection""""""\n\n    def __init__(self, name, training_path):\n        self.name = name\n        self.training_path = training_path\n        self.test_path = self.training_path.replace(""train"", ""test"")\n        self._data = None\n\n    def data(self):\n        if self._data is None:\n            with open(self.training_path, \'rb\') as f:\n                X_train = pd.DataFrame(pickle.load(f))\n            X_train = X_train.iloc[:, :-1]\n\n            mean, std = X_train.mean(), X_train.std()\n            X_train = (X_train - mean) / std\n\n            with open(self.test_path, \'rb\') as f:\n                X_test = pd.DataFrame(pickle.load(f))\n            y_test = X_test.iloc[:, -1]\n            X_test = X_test.iloc[:, :-1]\n            X_test = (X_test - mean) / std\n            self._data = X_train, np.zeros(len(X_train)), X_test, y_test\n        return self._data\n'"
src/datasets/synthetic_data_generator.py,0,"b'import numpy as np\nimport pandas as pd\nfrom agots.generators.behavior_generators import sine_generator\n\nfrom .dataset import Dataset\nfrom .synthetic_dataset import SyntheticDataset\n\nWINDOW_SIZE = 36\n\n\ndef generate_timestamps(start, end, percentage):\n    windows = np.arange(start, end - WINDOW_SIZE, WINDOW_SIZE)\n    timestamps = [(w, w + WINDOW_SIZE) for w in\n                  np.random.choice(windows, int(percentage * len(windows)), replace=False)]\n    return timestamps\n\n\nclass LongTermDependencyDataset(Dataset):\n    """""" Build a univariate data set which contains hills in regular distances with regular widths, heights and spaces\n    in between. The three builder function can be used to vary these properties given the indices of the hills.""""""\n\n    def __init__(self, name, file_name, random_state, n=100, gaussian_std=0.01):\n        super().__init__(name, file_name)\n        np.random.seed(random_state)\n        self.train_split = 0.7\n        self.n = n\n        self.hill_mask = np.tile([0, 1], n)\n        self.length_mask = np.tile([40, 20], n)\n        self.height_mask = np.tile([0, 1.], n)\n        self.y = np.zeros(len(self.hill_mask))  # All hills are normal by default\n        self.length = sum(self.length_mask)\n        self.gaussian_std = gaussian_std\n\n    def build_missing_data(self, anomalous_hills):\n        for anomalous_hill in anomalous_hills:\n            self.hill_mask[anomalous_hill] = 0\n            self.y[anomalous_hill] = 1\n        self.y = np.repeat(self.y, self.length_mask)  # Each data point needs a label\n        return self\n\n    def build_halved_height(self, anomalous_hills):\n        for anomalous_hill in anomalous_hills:\n            self.height_mask[anomalous_hill] = 0.5\n            self.y[anomalous_hill] = 1\n        self.y = np.repeat(self.y, self.length_mask)\n        return self\n\n    def build_irregular_widths(self, anomalous_hills, widths):\n        for anomalous_hill, width in zip(anomalous_hills, widths):\n            self.length_mask[anomalous_hill] = width\n            self.y[anomalous_hill] = 1\n        self.y = np.repeat(self.y, self.length_mask)\n        return self\n\n    def create_long_term_data(self):\n        y = np.array([])\n        for idx, indicator in enumerate(self.hill_mask):\n            if indicator == 0:\n                y = np.append(y, np.zeros(self.length_mask[idx]))\n            else:\n                x_hill = np.linspace(0, 1, self.length_mask[idx])\n                y_hill = self.height_mask[idx] * np.sin(np.pi * x_hill)\n                y = np.append(y, y_hill)\n        y += np.random.normal(0, self.gaussian_std, len(y))\n        return y\n\n    def load(self):\n        X = pd.DataFrame(data=self.create_long_term_data())\n        y = pd.DataFrame(data=self.y)\n        train_split_point = int(self.train_split * self.length)\n        X_train = X[:train_split_point]\n        y_train = y[:train_split_point]\n        X_test = X[train_split_point:]\n        y_test = y[train_split_point:]\n        self._data = X_train, y_train, X_test, y_test\n\n\nclass SyntheticDataGenerator:\n    """"""\n    shift_config (starting at 0):\n    {1: 25, 2:20}\n\n    behavior_config for sine_generator:\n    {\'cycle_duration\': 20\n     \'phase_shift\': np.pi,\n     \'amplitude\': 0.3}\n\n    baseline_config:\n    {\'initial_value_min\': -4,\n     \'initial_value_max\': 4,\n     \'correlation_min\': 0.9,\n     \'correlation_max\': 0.7}\n\n    outlier_config:\n    {\'extreme\': [{\'n\': 0, \'timestamps\': [(50,), (190,)]}],\n     \'shift\':   [{\'n\': 1, \'timestamps\': [(100, 190)]}],\n     \'trend\':   [{\'n\': 2, \'timestamps\': [(20, 150)]}],\n     \'variance\':[{\'n\': 3, \'timestamps\': [(50, 100)]}]}\n\n    pollution_config:\n    * see outlier_config\n    """"""\n\n    # Get a dataset by passing the method name as string. All following parameters\n    # are passed through. Throws AttributeError if attribute was not found.\n    @staticmethod\n    def get(method, *args, **kwargs):\n        func = getattr(SyntheticDataGenerator, method)\n        return func(*args, **kwargs)\n\n    @staticmethod\n    def extreme_1(seed, n=1, k=1, anomaly_percentage=0.023):\n        np.random.seed(seed)\n        # train begins at 2100\n        length = 3000\n        train_split = 0.7\n        shift_config = {}\n        behavior = None\n        behavior_config = {}\n        baseline_config = {}\n\n        # outliers randomly distributed over all dimensions\n        train_size = int(length * train_split)\n        timestamps = [(t,) for t in np.random.randint(0, length - train_size,\n                                                      int(anomaly_percentage * (length - train_size))) + train_size]\n\n        dim = np.random.choice(n, len(timestamps))\n        outlier_config = {\'extreme\':\n                              [{\'n\': i, \'timestamps\': [ts for d, ts in zip(dim, timestamps) if d == i]} for i in range(n)]}\n\n        pollution_config = {}\n        random_state = seed\n\n        return SyntheticDataset(name=f\'Syn Extreme Outliers (dim={n})\', file_name=\'extreme1.pkl\',\n                                length=length, n=n, k=k,\n                                baseline_config=baseline_config, shift_config=shift_config,\n                                behavior=behavior, behavior_config=behavior_config,\n                                outlier_config=outlier_config, pollution_config=pollution_config,\n                                train_split=train_split, random_state=random_state)\n\n    @staticmethod\n    def extreme_1_polluted(seed, pollution_percentage=0.2, n=1, anomaly_percentage=0.023):\n        """"""Full pollution -> All anomalies from test set are in train set""""""\n        np.random.seed(seed)\n        dataset = SyntheticDataGenerator.extreme_1(seed, n, anomaly_percentage=anomaly_percentage)\n\n        train_size = int(dataset.length * dataset.train_split)\n        timestamps = [(t,) for t in np.random.randint(0, train_size, int(pollution_percentage * train_size))]\n\n        dim = np.random.choice(n, len(timestamps))\n        pollution_config = {\'extreme\': [{\'n\': i, \'timestamps\':\n            [ts for d, ts in zip(dim, timestamps) if d == i]} for i in range(n)]}\n        dataset.pollution_config = pollution_config\n\n        dataset.name = f\'Syn Extreme Outliers (pol={pollution_percentage}, anom={anomaly_percentage})\'\n        return dataset\n\n    @staticmethod\n    def extreme_1_extremeness(seed, extreme_value=10, n=1):\n        """"""Full pollution -> All anomalies from test set are in train set""""""\n        dataset = SyntheticDataGenerator.extreme_1(seed, n)\n\n        dataset.outlier_config[\'extreme\'][0][\'factor\'] = extreme_value\n\n        dataset.name = f\'Syn Extreme Outliers (extremeness={extreme_value})\'\n        return dataset\n\n    @staticmethod\n    def extreme_1_missing(seed, missing_percentage=0.1, n=1):\n        dataset = SyntheticDataGenerator.extreme_1(seed, n)\n        dataset.load()\n        dataset.add_missing_values(missing_percentage=missing_percentage)\n        dataset.name = f\'Syn Extreme Outliers (mis={missing_percentage})\'\n        return dataset\n\n    @staticmethod\n    def shift_1(seed, n=1, k=1, anomaly_percentage=0.2):\n        length = 3000\n        train_split = 0.7\n        shift_config = {}\n        behavior = None\n        behavior_config = {}\n        baseline_config = {}\n\n        timestamps = generate_timestamps(int(train_split * length), length, anomaly_percentage)\n\n        # outliers randomly distributed over all dimensions\n        dim = np.random.choice(n, len(timestamps))\n        outlier_config = {\'shift\':\n                              [{\'n\': i, \'timestamps\': [ts for d, ts in zip(dim, timestamps) if d == i]} for i in range(n)]}\n\n        pollution_config = {}\n        random_state = seed\n\n        return SyntheticDataset(name=f\'Syn Shift Outliers (dim={n})\', file_name=\'shift1.pkl\', length=length, n=n, k=k,\n                                baseline_config=baseline_config, shift_config=shift_config,\n                                behavior=behavior, behavior_config=behavior_config,\n                                outlier_config=outlier_config, pollution_config=pollution_config,\n                                train_split=train_split, random_state=random_state)\n\n    @staticmethod\n    def shift_1_missing(seed, missing_percentage=0.1, n=1):\n        dataset = SyntheticDataGenerator.shift_1(seed, n)\n        dataset.load()\n        dataset.add_missing_values(missing_percentage=missing_percentage)\n        dataset.name = f\'Syn Shift Outliers (mis={missing_percentage})\'\n        return dataset\n\n    @staticmethod\n    def shift_1_polluted(seed, pollution_percentage=0.2, n=1, anomaly_percentage=0.2):\n        np.random.seed(seed)\n        dataset = SyntheticDataGenerator.shift_1(seed, n, anomaly_percentage=anomaly_percentage)\n\n        timestamps = generate_timestamps(0, int(dataset.train_split * dataset.length), pollution_percentage)\n\n        dim = np.random.choice(n, len(timestamps))\n        pollution_config = {\'shift\': [{\'n\': i, \'timestamps\':\n            [ts for d, ts in zip(dim, timestamps) if d == i]} for i in range(n)]}\n        dataset.pollution_config = pollution_config\n\n        dataset.name = f\'Syn Shift Outliers (pol={pollution_percentage}, anom={anomaly_percentage}))\'\n        return dataset\n\n    @staticmethod\n    def shift_1_extremeness(seed, extreme_value=10, n=1):\n        """"""Full pollution -> All anomalies from test set are in train set""""""\n        dataset = SyntheticDataGenerator.shift_1(seed, n)\n\n        dataset.outlier_config[\'shift\'][0][\'factor\'] = extreme_value\n\n        dataset.name = f\'Syn Shift Outliers (extremeness={extreme_value})\'\n        return dataset\n\n    @staticmethod\n    def variance_1(seed, n=1, k=1, anomaly_percentage=0.2):\n        length = 3000\n        train_split = 0.7\n        shift_config = {}\n        behavior = None\n        behavior_config = {}\n        baseline_config = {}\n\n        timestamps = generate_timestamps(int(train_split * length), length, anomaly_percentage)\n\n        # outliers randomly distributed over all dimensions\n        dim = np.random.choice(n, len(timestamps))\n        outlier_config = {\'variance\':\n                              [{\'n\': i, \'timestamps\': [ts for d, ts in zip(dim, timestamps) if d == i]} for i in range(n)]}\n\n        pollution_config = {}\n        random_state = seed\n\n        return SyntheticDataset(name=f\'Syn Variance Outliers (dim={n})\', file_name=\'variance1.pkl\',\n                                length=length, n=n, k=k,\n                                baseline_config=baseline_config, shift_config=shift_config,\n                                behavior=behavior, behavior_config=behavior_config,\n                                outlier_config=outlier_config, pollution_config=pollution_config,\n                                train_split=train_split, random_state=random_state)\n\n    @staticmethod\n    def variance_1_missing(seed, missing_percentage=0.1, n=1):\n        dataset = SyntheticDataGenerator.variance_1(seed, n)\n        dataset.load()\n        dataset.add_missing_values(missing_percentage=missing_percentage)\n        dataset.name = f\'Syn Variance Outliers (mis={missing_percentage})\'\n        return dataset\n\n    @staticmethod\n    def variance_1_polluted(seed, pollution_percentage=0.2, n=1, anomaly_percentage=0.2):\n        np.random.seed(seed)\n        dataset = SyntheticDataGenerator.variance_1(seed, n, anomaly_percentage=anomaly_percentage)\n\n        timestamps = generate_timestamps(0, int(dataset.train_split * dataset.length), pollution_percentage)\n\n        dim = np.random.choice(n, len(timestamps))\n        pollution_config = {\'variance\': [{\'n\': i, \'timestamps\':\n            [ts for d, ts in zip(dim, timestamps) if d == i]} for i in range(n)]}\n        dataset.pollution_config = pollution_config\n\n        dataset.name = f\'Syn Variance Outliers (pol={pollution_percentage}, anom={anomaly_percentage}))\'\n        return dataset\n\n    @staticmethod\n    def variance_1_extremeness(seed, extreme_value=10, n=1):\n        """"""Full pollution -> All anomalies from test set are in train set""""""\n        dataset = SyntheticDataGenerator.variance_1(seed, n)\n\n        dataset.outlier_config[\'variance\'][0][\'factor\'] = extreme_value\n\n        dataset.name = f\'Syn Variance Outliers (extremeness={extreme_value})\'\n        return dataset\n\n    @staticmethod\n    def trend_1(seed, n=1, k=1, anomaly_percentage=0.2):\n        length = 3000\n        train_split = 0.7\n        shift_config = {}\n        behavior = None\n        behavior_config = {}\n        baseline_config = {}\n\n        timestamps = generate_timestamps(int(train_split * length), length, anomaly_percentage)\n\n        dim = np.random.choice(n, len(timestamps))\n        outlier_config = {\'trend\':\n                              [{\'n\': i, \'timestamps\': [ts for d, ts in zip(dim, timestamps) if d == i]} for i in range(n)]}\n\n        pollution_config = {}\n        random_state = seed\n\n        return SyntheticDataset(name=f\'Syn Trend Outliers (dim={n})\', file_name=\'trend1.pkl\', length=length, n=n, k=k,\n                                baseline_config=baseline_config, shift_config=shift_config,\n                                behavior=behavior, behavior_config=behavior_config,\n                                outlier_config=outlier_config, pollution_config=pollution_config,\n                                train_split=train_split, random_state=random_state)\n\n    @staticmethod\n    def trend_1_missing(seed, missing_percentage=0.1, n=1):\n        dataset = SyntheticDataGenerator.trend_1(seed, n)\n        dataset.load()\n        dataset.add_missing_values(missing_percentage=missing_percentage)\n        dataset.name = f\'Syn Trend Outliers (mis={missing_percentage})\'\n        return dataset\n\n    @staticmethod\n    def trend_1_polluted(seed, pollution_percentage=0.2, n=1, anomaly_percentage=0.2):\n        np.random.seed(seed)\n        dataset = SyntheticDataGenerator.trend_1(seed, n, anomaly_percentage=anomaly_percentage)\n\n        timestamps = generate_timestamps(0, int(dataset.train_split * dataset.length), pollution_percentage)\n\n        dim = np.random.choice(n, len(timestamps))\n        pollution_config = {\'trend\': [{\'n\': i, \'timestamps\':\n            [ts for d, ts in zip(dim, timestamps) if d == i]} for i in range(n)]}\n        dataset.pollution_config = pollution_config\n\n        dataset.name = f\'Syn Trend Outliers (pol={pollution_percentage}, anom={anomaly_percentage}))\'\n        return dataset\n\n    @staticmethod\n    def trend_1_extremeness(seed, extreme_value=10, n=1):\n        """"""Full pollution -> All anomalies from test set are in train set""""""\n        dataset = SyntheticDataGenerator.trend_1(seed, n)\n\n        dataset.outlier_config[\'trend\'][0][\'factor\'] = extreme_value\n\n        dataset.name = f\'Syn Trend Outliers (extremeness={extreme_value})\'\n        return dataset\n\n    @staticmethod\n    def combined_1(seed, n=1, k=1):\n        # train begins at 2100\n        length = 3000\n        train_split = 0.7\n        shift_config = {}\n        behavior = None\n        behavior_config = {}\n        baseline_config = {}\n        timestamps_ext = [(2192,), (2212,), (2258,), (2262,), (2319,), (2343,),\n                          (2361,), (2369,), (2428,), (2510,), (2512,), (2538,),\n                          (2567,), (2589,), (2695,), (2819,), (2892,), (2940,),\n                          (2952,), (2970,)]\n        dim_ext = np.random.choice(n, len(timestamps_ext))\n        timestamps_shi = [(2210, 2270), (2300, 2340), (2500, 2580), (2600, 2650), (2800, 2900)]\n        dim_shi = np.random.choice(n, len(timestamps_shi))\n        timestamps_var = [(2300, 2310), (2400, 2420), (2500, 2550), (2800, 2900)]\n        dim_var = np.random.choice(n, len(timestamps_var))\n        timestamps_tre = [(2200, 2400), (2400, 2420), (2500, 2550), (2800, 2950)]\n        dim_tre = np.random.choice(n, len(timestamps_tre))\n        outlier_config = {\'extreme\': [{\'n\': i, \'timestamps\':\n            [ts for d, ts in zip(dim_ext, timestamps_ext) if d == i]} for i in range(n)],\n                          \'shift\': [{\'n\': i, \'timestamps\':\n                              [ts for d, ts in zip(dim_shi, timestamps_shi) if d == i]} for i in range(n)],\n                          \'variance\': [{\'n\': i, \'timestamps\':\n                              [ts for d, ts in zip(dim_var, timestamps_var) if d == i]} for i in range(n)],\n                          \'trend\': [{\'n\': i, \'timestamps\':\n                              [ts for d, ts in zip(dim_tre, timestamps_tre) if d == i]} for i in range(n)]}\n        pollution_config = {}\n        random_state = seed\n\n        return SyntheticDataset(name=f\'Synthetic Combined Outliers (dim={n})\', file_name=\'combined1.pkl\', length=length,\n                                n=n, k=k, baseline_config=baseline_config, shift_config=shift_config,\n                                behavior=behavior, behavior_config=behavior_config,\n                                outlier_config=outlier_config, pollution_config=pollution_config,\n                                train_split=train_split, random_state=random_state)\n\n    @staticmethod\n    def combined_1_missing(seed, missing_percentage=0.1, n=1):\n        dataset = SyntheticDataGenerator.combined_1(seed, n)\n        dataset.load()\n        dataset.add_missing_values(missing_percentage=missing_percentage)\n        dataset.name = f\'Syn Combined Outliers (mis={missing_percentage})\'\n        return dataset\n\n    @staticmethod\n    def combined_4(seed, n=4, k=4):\n        # train begins at 2100\n        length = 3000\n        train_split = 0.7\n        shift_config = {}\n        behavior = None\n        behavior_config = {}\n        baseline_config = {}\n\n        indices = range(0, n + 1, n // 4)\n        outl_chunks = [np.arange(j, indices[i + 1]) for i, j in enumerate(indices[:-1])]\n        timestamps_ext = [(2192,), (2212,), (2258,), (2262,), (2319,), (2343,),\n                          (2361,), (2369,), (2428,), (2510,), (2512,), (2538,),\n                          (2567,), (2589,), (2695,), (2819,), (2892,), (2940,),\n                          (2952,), (2970,)]\n        dim_ext = np.random.choice(outl_chunks[0], len(timestamps_ext))\n        timestamps_shi = [(2210, 2270), (2300, 2340), (2500, 2580), (2600, 2650), (2800, 2900)]\n        dim_shi = np.random.choice(outl_chunks[1], len(timestamps_shi))\n        timestamps_var = [(2300, 2310), (2400, 2420), (2500, 2550), (2800, 2900)]\n        dim_var = np.random.choice(outl_chunks[2], len(timestamps_var))\n        timestamps_tre = [(2200, 2400), (2550, 2420), (2500, 2550), (2800, 2950)]\n        dim_tre = np.random.choice(outl_chunks[3], len(timestamps_tre))\n        outlier_config = {\'extreme\': [{\'n\': i, \'timestamps\':\n            [ts for d, ts in zip(dim_ext, timestamps_ext) if d == i]} for i in range(n)],\n                          \'shift\': [{\'n\': i, \'timestamps\':\n                              [ts for d, ts in zip(dim_shi, timestamps_shi) if d == i]} for i in range(n)],\n                          \'variance\': [{\'n\': i, \'timestamps\':\n                              [ts for d, ts in zip(dim_var, timestamps_var) if d == i]} for i in range(n)],\n                          \'trend\': [{\'n\': i, \'timestamps\':\n                              [ts for d, ts in zip(dim_tre, timestamps_tre) if d == i]} for i in range(n)]}\n        pollution_config = {}\n        random_state = seed\n\n        return SyntheticDataset(name=f\'Synthetic Combined Outliers (dim={n})\', file_name=\'combined4.pkl\',\n                                length=length, n=n, k=k,\n                                baseline_config=baseline_config, shift_config=shift_config,\n                                behavior=behavior, behavior_config=behavior_config,\n                                outlier_config=outlier_config, pollution_config=pollution_config,\n                                train_split=train_split, random_state=random_state)\n\n    @staticmethod\n    def combined_4_missing(seed, missing_percentage=0.1, n=4):\n        dataset = SyntheticDataGenerator.combined_4(seed, n)\n        dataset.load()\n        dataset.add_missing_values(missing_percentage=missing_percentage)\n        dataset.name = f\'Syn Combined Outliers 4D (mis={missing_percentage})\'\n        return dataset\n\n    # for accurate high-dimensional multivariate datasets (>2) rather use synthetic_multivariate_dataset.py\n    @staticmethod\n    def mv_extreme_1(seed, n=2, k=2):\n        # train begins at 2100\n        length = 3000\n        train_split = 0.7\n        shift_config = {}\n        behavior = None\n        behavior_config = {}\n        baseline_config = {}\n        outlier_config = {\n            \'extreme\': [\n                {\n                    \'n\': 0,\n                    \'timestamps\': [\n                        (2192,), (2212,), (2258,), (2262,), (2319,), (2343,),\n                        (2361,), (2369,), (2428,), (2510,), (2512,), (2538,),\n                        (2567,), (2589,), (2695,), (2819,), (2892,), (2940,),\n                        (2952,), (2970,)\n                    ]\n                },\n                {\n                    \'n\': 1,\n                    \'timestamps\': [\n                        (2192,), (2212,), (2258,), (2262,), (2319,), (2343,),\n                        (2361,), (2369,), (2428,), (2510,), (2512,), (2538,),\n                        (2567,), (2589,), (2819,), (2892,), (2940,),\n                        (2952,), (2970,)\n                    ]\n                }\n            ]\n        }\n        pollution_config = {\n            \'extreme\': [\n                {\n                    \'n\': 0,\n                    \'timestamps\': [\n                        (222,), (254,), (258,), (317,), (322,), (366,), (399,),\n                        (736,), (769,), (770,), (784,), (795,), (819,), (842,),\n                        (1163,), (1214,), (1319,), (1352,), (1366,), (1485,),\n                        (1622,), (1639,), (1676,), (1686,), (1770,), (1820,),\n                        (1877,), (1913,), (1931,), (2070,)\n                    ]\n                },\n                {\n                    \'n\': 1,\n                    \'timestamps\': [\n                        (222,), (254,), (258,), (317,), (322,), (366,), (399,),\n                        (736,), (769,), (770,), (784,), (795,), (819,), (842,),\n                        (1163,), (1214,), (1319,), (1352,), (1366,), (1485,),\n                        (1622,), (1639,), (1676,), (1686,), (1770,), (1820,),\n                        (1877,), (1913,), (1931,), (2070,)\n                    ]\n                }\n            ]\n        }\n        label_config = {\n            \'outlier_at\': [\n                {\n                    \'timestamps\': [\n                        (2695,)\n                    ]\n                }\n            ]\n        }\n        random_state = seed\n\n        return SyntheticDataset(name=f\'Synthetic Multivariate Extreme Outliers (dim={n})\', file_name=\'mv_extreme1.pkl\',\n                                length=length, n=n, k=k,\n                                baseline_config=baseline_config, shift_config=shift_config,\n                                behavior=behavior, behavior_config=behavior_config,\n                                outlier_config=outlier_config, pollution_config=pollution_config,\n                                label_config=label_config,\n                                train_split=train_split, random_state=random_state)\n\n    @staticmethod\n    def mv_shift_1(seed, n=2, k=2):\n        length = 3000\n        train_split = 0.7\n        shift_config = {}\n        behavior = None\n        behavior_config = {}\n        baseline_config = {}\n        outlier_config = {\n            \'shift\': [\n                {\'n\': 0, \'timestamps\': [\n                    (2210, 2270), (2300, 2340), (2500, 2580), (2600, 2650), (2800, 2900)\n                ]},\n                {\'n\': 1, \'timestamps\': [\n                    (2210, 2270), (2500, 2580), (2800, 2900)\n                ]}\n            ]\n        }\n        pollution_config = {\n            \'shift\': [\n                {\'n\': 0, \'timestamps\': [\n                    (152, 248), (229, 285), (707, 779), (720, 754), (836, 901),\n                    (847, 928), (883, 989), (922, 987), (1258, 1351), (1289, 1340),\n                    (1401, 1424), (1717, 1742), (1808, 1836), (1828, 1895),\n                    (1830, 1891)]},\n                {\'n\': 1, \'timestamps\': [\n                    (152, 248), (229, 285), (707, 779), (720, 754), (836, 901),\n                    (847, 928), (883, 989), (922, 987), (1258, 1351), (1289, 1340),\n                    (1401, 1424), (1717, 1742), (1808, 1836), (1828, 1895),\n                    (1830, 1891)]}\n            ]\n        }\n        label_config = {\n            \'outlier_at\': [\n                {\n                    \'timestamps\': [\n                        (2300, 2340), (2600, 2650)\n                    ]\n                }\n            ]\n        }\n        random_state = seed\n\n        return SyntheticDataset(name=f\'Synthetic Multivariate Shift Outliers (dim={n})\', file_name=\'mv_shift1.pkl\',\n                                length=length, n=n, k=k,\n                                baseline_config=baseline_config, shift_config=shift_config,\n                                behavior=behavior, behavior_config=behavior_config,\n                                outlier_config=outlier_config, pollution_config=pollution_config,\n                                label_config=label_config,\n                                train_split=train_split, random_state=random_state)\n\n    @staticmethod\n    def mv_variance_1(seed, n=2, k=2):\n        length = 3000\n        train_split = 0.7\n        shift_config = {}\n        behavior = None\n        behavior_config = {}\n        baseline_config = {}\n        outlier_config = {\n            \'variance\': [{\'n\': 0, \'timestamps\': [(2300, 2310), (2400, 2420), (2500, 2550), (2800, 2900)]},\n                         {\'n\': 1, \'timestamps\': [(2300, 2310), (2400, 2420), (2800, 2900)]}],\n        }\n        pollution_config = {\n            \'variance\': [\n                {\'n\': 0, \'timestamps\': [\n                    (171, 280), (482, 675), (1104, 1366), (1519, 1724), (1996, 2159)]},\n                {\'n\': 1, \'timestamps\': [\n                    (171, 280), (482, 675), (1104, 1366), (1519, 1724), (1996, 2159)]},\n            ]\n        }\n        label_config = {\n            \'outlier_at\': [\n                {\n                    \'timestamps\': [\n                        (2500, 2550)\n                    ]\n                }\n            ]\n        }\n        random_state = seed\n\n        return SyntheticDataset(name=f\'Synthetic Multivariate Variance Outliers (dim={n})\',\n                                file_name=\'mv_variance1.pkl\',\n                                length=length, n=n, k=k,\n                                baseline_config=baseline_config, shift_config=shift_config,\n                                behavior=behavior, behavior_config=behavior_config,\n                                outlier_config=outlier_config, pollution_config=pollution_config,\n                                label_config=label_config,\n                                train_split=train_split, random_state=random_state)\n\n    @staticmethod\n    def mv_trend_1(seed, n=2, k=2):\n        length = 3000\n        train_split = 0.7\n        shift_config = {}\n        behavior = None\n        behavior_config = {}\n        baseline_config = {}\n        outlier_config = {\n            \'trend\': [{\'n\': 0, \'timestamps\': [(2200, 2400), (2450, 2480), (2700, 2950)]},\n                      {\'n\': 1, \'timestamps\': [(2200, 2400), (2450, 2480), (2500, 2550), (2700, 2950)]}]\n        }\n        pollution_config = {\n            \'trend\': [{\'n\': 0, \'timestamps\': [(200, 400), (550, 420), (500, 550), (1200, 1350)]},\n                      {\'n\': 1, \'timestamps\': [(200, 400), (550, 420), (500, 550), (1200, 1350)]}]\n        }\n        label_config = {\n            \'outlier_at\': [\n                {\n                    \'timestamps\': [\n                        (2500, 2550)\n                    ]\n                }\n            ]\n        }\n        random_state = seed\n\n        return SyntheticDataset(name=f\'Synthetic Multivariate Trend Outliers (dim={n})\', file_name=\'mv_trend1.pkl\',\n                                length=length, n=n, k=k,\n                                baseline_config=baseline_config, shift_config=shift_config,\n                                behavior=behavior, behavior_config=behavior_config,\n                                outlier_config=outlier_config, pollution_config=pollution_config,\n                                label_config=label_config,\n                                train_split=train_split, random_state=random_state)\n\n    @staticmethod\n    def mv_xor_extreme_1(seed, n=2, k=2):\n        # train begins at 2100\n        length = 3000\n        train_split = 0.7\n        shift_config = {}\n        behavior = None\n        behavior_config = {}\n        baseline_config = {}\n        outlier_config = {\n            \'extreme\': [\n                {\n                    \'n\': 0,\n                    \'timestamps\': [\n                        (2192,), (2262,), (2319,),\n                        (2361,), (2428,), (2512,),\n                        (2567,), (2695,), (2892,),\n                        (2952,)\n                    ]\n                },\n                {\n                    \'n\': 1,\n                    \'timestamps\': [\n                        (2212,), (2262,), (2343,),\n                        (2369,), (2510,), (2538,),\n                        (2589,), (2819,), (2940,),\n                        (2952,), (2970,)\n                    ]\n                }\n            ]\n        }\n        pollution_config = {\n            \'extreme\': [\n                {\n                    \'n\': 0,\n                    \'timestamps\': [\n                        (222,), (258,), (322,), (399,),\n                        (769,), (784,), (819,),\n                        (1214,), (1352,), (1485,),\n                        (1639,), (1686,), (1820,),\n                        (1913,), (2070,)\n                    ]\n                },\n                {\n                    \'n\': 1,\n                    \'timestamps\': [\n                        (254,), (317,), (366,),\n                        (736,), (770,), (795,), (842,),\n                        (1163,), (1319,), (1366,),\n                        (1622,), (1676,), (1770,),\n                        (1877,), (1931,),\n                    ]\n                }\n            ]\n        }\n        label_config = {\n            \'outlier_at\': [\n                {\n                    \'timestamps\': [\n                        (2262,), (2952,),\n                    ]\n                }\n            ]\n        }\n        random_state = seed\n\n        return SyntheticDataset(name=f\'Synthetic Multivariate XOR Extreme Outliers (dim={n})\',\n                                file_name=\'mv_xor_extreme1.pkl\', length=length, n=n, k=k,\n                                baseline_config=baseline_config, shift_config=shift_config,\n                                behavior=behavior, behavior_config=behavior_config,\n                                outlier_config=outlier_config, pollution_config=pollution_config,\n                                label_config=label_config,\n                                train_split=train_split, random_state=random_state)\n\n    @staticmethod\n    def behavior_sine_1(seed, cycle_length=200, n=1, k=1):\n        """"""Test seasonality (long term frequency)""""""\n        length = 3000\n        train_split = 0.7\n        shift_config = {}\n        behavior = sine_generator\n        behavior_config = {\'cycle_duration\': cycle_length, \'amplitude\': 0.1}\n        baseline_config = {}\n        outlier_config = {}\n        pollution_config = {}\n        random_state = seed\n\n        return SyntheticDataset(name=\'Synthetic Seasonal Outliers (dim={n})\', file_name=\'sine1.pkl\', length=length,\n                                n=n, k=k, baseline_config=baseline_config, shift_config=shift_config,\n                                behavior=behavior, behavior_config=behavior_config,\n                                outlier_config=outlier_config, pollution_config=pollution_config,\n                                train_split=train_split, random_state=random_state)\n\n    @staticmethod\n    def long_term_dependencies_missing(seed):\n        return LongTermDependencyDataset(\'Long Term Dependencies Missing\', \'long_term_missing\',\n                                         seed).build_missing_data([153, 155])\n\n    @staticmethod\n    def long_term_dependencies_height(seed):\n        return LongTermDependencyDataset(\'Long Term Dependencies Halved Height\', \'long_term_halved_height\',\n                                         seed).build_halved_height([153, 155])\n\n    @staticmethod\n    def long_term_dependencies_width(seed):\n        return LongTermDependencyDataset(\'Long Term Dependencies Irregular Length\', \'long_term_irregular_length\',\n                                         seed).build_irregular_widths([153, 155], [10, 30])\n'"
src/datasets/synthetic_dataset.py,0,"b""import numpy as np\nimport pandas as pd\nfrom agots.multivariate_generators.multivariate_data_generator import MultivariateDataGenerator\n\nfrom . import Dataset\n\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, length: int = 1000, n: int = 4, k: int = 2,\n                 shift_config: dict = None,\n                 behavior: object = None,\n                 behavior_config: dict = None,\n                 baseline_config: dict = None,\n                 outlier_config: dict = None,\n                 pollution_config: dict = None,\n                 label_config: dict = None,\n                 train_split: float = 0.7,\n                 random_state: int = None, **kwargs):\n        super().__init__(**kwargs)\n\n        self.length = length\n        self.n = n\n        self.k = k\n        self.shift_config = shift_config if shift_config is not None else {}\n        self.behavior = behavior\n        self.behavior_config = behavior_config if behavior_config is not None else {}\n        self.baseline_config = baseline_config if baseline_config is not None else {}\n        self.outlier_config = outlier_config if outlier_config is not None else {}\n        self.pollution_config = pollution_config if pollution_config is not None else {}\n        self.label_config = label_config\n        self.train_split = train_split\n        np.random.seed(random_state)\n\n    def load(self):\n        generator = MultivariateDataGenerator(self.length, self.n, self.k, shift_config=self.shift_config,\n                                              behavior=self.behavior, behavior_config=self.behavior_config)\n        generator.generate_baseline(**self.baseline_config)\n\n        train_split_point = int(self.train_split * self.length)\n        X_train = generator.add_outliers(self.pollution_config)[:train_split_point]\n        y_train = self._label_outliers(self.label_config or self.pollution_config)[:train_split_point]\n\n        X_test = generator.add_outliers(self.outlier_config)[train_split_point:]\n        y_test = self._label_outliers(self.label_config or self.outlier_config)[train_split_point:]\n\n        # Normalize\n        train_mean, train_std = np.mean(X_train, axis=0), np.std(X_train, axis=0)\n        X_train = (X_train - train_mean) / train_std\n        X_test = (X_test - train_mean) / train_std\n\n        self._data = X_train, y_train, X_test, y_test\n\n    def _label_outliers(self, config: dict) -> pd.Series:\n        timestamps = []\n        for _, outliers in config.items():\n            for outlier in outliers:\n                for ts in outlier['timestamps']:\n                    if len(ts) == 1:  # tuple length 1\n                        timestamps.append(int(*ts))\n                    else:\n                        timestamps.extend(range(ts[0], ts[1]))\n\n        y = np.zeros(self.length)\n        y[timestamps] = 1\n        return pd.Series(y)\n\n    def add_missing_values(self, missing_percentage: float, per_column: bool = True):\n        X_train, y_train, X_test, y_test = self._data\n        if per_column:\n            for col in X_train.columns:\n                missing_idxs = np.random.choice(len(X_train), int(missing_percentage * len(X_train)), replace=False)\n                X_train[col][missing_idxs] = np.nan\n        else:\n            missing_idxs = np.random.choice(len(X_train), int(missing_percentage * len(X_train)), replace=False)\n            X_train.iloc[missing_idxs] = [np.nan] * X_train.shape[-1]\n"""
src/datasets/synthetic_multivariate_dataset.py,0,"b'import logging\nfrom typing import Tuple, Callable\n\nimport numpy as np\nimport pandas as pd\n\nfrom . import Dataset\n\n\nclass SyntheticMultivariateDataset(Dataset):\n    def __init__(self,\n                 # anomaly_func: Lambda for curve values of 2nd dimension\n                 anomaly_func: Callable[[np.ndarray, bool, int], Tuple[np.ndarray, int, int]],\n                 name: str = \'Synthetic Multivariate Curve Outliers\',\n                 length: int = 5000,\n                 mean_curve_length: int = 40,  # varies between -5 and +5\n                 mean_curve_amplitude: int = 1,  # By default varies between -0.5 and 1.5\n                 pause_range: Tuple[int, int] = (5, 75),  # min and max value for this a pause\n                 labels_padding: int = 6,\n                 random_seed: int = None,\n                 features: int = 2,\n                 group_size: int = None,\n                 test_pollution: float = 0.5,\n                 train_pollution: float = 0,\n                 global_noise: float = 0.1,  # Noise added to all dimensions over the whole timeseries\n                 file_name: str = \'synthetic_mv1.pkl\'):\n        super().__init__(f\'{name} (f={anomaly_func.__name__})\', file_name)\n        self.length = length\n        self.mean_curve_length = mean_curve_length\n        self.mean_curve_amplitude = mean_curve_amplitude\n        self.global_noise = global_noise\n        self.anomaly_func = anomaly_func\n        self.pause_range = pause_range\n        self.labels_padding = labels_padding\n        self.random_seed = random_seed\n        self.test_pollution = test_pollution\n        self.train_pollution = train_pollution\n\n        assert features >= 2, \'At least two dimensions are required for generating MV outliers\'\n        self.features = features\n        assert group_size is None or (features >= group_size > 0), \'Group size may not be greater \' \\\n                                                                   \'than amount of dimensions\'\n        self.group_size = group_size or self.features\n        assert self.group_size <= self.features\n        if self.features % self.group_size == 1:  # How many dimensions each correlated group has\n            logging.warn(\'Group size results in one overhanging univariate group. Generating multivariate\'\n                         \'anomalies on univariate data is impossible.\')\n\n        if self.train_pollution > 0:\n            self.name = self.name + f\'(pol={self.train_pollution})\'\n\n    @staticmethod\n    def get_noisy_value(x, strength=1):\n        return x + np.random.random(np.shape(x)) * strength - strength / 2\n\n    # Use part of sinus to create a curve starting and ending with zero gradients.\n    # Using `length` and `amplitude` you can adjust it in both dimensions.\n    @staticmethod\n    def get_curve(length, amplitude):\n        # Transformed sinus curve: [-1, 1] -> [0, amplitude]\n        def curve(t: int):\n            return amplitude * (np.sin(t) / 2 + 0.5)\n\n        # Start and end of one curve section in sinus\n        from_ = 1.5 * np.pi\n        to_ = 3.5 * np.pi\n        return np.array([curve(t) for t in np.linspace(from_, to_, length)])\n\n    # Randomly adjust curve size by adding noise to the passed parameters\n    def get_random_curve(self, length_randomness=10, amplitude_randomness=1):\n        is_negative = np.random.choice([True, False])\n        sign = -1 if is_negative else 1\n        new_length = self.get_noisy_value(self.mean_curve_length, length_randomness)\n        new_amplitude = self.get_noisy_value(sign * self.mean_curve_amplitude, amplitude_randomness)\n        return self.get_curve(new_length, new_amplitude)\n\n    # The interval between two curves must be random so a detector doesn\'t recognize a pattern\n    def create_pause(self):\n        xmin, xmax = self.pause_range\n        diff = xmax - xmin\n        return xmin + np.random.randint(diff)\n\n    def add_global_noise(self, x):\n        return self.get_noisy_value(x, self.global_noise)\n\n    def generate_correlated_group(self, dimensions, pollution):\n        values = np.zeros((self.length, dimensions))\n        labels = np.zeros(self.length)\n\n        # First pos data points are noise (don\'t start directly with curve)\n        pos = self.create_pause()\n        values[:pos, :] = self.add_global_noise(values[:pos])\n\n        # Keep space (20) at the end\n        while pos < self.length - self.mean_curve_length - 20:\n            # General outline for the repeating curves, varying height and length\n            curve = self.get_random_curve()\n            # Outlier generation in second dimension\n            create_anomaly = np.random.choice([False, True], p=[1 - pollution, pollution])\n            # After curve add pause, only noise\n            end_of_interval = pos + len(curve) + self.create_pause()\n            self.insert_features(values[pos:end_of_interval, :], labels[pos:end_of_interval], curve, create_anomaly)\n            pos = end_of_interval\n        # rest of values is noise\n        values[:pos, :] = self.add_global_noise(values[:pos, :])\n        return pd.DataFrame(values), pd.Series(labels)\n\n    """"""\n        pollution: Portion of anomalous curves. Because it\'s not known how many curves there are\n            in the end. It\'s randomly chosen based on this value. To avoid anomalies set this to zero.\n    """"""\n\n    def generate_data(self, pollution):\n        value_dfs, label_series = [], []\n        for i in range(0, self.features, self.group_size):\n            values, labels = self.generate_correlated_group(min(self.group_size, self.features - i),\n                                                            pollution=pollution * self.group_size / self.features)\n            value_dfs.append(values)\n            label_series.append(labels)\n        labels = pd.Series(np.logical_or.reduce(label_series))\n        values = pd.concat(value_dfs, axis=1, ignore_index=True)\n        return values, labels\n\n    """"""\n        Insert values for curve and following pause over all dimensions.\n        interval_values is changed by reference so this function doesn\'t return anything.\n        (this is done by using numpy place function/slice operator)\n\n    """"""\n\n    def insert_features(self, interval_values: np.ndarray, interval_labels: np.ndarray,\n                        curve: np.ndarray, create_anomaly: bool):\n        # Randomly switch between dimensions for inserting the anomaly_func\n        anomaly_dim = np.random.randint(1, interval_values.shape[1])\n\n        # Insert curve and pause in first dimension (after adding the global noise)\n        for i in set(range(interval_values.shape[1])) - {anomaly_dim}:\n            # Add to interval_values because they already contain a shift based on the dimension\n            interval_values[:len(curve), i] += self.add_global_noise(curve)\n            interval_values[len(curve):, i] = self.add_global_noise(interval_values[len(curve):, i])\n\n        # Get values of anomaly_func and fill missing spots with noise\n        # anomaly_func function gets the clean curve values (not noisy)\n        interval_length = interval_values.shape[0]\n        anomaly_values, start, end = self.anomaly_func(curve, create_anomaly, interval_length)\n        assert len(anomaly_values) <= interval_length, f\'Interval too long: {len(anomaly_values)} > {interval_length}\'\n\n        interval_values[:len(anomaly_values), anomaly_dim] += self.add_global_noise(anomaly_values)\n        # Fill interval up with noisy zero values\n        interval_values[len(anomaly_values):, anomaly_dim] = self.add_global_noise(\n            interval_values[len(anomaly_values):, anomaly_dim])\n\n        # Add anomaly labels with slight padding (dont start with the first interval value).\n        # The padding is curve_length / padding_factor\n        if create_anomaly:\n            assert end > start and start >= 0, f\'Invalid anomaly indizes: {start} to {end}\'\n            padding = (end - start) // self.labels_padding\n            interval_labels[start + padding:end - padding] += 1\n\n    def load(self):\n        np.random.seed(self.random_seed)\n        X_train, y_train = self.generate_data(pollution=self.train_pollution)\n        X_test, y_test = self.generate_data(pollution=self.test_pollution)\n        self._data = X_train, y_train, X_test, y_test\n'"
src/evaluation/__init__.py,0,"b""from .evaluator import Evaluator\nfrom .plotter import Plotter\n\n__all__ = [\n    'Evaluator',\n    'Plotter',\n]\n"""
src/evaluation/config.py,0,"b""import logging\nimport os\nimport sys\n\nimport re\nimport time\n\n# Use: logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR, logging.CRITICAL\nLOG_LEVEL = logging.DEBUG\nCONSOLE_LOG_LEVEL = logging.INFO\n\n\ndef init_logging(output_dir='reports/logs'):\n    # Prepare directory and file path for storing the logs\n    timestamp = time.strftime('%Y-%m-%d-%H%M%S')\n    log_file_path = os.path.join(output_dir, '{}.log'.format(timestamp))\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Actually initialize the logging module\n    log_formatter = logging.Formatter(fmt='%(asctime)s [%(levelname)s] %(name)s: %(message)s',\n                                      datefmt='%Y-%m-%d %H:%M:%S')\n    root_logger = logging.getLogger()\n    root_logger.setLevel(LOG_LEVEL)\n    # Removes previous handlers (required for running pipeline multiple times)\n    root_logger.handlers = []\n\n    # Store logs in a log file in reports/logs\n    file_handler = logging.FileHandler(log_file_path)  # mode='w'\n    file_handler.setFormatter(log_formatter)\n    root_logger.addHandler(file_handler)\n\n    # Also print logs in the standard output\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setFormatter(log_formatter)\n    console_handler.setLevel(CONSOLE_LOG_LEVEL)\n    console_handler.addFilter(DebugModuleFilter(['^src\\.', '^root$']))\n    root_logger.addHandler(console_handler)\n\n    # Create logger instance for the config file\n    logger = logging.getLogger(__name__)\n    logger.debug('Logger initialized')\n\n\nclass DebugModuleFilter(logging.Filter):\n    def __init__(self, pattern=[]):\n        logging.Filter.__init__(self)\n        self.module_pattern = [re.compile(x) for x in pattern]\n\n    def filter(self, record):\n        # This filter assumes that we want INFO logging from all\n        # modules and DEBUG logging from only selected ones, but\n        # easily could be adapted for other policies.\n        if record.levelno == logging.DEBUG:\n            # e.g. src.evaluator.evaluation\n            return any([x.match(record.name) for x in self.module_pattern])\n        return True\n"""
src/evaluation/evaluator.py,0,"b'import gc\nimport logging\nimport os\nimport pickle\nimport re\nimport sys\nimport traceback\nfrom textwrap import wrap\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patheffects as path_effects\nfrom matplotlib.font_manager import FontProperties\nimport numpy as np\nimport pandas as pd\nimport progressbar\nimport time\nfrom sklearn.metrics import accuracy_score, fbeta_score\nfrom sklearn.metrics import precision_recall_fscore_support as prf\nfrom sklearn.metrics import roc_curve, auc\nfrom tabulate import tabulate\n\nfrom .config import init_logging\n\n\nclass Evaluator:\n    def __init__(self, datasets: list, detectors: callable, output_dir: {str} = None, seed: int = None,\n                 create_log_file=True):\n        """"""\n        :param datasets: list of datasets\n        :param detectors: callable that returns list of detectors\n        """"""\n        assert np.unique([x.name for x in datasets]).size == len(datasets), \'Some datasets have the same name!\'\n        self.datasets = datasets\n        self._detectors = detectors\n        self.output_dir = output_dir or \'reports\'\n        self.results = dict()\n        if create_log_file:\n            init_logging(os.path.join(self.output_dir, \'logs\'))\n        self.logger = logging.getLogger(__name__)\n        # Dirty hack: Is set by the main.py to insert results from multiple evaluator runs\n        self.benchmark_results = None\n        # Last passed seed value in evaluate()\n        self.seed = seed\n\n    @property\n    def detectors(self):\n        detectors = self._detectors(self.seed)\n        assert np.unique([x.name for x in detectors]).size == len(detectors), \'Some detectors have the same name!\'\n        return detectors\n\n    def set_benchmark_results(self, benchmark_result):\n        self.benchmark_results = benchmark_result\n\n    def export_results(self, name):\n        output_dir = os.path.join(self.output_dir, \'evaluators\')\n        os.makedirs(output_dir, exist_ok=True)\n        timestamp = time.strftime(\'%Y-%m-%d-%H%M%S\')\n        path = os.path.join(output_dir, f\'{name}-{timestamp}.pkl\')\n        self.logger.info(f\'Store evaluator results at {os.path.abspath(path)}\')\n        save_dict = {\n            \'datasets\': [x.name for x in self.datasets],\n            \'detectors\': [x.name for x in self.detectors],\n            \'benchmark_results\': self.benchmark_results,\n            \'results\': self.results,\n            \'output_dir\': self.output_dir,\n            \'seed\': int(self.seed),\n        }\n        with open(path, \'wb\') as f:\n            pickle.dump(save_dict, f)\n        return path\n\n    # Import benchmark_results if this evaluator uses the same detectors and datasets\n    # self.results are not available because they are overwritten by each run\n    def import_results(self, name):\n        output_dir = os.path.join(self.output_dir, \'evaluators\')\n        path = os.path.join(output_dir, f\'{name}.pkl\')\n        self.logger.info(f\'Read evaluator results at {os.path.abspath(path)}\')\n        with open(path, \'rb\') as f:\n            save_dict = pickle.load(f)\n\n        self.logger.debug(f\'Importing detectors {""; "".join(save_dict[""detectors""])}\')\n        my_detectors = [x.name for x in self.detectors]\n        assert np.array_equal(save_dict[\'detectors\'], my_detectors), \'Detectors should be the same\'\n\n        self.logger.debug(f\'Importing datasets {""; "".join(save_dict[""datasets""])}\')\n        my_datasets = [x.name for x in self.datasets]\n        assert np.array_equal(save_dict[\'datasets\'], my_datasets), \'Datasets should be the same\'\n\n        self.benchmark_results = save_dict[\'benchmark_results\']\n        self.seed = save_dict[\'seed\']\n        self.results = save_dict[\'results\']\n\n    @staticmethod\n    def get_accuracy_precision_recall_fscore(y_true: list, y_pred: list):\n        accuracy = accuracy_score(y_true, y_pred)\n        # warn_for=() avoids log warnings for any result being zero\n        precision, recall, f_score, _ = prf(y_true, y_pred, average=\'binary\', warn_for=())\n        if precision == 0 and recall == 0:\n            f01_score = 0\n        else:\n            f01_score = fbeta_score(y_true, y_pred, average=\'binary\', beta=0.1)\n        return accuracy, precision, recall, f_score, f01_score\n\n    @staticmethod\n    def get_auroc(det, ds, score):\n        if np.isnan(score).all():\n            score = np.zeros_like(score)\n        _, _, _, y_test = ds.data()\n        score_nonan = score.copy()\n        # Rank NaN below every other value in terms of anomaly score\n        score_nonan[np.isnan(score_nonan)] = np.nanmin(score_nonan) - sys.float_info.epsilon\n        fpr, tpr, _ = roc_curve(y_test, score_nonan)\n        return auc(fpr, tpr)\n\n    def get_optimal_threshold(self, det, y_test, score, steps=100, return_metrics=False):\n        maximum = np.nanmax(score)\n        minimum = np.nanmin(score)\n        threshold = np.linspace(minimum, maximum, steps)\n        metrics = list(self.get_metrics_by_thresholds(y_test, score, threshold))\n        metrics = np.array(metrics).T\n        anomalies, acc, prec, rec, f_score, f01_score = metrics\n        if return_metrics:\n            return anomalies, acc, prec, rec, f_score, f01_score, threshold\n        else:\n            return threshold[np.argmax(f_score)]\n\n    def evaluate(self):\n        for ds in progressbar.progressbar(self.datasets):\n            (X_train, y_train, X_test, y_test) = ds.data()\n            for det in progressbar.progressbar(self.detectors):\n                self.logger.info(f\'Training {det.name} on {ds.name} with seed {self.seed}\')\n                try:\n                    det.fit(X_train.copy())\n                    score = det.predict(X_test.copy())\n                    self.results[(ds.name, det.name)] = score\n                    try:\n                        self.plot_details(det, ds, score)\n                    except Exception:\n                        pass\n                except Exception as e:\n                    self.logger.error(f\'An exception occurred while training {det.name} on {ds}: {e}\')\n                    self.logger.error(traceback.format_exc())\n                    self.results[(ds.name, det.name)] = np.zeros_like(y_test)\n            gc.collect()\n\n    def benchmarks(self) -> pd.DataFrame:\n        df = pd.DataFrame()\n        for ds in self.datasets:\n            _, _, _, y_test = ds.data()\n            for det in self.detectors:\n                score = self.results[(ds.name, det.name)]\n                y_pred = self.binarize(score, self.get_optimal_threshold(det, y_test, np.array(score)))\n                acc, prec, rec, f1_score, f01_score = self.get_accuracy_precision_recall_fscore(y_test, y_pred)\n                score = self.results[(ds.name, det.name)]\n                auroc = self.get_auroc(det, ds, score)\n                df = df.append({\'dataset\': ds.name,\n                                \'algorithm\': det.name,\n                                \'accuracy\': acc,\n                                \'precision\': prec,\n                                \'recall\': rec,\n                                \'F1-score\': f1_score,\n                                \'F0.1-score\': f01_score,\n                                \'auroc\': auroc},\n                               ignore_index=True)\n        return df\n\n    def get_metrics_by_thresholds(self, y_test: list, score: list, thresholds: list):\n        for threshold in thresholds:\n            anomaly = self.binarize(score, threshold=threshold)\n            metrics = Evaluator.get_accuracy_precision_recall_fscore(y_test, anomaly)\n            yield (anomaly.sum(), *metrics)\n\n    def plot_scores(self, store=True):\n        detectors = self.detectors\n        plt.close(\'all\')\n        figures = []\n        for ds in self.datasets:\n            X_train, y_train, X_test, y_test = ds.data()\n            subtitle_loc = \'left\'\n            fig = plt.figure(figsize=(15, 15))\n            fig.canvas.set_window_title(ds.name)\n\n            sp = fig.add_subplot((2 * len(detectors) + 3), 1, 1)\n            sp.set_title(\'original training data\', loc=subtitle_loc)\n            for col in X_train.columns:\n                plt.plot(X_train[col])\n            sp = fig.add_subplot((2 * len(detectors) + 3), 1, 2)\n            sp.set_title(\'original test set\', loc=subtitle_loc)\n            for col in X_test.columns:\n                plt.plot(X_test[col])\n\n            sp = fig.add_subplot((2 * len(detectors) + 3), 1, 3)\n            sp.set_title(\'binary labels of test set\', loc=subtitle_loc)\n            plt.plot(y_test)\n\n            subplot_num = 4\n            for det in detectors:\n                sp = fig.add_subplot((2 * len(detectors) + 3), 1, subplot_num)\n                sp.set_title(f\'scores of {det.name}\', loc=subtitle_loc)\n                score = self.results[(ds.name, det.name)]\n                plt.plot(np.arange(len(score)), [x for x in score])\n                threshold_line = len(score) * [self.get_optimal_threshold(det, y_test, np.array(score))]\n                plt.plot([x for x in threshold_line])\n                subplot_num += 1\n\n                sp = fig.add_subplot((2 * len(detectors) + 3), 1, subplot_num)\n                sp.set_title(f\'binary labels of {det.name}\', loc=subtitle_loc)\n                plt.plot(np.arange(len(score)),\n                         [x for x in self.binarize(score, self.get_optimal_threshold(det, y_test, np.array(score)))])\n                subplot_num += 1\n            fig.subplots_adjust(top=0.9, hspace=0.4)\n            fig.tight_layout()\n            if store:\n                self.store(fig, f\'scores_{ds.name}\')\n            figures.append(fig)\n        return figures\n\n    def plot_threshold_comparison(self, steps=40, store=True):\n        detectors = self.detectors\n        plt.close(\'all\')\n        plots_shape = len(detectors), len(self.datasets)\n        fig, axes = plt.subplots(*plots_shape, figsize=(len(detectors) * 15, len(self.datasets) * 5))\n        # Ensure two dimensions for iteration\n        axes = np.array(axes).reshape(*plots_shape).T\n        plt.suptitle(\'Compare thresholds\', fontsize=10)\n        for ds, axes_row in zip(self.datasets, axes):\n            _, _, X_test, y_test = ds.data()\n\n            for det, ax in zip(detectors, axes_row):\n                score = np.array(self.results[(ds.name, det.name)])\n\n                anomalies, _, prec, rec, f_score, f01_score, thresh = self.get_optimal_threshold(\n                    det, y_test, score, return_metrics=True)\n\n                ax.plot(thresh, anomalies / len(y_test),\n                        label=fr\'anomalies ({len(y_test)} $\\rightarrow$ 1)\')\n                ax.plot(thresh, prec, label=\'precision\')\n                ax.plot(thresh, rec, label=\'recall\')\n                ax.plot(thresh, f_score, label=\'f_score\', linestyle=\'dashed\')\n                ax.plot(thresh, f01_score, label=\'f01_score\', linestyle=\'dashed\')\n                ax.set_title(f\'{det.name} on {ds.name}\')\n                ax.set_xlabel(\'Threshold\')\n                ax.legend()\n\n        # Avoid overlapping title and axis labels\n        plt.xlim([0.0, 1.0])\n        fig.subplots_adjust(top=0.9, hspace=0.4, right=1, left=0)\n        fig.tight_layout()\n        if store:\n            self.store(fig, \'metrics_by_thresholds\')\n        return fig\n\n    def plot_roc_curves(self, store=True):\n        detectors = self.detectors\n        plt.close(\'all\')\n        figures = []\n        for ds in self.datasets:\n            _, _, _, y_test = ds.data()\n            fig_scale = 3\n            fig = plt.figure(figsize=(fig_scale * len(detectors), fig_scale))\n            fig.canvas.set_window_title(ds.name + \' ROC\')\n            fig.suptitle(f\'ROC curve on {ds.name}\', fontsize=14, y=\'1.1\')\n            subplot_count = 1\n            for det in detectors:\n                self.logger.info(f\'Plotting ROC curve for {det.name} on {ds.name}\')\n                score = self.results[(ds.name, det.name)]\n                if np.isnan(score).all():\n                    score = np.zeros_like(score)\n                # Rank NaN below every other value in terms of anomaly score\n                score[np.isnan(score)] = np.nanmin(score) - sys.float_info.epsilon\n                fpr, tpr, _ = roc_curve(y_test, score)\n                roc_auc = auc(fpr, tpr)\n                plt.subplot(1, len(detectors), subplot_count)\n                plt.plot(fpr, tpr, color=\'darkorange\',\n                         lw=2, label=\'area = %0.2f\' % roc_auc)\n                subplot_count += 1\n                plt.plot([0, 1], [0, 1], color=\'navy\', lw=2, linestyle=\'--\')\n                plt.xlim([0.0, 1.0])\n                plt.ylim([0.0, 1.05])\n                plt.xlabel(\'False Positive Rate\')\n                plt.ylabel(\'True Positive Rate\')\n                plt.gca().set_aspect(\'equal\', adjustable=\'box\')\n                plt.title(\'\\n\'.join(wrap(det.name, 20)))\n                plt.legend(loc=\'lower right\')\n            plt.tight_layout()\n            if store:\n                self.store(fig, f\'roc_{ds.name}\')\n            figures.append(fig)\n        return figures\n\n    def plot_auroc(self, store=True, title=\'AUROC\'):\n        plt.close(\'all\')\n        self.benchmark_results[[\'dataset\', \'algorithm\', \'auroc\']].pivot(\n            index=\'algorithm\', columns=\'dataset\', values=\'auroc\').plot(kind=\'bar\')\n        plt.legend(loc=3, framealpha=0.5)\n        plt.xticks(rotation=20)\n        plt.ylabel(\'AUC\', rotation=\'horizontal\', labelpad=20)\n        plt.title(title)\n        plt.ylim(ymin=0, ymax=1)\n        plt.tight_layout()\n        if store:\n            self.store(plt.gcf(), \'auroc\', store_in_figures=True)\n\n    def plot_details(self, det, ds, score, store=True):\n        if not det.details:\n            return\n        plt.close(\'all\')\n        cmap = plt.get_cmap(\'inferno\')\n        _, _, X_test, y_test = ds.data()\n\n        grid = 0\n        for value in det.prediction_details.values():\n            grid += 1 if value.ndim == 1 else value.shape[0]\n        grid += X_test.shape[1]  # data\n        grid += 1 + 1  # score and gt\n\n        fig, axes = plt.subplots(grid, 1, figsize=(15, 1.5 * grid))\n\n        i = 0\n        c = cmap(i / grid)\n        axes[i].set_title(\'test data\')\n        for col in X_test.values.T:\n            axes[i].plot(col, color=c)\n            i += 1\n        c = cmap(i / grid)\n\n        axes[i].set_title(\'test gt data\')\n        axes[i].plot(y_test.values, color=c)\n        i += 1\n        c = cmap(i / grid)\n\n        axes[i].set_title(\'scores\')\n        axes[i].plot(score, color=c)\n        i += 1\n        c = cmap(i / grid)\n\n        for key, values in det.prediction_details.items():\n            axes[i].set_title(key)\n            if values.ndim == 1:\n                axes[i].plot(values, color=c)\n                i += 1\n            elif values.ndim == 2:\n                for v in values:\n                    axes[i].plot(v, color=c)\n                    i += 1\n            else:\n                self.logger.warning(\'plot_details: not sure what to do\')\n            c = cmap(i / grid)\n\n        fig.tight_layout()\n        if store:\n            self.store(fig, f\'details_{det.name}_{ds.name}\')\n        return fig\n\n    # create boxplot diagrams for auc values for each algorithm/dataset per algorithm/dataset\n    def create_boxplots(self, runs, data, detectorwise=True, store=True):\n        target = \'algorithm\' if detectorwise else \'dataset\'\n        grouped_by = \'dataset\' if detectorwise else \'algorithm\'\n        relevant_results = data[[\'algorithm\', \'dataset\', \'auroc\']]\n        figures = []\n        for det_or_ds in (self.detectors if detectorwise else self.datasets):\n            relevant_results[relevant_results[target] == det_or_ds.name].boxplot(by=grouped_by, figsize=(15, 15))\n            plt.suptitle(\'\')  # boxplot() adds a suptitle\n            plt.title(f\'AUC grouped by {grouped_by} for {det_or_ds.name} over {runs} runs\')\n            plt.ylim(ymin=0, ymax=1)\n            plt.tight_layout()\n            figures.append(plt.gcf())\n            if store:\n                self.store(plt.gcf(), f\'boxplot_auc_for_{det_or_ds.name}_{runs}_runs\', store_in_figures=True)\n        return figures\n\n    # create bar charts for averaged pipeline results per algorithm/dataset\n    def create_bar_charts(self, runs, detectorwise=True, store=True):\n        target = \'algorithm\' if detectorwise else \'dataset\'\n        grouped_by = \'dataset\' if detectorwise else \'algorithm\'\n        relevant_results = self.benchmark_results[[\'algorithm\', \'dataset\', \'auroc\']]\n        figures = []\n        for det_or_ds in (self.detectors if detectorwise else self.datasets):\n            relevant_results[relevant_results[target] == det_or_ds.name].plot(x=grouped_by, kind=\'bar\', figsize=(7, 7))\n            plt.suptitle(\'\')  # boxplot() adds a suptitle\n            plt.title(f\'AUC for {target} {det_or_ds.name} over {runs} runs\')\n            plt.ylim(ymin=0, ymax=1)\n            plt.tight_layout()\n            figures.append(plt.gcf())\n            if store:\n                self.store(plt.gcf(), f\'barchart_auc_for_{det_or_ds.name}_{runs}_runs\', store_in_figures=True)\n        return figures\n\n    def store(self, fig, title, extension=\'pdf\', no_counters=False, store_in_figures=False):\n        timestamp = time.strftime(\'%Y-%m-%d-%H%M%S\')\n        if store_in_figures:\n            output_dir = os.path.join(self.output_dir, \'figures\')\n        else:\n            output_dir = os.path.join(self.output_dir, \'figures\', f\'seed-{self.seed}\')\n        os.makedirs(output_dir, exist_ok=True)\n        counters_str = \'\' if no_counters else f\'-{len(self.detectors)}-{len(self.datasets)}\'\n        path = os.path.join(output_dir, f\'{title}{counters_str}-{timestamp}.{extension}\')\n        fig.savefig(path)\n        self.logger.info(f\'Stored plot at {path}\')\n\n    def store_text(self, content, title, extension=\'txt\'):\n        timestamp = int(time.time())\n        output_dir = os.path.join(self.output_dir, \'tables\', f\'seed-{self.seed}\')\n        path = os.path.join(output_dir, f\'{title}-{len(self.detectors)}-{len(self.datasets)}-{timestamp}.{extension}\')\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        with open(path, \'w\') as f:\n            f.write(content)\n        self.logger.info(f\'Stored {extension} file at {path}\')\n\n    def print_merged_table_per_dataset(self, results):\n        for ds in self.datasets:\n            table = tabulate(results[results[\'dataset\'] == ds.name], headers=\'keys\', tablefmt=\'psql\')\n            self.logger.info(f\'Dataset: {ds.name}\\n{table}\')\n\n    def gen_merged_latex_per_dataset(self, results, title_suffix=None, store=True):\n        title = f\'latex_merged{f""_{title_suffix}"" if title_suffix else """"}\'\n        content = \'\'\n        for ds in self.datasets:\n            content += f\'\'\'{ds.name}:\\n\\n{tabulate(results[results[\'dataset\'] == ds.name],\n                                                   headers=\'keys\', tablefmt=\'latex\')}\\n\\n\'\'\'\n        if store:\n            self.store_text(content=content, title=title, extension=\'tex\')\n        return content\n\n    def print_merged_table_per_algorithm(self, results):\n        for det in self.detectors:\n            table = tabulate(results[results[\'algorithm\'] == det.name], headers=\'keys\', tablefmt=\'psql\')\n            self.logger.info(f\'Detector: {det.name}\\n{table}\')\n\n    def gen_merged_latex_per_algorithm(self, results, title_suffix=None, store=True):\n        title = f\'latex_merged{f""_{title_suffix}"" if title_suffix else """"}\'\n        content = \'\'\n        for det in self.detectors:\n            content += f\'\'\'{det.name}:\\n\\n{tabulate(results[results[\'algorithm\'] == det.name],\n                                   headers=\'keys\', tablefmt=\'latex\')}\\n\\n\'\'\'\n        if store:\n            self.store_text(content=content, title=title, extension=\'tex\')\n        return content\n\n    @staticmethod\n    def translate_var_key(key_name):\n        if key_name == \'pol\':\n            return \'Pollution\'\n        if key_name == \'mis\':\n            return \'Missing\'\n        if key_name == \'extremeness\':\n            return \'Extremeness\'\n        if key_name == \'f\':\n            return \'Multivariate\'\n        # self.logger(\'Unexpected dataset name (unknown variable in name)\')\n        return None\n\n    @staticmethod\n    def get_key_and_value(dataset_name):\n        # Extract var name and value from dataset name\n        var_re = re.compile(r\'.+\\((\\w*)=(.*)\\)\')\n        # e.g. \'Syn Extreme Outliers (pol=0.1)\'\n        match = var_re.search(dataset_name)\n        if not match:\n            # self.logger.warn(\'Unexpected dataset name (not variable in name)\')\n            return \'-\', dataset_name\n        var_key = match.group(1)\n        var_value = match.group(2)\n        return Evaluator.translate_var_key(var_key), var_value\n\n    @staticmethod\n    def get_dataset_types(mi_df):\n        types = mi_df.index.get_level_values(\'Type\')\n        indexes = np.unique(types, return_index=True)[1]\n        return [types[index] for index in sorted(indexes)]\n\n    @staticmethod\n    def insert_multi_index_yaxis(ax, mi_df):\n        type_title_offset = -1.6  # depends on string length of xaxis ticklabels\n\n        datasets = mi_df.index\n        dataset_types = Evaluator.get_dataset_types(mi_df)  # Returns unique entries keeping original order\n        logging.getLogger(__name__).debug(\'Plotting heatmap for groups {"" "".join(dataset_types)}\')\n\n        ax.set_yticks(np.arange(len(datasets)))\n        ax.set_yticklabels([x[1] for x in datasets])\n\n        y_axis_title_pos = 0  # Store at which position we are for plotting the next title\n        for idx, dataset_type in enumerate(dataset_types):\n            section_frame = mi_df.iloc[mi_df.index.get_level_values(\'Type\') == dataset_type]\n            # Somehow it\'s sorted by its occurence (which is what we want here)\n            dataset_levels = section_frame.index.remove_unused_levels().levels[1]\n            title_pos = y_axis_title_pos + 0.5 * (len(dataset_levels) - 1)\n            ax.text(type_title_offset, title_pos, dataset_type, ha=\'center\', va=\'center\', rotation=90,\n                    fontproperties=FontProperties(weight=\'bold\'))\n            if idx < len(dataset_types) - 1:\n                sep_pos = y_axis_title_pos + (len(dataset_levels) - 0.6)\n                ax.text(-0.5, sep_pos, \'_\' * int(type_title_offset * -10), ha=\'right\', va=\'center\')\n            y_axis_title_pos += len(dataset_levels)\n\n    @staticmethod\n    def to_multi_index_frame(evaluators):\n        evaluator = evaluators[0]\n        for other_evaluator in evaluators[1:]:\n            assert evaluator.detectors == other_evaluator.detectors, \'All evaluators should use the same detectors\'\n        pivot_benchmarks = [ev.benchmark_results.pivot(index=\'dataset\', columns=\'algorithm\',\n                                                       values=\'auroc\') for ev in evaluators]\n\n        concat_benchmarks = pd.concat(pivot_benchmarks)\n        auroc_matrix = concat_benchmarks.groupby([\'dataset\']).mean()\n\n        datasets = [[evaluator.get_key_and_value(str(d)) for d in ev.index.values]\n                    for ev in pivot_benchmarks]\n        datasets = [tuple(d) for d in np.concatenate(datasets)]  # Required for MultiIndex.from_tuples\n        datasets = pd.MultiIndex.from_tuples(datasets, names=[\'Type\', \'Level\'])\n        auroc_matrix.index = datasets\n        return auroc_matrix\n\n    def get_multi_index_dataframe(self):\n        return Evaluator.to_multi_index_frame([self])\n\n    @staticmethod\n    def plot_heatmap(evaluators, store=True):\n        mi_df = Evaluator.to_multi_index_frame(evaluators)\n        detectors, datasets = mi_df.columns, mi_df.index\n\n        fig, ax = plt.subplots(figsize=(len(detectors) + 2, len(datasets)))\n        im = ax.imshow(mi_df, cmap=plt.get_cmap(\'YlOrRd\'), vmin=0, vmax=1)\n        plt.colorbar(im)\n\n        # Show MultiIndex for ordinate\n        Evaluator.insert_multi_index_yaxis(ax, mi_df)\n\n        # Rotate the tick labels and set their alignment.\n        ax.set_xticks(np.arange(len(detectors)))\n        ax.set_xticklabels(detectors)\n        plt.setp(ax.get_xticklabels(), rotation=45, ha=\'right\', rotation_mode=\'anchor\')\n\n        # Loop over data dimensions and create text annotations.\n        for i in range(len(detectors)):\n            for j in range(len(datasets)):\n                ax.text(i, j, f\'{mi_df.iloc[j, i]:.2f}\', ha=\'center\', va=\'center\', color=\'w\',\n                        path_effects=[path_effects.withSimplePatchShadow(\n                            offset=(1, -1), shadow_rgbFace=\'b\', alpha=0.9)])\n\n        ax.set_title(\'AUROC over all datasets and detectors\')\n        # Prevent bug where x axis ticks are completely outside of bounds (matplotlib/issues/5456)\n        if len(datasets) > 2:\n            fig.tight_layout()\n        if store:\n            evaluators[0].store(fig, \'heatmap\', no_counters=True, store_in_figures=True)\n        return fig\n\n    def plot_single_heatmap(self, store=True):\n        Evaluator.plot_heatmap([self], store)\n\n    @staticmethod\n    def get_printable_runs_results(results):\n        print_order = [\'dataset\', \'algorithm\', \'accuracy\', \'precision\', \'recall\', \'F1-score\', \'F0.1-score\', \'auroc\']\n        rename_columns = [col for col in print_order if col not in [\'dataset\', \'algorithm\']]\n\n        # calc std and mean for each algorithm per dataset\n        std_results = results.groupby([\'dataset\', \'algorithm\']).std(ddof=0).fillna(0)\n        # get rid of multi-index\n        std_results = std_results.reset_index()\n        std_results = std_results[print_order]\n        std_results.rename(inplace=True, index=str,\n                           columns=dict([(old_col, old_col + \'_std\') for old_col in rename_columns]))\n\n        avg_results = results.groupby([\'dataset\', \'algorithm\'], as_index=False).mean()\n        avg_results = avg_results[print_order]\n\n        avg_results_renamed = avg_results.rename(\n            index=str, columns=dict([(old_col, old_col + \'_avg\') for old_col in rename_columns]))\n        return std_results, avg_results, avg_results_renamed\n\n    def gen_merged_tables(self, results, title_suffix=None, store=True):\n        title_suffix = f\'_{title_suffix}\' if title_suffix else \'\'\n        std_results, avg_results, avg_results_renamed = Evaluator.get_printable_runs_results(results)\n\n        ds_title_suffix = f\'per_dataset{title_suffix}\'\n        self.print_merged_table_per_dataset(std_results)\n        self.gen_merged_latex_per_dataset(std_results, f\'std_{ds_title_suffix}\', store=store)\n\n        self.print_merged_table_per_dataset(avg_results_renamed)\n        self.gen_merged_latex_per_dataset(avg_results_renamed, f\'avg_{ds_title_suffix}\', store=store)\n\n        det_title_suffix = f\'per_algorithm{title_suffix}\'\n        self.print_merged_table_per_algorithm(std_results)\n\n        self.gen_merged_latex_per_algorithm(std_results, f\'std_{det_title_suffix}\', store=store)\n        self.print_merged_table_per_algorithm(avg_results_renamed)\n        self.gen_merged_latex_per_algorithm(avg_results_renamed, f\'avg_{det_title_suffix}\', store=store)\n\n    def binarize(self, score, threshold=None):\n        threshold = threshold if threshold is not None else self.threshold(score)\n        score = np.where(np.isnan(score), np.nanmin(score) - sys.float_info.epsilon, score)\n        return np.where(score >= threshold, 1, 0)\n\n    def threshold(self, score):\n        return np.nanmean(score) + 2 * np.nanstd(score)\n'"
src/evaluation/plotter.py,0,"b'import os\nimport pickle\nimport logging\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom .evaluator import Evaluator\n\n# For supporting pickles from old versions we need to map them\nNAMES_TRANSLATION = {\n    \'DAGMM_NNAutoEncoder_withWindow\': \'DAGMM-NW\',\n    \'DAGMM_NNAutoEncoder_withoutWindow\': \'DAGMM-NN\',\n    \'DAGMM_LSTMAutoEncoder_withWindow\': \'DAGMM-LW\',\n    \'Recurrent EBM\': \'REBM\',\n}\n\n\nclass Plotter:\n    def __init__(self, output_dir, pickle_dirs=None, dataset_names=None, detector_names=None):\n        self.output_dir = output_dir\n        self.dataset_names = dataset_names\n        self.detector_names = detector_names\n        self.results = None\n        self.logger = logging.getLogger(__name__)\n        if pickle_dirs is not None:\n            self.results = self.import_results_for_runs(pickle_dirs)\n\n    # pickle_dirs is an array of directories where to look for a\n    # subdirectory \'evaluators\' with stored pickles. The datasets and detectors\n    # used for these pickles must match the ones passed to this Plotter instance\n    def import_results_for_runs(self, pickle_dirs=[\'data\']):\n        if not isinstance(pickle_dirs, list):\n            pickle_dirs = [pickle_dirs]\n        all_results = []\n        for dir_ in pickle_dirs:\n            for path in os.listdir(os.path.join(dir_, \'evaluators\')):\n                self.logger.debug(""Importing evaluator from \'{path}\'"")\n                with open(os.path.join(dir_, \'evaluators\', path), \'rb\') as f:\n                    save_dict = pickle.load(f)\n                benchmark_results = save_dict[\'benchmark_results\']\n                assert self.dataset_names is None or np.array_equal(self.dataset_names, save_dict[\'datasets\']), \\\n                    \'Runs should be executed on same datasets\'\n                self.dataset_names = save_dict[\'datasets\']\n                self.detector_names = save_dict[\'detectors\']\n                if benchmark_results is not None:\n                    all_results.append(benchmark_results)\n                else:\n                    self.logger.warn(\'benchmark_results was None\')\n        return all_results\n\n    # --- Final plot functions ----------------------------------------------- #\n\n    # results is an array of benchmark_results (e.g. returned by get_results_for_runs)\n    def barplots(self, title):\n        if len(self.detector_names) == 1:\n            return self.single_barplot(title)\n        aurocs = [x[[\'algorithm\', \'dataset\', \'auroc\']] for x in self.results]\n        aurocs_df = pd.concat(aurocs, axis=0, ignore_index=True)\n\n        fig, axes = plt.subplots(\n            ncols=len(self.detector_names), figsize=(1.5 * len(self.detector_names), 4), sharey=True)\n        for ax, det in zip(axes.flat, self.detector_names):\n            self._styled_boxplot(ax, aurocs_df, det)\n\n        fig.subplots_adjust(wspace=0)\n        fig.suptitle(f\'Area under ROC for {title} (runs={len(self.results)})\')\n        self.store(fig, f\'boxplot-experiment-{title}\', \'pdf\', bbox_inches=\'tight\')\n        return fig\n\n    def lineplot(self, title):\n        self.logger.warn(\'Final lineplot function is not implemented\')\n        pass\n\n    def heatmap(self, title):\n        self.logger.warn(\'Final heatmap function is not implemented\')\n        pass\n\n    # --- Helper functions --------------------------------------------------- #\n\n    def single_barplot(self, title):\n        aurocs = [x[[\'algorithm\', \'dataset\', \'auroc\']] for x in self.results]\n        aurocs_df = pd.concat(aurocs, axis=0, ignore_index=True)\n        det = self.detector_names[0]\n\n        fig, ax = plt.subplots(figsize=(4, 4))\n        self._styled_boxplot(ax, aurocs_df, det)\n        ax.set_xlabel(None)\n\n        final_det_name = NAMES_TRANSLATION.get(det, det)\n        fig.subplots_adjust(wspace=0)\n        fig.suptitle(f\'Area under ROC for {final_det_name} (runs={len(self.results)})\')\n        self.store(fig, f\'boxplot-{final_det_name}-{title}\', \'pdf\', bbox_inches=\'tight\')\n        return fig\n\n    def _styled_boxplot(self, ax, aurocs_df, det):\n        values = aurocs_df[aurocs_df[\'algorithm\'] == det].drop(columns=\'algorithm\')\n        ds_groups = values.groupby(\'dataset\')\n        ax.boxplot([ds_groups.get_group(x)[\'auroc\'].values for x in self.dataset_names],\n                   positions=np.linspace(0, 1, len(self.dataset_names)), widths=0.15,\n                   medianprops={\'linewidth\': 1},\n                   whiskerprops={\'color\': \'darkblue\', \'linestyle\': \'--\'},\n                   flierprops={\'markersize\': 3},\n                   boxprops={\'color\': \'darkblue\'})\n        ax.set_xticklabels([f\'{float(Evaluator.get_key_and_value(x)[1]):.2f}\' for x in self.dataset_names],\n                           rotation=90)\n\n        ax.set_xlabel(NAMES_TRANSLATION.get(det, det))\n        ax.set_ylim((0, 1.05))\n        ax.yaxis.grid(True)\n        ax.set_xlim((-0.15, 1.15))\n        ax.margins(0.05)\n\n    def store(self, fig, title, extension=\'pdf\', **kwargs):\n        timestamp = time.strftime(\'%Y-%m-%d-%H%M%S\')\n        output_dir = os.path.join(self.output_dir, \'figures\')\n        os.makedirs(output_dir, exist_ok=True)\n        path = os.path.join(output_dir, f\'{title}-{timestamp}.{extension}\')\n        fig.savefig(path, **kwargs)\n        self.logger.info(f\'Stored plot at {path}\')\n'"
