file_path,api_count,code
train_val.py,14,"b'# --------------------------------------------------------\n# CGNL Network\n# Copyright (c) 2018 Kaiyu Yue\n# Licensed under The MIT License [see LICENSE for details]\n# --------------------------------------------------------\n\nimport os\nimport argparse\nimport time\nimport shutil\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.nn.parallel\n\nfrom torchvision import transforms\nfrom termcolor import cprint\nfrom lib import dataloader\nfrom model import resnet\n\n# torch version\ncprint(\'=> Torch Vresion: \' + torch.__version__, \'green\')\n\n# args\nparser = argparse.ArgumentParser(description=\'PyTorch Training\')\nparser.add_argument(\'--debug\', \'-d\', dest=\'debug\', action=\'store_true\',\n        help=\'enable debug mode\')\nparser.add_argument(\'--warmup\', \'-w\', dest=\'warmup\', action=\'store_true\',\n        help=\'using warmup strategy\')\nparser.add_argument(\'--print-freq\', \'-p\', default=1, type=int, metavar=\'N\',\n        help=\'print frequency (default: 10)\')\nparser.add_argument(\'--nl-nums\', default=0, type=int, metavar=\'N\',\n        help=\'number of the NL | CGNL block (default: 0)\')\nparser.add_argument(\'--nl-type\', default=None, type=str,\n        help=\'choose NL | CGNL | CGNLx block to add (default: None)\')\nparser.add_argument(\'--arch\', default=\'50\', type=str,\n        help=\'the depth of resnet (default: 50)\')\nparser.add_argument(\'--valid\', \'-v\', dest=\'valid\',\n        action=\'store_true\', help=\'just run validation\')\nparser.add_argument(\'--checkpoints\', default=\'\', type=str,\n        help=\'the dir of checkpoints\')\nparser.add_argument(\'--dataset\', default=\'cub\', type=str,\n        help=\'cub | imagenet (default: cub)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.01, type=float, metavar=\'LR\',\n        help=\'initial learning rate (default: 0.01)\')\n\nbest_prec1 = 0\nbest_prec5 = 0\n\ndef main():\n    global args\n    global best_prec1, best_prec5\n\n    args = parser.parse_args()\n\n    # simple args\n    debug = args.debug\n    if debug: cprint(\'=> WARN: Debug Mode\', \'yellow\')\n\n    dataset = args.dataset\n    num_classes = 200 if dataset == \'cub\' else 1000\n    base_size = 512 if dataset == \'cub\' else 256\n    pool_size = 14 if base_size == 512 else 7\n    workers = 0 if debug else 8\n    batch_size = 2 if debug else 256\n    if base_size == 512 and \\\n        args.arch == \'152\':\n        batch_size = 128\n    drop_ratio = 0.1\n    lr_drop_epoch_list = [31, 61, 81]\n    epochs = 100\n    eval_freq = 1\n    gpu_ids = [0] if debug else [0,1,2,3,4,5,6,7]\n    crop_size = 224 if base_size == 256 else 448\n\n    # args for the nl and cgnl block\n    arch = args.arch\n    nl_type  = args.nl_type # \'cgnl\' | \'cgnlx\' | \'nl\'\n    nl_nums  = args.nl_nums # 1: stage res4\n\n    # warmup setting\n    WARMUP_LRS = [args.lr * (drop_ratio**len(lr_drop_epoch_list)), args.lr]\n    WARMUP_EPOCHS = 10\n\n    # data loader\n    if dataset == \'cub\':\n        data_root = \'data/cub\'\n        imgs_fold = os.path.join(data_root, \'images\')\n        train_ann_file = os.path.join(data_root, \'cub_train.list\')\n        valid_ann_file = os.path.join(data_root, \'cub_val.list\')\n    elif dataset == \'imagenet\':\n        data_root = \'data/imagenet\'\n        imgs_fold = os.path.join(data_root)\n        train_ann_file = os.path.join(data_root, \'imagenet_train.list\')\n        valid_ann_file = os.path.join(data_root, \'imagenet_val.list\')\n    else:\n        raise NameError(""WARN: The dataset \'{}\' is not supported yet."")\n\n    train_dataset = dataloader.ImgLoader(\n            root = imgs_fold,\n            ann_file = train_ann_file,\n            transform = transforms.Compose([\n                transforms.RandomResizedCrop(\n                    size=crop_size, scale=(0.08, 1.25)),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    [0.485, 0.456, 0.406],\n                    [0.229, 0.224, 0.225])\n                ]))\n\n    val_dataset = dataloader.ImgLoader(\n            root = imgs_fold,\n            ann_file = valid_ann_file,\n            transform = transforms.Compose([\n                transforms.Resize(base_size),\n                transforms.CenterCrop(crop_size),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    [0.485, 0.456, 0.406],\n                    [0.229, 0.224, 0.225])\n                ]))\n\n    train_loader = torch.utils.data.DataLoader(\n            train_dataset,\n            batch_size = batch_size,\n            shuffle = True,\n            num_workers = workers,\n            pin_memory = True)\n\n    val_loader = torch.utils.data.DataLoader(\n            val_dataset,\n            batch_size = batch_size,\n            shuffle = False,\n            num_workers = workers,\n            pin_memory = True)\n\n    # build model\n    model = resnet.model_hub(arch,\n                             pretrained=True,\n                             nl_type=nl_type,\n                             nl_nums=nl_nums,\n                             pool_size=pool_size)\n\n    # change the fc layer\n    model._modules[\'fc\'] = torch.nn.Linear(in_features=2048,\n                                           out_features=num_classes)\n    torch.nn.init.kaiming_normal_(model._modules[\'fc\'].weight,\n                                  mode=\'fan_out\', nonlinearity=\'relu\')\n    print(model)\n\n    # parallel\n    model = torch.nn.DataParallel(model, device_ids=gpu_ids).cuda()\n\n    # define loss function (criterion) and optimizer\n    criterion = nn.CrossEntropyLoss().cuda()\n\n    # optimizer\n    optimizer = torch.optim.SGD(\n            model.parameters(),\n            args.lr,\n            momentum=0.9,\n            weight_decay=1e-4)\n\n    # cudnn\n    cudnn.benchmark = True\n\n    # warmup\n    if args.warmup:\n        epochs += WARMUP_EPOCHS\n        lr_drop_epoch_list = list(\n                np.array(lr_drop_epoch_list) + WARMUP_EPOCHS)\n        cprint(\'=> WARN: warmup is used in the first {} epochs\'.format(\n            WARMUP_EPOCHS), \'yellow\')\n\n    # valid\n    if args.valid:\n        cprint(\'=> WARN: Validation Mode\', \'yellow\')\n        print(\'start validation ...\')\n        checkpoint_fold = args.checkpoints\n        checkpoint_best = os.path.join(checkpoint_fold, \'model_best.pth.tar\')\n        print(\'=> loading state_dict from {}\'.format(checkpoint_best))\n        model.load_state_dict(\n                torch.load(checkpoint_best)[\'state_dict\'])\n        prec1, prec5 = validate(val_loader, model, criterion)\n        print(\' * Final Accuracy: Prec@1 {:.3f}, Prec@5 {:.3f}\'.format(prec1, prec5))\n        exit(0)\n\n    # train\n    print(\'start training ...\')\n    for epoch in range(0, epochs):\n        current_lr = adjust_learning_rate(optimizer, drop_ratio, epoch, lr_drop_epoch_list,\n                                          WARMUP_EPOCHS, WARMUP_LRS)\n        # train one epoch\n        train(train_loader, model, criterion, optimizer, epoch, epochs, current_lr)\n\n        if nl_nums > 0:\n            checkpoint_name = \'{}-r-{}-w-{}{}-block.pth.tar\'.format(dataset, arch, nl_nums, nl_type)\n        else:\n            checkpoint_name = \'{}-r-{}-base.pth.tar\'.format(dataset, arch)\n\n        if (epoch + 1) % eval_freq == 0:\n            prec1, prec5 = validate(val_loader, model, criterion)\n            is_best = prec1 > best_prec1\n            best_prec1 = max(prec1, best_prec1)\n            best_prec5 = max(prec5, best_prec5)\n            print(\' * Best accuracy: Prec@1 {:.3f}, Prec@5 {:.3f}\'.format(best_prec1, best_prec5))\n            save_checkpoint({\n                \'epoch\': epoch + 1,\n                \'state_dict\': model.state_dict(),\n                \'best_prec1\': best_prec1,\n                \'optimizer\' : optimizer.state_dict(),\n            }, is_best, filename=checkpoint_name)\n\n\ndef train(train_loader, model, criterion, optimizer, epoch, epochs, current_lr):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (input, target) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        target = target.cuda(non_blocking=True)\n\n        # compute output\n        output = model(input)\n        loss = criterion(output, target)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n        losses.update(loss.item(), input.size(0))\n        top1.update(prec1[0], input.size(0))\n        top5.update(prec5[0], input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print(\'Epoch: [{0:3d}/{1:3d}][{2:3d}/{3:3d}]\\t\'\n                  \'LR: {lr:.7f}\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                  \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                   epoch, epochs, i, len(train_loader), \n                   lr=current_lr, batch_time=batch_time,\n                   data_time=data_time, loss=losses, top1=top1, top5=top5))\n\n\ndef validate(val_loader, model, criterion):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    with torch.no_grad():\n        end = time.time()\n        for i, (input, target) in enumerate(val_loader):\n            target = target.cuda(non_blocking=True)\n\n            # compute output\n            output = model(input)\n            loss = criterion(output, target)\n\n            # measure accuracy and record loss\n            prec1, prec5 = accuracy(output, target, topk=(1, 5))\n            losses.update(loss.item(), input.size(0))\n            top1.update(prec1[0], input.size(0))\n            top5.update(prec5[0], input.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % args.print_freq == 0:\n                print(\'Test: [{0}/{1}]\\t\'\n                      \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                      \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                      \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                      \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                       i, len(val_loader), batch_time=batch_time, loss=losses,\n                       top1=top1, top5=top5))\n\n        print(\' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}\'\n              .format(top1=top1, top5=top5))\n\n    return top1.avg, top5.avg\n\n\ndef adjust_learning_rate(optimizer, drop_ratio, epoch, lr_drop_epoch_list,\n                         WARMUP_EPOCHS, WARMUP_LRS):\n    if args.warmup and epoch < WARMUP_EPOCHS:\n        # achieve the warmup lr\n        lrs = np.linspace(WARMUP_LRS[0], WARMUP_LRS[1], num=WARMUP_EPOCHS)\n        cprint(\'=> warmup lrs {}\'.format(lrs), \'green\')\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = lrs[epoch]\n        current_lr = lrs[epoch]\n    else:\n        decay = drop_ratio if epoch in lr_drop_epoch_list else 1.0\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = args.lr * decay\n        args.lr *= decay\n        current_lr = args.lr\n    return current_lr\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k\n    """"""\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\ndef save_checkpoint(state, is_best, filename=\'checkpoint.pth.tar\'):\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, \'model_best.pth.tar\')\n\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nif __name__ == \'__main__\':\n    main()\n'"
lib/dataloader.py,1,"b'# --------------------------------------------------------\n# CGNL Network\n# Copyright (c) 2018 Kaiyu Yue\n# Licensed under The MIT License [see LICENSE for details]\n# --------------------------------------------------------\n\n""""""Functions for dataloader\n""""""\n\nimport os\nimport torch.utils.data as data\nfrom PIL import Image, ImageFile\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nclass ImgLoader(data.Dataset):\n    def __init__(self, root, ann_file, transform=None, target_transform=None):\n        print(\'=> loading annotations from: \' + os.path.basename(ann_file) + \' ...\')\n        self.root = root\n        with open(ann_file, \'r\') as f:\n            self.imgs = f.readlines()\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __getitem__(self, index):\n        ls = self.imgs[index].strip().split()\n        img_path = ls[0]\n        target = int(ls[1])\n        img = Image.open(\n                os.path.join(self.root, img_path)).convert(\'RGB\')\n        return self.transform(img), target\n\n    def __len__(self):\n        return len(self.imgs)\n\n'"
model/resnet.py,26,"b'# --------------------------------------------------------\n# CGNL Network\n# Copyright (c) 2018 Kaiyu Yue\n# Licensed under The MIT License [see LICENSE for details]\n# --------------------------------------------------------\n\n""""""Functions for model building.\n   Based on https://github.com/pytorch/vision\n""""""\n\nimport math\nimport torch\nimport torch.nn as nn\n\nfrom termcolor import cprint\nfrom collections import OrderedDict\n\n__all__ = [\'ResNet\', \'resnet50\', \'resnet101\', \'resnet152\']\n\n\ndef model_hub(arch, pretrained=True, nl_type=None, nl_nums=None,\n              pool_size=7):\n    """"""Model hub.\n    """"""\n    if arch == \'50\':\n        return resnet50(pretrained=pretrained,\n                        nl_type=nl_type,\n                        nl_nums=nl_nums,\n                        pool_size=pool_size)\n    elif arch == \'101\':\n        return resnet101(pretrained=pretrained,\n                         nl_type=nl_type,\n                         nl_nums=nl_nums,\n                         pool_size=pool_size)\n    elif arch == \'152\':\n        return resnet152(pretrained=pretrained,\n                         nl_type=nl_type,\n                         nl_nums=nl_nums,\n                         pool_size=pool_size)\n    else:\n        raise NameError(""The arch \'{}\' is not supported yet in this repo. \\\n                You can add it by yourself."".format(arch))\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding.\n    """"""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SpatialCGNL(nn.Module):\n    """"""Spatial CGNL block with dot production kernel for image classfication.\n    """"""\n    def __init__(self, inplanes, planes, use_scale=False, groups=None):\n        self.use_scale = use_scale\n        self.groups = groups\n\n        super(SpatialCGNL, self).__init__()\n        # conv theta\n        self.t = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        # conv phi\n        self.p = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        # conv g\n        self.g = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        # conv z\n        self.z = nn.Conv2d(planes, inplanes, kernel_size=1, stride=1,\n                                                  groups=self.groups, bias=False)\n        self.gn = nn.GroupNorm(num_groups=self.groups, num_channels=inplanes)\n\n        if self.use_scale:\n            cprint(""=> WARN: SpatialCGNL block uses \'SCALE\'"", \\\n                   \'yellow\')\n        if self.groups:\n            cprint(""=> WARN: SpatialCGNL block uses \'{}\' groups"".format(self.groups), \\\n                   \'yellow\')\n\n    def kernel(self, t, p, g, b, c, h, w):\n        """"""The linear kernel (dot production).\n\n        Args:\n            t: output of conv theata\n            p: output of conv phi\n            g: output of conv g\n            b: batch size\n            c: channels number\n            h: height of featuremaps\n            w: width of featuremaps\n        """"""\n        t = t.view(b, 1, c * h * w)\n        p = p.view(b, 1, c * h * w)\n        g = g.view(b, c * h * w, 1)\n\n        att = torch.bmm(p, g)\n\n        if self.use_scale:\n            att = att.div((c*h*w)**0.5)\n\n        x = torch.bmm(att, t)\n        x = x.view(b, c, h, w)\n\n        return x\n\n    def forward(self, x):\n        residual = x\n\n        t = self.t(x)\n        p = self.p(x)\n        g = self.g(x)\n\n        b, c, h, w = t.size()\n\n        if self.groups and self.groups > 1:\n            _c = int(c / self.groups)\n\n            ts = torch.split(t, split_size_or_sections=_c, dim=1)\n            ps = torch.split(p, split_size_or_sections=_c, dim=1)\n            gs = torch.split(g, split_size_or_sections=_c, dim=1)\n\n            _t_sequences = []\n            for i in range(self.groups):\n                _x = self.kernel(ts[i], ps[i], gs[i],\n                                 b, _c, h, w)\n                _t_sequences.append(_x)\n\n            x = torch.cat(_t_sequences, dim=1)\n        else:\n            x = self.kernel(t, p, g,\n                            b, c, h, w)\n\n        x = self.z(x)\n        x = self.gn(x) + residual\n\n        return x\n\n\nclass SpatialCGNLx(nn.Module):\n    """"""Spatial CGNL block with Gaussian RBF kernel for image classification.\n    """"""\n    def __init__(self, inplanes, planes, use_scale=False, groups=None, order=2):\n        self.use_scale = use_scale\n        self.groups = groups\n        self.order = order\n\n        super(SpatialCGNLx, self).__init__()\n        # conv theta\n        self.t = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        # conv phi\n        self.p = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        # conv g\n        self.g = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        # conv z\n        self.z = nn.Conv2d(planes, inplanes, kernel_size=1, stride=1,\n                                                  groups=self.groups, bias=False)\n        self.gn = nn.GroupNorm(num_groups=self.groups, num_channels=inplanes)\n\n        if self.use_scale:\n            cprint(""=> WARN: SpatialCGNLx block uses \'SCALE\'"", \\\n                   \'yellow\')\n        if self.groups:\n            cprint(""=> WARN: SpatialCGNLx block uses \'{}\' groups"".format(self.groups), \\\n                   \'yellow\')\n\n        cprint(\'=> WARN: The Taylor expansion order in SpatialCGNLx block is {}\'.format(self.order), \\\n               \'yellow\')\n\n    def kernel(self, t, p, g, b, c, h, w):\n        """"""The non-linear kernel (Gaussian RBF).\n\n        Args:\n            t: output of conv theata\n            p: output of conv phi\n            g: output of conv g\n            b: batch size\n            c: channels number\n            h: height of featuremaps\n            w: width of featuremaps\n        """"""\n\n        t = t.view(b, 1, c * h * w)\n        p = p.view(b, 1, c * h * w)\n        g = g.view(b, c * h * w, 1)\n\n        # gamma\n        gamma = torch.Tensor(1).fill_(1e-4)\n\n        # NOTE:\n        # We want to keep the high-order feature spaces in Taylor expansion to \n        # rich the feature representation, so the l2 norm is not used here.\n        # \n        # Under the above precondition, the \xce\xb2 should be calculated \n        # by \xce\xb2 = exp(\xe2\x88\x92\xce\xb3(\xe2\x88\xa5\xce\xb8\xe2\x88\xa5^2 +\xe2\x88\xa5\xcf\x86\xe2\x88\xa5^2)). \n        # But in the experiments, we found training becomes very difficult. \n        # So we simplify the implementation to \n        # ease the gradient computation through calculating the \xce\xb2 = exp(\xe2\x88\x922\xce\xb3).\n\n        # beta\n        beta = torch.exp(-2 * gamma)\n\n        t_taylor = []\n        p_taylor = []\n        for order in range(self.order+1):\n            # alpha\n            alpha = torch.mul(\n                    torch.div(\n                    torch.pow(\n                        (2 * gamma),\n                        order),\n                        math.factorial(order)),\n                        beta)\n\n            alpha = torch.sqrt(\n                        alpha.cuda())\n\n            _t = t.pow(order).mul(alpha)\n            _p = p.pow(order).mul(alpha)\n\n            t_taylor.append(_t)\n            p_taylor.append(_p)\n\n        t_taylor = torch.cat(t_taylor, dim=1)\n        p_taylor = torch.cat(p_taylor, dim=1)\n\n        att = torch.bmm(p_taylor, g)\n\n        if self.use_scale:\n            att = att.div((c*h*w)**0.5)\n\n        att = att.view(b, 1, int(self.order+1))\n        x = torch.bmm(att, t_taylor)\n        x = x.view(b, c, h, w)\n\n        return x\n\n    def forward(self, x):\n        residual = x\n\n        t = self.t(x)\n        p = self.p(x)\n        g = self.g(x)\n\n        b, c, h, w = t.size()\n\n        if self.groups and self.groups > 1:\n            _c = int(c / self.groups)\n\n            ts = torch.split(t, split_size_or_sections=_c, dim=1)\n            ps = torch.split(p, split_size_or_sections=_c, dim=1)\n            gs = torch.split(g, split_size_or_sections=_c, dim=1)\n\n            _t_sequences = []\n            for i in range(self.groups):\n                _x = self.kernel(ts[i], ps[i], gs[i],\n                                 b, _c, h, w)\n                _t_sequences.append(_x)\n\n            x = torch.cat(_t_sequences, dim=1)\n        else:\n            x = self.kernel(t, p, g,\n                            b, c, h, w)\n\n        x = self.z(x)\n        x = self.gn(x) + residual\n\n        return x\n\n\nclass SpatialNL(nn.Module):\n    """"""Spatial NL block for image classification.\n       [https://github.com/facebookresearch/video-nonlocal-net].\n    """"""\n    def __init__(self, inplanes, planes, use_scale=False):\n        self.use_scale = use_scale\n\n        super(SpatialNL, self).__init__()\n        self.t = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        self.p = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        self.g = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        self.softmax = nn.Softmax(dim=2)\n        self.z = nn.Conv2d(planes, inplanes, kernel_size=1, stride=1, bias=False)\n        self.bn = nn.BatchNorm2d(inplanes)\n\n        if self.use_scale:\n            cprint(""=> WARN: SpatialNL block uses \'SCALE\' before softmax"", \'yellow\')\n\n    def forward(self, x):\n        residual = x\n\n        t = self.t(x)\n        p = self.p(x)\n        g = self.g(x)\n\n        b, c, h, w = t.size()\n\n        t = t.view(b, c, -1).permute(0, 2, 1)\n        p = p.view(b, c, -1)\n        g = g.view(b, c, -1).permute(0, 2, 1)\n\n        att = torch.bmm(t, p)\n\n        if self.use_scale:\n            att = att.div(c**0.5)\n\n        att = self.softmax(att)\n        x = torch.bmm(att, g)\n\n        x = x.permute(0, 2, 1)\n        x = x.contiguous()\n        x = x.view(b, c, h, w)\n\n        x = self.z(x)\n        x = self.bn(x) + residual\n\n        return x\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000,\n                 nl_type=None, nl_nums=None, pool_size=7):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n\n        if not nl_nums:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        else:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                           nl_type=nl_type, nl_nums=nl_nums)\n\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(pool_size, stride=1)\n        self.dropout = nn.Dropout(0.5)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for name, m in self.named_modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        if nl_nums == 1:\n            for name, m in self._modules[\'layer3\'][-2].named_modules():\n                if isinstance(m, nn.Conv2d):\n                    nn.init.normal_(m.weight, mean=0, std=0.01)\n                elif isinstance(m, nn.BatchNorm2d):\n                    nn.init.constant_(m.weight, 0)\n                    nn.init.constant_(m.bias, 0)\n                elif isinstance(m, nn.GroupNorm):\n                    nn.init.constant_(m.weight, 0)\n                    nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, nl_type=None, nl_nums=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            if (i == 5 and blocks == 6) or \\\n               (i == 22 and blocks == 23) or \\\n               (i == 35 and blocks == 36):\n                if nl_type == \'nl\':\n                    layers.append(SpatialNL(\n                        self.inplanes,\n                        int(self.inplanes/2),\n                        use_scale=True))\n                elif nl_type == \'cgnl\':\n                    layers.append(SpatialCGNL(\n                        self.inplanes,\n                        int(self.inplanes/2),\n                        use_scale=False,\n                        groups=8))\n                elif nl_type == \'cgnlx\':\n                    layers.append(SpatialCGNLx(\n                        self.inplanes,\n                        int(self.inplanes/2),\n                        use_scale=False,\n                        groups=8,\n                        order=3))\n                else:\n                    pass\n\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(x)\n        x = self.fc(x)\n\n        return x\n\n\ndef load_partial_weight(model, pretrained, nl_nums, nl_layer_id):\n    """"""Loads the partial weights for NL/CGNL network.\n    """"""\n    _pretrained = pretrained\n    _model_dict = model.state_dict()\n    _pretrained_dict = OrderedDict()\n    for k, v in _pretrained.items():\n        ks = k.split(\'.\')\n        layer_name = \'.\'.join(ks[0:2])\n        if nl_nums == 1 and \\\n                layer_name == \'layer3.{}\'.format(nl_layer_id):\n            ks[1] = str(int(ks[1]) + 1)\n            k = \'.\'.join(ks)\n        _pretrained_dict[k] = v\n    _model_dict.update(_pretrained_dict)\n    return _model_dict\n\n\ndef resnet50(pretrained=False, nl_type=None, nl_nums=None, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\n                   nl_type=nl_type, nl_nums=nl_nums, **kwargs)\n    if pretrained:\n        _pretrained = torch.load(\'pretrained/resnet50-19c8e357.pth\')\n        _model_dict = load_partial_weight(model, _pretrained, nl_nums, 5)\n        model.load_state_dict(_model_dict)\n    return model\n\n\ndef resnet101(pretrained=False, nl_type=None, nl_nums=None, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3],\n                   nl_type=nl_type, nl_nums=nl_nums, **kwargs)\n    if pretrained:\n        _pretrained = torch.load(\'pretrained/resnet101-5d3b4d8f.pth\')\n        _model_dict = load_partial_weight(model, _pretrained, nl_nums, 22)\n        model.load_state_dict(_model_dict)\n    return model\n\n\ndef resnet152(pretrained=False, nl_type=None, nl_nums=None, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3],\n                   nl_type=nl_type, nl_nums=nl_nums, **kwargs)\n    if pretrained:\n        _pretrained = torch.load(\'pretrained/resnet152-b121ed2d.pth\')\n        _model_dict = load_partial_weight(model, _pretrained, nl_nums, 35)\n        model.load_state_dict(_model_dict)\n    return model\n'"
maskrcnn_benchmark/utils/c2_model_loading.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport logging\nimport pickle\nfrom collections import OrderedDict\n\nimport torch\n\nfrom maskrcnn_benchmark.utils.model_serialization import load_state_dict\n\n# -----------------------------------------------------------------------------\n# Global cfgs for CGNL blocks\n# -----------------------------------------------------------------------------\nnl_nums = 1\nnl_layer_id = 5 # r-50: 5 | r-101: 22 | r-152: 35\n\n\ndef _rename_basic_resnet_weights(layer_keys):\n    layer_keys = [k.replace(""_"", ""."") for k in layer_keys]\n    layer_keys = [k.replace("".w"", "".weight"") for k in layer_keys]\n    layer_keys = [k.replace("".bn"", ""_bn"") for k in layer_keys]\n    layer_keys = [k.replace("".b"", "".bias"") for k in layer_keys]\n    layer_keys = [k.replace(""_bn.s"", ""_bn.scale"") for k in layer_keys]\n    layer_keys = [k.replace("".biasranch"", "".branch"") for k in layer_keys]\n    layer_keys = [k.replace(""bbox.pred"", ""bbox_pred"") for k in layer_keys]\n    layer_keys = [k.replace(""cls.score"", ""cls_score"") for k in layer_keys]\n    layer_keys = [k.replace(""res.conv1_"", ""conv1_"") for k in layer_keys]\n\n    # RPN / Faster RCNN\n    layer_keys = [k.replace("".biasbox"", "".bbox"") for k in layer_keys]\n    layer_keys = [k.replace(""conv.rpn"", ""rpn.conv"") for k in layer_keys]\n    layer_keys = [k.replace(""rpn.bbox.pred"", ""rpn.bbox_pred"") for k in layer_keys]\n    layer_keys = [k.replace(""rpn.cls.logits"", ""rpn.cls_logits"") for k in layer_keys]\n\n    # Affine-Channel -> BatchNorm enaming\n    layer_keys = [k.replace(""_bn.scale"", ""_bn.weight"") for k in layer_keys]\n\n    # Make torchvision-compatible\n    layer_keys = [k.replace(""conv1_bn."", ""bn1."") for k in layer_keys]\n\n    layer_keys = [k.replace(""res2."", ""layer1."") for k in layer_keys]\n    layer_keys = [k.replace(""res3."", ""layer2."") for k in layer_keys]\n    layer_keys = [k.replace(""res4."", ""layer3."") for k in layer_keys]\n    layer_keys = [k.replace(""res5."", ""layer4."") for k in layer_keys]\n\n    layer_keys = [k.replace("".branch2a."", "".conv1."") for k in layer_keys]\n    layer_keys = [k.replace("".branch2a_bn."", "".bn1."") for k in layer_keys]\n    layer_keys = [k.replace("".branch2b."", "".conv2."") for k in layer_keys]\n    layer_keys = [k.replace("".branch2b_bn."", "".bn2."") for k in layer_keys]\n    layer_keys = [k.replace("".branch2c."", "".conv3."") for k in layer_keys]\n    layer_keys = [k.replace("".branch2c_bn."", "".bn3."") for k in layer_keys]\n\n    layer_keys = [k.replace("".branch1."", "".downsample.0."") for k in layer_keys]\n    layer_keys = [k.replace("".branch1_bn."", "".downsample.1."") for k in layer_keys]\n\n    # Make CGNL networks compatible\n    for idx, k in enumerate(layer_keys):\n        ks = k.split(\'.\')\n        layer_name = \'.\'.join(ks[0:2])\n        if nl_nums == 1 and \\\n                layer_name == \'layer3.{}\'.format(nl_layer_id):\n            ks[1] = str(int(ks[1]) + 1)\n            k = \'.\'.join(ks)\n            # rename the key\n            layer_keys[idx] = k\n\n    return layer_keys\n\ndef _rename_fpn_weights(layer_keys, stage_names):\n    for mapped_idx, stage_name in enumerate(stage_names, 1):\n        suffix = """"\n        if mapped_idx < 4:\n            suffix = "".lateral""\n        layer_keys = [\n            k.replace(""fpn.inner.layer{}.sum{}"".format(stage_name, suffix), ""fpn_inner{}"".format(mapped_idx)) for k in layer_keys\n        ]\n        layer_keys = [k.replace(""fpn.layer{}.sum"".format(stage_name), ""fpn_layer{}"".format(mapped_idx)) for k in layer_keys]\n\n\n    layer_keys = [k.replace(""rpn.conv.fpn2"", ""rpn.conv"") for k in layer_keys]\n    layer_keys = [k.replace(""rpn.bbox_pred.fpn2"", ""rpn.bbox_pred"") for k in layer_keys]\n    layer_keys = [\n        k.replace(""rpn.cls_logits.fpn2"", ""rpn.cls_logits"") for k in layer_keys\n    ]\n\n    return layer_keys\n\n\ndef _rename_weights_for_resnet(weights, stage_names):\n    original_keys = sorted(weights.keys())\n    layer_keys = sorted(weights.keys())\n\n    # for X-101, rename output to fc1000 to avoid conflicts afterwards\n    layer_keys = [k if k != ""pred_b"" else ""fc1000_b"" for k in layer_keys]\n    layer_keys = [k if k != ""pred_w"" else ""fc1000_w"" for k in layer_keys]\n\n    # performs basic renaming: _ -> . , etc\n    layer_keys = _rename_basic_resnet_weights(layer_keys)\n\n    # FPN\n    layer_keys = _rename_fpn_weights(layer_keys, stage_names)\n\n    # Mask R-CNN\n    layer_keys = [k.replace(""mask.fcn.logits"", ""mask_fcn_logits"") for k in layer_keys]\n    layer_keys = [k.replace("".[mask].fcn"", ""mask_fcn"") for k in layer_keys]\n    layer_keys = [k.replace(""conv5.mask"", ""conv5_mask"") for k in layer_keys]\n\n    # Keypoint R-CNN\n    layer_keys = [k.replace(""kps.score.lowres"", ""kps_score_lowres"") for k in layer_keys]\n    layer_keys = [k.replace(""kps.score"", ""kps_score"") for k in layer_keys]\n    layer_keys = [k.replace(""conv.fcn"", ""conv_fcn"") for k in layer_keys]\n\n    # Rename for our RPN structure\n    layer_keys = [k.replace(""rpn."", ""rpn.head."") for k in layer_keys]\n\n    key_map = {k: v for k, v in zip(original_keys, layer_keys)}\n\n    logger = logging.getLogger(__name__)\n    logger.info(""Remapping C2 weights"")\n    max_c2_key_size = max([len(k) for k in original_keys if ""_momentum"" not in k])\n\n    new_weights = OrderedDict()\n    for k in original_keys:\n        v = weights[k]\n        if ""_momentum"" in k:\n            continue\n        # if \'fc1000\' in k:\n        #     continue\n        w = torch.from_numpy(v)\n        # if ""bn"" in k:\n        #     w = w.view(1, -1, 1, 1)\n        logger.info(""C2 name: {: <{}} mapped name: {}"".format(k, max_c2_key_size, key_map[k]))\n        new_weights[key_map[k]] = w\n\n    return new_weights\n\n\ndef _load_c2_pickled_weights(file_path):\n    with open(file_path, ""rb"") as f:\n        if torch._six.PY3:\n            data = pickle.load(f, encoding=""latin1"")\n        else:\n            data = pickle.load(f)\n    if ""blobs"" in data:\n        weights = data[""blobs""]\n    else:\n        weights = data\n    return weights\n\n\n_C2_STAGE_NAMES = {\n    ""R-50"": [""1.2"", ""2.3"", ""3.5"", ""4.2""],\n    ""R-101"": [""1.2"", ""2.3"", ""3.22"", ""4.2""],\n}\n\ndef load_c2_format(cfg, f):\n    # TODO make it support other architectures\n    state_dict = _load_c2_pickled_weights(f)\n    conv_body = cfg.MODEL.BACKBONE.CONV_BODY\n    arch = conv_body.replace(""-C4"", """").replace(""-FPN"", """")\n    stages = _C2_STAGE_NAMES[arch]\n    state_dict = _rename_weights_for_resnet(state_dict, stages)\n    return dict(model=state_dict)\n'"
maskrcnn_benchmark/modeling/backbone/resnet.py,24,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n""""""\nVariant of the resnet module that takes cfg as an argument.\nExample usage. Strings may be specified in the config file.\n    model = ResNet(\n        ""StemWithFixedBatchNorm"",\n        ""BottleneckWithFixedBatchNorm"",\n        ""ResNet50StagesTo4"",\n    )\nCustom implementations may be written in user code and hooked in via the\n`register_*` functions.\n""""""\nfrom collections import namedtuple\n\nimport math\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom termcolor import cprint\n\nfrom maskrcnn_benchmark.layers import FrozenBatchNorm2d\nfrom maskrcnn_benchmark.layers import Conv2d\n\n# ResNet stage specification\nStageSpec = namedtuple(\n    ""StageSpec"",\n    [\n        ""index"",  # Index of the stage, eg 1, 2, ..,. 5\n        ""block_count"",  # Numer of residual blocks in the stage\n        ""return_features"",  # True => return the last feature map from this stage\n    ],\n)\n\n# -----------------------------------------------------------------------------\n# Global cfgs for CGNL blocks\n# -----------------------------------------------------------------------------\nnl_nums = 1\nnl_type = \'cgnlx\' # cgnl | cgnlx | nl\n\n# -----------------------------------------------------------------------------\n# Standard ResNet models\n# -----------------------------------------------------------------------------\n# ResNet-50 (including all stages)\nResNet50StagesTo5 = (\n    StageSpec(index=i, block_count=c, return_features=r)\n    for (i, c, r) in ((1, 3, False), (2, 4, False), (3, 6, False), (4, 3, True))\n)\n# ResNet-50 up to stage 4 (excludes stage 5)\nResNet50StagesTo4 = (\n    StageSpec(index=i, block_count=c, return_features=r)\n    for (i, c, r) in ((1, 3, False), (2, 4, False), (3, 6, True))\n)\n# ResNet-50-FPN (including all stages)\nResNet50FPNStagesTo5 = (\n    StageSpec(index=i, block_count=c, return_features=r)\n    for (i, c, r) in ((1, 3, True), (2, 4, True), (3, 6, True), (4, 3, True))\n)\n# ResNet-101-FPN (including all stages)\nResNet101FPNStagesTo5 = (\n    StageSpec(index=i, block_count=c, return_features=r)\n    for (i, c, r) in ((1, 3, True), (2, 4, True), (3, 23, True), (4, 3, True))\n)\n\nclass SpatialCGNL(nn.Module):\n    """"""Spatial CGNL block with dot production kernel for image classfication.\n    """"""\n    def __init__(self, inplanes, planes, use_scale=False, groups=None):\n        self.use_scale = use_scale\n        self.groups = groups\n\n        super(SpatialCGNL, self).__init__()\n        # conv theta\n        self.t = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        # conv phi\n        self.p = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        # conv g\n        self.g = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        # conv z\n        self.z = nn.Conv2d(planes, inplanes, kernel_size=1, stride=1,\n                                                  groups=self.groups, bias=False)\n        self.gn = nn.GroupNorm(num_groups=self.groups, num_channels=inplanes)\n\n        if self.use_scale:\n            cprint(""=> WARN: SpatialCGNL block uses \'SCALE\'"", \\\n                   \'yellow\')\n        if self.groups:\n            cprint(""=> WARN: SpatialCGNL block uses \'{}\' groups"".format(self.groups), \\\n                   \'yellow\')\n\n    def kernel(self, t, p, g, b, c, h, w):\n        """"""The linear kernel (dot production).\n        Args:\n            t: output of conv theata\n            p: output of conv phi\n            g: output of conv g\n            b: batch size\n            c: channels number\n            h: height of featuremaps\n            w: width of featuremaps\n        """"""\n        t = t.view(b, 1, c * h * w)\n        p = p.view(b, 1, c * h * w)\n        g = g.view(b, c * h * w, 1)\n\n        att = torch.bmm(p, g)\n\n        if self.use_scale:\n            att = att.vid((c*h*w)**0.5)\n\n        x = torch.bmm(att, t)\n        x = x.view(b, c, h, w)\n\n        return x\n\n    def forward(self, x):\n        residual = x\n\n        t = self.t(x)\n        p = self.p(x)\n        g = self.g(x)\n\n        b, c, h, w = t.size()\n\n        if self.groups and self.groups > 1:\n            _c = int(c / self.groups)\n\n            ts = torch.split(t, split_size_or_sections=_c, dim=1)\n            ps = torch.split(p, split_size_or_sections=_c, dim=1)\n            gs = torch.split(g, split_size_or_sections=_c, dim=1)\n\n            _t_sequences = []\n            for i in range(self.groups):\n                _x = self.kernel(ts[i], ps[i], gs[i],\n                                 b, _c, h, w)\n                _t_sequences.append(_x)\n\n            x = torch.cat(_t_sequences, dim=1)\n        else:\n            x = self.kernel(t, p, g,\n                            b, c, h, w)\n\n        x = self.z(x)\n        x = self.gn(x) + residual\n\n        return x\n\nclass SpatialCGNLx(nn.Module):\n    """"""Spatial CGNL block with Gaussian RBF kernel for image classification.\n    """"""\n    def __init__(self, inplanes, planes, use_scale=False, groups=None, order=2):\n        self.use_scale = use_scale\n        self.groups = groups\n        self.order = order\n\n        super(SpatialCGNLx, self).__init__()\n        # conv theta\n        self.t = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        # conv phi\n        self.p = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        # conv g\n        self.g = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        # conv z\n        self.z = nn.Conv2d(planes, inplanes, kernel_size=1, stride=1,\n                                                  groups=self.groups, bias=False)\n        self.gn = nn.GroupNorm(num_groups=self.groups, num_channels=inplanes)\n\n        if self.use_scale:\n            cprint(""=> WARN: SpatialCGNLx block uses \'SCALE\'"", \\\n                   \'yellow\')\n        if self.groups:\n            cprint(""=> WARN: SpatialCGNLx block uses \'{}\' groups"".format(self.groups), \\\n                   \'yellow\')\n\n        cprint(\'=> WARN: The Taylor expansion order in SpatialCGNLx block is {}\'.format(self.order), \\\n               \'yellow\')\n\n    def kernel(self, t, p, g, b, c, h, w):\n        """"""The non-linear kernel (Gaussian RBF).\n        Args:\n            t: output of conv theata\n            p: output of conv phi\n            g: output of conv g\n            b: batch size\n            c: channels number\n            h: height of featuremaps\n            w: width of featuremaps\n        """"""\n\n        t = t.view(b, 1, c * h * w)\n        p = p.view(b, 1, c * h * w)\n        g = g.view(b, c * h * w, 1)\n\n        # gamma\n        gamma = torch.Tensor(1).fill_(1e-4)\n\n        # beta\n        beta = torch.exp(-2 * gamma)\n\n        t_taylor = []\n        p_taylor = []\n        for order in range(self.order+1):\n            # alpha\n            alpha = torch.mul(\n                    torch.div(\n                    torch.pow(\n                        (2 * gamma),\n                        order),\n                        math.factorial(order)),\n                        beta)\n\n            alpha = torch.sqrt(\n                        alpha.cuda())\n\n            _t = t.pow(order).mul(alpha)\n            _p = p.pow(order).mul(alpha)\n\n            t_taylor.append(_t)\n            p_taylor.append(_p)\n\n        t_taylor = torch.cat(t_taylor, dim=1)\n        p_taylor = torch.cat(p_taylor, dim=1)\n\n        att = torch.bmm(p_taylor, g)\n\n        if self.use_scale:\n            att = att.div((c*h*w)**0.5)\n\n        att = att.view(b, 1, int(self.order+1))\n        x = torch.bmm(att, t_taylor)\n        x = x.view(b, c, h, w)\n\n        return x\n\n    def forward(self, x):\n        residual = x\n\n        t = self.t(x)\n        p = self.p(x)\n        g = self.g(x)\n\n        b, c, h, w = t.size()\n\n        if self.groups and self.groups > 1:\n            _c = int(c / self.groups)\n\n            ts = torch.split(t, split_size_or_sections=_c, dim=1)\n            ps = torch.split(p, split_size_or_sections=_c, dim=1)\n            gs = torch.split(g, split_size_or_sections=_c, dim=1)\n\n            _t_sequences = []\n            for i in range(self.groups):\n                _x = self.kernel(ts[i], ps[i], gs[i],\n                                 b, _c, h, w)\n                _t_sequences.append(_x)\n\n            x = torch.cat(_t_sequences, dim=1)\n        else:\n            x = self.kernel(t, p, g,\n                            b, c, h, w)\n\n        x = self.z(x)\n        x = self.gn(x) + residual\n\n        return x\n\n\nclass SpatialNL(nn.Module):\n    """"""Spatial NL block for image classification.\n       [https://github.com/facebookresearch/video-nonlocal-net].\n    """"""\n    def __init__(self, inplanes, planes, use_scale=False):\n        self.use_scale = use_scale\n\n        super(SpatialNL, self).__init__()\n        self.t = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        self.p = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        self.g = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        self.softmax = nn.Softmax(dim=2)\n        self.z = nn.Conv2d(planes, inplanes, kernel_size=1, stride=1, bias=False)\n        self.bn = nn.BatchNorm2d(inplanes)\n\n        if self.use_scale:\n            cprint(""=> WARN: SpatialNL block uses \'SCALE\' before softmax"", \'yellow\')\n\n    def forward(self, x):\n        residual = x\n\n        t = self.t(x)\n        p = self.p(x)\n        g = self.g(x)\n\n        b, c, h, w = t.size()\n\n        t = t.view(b, c, -1).permute(0, 2, 1)\n        p = p.view(b, c, -1)\n        g = g.view(b, c, -1).permute(0, 2, 1)\n\n        att = torch.bmm(t, p)\n\n        if self.use_scale:\n            att = att.div(c**0.5)\n\n        att = self.softmax(att)\n        x = torch.bmm(att, g)\n\n        x = x.permute(0, 2, 1)\n        x = x.contiguous()\n        x = x.view(b, c, h, w)\n\n        x = self.z(x)\n        x = self.bn(x) + residual\n\n        return x\n\n\nclass ResNet(nn.Module):\n    def __init__(self, cfg):\n        super(ResNet, self).__init__()\n\n        # If we want to use the cfg in forward(), then we should make a copy\n        # of it and store it for later use:\n        # self.cfg = cfg.clone()\n\n        # Translate string names to implementations\n        stem_module = _STEM_MODULES[cfg.MODEL.RESNETS.STEM_FUNC]\n        stage_specs = _STAGE_SPECS[cfg.MODEL.BACKBONE.CONV_BODY]\n        transformation_module = _TRANSFORMATION_MODULES[cfg.MODEL.RESNETS.TRANS_FUNC]\n\n        # Construct the stem module\n        self.stem = stem_module(cfg)\n\n        # Constuct the specified ResNet stages\n        num_groups = cfg.MODEL.RESNETS.NUM_GROUPS\n        width_per_group = cfg.MODEL.RESNETS.WIDTH_PER_GROUP\n        in_channels = cfg.MODEL.RESNETS.STEM_OUT_CHANNELS\n        stage2_bottleneck_channels = num_groups * width_per_group\n        stage2_out_channels = cfg.MODEL.RESNETS.RES2_OUT_CHANNELS\n        self.stages = []\n        self.return_features = {}\n        for stage_spec in stage_specs:\n            name = ""layer"" + str(stage_spec.index)\n            stage2_relative_factor = 2 ** (stage_spec.index - 1)\n            bottleneck_channels = stage2_bottleneck_channels * stage2_relative_factor\n            out_channels = stage2_out_channels * stage2_relative_factor\n            module = _make_stage(\n                transformation_module,\n                in_channels,\n                bottleneck_channels,\n                out_channels,\n                stage_spec.block_count,\n                num_groups,\n                cfg.MODEL.RESNETS.STRIDE_IN_1X1,\n                first_stride=int(stage_spec.index > 1) + 1,\n            )\n            in_channels = out_channels\n            self.add_module(name, module)\n            self.stages.append(name)\n            self.return_features[name] = stage_spec.return_features\n\n        # Optionally freeze (requires_grad=False) parts of the backbone\n        self._freeze_backbone(cfg.MODEL.BACKBONE.FREEZE_CONV_BODY_AT)\n\n        if nl_nums == 1:\n            for name, m in self._modules[\'layer3\'][-2].named_modules():\n                if isinstance(m, nn.Conv2d):\n                    nn.init.normal_(m.weight, mean=0, std=0.01)\n                elif isinstance(m, nn.BatchNorm2d):\n                    nn.init.constant_(m.weight, 0)\n                    nn.init.constant_(m.bias, 0)\n                elif isinstance(m, nn.GroupNorm):\n                    nn.init.constant_(m.weight, 0)\n                    nn.init.constant_(m.bias, 0)\n\n\n    def _freeze_backbone(self, freeze_at):\n        for stage_index in range(freeze_at):\n            if stage_index == 0:\n                m = self.stem  # stage 0 is the stem\n            else:\n                m = getattr(self, ""layer"" + str(stage_index))\n            for p in m.parameters():\n                p.requires_grad = False\n\n    def forward(self, x):\n        outputs = []\n        x = self.stem(x)\n        for stage_name in self.stages:\n            x = getattr(self, stage_name)(x)\n            if self.return_features[stage_name]:\n                outputs.append(x)\n        return outputs\n\n\nclass ResNetHead(nn.Module):\n    def __init__(\n        self,\n        block_module,\n        stages,\n        num_groups=1,\n        width_per_group=64,\n        stride_in_1x1=True,\n        stride_init=None,\n        res2_out_channels=256,\n    ):\n        super(ResNetHead, self).__init__()\n\n        stage2_relative_factor = 2 ** (stages[0].index - 1)\n        stage2_bottleneck_channels = num_groups * width_per_group\n        out_channels = res2_out_channels * stage2_relative_factor\n        in_channels = out_channels // 2\n        bottleneck_channels = stage2_bottleneck_channels * stage2_relative_factor\n\n        block_module = _TRANSFORMATION_MODULES[block_module]\n\n        self.stages = []\n        stride = stride_init\n        for stage in stages:\n            name = ""layer"" + str(stage.index)\n            if not stride:\n                stride = int(stage.index > 1) + 1\n            module = _make_stage(\n                block_module,\n                in_channels,\n                bottleneck_channels,\n                out_channels,\n                stage.block_count,\n                num_groups,\n                stride_in_1x1,\n                first_stride=stride,\n            )\n            stride = None\n            self.add_module(name, module)\n            self.stages.append(name)\n\n    def forward(self, x):\n        for stage in self.stages:\n            x = getattr(self, stage)(x)\n        return x\n\n\ndef _make_stage(\n    transformation_module,\n    in_channels,\n    bottleneck_channels,\n    out_channels,\n    block_count,\n    num_groups,\n    stride_in_1x1,\n    first_stride,\n):\n    blocks = []\n    stride = first_stride\n\n    for idx in range(block_count):\n        if idx == 5 and block_count == 6:\n            # print(in_channels, bottleneck_channels, out_channels, num_groups)\n            # 1024 256 1024 1\n            if nl_type == \'nl\':\n                blocks.append(SpatialNL(\n                    in_channels,\n                    int(in_channels/2),\n                    use_scale=True))\n            elif nl_type == \'cgnl\':\n                blocks.append(SpatialCGNL(\n                    in_channels,\n                    int(in_channels/2),\n                    use_scale=False,\n                    groups=8))\n            elif nl_type == \'cgnlx\':\n                blocks.append(SpatialCGNLx(\n                    in_channels,\n                    int(in_channels/2),\n                    use_scale=False,\n                    groups=8,\n                    order=3))\n            else:\n                pass\n\n        blocks.append(\n            transformation_module(\n                in_channels,\n                bottleneck_channels,\n                out_channels,\n                num_groups,\n                stride_in_1x1,\n                stride,\n            )\n        )\n        stride = 1\n        in_channels = out_channels\n    return nn.Sequential(*blocks)\n\n\nclass BottleneckWithFixedBatchNorm(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        bottleneck_channels,\n        out_channels,\n        num_groups=1,\n        stride_in_1x1=True,\n        stride=1,\n    ):\n        super(BottleneckWithFixedBatchNorm, self).__init__()\n\n        self.downsample = None\n        if in_channels != out_channels:\n            self.downsample = nn.Sequential(\n                Conv2d(\n                    in_channels, out_channels, kernel_size=1, stride=stride, bias=False\n                ),\n                FrozenBatchNorm2d(out_channels),\n            )\n\n        # The original MSRA ResNet models have stride in the first 1x1 conv\n        # The subsequent fb.torch.resnet and Caffe2 ResNe[X]t implementations have\n        # stride in the 3x3 conv\n        stride_1x1, stride_3x3 = (stride, 1) if stride_in_1x1 else (1, stride)\n\n        self.conv1 = Conv2d(\n            in_channels,\n            bottleneck_channels,\n            kernel_size=1,\n            stride=stride_1x1,\n            bias=False,\n        )\n        self.bn1 = FrozenBatchNorm2d(bottleneck_channels)\n        # TODO: specify init for the above\n\n        self.conv2 = Conv2d(\n            bottleneck_channels,\n            bottleneck_channels,\n            kernel_size=3,\n            stride=stride_3x3,\n            padding=1,\n            bias=False,\n            groups=num_groups,\n        )\n        self.bn2 = FrozenBatchNorm2d(bottleneck_channels)\n\n        self.conv3 = Conv2d(\n            bottleneck_channels, out_channels, kernel_size=1, bias=False\n        )\n        self.bn3 = FrozenBatchNorm2d(out_channels)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = F.relu_(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = F.relu_(out)\n\n        out0 = self.conv3(out)\n        out = self.bn3(out0)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = F.relu_(out)\n\n        return out\n\n\nclass StemWithFixedBatchNorm(nn.Module):\n    def __init__(self, cfg):\n        super(StemWithFixedBatchNorm, self).__init__()\n\n        out_channels = cfg.MODEL.RESNETS.STEM_OUT_CHANNELS\n\n        self.conv1 = Conv2d(\n            3, out_channels, kernel_size=7, stride=2, padding=3, bias=False\n        )\n        self.bn1 = FrozenBatchNorm2d(out_channels)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu_(x)\n        x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n        return x\n\n\n_TRANSFORMATION_MODULES = {""BottleneckWithFixedBatchNorm"": BottleneckWithFixedBatchNorm}\n\n_STEM_MODULES = {""StemWithFixedBatchNorm"": StemWithFixedBatchNorm}\n\n_STAGE_SPECS = {\n    ""R-50-C4"": ResNet50StagesTo4,\n    ""R-50-C5"": ResNet50StagesTo5,\n    ""R-50-FPN"": ResNet50FPNStagesTo5,\n    ""R-101-FPN"": ResNet101FPNStagesTo5,\n}\n\n\ndef register_transformation_module(module_name, module):\n    _register_generic(_TRANSFORMATION_MODULES, module_name, module)\n\n\ndef register_stem_module(module_name, module):\n    _register_generic(_STEM_MODULES, module_name, module)\n\n\ndef register_stage_spec(stage_spec_name, stage_spec):\n    _register_generic(_STAGE_SPECS, stage_spec_name, stage_spec)\n\n\ndef _register_generic(module_dict, module_name, module):\n    assert module_name not in module_dict\n    module_dict[module_name] = module\n'"
