file_path,api_count,code
core/a2c.py,1,"b'import torch\n\n\ndef a2c_step(policy_net, value_net, optimizer_policy, optimizer_value, states, actions, returns, advantages, l2_reg):\n\n    """"""update critic""""""\n    values_pred = value_net(states)\n    value_loss = (values_pred - returns).pow(2).mean()\n    # weight decay\n    for param in value_net.parameters():\n        value_loss += param.pow(2).sum() * l2_reg\n    optimizer_value.zero_grad()\n    value_loss.backward()\n    optimizer_value.step()\n\n    """"""update policy""""""\n    log_probs = policy_net.get_log_prob(states, actions)\n    policy_loss = -(log_probs * advantages).mean()\n    optimizer_policy.zero_grad()\n    policy_loss.backward()\n    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 40)\n    optimizer_policy.step()\n'"
core/agent.py,3,"b""import multiprocessing\nfrom utils.replay_memory import Memory\nfrom utils.torch import *\nimport math\nimport time\n\n\ndef collect_samples(pid, queue, env, policy, custom_reward,\n                    mean_action, render, running_state, min_batch_size):\n    torch.randn(pid)\n    log = dict()\n    memory = Memory()\n    num_steps = 0\n    total_reward = 0\n    min_reward = 1e6\n    max_reward = -1e6\n    total_c_reward = 0\n    min_c_reward = 1e6\n    max_c_reward = -1e6\n    num_episodes = 0\n\n    while num_steps < min_batch_size:\n        state = env.reset()\n        if running_state is not None:\n            state = running_state(state)\n        reward_episode = 0\n\n        for t in range(10000):\n            state_var = tensor(state).unsqueeze(0)\n            with torch.no_grad():\n                if mean_action:\n                    action = policy(state_var)[0][0].numpy()\n                else:\n                    action = policy.select_action(state_var)[0].numpy()\n            action = int(action) if policy.is_disc_action else action.astype(np.float64)\n            next_state, reward, done, _ = env.step(action)\n            reward_episode += reward\n            if running_state is not None:\n                next_state = running_state(next_state)\n\n            if custom_reward is not None:\n                reward = custom_reward(state, action)\n                total_c_reward += reward\n                min_c_reward = min(min_c_reward, reward)\n                max_c_reward = max(max_c_reward, reward)\n\n            mask = 0 if done else 1\n\n            memory.push(state, action, mask, next_state, reward)\n\n            if render:\n                env.render()\n            if done:\n                break\n\n            state = next_state\n\n        # log stats\n        num_steps += (t + 1)\n        num_episodes += 1\n        total_reward += reward_episode\n        min_reward = min(min_reward, reward_episode)\n        max_reward = max(max_reward, reward_episode)\n\n    log['num_steps'] = num_steps\n    log['num_episodes'] = num_episodes\n    log['total_reward'] = total_reward\n    log['avg_reward'] = total_reward / num_episodes\n    log['max_reward'] = max_reward\n    log['min_reward'] = min_reward\n    if custom_reward is not None:\n        log['total_c_reward'] = total_c_reward\n        log['avg_c_reward'] = total_c_reward / num_steps\n        log['max_c_reward'] = max_c_reward\n        log['min_c_reward'] = min_c_reward\n\n    if queue is not None:\n        queue.put([pid, memory, log])\n    else:\n        return memory, log\n\n\ndef merge_log(log_list):\n    log = dict()\n    log['total_reward'] = sum([x['total_reward'] for x in log_list])\n    log['num_episodes'] = sum([x['num_episodes'] for x in log_list])\n    log['num_steps'] = sum([x['num_steps'] for x in log_list])\n    log['avg_reward'] = log['total_reward'] / log['num_episodes']\n    log['max_reward'] = max([x['max_reward'] for x in log_list])\n    log['min_reward'] = min([x['min_reward'] for x in log_list])\n    if 'total_c_reward' in log_list[0]:\n        log['total_c_reward'] = sum([x['total_c_reward'] for x in log_list])\n        log['avg_c_reward'] = log['total_c_reward'] / log['num_steps']\n        log['max_c_reward'] = max([x['max_c_reward'] for x in log_list])\n        log['min_c_reward'] = min([x['min_c_reward'] for x in log_list])\n\n    return log\n\n\nclass Agent:\n\n    def __init__(self, env, policy, device, custom_reward=None,\n                 mean_action=False, render=False, running_state=None, num_threads=1):\n        self.env = env\n        self.policy = policy\n        self.device = device\n        self.custom_reward = custom_reward\n        self.mean_action = mean_action\n        self.running_state = running_state\n        self.render = render\n        self.num_threads = num_threads\n\n    def collect_samples(self, min_batch_size):\n        t_start = time.time()\n        to_device(torch.device('cpu'), self.policy)\n        thread_batch_size = int(math.floor(min_batch_size / self.num_threads))\n        queue = multiprocessing.Queue()\n        workers = []\n\n        for i in range(self.num_threads-1):\n            worker_args = (i+1, queue, self.env, self.policy, self.custom_reward, self.mean_action,\n                           False, self.running_state, thread_batch_size)\n            workers.append(multiprocessing.Process(target=collect_samples, args=worker_args))\n        for worker in workers:\n            worker.start()\n\n        memory, log = collect_samples(0, None, self.env, self.policy, self.custom_reward, self.mean_action,\n                                      self.render, self.running_state, thread_batch_size)\n\n        worker_logs = [None] * len(workers)\n        worker_memories = [None] * len(workers)\n        for _ in workers:\n            pid, worker_memory, worker_log = queue.get()\n            worker_memories[pid - 1] = worker_memory\n            worker_logs[pid - 1] = worker_log\n        for worker_memory in worker_memories:\n            memory.append(worker_memory)\n        batch = memory.sample()\n        if self.num_threads > 1:\n            log_list = [log] + worker_logs\n            log = merge_log(log_list)\n        to_device(self.device, self.policy)\n        t_end = time.time()\n        log['sample_time'] = t_end - t_start\n        log['action_mean'] = np.mean(np.vstack(batch.action), axis=0)\n        log['action_min'] = np.min(np.vstack(batch.action), axis=0)\n        log['action_max'] = np.max(np.vstack(batch.action), axis=0)\n        return batch, log\n"""
core/common.py,1,"b""import torch\nfrom utils import to_device\n\n\ndef estimate_advantages(rewards, masks, values, gamma, tau, device):\n    rewards, masks, values = to_device(torch.device('cpu'), rewards, masks, values)\n    tensor_type = type(rewards)\n    deltas = tensor_type(rewards.size(0), 1)\n    advantages = tensor_type(rewards.size(0), 1)\n\n    prev_value = 0\n    prev_advantage = 0\n    for i in reversed(range(rewards.size(0))):\n        deltas[i] = rewards[i] + gamma * prev_value * masks[i] - values[i]\n        advantages[i] = deltas[i] + gamma * tau * prev_advantage * masks[i]\n\n        prev_value = values[i, 0]\n        prev_advantage = advantages[i, 0]\n\n    returns = values + advantages\n    advantages = (advantages - advantages.mean()) / advantages.std()\n\n    advantages, returns = to_device(device, advantages, returns)\n    return advantages, returns\n"""
core/ppo.py,4,"b'import torch\n\n\ndef ppo_step(policy_net, value_net, optimizer_policy, optimizer_value, optim_value_iternum, states, actions,\n             returns, advantages, fixed_log_probs, clip_epsilon, l2_reg):\n\n    """"""update critic""""""\n    for _ in range(optim_value_iternum):\n        values_pred = value_net(states)\n        value_loss = (values_pred - returns).pow(2).mean()\n        # weight decay\n        for param in value_net.parameters():\n            value_loss += param.pow(2).sum() * l2_reg\n        optimizer_value.zero_grad()\n        value_loss.backward()\n        optimizer_value.step()\n\n    """"""update policy""""""\n    log_probs = policy_net.get_log_prob(states, actions)\n    ratio = torch.exp(log_probs - fixed_log_probs)\n    surr1 = ratio * advantages\n    surr2 = torch.clamp(ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * advantages\n    policy_surr = -torch.min(surr1, surr2).mean()\n    optimizer_policy.zero_grad()\n    policy_surr.backward()\n    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 40)\n    optimizer_policy.step()\n'"
core/trpo.py,13,"b'import numpy as np\nimport scipy.optimize\nfrom utils import *\n\n\ndef conjugate_gradients(Avp_f, b, nsteps, rdotr_tol=1e-10):\n    x = zeros(b.size(), device=b.device)\n    r = b.clone()\n    p = b.clone()\n    rdotr = torch.dot(r, r)\n    for i in range(nsteps):\n        Avp = Avp_f(p)\n        alpha = rdotr / torch.dot(p, Avp)\n        x += alpha * p\n        r -= alpha * Avp\n        new_rdotr = torch.dot(r, r)\n        betta = new_rdotr / rdotr\n        p = r + betta * p\n        rdotr = new_rdotr\n        if rdotr < rdotr_tol:\n            break\n    return x\n\n\ndef line_search(model, f, x, fullstep, expected_improve_full, max_backtracks=10, accept_ratio=0.1):\n    fval = f(True).item()\n\n    for stepfrac in [.5**x for x in range(max_backtracks)]:\n        x_new = x + stepfrac * fullstep\n        set_flat_params_to(model, x_new)\n        fval_new = f(True).item()\n        actual_improve = fval - fval_new\n        expected_improve = expected_improve_full * stepfrac\n        ratio = actual_improve / expected_improve\n\n        if ratio > accept_ratio:\n            return True, x_new\n    return False, x\n\n\ndef trpo_step(policy_net, value_net, states, actions, returns, advantages, max_kl, damping, l2_reg, use_fim=True):\n\n    """"""update critic""""""\n\n    def get_value_loss(flat_params):\n        set_flat_params_to(value_net, tensor(flat_params))\n        for param in value_net.parameters():\n            if param.grad is not None:\n                param.grad.data.fill_(0)\n        values_pred = value_net(states)\n        value_loss = (values_pred - returns).pow(2).mean()\n\n        # weight decay\n        for param in value_net.parameters():\n            value_loss += param.pow(2).sum() * l2_reg\n        value_loss.backward()\n        return value_loss.item(), get_flat_grad_from(value_net.parameters()).cpu().numpy()\n\n    flat_params, _, opt_info = scipy.optimize.fmin_l_bfgs_b(get_value_loss,\n                                                            get_flat_params_from(value_net).detach().cpu().numpy(),\n                                                            maxiter=25)\n    set_flat_params_to(value_net, tensor(flat_params))\n\n    """"""update policy""""""\n    with torch.no_grad():\n        fixed_log_probs = policy_net.get_log_prob(states, actions)\n    """"""define the loss function for TRPO""""""\n    def get_loss(volatile=False):\n        with torch.set_grad_enabled(not volatile):\n            log_probs = policy_net.get_log_prob(states, actions)\n            action_loss = -advantages * torch.exp(log_probs - fixed_log_probs)\n            return action_loss.mean()\n\n    """"""use fisher information matrix for Hessian*vector""""""\n    def Fvp_fim(v):\n        M, mu, info = policy_net.get_fim(states)\n        mu = mu.view(-1)\n        filter_input_ids = set() if policy_net.is_disc_action else set([info[\'std_id\']])\n\n        t = ones(mu.size(), requires_grad=True, device=mu.device)\n        mu_t = (mu * t).sum()\n        Jt = compute_flat_grad(mu_t, policy_net.parameters(), filter_input_ids=filter_input_ids, create_graph=True)\n        Jtv = (Jt * v).sum()\n        Jv = torch.autograd.grad(Jtv, t)[0]\n        MJv = M * Jv.detach()\n        mu_MJv = (MJv * mu).sum()\n        JTMJv = compute_flat_grad(mu_MJv, policy_net.parameters(), filter_input_ids=filter_input_ids).detach()\n        JTMJv /= states.shape[0]\n        if not policy_net.is_disc_action:\n            std_index = info[\'std_index\']\n            JTMJv[std_index: std_index + M.shape[0]] += 2 * v[std_index: std_index + M.shape[0]]\n        return JTMJv + v * damping\n\n    """"""directly compute Hessian*vector from KL""""""\n    def Fvp_direct(v):\n        kl = policy_net.get_kl(states)\n        kl = kl.mean()\n\n        grads = torch.autograd.grad(kl, policy_net.parameters(), create_graph=True)\n        flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n\n        kl_v = (flat_grad_kl * v).sum()\n        grads = torch.autograd.grad(kl_v, policy_net.parameters())\n        flat_grad_grad_kl = torch.cat([grad.contiguous().view(-1) for grad in grads]).detach()\n\n        return flat_grad_grad_kl + v * damping\n\n    Fvp = Fvp_fim if use_fim else Fvp_direct\n\n    loss = get_loss()\n    grads = torch.autograd.grad(loss, policy_net.parameters())\n    loss_grad = torch.cat([grad.view(-1) for grad in grads]).detach()\n    stepdir = conjugate_gradients(Fvp, -loss_grad, 10)\n\n    shs = 0.5 * (stepdir.dot(Fvp(stepdir)))\n    lm = math.sqrt(max_kl / shs)\n    fullstep = stepdir * lm\n    expected_improve = -loss_grad.dot(fullstep)\n\n    prev_params = get_flat_params_from(policy_net)\n    success, new_params = line_search(policy_net, get_loss, prev_params, fullstep, expected_improve)\n    set_flat_params_to(policy_net, new_params)\n\n    return success\n'"
examples/a2c_gym.py,15,"b'import argparse\nimport gym\nimport os\nimport sys\nimport pickle\nimport time\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \'..\')))\n\nfrom utils import *\nfrom models.mlp_policy import Policy\nfrom models.mlp_critic import Value\nfrom models.mlp_policy_disc import DiscretePolicy\nfrom core.a2c import a2c_step\nfrom core.common import estimate_advantages\nfrom core.agent import Agent\n\n\nparser = argparse.ArgumentParser(description=\'PyTorch A2C example\')\nparser.add_argument(\'--env-name\', default=""Hopper-v2"", metavar=\'G\',\n                    help=\'name of the environment to run\')\nparser.add_argument(\'--model-path\', metavar=\'G\',\n                    help=\'path of pre-trained model\')\nparser.add_argument(\'--render\', action=\'store_true\', default=False,\n                    help=\'render the environment\')\nparser.add_argument(\'--log-std\', type=float, default=-0.0, metavar=\'G\',\n                    help=\'log std for the policy (default: -0.0)\')\nparser.add_argument(\'--gamma\', type=float, default=0.99, metavar=\'G\',\n                    help=\'discount factor (default: 0.99)\')\nparser.add_argument(\'--tau\', type=float, default=0.95, metavar=\'G\',\n                    help=\'gae (default: 0.95)\')\nparser.add_argument(\'--l2-reg\', type=float, default=1e-3, metavar=\'G\',\n                    help=\'l2 regularization regression (default: 1e-3)\')\nparser.add_argument(\'--num-threads\', type=int, default=4, metavar=\'N\',\n                    help=\'number of threads for agent (default: 4)\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'N\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--min-batch-size\', type=int, default=2048, metavar=\'N\',\n                    help=\'minimal batch size per A2C update (default: 2048)\')\nparser.add_argument(\'--max-iter-num\', type=int, default=500, metavar=\'N\',\n                    help=\'maximal number of main iterations (default: 500)\')\nparser.add_argument(\'--log-interval\', type=int, default=1, metavar=\'N\',\n                    help=\'interval between training status logs (default: 1)\')\nparser.add_argument(\'--save-model-interval\', type=int, default=0, metavar=\'N\',\n                    help=""interval between saving model (default: 0, means don\'t save)"")\nparser.add_argument(\'--gpu-index\', type=int, default=0, metavar=\'N\')\nargs = parser.parse_args()\n\ndtype = torch.float64\ntorch.set_default_dtype(dtype)\ndevice = torch.device(\'cuda\', index=args.gpu_index) if torch.cuda.is_available() else torch.device(\'cpu\')\nif torch.cuda.is_available():\n    torch.cuda.set_device(args.gpu_index)\n\n""""""environment""""""\nenv = gym.make(args.env_name)\nstate_dim = env.observation_space.shape[0]\nis_disc_action = len(env.action_space.shape) == 0\nrunning_state = ZFilter((state_dim,), clip=5)\n# running_reward = ZFilter((1,), demean=False, clip=10)\n\n""""""seeding""""""\nnp.random.seed(args.seed)\ntorch.manual_seed(args.seed)\nenv.seed(args.seed)\n\n""""""define actor and critic""""""\nif args.model_path is None:\n    if is_disc_action:\n        policy_net = DiscretePolicy(state_dim, env.action_space.n)\n    else:\n        policy_net = Policy(state_dim, env.action_space.shape[0], log_std=args.log_std)\n    value_net = Value(state_dim)\nelse:\n    policy_net, value_net, running_state = pickle.load(open(args.model_path, ""rb""))\npolicy_net.to(device)\nvalue_net.to(device)\n\noptimizer_policy = torch.optim.Adam(policy_net.parameters(), lr=0.01)\noptimizer_value = torch.optim.Adam(value_net.parameters(), lr=0.01)\n\n""""""create agent""""""\nagent = Agent(env, policy_net, device, running_state=running_state, render=args.render, num_threads=args.num_threads)\n\n\ndef update_params(batch):\n    states = torch.from_numpy(np.stack(batch.state)).to(dtype).to(device)\n    actions = torch.from_numpy(np.stack(batch.action)).to(dtype).to(device)\n    rewards = torch.from_numpy(np.stack(batch.reward)).to(dtype).to(device)\n    masks = torch.from_numpy(np.stack(batch.mask)).to(dtype).to(device)\n    with torch.no_grad():\n        values = value_net(states)\n\n    """"""get advantage estimation from the trajectories""""""\n    advantages, returns = estimate_advantages(rewards, masks, values, args.gamma, args.tau, device)\n\n    """"""perform TRPO update""""""\n    a2c_step(policy_net, value_net, optimizer_policy, optimizer_value, states, actions, returns, advantages, args.l2_reg)\n\n\ndef main_loop():\n    for i_iter in range(args.max_iter_num):\n        """"""generate multiple trajectories that reach the minimum batch_size""""""\n        batch, log = agent.collect_samples(args.min_batch_size)\n        t0 = time.time()\n        update_params(batch)\n        t1 = time.time()\n\n        if i_iter % args.log_interval == 0:\n            print(\'{}\\tT_sample {:.4f}\\tT_update {:.4f}\\tR_min {:.2f}\\tR_max {:.2f}\\tR_avg {:.2f}\'.format(\n                i_iter, log[\'sample_time\'], t1-t0, log[\'min_reward\'], log[\'max_reward\'], log[\'avg_reward\']))\n\n        if args.save_model_interval > 0 and (i_iter+1) % args.save_model_interval == 0:\n            to_device(torch.device(\'cpu\'), policy_net, value_net)\n            pickle.dump((policy_net, value_net, running_state),\n                        open(os.path.join(assets_dir(), \'learned_models/{}_a2c.p\'.format(args.env_name)), \'wb\'))\n            to_device(device, policy_net, value_net)\n\n        """"""clean up gpu memory""""""\n        torch.cuda.empty_cache()\n\n\nmain_loop()\n'"
examples/ppo_gym.py,15,"b'import argparse\nimport gym\nimport os\nimport sys\nimport pickle\nimport time\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \'..\')))\n\nfrom utils import *\nfrom models.mlp_policy import Policy\nfrom models.mlp_critic import Value\nfrom models.mlp_policy_disc import DiscretePolicy\nfrom core.ppo import ppo_step\nfrom core.common import estimate_advantages\nfrom core.agent import Agent\n\n\nparser = argparse.ArgumentParser(description=\'PyTorch PPO example\')\nparser.add_argument(\'--env-name\', default=""Hopper-v2"", metavar=\'G\',\n                    help=\'name of the environment to run\')\nparser.add_argument(\'--model-path\', metavar=\'G\',\n                    help=\'path of pre-trained model\')\nparser.add_argument(\'--render\', action=\'store_true\', default=False,\n                    help=\'render the environment\')\nparser.add_argument(\'--log-std\', type=float, default=-0.0, metavar=\'G\',\n                    help=\'log std for the policy (default: -0.0)\')\nparser.add_argument(\'--gamma\', type=float, default=0.99, metavar=\'G\',\n                    help=\'discount factor (default: 0.99)\')\nparser.add_argument(\'--tau\', type=float, default=0.95, metavar=\'G\',\n                    help=\'gae (default: 0.95)\')\nparser.add_argument(\'--l2-reg\', type=float, default=1e-3, metavar=\'G\',\n                    help=\'l2 regularization regression (default: 1e-3)\')\nparser.add_argument(\'--learning-rate\', type=float, default=3e-4, metavar=\'G\',\n                    help=\'learning rate (default: 3e-4)\')\nparser.add_argument(\'--clip-epsilon\', type=float, default=0.2, metavar=\'N\',\n                    help=\'clipping epsilon for PPO\')\nparser.add_argument(\'--num-threads\', type=int, default=4, metavar=\'N\',\n                    help=\'number of threads for agent (default: 4)\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'N\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--min-batch-size\', type=int, default=2048, metavar=\'N\',\n                    help=\'minimal batch size per PPO update (default: 2048)\')\nparser.add_argument(\'--max-iter-num\', type=int, default=500, metavar=\'N\',\n                    help=\'maximal number of main iterations (default: 500)\')\nparser.add_argument(\'--log-interval\', type=int, default=1, metavar=\'N\',\n                    help=\'interval between training status logs (default: 10)\')\nparser.add_argument(\'--save-model-interval\', type=int, default=0, metavar=\'N\',\n                    help=""interval between saving model (default: 0, means don\'t save)"")\nparser.add_argument(\'--gpu-index\', type=int, default=0, metavar=\'N\')\nargs = parser.parse_args()\n\ndtype = torch.float64\ntorch.set_default_dtype(dtype)\ndevice = torch.device(\'cuda\', index=args.gpu_index) if torch.cuda.is_available() else torch.device(\'cpu\')\nif torch.cuda.is_available():\n    torch.cuda.set_device(args.gpu_index)\n\n""""""environment""""""\nenv = gym.make(args.env_name)\nstate_dim = env.observation_space.shape[0]\nis_disc_action = len(env.action_space.shape) == 0\nrunning_state = ZFilter((state_dim,), clip=5)\n# running_reward = ZFilter((1,), demean=False, clip=10)\n\n""""""seeding""""""\nnp.random.seed(args.seed)\ntorch.manual_seed(args.seed)\nenv.seed(args.seed)\n\n""""""define actor and critic""""""\nif args.model_path is None:\n    if is_disc_action:\n        policy_net = DiscretePolicy(state_dim, env.action_space.n)\n    else:\n        policy_net = Policy(state_dim, env.action_space.shape[0], log_std=args.log_std)\n    value_net = Value(state_dim)\nelse:\n    policy_net, value_net, running_state = pickle.load(open(args.model_path, ""rb""))\npolicy_net.to(device)\nvalue_net.to(device)\n\noptimizer_policy = torch.optim.Adam(policy_net.parameters(), lr=args.learning_rate)\noptimizer_value = torch.optim.Adam(value_net.parameters(), lr=args.learning_rate)\n\n# optimization epoch number and batch size for PPO\noptim_epochs = 10\noptim_batch_size = 64\n\n""""""create agent""""""\nagent = Agent(env, policy_net, device, running_state=running_state, render=args.render, num_threads=args.num_threads)\n\n\ndef update_params(batch, i_iter):\n    states = torch.from_numpy(np.stack(batch.state)).to(dtype).to(device)\n    actions = torch.from_numpy(np.stack(batch.action)).to(dtype).to(device)\n    rewards = torch.from_numpy(np.stack(batch.reward)).to(dtype).to(device)\n    masks = torch.from_numpy(np.stack(batch.mask)).to(dtype).to(device)\n    with torch.no_grad():\n        values = value_net(states)\n        fixed_log_probs = policy_net.get_log_prob(states, actions)\n\n    """"""get advantage estimation from the trajectories""""""\n    advantages, returns = estimate_advantages(rewards, masks, values, args.gamma, args.tau, device)\n\n    """"""perform mini-batch PPO update""""""\n    optim_iter_num = int(math.ceil(states.shape[0] / optim_batch_size))\n    for _ in range(optim_epochs):\n        perm = np.arange(states.shape[0])\n        np.random.shuffle(perm)\n        perm = LongTensor(perm).to(device)\n\n        states, actions, returns, advantages, fixed_log_probs = \\\n            states[perm].clone(), actions[perm].clone(), returns[perm].clone(), advantages[perm].clone(), fixed_log_probs[perm].clone()\n\n        for i in range(optim_iter_num):\n            ind = slice(i * optim_batch_size, min((i + 1) * optim_batch_size, states.shape[0]))\n            states_b, actions_b, advantages_b, returns_b, fixed_log_probs_b = \\\n                states[ind], actions[ind], advantages[ind], returns[ind], fixed_log_probs[ind]\n\n            ppo_step(policy_net, value_net, optimizer_policy, optimizer_value, 1, states_b, actions_b, returns_b,\n                     advantages_b, fixed_log_probs_b, args.clip_epsilon, args.l2_reg)\n\n\ndef main_loop():\n    for i_iter in range(args.max_iter_num):\n        """"""generate multiple trajectories that reach the minimum batch_size""""""\n        batch, log = agent.collect_samples(args.min_batch_size)\n        t0 = time.time()\n        update_params(batch, i_iter)\n        t1 = time.time()\n\n        if i_iter % args.log_interval == 0:\n            print(\'{}\\tT_sample {:.4f}\\tT_update {:.4f}\\tR_min {:.2f}\\tR_max {:.2f}\\tR_avg {:.2f}\'.format(\n                i_iter, log[\'sample_time\'], t1-t0, log[\'min_reward\'], log[\'max_reward\'], log[\'avg_reward\']))\n\n        if args.save_model_interval > 0 and (i_iter+1) % args.save_model_interval == 0:\n            to_device(torch.device(\'cpu\'), policy_net, value_net)\n            pickle.dump((policy_net, value_net, running_state),\n                        open(os.path.join(assets_dir(), \'learned_models/{}_ppo.p\'.format(args.env_name)), \'wb\'))\n            to_device(device, policy_net, value_net)\n\n        """"""clean up gpu memory""""""\n        torch.cuda.empty_cache()\n\n\nmain_loop()\n'"
examples/trpo_gym.py,13,"b'import argparse\nimport gym\nimport os\nimport sys\nimport pickle\nimport time\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \'..\')))\n\nfrom utils import *\nfrom models.mlp_policy import Policy\nfrom models.mlp_critic import Value\nfrom models.mlp_policy_disc import DiscretePolicy\nfrom core.trpo import trpo_step\nfrom core.common import estimate_advantages\nfrom core.agent import Agent\n\n\nparser = argparse.ArgumentParser(description=\'PyTorch TRPO example\')\nparser.add_argument(\'--env-name\', default=""Hopper-v2"", metavar=\'G\',\n                    help=\'name of the environment to run\')\nparser.add_argument(\'--model-path\', metavar=\'G\',\n                    help=\'path of pre-trained model\')\nparser.add_argument(\'--render\', action=\'store_true\', default=False,\n                    help=\'render the environment\')\nparser.add_argument(\'--log-std\', type=float, default=-0.0, metavar=\'G\',\n                    help=\'log std for the policy (default: -0.0)\')\nparser.add_argument(\'--gamma\', type=float, default=0.99, metavar=\'G\',\n                    help=\'discount factor (default: 0.99)\')\nparser.add_argument(\'--tau\', type=float, default=0.95, metavar=\'G\',\n                    help=\'gae (default: 0.95)\')\nparser.add_argument(\'--l2-reg\', type=float, default=1e-3, metavar=\'G\',\n                    help=\'l2 regularization regression (default: 1e-3)\')\nparser.add_argument(\'--max-kl\', type=float, default=1e-2, metavar=\'G\',\n                    help=\'max kl value (default: 1e-2)\')\nparser.add_argument(\'--damping\', type=float, default=1e-2, metavar=\'G\',\n                    help=\'damping (default: 1e-2)\')\nparser.add_argument(\'--num-threads\', type=int, default=4, metavar=\'N\',\n                    help=\'number of threads for agent (default: 4)\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'N\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--min-batch-size\', type=int, default=2048, metavar=\'N\',\n                    help=\'minimal batch size per TRPO update (default: 2048)\')\nparser.add_argument(\'--max-iter-num\', type=int, default=500, metavar=\'N\',\n                    help=\'maximal number of main iterations (default: 500)\')\nparser.add_argument(\'--log-interval\', type=int, default=1, metavar=\'N\',\n                    help=\'interval between training status logs (default: 10)\')\nparser.add_argument(\'--save-model-interval\', type=int, default=0, metavar=\'N\',\n                    help=""interval between saving model (default: 0, means don\'t save)"")\nparser.add_argument(\'--gpu-index\', type=int, default=0, metavar=\'N\')\nargs = parser.parse_args()\n\ndtype = torch.float64\ntorch.set_default_dtype(dtype)\ndevice = torch.device(\'cuda\', index=args.gpu_index) if torch.cuda.is_available() else torch.device(\'cpu\')\nif torch.cuda.is_available():\n    torch.cuda.set_device(args.gpu_index)\n\n""""""environment""""""\nenv = gym.make(args.env_name)\nstate_dim = env.observation_space.shape[0]\nis_disc_action = len(env.action_space.shape) == 0\nrunning_state = ZFilter((state_dim,), clip=5)\n# running_reward = ZFilter((1,), demean=False, clip=10)\n\n""""""seeding""""""\nnp.random.seed(args.seed)\ntorch.manual_seed(args.seed)\nenv.seed(args.seed)\n\n""""""define actor and critic""""""\nif args.model_path is None:\n    if is_disc_action:\n        policy_net = DiscretePolicy(state_dim, env.action_space.n)\n    else:\n        policy_net = Policy(state_dim, env.action_space.shape[0], log_std=args.log_std)\n    value_net = Value(state_dim)\nelse:\n    policy_net, value_net, running_state = pickle.load(open(args.model_path, ""rb""))\npolicy_net.to(device)\nvalue_net.to(device)\n\n""""""create agent""""""\nagent = Agent(env, policy_net, device, running_state=running_state, render=args.render, num_threads=args.num_threads)\n\n\ndef update_params(batch):\n    states = torch.from_numpy(np.stack(batch.state)).to(dtype).to(device)\n    actions = torch.from_numpy(np.stack(batch.action)).to(dtype).to(device)\n    rewards = torch.from_numpy(np.stack(batch.reward)).to(dtype).to(device)\n    masks = torch.from_numpy(np.stack(batch.mask)).to(dtype).to(device)\n    with torch.no_grad():\n        values = value_net(states)\n\n    """"""get advantage estimation from the trajectories""""""\n    advantages, returns = estimate_advantages(rewards, masks, values, args.gamma, args.tau, device)\n\n    """"""perform TRPO update""""""\n    trpo_step(policy_net, value_net, states, actions, returns, advantages, args.max_kl, args.damping, args.l2_reg)\n\n\ndef main_loop():\n    for i_iter in range(args.max_iter_num):\n        """"""generate multiple trajectories that reach the minimum batch_size""""""\n        batch, log = agent.collect_samples(args.min_batch_size)\n        t0 = time.time()\n        update_params(batch)\n        t1 = time.time()\n\n        if i_iter % args.log_interval == 0:\n            print(\'{}\\tT_sample {:.4f}\\tT_update {:.4f}\\tR_min {:.2f}\\tR_max {:.2f}\\tR_avg {:.2f}\'.format(\n                i_iter, log[\'sample_time\'], t1-t0, log[\'min_reward\'], log[\'max_reward\'], log[\'avg_reward\']))\n\n        if args.save_model_interval > 0 and (i_iter+1) % args.save_model_interval == 0:\n            to_device(torch.device(\'cpu\'), policy_net, value_net)\n            pickle.dump((policy_net, value_net, running_state),\n                        open(os.path.join(assets_dir(), \'learned_models/{}_trpo.p\'.format(args.env_name)), \'wb\'))\n            to_device(device, policy_net, value_net)\n\n        """"""clean up gpu memory""""""\n        torch.cuda.empty_cache()\n\n\nmain_loop()\n'"
gail/gail_gym.py,20,"b'import argparse\nimport gym\nimport os\nimport sys\nimport pickle\nimport time\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \'..\')))\n\nfrom utils import *\nfrom models.mlp_policy import Policy\nfrom models.mlp_critic import Value\nfrom models.mlp_policy_disc import DiscretePolicy\nfrom models.mlp_discriminator import Discriminator\nfrom torch import nn\nfrom core.ppo import ppo_step\nfrom core.common import estimate_advantages\nfrom core.agent import Agent\n\n\nparser = argparse.ArgumentParser(description=\'PyTorch GAIL example\')\nparser.add_argument(\'--env-name\', default=""Hopper-v2"", metavar=\'G\',\n                    help=\'name of the environment to run\')\nparser.add_argument(\'--expert-traj-path\', metavar=\'G\',\n                    help=\'path of the expert trajectories\')\nparser.add_argument(\'--render\', action=\'store_true\', default=False,\n                    help=\'render the environment\')\nparser.add_argument(\'--log-std\', type=float, default=-0.0, metavar=\'G\',\n                    help=\'log std for the policy (default: -0.0)\')\nparser.add_argument(\'--gamma\', type=float, default=0.99, metavar=\'G\',\n                    help=\'discount factor (default: 0.99)\')\nparser.add_argument(\'--tau\', type=float, default=0.95, metavar=\'G\',\n                    help=\'gae (default: 0.95)\')\nparser.add_argument(\'--l2-reg\', type=float, default=1e-3, metavar=\'G\',\n                    help=\'l2 regularization regression (default: 1e-3)\')\nparser.add_argument(\'--learning-rate\', type=float, default=3e-4, metavar=\'G\',\n                    help=\'gae (default: 3e-4)\')\nparser.add_argument(\'--clip-epsilon\', type=float, default=0.2, metavar=\'N\',\n                    help=\'clipping epsilon for PPO\')\nparser.add_argument(\'--num-threads\', type=int, default=4, metavar=\'N\',\n                    help=\'number of threads for agent (default: 4)\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'N\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--min-batch-size\', type=int, default=2048, metavar=\'N\',\n                    help=\'minimal batch size per PPO update (default: 2048)\')\nparser.add_argument(\'--max-iter-num\', type=int, default=500, metavar=\'N\',\n                    help=\'maximal number of main iterations (default: 500)\')\nparser.add_argument(\'--log-interval\', type=int, default=1, metavar=\'N\',\n                    help=\'interval between training status logs (default: 10)\')\nparser.add_argument(\'--save-model-interval\', type=int, default=0, metavar=\'N\',\n                    help=""interval between saving model (default: 0, means don\'t save)"")\nparser.add_argument(\'--gpu-index\', type=int, default=0, metavar=\'N\')\nargs = parser.parse_args()\n\ndtype = torch.float64\ntorch.set_default_dtype(dtype)\ndevice = torch.device(\'cuda\', index=args.gpu_index) if torch.cuda.is_available() else torch.device(\'cpu\')\nif torch.cuda.is_available():\n    torch.cuda.set_device(args.gpu_index)\n\n""""""environment""""""\nenv = gym.make(args.env_name)\nstate_dim = env.observation_space.shape[0]\nis_disc_action = len(env.action_space.shape) == 0\naction_dim = 1 if is_disc_action else env.action_space.shape[0]\nrunning_state = ZFilter((state_dim,), clip=5)\n# running_reward = ZFilter((1,), demean=False, clip=10)\n\n""""""seeding""""""\nnp.random.seed(args.seed)\ntorch.manual_seed(args.seed)\nenv.seed(args.seed)\n\n""""""define actor and critic""""""\nif is_disc_action:\n    policy_net = DiscretePolicy(state_dim, env.action_space.n)\nelse:\n    policy_net = Policy(state_dim, env.action_space.shape[0], log_std=args.log_std)\nvalue_net = Value(state_dim)\ndiscrim_net = Discriminator(state_dim + action_dim)\ndiscrim_criterion = nn.BCELoss()\nto_device(device, policy_net, value_net, discrim_net, discrim_criterion)\n\noptimizer_policy = torch.optim.Adam(policy_net.parameters(), lr=args.learning_rate)\noptimizer_value = torch.optim.Adam(value_net.parameters(), lr=args.learning_rate)\noptimizer_discrim = torch.optim.Adam(discrim_net.parameters(), lr=args.learning_rate)\n\n# optimization epoch number and batch size for PPO\noptim_epochs = 10\noptim_batch_size = 64\n\n# load trajectory\nexpert_traj, running_state = pickle.load(open(args.expert_traj_path, ""rb""))\nrunning_state.fix = True\n\n\ndef expert_reward(state, action):\n    state_action = tensor(np.hstack([state, action]), dtype=dtype)\n    with torch.no_grad():\n        return -math.log(discrim_net(state_action)[0].item())\n\n\n""""""create agent""""""\nagent = Agent(env, policy_net, device, custom_reward=expert_reward,\n              running_state=running_state, render=args.render, num_threads=args.num_threads)\n\n\ndef update_params(batch, i_iter):\n    states = torch.from_numpy(np.stack(batch.state)).to(dtype).to(device)\n    actions = torch.from_numpy(np.stack(batch.action)).to(dtype).to(device)\n    rewards = torch.from_numpy(np.stack(batch.reward)).to(dtype).to(device)\n    masks = torch.from_numpy(np.stack(batch.mask)).to(dtype).to(device)\n    with torch.no_grad():\n        values = value_net(states)\n        fixed_log_probs = policy_net.get_log_prob(states, actions)\n\n    """"""get advantage estimation from the trajectories""""""\n    advantages, returns = estimate_advantages(rewards, masks, values, args.gamma, args.tau, device)\n\n    """"""update discriminator""""""\n    for _ in range(1):\n        expert_state_actions = torch.from_numpy(expert_traj).to(dtype).to(device)\n        g_o = discrim_net(torch.cat([states, actions], 1))\n        e_o = discrim_net(expert_state_actions)\n        optimizer_discrim.zero_grad()\n        discrim_loss = discrim_criterion(g_o, ones((states.shape[0], 1), device=device)) + \\\n            discrim_criterion(e_o, zeros((expert_traj.shape[0], 1), device=device))\n        discrim_loss.backward()\n        optimizer_discrim.step()\n\n    """"""perform mini-batch PPO update""""""\n    optim_iter_num = int(math.ceil(states.shape[0] / optim_batch_size))\n    for _ in range(optim_epochs):\n        perm = np.arange(states.shape[0])\n        np.random.shuffle(perm)\n        perm = LongTensor(perm).to(device)\n\n        states, actions, returns, advantages, fixed_log_probs = \\\n            states[perm].clone(), actions[perm].clone(), returns[perm].clone(), advantages[perm].clone(), fixed_log_probs[perm].clone()\n\n        for i in range(optim_iter_num):\n            ind = slice(i * optim_batch_size, min((i + 1) * optim_batch_size, states.shape[0]))\n            states_b, actions_b, advantages_b, returns_b, fixed_log_probs_b = \\\n                states[ind], actions[ind], advantages[ind], returns[ind], fixed_log_probs[ind]\n\n            ppo_step(policy_net, value_net, optimizer_policy, optimizer_value, 1, states_b, actions_b, returns_b,\n                     advantages_b, fixed_log_probs_b, args.clip_epsilon, args.l2_reg)\n\n\ndef main_loop():\n    for i_iter in range(args.max_iter_num):\n        """"""generate multiple trajectories that reach the minimum batch_size""""""\n        discrim_net.to(torch.device(\'cpu\'))\n        batch, log = agent.collect_samples(args.min_batch_size)\n        discrim_net.to(device)\n\n        t0 = time.time()\n        update_params(batch, i_iter)\n        t1 = time.time()\n\n        if i_iter % args.log_interval == 0:\n            print(\'{}\\tT_sample {:.4f}\\tT_update {:.4f}\\texpert_R_avg {:.2f}\\tR_avg {:.2f}\'.format(\n                i_iter, log[\'sample_time\'], t1-t0, log[\'avg_c_reward\'], log[\'avg_reward\']))\n\n        if args.save_model_interval > 0 and (i_iter+1) % args.save_model_interval == 0:\n            to_device(torch.device(\'cpu\'), policy_net, value_net, discrim_net)\n            pickle.dump((policy_net, value_net, discrim_net), open(os.path.join(assets_dir(), \'learned_models/{}_gail.p\'.format(args.env_name)), \'wb\'))\n            to_device(device, policy_net, value_net, discrim_net)\n\n        """"""clean up gpu memory""""""\n        torch.cuda.empty_cache()\n\n\nmain_loop()\n'"
gail/save_expert_traj.py,3,"b'import argparse\nimport gym\nimport os\nimport sys\nimport pickle\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \'..\')))\n\nfrom itertools import count\nfrom utils import *\n\n\nparser = argparse.ArgumentParser(description=\'Save expert trajectory\')\nparser.add_argument(\'--env-name\', default=""Hopper-v2"", metavar=\'G\',\n                    help=\'name of the environment to run\')\nparser.add_argument(\'--model-path\', metavar=\'G\',\n                    help=\'name of the expert model\')\nparser.add_argument(\'--render\', action=\'store_true\', default=False,\n                    help=\'render the environment\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'N\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--max-expert-state-num\', type=int, default=50000, metavar=\'N\',\n                    help=\'maximal number of main iterations (default: 50000)\')\nargs = parser.parse_args()\n\ndtype = torch.float64\ntorch.set_default_dtype(dtype)\nenv = gym.make(args.env_name)\nenv.seed(args.seed)\ntorch.manual_seed(args.seed)\nis_disc_action = len(env.action_space.shape) == 0\nstate_dim = env.observation_space.shape[0]\n\npolicy_net, _, running_state = pickle.load(open(args.model_path, ""rb""))\nrunning_state.fix = True\nexpert_traj = []\n\n\ndef main_loop():\n\n    num_steps = 0\n\n    for i_episode in count():\n\n        state = env.reset()\n        state = running_state(state)\n        reward_episode = 0\n\n        for t in range(10000):\n            state_var = tensor(state).unsqueeze(0).to(dtype)\n            # choose mean action\n            action = policy_net(state_var)[0][0].detach().numpy()\n            # choose stochastic action\n            # action = policy_net.select_action(state_var)[0].cpu().numpy()\n            action = int(action) if is_disc_action else action.astype(np.float64)\n            next_state, reward, done, _ = env.step(action)\n            next_state = running_state(next_state)\n            reward_episode += reward\n            num_steps += 1\n\n            expert_traj.append(np.hstack([state, action]))\n\n            if args.render:\n                env.render()\n            if done or num_steps >= args.max_expert_state_num:\n                break\n\n            state = next_state\n\n        print(\'Episode {}\\t reward: {:.2f}\'.format(i_episode, reward_episode))\n\n        if num_steps >= args.max_expert_state_num:\n            break\n\n\nmain_loop()\nexpert_traj = np.stack(expert_traj)\npickle.dump((expert_traj, running_state), open(os.path.join(assets_dir(), \'expert_traj/{}_expert_traj.p\'.format(args.env_name)), \'wb\'))\n'"
models/mlp_critic.py,4,"b""import torch.nn as nn\nimport torch\n\n\nclass Value(nn.Module):\n    def __init__(self, state_dim, hidden_size=(128, 128), activation='tanh'):\n        super().__init__()\n        if activation == 'tanh':\n            self.activation = torch.tanh\n        elif activation == 'relu':\n            self.activation = torch.relu\n        elif activation == 'sigmoid':\n            self.activation = torch.sigmoid\n\n        self.affine_layers = nn.ModuleList()\n        last_dim = state_dim\n        for nh in hidden_size:\n            self.affine_layers.append(nn.Linear(last_dim, nh))\n            last_dim = nh\n\n        self.value_head = nn.Linear(last_dim, 1)\n        self.value_head.weight.data.mul_(0.1)\n        self.value_head.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        for affine in self.affine_layers:\n            x = self.activation(affine(x))\n\n        value = self.value_head(x)\n        return value\n"""
models/mlp_discriminator.py,5,"b""import torch.nn as nn\nimport torch\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, num_inputs, hidden_size=(128, 128), activation='tanh'):\n        super().__init__()\n        if activation == 'tanh':\n            self.activation = torch.tanh\n        elif activation == 'relu':\n            self.activation = torch.relu\n        elif activation == 'sigmoid':\n            self.activation = torch.sigmoid\n\n        self.affine_layers = nn.ModuleList()\n        last_dim = num_inputs\n        for nh in hidden_size:\n            self.affine_layers.append(nn.Linear(last_dim, nh))\n            last_dim = nh\n\n        self.logic = nn.Linear(last_dim, 1)\n        self.logic.weight.data.mul_(0.1)\n        self.logic.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        for affine in self.affine_layers:\n            x = self.activation(affine(x))\n\n        prob = torch.sigmoid(self.logic(x))\n        return prob\n"""
models/mlp_policy.py,7,"b'import torch.nn as nn\nimport torch\nfrom utils.math import *\n\n\nclass Policy(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_size=(128, 128), activation=\'tanh\', log_std=0):\n        super().__init__()\n        self.is_disc_action = False\n        if activation == \'tanh\':\n            self.activation = torch.tanh\n        elif activation == \'relu\':\n            self.activation = torch.relu\n        elif activation == \'sigmoid\':\n            self.activation = torch.sigmoid\n\n        self.affine_layers = nn.ModuleList()\n        last_dim = state_dim\n        for nh in hidden_size:\n            self.affine_layers.append(nn.Linear(last_dim, nh))\n            last_dim = nh\n\n        self.action_mean = nn.Linear(last_dim, action_dim)\n        self.action_mean.weight.data.mul_(0.1)\n        self.action_mean.bias.data.mul_(0.0)\n\n        self.action_log_std = nn.Parameter(torch.ones(1, action_dim) * log_std)\n\n    def forward(self, x):\n        for affine in self.affine_layers:\n            x = self.activation(affine(x))\n\n        action_mean = self.action_mean(x)\n        action_log_std = self.action_log_std.expand_as(action_mean)\n        action_std = torch.exp(action_log_std)\n\n        return action_mean, action_log_std, action_std\n\n    def select_action(self, x):\n        action_mean, _, action_std = self.forward(x)\n        action = torch.normal(action_mean, action_std)\n        return action\n\n    def get_kl(self, x):\n        mean1, log_std1, std1 = self.forward(x)\n\n        mean0 = mean1.detach()\n        log_std0 = log_std1.detach()\n        std0 = std1.detach()\n        kl = log_std1 - log_std0 + (std0.pow(2) + (mean0 - mean1).pow(2)) / (2.0 * std1.pow(2)) - 0.5\n        return kl.sum(1, keepdim=True)\n\n    def get_log_prob(self, x, actions):\n        action_mean, action_log_std, action_std = self.forward(x)\n        return normal_log_density(actions, action_mean, action_log_std, action_std)\n\n    def get_fim(self, x):\n        mean, _, _ = self.forward(x)\n        cov_inv = self.action_log_std.exp().pow(-2).squeeze(0).repeat(x.size(0))\n        param_count = 0\n        std_index = 0\n        id = 0\n        for name, param in self.named_parameters():\n            if name == ""action_log_std"":\n                std_id = id\n                std_index = param_count\n            param_count += param.view(-1).shape[0]\n            id += 1\n        return cov_inv.detach(), mean, {\'std_id\': std_id, \'std_index\': std_index}\n\n\n'"
models/mlp_policy_disc.py,7,"b""import torch.nn as nn\nimport torch\nfrom utils.math import *\n\n\nclass DiscretePolicy(nn.Module):\n    def __init__(self, state_dim, action_num, hidden_size=(128, 128), activation='tanh'):\n        super().__init__()\n        self.is_disc_action = True\n        if activation == 'tanh':\n            self.activation = torch.tanh\n        elif activation == 'relu':\n            self.activation = torch.relu\n        elif activation == 'sigmoid':\n            self.activation = torch.sigmoid\n\n        self.affine_layers = nn.ModuleList()\n        last_dim = state_dim\n        for nh in hidden_size:\n            self.affine_layers.append(nn.Linear(last_dim, nh))\n            last_dim = nh\n\n        self.action_head = nn.Linear(last_dim, action_num)\n        self.action_head.weight.data.mul_(0.1)\n        self.action_head.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        for affine in self.affine_layers:\n            x = self.activation(affine(x))\n\n        action_prob = torch.softmax(self.action_head(x), dim=1)\n        return action_prob\n\n    def select_action(self, x):\n        action_prob = self.forward(x)\n        action = action_prob.multinomial(1)\n        return action\n\n    def get_kl(self, x):\n        action_prob1 = self.forward(x)\n        action_prob0 = action_prob1.detach()\n        kl = action_prob0 * (torch.log(action_prob0) - torch.log(action_prob1))\n        return kl.sum(1, keepdim=True)\n\n    def get_log_prob(self, x, actions):\n        action_prob = self.forward(x)\n        return torch.log(action_prob.gather(1, actions.long().unsqueeze(1)))\n\n    def get_fim(self, x):\n        action_prob = self.forward(x)\n        M = action_prob.pow(-1).view(-1).detach()\n        return M, action_prob, {}\n\n"""
utils/__init__.py,0,b'from utils.replay_memory import *\nfrom utils.zfilter import *\nfrom utils.torch import *\nfrom utils.math import *\nfrom utils.tools import *\n'
utils/math.py,1,"b'import torch\nimport math\n\n\ndef normal_entropy(std):\n    var = std.pow(2)\n    entropy = 0.5 + 0.5 * torch.log(2 * var * math.pi)\n    return entropy.sum(1, keepdim=True)\n\n\ndef normal_log_density(x, mean, log_std, std):\n    var = std.pow(2)\n    log_density = -(x - mean).pow(2) / (2 * var) - 0.5 * math.log(2 * math.pi) - log_std\n    return log_density.sum(1, keepdim=True)\n'"
utils/replay_memory.py,0,"b'from collections import namedtuple\nimport random\n\n# Taken from\n# https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb\n\nTransition = namedtuple(\'Transition\', (\'state\', \'action\', \'mask\', \'next_state\',\n                                       \'reward\'))\n\n\nclass Memory(object):\n    def __init__(self):\n        self.memory = []\n\n    def push(self, *args):\n        """"""Saves a transition.""""""\n        self.memory.append(Transition(*args))\n\n    def sample(self, batch_size=None):\n        if batch_size is None:\n            return Transition(*zip(*self.memory))\n        else:\n            random_batch = random.sample(self.memory, batch_size)\n            return Transition(*zip(*random_batch))\n\n    def append(self, new_memory):\n        self.memory += new_memory.memory\n\n    def __len__(self):\n        return len(self.memory)\n'"
utils/tools.py,0,"b""from os import path\n\n\ndef assets_dir():\n    return path.abspath(path.join(path.dirname(path.abspath(__file__)), '../assets'))\n"""
utils/torch.py,11,"b'import torch\nimport numpy as np\n\ntensor = torch.tensor\nDoubleTensor = torch.DoubleTensor\nFloatTensor = torch.FloatTensor\nLongTensor = torch.LongTensor\nByteTensor = torch.ByteTensor\nones = torch.ones\nzeros = torch.zeros\n\n\ndef to_device(device, *args):\n    return [x.to(device) for x in args]\n\n\ndef get_flat_params_from(model):\n    params = []\n    for param in model.parameters():\n        params.append(param.view(-1))\n\n    flat_params = torch.cat(params)\n    return flat_params\n\n\ndef set_flat_params_to(model, flat_params):\n    prev_ind = 0\n    for param in model.parameters():\n        flat_size = int(np.prod(list(param.size())))\n        param.data.copy_(\n            flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n        prev_ind += flat_size\n\n\ndef get_flat_grad_from(inputs, grad_grad=False):\n    grads = []\n    for param in inputs:\n        if grad_grad:\n            grads.append(param.grad.grad.view(-1))\n        else:\n            if param.grad is None:\n                grads.append(zeros(param.view(-1).shape))\n            else:\n                grads.append(param.grad.view(-1))\n\n    flat_grad = torch.cat(grads)\n    return flat_grad\n\n\ndef compute_flat_grad(output, inputs, filter_input_ids=set(), retain_graph=False, create_graph=False):\n    if create_graph:\n        retain_graph = True\n\n    inputs = list(inputs)\n    params = []\n    for i, param in enumerate(inputs):\n        if i not in filter_input_ids:\n            params.append(param)\n\n    grads = torch.autograd.grad(output, params, retain_graph=retain_graph, create_graph=create_graph)\n\n    j = 0\n    out_grads = []\n    for i, param in enumerate(inputs):\n        if i in filter_input_ids:\n            out_grads.append(zeros(param.view(-1).shape, device=param.device, dtype=param.dtype))\n        else:\n            out_grads.append(grads[j].view(-1))\n            j += 1\n    grads = torch.cat(out_grads)\n\n    for param in params:\n        param.grad = None\n    return grads\n'"
utils/zfilter.py,0,"b'import numpy as np\n\n# from https://github.com/joschu/modular_rl\n# http://www.johndcook.com/blog/standard_deviation/\n\n\nclass RunningStat(object):\n    def __init__(self, shape):\n        self._n = 0\n        self._M = np.zeros(shape)\n        self._S = np.zeros(shape)\n\n    def push(self, x):\n        x = np.asarray(x)\n        assert x.shape == self._M.shape\n        self._n += 1\n        if self._n == 1:\n            self._M[...] = x\n        else:\n            oldM = self._M.copy()\n            self._M[...] = oldM + (x - oldM) / self._n\n            self._S[...] = self._S + (x - oldM) * (x - self._M)\n\n    @property\n    def n(self):\n        return self._n\n\n    @property\n    def mean(self):\n        return self._M\n\n    @property\n    def var(self):\n        return self._S / (self._n - 1) if self._n > 1 else np.square(self._M)\n\n    @property\n    def std(self):\n        return np.sqrt(self.var)\n\n    @property\n    def shape(self):\n        return self._M.shape\n\n\nclass ZFilter:\n    """"""\n    y = (x-mean)/std\n    using running estimates of mean,std\n    """"""\n\n    def __init__(self, shape, demean=True, destd=True, clip=10.0):\n        self.demean = demean\n        self.destd = destd\n        self.clip = clip\n\n        self.rs = RunningStat(shape)\n        self.fix = False\n\n    def __call__(self, x, update=True):\n        if update and not self.fix:\n            self.rs.push(x)\n        if self.demean:\n            x = x - self.rs.mean\n        if self.destd:\n            x = x / (self.rs.std + 1e-8)\n        if self.clip:\n            x = np.clip(x, -self.clip, self.clip)\n        return x\n\n'"
