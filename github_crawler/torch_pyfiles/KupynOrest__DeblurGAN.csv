file_path,api_count,code
test.py,0,"b""import time\nimport os\nfrom options.test_options import TestOptions\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nfrom util.visualizer import Visualizer\nfrom pdb import set_trace as st\nfrom util import html\nfrom util.metrics import PSNR\nfrom ssim import SSIM\nfrom PIL import Image\n\nopt = TestOptions().parse()\nopt.nThreads = 1   # test code only supports nThreads = 1\nopt.batchSize = 1  # test code only supports batchSize = 1\nopt.serial_batches = True  # no shuffle\nopt.no_flip = True  # no flip\n\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\nmodel = create_model(opt)\nvisualizer = Visualizer(opt)\n# create website\nweb_dir = os.path.join(opt.results_dir, opt.name, '%s_%s' % (opt.phase, opt.which_epoch))\nwebpage = html.HTML(web_dir, 'Experiment = %s, Phase = %s, Epoch = %s' % (opt.name, opt.phase, opt.which_epoch))\n# test\navgPSNR = 0.0\navgSSIM = 0.0\ncounter = 0\n\nfor i, data in enumerate(dataset):\n\tif i >= opt.how_many:\n\t\tbreak\n\tcounter = i\n\tmodel.set_input(data)\n\tmodel.test()\n\tvisuals = model.get_current_visuals()\n\t#avgPSNR += PSNR(visuals['fake_B'],visuals['real_B'])\n\t#pilFake = Image.fromarray(visuals['fake_B'])\n\t#pilReal = Image.fromarray(visuals['real_B'])\n\t#avgSSIM += SSIM(pilFake).cw_ssim_value(pilReal)\n\timg_path = model.get_image_paths()\n\tprint('process image... %s' % img_path)\n\tvisualizer.save_images(webpage, visuals, img_path)\n\t\n#avgPSNR /= counter\n#avgSSIM /= counter\n#print('PSNR = %f, SSIM = %f' %\n#\t\t\t\t  (avgPSNR, avgSSIM))\n\nwebpage.save()\n"""
train.py,0,"b'import time\nfrom options.train_options import TrainOptions\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nfrom util.visualizer import Visualizer\nfrom util.metrics import PSNR, SSIM\nfrom multiprocessing import freeze_support\n\ndef train(opt, data_loader, model, visualizer):\n\tdataset = data_loader.load_data()\n\tdataset_size = len(data_loader)\n\tprint(\'#training images = %d\' % dataset_size)\n\ttotal_steps = 0\n\tfor epoch in range(opt.epoch_count, opt.niter + opt.niter_decay + 1):\n\t\tepoch_start_time = time.time()\n\t\tepoch_iter = 0\n\t\tfor i, data in enumerate(dataset):\n\t\t\titer_start_time = time.time()\n\t\t\ttotal_steps += opt.batchSize\n\t\t\tepoch_iter += opt.batchSize\n\t\t\tmodel.set_input(data)\n\t\t\tmodel.optimize_parameters()\n\n\t\t\tif total_steps % opt.display_freq == 0:\n\t\t\t\tresults = model.get_current_visuals()\n\t\t\t\tpsnrMetric = PSNR(results[\'Restored_Train\'], results[\'Sharp_Train\'])\n\t\t\t\tprint(\'PSNR on Train = %f\' % psnrMetric)\n\t\t\t\tvisualizer.display_current_results(results, epoch)\n\n\t\t\tif total_steps % opt.print_freq == 0:\n\t\t\t\terrors = model.get_current_errors()\n\t\t\t\tt = (time.time() - iter_start_time) / opt.batchSize\n\t\t\t\tvisualizer.print_current_errors(epoch, epoch_iter, errors, t)\n\t\t\t\tif opt.display_id > 0:\n\t\t\t\t\tvisualizer.plot_current_errors(epoch, float(epoch_iter)/dataset_size, opt, errors)\n\n\t\t\tif total_steps % opt.save_latest_freq == 0:\n\t\t\t\tprint(\'saving the latest model (epoch %d, total_steps %d)\' % (epoch, total_steps))\n\t\t\t\tmodel.save(\'latest\')\n\n\t\tif epoch % opt.save_epoch_freq == 0:\n\t\t\tprint(\'saving the model at the end of epoch %d, iters %d\' % (epoch, total_steps))\n\t\t\tmodel.save(\'latest\')\n\t\t\tmodel.save(epoch)\n\n\t\tprint(\'End of epoch %d / %d \\t Time Taken: %d sec\' % (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n\n\t\tif epoch > opt.niter:\n\t\t\tmodel.update_learning_rate()\n\n\nif __name__ == \'__main__\':\n\tfreeze_support()\n\n\t# python train.py --dataroot /.path_to_your_data --learn_residual --resize_or_crop crop --fineSize CROP_SIZE (we used 256)\n\n\topt = TrainOptions().parse()\n\topt.dataroot = \'D:\\Photos\\TrainingData\\BlurredSharp\\combined\'\n\topt.learn_residual = True\n\topt.resize_or_crop = ""crop""\n\topt.fineSize = 256\n\topt.gan_type = ""gan""\n\t# opt.which_model_netG = ""unet_256""\n\n\t# default = 5000\n\topt.save_latest_freq = 100\n\n\t# default = 100\n\topt.print_freq = 20\n\n\tdata_loader = CreateDataLoader(opt)\n\tmodel = create_model(opt)\n\tvisualizer = Visualizer(opt)\n\ttrain(opt, data_loader, model, visualizer)\n'"
data/__init__.py,0,b''
data/aligned_dataset.py,1,"b""import os.path\nimport random\nimport torchvision.transforms as transforms\nimport torch\nfrom data.base_dataset import BaseDataset\nfrom data.image_folder import make_dataset\nfrom PIL import Image\n\n\nclass AlignedDataset(BaseDataset):\n    def __init__(self, opt):\n        # super(AlignedDataset,self).__init__(opt)\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_AB = os.path.join(opt.dataroot, opt.phase)\n\n        self.AB_paths = sorted(make_dataset(self.dir_AB))\n\n        #assert(opt.resize_or_crop == 'resize_and_crop')\n\n        transform_list = [transforms.ToTensor(),\n                          transforms.Normalize((0.5, 0.5, 0.5),\n                                               (0.5, 0.5, 0.5))]\n\n        self.transform = transforms.Compose(transform_list)\n\n    def __getitem__(self, index):\n        AB_path = self.AB_paths[index]\n        AB = Image.open(AB_path).convert('RGB')\n        AB = AB.resize((self.opt.loadSizeX * 2, self.opt.loadSizeY), Image.BICUBIC)\n        AB = self.transform(AB)\n\n        w_total = AB.size(2)\n        w = int(w_total / 2)\n        h = AB.size(1)\n        w_offset = random.randint(0, max(0, w - self.opt.fineSize - 1))\n        h_offset = random.randint(0, max(0, h - self.opt.fineSize - 1))\n\n        A = AB[:, h_offset:h_offset + self.opt.fineSize,\n               w_offset:w_offset + self.opt.fineSize]\n        B = AB[:, h_offset:h_offset + self.opt.fineSize,\n               w + w_offset:w + w_offset + self.opt.fineSize]\n\n        if (not self.opt.no_flip) and random.random() < 0.5:\n            idx = [i for i in range(A.size(2) - 1, -1, -1)]\n            idx = torch.LongTensor(idx)\n            A = A.index_select(2, idx)\n            B = B.index_select(2, idx)\n\n        return {'A': A, 'B': B,\n                'A_paths': AB_path, 'B_paths': AB_path}\n\n    def __len__(self):\n        return len(self.AB_paths)\n\n    def name(self):\n        return 'AlignedDataset'\n"""
data/base_data_loader.py,0,"b'\nclass BaseDataLoader():\n    def __init__(self):\n        pass\n    \n    def initialize(self, opt):\n        self.opt = opt\n        pass\n\n    def load_data():\n        return None\n\n        \n        \n'"
data/base_dataset.py,1,"b""import torch.utils.data as data\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\nclass BaseDataset(data.Dataset):\n    def __init__(self):\n        super(BaseDataset, self).__init__()\n\n    def name(self):\n        return 'BaseDataset'\n\n    # def initialize(self, opt):\n    #     pass\n\ndef get_transform(opt):\n    transform_list = []\n    if opt.resize_or_crop == 'resize_and_crop':\n        osize = [opt.loadSizeX, opt.loadSizeY]\n        transform_list.append(transforms.Resize(osize, Image.BICUBIC))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == 'crop':\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == 'scale_width':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.fineSize)))\n    elif opt.resize_or_crop == 'scale_width_and_crop':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.loadSizeX)))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n\n    if opt.isTrain and not opt.no_flip:\n        transform_list.append(transforms.RandomHorizontalFlip())\n\n    transform_list += [transforms.ToTensor(),\n                       transforms.Normalize((0.5, 0.5, 0.5),\n                                            (0.5, 0.5, 0.5))]\n    return transforms.Compose(transform_list)\n\ndef __scale_width(img, target_width):\n    ow, oh = img.size\n    if (ow == target_width):\n        return img\n    w = target_width\n    h = int(target_width * oh / ow)\n    return img.resize((w, h), Image.BICUBIC)\n"""
data/custom_dataset_data_loader.py,2,"b'import torch.utils.data\nfrom data.base_data_loader import BaseDataLoader\n\n\ndef CreateDataset(opt):\n    dataset = None\n    if opt.dataset_mode == \'aligned\':\n        from data.aligned_dataset import AlignedDataset\n        dataset = AlignedDataset(opt)\n    elif opt.dataset_mode == \'unaligned\':\n        from data.unaligned_dataset import UnalignedDataset\n        dataset = UnalignedDataset()\n    elif opt.dataset_mode == \'single\':\n        from data.single_dataset import SingleDataset\n        dataset = SingleDataset()\n        dataset.initialize(opt)\n    else:\n        raise ValueError(""Dataset [%s] not recognized."" % opt.dataset_mode)\n\n    print(""dataset [%s] was created"" % (dataset.name()))\n    # dataset.initialize(opt)\n    return dataset\n\n\nclass CustomDatasetDataLoader(BaseDataLoader):\n    def name(self):\n        return \'CustomDatasetDataLoader\'\n\n    def __init__(self, opt):\n        super(CustomDatasetDataLoader,self).initialize(opt)\n        print(""Opt.nThreads = "", opt.nThreads)\n        self.dataset = CreateDataset(opt)\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=opt.batchSize,\n            shuffle=not opt.serial_batches,\n            num_workers=int(opt.nThreads)\n        )\n\n    def load_data(self):\n        return self.dataloader\n\n    def __len__(self):\n        return min(len(self.dataset), self.opt.max_dataset_size)\n'"
data/data_loader.py,0,b'\ndef CreateDataLoader(opt):\n    from data.custom_dataset_data_loader import CustomDatasetDataLoader\n    data_loader = CustomDatasetDataLoader(opt)\n    print(data_loader.name())\n    # data_loader.initialize(opt)\n    return data_loader\n'
data/image_folder.py,1,"b'###############################################################################\n# Code from\n# https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n# Modified the original code so that it also loads images from the current\n# directory as well as the subdirectories\n###############################################################################\n\nimport torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef make_dataset(dir):\n    images = []\n    assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n\n    for root, _, fnames in sorted(os.walk(dir)):\n        for fname in fnames:\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                images.append(path)\n\n    return images\n\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\n\nclass ImageFolder(data.Dataset):\n\n    def __init__(self, root, transform=None, return_paths=False,\n                 loader=default_loader):\n        imgs = make_dataset(root)\n        if len(imgs) == 0:\n            raise(RuntimeError(""Found 0 images in: "" + root + ""\\n""\n                               ""Supported image extensions are: "" +\n                               "","".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n        self.return_paths = return_paths\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.return_paths:\n            return img, path\n        else:\n            return img\n\n    def __len__(self):\n        return len(self.imgs)\n'"
data/single_dataset.py,0,"b""import os.path\nimport torchvision.transforms as transforms\nfrom data.base_dataset import BaseDataset, get_transform\nfrom data.image_folder import make_dataset\nfrom PIL import Image\n\n\nclass SingleDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_A = os.path.join(opt.dataroot)\n\n        self.A_paths = make_dataset(self.dir_A)\n\n        self.A_paths = sorted(self.A_paths)\n\n        self.transform = get_transform(opt)\n\n    def __getitem__(self, index):\n        A_path = self.A_paths[index]\n\n        A_img = Image.open(A_path).convert('RGB')\n\n        A_img = self.transform(A_img)\n\n        return {'A': A_img, 'A_paths': A_path}\n\n    def __len__(self):\n        return len(self.A_paths)\n\n    def name(self):\n        return 'SingleImageDataset'\n"""
data/unaligned_dataset.py,0,"b""import os.path\nimport torchvision.transforms as transforms\nfrom data.base_dataset import BaseDataset, get_transform\nfrom data.image_folder import make_dataset\nfrom PIL import Image\nimport PIL\nfrom pdb import set_trace as st\nimport random\nimport cv2\n\nclass UnalignedDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_A = os.path.join(opt.dataroot, opt.phase + 'A')\n        self.dir_B = os.path.join(opt.dataroot, opt.phase + 'B')\n\n        self.A_paths = make_dataset(self.dir_A)\n        self.B_paths = make_dataset(self.dir_B)\n\n        self.A_paths = sorted(self.A_paths)\n        self.B_paths = sorted(self.B_paths)\n        self.A_size = len(self.A_paths)\n        self.B_size = len(self.B_paths)\n        self.transform = get_transform(opt)\n\n    def __getitem__(self, index):\n        A_path = self.A_paths[index % self.A_size]\n        index_A = index % self.A_size\n        B_path = self.B_paths[index % self.A_size]\n        # print('(A, B) = (%d, %d)' % (index_A, index_B))\n        A_img = Image.open(A_path).convert('L')\n        B_img = Image.open(B_path).convert('RGB')\n\n        A_img = self.transform(A_img)\n        B_img = self.transform(B_img)\n\n        return {'A': A_img, 'B': B_img,\n                'A_paths': A_path, 'B_paths': B_path}\n\n    def __len__(self):\n        return max(self.A_size, self.B_size)\n\n    def name(self):\n        return 'UnalignedDataset'\n"""
datasets/combine_A_and_B.py,0,"b""from pdb import set_trace as st\nimport os\nimport numpy as np\nimport cv2\nimport argparse\n\nparser = argparse.ArgumentParser('create image pairs')\nparser.add_argument('--fold_A', dest='fold_A', help='input directory for image A', type=str, default='../dataset/50kshoes_edges')\nparser.add_argument('--fold_B', dest='fold_B', help='input directory for image B', type=str, default='../dataset/50kshoes_jpg')\nparser.add_argument('--fold_AB', dest='fold_AB', help='output directory', type=str, default='../dataset/test_AB')\nparser.add_argument('--num_imgs', dest='num_imgs', help='number of images',type=int, default=1000000)\nparser.add_argument('--use_AB', dest='use_AB', help='if true: (0001_A, 0001_B) to (0001_AB)',action='store_true')\nargs = parser.parse_args()\n\nfor arg in vars(args):\n    print('[%s] = ' % arg,  getattr(args, arg))\n\nsplits = os.listdir(args.fold_A)\n\nfor sp in splits:\n    img_fold_A = os.path.join(args.fold_A, sp)\n    img_fold_B = os.path.join(args.fold_B, sp)\n    img_list = os.listdir(img_fold_A)\n    if args.use_AB: \n        img_list = [img_path for img_path in img_list if '_A.' in img_path]\n\n    num_imgs = min(args.num_imgs, len(img_list))\n    print('split = %s, use %d/%d images' % (sp, num_imgs, len(img_list)))\n    img_fold_AB = os.path.join(args.fold_AB, sp)\n    if not os.path.isdir(img_fold_AB):\n        os.makedirs(img_fold_AB)\n    print('split = %s, number of images = %d' % (sp, num_imgs))\n    for n in range(num_imgs):\n        name_A = img_list[n]\n        path_A = os.path.join(img_fold_A, name_A)\n        if args.use_AB:\n            name_B = name_A.replace('_A.', '_B.')\n        else:\n            name_B = name_A\n        path_B = os.path.join(img_fold_B, name_B)\n        if os.path.isfile(path_A) and os.path.isfile(path_B):\n            name_AB = name_A\n            if args.use_AB:\n                name_AB = name_AB.replace('_A.', '.') # remove _A\n            path_AB = os.path.join(img_fold_AB, name_AB)\n            im_A = cv2.imread(path_A, cv2.IMREAD_COLOR)\n            im_B = cv2.imread(path_B, cv2.IMREAD_COLOR)\n            im_AB = np.concatenate([im_A, im_B], 1)\n            cv2.imwrite(path_AB, im_AB)\n"""
models/__init__.py,0,b''
models/base_model.py,4,"b""import os\nimport torch\n\n\nclass BaseModel():\n    def name(self):\n        return 'BaseModel'\n\n    def __init__(self, opt):\n        self.opt = opt\n        self.gpu_ids = opt.gpu_ids\n        self.isTrain = opt.isTrain\n        self.Tensor = torch.cuda.FloatTensor if self.gpu_ids else torch.Tensor\n        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n\n    def set_input(self, input):\n        self.input = input\n\n    def forward(self):\n        pass\n\n    # used in test time, no backprop\n    def test(self):\n        pass\n\n    def get_image_paths(self):\n        pass\n\n    def optimize_parameters(self):\n        pass\n\n    def get_current_visuals(self):\n        return self.input\n\n    def get_current_errors(self):\n        return {}\n\n    def save(self, label):\n        pass\n\n    # helper saving function that can be used by subclasses\n    def save_network(self, network, network_label, epoch_label, gpu_ids):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(self.save_dir, save_filename)\n        torch.save(network.cpu().state_dict(), save_path)\n        if len(gpu_ids) and torch.cuda.is_available():\n            network.cuda(device=gpu_ids[0])\n\n\n    # helper loading function that can be used by subclasses\n    def load_network(self, network, network_label, epoch_label):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(self.save_dir, save_filename)\n        network.load_state_dict(torch.load(save_path))\n\n    def update_learning_rate():\n        pass\n"""
models/conditional_gan_model.py,3,"b'import numpy as np\nimport torch\nimport os\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nimport util.util as util\nfrom util.image_pool import ImagePool\nfrom .base_model import BaseModel\nfrom . import networks\nfrom .losses import init_loss\n\ntry:\n\txrange          # Python2\nexcept NameError:\n\txrange = range  # Python 3\n\nclass ConditionalGAN(BaseModel):\n\tdef name(self):\n\t\treturn \'ConditionalGANModel\'\n\n\tdef __init__(self, opt):\n\t\tsuper(ConditionalGAN, self).__init__(opt)\n\t\tself.isTrain = opt.isTrain\n\t\t# define tensors\n\t\tself.input_A = self.Tensor(opt.batchSize, opt.input_nc,  opt.fineSize, opt.fineSize)\n\t\tself.input_B = self.Tensor(opt.batchSize, opt.output_nc, opt.fineSize, opt.fineSize)\n\n\t\t# load/define networks\n\t\t# Temp Fix for nn.parallel as nn.parallel crashes oc calculating gradient penalty\n\t\tuse_parallel = not opt.gan_type == \'wgan-gp\'\n\t\tprint(""Use Parallel = "", ""True"" if use_parallel else ""False"")\n\t\tself.netG = networks.define_G(\n\t\t\topt.input_nc, opt.output_nc, opt.ngf, opt.which_model_netG, opt.norm,\n\t\t\tnot opt.no_dropout, self.gpu_ids, use_parallel, opt.learn_residual\n\t\t)\n\t\tif self.isTrain:\n\t\t\tuse_sigmoid = opt.gan_type == \'gan\'\n\t\t\tself.netD = networks.define_D(\n\t\t\t\topt.output_nc, opt.ndf, opt.which_model_netD,\n\t\t\t\topt.n_layers_D, opt.norm, use_sigmoid, self.gpu_ids, use_parallel\n\t\t\t)\n\t\tif not self.isTrain or opt.continue_train:\n\t\t\tself.load_network(self.netG, \'G\', opt.which_epoch)\n\t\t\tif self.isTrain:\n\t\t\t\tself.load_network(self.netD, \'D\', opt.which_epoch)\n\n\t\tif self.isTrain:\n\t\t\tself.fake_AB_pool = ImagePool(opt.pool_size)\n\t\t\tself.old_lr = opt.lr\n\n\t\t\t# initialize optimizers\n\t\t\tself.optimizer_G = torch.optim.Adam( self.netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999) )\n\t\t\tself.optimizer_D = torch.optim.Adam( self.netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999) )\n\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\tself.criticUpdates = 5 if opt.gan_type == \'wgan-gp\' else 1\n\t\t\t\n\t\t\t# define loss functions\n\t\t\tself.discLoss, self.contentLoss = init_loss(opt, self.Tensor)\n\n\t\tprint(\'---------- Networks initialized -------------\')\n\t\tnetworks.print_network(self.netG)\n\t\tif self.isTrain:\n\t\t\tnetworks.print_network(self.netD)\n\t\tprint(\'-----------------------------------------------\')\n\n\tdef set_input(self, input):\n\t\tAtoB = self.opt.which_direction == \'AtoB\'\n\t\tinputA = input[\'A\' if AtoB else \'B\']\n\t\tinputB = input[\'B\' if AtoB else \'A\']\n\t\tself.input_A.resize_(inputA.size()).copy_(inputA)\n\t\tself.input_B.resize_(inputB.size()).copy_(inputB)\n\t\tself.image_paths = input[\'A_paths\' if AtoB else \'B_paths\']\n\n\tdef forward(self):\n\t\tself.real_A = Variable(self.input_A)\n\t\tself.fake_B = self.netG.forward(self.real_A)\n\t\tself.real_B = Variable(self.input_B)\n\n\t# no backprop gradients\n\tdef test(self):\n\t\tself.real_A = Variable(self.input_A, volatile=True)\n\t\tself.fake_B = self.netG.forward(self.real_A)\n\t\tself.real_B = Variable(self.input_B, volatile=True)\n\n\t# get image paths\n\tdef get_image_paths(self):\n\t\treturn self.image_paths\n\n\tdef backward_D(self):\n\t\tself.loss_D = self.discLoss.get_loss(self.netD, self.real_A, self.fake_B, self.real_B)\n\n\t\tself.loss_D.backward(retain_graph=True)\n\n\tdef backward_G(self):\n\t\tself.loss_G_GAN = self.discLoss.get_g_loss(self.netD, self.real_A, self.fake_B)\n\t\t# Second, G(A) = B\n\t\tself.loss_G_Content = self.contentLoss.get_loss(self.fake_B, self.real_B) * self.opt.lambda_A\n\n\t\tself.loss_G = self.loss_G_GAN + self.loss_G_Content\n\n\t\tself.loss_G.backward()\n\n\tdef optimize_parameters(self):\n\t\tself.forward()\n\n\t\tfor iter_d in xrange(self.criticUpdates):\n\t\t\tself.optimizer_D.zero_grad()\n\t\t\tself.backward_D()\n\t\t\tself.optimizer_D.step()\n\n\t\tself.optimizer_G.zero_grad()\n\t\tself.backward_G()\n\t\tself.optimizer_G.step()\n\n\tdef get_current_errors(self):\n\t\treturn OrderedDict([(\'G_GAN\', self.loss_G_GAN.item()),\n\t\t\t\t\t\t\t(\'G_L1\', self.loss_G_Content.item()),\n\t\t\t\t\t\t\t(\'D_real+fake\', self.loss_D.item())\n\t\t\t\t\t\t\t])\n\n\tdef get_current_visuals(self):\n\t\treal_A = util.tensor2im(self.real_A.data)\n\t\tfake_B = util.tensor2im(self.fake_B.data)\n\t\treal_B = util.tensor2im(self.real_B.data)\n\t\treturn OrderedDict([(\'Blurred_Train\', real_A), (\'Restored_Train\', fake_B), (\'Sharp_Train\', real_B)])\n\n\tdef save(self, label):\n\t\tself.save_network(self.netG, \'G\', label, self.gpu_ids)\n\t\tself.save_network(self.netD, \'D\', label, self.gpu_ids)\n\n\tdef update_learning_rate(self):\n\t\tlrd = self.opt.lr / self.opt.niter_decay\n\t\tlr = self.old_lr - lrd\n\t\tfor param_group in self.optimizer_D.param_groups:\n\t\t\tparam_group[\'lr\'] = lr\n\t\tfor param_group in self.optimizer_G.param_groups:\n\t\t\tparam_group[\'lr\'] = lr\n\t\tprint(\'update learning rate: %f -> %f\' % (self.old_lr, lr))\n\t\tself.old_lr = lr\n'"
models/losses.py,7,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport functools\nimport torch.autograd as autograd\nimport numpy as np\nimport torchvision.models as models\nimport util.util as util\nfrom util.image_pool import ImagePool\nfrom torch.autograd import Variable\n###############################################################################\n# Functions\n###############################################################################\n\nclass ContentLoss:\n\tdef __init__(self, loss):\n\t\tself.criterion = loss\n\t\t\t\n\tdef get_loss(self, fakeIm, realIm):\n\t\treturn self.criterion(fakeIm, realIm)\n\nclass PerceptualLoss():\n\t\n\tdef contentFunc(self):\n\t\tconv_3_3_layer = 14\n\t\tcnn = models.vgg19(pretrained=True).features\n\t\tcnn = cnn.cuda()\n\t\tmodel = nn.Sequential()\n\t\tmodel = model.cuda()\n\t\tfor i,layer in enumerate(list(cnn)):\n\t\t\tmodel.add_module(str(i),layer)\n\t\t\tif i == conv_3_3_layer:\n\t\t\t\tbreak\n\t\treturn model\n\t\t\n\tdef __init__(self, loss):\n\t\tself.criterion = loss\n\t\tself.contentFunc = self.contentFunc()\n\t\t\t\n\tdef get_loss(self, fakeIm, realIm):\n\t\tf_fake = self.contentFunc.forward(fakeIm)\n\t\tf_real = self.contentFunc.forward(realIm)\n\t\tf_real_no_grad = f_real.detach()\n\t\tloss = self.criterion(f_fake, f_real_no_grad)\n\t\treturn loss\n\t\t\nclass GANLoss(nn.Module):\n\tdef __init__(\n\t\t\tself, use_l1=True, target_real_label=1.0,\n\t\t\ttarget_fake_label=0.0, tensor=torch.FloatTensor):\n\t\tsuper(GANLoss, self).__init__()\n\t\tself.real_label = target_real_label\n\t\tself.fake_label = target_fake_label\n\t\tself.real_label_var = None\n\t\tself.fake_label_var = None\n\t\tself.Tensor = tensor\n\t\tif use_l1:\n\t\t\tself.loss = nn.L1Loss()\n\t\telse:\n\t\t\tself.loss = nn.BCELoss()\n\n\tdef get_target_tensor(self, input, target_is_real):\n\t\ttarget_tensor = None\n\t\tif target_is_real:\n\t\t\tcreate_label = ((self.real_label_var is None) or\n\t\t\t\t\t\t\t(self.real_label_var.numel() != input.numel()))\n\t\t\tif create_label:\n\t\t\t\treal_tensor = self.Tensor(input.size()).fill_(self.real_label)\n\t\t\t\tself.real_label_var = Variable(real_tensor, requires_grad=False)\n\t\t\ttarget_tensor = self.real_label_var\n\t\telse:\n\t\t\tcreate_label = ((self.fake_label_var is None) or\n\t\t\t\t\t\t\t(self.fake_label_var.numel() != input.numel()))\n\t\t\tif create_label:\n\t\t\t\tfake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n\t\t\t\tself.fake_label_var = Variable(fake_tensor, requires_grad=False)\n\t\t\ttarget_tensor = self.fake_label_var\n\t\treturn target_tensor\n\n\tdef __call__(self, input, target_is_real):\n\t\ttarget_tensor = self.get_target_tensor(input, target_is_real)\n\t\treturn self.loss(input, target_tensor)\n\nclass DiscLoss:\n\tdef name(self):\n\t\treturn \'DiscLoss\'\n\n\tdef __init__(self, opt, tensor):\n\t\tself.criterionGAN = GANLoss(use_l1=False, tensor=tensor)\n\t\tself.fake_AB_pool = ImagePool(opt.pool_size)\n\t\t\n\tdef get_g_loss(self,net, realA, fakeB):\n\t\t# First, G(A) should fake the discriminator\n\t\tpred_fake = net.forward(fakeB)\n\t\treturn self.criterionGAN(pred_fake, 1)\n\t\t\n\tdef get_loss(self, net, realA, fakeB, realB):\n\t\t# Fake\n\t\t# stop backprop to the generator by detaching fake_B\n\t\t# Generated Image Disc Output should be close to zero\n\t\tself.pred_fake = net.forward(fakeB.detach())\n\t\tself.loss_D_fake = self.criterionGAN(self.pred_fake, 0)\n\n\t\t# Real\n\t\tself.pred_real = net.forward(realB)\n\t\tself.loss_D_real = self.criterionGAN(self.pred_real, 1)\n\n\t\t# Combined loss\n\t\tself.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n\t\treturn self.loss_D\n\t\t\nclass DiscLossLS(DiscLoss):\n\tdef name(self):\n\t\treturn \'DiscLossLS\'\n\n\tdef __init__(self, opt, tensor):\n\t\tsuper(DiscLoss, self).__init__(opt, tensor)\n\t\t# DiscLoss.initialize(self, opt, tensor)\n\t\tself.criterionGAN = GANLoss(use_l1=True, tensor=tensor)\n\t\t\n\tdef get_g_loss(self,net, realA, fakeB):\n\t\treturn DiscLoss.get_g_loss(self,net, realA, fakeB)\n\t\t\n\tdef get_loss(self, net, realA, fakeB, realB):\n\t\treturn DiscLoss.get_loss(self, net, realA, fakeB, realB)\n\t\t\nclass DiscLossWGANGP(DiscLossLS):\n\tdef name(self):\n\t\treturn \'DiscLossWGAN-GP\'\n\n\tdef __init__(self, opt, tensor):\n\t\tsuper(DiscLossWGANGP, self).__init__(opt, tensor)\n\t\t# DiscLossLS.initialize(self, opt, tensor)\n\t\tself.LAMBDA = 10\n\t\t\n\tdef get_g_loss(self, net, realA, fakeB):\n\t\t# First, G(A) should fake the discriminator\n\t\tself.D_fake = net.forward(fakeB)\n\t\treturn -self.D_fake.mean()\n\t\t\n\tdef calc_gradient_penalty(self, netD, real_data, fake_data):\n\t\talpha = torch.rand(1, 1)\n\t\talpha = alpha.expand(real_data.size())\n\t\talpha = alpha.cuda()\n\n\t\tinterpolates = alpha * real_data + ((1 - alpha) * fake_data)\n\n\t\tinterpolates = interpolates.cuda()\n\t\tinterpolates = Variable(interpolates, requires_grad=True)\n\t\t\n\t\tdisc_interpolates = netD.forward(interpolates)\n\n\t\tgradients = autograd.grad(\n\t\t\toutputs=disc_interpolates, inputs=interpolates, grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n\t\t\tcreate_graph=True, retain_graph=True, only_inputs=True\n\t\t)[0]\n\n\t\tgradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * self.LAMBDA\n\t\treturn gradient_penalty\n\t\t\n\tdef get_loss(self, net, realA, fakeB, realB):\n\t\tself.D_fake = net.forward(fakeB.detach())\n\t\tself.D_fake = self.D_fake.mean()\n\t\t\n\t\t# Real\n\t\tself.D_real = net.forward(realB)\n\t\tself.D_real = self.D_real.mean()\n\t\t# Combined loss\n\t\tself.loss_D = self.D_fake - self.D_real\n\t\tgradient_penalty = self.calc_gradient_penalty(net, realB.data, fakeB.data)\n\t\treturn self.loss_D + gradient_penalty\n\n\ndef init_loss(opt, tensor):\n\t# disc_loss = None\n\t# content_loss = None\n\t\n\tif opt.model == \'content_gan\':\n\t\tcontent_loss = PerceptualLoss(nn.MSELoss())\n\t\t# content_loss.initialize(nn.MSELoss())\n\telif opt.model == \'pix2pix\':\n\t\tcontent_loss = ContentLoss(nn.L1Loss())\n\t\t# content_loss.initialize(nn.L1Loss())\n\telse:\n\t\traise ValueError(""Model [%s] not recognized."" % opt.model)\n\t\n\tif opt.gan_type == \'wgan-gp\':\n\t\tdisc_loss = DiscLossWGANGP(opt, tensor)\n\telif opt.gan_type == \'lsgan\':\n\t\tdisc_loss = DiscLossLS(opt, tensor)\n\telif opt.gan_type == \'gan\':\n\t\tdisc_loss = DiscLoss(opt, tensor)\n\telse:\n\t\traise ValueError(""GAN [%s] not recognized."" % opt.gan_type)\n\t# disc_loss.initialize(opt, tensor)\n\treturn disc_loss, content_loss'"
models/models.py,0,"b'from .conditional_gan_model import ConditionalGAN\n\ndef create_model(opt):\n\tmodel = None\n\tif opt.model == \'test\':\n\t\tassert (opt.dataset_mode == \'single\')\n\t\tfrom .test_model import TestModel\n\t\tmodel = TestModel( opt )\n\telse:\n\t\tmodel = ConditionalGAN(opt)\n\t# model.initialize(opt)\n\tprint(""model [%s] was created"" % (model.name()))\n\treturn model\n'"
models/networks.py,11,"b""import torch\nimport torch.nn as nn\n# from torch.nn import init\nimport functools\n# from torch.autograd import Variable\nimport numpy as np\n\n\n###############################################################################\n# Functions\n###############################################################################\n\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n        if hasattr(m.bias, 'data'):\n            m.bias.data.fill_(0)\n    elif classname.find('BatchNorm2d') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n\n\ndef get_norm_layer(norm_type='instance'):\n    if norm_type == 'batch':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n    elif norm_type == 'instance':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True)\n    else:\n        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n    return norm_layer\n\n\ndef define_G(input_nc, output_nc, ngf, which_model_netG, norm='batch', use_dropout=False, gpu_ids=[], use_parallel=True,\n             learn_residual=False):\n    netG = None\n    use_gpu = len(gpu_ids) > 0\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if use_gpu:\n        assert (torch.cuda.is_available())\n\n    if which_model_netG == 'resnet_9blocks':\n        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9,\n                               gpu_ids=gpu_ids, use_parallel=use_parallel, learn_residual=learn_residual)\n    elif which_model_netG == 'resnet_6blocks':\n        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6,\n                               gpu_ids=gpu_ids, use_parallel=use_parallel, learn_residual=learn_residual)\n    elif which_model_netG == 'unet_128':\n        netG = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout,\n                             gpu_ids=gpu_ids, use_parallel=use_parallel, learn_residual=learn_residual)\n    elif which_model_netG == 'unet_256':\n        netG = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout,\n                             gpu_ids=gpu_ids, use_parallel=use_parallel, learn_residual=learn_residual)\n    else:\n        raise NotImplementedError('Generator model name [%s] is not recognized' % which_model_netG)\n    if len(gpu_ids) > 0:\n        netG.cuda(gpu_ids[0])\n    netG.apply(weights_init)\n    return netG\n\n\ndef define_D(input_nc, ndf, which_model_netD, n_layers_D=3, norm='batch', use_sigmoid=False, gpu_ids=[],\n             use_parallel=True):\n    netD = None\n    use_gpu = len(gpu_ids) > 0\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if use_gpu:\n        assert (torch.cuda.is_available())\n    if which_model_netD == 'basic':\n        netD = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer, use_sigmoid=use_sigmoid,\n                                   gpu_ids=gpu_ids, use_parallel=use_parallel)\n    elif which_model_netD == 'n_layers':\n        netD = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer, use_sigmoid=use_sigmoid,\n                                   gpu_ids=gpu_ids, use_parallel=use_parallel)\n    else:\n        raise NotImplementedError('Discriminator model name [%s] is not recognized' % which_model_netD)\n    if use_gpu:\n        netD.cuda(gpu_ids[0])\n    netD.apply(weights_init)\n    return netD\n\n\ndef print_network(net):\n    num_params = 0\n    for param in net.parameters():\n        num_params += param.numel()\n    print(net)\n    print('Total number of parameters: %d' % num_params)\n\n\n##############################################################################\n# Classes\n##############################################################################\n\n\n# Defines the generator that consists of Resnet blocks between a few\n# downsampling/upsampling operations.\n# Code and idea originally from Justin Johnson's architecture.\n# https://github.com/jcjohnson/fast-neural-style/\nclass ResnetGenerator(nn.Module):\n    def __init__(\n            self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False,\n            n_blocks=6, gpu_ids=[], use_parallel=True, learn_residual=False, padding_type='reflect'):\n        assert (n_blocks >= 0)\n        super(ResnetGenerator, self).__init__()\n        self.input_nc = input_nc\n        self.output_nc = output_nc\n        self.ngf = ngf\n        self.gpu_ids = gpu_ids\n        self.use_parallel = use_parallel\n        self.learn_residual = learn_residual\n\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n            norm_layer(ngf),\n            nn.ReLU(True)\n        ]\n\n        n_downsampling = 2\n\n        # \xe4\xb8\x8b\xe9\x87\x87\xe6\xa0\xb7\n        # for i in range(n_downsampling): # [0,1]\n        # \tmult = 2**i\n        #\n        # \tmodel += [\n        # \t\tnn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n        # \t\tnorm_layer(ngf * mult * 2),\n        # \t\tnn.ReLU(True)\n        # \t]\n\n        model += [\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=use_bias),\n            norm_layer(128),\n            nn.ReLU(True),\n\n            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=use_bias),\n            norm_layer(256),\n            nn.ReLU(True)\n        ]\n\n        # \xe4\xb8\xad\xe9\x97\xb4\xe7\x9a\x84\xe6\xae\x8b\xe5\xb7\xae\xe7\xbd\x91\xe7\xbb\x9c\n        # mult = 2**n_downsampling\n        for i in range(n_blocks):\n            # model += [\n            # \tResnetBlock(\n            # \t\tngf * mult, padding_type=padding_type, norm_layer=norm_layer,\n            # \t\tuse_dropout=use_dropout, use_bias=use_bias)\n            # ]\n            model += [\n                ResnetBlock(256, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)\n            ]\n\n        # \xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\n        # for i in range(n_downsampling):\n        # \tmult = 2**(n_downsampling - i)\n        #\n        # \tmodel += [\n        # \t\tnn.ConvTranspose2d(\n        # \t\t\tngf * mult, int(ngf * mult / 2), kernel_size=3, stride=2,\n        # \t\t\tpadding=1, output_padding=1, bias=use_bias),\n        # \t\tnorm_layer(int(ngf * mult / 2)),\n        # \t\tnn.ReLU(True)\n        # \t]\n        model += [\n            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1, bias=use_bias),\n            norm_layer(128),\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1, bias=use_bias),\n            norm_layer(64),\n            nn.ReLU(True),\n        ]\n\n        model += [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(64, output_nc, kernel_size=7, padding=0),\n            nn.Tanh()\n        ]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        if self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor) and self.use_parallel:\n            output = nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n        else:\n            output = self.model(input)\n        if self.learn_residual:\n            # output = input + output\n            output = torch.clamp(input + output, min=-1, max=1)\n        return output\n\n\n# Define a resnet block\nclass ResnetBlock(nn.Module):\n\n\tdef __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n\t\tsuper(ResnetBlock, self).__init__()\n\n\t\tpadAndConv = {\n\t\t\t'reflect': [\n                nn.ReflectionPad2d(1),\n                nn.Conv2d(dim, dim, kernel_size=3, bias=use_bias)],\n\t\t\t'replicate': [\n                nn.ReplicationPad2d(1),\n                nn.Conv2d(dim, dim, kernel_size=3, bias=use_bias)],\n\t\t\t'zero': [\n                nn.Conv2d(dim, dim, kernel_size=3, padding=1, bias=use_bias)]\n\t\t}\n\n\t\ttry:\n\t\t\tblocks = padAndConv[padding_type] + [\n\t\t\t\tnorm_layer(dim),\n\t\t\t\tnn.ReLU(True)\n            ] + [\n\t\t\t\tnn.Dropout(0.5)\n\t\t\t] if use_dropout else [] + padAndConv[padding_type] + [\n\t\t\t\tnorm_layer(dim)\n\t\t\t]\n\t\texcept:\n\t\t\traise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n\t\tself.conv_block = nn.Sequential(*blocks)\n\n\t\t# self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n\t\t# def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n\t\t#     padAndConv = {\n\t\t#         'reflect': [nn.ReflectionPad2d(1), nn.Conv2d(dim, dim, kernel_size=3, bias=use_bias)],\n\t\t#         'replicate': [nn.ReplicationPad2d(1), nn.Conv2d(dim, dim, kernel_size=3, bias=use_bias)],\n\t\t#         'zero': [nn.Conv2d(dim, dim, kernel_size=3, padding=1, bias=use_bias)]\n\t\t#     }\n\t\t#     try:\n\t\t#         blocks = [\n\t\t#             padAndConv[padding_type],\n\t\t#\n\t\t#             norm_layer(dim),\n\t\t#             nn.ReLU(True),\n\t\t#             nn.Dropout(0.5) if use_dropout else None,\n\t\t#\n\t\t#             padAndConv[padding_type],\n\t\t#\n\t\t#             norm_layer(dim)\n\t\t#         ]\n\t\t#     except:\n\t\t#         raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\t\t#\n\t\t#     return nn.Sequential(*blocks)\n\n\t\t# blocks = []\n\t\t# if padding_type == 'reflect':\n\t\t# \tblocks += [nn.ReflectionPad2d(1),  nn.Conv2d(dim, dim, kernel_size=3, bias=use_bias)]\n\t\t# elif padding_type == 'replicate':\n\t\t# \tblocks += [nn.ReplicationPad2d(1), nn.Conv2d(dim, dim, kernel_size=3, bias=use_bias)]\n\t\t# elif padding_type == 'zero':\n\t\t# \tblocks += [nn.Conv2d(dim, dim, kernel_size=3, padding=1, bias=use_bias)]\n\t\t# else:\n\t\t# \traise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\t\t#\n\t\t# blocks += [\n\t\t# \tnorm_layer(dim),\n\t\t# \tnn.ReLU(True),\n\t\t# \tnn.Dropout(0.5) if use_dropout else None\n\t\t# ]\n\t\t#\n\t\t# if padding_type == 'reflect':\n\t\t# \tblocks += [nn.ReflectionPad2d(1),  nn.Conv2d(dim, dim, kernel_size=3, bias=use_bias)]\n\t\t# elif padding_type == 'replicate':\n\t\t# \tblocks += [nn.ReplicationPad2d(1), nn.Conv2d(dim, dim, kernel_size=3, bias=use_bias)]\n\t\t# elif padding_type == 'zero':\n\t\t# \tblocks += [nn.Conv2d(dim, dim, kernel_size=3, padding=1, bias=use_bias)]\n\t\t# else:\n\t\t# \traise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\t\t#\n\t\t# blocks += [\n\t\t# \tnorm_layer(dim)\n\t\t# ]\n\t\t#\n\t\t# return nn.Sequential(*blocks)\n\n\tdef forward(self, x):\n\t\tout = x + self.conv_block(x)\n\t\treturn out\n\n\n# Defines the Unet generator.\n# |num_downs|: number of downsamplings in UNet. For example,\n# if |num_downs| == 7, image of size 128x128 will become of size 1x1\n# at the bottleneck\nclass UnetGenerator(nn.Module):\n    def __init__(\n            self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d,\n            use_dropout=False, gpu_ids=[], use_parallel=True, learn_residual=False):\n        super(UnetGenerator, self).__init__()\n        self.gpu_ids = gpu_ids\n        self.use_parallel = use_parallel\n        self.learn_residual = learn_residual\n        # currently support only input_nc == output_nc\n        assert (input_nc == output_nc)\n\n        # construct unet structure\n        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, norm_layer=norm_layer, innermost=True)\n        for i in range(num_downs - 5):\n            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, unet_block, norm_layer=norm_layer,\n                                                 use_dropout=use_dropout)\n        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(output_nc, ngf, unet_block, outermost=True, norm_layer=norm_layer)\n\n        self.model = unet_block\n\n    def forward(self, input):\n        if self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor) and self.use_parallel:\n            output = nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n        else:\n            output = self.model(input)\n        if self.learn_residual:\n            output = input + output\n            output = torch.clamp(output, min=-1, max=1)\n        return output\n\n\n# Defines the submodule with skip connection.\n# X -------------------identity---------------------- X\n#   |-- downsampling -- |submodule| -- upsampling --|\nclass UnetSkipConnectionBlock(nn.Module):\n    def __init__(\n            self, outer_nc, inner_nc, submodule=None,\n            outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(UnetSkipConnectionBlock, self).__init__()\n        self.outermost = outermost\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        dConv = nn.Conv2d(outer_nc, inner_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n        dRelu = nn.LeakyReLU(0.2, True)\n        dNorm = norm_layer(inner_nc)\n        uRelu = nn.ReLU(True)\n        uNorm = norm_layer(outer_nc)\n\n        if outermost:\n            uConv = nn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1)\n            dModel = [dConv]\n            uModel = [uRelu, uConv, nn.Tanh()]\n            model = [\n                dModel,\n                submodule,\n                uModel\n            ]\n        # model = [\n        # \t# Down\n        # \tnn.Conv2d( outer_nc, inner_nc, kernel_size=4, stride=2, padding=1, bias=use_bias),\n        #\n        # \tsubmodule,\n        # \t# Up\n        # \tnn.ReLU(True),\n        # \tnn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1),\n        # \tnn.Tanh()\n        # ]\n        elif innermost:\n            uConv = nn.ConvTranspose2d(inner_nc, outer_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n            dModel = [dRelu, dConv]\n            uModel = [uRelu, uConv, uNorm]\n            model = [\n                dModel,\n                uModel\n            ]\n        # model = [\n        # \t# down\n        # \tnn.LeakyReLU(0.2, True),\n        # \t# up\n        # \tnn.ReLU(True),\n        # \tnn.ConvTranspose2d(inner_nc, outer_nc, kernel_size=4, stride=2, padding=1, bias=use_bias),\n        # \tnorm_layer(outer_nc)\n        # ]\n        else:\n            uConv = nn.ConvTranspose2d(inner_nc * 2, outer_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n            dModel = [dRelu, dConv, dNorm]\n            uModel = [uRelu, uConv, uNorm]\n\n            model = [\n                dModel,\n                submodule,\n                uModel\n            ]\n            model += [nn.Dropout(0.5)] if use_dropout else []\n\n        # if use_dropout:\n        # \tmodel = down + [submodule] + up + [nn.Dropout(0.5)]\n        # else:\n        # \tmodel = down + [submodule] + up\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        if self.outermost:\n            return self.model(x)\n        else:\n            return torch.cat([self.model(x), x], 1)\n\n\n# Defines the PatchGAN discriminator with the specified arguments.\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, gpu_ids=[],\n                 use_parallel=True):\n        super(NLayerDiscriminator, self).__init__()\n        self.gpu_ids = gpu_ids\n        self.use_parallel = use_parallel\n\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        kw = 4\n        padw = int(np.ceil((kw - 1) / 2))\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2 ** n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2 ** n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n\n        if use_sigmoid:\n            sequence += [nn.Sigmoid()]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        if len(self.gpu_ids) and isinstance(input.data, torch.cuda.FloatTensor) and self.use_parallel:\n            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n        else:\n            return self.model(input)\n"""
models/test_model.py,2,"b""import torch\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nimport util.util as util\nfrom .base_model import BaseModel\nfrom . import networks\n\n\nclass TestModel(BaseModel):\n    def name(self):\n        return 'TestModel'\n\n    def __init__(self, opt):\n        assert(not opt.isTrain)\n        super(TestModel, self).__init__(opt)\n        self.input_A = self.Tensor(opt.batchSize, opt.input_nc, opt.fineSize, opt.fineSize)\n\n        self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf,\n                                      opt.which_model_netG, opt.norm, not opt.no_dropout, self.gpu_ids, False,\n                                      opt.learn_residual)\n        which_epoch = opt.which_epoch\n        self.load_network(self.netG, 'G', which_epoch)\n\n        print('---------- Networks initialized -------------')\n        networks.print_network(self.netG)\n        print('-----------------------------------------------')\n\n    def set_input(self, input):\n        # we need to use single_dataset mode\n        input_A = input['A']\n        temp = self.input_A.clone()\n        temp.resize_(input_A.size()).copy_(input_A)\n        self.input_A = temp\n        self.image_paths = input['A_paths']\n\n    def test(self):\n        with torch.no_grad():\n            self.real_A = Variable(self.input_A)\n            self.fake_B = self.netG.forward(self.real_A)\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n    def get_current_visuals(self):\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n        return OrderedDict([('real_A', real_A), ('fake_B', fake_B)])\n"""
motion_blur/__init__.py,0,b''
motion_blur/blur_image.py,0,"b'import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nfrom scipy import signal\nfrom scipy import misc\nfrom motion_blur.generate_PSF import PSF\nfrom motion_blur.generate_trajectory import Trajectory\n\n\nclass BlurImage(object):\n\n    def __init__(self, image_path, PSFs=None, part=None, path__to_save=None):\n        """"""\n\n        :param image_path: path to square, RGB image.\n        :param PSFs: array of Kernels.\n        :param part: int number of kernel to use.\n        :param path__to_save: folder to save results.\n        """"""\n        if os.path.isfile(image_path):\n            self.image_path = image_path\n            self.original = misc.imread(self.image_path)\n            self.shape = self.original.shape\n            if len(self.shape) < 3:\n                raise Exception(\'We support only RGB images yet.\')\n            elif self.shape[0] != self.shape[1]:\n                raise Exception(\'We support only square images yet.\')\n        else:\n            raise Exception(\'Not correct path to image.\')\n        self.path_to_save = path__to_save\n        if PSFs is None:\n            if self.path_to_save is None:\n                self.PSFs = PSF(canvas=self.shape[0]).fit()\n            else:\n                self.PSFs = PSF(canvas=self.shape[0], path_to_save=os.path.join(self.path_to_save,\n                                                                                \'PSFs.png\')).fit(save=True)\n        else:\n            self.PSFs = PSFs\n\n        self.part = part\n        self.result = []\n\n    def blur_image(self, save=False, show=False):\n        if self.part is None:\n            psf = self.PSFs\n        else:\n            psf = [self.PSFs[self.part]]\n        yN, xN, channel = self.shape\n        key, kex = self.PSFs[0].shape\n        delta = yN - key\n        assert delta >= 0, \'resolution of image should be higher than kernel\'\n        result=[]\n        if len(psf) > 1:\n            for p in psf:\n                tmp = np.pad(p, delta // 2, \'constant\')\n                cv2.normalize(tmp, tmp, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n                # blured = np.zeros(self.shape)\n                blured = cv2.normalize(self.original, self.original, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX,\n                                       dtype=cv2.CV_32F)\n                blured[:, :, 0] = np.array(signal.fftconvolve(blured[:, :, 0], tmp, \'same\'))\n                blured[:, :, 1] = np.array(signal.fftconvolve(blured[:, :, 1], tmp, \'same\'))\n                blured[:, :, 2] = np.array(signal.fftconvolve(blured[:, :, 2], tmp, \'same\'))\n                blured = cv2.normalize(blured, blured, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n                blured = cv2.cvtColor(blured, cv2.COLOR_RGB2BGR)\n                result.append(np.abs(blured))\n        else:\n            psf = psf[0]\n            tmp = np.pad(psf, delta // 2, \'constant\')\n            cv2.normalize(tmp, tmp, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n            blured = cv2.normalize(self.original, self.original, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX,\n                                   dtype=cv2.CV_32F)\n            blured[:, :, 0] = np.array(signal.fftconvolve(blured[:, :, 0], tmp, \'same\'))\n            blured[:, :, 1] = np.array(signal.fftconvolve(blured[:, :, 1], tmp, \'same\'))\n            blured[:, :, 2] = np.array(signal.fftconvolve(blured[:, :, 2], tmp, \'same\'))\n            blured = cv2.normalize(blured, blured, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n            blured = cv2.cvtColor(blured, cv2.COLOR_RGB2BGR)\n            result.append(np.abs(blured))\n        self.result = result\n        if show or save:\n            self.__plot_canvas(show, save)\n\n    def __plot_canvas(self, show, save):\n        if len(self.result) == 0:\n            raise Exception(\'Please run blur_image() method first.\')\n        else:\n            plt.close()\n            plt.axis(\'off\')\n            fig, axes = plt.subplots(1, len(self.result), figsize=(10, 10))\n            if len(self.result) > 1:\n                for i in range(len(self.result)):\n                        axes[i].imshow(self.result[i])\n            else:\n                plt.axis(\'off\')\n\n                plt.imshow(self.result[0])\n            if show and save:\n                if self.path_to_save is None:\n                    raise Exception(\'Please create Trajectory instance with path_to_save\')\n                cv2.imwrite(os.path.join(self.path_to_save, self.image_path.split(\'/\')[-1]), self.result[0] * 255)\n                plt.show()\n            elif save:\n                if self.path_to_save is None:\n                    raise Exception(\'Please create Trajectory instance with path_to_save\')\n                cv2.imwrite(os.path.join(self.path_to_save, self.image_path.split(\'/\')[-1]), self.result[0] * 255)\n            elif show:\n                plt.show()\n\n\nif __name__ == \'__main__\':\n    folder = \'/Users/mykolam/PycharmProjects/University/DeblurGAN2/results_sharp\'\n    folder_to_save = \'/Users/mykolam/PycharmProjects/University/DeblurGAN2/blured\'\n    params = [0.01, 0.009, 0.008, 0.007, 0.005, 0.003]\n    for path in os.listdir(folder):\n        print(path)\n        trajectory = Trajectory(canvas=64, max_len=60, expl=np.random.choice(params)).fit()\n        psf = PSF(canvas=64, trajectory=trajectory).fit()\n        BlurImage(os.path.join(folder, path), PSFs=psf,\n                  path__to_save=folder_to_save, part=np.random.choice([1, 2, 3])).\\\n            blur_image(save=True)\n'"
motion_blur/generate_PSF.py,0,"b'import numpy as np\nfrom math import ceil\nimport matplotlib.pyplot as plt\nfrom motion_blur.generate_trajectory import Trajectory\n\n\nclass PSF(object):\n    def __init__(self, canvas=None, trajectory=None, fraction=None, path_to_save=None):\n        if canvas is None:\n            self.canvas = (canvas, canvas)\n        else:\n            self.canvas = (canvas, canvas)\n        if trajectory is None:\n            self.trajectory = Trajectory(canvas=canvas, expl=0.005).fit(show=False, save=False)\n        else:\n            self.trajectory = trajectory.x\n        if fraction is None:\n            self.fraction = [1/100, 1/10, 1/2, 1]\n        else:\n            self.fraction = fraction\n        self.path_to_save = path_to_save\n        self.PSFnumber = len(self.fraction)\n        self.iters = len(self.trajectory)\n        self.PSFs = []\n\n    def fit(self, show=False, save=False):\n        PSF = np.zeros(self.canvas)\n\n        triangle_fun = lambda x: np.maximum(0, (1 - np.abs(x)))\n        triangle_fun_prod = lambda x, y: np.multiply(triangle_fun(x), triangle_fun(y))\n        for j in range(self.PSFnumber):\n            if j == 0:\n                prevT = 0\n            else:\n                prevT = self.fraction[j - 1]\n\n            for t in range(len(self.trajectory)):\n                # print(j, t)\n                if (self.fraction[j] * self.iters >= t) and (prevT * self.iters < t - 1):\n                    t_proportion = 1\n                elif (self.fraction[j] * self.iters >= t - 1) and (prevT * self.iters < t - 1):\n                    t_proportion = self.fraction[j] * self.iters - (t - 1)\n                elif (self.fraction[j] * self.iters >= t) and (prevT * self.iters < t):\n                    t_proportion = t - (prevT * self.iters)\n                elif (self.fraction[j] * self.iters >= t - 1) and (prevT * self.iters < t):\n                    t_proportion = (self.fraction[j] - prevT) * self.iters\n                else:\n                    t_proportion = 0\n\n                m2 = int(np.minimum(self.canvas[1] - 1, np.maximum(1, np.math.floor(self.trajectory[t].real))))\n                M2 = int(m2 + 1)\n                m1 = int(np.minimum(self.canvas[0] - 1, np.maximum(1, np.math.floor(self.trajectory[t].imag))))\n                M1 = int(m1 + 1)\n\n                PSF[m1, m2] += t_proportion * triangle_fun_prod(\n                    self.trajectory[t].real - m2, self.trajectory[t].imag - m1\n                )\n                PSF[m1, M2] += t_proportion * triangle_fun_prod(\n                    self.trajectory[t].real - M2, self.trajectory[t].imag - m1\n                )\n                PSF[M1, m2] += t_proportion * triangle_fun_prod(\n                    self.trajectory[t].real - m2, self.trajectory[t].imag - M1\n                )\n                PSF[M1, M2] += t_proportion * triangle_fun_prod(\n                    self.trajectory[t].real - M2, self.trajectory[t].imag - M1\n                )\n\n            self.PSFs.append(PSF / (self.iters))\n        if show or save:\n            self.__plot_canvas(show, save)\n\n        return self.PSFs\n\n    def __plot_canvas(self, show, save):\n        if len(self.PSFs) == 0:\n            raise Exception(""Please run fit() method first."")\n        else:\n            plt.close()\n            fig, axes = plt.subplots(1, self.PSFnumber, figsize=(10, 10))\n            for i in range(self.PSFnumber):\n                axes[i].imshow(self.PSFs[i], cmap=\'gray\')\n            if show and save:\n                if self.path_to_save is None:\n                    raise Exception(\'Please create Trajectory instance with path_to_save\')\n                plt.savefig(self.path_to_save)\n                plt.show()\n            elif save:\n                if self.path_to_save is None:\n                    raise Exception(\'Please create Trajectory instance with path_to_save\')\n                plt.savefig(self.path_to_save)\n            elif show:\n                plt.show()\n\n\nif __name__ == \'__main__\':\n    psf = PSF(canvas=128, path_to_save=\'/Users/mykolam/PycharmProjects/University/RandomMotionBlur/psf.png\')\n    psf.fit(show=True, save=True)'"
motion_blur/generate_trajectory.py,0,"b'import numpy as np\nimport matplotlib.pyplot as plt\nfrom math import ceil\n\n\nclass Trajectory(object):\n    def __init__(self, canvas=64, iters=2000, max_len=60, expl=None, path_to_save=None):\n        """"""\n        Generates a variety of random motion trajectories in continuous domain as in [Boracchi and Foi 2012]. Each\n        trajectory consists of a complex-valued vector determining the discrete positions of a particle following a\n        2-D random motion in continuous domain. The particle has an initial velocity vector which, at each iteration,\n        is affected by a Gaussian perturbation and by a deterministic inertial component, directed toward the\n        previous particle position. In addition, with a small probability, an impulsive (abrupt) perturbation aiming\n        at inverting the particle velocity may arises, mimicking a sudden movement that occurs when the user presses\n        the camera button or tries to compensate the camera shake. At each step, the velocity is normalized to\n        guarantee that trajectories corresponding to equal exposures have the same length. Each perturbation (\n        Gaussian, inertial, and impulsive) is ruled by its own parameter. Rectilinear Blur as in [Boracchi and Foi\n        2011] can be obtained by setting anxiety to 0 (when no impulsive changes occurs\n        :param canvas: size of domain where our trajectory os defined.\n        :param iters: number of iterations for definition of our trajectory.\n        :param max_len: maximum length of our trajectory.\n        :param expl: this param helps to define probability of big shake. Recommended expl = 0.005.\n        :param path_to_save: where to save if you need.\n        """"""\n        self.canvas = canvas\n        self.iters = iters\n        self.max_len = max_len\n        if expl is None:\n            self.expl = 0.1 * np.random.uniform(0, 1)\n        else:\n            self.expl = expl\n        if path_to_save is None:\n            pass\n        else:\n            self.path_to_save = path_to_save\n        self.tot_length = None\n        self.big_expl_count = None\n        self.x = None\n\n    def fit(self, show=False, save=False):\n        """"""\n        Generate motion, you can save or plot, coordinates of motion you can find in x property.\n        Also you can fin properties tot_length, big_expl_count.\n        :param show: default False.\n        :param save: default False.\n        :return: x (vector of motion).\n        """"""\n        tot_length = 0\n        big_expl_count = 0\n        # how to be near the previous position\n        # TODO: I can change this paramether for 0.1 and make kernel at all image\n        centripetal = 0.7 * np.random.uniform(0, 1)\n        # probability of big shake\n        prob_big_shake = 0.2 * np.random.uniform(0, 1)\n        # term determining, at each sample, the random component of the new direction\n        gaussian_shake = 10 * np.random.uniform(0, 1)\n        init_angle = 360 * np.random.uniform(0, 1)\n\n        img_v0 = np.sin(np.deg2rad(init_angle))\n        real_v0 = np.cos(np.deg2rad(init_angle))\n\n        v0 = complex(real=real_v0, imag=img_v0)\n        v = v0 * self.max_len / (self.iters - 1)\n\n        if self.expl > 0:\n            v = v0 * self.expl\n\n        x = np.array([complex(real=0, imag=0)] * (self.iters))\n\n        for t in range(0, self.iters - 1):\n            if np.random.uniform() < prob_big_shake * self.expl:\n                next_direction = 2 * v * (np.exp(complex(real=0, imag=np.pi + (np.random.uniform() - 0.5))))\n                big_expl_count += 1\n            else:\n                next_direction = 0\n\n            dv = next_direction + self.expl * (\n                gaussian_shake * complex(real=np.random.randn(), imag=np.random.randn()) - centripetal * x[t]) * (\n                                      self.max_len / (self.iters - 1))\n\n            v += dv\n            v = (v / float(np.abs(v))) * (self.max_len / float((self.iters - 1)))\n            x[t + 1] = x[t] + v\n            tot_length = tot_length + abs(x[t + 1] - x[t])\n\n        # centere the motion\n        x += complex(real=-np.min(x.real), imag=-np.min(x.imag))\n        x = x - complex(real=x[0].real % 1., imag=x[0].imag % 1.) + complex(1, 1)\n        x += complex(real=ceil((self.canvas - max(x.real)) / 2), imag=ceil((self.canvas - max(x.imag)) / 2))\n\n        self.tot_length = tot_length\n        self.big_expl_count = big_expl_count\n        self.x = x\n\n        if show or save:\n            self.__plot_canvas(show, save)\n        return self\n\n    def __plot_canvas(self, show, save):\n        if self.x is None:\n            raise Exception(""Please run fit() method first"")\n        else:\n            plt.close()\n            plt.plot(self.x.real, self.x.imag, \'-\', color=\'blue\')\n\n            plt.xlim((0, self.canvas))\n            plt.ylim((0, self.canvas))\n            if show and save:\n                plt.savefig(self.path_to_save)\n                plt.show()\n            elif save:\n                if self.path_to_save is None:\n                    raise Exception(\'Please create Trajectory instance with path_to_save\')\n                plt.savefig(self.path_to_save)\n            elif show:\n                plt.show()\n\n\nif __name__ == \'__main__\':\n    trajectory = Trajectory(expl=0.005,\n                            path_to_save=\'/Users/mykolam/PycharmProjects/University/RandomMotionBlur/main.png\')\n    trajectory.fit(True, False)\n'"
options/__init__.py,0,b''
options/base_options.py,1,"b'import argparse\nimport os\nfrom util import util\nimport torch\n\nclass BaseOptions():\n\tdef __init__(self):\n\t\tself.parser = argparse.ArgumentParser()\n\t\tself.initialized = False\n\n\tdef initialize(self):\n\t\tself.parser.add_argument(\'--dataroot\', type=str, default=""D:\\Photos\\TrainingData\\BlurredSharp\\combined"", help=\'path to images (should have subfolders trainA, trainB, valA, valB, etc)\')\n\t\tself.parser.add_argument(\'--batchSize\', type=int, default=1, help=\'input batch size\')\n\t\tself.parser.add_argument(\'--loadSizeX\', type=int, default=640, help=\'scale images to this size\')\n\t\tself.parser.add_argument(\'--loadSizeY\', type=int, default=360, help=\'scale images to this size\')\n\t\tself.parser.add_argument(\'--fineSize\', type=int, default=256, help=\'then crop to this size\')\n\t\tself.parser.add_argument(\'--input_nc\', type=int, default=3, help=\'# of input image channels\')\n\t\tself.parser.add_argument(\'--output_nc\', type=int, default=3, help=\'# of output image channels\')\n\t\tself.parser.add_argument(\'--ngf\', type=int, default=64, help=\'# of gen filters in first conv layer\')\n\t\tself.parser.add_argument(\'--ndf\', type=int, default=64, help=\'# of discrim filters in first conv layer\')\n\t\tself.parser.add_argument(\'--which_model_netD\', type=str, default=\'basic\', help=\'selects model to use for netD\')\n\t\tself.parser.add_argument(\'--which_model_netG\', type=str, default=\'resnet_9blocks\', help=\'selects model to use for netG\')\n\t\tself.parser.add_argument(\'--learn_residual\', action=\'store_true\', help=\'if specified, model would learn only the residual to the input\')\n\t\tself.parser.add_argument(\'--gan_type\', type=str, default=\'wgan-gp\', help=\'wgan-gp : Wasserstein GAN with Gradient Penalty, lsgan : Least Sqaures GAN, gan : Vanilla GAN\')\n\t\tself.parser.add_argument(\'--n_layers_D\', type=int, default=3, help=\'only used if which_model_netD==n_layers\')\n\t\tself.parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\')\n\t\tself.parser.add_argument(\'--name\', type=str, default=\'experiment_name\', help=\'name of the experiment. It decides where to store samples and models\')\n\t\tself.parser.add_argument(\'--dataset_mode\', type=str, default=\'aligned\', help=\'chooses how datasets are loaded. [unaligned | aligned | single]\')\n\t\tself.parser.add_argument(\'--model\', type=str, default=\'content_gan\', help=\'chooses which model to use. pix2pix, test, content_gan\')\n\t\tself.parser.add_argument(\'--which_direction\', type=str, default=\'AtoB\', help=\'AtoB or BtoA\')\n\t\tself.parser.add_argument(\'--nThreads\', default=2, type=int, help=\'# threads for loading data\')\n\t\tself.parser.add_argument(\'--checkpoints_dir\', type=str, default=\'./checkpoints\', help=\'models are saved here\')\n\t\tself.parser.add_argument(\'--norm\', type=str, default=\'instance\', help=\'instance normalization or batch normalization\')\n\t\tself.parser.add_argument(\'--serial_batches\', action=\'store_true\', help=\'if true, takes images in order to make batches, otherwise takes them randomly\')\n\t\tself.parser.add_argument(\'--display_winsize\', type=int, default=256,  help=\'display window size\')\n\t\tself.parser.add_argument(\'--display_id\', type=int, default=1, help=\'window id of the web display\')\n\t\tself.parser.add_argument(\'--display_port\', type=int, default=8097, help=\'visdom port of the web display\')\n\t\tself.parser.add_argument(\'--display_single_pane_ncols\', type=int, default=0, help=\'if positive, display all images in a single visdom web panel with certain number of images per row.\')\n\t\tself.parser.add_argument(\'--no_dropout\', action=\'store_true\', help=\'no dropout for the generator\')\n\t\tself.parser.add_argument(\'--max_dataset_size\', type=int, default=float(""inf""), help=\'Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.\')\n\t\tself.parser.add_argument(\'--resize_or_crop\', type=str, default=\'resize_and_crop\', help=\'scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop]\')\n\t\tself.parser.add_argument(\'--no_flip\', action=\'store_true\', help=\'if specified, do not flip the images for data augmentation\')\n\n\t\tself.initialized = True\n\n\tdef parse(self):\n\t\tif not self.initialized:\n\t\t\tself.initialize()\n\t\tself.opt = self.parser.parse_args()\n\t\tself.opt.isTrain = self.isTrain   # train or test\n\n\t\tstr_ids = self.opt.gpu_ids.split(\',\')\n\t\tself.opt.gpu_ids = []\n\t\tfor str_id in str_ids:\n\t\t\tid = int(str_id)\n\t\t\tif id >= 0:\n\t\t\t\tself.opt.gpu_ids.append(id)\n\n\t\t# set gpu ids\n\t\tif len(self.opt.gpu_ids) > 0:\n\t\t\ttorch.cuda.set_device(self.opt.gpu_ids[0])\n\n\t\targs = vars(self.opt)\n\n\t\tprint(\'------------ Options -------------\')\n\t\tfor k, v in sorted(args.items()):\n\t\t\tprint(\'%s: %s\' % (str(k), str(v)))\n\t\tprint(\'-------------- End ----------------\')\n\n\t\t# save to the disk\n\t\texpr_dir = os.path.join(self.opt.checkpoints_dir, self.opt.name)\n\t\tutil.mkdirs(expr_dir)\n\t\tfile_name = os.path.join(expr_dir, \'opt.txt\')\n\t\twith open(file_name, \'wt\') as opt_file:\n\t\t\topt_file.write(\'------------ Options -------------\\n\')\n\t\t\tfor k, v in sorted(args.items()):\n\t\t\t\topt_file.write(\'%s: %s\\n\' % (str(k), str(v)))\n\t\t\topt_file.write(\'-------------- End ----------------\\n\')\n\t\treturn self.opt\n'"
options/test_options.py,0,"b'from .base_options import BaseOptions\n\n\nclass TestOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        self.parser.add_argument(\'--ntest\', type=int, default=float(""inf""), help=\'# of test examples.\')\n        self.parser.add_argument(\'--results_dir\', type=str, default=\'./results/\', help=\'saves results here.\')\n        self.parser.add_argument(\'--aspect_ratio\', type=float, default=1.0, help=\'aspect ratio of result images\')\n        self.parser.add_argument(\'--phase\', type=str, default=\'test\', help=\'train, val, test, etc\')\n        self.parser.add_argument(\'--which_epoch\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n        self.parser.add_argument(\'--how_many\', type=int, default=5000, help=\'how many test images to run\')\n        self.isTrain = False\n'"
options/train_options.py,0,"b""from .base_options import BaseOptions\n\n\nclass TrainOptions(BaseOptions):\n\tdef initialize(self):\n\t\tBaseOptions.initialize(self)\n\t\tself.parser.add_argument('--display_freq', type=int, default=100, help='frequency of showing training results on screen')\n\t\tself.parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n\t\tself.parser.add_argument('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')\n\t\tself.parser.add_argument('--save_epoch_freq', type=int, default=5, help='frequency of saving checkpoints at the end of epochs')\n\t\tself.parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n\t\tself.parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n\t\tself.parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n\t\tself.parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n\t\tself.parser.add_argument('--niter', type=int, default=150, help='# of iter at starting learning rate')\n\t\tself.parser.add_argument('--niter_decay', type=int, default=150, help='# of iter to linearly decay learning rate to zero')\n\t\tself.parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n\t\tself.parser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate for adam')\n\t\tself.parser.add_argument('--lambda_A', type=float, default=100.0, help='weight for cycle loss (A -> B -> A)')\n\t\tself.parser.add_argument('--lambda_B', type=float, default=10.0, help='weight for cycle loss (B -> A -> B)')\n\t\tself.parser.add_argument('--identity', type=float, default=0.0, help='use identity mapping. Setting identity other than 1 has an effect of scaling the weight of the identity mapping loss. For example, if the weight of the identity loss should be 10 times smaller than the weight of the reconstruction loss, please set optidentity = 0.1')\n\t\tself.parser.add_argument('--pool_size', type=int, default=50, help='the size of image buffer that stores previously generated images')\n\t\tself.parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n\t\tself.isTrain = True\n"""
util/__init__.py,0,b''
util/get_data.py,0,"b'from __future__ import print_function\nimport os\nimport tarfile\nimport requests\nfrom warnings import warn\nfrom zipfile import ZipFile\nfrom bs4 import BeautifulSoup\nfrom os.path import abspath, isdir, join, basename\n\n\nclass GetData(object):\n    """"""\n\n    Download CycleGAN or Pix2Pix Data.\n\n    Args:\n        technique : str\n            One of: \'cyclegan\' or \'pix2pix\'.\n        verbose : bool\n            If True, print additional information.\n\n    Examples:\n        >>> from util.get_data import GetData\n        >>> gd = GetData(technique=\'cyclegan\')\n        >>> new_data_path = gd.get(save_path=\'./datasets\')  # options will be displayed.\n\n    """"""\n\n    def __init__(self, technique=\'cyclegan\', verbose=True):\n        url_dict = {\n            \'pix2pix\': \'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets\',\n            \'cyclegan\': \'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets\'\n        }\n        self.url = url_dict.get(technique.lower())\n        self._verbose = verbose\n\n    def _print(self, text):\n        if self._verbose:\n            print(text)\n\n    @staticmethod\n    def _get_options(r):\n        soup = BeautifulSoup(r.text, \'lxml\')\n        options = [h.text for h in soup.find_all(\'a\', href=True)\n                   if h.text.endswith((\'.zip\', \'tar.gz\'))]\n        return options\n\n    def _present_options(self):\n        r = requests.get(self.url)\n        options = self._get_options(r)\n        print(\'Options:\\n\')\n        for i, o in enumerate(options):\n            print(""{0}: {1}"".format(i, o))\n        choice = input(""\\nPlease enter the number of the ""\n                       ""dataset above you wish to download:"")\n        return options[int(choice)]\n\n    def _download_data(self, dataset_url, save_path):\n        if not isdir(save_path):\n            os.makedirs(save_path)\n\n        base = basename(dataset_url)\n        temp_save_path = join(save_path, base)\n\n        with open(temp_save_path, ""wb"") as f:\n            r = requests.get(dataset_url)\n            f.write(r.content)\n\n        if base.endswith(\'.tar.gz\'):\n            obj = tarfile.open(temp_save_path)\n        elif base.endswith(\'.zip\'):\n            obj = ZipFile(temp_save_path, \'r\')\n        else:\n            raise ValueError(""Unknown File Type: {0}."".format(base))\n\n        self._print(""Unpacking Data..."")\n        obj.extractall(save_path)\n        obj.close()\n        os.remove(temp_save_path)\n\n    def get(self, save_path, dataset=None):\n        """"""\n\n        Download a dataset.\n\n        Args:\n            save_path : str\n                A directory to save the data to.\n            dataset : str, optional\n                A specific dataset to download.\n                Note: this must include the file extension.\n                If None, options will be presented for you\n                to choose from.\n\n        Returns:\n            save_path_full : str\n                The absolute path to the downloaded data.\n\n        """"""\n        if dataset is None:\n            selected_dataset = self._present_options()\n        else:\n            selected_dataset = dataset\n\n        save_path_full = join(save_path, selected_dataset.split(\'.\')[0])\n\n        if isdir(save_path_full):\n            warn(""\\n\'{0}\' already exists. Voiding Download."".format(\n                save_path_full))\n        else:\n            self._print(\'Downloading Data...\')\n            url = ""{0}/{1}"".format(self.url, selected_dataset)\n            self._download_data(url, save_path=save_path)\n\n        return abspath(save_path_full)\n'"
util/html.py,0,"b'import dominate\nfrom dominate.tags import *\nimport os\n\n\nclass HTML:\n    def __init__(self, web_dir, title, reflesh=0):\n        self.title = title\n        self.web_dir = web_dir\n        self.img_dir = os.path.join(self.web_dir, \'images\')\n        if not os.path.exists(self.web_dir):\n            os.makedirs(self.web_dir)\n        if not os.path.exists(self.img_dir):\n            os.makedirs(self.img_dir)\n        # print(self.img_dir)\n\n        self.doc = dominate.document(title=title)\n        if reflesh > 0:\n            with self.doc.head:\n                meta(http_equiv=""reflesh"", content=str(reflesh))\n\n    def get_image_dir(self):\n        return self.img_dir\n\n    def add_header(self, str):\n        with self.doc:\n            h3(str)\n\n    def add_table(self, border=1):\n        self.t = table(border=border, style=""table-layout: fixed;"")\n        self.doc.add(self.t)\n\n    def add_images(self, ims, txts, links, width=400):\n        self.add_table()\n        with self.t:\n            with tr():\n                for im, txt, link in zip(ims, txts, links):\n                    with td(style=""word-wrap: break-word;"", halign=""center"", valign=""top""):\n                        with p():\n                            with a(href=os.path.join(\'images\', link)):\n                                img(style=""width:%dpx"" % width, src=os.path.join(\'images\', im))\n                            br()\n                            p(txt)\n\n    def save(self):\n        html_file = \'%s/index.html\' % self.web_dir\n        f = open(html_file, \'wt\')\n        f.write(self.doc.render())\n        f.close()\n\n\nif __name__ == \'__main__\':\n    html = HTML(\'web/\', \'test_html\')\n    html.add_header(\'hello world\')\n\n    ims = []\n    txts = []\n    links = []\n    for n in range(4):\n        ims.append(\'image_%d.png\' % n)\n        txts.append(\'text_%d\' % n)\n        links.append(\'image_%d.png\' % n)\n    html.add_images(ims, txts, links)\n    html.save()\n'"
util/image_pool.py,3,"b'import random\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nclass ImagePool():\n    def __init__(self, pool_size):\n        self.pool_size = pool_size\n        if self.pool_size > 0:\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        if self.pool_size == 0:\n            return images\n        return_images = []\n        for image in images.data:\n            image = torch.unsqueeze(image, 0)\n            if self.num_imgs < self.pool_size:\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:\n                    random_id = random.randint(0, self.pool_size-1)\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:\n                    return_images.append(image)\n        return_images = Variable(torch.cat(return_images, 0))\n        return return_images\n'"
util/metrics.py,3,"b'import torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nfrom math import exp\nimport math\n\ndef gaussian(window_size, sigma):\n\tgauss = torch.Tensor([exp(-(x - window_size/2)**2/float(2*sigma**2)) for x in range(window_size)])\n\treturn gauss/gauss.sum()\n\ndef create_window(window_size, channel):\n\t_1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n\t_2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n\twindow = Variable(_2D_window.expand(channel, 1, window_size, window_size))\n\treturn window\n\ndef SSIM(img1, img2):\n\t(_, channel, _, _) = img1.size()\n\twindow_size = 11\n\twindow = create_window(window_size, channel)\n\tmu1 = F.conv2d(img1, window, padding = window_size/2, groups = channel)\n\tmu2 = F.conv2d(img2, window, padding = window_size/2, groups = channel)\n\n\tmu1_sq = mu1.pow(2)\n\tmu2_sq = mu2.pow(2)\n\tmu1_mu2 = mu1*mu2\n\n\tsigma1_sq = F.conv2d(img1*img1, window, padding = window_size/2, groups = channel) - mu1_sq\n\tsigma2_sq = F.conv2d(img2*img2, window, padding = window_size/2, groups = channel) - mu2_sq\n\tsigma12 = F.conv2d(img1*img2, window, padding = window_size/2, groups = channel) - mu1_mu2\n\n\tC1 = 0.01**2\n\tC2 = 0.03**2\n\n\tssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n\treturn ssim_map.mean()\n\t\ndef PSNR(img1, img2):\n\tmse = np.mean( (img1/255. - img2/255.) ** 2 )\n\tif mse == 0:\n\t\treturn 100\n\tPIXEL_MAX = 1\n\treturn 20 * math.log10(PIXEL_MAX / math.sqrt(mse))\n'"
util/png.py,0,"b'import struct\nimport zlib\n\ndef encode(buf, width, height):\n  """""" buf: must be bytes or a bytearray in py3, a regular string in py2. formatted RGBRGB... """"""\n  assert (width * height * 3 == len(buf))\n  bpp = 3\n\n  def raw_data():\n    # reverse the vertical line order and add null bytes at the start\n    row_bytes = width * bpp\n    for row_start in range((height - 1) * width * bpp, -1, -row_bytes):\n      yield b\'\\x00\'\n      yield buf[row_start:row_start + row_bytes]\n\n  def chunk(tag, data):\n    return [\n        struct.pack(""!I"", len(data)),\n        tag,\n        data,\n        struct.pack(""!I"", 0xFFFFFFFF & zlib.crc32(data, zlib.crc32(tag)))\n      ]\n\n  SIGNATURE = b\'\\x89PNG\\r\\n\\x1a\\n\'\n  COLOR_TYPE_RGB = 2\n  COLOR_TYPE_RGBA = 6\n  bit_depth = 8\n  return b\'\'.join(\n      [ SIGNATURE ] +\n      chunk(b\'IHDR\', struct.pack(""!2I5B"", width, height, bit_depth, COLOR_TYPE_RGB, 0, 0, 0)) +\n      chunk(b\'IDAT\', zlib.compress(b\'\'.join(raw_data()), 9)) +\n      chunk(b\'IEND\', b\'\')\n    )\n'"
util/util.py,1,"b'from __future__ import print_function\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport inspect, re\nimport numpy as np\nimport os\nimport collections\n\n# Converts a Tensor into a Numpy array\n# |imtype|: the desired type of the converted numpy array\ndef tensor2im(image_tensor, imtype=np.uint8):\n\timage_numpy = image_tensor[0].cpu().float().numpy()\n\timage_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n\treturn image_numpy.astype(imtype)\n\n\ndef diagnose_network(net, name=\'network\'):\n\tmean = 0.0\n\tcount = 0\n\tfor param in net.parameters():\n\t\tif param.grad is not None:\n\t\t\tmean += torch.mean(torch.abs(param.grad.data))\n\t\t\tcount += 1\n\tif count > 0:\n\t\tmean = mean / count\n\tprint(name)\n\tprint(mean)\n\n\ndef save_image(image_numpy, image_path):\n\timage_pil = None\n\tif image_numpy.shape[2] == 1:\n\t\timage_numpy = np.reshape(image_numpy, (image_numpy.shape[0],image_numpy.shape[1]))\n\t\timage_pil = Image.fromarray(image_numpy, \'L\')\n\telse:\n\t\timage_pil = Image.fromarray(image_numpy)\n\timage_pil.save(image_path)\n\ndef info(object, spacing=10, collapse=1):\n\t""""""Print methods and doc strings.\n\tTakes module, class, list, dictionary, or string.""""""\n\tmethodList = [e for e in dir(object) if isinstance(getattr(object, e), collections.Callable)]\n\tprocessFunc = collapse and (lambda s: "" "".join(s.split())) or (lambda s: s)\n\tprint( ""\\n"".join([""%s %s"" %\n\t\t\t\t\t (method.ljust(spacing),\n\t\t\t\t\t  processFunc(str(getattr(object, method).__doc__)))\n\t\t\t\t\t for method in methodList]) )\n\ndef varname(p):\n\tfor line in inspect.getframeinfo(inspect.currentframe().f_back)[3]:\n\t\tm = re.search(r\'\\bvarname\\s*\\(\\s*([A-Za-z_][A-Za-z0-9_]*)\\s*\\)\', line)\n\t\tif m:\n\t\t\treturn m.group(1)\n\ndef print_numpy(x, val=True, shp=False):\n\tx = x.astype(np.float64)\n\tif shp:\n\t\tprint(\'shape,\', x.shape)\n\tif val:\n\t\tx = x.flatten()\n\t\tprint(\'mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f\' % (\n\t\t\tnp.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n\n\ndef mkdirs(paths):\n\tif isinstance(paths, list) and not isinstance(paths, str):\n\t\tfor path in paths:\n\t\t\tmkdir(path)\n\telse:\n\t\tmkdir(paths)\n\n\ndef mkdir(path):\n\tif not os.path.exists(path):\n\t\tos.makedirs(path)\n'"
util/visualizer.py,0,"b'import numpy as np\nimport os\nimport ntpath\nimport time\nfrom . import util\nfrom . import html\n\nclass Visualizer():\n    def __init__(self, opt):\n        # self.opt = opt\n        self.display_id = opt.display_id\n        self.use_html = opt.isTrain and not opt.no_html\n        self.win_size = opt.display_winsize\n        self.name = opt.name\n        if self.display_id > 0:\n            import visdom\n            self.vis = visdom.Visdom(port = opt.display_port)\n            self.display_single_pane_ncols = opt.display_single_pane_ncols\n\n        if self.use_html:\n            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, \'web\')\n            self.img_dir = os.path.join(self.web_dir, \'images\')\n            print(\'create web directory %s...\' % self.web_dir)\n            util.mkdirs([self.web_dir, self.img_dir])\n        self.log_name = os.path.join(opt.checkpoints_dir, opt.name, \'loss_log.txt\')\n        with open(self.log_name, ""a"") as log_file:\n            now = time.strftime(""%c"")\n            log_file.write(\'================ Training Loss (%s) ================\\n\' % now)\n\n    # |visuals|: dictionary of images to display or save\n    def display_current_results(self, visuals, epoch):\n        if self.display_id > 0: # show images in the browser\n            if self.display_single_pane_ncols > 0:\n                h, w = next(iter(visuals.values())).shape[:2]\n                table_css = """"""<style>\n    table {border-collapse: separate; border-spacing:4px; white-space:nowrap; text-align:center}\n    table td {width: %dpx; height: %dpx; padding: 4px; outline: 4px solid black}\n</style>"""""" % (w, h)\n                ncols = self.display_single_pane_ncols\n                title = self.name\n                label_html = \'\'\n                label_html_row = \'\'\n                nrows = int(np.ceil(len(visuals.items()) / ncols))\n                images = []\n                idx = 0\n                for label, image_numpy in visuals.items():\n                    label_html_row += \'<td>%s</td>\' % label\n                    images.append(image_numpy.transpose([2, 0, 1]))\n                    idx += 1\n                    if idx % ncols == 0:\n                        label_html += \'<tr>%s</tr>\' % label_html_row\n                        label_html_row = \'\'\n                white_image = np.ones_like(image_numpy.transpose([2, 0, 1]))*255\n                while idx % ncols != 0:\n                    images.append(white_image)\n                    label_html_row += \'<td></td>\'\n                    idx += 1\n                if label_html_row != \'\':\n                    label_html += \'<tr>%s</tr>\' % label_html_row\n                # pane col = image row\n                self.vis.images(images, nrow=ncols, win=self.display_id + 1,\n                                padding=2, opts=dict(title=title + \' images\'))\n                label_html = \'<table>%s</table>\' % label_html\n                self.vis.text(table_css + label_html, win = self.display_id + 2,\n                              opts=dict(title=title + \' labels\'))\n            else:\n                idx = 1\n                for label, image_numpy in visuals.items():\n                    #image_numpy = np.flipud(image_numpy)\n                    self.vis.image(image_numpy.transpose([2,0,1]), opts=dict(title=label),\n                                       win=self.display_id + idx)\n                    idx += 1\n\n        if self.use_html: # save images to a html file\n            for label, image_numpy in visuals.items():\n                img_path = os.path.join(self.img_dir, \'epoch%.3d_%s.png\' % (epoch, label))\n                util.save_image(image_numpy, img_path)\n            # update website\n            webpage = html.HTML(self.web_dir, \'Experiment name = %s\' % self.name, reflesh=1)\n            for n in range(epoch, 0, -1):\n                webpage.add_header(\'Results of Epoch [%d]\' % n)\n                ims = []\n                txts = []\n                links = []\n\n                for label, image_numpy in visuals.items():\n                    img_path = \'epoch%.3d_%s.png\' % (n, label)\n                    ims.append(img_path)\n                    txts.append(label)\n                    links.append(img_path)\n                webpage.add_images(ims, txts, links, width=self.win_size)\n            webpage.save()\n\n    # errors: dictionary of error labels and values\n    def plot_current_errors(self, epoch, counter_ratio, opt, errors):\n        if not hasattr(self, \'plot_data\'):\n            self.plot_data = {\'X\':[],\'Y\':[], \'legend\':list(errors.keys())}\n        self.plot_data[\'X\'].append(epoch + counter_ratio)\n        self.plot_data[\'Y\'].append([errors[k] for k in self.plot_data[\'legend\']])\n        self.vis.line(\n            X=np.stack([np.array(self.plot_data[\'X\'])]*len(self.plot_data[\'legend\']),1),\n            Y=np.array(self.plot_data[\'Y\']),\n            opts={\n                \'title\': self.name + \' loss over time\',\n                \'legend\': self.plot_data[\'legend\'],\n                \'xlabel\': \'epoch\',\n                \'ylabel\': \'loss\'},\n            win=self.display_id)\n\n    # errors: same format as |errors| of plotCurrentErrors\n    def print_current_errors(self, epoch, i, errors, t):\n        message = \'(epoch: %d, iters: %d, time: %.3f) \' % (epoch, i, t)\n        for k, v in errors.items():\n            message += \'%s: %.3f \' % (k, v)\n\n        print(message)\n        with open(self.log_name, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n\n    # save image to the disk\n    def save_images(self, webpage, visuals, image_path):\n        image_dir = webpage.get_image_dir()\n        short_path = ntpath.basename(image_path[0])\n        name = os.path.splitext(short_path)[0]\n\n        webpage.add_header(name)\n        ims = []\n        txts = []\n        links = []\n\n        for label, image_numpy in visuals.items():\n            image_name = \'%s_%s.png\' % (name, label)\n            save_path = os.path.join(image_dir, image_name)\n            util.save_image(image_numpy, save_path)\n\n            ims.append(image_name)\n            txts.append(label)\n            links.append(image_name)\n        webpage.add_images(ims, txts, links, width=self.win_size)\n'"
datasets/helper functions/grayscale.py,0,"b""from pdb import set_trace as st\nimport os\nimport numpy as np\nimport cv2\nimport argparse\n\n# Helper script to create dataset for image colorization\n\nparser = argparse.ArgumentParser('create image pairs')\nparser.add_argument('--fold_A', dest='fold_A', help='input directory for images', type=str, default='../dataset/50kshoes_edges')\nparser.add_argument('--fold_B', dest='fold_B', help='output directory', type=str, default='../dataset/test_B')\nparser.add_argument('--num_imgs', dest='num_imgs', help='number of images',type=int, default=1000000)\nargs = parser.parse_args()\n\nfor arg in vars(args):\n    print('[%s] = ' % arg,  getattr(args, arg))\n\nsplits = os.listdir(args.fold_A)\n\nfor sp in splits:\n    img_fold_A = os.path.join(args.fold_A, sp)\n    img_list = os.listdir(img_fold_A)\n\t\n    num_imgs = min(args.num_imgs, len(img_list))\n    print('split = %s, use %d/%d images' % (sp, num_imgs, len(img_list)))\n    img_fold_B = os.path.join(args.fold_B, sp)\n    if not os.path.isdir(img_fold_B):\n        os.makedirs(img_fold_B)\n    print('split = %s, number of images = %d' % (sp, num_imgs))\n    for n in range(num_imgs):\n        name_A = img_list[n]\n        path_A = os.path.join(img_fold_A, name_A)\n\n        if os.path.isfile(path_A):\n            name_B = name_A\n\t\t\t\n            path_B = os.path.join(img_fold_B, name_B)\n            im_A = cv2.imread(path_A, 0)\n            cv2.imwrite(path_B, im_A)\n"""
