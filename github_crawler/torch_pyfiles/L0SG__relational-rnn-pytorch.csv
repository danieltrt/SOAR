file_path,api_count,code
data.py,1,"b'import os\nimport torch\nimport numpy as np\n\n\nclass Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n        self.idx2count = []\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.idx2count.append(1)\n            self.word2idx[word] = len(self.idx2word) - 1\n        else:\n            self.idx2count[self.word2idx[word]] += 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n\n\nclass Corpus(object):\n    def __init__(self, path):\n        self.dictionary = Dictionary()\n        tokens_train = self.add_corpus(os.path.join(path, \'train.txt\'))\n        tokens_valid = self.add_corpus(os.path.join(path, \'valid.txt\'))\n        tokens_test = self.add_corpus(os.path.join(path, \'test.txt\'))\n\n        # sort the words by word frequency in descending order\n        # this is for using adaptive softmax: it assumes that the most frequent word get index 0\n        idx_argsorted = np.flip(np.argsort(self.dictionary.idx2count), axis=-1)\n\n        # re-create given the sorted ones\n        self.dictionary.idx2count = np.array(self.dictionary.idx2count)[idx_argsorted].tolist()\n        self.dictionary.idx2word = np.array(self.dictionary.idx2word)[idx_argsorted].tolist()\n        self.dictionary.word2idx = dict(zip(self.dictionary.idx2word,\n                                            np.arange(len(self.dictionary.idx2word)).tolist()))\n\n        self.train = self.tokenize(os.path.join(path, \'train.txt\'), tokens_train)\n        self.valid = self.tokenize(os.path.join(path, \'valid.txt\'), tokens_valid)\n        self.test = self.tokenize(os.path.join(path, \'test.txt\'), tokens_test)\n\n    def add_corpus(self, path):\n        """"""Tokenizes a text file.""""""\n        assert os.path.exists(path)\n        # Add words to the dictionary\n        with open(path, \'r\', encoding=""utf8"") as f:\n            tokens = 0\n            for line in f:\n                words = line.split() + [\'<eos>\']\n                tokens += len(words)\n                for word in words:\n                    self.dictionary.add_word(word)\n\n        return tokens\n\n    def tokenize(self, path, tokens):\n        # Tokenize file content\n        with open(path, \'r\', encoding=""utf8"") as f:\n            ids = torch.LongTensor(tokens)\n            token = 0\n            for line in f:\n                words = line.split() + [\'<eos>\']\n                for word in words:\n                    ids[token] = self.dictionary.word2idx[word]\n                    token += 1\n\n        return ids\n'"
generate_rmc.py,7,"b'###############################################################################\n\n# This file generates new sentences sampled from the language model\n\n###############################################################################\n\nimport argparse\n\nimport torch\nimport pickle\nimport data\nimport os\n\nparser = argparse.ArgumentParser(description=\'PyTorch Wikitext-2 Language Model\')\n\n# Model parameters.\nparser.add_argument(\'--data\', type=str, default=\'./data/wikitext-2\',\n                    help=\'location of the data corpus\')\nparser.add_argument(\'--checkpoint\', type=str, default=None,\n                    help=\'model checkpoint to use\')\nparser.add_argument(\'--outf\', type=str, default=\'generated.txt\',\n                    help=\'output file for generated text\')\nparser.add_argument(\'--words\', type=int, default=\'1000\',\n                    help=\'number of words to generate\')\nparser.add_argument(\'--seed\', type=int, default=1111,\n                    help=\'random seed\')\nparser.add_argument(\'--cuda\', action=\'store_true\',\n                    help=\'use CUDA\')\nparser.add_argument(\'--temperature\', type=float, default=1.,\n                    help=\'temperature - higher will increase diversity\')\nparser.add_argument(\'--log-interval\', type=int, default=100,\n                    help=\'reporting interval\')\nargs = parser.parse_args()\n\nif args.checkpoint is None:\n    raise ValueError(""--checkpoint not provided. specify model_dump_(epoch).pt"")\n\n# Set the random seed manually for reproducibility.\ntorch.manual_seed(args.seed)\n\nif torch.cuda.is_available():\n    if not args.cuda:\n        print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n\ndevice = torch.device(""cuda"" if args.cuda else ""cpu"")\n\nif args.temperature < 1e-3:\n    parser.error(""--temperature has to be greater or equal 1e-3"")\n\nwith open(args.checkpoint, \'rb\') as f:\n    model = torch.load(f).to(device)\nmodel.eval()\n\ncorpus_name = os.path.basename(os.path.normpath(args.data))\ncorpus_filename = \'./data/corpus-\' + str(corpus_name) + str(\'.pkl\')\nif os.path.isfile(corpus_filename):\n    print(""loading pre-built "" + str(corpus_name) + "" corpus file..."")\n    loadfile = open(corpus_filename, \'rb\')\n    corpus = pickle.load(loadfile)\n    loadfile.close()\nelse:\n    print(""building "" + str(corpus_name) + "" corpus..."")\n    corpus = data.Corpus(args.data)\n    # save the corpus for later\n    savefile = open(corpus_filename, \'wb\')\n    pickle.dump(corpus, savefile)\n    savefile.close()\n    print(""corpus saved to pickle"")\n\nntokens = len(corpus.dictionary)\nmemory = model.module.initial_state(1, trainable=False).to(device)\n\ninput = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n\nwith open(args.outf, \'w\') as outf:\n    with torch.no_grad():  # no tracking history\n        for i in range(args.words):\n            output, _, memory = model(input, memory, None, require_logits=True)\n            word_weights = output.squeeze().div(args.temperature).exp().cpu()\n            word_idx = torch.multinomial(word_weights, 1)[0]\n            input.fill_(word_idx)\n            word = corpus.dictionary.idx2word[word_idx]\n\n            outf.write(word + (\'\\n\' if i % 20 == 19 else \' \'))\n\n            if i % args.log_interval == 0:\n                print(\'| Generated {}/{} words\'.format(i, args.words))\n'"
generate_rnn.py,7,"b'###############################################################################\n\n# This file generates new sentences sampled from the language model\n#\n###############################################################################\n\nimport argparse\n\nimport torch\nimport pickle\nimport data\nimport os\n\nparser = argparse.ArgumentParser(description=\'PyTorch Wikitext-2 Language Model\')\n\n# Model parameters.\nparser.add_argument(\'--data\', type=str, default=\'./data/wikitext-2\',\n                    help=\'location of the data corpus\')\nparser.add_argument(\'--checkpoint\', type=str, default=None,\n                    help=\'model checkpoint to use\')\nparser.add_argument(\'--outf\', type=str, default=\'generated.txt\',\n                    help=\'output file for generated text\')\nparser.add_argument(\'--words\', type=int, default=\'1000\',\n                    help=\'number of words to generate\')\nparser.add_argument(\'--seed\', type=int, default=1111,\n                    help=\'random seed\')\nparser.add_argument(\'--cuda\', action=\'store_true\',\n                    help=\'use CUDA\')\nparser.add_argument(\'--temperature\', type=float, default=1.,\n                    help=\'temperature - higher will increase diversity\')\nparser.add_argument(\'--log-interval\', type=int, default=100,\n                    help=\'reporting interval\')\nargs = parser.parse_args()\n\nif args.checkpoint is None:\n    raise ValueError(""--checkpoint not provided. specify model_dump_(epoch).pt"")\n\n# Set the random seed manually for reproducibility.\ntorch.manual_seed(args.seed)\n\nif torch.cuda.is_available():\n    if not args.cuda:\n        print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n\ndevice = torch.device(""cuda"" if args.cuda else ""cpu"")\n\nif args.temperature < 1e-3:\n    parser.error(""--temperature has to be greater or equal 1e-3"")\n\nwith open(args.checkpoint, \'rb\') as f:\n    model = torch.load(f).to(device)\nmodel.eval()\n\ncorpus_name = os.path.basename(os.path.normpath(args.data))\ncorpus_filename = \'./data/corpus-\' + str(corpus_name) + str(\'.pkl\')\nif os.path.isfile(corpus_filename):\n    print(""loading pre-built "" + str(corpus_name) + "" corpus file..."")\n    loadfile = open(corpus_filename, \'rb\')\n    corpus = pickle.load(loadfile)\n    loadfile.close()\nelse:\n    print(""building "" + str(corpus_name) + "" corpus..."")\n    corpus = data.Corpus(args.data)\n    # save the corpus for later\n    savefile = open(corpus_filename, \'wb\')\n    pickle.dump(corpus, savefile)\n    savefile.close()\n    print(""corpus saved to pickle"")\n\nntokens = len(corpus.dictionary)\nhidden = model.init_hidden(1)\ninput = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n\nwith open(args.outf, \'w\') as outf:\n    with torch.no_grad():  # no tracking history\n        for i in range(args.words):\n            output, hidden = model(input, hidden)\n            word_weights = output.squeeze().div(args.temperature).exp().cpu()\n            word_idx = torch.multinomial(word_weights, 1)[0]\n            input.fill_(word_idx)\n            word = corpus.dictionary.idx2word[word_idx]\n\n            outf.write(word + (\'\\n\' if i % 20 == 19 else \' \'))\n\n            if i % args.log_interval == 0:\n                print(\'| Generated {}/{} words\'.format(i, args.words))\n'"
relational_rnn_general.py,20,"b'import torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\n# this class largely follows the official sonnet implementation\n# https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/relational_memory.py\n\n\nclass RelationalMemory(nn.Module):\n    """"""\n    Constructs a `RelationalMemory` object.\n    This class is same as the RMC from relational_rnn_models.py, but without language modeling-specific variables.\n    Args:\n      mem_slots: The total number of memory slots to use.\n      head_size: The size of an attention head.\n      input_size: The size of input per step. i.e. the dimension of each input vector\n      num_heads: The number of attention heads to use. Defaults to 1.\n      num_blocks: Number of times to compute attention per time step. Defaults\n        to 1.\n      forget_bias: Bias to use for the forget gate, assuming we are using\n        some form of gating. Defaults to 1.\n      input_bias: Bias to use for the input gate, assuming we are using\n        some form of gating. Defaults to 0.\n      gate_style: Whether to use per-element gating (\'unit\'),\n        per-memory slot gating (\'memory\'), or no gating at all (None).\n        Defaults to `unit`.\n      attention_mlp_layers: Number of layers to use in the post-attention\n        MLP. Defaults to 2.\n      key_size: Size of vector to use for key & query vectors in the attention\n        computation. Defaults to None, in which case we use `head_size`.\n      name: Name of the module.\n\n      # NEW flag for this class\n      return_all_outputs: Whether the model returns outputs for each step (like seq2seq) or only the final output.\n    Raises:\n      ValueError: gate_style not one of [None, \'memory\', \'unit\'].\n      ValueError: num_blocks is < 1.\n      ValueError: attention_mlp_layers is < 1.\n    """"""\n\n    def __init__(self, mem_slots, head_size, input_size, num_heads=1, num_blocks=1, forget_bias=1., input_bias=0.,\n                 gate_style=\'unit\', attention_mlp_layers=2, key_size=None, return_all_outputs=False):\n        super(RelationalMemory, self).__init__()\n\n        ########## generic parameters for RMC ##########\n        self.mem_slots = mem_slots\n        self.head_size = head_size\n        self.num_heads = num_heads\n        self.mem_size = self.head_size * self.num_heads\n\n        # a new fixed params needed for pytorch port of RMC\n        # +1 is the concatenated input per time step : we do self-attention with the concatenated memory & input\n        # so if the mem_slots = 1, this value is 2\n        self.mem_slots_plus_input = self.mem_slots + 1\n\n        if num_blocks < 1:\n            raise ValueError(\'num_blocks must be >=1. Got: {}.\'.format(num_blocks))\n        self.num_blocks = num_blocks\n\n        if gate_style not in [\'unit\', \'memory\', None]:\n            raise ValueError(\n                \'gate_style must be one of [\\\'unit\\\', \\\'memory\\\', None]. got: \'\n                \'{}.\'.format(gate_style))\n        self.gate_style = gate_style\n\n        if attention_mlp_layers < 1:\n            raise ValueError(\'attention_mlp_layers must be >= 1. Got: {}.\'.format(\n                attention_mlp_layers))\n        self.attention_mlp_layers = attention_mlp_layers\n\n        self.key_size = key_size if key_size else self.head_size\n\n        ########## parameters for multihead attention ##########\n        # value_size is same as head_size\n        self.value_size = self.head_size\n        # total size for query-key-value\n        self.qkv_size = 2 * self.key_size + self.value_size\n        self.total_qkv_size = self.qkv_size * self.num_heads  # denoted as F\n\n        # each head has qkv_sized linear projector\n        # just using one big param is more efficient, rather than this line\n        # self.qkv_projector = [nn.Parameter(torch.randn((self.qkv_size, self.qkv_size))) for _ in range(self.num_heads)]\n        self.qkv_projector = nn.Linear(self.mem_size, self.total_qkv_size)\n        self.qkv_layernorm = nn.LayerNorm([self.mem_slots_plus_input, self.total_qkv_size])\n\n        # used for attend_over_memory function\n        self.attention_mlp = nn.ModuleList([nn.Linear(self.mem_size, self.mem_size)] * self.attention_mlp_layers)\n        self.attended_memory_layernorm = nn.LayerNorm([self.mem_slots_plus_input, self.mem_size])\n        self.attended_memory_layernorm2 = nn.LayerNorm([self.mem_slots_plus_input, self.mem_size])\n\n        ########## parameters for initial embedded input projection ##########\n        self.input_size = input_size\n        self.input_projector = nn.Linear(self.input_size, self.mem_size)\n\n        ########## parameters for gating ##########\n        self.num_gates = 2 * self.calculate_gate_size()\n        self.input_gate_projector = nn.Linear(self.mem_size, self.num_gates)\n        self.memory_gate_projector = nn.Linear(self.mem_size, self.num_gates)\n        # trainable scalar gate bias tensors\n        self.forget_bias = nn.Parameter(torch.tensor(forget_bias, dtype=torch.float32))\n        self.input_bias = nn.Parameter(torch.tensor(input_bias, dtype=torch.float32))\n\n        ########## number of outputs returned #####\n        self.return_all_outputs = return_all_outputs\n\n    def repackage_hidden(self, h):\n        """"""Wraps hidden states in new Tensors, to detach them from their history.""""""\n        # needed for truncated BPTT, called at every batch forward pass\n        if isinstance(h, torch.Tensor):\n            return h.detach()\n        else:\n            return tuple(self.repackage_hidden(v) for v in h)\n\n    def initial_state(self, batch_size, trainable=False):\n        """"""\n        Creates the initial memory.\n        We should ensure each row of the memory is initialized to be unique,\n        so initialize the matrix to be the identity. We then pad or truncate\n        as necessary so that init_state is of size\n        (batch_size, self.mem_slots, self.mem_size).\n        Args:\n          batch_size: The size of the batch.\n          trainable: Whether the initial state is trainable. This is always True.\n        Returns:\n          init_state: A truncated or padded matrix of size\n            (batch_size, self.mem_slots, self.mem_size).\n        """"""\n        init_state = torch.stack([torch.eye(self.mem_slots) for _ in range(batch_size)])\n\n        # pad the matrix with zeros\n        if self.mem_size > self.mem_slots:\n            difference = self.mem_size - self.mem_slots\n            pad = torch.zeros((batch_size, self.mem_slots, difference))\n            init_state = torch.cat([init_state, pad], -1)\n\n        # truncation. take the first \'self.mem_size\' components\n        elif self.mem_size < self.mem_slots:\n            init_state = init_state[:, :, :self.mem_size]\n\n        return init_state\n\n    def multihead_attention(self, memory):\n        """"""\n        Perform multi-head attention from \'Attention is All You Need\'.\n        Implementation of the attention mechanism from\n        https://arxiv.org/abs/1706.03762.\n        Args:\n          memory: Memory tensor to perform attention on.\n        Returns:\n          new_memory: New memory tensor.\n        """"""\n\n        # First, a simple linear projection is used to construct queries\n        qkv = self.qkv_projector(memory)\n        # apply layernorm for every dim except the batch dim\n        qkv = self.qkv_layernorm(qkv)\n\n        # mem_slots needs to be dynamically computed since mem_slots got concatenated with inputs\n        # example: self.mem_slots=10 and seq_length is 3, and then mem_slots is 10 + 1 = 11 for each 3 step forward pass\n        # this is the same as self.mem_slots_plus_input, but defined to keep the sonnet implementation code style\n        mem_slots = memory.shape[1]  # denoted as N\n\n        # split the qkv to multiple heads H\n        # [B, N, F] => [B, N, H, F/H]\n        qkv_reshape = qkv.view(qkv.shape[0], mem_slots, self.num_heads, self.qkv_size)\n\n        # [B, N, H, F/H] => [B, H, N, F/H]\n        qkv_transpose = qkv_reshape.permute(0, 2, 1, 3)\n\n        # [B, H, N, key_size], [B, H, N, key_size], [B, H, N, value_size]\n        q, k, v = torch.split(qkv_transpose, [self.key_size, self.key_size, self.value_size], -1)\n\n        # scale q with d_k, the dimensionality of the key vectors\n        q *= (self.key_size ** -0.5)\n\n        # make it [B, H, N, N]\n        dot_product = torch.matmul(q, k.permute(0, 1, 3, 2))\n        weights = F.softmax(dot_product, dim=-1)\n\n        # output is [B, H, N, V]\n        output = torch.matmul(weights, v)\n\n        # [B, H, N, V] => [B, N, H, V] => [B, N, H*V]\n        output_transpose = output.permute(0, 2, 1, 3).contiguous()\n        new_memory = output_transpose.view((output_transpose.shape[0], output_transpose.shape[1], -1))\n\n        return new_memory\n\n    @property\n    def state_size(self):\n        return [self.mem_slots, self.mem_size]\n\n    @property\n    def output_size(self):\n        return self.mem_slots * self.mem_size\n\n    def calculate_gate_size(self):\n        """"""\n        Calculate the gate size from the gate_style.\n        Returns:\n          The per sample, per head parameter size of each gate.\n        """"""\n        if self.gate_style == \'unit\':\n            return self.mem_size\n        elif self.gate_style == \'memory\':\n            return 1\n        else:  # self.gate_style == None\n            return 0\n\n    def create_gates(self, inputs, memory):\n        """"""\n        Create input and forget gates for this step using `inputs` and `memory`.\n        Args:\n          inputs: Tensor input.\n          memory: The current state of memory.\n        Returns:\n          input_gate: A LSTM-like insert gate.\n          forget_gate: A LSTM-like forget gate.\n        """"""\n        # We\'ll create the input and forget gates at once. Hence, calculate double\n        # the gate size.\n\n        # equation 8: since there is no output gate, h is just a tanh\'ed m\n        memory = torch.tanh(memory)\n\n        # TODO: check this input flattening is correct\n        # sonnet uses this, but i think it assumes time step of 1 for all cases\n        # if inputs is (B, T, features) where T > 1, this gets incorrect\n        # inputs = inputs.view(inputs.shape[0], -1)\n\n        # fixed implementation\n        if len(inputs.shape) == 3:\n            if inputs.shape[1] > 1:\n                raise ValueError(\n                    ""input seq length is larger than 1. create_gate function is meant to be called for each step, with input seq length of 1"")\n            inputs = inputs.view(inputs.shape[0], -1)\n            # matmul for equation 4 and 5\n            # there is no output gate, so equation 6 is not implemented\n            gate_inputs = self.input_gate_projector(inputs)\n            gate_inputs = gate_inputs.unsqueeze(dim=1)\n            gate_memory = self.memory_gate_projector(memory)\n        else:\n            raise ValueError(""input shape of create_gate function is 2, expects 3"")\n\n        # this completes the equation 4 and 5\n        gates = gate_memory + gate_inputs\n        gates = torch.split(gates, split_size_or_sections=int(gates.shape[2] / 2), dim=2)\n        input_gate, forget_gate = gates\n        assert input_gate.shape[2] == forget_gate.shape[2]\n\n        # to be used for equation 7\n        input_gate = torch.sigmoid(input_gate + self.input_bias)\n        forget_gate = torch.sigmoid(forget_gate + self.forget_bias)\n\n        return input_gate, forget_gate\n\n    def attend_over_memory(self, memory):\n        """"""\n        Perform multiheaded attention over `memory`.\n            Args:\n              memory: Current relational memory.\n            Returns:\n              The attended-over memory.\n        """"""\n        for _ in range(self.num_blocks):\n            attended_memory = self.multihead_attention(memory)\n\n            # Add a skip connection to the multiheaded attention\'s input.\n            memory = self.attended_memory_layernorm(memory + attended_memory)\n\n            # add a skip connection to the attention_mlp\'s input.\n            attention_mlp = memory\n            for i, l in enumerate(self.attention_mlp):\n                attention_mlp = self.attention_mlp[i](attention_mlp)\n                attention_mlp = F.relu(attention_mlp)\n            memory = self.attended_memory_layernorm2(memory + attention_mlp)\n\n        return memory\n\n    def forward_step(self, inputs, memory, treat_input_as_matrix=False):\n        """"""\n        Forward step of the relational memory core.\n        Args:\n          inputs: Tensor input.\n          memory: Memory output from the previous time step.\n          treat_input_as_matrix: Optional, whether to treat `input` as a sequence\n            of matrices. Default to False, in which case the input is flattened\n            into a vector.\n        Returns:\n          output: This time step\'s output.\n          next_memory: The next version of memory to use.\n        """"""\n\n        if treat_input_as_matrix:\n            # keep (Batch, Seq, ...) dim (0, 1), flatten starting from dim 2\n            inputs = inputs.view(inputs.shape[0], inputs.shape[1], -1)\n            # apply linear layer for dim 2\n            inputs_reshape = self.input_projector(inputs)\n        else:\n            # keep (Batch, ...) dim (0), flatten starting from dim 1\n            inputs = inputs.view(inputs.shape[0], -1)\n            # apply linear layer for dim 1\n            inputs = self.input_projector(inputs)\n            # unsqueeze the time step to dim 1\n            inputs_reshape = inputs.unsqueeze(dim=1)\n\n        memory_plus_input = torch.cat([memory, inputs_reshape], dim=1)\n        next_memory = self.attend_over_memory(memory_plus_input)\n\n        # cut out the concatenated input vectors from the original memory slots\n        n = inputs_reshape.shape[1]\n        next_memory = next_memory[:, :-n, :]\n\n        if self.gate_style == \'unit\' or self.gate_style == \'memory\':\n            # these gates are sigmoid-applied ones for equation 7\n            input_gate, forget_gate = self.create_gates(inputs_reshape, memory)\n            # equation 7 calculation\n            next_memory = input_gate * torch.tanh(next_memory)\n            next_memory += forget_gate * memory\n\n        output = next_memory.view(next_memory.shape[0], -1)\n\n        return output, next_memory\n\n    def forward(self, inputs, memory):\n        # Starting each batch, we detach the hidden state from how it was previously produced.\n        # If we didn\'t, the model would try backpropagating all the way to start of the dataset.\n        memory = self.repackage_hidden(memory)\n\n        # for loop implementation of (entire) recurrent forward pass of the model\n        # inputs is batch first [batch, seq], and output logit per step is [batch, vocab]\n        # so the concatenated logits are [seq * batch, vocab]\n\n        # targets are flattened [seq, batch] => [seq * batch], so the dimension is correct\n\n        logits = []\n        # shape[1] is seq_lenth T\n        for idx_step in range(inputs.shape[1]):\n            logit, memory = self.forward_step(inputs[:, idx_step], memory)\n            logits.append(logit)\n        logits = torch.cat(logits)\n\n        if self.return_all_outputs:\n            return logits, memory\n        else:\n            return logit, memory\n\n\n\n\n# ########## DEBUG: unit test code ##########\n# input_size = 44\n# seq_length = 1\n# batch_size = 32\n# model = RelationalMemory(mem_slots=10, head_size=20, input_size=input_size, num_tokens=66, num_heads=8, num_blocks=1, forget_bias=1., input_bias=0.)\n# model_memory = model.initial_state(batch_size=batch_size)\n#\n# # random input\n# random_input = torch.randn((32, seq_length, input_size))\n# # random targets\n# random_targets = torch.randn((32, seq_length, input_size))\n#\n# # take a one step forward\n# logit, next_memory = model(random_input, model_memory, random_targets, treat_input_as_matrix=True)\n'"
relational_rnn_models.py,20,"b'import torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport numpy as np\n\n# this class largely follows the official sonnet implementation\n# https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/relational_memory.py\n\n\nclass RelationalMemory(nn.Module):\n    """"""\n    Constructs a `RelationalMemory` object.\n    Args:\n      mem_slots: The total number of memory slots to use.\n      head_size: The size of an attention head.\n      input_size: The size of input per step. i.e. the dimension of each input vector\n      num_heads: The number of attention heads to use. Defaults to 1.\n      num_blocks: Number of times to compute attention per time step. Defaults\n        to 1.\n      forget_bias: Bias to use for the forget gate, assuming we are using\n        some form of gating. Defaults to 1.\n      input_bias: Bias to use for the input gate, assuming we are using\n        some form of gating. Defaults to 0.\n      gate_style: Whether to use per-element gating (\'unit\'),\n        per-memory slot gating (\'memory\'), or no gating at all (None).\n        Defaults to `unit`.\n      attention_mlp_layers: Number of layers to use in the post-attention\n        MLP. Defaults to 2.\n      key_size: Size of vector to use for key & query vectors in the attention\n        computation. Defaults to None, in which case we use `head_size`.\n      name: Name of the module.\n    Raises:\n      ValueError: gate_style not one of [None, \'memory\', \'unit\'].\n      ValueError: num_blocks is < 1.\n      ValueError: attention_mlp_layers is < 1.\n    """"""\n\n    def __init__(self, mem_slots, head_size, input_size, num_tokens, num_heads=1, num_blocks=1, forget_bias=1.,\n                 input_bias=0.,\n                 gate_style=\'unit\', attention_mlp_layers=2, key_size=None, use_adaptive_softmax=False, cutoffs=None):\n        super(RelationalMemory, self).__init__()\n\n        ########## generic parameters for RMC ##########\n        self.mem_slots = mem_slots\n        self.head_size = head_size\n        self.num_heads = num_heads\n        self.mem_size = self.head_size * self.num_heads\n\n        # a new fixed params needed for pytorch port of RMC\n        # +1 is the concatenated input per time step : we do self-attention with the concatenated memory & input\n        # so if the mem_slots = 1, this value is 2\n        self.mem_slots_plus_input = self.mem_slots + 1\n\n        if num_blocks < 1:\n            raise ValueError(\'num_blocks must be >=1. Got: {}.\'.format(num_blocks))\n        self.num_blocks = num_blocks\n\n        if gate_style not in [\'unit\', \'memory\', None]:\n            raise ValueError(\n                \'gate_style must be one of [\\\'unit\\\', \\\'memory\\\', None]. got: \'\n                \'{}.\'.format(gate_style))\n        self.gate_style = gate_style\n\n        if attention_mlp_layers < 1:\n            raise ValueError(\'attention_mlp_layers must be >= 1. Got: {}.\'.format(\n                attention_mlp_layers))\n        self.attention_mlp_layers = attention_mlp_layers\n\n        self.key_size = key_size if key_size else self.head_size\n\n        ########## parameters for multihead attention ##########\n        # value_size is same as head_size\n        self.value_size = self.head_size\n        # total size for query-key-value\n        self.qkv_size = 2 * self.key_size + self.value_size\n        self.total_qkv_size = self.qkv_size * self.num_heads  # denoted as F\n\n        # each head has qkv_sized linear projector\n        # just using one big param is more efficient, rather than this line\n        # self.qkv_projector = [nn.Parameter(torch.randn((self.qkv_size, self.qkv_size))) for _ in range(self.num_heads)]\n        self.qkv_projector = nn.Linear(self.mem_size, self.total_qkv_size)\n        self.qkv_layernorm = nn.LayerNorm([self.mem_slots_plus_input, self.total_qkv_size])\n\n        # used for attend_over_memory function\n        self.attention_mlp = nn.ModuleList([nn.Linear(self.mem_size, self.mem_size)] * self.attention_mlp_layers)\n        self.attended_memory_layernorm = nn.LayerNorm([self.mem_slots_plus_input, self.mem_size])\n        self.attended_memory_layernorm2 = nn.LayerNorm([self.mem_slots_plus_input, self.mem_size])\n\n        ########## parameters for initial embedded input projection ##########\n        self.input_size = input_size\n        self.input_projector = nn.Linear(self.input_size, self.mem_size)\n\n        ########## parameters for gating ##########\n        self.num_gates = 2 * self.calculate_gate_size()\n        self.input_gate_projector = nn.Linear(self.mem_size, self.num_gates)\n        self.memory_gate_projector = nn.Linear(self.mem_size, self.num_gates)\n        # trainable scalar gate bias tensors\n        self.forget_bias = nn.Parameter(torch.tensor(forget_bias, dtype=torch.float32))\n        self.input_bias = nn.Parameter(torch.tensor(input_bias, dtype=torch.float32))\n\n        ########## parameters for token-to-embed & output-to-token logit for softmax\n        self.dropout = nn.Dropout()\n        self.num_tokens = num_tokens\n        self.token_to_input_encoder = nn.Embedding(self.num_tokens, self.input_size)\n\n        # needs 2 linear layers for tying weights for embedding layers\n        # first match the ""output"" of the RMC to input_size, which is the embed dim\n        self.output_to_embed_decoder = nn.Linear(self.mem_slots * self.mem_size, self.input_size)\n        self.use_adaptive_softmax = use_adaptive_softmax\n        if not self.use_adaptive_softmax:\n            # then, this layer\'s weight can be tied to the embedding layer\n            self.embed_to_logit_decoder = nn.Linear(self.input_size, self.num_tokens)\n\n            # tie embedding weights of encoder & decoder\n            self.embed_to_logit_decoder.weight = self.token_to_input_encoder.weight\n\n            ########## loss function\n            self.criterion = nn.CrossEntropyLoss()\n        else:\n            # use adaptive softmax from the self.input_size logits, instead of the tied embed weights above\n            self.criterion_adaptive = nn.AdaptiveLogSoftmaxWithLoss(self.input_size, self.num_tokens,\n                                                                    cutoffs=cutoffs)\n\n    def repackage_hidden(self, h):\n        """"""Wraps hidden states in new Tensors, to detach them from their history.""""""\n        # needed for truncated BPTT, called at every batch forward pass\n        if isinstance(h, torch.Tensor):\n            return h.detach()\n        else:\n            return tuple(self.repackage_hidden(v) for v in h)\n\n    def initial_state(self, batch_size, trainable=False):\n        """"""\n        Creates the initial memory.\n        We should ensure each row of the memory is initialized to be unique,\n        so initialize the matrix to be the identity. We then pad or truncate\n        as necessary so that init_state is of size\n        (batch_size, self.mem_slots, self.mem_size).\n        Args:\n          batch_size: The size of the batch.\n          trainable: Whether the initial state is trainable. This is always True.\n        Returns:\n          init_state: A truncated or padded matrix of size\n            (batch_size, self.mem_slots, self.mem_size).\n        """"""\n        init_state = torch.stack([torch.eye(self.mem_slots) for _ in range(batch_size)])\n\n        # pad the matrix with zeros\n        if self.mem_size > self.mem_slots:\n            difference = self.mem_size - self.mem_slots\n            pad = torch.zeros((batch_size, self.mem_slots, difference))\n            init_state = torch.cat([init_state, pad], -1)\n\n        # truncation. take the first \'self.mem_size\' components\n        elif self.mem_size < self.mem_slots:\n            init_state = init_state[:, :, :self.mem_size]\n\n        return init_state\n\n    def multihead_attention(self, memory):\n        """"""\n        Perform multi-head attention from \'Attention is All You Need\'.\n        Implementation of the attention mechanism from\n        https://arxiv.org/abs/1706.03762.\n        Args:\n          memory: Memory tensor to perform attention on.\n        Returns:\n          new_memory: New memory tensor.\n        """"""\n\n        # First, a simple linear projection is used to construct queries\n        qkv = self.qkv_projector(memory)\n        # apply layernorm for every dim except the batch dim\n        qkv = self.qkv_layernorm(qkv)\n\n        # mem_slots needs to be dynamically computed since mem_slots got concatenated with inputs\n        # example: self.mem_slots=10 and seq_length is 3, and then mem_slots is 10 + 1 = 11 for each 3 step forward pass\n        # this is the same as self.mem_slots_plus_input, but defined to keep the sonnet implementation code style\n        mem_slots = memory.shape[1]  # denoted as N\n\n        # split the qkv to multiple heads H\n        # [B, N, F] => [B, N, H, F/H]\n        qkv_reshape = qkv.view(qkv.shape[0], mem_slots, self.num_heads, self.qkv_size)\n\n        # [B, N, H, F/H] => [B, H, N, F/H]\n        qkv_transpose = qkv_reshape.permute(0, 2, 1, 3)\n\n        # [B, H, N, key_size], [B, H, N, key_size], [B, H, N, value_size]\n        q, k, v = torch.split(qkv_transpose, [self.key_size, self.key_size, self.value_size], -1)\n\n        # scale q with d_k, the dimensionality of the key vectors\n        q *= (self.key_size ** -0.5)\n\n        # make it [B, H, N, N]\n        dot_product = torch.matmul(q, k.permute(0, 1, 3, 2))\n        weights = F.softmax(dot_product, dim=-1)\n\n        # output is [B, H, N, V]\n        output = torch.matmul(weights, v)\n\n        # [B, H, N, V] => [B, N, H, V] => [B, N, H*V]\n        output_transpose = output.permute(0, 2, 1, 3).contiguous()\n        new_memory = output_transpose.view((output_transpose.shape[0], output_transpose.shape[1], -1))\n\n        return new_memory\n\n    @property\n    def state_size(self):\n        return [self.mem_slots, self.mem_size]\n\n    @property\n    def output_size(self):\n        return self.mem_slots * self.mem_size\n\n    def calculate_gate_size(self):\n        """"""\n        Calculate the gate size from the gate_style.\n        Returns:\n          The per sample, per head parameter size of each gate.\n        """"""\n        if self.gate_style == \'unit\':\n            return self.mem_size\n        elif self.gate_style == \'memory\':\n            return 1\n        else:  # self.gate_style == None\n            return 0\n\n    def create_gates(self, inputs, memory):\n        """"""\n        Create input and forget gates for this step using `inputs` and `memory`.\n        Args:\n          inputs: Tensor input.\n          memory: The current state of memory.\n        Returns:\n          input_gate: A LSTM-like insert gate.\n          forget_gate: A LSTM-like forget gate.\n        """"""\n        # We\'ll create the input and forget gates at once. Hence, calculate double\n        # the gate size.\n\n        # equation 8: since there is no output gate, h is just a tanh\'ed m\n        memory = torch.tanh(memory)\n\n        # TODO: check this input flattening is correct\n        # sonnet uses this, but i think it assumes time step of 1 for all cases\n        # if inputs is (B, T, features) where T > 1, this gets incorrect\n        # inputs = inputs.view(inputs.shape[0], -1)\n\n        # fixed implementation\n        if len(inputs.shape) == 3:\n            if inputs.shape[1] > 1:\n                raise ValueError(\n                    ""input seq length is larger than 1. create_gate function is meant to be called for each step, with input seq length of 1"")\n            inputs = inputs.view(inputs.shape[0], -1)\n            # matmul for equation 4 and 5\n            # there is no output gate, so equation 6 is not implemented\n            gate_inputs = self.input_gate_projector(inputs)\n            gate_inputs = gate_inputs.unsqueeze(dim=1)\n            gate_memory = self.memory_gate_projector(memory)\n        else:\n            raise ValueError(""input shape of create_gate function is 2, expects 3"")\n\n        # this completes the equation 4 and 5\n        gates = gate_memory + gate_inputs\n        gates = torch.split(gates, split_size_or_sections=int(gates.shape[2] / 2), dim=2)\n        input_gate, forget_gate = gates\n        assert input_gate.shape[2] == forget_gate.shape[2]\n\n        # to be used for equation 7\n        input_gate = torch.sigmoid(input_gate + self.input_bias)\n        forget_gate = torch.sigmoid(forget_gate + self.forget_bias)\n\n        return input_gate, forget_gate\n\n    def attend_over_memory(self, memory):\n        """"""\n        Perform multiheaded attention over `memory`.\n            Args:\n              memory: Current relational memory.\n            Returns:\n              The attended-over memory.\n        """"""\n        for _ in range(self.num_blocks):\n            attended_memory = self.multihead_attention(memory)\n\n            # Add a skip connection to the multiheaded attention\'s input.\n            memory = self.attended_memory_layernorm(memory + attended_memory)\n\n            # add a skip connection to the attention_mlp\'s input.\n            attention_mlp = memory\n            for i, l in enumerate(self.attention_mlp):\n                attention_mlp = self.attention_mlp[i](attention_mlp)\n                attention_mlp = F.relu(attention_mlp)\n            memory = self.attended_memory_layernorm2(memory + attention_mlp)\n\n        return memory\n\n    def forward_step(self, inputs, memory, treat_input_as_matrix=False):\n        """"""\n        Forward step of the relational memory core.\n        Args:\n          inputs: Tensor input.\n          memory: Memory output from the previous time step.\n          treat_input_as_matrix: Optional, whether to treat `input` as a sequence\n            of matrices. Default to False, in which case the input is flattened\n            into a vector.\n        Returns:\n          output: This time step\'s output.\n          next_memory: The next version of memory to use.\n        """"""\n\n        # first embed the tokens into vectors\n        inputs_embed = self.dropout(self.token_to_input_encoder(inputs))\n\n        if treat_input_as_matrix:\n            # keep (Batch, Seq, ...) dim (0, 1), flatten starting from dim 2\n            inputs_embed = inputs_embed.view(inputs_embed.shape[0], inputs_embed.shape[1], -1)\n            # apply linear layer for dim 2\n            inputs_reshape = self.input_projector(inputs_embed)\n        else:\n            # keep (Batch, ...) dim (0), flatten starting from dim 1\n            inputs_embed = inputs_embed.view(inputs_embed.shape[0], -1)\n            # apply linear layer for dim 1\n            inputs_embed = self.input_projector(inputs_embed)\n            # unsqueeze the time step to dim 1\n            inputs_reshape = inputs_embed.unsqueeze(dim=1)\n\n        memory_plus_input = torch.cat([memory, inputs_reshape], dim=1)\n        next_memory = self.attend_over_memory(memory_plus_input)\n\n        # cut out the concatenated input vectors from the original memory slots\n        n = inputs_reshape.shape[1]\n        next_memory = next_memory[:, :-n, :]\n\n        if self.gate_style == \'unit\' or self.gate_style == \'memory\':\n            # these gates are sigmoid-applied ones for equation 7\n            input_gate, forget_gate = self.create_gates(inputs_reshape, memory)\n            # equation 7 calculation\n            next_memory = input_gate * torch.tanh(next_memory)\n            next_memory += forget_gate * memory\n\n        output = next_memory.view(next_memory.shape[0], -1)\n\n        # decode output to logit\n        output_embed = self.output_to_embed_decoder(output)\n        # TODO: this dropout is not mentioned in the paper. it\'s to match word-language-model dropout use case\n        output_embed = self.dropout(output_embed)\n\n        if not self.use_adaptive_softmax:\n            logit = self.embed_to_logit_decoder(output_embed)\n        else:\n            logit = output_embed\n\n        return logit, next_memory\n\n    def forward(self, inputs, memory, targets, require_logits=False):\n        # Starting each batch, we detach the hidden state from how it was previously produced.\n        # If we didn\'t, the model would try backpropagating all the way to start of the dataset.\n        memory = self.repackage_hidden(memory)\n\n        # for loop implementation of (entire) recurrent forward pass of the model\n        # inputs is batch first [batch, seq], and output logit per step is [batch, vocab]\n        # so the concatenated logits are [seq * batch, vocab]\n\n        # targets are flattened [seq, batch] => [seq * batch], so the dimension is correct\n\n        logits = []\n        # shape[1] is seq_lenth T\n        for idx_step in range(inputs.shape[1]):\n            logit, memory = self.forward_step(inputs[:, idx_step], memory)\n            logits.append(logit)\n        # concat the output from list(seq_length) of [batch, vocab] to [seq * batch, vocab]\n        logits = torch.cat(logits)\n\n        if targets is not None:\n            if not self.use_adaptive_softmax:\n                # calculate loss inside this forward pass for more even VRAM usage of DataParallel\n                loss = self.criterion(logits, targets)\n            else:\n                # calculate the loss using adaptive softmax\n                _, loss = self.criterion_adaptive(logits, targets)\n        else:\n            loss = None\n\n        # the forward pass only returns loss, because returning logits causes uneven VRAM usage of DataParallel\n        # logits are provided only for sampling stage\n        if not require_logits:\n            return loss, memory\n        else:\n            return logits, loss, memory\n\n\n\n# ########## DEBUG: unit test code ##########\n# input_size = 44\n# seq_length = 1\n# batch_size = 32\n# model = RelationalMemory(mem_slots=10, head_size=20, input_size=input_size, num_tokens=66, num_heads=8, num_blocks=1, forget_bias=1., input_bias=0.)\n# model_memory = model.initial_state(batch_size=batch_size)\n#\n# # random input\n# random_input = torch.randn((32, seq_length, input_size))\n# # random targets\n# random_targets = torch.randn((32, seq_length, input_size))\n#\n# # take a one step forward\n# logit, next_memory = model(random_input, model_memory, random_targets, treat_input_as_matrix=True)\n'"
rnn_models.py,4,"b'import torch.nn as nn\nimport torch\n\n\nclass RNNModel(nn.Module):\n    """"""Container module with an encoder, a recurrent module, and a decoder.""""""\n\n    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False, use_cudnn_version=True,\n                 use_adaptive_softmax=False, cutoffs=None):\n        super(RNNModel, self).__init__()\n        self.use_cudnn_version = use_cudnn_version\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        if use_cudnn_version:\n            if rnn_type in [\'LSTM\', \'GRU\']:\n                self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n            else:\n                try:\n                    nonlinearity = {\'RNN_TANH\': \'tanh\', \'RNN_RELU\': \'relu\'}[rnn_type]\n                except KeyError:\n                    raise ValueError(""""""An invalid option for `--model` was supplied,\n                                     options are [\'LSTM\', \'GRU\', \'RNN_TANH\' or \'RNN_RELU\']"""""")\n                self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n        else:\n            if rnn_type in [\'LSTM\', \'GRU\']:\n                rnn_type = str(rnn_type) + \'Cell\'\n                rnn_modulelist = []\n                for i in range(nlayers):\n                    rnn_modulelist.append(getattr(nn, rnn_type)(ninp, nhid))\n                    if i < nlayers - 1:\n                        rnn_modulelist.append(nn.Dropout(dropout))\n                self.rnn = nn.ModuleList(rnn_modulelist)\n            else:\n                raise ValueError(""non-cudnn version of (RNNCell) is not implemented. use LSTM or GRU instead"")\n\n        if not use_adaptive_softmax:\n            self.use_adaptive_softmax = use_adaptive_softmax\n            self.decoder = nn.Linear(nhid, ntoken)\n            # Optionally tie weights as in:\n            # ""Using the Output Embedding to Improve Language Models"" (Press & Wolf 2016)\n            # https://arxiv.org/abs/1608.05859\n            # and\n            # ""Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling"" (Inan et al. 2016)\n            # https://arxiv.org/abs/1611.01462\n            if tie_weights:\n                if nhid != ninp:\n                    raise ValueError(\'When using the tied flag, nhid must be equal to emsize\')\n                self.decoder.weight = self.encoder.weight\n        else:\n            # simple linear layer of nhid output size. used for adaptive softmax after\n            # directly applying softmax at the hidden states is a bad idea\n            self.decoder_adaptive = nn.Linear(nhid, nhid)\n            self.use_adaptive_softmax = use_adaptive_softmax\n            self.cutoffs = cutoffs\n            if tie_weights:\n                print(""Warning: if using adaptive softmax, tie_weights cannot be applied. Ignored."")\n\n        self.init_weights()\n\n        self.rnn_type = rnn_type\n        self.nhid = nhid\n        self.nlayers = nlayers\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        if not self.use_adaptive_softmax:\n            self.decoder.bias.data.zero_()\n            self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, input, hidden):\n        emb = self.drop(self.encoder(input))\n        if self.use_cudnn_version:\n            output, hidden = self.rnn(emb, hidden)\n        else:\n            # for loop implementation with RNNCell\n            layer_input = emb\n            new_hidden = [[], []]\n            for idx_layer in range(0, self.nlayers + 1, 2):\n                output = []\n                hx, cx = hidden[0][int(idx_layer / 2)], hidden[1][int(idx_layer / 2)]\n                for idx_step in range(input.shape[0]):\n                    hx, cx = self.rnn[idx_layer](layer_input[idx_step], (hx, cx))\n                    output.append(hx)\n                output = torch.stack(output)\n                if idx_layer + 1 < self.nlayers:\n                    output = self.rnn[idx_layer + 1](output)\n                layer_input = output\n                new_hidden[0].append(hx)\n                new_hidden[1].append(cx)\n            new_hidden[0] = torch.stack(new_hidden[0])\n            new_hidden[1] = torch.stack(new_hidden[1])\n            hidden = tuple(new_hidden)\n\n        output = self.drop(output)\n\n        if not self.use_adaptive_softmax:\n            decoded = self.decoder(output.view(output.size(0) * output.size(1), output.size(2)))\n            return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n        else:\n            decoded = self.decoder_adaptive(output.view(output.size(0) * output.size(1), output.size(2)))\n            return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n\n    def init_hidden(self, bsz):\n        weight = next(self.parameters())\n        if self.rnn_type == \'LSTM\' or self.rnn_type == \'LSTMCell\':\n            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n        else:\n            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n'"
train_embeddings.py,14,"b'""""""\nTemplate to use Relational RNN module\nto predict a scalar from a sequence of embeddings,\ne.g. a sentence.\n\nInput: fixed-length sequence of `num_words` words,\neach represented by a `num_embedding_dims` dimensional embedding.\n\nOutput: A scalar.\n\nAuthor: Jessica Yung\nAugust 2018\n\nRelational Memory Core implementation mostly written by Sang-gil Lee, adapted by Jessica Yung.\n""""""\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom relational_rnn_general import RelationalMemory\n\n# network params\nlearning_rate = 1e-3\nnum_epochs = 50\n# dtype = torch.float\n\n# data params\n# Input = seq of `num_words` words, embedding for each word has `num_embedding_dims` dims\nnum_words = 10\nnum_embedding_dims = 5\ninput_size = num_embedding_dims\n# Predicting a scalar\noutput_size = 1\n\nnum_examples = 20\ntest_size = 0.2\nnum_train = int((1 - test_size) * num_examples)\nbatch_size = 4\n\n####################\n# Generate data\n####################\n\nX = torch.rand((num_examples, num_words, num_embedding_dims))\n# Predicting a scalar per example\ny = torch.rand((num_examples, output_size))\n\nX_train = X[:num_train]\nX_test = X[num_train:]\ny_train = y[:num_train]\ny_test = y[num_train:]\n\n\nclass RMCArguments:\n    def __init__(self):\n        self.memslots = 1\n        self.headsize = 3\n        self.numheads = 4\n        self.input_size = input_size  # dimensions per timestep\n        self.numheads = 4\n        self.numblocks = 1\n        self.forgetbias = 1.\n        self.inputbias = 0.\n        self.attmlplayers = 3\n        self.batch_size = batch_size\n        self.clip = 0.1\n\n\nargs = RMCArguments()\n\ndevice = torch.device(""cpu"")\n\n\n####################\n# Build model\n####################\n\nclass RRNN(nn.Module):\n    def __init__(self, batch_size):\n        super(RRNN, self).__init__()\n        self.memory_size_per_row = args.headsize * args.numheads\n        self.relational_memory = RelationalMemory(mem_slots=args.memslots, head_size=args.headsize,\n                                                  input_size=args.input_size,\n                                                  num_heads=args.numheads, num_blocks=args.numblocks,\n                                                  forget_bias=args.forgetbias,\n                                                  input_bias=args.inputbias)\n        # Map from memory to logits (categorical predictions)\n        self.out = nn.Linear(self.memory_size_per_row, output_size)\n\n    def forward(self, input, memory):\n        logit, memory = self.relational_memory(input, memory)\n        out = self.out(logit)\n\n        return out, memory\n\n\nmodel = RRNN(batch_size).to(device)\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(""Model built, total trainable params: "" + str(total_params))\n\n\ndef get_batch(X, y, batch_num, device, batch_size=32, batch_first=True):\n    if not batch_first:\n        raise NotImplementedError\n    start = batch_num * batch_size\n    end = (batch_num + 1) * batch_size\n    return X[start:end].to(device), y[start:end].to(device)\n\n\nloss_fn = torch.nn.MSELoss()\n\noptimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, \'min\', factor=0.5, patience=5)\n\nnum_batches = int(len(X_train) / batch_size)\nnum_test_batches = int(len(X_test) / batch_size)\n\nmemory = model.relational_memory.initial_state(args.batch_size, trainable=True).to(device)\n\nhist = np.zeros(num_epochs)\n\n\ndef accuracy_score(y_pred, y_true):\n    return np.array(y_pred == y_true).sum() * 1.0 / len(y_true)\n\n\n####################\n# Train model\n####################\n\nfor t in range(num_epochs):\n    epoch_loss = np.zeros(num_batches)\n    # epoch_acc = np.zeros(num_batches)\n    epoch_test_loss = np.zeros(num_test_batches)\n    # epoch_test_acc = np.zeros(num_test_batches)\n    for i in range(num_batches):\n        data, targets = get_batch(X_train, y_train, i, device=device, batch_size=batch_size)\n        model.zero_grad()\n\n        # forward pass\n        # replace ""_"" with ""memory"" if you want to make the RNN stateful\n        y_pred, memory = model(data, memory)\n\n        loss = loss_fn(y_pred, targets)\n        loss = torch.mean(loss)\n        # y_pred = torch.argmax(y_pred, dim=1)\n        # acc = accuracy_score(y_pred, targets)\n        epoch_loss[i] = loss\n        # epoch_acc[i] = acc\n\n        # Zero out gradient, else they will accumulate between epochs\n        optimiser.zero_grad()\n\n        # backward pass\n        loss.backward()\n\n        # torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n\n        # update parameters\n        optimiser.step()\n\n    # test examples\n    hist[t] = np.mean(epoch_loss).item()\n    if t % 10 == 0:\n        print(""train: "", y_pred.squeeze().detach().cpu().numpy(), targets.squeeze().detach().cpu().numpy())\n    for i in range(num_test_batches):\n        with torch.no_grad():\n            data, targets = get_batch(X_test, y_test, i, device=device, batch_size=batch_size)\n            ytest_pred, memory = model(data, memory)\n\n            test_loss = loss_fn(ytest_pred, targets)\n            test_loss = torch.mean(test_loss)\n            # ytest_pred = torch.argmax(ytest_pred, dim=1)\n            # test_acc = accuracy_score(ytest_pred, targets)\n            epoch_test_loss[i] = loss\n            # epoch_test_acc[i] = acc\n\n    if t % 10 == 0:\n        # print(epoch_test_loss)\n        # print(epoch_test_acc)\n        print(""Epoch {} train loss: {}"".format(t, np.mean(epoch_test_loss).item()))\n        print(""Epoch {} test  loss: {}"".format(t, np.mean(epoch_test_loss).item()))\n        # print(""Epoch {} train  acc: {:.2f}"".format(t, np.mean(epoch_acc).item()))\n        # print(""Epoch {} test   acc: {:.2f}"".format(t, np.mean(epoch_test_acc).item()))\n        print(""test: "", ytest_pred.squeeze().detach().cpu().numpy(), targets.squeeze().detach().cpu().numpy())\n\n####################\n# Plot losses\n####################\n\nplt.plot(hist, label=""Training loss"")\nplt.legend()\nplt.show()\n\n""""""\n# TODO: visualise preds\nplt.plot(y_pred.detach().numpy(), label=""Preds"")\nplt.plot(y_train.detach().numpy(), label=""Data"")\nplt.legend()\nplt.show()\n""""""\n'"
train_nth_farthest.py,15,"b'""""""\nImplementation of \'Nth Farthest\' task\nas defined in Santoro, Faulkner and Raposo et. al., 2018\n(Relational recurrent neural networks, https://arxiv.org/abs/1806.01822)\n\nNote: The training data is re-generated each epoch as in the\nSonnet implementation. This avoids overfitting but means that the\nexperiments may take longer.\n\nAuthor: Jessica Yung\nAugust 2018\n\nRelational Memory Core implementation mostly written by Sang-gil Lee, adapted by Jessica Yung.\n""""""\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom argparse import ArgumentParser\n\nfrom relational_rnn_general import RelationalMemory\n\nparser = ArgumentParser()\n\n# Model parameters.\nparser.add_argument(\'--cuda\', action=\'store_true\',\n                    help=\'use CUDA\')\n\nparse_args = parser.parse_args()\n\nif torch.cuda.is_available():\n    if not parse_args.cuda:\n        print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n\ndevice = torch.device(""cuda"" if parse_args.cuda else ""cpu"")\n\n# network params\nlearning_rate = 1e-4\nnum_epochs = 10000000\ndtype = torch.float\nmlp_size = 256\n\n# data params\nnum_vectors = 8\nnum_dims = 16\nbatch_size = 1600\nnum_batches = 6  # set batches per epoch because we are generating data from scratch each time\nnum_test_examples = 3200\n\n####################\n# Generate data\n####################\n\n# For each example\ninput_size = num_dims + num_vectors * 3\n\n\ndef one_hot_encode(array, num_dims=8):\n    one_hot = np.zeros((len(array), num_dims))\n    for i in range(len(array)):\n        one_hot[i, array[i]] = 1\n    return one_hot\n\n\ndef get_example(num_vectors, num_dims):\n    input_size = num_dims + num_vectors * 3\n    n = np.random.choice(num_vectors, 1)  # nth farthest from target vector\n    labels = np.random.choice(num_vectors, num_vectors, replace=False)\n    m_index = np.random.choice(num_vectors, 1)  # m comes after the m_index-th vector\n    m = labels[m_index]\n\n    # Vectors sampled from U(-1,1)\n    vectors = np.random.rand(num_vectors, num_dims) * 2 - 1\n    target_vector = vectors[m_index]\n    dist_from_target = np.linalg.norm(vectors - target_vector, axis=1)\n    X_single = np.zeros((num_vectors, input_size))\n    X_single[:, :num_dims] = vectors\n    labels_onehot = one_hot_encode(labels, num_dims=num_vectors)\n    X_single[:, num_dims:num_dims + num_vectors] = labels_onehot\n    nm_onehot = np.reshape(one_hot_encode([n, m], num_dims=num_vectors), -1)\n    X_single[:, num_dims + num_vectors:] = np.tile(nm_onehot, (num_vectors, 1))\n    y_single = labels[np.argsort(dist_from_target)[-(n + 1)]]\n\n    return X_single, y_single\n\n\ndef get_examples(num_examples, num_vectors, num_dims, device):\n    X = np.zeros((num_examples, num_vectors, input_size))\n    y = np.zeros(num_examples)\n    for i in range(num_examples):\n        X_single, y_single = get_example(num_vectors, num_dims)\n        X[i, :] = X_single\n        y[i] = y_single\n\n    X = torch.Tensor(X).to(device)\n    y = torch.LongTensor(y).to(device)\n\n    return X, y\n\n\nX_test, y_test = get_examples(num_test_examples, num_vectors, num_dims, device)\n\n\nclass RMCArguments:\n    def __init__(self):\n        self.memslots = 8\n        self.numheads = 8\n        self.headsize = int(2048 / (self.numheads * self.memslots))\n        self.input_size = input_size  # dimensions per timestep\n        self.numblocks = 1\n        self.forgetbias = 1.\n        self.inputbias = 0.\n        self.attmlplayers = 2\n        self.batch_size = batch_size\n        self.clip = 0.1\n\n\nargs = RMCArguments()\n\n\n####################\n# Build model\n####################\n\nclass RRNN(nn.Module):\n    def __init__(self, mlp_size):\n        super(RRNN, self).__init__()\n        self.mlp_size = mlp_size\n        self.memory_size_per_row = args.headsize * args.numheads * args.memslots\n        self.relational_memory = RelationalMemory(mem_slots=args.memslots, head_size=args.headsize,\n                                                  input_size=args.input_size,\n                                                  num_heads=args.numheads, num_blocks=args.numblocks,\n                                                  forget_bias=args.forgetbias, input_bias=args.inputbias)\n        # Map from memory to logits (categorical predictions)\n        self.mlp = nn.Sequential(\n            nn.Linear(self.memory_size_per_row, self.mlp_size),\n            nn.ReLU(),\n            nn.Linear(self.mlp_size, self.mlp_size),\n            nn.ReLU(),\n            nn.Linear(self.mlp_size, self.mlp_size),\n            nn.ReLU(),\n            nn.Linear(self.mlp_size, self.mlp_size),\n            nn.ReLU()\n        )\n        self.out = nn.Linear(self.mlp_size, num_vectors)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, input, memory):\n        logit, memory = self.relational_memory(input, memory)\n        mlp = self.mlp(logit)\n        out = self.out(mlp)\n        out = self.softmax(out)\n\n        return out, memory\n\n\nmodel = RRNN(mlp_size).to(device)\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(""Model built, total trainable params: "" + str(total_params))\n\n\ndef get_batch(X, y, batch_num, batch_size=32, batch_first=True):\n    if not batch_first:\n        raise NotImplementedError\n    start = batch_num * batch_size\n    end = (batch_num + 1) * batch_size\n    return X[start:end], y[start:end]\n\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\noptimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, \'min\', factor=0.5, patience=5, min_lr=8e-5)\n\n# num_batches = int(len(X_train) / batch_size)\nnum_test_batches = int(len(X_test) / batch_size)\n\nmemory = model.relational_memory.initial_state(args.batch_size, trainable=True).to(device)\n\nhist = np.zeros(num_epochs)\nhist_acc = np.zeros(num_epochs)\ntest_hist = np.zeros(num_epochs)\ntest_hist_acc = np.zeros(num_epochs)\n\n\ndef accuracy_score(y_pred, y_true):\n    return np.array(y_pred == y_true).sum() * 1.0 / len(y_true)\n\n\n####################\n# Train model\n####################\n\nfor t in range(num_epochs):\n    epoch_loss = np.zeros(num_batches)\n    epoch_acc = np.zeros(num_batches)\n    epoch_test_loss = np.zeros(num_test_batches)\n    epoch_test_acc = np.zeros(num_test_batches)\n    for i in range(num_batches):\n        data, targets = get_examples(batch_size, num_vectors, num_dims, device)\n        model.zero_grad()\n\n        # forward pass\n        # replace ""_"" with ""memory"" if you want to make the RNN stateful\n        y_pred, _ = model(data, memory)\n\n        loss = loss_fn(y_pred, targets)\n        loss = torch.mean(loss)\n        y_pred = torch.argmax(y_pred, dim=1)\n        acc = accuracy_score(y_pred, targets)\n        epoch_loss[i] = loss\n        epoch_acc[i] = acc\n\n        # Zero out gradient, else they will accumulate between epochs\n        optimiser.zero_grad()\n\n        # backward pass\n        loss.backward()\n\n        # this helps prevent exploding gradient in RNNs\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n\n        # update parameters\n        optimiser.step()\n\n    # test examples\n    for i in range(num_test_batches):\n        with torch.no_grad():\n            data, targets = get_batch(X_test, y_test, i, batch_size=batch_size)\n            ytest_pred, _ = model(data, memory)\n\n            test_loss = loss_fn(ytest_pred, targets)\n            test_loss = torch.mean(test_loss)\n            ytest_pred = torch.argmax(ytest_pred, dim=1)\n            test_acc = accuracy_score(ytest_pred, targets)\n            epoch_test_loss[i] = test_loss\n            epoch_test_acc[i] = test_acc\n\n    loss = np.mean(epoch_loss)\n    acc = np.mean(epoch_acc)\n    test_loss = np.mean(epoch_test_loss)\n    test_acc = np.mean(epoch_test_acc)\n\n    hist[t] = loss\n    hist_acc[t] = acc\n    test_hist[t] = test_loss\n    test_hist_acc[t] = test_acc\n\n    if t % 10 == 0:\n        print(""Epoch {} train loss: {}"".format(t, loss))\n        print(""Epoch {} test  loss: {}"".format(t, test_loss))\n        print(""Epoch {} train  acc: {:.2f}"".format(t, acc))\n        print(""Epoch {} test   acc: {:.2f}"".format(t, test_acc))\n\n####################\n# Plot losses\n####################\n\nplt.plot(hist, label=""Training loss"")\nplt.plot(test_hist, label=""Test loss"")\nplt.legend()\nplt.title(""Cross entropy loss"")\nplt.show()\n\n# Plot accuracy\nplt.plot(hist_acc, label=""Training accuracy"")\nplt.plot(test_hist_acc, label=""Test accuracy"")\nplt.title(""Accuracy"")\nplt.legend()\nplt.show()\n'"
train_rmc.py,22,"b'# copypasta from main.py of pytorch word_language_model code\n# coding: utf-8\nimport argparse\nimport time\nimport math\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.onnx\nimport datetime\nimport shutil\nimport pickle\nimport data\nfrom relational_rnn_models import RelationalMemory\n\n# is it faster?\ntorch.backends.cudnn.benchmark = True\n\nparser = argparse.ArgumentParser(description=\'PyTorch Wikitext-2 RNN/LSTM Language Model\')\n# hyperparams for text data\nparser.add_argument(\'--data\', type=str, default=\'./data/wikitext-2\',\n                    help=\'location of the data corpus\')\nparser.add_argument(\'--emsize\', type=int, default=192,\n                    help=\'size of word embeddings\')\n\n# NEW!: hyperparams for relational memory core (RMC)\nparser.add_argument(\'--memslots\', type=int, default=1,\n                    help=\'number of memory slots of the relational memory core\')\nparser.add_argument(\'--headsize\', type=int, default=192,\n                    help=\'size of the each head for multihead attention\')\nparser.add_argument(\'--numheads\', type=int, default=4,\n                    help=\'total number of heads for multihead attention\')\nparser.add_argument(\'--numblocks\', type=int, default=1,\n                    help=\'Number of times to compute attention per time step\')\nparser.add_argument(\'--forgetbias\', type=float, default=1.,\n                    help=\'Bias to use for the forget gate, assuming we are using some form of gating\')\nparser.add_argument(\'--inputbias\', type=float, default=0.,\n                    help=\'Bias to use for the input gate, assuming we are using some form of gating\')\nparser.add_argument(\'--gatestyle\', type=str, default=\'unit\',\n                    help=\'Whether to use per-element gating (\\\'unit\\\'), per-memory slot gating (\\\'memory\\\'), or no gating at all (None).\')\nparser.add_argument(\'--attmlplayers\', type=int, default=3,\n                    help=\'Number of layers to use in the post-attention MLP\')\nparser.add_argument(\'--keysize\', type=int, default=64,\n                    help=\'Size of vector to use for key & query vectors in the attention\'\n                         \'computation. Defaults to None, in which case we use `head_size`\')\n# parameters for adaptive softmax\nparser.add_argument(\'--adaptivesoftmax\', action=\'store_true\',\n                    help=\'use adaptive softmax during hidden state to output logits.\'\n                         \'it uses less memory by approximating softmax of large vocabulary.\')\nparser.add_argument(\'--cutoffs\', nargs=""*"", type=int, default=[10000, 50000, 100000],\n                    help=\'cutoff values for adaptive softmax. list of integers.\'\n                         \'optimal values are based on word frequencey and vocabulary size of the dataset.\')\n\n# other hyperparams for general RNN mechanics\nparser.add_argument(\'--lr\', type=float, default=0.001,\n                    help=\'initial learning rate\')\nparser.add_argument(\'--clip\', type=float, default=0.1,\n                    help=\'gradient clipping\')\nparser.add_argument(\'--epochs\', type=int, default=100,\n                    help=\'upper epoch limit\')\nparser.add_argument(\'--batch_size\', type=int, default=64, metavar=\'N\',\n                    help=\'batch size\')\nparser.add_argument(\'--bptt\', type=int, default=100,\n                    help=\'sequence length\')\n# dropout of RMC is hard-bound to 0.5 at the embedding layer\n# parser.add_argument(\'--dropout\', type=float, default=0.2,\n#                     help=\'dropout applied to layers (0 = no dropout)\')\n# embed weight tying is set always to true\n# parser.add_argument(\'--tied\', action=\'store_true\',\n#                     help=\'tie the word embedding and softmax weights\')\nparser.add_argument(\'--seed\', type=int, default=1111,\n                    help=\'random seed\')\nparser.add_argument(\'--cuda\', action=\'store_true\',\n                    help=\'use CUDA\')\nparser.add_argument(\'--log-interval\', type=int, default=100, metavar=\'N\',\n                    help=\'report interval\')\nparser.add_argument(\'--onnx-export\', type=str, default=\'\',\n                    help=\'path to export the final model in onnx format\')\nparser.add_argument(\'--resume\', type=int, default=None,\n                    help=\'if specified with the 1-indexed global epoch, loads the checkpoint and resumes training\')\n\n# experiment name for this run\nparser.add_argument(\'--name\', type=str, default=None,\n                    help=\'name for this experiment. generates folder with the name if specified.\')\n\nargs = parser.parse_args()\n\n# Set the random seed manually for reproducibility.\ntorch.manual_seed(args.seed)\n\nif torch.cuda.is_available():\n    if not args.cuda:\n        print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n\ndevice = torch.device(""cuda"" if args.cuda else ""cpu"")\n###############################################################################\n# Load data\n###############################################################################\ncorpus_name = os.path.basename(os.path.normpath(args.data))\ncorpus_filename = \'./data/corpus-\' + str(corpus_name) + str(\'.pkl\')\nif os.path.isfile(corpus_filename):\n    print(""loading pre-built "" + str(corpus_name) + "" corpus file..."")\n    loadfile = open(corpus_filename, \'rb\')\n    corpus = pickle.load(loadfile)\n    loadfile.close()\nelse:\n    print(""building "" + str(corpus_name) + "" corpus..."")\n    corpus = data.Corpus(args.data)\n    # save the corpus for later\n    savefile = open(corpus_filename, \'wb\')\n    pickle.dump(corpus, savefile)\n    savefile.close()\n    print(""corpus saved to pickle"")\n\n\n# Starting from sequential data, batchify arranges the dataset into columns.\n# For instance, with the alphabet as the sequence and batch size 4, we\'d get\n# \xe2\x94\x8c a g m s \xe2\x94\x90\n# \xe2\x94\x82 b h n t \xe2\x94\x82\n# \xe2\x94\x82 c i o u \xe2\x94\x82\n# \xe2\x94\x82 d j p v \xe2\x94\x82\n# \xe2\x94\x82 e k q w \xe2\x94\x82\n# \xe2\x94\x94 f l r x \xe2\x94\x98.\n# These columns are treated as independent by the model, which means that the\n# dependence of e. g. \'g\' on \'f\' can not be learned, but allows more efficient\n# batch processing.\n\ndef batchify(data, bsz):\n    # Work out how cleanly we can divide the dataset into bsz parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn\'t cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    data = data.view(bsz, -1).t().contiguous()\n    return data.to(device)\n\n\neval_batch_size = 32\ntrain_data = batchify(corpus.train, args.batch_size)\nval_data = batchify(corpus.valid, eval_batch_size)\ntest_data = batchify(corpus.test, eval_batch_size)\n\n# create folder for current experiments\n# name: args.name + current time\n# includes: entire scripts for faithful reproduction, train & test logs\nfolder_name = str(datetime.datetime.now())[:-7]\nif args.name is not None:\n    folder_name = str(args.name) + \' \' + folder_name\n\nos.mkdir(folder_name)\nfor file in os.listdir(os.getcwd()):\n    if file.endswith("".py""):\n        shutil.copy2(file, os.path.join(os.getcwd(), folder_name))\nlogger_train = open(os.path.join(os.getcwd(), folder_name, \'train_log.txt\'), \'w+\')\nlogger_test = open(os.path.join(os.getcwd(), folder_name, \'test_log.txt\'), \'w+\')\n\n# save args to logger\nlogger_train.write(str(args) + \'\\n\')\n\n# define saved model file location\nsavepath = os.path.join(os.getcwd(), folder_name)\n\n###############################################################################\n# Build the model\n###############################################################################\n\nntokens = len(corpus.dictionary)\nprint(""vocabulary size (ntokens): "" + str(ntokens))\nif args.adaptivesoftmax:\n    print(""Adaptive Softmax is on: the performance depends on cutoff values. check if the cutoff is properly set"")\n    print(""Cutoffs: "" + str(args.cutoffs))\n    if args.cutoffs[-1] > ntokens:\n        raise ValueError(""the last element of cutoff list must be lower than vocab size of the dataset"")\n\nmodel = RelationalMemory(mem_slots=args.memslots, head_size=args.headsize, input_size=args.emsize, num_tokens=ntokens,\n                         num_heads=args.numheads, num_blocks=args.numblocks, forget_bias=args.forgetbias,\n                         input_bias=args.inputbias, attention_mlp_layers=args.attmlplayers, key_size=args.keysize,\n                         use_adaptive_softmax=args.adaptivesoftmax, cutoffs=args.cutoffs).to(device)\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nmodel = nn.DataParallel(model)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \'min\', factor=0.5, patience=5)\n\n###############################################################################\n# Load the model checkpoint if specified and restore the global & best epoch\n###############################################################################\nif args.resume is not None:\n    print(""--resume detected. loading checkpoint..."")\nglobal_epoch = args.resume if args.resume is not None else 0\nbest_epoch = args.resume if args.resume is not None else 0\nif args.resume is not None:\n    loadpath = os.path.join(os.getcwd(), ""model_{}.pt"".format(args.resume))\n    if not os.path.isfile(loadpath):\n        raise FileNotFoundError(\n            ""model_{}.pt not found. place the model checkpoint file to the current working directory."".format(\n                args.resume))\n    checkpoint = torch.load(loadpath)\n    model.load_state_dict(checkpoint[""state_dict""])\n    optimizer.load_state_dict(checkpoint[""optimizer""])\n    scheduler.load_state_dict(checkpoint[""scheduler""])\n    global_epoch = checkpoint[""global_epoch""]\n    best_epoch = checkpoint[""best_epoch""]\n\nprint(""model built, total trainable params: "" + str(total_params))\n\n\n###############################################################################\n# Training code\n###############################################################################\n\n# get_batch subdivides the source data into chunks of length args.bptt.\n# If source is equal to the example output of the batchify function, with\n# a bptt-limit of 2, we\'d get the following two Variables for i = 0:\n# \xe2\x94\x8c a g m s \xe2\x94\x90 \xe2\x94\x8c b h n t \xe2\x94\x90\n# \xe2\x94\x94 b h n t \xe2\x94\x98 \xe2\x94\x94 c i o u \xe2\x94\x98\n# Note that despite the name of the function, the subdivison of data is not\n# done along the batch dimension (i.e. dimension 1), since that was handled\n# by the batchify function. The chunks are along dimension 0, corresponding\n# to the seq_len dimension in the LSTM.\n\n\ndef get_batch(source, i):\n    seq_len = min(args.bptt, len(source) - 1 - i)\n    data = source[i:i + seq_len]\n    target = source[i + 1:i + 1 + seq_len].view(-1)\n    return data, target\n\n\ndef evaluate(data_source):\n    # Turn on evaluation mode which disables dropout.\n    model.eval()\n    total_loss = 0.\n    ntokens = len(corpus.dictionary)\n    memory = model.module.initial_state(eval_batch_size, trainable=False).to(device)\n\n    with torch.no_grad():\n        for i in range(0, data_source.size(0) - 1, args.bptt):\n            data, targets = get_batch(data_source, i)\n            data = torch.t(data)\n\n            loss, memory = model(data, memory, targets)\n            loss = torch.mean(loss)\n\n            # data has shape [T * B, N]\n            total_loss += args.bptt * loss.item()\n\n    return total_loss / len(data_source)\n\n\ndef train():\n    # Turn on training mode which enables dropout.\n    model.train()\n    total_loss = 0.\n    forward_elapsed_time = 0.\n    start_time = time.time()\n    ntokens = len(corpus.dictionary)\n    # in RMC, ""hidden state"" is called ""memory"" instead. so use the name ""memory""\n    memory = model.module.initial_state(args.batch_size, trainable=True).to(device)\n\n    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n        data, targets = get_batch(train_data, i)\n        # transpose the data to [batch, seq]\n        data = torch.t(data)\n\n        # synchronize cuda for a proper speed benchmark\n        torch.cuda.synchronize()\n\n        forward_start_time = time.time()\n        model.zero_grad()\n\n        # the forward pass of RMC just returns loss and does not return logits (DataParallel code optimization)\n        loss, memory = model(data, memory, targets)\n        loss = torch.mean(loss)\n        total_loss += loss.item()\n\n        # synchronize cuda for a proper speed benchmark\n        torch.cuda.synchronize()\n\n        forward_elapsed = time.time() - forward_start_time\n        forward_elapsed_time += forward_elapsed\n\n        loss.backward()\n\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n\n        optimizer.step()\n\n        if batch % args.log_interval == 0 and batch > 0:\n            cur_loss = total_loss / args.log_interval\n            elapsed = time.time() - start_time\n            printlog = \'| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.5f} | ms/batch {:5.2f} | forward ms/batch {:5.2f} | loss {:5.2f} | ppl {:8.2f}\'.format(\n                epoch, batch, len(train_data) // args.bptt, optimizer.param_groups[0][\'lr\'],\n                              elapsed * 1000 / args.log_interval, forward_elapsed_time * 1000 / args.log_interval,\n                cur_loss, math.exp(cur_loss))\n            # print and save the log\n            print(printlog)\n            logger_train.write(printlog + \'\\n\')\n            logger_train.flush()\n            total_loss = 0.\n            # reset timer\n            start_time = time.time()\n            forward_start_time = time.time()\n            forward_elapsed_time = 0.\n\n\ndef export_onnx(path, batch_size, seq_len):\n    print(\'The model is also exported in ONNX format at {}\'.\n          format(os.path.realpath(args.onnx_export)))\n    model.eval()\n    dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)\n    hidden = model.init_hidden(batch_size)\n    torch.onnx.export(model, (dummy_input, hidden), path)\n\n\n# Loop over epochs.\nbest_val_loss = None\n\n# At any point you can hit Ctrl + C to break out of training early.\ntry:\n    print(""training started..."")\n    if global_epoch > args.epochs:\n        raise ValueError(""global_epoch is higher than args.epochs when resuming training."")\n    for epoch in range(global_epoch + 1, args.epochs + 1):\n        global_epoch += 1\n        epoch_start_time = time.time()\n        train()\n        val_loss = evaluate(val_data)\n\n        print(\'-\' * 89)\n        testlog = \'| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | valid ppl {:8.2f}\'.format(epoch, (\n                time.time() - epoch_start_time), val_loss, math.exp(val_loss))\n        print(testlog)\n        logger_test.write(testlog + \'\\n\')\n        logger_test.flush()\n        print(\'-\' * 89)\n\n        scheduler.step(val_loss)\n\n        # Save the model if the validation loss is the best we\'ve seen so far.\n        # model_{} contains state_dict and other states, model_dump_{} contains all the dependencies for generate_rmc.py\n        if not best_val_loss or val_loss < best_val_loss:\n            try:\n                os.remove(os.path.join(savepath, ""model_{}.pt"".format(best_epoch)))\n                os.remove(os.path.join(savepath, ""model_dump_{}.pt"").format(best_epoch))\n            except FileNotFoundError:\n                pass\n            best_epoch = global_epoch\n            torch.save(model, os.path.join(savepath, ""model_dump_{}.pt"".format(global_epoch)))\n            with open(os.path.join(savepath, ""model_{}.pt"".format(global_epoch)), \'wb\') as f:\n                optimizer_state = optimizer.state_dict()\n                scheduler_state = scheduler.state_dict()\n                torch.save({""state_dict"": model.state_dict(),\n                            ""optimizer"": optimizer_state,\n                            ""scheduler"": scheduler_state,\n                            ""global_epoch"": global_epoch,\n                            ""best_epoch"": best_epoch}, f)\n            best_val_loss = val_loss\n        else:\n            pass\n\nexcept KeyboardInterrupt:\n    print(\'-\' * 89)\n    print(\'Exiting from training early: loading checkpoint from the best epoch {}...\'.format(best_epoch))\n\n# Load the best saved model.\nwith open(os.path.join(savepath, ""model_{}.pt"".format(best_epoch)), \'rb\') as f:\n    checkpoint = torch.load(f)\n    model.load_state_dict(checkpoint[""state_dict""])\n\n# Run on test data.\ntest_loss = evaluate(test_data)\n\nprint(\'=\' * 89)\ntestlog = \'| End of training | test loss {:5.2f} | test ppl {:8.2f}\'.format(\n    test_loss, math.exp(test_loss))\nprint(testlog)\nlogger_test.write(testlog + \'\\n\')\nlogger_test.flush()\nprint(\'=\' * 89)\n\nif len(args.onnx_export) > 0:\n    # Export the model in ONNX format.\n    export_onnx(args.onnx_export, batch_size=1, seq_len=args.bptt)\n'"
train_rnn.py,19,"b'# coding: utf-8\nimport argparse\nimport time\nimport math\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.onnx\nimport datetime\nimport shutil\nimport pickle\nimport data\nimport rnn_models\n\n# is it faster?\ntorch.backends.cudnn.benchmark = True\n\n# same hyperparameter scheme as word-language-model\nparser = argparse.ArgumentParser(description=\'PyTorch Wikitext-2 RNN/LSTM Language Model\')\nparser.add_argument(\'--data\', type=str, default=\'./data/wikitext-2\',\n                    help=\'location of the data corpus\')\nparser.add_argument(\'--model\', type=str, default=\'LSTM\',\n                    help=\'type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)\')\nparser.add_argument(\'--emsize\', type=int, default=300,\n                    help=\'size of word embeddings\')\nparser.add_argument(\'--nhid\', type=int, default=300,\n                    help=\'number of hidden units per layer\')\nparser.add_argument(\'--nlayers\', type=int, default=1,\n                    help=\'number of layers\')\nparser.add_argument(\'--lr\', type=float, default=0.001,\n                    help=\'initial learning rate\')\nparser.add_argument(\'--clip\', type=float, default=0.1,\n                    help=\'gradient clipping\')\nparser.add_argument(\'--epochs\', type=int, default=100,\n                    help=\'upper epoch limit\')\nparser.add_argument(\'--batch_size\', type=int, default=64, metavar=\'N\',\n                    help=\'batch size\')\nparser.add_argument(\'--bptt\', type=int, default=100,\n                    help=\'sequence length\')\nparser.add_argument(\'--dropout\', type=float, default=0.5,\n                    help=\'dropout applied to layers (0 = no dropout)\')\nparser.add_argument(\'--tied\', action=\'store_true\', default=True,\n                    help=\'tie the word embedding and softmax weights\')\nparser.add_argument(\'--seed\', type=int, default=1111,\n                    help=\'random seed\')\nparser.add_argument(\'--cuda\', action=\'store_true\',\n                    help=\'use CUDA\')\nparser.add_argument(\'--cudnn\', action=\'store_true\',\n                    help=\'use cudnn optimized version. i.e. use RNN instead of RNNCell with for loop\')\nparser.add_argument(\'--log-interval\', type=int, default=100, metavar=\'N\',\n                    help=\'report interval\')\nparser.add_argument(\'--save\', type=str, default=\'model.pt\',\n                    help=\'path to save the final model\')\nparser.add_argument(\'--onnx-export\', type=str, default=\'\',\n                    help=\'path to export the final model in onnx format\')\nparser.add_argument(\'--resume\', type=int, default=None,\n                    help=\'if specified with the 1-indexed global epoch, loads the checkpoint and resumes training\')\n\n# parameters for adaptive softmax\nparser.add_argument(\'--adaptivesoftmax\', action=\'store_true\',\n                    help=\'use adaptive softmax during hidden state to output logits.\'\n                         \'it uses less memory by approximating softmax of large vocabulary.\')\nparser.add_argument(\'--cutoffs\', nargs=""*"", type=int, default=[10000, 50000, 100000],\n                    help=\'cutoff values for adaptive softmax. list of integers.\'\n                         \'optimal values are based on word frequencey and vocabulary size of the dataset.\')\n\n# experiment name for this run\nparser.add_argument(\'--name\', type=str, default=None,\n                    help=\'name for this experiment. generates folder with the name if specified.\')\n\nargs = parser.parse_args()\n\n# Set the random seed manually for reproducibility.\ntorch.manual_seed(args.seed)\n\nif torch.cuda.is_available():\n    if not args.cuda:\n        print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n\ndevice = torch.device(""cuda"" if args.cuda else ""cpu"")\n###############################################################################\n# Load data\n###############################################################################\ncorpus_name = os.path.basename(os.path.normpath(args.data))\ncorpus_filename = \'./data/corpus-\' + str(corpus_name) + str(\'.pkl\')\nif os.path.isfile(corpus_filename):\n    print(""loading pre-built "" + str(corpus_name) + "" corpus file..."")\n    loadfile = open(corpus_filename, \'rb\')\n    corpus = pickle.load(loadfile)\n    loadfile.close()\nelse:\n    print(""building "" + str(corpus_name) + "" corpus..."")\n    corpus = data.Corpus(args.data)\n    # save the corpus for later\n    savefile = open(corpus_filename, \'wb\')\n    pickle.dump(corpus, savefile)\n    savefile.close()\n    print(""corpus saved to pickle"")\n\n\n# Starting from sequential data, batchify arranges the dataset into columns.\n# For instance, with the alphabet as the sequence and batch size 4, we\'d get\n# \xe2\x94\x8c a g m s \xe2\x94\x90\n# \xe2\x94\x82 b h n t \xe2\x94\x82\n# \xe2\x94\x82 c i o u \xe2\x94\x82\n# \xe2\x94\x82 d j p v \xe2\x94\x82\n# \xe2\x94\x82 e k q w \xe2\x94\x82\n# \xe2\x94\x94 f l r x \xe2\x94\x98.\n# These columns are treated as independent by the model, which means that the\n# dependence of e. g. \'g\' on \'f\' can not be learned, but allows more efficient\n# batch processing.\n\ndef batchify(data, bsz):\n    # Work out how cleanly we can divide the dataset into bsz parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn\'t cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    data = data.view(bsz, -1).t().contiguous()\n    return data.to(device)\n\n\neval_batch_size = 32\ntrain_data = batchify(corpus.train, args.batch_size)\nval_data = batchify(corpus.valid, eval_batch_size)\ntest_data = batchify(corpus.test, eval_batch_size)\n\n# create folder for current experiments\n# name: args.name + current time\n# includes: entire scripts for faithful reproduction, train & test logs\nfolder_name = str(datetime.datetime.now())[:-7]\nif args.name is not None:\n    folder_name = str(args.name) + \' \' + folder_name\n\nos.mkdir(folder_name)\nfor file in os.listdir(os.getcwd()):\n    if file.endswith("".py""):\n        shutil.copy2(file, os.path.join(os.getcwd(), folder_name))\nlogger_train = open(os.path.join(os.getcwd(), folder_name, \'train_log.txt\'), \'w+\')\nlogger_test = open(os.path.join(os.getcwd(), folder_name, \'test_log.txt\'), \'w+\')\n\n# save args to logger\nlogger_train.write(str(args) + \'\\n\')\n\n# define saved model file location\nsavepath = os.path.join(os.getcwd(), folder_name)\n\n###############################################################################\n# Build the model\n###############################################################################\n\nntokens = len(corpus.dictionary)\nprint(""vocabulary size (ntokens): "" + str(ntokens))\nif args.adaptivesoftmax:\n    print(""Adaptive Softmax is on: the performance depends on cutoff values. check if the cutoff is properly set"")\n    print(""Cutoffs: "" + str(args.cutoffs))\n    if args.cutoffs[-1] > ntokens:\n        raise ValueError(""the last element of cutoff list must be lower than vocab size of the dataset"")\n    criterion_adaptive = nn.AdaptiveLogSoftmaxWithLoss(args.nhid, ntokens, cutoffs=args.cutoffs).to(device)\nelse:\n    criterion = nn.CrossEntropyLoss()\n\nmodel = rnn_models.RNNModel(args.model, ntokens, args.emsize, args.nhid,\n                            args.nlayers, args.dropout, args.tied,\n                            use_cudnn_version=args.cudnn, use_adaptive_softmax=args.adaptivesoftmax,\n                            cutoffs=args.cutoffs).to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(""model built, total trainable params: "" + str(total_params))\nif not args.cudnn:\n    print(\n        ""--cudnn is set to False. the model will use RNNCell with for loop, instead of cudnn-optimzed RNN API. Expect a minor slowdown."")\n\noptimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \'min\', factor=0.5, patience=5)\n\n###############################################################################\n# Load the model checkpoint if specified and restore the global & best epoch\n###############################################################################\nif args.resume is not None:\n    print(""--resume detected. loading checkpoint..."")\nglobal_epoch = args.resume if args.resume is not None else 0\nbest_epoch = args.resume if args.resume is not None else 0\nif args.resume is not None:\n    loadpath = os.path.join(os.getcwd(), ""model_{}.pt"".format(args.resume))\n    if not os.path.isfile(loadpath):\n        raise FileNotFoundError(\n            ""model_{}.pt not found. place the model checkpoint file to the current working directory."".format(\n                args.resume))\n    checkpoint = torch.load(loadpath)\n    model.load_state_dict(checkpoint[""state_dict""])\n    optimizer.load_state_dict(checkpoint[""optimizer""])\n    scheduler.load_state_dict(checkpoint[""scheduler""])\n    global_epoch = checkpoint[""global_epoch""]\n    best_epoch = checkpoint[""best_epoch""]\n\nprint(""model built, total trainable params: "" + str(total_params))\n\n\n###############################################################################\n# Training code\n###############################################################################\n\ndef repackage_hidden(h):\n    """"""Wraps hidden states in new Tensors, to detach them from their history.""""""\n    if isinstance(h, torch.Tensor):\n        return h.detach()\n    else:\n        return tuple(repackage_hidden(v) for v in h)\n\n\n# get_batch subdivides the source data into chunks of length args.bptt.\n# If source is equal to the example output of the batchify function, with\n# a bptt-limit of 2, we\'d get the following two Variables for i = 0:\n# \xe2\x94\x8c a g m s \xe2\x94\x90 \xe2\x94\x8c b h n t \xe2\x94\x90\n# \xe2\x94\x94 b h n t \xe2\x94\x98 \xe2\x94\x94 c i o u \xe2\x94\x98\n# Note that despite the name of the function, the subdivison of data is not\n# done along the batch dimension (i.e. dimension 1), since that was handled\n# by the batchify function. The chunks are along dimension 0, corresponding\n# to the seq_len dimension in the LSTM.\n\n\ndef get_batch(source, i):\n    seq_len = min(args.bptt, len(source) - 1 - i)\n    data = source[i:i + seq_len]\n    target = source[i + 1:i + 1 + seq_len].view(-1)\n    return data, target\n\n\ndef evaluate(data_source):\n    # Turn on evaluation mode which disables dropout.\n    model.eval()\n    total_loss = 0.\n    ntokens = len(corpus.dictionary)\n    hidden = model.init_hidden(eval_batch_size)\n    with torch.no_grad():\n        for i in range(0, data_source.size(0) - 1, args.bptt):\n            data, targets = get_batch(data_source, i)\n            output, hidden = model(data, hidden)\n            if not args.adaptivesoftmax:\n                loss = criterion(output.view(-1, ntokens), targets)\n            else:\n                _, loss = criterion_adaptive(output.view(-1, args.nhid), targets)\n            total_loss += len(data) * loss.item()\n            hidden = repackage_hidden(hidden)\n    return total_loss / len(data_source)\n\n\ndef train():\n    # Turn on training mode which enables dropout.\n    model.train()\n    total_loss = 0.\n    forward_elapsed_time = 0.\n    start_time = time.time()\n    ntokens = len(corpus.dictionary)\n    hidden = model.init_hidden(args.batch_size)\n    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n        data, targets = get_batch(train_data, i)\n\n        # synchronize cuda for a proper speed benchmark\n        torch.cuda.synchronize()\n\n        # Starting each batch, we detach the hidden state from how it was previously produced.\n        # If we didn\'t, the model would try backpropagating all the way to start of the dataset.\n        forward_start_time = time.time()\n\n        hidden = repackage_hidden(hidden)\n        model.zero_grad()\n\n        output, hidden = model(data, hidden)\n        if not args.adaptivesoftmax:\n            loss = criterion(output.view(-1, ntokens), targets)\n        else:\n            _, loss = criterion_adaptive(output.view(-1, args.nhid), targets)\n        total_loss += loss.item()\n\n        # synchronize cuda for a proper speed benchmark\n        torch.cuda.synchronize()\n\n        forward_elapsed = time.time() - forward_start_time\n        forward_elapsed_time += forward_elapsed\n\n        loss.backward()\n\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n\n        optimizer.step()\n\n        if batch % args.log_interval == 0 and batch > 0:\n            cur_loss = total_loss / args.log_interval\n            elapsed = time.time() - start_time\n            printlog = \'| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.5f} | ms/batch {:5.2f} | forward ms/batch {:5.2f} | loss {:5.2f} | ppl {:8.2f}\'.format(\n                epoch, batch, len(train_data) // args.bptt, optimizer.param_groups[0][\'lr\'],\n                              elapsed * 1000 / args.log_interval, forward_elapsed_time * 1000 / args.log_interval,\n                cur_loss, math.exp(cur_loss))\n            # print and save the log\n            print(printlog)\n            logger_train.write(printlog + \'\\n\')\n            logger_train.flush()\n            total_loss = 0.\n            # reset timer\n            start_time = time.time()\n            forward_start_time = time.time()\n            forward_elapsed_time = 0.\n\n\ndef export_onnx(path, batch_size, seq_len):\n    print(\'The model is also exported in ONNX format at {}\'.\n          format(os.path.realpath(args.onnx_export)))\n    model.eval()\n    dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)\n    hidden = model.init_hidden(batch_size)\n    torch.onnx.export(model, (dummy_input, hidden), path)\n\n\n# Loop over epochs.\nbest_val_loss = None\n\n# At any point you can hit Ctrl + C to break out of training early.\ntry:\n    print(""training started..."")\n    if global_epoch > args.epochs:\n        raise ValueError(""global_epoch is higher than args.epochs when resuming training."")\n    for epoch in range(global_epoch + 1, args.epochs + 1):\n        global_epoch += 1\n        epoch_start_time = time.time()\n        train()\n        val_loss = evaluate(val_data)\n\n        print(\'-\' * 89)\n        testlog = \'| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | valid ppl {:8.2f}\'.format(epoch, (\n                time.time() - epoch_start_time), val_loss, math.exp(val_loss))\n        print(testlog)\n        logger_test.write(testlog + \'\\n\')\n        logger_test.flush()\n        print(\'-\' * 89)\n\n        scheduler.step(val_loss)\n\n        # Save the model if the validation loss is the best we\'ve seen so far.\n        # model_{} contains state_dict and other states, model_dump_{} contains all the dependencies for generate_rmc.py\n        if not best_val_loss or val_loss < best_val_loss:\n            try:\n                os.remove(os.path.join(savepath, ""model_{}.pt"".format(best_epoch)))\n                os.remove(os.path.join(savepath, ""model_dump_{}.pt"").format(best_epoch))\n            except FileNotFoundError:\n                pass\n            best_epoch = global_epoch\n            torch.save(model, os.path.join(savepath, ""model_dump_{}.pt"".format(global_epoch)))\n            with open(os.path.join(savepath, ""model_{}.pt"".format(global_epoch)), \'wb\') as f:\n                optimizer_state = optimizer.state_dict()\n                scheduler_state = scheduler.state_dict()\n                torch.save({""state_dict"": model.state_dict(),\n                            ""optimizer"": optimizer_state,\n                            ""scheduler"": scheduler_state,\n                            ""global_epoch"": global_epoch,\n                            ""best_epoch"": best_epoch}, f)\n            best_val_loss = val_loss\n        else:\n            pass\n\nexcept KeyboardInterrupt:\n    print(\'-\' * 89)\n    print(\'Exiting from training early: loading checkpoint from the best epoch {}...\'.format(best_epoch))\n\n# Load the best saved model.\nwith open(os.path.join(savepath, ""model_{}.pt"".format(best_epoch)), \'rb\') as f:\n    checkpoint = torch.load(f)\n    model.load_state_dict(checkpoint[""state_dict""])\n    # after load the rnn params are not a continuous chunk of memory\n    # this makes them a continuous chunk, and will speed up forward pass\n    if args.cudnn:\n        model.rnn.flatten_parameters()\n\n# Run on test data.\ntest_loss = evaluate(test_data)\n\nprint(\'=\' * 89)\ntestlog = \'| End of training | test loss {:5.2f} | test ppl {:8.2f}\'.format(\n    test_loss, math.exp(test_loss))\nprint(testlog)\nlogger_test.write(testlog + \'\\n\')\nlogger_test.flush()\nprint(\'=\' * 89)\n\nif len(args.onnx_export) > 0:\n    # Export the model in ONNX format.\n    export_onnx(args.onnx_export, batch_size=1, seq_len=args.bptt)\n'"
