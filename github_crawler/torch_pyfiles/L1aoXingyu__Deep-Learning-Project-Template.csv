file_path,api_count,code
config/__init__.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\nfrom .defaults import _C as cfg\n'"
config/defaults.py,0,"b'from yacs.config import CfgNode as CN\n\n# -----------------------------------------------------------------------------\n# Convention about Training / Test specific parameters\n# -----------------------------------------------------------------------------\n# Whenever an argument can be either used for training or for testing, the\n# corresponding name will be post-fixed by a _TRAIN for a training parameter,\n# or _TEST for a test-specific parameter.\n# For example, the number of images during training will be\n# IMAGES_PER_BATCH_TRAIN, while the number of images for testing will be\n# IMAGES_PER_BATCH_TEST\n\n# -----------------------------------------------------------------------------\n# Config definition\n# -----------------------------------------------------------------------------\n\n_C = CN()\n\n_C.MODEL = CN()\n_C.MODEL.DEVICE = ""cuda""\n_C.MODEL.NUM_CLASSES = 10\n\n# -----------------------------------------------------------------------------\n# INPUT\n# -----------------------------------------------------------------------------\n_C.INPUT = CN()\n# Size of the image during training\n_C.INPUT.SIZE_TRAIN = 32\n# Size of the image during test\n_C.INPUT.SIZE_TEST = 32\n# Minimum scale for the image during training\n_C.INPUT.MIN_SCALE_TRAIN = 0.5\n# Maximum scale for the image during test\n_C.INPUT.MAX_SCALE_TRAIN = 1.2\n# Random probability for image horizontal flip\n_C.INPUT.PROB = 0.5\n# Values to be used for image normalization\n_C.INPUT.PIXEL_MEAN = [0.1307, ]\n# Values to be used for image normalization\n_C.INPUT.PIXEL_STD = [0.3081, ]\n\n# -----------------------------------------------------------------------------\n# Dataset\n# -----------------------------------------------------------------------------\n_C.DATASETS = CN()\n# List of the dataset names for training, as present in paths_catalog.py\n_C.DATASETS.TRAIN = ()\n# List of the dataset names for testing, as present in paths_catalog.py\n_C.DATASETS.TEST = ()\n\n# -----------------------------------------------------------------------------\n# DataLoader\n# -----------------------------------------------------------------------------\n_C.DATALOADER = CN()\n# Number of data loading threads\n_C.DATALOADER.NUM_WORKERS = 8\n\n# ---------------------------------------------------------------------------- #\n# Solver\n# ---------------------------------------------------------------------------- #\n_C.SOLVER = CN()\n_C.SOLVER.OPTIMIZER_NAME = ""SGD""\n\n_C.SOLVER.MAX_EPOCHS = 50\n\n_C.SOLVER.BASE_LR = 0.001\n_C.SOLVER.BIAS_LR_FACTOR = 2\n\n_C.SOLVER.MOMENTUM = 0.9\n\n_C.SOLVER.WEIGHT_DECAY = 0.0005\n_C.SOLVER.WEIGHT_DECAY_BIAS = 0\n\n_C.SOLVER.GAMMA = 0.1\n_C.SOLVER.STEPS = (30000,)\n\n_C.SOLVER.WARMUP_FACTOR = 1.0 / 3\n_C.SOLVER.WARMUP_ITERS = 500\n_C.SOLVER.WARMUP_METHOD = ""linear""\n\n_C.SOLVER.CHECKPOINT_PERIOD = 10\n_C.SOLVER.LOG_PERIOD = 100\n\n# Number of images per batch\n# This is global, so if we have 8 GPUs and IMS_PER_BATCH = 16, each GPU will\n# see 2 images per batch\n_C.SOLVER.IMS_PER_BATCH = 16\n\n# This is global, so if we have 8 GPUs and IMS_PER_BATCH = 16, each GPU will\n# see 2 images per batch\n_C.TEST = CN()\n_C.TEST.IMS_PER_BATCH = 8\n_C.TEST.WEIGHT = """"\n\n# ---------------------------------------------------------------------------- #\n# Misc options\n# ---------------------------------------------------------------------------- #\n_C.OUTPUT_DIR = """"\n'"
data/__init__.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\nfrom .build import make_data_loader\n'"
data/build.py,1,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\nfrom torch.utils import data\n\nfrom .datasets.mnist import MNIST\nfrom .transforms import build_transforms\n\n\ndef build_dataset(transforms, is_train=True):\n    datasets = MNIST(root=\'./\', train=is_train, transform=transforms, download=True)\n    return datasets\n\n\ndef make_data_loader(cfg, is_train=True):\n    if is_train:\n        batch_size = cfg.SOLVER.IMS_PER_BATCH\n        shuffle = True\n    else:\n        batch_size = cfg.TEST.IMS_PER_BATCH\n        shuffle = False\n\n    transforms = build_transforms(cfg, is_train)\n    datasets = build_dataset(transforms, is_train)\n\n    num_workers = cfg.DATALOADER.NUM_WORKERS\n    data_loader = data.DataLoader(\n        datasets, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers\n    )\n\n    return data_loader\n'"
data/collate_batch.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n'"
engine/example_inference.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\nimport logging\n\nfrom ignite.engine import Events\nfrom ignite.engine import create_supervised_evaluator\nfrom ignite.metrics import Accuracy\n\n\ndef inference(\n        cfg,\n        model,\n        val_loader\n):\n    device = cfg.MODEL.DEVICE\n\n    logger = logging.getLogger(""template_model.inference"")\n    logger.info(""Start inferencing"")\n    evaluator = create_supervised_evaluator(model, metrics={\'accuracy\': Accuracy()},\n                                            device=device)\n\n    # adding handlers using `evaluator.on` decorator API\n    @evaluator.on(Events.EPOCH_COMPLETED)\n    def print_validation_results(engine):\n        metrics = evaluator.state.metrics\n        avg_acc = metrics[\'accuracy\']\n        logger.info(""Validation Results - Accuracy: {:.3f}"".format(avg_acc))\n\n    evaluator.run(val_loader)\n'"
engine/example_trainer.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\nimport logging\n\nfrom ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\nfrom ignite.handlers import ModelCheckpoint, Timer\nfrom ignite.metrics import Accuracy, Loss, RunningAverage\n\n\ndef do_train(\n        cfg,\n        model,\n        train_loader,\n        val_loader,\n        optimizer,\n        scheduler,\n        loss_fn,\n):\n    log_period = cfg.SOLVER.LOG_PERIOD\n    checkpoint_period = cfg.SOLVER.CHECKPOINT_PERIOD\n    output_dir = cfg.OUTPUT_DIR\n    device = cfg.MODEL.DEVICE\n    epochs = cfg.SOLVER.MAX_EPOCHS\n\n    logger = logging.getLogger(""template_model.train"")\n    logger.info(""Start training"")\n    trainer = create_supervised_trainer(model, optimizer, loss_fn, device=device)\n    evaluator = create_supervised_evaluator(model, metrics={\'accuracy\': Accuracy(),\n                                                            \'ce_loss\': Loss(loss_fn)}, device=device)\n    checkpointer = ModelCheckpoint(output_dir, \'mnist\', checkpoint_period, n_saved=10, require_empty=False)\n    timer = Timer(average=True)\n\n    trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {\'model\': model.state_dict(),\n                                                                     \'optimizer\': optimizer.state_dict()})\n    timer.attach(trainer, start=Events.EPOCH_STARTED, resume=Events.ITERATION_STARTED,\n                 pause=Events.ITERATION_COMPLETED, step=Events.ITERATION_COMPLETED)\n\n    RunningAverage(output_transform=lambda x: x).attach(trainer, \'avg_loss\')\n\n    @trainer.on(Events.ITERATION_COMPLETED)\n    def log_training_loss(engine):\n        iter = (engine.state.iteration - 1) % len(train_loader) + 1\n\n        if iter % log_period == 0:\n            logger.info(""Epoch[{}] Iteration[{}/{}] Loss: {:.2f}""\n                        .format(engine.state.epoch, iter, len(train_loader), engine.state.metrics[\'avg_loss\']))\n\n    @trainer.on(Events.EPOCH_COMPLETED)\n    def log_training_results(engine):\n        evaluator.run(train_loader)\n        metrics = evaluator.state.metrics\n        avg_accuracy = metrics[\'accuracy\']\n        avg_loss = metrics[\'ce_loss\']\n        logger.info(""Training Results - Epoch: {} Avg accuracy: {:.3f} Avg Loss: {:.3f}""\n                    .format(engine.state.epoch, avg_accuracy, avg_loss))\n\n    if val_loader is not None:\n        @trainer.on(Events.EPOCH_COMPLETED)\n        def log_validation_results(engine):\n            evaluator.run(val_loader)\n            metrics = evaluator.state.metrics\n            avg_accuracy = metrics[\'accuracy\']\n            avg_loss = metrics[\'ce_loss\']\n            logger.info(""Validation Results - Epoch: {} Avg accuracy: {:.3f} Avg Loss: {:.3f}""\n                        .format(engine.state.epoch, avg_accuracy, avg_loss)\n                        )\n\n    # adding handlers using `trainer.on` decorator API\n    @trainer.on(Events.EPOCH_COMPLETED)\n    def print_times(engine):\n        logger.info(\'Epoch {} done. Time per batch: {:.3f}[s] Speed: {:.1f}[samples/s]\'\n                    .format(engine.state.epoch, timer.value() * timer.step_count,\n                            train_loader.batch_size / timer.value()))\n        timer.reset()\n\n    trainer.run(train_loader, max_epochs=epochs)\n'"
layers/conv_layer.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\nfrom torch import nn\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n'"
modeling/__init__.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\nfrom .example_model import ResNet18\n\n\ndef build_model(cfg):\n    model = ResNet18(cfg.MODEL.NUM_CLASSES)\n    return model\n'"
modeling/example_model.py,1,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom layers.conv_layer import conv3x3\n\n\ndef conv_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        nn.init.xavier_uniform_(m.weight, gain=np.sqrt(2))\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(in_planes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride),\n                nn.BatchNorm2d(self.expansion * planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n\n        return out\n\n\nclass ResNet18(nn.Module):\n    def __init__(self, num_classes):\n        super(ResNet18, self).__init__()\n        self.in_planes = 16\n\n        self.conv1 = conv3x3(1, 16)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.layer1 = self._make_layer(BasicBlock, 16, 2, stride=1)\n        self.layer2 = self._make_layer(BasicBlock, 32, 2, stride=2)\n        self.layer3 = self._make_layer(BasicBlock, 64, 2, stride=2)\n        self.linear = nn.Linear(64 * BasicBlock.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.avg_pool2d(out, 8)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n\n        return out\n'"
solver/__init__.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\nfrom .build import make_optimizer\n'"
solver/build.py,1,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\nimport torch\n\n\ndef make_optimizer(cfg, model):\n    params = []\n    for key, value in model.named_parameters():\n        if not value.requires_grad:\n            continue\n        lr = cfg.SOLVER.BASE_LR\n        weight_decay = cfg.SOLVER.WEIGHT_DECAY\n        if ""bias"" in key:\n            lr = cfg.SOLVER.BASE_LR * cfg.SOLVER.BIAS_LR_FACTOR\n            weight_decay = cfg.SOLVER.WEIGHT_DECAY_BIAS\n        params += [{""params"": [value], ""lr"": lr, ""weight_decay"": weight_decay}]\n    optimizer = getattr(torch.optim, cfg.SOLVER.OPTIMIZER_NAME)(params, momentum=cfg.SOLVER.MOMENTUM)\n    return optimizer\n'"
solver/lr_scheduler.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n'"
tests/__init__.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n'"
tests/test_data_samplers.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\nimport sys\nimport unittest\n\nsys.path.append(\'.\')\nfrom config.defaults import _C as cfg\nfrom data.transforms import build_transforms\nfrom data.build import build_dataset\nfrom solver.build import make_optimizer\nfrom modeling import build_model\n\n\nclass TestDataSet(unittest.TestCase):\n    def test_optimzier(self):\n        model = build_model(cfg)\n        optimizer = make_optimizer(cfg, model)\n        from IPython import embed;\n        embed()\n\n    def test_cfg(self):\n        cfg.merge_from_file(\'configs/train_mnist_softmax.yml\')\n        from IPython import embed;\n        embed()\n\n    def test_dataset(self):\n        train_transform = build_transforms(cfg)\n        val_transform = build_transforms(cfg, False)\n        train_set = build_dataset(train_transform)\n        val_test = build_dataset(val_transform, False)\n        from IPython import embed;\n        embed()\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tools/__init__.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n'"
tools/test_net.py,1,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\nimport argparse\nimport os\nimport sys\nfrom os import mkdir\n\nimport torch\n\nsys.path.append(\'.\')\nfrom config import cfg\nfrom data import make_data_loader\nfrom engine.example_inference import inference\nfrom modeling import build_model\nfrom utils.logger import setup_logger\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=""PyTorch Template MNIST Inference"")\n    parser.add_argument(\n        ""--config_file"", default="""", help=""path to config file"", type=str\n    )\n    parser.add_argument(""opts"", help=""Modify config options using the command-line"", default=None,\n                        nargs=argparse.REMAINDER)\n\n    args = parser.parse_args()\n\n    num_gpus = int(os.environ[""WORLD_SIZE""]) if ""WORLD_SIZE"" in os.environ else 1\n\n    if args.config_file != """":\n        cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    output_dir = cfg.OUTPUT_DIR\n    if output_dir and not os.path.exists(output_dir):\n        mkdir(output_dir)\n\n    logger = setup_logger(""template_model"", output_dir, 0)\n    logger.info(""Using {} GPUS"".format(num_gpus))\n    logger.info(args)\n\n    if args.config_file != """":\n        logger.info(""Loaded configuration file {}"".format(args.config_file))\n        with open(args.config_file, \'r\') as cf:\n            config_str = ""\\n"" + cf.read()\n            logger.info(config_str)\n    logger.info(""Running with config:\\n{}"".format(cfg))\n\n    model = build_model(cfg)\n    model.load_state_dict(torch.load(cfg.TEST.WEIGHT))\n    val_loader = make_data_loader(cfg, is_train=False)\n\n    inference(cfg, model, val_loader)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/train_net.py,1,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\nimport argparse\nimport os\nimport sys\nfrom os import mkdir\n\nimport torch.nn.functional as F\n\nsys.path.append(\'.\')\nfrom config import cfg\nfrom data import make_data_loader\nfrom engine.example_trainer import do_train\nfrom modeling import build_model\nfrom solver import make_optimizer\n\nfrom utils.logger import setup_logger\n\n\ndef train(cfg):\n    model = build_model(cfg)\n    device = cfg.MODEL.DEVICE\n\n    optimizer = make_optimizer(cfg, model)\n    scheduler = None\n\n    arguments = {}\n\n    train_loader = make_data_loader(cfg, is_train=True)\n    val_loader = make_data_loader(cfg, is_train=False)\n\n    do_train(\n        cfg,\n        model,\n        train_loader,\n        val_loader,\n        optimizer,\n        None,\n        F.cross_entropy,\n    )\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=""PyTorch Template MNIST Training"")\n    parser.add_argument(\n        ""--config_file"", default="""", help=""path to config file"", type=str\n    )\n    parser.add_argument(""opts"", help=""Modify config options using the command-line"", default=None,\n                        nargs=argparse.REMAINDER)\n\n    args = parser.parse_args()\n\n    num_gpus = int(os.environ[""WORLD_SIZE""]) if ""WORLD_SIZE"" in os.environ else 1\n\n    if args.config_file != """":\n        cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    output_dir = cfg.OUTPUT_DIR\n    if output_dir and not os.path.exists(output_dir):\n        mkdir(output_dir)\n\n    logger = setup_logger(""template_model"", output_dir, 0)\n    logger.info(""Using {} GPUS"".format(num_gpus))\n    logger.info(args)\n\n    if args.config_file != """":\n        logger.info(""Loaded configuration file {}"".format(args.config_file))\n        with open(args.config_file, \'r\') as cf:\n            config_str = ""\\n"" + cf.read()\n            logger.info(config_str)\n    logger.info(""Running with config:\\n{}"".format(cfg))\n\n    train(cfg)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils/__init__.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\n'"
utils/logger.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\nimport logging\nimport os\nimport sys\n\n\ndef setup_logger(name, save_dir, distributed_rank):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    # don\'t log results for the non-master process\n    if distributed_rank > 0:\n        return logger\n    ch = logging.StreamHandler(stream=sys.stdout)\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(""%(asctime)s %(name)s %(levelname)s: %(message)s"")\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    if save_dir:\n        fh = logging.FileHandler(os.path.join(save_dir, ""log.txt""), mode=\'w\')\n        fh.setLevel(logging.DEBUG)\n        fh.setFormatter(formatter)\n        logger.addHandler(fh)\n\n    return logger\n'"
data/datasets/__init__.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\n'"
data/datasets/mnist.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\nfrom torchvision.datasets import MNIST\n\n'"
data/transforms/__init__.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\nfrom .build import build_transforms\n'"
data/transforms/build.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\nimport torchvision.transforms as T\n\nfrom .transforms import RandomErasing\n\n\ndef build_transforms(cfg, is_train=True):\n    normalize_transform = T.Normalize(mean=cfg.INPUT.PIXEL_MEAN, std=cfg.INPUT.PIXEL_STD)\n    if is_train:\n        transform = T.Compose([\n            T.RandomResizedCrop(size=cfg.INPUT.SIZE_TRAIN,\n                                scale=(cfg.INPUT.MIN_SCALE_TRAIN, cfg.INPUT.MAX_SCALE_TRAIN)),\n            T.RandomHorizontalFlip(p=cfg.INPUT.PROB),\n            T.ToTensor(),\n            normalize_transform,\n            RandomErasing(probability=cfg.INPUT.PROB, mean=cfg.INPUT.PIXEL_MEAN)\n        ])\n    else:\n        transform = T.Compose([\n            T.Resize(cfg.INPUT.SIZE_TEST),\n            T.ToTensor(),\n            normalize_transform\n        ])\n\n    return transform\n'"
data/transforms/transforms.py,0,"b'# encoding: utf-8\n""""""\n@author:  sherlock\n@contact: sherlockliao01@gmail.com\n""""""\n\nimport math\nimport random\n\n\nclass RandomErasing(object):\n    """""" Randomly selects a rectangle region in an image and erases its pixels.\n        \'Random Erasing Data Augmentation\' by Zhong et al.\n        See https://arxiv.org/pdf/1708.04896.pdf\n    Args:\n         probability: The probability that the Random Erasing operation will be performed.\n         sl: Minimum proportion of erased area against input image.\n         sh: Maximum proportion of erased area against input image.\n         r1: Minimum aspect ratio of erased area.\n         mean: Erasing value.\n    """"""\n\n    def __init__(self, probability=0.5, sl=0.02, sh=0.4, r1=0.3, mean=(0.4914, 0.4822, 0.4465)):\n        self.probability = probability\n        self.mean = mean\n        self.sl = sl\n        self.sh = sh\n        self.r1 = r1\n\n    def __call__(self, img):\n\n        if random.uniform(0, 1) > self.probability:\n            return img\n\n        for attempt in range(100):\n            area = img.size()[1] * img.size()[2]\n\n            target_area = random.uniform(self.sl, self.sh) * area\n            aspect_ratio = random.uniform(self.r1, 1 / self.r1)\n\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w < img.size()[2] and h < img.size()[1]:\n                x1 = random.randint(0, img.size()[1] - h)\n                y1 = random.randint(0, img.size()[2] - w)\n                if img.size()[0] == 3:\n                    img[0, x1:x1 + h, y1:y1 + w] = self.mean[0]\n                    img[1, x1:x1 + h, y1:y1 + w] = self.mean[1]\n                    img[2, x1:x1 + h, y1:y1 + w] = self.mean[2]\n                else:\n                    img[0, x1:x1 + h, y1:y1 + w] = self.mean[0]\n                return img\n\n        return img\n'"
