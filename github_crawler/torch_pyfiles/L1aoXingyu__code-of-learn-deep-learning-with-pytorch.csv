file_path,api_count,code
utils.py,5,"b'from datetime import datetime\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.autograd import Variable\n\n\ndef get_acc(output, label):\n    total = output.shape[0]\n    _, pred_label = output.max(1)\n    num_correct = (pred_label == label).sum().data[0]\n    return num_correct / total\n\n\ndef train(net, train_data, valid_data, num_epochs, optimizer, criterion):\n    if torch.cuda.is_available():\n        net = net.cuda()\n    prev_time = datetime.now()\n    for epoch in range(num_epochs):\n        train_loss = 0\n        train_acc = 0\n        net = net.train()\n        for im, label in train_data:\n            if torch.cuda.is_available():\n                im = Variable(im.cuda())  # (bs, 3, h, w)\n                label = Variable(label.cuda())  # (bs, h, w)\n            else:\n                im = Variable(im)\n                label = Variable(label)\n            # forward\n            output = net(im)\n            loss = criterion(output, label)\n            # backward\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.data[0]\n            train_acc += get_acc(output, label)\n\n        cur_time = datetime.now()\n        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n        m, s = divmod(remainder, 60)\n        time_str = ""Time %02d:%02d:%02d"" % (h, m, s)\n        if valid_data is not None:\n            valid_loss = 0\n            valid_acc = 0\n            net = net.eval()\n            for im, label in valid_data:\n                if torch.cuda.is_available():\n                    im = Variable(im.cuda(), volatile=True)\n                    label = Variable(label.cuda(), volatile=True)\n                else:\n                    im = Variable(im, volatile=True)\n                    label = Variable(label, volatile=True)\n                output = net(im)\n                loss = criterion(output, label)\n                valid_loss += loss.data[0]\n                valid_acc += get_acc(output, label)\n            epoch_str = (\n                ""Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, ""\n                % (epoch, train_loss / len(train_data),\n                   train_acc / len(train_data), valid_loss / len(valid_data),\n                   valid_acc / len(valid_data)))\n        else:\n            epoch_str = (""Epoch %d. Train Loss: %f, Train Acc: %f, "" %\n                         (epoch, train_loss / len(train_data),\n                          train_acc / len(train_data)))\n        prev_time = cur_time\n        print(epoch_str + time_str)\n\n\ndef conv3x3(in_channel, out_channel, stride=1):\n    return nn.Conv2d(\n        in_channel, out_channel, 3, stride=stride, padding=1, bias=False)\n\n\nclass residual_block(nn.Module):\n    def __init__(self, in_channel, out_channel, same_shape=True):\n        super(residual_block, self).__init__()\n        self.same_shape = same_shape\n        stride = 1 if self.same_shape else 2\n\n        self.conv1 = conv3x3(in_channel, out_channel, stride=stride)\n        self.bn1 = nn.BatchNorm2d(out_channel)\n\n        self.conv2 = conv3x3(out_channel, out_channel)\n        self.bn2 = nn.BatchNorm2d(out_channel)\n        if not self.same_shape:\n            self.conv3 = nn.Conv2d(in_channel, out_channel, 1, stride=stride)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = F.relu(self.bn1(out), True)\n        out = self.conv2(out)\n        out = F.relu(self.bn2(out), True)\n\n        if not self.same_shape:\n            x = self.conv3(x)\n        return F.relu(x + out, True)\n\n\nclass resnet(nn.Module):\n    def __init__(self, in_channel, num_classes, verbose=False):\n        super(resnet, self).__init__()\n        self.verbose = verbose\n\n        self.block1 = nn.Conv2d(in_channel, 64, 7, 2)\n\n        self.block2 = nn.Sequential(\n            nn.MaxPool2d(3, 2), residual_block(64, 64), residual_block(64, 64))\n\n        self.block3 = nn.Sequential(\n            residual_block(64, 128, False), residual_block(128, 128))\n\n        self.block4 = nn.Sequential(\n            residual_block(128, 256, False), residual_block(256, 256))\n\n        self.block5 = nn.Sequential(\n            residual_block(256, 512, False),\n            residual_block(512, 512), nn.AvgPool2d(3))\n\n        self.classifier = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = self.block1(x)\n        if self.verbose:\n            print(\'block 1 output: {}\'.format(x.shape))\n        x = self.block2(x)\n        if self.verbose:\n            print(\'block 2 output: {}\'.format(x.shape))\n        x = self.block3(x)\n        if self.verbose:\n            print(\'block 3 output: {}\'.format(x.shape))\n        x = self.block4(x)\n        if self.verbose:\n            print(\'block 4 output: {}\'.format(x.shape))\n        x = self.block5(x)\n        if self.verbose:\n            print(\'block 5 output: {}\'.format(x.shape))\n        x = x.view(x.shape[0], -1)\n        x = self.classifier(x)\n        return x\n'"
chapter4_CNN/utils.py,5,"b'from datetime import datetime\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.autograd import Variable\n\n\ndef get_acc(output, label):\n    total = output.shape[0]\n    _, pred_label = output.max(1)\n    num_correct = (pred_label == label).sum().data[0]\n    return num_correct / total\n\n\ndef train(net, train_data, valid_data, num_epochs, optimizer, criterion):\n    if torch.cuda.is_available():\n        net = net.cuda()\n    prev_time = datetime.now()\n    for epoch in range(num_epochs):\n        train_loss = 0\n        train_acc = 0\n        net = net.train()\n        for im, label in train_data:\n            if torch.cuda.is_available():\n                im = Variable(im.cuda())  # (bs, 3, h, w)\n                label = Variable(label.cuda())  # (bs, h, w)\n            else:\n                im = Variable(im)\n                label = Variable(label)\n            # forward\n            output = net(im)\n            loss = criterion(output, label)\n            # backward\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.data[0]\n            train_acc += get_acc(output, label)\n\n        cur_time = datetime.now()\n        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n        m, s = divmod(remainder, 60)\n        time_str = ""Time %02d:%02d:%02d"" % (h, m, s)\n        if valid_data is not None:\n            valid_loss = 0\n            valid_acc = 0\n            net = net.eval()\n            for im, label in valid_data:\n                if torch.cuda.is_available():\n                    im = Variable(im.cuda(), volatile=True)\n                    label = Variable(label.cuda(), volatile=True)\n                else:\n                    im = Variable(im, volatile=True)\n                    label = Variable(label, volatile=True)\n                output = net(im)\n                loss = criterion(output, label)\n                valid_loss += loss.data[0]\n                valid_acc += get_acc(output, label)\n            epoch_str = (\n                ""Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, ""\n                % (epoch, train_loss / len(train_data),\n                   train_acc / len(train_data), valid_loss / len(valid_data),\n                   valid_acc / len(valid_data)))\n        else:\n            epoch_str = (""Epoch %d. Train Loss: %f, Train Acc: %f, "" %\n                         (epoch, train_loss / len(train_data),\n                          train_acc / len(train_data)))\n        prev_time = cur_time\n        print(epoch_str + time_str)\n\n\ndef conv3x3(in_channel, out_channel, stride=1):\n    return nn.Conv2d(\n        in_channel, out_channel, 3, stride=stride, padding=1, bias=False)\n\n\nclass residual_block(nn.Module):\n    def __init__(self, in_channel, out_channel, same_shape=True):\n        super(residual_block, self).__init__()\n        self.same_shape = same_shape\n        stride = 1 if self.same_shape else 2\n\n        self.conv1 = conv3x3(in_channel, out_channel, stride=stride)\n        self.bn1 = nn.BatchNorm2d(out_channel)\n\n        self.conv2 = conv3x3(out_channel, out_channel)\n        self.bn2 = nn.BatchNorm2d(out_channel)\n        if not self.same_shape:\n            self.conv3 = nn.Conv2d(in_channel, out_channel, 1, stride=stride)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = F.relu(self.bn1(out), True)\n        out = self.conv2(out)\n        out = F.relu(self.bn2(out), True)\n\n        if not self.same_shape:\n            x = self.conv3(x)\n        return F.relu(x + out, True)\n\n\nclass resnet(nn.Module):\n    def __init__(self, in_channel, num_classes, verbose=False):\n        super(resnet, self).__init__()\n        self.verbose = verbose\n\n        self.block1 = nn.Conv2d(in_channel, 64, 7, 2)\n\n        self.block2 = nn.Sequential(\n            nn.MaxPool2d(3, 2), residual_block(64, 64), residual_block(64, 64))\n\n        self.block3 = nn.Sequential(\n            residual_block(64, 128, False), residual_block(128, 128))\n\n        self.block4 = nn.Sequential(\n            residual_block(128, 256, False), residual_block(256, 256))\n\n        self.block5 = nn.Sequential(\n            residual_block(256, 512, False),\n            residual_block(512, 512), nn.AvgPool2d(3))\n\n        self.classifier = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = self.block1(x)\n        if self.verbose:\n            print(\'block 1 output: {}\'.format(x.shape))\n        x = self.block2(x)\n        if self.verbose:\n            print(\'block 2 output: {}\'.format(x.shape))\n        x = self.block3(x)\n        if self.verbose:\n            print(\'block 3 output: {}\'.format(x.shape))\n        x = self.block4(x)\n        if self.verbose:\n            print(\'block 4 output: {}\'.format(x.shape))\n        x = self.block5(x)\n        if self.verbose:\n            print(\'block 5 output: {}\'.format(x.shape))\n        x = x.view(x.shape[0], -1)\n        x = self.classifier(x)\n        return x\n'"
chapter5_RNN/utils.py,5,"b'from datetime import datetime\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.autograd import Variable\n\n\ndef get_acc(output, label):\n    total = output.shape[0]\n    _, pred_label = output.max(1)\n    num_correct = (pred_label == label).sum().data[0]\n    return num_correct / total\n\n\ndef train(net, train_data, valid_data, num_epochs, optimizer, criterion):\n    if torch.cuda.is_available():\n        net = net.cuda()\n    prev_time = datetime.now()\n    for epoch in range(num_epochs):\n        train_loss = 0\n        train_acc = 0\n        net = net.train()\n        for im, label in train_data:\n            if torch.cuda.is_available():\n                im = Variable(im.cuda())  # (bs, 3, h, w)\n                label = Variable(label.cuda())  # (bs, h, w)\n            else:\n                im = Variable(im)\n                label = Variable(label)\n            # forward\n            output = net(im)\n            loss = criterion(output, label)\n            # backward\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.data[0]\n            train_acc += get_acc(output, label)\n\n        cur_time = datetime.now()\n        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n        m, s = divmod(remainder, 60)\n        time_str = ""Time %02d:%02d:%02d"" % (h, m, s)\n        if valid_data is not None:\n            valid_loss = 0\n            valid_acc = 0\n            net = net.eval()\n            for im, label in valid_data:\n                if torch.cuda.is_available():\n                    im = Variable(im.cuda(), volatile=True)\n                    label = Variable(label.cuda(), volatile=True)\n                else:\n                    im = Variable(im, volatile=True)\n                    label = Variable(label, volatile=True)\n                output = net(im)\n                loss = criterion(output, label)\n                valid_loss += loss.data[0]\n                valid_acc += get_acc(output, label)\n            epoch_str = (\n                ""Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, ""\n                % (epoch, train_loss / len(train_data),\n                   train_acc / len(train_data), valid_loss / len(valid_data),\n                   valid_acc / len(valid_data)))\n        else:\n            epoch_str = (""Epoch %d. Train Loss: %f, Train Acc: %f, "" %\n                         (epoch, train_loss / len(train_data),\n                          train_acc / len(train_data)))\n        prev_time = cur_time\n        print(epoch_str + time_str)\n\n\ndef conv3x3(in_channel, out_channel, stride=1):\n    return nn.Conv2d(\n        in_channel, out_channel, 3, stride=stride, padding=1, bias=False)\n\n\nclass residual_block(nn.Module):\n    def __init__(self, in_channel, out_channel, same_shape=True):\n        super(residual_block, self).__init__()\n        self.same_shape = same_shape\n        stride = 1 if self.same_shape else 2\n\n        self.conv1 = conv3x3(in_channel, out_channel, stride=stride)\n        self.bn1 = nn.BatchNorm2d(out_channel)\n\n        self.conv2 = conv3x3(out_channel, out_channel)\n        self.bn2 = nn.BatchNorm2d(out_channel)\n        if not self.same_shape:\n            self.conv3 = nn.Conv2d(in_channel, out_channel, 1, stride=stride)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = F.relu(self.bn1(out), True)\n        out = self.conv2(out)\n        out = F.relu(self.bn2(out), True)\n\n        if not self.same_shape:\n            x = self.conv3(x)\n        return F.relu(x + out, True)\n\n\nclass resnet(nn.Module):\n    def __init__(self, in_channel, num_classes, verbose=False):\n        super(resnet, self).__init__()\n        self.verbose = verbose\n\n        self.block1 = nn.Conv2d(in_channel, 64, 7, 2)\n\n        self.block2 = nn.Sequential(\n            nn.MaxPool2d(3, 2), residual_block(64, 64), residual_block(64, 64))\n\n        self.block3 = nn.Sequential(\n            residual_block(64, 128, False), residual_block(128, 128))\n\n        self.block4 = nn.Sequential(\n            residual_block(128, 256, False), residual_block(256, 256))\n\n        self.block5 = nn.Sequential(\n            residual_block(256, 512, False),\n            residual_block(512, 512), nn.AvgPool2d(3))\n\n        self.classifier = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = self.block1(x)\n        if self.verbose:\n            print(\'block 1 output: {}\'.format(x.shape))\n        x = self.block2(x)\n        if self.verbose:\n            print(\'block 2 output: {}\'.format(x.shape))\n        x = self.block3(x)\n        if self.verbose:\n            print(\'block 3 output: {}\'.format(x.shape))\n        x = self.block4(x)\n        if self.verbose:\n            print(\'block 4 output: {}\'.format(x.shape))\n        x = self.block5(x)\n        if self.verbose:\n            print(\'block 5 output: {}\'.format(x.shape))\n        x = x.view(x.shape[0], -1)\n        x = self.classifier(x)\n        return x\n'"
chapter7_RL/dqn.py,9,"b""# coding: utf-8\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport gym\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xba\x9b\xe8\xb6\x85\xe5\x8f\x82\xe6\x95\xb0\n\n\nbatch_size = 32\nlr = 0.01\nepsilon = 0.9\ngamma = 0.9\ntarget_replace_iter = 100\nmemory_capacity = 2000\nenv = gym.make('CartPole-v0')\nenv = env.unwrapped\nn_actions = env.action_space.n\nn_states = env.observation_space.shape[0]\n\n\nclass q_net(nn.Module):\n    def __init__(self, hidden=50):\n        super(q_net, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(n_states, hidden),\n            nn.ReLU(True),\n            nn.Linear(hidden, n_actions)\n        )\n\n        nn.init.normal(self.fc[0].weight, std=0.1)  # \xe4\xbd\xbf\xe7\x94\xa8\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\xe6\x98\xaf 0.1 \xe7\x9a\x84\xe6\xad\xa3\xe6\x80\x81\xe5\x88\x86\xe5\xb8\x83\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n        nn.init.normal(self.fc[2].weight, std=0.1)  # \xe4\xbd\xbf\xe7\x94\xa8\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\xe6\x98\xaf 0.1 \xe7\x9a\x84\xe6\xad\xa3\xe6\x80\x81\xe5\x88\x86\xe5\xb8\x83\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n\n    def forward(self, x):\n        actions_value = self.fc(x)\n        return actions_value\n\n\nclass DQN(object):\n    def __init__(self):\n        self.eval_net, self.target_net = q_net(), q_net()\n\n        self.learn_step_counter = 0\n        self.memory_counter = 0\n        self.memory = np.zeros((memory_capacity, n_states * 2 + 2))  # \xe5\xbd\x93\xe5\x89\x8d\xe7\x9a\x84\xe7\x8a\xb6\xe6\x80\x81\xe5\x92\x8c\xe5\x8a\xa8\xe4\xbd\x9c\xef\xbc\x8c\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe7\x8a\xb6\xe6\x80\x81\xe5\x92\x8c\xe5\x8a\xa8\xe4\xbd\x9c\n        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=lr)\n        self.criterion = nn.MSELoss()\n\n    def choose_action(self, s):\n        '''\n        \xe6\xa0\xb9\xe6\x8d\xae\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe7\x8a\xb6\xe6\x80\x81\xe5\xbe\x97\xe5\x88\xb0\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\xaf\xe8\xa1\x8c\xe5\x8a\xa8\xe4\xbd\x9c\xe7\x9a\x84\xe4\xbb\xb7\xe5\x80\xbc\xe4\xbc\xb0\xe8\xae\xa1\n        '''\n        s = Variable(torch.unsqueeze(torch.FloatTensor(s), 0))\n        # input only one sample\n        if np.random.uniform() < epsilon:  # greedy \xe8\xb4\xaa\xe5\xa9\xaa\xe7\xae\x97\xe6\xb3\x95\n            actions_value = self.eval_net(s)\n            action = torch.max(actions_value, 1)[1].data[0]\n        else:  # random \xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\n            action = np.random.randint(0, n_actions)\n        return action\n\n    def store_transition(self, s, a, r, s_):\n        transition = np.hstack((s, [a, r], s_))\n        # \xe7\x94\xa8\xe6\x96\xb0\xe7\x9a\x84\xe8\xae\xb0\xe5\xbf\x86\xe6\x9b\xbf\xe6\x8d\xa2\xe6\x97\xa7\xe7\x9a\x84\xe8\xae\xb0\xe5\xbf\x86\n        index = self.memory_counter % memory_capacity\n        self.memory[index, :] = transition\n        self.memory_counter += 1\n\n    def learn(self):\n        # target net \xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\x9b\xb4\xe6\x96\xb0\n        if self.learn_step_counter % target_replace_iter == 0:\n            self.target_net.load_state_dict(self.eval_net.state_dict())\n        self.learn_step_counter += 1\n\n        # \xe5\x8f\x96\xe6\xa0\xb7\xe8\xae\xb0\xe5\xbf\x86\xe4\xb8\xad\xe7\x9a\x84\xe7\xbb\x8f\xe5\x8e\x86\n        sample_index = np.random.choice(memory_capacity, batch_size)\n        b_memory = self.memory[sample_index, :]\n        b_s = Variable(torch.FloatTensor(b_memory[:, :n_states]))\n        b_a = Variable(\n            torch.LongTensor(b_memory[:, n_states:n_states + 1].astype(int)))\n        b_r = Variable(\n            torch.FloatTensor(b_memory[:, n_states + 1:n_states + 2]))\n        b_s_ = Variable(torch.FloatTensor(b_memory[:, -n_states:]))\n\n        # q_eval net \xe8\xaf\x84\xe4\xbc\xb0\xe7\x8a\xb6\xe6\x80\x81\xe4\xb8\x8b\xe5\x8a\xa8\xe4\xbd\x9c\xe7\x9a\x84 value\n        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1) \xe9\x80\x89\xe6\x8b\xa9\xe5\xaf\xb9\xe5\xba\x94 action \xe7\x9a\x84\xe5\x8a\xa8\xe4\xbd\x9c\n        q_next = self.target_net(\n            b_s_).detach()  # detach from graph, don't backpropagate\n        q_target = b_r + gamma * q_next.max(1)[0].view(batch_size, 1)  # shape (batch, 1)\n        loss = self.criterion(q_eval, q_target)  # mse \xe4\xbd\x9c\xe4\xb8\xba loss \xe5\x87\xbd\xe6\x95\xb0\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe7\xbd\x91\xe7\xbb\x9c\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n\ndqn_trainer = DQN()\n\nprint('collecting experience ... ')\nall_reward = []\nfor i_episode in range(300):\n    s = env.reset()\n    reward = 0\n    while True:\n        if dqn_trainer.memory_counter > memory_capacity:\n            env.render()\n        a = dqn_trainer.choose_action(s)\n\n        # \xe7\x8e\xaf\xe5\xa2\x83\xe9\x87\x87\xe5\x8f\x96\xe5\x8a\xa8\xe4\xbd\x9c\xe5\xbe\x97\xe5\x88\xb0\xe7\xbb\x93\xe6\x9e\x9c\n        s_, r, done, info = env.step(a)\n\n        # \xe4\xbf\xae\xe6\x94\xb9\xe5\xa5\x96\xe5\x8a\xb1\xe4\xbb\xa5\xe4\xbe\xbf\xe6\x9b\xb4\xe5\xbf\xab\xe6\x94\xb6\xe6\x95\x9b\n        x, x_dot, theta, theta_dot = s_\n        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n        r = r1 + r2\n\n        dqn_trainer.store_transition(s, a, r, s_)\n\n        reward += r\n        if dqn_trainer.memory_counter > memory_capacity:  # \xe8\xae\xb0\xe5\xbf\x86\xe6\x94\xb6\xe9\x9b\x86\xe5\xa4\x9f\xe5\xbc\x80\xe5\xa7\x8b\xe5\xad\xa6\xe4\xb9\xa0\n            dqn_trainer.learn()\n            if done:\n                print('Ep: {} | reward: {:.3f}'.format(i_episode, round(reward, 3)))\n                all_reward.append(reward)\n                break\n\n        if done:\n            break\n        s = s_\n"""
chapter7_RL/mount-car.py,0,"b'import numpy as np\n\nimport gym\n\nn_states = 40  # \xe5\x8f\x96\xe6\xa0\xb7 40 \xe4\xb8\xaa\xe7\x8a\xb6\xe6\x80\x81\niter_max = 10000\n\ninitial_lr = 1.0  # Learning rate\nmin_lr = 0.003\ngamma = 1.0\nt_max = 10000\neps = 0.02\n\n\ndef run_episode(env, policy=None, render=False):\n    obs = env.reset()\n    total_reward = 0\n    step_idx = 0\n    for _ in range(t_max):\n        if render:\n            env.render()\n        if policy is None:  # \xe5\xa6\x82\xe6\x9e\x9c\xe6\xb2\xa1\xe6\x9c\x89\xe7\xad\x96\xe7\x95\xa5\xef\xbc\x8c\xe5\xb0\xb1\xe9\x9a\x8f\xe6\x9c\xba\xe5\x8f\x96\xe6\xa0\xb7\n            action = env.action_space.sample()\n        else:\n            a, b = obs_to_state(env, obs)\n            action = policy[a][b]\n        obs, reward, done, _ = env.step(action)\n        total_reward += gamma ** step_idx * reward\n        step_idx += 1\n        if done:\n            break\n    return total_reward\n\n\ndef obs_to_state(env, obs):\n    """"""\n    \xe5\xb0\x86\xe8\xa7\x82\xe5\xaf\x9f\xe7\x9a\x84\xe8\xbf\x9e\xe7\xbb\xad\xe7\x8e\xaf\xe5\xa2\x83\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0\xe7\xa6\xbb\xe6\x95\xa3\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe7\x8a\xb6\xe6\x80\x81\n    """"""\n    env_low = env.observation_space.low\n    env_high = env.observation_space.high\n    env_dx = (env_high - env_low) / n_states\n    a = int((obs[0] - env_low[0]) / env_dx[0])\n    b = int((obs[1] - env_low[1]) / env_dx[1])\n    return a, b\n\n\nif __name__ == \'__main__\':\n    env_name = \'MountainCar-v0\'\n    env = gym.make(env_name)\n    env.seed(0)\n    np.random.seed(0)\n    print(\'----- using Q Learning -----\')\n    q_table = np.zeros((n_states, n_states, 3))\n    for i in range(iter_max):\n        obs = env.reset()\n        total_reward = 0\n        ## eta: \xe6\xaf\x8f\xe4\xb8\x80\xe6\xad\xa5\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe9\x83\xbd\xe4\xb8\x8d\xe6\x96\xad\xe5\x87\x8f\xe5\xb0\x8f\n        eta = max(min_lr, initial_lr * (0.85 ** (i // 100)))\n        for j in range(t_max):\n            x, y = obs_to_state(env, obs)\n            if np.random.uniform(0, 1) < eps:  # greedy \xe8\xb4\xaa\xe5\xbf\x83\xe7\xae\x97\xe6\xb3\x95\n                action = np.random.choice(env.action_space.n)\n            else:\n                logits = q_table[x, y, :]\n                logits_exp = np.exp(logits)\n                probs = logits_exp / np.sum(logits_exp)  # \xe7\xae\x97\xe5\x87\xba\xe4\xb8\x89\xe4\xb8\xaa\xe5\x8a\xa8\xe4\xbd\x9c\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\n                action = np.random.choice(env.action_space.n, p=probs)  # \xe4\xbe\x9d\xe6\xa6\x82\xe7\x8e\x87\xe6\x9d\xa5\xe9\x80\x89\xe6\x8b\xa9\xe5\x8a\xa8\xe4\xbd\x9c\n            obs, reward, done, _ = env.step(action)\n            total_reward += reward\n            # \xe6\x9b\xb4\xe6\x96\xb0 q \xe8\xa1\xa8\n            x_, y_ = obs_to_state(env, obs)\n            q_table[x, y, action] = q_table[x, y, action] + eta * (\n                    reward + gamma * np.max(q_table[x_, y_, :]) -\n                    q_table[x, y, action])\n            if done:\n                break\n        if i % 100 == 0:\n            print(\'Iteration #%d -- Total reward = %d.\' % (i + 1,\n                                                           total_reward))\n    solution_policy = np.argmax(q_table, axis=2)  # \xe5\x9c\xa8 q \xe8\xa1\xa8\xe4\xb8\xad\xe6\xaf\x8f\xe4\xb8\xaa\xe7\x8a\xb6\xe6\x80\x81\xe4\xb8\x8b\xe9\x83\xbd\xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x80\xbc\xe5\xbe\x97\xe5\x8a\xa8\xe4\xbd\x9c\n    solution_policy_scores = [\n        run_episode(env, solution_policy, False) for _ in range(100)\n    ]\n    print(""Average score of solution = "", np.mean(solution_policy_scores))\n    # Animate it\n    run_episode(env, solution_policy, True)\n'"
chapter10_Natural-Language-Process/char_rnn/config.py,0,"b'# encoding: utf-8\n""""""\n@author: xyliao\n@contact: xyliao1993@qq.com\n""""""\nimport warnings\nfrom pprint import pprint\n\n\nclass DefaultConfig(object):\n    model = \'CharRNN\'\n\n    # Dataset.\n    txt = \'./dataset/poetry.txt\'\n    len = 20\n    max_vocab = 8000\n    begin = \'\xe5\xa4\xa9\xe9\x9d\x92\xe8\x89\xb2\xe7\xad\x89\xe7\x83\x9f\xe9\x9b\xa8\'  # begin word of text\n    predict_len = 50  # predict length\n\n    # Store result and save models.\n    result_file = \'result.txt\'\n    save_file = \'./checkpoints/\'\n    save_freq = 30  # save model every N epochs\n    save_best = True\n\n    # Predict mode and generate contexts\n    load_model = \'./checkpoints/CharRNN_best_model.pth\'\n    write_file = \'./write_context.txt\'\n\n    # Visualization parameters.\n    vis_dir = \'./vis/\'\n    plot_freq = 100  # plot in tensorboard every N iterations\n\n    # Model parameters.\n    embed_dim = 512\n    hidden_size = 512\n    num_layers = 2\n    dropout = 0.5\n\n    # Model hyperparameters.\n    use_gpu = True  # use GPU or not\n    ctx = 0  # running on which cuda device\n    batch_size = 128  # batch size\n    num_workers = 4  # how many workers for loading data\n    max_epoch = 200\n    lr = 1e-3  # initial learning rate\n    weight_decay = 1e-4\n\n    def _parse(self, kwargs):\n        for k, v in kwargs.items():\n            if not hasattr(self, k):\n                warnings.warn(""Warning: opt has not attribut %s"" % k)\n            setattr(self, k, v)\n\n        print(\'=========user config==========\')\n        pprint(self._state_dict())\n        print(\'============end===============\')\n\n    def _state_dict(self):\n        return {k: getattr(self, k) for k, _ in DefaultConfig.__dict__.items()\n                if not k.startswith(\'_\')}\n\n\nopt = DefaultConfig()\n'"
chapter10_Natural-Language-Process/char_rnn/main.py,13,"b'# encoding: utf-8\n""""""\n@author: xyliao\n@contact: xyliao1993@qq.com\n""""""\nfrom copy import deepcopy\n\nimport numpy as np\nimport torch\nfrom mxtorch import meter\nfrom mxtorch.trainer import Trainer, ScheduledOptim\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nimport models\nfrom config import opt\nfrom data import TextDataset, TextConverter\n\n\ndef get_data(convert):\n    dataset = TextDataset(opt.txt, opt.len, convert.text_to_arr)\n    return DataLoader(dataset, opt.batch_size, shuffle=True, num_workers=opt.num_workers)\n\n\ndef get_model(convert):\n    model = getattr(models, opt.model)(convert.vocab_size,\n                                       opt.embed_dim,\n                                       opt.hidden_size,\n                                       opt.num_layers,\n                                       opt.dropout)\n    if opt.use_gpu:\n        model = model.cuda()\n    return model\n\n\ndef get_loss(score, label):\n    return nn.CrossEntropyLoss()(score, label.view(-1))\n\n\ndef get_optimizer(model):\n    optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr)\n    return ScheduledOptim(optimizer)\n\n\ndef pick_top_n(preds, top_n=5):\n    top_pred_prob, top_pred_label = torch.topk(preds, top_n, 1)\n    top_pred_prob /= torch.sum(top_pred_prob)\n    top_pred_prob = top_pred_prob.squeeze(0).cpu().numpy()\n    top_pred_label = top_pred_label.squeeze(0).cpu().numpy()\n    c = np.random.choice(top_pred_label, size=1, p=top_pred_prob)\n    return c\n\n\nclass CharRNNTrainer(Trainer):\n    def __init__(self, convert):\n        self.convert = convert\n\n        model = get_model(convert)\n        criterion = get_loss\n        optimizer = get_optimizer(model)\n        super().__init__(model, criterion, optimizer)\n        self.config += (\'text: \' + opt.txt + \'\\n\' + \'train text length: \' + str(opt.len) + \'\\n\')\n        self.config += (\'predict text length: \' + str(opt.predict_len) + \'\\n\')\n\n        self.metric_meter[\'loss\'] = meter.AverageValueMeter()\n\n    def train(self, kwargs):\n        self.reset_meter()\n        self.model.train()\n        train_data = kwargs[\'train_data\']\n        for data in tqdm(train_data):\n            x, y = data\n            y = y.long()\n            if opt.use_gpu:\n                x = x.cuda()\n                y = y.cuda()\n            x, y = Variable(x), Variable(y)\n\n            # Forward.\n            score, _ = self.model(x)\n            loss = self.criterion(score, y)\n\n            # Backward.\n            self.optimizer.zero_grad()\n            loss.backward()\n            # Clip gradient.\n            nn.utils.clip_grad_norm(self.model.parameters(), 5)\n            self.optimizer.step()\n\n            self.metric_meter[\'loss\'].add(loss.data[0])\n\n            # Update to tensorboard.\n            if (self.n_iter + 1) % opt.plot_freq == 0:\n                self.writer.add_scalar(\'perplexity\', np.exp(self.metric_meter[\'loss\'].value()[0]), self.n_plot)\n                self.n_plot += 1\n\n            self.n_iter += 1\n\n        # Log the train metrics to dict.\n        self.metric_log[\'perplexity\'] = np.exp(self.metric_meter[\'loss\'].value()[0])\n\n    def test(self, kwargs):\n        """"""Set beginning words and predicted length, using model to generate texts.\n\n        Returns:\n            predicted generating text\n        """"""\n        self.model.eval()\n        begin = np.array([i for i in kwargs[\'begin\']])\n        begin = np.random.choice(begin, size=1)\n        text_len = kwargs[\'predict_len\']\n        samples = [self.convert.word_to_int(c) for c in begin]\n        input_txt = torch.LongTensor(samples)[None]\n        if opt.use_gpu:\n            input_txt = input_txt.cuda()\n        input_txt = Variable(input_txt)\n        _, init_state = self.model(input_txt)\n        result = samples\n        model_input = input_txt[:, -1][:, None]\n        for i in range(text_len):\n            out, init_state = self.model(model_input, init_state)\n            pred = pick_top_n(out.data)\n            model_input = Variable(torch.LongTensor(pred))[None]\n            if opt.use_gpu:\n                model_input = model_input.cuda()\n            result.append(pred[0])\n\n        # Update generating txt to tensorboard.\n        self.writer.add_text(\'text\', self.convert.arr_to_text(result), self.n_plot)\n        self.n_plot += 1\n        print(self.convert.arr_to_text(result))\n\n    def predict(self, begin, predict_len):\n        self.model.eval()\n        samples = [self.convert.word_to_int(c) for c in begin]\n        input_txt = torch.LongTensor(samples)[None]\n        if opt.use_gpu:\n            input_txt = input_txt.cuda()\n        input_txt = Variable(input_txt)\n        _, init_state = self.model(input_txt)\n        result = samples\n        model_input = input_txt[:, -1][:, None]\n        for i in range(predict_len):\n            out, init_state = self.model(model_input, init_state)\n            pred = pick_top_n(out.data)\n            model_input = Variable(torch.LongTensor(pred))[None]\n            if opt.use_gpu:\n                model_input = model_input.cuda()\n            result.append(pred[0])\n        text = self.convert.arr_to_text(result)\n        print(\'Generate text is: {}\'.format(text))\n        with open(opt.write_file, \'a\') as f:\n            f.write(text)\n\n    def load_state_dict(self, checkpoints):\n        self.model.load_state_dict(torch.load(checkpoints))\n\n    def get_best_model(self):\n        if self.metric_log[\'perplexity\'] < self.best_metric:\n            self.best_model = deepcopy(self.model.state_dict())\n            self.best_metric = self.metric_log[\'perplexity\']\n\n\ndef train(**kwargs):\n    opt._parse(kwargs)\n    torch.cuda.set_device(opt.ctx)\n    convert = TextConverter(opt.txt, max_vocab=opt.max_vocab)\n    train_data = get_data(convert)\n    char_rnn_trainer = CharRNNTrainer(convert)\n    char_rnn_trainer.fit(train_data=train_data,\n                         epochs=opt.max_epoch,\n                         begin=opt.begin,\n                         predict_len=opt.predict_len)\n\n\ndef predict(**kwargs):\n    opt._parse(kwargs)\n    torch.cuda.set_device(opt.ctx)\n    convert = TextConverter(opt.txt, max_vocab=opt.max_vocab)\n    char_rnn_trainer = CharRNNTrainer(convert)\n    char_rnn_trainer.load_state_dict(opt.load_model)\n    char_rnn_trainer.predict(opt.begin, opt.predict_len)\n\n\nif __name__ == \'__main__\':\n    import fire\n\n    fire.Fire()\n'"
chapter10_Natural-Language-Process/seq2seq-translation/dataset.py,2,"b'import random\nimport re\nimport string\nimport unicodedata\n\nimport torch\nfrom torch.utils.data import Dataset\n\nSOS_token = 0\nEOS_token = 1\nMAX_LENGTH = 10\n\n\nclass Lang(object):\n    def __init__(self, name):\n        self.name = name\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {0: ""SOS"", 1: ""EOS""}\n        self.n_words = 2  # Count SOS and EOS\n\n    def addSentence(self, sentence):\n        for word in sentence.split(\' \'):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1\n\n\ndef unicodeToAscii(s):\n    return \'\'.join(\n        c for c in unicodedata.normalize(\'NFD\', s)\n        if unicodedata.category(c) != \'Mn\')\n\n\n# Lowercase, trim, and remove non-letter characters\n\n\ndef normalizeString(s):\n    s = unicodeToAscii(s.lower().strip())\n    s = re.sub(r""([.!?])"", r"" \\1"", s)\n    s = re.sub(r""[^a-zA-Z.!?]+"", r"" "", s)\n    return s\n\n\ndef readLangs(lang1, lang2, reverse=False):\n    print(""Reading lines..."")\n\n    # Read the file and split into lines\n    lines = open(\'data/%s-%s.txt\' % (lang1, lang2), encoding=\'utf-8\').\\\n        read().strip().split(\'\\n\')\n\n    # Split every line into pairs and normalize\n    pairs = [[normalizeString(s) for s in l.split(\'\\t\')] for l in lines]\n\n    # Reverse pairs, make Lang instances\n    if reverse:\n        pairs = [list(reversed(p)) for p in pairs]\n        input_lang = Lang(lang2)\n        output_lang = Lang(lang1)\n    else:\n        input_lang = Lang(lang1)\n        output_lang = Lang(lang2)\n\n    return input_lang, output_lang, pairs\n\n\neng_prefixes = (""i am "", ""i m "", ""he is"", ""he s "", ""she is"", ""she s"",\n                ""you are"", ""you re "", ""we are"", ""we re "", ""they are"",\n                ""they re "")\n\n\ndef filterPair(p):\n    return len(p[0].split(\' \')) < MAX_LENGTH and \\\n        len(p[1].split(\' \')) < MAX_LENGTH and \\\n        p[1].startswith(eng_prefixes)\n\n\ndef filterPairs(pairs):\n    return [pair for pair in pairs if filterPair(pair)]\n\n\ndef prepareData(lang1, lang2, reverse=False):\n    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n    print(""Read %s sentence pairs"" % len(pairs))\n    pairs = filterPairs(pairs)\n    print(""Trimmed to %s sentence pairs"" % len(pairs))\n    print(""Counting words..."")\n    for pair in pairs:\n        input_lang.addSentence(pair[0])\n        output_lang.addSentence(pair[1])\n    print(""Counted words:"")\n    print(input_lang.name, input_lang.n_words)\n    print(output_lang.name, output_lang.n_words)\n    print(random.choice(pairs))\n    return input_lang, output_lang, pairs\n\n\ndef indexesFromSentence(lang, sentence):\n    return [lang.word2index[word] for word in sentence.split(\' \')]\n\n\ndef tensorFromSentence(lang, sentence):\n    indexes = indexesFromSentence(lang, sentence)\n    indexes.append(EOS_token)\n    result = torch.LongTensor(indexes)\n    return result\n\n\ndef tensorFromPair(input_lang, output_lang, pair):\n    input_tensor = tensorFromSentence(input_lang, pair[0])\n    target_tensor = tensorFromSentence(output_lang, pair[1])\n    return input_tensor, target_tensor\n\n\nclass TextDataset(Dataset):\n    def __init__(self, dataload=prepareData, lang=[\'eng\', \'fra\']):\n        self.input_lang, self.output_lang, self.pairs = dataload(\n            lang[0], lang[1], reverse=True)\n        self.input_lang_words = self.input_lang.n_words\n        self.output_lang_words = self.output_lang.n_words\n\n    def __getitem__(self, index):\n        return tensorFromPair(self.input_lang, self.output_lang,\n                              self.pairs[index])\n\n    def __len__(self):\n        return len(self.pairs)\n'"
chapter10_Natural-Language-Process/seq2seq-translation/evaluate.py,10,"b""import random\n\nimport torch\nfrom torch.autograd import Variable\n\nfrom dataset import TextDataset\nfrom model.seq2seq import AttnDecoderRNN, DecoderRNN, EncoderRNN\nimport matplotlib.pyplot as plt\nSOS_token = 0\nEOS_token = 1\nMAX_LENGTH = 10\nuse_attn = True\nuse_cuda = torch.cuda.is_available()\nlang_dataset = TextDataset()\nprint('*' * 10)\n\n\ndef evaluate(encoder, decoder, in_lang, max_length=MAX_LENGTH):\n    if use_cuda:\n        in_lang = in_lang.cuda()\n    input_variable = Variable(in_lang)\n    input_variable = input_variable.unsqueeze(0)\n    input_length = input_variable.size(1)\n    encoder_hidden = encoder.initHidden()\n\n    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size))\n    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n\n    for ei in range(input_length):\n        encoder_output, encoder_hidden = encoder(input_variable[:, ei],\n                                                 encoder_hidden)\n        encoder_outputs[ei] = encoder_output[0][0]\n\n    decoder_input = Variable(torch.LongTensor([[SOS_token]]))  # SOS\n    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n\n    decoder_hidden = encoder_hidden\n\n    decoded_words = []\n    decoder_attentions = torch.zeros(max_length, max_length)\n\n    if use_attn:\n        for di in range(max_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            decoder_attentions[di] = decoder_attention.data\n            topv, topi = decoder_output.data.topk(1)\n            ni = topi[0][0]\n            if ni == EOS_token:\n                decoded_words.append('<EOS>')\n                break\n            else:\n                decoded_words.append(lang_dataset.output_lang.index2word[ni])\n\n            decoder_input = Variable(torch.LongTensor([[ni]]))\n            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n    else:\n        for di in range(max_length):\n            decoder_output, decoder_hidden = decoder(decoder_input,\n                                                     decoder_hidden)\n            topv, topi = decoder_output.data.topk(1)\n            ni = topi[0][0]\n            if ni == EOS_token:\n                decoded_words.append('<EOS>')\n                break\n            else:\n                decoded_words.append(lang_dataset.output_lang.index2word[ni])\n\n            decoder_input = Variable(torch.LongTensor([[ni]]))\n            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n    if use_attn:\n        return decoded_words, decoder_attentions[:di + 1]\n    else:\n        return decoded_words\n\n\ndef evaluateRandomly(encoder, decoder, n=10):\n    for i in range(n):\n        pair_idx = random.choice(list(range(len(lang_dataset))))\n        pair = lang_dataset.pairs[pair_idx]\n        in_lang, out_lang = lang_dataset[pair_idx]\n        print('>', pair[0])\n        print('=', pair[1])\n        if use_attn:\n            output_words, attentions = evaluate(encoder, decoder, in_lang)\n        else:\n            output_words = evaluate(encoder, decoder, in_lang)\n        output_sentence = ' '.join(output_words)\n        print('<', output_sentence)\n        print('')\n\n\ninput_size = lang_dataset.input_lang_words\nhidden_size = 256\noutput_size = lang_dataset.output_lang_words\n\nencoder = EncoderRNN(input_size, hidden_size)\nencoder.load_state_dict(torch.load('./encoder.pth'))\nif use_attn:\n    decoder = AttnDecoderRNN(hidden_size, output_size, n_layers=2)\n    decoder.load_state_dict(torch.load('./attn_decoder.pth'))\nelse:\n    decoder = DecoderRNN(hidden_size, output_size, n_layers=2)\n    decoder.load_state_dict(torch.load('./decoder.pth'))\n\nif use_cuda:\n    encoder = encoder.cuda()\n    decoder = decoder.cuda()\n\nevaluateRandomly(encoder, decoder)\n\nif use_attn:\n    pair_idx = random.choice(list(range(len(lang_dataset))))\n    pairs = lang_dataset.pairs[pair_idx]\n    print('>')\n    print(pairs[0])\n    in_lang, out_lang = lang_dataset[pair_idx]\n    output_words, attentions = evaluate(encoder, decoder, in_lang)\n    plt.matshow(attentions.cpu().numpy())\n    plt.show()"""
chapter10_Natural-Language-Process/seq2seq-translation/train.py,16,"b""import time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom torch import nn, optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\nfrom dataset import TextDataset\nfrom model.seq2seq import AttnDecoderRNN, DecoderRNN, EncoderRNN\n\nSOS_token = 0\nEOS_token = 1\nMAX_LENGTH = 10\nlang_dataset = TextDataset()\nlang_dataloader = DataLoader(lang_dataset, shuffle=True)\nprint()\n\ninput_size = lang_dataset.input_lang_words\nhidden_size = 256\noutput_size = lang_dataset.output_lang_words\ntotal_epoch = 20\n\nencoder = EncoderRNN(input_size, hidden_size)\ndecoder = DecoderRNN(hidden_size, output_size, n_layers=2)\nattn_decoder = AttnDecoderRNN(hidden_size, output_size, n_layers=2)\nuse_attn = True\n\nif torch.cuda.is_available():\n    encoder = encoder.cuda()\n    decoder = decoder.cuda()\n    attn_decoder = attn_decoder.cuda()\n\n\ndef showPlot(points):\n    plt.figure()\n    x = np.arange(len(points))\n    plt.plot(x, points)\n    plt.show()\n\n\ndef train(encoder, decoder, total_epoch, use_attn):\n\n    param = list(encoder.parameters()) + list(decoder.parameters())\n    optimizer = optim.Adam(param, lr=1e-3)\n    criterion = nn.NLLLoss()\n    plot_losses = []\n    for epoch in range(total_epoch):\n        since = time.time()\n        running_loss = 0\n        print_loss_total = 0\n        total_loss = 0\n        for i, data in enumerate(lang_dataloader):\n            in_lang, out_lang = data\n            if torch.cuda.is_available():\n                in_lang = in_lang.cuda()\n                out_lang = out_lang.cuda()\n            in_lang = Variable(in_lang)  # batch=1, length\n            out_lang = Variable(out_lang)\n\n            encoder_outputs = Variable(\n                torch.zeros(MAX_LENGTH, encoder.hidden_size))\n            if torch.cuda.is_available():\n                encoder_outputs = encoder_outputs.cuda()\n            encoder_hidden = encoder.initHidden()\n            for ei in range(in_lang.size(1)):\n                encoder_output, encoder_hidden = encoder(\n                    in_lang[:, ei], encoder_hidden)\n                encoder_outputs[ei] = encoder_output[0][0]\n\n            decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n            if torch.cuda.is_available():\n                decoder_input = decoder_input.cuda()\n            decoder_hidden = encoder_hidden\n            loss = 0\n            if use_attn:\n                for di in range(out_lang.size(1)):\n                    decoder_output, decoder_hidden, decoder_attention = attn_decoder(\n                        decoder_input, decoder_hidden, encoder_outputs)\n                    loss += criterion(decoder_output, out_lang[:, di])\n                    topv, topi = decoder_output.data.topk(1)\n                    ni = topi[0][0]\n\n                    decoder_input = Variable(torch.LongTensor([[ni]]))\n                    if torch.cuda.is_available():\n                        decoder_input = decoder_input.cuda()\n                    if ni == EOS_token:\n                        break\n            else:\n                for di in range(out_lang.size(1)):\n                    decoder_output, decoder_hidden = decoder(\n                        decoder_input, decoder_hidden)\n                    loss += criterion(decoder_output, out_lang[:, di])\n                    topv, topi = decoder_output.data.topk(1)\n                    ni = topi[0][0]\n\n                    decoder_input = Variable(torch.LongTensor([[ni]]))\n                    if torch.cuda.is_available():\n                        decoder_input = decoder_input.cuda()\n                    if ni == EOS_token:\n                        break\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.data[0]\n            print_loss_total += loss.data[0]\n            total_loss += loss.data[0]\n            if (i + 1) % 5000 == 0:\n                print('{}/{}, Loss:{:.6f}'.format(\n                    i + 1, len(lang_dataloader), running_loss / 5000))\n                running_loss = 0\n            if (i + 1) % 100 == 0:\n                plot_loss = print_loss_total / 100\n                plot_losses.append(plot_loss)\n                print_loss_total = 0\n        during = time.time() - since\n        print('Finish {}/{} , Loss:{:.6f}, Time:{:.0f}s'.format(\n            epoch + 1, total_epoch, total_loss / len(lang_dataset), during))\n        print()\n    showPlot(plot_losses)\n\n\nif use_attn:\n    train(encoder, attn_decoder, total_epoch, use_attn=True)\nelse:\n    train(encoder, decoder, total_epoch, use_attn=False)\n\nprint('finish training!')\nif use_attn:\n    torch.save(encoder.state_dict(), './encoder.pth')\n    torch.save(attn_decoder.state_dict(), './attn_decoder.pth')\nelse:\n    torch.save(encoder.state_dict(), './encoder.pth')\n    torch.save(decoder.state_dict(), './decoder.pth')\n"""
chapter9_Computer-Vision/Deep-Dream/deepdream.py,3,"b'import numpy as np\nimport torch\nfrom util import showtensor\nimport scipy.ndimage as nd\nfrom torch.autograd import Variable\n\n\ndef objective_L2(dst, guide_features):\n    return dst.data\n\n\ndef make_step(img, model, control=None, distance=objective_L2):\n    mean = np.array([0.485, 0.456, 0.406]).reshape([3, 1, 1])\n    std = np.array([0.229, 0.224, 0.225]).reshape([3, 1, 1])\n\n    learning_rate = 2e-2\n    max_jitter = 32\n    num_iterations = 20\n    show_every = 10\n    end_layer = 3\n    guide_features = control\n\n    for i in range(num_iterations):\n        shift_x, shift_y = np.random.randint(-max_jitter, max_jitter + 1, 2)\n        img = np.roll(np.roll(img, shift_x, -1), shift_y, -2)\n        # apply jitter shift\n        model.zero_grad()\n        img_tensor = torch.Tensor(img)\n        if torch.cuda.is_available():\n            img_variable = Variable(img_tensor.cuda(), requires_grad=True)\n        else:\n            img_variable = Variable(img_tensor, requires_grad=True)\n\n        act_value = model.forward(img_variable, end_layer)\n        diff_out = distance(act_value, guide_features)\n        act_value.backward(diff_out)\n        ratio = np.abs(img_variable.grad.data.cpu().numpy()).mean()\n        learning_rate_use = learning_rate / ratio\n        img_variable.data.add_(img_variable.grad.data * learning_rate_use)\n        img = img_variable.data.cpu().numpy()  # b, c, h, w\n        img = np.roll(np.roll(img, -shift_x, -1), -shift_y, -2)\n        img[0, :, :, :] = np.clip(img[0, :, :, :], -mean / std,\n                                  (1 - mean) / std)\n        if i == 0 or (i + 1) % show_every == 0:\n            showtensor(img)\n    return img\n\n\ndef dream(model,\n          base_img,\n          octave_n=6,\n          octave_scale=1.4,\n          control=None,\n          distance=objective_L2):\n    octaves = [base_img]\n    for i in range(octave_n - 1):\n        octaves.append(\n            nd.zoom(\n                octaves[-1], (1, 1, 1.0 / octave_scale, 1.0 / octave_scale),\n                order=1))\n\n    detail = np.zeros_like(octaves[-1])\n    for octave, octave_base in enumerate(octaves[::-1]):\n        h, w = octave_base.shape[-2:]\n        if octave > 0:\n            h1, w1 = detail.shape[-2:]\n            detail = nd.zoom(\n                detail, (1, 1, 1.0 * h / h1, 1.0 * w / w1), order=1)\n\n        input_oct = octave_base + detail\n        print(input_oct.shape)\n        out = make_step(input_oct, model, control, distance=distance)\n        detail = out - octave_base\n'"
chapter9_Computer-Vision/Deep-Dream/resnet.py,6,"b'__author__ = \'SherlockLiao\'\n\nimport torch\nfrom torch import nn\nfrom torchvision import models\nimport torch.utils.model_zoo as model_zoo\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass CustomResNet(models.resnet.ResNet):\n    def forward(self, x, end_layer):\n        """"""\n        end_layer range from 1 to 4\n        """"""\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        layers = [self.layer1, self.layer2, self.layer3, self.layer4]\n        for i in range(end_layer):\n            x = layers[i](x)\n        return x\n\n\ndef resnet50(pretrained=False, **kwargs):\n    model = CustomResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n'"
chapter9_Computer-Vision/Deep-Dream/util.py,0,"b""import PIL.Image\r\nfrom io import BytesIO\r\nfrom IPython.display import clear_output, Image, display\r\nimport numpy as np\r\n\r\n\r\ndef showarray(a, fmt='jpeg'):\r\n    a = np.uint8(np.clip(a, 0, 255))\r\n    f = BytesIO()\r\n    PIL.Image.fromarray(a).save(f, fmt)\r\n    display(Image(data=f.getvalue()))\r\n\r\n\r\ndef showtensor(a):\r\n    mean = np.array([0.485, 0.456, 0.406]).reshape([1, 1, 3])\r\n    std = np.array([0.229, 0.224, 0.225]).reshape([1, 1, 3])\r\n    inp = a[0, :, :, :]\r\n    inp = inp.transpose(1, 2, 0)\r\n    inp = std * inp + mean\r\n    inp *= 255\r\n    showarray(inp)\r\n    clear_output(wait=True)\r\n"""
chapter9_Computer-Vision/fine_tune/config.py,0,"b'# encoding: utf-8\n""""""\n@author: xyliao\n@contact: xyliao1993@qq.com\n""""""\nimport warnings\nfrom pprint import pprint\n\n\nclass DefaultConfig(object):\n    model = \'resnet50\'\n    # Dataset.\n    train_data_path = \'./hymenoptera_data/train/\'\n    test_data_path = \'./hymenoptera_data/val/\'\n\n    # Store result and save models.\n    # result_file = \'result.txt\'\n    save_file = \'./checkpoints/\'\n    save_freq = 30  # save model every N epochs\n    save_best = True  # If save best test metric model.\n\n    # Visualization results on tensorboard.\n    # vis_dir = \'./vis/\'\n    plot_freq = 100  # plot in tensorboard every N iterations\n\n    # Model hyperparameters.\n    use_gpu = True  # use GPU or not\n    ctx = 0  # running on which cuda device\n    batch_size = 64  # batch size\n    num_workers = 4  # how many workers for loading data\n    max_epoch = 30\n    lr = 1e-2  # initial learning rate\n    momentum = 0\n    weight_decay = 1e-4\n    lr_decay = 0.95\n    # lr_decay_freq = 10\n\n    def _parse(self, kwargs):\n        for k, v in kwargs.items():\n            if not hasattr(self, k):\n                warnings.warn(""Warning: opt has not attribut %s"" % k)\n            setattr(self, k, v)\n\n        print(\'=========user config==========\')\n        pprint(self._state_dict())\n        print(\'============end===============\')\n\n    def _state_dict(self):\n        return {k: getattr(self, k) for k, _ in DefaultConfig.__dict__.items()\n                if not k.startswith(\'_\')}\n\n\nopt = DefaultConfig()\n'"
chapter9_Computer-Vision/fine_tune/main.py,5,"b'# encoding: utf-8\n""""""\n@author: xyliao\n@contact: xyliao1993@qq.com\n""""""\nimport copy\n\nimport torch\nfrom config import opt\nfrom mxtorch import meter\nfrom mxtorch import transforms as tfs\nfrom mxtorch.trainer import *\nfrom mxtorch.vision import model_zoo\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom tqdm import tqdm\n\ntrain_tf = tfs.Compose([\n    tfs.RandomResizedCrop(224),\n    tfs.RandomHorizontalFlip(),\n    tfs.ToTensor(),\n    tfs.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n\ndef test_tf(img):\n    img = tfs.Resize(256)(img)\n    img, _ = tfs.CenterCrop(224)(img)\n    normalize = tfs.Compose([\n        tfs.ToTensor(),\n        tfs.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    img = normalize(img)\n    return img\n\n\ndef get_train_data():\n    train_set = ImageFolder(opt.train_data_path, train_tf)\n    return DataLoader(\n        train_set, opt.batch_size, True, num_workers=opt.num_workers)\n\n\ndef get_test_data():\n    test_set = ImageFolder(opt.test_data_path, test_tf)\n    return DataLoader(\n        test_set, opt.batch_size, True, num_workers=opt.num_workers)\n\n\ndef get_model():\n    model = model_zoo.resnet50(pretrained=True)\n    model.fc = nn.Linear(2048, 2)\n    if opt.use_gpu:\n        model = model.cuda(opt.ctx)\n    return model\n\n\ndef get_loss(score, label):\n    return nn.CrossEntropyLoss()(score, label)\n\n\ndef get_optimizer(model):\n    optimizer = torch.optim.SGD(\n        model.parameters(),\n        lr=opt.lr,\n        momentum=opt.momentum,\n        weight_decay=opt.weight_decay)\n    return ScheduledOptim(optimizer)\n\n\nclass FineTuneTrainer(Trainer):\n    def __init__(self):\n        model = get_model()\n        criterion = get_loss\n        optimizer = get_optimizer(model)\n        super().__init__(model, criterion, optimizer)\n\n        self.metric_meter[\'loss\'] = meter.AverageValueMeter()\n        self.metric_meter[\'acc\'] = meter.AverageValueMeter()\n\n    def train(self, kwargs):\n        self.reset_meter()\n        self.model.train()\n        train_data = kwargs[\'train_data\']\n        for data in tqdm(train_data):\n            img, label = data\n            if opt.use_gpu:\n                img = img.cuda(opt.ctx)\n                label = label.cuda(opt.ctx)\n            img = Variable(img)\n            label = Variable(label)\n\n            # Forward.\n            score = self.model(img)\n            loss = self.criterion(score, label)\n\n            # Backward.\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n            # Update meters.\n            acc = (score.max(1)[1] == label).float().mean()\n            self.metric_meter[\'loss\'].add(loss.data[0])\n            self.metric_meter[\'acc\'].add(acc.data[0])\n\n            # Update to tensorboard.\n            if (self.n_iter + 1) % opt.plot_freq == 0:\n                self.writer.add_scalars(\n                    \'loss\', {\'train\': self.metric_meter[\'loss\'].value()[0]},\n                    self.n_plot)\n                self.writer.add_scalars(\n                    \'acc\', {\'train\': self.metric_meter[\'acc\'].value()[0]},\n                    self.n_plot)\n                self.n_plot += 1\n            self.n_iter += 1\n\n        # Log the train metric dict to print result.\n        self.metric_log[\'train loss\'] = self.metric_meter[\'loss\'].value()[0]\n        self.metric_log[\'train acc\'] = self.metric_meter[\'acc\'].value()[0]\n\n    def test(self, kwargs):\n        self.reset_meter()\n        self.model.eval()\n        test_data = kwargs[\'test_data\']\n        for data in tqdm(test_data):\n            img, label = data\n            if opt.use_gpu:\n                img = img.cuda(opt.ctx)\n                label = label.cuda(opt.ctx)\n            img = Variable(img, volatile=True)\n            label = Variable(label, volatile=True)\n\n            score = self.model(img)\n            loss = self.criterion(score, label)\n            acc = (score.max(1)[1] == label).float().mean()\n\n            self.metric_meter[\'loss\'].add(loss.data[0])\n            self.metric_meter[\'acc\'].add(acc.data[0])\n\n        # Update to tensorboard.\n        self.writer.add_scalars(\'loss\',\n                                {\'test\': self.metric_meter[\'loss\'].value()[0]},\n                                self.n_plot)\n        self.writer.add_scalars(\n            \'acc\', {\'test\': self.metric_meter[\'acc\'].value()[0]}, self.n_plot)\n        self.n_plot += 1\n\n        # Log the test metric to dict.\n        self.metric_log[\'test loss\'] = self.metric_meter[\'loss\'].value()[0]\n        self.metric_log[\'test acc\'] = self.metric_meter[\'acc\'].value()[0]\n\n    def get_best_model(self):\n        if self.metric_log[\'test loss\'] < self.best_metric:\n            self.best_model = copy.deepcopy(self.model.state_dict())\n            self.best_metric = self.metric_log[\'test loss\']\n\n\ndef train(**kwargs):\n    opt._parse(kwargs)\n\n    train_data = get_train_data()\n    test_data = get_test_data()\n\n    fine_tune_trainer = FineTuneTrainer()\n    fine_tune_trainer.fit(train_data=train_data, test_data=test_data)\n\n\nif __name__ == \'__main__\':\n    import fire\n\n    fire.Fire()\n'"
chapter9_Computer-Vision/neural-transfer/build_model.py,4,"b""import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nimport loss\n\nvgg = models.vgg19(pretrained=True).features\nif torch.cuda.is_available():\n    vgg = vgg.cuda()\n\ncontent_layers_default = ['conv_4']\nstyle_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n\n\ndef get_style_model_and_loss(style_img,\n                             content_img,\n                             cnn=vgg,\n                             style_weight=1000,\n                             content_weight=1,\n                             content_layers=content_layers_default,\n                             style_layers=style_layers_default):\n\n    content_loss_list = []\n    style_loss_list = []\n\n    model = nn.Sequential()\n    if torch.cuda.is_available():\n        model = model.cuda()\n    gram = loss.Gram()\n    if torch.cuda.is_available():\n        gram = gram.cuda()\n\n    i = 1\n    for layer in cnn:\n        if isinstance(layer, nn.Conv2d):\n            name = 'conv_' + str(i)\n            model.add_module(name, layer)\n\n            if name in content_layers_default:\n                target = model(content_img)\n                content_loss = loss.Content_Loss(target, content_weight)\n                model.add_module('content_loss_' + str(i), content_loss)\n                content_loss_list.append(content_loss)\n\n            if name in style_layers_default:\n                target = model(style_img)\n                target = gram(target)\n                style_loss = loss.Style_Loss(target, style_weight)\n                model.add_module('style_loss_' + str(i), style_loss)\n                style_loss_list.append(style_loss)\n\n            i += 1\n        if isinstance(layer, nn.MaxPool2d):\n            name = 'pool_' + str(i)\n            model.add_module(name, layer)\n\n        if isinstance(layer, nn.ReLU):\n            name = 'relu' + str(i)\n            model.add_module(name, layer)\n\n    return model, style_loss_list, content_loss_list\n"""
chapter9_Computer-Vision/neural-transfer/load_img.py,0,"b""import PIL.Image as Image\nimport torchvision.transforms as transforms\n\nimg_size = 512\n\n\ndef load_img(img_path):\n    img = Image.open(img_path).convert('RGB')\n    img = img.resize((img_size, img_size))\n    img = transforms.ToTensor()(img)\n    img = img.unsqueeze(0)\n    return img\n\n\ndef show_img(img):\n    img = img.squeeze(0)\n    img = transforms.ToPILImage()(img)\n    img.show()\n"""
chapter9_Computer-Vision/neural-transfer/loss.py,2,"b'import torch.nn as nn\nimport torch\n\n\nclass Content_Loss(nn.Module):\n    def __init__(self, target, weight):\n        super(Content_Loss, self).__init__()\n        self.weight = weight\n        self.target = target.detach() * self.weight\n        # \xe5\xbf\x85\xe9\xa1\xbb\xe8\xa6\x81\xe7\x94\xa8detach\xe6\x9d\xa5\xe5\x88\x86\xe7\xa6\xbb\xe5\x87\xbatarget\xef\xbc\x8c\xe8\xbf\x99\xe6\x97\xb6\xe5\x80\x99target\xe4\xb8\x8d\xe5\x86\x8d\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaaVariable\xef\xbc\x8c\xe8\xbf\x99\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe5\x8a\xa8\xe6\x80\x81\xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99forward\xe4\xbc\x9a\xe5\x87\xba\xe9\x94\x99\xef\xbc\x8c\xe4\xb8\x8d\xe8\x83\xbd\xe5\x90\x91\xe5\x89\x8d\xe4\xbc\xa0\xe6\x92\xad\n        self.criterion = nn.MSELoss()\n\n    def forward(self, input):\n        self.loss = self.criterion(input * self.weight, self.target)\n        out = input.clone()\n        return out\n\n    def backward(self, retain_variabels=True):\n        self.loss.backward(retain_variables=retain_variabels)\n        return self.loss\n\n\nclass Gram(nn.Module):\n    def __init__(self):\n        super(Gram, self).__init__()\n\n    def forward(self, input):\n        a, b, c, d = input.size()\n        feature = input.view(a * b, c * d)\n        gram = torch.mm(feature, feature.t())\n        gram /= (a * b * c * d)\n        return gram\n\n\nclass Style_Loss(nn.Module):\n    def __init__(self, target, weight):\n        super(Style_Loss, self).__init__()\n        self.weight = weight\n        self.target = target.detach() * self.weight\n        self.gram = Gram()\n        self.criterion = nn.MSELoss()\n\n    def forward(self, input):\n        G = self.gram(input) * self.weight\n        self.loss = self.criterion(G, self.target)\n        out = input.clone()\n        return out\n\n    def backward(self, retain_variabels=True):\n        self.loss.backward(retain_variables=retain_variabels)\n        return self.loss\n'"
chapter9_Computer-Vision/neural-transfer/run_code.py,2,"b'import torch.nn as nn\nimport torch.optim as optim\n\nfrom build_model import get_style_model_and_loss\n\n\ndef get_input_param_optimier(input_img):\n    """"""\n    input_img is a Variable\n    """"""\n    input_param = nn.Parameter(input_img.data)\n    optimizer = optim.LBFGS([input_param])\n    return input_param, optimizer\n\n\ndef run_style_transfer(content_img, style_img, input_img, num_epoches=300):\n    print(\'Building the style transfer model..\')\n    model, style_loss_list, content_loss_list = get_style_model_and_loss(\n        style_img, content_img)\n    input_param, optimizer = get_input_param_optimier(input_img)\n\n    print(\'Opimizing...\')\n    epoch = [0]\n    while epoch[0] < num_epoches:\n\n        def closure():\n            input_param.data.clamp_(0, 1)\n\n            model(input_param)\n            style_score = 0\n            content_score = 0\n\n            optimizer.zero_grad()\n            for sl in style_loss_list:\n                style_score += sl.backward()\n            for cl in content_loss_list:\n                content_score += cl.backward()\n\n            epoch[0] += 1\n            if epoch[0] % 50 == 0:\n                print(\'run {}\'.format(epoch))\n                print(\'Style Loss: {:.4f} Content Loss: {:.4f}\'.format(\n                    style_score.data[0], content_score.data[0]))\n                print()\n\n            return style_score + content_score\n\n        optimizer.step(closure)\n\n        input_param.data.clamp_(0, 1)\n\n    return input_param.data\n'"
chapter9_Computer-Vision/segmentation/config.py,0,"b'# encoding: utf-8\n""""""\n@author: xyliao\n@contact: xyliao1993@qq.com\n""""""\nimport warnings\nfrom pprint import pprint\n\n\nclass DefaultConfig(object):\n    model = \'FcnResNet\'\n\n    # Dataset.\n    voc_root = \'./dataset/VOCdevkit/VOC2012/\'\n    crop_size = (320, 480)\n    num_classes = 21\n\n    # Store result and save models.\n    result_file = \'result.txt\'\n    save_file = \'./checkpoints/\'\n    save_freq = 20  # save model every N epochs\n    save_best = True  # If save best test metric model.\n\n    # Visualization parameters.\n    vis_dir = \'./vis/\'\n    plot_freq = 30  # plot in tensorboard every N iterations\n\n    # Model hyperparameters.\n    use_gpu = True  # use GPU or not\n    ctx = 0  # running on which cuda device\n    batch_size = 32  # batch size\n    num_workers = 4  # how many workers for loading data\n    max_epoch = 80\n    lr = 1e-2  # initial learning rate\n    lr_decay = 0.1\n    lr_decay_freq = 50\n    weight_decay = 1e-4\n\n    def _parse(self, kwargs):\n        for k, v in kwargs.items():\n            if not hasattr(self, k):\n                warnings.warn(""Warning: opt has not attribute %s"" % k)\n            setattr(self, k, v)\n\n        print(\'=========user config==========\')\n        pprint(self._state_dict())\n        print(\'============end===============\')\n\n    def _state_dict(self):\n        return {\n            k: getattr(self, k)\n            for k, _ in DefaultConfig.__dict__.items() if not k.startswith(\'_\')\n        }\n\n\nopt = DefaultConfig()\n'"
chapter9_Computer-Vision/segmentation/main.py,8,"b'# encoding: utf-8\n""""""\n@author: xyliao\n@contact: xyliao1993@qq.com\n""""""\nimport warnings\nfrom copy import deepcopy\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom mxtorch import meter\nfrom mxtorch.trainer import Trainer, ScheduledOptim\nfrom mxtorch.vision.eval_tools import eval_semantic_segmentation\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nimport models\nfrom config import opt\nfrom data import VocSegDataset, img_transforms, COLORMAP, inverse_normalization\n\nwarnings.filterwarnings(\'ignore\')\n\ncm = np.array(COLORMAP, dtype=np.uint8)\n\n\ndef get_data(is_train):\n    voc_data = VocSegDataset(opt.voc_root, is_train, opt.crop_size,\n                             img_transforms)\n    return DataLoader(\n        voc_data, opt.batch_size, True, num_workers=opt.num_workers)\n\n\ndef get_model(num_classes):\n    model = getattr(models, opt.model)(num_classes)\n    if opt.use_gpu:\n        model.cuda()\n    return model\n\n\ndef get_optimizer(model):\n    optimizer = torch.optim.SGD(\n        model.parameters(), lr=opt.lr, weight_decay=opt.weight_decay)\n    return ScheduledOptim(optimizer)\n\n\ndef get_loss(scores, labels):\n    scores = F.log_softmax(scores, dim=1)\n    return torch.nn.NLLLoss2d()(scores, labels)\n\n\nall_metrcis = [\'loss\', \'acc\', \'iou\']\n\n\nclass FcnTrainer(Trainer):\n    def __init__(self):\n        model = get_model(opt.num_classes)\n        criterion = get_loss\n        optimizer = get_optimizer(model)\n\n        super().__init__(model=model, criterion=criterion, optimizer=optimizer)\n\n        self.config += (\'Crop size: \' + str(opt.crop_size) + \'\\n\')\n        self.best_metric = 0\n        for m in all_metrcis:\n            self.metric_meter[m] = meter.AverageValueMeter()\n\n    def train(self, kwargs):\n        self.reset_meter()\n        self.model.train()\n        train_data = kwargs[\'train_data\']\n        for data in tqdm(train_data):\n            imgs, labels = data\n            if opt.use_gpu:\n                imgs = imgs.cuda()\n                labels = labels.cuda()\n            imgs = Variable(imgs)\n            labels = Variable(labels)\n\n            # Forward.\n            scores = self.model(imgs)\n            loss = self.criterion(scores, labels)\n\n            # Backward.\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n            # Update to metrics.\n            pred_labels = scores.max(dim=1)[1].data.cpu().numpy()\n            pred_labels = [i for i in pred_labels]\n\n            true_labels = labels.data.cpu().numpy()\n            true_labels = [i for i in true_labels]\n\n            eval_metrics = eval_semantic_segmentation(pred_labels, true_labels)\n            self.metric_meter[\'loss\'].add(loss.data[0])\n            self.metric_meter[\'acc\'].add(eval_metrics[\'mean_class_accuracy\'])\n            self.metric_meter[\'iou\'].add(eval_metrics[\'miou\'])\n\n            if (self.n_iter + 1) % opt.plot_freq == 0:\n                # Plot metrics curve in tensorboard.\n                self.writer.add_scalars(\n                    \'loss\', {\'train\': self.metric_meter[\'loss\'].value()[0]},\n                    self.n_plot)\n                self.writer.add_scalars(\n                    \'acc\', {\'train\': self.metric_meter[\'acc\'].value()[0]},\n                    self.n_plot)\n                self.writer.add_scalars(\n                    \'iou\', {\'train\': self.metric_meter[\'iou\'].value()[0]},\n                    self.n_plot)\n\n                # Show segmentation images.\n                # Get prediction segmentation and ground truth segmentation.\n                origin_image = inverse_normalization(imgs[0].cpu().data)\n                pred_seg = cm[pred_labels[0]]\n                gt_seg = cm[true_labels[0]]\n\n                self.writer.add_image(\'train ori_img\', origin_image,\n                                      self.n_plot)\n                self.writer.add_image(\'train gt\', gt_seg, self.n_plot)\n                self.writer.add_image(\'train pred\', pred_seg, self.n_plot)\n                self.n_plot += 1\n\n            self.n_iter += 1\n\n        self.metric_log[\'Train Loss\'] = self.metric_meter[\'loss\'].value()[0]\n        self.metric_log[\'Train Mean Class Accuracy\'] = self.metric_meter[\n            \'acc\'].value()[0]\n        self.metric_log[\'Train Mean IoU\'] = self.metric_meter[\'iou\'].value()[0]\n\n    def test(self, kwargs):\n        self.reset_meter()\n        self.model.eval()\n        test_data = kwargs[\'test_data\']\n        for data in tqdm(test_data):\n            imgs, labels = data\n            if opt.use_gpu:\n                imgs = imgs.cuda()\n                labels = labels.cuda()\n            imgs = Variable(imgs, volatile=True)\n            labels = Variable(labels, volatile=True)\n\n            # Forward.\n            scores = self.model(imgs)\n            loss = self.criterion(scores, labels)\n\n            # Update to metrics.\n            pred_labels = scores.max(dim=1)[1].data.cpu().numpy()\n            pred_labels = [i for i in pred_labels]\n\n            true_labels = labels.data.cpu().numpy()\n            true_labels = [i for i in true_labels]\n\n            eval_metrics = eval_semantic_segmentation(pred_labels, true_labels)\n            self.metric_meter[\'loss\'].add(loss.data[0])\n            self.metric_meter[\'acc\'].add(eval_metrics[\'mean_class_accuracy\'])\n            self.metric_meter[\'iou\'].add(eval_metrics[\'miou\'])\n\n        # Plot metrics curve in tensorboard.\n        self.writer.add_scalars(\'loss\',\n                                {\'test\': self.metric_meter[\'loss\'].value()[0]},\n                                self.n_plot)\n        self.writer.add_scalars(\n            \'acc\', {\'test\': self.metric_meter[\'acc\'].value()[0]}, self.n_plot)\n        self.writer.add_scalars(\n            \'iou\', {\'test\': self.metric_meter[\'iou\'].value()[0]}, self.n_plot)\n\n        origin_img = inverse_normalization(imgs[0].cpu().data)\n        pred_seg = cm[pred_labels[0]]\n        gt_seg = cm[true_labels[0]]\n        self.writer.add_image(\'test ori_img\', origin_img, self.n_plot)\n        self.writer.add_image(\'test gt\', gt_seg, self.n_plot)\n        self.writer.add_image(\'test pred\', pred_seg, self.n_plot)\n\n        self.n_plot += 1\n\n        self.metric_log[\'Test Loss\'] = self.metric_meter[\'loss\'].value()[0]\n        self.metric_log[\'Test Mean Class Accuracy\'] = self.metric_meter[\n            \'acc\'].value()[0]\n        self.metric_log[\'Test Mean IoU\'] = self.metric_meter[\'iou\'].value()[0]\n\n    def get_best_model(self):\n        if self.metric_log[\'Test Mean IoU\'] > self.best_metric:\n            self.best_model = deepcopy(self.model.state_dict())\n            self.best_metric = self.metric_log[\'Test Mean IoU\']\n\n\ndef train(**kwargs):\n    opt._parse(kwargs)\n\n    # Set default cuda device.\n    torch.cuda.set_device(opt.ctx)\n\n    fcn_trainer = FcnTrainer()\n    train_data = get_data(is_train=True)\n    test_data = get_data(is_train=False)\n    fcn_trainer.fit(\n        train_data=train_data, test_data=test_data, epochs=opt.max_epoch)\n\n\nif __name__ == \'__main__\':\n    import fire\n\n    fire.Fire()\n'"
chapter10_Natural-Language-Process/char_rnn/data/__init__.py,0,"b'# encoding: utf-8\n""""""\n@author: xyliao\n@contact: xyliao1993@qq.com\n""""""\nfrom .dataset import TextConverter, TextDataset\n'"
chapter10_Natural-Language-Process/char_rnn/data/dataset.py,2,"b'# encoding: utf-8\n""""""\n@author: xyliao\n@contact: xyliao1993@qq.com\n\nThis file is utils to convert text to index and create dataset to PyTorch training model.\n""""""\n\nimport numpy as np\nimport torch\n\n\nclass TextConverter(object):\n    def __init__(self, text_path, max_vocab=5000):\n        """"""Construct a text index converter.\n\n        Args:\n            text_path: txt file path.\n            max_vocab: maximum number of words.\n        """"""\n\n        with open(text_path, \'r\') as f:\n            text = f.read()\n        text = text.replace(\'\\n\', \' \').replace(\'\\r\', \' \').replace(\'\xef\xbc\x8c\', \' \').replace(\'\xe3\x80\x82\', \' \')\n        vocab = set(text)\n        # If the number of words is larger than limit, clip the words with minimum frequency.\n        vocab_count = {}\n        for word in vocab:\n            vocab_count[word] = 0\n        for word in text:\n            vocab_count[word] += 1\n        vocab_count_list = []\n        for word in vocab_count:\n            vocab_count_list.append((word, vocab_count[word]))\n        vocab_count_list.sort(key=lambda x: x[1], reverse=True)\n        if len(vocab_count_list) > max_vocab:\n            vocab_count_list = vocab_count_list[:max_vocab]\n        vocab = [x[0] for x in vocab_count_list]\n        self.vocab = vocab\n\n        self.word_to_int_table = {c: i for i, c in enumerate(self.vocab)}\n        self.int_to_word_table = dict(enumerate(self.vocab))\n\n    @property\n    def vocab_size(self):\n        return len(self.vocab) + 1\n\n    def word_to_int(self, word):\n        if word in self.word_to_int_table:\n            return self.word_to_int_table[word]\n        else:\n            return len(self.vocab)\n\n    def int_to_word(self, index):\n        if index == len(self.vocab):\n            return \'<unk>\'\n        elif index < len(self.vocab):\n            return self.int_to_word_table[index]\n        else:\n            raise Exception(\'Unknown index!\')\n\n    def text_to_arr(self, text):\n        arr = []\n        for word in text:\n            arr.append(self.word_to_int(word))\n        return np.array(arr)\n\n    def arr_to_text(self, arr):\n        words = []\n        for index in arr:\n            words.append(self.int_to_word(index))\n        return """".join(words)\n\n\nclass TextDataset(object):\n    def __init__(self, text_path, n_step, arr_to_idx):\n\n        with open(text_path, \'r\') as f:\n            text = f.read()\n        text = text.replace(\'\\n\', \' \').replace(\'\\r\', \' \').replace(\'\xef\xbc\x8c\', \' \').replace(\'\xe3\x80\x82\', \' \')\n        num_seq = int(len(text) / n_step)\n        self.num_seq = num_seq\n        self.n_step = n_step\n        # Clip more than maximum length.\n        text = text[:num_seq * n_step]\n        arr = arr_to_idx(text)\n        arr = arr.reshape((num_seq, -1))\n        self.arr = torch.from_numpy(arr)\n\n    def __getitem__(self, item):\n        x = self.arr[item, :]\n        y = torch.zeros(x.shape)\n        y[:-1], y[-1] = x[1:], x[0]\n        return x, y\n\n    def __len__(self):\n        return self.num_seq\n'"
chapter10_Natural-Language-Process/char_rnn/models/__init__.py,0,"b'# encoding: utf-8\n""""""\n@author: xyliao\n@contact: xyliao1993@qq.com\n""""""\nfrom .char_rnn import CharRNN\n'"
chapter10_Natural-Language-Process/char_rnn/models/char_rnn.py,2,"b'# encoding: utf-8\n""""""\n@author: xyliao\n@contact: xyliao1993@qq.com\n""""""\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\nfrom config import opt\n\n\nclass CharRNN(nn.Module):\n    def __init__(self, num_classes, embed_dim, hidden_size, num_layers,\n                 dropout):\n        super().__init__()\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n\n        self.word_to_vec = nn.Embedding(num_classes, embed_dim)\n        self.rnn = nn.GRU(embed_dim, hidden_size, num_layers, dropout)\n        self.project = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x, hs=None):\n        batch = x.shape[0]\n        if hs is None:\n            hs = Variable(\n                torch.zeros(self.num_layers, batch, self.hidden_size))\n            if opt.use_gpu:\n                hs = hs.cuda()\n        word_embed = self.word_to_vec(x)  # (batch, len, embed)\n        word_embed = word_embed.permute(1, 0, 2)  # (len, batch, embed)\n        out, h0 = self.rnn(word_embed, hs)  # (len, batch, hidden)\n        le, mb, hd = out.shape\n        out = out.view(le * mb, hd)\n        out = self.project(out)\n        out = out.view(le, mb, -1)\n        out = out.permute(1, 0, 2).contiguous()  # (batch, len, hidden)\n        return out.view(-1, out.shape[2]), h0\n'"
chapter10_Natural-Language-Process/seq2seq-translation/model/__init__.py,0,b''
chapter10_Natural-Language-Process/seq2seq-translation/model/seq2seq.py,9,"b""import torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.autograd import Variable\n\nMAX_LENGTH = 10\nuse_cuda = torch.cuda.is_available()\n\n\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, n_layers=1):\n        super(EncoderRNN, self).__init__()\n        self.n_layers = n_layers\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n\n    def forward(self, input, hidden):\n        input = input.unsqueeze(1)\n        embedded = self.embedding(input)  # batch, hidden\n        output = embedded.permute(1, 0, 2)\n        for i in range(self.n_layers):\n            output, hidden = self.gru(output, hidden)\n        return output, hidden\n\n    def initHidden(self):\n        result = Variable(torch.zeros(1, 1, self.hidden_size))\n        if use_cuda:\n            return result.cuda()\n        else:\n            return result\n\n\nclass DecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size, n_layers=1):\n        super(DecoderRNN, self).__init__()\n        self.n_layers = n_layers\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax()\n\n    def forward(self, input, hidden):\n        output = self.embedding(input)  # batch, 1, hidden\n        output = output.permute(1, 0, 2)  # 1, batch, hidden\n        for i in range(self.n_layers):\n            output = F.relu(output)\n            output, hidden = self.gru(output, hidden)\n        output = self.softmax(self.out(output[0]))\n        return output, hidden\n\n    def initHidden(self):\n        result = Variable(torch.zeros(1, 1, self.hidden_size))\n        if use_cuda:\n            return result.cuda()\n        else:\n            return result\n\n\nclass AttnDecoderRNN(nn.Module):\n    def __init__(self,\n                 hidden_size,\n                 output_size,\n                 n_layers=1,\n                 dropout_p=0.1,\n                 max_length=MAX_LENGTH):\n        super(AttnDecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n        self.dropout = nn.Dropout(self.dropout_p)\n        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n        self.out = nn.Linear(self.hidden_size, self.output_size)\n\n    def forward(self, input, hidden, encoder_outputs):\n        '''\n        input: batch, 1\n        hidden: 1, batch, hidden\n        encoder_outputs: length, hidden\n        '''\n        embedded = self.embedding(input)  # batch, 1, hidden\n        embedded = self.dropout(embedded)\n        embedded = embedded.squeeze(1)  # batch, hidden\n\n        attn_weights = F.softmax(\n            self.attn(torch.cat((embedded, hidden[0]), 1)))\n        # batch, max_length\n        encoder_outputs = encoder_outputs.unsqueeze(0)\n        # batch, max_length, hidden\n        attn_applied = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n        # batch, 1, hidden\n        output = torch.cat((embedded, attn_applied.squeeze(1)), 1)\n        # batch, 2xhidden\n        output = self.attn_combine(output).unsqueeze(0)\n        #1, batch, hidden\n\n        for i in range(self.n_layers):\n            output = F.relu(output)\n            output, hidden = self.gru(output, hidden)\n\n        output = F.log_softmax(self.out(output.squeeze(0)))\n        return output, hidden, attn_weights\n\n    def initHidden(self):\n        result = Variable(torch.zeros(1, 1, self.hidden_size))\n        if use_cuda:\n            return result.cuda()\n        else:\n            return result\n"""
chapter9_Computer-Vision/Deep-Dream/backward/backward.py,15,"b""import torch\nfrom torch.autograd import Variable\n\n# simple gradient\na = Variable(torch.FloatTensor([2, 3]), requires_grad=True)\nb = a + 3\nc = b * b * 3\nout = c.mean()\nout.backward()\nprint('*' * 10)\nprint('=====simple gradient======')\nprint('input')\nprint(a.data)\nprint('compute result is')\nprint(out.data[0])\nprint('input gradients are')\nprint(a.grad.data)\n\n# backward on non-scalar output\nm = Variable(torch.FloatTensor([[2, 3]]), requires_grad=True)\nn = Variable(torch.zeros(1, 2))\nn[0, 0] = m[0, 0]**2\nn[0, 1] = m[0, 1]**3\nn.backward(torch.FloatTensor([[1, 1]]))\nprint('*' * 10)\nprint('=====non scalar output======')\nprint('input')\nprint(m.data)\nprint('input gradients are')\nprint(m.grad.data)\n\n# jacobian\nj = torch.zeros(2, 2)\nk = Variable(torch.zeros(1, 2))\nm.grad.data.zero_()\nk[0, 0] = m[0, 0]**2 + 3 * m[0, 1]\nk[0, 1] = m[0, 1]**2 + 2 * m[0, 0]\nk.backward(torch.FloatTensor([[1, 0]]), retain_variables=True)\nj[:, 0] = m.grad.data\nm.grad.data.zero_()\nk.backward(torch.FloatTensor([[0, 1]]))\nj[:, 1] = m.grad.data\nprint('jacobian matrix is')\nprint(j)\n\n# compute jacobian matrix\nx = torch.FloatTensor([2, 1]).view(1, 2)\nx = Variable(x, requires_grad=True)\ny = Variable(torch.FloatTensor([[1, 2], [3, 4]]))\n\nz = torch.mm(x, y)\njacobian = torch.zeros((2, 2))\nz.backward(\n    torch.FloatTensor([[1, 0]]), retain_variables=True)  # dz1/dx1, dz2/dx1\njacobian[:, 0] = x.grad.data\nx.grad.data.zero_()\nz.backward(torch.FloatTensor([[0, 1]]))  # dz1/dx2, dz2/dx2\njacobian[:, 1] = x.grad.data\nprint('=========jacobian========')\nprint('x')\nprint(x.data)\nprint('y')\nprint(y.data)\nprint('compute result')\nprint(z.data)\nprint('jacobian matrix is')\nprint(jacobian)"""
chapter9_Computer-Vision/kaggle_dog_vs_cat/model/dataset.py,5,"b""__author__ = 'SherlockLiao'\n\nimport torch\nfrom torch.utils.data import Dataset\nimport h5py\n\n\nclass h5Dataset(Dataset):\n\n    def __init__(self, h5py_list):\n        label_file = h5py.File(h5py_list[0], 'r')\n        self.label = torch.from_numpy(label_file['label'].value)\n        self.nSamples = self.label.size(0)\n        temp_dataset = torch.FloatTensor()\n        for file in h5py_list:\n            h5_file = h5py.File(file, 'r')\n            dataset = torch.from_numpy(h5_file['data'].value)\n            temp_dataset = torch.cat((temp_dataset, dataset), 1)\n\n        self.dataset = temp_dataset\n\n    def __len__(self):\n        return self.nSamples\n\n    def __getitem__(self, index):\n        assert index < len(self), 'index range error'\n        data = self.dataset[index]\n        label = self.label[index]\n        return (data, label)\n"""
chapter9_Computer-Vision/kaggle_dog_vs_cat/model/feature_extraction.py,7,"b'__author__ = \'SherlockLiao\'\n\nimport os\nfrom tqdm import tqdm\nimport h5py\nimport numpy as np\nimport argparse\n\nimport torch\nfrom torchvision import models, transforms\nfrom torch import optim, nn\nfrom torch.autograd import Variable\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom net import feature_net, classifier\n\nparse = argparse.ArgumentParser()\nparse.add_argument(\n    \'--model\', required=True, help=\'vgg, inceptionv3, resnet152\')\nparse.add_argument(\'--bs\', type=int, default=32)\nparse.add_argument(\'--phase\', required=True, help=\'train, val\')\nopt = parse.parse_args()\nprint(opt)\n\nimg_transform = transforms.Compose([\n    transforms.Scale(320),\n    transforms.CenterCrop(299),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nroot = \'/media/sherlock/Files/kaggle_dog_vs_cat/data\'\ndata_folder = {\n    \'train\': ImageFolder(os.path.join(root, \'train\'), transform=img_transform),\n    \'val\': ImageFolder(os.path.join(root, \'val\'), transform=img_transform)\n}\n\n# define dataloader to load images\nbatch_size = opt.bs\ndataloader = {\n    \'train\':\n    DataLoader(\n        data_folder[\'train\'],\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=4),\n    \'val\':\n    DataLoader(\n        data_folder[\'val\'],\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=4)\n}\n\n# get train data size and validation data size\ndata_size = {\n    \'train\': len(dataloader[\'train\'].dataset),\n    \'val\': len(dataloader[\'val\'].dataset)\n}\n\n# get numbers of classes\nimg_classes = len(dataloader[\'train\'].dataset.classes)\n\n# test if using GPU\nuse_gpu = torch.cuda.is_available()\n\n\ndef CreateFeature(model, phase, outputPath=\'.\'):\n    """"""\n    Create h5py dataset for feature extraction.\n\n    ARGS:\n        outputPath    : h5py output path\n        model         : used model\n        labelList     : list of corresponding groundtruth texts\n    """"""\n    featurenet = feature_net(model)\n    if use_gpu:\n        featurenet.cuda()\n    feature_map = torch.FloatTensor()\n    label_map = torch.LongTensor()\n    for data in tqdm(dataloader[phase]):\n        img, label = data\n        if use_gpu:\n            img = Variable(img, volatile=True).cuda()\n        else:\n            img = Variable(img, volatile=True)\n        out = featurenet(img)\n        feature_map = torch.cat((feature_map, out.cpu().data), 0)\n        label_map = torch.cat((label_map, label), 0)\n    feature_map = feature_map.numpy()\n    label_map = label_map.numpy()\n    file_name = \'_feature_{}.hd5f\'.format(model)\n    h5_path = os.path.join(outputPath, phase) + file_name\n    with h5py.File(h5_path, \'w\') as h:\n        h.create_dataset(\'data\', data=feature_map)\n        h.create_dataset(\'label\', data=label_map)\n\n\nCreateFeature(opt.model, opt.phase)\n'"
chapter9_Computer-Vision/kaggle_dog_vs_cat/model/feature_train.py,7,"b""__author__ = 'SherlockLiao'\n\nimport argparse\nimport time\nimport os\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch import optim\nfrom torch.utils.data import DataLoader\n\nfrom dataset import h5Dataset\nfrom net import classifier\n\nparse = argparse.ArgumentParser()\nparse.add_argument(\n    '--model',\n    nargs='+',\n    help='inceptionv3, vgg, resnet152',\n    default=['vgg', 'inceptionv3', 'resnet152'])\nparse.add_argument('--batch_size', type=int, default=64)\nparse.add_argument('--epoch', type=int, default=20)\nparse.add_argument('--n_classes', default=2, type=int)\nparse.add_argument('--num_workers', type=int, default=8)\nopt = parse.parse_args()\nprint(opt)\n\nroot = '/media/sherlock/Files/kaggle_dog_vs_cat/'\ntrain_list = ['train_feature_{}.hd5f'.format(i) for i in opt.model]\nval_list = ['val_feature_{}.hd5f'.format(i) for i in opt.model]\n\ndataset = {'train': h5Dataset(train_list), 'val': h5Dataset(val_list)}\n\ndatasize = {\n    'train': dataset['train'].dataset.size(0),\n    'val': dataset['val'].dataset.size(0)\n}\n\nbatch_size = opt.batch_size\nepoches = opt.epoch\n\ndataloader = {\n    'train':\n    DataLoader(\n        dataset['train'],\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=opt.num_workers),\n    'val':\n    DataLoader(\n        dataset['val'],\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=opt.num_workers)\n}\n\ndimension = dataset['train'].dataset.size(1)\n\nmynet = classifier(dimension, opt.n_classes)\nmynet.cuda()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(mynet.parameters(), lr=1e-3)\n# train\nfor epoch in range(epoches):\n    print('{}'.format(epoch + 1))\n    print('*' * 10)\n    print('Train')\n    mynet.train()\n    since = time.time()\n\n    running_loss = 0.0\n    running_acc = 0.0\n    for i, data in enumerate(dataloader['train'], 1):\n        feature, label = data\n        feature = Variable(feature).cuda()\n        label = Variable(label).cuda()\n\n        # forward\n        out = mynet(feature)\n        loss = criterion(out, label)\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.data[0] * label.size(0)\n        _, pred = torch.max(out, 1)\n        num_correct = torch.sum(pred == label)\n        running_acc += num_correct.data[0]\n        if i % 50 == 0:\n            print('Loss: {:.6f}, Acc: {:.6f}'.format(running_loss / (\n                i * batch_size), running_acc / (i * batch_size)))\n\n    running_loss /= datasize['train']\n    running_acc /= datasize['train']\n    eplise_time = time.time() - since\n    print('Loss: {:.6f}, Acc: {:.6f}, Time: {:.0f}s'.format(\n        running_loss, running_acc, eplise_time))\n    print('Validation')\n    mynet.eval()\n    num_correct = 0.0\n    eval_loss = 0.0\n    for data in dataloader['val']:\n        feature, label = data\n        feature = Variable(feature, volatile=True).cuda()\n        label = Variable(label, volatile=True).cuda()\n        # forward\n        out = mynet(feature)\n        loss = criterion(out, label)\n\n        _, pred = torch.max(out, 1)\n        correct = torch.sum(pred == label)\n        num_correct += correct.data[0]\n        eval_loss += loss.data[0] * label.size(0)\n\n    print('Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / datasize['val'],\n                                             num_correct / datasize['val']))\nprint('Finish Training!')\n\nsave_path = os.path.join(root, 'model_save')\nif not os.path.exists(save_path):\n    os.mkdir(save_path)\n\ntorch.save(mynet.state_dict(), save_path + '/feature_model.pth')\n"""
chapter9_Computer-Vision/kaggle_dog_vs_cat/model/fix_train.py,7,"b""__author__ = 'SherlockLiao'\n\nimport os\nimport time\n\nimport torch\nfrom torchvision import models, transforms\nfrom torch import optim, nn\nfrom torch.autograd import Variable\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\n\n# define image transforms to do data augumentation\ndata_transforms = {\n    'train':\n    transforms.Compose([\n        transforms.RandomSizedCrop(299),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ]),\n    'val':\n    transforms.Compose([\n        transforms.Scale(320),\n        transforms.CenterCrop(299),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n}\n\n# define data folder using ImageFolder to get images and classes from folder\nroot = '/media/sherlock/Files/kaggle_dog_vs_cat/'\ndata_folder = {\n    'train':\n    ImageFolder(\n        os.path.join(root, 'data/train'), transform=data_transforms['train']),\n    'val':\n    ImageFolder(\n        os.path.join(root, 'data/val'), transform=data_transforms['val'])\n}\n\n# define dataloader to load images\nbatch_size = 32\ndataloader = {\n    'train':\n    DataLoader(\n        data_folder['train'],\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=4),\n    'val':\n    DataLoader(data_folder['val'], batch_size=batch_size, num_workers=4)\n}\n\n# get train data size and validation data size\ndata_size = {\n    'train': len(dataloader['train'].dataset),\n    'val': len(dataloader['val'].dataset)\n}\n\n# get numbers of classes\nimg_classes = len(dataloader['train'].dataset.classes)\n\n# test if using GPU\nuse_gpu = torch.cuda.is_available()\nfix_param = True\n# define model\ntransfer_model = models.resnet18(pretrained=True)\nif fix_param:\n    for param in transfer_model.parameters():\n        param.requires_grad = False\ndim_in = transfer_model.fc.in_features\ntransfer_model.fc = nn.Linear(dim_in, 2)\nif use_gpu:\n    transfer_model = transfer_model.cuda()\n\n# define optimize function and loss function\nif fix_param:\n    optimizer = optim.Adam(transfer_model.fc.parameters(), lr=1e-3)\nelse:\n    optimizer = optim.Adam(transfer_model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n# train\nnum_epoch = 10\n\nfor epoch in range(num_epoch):\n    print('{}/{}'.format(epoch + 1, num_epoch))\n    print('*' * 10)\n    print('Train')\n    transfer_model.train()\n    running_loss = 0.0\n    running_acc = 0.0\n    since = time.time()\n    for i, data in enumerate(dataloader['train'], 1):\n        img, label = data\n        if use_gpu:\n            img = img.cuda()\n            label = label.cuda()\n        img = Variable(img)\n        label = Variable(label)\n\n        # forward\n        out = transfer_model(img)\n        loss = criterion(out, label)\n        _, pred = torch.max(out, 1)\n\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.data[0] * label.size(0)\n        num_correct = torch.sum(pred == label)\n        running_acc += num_correct.data[0]\n        if i % 100 == 0:\n            print('Loss: {:.6f}, Acc: {:.4f}'.format(running_loss / (\n                i * batch_size), running_acc / (i * batch_size)))\n    running_loss /= data_size['train']\n    running_acc /= data_size['train']\n    elips_time = time.time() - since\n    print('Loss: {:.6f}, Acc: {:.4f}, Time: {:.0f}s'.format(\n        running_loss, running_acc, elips_time))\n    print('Validation')\n    transfer_model.eval()\n    num_correct = 0.0\n    total = 0.0\n    eval_loss = 0.0\n    for data in dataloader['val']:\n        img, label = data\n        img = Variable(img, volatile=True).cuda()\n        label = Variable(label, volatile=True).cuda()\n        out = transfer_model(img)\n        _, pred = torch.max(out.data, 1)\n        loss = criterion(out, label)\n        eval_loss += loss.data[0] * label.size(0)\n        num_correct += (pred.cpu() == label.data.cpu()).sum()\n        total += label.size(0)\n    print('Loss: {:.6f} Acc: {:.4f}'.format(eval_loss / total, num_correct /\n                                            total))\n    print()\nprint('Finish Training!')\nprint()\nsave_path = os.path.join(root, 'model_save')\nif not os.path.exists(save_path):\n    os.mkdir(save_path)\ntorch.save(transfer_model.state_dict(), save_path + '/resnet18.pth')\n"""
chapter9_Computer-Vision/kaggle_dog_vs_cat/model/net.py,0,"b'__author__ = \'SherlockLiao\'\n\nimport torch\nfrom torchvision import models\nfrom torch import nn\n\n\nclass feature_net(nn.Module):\n    def __init__(self, model):\n        super(feature_net, self).__init__()\n\n        if model == \'vgg\':\n            vgg = models.vgg19(pretrained=True)\n            self.feature = nn.Sequential(*list(vgg.children())[:-1])\n            self.feature.add_module(\'global average\', nn.AvgPool2d(9))\n        elif model == \'inceptionv3\':\n            inception = models.inception_v3(pretrained=True)\n            self.feature = nn.Sequential(*list(inception.children())[:-1])\n            self.feature._modules.pop(\'13\')\n            self.feature.add_module(\'global average\', nn.AvgPool2d(35))\n        elif model == \'resnet152\':\n            resnet = models.resnet152(pretrained=True)\n            self.feature = nn.Sequential(*list(resnet.children())[:-1])\n\n    def forward(self, x):\n        """"""\n        model includes vgg19, inceptionv3, resnet152\n        """"""\n        x = self.feature(x)\n        x = x.view(x.size(0), -1)\n        return x\n\n\nclass classifier(nn.Module):\n    def __init__(self, dim, n_classes):\n        super(classifier, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(dim, 1000),\n            nn.ReLU(True),\n            nn.Dropout(0.5),\n            nn.Linear(1000, n_classes)\n        )\n\n    def forward(self, x):\n        x = self.fc(x)\n        return x\n'"
chapter9_Computer-Vision/segmentation/data/__init__.py,0,"b'# encoding: utf-8\n""""""\n@author: xyliao\n@contact: xyliao1993@qq.com\n""""""\nfrom .voc import VocSegDataset, img_transforms, COLORMAP, CLASSES, inverse_normalization\n'"
chapter9_Computer-Vision/segmentation/data/voc.py,5,"b'# encoding: utf-8\n""""""\n@author: xyliao\n@contact: xyliao1993@qq.com\n""""""\nimport os\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom mxtorch import transforms as tfs\n\n\ndef read_images(root, train):\n    txt_fname = os.path.join(root, \'ImageSets/Segmentation/\') + (\'train.txt\' if train else \'val.txt\')\n    with open(txt_fname, \'r\') as f:\n        images = f.read().split()\n    data = [os.path.join(root, \'JPEGImages\', i + \'.jpg\') for i in images]\n    label = [os.path.join(root, \'SegmentationClass\', i + \'.png\') for i in images]\n    return data, label\n\n\ndef random_crop(data, label, crop_size):\n    height, width = crop_size\n    data, rect = tfs.RandomCrop((height, width))(data)\n    label = tfs.FixedCrop(*rect)(label)\n    return data, label\n\n\ndef image2label(img):\n    cm2lbl = np.zeros(256 ** 3)\n    for i, cm in enumerate(COLORMAP):\n        cm2lbl[(cm[0] * 256 + cm[1]) * 256 + cm[2]] = i\n\n    data = np.array(img, dtype=np.int32)\n    idx = (data[:, :, 0] * 256 + data[:, :, 1] * 256 + data[:, :, 2])\n    return np.array(cm2lbl[idx], dtype=np.int64)\n\n\ndef img_transforms(img, label, crop_size):\n    img, label = random_crop(img, label, crop_size)\n    img_tfs = tfs.Compose([\n        tfs.ToTensor(),\n        tfs.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n    img = img_tfs(img)\n    label = image2label(label)\n    label = torch.from_numpy(label)\n    return img, label\n\n\ndef inverse_normalization(img):\n    """"""Convert normalized image to origin image.\n\n    :param img:(~torch.FloatTensor) normalized image, (C, H, W)\n    :return:\n        Origin image.\n    """"""\n    img = img * torch.FloatTensor([0.229, 0.224, 0.225])[:, None, None] \\\n          + torch.FloatTensor([0.485, 0.456, 0.406])[:, None, None]\n    origin_img = torch.clamp(img, min=0, max=1) * 255\n    origin_img = origin_img.permute(1, 2, 0).numpy()\n    return origin_img.astype(np.uint8)\n\n\nclass VocSegDataset(object):\n    def __init__(self, voc_root, train, crop_size, transforms):\n        self.crop_size = crop_size\n        self.transforms = transforms\n        data_list, label_list = read_images(voc_root, train)\n        self.data_list = self._filter(data_list)\n        self.label_list = self._filter(label_list)\n\n    def _filter(self, images):\n        return [img for img in images if (Image.open(img).size[1] >= self.crop_size[0] and\n                                          Image.open(img).size[0] >= self.crop_size[1])]\n\n    def __getitem__(self, item):\n        img = self.data_list[item]\n        label = self.label_list[item]\n        img = Image.open(img)\n        label = Image.open(label).convert(\'RGB\')\n        img, label = self.transforms(img, label, self.crop_size)\n        return img, label\n\n    def __len__(self):\n        return len(self.data_list)\n\n\nCLASSES = [\'background\', \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n           \'bottle\', \'bus\', \'car\', \'cat\', \'chair\', \'cow\', \'diningtable\',\n           \'dog\', \'horse\', \'motorbike\', \'person\', \'potted plant\',\n           \'sheep\', \'sofa\', \'train\', \'tv/monitor\']\n\n# RGB color for each class.\nCOLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128],\n            [128, 0, 128], [0, 128, 128], [128, 128, 128], [64, 0, 0], [192, 0, 0],\n            [64, 128, 0], [192, 128, 0], [64, 0, 128], [192, 0, 128],\n            [64, 128, 128], [192, 128, 128], [0, 64, 0], [128, 64, 0],\n            [0, 192, 0], [128, 192, 0], [0, 64, 128]]\n'"
chapter9_Computer-Vision/segmentation/models/__init__.py,0,"b'# encoding: utf-8\n""""""\n@author: xyliao\n@contact: xyliao1993@qq.com\n""""""\n\nfrom .fcn import FcnResNet\n'"
chapter9_Computer-Vision/segmentation/models/fcn.py,2,"b'# encoding: utf-8\n""""""\n@author: xyliao\n@contact: xyliao1993@qq.com\n\nThis file contains three FCN models according to paper ""Fully Convolutional Networks for Semantic Segmentation""\n""""""\n\nimport numpy as np\nimport torch\nfrom mxtorch.vision import model_zoo\nfrom torch import nn\n\n\ndef bilinear_kernel(in_channels, out_channels, kernel_size):\n    """"""Define a bilinear kernel according to in channels and out channels.\n\n    Returns:\n        return a bilinear filter tensor\n    """"""\n    factor = (kernel_size + 1) // 2\n    if kernel_size % 2 == 1:\n        center = factor - 1\n    else:\n        center = factor - 0.5\n    og = np.ogrid[:kernel_size, :kernel_size]\n    bilinear_filter = (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size), dtype=np.float32)\n    weight[range(in_channels), range(out_channels), :, :] = bilinear_filter\n    return torch.from_numpy(weight)\n\n\npretrained_net = model_zoo.resnet34(pretrained=True)\n\n\nclass FcnResNet(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        self.stage1 = nn.Sequential(*list(pretrained_net.children())[:-4])\n        self.stage2 = list(pretrained_net.children())[-4]\n        self.stage3 = list(pretrained_net.children())[-3]\n\n        self.scores1 = nn.Conv2d(512, num_classes, 1)\n        self.scores2 = nn.Conv2d(256, num_classes, 1)\n        self.scores3 = nn.Conv2d(128, num_classes, 1)\n\n        self.upsample_8x = nn.ConvTranspose2d(num_classes, num_classes, 16, 8, 4, bias=False)\n        self.upsample_8x.weight.data = bilinear_kernel(num_classes, num_classes, 16)\n\n        self.upsample_4x = nn.ConvTranspose2d(num_classes, num_classes, 4, 2, 1, bias=False)\n        self.upsample_4x.weight.data = bilinear_kernel(num_classes, num_classes, 4)\n\n        self.upsample_2x = nn.ConvTranspose2d(num_classes, num_classes, 4, 2, 1, bias=False)\n        self.upsample_2x.weight.data = bilinear_kernel(num_classes, num_classes, 4)\n\n    def forward(self, x):\n        x = self.stage1(x)\n        s1 = x\n\n        x = self.stage2(x)\n        s2 = x\n\n        x = self.stage3(x)\n        s3 = x\n\n        s3 = self.scores1(s3)\n        s3 = self.upsample_2x(s3)\n        s2 = self.scores2(s2)\n        s2 = s2 + s3\n\n        s1 = self.scores3(s1)\n        s2 = self.upsample_4x(s2)\n        s = s1 + s2\n\n        s = self.upsample_8x(s)\n        return s\n\n\n'"
