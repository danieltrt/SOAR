file_path,api_count,code
examples/jit/resnet.py,2,"b'import torch\nimport torchvision\n\nmodel = torchvision.models.resnet18(pretrained=True)\nmodel.eval()\nexample = torch.rand(1, 3, 224, 224)\ntraced_script_module = torch.jit.trace(model, example)\ntraced_script_module.save(""model.pt"")\n'"
examples/reinforcement-learning/atari_wrappers.py,0,"b'import gym\nimport numpy as np\nfrom collections import deque\nfrom PIL import Image\nfrom multiprocessing import Process, Pipe\n\n# atari_wrappers.py\nclass NoopResetEnv(gym.Wrapper):\n    def __init__(self, env, noop_max=30):\n        """"""Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.noop_max = noop_max\n        self.override_num_noops = None\n        assert env.unwrapped.get_action_meanings()[0] == \'NOOP\'\n\n    def reset(self):\n        """""" Do no-op action for a number of steps in [1, noop_max].""""""\n        self.env.reset()\n        if self.override_num_noops is not None:\n            noops = self.override_num_noops\n        else:\n            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n        assert noops > 0\n        obs = None\n        for _ in range(noops):\n            obs, _, done, _ = self.env.step(0)\n            if done:\n                obs = self.env.reset()\n        return obs\n\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Take action on reset for environments that are fixed until firing.""""""\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == \'FIRE\'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self):\n        self.env.reset()\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset()\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset()\n        return obs\n\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done  = True\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if lives < self.lives and lives > 0:\n            # for Qbert somtimes we stay in lives == 0 condtion for a few frames\n            # so its important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info\n\n    def reset(self):\n        """"""Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        """"""\n        if self.was_real_done:\n            obs = self.env.reset()\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        """"""Return only every `skip`-th frame""""""\n        gym.Wrapper.__init__(self, env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = deque(maxlen=2)\n        self._skip       = skip\n\n    def step(self, action):\n        """"""Repeat action, sum reward, and max over last observations.""""""\n        total_reward = 0.0\n        done = None\n        for _ in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            self._obs_buffer.append(obs)\n            total_reward += reward\n            if done:\n                break\n        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n\n        return max_frame, total_reward, done, info\n\n    def reset(self):\n        """"""Clear past frame buffer and init. to first obs. from inner env.""""""\n        self._obs_buffer.clear()\n        obs = self.env.reset()\n        self._obs_buffer.append(obs)\n        return obs\n\nclass ClipRewardEnv(gym.RewardWrapper):\n    def reward(self, reward):\n        """"""Bin reward to {+1, 0, -1} by its sign.""""""\n        return np.sign(reward)\n\nclass WarpFrame(gym.ObservationWrapper):\n    def __init__(self, env):\n        """"""Warp frames to 84x84 as done in the Nature paper and later work.""""""\n        gym.ObservationWrapper.__init__(self, env)\n        self.res = 84\n        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(self.res, self.res, 1), dtype=\'uint8\')\n\n    def observation(self, obs):\n        frame = np.dot(obs.astype(\'float32\'), np.array([0.299, 0.587, 0.114], \'float32\'))\n        frame = np.array(Image.fromarray(frame).resize((self.res, self.res),\n            resample=Image.BILINEAR), dtype=np.uint8)\n        return frame.reshape((self.res, self.res, 1))\n\nclass FrameStack(gym.Wrapper):\n    def __init__(self, env, k):\n        """"""Buffer observations and stack across channels (last axis).""""""\n        gym.Wrapper.__init__(self, env)\n        self.k = k\n        self.frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        assert shp[2] == 1  # can only stack 1-channel frames\n        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(shp[0], shp[1], k), dtype=\'uint8\')\n\n    def reset(self):\n        """"""Clear buffer and re-fill by duplicating the first observation.""""""\n        ob = self.env.reset()\n        for _ in range(self.k): self.frames.append(ob)\n        return self.observation()\n\n    def step(self, action):\n        ob, reward, done, info = self.env.step(action)\n        self.frames.append(ob)\n        return self.observation(), reward, done, info\n\n    def observation(self):\n        assert len(self.frames) == self.k\n        return np.concatenate(self.frames, axis=2)\n\ndef wrap_deepmind(env, episode_life=True, clip_rewards=True):\n    """"""Configure environment for DeepMind-style Atari.\n\n    Note: this does not include frame stacking!""""""\n    assert \'NoFrameskip\' in env.spec.id  # required for DeepMind-style skip\n    if episode_life:\n        env = EpisodicLifeEnv(env)\n    env = NoopResetEnv(env, noop_max=30)\n    env = MaxAndSkipEnv(env, skip=4)\n    if \'FIRE\' in env.unwrapped.get_action_meanings():\n        env = FireResetEnv(env)\n    env = WarpFrame(env)\n    if clip_rewards:\n        env = ClipRewardEnv(env)\n    return env\n\n# envs.py\ndef make_env(env_id, seed, rank):\n    def _thunk():\n        env = gym.make(env_id)\n        env.seed(seed + rank)\n        env = wrap_deepmind(env)\n        env = WrapPyTorch(env)\n        return env\n\n    return _thunk\n\nclass WrapPyTorch(gym.ObservationWrapper):\n    def __init__(self, env=None):\n        super(WrapPyTorch, self).__init__(env)\n        self.observation_space = gym.spaces.Box(0.0, 1.0, [1, 84, 84], dtype=\'float32\')\n\n    def observation(self, observation):\n        return observation.transpose(2, 0, 1)\n\n# vecenv.py\nclass VecEnv(object):\n    """"""\n    Vectorized environment base class\n    """"""\n    def step(self, vac):\n        """"""\n        Apply sequence of actions to sequence of environments\n        actions -> (observations, rewards, news)\n\n        where \'news\' is a boolean vector indicating whether each element is new.\n        """"""\n        raise NotImplementedError\n    def reset(self):\n        """"""\n        Reset all environments\n        """"""\n        raise NotImplementedError\n    def close(self):\n        pass\n\n# subproc_vec_env.py\ndef worker(remote, env_fn_wrapper):\n    env = env_fn_wrapper.x()\n    while True:\n        cmd, data = remote.recv()\n        if cmd == \'step\':\n            ob, reward, done, info = env.step(data)\n            if done:\n                ob = env.reset()\n            remote.send((ob, reward, done, info))\n        elif cmd == \'reset\':\n            ob = env.reset()\n            remote.send(ob)\n        elif cmd == \'close\':\n            remote.close()\n            break\n        elif cmd == \'get_spaces\':\n            remote.send((env.action_space, env.observation_space))\n        else:\n            raise NotImplementedError\n\nclass CloudpickleWrapper(object):\n    """"""\n    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n    """"""\n    def __init__(self, x):\n        self.x = x\n    def __getstate__(self):\n        import cloudpickle\n        return cloudpickle.dumps(self.x)\n    def __setstate__(self, ob):\n        import pickle\n        self.x = pickle.loads(ob)\n\nclass SubprocVecEnv(VecEnv):\n    def __init__(self, env_fns):\n        """"""\n        envs: list of gym environments to run in subprocesses\n        """"""\n        nenvs = len(env_fns)\n        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])        \n        self.ps = [Process(target=worker, args=(work_remote, CloudpickleWrapper(env_fn))) \n            for (work_remote, env_fn) in zip(self.work_remotes, env_fns)]\n        for p in self.ps:\n            p.start()\n\n        self.remotes[0].send((\'get_spaces\', None))\n        self.action_space, self.observation_space = self.remotes[0].recv()\n\n\n    def step(self, actions):\n        for remote, action in zip(self.remotes, actions):\n            remote.send((\'step\', action))\n        results = [remote.recv() for remote in self.remotes]\n        obs, rews, dones, infos = zip(*results)\n        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n\n    def reset(self):\n        for remote in self.remotes:\n            remote.send((\'reset\', None))\n        return np.stack([remote.recv() for remote in self.remotes])\n\n    def close(self):\n        for remote in self.remotes:\n            remote.send((\'close\', None))\n        for p in self.ps:\n            p.join()\n\n    @property\n    def num_envs(self):\n        return len(self.remotes)\n\n# Create the environment.\ndef make(env_name, num_processes):\n    envs = SubprocVecEnv([\n        make_env(env_name, 1337, i) for i in range(num_processes)\n    ])\n    return envs\n'"
src/tests/trace.py,6,"b""import torch\n\nclass Foo(torch.jit.ScriptModule):\n    def __init__(self):\n        super(Foo, self).__init__()\n\n    def forward(self, x, y):\n        return 2 * x + y\n\nfoo = Foo()\ntraced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\ntraced_foo.save('foo.pt')\n\nclass Foo2(torch.jit.ScriptModule):\n    def __init__(self):\n        super(Foo2, self).__init__()\n\n    def forward(self, x, y):\n        return (2 * x + y, x - y)\n\nfoo = Foo2()\ntraced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\ntraced_foo.save('foo2.pt')\n\nclass Foo3(torch.jit.ScriptModule):\n    def __init__(self):\n        super(Foo3, self).__init__()\n\n    @torch.jit.script_method\n    def forward(self, x):\n        result = x[0]\n        for i in range(x.size(0)):\n            if i: result = result * x[i]\n        return result\n\nfoo = Foo3()\nfoo.save('foo3.pt')\n"""
src/vision/export_model.py,0,"b""# This script exports pre-trained model weights in numpy format.\n# These weights can then be converted to the libtorch native format via:\n# bin/tensor_tools.exe npz-to-pytorch resnet18.npz resnet18.ot\nimport numpy as np\nimport torch\nimport torchvision\n\nm = torchvision.models.resnet18(pretrained=True)\nnps = {}\nfor k, v in m.state_dict().items(): nps[k] = v.numpy()\nnp.savez('resnet18.npz', **nps)\n"""
