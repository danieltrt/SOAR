file_path,api_count,code
datasets/__init__.py,0,b''
datasets/coco.py,0,"b'import os\nimport json\nfrom torchvision.transforms import ToTensor\nfrom datasets.coco_data.COCO_data_pipeline import Cocokeypoints, Cocobbox, bbox_collater\nfrom datasets.dataloader import sDataLoader\nfrom pycocotools.coco import COCO\n\n\ndef get_loader(json_path, data_dir, mask_dir, inp_size, feat_stride, preprocess,\n               batch_size, training=True, shuffle=True, num_workers=3, subnet=\'keypoint_subnet\'):\n    """""" Build a COCO dataloader\n    :param json_path: string, path to jso file\n    :param datadir: string, path to coco data\n    :returns : the data_loader\n    """"""\n    with open(json_path) as data_file:\n        data_this = json.load(data_file)\n        data = data_this[\'root\']\n\n    num_samples = len(data)\n    train_indexes = []\n    val_indexes = []\n\n    if subnet == \'keypoint_subnet\':\n        for count in range(num_samples):\n            if data[count][\'isValidation\'] != 0.:\n                val_indexes.append(count)\n            else:\n                train_indexes.append(count)\n\n        coco_data = Cocokeypoints(root=data_dir, mask_dir=mask_dir,\n                                  index_list=train_indexes if training else val_indexes,\n                                  data=data, inp_size=inp_size, feat_stride=feat_stride,\n                                  preprocess=preprocess, transform=ToTensor())\n        data_loader = sDataLoader(coco_data, batch_size=batch_size,\n                                  shuffle=shuffle, num_workers=num_workers)\n\n    elif subnet == \'detection_subnet\':\n        if training:\n            anno_path = os.path.join(mask_dir, \'annotations\', \'person_keypoints_train2017.json\')\n        else:\n            anno_path = os.path.join(mask_dir, \'annotations\', \'person_keypoints_val2017.json\')\n        coco = COCO(anno_path)\n        images_ids = coco.getImgIds()\n\n        data_indexes = []\n        for count in range(num_samples):\n            if int(data[count][\'image_id\']) in images_ids:\n                data_indexes.append(count)\n\n        coco_data = Cocobbox(root=data_dir, mask_dir=mask_dir, index_list=data_indexes,\n                             data=data, inp_size=inp_size, feat_stride=feat_stride, coco=coco,\n                             preprocess=preprocess, training=True if training else False)\n\n        data_loader = sDataLoader(coco_data, batch_size=batch_size, shuffle=shuffle,\n                                  num_workers=num_workers, collate_fn=bbox_collater)\n\n    return data_loader\n'"
datasets/data_parallel.py,4,"b'import itertools\nimport torch\nfrom torch.nn import DataParallel\nfrom torch.autograd import Variable\nfrom torch.nn.parallel._functions import Scatter, Gather\n\n\nclass ScatterList(list):\n    pass\n\n\nclass ConstList(list):\n    pass\n\n\nclass ListDataParallel(DataParallel):\n    def scatter(self, inputs, kwargs, device_ids):\n        return pose_scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\n\n    def gather(self, outputs, output_device):\n        return pose_gather(outputs, output_device, dim=self.dim)\n\n\ndef scatter(inputs, target_gpus, dim=0):\n    r""""""\n    Slices variables into approximately equal chunks and\n    distributes them across given GPUs. Duplicates\n    references to objects that are not variables. Does not\n    support Tensors.\n    """"""\n    def scatter_map(obj):\n        if isinstance(obj, Variable):\n            return Scatter.apply(target_gpus, None, dim, obj)\n        assert not torch.is_tensor(obj), ""Tensors not supported in scatter.""\n        if isinstance(obj, ScatterList):\n            assert len(obj) == len(target_gpus)\n            return [obj[i] for i in range(len(target_gpus))]\n        if isinstance(obj, tuple) and len(obj) > 0:\n            return list(zip(*map(scatter_map, obj)))\n        if isinstance(obj, list) and len(obj) > 0:\n            return list(map(list, zip(*map(scatter_map, obj))))\n        if isinstance(obj, dict) and len(obj) > 0:\n            return list(map(type(obj), zip(*map(scatter_map, obj.items()))))\n        return [obj for targets in target_gpus]\n\n    return scatter_map(inputs)\n\n\ndef pose_scatter_kwargs(inputs, kwargs, target_gpus, dim=0):\n    r""""""Scatter with support for kwargs dictionary""""""\n    inputs = scatter(inputs, target_gpus, dim) if inputs else []\n    kwargs = scatter(kwargs, target_gpus, dim) if kwargs else []\n    if len(inputs) < len(kwargs):\n        inputs.extend([() for _ in range(len(kwargs) - len(inputs))])\n    elif len(kwargs) < len(inputs):\n        kwargs.extend([{} for _ in range(len(inputs) - len(kwargs))])\n    inputs = tuple(inputs)\n    kwargs = tuple(kwargs)\n    return inputs, kwargs\n\n\ndef pose_gather(outputs, target_device, dim=0):\n    r""""""\n    Gathers variables from different GPUs on a specified device\n      (-1 means the CPU).\n    """"""\n    def gather_map(outputs):\n        if isinstance(outputs, Variable):\n            if target_device == -1:\n                return outputs.cpu()\n            return outputs.cuda(target_device)\n\n        out = outputs[0]\n        if isinstance(out, Variable):\n            return Gather.apply(target_device, dim, *outputs)\n        if out is None:\n            return None\n\n        if isinstance(out, str):\n            return out\n        if isinstance(out, ConstList):\n            return out\n        if isinstance(out, ScatterList):\n            return tuple(map(gather_map, itertools.chain(*outputs)))\n\n        return type(out)(map(gather_map, zip(*outputs)))\n    return gather_map(outputs)\n'"
datasets/dataloader.py,1,"b'from typing import Generator\nfrom torch.utils.data.dataloader import DataLoader, _DataLoaderIter\nfrom lib.utils.log import logger\n\n\nclass sDataLoader(DataLoader):\n    def get_stream(self):\n        """"""\n        Return a generate that can yield endless data.\n        :Example:\n        stream = get_stream()\n        for i in range(100):\n            batch = next(stream)\n\n        :return: stream\n        :rtype: Generator\n        """"""\n        while True:\n            for data in _DataLoaderIter(self):\n                yield data\n\n    @staticmethod\n    def copy(loader):\n        """"""\n        Init a sDataloader from an existing Dataloader\n        :param loader: an instance of Dataloader\n        :type loader: DataLoader\n        :return: a new instance of sDataloader\n        :rtype: sDataLoader\n        """"""\n        if not isinstance(loader, DataLoader):\n            logger(\'loader should be an instance of Dataloader, but got {}\'.format(type(loader)))\n            return loader\n\n        new_loader = sDataLoader(loader.dataset)\n        for k, v in loader.__dict__.items():\n            setattr(new_loader, k, v)\n        return new_loader\n'"
evaluate/__init__.py,0,b''
evaluate/multipose_coco_eval.py,0,"b""import os, sys\nroot_path = os.path.realpath(__file__).split('/evaluate/multipose_coco_eval.py')[0]\nos.chdir(root_path)\nsys.path.append(root_path)\n\nfrom network.posenet import poseNet\nfrom evaluate.tester import Tester\n\nbackbone = 'resnet101'\n\n# Set Training parameters\nparams = Tester.TestParams()\nparams.subnet_name = 'both'\nparams.inp_size = 480  # input picture size = (inp_size, inp_size)\nparams.coeff = 2\nparams.in_thres = 0.21\nparams.coco_root = '/data/COCO/'\nparams.testresult_write_json = False  # Whether to write json result\nparams.coco_result_filename = './demo/multipose_coco2017_results.json'\nparams.ckpt = './demo/models/ckpt_baseline_resnet101.h5'\n\n# model\nif backbone == 'resnet101':\n    model = poseNet(101)\nelif backbone == 'resnet50':\n    model = poseNet(50)\n\nfor name, module in model.named_children():\n    for para in module.parameters():\n        para.requires_grad = False\n\ntester = Tester(model, params)\ntester.coco_eval()  # pic_test\n"""
evaluate/multipose_detection_val.py,0,"b""import os, sys\nroot_path = os.path.realpath(__file__).split('/evaluate/multipose_detection_val.py')[0]\nos.chdir(root_path)\nsys.path.append(root_path)\n\nfrom training.batch_processor import batch_processor\nfrom network.posenet import poseNet\nfrom datasets.coco import get_loader\nfrom evaluate.tester import Tester\n\n# Hyper-params\ncoco_root = '/data/COCO/'\nbackbone = 'resnet101'  # 'resnet50'\ndata_dir = coco_root+'images/'\nmask_dir = coco_root\njson_path = coco_root+'COCO.json'\ninp_size = 608  # input size 608*608\nfeat_stride = 4\n\n# Set Training parameters\nparams = Tester.TestParams()\nparams.subnet_name = 'detection_subnet'\nparams.gpus = [0]\nparams.ckpt = './demo/models/ckpt_baseline_resnet101.h5'\nparams.batch_size = 25 * len(params.gpus)\nparams.print_freq = 100\n\n# validation data\nvalid_data = get_loader(json_path, data_dir, mask_dir, inp_size, feat_stride,\n                        preprocess='resnet', batch_size=params.batch_size-10*len(params.gpus), training=False,\n                        shuffle=False, num_workers=8, subnet=params.subnet_name)\nprint('val dataset len: {}'.format(len(valid_data.dataset)))\n\n# model\nif backbone == 'resnet101':\n    model = poseNet(101)\nelif backbone == 'resnet50':\n    model = poseNet(50)\n\nfor name, module in model.named_children():\n    for para in module.parameters():\n        para.requires_grad = False\n\ntester = Tester(model, params, batch_processor, valid_data)\ntester.val()\n"""
evaluate/multipose_keypoint_val.py,0,"b""import os, sys\nroot_path = os.path.realpath(__file__).split('/evaluate/multipose_keypoint_val.py')[0]\nos.chdir(root_path)\nsys.path.append(root_path)\n\nfrom training.batch_processor import batch_processor\nfrom network.posenet import poseNet\nfrom datasets.coco import get_loader\nfrom evaluate.tester import Tester\n\n# Hyper-params\ncoco_root = '/data/COCO/'\nbackbone = 'resnet101'  # 'resnet50'\ndata_dir = coco_root+'images/'\nmask_dir = coco_root\njson_path = coco_root+'COCO.json'\ninp_size = 480  # input size 480*480\nfeat_stride = 4\n\n# Set Training parameters\nparams = Tester.TestParams()\nparams.subnet_name = 'keypoint_subnet'\nparams.gpus = [0]\nparams.ckpt = './demo/models/ckpt_baseline_resnet101.h5'\nparams.batch_size = 6 * len(params.gpus)\nparams.print_freq = 50\n\n# validation data\nvalid_data = get_loader(json_path, data_dir, mask_dir, inp_size, feat_stride,\n                        preprocess='resnet', batch_size=params.batch_size-2*len(params.gpus), training=False,\n                        shuffle=False, num_workers=4, subnet=params.subnet_name)\nprint('val dataset len: {}'.format(len(valid_data.dataset)))\n\n# model\nif backbone == 'resnet101':\n    model = poseNet(101)\nelif backbone == 'resnet50':\n    model = poseNet(50)\n\nfor name, module in model.named_children():\n    for para in module.parameters():\n        para.requires_grad = False\n\ntester = Tester(model, params, batch_processor, valid_data)\ntester.val()\n"""
evaluate/multipose_prn_val.py,1,"b""import os, sys\nroot_path = os.path.realpath(__file__).split('/evaluate/multipose_prn_val.py')[0]\nos.chdir(root_path)\nsys.path.append(root_path)\n\nfrom network.posenet import poseNet\nfrom pycocotools.coco import COCO\nfrom datasets.coco_data.prn_data_pipeline import PRN_CocoDataset\nfrom torch.utils.data import DataLoader\nfrom training.batch_processor import batch_processor\nfrom evaluate.tester import Tester\n\n\n# Hyper-params\ncoco_root = '/data/COCO/'\nbackbone='resnet101'  # 'resnet50'\ninp_size = 480  # input size 480*480\nfeat_stride = 4\nnode_count = 1024  # Hidden Layer Node Count\ncoeff = 2  # Coefficient of bbox size\nthreshold = 0.21  # BBOX threshold\nnum_of_keypoints = 3  # Minimum number of keypoints for each bbox in training\n\n# Set Training parameters\nparams = Tester.TestParams()\nparams.subnet_name = 'prn_subnet'\nparams.gpus = [0]\nparams.ckpt = './demo/models/ckpt_baseline_resnet101.h5'\nparams.batch_size = 8 * len(params.gpus)\nparams.print_freq = 500\n\n# validation data\ncoco_val = COCO(os.path.join(coco_root, 'annotations/person_keypoints_val2017.json'))\nvalid_data = DataLoader(dataset=PRN_CocoDataset(\n    coco_val, num_of_keypoints=num_of_keypoints, coeff=coeff, threshold=threshold,\n    inp_size=inp_size, feat_stride=feat_stride), batch_size=params.batch_size, num_workers=4, shuffle=False)\nprint('val dataset len: {}'.format(len(valid_data.dataset)))\n\n# model\nif backbone == 'resnet101':\n    model = poseNet(101, prn_node_count=node_count, prn_coeff=coeff)\nelif backbone == 'resnet50':\n    model = poseNet(50, prn_node_count=node_count, prn_coeff=coeff)\n\nfor name, module in model.named_children():\n    for para in module.parameters():\n        para.requires_grad = False\n\ntester = Tester(model, params, batch_processor, valid_data)\ntester.val()\n"""
evaluate/multipose_test.py,0,"b""import os, sys\nroot_path = os.path.realpath(__file__).split('/evaluate/multipose_test.py')[0]\nos.chdir(root_path)\nsys.path.append(root_path)\n\nfrom network.posenet import poseNet\nfrom evaluate.tester import Tester\n\nbackbone = 'resnet101'\n\n# Set Training parameters\nparams = Tester.TestParams()\nparams.subnet_name = 'both'\nparams.inp_size = 480  # input picture size = (inp_size, inp_size)\nparams.coeff = 2\nparams.in_thres = 0.21\nparams.testdata_dir = './demo/test_images/'\nparams.testresult_dir = './demo/output/'\nparams.testresult_write_image = True  # Whether to write result pictures\nparams.testresult_write_json = False  # Whether to write json result\nparams.ckpt = './demo/models/ckpt_baseline_resnet101.h5'\n\n# model\nif backbone == 'resnet101':\n    model = poseNet(101)\nelif backbone == 'resnet50':\n    model = poseNet(50)\n\nfor name, module in model.named_children():\n    for para in module.parameters():\n        para.requires_grad = False\n\ntester = Tester(model, params)\ntester.test()  # pic_test\n"""
evaluate/tester.py,6,"b'from __future__ import print_function\n\nimport os\nimport cv2\nimport math\nimport datetime\nimport numpy as np\nimport json\nfrom collections import OrderedDict\nfrom network.joint_utils import get_joint_list, plot_result\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom lib.utils.log import logger\nimport lib.utils.meter as meter_utils\nimport network.net_utils as net_utils\nfrom lib.utils.timer import Timer\nfrom datasets.coco_data.preprocessing import resnet_preprocess\nfrom datasets.coco_data.prn_gaussian import gaussian, crop\n\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\n\ndef _factor_closest(num, factor, is_ceil=True):\n    """"""Returns the closest integer to `num` that is divisible by `factor`\n\n    Actually, that\'s a lie. By default, we return the closest factor _greater_\n    than the input. If, however, you set `it_ceil` to `False`, we return the\n    closest factor _less_ than the input.\n    """"""\n    num = float(num) / factor\n    num = np.ceil(num) if is_ceil else np.floor(num)\n    return int(num) * factor\n\n\ndef crop_with_factor(im, dest_size, factor=32, pad_val=0, basedon=\'min\'):\n    """"""Scale and pad an image to the desired size and divisibility\n\n    Scale the specified dimension of the input image to `dest_size` then pad\n    the result until it is cleanly divisible by `factor`.\n\n    Args:\n        im (Image): The input image.\n        dest_size (int): The desired size of the unpadded, scaled image\'s\n            dimension specified by `basedon`.\n        factor (int): Pad the scaled image until it is factorable\n        pad_val (number): Value to pad with.\n        basedon (string): Specifies which dimension to base the scaling on.\n            Valid inputs are \'min\', \'max\', \'w\', and \'h\'. Defaults to \'min\'.\n\n    Returns:\n        A tuple of three elements:\n            - The scaled and padded image.\n            - The scaling factor.\n            - The size of the non-padded part of the resulting image.\n    """"""\n    # Compute the scaling factor.\n    im_size_min, im_size_max = np.min(im.shape[0:2]), np.max(im.shape[0:2])\n    im_base = {\'min\': im_size_min,\n               \'max\': im_size_max,\n               \'w\': im.shape[1],\n               \'h\': im.shape[0]}\n    im_scale = float(dest_size) / im_base.get(basedon, im_size_min)\n\n    # Scale the image.\n    im = cv2.resize(im, None, fx=im_scale, fy=im_scale)\n\n    # Compute the padded image shape. Ensure it\'s divisible by factor.\n    h, w = im.shape[:2]\n    new_h, new_w = _factor_closest(h, factor), _factor_closest(w, factor)\n    # new_ = max(new_h, new_w)\n    new_shape = [new_h, new_w] if im.ndim < 3 else [new_h, new_w, im.shape[-1]]\n    # new_shape = [new_, new_] if im.ndim < 3 else [new_, new_, im.shape[-1]]\n\n    # Pad the image.\n    im_padded = np.full(new_shape, fill_value=pad_val, dtype=im.dtype)\n    im_padded[0:h, 0:w] = im\n\n    return im_padded, im_scale, im.shape\n\n\nclass TestParams(object):\n\n    trunk = \'resnet101\'  # select the model\n    coeff = 2\n    in_thres = 0.21\n\n    testdata_dir = \'./demo/test_images/\'\n    testresult_dir = \'./demo/output/\'\n    testresult_write_image = False  # write image results or not\n    testresult_write_json = False  # write json results or not\n    gpus = [0]\n    ckpt = \'./demo/models/ckpt_baseline_resnet101.h5\'  # checkpoint file to load, no need to change this\n    coco_root = \'coco_root/\'\n    coco_result_filename = \'./extra/multipose_coco2017_results.json\'\n\n    # # required params\n    inp_size = 480  # input size 480*480\n    exp_name = \'multipose101\'\n    subnet_name = \'keypoint_subnet\'\n    batch_size = 32\n    print_freq = 20\n\nclass Tester(object):\n\n    TestParams = TestParams\n\n    def __init__(self, model, train_params, batch_processor=None, val_data=None):\n        assert isinstance(train_params, TestParams)\n        self.params = train_params\n        self.batch_timer = Timer()\n        self.data_timer = Timer()\n        self.val_data = val_data if val_data else None\n        self.batch_processor = batch_processor if batch_processor else None\n\n        # load model\n        self.model = model\n        ckpt = self.params.ckpt\n\n        if ckpt is not None:\n            self._load_ckpt(ckpt)\n            logger.info(\'Load ckpt from {}\'.format(ckpt))\n\n        self.model = nn.DataParallel(self.model, device_ids=self.params.gpus)\n        self.model = self.model.cuda(device=self.params.gpus[0])\n        self.model.eval()\n        self.model.module.freeze_bn()\n\n    def coco_eval(self):\n\n        coco_val = os.path.join(self.params.coco_root, \'annotations/person_keypoints_val2017.json\')\n        coco = COCO(coco_val)\n        img_ids = coco.getImgIds(catIds=[1])\n\n        multipose_results = []\n        coco_order = [0, 14, 13, 16, 15, 4, 1, 5, 2, 6, 3, 10, 7, 11, 8, 12, 9]\n\n        for img_id in tqdm(img_ids):\n\n            img_name = coco.loadImgs(img_id)[0][\'file_name\']\n\n            oriImg = cv2.imread(os.path.join(self.params.coco_root, \'images/val2017/\', img_name)).astype(np.float32)\n            multiplier = self._get_multiplier(oriImg)\n\n            # Get results of original image\n            orig_heat, orig_bbox_all = self._get_outputs(multiplier, oriImg)\n\n            # Get results of flipped image\n            swapped_img = oriImg[:, ::-1, :]\n            flipped_heat, flipped_bbox_all = self._get_outputs(multiplier, swapped_img)\n\n            # compute averaged heatmap\n            heatmaps = self._handle_heat(orig_heat, flipped_heat)\n\n            # segment_map = heatmaps[:, :, 17]\n            param = {\'thre1\': 0.1, \'thre2\': 0.05, \'thre3\': 0.5}\n            joint_list = get_joint_list(oriImg, param, heatmaps[:, :, :18], 1)\n            joint_list = joint_list.tolist()\n\n            joints = []\n            for joint in joint_list:\n                if int(joint[-1]) != 1:\n                    joint[-1] = max(0, int(joint[-1]) - 1)\n                    joints.append(joint)\n            joint_list = joints\n\n            prn_result = self.prn_process(joint_list, orig_bbox_all[1], img_name, img_id)\n            for result in prn_result:\n                keypoints = result[\'keypoints\']\n                coco_keypoint = []\n                for i in range(17):\n                    coco_keypoint.append(keypoints[coco_order[i] * 3])\n                    coco_keypoint.append(keypoints[coco_order[i] * 3 + 1])\n                    coco_keypoint.append(keypoints[coco_order[i] * 3 + 2])\n                result[\'keypoints\'] = coco_keypoint\n                multipose_results.append(result)\n\n        ann_filename = self.params.coco_result_filename\n        with open(ann_filename, ""w"") as f:\n            json.dump(multipose_results, f, indent=4)\n        # load results in COCO evaluation tool\n        coco_pred = coco.loadRes(ann_filename)\n        # run COCO evaluation\n        coco_eval = COCOeval(coco, coco_pred, \'keypoints\')\n        coco_eval.params.imgIds = img_ids\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        coco_eval.summarize()\n\n        if not self.params.testresult_write_json:\n            os.remove(ann_filename)\n\n    def test(self):\n\n        img_list = os.listdir(self.params.testdata_dir)\n        multipose_results = []\n\n        for img_name in tqdm(img_list):\n\n            img = cv2.imread(os.path.join(self.params.testdata_dir, img_name)).astype(np.float32)\n            shape_dst = np.max(img.shape)\n            scale = float(shape_dst) / self.params.inp_size\n            pad_size = np.abs(img.shape[1] - img.shape[0])\n            img_resized = np.pad(img, ([0, pad_size], [0, pad_size], [0, 0]), \'constant\')[:shape_dst, :shape_dst]\n            img_resized = cv2.resize(img_resized, (self.params.inp_size, self.params.inp_size))\n            img_input = resnet_preprocess(img_resized)\n            img_input = torch.from_numpy(np.expand_dims(img_input, 0))\n\n            with torch.no_grad():\n                img_input = img_input.cuda(device=self.params.gpus[0])\n\n            heatmaps, [scores, classification, transformed_anchors] = self.model([img_input, self.params.subnet_name])\n            heatmaps = heatmaps.cpu().detach().numpy()\n            heatmaps = np.squeeze(heatmaps, 0)\n            heatmaps = np.transpose(heatmaps, (1, 2, 0))\n            heatmap_max = np.max(heatmaps[:, :, :18], 2)\n            # segment_map = heatmaps[:, :, 17]\n            param = {\'thre1\': 0.1, \'thre2\': 0.05, \'thre3\': 0.5}\n            joint_list = get_joint_list(img_resized, param, heatmaps[:, :, :18], scale)\n            joint_list = joint_list.tolist()\n            del img_resized\n\n            joints = []\n            for joint in joint_list:\n                if int(joint[-1]) != 1:\n                    joint[-1] = max(0, int(joint[-1]) - 1)\n                    joints.append(joint)\n            joint_list = joints\n\n            # bounding box from retinanet\n            scores = scores.cpu().detach().numpy()\n            classification = classification.cpu().detach().numpy()\n            transformed_anchors = transformed_anchors.cpu().detach().numpy()\n            idxs = np.where(scores > 0.5)\n            bboxs=[]\n            for j in range(idxs[0].shape[0]):\n                bbox = transformed_anchors[idxs[0][j], :]*scale\n                if int(classification[idxs[0][j]]) == 0:  # class0=people\n                    bboxs.append(bbox.tolist())\n\n            prn_result = self.prn_process(joint_list, bboxs, img_name)\n            for result in prn_result:\n                multipose_results.append(result)\n\n            if self.params.testresult_write_image:\n                canvas = plot_result(img, prn_result)\n                cv2.imwrite(os.path.join(self.params.testresult_dir, img_name.split(\'.\', 1)[0] + \'_1heatmap.png\'), heatmap_max * 256)\n                cv2.imwrite(os.path.join(self.params.testresult_dir, img_name.split(\'.\', 1)[0] + \'_2canvas.png\'), canvas)\n\n        if self.params.testresult_write_json:\n            with open(self.params.testresult_dir+\'multipose_results.json\', ""w"") as f:\n                json.dump(multipose_results, f)\n\n    def _get_multiplier(self, img):\n        """"""Computes the sizes of image at different scales\n        :param img: numpy array, the current image\n        :returns : list of float. The computed scales\n        """"""\n        scale_search = [0.5, 1., 1.5, 2, 2.5]\n        return [x * self.params.inp_size / float(img.shape[0]) for x in scale_search]\n\n    def _get_outputs(self, multiplier, img):\n        """"""Computes the averaged heatmap and paf for the given image\n        :param multiplier:\n        :param origImg: numpy array, the image being processed\n        :param model: pytorch model\n        :returns: numpy arrays, the averaged paf and heatmap\n        """"""\n\n        heatmap_avg = np.zeros((img.shape[0], img.shape[1], 18))\n        bbox_all = []\n        # max_scale = multiplier[-1]\n        # max_size = max_scale * img.shape[0]\n        # # padding\n        # max_cropped, _, _ = crop_with_factor(\n        #     img, max_size, factor=32)\n\n        for m in range(len(multiplier)):\n            scale = multiplier[m]\n            inp_size = scale * img.shape[0]\n\n            # padding\n            im_cropped, im_scale, real_shape = crop_with_factor(\n                img, inp_size, factor=32, pad_val=128)\n            im_data = resnet_preprocess(im_cropped)\n\n            im_data = np.expand_dims(im_data, 0)\n            with torch.no_grad():\n                im_data = torch.from_numpy(im_data).type(torch.FloatTensor).cuda(device=self.params.gpus[0])\n\n            heatmaps, [scores, classification, transformed_anchors] = self.model([im_data, self.params.subnet_name])\n            heatmaps = heatmaps.cpu().detach().numpy().transpose(0, 2, 3, 1)\n            scores = scores.cpu().detach().numpy()\n            classification = classification.cpu().detach().numpy()\n            transformed_anchors = transformed_anchors.cpu().detach().numpy()\n\n            heatmap = heatmaps[0, :int(im_cropped.shape[0] / 4), :int(im_cropped.shape[1] / 4), :]\n            heatmap = cv2.resize(heatmap, None, fx=4, fy=4, interpolation=cv2.INTER_CUBIC)\n            heatmap = heatmap[0:real_shape[0], 0:real_shape[1], :]\n            heatmap = cv2.resize(\n                heatmap, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_CUBIC)\n\n            heatmap_avg = heatmap_avg + heatmap / len(multiplier)\n\n            # bboxs\n            idxs = np.where(scores > 0.5)\n            bboxs=[]\n            for j in range(idxs[0].shape[0]):\n                bbox = transformed_anchors[idxs[0][j], :]/im_scale\n                if int(classification[idxs[0][j]]) == 0:  # class0=people\n                    bboxs.append(bbox.tolist())\n            bbox_all.append(bboxs)\n\n        return heatmap_avg, bbox_all\n\n    def _handle_heat(self, normal_heat, flipped_heat):\n        """"""Compute the average of normal and flipped heatmap\n        :param normal_heat: numpy array, the normal heatmap\n        :param flipped_heat: numpy array, the flipped heatmap\n        :returns: numpy arrays, the averaged heatmap\n        """"""\n\n        # The order to swap left and right of heatmap\n        swap_heat = np.array((0, 1, 5, 6, 7, 2, 3, 4, 11, 12,\n                              13, 8, 9, 10, 15, 14, 17, 16))#, 18\n\n        averaged_heatmap = (normal_heat + flipped_heat[:, ::-1, :][:, :, swap_heat]) / 2.\n\n        return averaged_heatmap\n\n    def prn_process(self, kps, bbox_list, file_name, image_id=0):\n\n        prn_result = []\n\n        idx = 0\n        ks = []\n        for j in range(17):  # joint type\n            t = []\n            for k in kps:\n                if k[-1] == j:  # joint type\n                    x = k[0]\n                    y = k[1]\n                    v = 1  # k[2]\n                    if v > 0:\n                        t.append([x, y, 1, idx])\n                        idx += 1\n            ks.append(t)\n        peaks = ks\n\n        w = int(18 * self.params.coeff)\n        h = int(28 * self.params.coeff)\n\n        bboxes = []\n        for bbox_item in bbox_list:\n            bboxes.append([bbox_item[0], bbox_item[1], bbox_item[2]-bbox_item[0], bbox_item[3]-bbox_item[1]])\n\n        if len(bboxes) == 0 or len(peaks) == 0:\n            return prn_result\n\n        weights_bbox = np.zeros((len(bboxes), h, w, 4, 17))\n\n        for joint_id, peak in enumerate(peaks):  # joint_id: which joint\n            for instance_id, instance in enumerate(peak):  # instance_id: which people\n                p_x = instance[0]\n                p_y = instance[1]\n                for bbox_id, b in enumerate(bboxes):\n                    is_inside = p_x > b[0] - b[2] * self.params.in_thres and \\\n                                p_y > b[1] - b[3] * self.params.in_thres and \\\n                                p_x < b[0] + b[2] * (1.0 + self.params.in_thres) and \\\n                                p_y < b[1] + b[3] * (1.0 + self.params.in_thres)\n                    if is_inside:\n                        x_scale = float(w) / math.ceil(b[2])\n                        y_scale = float(h) / math.ceil(b[3])\n                        x0 = int((p_x - b[0]) * x_scale)\n                        y0 = int((p_y - b[1]) * y_scale)\n                        if x0 >= w and y0 >= h:\n                            x0 = w - 1\n                            y0 = h - 1\n                        elif x0 >= w:\n                            x0 = w - 1\n                        elif y0 >= h:\n                            y0 = h - 1\n                        elif x0 < 0 and y0 < 0:\n                            x0 = 0\n                            y0 = 0\n                        elif x0 < 0:\n                            x0 = 0\n                        elif y0 < 0:\n                            y0 = 0\n                        p = 1e-9\n                        weights_bbox[bbox_id, y0, x0, :, joint_id] = [1, instance[2], instance[3], p]\n        old_weights_bbox = np.copy(weights_bbox)\n\n        for j in range(weights_bbox.shape[0]):\n            for t in range(17):\n                weights_bbox[j, :, :, 0, t] = gaussian(weights_bbox[j, :, :, 0, t])\n\n        output_bbox = []\n        for j in range(weights_bbox.shape[0]):\n            inp = weights_bbox[j, :, :, 0, :]\n            input = torch.from_numpy(np.expand_dims(inp, axis=0)).cuda().float()\n            output, _ = self.model([input, \'prn_subnet\'])\n            temp = np.reshape(output.data.cpu().numpy(), (56, 36, 17))\n            output_bbox.append(temp)\n\n        output_bbox = np.array(output_bbox)\n\n        keypoints_score = []\n\n        for t in range(17):\n            indexes = np.argwhere(old_weights_bbox[:, :, :, 0, t] == 1)\n            keypoint = []\n            for i in indexes:\n                cr = crop(output_bbox[i[0], :, :, t], (i[1], i[2]), N=15)\n                score = np.sum(cr)\n\n                kp_id = old_weights_bbox[i[0], i[1], i[2], 2, t]\n                kp_score = old_weights_bbox[i[0], i[1], i[2], 1, t]\n                p_score = old_weights_bbox[i[0], i[1], i[2], 3, t]  ## ??\n                bbox_id = i[0]\n\n                score = kp_score * score\n\n                s = [kp_id, bbox_id, kp_score, score]\n\n                keypoint.append(s)\n            keypoints_score.append(keypoint)\n\n        bbox_keypoints = np.zeros((weights_bbox.shape[0], 17, 3))\n        bbox_ids = np.arange(len(bboxes)).tolist()\n\n        # kp_id, bbox_id, kp_score, my_score\n        for i in range(17):\n            joint_keypoints = keypoints_score[i]\n            if len(joint_keypoints) > 0:  # if have output result in one type keypoint\n\n                kp_ids = list(set([x[0] for x in joint_keypoints]))\n\n                table = np.zeros((len(bbox_ids), len(kp_ids), 4))\n\n                for b_id, bbox in enumerate(bbox_ids):\n                    for k_id, kp in enumerate(kp_ids):\n                        own = [x for x in joint_keypoints if x[0] == kp and x[1] == bbox]\n\n                        if len(own) > 0:\n                            table[bbox, k_id] = own[0]\n                        else:\n                            table[bbox, k_id] = [0] * 4\n\n                for b_id, bbox in enumerate(bbox_ids):  # all bbx, from 0 to ...\n\n                    row = np.argsort(-table[bbox, :, 3])  # in bbx(bbox), sort from big to small, keypoint score\n\n                    if table[bbox, row[0], 3] > 0:  # score\n                        for r in row:  # all keypoints\n                            if table[bbox, r, 3] > 0:\n                                column = np.argsort(\n                                    -table[:, r, 3])  # sort all keypoints r, from big to small, bbx score\n\n                                if bbox == column[0]:  # best bbx. best keypoint\n                                    bbox_keypoints[bbox, i, :] = [x[:3] for x in peaks[i] if x[3] == table[bbox, r, 0]][\n                                        0]\n                                    break\n                                else:  # for bbx column[0], the worst keypoint is row2[0],\n                                    row2 = np.argsort(table[column[0], :, 3])\n                                    if row2[0] == r:\n                                        bbox_keypoints[bbox, i, :] = \\\n                                            [x[:3] for x in peaks[i] if x[3] == table[bbox, r, 0]][0]\n                                        break\n            else:  # len(joint_keypoints) == 0:\n                for j in range(weights_bbox.shape[0]):\n                    b = bboxes[j]\n                    x_scale = float(w) / math.ceil(b[2])\n                    y_scale = float(h) / math.ceil(b[3])\n\n                    for t in range(17):\n                        indexes = np.argwhere(old_weights_bbox[j, :, :, 0, t] == 1)\n                        if len(indexes) == 0:\n                            max_index = np.argwhere(output_bbox[j, :, :, t] == np.max(output_bbox[j, :, :, t]))\n                            bbox_keypoints[j, t, :] = [max_index[0][1] / x_scale + b[0],\n                                                       max_index[0][0] / y_scale + b[1], 0]\n\n        my_keypoints = []\n\n        for i in range(bbox_keypoints.shape[0]):\n            k = np.zeros(51)\n            k[0::3] = bbox_keypoints[i, :, 0]\n            k[1::3] = bbox_keypoints[i, :, 1]\n            k[2::3] = bbox_keypoints[i, :, 2]\n\n            pose_score = 0\n            count = 0\n            for f in range(17):\n                if bbox_keypoints[i, f, 0] != 0 and bbox_keypoints[i, f, 1] != 0:\n                    count += 1\n                pose_score += bbox_keypoints[i, f, 2]\n            pose_score /= 17.0\n\n            my_keypoints.append(k)\n\n            image_data = {\n                \'image_id\': image_id,\n                \'file_name\': file_name,\n                \'category_id\': 1,\n                \'bbox\': bboxes[i],\n                \'score\': pose_score,\n                \'keypoints\': k.tolist()\n            }\n            prn_result.append(image_data)\n\n        return prn_result\n\n    def val(self):\n        self.model.eval()\n        logs = OrderedDict()\n        sum_loss = meter_utils.AverageValueMeter()\n        logger.info(\'Val on validation set...\')\n\n        self.batch_timer.clear()\n        self.data_timer.clear()\n        self.batch_timer.tic()\n        self.data_timer.tic()\n        for step, batch in enumerate(self.val_data):\n            self.data_timer.toc()\n\n            inputs, gts, _ = self.batch_processor(self, batch)\n            _, saved_for_loss = self.model(*inputs)\n            self.batch_timer.toc()\n\n            loss, saved_for_log = self.model.module.build_loss(saved_for_loss, *gts)\n            sum_loss.add(loss.item())\n            self._process_log(saved_for_log, logs)\n\n            if step % self.params.print_freq == 0:\n                self._print_log(step, logs, \'Validation\', max_n_batch=len(self.val_data))\n\n            self.data_timer.tic()\n            self.batch_timer.tic()\n\n        mean, std = sum_loss.value()\n        logger.info(\'\\n\\nValidation loss: mean: {}, std: {}\'.format(mean, std))\n\n    def _load_ckpt(self, ckpt):\n        _, _ = net_utils.load_net(ckpt, self.model, load_state_dict=True)\n\n    def _process_log(self, src_dict, dest_dict):\n        for k, v in src_dict.items():\n            if isinstance(v, (int, float)):\n                dest_dict.setdefault(k, meter_utils.AverageValueMeter())\n                dest_dict[k].add(float(v))\n            else:\n                dest_dict[k] = v\n\n    def _print_log(self, step, log_values, title=\'\', max_n_batch=None):\n        log_str = \'{}\\n\'.format(self.params.exp_name)\n        log_str += \'{}: epoch {}\'.format(title, 0)\n\n        log_str += \'[{}/{}]\'.format(step, max_n_batch)\n\n        i = 0\n        for k, v in log_values.items():\n            if isinstance(v, meter_utils.AverageValueMeter):\n                mean, std = v.value()\n                log_str += \'\\n\\t{}: {:.10f}\'.format(k, mean)\n                i += 1\n\n        if max_n_batch:\n            # print time\n            data_time = self.data_timer.duration + 1e-6\n            batch_time = self.batch_timer.duration + 1e-6\n            rest_seconds = int((max_n_batch - step) * batch_time)\n            log_str += \'\\n\\t({:.2f}/{:.2f}s,\' \\\n                       \' fps:{:.1f}, rest: {})\'.format(data_time, batch_time,\n                                                       self.params.batch_size / batch_time,\n                                                       str(datetime.timedelta(seconds=rest_seconds)))\n            self.batch_timer.clear()\n            self.data_timer.clear()\n\n        logger.info(log_str)\n'"
lib/__init__.py,0,b''
network/__init__.py,0,b''
network/anchors.py,2,"b'import numpy as np\nimport torch\nimport torch.nn as nn\n\n\nclass Anchors(nn.Module):\n    def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None):\n        super(Anchors, self).__init__()\n\n        if pyramid_levels is None:\n            self.pyramid_levels = [3, 4, 5, 6, 7]\n        if strides is None:\n            self.strides = [2 ** x for x in self.pyramid_levels]\n        if sizes is None:\n            self.sizes = [2 ** (x + 2) for x in self.pyramid_levels]\n        if ratios is None:\n            self.ratios = np.array([0.5, 1, 2])\n        if scales is None:\n            self.scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n\n    def forward(self, image):\n        \n        image_shape = image.shape[2:]\n        image_shape = np.array(image_shape)\n        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels]\n\n        # compute anchors over all pyramid levels\n        all_anchors = np.zeros((0, 4)).astype(np.float32)\n\n        for idx, p in enumerate(self.pyramid_levels):\n            anchors         = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales)\n            shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors)\n            all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n\n        all_anchors = np.expand_dims(all_anchors, axis=0)\n\n        return torch.from_numpy(all_anchors.astype(np.float32)).cuda()\n\ndef generate_anchors(base_size=16, ratios=None, scales=None):\n    """"""\n    Generate anchor (reference) windows by enumerating aspect ratios X\n    scales w.r.t. a reference window.\n    """"""\n\n    if ratios is None:\n        ratios = np.array([0.5, 1, 2])\n\n    if scales is None:\n        scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n\n    num_anchors = len(ratios) * len(scales)\n\n    # initialize output anchors\n    anchors = np.zeros((num_anchors, 4))\n\n    # scale base_size\n    anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T\n\n    # compute areas of anchors\n    areas = anchors[:, 2] * anchors[:, 3]\n\n    # correct for ratios\n    anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))\n    anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))\n\n    # transform from (x_ctr, y_ctr, w, h) -> (x1, y1, x2, y2)\n    anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T\n    anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T\n\n    return anchors\n\ndef compute_shape(image_shape, pyramid_levels):\n    """"""Compute shapes based on pyramid levels.\n\n    :param image_shape:\n    :param pyramid_levels:\n    :return:\n    """"""\n    image_shape = np.array(image_shape[:2])\n    image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in pyramid_levels]\n    return image_shapes\n\n\ndef anchors_for_shape(\n    image_shape,\n    pyramid_levels=None,\n    ratios=None,\n    scales=None,\n    strides=None,\n    sizes=None,\n    shapes_callback=None,\n):\n\n    image_shapes = compute_shape(image_shape, pyramid_levels)\n\n    # compute anchors over all pyramid levels\n    all_anchors = np.zeros((0, 4))\n    for idx, p in enumerate(pyramid_levels):\n        anchors         = generate_anchors(base_size=sizes[idx], ratios=ratios, scales=scales)\n        shifted_anchors = shift(image_shapes[idx], strides[idx], anchors)\n        all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n\n    return all_anchors\n\n\ndef shift(shape, stride, anchors):\n    shift_x = (np.arange(0, shape[1]) + 0.5) * stride\n    shift_y = (np.arange(0, shape[0]) + 0.5) * stride\n\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n\n    shifts = np.vstack((\n        shift_x.ravel(), shift_y.ravel(),\n        shift_x.ravel(), shift_y.ravel()\n    )).transpose()\n\n    # add A anchors (1, A, 4) to\n    # cell K shifts (K, 1, 4) to get\n    # shift anchors (K, A, 4)\n    # reshape to (K*A, 4) shifted anchors\n    A = anchors.shape[0]\n    K = shifts.shape[0]\n    all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n    all_anchors = all_anchors.reshape((K * A, 4))\n\n    return all_anchors\n\n'"
network/fpn.py,3,"b""# -*- coding:utf-8 -*-\n'''RetinaFPN in PyTorch.'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.downsample = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.downsample(x)\n        out = F.relu(out)\n        return out\n\n\nclass FPN(nn.Module):\n    def __init__(self, block, num_blocks):\n        super(FPN, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n\n        # Bottom-up layers\n        self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n\n        # fpn for detection subnet (RetinaNet) P6,P7\n        self.conv6 = nn.Conv2d(2048, 256, kernel_size=3, stride=2, padding=1)  # p6\n        self.conv7 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)  # p7\n\n        # pure fpn layers for detection subnet (RetinaNet)\n        # Lateral layers\n        self.latlayer1 = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)  # c5 -> p5\n        self.latlayer2 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)  # c4 -> p4\n        self.latlayer3 = nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)  # c3 -> p3\n        # smooth\n        self.toplayer0 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)  # smooth p5\n        self.toplayer1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)  # smooth p4\n        self.toplayer2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)  # smooth p3\n\n        # pure fpn layers for keypoint subnet\n        # Lateral layers\n        self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)  # c5 -> p5\n        self.flatlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)  # c4 -> p4\n        self.flatlayer2 = nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)  # c3 -> p3\n        self.flatlayer3 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)  # c2 -> p2\n        # smooth\n        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)  # smooth p4\n        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)  # smooth p3\n        self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)  # smooth p2\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def _upsample_add(self, x, y):\n        '''Upsample and add two feature maps.\n\n        Args:\n          x: top feature map to be upsampled.\n          y: lateral feature map.\n\n        Returns:\n          added feature map.\n        '''\n        _,_,H,W = y.size()\n        return F.upsample(x, size=(H,W), mode='nearest', align_corners=None) + y  # bilinear, False\n\n    def forward(self, x):\n        # Bottom-up\n        c1 = F.relu(self.bn1(self.conv1(x)))\n        c1 = F.max_pool2d(c1, kernel_size=3, stride=2, padding=1)\n        c2 = self.layer1(c1)\n        c3 = self.layer2(c2)\n        c4 = self.layer3(c3)\n        c5 = self.layer4(c4)\n\n        # pure fpn for detection subnet, RetinaNet\n        p6 = self.conv6(c5)\n        p7 = self.conv7(F.relu(p6))\n        p5 = self.latlayer1(c5)\n        p4 = self._upsample_add(p5, self.latlayer2(c4))\n        p3 = self._upsample_add(p4, self.latlayer3(c3))\n        p5 = self.toplayer0(p5)\n        p4 = self.toplayer1(p4)\n        p3 = self.toplayer2(p3)\n\n        # pure fpn for keypoints estimation\n        fp5 = self.toplayer(c5)\n        fp4 = self._upsample_add(fp5,self.flatlayer1(c4))\n        fp3 = self._upsample_add(fp4,self.flatlayer2(c3))\n        fp2 = self._upsample_add(fp3,self.flatlayer3(c2))\n        # Smooth\n        fp4 = self.smooth1(fp4)\n        fp3 = self.smooth2(fp3)\n        fp2 = self.smooth3(fp2)\n\n        return [[fp2,fp3,fp4,fp5],[p3, p4, p5, p6, p7]]\n\ndef FPN50():\n    # [3,4,6,3] -> resnet50\n    return FPN(Bottleneck, [3,4,6,3])\n\ndef FPN101():\n    # [3,4,23,3] -> resnet101\n    return FPN(Bottleneck, [3,4,23,3])\n"""
network/joint_utils.py,0,"b'import cv2\nimport math\nimport numpy as np\nfrom scipy.ndimage.filters import gaussian_filter, maximum_filter\nfrom scipy.ndimage.morphology import generate_binary_structure\n\n# Color code used to plot different joints and limbs (eg: joint_type=3 and\n# limb_type=3 will use colors[3])\ncolors = [\n    [255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0],\n    [85, 255, 0], [0, 255, 0], [0, 255, 85], [0, 255, 170], [0, 255, 255],\n    [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], [170, 0, 255],\n    [255, 0, 255], [255, 0, 170], [255, 0, 85], [255, 0, 0]]\nlimbSeq = [[0,1], [1,2], [2,3], [0,4], [4,5], [5,6], [0,7], [7,8], [8,9], [0,10], [10,11], [11,12], \\\n           [0,13], [13,15], [0,14],[14,16]]\nNUM_JOINTS = 18\n\n\ndef find_peaks(param, img):\n    """"""\n    Given a (grayscale) image, find local maxima whose value is above a given\n    threshold (param[\'thre1\'])\n    :param img: Input image (2d array) where we want to find peaks\n    :return: 2d np.array containing the [x,y] coordinates of each peak found\n    in the image\n    """"""\n\n    peaks_binary = (maximum_filter(img, footprint=generate_binary_structure(\n        2, 1)) == img) * (img > param[\'thre1\'])\n    # Note reverse ([::-1]): we return [[x y], [x y]...] instead of [[y x], [y\n    # x]...]\n    return np.array(np.nonzero(peaks_binary)[::-1]).T\n\n\ndef compute_resized_coords(coords, resizeFactor):\n    """"""\n    Given the index/coordinates of a cell in some input array (e.g. image),\n    provides the new coordinates if that array was resized by making it\n    resizeFactor times bigger.\n    E.g.: image of size 3x3 is resized to 6x6 (resizeFactor=2), we\'d like to\n    know the new coordinates of cell [1,2] -> Function would return [2.5,4.5]\n    :param coords: Coordinates (indices) of a cell in some input array\n    :param resizeFactor: Resize coefficient = shape_dest/shape_source. E.g.:\n    resizeFactor=2 means the destination array is twice as big as the\n    original one\n    :return: Coordinates in an array of size\n    shape_dest=resizeFactor*shape_source, expressing the array indices of the\n    closest point to \'coords\' if an image of size shape_source was resized to\n    shape_dest\n    """"""\n\n    # 1) Add 0.5 to coords to get coordinates of center of the pixel (e.g.\n    # index [0,0] represents the pixel at location [0.5,0.5])\n    # 2) Transform those coordinates to shape_dest, by multiplying by resizeFactor\n    # 3) That number represents the location of the pixel center in the new array,\n    # so subtract 0.5 to get coordinates of the array index/indices (revert\n    # step 1)\n    return (np.array(coords, dtype=float) + 0.5) * resizeFactor - 0.5\n\n\ndef NMS(param, heatmaps, upsampFactor=1., bool_refine_center=True, bool_gaussian_filt=False):\n    """"""\n    NonMaximaSuppression: find peaks (local maxima) in a set of grayscale images\n    :param heatmaps: set of grayscale images on which to find local maxima (3d np.array,\n    with dimensions image_height x image_width x num_heatmaps)\n    :param upsampFactor: Size ratio between CPM heatmap output and the input image size.\n    Eg: upsampFactor=16 if original image was 480x640 and heatmaps are 30x40xN\n    :param bool_refine_center: Flag indicating whether:\n     - False: Simply return the low-res peak found upscaled by upsampFactor (subject to grid-snap)\n     - True: (Recommended, very accurate) Upsample a small patch around each low-res peak and\n     fine-tune the location of the peak at the resolution of the original input image\n    :param bool_gaussian_filt: Flag indicating whether to apply a 1d-GaussianFilter (smoothing)\n    to each upsampled patch before fine-tuning the location of each peak.\n    :return: a NUM_JOINTS x 4 np.array where each row represents a joint type (0=nose, 1=neck...)\n    and the columns indicate the {x,y} position, the score (probability) and a unique id (counter)\n    """"""\n    # MODIFIED BY CARLOS: Instead of upsampling the heatmaps to heatmap_avg and\n    # then performing NMS to find peaks, this step can be sped up by ~25-50x by:\n    # (9-10ms [with GaussFilt] or 5-6ms [without GaussFilt] vs 250-280ms on RoG\n    # 1. Perform NMS at (low-res) CPM\'s output resolution\n    # 1.1. Find peaks using scipy.ndimage.filters.maximum_filter\n    # 2. Once a peak is found, take a patch of 5x5 centered around the peak, upsample it, and\n    # fine-tune the position of the actual maximum.\n    #  \'-> That\'s equivalent to having found the peak on heatmap_avg, but much faster because we only\n    #      upsample and scan the 5x5 patch instead of the full (e.g.) 480x640\n\n    joint_list_per_joint_type = []\n    cnt_total_joints = 0\n\n    # For every peak found, win_size specifies how many pixels in each\n    # direction from the peak we take to obtain the patch that will be\n    # upsampled. Eg: win_size=1 -> patch is 3x3; win_size=2 -> 5x5\n    # (for BICUBIC interpolation to be accurate, win_size needs to be >=2!)\n    win_size = 2\n\n    for joint in range(NUM_JOINTS):\n        map_orig = heatmaps[:, :, joint]\n        peak_coords = find_peaks(param, map_orig)\n        peaks = np.zeros((len(peak_coords), 4))\n        for i, peak in enumerate(peak_coords):\n            if bool_refine_center:\n                x_min, y_min = np.maximum(0, peak - win_size)\n                x_max, y_max = np.minimum(\n                    np.array(map_orig.T.shape) - 1, peak + win_size)\n\n                # Take a small patch around each peak and only upsample that\n                # tiny region\n                patch = map_orig[y_min:y_max + 1, x_min:x_max + 1]\n                map_upsamp = cv2.resize(\n                    patch, None, fx=upsampFactor, fy=upsampFactor, interpolation=cv2.INTER_CUBIC)\n\n                # Gaussian filtering takes an average of 0.8ms/peak (and there might be\n                # more than one peak per joint!) -> For now, skip it (it\'s\n                # accurate enough)\n                map_upsamp = gaussian_filter(\n                    map_upsamp, sigma=3) if bool_gaussian_filt else map_upsamp\n\n                # Obtain the coordinates of the maximum value in the patch\n                location_of_max = np.unravel_index(\n                    map_upsamp.argmax(), map_upsamp.shape)\n                # Remember that peaks indicates [x,y] -> need to reverse it for\n                # [y,x]\n                location_of_patch_center = compute_resized_coords(\n                    peak[::-1] - [y_min, x_min], upsampFactor)\n                # Calculate the offset wrt to the patch center where the actual\n                # maximum is\n                refined_center = (location_of_max - location_of_patch_center)\n                peak_score = map_upsamp[location_of_max]\n            else:\n                refined_center = [0, 0]\n                # Flip peak coordinates since they are [x,y] instead of [y,x]\n                peak_score = map_orig[tuple(peak[::-1])]\n            peaks[i, :] = tuple([int(round(x)) for x in compute_resized_coords(\n                peak_coords[i], upsampFactor) + refined_center[::-1]]) + (peak_score, cnt_total_joints)\n            cnt_total_joints += 1\n        joint_list_per_joint_type.append(peaks)\n\n    return joint_list_per_joint_type\n\n\ndef get_joint_list(img_orig, param, heatmaps, scale):\n\n    joint_list_per_joint_type = NMS(param,\n                                    heatmaps, img_orig.shape[0] / float(heatmaps.shape[0]))\n\n    for peaks in joint_list_per_joint_type:\n        peaks[:, :2] = peaks[:, :2]*scale\n\n    joint_list = np.array([tuple(peak) + (joint_type,) for joint_type, joint_peaks\n                           in enumerate(joint_list_per_joint_type) for peak in joint_peaks])\n\n    return joint_list\n\n\ndef draw(canvas, joints, bbox):\n\n    x1 = int(bbox[0])\n    y1 = int(bbox[1])\n    x2 = int(bbox[0]+bbox[2])\n    y2 = int(bbox[1]+bbox[3])\n    cv2.rectangle(canvas, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=2)\n\n    for i in range(17):\n        if int(joints[i][2]) == 0:\n            continue\n        x = int(joints[i][0])\n        y = int(joints[i][1])\n        cv2.circle(canvas, (x, y), 4, colors[i], thickness=-1)\n\n    #     cur_canvas = canvas.copy()\n    stickwidth = 2\n    for i in range(16):\n        if joints[limbSeq[i][0]][2] == 0 or joints[limbSeq[i][1]][2] == 0:\n            continue\n        X = (int(joints[limbSeq[i][0]][0]), int(joints[limbSeq[i][1]][0]))\n        Y = (int(joints[limbSeq[i][0]][1]), int(joints[limbSeq[i][1]][1]))\n        mX = np.mean(X)\n        mY = np.mean(Y)\n        length = ((X[0] - X[1]) ** 2 + (Y[0] - Y[1]) ** 2) ** 0.5\n        angle = math.degrees(math.atan2(Y[0] - Y[1], X[0] - X[1]))\n        polygon = cv2.ellipse2Poly((int(mX), int(mY)), (int(length / 2), stickwidth), int(angle), 0, 360, 1)\n        cv2.fillConvexPoly(canvas, polygon, colors[i])\n\n    return canvas\n\ndef plot_result(img_orig, result):\n\n    for idx, person_data in enumerate(result):\n\n        bbox = person_data[\'bbox\']\n        keypoints = person_data[\'keypoints\']\n\n        x = keypoints[0::3]\n        y = keypoints[1::3]\n        v = keypoints[2::3]\n\n        joints = []\n        for i in range(len(x)):\n            joints.append([x[i], y[i], v[i]])\n\n        img_orig = draw(img_orig, joints, bbox)\n    return img_orig'"
network/losses.py,34,"b'import numpy as np\nimport torch\nimport torch.nn as nn\n\ndef calc_iou(a, b):\n    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n\n    iw = torch.min(torch.unsqueeze(a[:, 2], dim=1), b[:, 2]) - torch.max(torch.unsqueeze(a[:, 0], 1), b[:, 0])\n    ih = torch.min(torch.unsqueeze(a[:, 3], dim=1), b[:, 3]) - torch.max(torch.unsqueeze(a[:, 1], 1), b[:, 1])\n\n    iw = torch.clamp(iw, min=0)\n    ih = torch.clamp(ih, min=0)\n\n    ua = torch.unsqueeze((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), dim=1) + area - iw * ih\n\n    ua = torch.clamp(ua, min=1e-8)\n\n    intersection = iw * ih\n\n    IoU = intersection / ua\n\n    return IoU\n\nclass FocalLoss(nn.Module):\n    #def __init__(self):\n\n    def forward(self, classifications, regressions, anchors, annotations):\n        alpha = 0.25\n        gamma = 2.0\n        batch_size = classifications.shape[0]\n        classification_losses = []\n        regression_losses = []\n\n        anchor = anchors[0, :, :]\n\n        anchor_widths  = anchor[:, 2] - anchor[:, 0]\n        anchor_heights = anchor[:, 3] - anchor[:, 1]\n        anchor_ctr_x   = anchor[:, 0] + 0.5 * anchor_widths\n        anchor_ctr_y   = anchor[:, 1] + 0.5 * anchor_heights\n\n        for j in range(batch_size):\n\n            classification = classifications[j, :, :]\n            regression = regressions[j, :, :]\n\n            bbox_annotation = annotations[j, :, :]\n            bbox_annotation = bbox_annotation[bbox_annotation[:, 4] != -1]\n\n            if bbox_annotation.shape[0] == 0:\n                regression_losses.append(torch.tensor(0, requires_grad=True).float().cuda())\n                classification_losses.append(torch.tensor(0, requires_grad=True).float().cuda())\n\n                continue\n\n            classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4)\n\n            IoU = calc_iou(anchors[0, :, :], bbox_annotation[:, :4]) # num_anchors x num_annotations\n\n            IoU_max, IoU_argmax = torch.max(IoU, dim=1) # num_anchors x 1\n\n            #import pdb\n            #pdb.set_trace()\n\n            # compute the loss for classification\n            targets = torch.ones(classification.shape) * -1\n            targets = targets.cuda()\n\n            targets[torch.lt(IoU_max, 0.4), :] = 0\n\n            positive_indices = torch.ge(IoU_max, 0.5)\n\n            num_positive_anchors = positive_indices.sum()\n\n            assigned_annotations = bbox_annotation[IoU_argmax, :]\n\n            targets[positive_indices, :] = 0\n            targets[positive_indices, assigned_annotations[positive_indices, 4].long()] = 1\n\n            alpha_factor = torch.ones(targets.shape).cuda() * alpha\n\n            alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor)\n            focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification)\n            focal_weight = alpha_factor * torch.pow(focal_weight, gamma)\n\n            bce = -(targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification))\n\n            # cls_loss = focal_weight * torch.pow(bce, gamma)\n            cls_loss = focal_weight * bce\n\n            cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, torch.zeros(cls_loss.shape).cuda())\n\n            classification_losses.append(cls_loss.sum()/torch.clamp(num_positive_anchors.float(), min=1.0))\n\n            # compute the loss for regression\n\n            if positive_indices.sum() > 0:\n                assigned_annotations = assigned_annotations[positive_indices, :]\n\n                anchor_widths_pi = anchor_widths[positive_indices]\n                anchor_heights_pi = anchor_heights[positive_indices]\n                anchor_ctr_x_pi = anchor_ctr_x[positive_indices]\n                anchor_ctr_y_pi = anchor_ctr_y[positive_indices]\n\n                gt_widths  = assigned_annotations[:, 2] - assigned_annotations[:, 0]\n                gt_heights = assigned_annotations[:, 3] - assigned_annotations[:, 1]\n                gt_ctr_x   = assigned_annotations[:, 0] + 0.5 * gt_widths\n                gt_ctr_y   = assigned_annotations[:, 1] + 0.5 * gt_heights\n\n                # clip widths to 1\n                gt_widths  = torch.clamp(gt_widths, min=1)\n                gt_heights = torch.clamp(gt_heights, min=1)\n\n                targets_dx = (gt_ctr_x - anchor_ctr_x_pi) / anchor_widths_pi\n                targets_dy = (gt_ctr_y - anchor_ctr_y_pi) / anchor_heights_pi\n                targets_dw = torch.log(gt_widths / anchor_widths_pi)\n                targets_dh = torch.log(gt_heights / anchor_heights_pi)\n\n                targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh))\n                targets = targets.t()\n\n                targets = targets/torch.Tensor([[0.1, 0.1, 0.2, 0.2]]).cuda()\n\n\n                negative_indices = 1 - positive_indices\n\n                regression_diff = torch.abs(targets - regression[positive_indices, :])\n\n                regression_loss = torch.where(\n                    torch.le(regression_diff, 1.0 / 9.0),\n                    0.5 * 9.0 * torch.pow(regression_diff, 2),\n                    regression_diff - 0.5 / 9.0\n                )\n                regression_losses.append(regression_loss.mean())\n            else:\n                regression_losses.append(torch.tensor(0).float().cuda())\n\n        return torch.stack(classification_losses).mean(dim=0, keepdim=True), torch.stack(regression_losses).mean(dim=0, keepdim=True)\n\n    \n'"
network/net_utils.py,2,"b'import os\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom copy import deepcopy\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\nfrom lib.utils.log import logger\n\ndef set_optimizer_state_devices(state, device_id=None):\n    """"""\n    set state in optimizer to a device. move to cpu if device_id==None\n    :param state: optimizer.state\n    :param device_id: None or a number\n    :return:\n    """"""\n    for k, v in state.items():\n        for k2 in v.keys():\n            if hasattr(v[k2], \'cuda\'):\n                if device_id is None:\n                    v[k2] = v[k2].cpu()\n                else:\n                    v[k2] = v[k2].cuda(device_id)\n\n    return state\n\n\ndef save_net(fname, net, epoch=-1, optimizers=None, rm_prev_opt=False, max_n_ckpts=-1):\n    import h5py\n    with h5py.File(fname, mode=\'w\') as h5f:\n        for k, v in net.state_dict().items():\n            h5f.create_dataset(k, data=v.cpu().numpy())\n        h5f.attrs[\'epoch\'] = epoch\n\n    if optimizers is not None:\n        state_dicts = []\n        for optimizer in optimizers:\n            state_dict = deepcopy(optimizer.state_dict())\n            state_dict[\'state\'] = set_optimizer_state_devices(state_dict[\'state\'], device_id=None)\n            state_dicts.append(state_dict)\n\n        state_file = fname + \'.optimizer_state.pk\'\n        with open(state_file, \'wb\') as f:\n            pickle.dump(state_dicts, f)\n\n        # remove\n        if rm_prev_opt:\n            root = os.path.split(fname)[0]\n            for filename in os.listdir(root):\n                filename = os.path.join(root, filename)\n                if filename.endswith(\'.optimizer_state.pk\') and filename != state_file:\n                    logger.info((\'Remove {}\'.format(filename)))\n                    os.remove(filename)\n\n        # remove ckpt\n        if max_n_ckpts > 0:\n            root = os.path.split(fname)[0]\n            ckpts = [fname for fname in os.listdir(root) if os.path.splitext(fname)[-1] == \'.h5\']\n            ckpts = sorted(ckpts, key=lambda name: int(os.path.splitext(name)[0].split(\'_\')[-1]))\n            if len(ckpts) > max_n_ckpts:\n                for ckpt in ckpts[0:-max_n_ckpts]:\n                    filename = os.path.join(root, ckpt)\n                    logger.info(\'Remove {}\'.format(filename))\n                    os.remove(filename)\n\n\ndef load_net(fname, net, prefix=\'\', load_state_dict=False):\n    import h5py\n    with h5py.File(fname, mode=\'r\') as h5f:\n        h5f_is_module = True\n        for k in h5f.keys():\n            if not str(k).startswith(\'module.\'):\n                h5f_is_module = False\n                break\n        if prefix == \'\' and not isinstance(net, nn.DataParallel) and h5f_is_module:\n            prefix = \'module.\'\n\n        for k, v in net.state_dict().items():\n            k = prefix + k\n            if k in h5f:\n                param = torch.from_numpy(np.asarray(h5f[k]))\n                if v.size() != param.size():\n                    logger.warning(\'Inconsistent shape: {}, {}\'.format(v.size(), param.size()))\n                else:\n                    v.copy_(param)\n            else:\n                logger.warning(\'No layer: {}\'.format(k))\n\n        epoch = h5f.attrs[\'epoch\'] if \'epoch\' in h5f.attrs else -1\n\n        if not load_state_dict:\n            if \'learning_rates\' in h5f.attrs:\n                lr = h5f.attrs[\'learning_rates\']\n            else:\n                lr = h5f.attrs.get(\'lr\', -1)\n                lr = np.asarray([lr] if lr > 0 else [], dtype=np.float)\n\n            return epoch, lr\n\n        state_file = fname + \'.optimizer_state.pk\'\n        if os.path.isfile(state_file):\n            with open(state_file, \'rb\') as f:\n                state_dicts = pickle.load(f)\n                if not isinstance(state_dicts, list):\n                    state_dicts = [state_dicts]\n        else:\n            state_dicts = None\n        return epoch, state_dicts\n\n\n'"
network/posenet.py,17,"b'# -*- coding:utf-8 -*-\n# keypoint subnet + detection subnet(RetinaNet) + PRN\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom network.fpn import FPN50, FPN101\nfrom torch.nn import init\n\nfrom network.utils import BBoxTransform, ClipBoxes\nfrom network.anchors import Anchors\nimport network.losses as losses\nfrom lib.nms.pth_nms import pth_nms\n\n\ndef nms(dets, thresh):\n    ""Dispatch to either CPU or GPU NMS implementations.\\\n    Accept dets as tensor""""""\n    return pth_nms(dets, thresh)\n\n\nclass Concat(nn.Module):\n    def __init__(self):\n        super(Concat, self).__init__()\n\n    def forward(self, up1, up2, up3, up4):\n        return torch.cat((up1, up2, up3, up4), 1)\n\n\nclass RegressionModel(nn.Module):\n    def __init__(self, num_features_in, num_anchors=9, feature_size=256):\n        super(RegressionModel, self).__init__()\n\n        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n        self.act1 = nn.ReLU()\n\n        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n        self.act2 = nn.ReLU()\n\n        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n        self.act3 = nn.ReLU()\n\n        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n        self.act4 = nn.ReLU()\n\n        self.output = nn.Conv2d(feature_size, num_anchors * 4, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.act1(out)\n\n        out = self.conv2(out)\n        out = self.act2(out)\n\n        out = self.conv3(out)\n        out = self.act3(out)\n\n        out = self.conv4(out)\n        out = self.act4(out)\n\n        out = self.output(out)\n\n        # out is B x C x W x H, with C = 4*num_anchors\n        out = out.permute(0, 2, 3, 1)\n\n        return out.contiguous().view(out.shape[0], -1, 4)\n\n\nclass ClassificationModel(nn.Module):\n    def __init__(self, num_features_in, num_anchors=9, num_classes=80, prior=0.01, feature_size=256):\n        super(ClassificationModel, self).__init__()\n\n        self.num_classes = num_classes\n        self.num_anchors = num_anchors\n\n        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n        self.act1 = nn.ReLU()\n\n        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n        self.act2 = nn.ReLU()\n\n        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n        self.act3 = nn.ReLU()\n\n        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n        self.act4 = nn.ReLU()\n\n        self.output = nn.Conv2d(feature_size, num_anchors * num_classes, kernel_size=3, padding=1)\n        self.output_act = nn.Sigmoid()\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.act1(out)\n\n        out = self.conv2(out)\n        out = self.act2(out)\n\n        out = self.conv3(out)\n        out = self.act3(out)\n\n        out = self.conv4(out)\n        out = self.act4(out)\n\n        out = self.output(out)\n        out = self.output_act(out)\n\n        # out is B x C x W x H, with C = n_classes + n_anchors\n        out1 = out.permute(0, 2, 3, 1)\n\n        batch_size, width, height, channels = out1.shape\n\n        out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes)\n\n        return out2.contiguous().view(x.shape[0], -1, self.num_classes)\n\n\nclass Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n\n\nclass Add(nn.Module):\n    def forward(self, input1, input2):\n        return torch.add(input1, input2)\n\n\nclass PRN(nn.Module):\n    def __init__(self,node_count, coeff):\n        super(PRN, self).__init__()\n        self.flatten   = Flatten()\n        self.height    = coeff*28\n        self.width     = coeff*18\n        self.dens1     = nn.Linear(self.height*self.width*17, node_count)\n        self.bneck     = nn.Linear(node_count, node_count)\n        self.dens2     = nn.Linear(node_count, self.height*self.width*17)\n        self.drop      = nn.Dropout()\n        self.add       = Add()\n        self.softmax   = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        res = self.flatten(x)\n        out = self.drop(F.relu(self.dens1(res)))\n        out = self.drop(F.relu(self.bneck(out)))\n        out = F.relu(self.dens2(out))\n        out = self.add(out, res)\n        out = self.softmax(out)\n        out = out.view(out.size()[0], self.height, self.width, 17)\n\n        return out\n\nclass poseNet(nn.Module):\n    def __init__(self, layers, prn_node_count=1024, prn_coeff=2):\n        super(poseNet, self).__init__()\n        if layers == 101:\n            self.fpn = FPN101()\n        if layers == 50:\n            self.fpn = FPN50()\n\n        ##################################################################################\n        # keypoints subnet\n        # intermediate supervision\n        self.convfin_k2 = nn.Conv2d(256, 19, kernel_size=1, stride=1, padding=0)\n        self.convfin_k3 = nn.Conv2d(256, 19, kernel_size=1, stride=1, padding=0)\n        self.convfin_k4 = nn.Conv2d(256, 19, kernel_size=1, stride=1, padding=0)\n        self.convfin_k5 = nn.Conv2d(256, 19, kernel_size=1, stride=1, padding=0)\n\n        # 2 conv(kernel=3x3)\xef\xbc\x8cchange channels from 256 to 128\n        self.convt1 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n        self.convt2 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n        self.convt3 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n        self.convt4 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n        self.convs1 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n        self.convs2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n        self.convs3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n        self.convs4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n\n        self.upsample1 = nn.Upsample(scale_factor=8, mode=\'nearest\', align_corners=None)\n        self.upsample2 = nn.Upsample(scale_factor=4, mode=\'nearest\', align_corners=None)\n        self.upsample3 = nn.Upsample(scale_factor=2, mode=\'nearest\', align_corners=None)\n        # self.upsample4 = nn.Upsample(size=(120,120),mode=\'bilinear\',align_corners=True)\n\n        self.concat = Concat()\n        self.conv2 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n        self.convfin = nn.Conv2d(256, 18, kernel_size=1, stride=1, padding=0)\n\n        ##################################################################################\n        # detection subnet\n        self.regressionModel = RegressionModel(256)\n        self.classificationModel = ClassificationModel(256, num_classes=1)\n        self.anchors = Anchors()\n        self.regressBoxes = BBoxTransform()\n        self.clipBoxes = ClipBoxes()\n        self.focalLoss = losses.FocalLoss()\n\n        ##################################################################################\n        # prn subnet\n        self.prn = PRN(prn_node_count, prn_coeff)\n\n        ##################################################################################\n        # initialize weights\n        self._initialize_weights_norm()\n        prior = 0.01\n        self.classificationModel.output.weight.data.fill_(0)\n        self.classificationModel.output.bias.data.fill_(-math.log((1.0 - prior) / prior))\n        self.regressionModel.output.weight.data.fill_(0)\n        self.regressionModel.output.bias.data.fill_(0)\n\n        self.freeze_bn()  # from retinanet\n\n    def _initialize_weights_norm(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.normal_(m.weight, std=0.01)\n                if m.bias is not None:  # resnet101 conv2d doesn\'t add bias\n                    init.constant_(m.bias, 0.0)\n\n    def freeze_bn(self):\n        \'\'\'Freeze BatchNorm layers.\'\'\'\n        for layer in self.modules():\n            if isinstance(layer, nn.BatchNorm2d):\n                layer.eval()\n\n    def forward(self, x):\n\n        img_batch, subnet_name = x\n\n        if subnet_name == \'keypoint_subnet\':\n            return self.keypoint_forward(img_batch)\n        elif subnet_name == \'detection_subnet\':\n            return self.detection_forward(img_batch)\n        elif subnet_name == \'prn_subnet\':\n            return self.prn_forward(img_batch)\n        else:  # entire_net\n            features = self.fpn(img_batch)\n            p2, p3, p4, p5 = features[0]  # fpn features for keypoint subnet\n            features = features[1]  # fpn features for detection subnet\n\n            ##################################################################################\n            # keypoints subnet\n            p5 = self.convt1(p5)\n            p5 = self.convs1(p5)\n            p4 = self.convt2(p4)\n            p4 = self.convs2(p4)\n            p3 = self.convt3(p3)\n            p3 = self.convs3(p3)\n            p2 = self.convt4(p2)\n            p2 = self.convs4(p2)\n\n            p5 = self.upsample1(p5)\n            p4 = self.upsample2(p4)\n            p3 = self.upsample3(p3)\n\n            concat = self.concat(p5, p4, p3, p2)\n            predict_keypoint = self.convfin(F.relu(self.conv2(concat)))\n            del p5, p4, p3, p2, concat\n\n            ##################################################################################\n            # detection subnet\n            regression = torch.cat([self.regressionModel(feature) for feature in features], dim=1)\n            classification = torch.cat([self.classificationModel(feature) for feature in features], dim=1)\n            anchors = self.anchors(img_batch)\n\n            transformed_anchors = self.regressBoxes(anchors, regression)\n            transformed_anchors = self.clipBoxes(transformed_anchors, img_batch)\n\n            scores = torch.max(classification, dim=2, keepdim=True)[0]\n\n            scores_over_thresh = (scores > 0.05)[0, :, 0]#0.05\n\n            if scores_over_thresh.sum() == 0:\n                # no boxes to NMS, just return\n                return predict_keypoint, [torch.zeros(0), torch.zeros(0), torch.zeros(0, 4)]\n\n            classification = classification[:, scores_over_thresh, :]\n            transformed_anchors = transformed_anchors[:, scores_over_thresh, :]\n            scores = scores[:, scores_over_thresh, :]\n\n            anchors_nms_idx = nms(torch.cat([transformed_anchors, scores], dim=2)[0, :, :], 0.5)  # threshold = 0.5, inpsize=480\n\n            nms_scores, nms_class = classification[0, anchors_nms_idx, :].max(dim=1)\n\n            return predict_keypoint, [nms_scores, nms_class, transformed_anchors[0, anchors_nms_idx, :]]\n\n\n    def keypoint_forward(self, img_batch):\n        saved_for_loss = []\n\n        p2, p3, p4, p5 = self.fpn(img_batch)[0] # fpn features for keypoint subnet\n\n        ##################################################################################\n        # keypoints subnet\n        # intermediate supervision\n        saved_for_loss.append(self.convfin_k2(p2))\n        saved_for_loss.append(self.upsample3(self.convfin_k3(p3)))\n        saved_for_loss.append(self.upsample2(self.convfin_k4(p4)))\n        saved_for_loss.append(self.upsample1(self.convfin_k5(p5)))\n\n        #\n        p5 = self.convt1(p5)\n        p5 = self.convs1(p5)\n        p4 = self.convt2(p4)\n        p4 = self.convs2(p4)\n        p3 = self.convt3(p3)\n        p3 = self.convs3(p3)\n        p2 = self.convt4(p2)\n        p2 = self.convs4(p2)\n\n        p5 = self.upsample1(p5)\n        p4 = self.upsample2(p4)\n        p3 = self.upsample3(p3)\n\n        predict_keypoint = self.convfin(F.relu(self.conv2(self.concat(p5, p4, p3, p2))))\n        saved_for_loss.append(predict_keypoint)\n\n        return predict_keypoint, saved_for_loss\n\n    def detection_forward(self, img_batch):\n        saved_for_loss = []\n\n        features = self.fpn(img_batch)[1]  # fpn features for detection subnet\n\n        ##################################################################################\n        # detection subnet\n        regression = torch.cat([self.regressionModel(feature) for feature in features], dim=1)\n        classification = torch.cat([self.classificationModel(feature) for feature in features], dim=1)\n        anchors = self.anchors(img_batch)\n\n        saved_for_loss.append(classification)\n        saved_for_loss.append(regression)\n        saved_for_loss.append(anchors)\n\n        return [], saved_for_loss\n\n    def prn_forward(self, img_batch):\n        saved_for_loss = []\n\n        res = self.prn.flatten(img_batch)\n        out = self.prn.drop(F.relu(self.prn.dens1(res)))\n        out = self.prn.drop(F.relu(self.prn.bneck(out)))\n        out = F.relu(self.prn.dens2(out))\n        out = self.prn.add(out,res)\n        out = self.prn.softmax(out)\n        out = out.view(out.size()[0], self.prn.height, self.prn.width, 17)\n\n        saved_for_loss.append(out)\n\n        return out, saved_for_loss\n\n    @staticmethod\n    def build_loss(saved_for_loss, *args):\n\n        subnet_name = args[0]\n\n        if subnet_name == \'keypoint_subnet\':\n            return build_keypoint_loss(saved_for_loss, args[1], args[2])\n        elif subnet_name == \'detection_subnet\':\n            return build_detection_loss(saved_for_loss, args[1])\n        elif subnet_name == \'prn_subnet\':\n            return build_prn_loss(saved_for_loss, args[1])\n        else:\n            return 0\n\n\ndef build_keypoint_loss(saved_for_loss, heat_temp, heat_weight):\n\n    names = build_names()\n    saved_for_log = OrderedDict()\n    criterion = nn.MSELoss(size_average=True).cuda()\n    total_loss = 0\n    div1 = 1.\n    #div2 = 100.\n\n    for j in range(5):\n\n        pred1 = saved_for_loss[j][:, :18, :, :] * heat_weight\n        gt1 = heat_weight * heat_temp\n\n        #pred2 = saved_for_loss[j][:, 18:, :, :]\n        #gt2 = mask_all\n\n        # Compute losses\n        loss1 = criterion(pred1, gt1)/div1  # heatmap_loss\n        #loss2 = criterion(pred2, gt2)/div2  # mask_loss\n        total_loss += loss1\n        #total_loss += loss2\n\n        # Get value from Tensor and save for log\n        saved_for_log[names[j*2]] = loss1.item()\n        #saved_for_log[names[j*2+1]] = loss2.item()\n\n    saved_for_log[\'max_ht\'] = torch.max(\n        saved_for_loss[-1].data[:, :18, :, :]).item()\n    saved_for_log[\'min_ht\'] = torch.min(\n        saved_for_loss[-1].data[:, :18, :, :]).item()\n    #saved_for_log[\'max_mask\'] = torch.max(\n    #    saved_for_loss[-1].data[:, 18:, :, :]).item()\n    #saved_for_log[\'min_mask\'] = torch.min(\n    #    saved_for_loss[-1].data[:, 18:, :, :]).item()\n\n    return total_loss, saved_for_log\n\ndef build_detection_loss(saved_for_loss, anno):\n    \'\'\'\n    :param saved_for_loss: [classifications, regressions, anchors]\n    :param anno: annotations\n    :return: classification_loss, regression_loss\n    \'\'\'\n    saved_for_log = OrderedDict()\n\n    # Compute losses\n    focalLoss = losses.FocalLoss()\n    classification_loss, regression_loss = focalLoss(*saved_for_loss, anno)\n    classification_loss = classification_loss.mean()\n    regression_loss = regression_loss.mean()\n    total_loss = classification_loss + regression_loss\n\n    # Get value from Tensor and save for log\n    saved_for_log[\'total_loss\'] = total_loss.item()\n    saved_for_log[\'classification_loss\'] = classification_loss.item()\n    saved_for_log[\'regression_loss\'] = regression_loss.item()\n\n    return total_loss, saved_for_log\n\ndef build_prn_loss(saved_for_loss, label):\n    \'\'\'\n    :param saved_for_loss: [out]\n    :param label: label\n    :return: prn loss\n    \'\'\'\n    saved_for_log = OrderedDict()\n\n    criterion = nn.BCELoss(size_average=True).cuda()\n    total_loss = 0\n\n    # Compute losses\n    loss1 = criterion(saved_for_loss[0], label)\n    total_loss += loss1\n\n    # Get value from Tensor and save for log\n    saved_for_log[\'PRN loss\'] = loss1.item()\n\n    return total_loss, saved_for_log\n\ndef build_names():\n    names = []\n    for j in range(2, 6):\n        names.append(\'heatmap_loss_k%d\' % j)\n        names.append(\'seg_loss_k%d\' % j)\n    names.append(\'heatmap_loss\')\n    names.append(\'seg_loss\')\n    return names\n\n'"
network/utils.py,10,"b'import torch\nimport torch.nn as nn\nimport numpy as np\n\n\nclass BBoxTransform(nn.Module):\n\n    def __init__(self, mean=None, std=None):\n        super(BBoxTransform, self).__init__()\n        if mean is None:\n            self.mean = torch.from_numpy(np.array([0, 0, 0, 0]).astype(np.float32)).cuda()\n        else:\n            self.mean = mean\n        if std is None:\n            self.std = torch.from_numpy(np.array([0.1, 0.1, 0.2, 0.2]).astype(np.float32)).cuda()\n        else:\n            self.std = std\n\n    def forward(self, boxes, deltas):\n\n        widths  = boxes[:, :, 2] - boxes[:, :, 0]\n        heights = boxes[:, :, 3] - boxes[:, :, 1]\n        ctr_x   = boxes[:, :, 0] + 0.5 * widths\n        ctr_y   = boxes[:, :, 1] + 0.5 * heights\n\n        dx = deltas[:, :, 0] * self.std[0] + self.mean[0]\n        dy = deltas[:, :, 1] * self.std[1] + self.mean[1]\n        dw = deltas[:, :, 2] * self.std[2] + self.mean[2]\n        dh = deltas[:, :, 3] * self.std[3] + self.mean[3]\n\n        pred_ctr_x = ctr_x + dx * widths\n        pred_ctr_y = ctr_y + dy * heights\n        pred_w     = torch.exp(dw) * widths\n        pred_h     = torch.exp(dh) * heights\n\n        pred_boxes_x1 = pred_ctr_x - 0.5 * pred_w\n        pred_boxes_y1 = pred_ctr_y - 0.5 * pred_h\n        pred_boxes_x2 = pred_ctr_x + 0.5 * pred_w\n        pred_boxes_y2 = pred_ctr_y + 0.5 * pred_h\n\n        pred_boxes = torch.stack([pred_boxes_x1, pred_boxes_y1, pred_boxes_x2, pred_boxes_y2], dim=2)\n\n        return pred_boxes\n\n\nclass ClipBoxes(nn.Module):\n\n    def __init__(self, width=None, height=None):\n        super(ClipBoxes, self).__init__()\n\n    def forward(self, boxes, img):\n\n        batch_size, num_channels, height, width = img.shape\n\n        boxes[:, :, 0] = torch.clamp(boxes[:, :, 0], min=0)\n        boxes[:, :, 1] = torch.clamp(boxes[:, :, 1], min=0)\n\n        boxes[:, :, 2] = torch.clamp(boxes[:, :, 2], max=width)\n        boxes[:, :, 3] = torch.clamp(boxes[:, :, 3], max=height)\n      \n        return boxes\n'"
training/__init__.py,0,b''
training/batch_processor.py,3,"b""'''\nUtility functions for rtpose project\n--------------------------------------------\nChange to pytorch=0.4.0 by @LiMeng95\nUtility functions for Multipose project\n'''\n\nimport torch\n\ndef batch_processor(state, batch):\n    gpus = state.params.gpus\n    subnet_name = state.params.subnet_name  # 'detection_subnet'/'keypoint_subnet'/'prn_subnet'\n\n    if subnet_name == 'keypoint_subnet':\n        inp, heat_temp, heat_weight = batch\n\n        if not state.model.training:  # used for inference\n            with torch.no_grad():\n                input_var = inp.cuda(device=gpus[0])\n                heat_weight_var = heat_weight.cuda(device=gpus[0], async=False)\n                heat_temp_var = heat_temp.cuda(device=gpus[0], async=False)\n        else:\n            input_var = inp.cuda(device=gpus[0])\n            heat_weight_var = heat_weight.cuda(device=gpus[0], async=False)\n            heat_temp_var = heat_temp.cuda(device=gpus[0], async=False)\n\n        inputs = [[input_var, subnet_name]]\n        gts = [subnet_name, heat_temp_var, heat_weight_var]\n        saved_for_eval = []\n    elif subnet_name == 'detection_subnet':  #'detection_subnet'\n        inp, anno = batch  # anno: [x1, y1, x2, y2, category_id]\n\n        if not state.model.training:  # used for inference\n            with torch.no_grad():\n                input_var = inp.cuda(device=gpus[0])\n                anno_var = anno.cuda(device=gpus[0])\n        else:\n            input_var = inp.cuda(device=gpus[0])\n            anno_var = anno.cuda(device=gpus[0])\n\n        inputs = [[input_var, subnet_name]]\n        gts = [subnet_name, anno_var]\n        saved_for_eval = []\n    else:  #'prn_subnet'\n        inp, label = batch  # input, label\n\n        if not state.model.training:  # used for inference\n            with torch.no_grad():\n                input_var = inp.cuda(device=gpus[0]).float()\n                anno_var = label.cuda(device=gpus[0]).float()\n        else:\n            input_var = inp.cuda(device=gpus[0]).float()\n            anno_var = label.cuda(device=gpus[0]).float()\n\n        inputs = [[input_var, subnet_name]]\n        gts = [subnet_name, anno_var]\n        saved_for_eval = []\n\n    return inputs, gts, saved_for_eval\n\n"""
training/multipose_detection_train.py,2,"b'import os, sys\nroot_path = os.path.realpath(__file__).split(\'/training/multipose_detection_train.py\')[0]\nos.chdir(root_path)\nsys.path.append(root_path)\n\nimport torch\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom training.batch_processor import batch_processor\nfrom network.posenet import poseNet\nfrom datasets.coco import get_loader\nfrom training.trainer import Trainer\n\n# Hyper-params\ncoco_root = \'/data/COCO/\'\nbackbone = \'resnet101\'  # \'resnet50\'\nopt = \'adam\'\nweight_decay = 0.000\ninp_size = 608  # input size 608*608\nfeat_stride = 4\n\n# model parameters in MultiPoseNet\nfpn_resnet_para = [\'conv1\', \'bn1\', \'layer1\', \'layer2\', \'layer3\', \'layer4\']\nfpn_retinanet_para = [\'conv6\', \'conv7\', \'latlayer1\', \'latlayer2\',\n                      \'latlayer3\', \'toplayer0\', \'toplayer1\', \'toplayer2\']\nfpn_keypoint_para = [\'toplayer\', \'flatlayer1\', \'flatlayer2\',\n                     \'flatlayer3\', \'smooth1\', \'smooth2\', \'smooth3\']\nretinanet_para = [\'regressionModel\', \'classificationModel\']\nkeypoint_para = [\'convt1\', \'convt2\', \'convt3\', \'convt4\', \'convs1\', \'convs2\', \'convs3\', \'convs4\', \'upsample1\',\n                 \'upsample2\', \'upsample3\', \'conv2\', \'convfin\', \'convfin_k2\', \'convfin_k3\', \'convfin_k4\', \'convfin_k5\']\nprn_para = [\'prn\']\n\n#####################################################################\n# train detection subnet\ndata_dir = coco_root+\'images/\'\nmask_dir = coco_root\njson_path = coco_root+\'COCO.json\'\n\n# Set Training parameters\nparams = Trainer.TrainParams()\nparams.exp_name = \'res101_detection_subnet/\'\nparams.subnet_name = \'detection_subnet\'\nparams.save_dir = \'./extra/models/{}\'.format(params.exp_name)\nparams.ckpt = \'./demo/models/ckpt_baseline_resnet101.h5\'\nparams.ignore_opt_state = True\n\nparams.max_epoch = 50\nparams.init_lr = 1.e-5\nparams.lr_decay = 0.1\n\nparams.gpus = [0]\nparams.batch_size = 25 * len(params.gpus)\nparams.val_nbatch_end_epoch = 2000\n\nparams.print_freq = 50\n\n# model\nif backbone == \'resnet101\':\n    model = poseNet(101)\nelif backbone == \'resnet50\':\n    model = poseNet(50)\n\n# Train detection subnet (RetinaNet), Fix the weights in backbone (ResNet) ans Key-point Subnet\nfor name, module in model.fpn.named_children():\n    if name in fpn_resnet_para:\n        for para in module.parameters():\n            para.requires_grad = False\nfor name, module in model.fpn.named_children():\n    if name in fpn_keypoint_para:\n        for para in module.parameters():\n            para.requires_grad = False\nfor name, module in model.named_children():\n    if name in keypoint_para:\n        for para in module.parameters():\n            para.requires_grad = False\nfor name, module in model.named_children():\n    if name in prn_para:\n        for para in module.parameters():\n            para.requires_grad = False\n\nprint(""Loading dataset..."")\n# load training data\ntrain_data = get_loader(json_path, data_dir, mask_dir, inp_size, feat_stride,\n                        preprocess=\'resnet\', batch_size=params.batch_size, training=True,\n                        shuffle=True, num_workers=8, subnet=params.subnet_name)\nprint(\'train dataset len: {}\'.format(len(train_data.dataset)))\n\n# load validation data\nvalid_data = get_loader(json_path, data_dir, mask_dir, inp_size, feat_stride,\n                        preprocess=\'resnet\', batch_size=params.batch_size-10*len(params.gpus), training=False,\n                        shuffle=False, num_workers=8, subnet=params.subnet_name)\nprint(\'val dataset len: {}\'.format(len(valid_data.dataset)))\n\ntrainable_vars = [param for param in model.parameters() if param.requires_grad]\nif opt == \'adam\':\n    print(""Training with adam"")\n    params.optimizer = torch.optim.Adam(\n        trainable_vars, lr=params.init_lr, weight_decay=weight_decay)\n\nparams.lr_scheduler = ReduceLROnPlateau(\n    params.optimizer, \'min\', factor=params.lr_decay, patience=3, verbose=True)\ntrainer = Trainer(model, params, batch_processor, train_data, valid_data)\ntrainer.train()\n'"
training/multipose_keypoint_train.py,8,"b'import os, sys\nroot_path = os.path.realpath(__file__).split(\'/training/multipose_keypoint_train.py\')[0]\nos.chdir(root_path)\nsys.path.append(root_path)\n\nimport torch\nimport torch.utils.model_zoo as model_zoo\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom training.batch_processor import batch_processor\nfrom network.posenet import poseNet\nfrom datasets.coco import get_loader\nfrom training.trainer import Trainer\n\n# Hyper-params\ncoco_root = \'/data/COCO/\'\nbackbone = \'resnet101\'  # \'resnet50\'\nopt = \'adam\'\nweight_decay = 0.000\ninp_size = 480  # input size 480*480\nfeat_stride = 4\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n# model parameters in MultiPoseNet\nfpn_resnet_para = [\'conv1\', \'bn1\', \'layer1\', \'layer2\', \'layer3\', \'layer4\']\nfpn_retinanet_para = [\'conv6\', \'conv7\', \'latlayer1\', \'latlayer2\',\n                      \'latlayer3\', \'toplayer0\', \'toplayer1\', \'toplayer2\']\nfpn_keypoint_para = [\'toplayer\', \'flatlayer1\', \'flatlayer2\',\n                     \'flatlayer3\', \'smooth1\', \'smooth2\', \'smooth3\']\nretinanet_para = [\'regressionModel\', \'classificationModel\']\nkeypoint_para = [\'convt1\', \'convt2\', \'convt3\', \'convt4\', \'convs1\', \'convs2\', \'convs3\', \'convs4\', \'upsample1\',\n                 \'upsample2\', \'upsample3\', \'conv2\', \'convfin\', \'convfin_k2\', \'convfin_k3\', \'convfin_k4\', \'convfin_k5\']\nprn_para = [\'prn\']\n\n#####################################################################\n# train keypoint subnet\ndata_dir = coco_root+\'images/\'\nmask_dir = coco_root\njson_path = coco_root+\'COCO.json\'\n\n# Set Training parameters\nparams = Trainer.TrainParams()\nparams.exp_name = \'res101_keypoint_subnet/\'\nparams.subnet_name = \'keypoint_subnet\'\nparams.save_dir = \'./extra/models/{}\'.format(params.exp_name)\nparams.ckpt = None  # None checkpoint file to load\nparams.ignore_opt_state = False\n\nparams.max_epoch = 80\nparams.init_lr = 1.e-4\nparams.lr_decay = 0.1\n\nparams.gpus = [0]\nparams.batch_size = 6 * len(params.gpus)\nparams.val_nbatch_end_epoch = 2000\n\nparams.print_freq = 50\n\n# model\nif backbone == \'resnet101\':\n    model = poseNet(101)\nelif backbone == \'resnet50\':\n    model = poseNet(50)\n\n# load pretrained\nif params.ckpt is None:\n    model.fpn.load_state_dict(model_zoo.load_url(\n        model_urls[backbone]), strict=False)\n\n# Train Key-point Subnet, Fix the weights in detection subnet (RetinaNet)\nfor name, module in model.fpn.named_children():\n    if name in fpn_retinanet_para:\n        for para in module.parameters():\n            para.requires_grad = False\nfor name, module in model.named_children():\n    if name in retinanet_para:\n        for para in module.parameters():\n            para.requires_grad = False\nfor name, module in model.named_children():\n    if name in prn_para:\n        for para in module.parameters():\n            para.requires_grad = False\n\nprint(""Loading dataset..."")\n# load training data\ntrain_data = get_loader(json_path, data_dir, mask_dir, inp_size, feat_stride,\n                        preprocess=\'resnet\', batch_size=params.batch_size, training=True,\n                        shuffle=True, num_workers=8, subnet=params.subnet_name)\nprint(\'train dataset len: {}\'.format(len(train_data.dataset)))\n\n# load validation data\nvalid_data = None\nif params.val_nbatch > 0:\n    valid_data = get_loader(json_path, data_dir, mask_dir, inp_size, feat_stride,\n                            preprocess=\'resnet\', batch_size=params.batch_size-3*len(params.gpus), training=False,\n                            shuffle=False, num_workers=8, subnet=params.subnet_name)\n    print(\'val dataset len: {}\'.format(len(valid_data.dataset)))\n\ntrainable_vars = [param for param in model.parameters() if param.requires_grad]\nif opt == \'adam\':\n    print(""Training with adam"")\n    params.optimizer = torch.optim.Adam(\n        trainable_vars, lr=params.init_lr, weight_decay=weight_decay)\n\nparams.lr_scheduler = ReduceLROnPlateau(\n    params.optimizer, \'min\', factor=params.lr_decay, patience=3, verbose=True)\ntrainer = Trainer(model, params, batch_processor, train_data, valid_data)\ntrainer.train()\n'"
training/multipose_prn_train.py,4,"b'import os, sys\nroot_path = os.path.realpath(__file__).split(\'/training/multipose_prn_train.py\')[0]\nos.chdir(root_path)\nsys.path.append(root_path)\n\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom pycocotools.coco import COCO\nfrom torch.utils.data import DataLoader\nfrom training.trainer import Trainer\nfrom datasets.coco_data.prn_data_pipeline import PRN_CocoDataset\nfrom network.posenet import poseNet\nfrom training.batch_processor import batch_processor\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# Hyper-params\ncoco_root = \'/data/COCO/\'\nbackbone=\'resnet101\'  # \'resnet50\'\nopt = \'adam\'\ninp_size = 480  # input size 480*480\nfeat_stride = 4\nnode_count = 1024  # Hidden Layer Node Count\ncoeff = 2  # Coefficient of bbox size\nthreshold = 0.21  # BBOX threshold\nnum_of_keypoints = 3  # Minimum number of keypoints for each bbox in training\n\n# model parameters in MultiPoseNet\nprn_para = [\'prn\']\n\n#####################################################################\n# Set Training parameters\nparams = Trainer.TrainParams()\nparams.exp_name = \'prn_subnet/\'\nparams.subnet_name = \'prn_subnet\'\nparams.save_dir = \'./extra/models/{}\'.format(params.exp_name)\nparams.ckpt = \'./demo/models/ckpt_baseline_resnet101.h5\'\nparams.ignore_opt_state = True\n\nparams.max_epoch = 40\nparams.init_lr = 1.0e-3\nparams.lr_decay = 0.9\n\nparams.gpus = [0]\nparams.batch_size = 8 * len(params.gpus)\nparams.val_nbatch_end_epoch = 2000\n\nparams.print_freq = 1000\n\n# model\nif backbone == \'resnet101\':\n    model = poseNet(101, prn_node_count=node_count, prn_coeff=coeff)\nelif backbone == \'resnet50\':\n    model = poseNet(50, prn_node_count=node_count, prn_coeff=coeff)\n\n# Train Key-point Subnet, Fix the weights in detection subnet (RetinaNet)\nfor name, module in model.named_children():\n    if name not in prn_para:\n        for para in module.parameters():\n            para.requires_grad = False\n\nprint(""Loading dataset..."")\n# load training data\ncoco_train = COCO(os.path.join(coco_root, \'annotations/person_keypoints_train2017.json\'))\ntrain_data = DataLoader(dataset=PRN_CocoDataset(\n    coco_train, num_of_keypoints=num_of_keypoints, coeff=coeff, threshold=threshold,\n    inp_size=inp_size, feat_stride=feat_stride),batch_size=params.batch_size, num_workers=4, shuffle=True)\nprint(\'train dataset len: {}\'.format(len(train_data.dataset)))\n\n# load validation data\nvalid_data = None\nif params.val_nbatch > 0:\n    coco_val = COCO(os.path.join(coco_root, \'annotations/person_keypoints_val2017.json\'))\n    valid_data = DataLoader(dataset=PRN_CocoDataset(\n        coco_val, num_of_keypoints=num_of_keypoints, coeff=coeff, threshold=threshold,\n        inp_size=inp_size, feat_stride=feat_stride), batch_size=params.batch_size, num_workers=4, shuffle=True)\n    print(\'val dataset len: {}\'.format(len(valid_data.dataset)))\n\ntrainable_vars = [param for param in model.parameters() if param.requires_grad]\nif opt == \'adam\':\n    print(""Training with adam"")\n    params.optimizer = torch.optim.Adam(\n        trainable_vars, lr=params.init_lr)\n\ncudnn.benchmark = True\nparams.lr_scheduler = ReduceLROnPlateau(params.optimizer, \'min\', factor=params.lr_decay, patience=2, verbose=True)\ntrainer = Trainer(model, params, batch_processor, train_data, valid_data)\ntrainer.train()\n'"
training/trainer.py,3,"b'from __future__ import print_function\n\nimport os\nimport sys\nimport datetime\nimport numpy as np\nfrom collections import OrderedDict\nimport shutil\n\n#import encoding\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, _LRScheduler\nfrom torch.optim.optimizer import Optimizer\n\nfrom lib.utils.log import logger\nfrom lib.utils.timer import Timer\nfrom lib.utils.path import mkdir\nimport lib.utils.meter as meter_utils\nimport network.net_utils as net_utils\nfrom datasets.data_parallel import ListDataParallel\n\n\ndef get_learning_rates(optimizer):\n    lrs = [pg[\'lr\'] for pg in optimizer.param_groups]\n    lrs = np.asarray(lrs, dtype=np.float)\n    return lrs\n\n\ndef default_visualization_fn(writer, step, log_dict):\n    """"""\n    Visualization with tensorboard\n    :type writer: SummaryWriter\n    :type step: int\n    :type log_dict: dict\n    :return:\n    """"""\n    for k, v in log_dict.items():\n        if isinstance(v, (float, int)):\n            writer.add_scalar(k, v, step)\n        elif isinstance(v, np.ndarray):\n            writer.add_image(k, v, step)\n\n\nclass TrainParams(object):\n    # required params\n    exp_name = \'experiment_name\'\n    subnet_name = \'keypoint_subnet\'\n    batch_size = 32\n    max_epoch = 30\n    optimizer = None\n\n    # learning rate scheduler\n    lr_scheduler = None         # should be an instance of ReduceLROnPlateau or _LRScheduler\n    max_grad_norm = np.inf\n\n    # params based on your local env\n    gpus = [0]\n    save_dir = None             # default `save_dir` is `outputs/{exp_name}`\n\n    # loading existing checkpoint\n    ckpt = None                 # path to the ckpt file, will load the last ckpt in the `save_dir` if `None`\n    re_init = False             # ignore ckpt if `True`\n    zero_epoch = False          # force `last_epoch` to zero\n    ignore_opt_state = False    # ignore the saved optimizer states\n\n    # saving checkpoints\n    save_freq_epoch = 1             # save one ckpt per `save_freq_epoch` epochs\n    save_freq_step = sys.maxsize    # save one ckpt per `save_freq_setp` steps, default value is inf\n    save_nckpt_max = sys.maxsize    # max number of saved ckpts\n\n    # validation during training\n    val_freq = 500              # run validation per `val_freq` steps\n    val_nbatch = 10             # number of batches to be validated\n    val_nbatch_end_epoch = 200  # max number of batches to be validated after each epoch\n\n    # visualization\n    print_freq = 20             # print log per `print_freq` steps\n    use_tensorboard = False     # use tensorboardX if True\n    visualization_fn = None     # custom function to handle `log_dict`, default value is `default_visualization_fn`\n\n    def update(self, params_dict):\n        state_dict = self.state_dict()\n        for k, v in params_dict.items():\n            if k in state_dict or hasattr(self, k):\n                setattr(self, k, v)\n            else:\n                logger.warning(\'Unknown option: {}: {}\'.format(k, v))\n\n    def state_dict(self):\n        state_dict = OrderedDict()\n        for k in TrainParams.__dict__.keys():\n            if not k.startswith(\'_\'):\n                state_dict[k] = getattr(self, k)\n        del state_dict[\'update\']\n        del state_dict[\'state_dict\']\n\n        return state_dict\n\n    def __str__(self):\n        state_dict = self.state_dict()\n        text = \'TrainParams {\\n\'\n        for k, v in state_dict.items():\n            text += \'\\t{}: {}\\n\'.format(k, v)\n        text += \'}\\n\'\n        return text\n\n\nclass Trainer(object):\n\n    TrainParams = TrainParams\n\n    # hooks\n    on_start_epoch_hooks = []\n    on_end_epoch_hooks = []\n\n    def __init__(self, model, train_params, batch_processor, train_data, val_data=None):\n        assert isinstance(train_params, TrainParams)\n        self.params = train_params\n\n        # Data loaders\n        self.train_data = train_data\n        self.val_data = val_data # sDataLoader.copy(val_data) if isinstance(val_data, DataLoader) else val_data\n        # self.val_stream = self.val_data.get_stream() if self.val_data else None\n\n        self.batch_processor = batch_processor\n        self.batch_per_epoch = len(self.train_data)\n\n        # set CUDA_VISIBLE_DEVICES=gpus\n        gpus = \',\'.join([str(x) for x in self.params.gpus])\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = gpus\n        self.params.gpus = tuple(range(len(self.params.gpus)))\n        logger.info(\'Set CUDA_VISIBLE_DEVICES to {}...\'.format(gpus))\n\n        # Optimizer and learning rate\n        self.last_epoch = 0\n        self.optimizer = self.params.optimizer  # type: Optimizer\n        if not isinstance(self.optimizer, Optimizer):\n            logger.error(\'optimizer should be an instance of Optimizer, \'\n                         \'but got {}\'.format(type(self.optimizer)))\n            raise ValueError\n        self.lr_scheduler = self.params.lr_scheduler  # type: ReduceLROnPlateau or _LRScheduler\n        if self.lr_scheduler and not isinstance(self.lr_scheduler, (ReduceLROnPlateau, _LRScheduler)):\n            logger.error(\'lr_scheduler should be an instance of _LRScheduler or ReduceLROnPlateau, \'\n                         \'but got {}\'.format(type(self.lr_scheduler)))\n            raise ValueError\n        logger.info(\'Set lr_scheduler to {}\'.format(type(self.lr_scheduler)))\n\n        self.log_values = OrderedDict()\n        self.batch_timer = Timer()\n        self.data_timer = Timer()\n\n        # load model\n        self.model = model\n        ckpt = self.params.ckpt\n        if not self.params.save_dir:\n            self.params.save_dir = os.path.join(\'outputs\', self.params.exp_name)\n        mkdir(self.params.save_dir)\n        logger.info(\'Set output dir to {}\'.format(self.params.save_dir))\n        if ckpt is None:\n            # find the last ckpt\n            ckpts = [fname for fname in os.listdir(self.params.save_dir) if os.path.splitext(fname)[-1] == \'.h5\']\n            ckpt = os.path.join(\n                self.params.save_dir, sorted(ckpts, key=lambda name: int(os.path.splitext(name)[0].split(\'_\')[-1]))[-1]\n            ) if len(ckpts) > 0 else None\n\n        if ckpt is not None and not self.params.re_init:\n           self._load_ckpt(ckpt)\n           logger.info(\'Load ckpt from {}\'.format(ckpt))\n\n        self.model = ListDataParallel(self.model, device_ids=self.params.gpus)\n        self.model = self.model.cuda(self.params.gpus[0])\n        self.model.train()\n        if self.params.subnet_name != \'keypoint_subnet\':\n            self.model.module.freeze_bn()  # nn.BatchNorm2d.eval() if not \'keypoint_subnet\'\n\n    def train(self):\n        best_loss = np.inf\n        for epoch in range(self.last_epoch, self.params.max_epoch):\n            self.last_epoch += 1\n            logger.info(\'Start training epoch {}\'.format(self.last_epoch))\n\n            for fun in self.on_start_epoch_hooks:\n                fun(self)\n\n            # adjust learning rate\n            if isinstance(self.lr_scheduler, _LRScheduler):\n                cur_lrs = get_learning_rates(self.optimizer)\n                self.lr_scheduler.step(self.last_epoch)\n                logger.info(\'Set learning rates from {} to {}\'.format(cur_lrs, get_learning_rates(self.optimizer)))\n\n            train_loss = self._train_one_epoch()\n\n            for fun in self.on_end_epoch_hooks:\n                fun(self)\n\n            # save model\n            if (self.last_epoch % self.params.save_freq_epoch == 0) or (self.last_epoch == self.params.max_epoch - 1):\n                save_name = \'ckpt_{}.h5\'.format(self.last_epoch)\n                save_to = os.path.join(self.params.save_dir, save_name)\n                self._save_ckpt(save_to)\n\n                # find best model\n                if self.params.val_nbatch_end_epoch > 0:\n                    val_loss = self._val_one_epoch(self.params.val_nbatch_end_epoch)\n                    if val_loss < best_loss:\n                        best_file = os.path.join(self.params.save_dir,\n                                                 \'ckpt_{}_{:.5f}.h5.best\'.format(self.last_epoch, val_loss))\n                        shutil.copyfile(save_to, best_file)\n                        logger.info(\'Found a better ckpt ({:.5f} -> {:.5f}), \'\n                                    \'saved to {}\'.format(best_loss, val_loss, best_file))\n                        best_loss = val_loss\n\n                    if isinstance(self.lr_scheduler, ReduceLROnPlateau):\n                        self.lr_scheduler.step(val_loss, self.last_epoch)\n\n    def _save_ckpt(self, save_to):\n        model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model\n        net_utils.save_net(save_to, model, epoch=self.last_epoch,\n                           optimizers=[self.optimizer], rm_prev_opt=True, max_n_ckpts=self.params.save_nckpt_max)\n        logger.info(\'Save ckpt to {}\'.format(save_to))\n\n    def _load_ckpt(self, ckpt):\n        epoch, state_dicts = net_utils.load_net(ckpt, self.model, load_state_dict=True)\n        if not self.params.ignore_opt_state and not self.params.zero_epoch and epoch >= 0:\n            self.last_epoch = epoch\n            logger.info(\'Set last epoch to {}\'.format(self.last_epoch))\n            if state_dicts is not None:\n                self.optimizer.load_state_dict(state_dicts[0])\n                net_utils.set_optimizer_state_devices(self.optimizer.state, self.params.gpus[0])\n                logger.info(\'Load optimizer state from checkpoint, \'\n                            \'new learning rate: {}\'.format(get_learning_rates(self.optimizer)))\n\n    def _train_one_epoch(self):\n        self.batch_timer.clear()\n        self.data_timer.clear()\n        self.batch_timer.tic()\n        self.data_timer.tic()\n        total_loss = meter_utils.AverageValueMeter()\n        for step, batch in enumerate(self.train_data):\n            inputs, gts, _ = self.batch_processor(self, batch)\n\n            self.data_timer.toc()\n\n            # forward\n            output, saved_for_loss = self.model(*inputs)\n\n            loss, saved_for_log = self.model.module.build_loss(saved_for_loss, *gts)\n\n            # backward\n            self.optimizer.zero_grad()\n            loss.backward()\n            total_loss.add(loss.item())\n\n            # clip grad\n            if not np.isinf(self.params.max_grad_norm):\n                max_norm = nn.utils.clip_grad_norm(self.model.parameters(), self.params.max_grad_norm, float(\'inf\'))\n                saved_for_log[\'max_grad\'] = max_norm\n\n            self.optimizer.step(None)\n\n            self._process_log(saved_for_log, self.log_values)\n            self.batch_timer.toc()\n\n            # print log\n            reset = False\n\n            if step % self.params.print_freq == 0:\n                self._print_log(step, self.log_values, title=\'Training\', max_n_batch=self.batch_per_epoch)\n                reset = True\n\n            if step % self.params.save_freq_step == 0 and step > 0:\n                save_to = os.path.join(self.params.save_dir,\n                                       \'ckpt_{}.h5.ckpt\'.format((self.last_epoch - 1) * self.batch_per_epoch + step))\n                self._save_ckpt(save_to)\n\n            if reset:\n                self._reset_log(self.log_values)\n\n            self.data_timer.tic()\n            self.batch_timer.tic()\n\n        total_loss, std = total_loss.value()\n        return total_loss\n\n    def _val_one_epoch(self, n_batch):\n        training_mode = self.model.training\n        self.model.eval()\n        logs = OrderedDict()\n        sum_loss = meter_utils.AverageValueMeter()\n        logger.info(\'Val on validation set...\')\n\n        self.batch_timer.clear()\n        self.data_timer.clear()\n        self.batch_timer.tic()\n        self.data_timer.tic()\n        for step, batch in enumerate(self.val_data):\n            self.data_timer.toc()\n            if step > n_batch:\n                break\n\n            inputs, gts, _ = self.batch_processor(self, batch)\n            _, saved_for_loss = self.model(*inputs)\n            self.batch_timer.toc()\n\n            loss, saved_for_log = self.model.module.build_loss(saved_for_loss, *gts)\n            sum_loss.add(loss.item())\n            self._process_log(saved_for_log, logs)\n\n            if step % self.params.print_freq == 0 or step == len(self.val_data)-1:\n                self._print_log(step, logs, \'Validation\', max_n_batch=min(n_batch, len(self.val_data)))\n\n            self.data_timer.tic()\n            self.batch_timer.tic()\n\n        mean, std = sum_loss.value()\n        logger.info(\'Validation loss: mean: {}, std: {}\'.format(mean, std))\n        self.model.train(mode=training_mode)\n        if self.params.subnet_name != \'keypoint_subnet\':\n            self.model.module.freeze_bn()\n        return mean\n\n    def _process_log(self, src_dict, dest_dict):\n        for k, v in src_dict.items():\n            if isinstance(v, (int, float)):\n                dest_dict.setdefault(k, meter_utils.AverageValueMeter())\n                dest_dict[k].add(float(v))\n            else:\n                dest_dict[k] = v\n\n    def _print_log(self, step, log_values, title=\'\', max_n_batch=None):\n        log_str = \'{}\\n\'.format(self.params.exp_name)\n        log_str += \'{}: epoch {}\'.format(title, self.last_epoch)\n\n        if max_n_batch:\n            log_str += \'[{}/{}], lr: {}\'.format(step, max_n_batch, get_learning_rates(self.optimizer))\n\n        i = 0\n        # global_step = step + (self.last_epoch - 1) * self.batch_per_epoch\n        for k, v in log_values.items():\n            if isinstance(v, meter_utils.AverageValueMeter):\n                mean, std = v.value()\n                log_str += \'\\n\\t{}: {:.10f}\'.format(k, mean)\n                i += 1\n\n        if max_n_batch:\n            # print time\n            data_time = self.data_timer.duration + 1e-6\n            batch_time = self.batch_timer.duration + 1e-6\n            rest_seconds = int((max_n_batch - step) * batch_time)\n            log_str += \'\\n\\t({:.2f}/{:.2f}s,\' \\\n                       \' fps:{:.1f}, rest: {})\'.format(data_time, batch_time,\n                                                       self.params.batch_size / batch_time,\n                                                       str(datetime.timedelta(seconds=rest_seconds)))\n            self.batch_timer.clear()\n            self.data_timer.clear()\n\n        logger.info(log_str)\n\n    def _reset_log(self, log_values):\n        for k, v in log_values.items():\n            if isinstance(v, meter_utils.AverageValueMeter):\n                v.reset()\n'"
datasets/coco_data/COCO_data_pipeline.py,9,"b'# coding=utf-8\nimport os\n\nimport cv2\nimport numpy as np\n\nimport torch\nfrom datasets.coco_data.heatmap import putGaussianMaps\nfrom datasets.coco_data.ImageAugmentation import (aug_croppad, aug_flip, aug_rotate, aug_scale,\n                                                  aug_croppad_bbox, aug_flip_bbox, aug_rotate_bbox, aug_scale_bbox)\nfrom datasets.coco_data.preprocessing import resnet_preprocess\nfrom torch.utils.data import DataLoader, Dataset\nfrom functools import partial, reduce\n\nfrom pycocotools.coco import COCO, maskUtils\n\n\'\'\'\ntrain2014  : 82783 simages\nval2014    : 40504 images\n\nfirst 2644 of val2014 marked by \'isValidation = 1\', as our minval dataset.\nSo all training data have 82783+40504-2644 = 120643 samples\n\'\'\'\n\nparams_transform = dict()\nparams_transform[\'mode\'] = 5\n# === aug_scale ===\nparams_transform[\'scale_min\'] = 0.8\nparams_transform[\'scale_max\'] = 1.2\nparams_transform[\'scale_prob\'] = 1\nparams_transform[\'target_dist\'] = 0.6\n# === aug_rotate ===\nparams_transform[\'max_rotate_degree\'] = 40\n\n# ===\nparams_transform[\'center_perterb_max\'] = 40\n\n# === aug_flip ===\nparams_transform[\'flip_prob\'] = 0.3\n\nparams_transform[\'np\'] = 56\nparams_transform[\'sigma\'] = 7.0\n\ndef annToRLE(ann, height, width):\n    """"""\n    Convert annotation which can be polygons, uncompressed RLE to RLE.\n    :return: binary mask (numpy 2D array)\n    """"""\n    segm = ann[\'segmentation\']\n    if isinstance(segm, list):\n        # polygon -- a single object might consist of multiple parts\n        # we merge all parts into one mask rle code\n        rles = maskUtils.frPyObjects(segm, height, width)\n        rle = maskUtils.merge(rles)\n    elif isinstance(segm[\'counts\'], list):\n        # uncompressed RLE\n        rle = maskUtils.frPyObjects(segm, height, width)\n    else:\n        # rle\n        rle = ann[\'segmentation\']\n    return rle\n\n\ndef annToMask(ann, height, width):\n    """"""\n    Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n    :return: binary mask (numpy 2D array)\n    """"""\n    rle = annToRLE(ann, height, width)\n    m = maskUtils.decode(rle)\n    return m\n\nclass Cocokeypoints(Dataset):\n    def __init__(self, root, mask_dir, index_list, data, inp_size, feat_stride, preprocess=\'resnet\', transform=None,\n                 target_transform=None):\n\n        params_transform[\'crop_size_x\'] = inp_size\n        params_transform[\'crop_size_y\'] = inp_size\n        params_transform[\'stride\'] = feat_stride\n\n        # add preprocessing as a choice, so we don\'t modify it manually.\n        self.preprocess = preprocess\n        self.data = data\n        self.mask_dir = mask_dir\n        self.numSample = len(index_list)\n        self.index_list = index_list\n        self.root = root\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def get_anno(self, meta_data):\n        """"""\n        get meta information\n        """"""\n        anno = dict()\n        anno[\'dataset\'] = meta_data[\'dataset\']\n        anno[\'img_height\'] = int(meta_data[\'img_height\'])\n        anno[\'img_width\'] = int(meta_data[\'img_width\'])\n\n        anno[\'isValidation\'] = meta_data[\'isValidation\']\n        anno[\'people_index\'] = int(meta_data[\'people_index\'])\n        anno[\'annolist_index\'] = int(meta_data[\'annolist_index\'])\n\n        # (b) objpos_x (float), objpos_y (float)\n        anno[\'objpos\'] = np.array(meta_data[\'objpos\'])\n        anno[\'scale_provided\'] = meta_data[\'scale_provided\']\n        anno[\'joint_self\'] = np.array(meta_data[\'joint_self\'])\n\n        anno[\'numOtherPeople\'] = int(meta_data[\'numOtherPeople\'])\n        anno[\'num_keypoints_other\'] = np.array(\n            meta_data[\'num_keypoints_other\'])\n        anno[\'joint_others\'] = np.array(meta_data[\'joint_others\'])\n        anno[\'objpos_other\'] = np.array(meta_data[\'objpos_other\'])\n        anno[\'scale_provided_other\'] = meta_data[\'scale_provided_other\']\n        anno[\'bbox_other\'] = meta_data[\'bbox_other\']\n        anno[\'segment_area_other\'] = meta_data[\'segment_area_other\']\n\n        if anno[\'numOtherPeople\'] == 1:\n            anno[\'joint_others\'] = np.expand_dims(anno[\'joint_others\'], 0)\n            anno[\'objpos_other\'] = np.expand_dims(anno[\'objpos_other\'], 0)\n        return anno\n\n    def add_neck(self, meta):\n        \'\'\'\n        MS COCO annotation order:\n        0: nose\t   \t\t1: l eye\t\t2: r eye\t3: l ear\t4: r ear\n        5: l shoulder\t6: r shoulder\t7: l elbow\t8: r elbow\n        9: l wrist\t\t10: r wrist\t\t11: l hip\t12: r hip\t13: l knee\n        14: r knee\t\t15: l ankle\t\t16: r ankle\n\n        The order in this work:\n        (0-\'nose\'\t1-\'neck\' 2-\'right_shoulder\' 3-\'right_elbow\' 4-\'right_wrist\'\n        5-\'left_shoulder\' 6-\'left_elbow\'\t    7-\'left_wrist\'  8-\'right_hip\'\n        9-\'right_knee\'\t 10-\'right_ankle\'\t11-\'left_hip\'   12-\'left_knee\'\n        13-\'left_ankle\'\t 14-\'right_eye\'\t    15-\'left_eye\'   16-\'right_ear\'\n        17-\'left_ear\' )\n        \'\'\'\n        our_order = [0, 17, 6, 8, 10, 5, 7, 9,\n                     12, 14, 16, 11, 13, 15, 2, 1, 4, 3]\n        # Index 6 is right shoulder and Index 5 is left shoulder\n        right_shoulder = meta[\'joint_self\'][6, :]\n        left_shoulder = meta[\'joint_self\'][5, :]\n        neck = (right_shoulder + left_shoulder) / 2\n        if right_shoulder[2] == 2 or left_shoulder[2] == 2:\n            neck[2] = 2\n        elif right_shoulder[2] == 1 or left_shoulder[2] == 1:\n            neck[2] = 1\n        else:\n            neck[2] = right_shoulder[2] * left_shoulder[2]\n\n        neck = neck.reshape(1, len(neck))\n        neck = np.round(neck)\n        meta[\'joint_self\'] = np.vstack((meta[\'joint_self\'], neck))\n        meta[\'joint_self\'] = meta[\'joint_self\'][our_order, :]\n        temp = []\n\n        for i in range(meta[\'numOtherPeople\']):\n            right_shoulder = meta[\'joint_others\'][i, 6, :]\n            left_shoulder = meta[\'joint_others\'][i, 5, :]\n            neck = (right_shoulder + left_shoulder) / 2\n            if (right_shoulder[2] == 2 or left_shoulder[2] == 2):\n                neck[2] = 2\n            elif (right_shoulder[2] == 1 or left_shoulder[2] == 1):\n                neck[2] = 1\n            else:\n                neck[2] = right_shoulder[2] * left_shoulder[2]\n            neck = neck.reshape(1, len(neck))\n            neck = np.round(neck)\n            single_p = np.vstack((meta[\'joint_others\'][i], neck))\n            single_p = single_p[our_order, :]\n            temp.append(single_p)\n        meta[\'joint_others\'] = np.array(temp)\n\n        return meta\n\n    def remove_illegal_joint(self, meta):\n        crop_x = int(params_transform[\'crop_size_x\'])\n        crop_y = int(params_transform[\'crop_size_y\'])\n        mask = np.logical_or.reduce((meta[\'joint_self\'][:, 0] >= crop_x,\n                                     meta[\'joint_self\'][:, 0] < 0,\n                                     meta[\'joint_self\'][:, 1] >= crop_y,\n                                     meta[\'joint_self\'][:, 1] < 0))\n        # out_bound = np.nonzero(mask)\n        # print(mask.shape)\n        meta[\'joint_self\'][mask == True, :] = (1, 1, 2)\n        if (meta[\'numOtherPeople\'] != 0):\n            mask = np.logical_or.reduce((meta[\'joint_others\'][:, :, 0] >= crop_x,\n                                         meta[\'joint_others\'][:, :, 0] < 0,\n                                         meta[\'joint_others\'][:,\n                                                              :, 1] >= crop_y,\n                                         meta[\'joint_others\'][:, :, 1] < 0))\n            meta[\'joint_others\'][mask == True, :] = (1, 1, 2)\n\n        return meta\n\n    def get_ground_truth(self, meta, mask_miss):\n\n        number_keypoints = 18\n\n        stride = params_transform[\'stride\']\n        mode = params_transform[\'mode\']\n        crop_size_y = params_transform[\'crop_size_y\']\n        crop_size_x = params_transform[\'crop_size_x\']\n        num_parts = params_transform[\'np\']\n        nop = meta[\'numOtherPeople\']\n        grid_y = int(crop_size_y / stride)\n        grid_x = int(crop_size_x / stride)\n        channels = (num_parts + 1) * 2\n        heatmaps = np.zeros((grid_y, grid_x, number_keypoints))\n\n        mask_miss = cv2.resize(mask_miss, (0, 0), fx=1.0 / stride, fy=1.0 / stride,\n                               interpolation=cv2.INTER_CUBIC).astype(np.float32)\n        mask_miss = mask_miss / 255.\n        mask_miss = np.expand_dims(mask_miss, axis=2)\n        heat_mask = np.repeat(mask_miss, number_keypoints, axis=2)  # 19\n\n        #mask_all = cv2.resize(mask_all, (0, 0), fx=1.0 / stride, fy=1.0 / stride,\n        #                      interpolation=cv2.INTER_CUBIC).astype(np.float32)\n        #mask_all = mask_all / 255.\n        #mask_all = np.expand_dims(mask_all, axis=2)\n\n        # confidance maps for body parts\n        for i in range(number_keypoints):\n            if (meta[\'joint_self\'][i, 2] <= 1):\n                center = meta[\'joint_self\'][i, :2]\n                gaussian_map = heatmaps[:, :, i]\n                heatmaps[:, :, i] = putGaussianMaps(\n                    center, gaussian_map, params_transform=params_transform)\n            for j in range(nop):\n                if (meta[\'joint_others\'][j, i, 2] <= 1):\n                    center = meta[\'joint_others\'][j, i, :2]\n                    gaussian_map = heatmaps[:, :, i]\n                    heatmaps[:, :, i] = putGaussianMaps(\n                        center, gaussian_map, params_transform=params_transform)\n\n        return heat_mask, heatmaps\n\n    def __getitem__(self, index):\n        idx = self.index_list[index]\n        img = cv2.imread(os.path.join(self.root, self.data[idx][\'img_paths\']))\n        img_idx = self.data[idx][\'img_paths\'][-16:-3]\n#        print img.shape\n        if ""COCO_val"" in self.data[idx][\'dataset\']:\n            mask_miss = cv2.imread(\n                self.mask_dir + \'mask2014/val2014_mask_miss_\' + img_idx + \'png\', 0)\n            #mask_all = cv2.imread(\n            #    self.mask_dir + \'mask2014/val2014_mask_all_\' + img_idx + \'png\', 0)\n        elif ""COCO"" in self.data[idx][\'dataset\']:\n            mask_miss = cv2.imread(\n                self.mask_dir + \'mask2014/train2014_mask_miss_\' + img_idx + \'png\', 0)\n            #mask_all = cv2.imread(\n            #    self.mask_dir + \'mask2014/train2014_mask_all_\' + img_idx + \'png\', 0)\n        meta_data = self.get_anno(self.data[idx])\n\n        meta_data = self.add_neck(meta_data)\n\n        augmentations = [\n            partial(aug_meth, params_transform=params_transform)\n            for aug_meth in [\n                aug_scale,\n                aug_rotate,\n                aug_croppad,\n                aug_flip\n            ]\n        ]\n\n        meta_data, img, mask_miss = reduce(\n            lambda md_i_mm_ma, f: f(*md_i_mm_ma),\n            augmentations,\n            (meta_data, img, mask_miss)\n        )\n\n        meta_data = self.remove_illegal_joint(meta_data)\n\n        heat_mask, heatmaps = self.get_ground_truth(\n            meta_data, mask_miss)\n\n        # image preprocessing, which comply the model\n        # trianed on Imagenet dataset\n        if self.preprocess == \'resnet\':\n            img = resnet_preprocess(img)\n\n        img = torch.from_numpy(img)\n        heatmaps = torch.from_numpy(\n            heatmaps.transpose((2, 0, 1)).astype(np.float32))\n        heat_mask = torch.from_numpy(\n            heat_mask.transpose((2, 0, 1)).astype(np.float32))\n        #mask_all = torch.from_numpy(\n        #    mask_all.transpose((2, 0, 1)).astype(np.float32))\n\n        return img, heatmaps, heat_mask#, mask_all\n\n    def __len__(self):\n        return self.numSample\n\nclass Cocobbox(Dataset):\n    def __init__(self, root, mask_dir, index_list, data, inp_size, feat_stride, coco,\n                 preprocess=\'resnet\', training=True):\n\n        params_transform[\'crop_size_x\'] = inp_size\n        params_transform[\'crop_size_y\'] = inp_size\n        params_transform[\'stride\'] = feat_stride\n\n        # add preprocessing as a choice, so we don\'t modify it manually.\n        self.preprocess = preprocess\n        self.data = data\n        self.index_list = index_list\n        self.numSample = len(self.index_list)\n        self.training = training\n\n        if self.training:\n            img_path = os.path.join(root, \'train2017\')\n        else:\n            img_path = os.path.join(root, \'val2017\')\n\n        self.instance_info_list, self.image_path_list = self.get_instance_info_list(img_path, coco)\n\n    def get_instance_info_list(self, img_path, coco):\n\n        instance_info_list = []\n        image_path_list = []\n\n        for idx in self.index_list:\n            image_info = coco.loadImgs(int(self.data[idx][\'image_id\']))[0]\n            image_path = os.path.join(img_path, image_info[\'file_name\'])\n            if not os.path.exists(image_path):\n                print(\n                    ""[skip] json annotation found, but cannot found image: {}"".format(image_path))\n                continue\n            image_path_list.append(image_path)\n\n            annos_ids = coco.getAnnIds(imgIds=self.data[idx][\'image_id\'])\n            annos_info = coco.loadAnns(annos_ids)\n            instance_info = {}\n            instance_info[""anns""] = annos_info\n            instance_info[""height""] = image_info[""height""]\n            instance_info[""width""] = image_info[""width""]\n            instance_info_list.append(instance_info)\n\n        return instance_info_list, image_path_list\n\n    def get_instance_mask(self, instance_info):\n        height = instance_info[\'height\']\n        width = instance_info[\'width\']\n        anns = instance_info[\'anns\']\n\n        instance_masks = []\n        class_ids = []\n        for anno in anns:\n            class_id = 1\n            m = annToMask(anno, height, width)\n            # Some objects are so small that they\'re less than 1 pixel area\n            # and end up rounded out. Skip those objects.\n            if m.max() < 1:\n                continue\n            # Is it a crowd? If so, use a negative class ID.\n            if anno[\'iscrowd\']:\n                # Use negative class ID for crowds\n                class_id = -1\n                # For crowd masks, annToMask() sometimes returns a mask\n                # smaller than the given dimensions. If so, resize it.\n                if m.shape[0] != height or m.shape[1] != width:\n                    m = np.ones([height, width], dtype=bool)\n            instance_masks.append(m)\n            class_ids.append(class_id)\n        return instance_masks, class_ids\n\n    def get_anno(self, meta_data, instance_info):\n        """"""\n        get meta information\n        """"""\n        anno = dict()\n\n        # (b) objpos_x (float), objpos_y (float)\n        anno[\'objpos\'] = np.array(meta_data[\'objpos\'])\n        anno[\'scale_provided\'] = meta_data[\'scale_provided\']\n\n        anno[\'instance_mask_list\'], anno[\'instance_cls_list\'] = self.get_instance_mask(instance_info)\n\n        return anno\n\n    def get_ground_truth(self, meta, instance_info):\n        extracted_bbox = []\n\n        for m_idx, m in enumerate(meta[\'instance_mask_list\']):\n            if meta[\'instance_cls_list\'][m_idx] == -1:  # is_crowd = 1\n                if instance_info[\'anns\'][m_idx][\'iscrowd\'] != 1:\n                    print(\'is_crowd error\')\n                continue\n            horizontal_indicies = np.where(np.any(m, axis=0))[0]\n            vertical_indicies = np.where(np.any(m, axis=1))[0]\n            if horizontal_indicies.shape[0]:\n                x1, x2 = horizontal_indicies[[0, -1]]\n                y1, y2 = vertical_indicies[[0, -1]]\n                # x2 and y2 should not be part of the box. Increment by 1.\n                x2 += 1\n                y2 += 1\n                bbox_cls = 0\n            else:\n                # No mask for this instance. Might happen due to\n                # resizing or cropping. Set bbox to zeros\n                x1, x2, y1, y2, bbox_cls = -1, -1, -1, -1, -1\n            extracted_bbox.append([x1, y1, x2, y2, bbox_cls])\n\n        return extracted_bbox\n\n    def __getitem__(self, index):\n        img = cv2.imread(self.image_path_list[index])\n\n        idx = self.index_list[index]\n        meta_data = self.get_anno(self.data[idx], self.instance_info_list[index])\n\n        augmentations = [\n            partial(aug_meth, params_transform=params_transform)\n            for aug_meth in [\n                aug_scale_bbox,\n                aug_rotate_bbox,\n                aug_croppad_bbox,\n                aug_flip_bbox\n            ]\n        ]\n\n        meta_data, img = reduce(\n            lambda md_i_mm_ma, f: f(*md_i_mm_ma),\n            augmentations,\n            (meta_data, img)\n        )\n\n        extracted_bbox = self.get_ground_truth(meta_data, self.instance_info_list[index])\n\n        # image preprocessing, which comply the model\n        # trianed on Imagenet dataset\n        if self.preprocess == \'resnet\':\n            img = resnet_preprocess(img)\n\n        img = torch.from_numpy(img)\n        bbox = torch.from_numpy(np.array(extracted_bbox).astype(np.float32))\n\n        return img, bbox\n\n    def __len__(self):\n        return self.numSample\n\ndef bbox_collater(data):\n    imgs = torch.stack([s[0] for s in data], 0)\n    bbox = [s[1] for s in data]\n\n    max_num_annots = max(bb.shape[0] for bb in bbox)\n\n    bbox_padded = torch.ones((len(bbox), max_num_annots, 5)) * -1\n    #print(annot_padded.shape)\n    if max_num_annots > 0:\n        for idx, bb in enumerate(bbox):\n            #print(annot.shape)\n            if bb.shape[0] > 0:\n                bbox_padded[idx, :bb.shape[0], :] = bb\n\n    return imgs, bbox_padded'"
datasets/coco_data/ImageAugmentation.py,0,"b'# coding=utf-8\n\nimport random\nimport sys\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import misc, ndimage\n\n\n""""""The purpose of Augmentor is to automate image augmentation \n   in order to expand datasets as input for our algorithms.\n:aut_scale : Scales them by dice2 (<1, so it is zoom out). \n:aug_croppad centerB: int with shape (2,), centerB will point to centerA.\n:aug_flip: Mirrors the image around a vertical line running through its center.\n:aug_rotate: Rotates the image. The angle of rotation, in degrees, \n             is specified by a random integer value that is included\n             in the transform argument.\n             \n:param params_transform: store the value of stride and crop_szie_y, crop_size_x                 \n""""""\n\n\ndef aug_scale(meta, img, mask_miss, params_transform):\n    dice = random.random()  # (0,1)\n    if (dice > params_transform[\'scale_prob\']):\n\n        scale_multiplier = 1\n    else:\n        dice2 = random.random()\n        # linear shear into [scale_min, scale_max]\n        scale_multiplier = (\n            params_transform[\'scale_max\'] - params_transform[\'scale_min\']) * dice2 + \\\n            params_transform[\'scale_min\']\n    scale_abs = params_transform[\'target_dist\'] / meta[\'scale_provided\']\n    scale = scale_abs * scale_multiplier\n    img = cv2.resize(img, (0, 0), fx=scale, fy=scale,\n                     interpolation=cv2.INTER_CUBIC)\n\n    mask_miss = cv2.resize(mask_miss, (0, 0), fx=scale,\n                           fy=scale, interpolation=cv2.INTER_CUBIC)\n    #mask_all = cv2.resize(mask_all, (0, 0), fx=scale,\n    #                      fy=scale, interpolation=cv2.INTER_CUBIC)\n\n    # modify meta data\n    meta[\'objpos\'] *= scale\n    meta[\'joint_self\'][:, :2] *= scale\n    if (meta[\'numOtherPeople\'] != 0):\n        meta[\'objpos_other\'] *= scale\n        meta[\'joint_others\'][:, :, :2] *= scale\n    return meta, img, mask_miss#, mask_all\n\n\ndef aug_croppad(meta, img, mask_miss, params_transform):\n    dice_x = random.random()\n    dice_y = random.random()\n    crop_x = int(params_transform[\'crop_size_x\'])\n    crop_y = int(params_transform[\'crop_size_y\'])\n    x_offset = int((dice_x - 0.5) * 2 *\n                   params_transform[\'center_perterb_max\'])\n    y_offset = int((dice_y - 0.5) * 2 *\n                   params_transform[\'center_perterb_max\'])\n\n    center = meta[\'objpos\'] + np.array([x_offset, y_offset])\n    center = center.astype(int)\n\n    # pad up and down\n    pad_v = np.ones((crop_y, img.shape[1], 3), dtype=np.uint8) * 128\n    pad_v_mask_miss = np.ones(\n        (crop_y, mask_miss.shape[1]), dtype=np.uint8) * 255\n\n    img = np.concatenate((pad_v, img, pad_v), axis=0)\n    mask_miss = np.concatenate(\n        (pad_v_mask_miss, mask_miss, pad_v_mask_miss), axis=0)\n    #mask_all = np.concatenate(\n    #    (pad_v_mask_miss, mask_all, pad_v_mask_miss), axis=0)\n\n    # pad right and left\n    pad_h = np.ones((img.shape[0], crop_x, 3), dtype=np.uint8) * 128\n    pad_h_mask_miss = np.ones(\n        (mask_miss.shape[0], crop_x), dtype=np.uint8) * 255\n\n    img = np.concatenate((pad_h, img, pad_h), axis=1)\n    mask_miss = np.concatenate(\n        (pad_h_mask_miss, mask_miss, pad_h_mask_miss), axis=1)\n    #mask_all = np.concatenate(\n    #    (pad_h_mask_miss, mask_all, pad_h_mask_miss), axis=1)\n\n    img = img[center[1] + int(crop_y / 2):center[1] + int(crop_y / 2) + crop_y,\n              center[0] + int(crop_x / 2):center[0] + int(crop_x / 2) + crop_x, :]\n\n    mask_miss = mask_miss[center[1] + int(crop_y / 2):center[1] + int(crop_y / 2) + crop_y + 1, center[0] +int(crop_x / 2):center[0] + int(crop_x / 2) + crop_x + 1]\n    #mask_all = mask_all[center[1] + int(crop_y / 2):center[1] + int(crop_y / 2) + crop_y + 1, center[0] + int(crop_x / 2):center[0] + int(crop_x / 2) + crop_x + 1]\n\n    offset_left = crop_x / 2 - center[0]\n    offset_up = crop_y / 2 - center[1]\n\n    offset = np.array([offset_left, offset_up])\n    meta[\'objpos\'] += offset\n    meta[\'joint_self\'][:, :2] += offset\n    mask = np.logical_or.reduce((meta[\'joint_self\'][:, 0] >= crop_x,\n                                 meta[\'joint_self\'][:, 0] < 0,\n                                 meta[\'joint_self\'][:, 1] >= crop_y,\n                                 meta[\'joint_self\'][:, 1] < 0))\n\n    meta[\'joint_self\'][mask == True, 2] = 2\n    if (meta[\'numOtherPeople\'] != 0):\n        meta[\'objpos_other\'] += offset\n        meta[\'joint_others\'][:, :, :2] += offset\n        mask = np.logical_or.reduce((meta[\'joint_others\'][:, :, 0] >= crop_x,\n                                     meta[\'joint_others\'][:, :, 0] < 0,\n                                     meta[\'joint_others\'][:, :, 1] >= crop_y,\n                                     meta[\'joint_others\'][:, :, 1] < 0))\n\n        meta[\'joint_others\'][mask == True, 2] = 2\n\n    return meta, img, mask_miss#, mask_all\n\n\ndef aug_flip(meta, img, mask_miss, params_transform):\n    mode = params_transform[\'mode\']\n    num_other_people = meta[\'numOtherPeople\']\n    dice = random.random()\n    doflip = dice <= params_transform[\'flip_prob\']\n\n    if doflip:\n        img = img.copy()\n        cv2.flip(src=img, flipCode=1, dst=img)\n        w = img.shape[1]\n\n        mask_miss = mask_miss.copy()\n        #mask_all = mask_all.copy()\n        cv2.flip(src=mask_miss, flipCode=1, dst=mask_miss)\n        #cv2.flip(src=mask_all, flipCode=1, dst=mask_all)\n\n        \'\'\'\n        The order in this work:\n            (0-\'nose\'   1-\'neck\' 2-\'right_shoulder\' 3-\'right_elbow\' 4-\'right_wrist\'\n            5-\'left_shoulder\' 6-\'left_elbow\'        7-\'left_wrist\'  8-\'right_hip\'  \n            9-\'right_knee\'   10-\'right_ankle\'   11-\'left_hip\'   12-\'left_knee\' \n            13-\'left_ankle\'  14-\'right_eye\'     15-\'left_eye\'   16-\'right_ear\' \n            17-\'left_ear\' )\n        \'\'\'\n        meta[\'objpos\'][0] = w - 1 - meta[\'objpos\'][0]\n        meta[\'joint_self\'][:, 0] = w - 1 - meta[\'joint_self\'][:, 0]\n        # print meta[\'joint_self\']\n        meta[\'joint_self\'] = meta[\'joint_self\'][[0, 1, 5, 6,\n                                                 7, 2, 3, 4, 11, 12, 13, 8, 9, 10, 15, 14, 17, 16]]\n        if (num_other_people != 0):\n            meta[\'objpos_other\'][:, 0] = w - 1 - meta[\'objpos_other\'][:, 0]\n            meta[\'joint_others\'][:, :, 0] = w - \\\n                1 - meta[\'joint_others\'][:, :, 0]\n            for i in range(num_other_people):\n                meta[\'joint_others\'][i] = meta[\'joint_others\'][i][[\n                    0, 1, 5, 6, 7, 2, 3, 4, 11, 12, 13, 8, 9, 10, 15, 14, 17, 16]]\n\n    return meta, img, mask_miss#, mask_all\n\n\ndef rotatepoint(p, R):\n    point = np.zeros((3, 1))\n    point[0] = p[0]\n    point[1] = p[1]\n    point[2] = 1\n\n    new_point = R.dot(point)\n\n    p[0] = new_point[0]\n\n    p[1] = new_point[1]\n    return p\n\n\n# The correct way to rotation an image\n# http://www.pyimagesearch.com/2017/01/02/rotate-images-correctly-with-opencv-and-python/\n\n\ndef rotate_bound(image, angle, bordervalue):\n    # grab the dimensions of the image and then determine the\n    # center\n    (h, w) = image.shape[:2]\n    (cX, cY) = (w // 2, h // 2)\n\n    # grab the rotation matrix (applying the negative of the\n    # angle to rotate clockwise), then grab the sine and cosine\n    # (i.e., the rotation components of the matrix)\n    M = cv2.getRotationMatrix2D((cX, cY), -angle, 1.0)\n    cos = np.abs(M[0, 0])\n    sin = np.abs(M[0, 1])\n\n    # compute the new bounding dimensions of the image\n    nW = int((h * sin) + (w * cos))\n    nH = int((h * cos) + (w * sin))\n\n    # adjust the rotation matrix to take into account translation\n    M[0, 2] += (nW / 2) - cX\n    M[1, 2] += (nH / 2) - cY\n\n    # perform the actual rotation and return the image\n    return cv2.warpAffine(image, M, (nW, nH), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_CONSTANT,\n                          borderValue=bordervalue), M\n\n\ndef aug_rotate(meta, img, mask_miss, params_transform, type=""random"", input=0, fillType=""nearest"", constant=0):\n    dice = random.random()\n    degree = (dice - 0.5) * 2 * \\\n        params_transform[\'max_rotate_degree\']  # degree [-40,40]\n\n    img_rot, R = rotate_bound(img, np.copy(degree), (128, 128, 128))\n\n    # Not sure it will cause mask_miss to rotate rightly, just avoid it fails\n    # by np.copy().\n    mask_miss_rot, _ = rotate_bound(mask_miss, np.copy(degree), (255))\n    #mask_all_rot, _ = rotate_bound(mask_all, np.copy(degree), (255))\n\n    # modify meta data\n    meta[\'objpos\'] = rotatepoint(meta[\'objpos\'], R)\n\n    for i in range(18):\n        meta[\'joint_self\'][i, :] = rotatepoint(meta[\'joint_self\'][i, :], R)\n\n    for j in range(meta[\'numOtherPeople\']):\n\n        meta[\'objpos_other\'][j, :] = rotatepoint(meta[\'objpos_other\'][j, :], R)\n\n        for i in range(18):\n            meta[\'joint_others\'][j, i, :] = rotatepoint(\n                meta[\'joint_others\'][j, i, :], R)\n\n    return meta, img_rot, mask_miss_rot#, mask_all_rot\n\n\ndef aug_scale_bbox(meta, img, params_transform):\n    dice = random.random()  # (0,1)\n    if (dice > params_transform[\'scale_prob\']):\n\n        scale_multiplier = 1\n    else:\n        dice2 = random.random()\n        # linear shear into [scale_min, scale_max]\n        scale_multiplier = (\n            params_transform[\'scale_max\'] - params_transform[\'scale_min\']) * dice2 + \\\n            params_transform[\'scale_min\']\n    scale_abs = params_transform[\'target_dist\'] / meta[\'scale_provided\']\n    scale = scale_abs * scale_multiplier\n    img = cv2.resize(img, (0, 0), fx=scale, fy=scale,\n                     interpolation=cv2.INTER_CUBIC)\n\n    meta[\'objpos\'] *= scale\n    adjust_instance_list = []\n    for m in meta[\'instance_mask_list\']:\n        m = cv2.resize(m, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n        adjust_instance_list.append(m)\n    meta[\'instance_mask_list\'] = adjust_instance_list\n\n    return meta, img\n\n\ndef aug_croppad_bbox(meta, img, params_transform):\n    dice_x = random.random()\n    dice_y = random.random()\n    crop_x = int(params_transform[\'crop_size_x\'])\n    crop_y = int(params_transform[\'crop_size_y\'])\n    x_offset = int((dice_x - 0.5) * 2 *\n                   params_transform[\'center_perterb_max\'])\n    y_offset = int((dice_y - 0.5) * 2 *\n                   params_transform[\'center_perterb_max\'])\n\n    center = meta[\'objpos\'] + np.array([x_offset, y_offset])\n    center = center.astype(int)\n\n    # pad up and down\n    pad_v = np.ones((crop_y, img.shape[1], 3), dtype=np.uint8) * 128\n    mask = meta[\'instance_mask_list\'][0]\n    pad_v_instance_mask = np.zeros(\n        (crop_y, mask.shape[1]), dtype=np.uint8)\n\n    img = np.concatenate((pad_v, img, pad_v), axis=0)\n    adjust_instance_list = []\n    for m in meta[\'instance_mask_list\']:\n        m = np.concatenate((pad_v_instance_mask, m, pad_v_instance_mask), axis=0)\n        adjust_instance_list.append(m)\n    meta[\'instance_mask_list\'] = adjust_instance_list\n\n    # pad right and left\n    pad_h = np.ones((img.shape[0], crop_x, 3), dtype=np.uint8) * 128\n    mask = meta[\'instance_mask_list\'][0]\n    pad_h_instance_mask = np.zeros(\n        (mask.shape[0], crop_x), dtype=np.uint8)\n\n    img = np.concatenate((pad_h, img, pad_h), axis=1)\n    adjust_instance_list = []\n    for m in meta[\'instance_mask_list\']:\n        m = np.concatenate((pad_h_instance_mask, m, pad_h_instance_mask), axis=1)\n        m = m[center[1] + int(crop_y / 2):center[1] + int(crop_y / 2) + crop_y + 1,\n            center[0] + int(crop_x / 2):center[0] + int(crop_x / 2) + crop_x + 1]\n        adjust_instance_list.append(m)\n    meta[\'instance_mask_list\'] = adjust_instance_list\n\n    img = img[center[1] + int(crop_y / 2):center[1] + int(crop_y / 2) + crop_y,\n              center[0] + int(crop_x / 2):center[0] + int(crop_x / 2) + crop_x, :]\n\n    return meta, img\n\n\ndef aug_flip_bbox(meta, img, params_transform):\n    dice = random.random()\n    doflip = dice <= params_transform[\'flip_prob\']\n\n    if doflip:\n        img = img.copy()\n        cv2.flip(src=img, flipCode=1, dst=img)\n\n        adjust_instance_list = []\n        for m in meta[\'instance_mask_list\']:\n            m = m.copy()\n            cv2.flip(src=m, flipCode=1, dst=m)\n            adjust_instance_list.append(m)\n        meta[\'instance_mask_list\'] = adjust_instance_list\n\n    return meta, img\n\n\ndef aug_rotate_bbox(meta, img, params_transform, type=""random"", input=0, fillType=""nearest"", constant=0):\n    dice = random.random()\n    degree = (dice - 0.5) * 2 * \\\n        params_transform[\'max_rotate_degree\']  # degree [-40,40]\n\n    img_rot, _ = rotate_bound(img, np.copy(degree), (128, 128, 128))\n\n    # Not sure it will cause mask_miss to rotate rightly, just avoid it fails\n    # by np.copy().\n    adjust_instance_list = []\n    for m in meta[\'instance_mask_list\']:\n        m, _ = rotate_bound(m, np.copy(degree), (0))\n        adjust_instance_list.append(m)\n    meta[\'instance_mask_list\'] = adjust_instance_list\n\n    return meta, img_rot'"
datasets/coco_data/__init__.py,0,b'\n'
datasets/coco_data/heatmap.py,0,"b'# coding=utf-8\n\nimport random\nimport sys\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import misc, ndimage\n\n\n""""""Implement the generate of every channel of ground truth heatmap.\n:param centerA: int with shape (2,), every coordinate of person\'s keypoint.\n:param accumulate_confid_map: one channel of heatmap, which is accumulated, \n       np.log(100) is the max value of heatmap.\n:param params_transform: store the value of stride and crop_szie_y, crop_size_x                 \n""""""\n\n\ndef putGaussianMaps(center, accumulate_confid_map, params_transform):\n    crop_size_y = params_transform[\'crop_size_y\']\n    crop_size_x = params_transform[\'crop_size_x\']\n    stride = params_transform[\'stride\']\n    sigma = params_transform[\'sigma\']\n\n    grid_y = int(crop_size_y / stride)\n    grid_x = int(crop_size_x / stride)\n    start = stride / 2.0 - 0.5\n    y_range = [i for i in range(grid_y)]\n    x_range = [i for i in range(grid_x)]\n    xx, yy = np.meshgrid(x_range, y_range)\n    xx = xx * stride + start\n    yy = yy * stride + start\n    d2 = (xx - center[0]) ** 2 + (yy - center[1]) ** 2\n    exponent = d2 / 2.0 / sigma / sigma\n    mask = exponent <= 4.6052\n    cofid_map = np.exp(-exponent)\n    cofid_map = np.multiply(mask, cofid_map)\n    accumulate_confid_map += cofid_map\n    accumulate_confid_map[accumulate_confid_map > 1.0] = 1.0\n    return accumulate_confid_map\n'"
datasets/coco_data/preprocessing.py,0,"b'""""""\nProvides different utilities to preprocess images.\nArgs:\nimage: A np.array representing an image of (h,w,3).\n\nReturns:\nA preprocessed image. which dtype is np.float32\nand transposed to (3,h,w).\n\n""""""\n\nimport cv2\nimport numpy as np\n\ndef resnet_preprocess(image):\n    image = image.astype(np.float32) / 255.\n    means = [0.485, 0.456, 0.406]\n    stds = [0.229, 0.224, 0.225]\n\n    preprocessed_img = image.copy()[:, :, ::-1]\n    for i in range(3):\n        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - means[i]\n        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]\n\n    preprocessed_img = preprocessed_img.transpose((2, 0, 1)).astype(np.float32)\n    return preprocessed_img\n'"
datasets/coco_data/prn_data_pipeline.py,1,"b""import math\nimport numpy as np\nfrom skimage.filters import gaussian\nfrom torch.utils.data import Dataset\nfrom datasets.coco_data.heatmap import putGaussianMaps\n\nparams_transform = dict()\nparams_transform['sigma'] = 7.0\n\nclass PRN_CocoDataset(Dataset):\n    def __init__(self, coco_train, num_of_keypoints, coeff, threshold, inp_size, feat_stride):\n        self.coco_train = coco_train\n        self.num_of_keypoints = num_of_keypoints\n        self.anns = self.get_anns(self.coco_train)\n        self.bbox_height = coeff * 28\n        self.bbox_width = coeff * 18\n        self.threshold = threshold\n\n        params_transform['crop_size_x'] = inp_size/feat_stride\n        params_transform['crop_size_y'] = inp_size/feat_stride\n        params_transform['stride'] = 1\n\n    def __len__(self):\n        return len(self.anns)\n\n    def __getitem__(self, item):\n        ann_data = self.anns[item]\n\n        input, label = self.get_data(ann_data, self.coco_train)\n\n        return input, label\n\n    def get_data(self, ann_data, coco):\n        weights = np.zeros((self.bbox_height, self.bbox_width, 17))\n        output = np.zeros((self.bbox_height, self.bbox_width, 17))\n\n        bbox = ann_data['bbox']\n        x = int(bbox[0])\n        y = int(bbox[1])\n        w = float(bbox[2])\n        h = float(bbox[3])\n\n        x_scale = float(self.bbox_width) / math.ceil(w)\n        y_scale = float(self.bbox_height) / math.ceil(h)\n\n        kpx = ann_data['keypoints'][0::3]\n        kpy = ann_data['keypoints'][1::3]\n        kpv = ann_data['keypoints'][2::3]\n\n\n        for j in range(17):\n            if kpv[j] > 0:\n                x0 = int((kpx[j] - x) * x_scale)\n                y0 = int((kpy[j] - y) * y_scale)\n\n                if x0 >= self.bbox_width and y0 >= self.bbox_height:\n                    output[self.bbox_height - 1, self.bbox_width - 1, j] = 1\n                elif x0 >= self.bbox_width:\n                    output[y0, self.bbox_width - 1, j] = 1\n                elif y0 >= self.bbox_height:\n                    try:\n                        output[self.bbox_height - 1, x0, j] = 1\n                    except:\n                        output[self.bbox_height - 1, 0, j] = 1\n                elif x0 < 0 and y0 < 0:\n                    output[0, 0, j] = 1\n                elif x0 < 0:\n                    output[y0, 0, j] = 1\n                elif y0 < 0:\n                    output[0, x0, j] = 1\n                else:\n                    output[y0, x0, j] = 1\n\n        img_id = ann_data['image_id']\n        img_data = coco.loadImgs(img_id)[0]\n        ann_data = coco.loadAnns(coco.getAnnIds(img_data['id']))\n\n        for ann in ann_data:\n            kpx = ann['keypoints'][0::3]\n            kpy = ann['keypoints'][1::3]\n            kpv = ann['keypoints'][2::3]\n\n            for j in range(17):\n                if kpv[j] > 0:\n                    if (kpx[j] > bbox[0] - bbox[2] * self.threshold and kpx[j] < bbox[0] + bbox[2] * (1 + self.threshold)):\n                        if (kpy[j] > bbox[1] - bbox[3] * self.threshold and kpy[j] < bbox[1] + bbox[3] * (1 + self.threshold)):\n                            x0 = int((kpx[j] - x) * x_scale)\n                            y0 = int((kpy[j] - y) * y_scale)\n\n                            if x0 >= self.bbox_width and y0 >= self.bbox_height:\n                                weights[self.bbox_height - 1, self.bbox_width - 1, j] = 1\n                            elif x0 >= self.bbox_width:\n                                weights[y0, self.bbox_width - 1, j] = 1\n                            elif y0 >= self.bbox_height:\n                                weights[self.bbox_height - 1, x0, j] = 1\n                            elif x0 < 0 and y0 < 0:\n                                weights[0, 0, j] = 1\n                            elif x0 < 0:\n                                weights[y0, 0, j] = 1\n                            elif y0 < 0:\n                                weights[0, x0, j] = 1\n                            else:\n                                weights[y0, x0, j] = 1\n\n        for t in range(17):\n            weights[:, :, t] = gaussian(weights[:, :, t])\n        output = gaussian(output, sigma=2, mode='constant', multichannel=True)\n        our_order = [0, 6, 8, 10, 5, 7, 9, 12, 14, 16, 11, 13, 15, 2, 1, 4, 3]\n        weights = weights[:, :, our_order]\n        output = output[:, :, our_order]\n        return weights, output\n\n    def get_anns(self, coco):\n        #:param coco: COCO instance\n        #:return: anns: List of annotations that contain person with at least 6 keypoints\n        ann_ids = coco.getAnnIds()\n        anns = []\n        for i in ann_ids:\n            ann = coco.loadAnns(i)[0]\n            if ann['iscrowd'] == 0 and ann['num_keypoints'] > self.num_of_keypoints:\n                anns.append(ann)  # ann\n        sorted_list = sorted(anns, key=lambda k: k['num_keypoints'], reverse=True)\n        return sorted_list\n"""
datasets/coco_data/prn_gaussian.py,0,"b""import numpy as np\nfrom skimage.filters import gaussian\n\nsigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62, .62, 1.07, 1.07, .87, .87, .89, .89] * 100)\n\n\ndef multivariate_gaussian(N, sigma=2):\n    t = 4\n    X = np.linspace(-t, t, N)\n    Y = np.linspace(-t, t, N)\n    X, Y = np.meshgrid(X, Y)\n    pos = np.empty(X.shape + (2,))\n    pos[:, :, 0] = X\n    pos[:, :, 1] = Y\n    mu = np.array([0., 0.])\n    sigma = np.array([[sigma, 0], [0, sigma]])\n    n = mu.shape[0]\n    Sigma_det = np.linalg.det(sigma)\n    Sigma_inv = np.linalg.inv(sigma)\n    N = np.sqrt((2 * np.pi) ** n * Sigma_det)\n    fac = np.einsum('...k,kl,...l->...', pos - mu, Sigma_inv, pos - mu)\n    return np.exp(-fac / 2) / N\n\n\ndef crop_paste(img, c, N=13, sigma=2):\n    Z = multivariate_gaussian(N, sigma)\n\n    H = img.shape[1]\n    W = img.shape[0]\n\n    h = (Z.shape[0] - 1) / 2\n\n    N = Z.shape[0]\n    x1 = (c[0] - h)\n    y1 = (c[1] - h)\n\n    x2 = (c[0] + h) + 1\n    y2 = (c[1] + h) + 1\n\n    zx1 = 0\n    zy1 = 0\n    zx2 = N + 1\n    zy2 = N + 1\n\n    if x1 < 0:\n        x1 = 0\n        zx1 = 0 - (c[0] - h)\n\n    if y1 < 0:\n        y1 = 0\n        zy1 = 0 - (c[1] - h)\n\n    if x2 > W - 1:\n        x2 = W - 1\n        zx2 = x2 - x1 + 1\n        x2 = W\n\n    if y2 > H - 1:\n        y2 = H - 1\n        zy2 = y2 - y1 + 1\n        y2 = H\n\n    img[x1:x2, y1:y2] = np.maximum(Z[zx1:zx2, zy1:zy2], img[x1:x2, y1:y2])\n\n\n'''\ndef gaussian(img, N = 13, sigma=2):\n    cs = np.where(img==1)\n    img = np.zeros_like(img)\n    for c in zip(cs[0], cs[1]):\n        crop_paste(img, c, N, sigma)\n    return img\n'''\n\n\ndef gaussian_multi_input_mp(inp):\n    '''\n    :param inp: Multi person ground truth heatmap input (17 ch) Each channel contains multiple joints.\n    :return: out: Gaussian augmented output. Values are between 0. and 1.\n    '''\n\n    h, w, ch = inp.shape\n    out = np.zeros_like(inp)\n    for i in range(ch):\n        layer = inp[:, :, i]\n        ind = np.argwhere(layer == 1)\n        b = []\n        if len(ind) > 0:\n            for j in ind:\n                t = np.zeros((h, w))\n                t[j[0], j[1]] = 1\n                t = gaussian(t, sigma=2, mode='constant')\n                t = t * (1 / t.max())\n                b.append(t)\n\n            out[:, :, i] = np.maximum.reduce(b)\n        else:\n            out[:, :, i] = np.zeros((h, w))\n    return out\n\n\ndef gaussian_multi_output(inp):\n    '''\n    :param inp: Single person ground truth heatmap input (17 ch) Each channel contains one joint.\n    :return: out: Gaussian augmented output. Values are between 0. and 1.\n    '''\n    h, w, ch = inp.shape\n    out = np.zeros_like(inp)\n    for i in range(ch):\n        j = np.argwhere(inp[:, :, i] == 1)\n        if len(j) == 0:\n            out[:, :, i] = np.zeros((h, w))\n            continue\n        j = j[0]\n        t = np.zeros((h, w))\n        t[j[0], j[1]] = 1\n        t = gaussian(t, sigma=5, mode='constant')\n        out[:, :, i] = t * (1 / t.max())\n    return out\n\n\ndef crop(img, c, N=13):\n    H = img.shape[1]\n    W = img.shape[0]\n\n    h = (N - 1) / 2\n\n    x1 = int(c[0] - h)\n    y1 = int(c[1] - h)\n\n    x2 = int(c[0] + h) + 1\n    y2 = int(c[1] + h) + 1\n\n    if x1 < 0:\n        x1 = 0\n\n    if y1 < 0:\n        y1 = 0\n\n    if x2 > W - 1:\n        x2 = W\n\n    if y2 > H - 1:\n        y2 = H\n\n    return img[x1:x2, y1:y2]\n\n"""
lib/core/__init__.py,0,b''
lib/core/config.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport yaml\n\nimport numpy as np\nfrom easydict import EasyDict as edict\n\n\nconfig = edict()\n\nconfig.OUTPUT_DIR = \'\'\nconfig.LOG_DIR = \'\'\nconfig.DATA_DIR = \'\'\nconfig.GPUS = \'0\'\nconfig.WORKERS = 4\nconfig.PRINT_FREQ = 20\n\n# Cudnn related params\nconfig.CUDNN = edict()\nconfig.CUDNN.BENCHMARK = True\nconfig.CUDNN.DETERMINISTIC = False\nconfig.CUDNN.ENABLED = True\n\n# pose_resnet related params\nPOSE_RESNET = edict()\nPOSE_RESNET.NUM_LAYERS = 50\nPOSE_RESNET.DECONV_WITH_BIAS = False\nPOSE_RESNET.NUM_DECONV_LAYERS = 3\nPOSE_RESNET.NUM_DECONV_FILTERS = [256, 256, 256]\nPOSE_RESNET.NUM_DECONV_KERNELS = [4, 4, 4]\nPOSE_RESNET.FINAL_CONV_KERNEL = 1\nPOSE_RESNET.TARGET_TYPE = \'gaussian\'\nPOSE_RESNET.HEATMAP_SIZE = [64, 64]  # width * height, ex: 24 * 32\nPOSE_RESNET.SIGMA = 2\n\nMODEL_EXTRAS = {\n    \'pose_resnet\': POSE_RESNET,\n}\n\n# common params for NETWORK\nconfig.MODEL = edict()\nconfig.MODEL.NAME = \'pose_resnet\'\nconfig.MODEL.INIT_WEIGHTS = True\nconfig.MODEL.PRETRAINED = \'\'\nconfig.MODEL.NUM_JOINTS = 16\nconfig.MODEL.IMAGE_SIZE = [256, 256]  # width * height, ex: 192 * 256\nconfig.MODEL.EXTRA = MODEL_EXTRAS[config.MODEL.NAME]\n\nconfig.LOSS = edict()\nconfig.LOSS.USE_TARGET_WEIGHT = True\n\n# DATASET related params\nconfig.DATASET = edict()\nconfig.DATASET.ROOT = \'\'\nconfig.DATASET.DATASET = \'mpii\'\nconfig.DATASET.TRAIN_SET = \'train\'\nconfig.DATASET.TEST_SET = \'valid\'\nconfig.DATASET.DATA_FORMAT = \'jpg\'\nconfig.DATASET.HYBRID_JOINTS_TYPE = \'\'\nconfig.DATASET.SELECT_DATA = False\n\n# training data augmentation\nconfig.DATASET.FLIP = True\nconfig.DATASET.SCALE_FACTOR = 0.25\nconfig.DATASET.ROT_FACTOR = 30\n\n# train\nconfig.TRAIN = edict()\n\nconfig.TRAIN.LR_FACTOR = 0.1\nconfig.TRAIN.LR_STEP = [90, 110]\nconfig.TRAIN.LR = 0.001\n\nconfig.TRAIN.OPTIMIZER = \'adam\'\nconfig.TRAIN.MOMENTUM = 0.9\nconfig.TRAIN.WD = 0.0001\nconfig.TRAIN.NESTEROV = False\nconfig.TRAIN.GAMMA1 = 0.99\nconfig.TRAIN.GAMMA2 = 0.0\n\nconfig.TRAIN.BEGIN_EPOCH = 0\nconfig.TRAIN.END_EPOCH = 140\n\nconfig.TRAIN.RESUME = False\nconfig.TRAIN.CHECKPOINT = \'\'\n\nconfig.TRAIN.BATCH_SIZE = 32\nconfig.TRAIN.SHUFFLE = True\n\n# testing\nconfig.TEST = edict()\n\n# size of images for each device\nconfig.TEST.BATCH_SIZE = 32\n# Test Model Epoch\nconfig.TEST.FLIP_TEST = False\nconfig.TEST.POST_PROCESS = True\nconfig.TEST.SHIFT_HEATMAP = True\n\nconfig.TEST.USE_GT_BBOX = False\n# nms\nconfig.TEST.OKS_THRE = 0.5\nconfig.TEST.IN_VIS_THRE = 0.0\nconfig.TEST.COCO_BBOX_FILE = \'\'\nconfig.TEST.BBOX_THRE = 1.0\nconfig.TEST.MODEL_FILE = \'\'\nconfig.TEST.IMAGE_THRE = 0.0\nconfig.TEST.NMS_THRE = 1.0\n\n# debug\nconfig.DEBUG = edict()\nconfig.DEBUG.DEBUG = False\nconfig.DEBUG.SAVE_BATCH_IMAGES_GT = False\nconfig.DEBUG.SAVE_BATCH_IMAGES_PRED = False\nconfig.DEBUG.SAVE_HEATMAPS_GT = False\nconfig.DEBUG.SAVE_HEATMAPS_PRED = False\n\n\ndef _update_dict(k, v):\n    if k == \'DATASET\':\n        if \'MEAN\' in v and v[\'MEAN\']:\n            v[\'MEAN\'] = np.array([eval(x) if isinstance(x, str) else x\n                                  for x in v[\'MEAN\']])\n        if \'STD\' in v and v[\'STD\']:\n            v[\'STD\'] = np.array([eval(x) if isinstance(x, str) else x\n                                 for x in v[\'STD\']])\n    if k == \'MODEL\':\n        if \'EXTRA\' in v and \'HEATMAP_SIZE\' in v[\'EXTRA\']:\n            if isinstance(v[\'EXTRA\'][\'HEATMAP_SIZE\'], int):\n                v[\'EXTRA\'][\'HEATMAP_SIZE\'] = np.array(\n                    [v[\'EXTRA\'][\'HEATMAP_SIZE\'], v[\'EXTRA\'][\'HEATMAP_SIZE\']])\n            else:\n                v[\'EXTRA\'][\'HEATMAP_SIZE\'] = np.array(\n                    v[\'EXTRA\'][\'HEATMAP_SIZE\'])\n        if \'IMAGE_SIZE\' in v:\n            if isinstance(v[\'IMAGE_SIZE\'], int):\n                v[\'IMAGE_SIZE\'] = np.array([v[\'IMAGE_SIZE\'], v[\'IMAGE_SIZE\']])\n            else:\n                v[\'IMAGE_SIZE\'] = np.array(v[\'IMAGE_SIZE\'])\n    for vk, vv in v.items():\n        if vk in config[k]:\n            config[k][vk] = vv\n        else:\n            raise ValueError(""{}.{} not exist in config.py"".format(k, vk))\n\n\ndef update_config(config_file):\n    exp_config = None\n    with open(config_file) as f:\n        exp_config = edict(yaml.load(f))\n        for k, v in exp_config.items():\n            if k in config:\n                if isinstance(v, dict):\n                    _update_dict(k, v)\n                else:\n                    if k == \'SCALES\':\n                        config[k][0] = (tuple(v))\n                    else:\n                        config[k] = v\n            else:\n                raise ValueError(""{} not exist in config.py"".format(k))\n\n\ndef gen_config(config_file):\n    cfg = dict(config)\n    for k, v in cfg.items():\n        if isinstance(v, edict):\n            cfg[k] = dict(v)\n\n    with open(config_file, \'w\') as f:\n        yaml.dump(dict(cfg), f, default_flow_style=False)\n\n\ndef update_dir(model_dir, log_dir, data_dir):\n    if model_dir:\n        config.OUTPUT_DIR = model_dir\n\n    if log_dir:\n        config.LOG_DIR = log_dir\n\n    if data_dir:\n        config.DATA_DIR = data_dir\n\n    config.DATASET.ROOT = os.path.join(\n            config.DATA_DIR, config.DATASET.ROOT)\n\n    config.TEST.COCO_BBOX_FILE = os.path.join(\n            config.DATA_DIR, config.TEST.COCO_BBOX_FILE)\n\n    config.MODEL.PRETRAINED = os.path.join(\n            config.DATA_DIR, config.MODEL.PRETRAINED)\n\n\ndef get_model_name(cfg):\n    name = cfg.MODEL.NAME\n    full_name = cfg.MODEL.NAME\n    extra = cfg.MODEL.EXTRA\n    if name in [\'pose_resnet\']:\n        name = \'{model}_{num_layers}\'.format(\n            model=name,\n            num_layers=extra.NUM_LAYERS)\n        deconv_suffix = \'\'.join(\n            \'d{}\'.format(num_filters)\n            for num_filters in extra.NUM_DECONV_FILTERS)\n        full_name = \'{height}x{width}_{name}_{deconv_suffix}\'.format(\n            height=cfg.MODEL.IMAGE_SIZE[1],\n            width=cfg.MODEL.IMAGE_SIZE[0],\n            name=name,\n            deconv_suffix=deconv_suffix)\n    else:\n        raise ValueError(\'Unkown model: {}\'.format(cfg.MODEL))\n\n    return name, full_name\n\n\nif __name__ == \'__main__\':\n    import sys\n    name, full_name = gen_config(sys.argv[1])\n    print(name)\n    print(full_name)\n'"
lib/nms/__init__.py,0,b''
lib/nms/build.py,2,"b""import os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n\nsources = ['src/nms.c']\nheaders = ['src/nms.h']\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/nms_cuda.c']\n    headers += ['src/nms_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/cuda/nms_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.nms',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects,\n    extra_compile_args=['-std=c99']\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
lib/nms/pth_nms.py,8,"b'import torch\nfrom ._ext import nms\nimport numpy as np\n\ndef pth_nms(dets, thresh):\n  """"""\n  dets has to be a tensor\n  """"""\n  if not dets.is_cuda:\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.sort(0, descending=True)[1]\n    # order = torch.from_numpy(np.ascontiguousarray(scores.numpy().argsort()[::-1])).long()\n\n    keep = torch.LongTensor(dets.size(0))\n    num_out = torch.LongTensor(1)\n    nms.cpu_nms(keep, num_out, dets, order, areas, thresh)\n\n    return keep[:num_out[0]]\n  else:\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.sort(0, descending=True)[1]\n    # order = torch.from_numpy(np.ascontiguousarray(scores.cpu().numpy().argsort()[::-1])).long().cuda()\n\n    dets = dets[order].contiguous()\n\n    keep = torch.LongTensor(dets.size(0))\n    num_out = torch.LongTensor(1)\n    # keep = torch.cuda.LongTensor(dets.size(0))\n    # num_out = torch.cuda.LongTensor(1)\n    nms.gpu_nms(keep, num_out, dets, thresh)\n\n    return order[keep[:num_out[0]].cuda()].contiguous()\n    # return order[keep[:num_out[0]]].contiguous()\n\n'"
lib/utils/__init__.py,0,b''
lib/utils/log.py,0,"b""import logging\n\n\ndef get_logger(name='root'):\n    formatter = logging.Formatter(\n        # fmt='%(asctime)s [%(levelname)s]: %(filename)s(%(funcName)s:%(lineno)s) >> %(message)s')\n        fmt='%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    logger.addHandler(handler)\n    return logger\n\n\nlogger = get_logger('root')\n"""
lib/utils/meter.py,0,"b'import math\nimport numpy as np\n\n\nclass Meter(object):\n    def reset(self):\n        pass\n\n    def add(self):\n        pass\n\n    def value(self):\n        pass\n\n\nclass AverageValueMeter(Meter):\n    def __init__(self):\n        super(AverageValueMeter, self).__init__()\n        self.reset()\n\n    def add(self, value, n=1):\n        self.sum += value\n        self.var += value * value\n        self.n += n\n\n    def value(self):\n        n = self.n\n        if n == 0:\n            mean, std = np.nan, np.nan\n        elif n == 1:\n            return self.sum, np.inf\n        else:\n            mean = self.sum / n\n            std = math.sqrt((self.var - n * mean * mean) / (n - 1.0))\n        return mean, std\n\n    def reset(self):\n        self.sum = 0.0\n        self.n = 0\n        self.var = 0.0\n\n    def __float__(self):\n        return self.value()[0]'"
lib/utils/path.py,0,"b'import os\nimport shutil\n\n\ndef mkdir(path, rm_exist=False):\n    if os.path.isdir(path):\n        if not rm_exist:\n            return\n        shutil.rmtree(path)\n\n    os.makedirs(path)\n'"
lib/utils/timer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport time\n\n\nclass Timer(object):\n    """"""A simple timer.""""""\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n        self.duration = 0.\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            self.duration = self.average_time\n        else:\n            self.duration = self.diff\n        return self.duration\n\n    def clear(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n        self.duration = 0.\n\n'"
lib/nms/_ext/__init__.py,0,b''
lib/nms/_ext/nms/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._nms import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
