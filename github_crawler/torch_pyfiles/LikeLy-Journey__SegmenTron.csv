file_path,api_count,code
setup.py,5,"b'#!/usr/bin/env python\n\nimport glob\nimport os\nfrom setuptools import find_packages, setup\nimport torch\nfrom torch.utils.cpp_extension import CUDA_HOME, CppExtension, CUDAExtension\n\ntorch_ver = [int(x) for x in torch.__version__.split(""."")[:2]]\nassert torch_ver >= [1, 1], ""Requires PyTorch >= 1.1""\n\n\ndef get_extensions():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    extensions_dir = os.path.join(this_dir, ""segmentron"", ""modules"", ""csrc"")\n\n    main_source = os.path.join(extensions_dir, ""vision.cpp"")\n    sources = glob.glob(os.path.join(extensions_dir, ""**"", ""*.cpp""))\n    source_cuda = glob.glob(os.path.join(extensions_dir, ""**"", ""*.cu"")) + glob.glob(\n        os.path.join(extensions_dir, ""*.cu"")\n    )\n\n    sources = [main_source] + sources\n\n    extension = CppExtension\n\n    extra_compile_args = {""cxx"": []}\n    define_macros = []\n\n    if (torch.cuda.is_available() and CUDA_HOME is not None) or os.getenv(""FORCE_CUDA"", ""0"") == ""1"":\n        extension = CUDAExtension\n        sources += source_cuda\n        define_macros += [(""WITH_CUDA"", None)]\n        extra_compile_args[""nvcc""] = [\n            ""-DCUDA_HAS_FP16=1"",\n            ""-D__CUDA_NO_HALF_OPERATORS__"",\n            ""-D__CUDA_NO_HALF_CONVERSIONS__"",\n            ""-D__CUDA_NO_HALF2_OPERATORS__"",\n        ]\n\n        # It\'s better if pytorch can do this by default ..\n        CC = os.environ.get(""CC"", None)\n        if CC is not None:\n            extra_compile_args[""nvcc""].append(""-ccbin={}"".format(CC))\n\n    sources = [os.path.join(extensions_dir, s) for s in sources]\n\n    include_dirs = [extensions_dir]\n\n    ext_modules = [\n        extension(\n            ""segmentron._C"",\n            sources,\n            include_dirs=include_dirs,\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args,\n        )\n    ]\n\n    return ext_modules\n\n\nsetup(\n    name=""segmentron"",\n    version=""0.1"",\n    author=""LikeLy-Journey"",\n    url=""https://github.com/LikeLy-Journey/SegmenTron"",\n    description=""platform for semantic segmentation base on pytorch."",\n    # packages=find_packages(exclude=(""configs"", ""tests"")),\n    # python_requires="">=3.6"",\n    # install_requires=[\n    #     ""termcolor>=1.1"",\n    #     ""Pillow"",\n    #     ""yacs>=0.1.6"",\n    #     ""tabulate"",\n    #     ""cloudpickle"",\n    #     ""matplotlib"",\n    #     ""tqdm>4.29.0"",\n    #     ""tensorboard"",\n    # ],\n    # extras_require={""all"": [""shapely"", ""psutil""]},\n    ext_modules=get_extensions(),\n    cmdclass={""build_ext"": torch.utils.cpp_extension.BuildExtension},\n)\n'"
segmentron/__init__.py,0,"b'from . import modules, models, utils, data'"
tools/demo.py,2,"b""import os\nimport sys\nimport torch\n\ncur_path = os.path.abspath(os.path.dirname(__file__))\nroot_path = os.path.split(cur_path)[0]\nsys.path.append(root_path)\n\nfrom torchvision import transforms\nfrom PIL import Image\nfrom segmentron.utils.visualize import get_color_pallete\nfrom segmentron.models.model_zoo import get_segmentation_model\nfrom segmentron.utils.options import parse_args\nfrom segmentron.utils.default_setup import default_setup\nfrom segmentron.config import cfg\n\n\ndef demo():\n    args = parse_args()\n    cfg.update_from_file(args.config_file)\n    cfg.PHASE = 'test'\n    cfg.ROOT_PATH = root_path\n    cfg.check_and_freeze()\n    default_setup(args)\n\n    # output folder\n    output_dir = os.path.join(cfg.VISUAL.OUTPUT_DIR, 'vis_result_{}_{}_{}_{}'.format(\n        cfg.MODEL.MODEL_NAME, cfg.MODEL.BACKBONE, cfg.DATASET.NAME, cfg.TIME_STAMP))\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # image transform\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(cfg.DATASET.MEAN, cfg.DATASET.STD),\n    ])\n\n    model = get_segmentation_model().to(args.device)\n    model.eval()\n\n    if os.path.isdir(args.input_img):\n        img_paths = [os.path.join(args.input_img, x) for x in os.listdir(args.input_img)]\n    else:\n        img_paths = [args.input_img]\n    for img_path in img_paths:\n        image = Image.open(img_path).convert('RGB')\n        images = transform(image).unsqueeze(0).to(args.device)\n        with torch.no_grad():\n            output = model(images)\n\n        pred = torch.argmax(output[0], 1).squeeze(0).cpu().data.numpy()\n        mask = get_color_pallete(pred, cfg.DATASET.NAME)\n        outname = os.path.splitext(os.path.split(img_path)[-1])[0] + '.png'\n        mask.save(os.path.join(output_dir, outname))\n\n\nif __name__ == '__main__':\n    demo()\n"""
tools/eval.py,5,"b'from __future__ import print_function\n\nimport os\nimport sys\n\ncur_path = os.path.abspath(os.path.dirname(__file__))\nroot_path = os.path.split(cur_path)[0]\nsys.path.append(root_path)\n\nimport logging\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.nn.functional as F\n\nfrom tabulate import tabulate\nfrom torchvision import transforms\nfrom segmentron.data.dataloader import get_segmentation_dataset\nfrom segmentron.models.model_zoo import get_segmentation_model\nfrom segmentron.utils.score import SegmentationMetric\nfrom segmentron.utils.distributed import synchronize, make_data_sampler, make_batch_data_sampler\nfrom segmentron.config import cfg\nfrom segmentron.utils.options import parse_args\nfrom segmentron.utils.default_setup import default_setup\n\n\nclass Evaluator(object):\n    def __init__(self, args):\n        self.args = args\n        self.device = torch.device(args.device)\n\n        # image transform\n        input_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(cfg.DATASET.MEAN, cfg.DATASET.STD),\n        ])\n\n        # dataset and dataloader\n        val_dataset = get_segmentation_dataset(cfg.DATASET.NAME, split=\'val\', mode=\'testval\', transform=input_transform)\n        val_sampler = make_data_sampler(val_dataset, False, args.distributed)\n        val_batch_sampler = make_batch_data_sampler(val_sampler, images_per_batch=cfg.TEST.BATCH_SIZE, drop_last=False)\n        self.val_loader = data.DataLoader(dataset=val_dataset,\n                                          batch_sampler=val_batch_sampler,\n                                          num_workers=cfg.DATASET.WORKERS,\n                                          pin_memory=True)\n        self.classes = val_dataset.classes\n        # create network\n        self.model = get_segmentation_model().to(self.device)\n\n        if hasattr(self.model, \'encoder\') and hasattr(self.model.encoder, \'named_modules\') and \\\n            cfg.MODEL.BN_EPS_FOR_ENCODER:\n            logging.info(\'set bn custom eps for bn in encoder: {}\'.format(cfg.MODEL.BN_EPS_FOR_ENCODER))\n            self.set_batch_norm_attr(self.model.encoder.named_modules(), \'eps\', cfg.MODEL.BN_EPS_FOR_ENCODER)\n\n        if args.distributed:\n            self.model = nn.parallel.DistributedDataParallel(self.model,\n                device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True)\n        self.model.to(self.device)\n\n        self.metric = SegmentationMetric(val_dataset.num_class, args.distributed)\n\n    def set_batch_norm_attr(self, named_modules, attr, value):\n        for m in named_modules:\n            if isinstance(m[1], nn.BatchNorm2d) or isinstance(m[1], nn.SyncBatchNorm):\n                setattr(m[1], attr, value)\n\n    def eval(self):\n        self.metric.reset()\n        self.model.eval()\n        if self.args.distributed:\n            model = self.model.module\n        else:\n            model = self.model\n\n        logging.info(""Start validation, Total sample: {:d}"".format(len(self.val_loader)))\n        import time\n        time_start = time.time()\n        for i, (image, target, filename) in enumerate(self.val_loader):\n            image = image.to(self.device)\n            target = target.to(self.device)\n\n            with torch.no_grad():\n                output = model.evaluate(image)\n\n            self.metric.update(output, target)\n            pixAcc, mIoU = self.metric.get()\n            logging.info(""Sample: {:d}, validation pixAcc: {:.3f}, mIoU: {:.3f}"".format(\n                i + 1, pixAcc * 100, mIoU * 100))\n\n        synchronize()\n        pixAcc, mIoU, category_iou = self.metric.get(return_category_iou=True)\n        logging.info(\'Eval use time: {:.3f} second\'.format(time.time() - time_start))\n        logging.info(\'End validation pixAcc: {:.3f}, mIoU: {:.3f}\'.format(\n                pixAcc * 100, mIoU * 100))\n\n        headers = [\'class id\', \'class name\', \'iou\']\n        table = []\n        for i, cls_name in enumerate(self.classes):\n            table.append([cls_name, category_iou[i]])\n        logging.info(\'Category iou: \\n {}\'.format(tabulate(table, headers, tablefmt=\'grid\', showindex=""always"",\n                                                           numalign=\'center\', stralign=\'center\')))\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    cfg.update_from_file(args.config_file)\n    cfg.update_from_list(args.opts)\n    cfg.PHASE = \'test\'\n    cfg.ROOT_PATH = root_path\n    cfg.check_and_freeze()\n\n    default_setup(args)\n\n    evaluator = Evaluator(args)\n    evaluator.eval()\n'"
tools/train.py,7,"b'import time\nimport copy\nimport datetime\nimport os\nimport sys\n\ncur_path = os.path.abspath(os.path.dirname(__file__))\nroot_path = os.path.split(cur_path)[0]\nsys.path.append(root_path)\n\nimport logging\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.nn.functional as F\n\nfrom torchvision import transforms\nfrom segmentron.data.dataloader import get_segmentation_dataset\nfrom segmentron.models.model_zoo import get_segmentation_model\nfrom segmentron.solver.loss import get_segmentation_loss\nfrom segmentron.solver.optimizer import get_optimizer\nfrom segmentron.solver.lr_scheduler import get_scheduler\nfrom segmentron.utils.distributed import *\nfrom segmentron.utils.score import SegmentationMetric\nfrom segmentron.utils.filesystem import save_checkpoint\nfrom segmentron.utils.options import parse_args\nfrom segmentron.utils.default_setup import default_setup\nfrom segmentron.utils.visualize import show_flops_params\nfrom segmentron.config import cfg\n\nclass Trainer(object):\n    def __init__(self, args):\n        self.args = args\n        self.device = torch.device(args.device)\n\n        # image transform\n        input_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(cfg.DATASET.MEAN, cfg.DATASET.STD),\n        ])\n        # dataset and dataloader\n        data_kwargs = {\'transform\': input_transform, \'base_size\': cfg.TRAIN.BASE_SIZE,\n                       \'crop_size\': cfg.TRAIN.CROP_SIZE}\n        train_dataset = get_segmentation_dataset(cfg.DATASET.NAME, split=\'train\', mode=\'train\', **data_kwargs)\n        val_dataset = get_segmentation_dataset(cfg.DATASET.NAME, split=\'val\', mode=cfg.DATASET.MODE, **data_kwargs)\n        self.iters_per_epoch = len(train_dataset) // (args.num_gpus * cfg.TRAIN.BATCH_SIZE)\n        self.max_iters = cfg.TRAIN.EPOCHS * self.iters_per_epoch\n\n        train_sampler = make_data_sampler(train_dataset, shuffle=True, distributed=args.distributed)\n        train_batch_sampler = make_batch_data_sampler(train_sampler, cfg.TRAIN.BATCH_SIZE, self.max_iters, drop_last=True)\n        val_sampler = make_data_sampler(val_dataset, False, args.distributed)\n        val_batch_sampler = make_batch_data_sampler(val_sampler, cfg.TEST.BATCH_SIZE, drop_last=False)\n\n        self.train_loader = data.DataLoader(dataset=train_dataset,\n                                            batch_sampler=train_batch_sampler,\n                                            num_workers=cfg.DATASET.WORKERS,\n                                            pin_memory=True)\n        self.val_loader = data.DataLoader(dataset=val_dataset,\n                                          batch_sampler=val_batch_sampler,\n                                          num_workers=cfg.DATASET.WORKERS,\n                                          pin_memory=True)\n\n        # create network\n        self.model = get_segmentation_model().to(self.device)\n        \n        # print params and flops\n        if get_rank() == 0:\n            try:\n                show_flops_params(copy.deepcopy(self.model), args.device)\n            except Exception as e:\n                logging.warning(\'get flops and params error: {}\'.format(e))\n\n        if cfg.MODEL.BN_TYPE not in [\'BN\']:\n            logging.info(\'Batch norm type is {}, convert_sync_batchnorm is not effective\'.format(cfg.MODEL.BN_TYPE))\n        elif args.distributed and cfg.TRAIN.SYNC_BATCH_NORM:\n            self.model = nn.SyncBatchNorm.convert_sync_batchnorm(self.model)\n            logging.info(\'SyncBatchNorm is effective!\')\n        else:\n            logging.info(\'Not use SyncBatchNorm!\')\n\n        # create criterion\n        self.criterion = get_segmentation_loss(cfg.MODEL.MODEL_NAME, use_ohem=cfg.SOLVER.OHEM,\n                                               aux=cfg.SOLVER.AUX, aux_weight=cfg.SOLVER.AUX_WEIGHT,\n                                               ignore_index=cfg.DATASET.IGNORE_INDEX).to(self.device)\n\n        # optimizer, for model just includes encoder, decoder(head and auxlayer).\n        self.optimizer = get_optimizer(self.model)\n\n        # lr scheduling\n        self.lr_scheduler = get_scheduler(self.optimizer, max_iters=self.max_iters,\n                                          iters_per_epoch=self.iters_per_epoch)\n\n        # resume checkpoint if needed\n        self.start_epoch = 0\n        if args.resume and os.path.isfile(args.resume):\n            name, ext = os.path.splitext(args.resume)\n            assert ext == \'.pkl\' or \'.pth\', \'Sorry only .pth and .pkl files supported.\'\n            logging.info(\'Resuming training, loading {}...\'.format(args.resume))\n            resume_sate = torch.load(args.resume)\n            self.model.load_state_dict(resume_sate[\'state_dict\'])\n            self.start_epoch = resume_sate[\'epoch\']\n            logging.info(\'resume train from epoch: {}\'.format(self.start_epoch))\n            if resume_sate[\'optimizer\'] is not None and resume_sate[\'lr_scheduler\'] is not None:\n                logging.info(\'resume optimizer and lr scheduler from resume state..\')\n                self.optimizer.load_state_dict(resume_sate[\'optimizer\'])\n                self.lr_scheduler.load_state_dict(resume_sate[\'lr_scheduler\'])\n\n        if args.distributed:\n            self.model = nn.parallel.DistributedDataParallel(self.model, device_ids=[args.local_rank],\n                                                             output_device=args.local_rank,\n                                                             find_unused_parameters=True)\n\n        # evaluation metrics\n        self.metric = SegmentationMetric(train_dataset.num_class, args.distributed)\n        self.best_pred = 0.0\n\n\n    def train(self):\n        self.save_to_disk = get_rank() == 0\n        epochs, max_iters, iters_per_epoch = cfg.TRAIN.EPOCHS, self.max_iters, self.iters_per_epoch\n        log_per_iters, val_per_iters = self.args.log_iter, self.args.val_epoch * self.iters_per_epoch\n\n        start_time = time.time()\n        logging.info(\'Start training, Total Epochs: {:d} = Total Iterations {:d}\'.format(epochs, max_iters))\n\n        self.model.train()\n        iteration = self.start_epoch * iters_per_epoch if self.start_epoch > 0 else 0\n        for (images, targets, _) in self.train_loader:\n            epoch = iteration // iters_per_epoch + 1\n            iteration += 1\n\n            images = images.to(self.device)\n            targets = targets.to(self.device)\n\n            outputs = self.model(images)\n            loss_dict = self.criterion(outputs, targets)\n\n            losses = sum(loss for loss in loss_dict.values())\n\n            # reduce losses over all GPUs for logging purposes\n            loss_dict_reduced = reduce_loss_dict(loss_dict)\n            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n\n            self.optimizer.zero_grad()\n            losses.backward()\n            self.optimizer.step()\n            self.lr_scheduler.step()\n\n            eta_seconds = ((time.time() - start_time) / iteration) * (max_iters - iteration)\n            eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n\n            if iteration % log_per_iters == 0 and self.save_to_disk:\n                logging.info(\n                    ""Epoch: {:d}/{:d} || Iters: {:d}/{:d} || Lr: {:.6f} || ""\n                    ""Loss: {:.4f} || Cost Time: {} || Estimated Time: {}"".format(\n                        epoch, epochs, iteration % iters_per_epoch, iters_per_epoch,\n                        self.optimizer.param_groups[0][\'lr\'], losses_reduced.item(),\n                        str(datetime.timedelta(seconds=int(time.time() - start_time))),\n                        eta_string))\n\n            if iteration % self.iters_per_epoch == 0 and self.save_to_disk:\n                save_checkpoint(self.model, epoch, self.optimizer, self.lr_scheduler, is_best=False)\n\n            if not self.args.skip_val and iteration % val_per_iters == 0:\n                self.validation(epoch)\n                self.model.train()\n\n        total_training_time = time.time() - start_time\n        total_training_str = str(datetime.timedelta(seconds=total_training_time))\n        logging.info(\n            ""Total training time: {} ({:.4f}s / it)"".format(\n                total_training_str, total_training_time / max_iters))\n\n    def validation(self, epoch):\n        self.metric.reset()\n        if self.args.distributed:\n            model = self.model.module\n        else:\n            model = self.model\n        torch.cuda.empty_cache()\n        model.eval()\n        for i, (image, target, filename) in enumerate(self.val_loader):\n            image = image.to(self.device)\n            target = target.to(self.device)\n\n            with torch.no_grad():\n                if cfg.DATASET.MODE == \'val\' or cfg.TEST.CROP_SIZE is None:\n                    output = model(image)[0]\n                else:\n                    size = image.size()[2:]\n                    pad_height = cfg.TEST.CROP_SIZE[0] - size[0]\n                    pad_width = cfg.TEST.CROP_SIZE[1] - size[1]\n                    image = F.pad(image, (0, pad_height, 0, pad_width))\n                    output = model(image)[0]\n                    output = output[..., :size[0], :size[1]]\n\n            self.metric.update(output, target)\n            pixAcc, mIoU = self.metric.get()\n            logging.info(""[EVAL] Sample: {:d}, pixAcc: {:.3f}, mIoU: {:.3f}"".format(i + 1, pixAcc * 100, mIoU * 100))\n        pixAcc, mIoU = self.metric.get()\n        logging.info(""[EVAL END] Epoch: {:d}, pixAcc: {:.3f}, mIoU: {:.3f}"".format(epoch, pixAcc * 100, mIoU * 100))\n        synchronize()\n        if self.best_pred < mIoU and self.save_to_disk:\n            self.best_pred = mIoU\n            logging.info(\'Epoch {} is the best model, best pixAcc: {:.3f}, mIoU: {:.3f}, save the model..\'.format(epoch, pixAcc * 100, mIoU * 100))\n            save_checkpoint(model, epoch, is_best=True)\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    # get config\n    cfg.update_from_file(args.config_file)\n    cfg.update_from_list(args.opts)\n    cfg.PHASE = \'train\'\n    cfg.ROOT_PATH = root_path\n    cfg.check_and_freeze()\n\n    # setup python train environment, logger, seed..\n    default_setup(args)\n\n    # create a trainer and start train\n    trainer = Trainer(args)\n    trainer.train()\n'"
segmentron/config/__init__.py,0,b'from .settings import cfg'
segmentron/config/config.py,0,"b'from __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import unicode_literals\n\nimport codecs\nimport yaml\nimport six\nimport time\n\nfrom ast import literal_eval\n\nclass SegmentronConfig(dict):\n    def __init__(self, *args, **kwargs):\n        super(SegmentronConfig, self).__init__(*args, **kwargs)\n        self.immutable = False\n\n    def __setattr__(self, key, value, create_if_not_exist=True):\n        if key in [""immutable""]:\n            self.__dict__[key] = value\n            return\n\n        t = self\n        keylist = key.split(""."")\n        for k in keylist[:-1]:\n            t = t.__getattr__(k, create_if_not_exist)\n\n        t.__getattr__(keylist[-1], create_if_not_exist)\n        t[keylist[-1]] = value\n\n    def __getattr__(self, key, create_if_not_exist=True):\n        if key in [""immutable""]:\n            if key not in self.__dict__:\n                self.__dict__[key] = False\n            return self.__dict__[key]\n\n        if not key in self:\n            if not create_if_not_exist:\n                raise KeyError\n            self[key] = SegmentronConfig()\n        return self[key]\n\n    def __setitem__(self, key, value):\n        #\n        if self.immutable:\n            raise AttributeError(\n                \'Attempted to set ""{}"" to ""{}"", but SegConfig is immutable\'.\n                format(key, value))\n        #\n        if isinstance(value, six.string_types):\n            try:\n                value = literal_eval(value)\n            except ValueError:\n                pass\n            except SyntaxError:\n                pass\n        super(SegmentronConfig, self).__setitem__(key, value)\n\n    def update_from_other_cfg(self, other):\n        if isinstance(other, dict):\n            other = SegmentronConfig(other)\n        assert isinstance(other, SegmentronConfig)\n        cfg_list = [("""", other)]\n        while len(cfg_list):\n            prefix, tdic = cfg_list[0]\n            cfg_list = cfg_list[1:]\n            for key, value in tdic.items():\n                key = ""{}.{}"".format(prefix, key) if prefix else key\n                if isinstance(value, dict):\n                    cfg_list.append((key, value))\n                    continue\n                try:\n                    self.__setattr__(key, value, create_if_not_exist=False)\n                except KeyError:\n                    raise KeyError(\'Non-existent config key: {}\'.format(key))\n\n    def remove_irrelevant_cfg(self):\n        model_name = self.MODEL.MODEL_NAME\n\n        from ..models.model_zoo import MODEL_REGISTRY\n        model_list = MODEL_REGISTRY.get_list()\n        model_list_lower = [x.lower() for x in model_list]\n        \n        assert model_name.lower() in model_list_lower, ""Expected model name in {}, but received {}""\\\n            .format(model_list, model_name)\n        pop_keys = []\n        for key in self.MODEL.keys():\n            if key.lower() in model_list_lower:\n                if model_name.lower() == \'pointrend\' and \\\n                    key.lower() == self.MODEL.POINTREND.BASEMODEL.lower():\n                    continue\n            if key.lower() in model_list_lower and key.lower() != model_name.lower():\n                pop_keys.append(key)\n        for key in pop_keys:\n            self.MODEL.pop(key)\n\n\n\n    def check_and_freeze(self):\n        self.TIME_STAMP = time.strftime(\'%Y-%m-%d-%H-%M\', time.localtime())\n        # TODO: remove irrelevant config and then freeze\n        self.remove_irrelevant_cfg()\n        self.immutable = True\n\n    def update_from_list(self, config_list):\n        if len(config_list) % 2 != 0:\n            raise ValueError(\n                ""Command line options config format error! Please check it: {}"".\n                format(config_list))\n        for key, value in zip(config_list[0::2], config_list[1::2]):\n            try:\n                self.__setattr__(key, value, create_if_not_exist=False)\n            except KeyError:\n                raise KeyError(\'Non-existent config key: {}\'.format(key))\n\n    def update_from_file(self, config_file):\n        with codecs.open(config_file, \'r\', \'utf-8\') as file:\n            loaded_cfg = yaml.load(file, Loader=yaml.FullLoader)\n        self.update_from_other_cfg(loaded_cfg)\n\n    def set_immutable(self, immutable):\n        self.immutable = immutable\n        for value in self.values():\n            if isinstance(value, SegmentronConfig):\n                value.set_immutable(immutable)\n\n    def is_immutable(self):\n        return self.immutable'"
segmentron/config/settings.py,0,"b'from .config import SegmentronConfig\n\ncfg = SegmentronConfig()\n\n########################## basic set ###########################################\n# random seed\ncfg.SEED = 1024\n# train time stamp, auto generate, do not need to set\ncfg.TIME_STAMP = \'\'\n# root path\ncfg.ROOT_PATH = \'\'\n# model phase [\'train\', \'test\']\ncfg.PHASE = \'train\'\n\n########################## dataset config #########################################\n# dataset name\ncfg.DATASET.NAME = \'\'\n# pixel mean\ncfg.DATASET.MEAN = [0.5, 0.5, 0.5]\n# pixel std\ncfg.DATASET.STD = [0.5, 0.5, 0.5]\n# dataset ignore index\ncfg.DATASET.IGNORE_INDEX = -1\n# workers\ncfg.DATASET.WORKERS = 4\n# val dataset mode\ncfg.DATASET.MODE = \'testval\'\n########################### data augment ######################################\n# data augment image mirror\ncfg.AUG.MIRROR = True\n# blur probability\ncfg.AUG.BLUR_PROB = 0.0\n# blur radius\ncfg.AUG.BLUR_RADIUS = 0.0\n# color jitter, float or tuple: (0.1, 0.2, 0.3, 0.4)\ncfg.AUG.COLOR_JITTER = None\n########################### train config ##########################################\n# epochs\ncfg.TRAIN.EPOCHS = 30\n# batch size\ncfg.TRAIN.BATCH_SIZE = 1\n# train crop size\ncfg.TRAIN.CROP_SIZE = 769\n# train base size\ncfg.TRAIN.BASE_SIZE = 1024\n# model output dir\ncfg.TRAIN.MODEL_SAVE_DIR = \'runs/checkpoints/\'\n# log dir\ncfg.TRAIN.LOG_SAVE_DIR = \'runs/logs/\'\n# pretrained model for eval or finetune\ncfg.TRAIN.PRETRAINED_MODEL_PATH = \'\'\n# use pretrained backbone model over imagenet\ncfg.TRAIN.BACKBONE_PRETRAINED = True\n# backbone pretrained model path, if not specific, will load from url when backbone pretrained enabled\ncfg.TRAIN.BACKBONE_PRETRAINED_PATH = \'\'\n# resume model path\ncfg.TRAIN.RESUME_MODEL_PATH = \'\'\n# whether to use synchronize bn\ncfg.TRAIN.SYNC_BATCH_NORM = True\n# save model every checkpoint-epoch\ncfg.TRAIN.SNAPSHOT_EPOCH = 10\n\n########################### optimizer config ##################################\n# base learning rate\ncfg.SOLVER.LR = 1e-4\n# optimizer method\ncfg.SOLVER.OPTIMIZER = ""sgd""\n# optimizer epsilon\ncfg.SOLVER.EPSILON = 1e-8\n# optimizer momentum\ncfg.SOLVER.MOMENTUM = 0.9\n# weight decay\ncfg.SOLVER.WEIGHT_DECAY = 1e-4 #0.00004\n# decoder lr x10\ncfg.SOLVER.DECODER_LR_FACTOR = 10.0\n# lr scheduler mode\ncfg.SOLVER.LR_SCHEDULER = ""poly""\n# poly power\ncfg.SOLVER.POLY.POWER = 0.9\n# step gamma\ncfg.SOLVER.STEP.GAMMA = 0.1\n# milestone of step lr scheduler\ncfg.SOLVER.STEP.DECAY_EPOCH = [10, 20]\n# warm up epochs can be float\ncfg.SOLVER.WARMUP.EPOCHS = 0.\n# warm up factor\ncfg.SOLVER.WARMUP.FACTOR = 1.0 / 3\n# warm up method\ncfg.SOLVER.WARMUP.METHOD = \'linear\'\n# whether to use ohem\ncfg.SOLVER.OHEM = False\n# whether to use aux loss\ncfg.SOLVER.AUX = False\n# aux loss weight\ncfg.SOLVER.AUX_WEIGHT = 0.4\n# loss name\ncfg.SOLVER.LOSS_NAME = \'\'\n########################## test config ###########################################\n# val/test model path\ncfg.TEST.TEST_MODEL_PATH = \'\'\n# test batch size\ncfg.TEST.BATCH_SIZE = 1\n# eval crop size\ncfg.TEST.CROP_SIZE = None\n# multiscale eval\ncfg.TEST.SCALES = [1.0]\n# flip\ncfg.TEST.FLIP = False\n\n########################## visual config ###########################################\n# visual result output dir\ncfg.VISUAL.OUTPUT_DIR = \'../runs/visual/\'\n\n########################## model #######################################\n# model name\ncfg.MODEL.MODEL_NAME = \'\'\n# model backbone\ncfg.MODEL.BACKBONE = \'\'\n# model backbone channel scale\ncfg.MODEL.BACKBONE_SCALE = 1.0\n# support resnet b, c. b is standard resnet in pytorch official repo\n# cfg.MODEL.RESNET_VARIANT = \'b\'\n# multi branch loss weight\ncfg.MODEL.MULTI_LOSS_WEIGHT = [1.0]\n# gn groups\ncfg.MODEL.DEFAULT_GROUP_NUMBER = 32\n# whole model default epsilon\ncfg.MODEL.DEFAULT_EPSILON = 1e-5\n# batch norm, support [\'BN\', \'SyncBN\', \'FrozenBN\', \'GN\', \'nnSyncBN\']\ncfg.MODEL.BN_TYPE = \'BN\'\n# batch norm epsilon for encoder, if set None will use api default value.\ncfg.MODEL.BN_EPS_FOR_ENCODER = None\n# batch norm epsilon for encoder, if set None will use api default value.\ncfg.MODEL.BN_EPS_FOR_DECODER = None\n# backbone output stride\ncfg.MODEL.OUTPUT_STRIDE = 16\n# BatchNorm momentum, if set None will use api default value.\ncfg.MODEL.BN_MOMENTUM = None\n\n########################## DANet config ####################################\n# danet param\ncfg.MODEL.DANET.MULTI_DILATION = None\n# danet param\ncfg.MODEL.DANET.MULTI_GRID = False\n\n########################## DeepLab config ####################################\n# whether to use aspp\ncfg.MODEL.DEEPLABV3_PLUS.USE_ASPP = True\n# whether to use decoder\ncfg.MODEL.DEEPLABV3_PLUS.ENABLE_DECODER = True\n# whether aspp use sep conv\ncfg.MODEL.DEEPLABV3_PLUS.ASPP_WITH_SEP_CONV = True\n# whether decoder use sep conv\ncfg.MODEL.DEEPLABV3_PLUS.DECODER_USE_SEP_CONV = True\n\n########################## UNET config #######################################\n# upsample mode\n# cfg.MODEL.UNET.UPSAMPLE_MODE = \'bilinear\'\n\n########################## OCNet config ######################################\n# [\'base\', \'pyramid\', \'asp\']\ncfg.MODEL.OCNet.OC_ARCH = \'base\'\n\n########################## EncNet config ######################################\ncfg.MODEL.ENCNET.SE_LOSS = True\ncfg.MODEL.ENCNET.SE_WEIGHT = 0.2\ncfg.MODEL.ENCNET.LATERAL = True\n\n\n########################## CCNET config ######################################\ncfg.MODEL.CCNET.RECURRENCE = 2\n\n########################## CGNET config ######################################\ncfg.MODEL.CGNET.STAGE2_BLOCK_NUM = 3\ncfg.MODEL.CGNET.STAGE3_BLOCK_NUM = 21\n\n########################## PointRend config ##################################\ncfg.MODEL.POINTREND.BASEMODEL = \'DeepLabV3_Plus\'\n\n########################## hrnet config ######################################\ncfg.MODEL.HRNET.PRETRAINED_LAYERS = [\'*\']\ncfg.MODEL.HRNET.STEM_INPLANES = 64\ncfg.MODEL.HRNET.FINAL_CONV_KERNEL = 1\ncfg.MODEL.HRNET.WITH_HEAD = True\n# stage 1\ncfg.MODEL.HRNET.STAGE1.NUM_MODULES = 1\ncfg.MODEL.HRNET.STAGE1.NUM_BRANCHES = 1\ncfg.MODEL.HRNET.STAGE1.NUM_BLOCKS = [1]\ncfg.MODEL.HRNET.STAGE1.NUM_CHANNELS = [32]\ncfg.MODEL.HRNET.STAGE1.BLOCK = \'BOTTLENECK\'\ncfg.MODEL.HRNET.STAGE1.FUSE_METHOD = \'SUM\'\n# stage 2\ncfg.MODEL.HRNET.STAGE2.NUM_MODULES = 1\ncfg.MODEL.HRNET.STAGE2.NUM_BRANCHES = 2\ncfg.MODEL.HRNET.STAGE2.NUM_BLOCKS = [4, 4]\ncfg.MODEL.HRNET.STAGE2.NUM_CHANNELS = [32, 64]\ncfg.MODEL.HRNET.STAGE2.BLOCK = \'BASIC\'\ncfg.MODEL.HRNET.STAGE2.FUSE_METHOD = \'SUM\'\n# stage 3\ncfg.MODEL.HRNET.STAGE3.NUM_MODULES = 1\ncfg.MODEL.HRNET.STAGE3.NUM_BRANCHES = 3\ncfg.MODEL.HRNET.STAGE3.NUM_BLOCKS = [4, 4, 4]\ncfg.MODEL.HRNET.STAGE3.NUM_CHANNELS = [32, 64, 128]\ncfg.MODEL.HRNET.STAGE3.BLOCK = \'BASIC\'\ncfg.MODEL.HRNET.STAGE3.FUSE_METHOD = \'SUM\'\n# stage 4\ncfg.MODEL.HRNET.STAGE4.NUM_MODULES = 1\ncfg.MODEL.HRNET.STAGE4.NUM_BRANCHES = 4\ncfg.MODEL.HRNET.STAGE4.NUM_BLOCKS = [4, 4, 4, 4]\ncfg.MODEL.HRNET.STAGE4.NUM_CHANNELS = [32, 64, 128, 256]\ncfg.MODEL.HRNET.STAGE4.BLOCK = \'BASIC\'\ncfg.MODEL.HRNET.STAGE4.FUSE_METHOD = \'SUM\'\n\n'"
segmentron/data/__init__.py,0,b''
segmentron/models/__init__.py,0,"b'""""""Model Zoo""""""\nfrom .model_zoo import MODEL_REGISTRY\nfrom .fast_scnn import FastSCNN\nfrom .deeplabv3_plus import DeepLabV3Plus\nfrom .hrnet_seg import HighResolutionNet\nfrom .fcn import FCN\nfrom .dfanet import DFANet\nfrom .pspnet import PSPNet\nfrom .icnet import ICNet\nfrom .danet import DANet\n# from .ccnet import CCNet\nfrom .bisenet import BiSeNet\nfrom .cgnet import CGNet\nfrom .denseaspp import DenseASPP\nfrom .dunet import DUNet\nfrom .encnet import EncNet\nfrom .lednet import LEDNet\nfrom .ocnet import OCNet\nfrom .hardnet import HardNet\nfrom .refinenet import RefineNet\nfrom .dabnet import DABNet\nfrom .unet import UNet\nfrom .fpenet import FPENet\nfrom .contextnet import ContextNet\nfrom .espnetv2 import ESPNetV2\nfrom .enet import ENet\nfrom .edanet import EDANet\nfrom .pointrend import PointRend\n'"
segmentron/models/bisenet.py,3,"b'""""""Bilateral Segmentation Network""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom ..modules import _ConvBNReLU\n\n__all__ = [\'BiSeNet\']\n\n\n@MODEL_REGISTRY.register()\nclass BiSeNet(SegBaseModel):\n    r""""""BiSeNet\n    Reference:\n        Changqian Yu, et al. ""BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation""\n        arXiv preprint arXiv:1808.00897 (2018).\n    """"""\n    def __init__(self):\n        super(BiSeNet, self).__init__()\n        self.spatial_path = SpatialPath(3, 128, norm_layer=self.norm_layer)\n        self.context_path = ContextPath(norm_layer=self.norm_layer)\n        self.ffm = FeatureFusion(256, 256, 4)\n        self.head = _BiSeHead(256, 64, self.nclass)\n        if self.aux:\n            self.auxlayer1 = _BiSeHead(128, 256, self.nclass)\n            self.auxlayer2 = _BiSeHead(128, 256, self.nclass)\n\n        self.__setattr__(\'decoder\',\n                         [\'spatial_path\', \'context_path\', \'ffm\', \'head\', \'auxlayer1\', \'auxlayer2\'] if self.aux else [\n                             \'spatial_path\', \'context_path\', \'ffm\', \'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        spatial_out = self.spatial_path(x)\n        c1, c2, c3, c4 = self.encoder(x)\n        context_out = self.context_path(c1, c2, c3, c4)\n        fusion_out = self.ffm(spatial_out, context_out[-1])\n        outputs = []\n        x = self.head(fusion_out)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n\n        if self.aux:\n            auxout1 = self.auxlayer1(context_out[0])\n            auxout1 = F.interpolate(auxout1, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout1)\n            auxout2 = self.auxlayer2(context_out[1])\n            auxout2 = F.interpolate(auxout2, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout2)\n        return tuple(outputs)\n\n\nclass _BiSeHead(nn.Module):\n    def __init__(self, in_channels, inter_channels, nclass, norm_layer=nn.BatchNorm2d):\n        super(_BiSeHead, self).__init__()\n        self.block = nn.Sequential(\n            _ConvBNReLU(in_channels, inter_channels, 3, 1, 1, norm_layer=norm_layer),\n            nn.Dropout(0.1),\n            nn.Conv2d(inter_channels, nclass, 1)\n        )\n\n    def forward(self, x):\n        x = self.block(x)\n        return x\n\n\nclass SpatialPath(nn.Module):\n    """"""Spatial path""""""\n\n    def __init__(self, in_channels, out_channels, norm_layer=nn.BatchNorm2d):\n        super(SpatialPath, self).__init__()\n        inter_channels = 64\n        self.conv7x7 = _ConvBNReLU(in_channels, inter_channels, 7, 2, 3, norm_layer=norm_layer)\n        self.conv3x3_1 = _ConvBNReLU(inter_channels, inter_channels, 3, 2, 1, norm_layer=norm_layer)\n        self.conv3x3_2 = _ConvBNReLU(inter_channels, inter_channels, 3, 2, 1, norm_layer=norm_layer)\n        self.conv1x1 = _ConvBNReLU(inter_channels, out_channels, 1, 1, 0, norm_layer=norm_layer)\n\n    def forward(self, x):\n        x = self.conv7x7(x)\n        x = self.conv3x3_1(x)\n        x = self.conv3x3_2(x)\n        x = self.conv1x1(x)\n\n        return x\n\n\nclass _GlobalAvgPooling(nn.Module):\n    def __init__(self, in_channels, out_channels, norm_layer):\n        super(_GlobalAvgPooling, self).__init__()\n        self.gap = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        size = x.size()[2:]\n        pool = self.gap(x)\n        out = F.interpolate(pool, size, mode=\'bilinear\', align_corners=True)\n        return out\n\n\nclass AttentionRefinmentModule(nn.Module):\n    def __init__(self, in_channels, out_channels, norm_layer=nn.BatchNorm2d):\n        super(AttentionRefinmentModule, self).__init__()\n        self.conv3x3 = _ConvBNReLU(in_channels, out_channels, 3, 1, 1, norm_layer=norm_layer)\n        self.channel_attention = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            _ConvBNReLU(out_channels, out_channels, 1, 1, 0, norm_layer=norm_layer),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.conv3x3(x)\n        attention = self.channel_attention(x)\n        x = x * attention\n        return x\n\n\nclass ContextPath(nn.Module):\n    def __init__(self, norm_layer=nn.BatchNorm2d):\n        super(ContextPath, self).__init__()\n\n        inter_channels = 128\n        self.global_context = _GlobalAvgPooling(512, inter_channels, norm_layer)\n\n        self.arms = nn.ModuleList(\n            [AttentionRefinmentModule(512, inter_channels, norm_layer),\n             AttentionRefinmentModule(256, inter_channels, norm_layer)]\n        )\n        self.refines = nn.ModuleList(\n            [_ConvBNReLU(inter_channels, inter_channels, 3, 1, 1, norm_layer=norm_layer),\n             _ConvBNReLU(inter_channels, inter_channels, 3, 1, 1, norm_layer=norm_layer)]\n        )\n\n    def forward(self, c1, c2, c3, c4):\n        # x = self.conv1(x)\n        # x = self.bn1(x)\n        # x = self.relu(x)\n        # x = self.maxpool(x)\n        # x = self.layer1(x)\n        #\n        # context_blocks = []\n        # context_blocks.append(x)\n        # x = self.layer2(x)\n        # context_blocks.append(x)\n        # c3 = self.layer3(x)\n        # context_blocks.append(c3)\n        # c4 = self.layer4(c3)\n        # context_blocks.append(c4)\n        context_blocks = [c4, c3, c2, c1]\n\n        global_context = self.global_context(c4)\n        last_feature = global_context\n        context_outputs = []\n        for i, (feature, arm, refine) in enumerate(zip(context_blocks[:2], self.arms, self.refines)):\n            feature = arm(feature)\n            feature += last_feature\n            last_feature = F.interpolate(feature, size=context_blocks[i + 1].size()[2:],\n                                         mode=\'bilinear\', align_corners=True)\n            last_feature = refine(last_feature)\n            context_outputs.append(last_feature)\n\n        return context_outputs\n\n\nclass FeatureFusion(nn.Module):\n    def __init__(self, in_channels, out_channels, reduction=1, norm_layer=nn.BatchNorm2d):\n        super(FeatureFusion, self).__init__()\n        self.conv1x1 = _ConvBNReLU(in_channels, out_channels, 1, 1, 0, norm_layer=norm_layer)\n        self.channel_attention = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            _ConvBNReLU(out_channels, out_channels // reduction, 1, 1, 0, norm_layer=norm_layer),\n            _ConvBNReLU(out_channels // reduction, out_channels, 1, 1, 0, norm_layer=norm_layer),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x1, x2):\n        fusion = torch.cat([x1, x2], dim=1)\n        out = self.conv1x1(fusion)\n        attention = self.channel_attention(out)\n        out = out + out * attention\n        return out\n'"
segmentron/models/ccnet.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom ..modules import _FCNHead\nfrom ..modules.cc_attention import CrissCrossAttention\nfrom ..config import cfg\n\n@MODEL_REGISTRY.register()\nclass CCNet(SegBaseModel):\n    r""""""CCNet\n    Reference:\n        Zilong Huang, et al. ""CCNet: Criss-Cross Attention for Semantic Segmentation.""\n        arXiv preprint arXiv:1811.11721 (2018).\n    """"""\n\n    def __init__(self):\n        super(CCNet, self).__init__()\n        self.head = _CCHead(self.nclass, norm_layer=self.norm_layer)\n        if self.aux:\n            self.auxlayer = _FCNHead(1024, self.nclass, norm_layer=self.norm_layer)\n\n        self.__setattr__(\'decoder\', [\'head\', \'auxlayer\'] if self.aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        _, _, c3, c4 = self.base_forward(x)\n        outputs = list()\n        x = self.head(c4)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n        return tuple(outputs)\n\n\nclass _CCHead(nn.Module):\n    def __init__(self, nclass, norm_layer=nn.BatchNorm2d):\n        super(_CCHead, self).__init__()\n        self.rcca = _RCCAModule(2048, 512, norm_layer)\n        self.out = nn.Conv2d(512, nclass, 1)\n\n    def forward(self, x):\n        x = self.rcca(x)\n        x = self.out(x)\n        return x\n\n\nclass _RCCAModule(nn.Module):\n    def __init__(self, in_channels, out_channels, norm_layer):\n        super(_RCCAModule, self).__init__()\n        self.recurrence = cfg.MODEL.CCNET.RECURRENCE\n        inter_channels = in_channels // 4\n        self.conva = nn.Sequential(\n            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(True))\n        self.cca = CrissCrossAttention(inter_channels)\n        self.convb = nn.Sequential(\n            nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(True))\n\n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(in_channels + inter_channels, out_channels, 3, padding=1, bias=False),\n            norm_layer(out_channels),\n            nn.Dropout2d(0.1))\n\n    def forward(self, x):\n        out = self.conva(x)\n        for i in range(self.recurrence):\n            out = self.cca(out)\n        out = self.convb(out)\n        out = torch.cat([x, out], dim=1)\n        out = self.bottleneck(out)\n\n        return out\n'"
segmentron/models/cgnet.py,7,"b'""""""Context Guided Network for Semantic Segmentation""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom ..modules import _ConvBNPReLU, _BNPReLU\nfrom ..config import cfg\n\n__all__ = [\'CGNet\']\n\n\n@MODEL_REGISTRY.register()\nclass CGNet(SegBaseModel):\n    r""""""CGNet\n    Reference:\n        Tianyi Wu, et al. ""CGNet: A Light-weight Context Guided Network for Semantic Segmentation.""\n        arXiv preprint arXiv:1811.08201 (2018).\n    """"""\n\n    def __init__(self):\n        super(CGNet, self).__init__(need_backbone=False)\n\n        stage2_block_num = cfg.MODEL.CGNET.STAGE2_BLOCK_NUM\n        stage3_block_num = cfg.MODEL.CGNET.STAGE3_BLOCK_NUM\n        # stage 1\n        self.stage1_0 = _ConvBNPReLU(3, 32, 3, 2, 1, norm_layer=self.norm_layer)\n        self.stage1_1 = _ConvBNPReLU(32, 32, 3, 1, 1, norm_layer=self.norm_layer)\n        self.stage1_2 = _ConvBNPReLU(32, 32, 3, 1, 1, norm_layer=self.norm_layer)\n\n        self.sample1 = _InputInjection(1)\n        self.sample2 = _InputInjection(2)\n        self.bn_prelu1 = _BNPReLU(32 + 3, norm_layer=self.norm_layer)\n\n        # stage 2\n        self.stage2_0 = ContextGuidedBlock(32 + 3, 64, dilation=2, reduction=8, down=True,\n                                           residual=False, norm_layer=self.norm_layer)\n        self.stage2 = nn.ModuleList()\n        for i in range(0, stage2_block_num - 1):\n            self.stage2.append(ContextGuidedBlock(64, 64, dilation=2, reduction=8, norm_layer=self.norm_layer))\n        self.bn_prelu2 = _BNPReLU(128 + 3, norm_layer=self.norm_layer)\n\n        # stage 3\n        self.stage3_0 = ContextGuidedBlock(128 + 3, 128, dilation=4, reduction=16, down=True,\n                                           residual=False, norm_layer=self.norm_layer)\n        self.stage3 = nn.ModuleList()\n        for i in range(0, stage3_block_num - 1):\n            self.stage3.append(ContextGuidedBlock(128, 128, dilation=4, reduction=16, norm_layer=self.norm_layer))\n        self.bn_prelu3 = _BNPReLU(256, norm_layer=self.norm_layer)\n\n        self.head = nn.Sequential(\n            nn.Dropout2d(0.1, False),\n            nn.Conv2d(256, self.nclass, 1))\n\n        self.__setattr__(\'decoder\', [\'stage1_0\', \'stage1_1\', \'stage1_2\', \'sample1\', \'sample2\',\n                                       \'bn_prelu1\', \'stage2_0\', \'stage2\', \'bn_prelu2\', \'stage3_0\',\n                                       \'stage3\', \'bn_prelu3\', \'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        # stage1\n        out0 = self.stage1_0(x)\n        out0 = self.stage1_1(out0)\n        out0 = self.stage1_2(out0)\n\n        inp1 = self.sample1(x)\n        inp2 = self.sample2(x)\n\n        # stage 2\n        out0_cat = self.bn_prelu1(torch.cat([out0, inp1], dim=1))\n        out1_0 = self.stage2_0(out0_cat)\n        for i, layer in enumerate(self.stage2):\n            if i == 0:\n                out1 = layer(out1_0)\n            else:\n                out1 = layer(out1)\n        out1_cat = self.bn_prelu2(torch.cat([out1, out1_0, inp2], dim=1))\n\n        # stage 3\n        out2_0 = self.stage3_0(out1_cat)\n        for i, layer in enumerate(self.stage3):\n            if i == 0:\n                out2 = layer(out2_0)\n            else:\n                out2 = layer(out2)\n        out2_cat = self.bn_prelu3(torch.cat([out2_0, out2], dim=1))\n\n        outputs = []\n        out = self.head(out2_cat)\n        out = F.interpolate(out, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(out)\n        return tuple(outputs)\n\n\nclass _ChannelWiseConv(nn.Module):\n    def __init__(self, in_channels, out_channels, dilation=1):\n        super(_ChannelWiseConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, 3, 1, dilation, dilation, groups=in_channels, bias=False)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass _FGlo(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super(_FGlo, self).__init__()\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(in_channels, in_channels // reduction),\n            nn.ReLU(True),\n            nn.Linear(in_channels // reduction, in_channels),\n            nn.Sigmoid())\n\n    def forward(self, x):\n        n, c, _, _ = x.size()\n        out = self.gap(x).view(n, c)\n        out = self.fc(out).view(n, c, 1, 1)\n        return x * out\n\n\nclass _InputInjection(nn.Module):\n    def __init__(self, ratio):\n        super(_InputInjection, self).__init__()\n        self.pool = nn.ModuleList()\n        for i in range(0, ratio):\n            self.pool.append(nn.AvgPool2d(3, 2, 1))\n\n    def forward(self, x):\n        for pool in self.pool:\n            x = pool(x)\n        return x\n\n\nclass _ConcatInjection(nn.Module):\n    def __init__(self, in_channels, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_ConcatInjection, self).__init__()\n        self.bn = norm_layer(in_channels)\n        self.prelu = nn.PReLU(in_channels)\n\n    def forward(self, x1, x2):\n        out = torch.cat([x1, x2], dim=1)\n        out = self.bn(out)\n        out = self.prelu(out)\n        return out\n\n\nclass ContextGuidedBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, dilation=2, reduction=16, down=False,\n                 residual=True, norm_layer=nn.BatchNorm2d):\n        super(ContextGuidedBlock, self).__init__()\n        inter_channels = out_channels // 2 if not down else out_channels\n        if down:\n            self.conv = _ConvBNPReLU(in_channels, inter_channels, 3, 2, 1, norm_layer=norm_layer)\n            self.reduce = nn.Conv2d(inter_channels * 2, out_channels, 1, bias=False)\n        else:\n            self.conv = _ConvBNPReLU(in_channels, inter_channels, 1, 1, 0, norm_layer=norm_layer)\n        self.f_loc = _ChannelWiseConv(inter_channels, inter_channels)\n        self.f_sur = _ChannelWiseConv(inter_channels, inter_channels, dilation)\n        self.bn = norm_layer(inter_channels * 2)\n        self.prelu = nn.PReLU(inter_channels * 2)\n        self.f_glo = _FGlo(out_channels, reduction)\n        self.down = down\n        self.residual = residual\n\n    def forward(self, x):\n        out = self.conv(x)\n        loc = self.f_loc(out)\n        sur = self.f_sur(out)\n\n        joi_feat = torch.cat([loc, sur], dim=1)\n        joi_feat = self.prelu(self.bn(joi_feat))\n        if self.down:\n            joi_feat = self.reduce(joi_feat)\n\n        out = self.f_glo(joi_feat)\n        if self.residual:\n            out = out + x\n\n        return out\n'"
segmentron/models/contextnet.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\n\n__all__ = [""ContextNet""]\n\n\n@MODEL_REGISTRY.register()\nclass ContextNet(SegBaseModel):\n    def __init__(self):\n        super(ContextNet, self).__init__(need_backbone=False)\n        self.spatial_detail = Shallow_net(32, 64, 128)\n        self.context_feature_extractor = Deep_net(32, [32, 32, 48, 64, 96, 128],\n                                                  [1, 6, 6, 6, 6, 6], [1, 1, 3, 3, 2, 2])\n        self.feature_fusion = FeatureFusionModule(128, 128, 128)\n        self.classifier = Classifer(128, self.nclass)\n        if self.aux:\n            self.auxlayer = nn.Sequential(\n                nn.Conv2d(128, 32, 3, padding=1, bias=False),\n                nn.BatchNorm2d(32),\n                nn.ReLU(True),\n                nn.Dropout(0.1),\n                nn.Conv2d(32, self.nclass, 1)\n            )\n\n    def forward(self, x):\n        size = x.size()[2:]\n\n        higher_res_features = self.spatial_detail(x)\n\n        x_low = F.interpolate(x, scale_factor=0.25, mode=\'bilinear\', align_corners=True)\n\n        x = self.context_feature_extractor(x_low)\n\n        x = self.feature_fusion(higher_res_features, x)\n\n        x = self.classifier(x)\n\n        outputs = []\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n\n        outputs.append(x)\n        if self.aux:\n            auxout = self.auxlayer(higher_res_features)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n\n        return outputs\n\n\nclass Custom_Conv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, **kwargs):\n        super(Custom_Conv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass DepthSepConv(nn.Module):\n    def __init__(self, dw_channels, out_channels, stride=1, **kwargs):\n        super(DepthSepConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(dw_channels, dw_channels, 3, stride, 1, groups=dw_channels, bias=False),\n            nn.BatchNorm2d(dw_channels),\n            nn.ReLU(True),\n            nn.Conv2d(dw_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass DepthConv(nn.Module):\n    def __init__(self, dw_channels, out_channels, stride=1, **kwargs):\n        super(DepthConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(dw_channels, out_channels, 3, stride, 1, groups=dw_channels, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass LinearBottleneck(nn.Module):\n    def __init__(self, in_channels, out_channels, t=6, stride=2, **kwargs):\n        super(LinearBottleneck, self).__init__()\n        self.use_shortcut = stride == 1 and in_channels == out_channels\n        self.block = nn.Sequential(\n            Custom_Conv(in_channels, in_channels * t, 1),\n            DepthConv(in_channels * t, in_channels * t, stride),\n            nn.Conv2d(in_channels * t, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels)\n        )\n\n    def forward(self, x):\n        out = self.block(x)\n        if self.use_shortcut:\n            out = x + out\n        return out\n\n\nclass Shallow_net(nn.Module):\n    def __init__(self, dw_channels1=32, dw_channels2=64, out_channels=128, **kwargs):\n        super(Shallow_net, self).__init__()\n        self.conv = Custom_Conv(3, dw_channels1, 3, 2)\n        self.dsconv1 = DepthSepConv(dw_channels1, dw_channels2, 2)\n        self.dsconv2 = DepthSepConv(dw_channels2, out_channels, 2)\n        self.dsconv3 = DepthSepConv(out_channels, out_channels, 1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.dsconv1(x)\n        x = self.dsconv2(x)\n        x = self.dsconv3(x)\n        return x\n\n\nclass Deep_net(nn.Module):\n    def __init__(self, in_channels, block_channels,\n                 t, num_blocks, **kwargs):\n        super(Deep_net, self).__init__()\n        self.block_channels = block_channels\n        self.t = t\n        self.num_blocks = num_blocks\n\n        self.conv_ = Custom_Conv(3, in_channels, 3, 2)\n        self.bottleneck1 = self._layer(LinearBottleneck, in_channels, block_channels[0], num_blocks[0], t[0], 1)\n        self.bottleneck2 = self._layer(LinearBottleneck, block_channels[0], block_channels[1], num_blocks[1], t[1], 1)\n        self.bottleneck3 = self._layer(LinearBottleneck, block_channels[1], block_channels[2], num_blocks[2], t[2], 2)\n        self.bottleneck4 = self._layer(LinearBottleneck, block_channels[2], block_channels[3], num_blocks[3], t[3], 2)\n        self.bottleneck5 = self._layer(LinearBottleneck, block_channels[3], block_channels[4], num_blocks[4], t[4], 1)\n        self.bottleneck6 = self._layer(LinearBottleneck, block_channels[4], block_channels[5], num_blocks[5], t[5], 1)\n\n    def _layer(self, block, in_channels, out_channels, blocks, t, stride):\n        layers = []\n        layers.append(block(in_channels, out_channels, t, stride))\n        for i in range(1, blocks):\n            layers.append(block(out_channels, out_channels, t, 1))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv_(x)\n        x = self.bottleneck1(x)\n        x = self.bottleneck2(x)\n        x = self.bottleneck3(x)\n        x = self.bottleneck4(x)\n        x = self.bottleneck5(x)\n        x = self.bottleneck6(x)\n        return x\n\n\nclass FeatureFusionModule(nn.Module):\n    def __init__(self, highter_in_channels, lower_in_channels, out_channels, scale_factor=4, **kwargs):\n        super(FeatureFusionModule, self).__init__()\n        self.scale_factor = scale_factor\n        self.dwconv = DepthConv(lower_in_channels, out_channels, 1)\n        self.conv_lower_res = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, 1),\n            nn.BatchNorm2d(out_channels)\n        )\n        self.conv_higher_res = nn.Sequential(\n            nn.Conv2d(highter_in_channels, out_channels, 1),\n            nn.BatchNorm2d(out_channels)\n        )\n        self.relu = nn.ReLU(True)\n\n    def forward(self, higher_res_feature, lower_res_feature):\n        _, _, h, w = higher_res_feature.size()\n        lower_res_feature = F.interpolate(lower_res_feature, size=(h, w), mode=\'bilinear\', align_corners=True)\n        lower_res_feature = self.dwconv(lower_res_feature)\n        lower_res_feature = self.conv_lower_res(lower_res_feature)\n\n        higher_res_feature = self.conv_higher_res(higher_res_feature)\n        out = higher_res_feature + lower_res_feature\n        return self.relu(out)\n\n\nclass Classifer(nn.Module):\n    def __init__(self, dw_channels, num_classes, stride=1, **kwargs):\n        super(Classifer, self).__init__()\n        self.dsconv1 = DepthSepConv(dw_channels, dw_channels, stride)\n        self.dsconv2 = DepthSepConv(dw_channels, dw_channels, stride)\n        self.conv = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Conv2d(dw_channels, num_classes, 1)\n        )\n\n    def forward(self, x):\n        x = self.dsconv1(x)\n        x = self.dsconv2(x)\n        x = self.conv(x)\n        return x'"
segmentron/models/dabnet.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom ..modules import  _FCNHead\nfrom ..config import cfg\n\n__all__ = [""DABNet""]\n\n\n@MODEL_REGISTRY.register()\nclass DABNet(SegBaseModel):\n    def __init__(self, block_1=3, block_2=6):\n        super().__init__(need_backbone=False)\n        self.init_conv = nn.Sequential(\n            Conv(3, 32, 3, 2, padding=1, bn_acti=True),\n            Conv(32, 32, 3, 1, padding=1, bn_acti=True),\n            Conv(32, 32, 3, 1, padding=1, bn_acti=True),\n        )\n\n        self.down_1 = InputInjection(1)  # down-sample the image 1 times\n        self.down_2 = InputInjection(2)  # down-sample the image 2 times\n        self.down_3 = InputInjection(3)  # down-sample the image 3 times\n\n        self.bn_prelu_1 = BNPReLU(32 + 3)\n\n        # DAB Block 1\n        self.downsample_1 = DownSamplingBlock(32 + 3, 64)\n        self.DAB_Block_1 = nn.Sequential()\n        for i in range(0, block_1):\n            self.DAB_Block_1.add_module(""DAB_Module_1_"" + str(i), DABModule(64, d=2))\n        self.bn_prelu_2 = BNPReLU(128 + 3)\n\n        # DAB Block 2\n        dilation_block_2 = [4, 4, 8, 8, 16, 16]\n        self.downsample_2 = DownSamplingBlock(128 + 3, 128)\n        self.DAB_Block_2 = nn.Sequential()\n        for i in range(0, block_2):\n            self.DAB_Block_2.add_module(""DAB_Module_2_"" + str(i),\n                                        DABModule(128, d=dilation_block_2[i]))\n        self.bn_prelu_3 = BNPReLU(256 + 3)\n\n        self.classifier = nn.Sequential(Conv(259, self.nclass, 1, 1, padding=0))\n\n    def forward(self, input):\n\n        output0 = self.init_conv(input)\n\n        down_1 = self.down_1(input)\n        down_2 = self.down_2(input)\n        down_3 = self.down_3(input)\n\n        output0_cat = self.bn_prelu_1(torch.cat([output0, down_1], 1))\n\n        # DAB Block 1\n        output1_0 = self.downsample_1(output0_cat)\n        output1 = self.DAB_Block_1(output1_0)\n        output1_cat = self.bn_prelu_2(torch.cat([output1, output1_0, down_2], 1))\n\n        # DAB Block 2\n        output2_0 = self.downsample_2(output1_cat)\n        output2 = self.DAB_Block_2(output2_0)\n        output2_cat = self.bn_prelu_3(torch.cat([output2, output2_0, down_3], 1))\n\n        out = self.classifier(output2_cat)\n        out = F.interpolate(out, input.size()[2:], mode=\'bilinear\', align_corners=False)\n\n        outputs = list()\n        outputs.append(out)\n        return outputs\n\n\nclass Conv(nn.Module):\n    def __init__(self, nIn, nOut, kSize, stride, padding, dilation=(1, 1), groups=1, bn_acti=False, bias=False):\n        super().__init__()\n\n        self.bn_acti = bn_acti\n\n        self.conv = nn.Conv2d(nIn, nOut, kernel_size=kSize,\n                              stride=stride, padding=padding,\n                              dilation=dilation, groups=groups, bias=bias)\n\n        if self.bn_acti:\n            self.bn_prelu = BNPReLU(nOut)\n\n    def forward(self, input):\n        output = self.conv(input)\n\n        if self.bn_acti:\n            output = self.bn_prelu(output)\n\n        return output\n\n\nclass BNPReLU(nn.Module):\n    def __init__(self, nIn):\n        super().__init__()\n        self.bn = nn.BatchNorm2d(nIn, eps=1e-3)\n        self.acti = nn.PReLU(nIn)\n\n    def forward(self, input):\n        output = self.bn(input)\n        output = self.acti(output)\n\n        return output\n\n\nclass DABModule(nn.Module):\n    def __init__(self, nIn, d=1, kSize=3, dkSize=3):\n        super().__init__()\n\n        self.bn_relu_1 = BNPReLU(nIn)\n        self.conv3x3 = Conv(nIn, nIn // 2, kSize, 1, padding=1, bn_acti=True)\n\n        self.dconv3x1 = Conv(nIn // 2, nIn // 2, (dkSize, 1), 1,\n                             padding=(1, 0), groups=nIn // 2, bn_acti=True)\n        self.dconv1x3 = Conv(nIn // 2, nIn // 2, (1, dkSize), 1,\n                             padding=(0, 1), groups=nIn // 2, bn_acti=True)\n        self.ddconv3x1 = Conv(nIn // 2, nIn // 2, (dkSize, 1), 1,\n                              padding=(1 * d, 0), dilation=(d, 1), groups=nIn // 2, bn_acti=True)\n        self.ddconv1x3 = Conv(nIn // 2, nIn // 2, (1, dkSize), 1,\n                              padding=(0, 1 * d), dilation=(1, d), groups=nIn // 2, bn_acti=True)\n\n        self.bn_relu_2 = BNPReLU(nIn // 2)\n        self.conv1x1 = Conv(nIn // 2, nIn, 1, 1, padding=0, bn_acti=False)\n\n    def forward(self, input):\n        output = self.bn_relu_1(input)\n        output = self.conv3x3(output)\n\n        br1 = self.dconv3x1(output)\n        br1 = self.dconv1x3(br1)\n        br2 = self.ddconv3x1(output)\n        br2 = self.ddconv1x3(br2)\n\n        output = br1 + br2\n        output = self.bn_relu_2(output)\n        output = self.conv1x1(output)\n\n        return output + input\n\n\nclass DownSamplingBlock(nn.Module):\n    def __init__(self, nIn, nOut):\n        super().__init__()\n        self.nIn = nIn\n        self.nOut = nOut\n\n        if self.nIn < self.nOut:\n            nConv = nOut - nIn\n        else:\n            nConv = nOut\n\n        self.conv3x3 = Conv(nIn, nConv, kSize=3, stride=2, padding=1)\n        self.max_pool = nn.MaxPool2d(2, stride=2)\n        self.bn_prelu = BNPReLU(nOut)\n\n    def forward(self, input):\n        output = self.conv3x3(input)\n\n        if self.nIn < self.nOut:\n            max_pool = self.max_pool(input)\n            output = torch.cat([output, max_pool], 1)\n\n        output = self.bn_prelu(output)\n\n        return output\n\n\nclass InputInjection(nn.Module):\n    def __init__(self, ratio):\n        super().__init__()\n        self.pool = nn.ModuleList()\n        for i in range(0, ratio):\n            self.pool.append(nn.AvgPool2d(3, stride=2, padding=1))\n\n    def forward(self, input):\n        for pool in self.pool:\n            input = pool(input)\n\n        return input'"
segmentron/models/danet.py,2,"b'from __future__ import division\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom ..modules import _FCNHead, PAM_Module, CAM_Module\n\n__all__ = [\'DANet\']\n\n\n@MODEL_REGISTRY.register()\nclass DANet(SegBaseModel):\n    r""""""DANet model from the paper `""Dual Attention Network for Scene Segmentation""\n    <https://arxiv.org/abs/1809.02983.pdf>`\n    """"""\n    def __init__(self):\n        super(DANet, self).__init__()\n        self.head = DANetHead(2048, self.nclass)\n        if self.aux:\n            self.auxlayer = _FCNHead(728, self.nclass)\n        self.__setattr__(\'decoder\', [\'head\', \'auxlayer\'] if self.aux else [\'head\'])\n\n    def forward(self, x):\n        imsize = x.size()[2:]\n        _, _, c3, c4 = self.encoder(x)\n\n        x = self.head(c4)\n        x = list(x)\n        x[0] = F.interpolate(x[0], imsize, mode=\'bilinear\', align_corners=True)\n        x[1] = F.interpolate(x[1], imsize, mode=\'bilinear\', align_corners=True)\n        x[2] = F.interpolate(x[2], imsize, mode=\'bilinear\', align_corners=True)\n\n        outputs = list()\n        outputs.append(x[0])\n        outputs.append(x[1])\n        outputs.append(x[2])\n\n        return tuple(outputs)\n\n\nclass DANetHead(nn.Module):\n    def __init__(self, in_channels, out_channels, norm_layer=nn.BatchNorm2d):\n        super(DANetHead, self).__init__()\n        inter_channels = in_channels // 4\n        self.conv5a = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n                                   norm_layer(inter_channels),\n                                   nn.ReLU())\n        \n        self.conv5c = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n                                   norm_layer(inter_channels),\n                                   nn.ReLU())\n\n        self.sa = PAM_Module(inter_channels)\n        self.sc = CAM_Module(inter_channels)\n        self.conv51 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n                                   norm_layer(inter_channels),\n                                   nn.ReLU())\n        self.conv52 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n                                   norm_layer(inter_channels),\n                                   nn.ReLU())\n\n        self.conv6 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(512, out_channels, 1))\n        self.conv7 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(512, out_channels, 1))\n\n        self.conv8 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(512, out_channels, 1))\n\n    def forward(self, x):\n        feat1 = self.conv5a(x)\n        sa_feat = self.sa(feat1)\n        sa_conv = self.conv51(sa_feat)\n        sa_output = self.conv6(sa_conv)\n\n        feat2 = self.conv5c(x)\n        sc_feat = self.sc(feat2)\n        sc_conv = self.conv52(sc_feat)\n        sc_output = self.conv7(sc_conv)\n\n        feat_sum = sa_conv+sc_conv\n        \n        sasc_output = self.conv8(feat_sum)\n\n        output = [sasc_output]\n        output.append(sa_output)\n        output.append(sc_output)\n        return tuple(output)\n\n'"
segmentron/models/deeplabv3_plus.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom ..modules import _ConvBNReLU, SeparableConv2d, _ASPP, _FCNHead\nfrom ..config import cfg\n\n__all__ = [\'DeepLabV3Plus\']\n\n\n@MODEL_REGISTRY.register(name=\'DeepLabV3_Plus\')\nclass DeepLabV3Plus(SegBaseModel):\n    r""""""DeepLabV3Plus\n    Reference:\n        Chen, Liang-Chieh, et al. ""Encoder-Decoder with Atrous Separable Convolution for Semantic\n        Image Segmentation.""\n    """"""\n    def __init__(self):\n        super(DeepLabV3Plus, self).__init__()\n        if self.backbone.startswith(\'mobilenet\'):\n            c1_channels = 24\n            c4_channels = 320\n        else:\n            c1_channels = 256\n            c4_channels = 2048\n        self.head = _DeepLabHead(self.nclass, c1_channels=c1_channels, c4_channels=c4_channels)\n        if self.aux:\n            self.auxlayer = _FCNHead(728, self.nclass)\n        self.__setattr__(\'decoder\', [\'head\', \'auxlayer\'] if self.aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        c1, _, c3, c4 = self.encoder(x)\n\n        outputs = list()\n        x = self.head(c4, c1)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n\n        outputs.append(x)\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n        return tuple(outputs)\n\n\nclass _DeepLabHead(nn.Module):\n    def __init__(self, nclass, c1_channels=256, c4_channels=2048, norm_layer=nn.BatchNorm2d):\n        super(_DeepLabHead, self).__init__()\n        self.use_aspp = cfg.MODEL.DEEPLABV3_PLUS.USE_ASPP\n        self.use_decoder = cfg.MODEL.DEEPLABV3_PLUS.ENABLE_DECODER\n        last_channels = c4_channels\n        if self.use_aspp:\n            self.aspp = _ASPP(c4_channels, 256)\n            last_channels = 256\n        if self.use_decoder:\n            self.c1_block = _ConvBNReLU(c1_channels, 48, 1, norm_layer=norm_layer)\n            last_channels += 48\n        self.block = nn.Sequential(\n            SeparableConv2d(last_channels, 256, 3, norm_layer=norm_layer, relu_first=False),\n            SeparableConv2d(256, 256, 3, norm_layer=norm_layer, relu_first=False),\n            nn.Conv2d(256, nclass, 1))\n\n    def forward(self, x, c1):\n        size = c1.size()[2:]\n        if self.use_aspp:\n            x = self.aspp(x)\n        if self.use_decoder:\n            x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n            c1 = self.c1_block(c1)\n            return self.block(torch.cat([x, c1], dim=1))\n\n        return self.block(x)\n'"
segmentron/models/denseaspp.py,7,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom .fcn import _FCNHead\n\n__all__ = ['DenseASPP']\n\n\n@MODEL_REGISTRY.register()\nclass DenseASPP(SegBaseModel):\n    def __init__(self):\n        super(DenseASPP, self).__init__()\n\n        in_channels = self.encoder.last_inp_channels\n\n        self.head = _DenseASPPHead(in_channels, self.nclass, norm_layer=self.norm_layer)\n\n        if self.aux:\n            self.auxlayer = _FCNHead(in_channels, self.nclass)\n\n        self.__setattr__('decoder', ['head', 'auxlayer'] if self.aux else ['head'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        _, _, c3, c4 = self.encoder(x)\n        # TODO add densenet as backbone\n        # if self.dilate_scale > 8:\n        #     features = F.interpolate(c4, scale_factor=2, mode='bilinear', align_corners=True)\n        outputs = []\n        x = self.head(c4)\n        x = F.interpolate(x, size, mode='bilinear', align_corners=True)\n        outputs.append(x)\n\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = F.interpolate(auxout, size, mode='bilinear', align_corners=True)\n            outputs.append(auxout)\n        return tuple(outputs)\n\n\nclass _DenseASPPHead(nn.Module):\n    def __init__(self, in_channels, nclass, norm_layer=nn.BatchNorm2d, norm_kwargs=None):\n        super(_DenseASPPHead, self).__init__()\n        self.dense_aspp_block = _DenseASPPBlock(in_channels, 256, 64, norm_layer, norm_kwargs)\n        self.block = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Conv2d(in_channels + 5 * 64, nclass, 1)\n        )\n\n    def forward(self, x):\n        x = self.dense_aspp_block(x)\n        return self.block(x)\n\n\nclass _DenseASPPConv(nn.Sequential):\n    def __init__(self, in_channels, inter_channels, out_channels, atrous_rate,\n                 drop_rate=0.1, norm_layer=nn.BatchNorm2d, norm_kwargs=None):\n        super(_DenseASPPConv, self).__init__()\n        self.add_module('conv1', nn.Conv2d(in_channels, inter_channels, 1)),\n        self.add_module('bn1', norm_layer(inter_channels, **({} if norm_kwargs is None else norm_kwargs))),\n        self.add_module('relu1', nn.ReLU(True)),\n        self.add_module('conv2', nn.Conv2d(inter_channels, out_channels, 3, dilation=atrous_rate, padding=atrous_rate)),\n        self.add_module('bn2', norm_layer(out_channels, **({} if norm_kwargs is None else norm_kwargs))),\n        self.add_module('relu2', nn.ReLU(True)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        features = super(_DenseASPPConv, self).forward(x)\n        if self.drop_rate > 0:\n            features = F.dropout(features, p=self.drop_rate, training=self.training)\n        return features\n\n\nclass _DenseASPPBlock(nn.Module):\n    def __init__(self, in_channels, inter_channels1, inter_channels2,\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None):\n        super(_DenseASPPBlock, self).__init__()\n        self.aspp_3 = _DenseASPPConv(in_channels, inter_channels1, inter_channels2, 3, 0.1,\n                                     norm_layer, norm_kwargs)\n        self.aspp_6 = _DenseASPPConv(in_channels + inter_channels2 * 1, inter_channels1, inter_channels2, 6, 0.1,\n                                     norm_layer, norm_kwargs)\n        self.aspp_12 = _DenseASPPConv(in_channels + inter_channels2 * 2, inter_channels1, inter_channels2, 12, 0.1,\n                                      norm_layer, norm_kwargs)\n        self.aspp_18 = _DenseASPPConv(in_channels + inter_channels2 * 3, inter_channels1, inter_channels2, 18, 0.1,\n                                      norm_layer, norm_kwargs)\n        self.aspp_24 = _DenseASPPConv(in_channels + inter_channels2 * 4, inter_channels1, inter_channels2, 24, 0.1,\n                                      norm_layer, norm_kwargs)\n\n    def forward(self, x):\n        aspp3 = self.aspp_3(x)\n        x = torch.cat([aspp3, x], dim=1)\n\n        aspp6 = self.aspp_6(x)\n        x = torch.cat([aspp6, x], dim=1)\n\n        aspp12 = self.aspp_12(x)\n        x = torch.cat([aspp12, x], dim=1)\n\n        aspp18 = self.aspp_18(x)\n        x = torch.cat([aspp18, x], dim=1)\n\n        aspp24 = self.aspp_24(x)\n        x = torch.cat([aspp24, x], dim=1)\n\n        return x\n"""
segmentron/models/dfanet.py,8,"b'"""""" Deep Feature Aggregation""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .backbones.xception import Enc, FCAttention\nfrom .model_zoo import MODEL_REGISTRY\nfrom ..modules import _ConvBNReLU\n\n__all__ = [\'DFANet\']\n\n\n@MODEL_REGISTRY.register()\nclass DFANet(SegBaseModel):\n    def __init__(self, **kwargs):\n        super(DFANet, self).__init__()\n\n        self.enc2_2 = Enc(240, 48, 4, **kwargs)\n        self.enc3_2 = Enc(144, 96, 6, **kwargs)\n        self.enc4_2 = Enc(288, 192, 4, **kwargs)\n        self.fca_2 = FCAttention(192, **kwargs)\n\n        self.enc2_3 = Enc(240, 48, 4, **kwargs)\n        self.enc3_3 = Enc(144, 96, 6, **kwargs)\n        self.enc3_4 = Enc(288, 192, 4, **kwargs)\n        self.fca_3 = FCAttention(192, **kwargs)\n\n        self.enc2_1_reduce = _ConvBNReLU(48, 32, 1, **kwargs)\n        self.enc2_2_reduce = _ConvBNReLU(48, 32, 1, **kwargs)\n        self.enc2_3_reduce = _ConvBNReLU(48, 32, 1, **kwargs)\n        self.conv_fusion = _ConvBNReLU(32, 32, 1, **kwargs)\n\n        self.fca_1_reduce = _ConvBNReLU(192, 32, 1, **kwargs)\n        self.fca_2_reduce = _ConvBNReLU(192, 32, 1, **kwargs)\n        self.fca_3_reduce = _ConvBNReLU(192, 32, 1, **kwargs)\n        self.conv_out = nn.Conv2d(32, self.nclass, 1)\n\n        self.__setattr__(\'decoder\', [\'enc2_2\', \'enc3_2\', \'enc4_2\', \'fca_2\', \'enc2_3\', \'enc3_3\', \'enc3_4\',\n                                     \'fca_3\', \'enc2_1_reduce\', \'enc2_2_reduce\', \'enc2_3_reduce\', \'conv_fusion\',\n                                     \'fca_1_reduce\', \'fca_2_reduce\', \'fca_3_reduce\', \'conv_out\'])\n\n    def forward(self, x):\n        # backbone\n        stage1_conv1 = self.encoder.conv1(x)\n        stage1_enc2 = self.encoder.enc2(stage1_conv1)\n        stage1_enc3 = self.encoder.enc3(stage1_enc2)\n        stage1_enc4 = self.encoder.enc4(stage1_enc3)\n        stage1_fca = self.encoder.fca(stage1_enc4)\n        stage1_out = F.interpolate(stage1_fca, scale_factor=4, mode=\'bilinear\', align_corners=True)\n\n        # stage2\n        stage2_enc2 = self.enc2_2(torch.cat([stage1_enc2, stage1_out], dim=1))\n        stage2_enc3 = self.enc3_2(torch.cat([stage1_enc3, stage2_enc2], dim=1))\n        stage2_enc4 = self.enc4_2(torch.cat([stage1_enc4, stage2_enc3], dim=1))\n        stage2_fca = self.fca_2(stage2_enc4)\n        stage2_out = F.interpolate(stage2_fca, scale_factor=4, mode=\'bilinear\', align_corners=True)\n\n        # stage3\n        stage3_enc2 = self.enc2_3(torch.cat([stage2_enc2, stage2_out], dim=1))\n        stage3_enc3 = self.enc3_3(torch.cat([stage2_enc3, stage3_enc2], dim=1))\n        stage3_enc4 = self.enc3_4(torch.cat([stage2_enc4, stage3_enc3], dim=1))\n        stage3_fca = self.fca_3(stage3_enc4)\n\n        stage1_enc2_decoder = self.enc2_1_reduce(stage1_enc2)\n        stage2_enc2_docoder = F.interpolate(self.enc2_2_reduce(stage2_enc2), scale_factor=2,\n                                            mode=\'bilinear\', align_corners=True)\n        stage3_enc2_decoder = F.interpolate(self.enc2_3_reduce(stage3_enc2), scale_factor=4,\n                                            mode=\'bilinear\', align_corners=True)\n        fusion = stage1_enc2_decoder + stage2_enc2_docoder + stage3_enc2_decoder\n        fusion = self.conv_fusion(fusion)\n\n        stage1_fca_decoder = F.interpolate(self.fca_1_reduce(stage1_fca), scale_factor=4,\n                                           mode=\'bilinear\', align_corners=True)\n        stage2_fca_decoder = F.interpolate(self.fca_2_reduce(stage2_fca), scale_factor=8,\n                                           mode=\'bilinear\', align_corners=True)\n        stage3_fca_decoder = F.interpolate(self.fca_3_reduce(stage3_fca), scale_factor=16,\n                                           mode=\'bilinear\', align_corners=True)\n        fusion = fusion + stage1_fca_decoder + stage2_fca_decoder + stage3_fca_decoder\n\n        outputs = list()\n        out = self.conv_out(fusion)\n        out = F.interpolate(out, scale_factor=4, mode=\'bilinear\', align_corners=True)\n        outputs.append(out)\n\n        return tuple(outputs)\n'"
segmentron/models/dunet.py,3,"b'""""""Decoders Matter for Semantic Segmentation""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom .fcn import _FCNHead\n\n__all__ = [\'DUNet\']\n\n\n@MODEL_REGISTRY.register()\nclass DUNet(SegBaseModel):\n    """"""Decoders Matter for Semantic Segmentation\n    Reference:\n        Zhi Tian, Tong He, Chunhua Shen, and Youliang Yan.\n        ""Decoders Matter for Semantic Segmentation:\n        Data-Dependent Decoding Enables Flexible Feature Aggregation."" CVPR, 2019\n    """"""\n\n    def __init__(self):\n        super(DUNet, self).__init__()\n        self.head = _DUHead(2144, norm_layer=self.norm_layer)\n        self.dupsample = DUpsampling(256, self.nclass, scale_factor=8)\n        if self.aux:\n            self.auxlayer = _FCNHead(1024, 256, norm_layer=self.norm_layer)\n            self.aux_dupsample = DUpsampling(256, self.nclass, scale_factor=8)\n\n        self.__setattr__(\'decoder\', [\'dupsample\', \'head\', \'auxlayer\', \'aux_dupsample\'] if self.aux else\n                                    [\'dupsample\', \'head\'])\n\n    def forward(self, x):\n        _, c2, c3, c4 = self.encoder(x)\n        outputs = []\n        x = self.head(c2, c3, c4)\n        x = self.dupsample(x)\n        outputs.append(x)\n\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = self.aux_dupsample(auxout)\n            outputs.append(auxout)\n        return tuple(outputs)\n\n\nclass FeatureFused(nn.Module):\n    """"""Module for fused features""""""\n\n    def __init__(self, inter_channels=48, norm_layer=nn.BatchNorm2d):\n        super(FeatureFused, self).__init__()\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(512, inter_channels, 1, bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(True)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(1024, inter_channels, 1, bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, c2, c3, c4):\n        size = c4.size()[2:]\n        c2 = self.conv2(F.interpolate(c2, size, mode=\'bilinear\', align_corners=True))\n        c3 = self.conv3(F.interpolate(c3, size, mode=\'bilinear\', align_corners=True))\n        fused_feature = torch.cat([c4, c3, c2], dim=1)\n        return fused_feature\n\n\nclass _DUHead(nn.Module):\n    def __init__(self, in_channels, norm_layer=nn.BatchNorm2d):\n        super(_DUHead, self).__init__()\n        self.fuse = FeatureFused(norm_layer=norm_layer)\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, 256, 3, padding=1, bias=False),\n            norm_layer(256),\n            nn.ReLU(True),\n            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n            norm_layer(256),\n            nn.ReLU(True)\n        )\n\n    def forward(self, c2, c3, c4):\n        fused_feature = self.fuse(c2, c3, c4)\n        out = self.block(fused_feature)\n        return out\n\n\nclass DUpsampling(nn.Module):\n    """"""DUsampling module""""""\n\n    def __init__(self, in_channels, out_channels, scale_factor=2):\n        super(DUpsampling, self).__init__()\n        self.scale_factor = scale_factor\n        self.conv_w = nn.Conv2d(in_channels, out_channels * scale_factor * scale_factor, 1, bias=False)\n\n    def forward(self, x):\n        x = self.conv_w(x)\n        n, c, h, w = x.size()\n\n        # N, C, H, W --> N, W, H, C\n        x = x.permute(0, 3, 2, 1).contiguous()\n\n        # N, W, H, C --> N, W, H * scale, C // scale\n        x = x.view(n, w, h * self.scale_factor, c // self.scale_factor)\n\n        # N, W, H * scale, C // scale --> N, H * scale, W, C // scale\n        x = x.permute(0, 2, 1, 3).contiguous()\n\n        # N, H * scale, W, C // scale --> N, H * scale, W * scale, C // (scale ** 2)\n        x = x.view(n, h * self.scale_factor, w * self.scale_factor, c // (self.scale_factor * self.scale_factor))\n\n        # N, H * scale, W * scale, C // (scale ** 2) -- > N, C // (scale ** 2), H * scale, W * scale\n        x = x.permute(0, 3, 1, 2)\n\n        return x\n'"
segmentron/models/edanet.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom ..modules import  _FCNHead\nfrom ..config import cfg\n\n__all__ = [""EDANet""]\n\n\n@MODEL_REGISTRY.register()\nclass EDANet(SegBaseModel):\n    def __init__(self):\n        super(EDANet, self).__init__(need_backbone=False)\n\n        self.layers = nn.ModuleList()\n        self.dilation1 = [1, 1, 1, 2, 2]\n        self.dilation2 = [2, 2, 4, 4, 8, 8, 16, 16]\n\n        # DownsamplerBlock1\n        self.layers.append(DownsamplerBlock(3, 15))\n\n        # DownsamplerBlock2\n        self.layers.append(DownsamplerBlock(15, 60))\n\n        # EDA module 1-1 ~ 1-5\n        for i in range(5):\n            self.layers.append(EDABlock(60 + 40 * i, self.dilation1[i]))\n\n        # DownsamplerBlock3\n        self.layers.append(DownsamplerBlock(260, 130))\n\n        # EDA module 2-1 ~ 2-8\n        for j in range(8):\n            self.layers.append(EDABlock(130 + 40 * j, self.dilation2[j]))\n\n        # Projection layer\n        self.project_layer = nn.Conv2d(450, self.nclass, kernel_size=1)\n\n        self.weights_init()\n\n    def weights_init(self):\n        for idx, m in enumerate(self.modules()):\n            classname = m.__class__.__name__\n            if classname.find(\'Conv\') != -1:\n                m.weight.data.normal_(0.0, 0.02)\n            elif classname.find(\'BatchNorm\') != -1:\n                m.weight.data.normal_(1.0, 0.02)\n                m.bias.data.fill_(0)\n\n    def forward(self, x):\n\n        output = x\n\n        for layer in self.layers:\n            output = layer(output)\n\n        output = self.project_layer(output)\n\n        output = F.interpolate(output, size=x.size()[2:], mode=\'bilinear\', align_corners=True)\n        # Bilinear interpolation x8\n        # output = F.interpolate(output, scale_factor=8, mode=\'bilinear\', align_corners=True)\n        #\n        # # Bilinear interpolation x2 (inference only)\n        # if not self.training:\n        #     output = F.interpolate(output, scale_factor=2, mode=\'bilinear\', align_corners=True)\n\n        return tuple([output])\n\n\nclass DownsamplerBlock(nn.Module):\n    def __init__(self, ninput, noutput):\n        super(DownsamplerBlock, self).__init__()\n\n        self.ninput = ninput\n        self.noutput = noutput\n\n        if self.ninput < self.noutput:\n            # Wout > Win\n            self.conv = nn.Conv2d(ninput, noutput - ninput, kernel_size=3, stride=2, padding=1)\n            self.pool = nn.MaxPool2d(2, stride=2)\n        else:\n            # Wout < Win\n            self.conv = nn.Conv2d(ninput, noutput, kernel_size=3, stride=2, padding=1)\n\n        self.bn = nn.BatchNorm2d(noutput)\n\n    def forward(self, x):\n        if self.ninput < self.noutput:\n            output = torch.cat([self.conv(x), self.pool(x)], 1)\n        else:\n            output = self.conv(x)\n\n        output = self.bn(output)\n        return F.relu(output)\n\n\nclass EDABlock(nn.Module):\n    def __init__(self, ninput, dilated, k=40, dropprob=0.02):\n        super(EDABlock, self).__init__()\n\n        # k: growthrate\n        # dropprob:a dropout layer between the last ReLU and the concatenation of each module\n\n        self.conv1x1 = nn.Conv2d(ninput, k, kernel_size=1)\n        self.bn0 = nn.BatchNorm2d(k)\n\n        self.conv3x1_1 = nn.Conv2d(k, k, kernel_size=(3, 1), padding=(1, 0))\n        self.conv1x3_1 = nn.Conv2d(k, k, kernel_size=(1, 3), padding=(0, 1))\n        self.bn1 = nn.BatchNorm2d(k)\n\n        self.conv3x1_2 = nn.Conv2d(k, k, (3, 1), stride=1, padding=(dilated, 0), dilation=dilated)\n        self.conv1x3_2 = nn.Conv2d(k, k, (1, 3), stride=1, padding=(0, dilated), dilation=dilated)\n        self.bn2 = nn.BatchNorm2d(k)\n\n        self.dropout = nn.Dropout2d(dropprob)\n\n    def forward(self, x):\n        input = x\n\n        output = self.conv1x1(x)\n        output = self.bn0(output)\n        output = F.relu(output)\n\n        output = self.conv3x1_1(output)\n        output = self.conv1x3_1(output)\n        output = self.bn1(output)\n        output = F.relu(output)\n\n        output = self.conv3x1_2(output)\n        output = self.conv1x3_2(output)\n        output = self.bn2(output)\n        output = F.relu(output)\n\n        if (self.dropout.p != 0):\n            output = self.dropout(output)\n\n        output = torch.cat([output, input], 1)\n        # print output.size() #check the output\n        return output'"
segmentron/models/encnet.py,5,"b'""""""Context Encoding for Semantic Segmentation""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom .fcn import _FCNHead\nfrom ..config import cfg\n\n__all__ = [\'EncNet\']\n\n\n@MODEL_REGISTRY.register()\nclass EncNet(SegBaseModel):\n    def __init__(self):\n        super(EncNet, self).__init__()\n        se_loss = cfg.MODEL.ENCNET.SE_LOSS\n        lateral = cfg.MODEL.ENCNET.LATERAL\n        self.head = _EncHead(2048, self.nclass, se_loss=se_loss, lateral=lateral,\n                             norm_layer=self.norm_layer)\n        if self.aux:\n            self.auxlayer = _FCNHead(1024, self.nclass, norm_layer=self.norm_layer)\n\n        self.__setattr__(\'decoder\', [\'head\', \'auxlayer\'] if self.aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        features = self.encoder(x)\n\n        x = list(self.head(*features))\n        x[0] = F.interpolate(x[0], size, mode=\'bilinear\', align_corners=True)\n        if self.aux:\n            auxout = self.auxlayer(features[2])\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            x.append(auxout)\n        return tuple(x)\n\n\nclass _EncHead(nn.Module):\n    def __init__(self, in_channels, nclass, se_loss=True, lateral=True,\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None):\n        super(_EncHead, self).__init__()\n        self.lateral = lateral\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(in_channels, 512, 3, padding=1, bias=False),\n            norm_layer(512, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True)\n        )\n        if lateral:\n            self.connect = nn.ModuleList([\n                nn.Sequential(\n                    nn.Conv2d(512, 512, 1, bias=False),\n                    norm_layer(512, **({} if norm_kwargs is None else norm_kwargs)),\n                    nn.ReLU(True)),\n                nn.Sequential(\n                    nn.Conv2d(1024, 512, 1, bias=False),\n                    norm_layer(512, **({} if norm_kwargs is None else norm_kwargs)),\n                    nn.ReLU(True)),\n            ])\n            self.fusion = nn.Sequential(\n                nn.Conv2d(3 * 512, 512, 3, padding=1, bias=False),\n                norm_layer(512, **({} if norm_kwargs is None else norm_kwargs)),\n                nn.ReLU(True)\n            )\n        self.encmodule = EncModule(512, nclass, ncodes=32, se_loss=se_loss,\n                                   norm_layer=norm_layer, norm_kwargs=norm_kwargs)\n        self.conv6 = nn.Sequential(\n            nn.Dropout(0.1, False),\n            nn.Conv2d(512, nclass, 1)\n        )\n\n    def forward(self, *inputs):\n        feat = self.conv5(inputs[-1])\n        if self.lateral:\n            c2 = self.connect[0](inputs[1])\n            c3 = self.connect[1](inputs[2])\n            feat = self.fusion(torch.cat([feat, c2, c3], 1))\n        outs = list(self.encmodule(feat))\n        outs[0] = self.conv6(outs[0])\n        return tuple(outs)\n\n\nclass EncModule(nn.Module):\n    def __init__(self, in_channels, nclass, ncodes=32, se_loss=True,\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None):\n        super(EncModule, self).__init__()\n        self.se_loss = se_loss\n        self.encoding = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, 1, bias=False),\n            norm_layer(in_channels, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True),\n            Encoding(D=in_channels, K=ncodes),\n            nn.BatchNorm1d(ncodes),\n            nn.ReLU(True),\n            Mean(dim=1)\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(in_channels, in_channels),\n            nn.Sigmoid()\n        )\n        if self.se_loss:\n            self.selayer = nn.Linear(in_channels, nclass)\n\n    def forward(self, x):\n        en = self.encoding(x)\n        b, c, _, _ = x.size()\n        gamma = self.fc(en)\n        y = gamma.view(b, c, 1, 1)\n        outputs = [F.relu_(x + x * y)]\n        if self.se_loss:\n            outputs.append(self.selayer(en))\n        return tuple(outputs)\n\n\nclass Encoding(nn.Module):\n    def __init__(self, D, K):\n        super(Encoding, self).__init__()\n        # init codewords and smoothing factor\n        self.D, self.K = D, K\n        self.codewords = nn.Parameter(torch.Tensor(K, D), requires_grad=True)\n        self.scale = nn.Parameter(torch.Tensor(K), requires_grad=True)\n        self.reset_params()\n\n    def reset_params(self):\n        std1 = 1. / ((self.K * self.D) ** (1 / 2))\n        self.codewords.data.uniform_(-std1, std1)\n        self.scale.data.uniform_(-1, 0)\n\n    def forward(self, X):\n        # input X is a 4D tensor\n        assert (X.size(1) == self.D)\n        B, D = X.size(0), self.D\n        if X.dim() == 3:\n            # BxDxN -> BxNxD\n            X = X.transpose(1, 2).contiguous()\n        elif X.dim() == 4:\n            # BxDxHxW -> Bx(HW)xD\n            X = X.view(B, D, -1).transpose(1, 2).contiguous()\n        else:\n            raise RuntimeError(\'Encoding Layer unknown input dims!\')\n        # assignment weights BxNxK\n        A = F.softmax(self.scale_l2(X, self.codewords, self.scale), dim=2)\n        # aggregate\n        E = self.aggregate(A, X, self.codewords)\n        return E\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\' \\\n               + \'N x\' + str(self.D) + \'=>\' + str(self.K) + \'x\' \\\n               + str(self.D) + \')\'\n\n    @staticmethod\n    def scale_l2(X, C, S):\n        S = S.view(1, 1, C.size(0), 1)\n        X = X.unsqueeze(2).expand(X.size(0), X.size(1), C.size(0), C.size(1))\n        C = C.unsqueeze(0).unsqueeze(0)\n        SL = S * (X - C)\n        SL = SL.pow(2).sum(3)\n        return SL\n\n    @staticmethod\n    def aggregate(A, X, C):\n        A = A.unsqueeze(3)\n        X = X.unsqueeze(2).expand(X.size(0), X.size(1), C.size(0), C.size(1))\n        C = C.unsqueeze(0).unsqueeze(0)\n        E = A * (X - C)\n        E = E.sum(1)\n        return E\n\n\nclass Mean(nn.Module):\n    def __init__(self, dim, keep_dim=False):\n        super(Mean, self).__init__()\n        self.dim = dim\n        self.keep_dim = keep_dim\n\n    def forward(self, input):\n        return input.mean(self.dim, self.keep_dim)\n'"
segmentron/models/enet.py,3,"b'""""""Efficient Neural Network""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom ..modules import  _FCNHead\nfrom ..config import cfg\n\n__all__ = [\'ENet\']\n\n\n@MODEL_REGISTRY.register()\nclass ENet(SegBaseModel):\n    """"""Efficient Neural Network""""""\n\n    def __init__(self, **kwargs):\n        super(ENet, self).__init__(need_backbone=False)\n        self.initial = InitialBlock(13, **kwargs)\n\n        self.bottleneck1_0 = Bottleneck(16, 16, 64, downsampling=True, **kwargs)\n        self.bottleneck1_1 = Bottleneck(64, 16, 64, **kwargs)\n        self.bottleneck1_2 = Bottleneck(64, 16, 64, **kwargs)\n        self.bottleneck1_3 = Bottleneck(64, 16, 64, **kwargs)\n        self.bottleneck1_4 = Bottleneck(64, 16, 64, **kwargs)\n\n        self.bottleneck2_0 = Bottleneck(64, 32, 128, downsampling=True, **kwargs)\n        self.bottleneck2_1 = Bottleneck(128, 32, 128, **kwargs)\n        self.bottleneck2_2 = Bottleneck(128, 32, 128, dilation=2, **kwargs)\n        self.bottleneck2_3 = Bottleneck(128, 32, 128, asymmetric=True, **kwargs)\n        self.bottleneck2_4 = Bottleneck(128, 32, 128, dilation=4, **kwargs)\n        self.bottleneck2_5 = Bottleneck(128, 32, 128, **kwargs)\n        self.bottleneck2_6 = Bottleneck(128, 32, 128, dilation=8, **kwargs)\n        self.bottleneck2_7 = Bottleneck(128, 32, 128, asymmetric=True, **kwargs)\n        self.bottleneck2_8 = Bottleneck(128, 32, 128, dilation=16, **kwargs)\n\n        self.bottleneck3_1 = Bottleneck(128, 32, 128, **kwargs)\n        self.bottleneck3_2 = Bottleneck(128, 32, 128, dilation=2, **kwargs)\n        self.bottleneck3_3 = Bottleneck(128, 32, 128, asymmetric=True, **kwargs)\n        self.bottleneck3_4 = Bottleneck(128, 32, 128, dilation=4, **kwargs)\n        self.bottleneck3_5 = Bottleneck(128, 32, 128, **kwargs)\n        self.bottleneck3_6 = Bottleneck(128, 32, 128, dilation=8, **kwargs)\n        self.bottleneck3_7 = Bottleneck(128, 32, 128, asymmetric=True, **kwargs)\n        self.bottleneck3_8 = Bottleneck(128, 32, 128, dilation=16, **kwargs)\n\n        self.bottleneck4_0 = UpsamplingBottleneck(128, 16, 64, **kwargs)\n        self.bottleneck4_1 = Bottleneck(64, 16, 64, **kwargs)\n        self.bottleneck4_2 = Bottleneck(64, 16, 64, **kwargs)\n\n        self.bottleneck5_0 = UpsamplingBottleneck(64, 4, 16, **kwargs)\n        self.bottleneck5_1 = Bottleneck(16, 4, 16, **kwargs)\n\n        self.fullconv = nn.ConvTranspose2d(16, self.nclass, 2, 2, bias=False)\n\n        self.__setattr__(\'decoder\', [\'bottleneck1_0\', \'bottleneck1_1\', \'bottleneck1_2\', \'bottleneck1_3\',\n                                       \'bottleneck1_4\', \'bottleneck2_0\', \'bottleneck2_1\', \'bottleneck2_2\',\n                                       \'bottleneck2_3\', \'bottleneck2_4\', \'bottleneck2_5\', \'bottleneck2_6\',\n                                       \'bottleneck2_7\', \'bottleneck2_8\', \'bottleneck3_1\', \'bottleneck3_2\',\n                                       \'bottleneck3_3\', \'bottleneck3_4\', \'bottleneck3_5\', \'bottleneck3_6\',\n                                       \'bottleneck3_7\', \'bottleneck3_8\', \'bottleneck4_0\', \'bottleneck4_1\',\n                                       \'bottleneck4_2\', \'bottleneck5_0\', \'bottleneck5_1\', \'fullconv\'])\n\n    def forward(self, x):\n        # init\n        x = self.initial(x)\n\n        # stage 1\n        x, max_indices1 = self.bottleneck1_0(x)\n        x = self.bottleneck1_1(x)\n        x = self.bottleneck1_2(x)\n        x = self.bottleneck1_3(x)\n        x = self.bottleneck1_4(x)\n\n        # stage 2\n        x, max_indices2 = self.bottleneck2_0(x)\n        x = self.bottleneck2_1(x)\n        x = self.bottleneck2_2(x)\n        x = self.bottleneck2_3(x)\n        x = self.bottleneck2_4(x)\n        x = self.bottleneck2_5(x)\n        x = self.bottleneck2_6(x)\n        x = self.bottleneck2_7(x)\n        x = self.bottleneck2_8(x)\n\n        # stage 3\n        x = self.bottleneck3_1(x)\n        x = self.bottleneck3_2(x)\n        x = self.bottleneck3_3(x)\n        x = self.bottleneck3_4(x)\n        x = self.bottleneck3_6(x)\n        x = self.bottleneck3_7(x)\n        x = self.bottleneck3_8(x)\n\n        # stage 4\n        x = self.bottleneck4_0(x, max_indices2)\n        x = self.bottleneck4_1(x)\n        x = self.bottleneck4_2(x)\n\n        # stage 5\n        x = self.bottleneck5_0(x, max_indices1)\n        x = self.bottleneck5_1(x)\n\n        # out\n        x = self.fullconv(x)\n        return tuple([x])\n\n\nclass InitialBlock(nn.Module):\n    """"""ENet initial block""""""\n\n    def __init__(self, out_channels, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(InitialBlock, self).__init__()\n        self.conv = nn.Conv2d(3, out_channels, 3, 2, 1, bias=False)\n        self.maxpool = nn.MaxPool2d(2, 2)\n        self.bn = norm_layer(out_channels + 3)\n        self.act = nn.PReLU()\n\n    def forward(self, x):\n        x_conv = self.conv(x)\n        x_pool = self.maxpool(x)\n        x = torch.cat([x_conv, x_pool], dim=1)\n        x = self.bn(x)\n        x = self.act(x)\n        return x\n\n\nclass Bottleneck(nn.Module):\n    """"""Bottlenecks include regular, asymmetric, downsampling, dilated""""""\n\n    def __init__(self, in_channels, inter_channels, out_channels, dilation=1, asymmetric=False,\n                 downsampling=False, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(Bottleneck, self).__init__()\n        self.downsamping = downsampling\n        if downsampling:\n            self.maxpool = nn.MaxPool2d(2, 2, return_indices=True)\n            self.conv_down = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 1, bias=False),\n                norm_layer(out_channels)\n            )\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, inter_channels, 1, bias=False),\n            norm_layer(inter_channels),\n            nn.PReLU()\n        )\n\n        if downsampling:\n            self.conv2 = nn.Sequential(\n                nn.Conv2d(inter_channels, inter_channels, 2, stride=2, bias=False),\n                norm_layer(inter_channels),\n                nn.PReLU()\n            )\n        else:\n            if asymmetric:\n                self.conv2 = nn.Sequential(\n                    nn.Conv2d(inter_channels, inter_channels, (5, 1), padding=(2, 0), bias=False),\n                    nn.Conv2d(inter_channels, inter_channels, (1, 5), padding=(0, 2), bias=False),\n                    norm_layer(inter_channels),\n                    nn.PReLU()\n                )\n            else:\n                self.conv2 = nn.Sequential(\n                    nn.Conv2d(inter_channels, inter_channels, 3, dilation=dilation, padding=dilation, bias=False),\n                    norm_layer(inter_channels),\n                    nn.PReLU()\n                )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(inter_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels),\n            nn.Dropout2d(0.1)\n        )\n        self.act = nn.PReLU()\n\n    def forward(self, x):\n        identity = x\n        if self.downsamping:\n            identity, max_indices = self.maxpool(identity)\n            identity = self.conv_down(identity)\n\n        out = self.conv1(x)\n        out = self.conv2(out)\n        out = self.conv3(out)\n        out = self.act(out + identity)\n\n        if self.downsamping:\n            return out, max_indices\n        else:\n            return out\n\n\nclass UpsamplingBottleneck(nn.Module):\n    """"""upsampling Block""""""\n\n    def __init__(self, in_channels, inter_channels, out_channels, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(UpsamplingBottleneck, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels)\n        )\n        self.upsampling = nn.MaxUnpool2d(2)\n\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, inter_channels, 1, bias=False),\n            norm_layer(inter_channels),\n            nn.PReLU(),\n            nn.ConvTranspose2d(inter_channels, inter_channels, 2, 2, bias=False),\n            norm_layer(inter_channels),\n            nn.PReLU(),\n            nn.Conv2d(inter_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels),\n            nn.Dropout2d(0.1)\n        )\n        self.act = nn.PReLU()\n\n    def forward(self, x, max_indices):\n        out_up = self.conv(x)\n        out_up = self.upsampling(out_up, max_indices)\n\n        out_ext = self.block(x)\n        out = self.act(out_up + out_ext)\n        return out\n\n'"
segmentron/models/espnetv2.py,6,"b'""ESPNetv2: A Light-weight, Power Efficient, and General Purpose for Semantic Segmentation""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom ..modules import _ConvBNPReLU, EESP, _BNPReLU, _FCNHead\nfrom ..config import cfg\n\n\n@MODEL_REGISTRY.register()\nclass ESPNetV2(SegBaseModel):\n    r""""""ESPNetV2\n    Reference:\n        Sachin Mehta, et al. ""ESPNetv2: A Light-weight, Power Efficient, and General Purpose Convolutional Neural Network.""\n        arXiv preprint arXiv:1811.11431 (2018).\n    """"""\n\n    def __init__(self, **kwargs):\n        super(ESPNetV2, self).__init__()\n        self.proj_L4_C = _ConvBNPReLU(256, 128, 1, **kwargs)\n        self.pspMod = nn.Sequential(\n            EESP(256, 128, stride=1, k=4, r_lim=7, **kwargs),\n            _PSPModule(128, 128, **kwargs))\n        self.project_l3 = nn.Sequential(\n            nn.Dropout2d(0.1),\n            nn.Conv2d(128, self.nclass, 1, bias=False))\n        self.act_l3 = _BNPReLU(self.nclass, **kwargs)\n        self.project_l2 = _ConvBNPReLU(64 + self.nclass, self.nclass, 1, **kwargs)\n        self.project_l1 = nn.Sequential(\n            nn.Dropout2d(0.1),\n            nn.Conv2d(32 + self.nclass, self.nclass, 1, bias=False))\n\n        self.__setattr__(\'exclusive\', [\'proj_L4_C\', \'pspMod\', \'project_l3\', \'act_l3\', \'project_l2\', \'project_l1\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        out_l1, out_l2, out_l3, out_l4 = self.encoder(x, seg=True)\n        out_l4_proj = self.proj_L4_C(out_l4)\n        up_l4_to_l3 = F.interpolate(out_l4_proj, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        merged_l3_upl4 = self.pspMod(torch.cat([out_l3, up_l4_to_l3], 1))\n        proj_merge_l3_bef_act = self.project_l3(merged_l3_upl4)\n        proj_merge_l3 = self.act_l3(proj_merge_l3_bef_act)\n        out_up_l3 = F.interpolate(proj_merge_l3, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        merge_l2 = self.project_l2(torch.cat([out_l2, out_up_l3], 1))\n        out_up_l2 = F.interpolate(merge_l2, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        merge_l1 = self.project_l1(torch.cat([out_l1, out_up_l2], 1))\n\n        outputs = list()\n        merge1_l1 = F.interpolate(merge_l1, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        outputs.append(merge1_l1)\n        if self.aux:\n            # different from paper\n            auxout = F.interpolate(proj_merge_l3_bef_act, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n\n        return tuple(outputs)\n\n\n# different from PSPNet\nclass _PSPModule(nn.Module):\n    def __init__(self, in_channels, out_channels=1024, sizes=(1, 2, 4, 8), **kwargs):\n        super(_PSPModule, self).__init__()\n        self.stages = nn.ModuleList(\n            [nn.Conv2d(in_channels, in_channels, 3, 1, 1, groups=in_channels, bias=False) for _ in sizes])\n        self.project = _ConvBNPReLU(in_channels * (len(sizes) + 1), out_channels, 1, 1, **kwargs)\n\n    def forward(self, x):\n        size = x.size()[2:]\n        feats = [x]\n        for stage in self.stages:\n            x = F.avg_pool2d(x, kernel_size=3, stride=2, padding=1)\n            upsampled = F.interpolate(stage(x), size, mode=\'bilinear\', align_corners=True)\n            feats.append(upsampled)\n        return self.project(torch.cat(feats, dim=1))\n'"
segmentron/models/fast_scnn.py,2,"b'""""""Fast Segmentation Convolutional Neural Network""""""\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .model_zoo import MODEL_REGISTRY\nfrom .segbase import SegBaseModel\nfrom ..modules.basic import SeparableConv2d, _ConvBNReLU, InvertedResidual\nfrom ..modules import PyramidPooling, get_norm\nfrom ..config import cfg\n\n__all__ = [\'FastSCNN\']\n\n\n@MODEL_REGISTRY.register()\nclass FastSCNN(SegBaseModel):\n    def __init__(self):\n        super(FastSCNN, self).__init__(need_backbone=False)\n        self.aux = cfg.SOLVER.AUX\n        self.norm_layer = get_norm(cfg.MODEL.BN_TYPE)\n        self.learning_to_downsample = LearningToDownsample(32, 48, 64, norm_layer=self.norm_layer)\n        self.global_feature_extractor = GlobalFeatureExtractor(64, [64, 96, 128], 128, 6, [3, 3, 3],\n                                                               norm_layer=self.norm_layer)\n        self.feature_fusion = FeatureFusionModule(64, 128, 128, norm_layer=self.norm_layer)\n        self.classifier = Classifer(128, self.nclass, norm_layer=self.norm_layer)\n\n        decoder_list = [\'learning_to_downsample\', \'global_feature_extractor\',\n                        \'feature_fusion\', \'classifier\']\n\n        if self.aux:\n            self.auxlayer1 = nn.Sequential(\n                nn.Conv2d(64, 32, 3, padding=1, bias=False),\n                self.norm_layer(32),\n                nn.ReLU(True),\n                nn.Dropout2d(0.1),\n                nn.Conv2d(32, self.nclass, 1)\n            )\n            self.auxlayer2 = nn.Sequential(\n                nn.Conv2d(128, 32, 3, padding=1, bias=False),\n                self.norm_layer(32),\n                nn.ReLU(True),\n                nn.Dropout2d(0.1),\n                nn.Conv2d(32, self.nclass, 1)\n            )\n            decoder_list += [\'auxlayer1\', \'auxlayer2\']\n\n        self.__setattr__(\'decoder\', decoder_list)\n\n    def forward(self, x):\n        size = x.size()[2:]\n        higher_res_features = self.learning_to_downsample(x)\n        lower_res_features = self.global_feature_extractor(higher_res_features)\n        x = self.feature_fusion(higher_res_features, lower_res_features)\n        x = self.classifier(x)\n        outputs = []\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n        if self.aux:\n            auxout1 = self.auxlayer1(higher_res_features)\n            auxout1 = F.interpolate(auxout1, size, mode=\'bilinear\', align_corners=True)\n            auxout2 = self.auxlayer2(lower_res_features)\n            auxout2 = F.interpolate(auxout2, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout1)\n            outputs.append(auxout2)\n        return tuple(outputs)\n\n\nclass LearningToDownsample(nn.Module):\n    """"""Learning to downsample module""""""\n\n    def __init__(self, dw_channels1=32, dw_channels2=48, out_channels=64, norm_layer=nn.BatchNorm2d):\n        super(LearningToDownsample, self).__init__()\n        self.conv = _ConvBNReLU(3, dw_channels1, 3, 2)\n        self.dsconv1 = SeparableConv2d(dw_channels1, dw_channels2, stride=2, relu_first=False, norm_layer=norm_layer)\n        self.dsconv2 = SeparableConv2d(dw_channels2, out_channels, stride=2, relu_first=False, norm_layer=norm_layer)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.dsconv1(x)\n        x = self.dsconv2(x)\n        return x\n\n\nclass GlobalFeatureExtractor(nn.Module):\n    """"""Global feature extractor module""""""\n\n    def __init__(self, in_channels=64, block_channels=(64, 96, 128), out_channels=128,\n                 t=6, num_blocks=(3, 3, 3), norm_layer=nn.BatchNorm2d):\n        super(GlobalFeatureExtractor, self).__init__()\n        self.bottleneck1 = self._make_layer(InvertedResidual, in_channels, block_channels[0], num_blocks[0],\n                                            t, 2, norm_layer=norm_layer)\n        self.bottleneck2 = self._make_layer(InvertedResidual, block_channels[0], block_channels[1],\n                                            num_blocks[1], t, 2, norm_layer=norm_layer)\n        self.bottleneck3 = self._make_layer(InvertedResidual, block_channels[1], block_channels[2],\n                                            num_blocks[2], t, 1, norm_layer=norm_layer)\n        self.ppm = PyramidPooling(block_channels[2], norm_layer=norm_layer)\n        self.out = _ConvBNReLU(block_channels[2] * 2, out_channels, 1, norm_layer=norm_layer)\n\n    def _make_layer(self, block, inplanes, planes, blocks, t=6, stride=1, norm_layer=nn.BatchNorm2d):\n        layers = []\n        layers.append(block(inplanes, planes, stride, t, norm_layer=norm_layer))\n        for i in range(1, blocks):\n            layers.append(block(planes, planes, 1, t, norm_layer=norm_layer))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.bottleneck1(x)\n        x = self.bottleneck2(x)\n        x = self.bottleneck3(x)\n        x = self.ppm(x)\n        x = self.out(x)\n        return x\n\n\nclass FeatureFusionModule(nn.Module):\n    """"""Feature fusion module""""""\n\n    def __init__(self, highter_in_channels, lower_in_channels, out_channels, scale_factor=4, norm_layer=nn.BatchNorm2d):\n        super(FeatureFusionModule, self).__init__()\n        self.scale_factor = scale_factor\n        self.dwconv = _ConvBNReLU(lower_in_channels, out_channels, 1, norm_layer=norm_layer)\n        self.conv_lower_res = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, 1),\n            norm_layer(out_channels)\n        )\n        self.conv_higher_res = nn.Sequential(\n            nn.Conv2d(highter_in_channels, out_channels, 1),\n            norm_layer(out_channels)\n        )\n        self.relu = nn.ReLU(True)\n\n    def forward(self, higher_res_feature, lower_res_feature):\n        lower_res_feature = F.interpolate(lower_res_feature, scale_factor=4, mode=\'bilinear\', align_corners=True)\n        lower_res_feature = self.dwconv(lower_res_feature)\n        lower_res_feature = self.conv_lower_res(lower_res_feature)\n\n        higher_res_feature = self.conv_higher_res(higher_res_feature)\n        out = higher_res_feature + lower_res_feature\n        return self.relu(out)\n\n\nclass Classifer(nn.Module):\n    """"""Classifer""""""\n\n    def __init__(self, dw_channels, num_classes, stride=1, norm_layer=nn.BatchNorm2d):\n        super(Classifer, self).__init__()\n        self.dsconv1 = SeparableConv2d(dw_channels, dw_channels, stride=stride, relu_first=False,\n                                       norm_layer=norm_layer)\n        self.dsconv2 = SeparableConv2d(dw_channels, dw_channels, stride=stride, relu_first=False,\n                                       norm_layer=norm_layer)\n        self.conv = nn.Sequential(\n            nn.Dropout2d(0.1),\n            nn.Conv2d(dw_channels, num_classes, 1)\n        )\n\n    def forward(self, x):\n        x = self.dsconv1(x)\n        x = self.dsconv2(x)\n        x = self.conv(x)\n        return x\n'"
segmentron/models/fcn.py,1,"b""from __future__ import division\n\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom ..modules import _FCNHead\n\n__all__ = ['FCN']\n\n\n@MODEL_REGISTRY.register()\nclass FCN(SegBaseModel):\n    def __init__(self):\n        super(FCN, self).__init__()\n        self.head = _FCNHead(2048, self.nclass)\n        if self.aux:\n            self.auxlayer = _FCNHead(1024, self.nclass)\n\n        self.__setattr__('decoder', ['head', 'auxlayer'] if self.aux else ['head'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        _, _, c3, c4 = self.base_forward(x)\n\n        outputs = []\n        x = self.head(c4)\n        x = F.interpolate(x, size, mode='bilinear', align_corners=True)\n        outputs.append(x)\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = F.interpolate(auxout, size, mode='bilinear', align_corners=True)\n            outputs.append(auxout)\n        return tuple(outputs)\n"""
segmentron/models/fpenet.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom ..modules import _ConvBNReLU, SeparableConv2d, _ASPP, _FCNHead\nfrom ..config import cfg\n\n\n__all__ = [""FPENet""]\n\n\n@MODEL_REGISTRY.register()\nclass FPENet(SegBaseModel):\n    def __init__(self):\n        super(FPENet, self).__init__(need_backbone=False)\n        width = 16\n        scales = 4\n        se = False\n        zero_init_residual = False\n        if self.norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        else:\n            norm_layer = self.norm_layer\n        outplanes = [int(width * 2 ** i) for i in range(3)] # planes=[16,32,64]\n\n        self.block_num = [1, 3, 9]\n        self.dilation = [1, 2, 4, 8]\n\n        self.inplanes = outplanes[0]\n        self.conv1 = nn.Conv2d(3, outplanes[0], kernel_size=3, stride=2, padding=1,bias=False)\n        self.bn1 = norm_layer(outplanes[0])\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(FPEBlock, outplanes[0], self.block_num[0], dilation=self.dilation,\n                                       stride=1, t=1, scales=scales, se=se, norm_layer=norm_layer)\n        self.layer2 = self._make_layer(FPEBlock, outplanes[1], self.block_num[1], dilation=self.dilation,\n                                       stride=2, t=4, scales=scales, se=se, norm_layer=norm_layer)\n        self.layer3 = self._make_layer(FPEBlock, outplanes[2], self.block_num[2], dilation=self.dilation,\n                                       stride=2, t=4, scales=scales, se=se, norm_layer=norm_layer)\n        self.meu1 = MEUModule(64, 32, 64)\n        self.meu2 = MEUModule(64, 16, 32)\n\n        # Projection layer\n        self.project_layer = nn.Conv2d(32, self.nclass, kernel_size = 1)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, FPEBlock):\n                    nn.init.constant_(m.bn3.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, dilation, stride=1, t=1, scales=4, se=False, norm_layer=None):\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        downsample = None\n        if stride != 1 or self.inplanes != planes:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes, stride),\n                norm_layer(planes),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, dilat=dilation, downsample=downsample, stride=stride, t=t, scales=scales, se=se,\n                            norm_layer=norm_layer))\n        self.inplanes = planes\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilat=dilation, scales=scales, se=se, norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        ## stage 1\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x_1 = self.layer1(x)\n\n        ## stage 2\n        x_2_0 = self.layer2[0](x_1)\n        x_2_1 = self.layer2[1](x_2_0)\n        x_2_2 = self.layer2[2](x_2_1)\n        x_2 = x_2_0 + x_2_2\n\n        ## stage 3\n        x_3_0 = self.layer3[0](x_2)\n        x_3_1 = self.layer3[1](x_3_0)\n        x_3_2 = self.layer3[2](x_3_1)\n        x_3_3 = self.layer3[3](x_3_2)\n        x_3_4 = self.layer3[4](x_3_3)\n        x_3_5 = self.layer3[5](x_3_4)\n        x_3_6 = self.layer3[6](x_3_5)\n        x_3_7 = self.layer3[7](x_3_6)\n        x_3_8 = self.layer3[8](x_3_7)\n        x_3 = x_3_0 + x_3_8\n\n\n\n        x2 = self.meu1(x_3, x_2)\n\n        x1 = self.meu2(x2, x_1)\n\n        output = self.project_layer(x1)\n\n        # Bilinear interpolation x2\n        output = F.interpolate(output,scale_factor=2, mode = \'bilinear\', align_corners=True)\n        outputs = list()\n        outputs.append(output)\n        return outputs\n\n\ndef conv3x3(in_planes, out_planes, stride=1, padding=1, dilation=1, groups=1, bias=False):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=padding, dilation=dilation, groups=groups,bias=bias)\n\n\ndef conv1x1(in_planes, out_planes, stride=1, bias=False):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=bias)\n\n\nclass SEModule(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1, padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1, padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input):\n        x = self.avg_pool(input)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return input * x\n\n\nclass FPEBlock(nn.Module):\n\n    def __init__(self, inplanes, outplanes, dilat, downsample=None, stride=1, t=1, scales=4, se=False, norm_layer=None):\n        super(FPEBlock, self).__init__()\n        if inplanes % scales != 0:\n            raise ValueError(\'Planes must be divisible by scales\')\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        bottleneck_planes = inplanes * t\n        self.conv1 = conv1x1(inplanes, bottleneck_planes, stride)\n        self.bn1 = norm_layer(bottleneck_planes)\n        self.conv2 = nn.ModuleList([conv3x3(bottleneck_planes // scales, bottleneck_planes // scales,\n                                            groups=(bottleneck_planes // scales),dilation=dilat[i],\n                                            padding=1*dilat[i]) for i in range(scales)])\n        self.bn2 = nn.ModuleList([norm_layer(bottleneck_planes // scales) for _ in range(scales)])\n        self.conv3 = conv1x1(bottleneck_planes, outplanes)\n        self.bn3 = norm_layer(outplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.se = SEModule(outplanes) if se else None\n        self.downsample = downsample\n        self.stride = stride\n        self.scales = scales\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        xs = torch.chunk(out, self.scales, 1)\n        ys = []\n        for s in range(self.scales):\n            if s == 0:\n                ys.append(self.relu(self.bn2[s](self.conv2[s](xs[s]))))\n            else:\n                ys.append(self.relu(self.bn2[s](self.conv2[s](xs[s] + ys[-1]))))\n        out = torch.cat(ys, 1)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.se is not None:\n            out = self.se(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(identity)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\n\nclass MEUModule(nn.Module):\n    def __init__(self, channels_high, channels_low, channel_out):\n        super(MEUModule, self).__init__()\n\n        self.conv1x1_low = nn.Conv2d(channels_low, channel_out, kernel_size=1, bias=False)\n        self.bn_low = nn.BatchNorm2d(channel_out)\n        self.sa_conv = nn.Conv2d(1, 1, kernel_size=1, bias=False)\n\n        self.conv1x1_high = nn.Conv2d(channels_high, channel_out, kernel_size=1, bias=False)\n        self.bn_high = nn.BatchNorm2d(channel_out)\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.ca_conv = nn.Conv2d(channel_out, channel_out, kernel_size=1, bias=False)\n\n        self.sa_sigmoid = nn.Sigmoid()\n        self.ca_sigmoid = nn.Sigmoid()\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, fms_high, fms_low):\n        """"""\n        :param fms_high:  High level Feature map. Tensor.\n        :param fms_low: Low level Feature map. Tensor.\n        """"""\n        _, _, h, w = fms_low.shape\n\n        #\n        fms_low = self.conv1x1_low(fms_low)\n        fms_low= self.bn_low(fms_low)\n        sa_avg_out = self.sa_sigmoid(self.sa_conv(torch.mean(fms_low, dim=1, keepdim=True)))\n\n        #\n        fms_high = self.conv1x1_high(fms_high)\n        fms_high = self.bn_high(fms_high)\n        ca_avg_out = self.ca_sigmoid(self.relu(self.ca_conv(self.avg_pool(fms_high))))\n\n        #\n        fms_high_up = F.interpolate(fms_high, size=(h,w), mode=\'bilinear\', align_corners=True)\n        fms_sa_att = sa_avg_out * fms_high_up\n        #\n        fms_ca_att = ca_avg_out * fms_low\n\n        out = fms_ca_att + fms_sa_att\n\n        return out'"
segmentron/models/hardnet.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\n\n\n@MODEL_REGISTRY.register()\nclass HardNet(SegBaseModel):\n    def __init__(self):\n        super(HardNet, self).__init__(need_backbone=False)\n\n        first_ch = [16, 24, 32, 48]\n        ch_list = [64, 96, 160, 224, 320]\n        grmul = 1.7\n        gr = [10, 16, 18, 24, 32]\n        n_layers = [4, 4, 8, 8, 8]\n\n        blks = len(n_layers)\n        self.shortcut_layers = []\n\n        self.base = nn.ModuleList([])\n        self.base.append(\n            ConvLayer(in_channels=3, out_channels=first_ch[0], kernel=3,\n                      stride=2))\n        self.base.append(ConvLayer(first_ch[0], first_ch[1], kernel=3))\n        self.base.append(ConvLayer(first_ch[1], first_ch[2], kernel=3, stride=2))\n        self.base.append(ConvLayer(first_ch[2], first_ch[3], kernel=3))\n\n        skip_connection_channel_counts = []\n        ch = first_ch[3]\n        for i in range(blks):\n            blk = HarDBlock(ch, gr[i], grmul, n_layers[i])\n            ch = blk.get_out_ch()\n            skip_connection_channel_counts.append(ch)\n            self.base.append(blk)\n            if i < blks - 1:\n                self.shortcut_layers.append(len(self.base) - 1)\n\n            self.base.append(ConvLayer(ch, ch_list[i], kernel=1))\n            ch = ch_list[i]\n\n            if i < blks - 1:\n                self.base.append(nn.AvgPool2d(kernel_size=2, stride=2))\n\n        cur_channels_count = ch\n        prev_block_channels = ch\n        n_blocks = blks - 1\n        self.n_blocks = n_blocks\n\n        self.transUpBlocks = nn.ModuleList([])\n        self.denseBlocksUp = nn.ModuleList([])\n        self.conv1x1_up = nn.ModuleList([])\n\n        for i in range(n_blocks - 1, -1, -1):\n            self.transUpBlocks.append(TransitionUp(prev_block_channels, prev_block_channels))\n            cur_channels_count = prev_block_channels + skip_connection_channel_counts[i]\n            self.conv1x1_up.append(ConvLayer(cur_channels_count, cur_channels_count // 2, kernel=1))\n            cur_channels_count = cur_channels_count // 2\n\n            blk = HarDBlock(cur_channels_count, gr[i], grmul, n_layers[i])\n\n            self.denseBlocksUp.append(blk)\n            prev_block_channels = blk.get_out_ch()\n            cur_channels_count = prev_block_channels\n\n        self.finalConv = nn.Conv2d(in_channels=cur_channels_count,\n                                   out_channels=self.nclass, kernel_size=1, stride=1,\n                                   padding=0, bias=True)\n\n    def forward(self, x):\n\n        skip_connections = []\n        size_in = x.size()\n\n        for i in range(len(self.base)):\n            x = self.base[i](x)\n            if i in self.shortcut_layers:\n                skip_connections.append(x)\n        out = x\n\n        for i in range(self.n_blocks):\n            skip = skip_connections.pop()\n            out = self.transUpBlocks[i](out, skip, True)\n            out = self.conv1x1_up[i](out)\n            out = self.denseBlocksUp[i](out)\n\n        out = self.finalConv(out)\n\n        out = F.interpolate(\n            out,\n            size=(size_in[2], size_in[3]),\n            mode=""bilinear"",\n            align_corners=True)\n        return [out]\n\n\nclass ConvLayer(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel=3, stride=1):\n        super().__init__()\n        self.add_module(\'conv\', nn.Conv2d(in_channels, out_channels, kernel_size=kernel,\n                                          stride=stride, padding=kernel // 2, bias=False))\n        self.add_module(\'norm\', nn.BatchNorm2d(out_channels))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        return super().forward(x)\n\n\nclass HarDBlock(nn.Module):\n    def get_link(self, layer, base_ch, growth_rate, grmul):\n        if layer == 0:\n            return base_ch, 0, []\n        out_channels = growth_rate\n        link = []\n        for i in range(10):\n            dv = 2 ** i\n            if layer % dv == 0:\n                k = layer - dv\n                link.append(k)\n                if i > 0:\n                    out_channels *= grmul\n        out_channels = int(int(out_channels + 1) / 2) * 2\n        in_channels = 0\n        for i in link:\n            ch, _, _ = self.get_link(i, base_ch, growth_rate, grmul)\n            in_channels += ch\n        return out_channels, in_channels, link\n\n    def get_out_ch(self):\n        return self.out_channels\n\n    def __init__(self, in_channels, growth_rate, grmul, n_layers, keepBase=False, residual_out=False):\n        super().__init__()\n        self.keepBase = keepBase\n        self.links = []\n        layers_ = []\n        self.out_channels = 0  # if upsample else in_channels\n        for i in range(n_layers):\n            outch, inch, link = self.get_link(i + 1, in_channels, growth_rate, grmul)\n            self.links.append(link)\n            use_relu = residual_out\n            layers_.append(ConvLayer(inch, outch))\n            if (i % 2 == 0) or (i == n_layers - 1):\n                self.out_channels += outch\n\n        self.layers = nn.ModuleList(layers_)\n\n    def forward(self, x):\n        layers_ = [x]\n        for layer in range(len(self.layers)):\n            link = self.links[layer]\n            tin = []\n            for i in link:\n                tin.append(layers_[i])\n            if len(tin) > 1:\n                x = torch.cat(tin, 1)\n            else:\n                x = tin[0]\n            out = self.layers[layer](x)\n            layers_.append(out)\n        t = len(layers_)\n        out_ = []\n        for i in range(t):\n            if (i == 0 and self.keepBase) or \\\n                    (i == t - 1) or (i % 2 == 1):\n                out_.append(layers_[i])\n        out = torch.cat(out_, 1)\n        return out\n\n\nclass TransitionUp(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n    def forward(self, x, skip, concat=True):\n        out = F.interpolate(\n            x,\n            size=(skip.size(2), skip.size(3)),\n            mode=""bilinear"",\n            align_corners=True,\n        )\n        if concat:\n            out = torch.cat([out, skip], 1)\n\n        return out\n'"
segmentron/models/hrnet_seg.py,4,"b""# this code is heavily based on https://github.com/HRNet\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch._utils\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom ..config import cfg\n\n\n@MODEL_REGISTRY.register(name='HRNet')\nclass HighResolutionNet(SegBaseModel):\n    def __init__(self):\n        super(HighResolutionNet, self).__init__()\n        self.hrnet_head = _HRNetHead(self.nclass, self.encoder.last_inp_channels)\n        self.__setattr__('decoder', ['hrnet_head'])\n\n    def forward(self, x):\n        shape = x.shape[2:]\n        x = self.encoder(x)\n        x = self.hrnet_head(x)\n        x = F.interpolate(x, size=shape, mode='bilinear', align_corners=False)\n        return [x]\n\n\nclass _HRNetHead(nn.Module):\n    def __init__(self, nclass, last_inp_channels, norm_layer=nn.BatchNorm2d):\n        super(_HRNetHead, self).__init__()\n\n        self.last_layer = nn.Sequential(\n            nn.Conv2d(\n                in_channels=last_inp_channels,\n                out_channels=last_inp_channels,\n                kernel_size=1,\n                stride=1,\n                padding=0),\n\n            norm_layer(last_inp_channels),\n            nn.ReLU(inplace=False),\n            nn.Conv2d(\n                in_channels=last_inp_channels,\n                out_channels=nclass,\n                kernel_size=cfg.MODEL.HRNET.FINAL_CONV_KERNEL,\n                stride=1,\n                padding=1 if cfg.MODEL.HRNET.FINAL_CONV_KERNEL == 3 else 0)\n        )\n\n    def forward(self, x):\n        # Upsampling\n        x0_h, x0_w = x[0].size(2), x[0].size(3)\n        x1 = F.interpolate(x[1], size=(x0_h, x0_w), mode='bilinear', align_corners=False)\n        x2 = F.interpolate(x[2], size=(x0_h, x0_w), mode='bilinear', align_corners=False)\n        x3 = F.interpolate(x[3], size=(x0_h, x0_w), mode='bilinear', align_corners=False)\n\n        x = torch.cat([x[0], x1, x2, x3], 1)\n        x = self.last_layer(x)\n        return x\n"""
segmentron/models/icnet.py,2,"b'""""""Image Cascade Network""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom ..modules.basic import _ConvBNReLU\nfrom ..config import cfg\n\n__all__ = [\'ICNet\']\n\n\n@MODEL_REGISTRY.register()\nclass ICNet(SegBaseModel):\n    """"""Image Cascade Network""""""\n\n    def __init__(self):\n        super(ICNet, self).__init__()\n        self.conv_sub1 = nn.Sequential(\n            _ConvBNReLU(3, 32, 3, 2),\n            _ConvBNReLU(32, 32, 3, 2),\n            _ConvBNReLU(32, 64, 3, 2)\n        )\n\n        self.head = _ICHead(self.nclass)\n        self.__setattr__(\'decoder\', [\'conv_sub1\', \'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        # sub 1\n        x_sub1 = self.conv_sub1(x)\n\n        # sub 2\n        x_sub2 = F.interpolate(x, scale_factor=0.5, mode=\'bilinear\', align_corners=True)\n        _, x_sub2, _, _ = self.encoder(x_sub2)\n\n        # sub 4\n        x_sub4 = F.interpolate(x, scale_factor=0.25, mode=\'bilinear\', align_corners=True)\n        _, _, _, x_sub4 = self.encoder(x_sub4)\n\n        outputs = self.head(x_sub1, x_sub2, x_sub4, size)\n\n        return tuple(outputs)\n\n\nclass _ICHead(nn.Module):\n    def __init__(self, nclass, norm_layer=nn.BatchNorm2d):\n        super(_ICHead, self).__init__()\n        scale = cfg.MODEL.BACKBONE_SCALE\n        self.cff_12 = CascadeFeatureFusion(int(512 * scale), 64, 128, nclass, norm_layer)\n        self.cff_24 = CascadeFeatureFusion(int(2048 * scale), int(512 * scale), 128, nclass, norm_layer)\n        self.conv_cls = nn.Conv2d(128, nclass, 1, bias=False)\n\n    def forward(self, x_sub1, x_sub2, x_sub4, size):\n        outputs = list()\n        x_cff_24, x_24_cls = self.cff_24(x_sub4, x_sub2)\n        outputs.append(x_24_cls)\n        x_cff_12, x_12_cls = self.cff_12(x_sub2, x_sub1)\n        outputs.append(x_12_cls)\n\n        up_x2 = F.interpolate(x_cff_12, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        up_x2 = self.conv_cls(up_x2)\n        outputs.append(up_x2)\n\n        up_x8 = F.interpolate(up_x2, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(up_x8)\n        # 1 -> 1/4 -> 1/8 -> 1/16\n        outputs.reverse()\n\n        return outputs\n\n\nclass CascadeFeatureFusion(nn.Module):\n    """"""CFF Unit""""""\n\n    def __init__(self, low_channels, high_channels, out_channels, nclass, norm_layer=nn.BatchNorm2d):\n        super(CascadeFeatureFusion, self).__init__()\n        self.conv_low = nn.Sequential(\n            nn.Conv2d(low_channels, out_channels, 3, padding=2, dilation=2, bias=False),\n            norm_layer(out_channels)\n        )\n        self.conv_high = nn.Sequential(\n            nn.Conv2d(high_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels)\n        )\n        self.conv_low_cls = nn.Conv2d(out_channels, nclass, 1, bias=False)\n\n    def forward(self, x_low, x_high):\n        x_low = F.interpolate(x_low, size=x_high.size()[2:], mode=\'bilinear\', align_corners=True)\n        x_low = self.conv_low(x_low)\n        x_high = self.conv_high(x_high)\n        x = x_low + x_high\n        x = F.relu(x, inplace=True)\n        x_low_cls = self.conv_low_cls(x_low)\n\n        return x, x_low_cls\n'"
segmentron/models/lednet.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom ..modules import _ConvBNReLU\n\n__all__ = [\'LEDNet\']\n\n\n@MODEL_REGISTRY.register()\nclass LEDNet(SegBaseModel):\n    r""""""LEDNet\n    Reference:\n        Yu Wang, et al. ""LEDNet: A Lightweight Encoder-Decoder Network for Real-Time Semantic Segmentation.""\n        arXiv preprint arXiv:1905.02423 (2019).\n    """"""\n\n    def __init__(self):\n        super(LEDNet, self).__init__(need_backbone=False)\n        self.encoder = nn.Sequential(\n            Downsampling(3, 32),\n            SSnbt(32, norm_layer=self.norm_layer),\n            SSnbt(32, norm_layer=self.norm_layer),\n            SSnbt(32, norm_layer=self.norm_layer),\n            Downsampling(32, 64),\n            SSnbt(64, norm_layer=self.norm_layer),\n            SSnbt(64, norm_layer=self.norm_layer),\n            Downsampling(64, 128),\n            SSnbt(128, norm_layer=self.norm_layer),\n            SSnbt(128, 2, norm_layer=self.norm_layer),\n            SSnbt(128, 5, norm_layer=self.norm_layer),\n            SSnbt(128, 9, norm_layer=self.norm_layer),\n            SSnbt(128, 2, norm_layer=self.norm_layer),\n            SSnbt(128, 5, norm_layer=self.norm_layer),\n            SSnbt(128, 9, norm_layer=self.norm_layer),\n            SSnbt(128, 17, norm_layer=self.norm_layer),\n        )\n        self.head = APNModule(128, self.nclass, norm_layer=self.norm_layer)\n\n        self.__setattr__(\'decoder\', [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        x = self.encoder(x)\n        x = self.head(x)\n        outputs = list()\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n\n        return tuple(outputs)\n\n\nclass Downsampling(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(Downsampling, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels // 2, 3, 2, 2, bias=False)\n        self.conv2 = nn.Conv2d(in_channels, out_channels // 2, 3, 2, 2, bias=False)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n\n    def forward(self, x):\n        x1 = self.conv1(x)\n        x1 = self.pool(x1)\n\n        x2 = self.conv2(x)\n        x2 = self.pool(x2)\n\n        return torch.cat([x1, x2], dim=1)\n\n\nclass SSnbt(nn.Module):\n    def __init__(self, in_channels, dilation=1, norm_layer=nn.BatchNorm2d):\n        super(SSnbt, self).__init__()\n        inter_channels = in_channels // 2\n        self.branch1 = nn.Sequential(\n            nn.Conv2d(inter_channels, inter_channels, (3, 1), padding=(1, 0), bias=False),\n            nn.ReLU(True),\n            nn.Conv2d(inter_channels, inter_channels, (1, 3), padding=(0, 1), bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(True),\n            nn.Conv2d(inter_channels, inter_channels, (3, 1), padding=(dilation, 0), dilation=(dilation, 1),\n                      bias=False),\n            nn.ReLU(True),\n            nn.Conv2d(inter_channels, inter_channels, (1, 3), padding=(0, dilation), dilation=(1, dilation),\n                      bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(True))\n\n        self.branch2 = nn.Sequential(\n            nn.Conv2d(inter_channels, inter_channels, (1, 3), padding=(0, 1), bias=False),\n            nn.ReLU(True),\n            nn.Conv2d(inter_channels, inter_channels, (3, 1), padding=(1, 0), bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(True),\n            nn.Conv2d(inter_channels, inter_channels, (1, 3), padding=(0, dilation), dilation=(1, dilation),\n                      bias=False),\n            nn.ReLU(True),\n            nn.Conv2d(inter_channels, inter_channels, (3, 1), padding=(dilation, 0), dilation=(dilation, 1),\n                      bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(True))\n\n        self.relu = nn.ReLU(True)\n\n    @staticmethod\n    def channel_shuffle(x, groups):\n        n, c, h, w = x.size()\n\n        channels_per_group = c // groups\n        x = x.view(n, groups, channels_per_group, h, w)\n        x = torch.transpose(x, 1, 2).contiguous()\n        x = x.view(n, -1, h, w)\n\n        return x\n\n    def forward(self, x):\n        # channels split\n        x1, x2 = x.split(x.size(1) // 2, 1)\n\n        x1 = self.branch1(x1)\n        x2 = self.branch2(x2)\n\n        out = torch.cat([x1, x2], dim=1)\n        out = self.relu(out + x)\n        out = self.channel_shuffle(out, groups=2)\n\n        return out\n\n\nclass APNModule(nn.Module):\n    def __init__(self, in_channels, nclass, norm_layer=nn.BatchNorm2d):\n        super(APNModule, self).__init__()\n        self.conv1 = _ConvBNReLU(in_channels, in_channels, 3, 2, 1, norm_layer=norm_layer)\n        self.conv2 = _ConvBNReLU(in_channels, in_channels, 5, 2, 2, norm_layer=norm_layer)\n        self.conv3 = _ConvBNReLU(in_channels, in_channels, 7, 2, 3, norm_layer=norm_layer)\n        self.level1 = _ConvBNReLU(in_channels, nclass, 1, norm_layer=norm_layer)\n        self.level2 = _ConvBNReLU(in_channels, nclass, 1, norm_layer=norm_layer)\n        self.level3 = _ConvBNReLU(in_channels, nclass, 1, norm_layer=norm_layer)\n        self.level4 = _ConvBNReLU(in_channels, nclass, 1, norm_layer=norm_layer)\n        self.level5 = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            _ConvBNReLU(in_channels, nclass, 1))\n\n    def forward(self, x):\n        w, h = x.size()[2:]\n        branch3 = self.conv1(x)\n        branch2 = self.conv2(branch3)\n        branch1 = self.conv3(branch2)\n\n        out = self.level1(branch1)\n        out = F.interpolate(out, ((w + 3) // 4, (h + 3) // 4), mode=\'bilinear\', align_corners=True)\n        out = self.level2(branch2) + out\n        out = F.interpolate(out, ((w + 1) // 2, (h + 1) // 2), mode=\'bilinear\', align_corners=True)\n        out = self.level3(branch3) + out\n        out = F.interpolate(out, (w, h), mode=\'bilinear\', align_corners=True)\n        out = self.level4(x) * out\n        out = self.level5(x) + out\n        return out\n'"
segmentron/models/model_zoo.py,2,"b'import logging\nimport torch\n\nfrom collections import OrderedDict\nfrom segmentron.utils.registry import Registry\nfrom ..config import cfg\n\nMODEL_REGISTRY = Registry(""MODEL"")\nMODEL_REGISTRY.__doc__ = """"""\nRegistry for segment model, i.e. the whole model.\n\nThe registered object will be called with `obj()`\nand expected to return a `nn.Module` object.\n""""""\n\n\ndef get_segmentation_model():\n    """"""\n    Built the whole model, defined by `cfg.MODEL.META_ARCHITECTURE`.\n    """"""\n    model_name = cfg.MODEL.MODEL_NAME\n    model = MODEL_REGISTRY.get(model_name)()\n    load_model_pretrain(model)\n    return model\n\n\ndef load_model_pretrain(model):\n    if cfg.PHASE == \'train\':\n        if cfg.TRAIN.PRETRAINED_MODEL_PATH:\n            logging.info(\'load pretrained model from {}\'.format(cfg.TRAIN.PRETRAINED_MODEL_PATH))\n            state_dict_to_load = torch.load(cfg.TRAIN.PRETRAINED_MODEL_PATH)\n            keys_wrong_shape = []\n            state_dict_suitable = OrderedDict()\n            state_dict = model.state_dict()\n            for k, v in state_dict_to_load.items():\n                if v.shape == state_dict[k].shape:\n                    state_dict_suitable[k] = v\n                else:\n                    keys_wrong_shape.append(k)\n            logging.info(\'Shape unmatched weights: {}\'.format(keys_wrong_shape))\n            msg = model.load_state_dict(state_dict_suitable, strict=False)\n            logging.info(msg)\n    else:\n        if cfg.TEST.TEST_MODEL_PATH:\n            logging.info(\'load test model from {}\'.format(cfg.TEST.TEST_MODEL_PATH))\n            msg = model.load_state_dict(torch.load(cfg.TEST.TEST_MODEL_PATH), strict=False)\n            logging.info(msg)'"
segmentron/models/ocnet.py,11,"b'import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nfrom .segbase import SegBaseModel\r\nfrom .model_zoo import MODEL_REGISTRY\r\nfrom ..modules import _FCNHead\r\nfrom ..config import cfg\r\n\r\n__all__ = [\'OCNet\']\r\n\r\n\r\n@MODEL_REGISTRY.register()\r\nclass OCNet(SegBaseModel):\r\n    r""""""OCNet\r\n    Reference:\r\n        Yuhui Yuan, Jingdong Wang. ""OCNet: Object Context Network for Scene Parsing.""\r\n        arXiv preprint arXiv:1809.00916 (2018).\r\n    """"""\r\n\r\n    def __init__(self):\r\n        super(OCNet, self).__init__()\r\n        oc_arch = cfg.MODEL.OCNet.OC_ARCH\r\n        self.head = _OCHead(self.nclass, oc_arch, norm_layer=self.norm_layer)\r\n        if self.aux:\r\n            self.auxlayer = _FCNHead(1024, self.nclass, norm_layer=self.norm_layer)\r\n\r\n        self.__setattr__(\'decoder\', [\'head\', \'auxlayer\'] if self.aux else [\'head\'])\r\n\r\n    def forward(self, x):\r\n        size = x.size()[2:]\r\n        _, _, c3, c4 = self.base_forward(x)\r\n        outputs = []\r\n        x = self.head(c4)\r\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\r\n        outputs.append(x)\r\n\r\n        if self.aux:\r\n            auxout = self.auxlayer(c3)\r\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\r\n            outputs.append(auxout)\r\n        return tuple(outputs)\r\n\r\n\r\nclass _OCHead(nn.Module):\r\n    def __init__(self, nclass, oc_arch, norm_layer=nn.BatchNorm2d, **kwargs):\r\n        super(_OCHead, self).__init__()\r\n        if oc_arch == \'base\':\r\n            self.context = nn.Sequential(\r\n                nn.Conv2d(2048, 512, 3, 1, padding=1, bias=False),\r\n                norm_layer(512),\r\n                nn.ReLU(True),\r\n                BaseOCModule(512, 512, 256, 256, scales=([1]), norm_layer=norm_layer, **kwargs))\r\n        elif oc_arch == \'pyramid\':\r\n            self.context = nn.Sequential(\r\n                nn.Conv2d(2048, 512, 3, 1, padding=1, bias=False),\r\n                norm_layer(512),\r\n                nn.ReLU(True),\r\n                PyramidOCModule(512, 512, 256, 512, scales=([1, 2, 3, 6]), norm_layer=norm_layer, **kwargs))\r\n        elif oc_arch == \'asp\':\r\n            self.context = ASPOCModule(2048, 512, 256, 512, norm_layer=norm_layer, **kwargs)\r\n        else:\r\n            raise ValueError(""Unknown OC architecture!"")\r\n\r\n        self.out = nn.Conv2d(512, nclass, 1)\r\n\r\n    def forward(self, x):\r\n        x = self.context(x)\r\n        return self.out(x)\r\n\r\n\r\nclass BaseAttentionBlock(nn.Module):\r\n    """"""The basic implementation for self-attention block/non-local block.""""""\r\n\r\n    def __init__(self, in_channels, out_channels, key_channels, value_channels,\r\n                 scale=1, norm_layer=nn.BatchNorm2d, **kwargs):\r\n        super(BaseAttentionBlock, self).__init__()\r\n        self.scale = scale\r\n        self.key_channels = key_channels\r\n        self.value_channels = value_channels\r\n        if scale > 1:\r\n            self.pool = nn.MaxPool2d(scale)\r\n\r\n        self.f_value = nn.Conv2d(in_channels, value_channels, 1)\r\n        self.f_key = nn.Sequential(\r\n            nn.Conv2d(in_channels, key_channels, 1),\r\n            norm_layer(key_channels),\r\n            nn.ReLU(True)\r\n        )\r\n        self.f_query = self.f_key\r\n        self.W = nn.Conv2d(value_channels, out_channels, 1)\r\n        nn.init.constant_(self.W.weight, 0)\r\n        nn.init.constant_(self.W.bias, 0)\r\n\r\n    def forward(self, x):\r\n        batch_size, c, w, h = x.size()\r\n        if self.scale > 1:\r\n            x = self.pool(x)\r\n\r\n        value = self.f_value(x).view(batch_size, self.value_channels, -1).permute(0, 2, 1)\r\n        query = self.f_query(x).view(batch_size, self.key_channels, -1).permute(0, 2, 1)\r\n        key = self.f_key(x).view(batch_size, self.key_channels, -1)\r\n\r\n        sim_map = torch.bmm(query, key) * (self.key_channels ** -.5)\r\n        sim_map = F.softmax(sim_map, dim=-1)\r\n\r\n        context = torch.bmm(sim_map, value).permute(0, 2, 1).contiguous()\r\n        context = context.view(batch_size, self.value_channels, *x.size()[2:])\r\n        context = self.W(context)\r\n        if self.scale > 1:\r\n            context = F.interpolate(context, size=(w, h), mode=\'bilinear\', align_corners=True)\r\n\r\n        return context\r\n\r\n\r\nclass BaseOCModule(nn.Module):\r\n    """"""Base-OC""""""\r\n\r\n    def __init__(self, in_channels, out_channels, key_channels, value_channels,\r\n                 scales=([1]), norm_layer=nn.BatchNorm2d, concat=True, **kwargs):\r\n        super(BaseOCModule, self).__init__()\r\n        self.stages = nn.ModuleList([\r\n            BaseAttentionBlock(in_channels, out_channels, key_channels, value_channels, scale, norm_layer, **kwargs)\r\n            for scale in scales])\r\n        in_channels = in_channels * 2 if concat else in_channels\r\n        self.project = nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 1),\r\n            norm_layer(out_channels),\r\n            nn.ReLU(True),\r\n            nn.Dropout2d(0.05)\r\n        )\r\n        self.concat = concat\r\n\r\n    def forward(self, x):\r\n        priors = [stage(x) for stage in self.stages]\r\n        context = priors[0]\r\n        for i in range(1, len(priors)):\r\n            context += priors[i]\r\n        if self.concat:\r\n            context = torch.cat([context, x], 1)\r\n        out = self.project(context)\r\n        return out\r\n\r\n\r\nclass PyramidAttentionBlock(nn.Module):\r\n    """"""The basic implementation for pyramid self-attention block/non-local block""""""\r\n\r\n    def __init__(self, in_channels, out_channels, key_channels, value_channels,\r\n                 scale=1, norm_layer=nn.BatchNorm2d, **kwargs):\r\n        super(PyramidAttentionBlock, self).__init__()\r\n        self.scale = scale\r\n        self.value_channels = value_channels\r\n        self.key_channels = key_channels\r\n\r\n        self.f_value = nn.Conv2d(in_channels, value_channels, 1)\r\n        self.f_key = nn.Sequential(\r\n            nn.Conv2d(in_channels, key_channels, 1),\r\n            norm_layer(key_channels),\r\n            nn.ReLU(True)\r\n        )\r\n        self.f_query = self.f_key\r\n        self.W = nn.Conv2d(value_channels, out_channels, 1)\r\n        nn.init.constant_(self.W.weight, 0)\r\n        nn.init.constant_(self.W.bias, 0)\r\n\r\n    def forward(self, x):\r\n        batch_size, c, w, h = x.size()\r\n\r\n        local_x = list()\r\n        local_y = list()\r\n        step_w, step_h = w // self.scale, h // self.scale\r\n        for i in range(self.scale):\r\n            for j in range(self.scale):\r\n                start_x, start_y = step_w * i, step_h * j\r\n                end_x, end_y = min(start_x + step_w, w), min(start_y + step_h, h)\r\n                if i == (self.scale - 1):\r\n                    end_x = w\r\n                if j == (self.scale - 1):\r\n                    end_y = h\r\n                local_x += [start_x, end_x]\r\n                local_y += [start_y, end_y]\r\n\r\n        value = self.f_value(x)\r\n        query = self.f_query(x)\r\n        key = self.f_key(x)\r\n\r\n        local_list = list()\r\n        local_block_cnt = (self.scale ** 2) * 2\r\n        for i in range(0, local_block_cnt, 2):\r\n            value_local = value[:, :, local_x[i]:local_x[i + 1], local_y[i]:local_y[i + 1]]\r\n            query_local = query[:, :, local_x[i]:local_x[i + 1], local_y[i]:local_y[i + 1]]\r\n            key_local = key[:, :, local_x[i]:local_x[i + 1], local_y[i]:local_y[i + 1]]\r\n\r\n            w_local, h_local = value_local.size(2), value_local.size(3)\r\n            value_local = value_local.contiguous().view(batch_size, self.value_channels, -1).permute(0, 2, 1)\r\n            query_local = query_local.contiguous().view(batch_size, self.key_channels, -1).permute(0, 2, 1)\r\n            key_local = key_local.contiguous().view(batch_size, self.key_channels, -1)\r\n\r\n            sim_map = torch.bmm(query_local, key_local) * (self.key_channels ** -.5)\r\n            sim_map = F.softmax(sim_map, dim=-1)\r\n\r\n            context_local = torch.bmm(sim_map, value_local).permute(0, 2, 1).contiguous()\r\n            context_local = context_local.view(batch_size, self.value_channels, w_local, h_local)\r\n            local_list.append(context_local)\r\n\r\n        context_list = list()\r\n        for i in range(0, self.scale):\r\n            row_tmp = list()\r\n            for j in range(self.scale):\r\n                row_tmp.append(local_list[j + i * self.scale])\r\n            context_list.append(torch.cat(row_tmp, 3))\r\n\r\n        context = torch.cat(context_list, 2)\r\n        context = self.W(context)\r\n\r\n        return context\r\n\r\n\r\nclass PyramidOCModule(nn.Module):\r\n    """"""Pyramid-OC""""""\r\n\r\n    def __init__(self, in_channels, out_channels, key_channels, value_channels,\r\n                 scales=([1]), norm_layer=nn.BatchNorm2d, **kwargs):\r\n        super(PyramidOCModule, self).__init__()\r\n        self.stages = nn.ModuleList([\r\n            PyramidAttentionBlock(in_channels, out_channels, key_channels, value_channels, scale, norm_layer, **kwargs)\r\n            for scale in scales])\r\n        self.up_dr = nn.Sequential(\r\n            nn.Conv2d(in_channels, in_channels * len(scales), 1),\r\n            norm_layer(in_channels * len(scales)),\r\n            nn.ReLU(True)\r\n        )\r\n        self.project = nn.Sequential(\r\n            nn.Conv2d(in_channels * len(scales) * 2, out_channels, 1),\r\n            norm_layer(out_channels),\r\n            nn.ReLU(True),\r\n            nn.Dropout2d(0.05)\r\n        )\r\n\r\n    def forward(self, x):\r\n        priors = [stage(x) for stage in self.stages]\r\n        context = [self.up_dr(x)]\r\n        for i in range(len(priors)):\r\n            context += [priors[i]]\r\n        context = torch.cat(context, 1)\r\n        out = self.project(context)\r\n        return out\r\n\r\n\r\nclass ASPOCModule(nn.Module):\r\n    """"""ASP-OC""""""\r\n\r\n    def __init__(self, in_channels, out_channels, key_channels, value_channels,\r\n                 atrous_rates=(12, 24, 36), norm_layer=nn.BatchNorm2d, **kwargs):\r\n        super(ASPOCModule, self).__init__()\r\n        self.context = nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\r\n            norm_layer(out_channels),\r\n            nn.ReLU(True),\r\n            BaseOCModule(out_channels, out_channels, key_channels, value_channels, ([2]), norm_layer, False, **kwargs))\r\n\r\n        rate1, rate2, rate3 = tuple(atrous_rates)\r\n        self.b1 = nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 3, padding=rate1, dilation=rate1, bias=False),\r\n            norm_layer(out_channels),\r\n            nn.ReLU(True))\r\n        self.b2 = nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 3, padding=rate2, dilation=rate2, bias=False),\r\n            norm_layer(out_channels),\r\n            nn.ReLU(True))\r\n        self.b3 = nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 3, padding=rate3, dilation=rate3, bias=False),\r\n            norm_layer(out_channels),\r\n            nn.ReLU(True))\r\n        self.b4 = nn.Sequential(\r\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\r\n            norm_layer(out_channels),\r\n            nn.ReLU(True))\r\n\r\n        self.project = nn.Sequential(\r\n            nn.Conv2d(out_channels * 5, out_channels, 1, bias=False),\r\n            norm_layer(out_channels),\r\n            nn.ReLU(True),\r\n            nn.Dropout2d(0.1)\r\n        )\r\n\r\n    def forward(self, x):\r\n        feat1 = self.context(x)\r\n        feat2 = self.b1(x)\r\n        feat3 = self.b2(x)\r\n        feat4 = self.b3(x)\r\n        feat5 = self.b4(x)\r\n        out = torch.cat((feat1, feat2, feat3, feat4, feat5), dim=1)\r\n        out = self.project(out)\r\n        return out\r\n'"
segmentron/models/pointrend.py,17,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torchvision.models._utils import IntermediateLayerGetter\nfrom .model_zoo import MODEL_REGISTRY\nfrom .segbase import SegBaseModel\nfrom ..config import cfg\n\n\n@MODEL_REGISTRY.register(name=\'PointRend\')\nclass PointRend(SegBaseModel):\n    def __init__(self):\n        super(PointRend, self).__init__(need_backbone=False)\n        model_name = cfg.MODEL.POINTREND.BASEMODEL\n        self.backbone =  MODEL_REGISTRY.get(model_name)()\n\n        self.head = PointHead(num_classes=self.nclass)\n\n    def forward(self, x):\n        c1, _, _, c4 = self.backbone.encoder(x)\n\n        out = self.backbone.head(c4, c1)\n        \n        result = {\'res2\': c1, \'coarse\': out}\n        result.update(self.head(x, result[""res2""], result[""coarse""]))\n        if not self.training:\n            return (result[\'fine\'],)\n        return result\n\n\nclass PointHead(nn.Module):\n    def __init__(self, in_c=275, num_classes=19, k=3, beta=0.75):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Conv1d(in_c, 256, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.ReLU(True),\n            nn.Conv1d(256, 256, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.ReLU(True),\n            nn.Conv1d(256, 256, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.ReLU(True),\n            nn.Conv1d(256, num_classes, 1)\n            )\n        self.k = k\n        self.beta = beta\n\n    def forward(self, x, res2, out):\n        """"""\n        1. Fine-grained features are interpolated from res2 for DeeplabV3\n        2. During training we sample as many points as there are on a stride 16 feature map of the input\n        3. To measure prediction uncertainty\n           we use the same strategy during training and inference: the difference between the most\n           confident and second most confident class probabilities.\n        """"""\n        if not self.training:\n            return self.inference(x, res2, out)\n\n        # out = F.interpolate(out, size=x.shape[-2:], mode=""bilinear"", align_corners=True)\n        # res2 = F.interpolate(res2, size=out.shape[-2:], mode=""bilinear"", align_corners=True)\n        N = x.shape[-1] // 16\n        points = sampling_points(out, N * N, self.k, self.beta)\n\n        coarse = point_sample(out, points, align_corners=False)\n        fine = point_sample(res2, points, align_corners=False)\n\n        feature_representation = torch.cat([coarse, fine], dim=1)\n\n        rend = self.mlp(feature_representation)\n\n        return {""rend"": rend, ""points"": points}\n\n    @torch.no_grad()\n    def inference(self, x, res2, out):\n        """"""\n        During inference, subdivision uses N=8096\n        (i.e., the number of points in the stride 16 map of a 1024\xc3\x972048 image)\n        """"""\n        num_points = 8096\n        \n        while out.shape[-1] * 2 < x.shape[-1]:\n            out = F.interpolate(out, scale_factor=2, mode=""bilinear"", align_corners=False)\n            # res2 = F.interpolate(res2, size=out.shape[-2:], mode=""bilinear"", align_corners=True)\n\n            points_idx, points = sampling_points(out, num_points, training=self.training)\n\n            coarse = point_sample(out, points, align_corners=False)\n            fine = point_sample(res2, points, align_corners=False)\n\n            feature_representation = torch.cat([coarse, fine], dim=1)\n\n            rend = self.mlp(feature_representation)\n\n            B, C, H, W = out.shape\n            points_idx = points_idx.unsqueeze(1).expand(-1, C, -1)\n            out = (out.reshape(B, C, -1)\n                      .scatter_(2, points_idx, rend)\n                      .view(B, C, H, W))\n        \n        out = F.interpolate(out, size=x.shape[-2:], mode=""bilinear"", align_corners=False)\n        # res2 = F.interpolate(res2, size=out.shape[-2:], mode=""bilinear"", align_corners=True)\n\n        points_idx, points = sampling_points(out, num_points, training=self.training)\n\n        coarse = point_sample(out, points, align_corners=False)\n        fine = point_sample(res2, points, align_corners=False)\n\n        feature_representation = torch.cat([coarse, fine], dim=1)\n\n        rend = self.mlp(feature_representation)\n\n        B, C, H, W = out.shape\n        points_idx = points_idx.unsqueeze(1).expand(-1, C, -1)\n        out = (out.reshape(B, C, -1)\n                    .scatter_(2, points_idx, rend)\n                    .view(B, C, H, W))\n        return {""fine"": out}\n\n\ndef point_sample(input, point_coords, **kwargs):\n    """"""\n    From Detectron2, point_features.py#19\n    A wrapper around :function:`torch.nn.functional.grid_sample` to support 3D point_coords tensors.\n    Unlike :function:`torch.nn.functional.grid_sample` it assumes `point_coords` to lie inside\n    [0, 1] x [0, 1] square.\n    Args:\n        input (Tensor): A tensor of shape (N, C, H, W) that contains features map on a H x W grid.\n        point_coords (Tensor): A tensor of shape (N, P, 2) or (N, Hgrid, Wgrid, 2) that contains\n        [0, 1] x [0, 1] normalized point coordinates.\n    Returns:\n        output (Tensor): A tensor of shape (N, C, P) or (N, C, Hgrid, Wgrid) that contains\n            features for points in `point_coords`. The features are obtained via bilinear\n            interplation from `input` the same way as :function:`torch.nn.functional.grid_sample`.\n    """"""\n    add_dim = False\n    if point_coords.dim() == 3:\n        add_dim = True\n        point_coords = point_coords.unsqueeze(2)\n    output = F.grid_sample(input, 2.0 * point_coords - 1.0, **kwargs)\n    if add_dim:\n        output = output.squeeze(3)\n    return output\n\n\n@torch.no_grad()\ndef sampling_points(mask, N, k=3, beta=0.75, training=True):\n    """"""\n    Follows 3.1. Point Selection for Inference and Training\n    In Train:, `The sampling strategy selects N points on a feature map to train on.`\n    In Inference, `then selects the N most uncertain points`\n    Args:\n        mask(Tensor): [B, C, H, W]\n        N(int): `During training we sample as many points as there are on a stride 16 feature map of the input`\n        k(int): Over generation multiplier\n        beta(float): ratio of importance points\n        training(bool): flag\n    Return:\n        selected_point(Tensor) : flattened indexing points [B, num_points, 2]\n    """"""\n    assert mask.dim() == 4, ""Dim must be N(Batch)CHW""\n    device = mask.device\n    B, _, H, W = mask.shape\n    mask, _ = mask.sort(1, descending=True)\n\n    if not training:\n        H_step, W_step = 1 / H, 1 / W\n        N = min(H * W, N)\n        uncertainty_map = -1 * (mask[:, 0] - mask[:, 1])\n        _, idx = uncertainty_map.view(B, -1).topk(N, dim=1)\n\n        points = torch.zeros(B, N, 2, dtype=torch.float, device=device)\n        points[:, :, 0] = W_step / 2.0 + (idx  % W).to(torch.float) * W_step\n        points[:, :, 1] = H_step / 2.0 + (idx // W).to(torch.float) * H_step\n        return idx, points\n\n    # Official Comment : point_features.py#92\n    # It is crucial to calculate uncertanty based on the sampled prediction value for the points.\n    # Calculating uncertainties of the coarse predictions first and sampling them for points leads\n    # to worse results. To illustrate the difference: a sampled point between two coarse predictions\n    # with -1 and 1 logits has 0 logit prediction and therefore 0 uncertainty value, however, if one\n    # calculates uncertainties for the coarse predictions first (-1 and -1) and sampe it for the\n    # center point, they will get -1 unceratinty.\n\n    over_generation = torch.rand(B, k * N, 2, device=device)\n    over_generation_map = point_sample(mask, over_generation, align_corners=False)\n\n    uncertainty_map = -1 * (over_generation_map[:, 0] - over_generation_map[:, 1])\n    _, idx = uncertainty_map.topk(int(beta * N), -1)\n\n    shift = (k * N) * torch.arange(B, dtype=torch.long, device=device)\n\n    idx += shift[:, None]\n\n    importance = over_generation.view(-1, 2)[idx.view(-1), :].view(B, int(beta * N), 2)\n    coverage = torch.rand(B, N - int(beta * N), 2, device=device)\n    return torch.cat([importance, coverage], 1).to(device)'"
segmentron/models/pspnet.py,2,"b'""""""Pyramid Scene Parsing Network""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom ..modules import _FCNHead, PyramidPooling\n\n__all__ = [\'PSPNet\']\n\n\n@MODEL_REGISTRY.register()\nclass PSPNet(SegBaseModel):\n    r""""""Pyramid Scene Parsing Network\n    Reference:\n        Zhao, Hengshuang, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.\n        ""Pyramid scene parsing network."" *CVPR*, 2017\n    """"""\n\n    def __init__(self):\n        super(PSPNet, self).__init__()\n        self.head = _PSPHead(self.nclass)\n        if self.aux:\n            self.auxlayer = _FCNHead(1024, self.nclass)\n\n        self.__setattr__(\'decoder\', [\'head\', \'auxlayer\'] if self.aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        _, _, c3, c4 = self.encoder(x)\n        outputs = []\n        x = self.head(c4)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n        outputs.append(x)\n\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = F.interpolate(auxout, size, mode=\'bilinear\', align_corners=True)\n            outputs.append(auxout)\n        return tuple(outputs)\n\n\nclass _PSPHead(nn.Module):\n    def __init__(self, nclass, norm_layer=nn.BatchNorm2d, norm_kwargs=None, **kwargs):\n        super(_PSPHead, self).__init__()\n        self.psp = PyramidPooling(2048, norm_layer=norm_layer, norm_kwargs=norm_kwargs)\n        self.block = nn.Sequential(\n            nn.Conv2d(4096, 512, 3, padding=1, bias=False),\n            norm_layer(512, **({} if norm_kwargs is None else norm_kwargs)),\n            nn.ReLU(True),\n            nn.Dropout(0.1),\n            nn.Conv2d(512, nclass, 1)\n        )\n\n    def forward(self, x):\n        x = self.psp(x)\n        return self.block(x)\n\n'"
segmentron/models/refinenet.py,2,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom ..modules import _FCNHead\n\n__all__ = ['RefineNet']\n\n\n@MODEL_REGISTRY.register()\nclass RefineNet(SegBaseModel):\n\n    def __init__(self):\n        super(RefineNet, self).__init__()\n        self.head = _RefineHead(self.nclass, norm_layer=self.norm_layer)\n        if self.aux:\n            self.auxlayer = _FCNHead(728, self.nclass)\n        self.__setattr__('decoder', ['head', 'auxlayer'] if self.aux else ['head'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        c1, c2, c3, c4 = self.encoder(x)\n\n        outputs = list()\n        x = self.head(c1, c2, c3, c4)\n        x = F.interpolate(x, size, mode='bilinear', align_corners=True)\n\n        outputs.append(x)\n        if self.aux:\n            auxout = self.auxlayer(c3)\n            auxout = F.interpolate(auxout, size, mode='bilinear', align_corners=True)\n            outputs.append(auxout)\n        return tuple(outputs)\n\n\nclass _RefineHead(nn.Module):\n    def __init__(self, nclass, norm_layer=nn.BatchNorm2d):\n        super(_RefineHead, self).__init__()\n        self.do = nn.Dropout(p=0.5)\n        self.relu = nn.ReLU(inplace=True)\n        self.p_ims1d2_outl1_dimred = nn.Conv2d(2048, 512, 1, bias=False)\n        self.mflow_conv_g1_pool = self._make_crp(512, 512, 4)\n        self.mflow_conv_g1_b3_joint_varout_dimred = nn.Conv2d(512, 256, 1, bias=False)\n        self.p_ims1d2_outl2_dimred = nn.Conv2d(1024, 256, 1, bias=False)\n        self.adapt_stage2_b2_joint_varout_dimred = nn.Conv2d(256, 256, 1, bias=False)\n        self.mflow_conv_g2_pool = self._make_crp(256, 256, 4)\n        self.mflow_conv_g2_b3_joint_varout_dimred = nn.Conv2d(256, 256, 1, bias=False)\n\n        self.p_ims1d2_outl3_dimred = nn.Conv2d(512, 256, 1, bias=False)\n        self.adapt_stage3_b2_joint_varout_dimred = nn.Conv2d(256, 256, 1, bias=False)\n        self.mflow_conv_g3_pool = self._make_crp(256, 256, 4)\n        self.mflow_conv_g3_b3_joint_varout_dimred = nn.Conv2d(256, 256, 1, bias=False)\n\n        self.p_ims1d2_outl4_dimred = nn.Conv2d(256, 256, 1, bias=False)\n        self.adapt_stage4_b2_joint_varout_dimred = nn.Conv2d(256, 256, 1, bias=False)\n        self.mflow_conv_g4_pool = self._make_crp(256, 256, 4)\n\n        self.clf_conv = nn.Conv2d(256, nclass, kernel_size=3, stride=1,\n                                  padding=1, bias=True)\n\n    def _make_crp(self, in_planes, out_planes, stages):\n        layers = [CRPBlock(in_planes, out_planes, stages)]\n        return nn.Sequential(*layers)\n\n    def forward(self, l1, l2, l3, l4):\n        l4 = self.do(l4)\n        l3 = self.do(l3)\n\n        x4 = self.p_ims1d2_outl1_dimred(l4)\n        x4 = self.relu(x4)\n        x4 = self.mflow_conv_g1_pool(x4)\n        x4 = self.mflow_conv_g1_b3_joint_varout_dimred(x4)\n        x4 = F.interpolate(x4, size=l3.size()[2:], mode='bilinear', align_corners=True)\n\n        x3 = self.p_ims1d2_outl2_dimred(l3)\n        x3 = self.adapt_stage2_b2_joint_varout_dimred(x3)\n        x3 = x3 + x4\n        x3 = F.relu(x3)\n        x3 = self.mflow_conv_g2_pool(x3)\n        x3 = self.mflow_conv_g2_b3_joint_varout_dimred(x3)\n        x3 = F.interpolate(x3, size=l2.size()[2:], mode='bilinear', align_corners=True)\n\n        x2 = self.p_ims1d2_outl3_dimred(l2)\n        x2 = self.adapt_stage3_b2_joint_varout_dimred(x2)\n        x2 = x2 + x3\n        x2 = F.relu(x2)\n        x2 = self.mflow_conv_g3_pool(x2)\n        x2 = self.mflow_conv_g3_b3_joint_varout_dimred(x2)\n        x2 = F.interpolate(x2, size=l1.size()[2:], mode='bilinear', align_corners=True)\n\n        x1 = self.p_ims1d2_outl4_dimred(l1)\n        x1 = self.adapt_stage4_b2_joint_varout_dimred(x1)\n        x1 = x1 + x2\n        x1 = F.relu(x1)\n        x1 = self.mflow_conv_g4_pool(x1)\n\n        out = self.clf_conv(x1)\n        return out\n\n\nclass CRPBlock(nn.Module):\n\n    def __init__(self, in_planes, out_planes, n_stages):\n        super(CRPBlock, self).__init__()\n        for i in range(n_stages):\n            setattr(self, '{}_{}'.format(i + 1, 'outvar_dimred'),\n                    nn.Conv2d(in_planes if (i == 0) else out_planes,\n                            out_planes, 1, stride=1,\n                            bias=False))\n        self.stride = 1\n        self.n_stages = n_stages\n        self.maxpool = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)\n\n    def forward(self, x):\n        top = x\n        for i in range(self.n_stages):\n            top = self.maxpool(top)\n            top = getattr(self, '{}_{}'.format(i + 1, 'outvar_dimred'))(top)\n            x = top + x\n        return x"""
segmentron/models/segbase.py,5,"b'""""""Base Model for Semantic Segmentation""""""\nimport math\nimport numbers\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .backbones import get_segmentation_backbone\nfrom ..data.dataloader import datasets\nfrom ..modules import get_norm\nfrom ..config import cfg\n__all__ = [\'SegBaseModel\']\n\n\nclass SegBaseModel(nn.Module):\n    r""""""Base Model for Semantic Segmentation\n    """"""\n    def __init__(self, need_backbone=True):\n        super(SegBaseModel, self).__init__()\n        self.nclass = datasets[cfg.DATASET.NAME].NUM_CLASS\n        self.aux = cfg.SOLVER.AUX\n        self.norm_layer = get_norm(cfg.MODEL.BN_TYPE)\n        self.backbone = None\n        self.encoder = None\n        if need_backbone:\n            self.get_backbone()\n\n    def get_backbone(self):\n        self.backbone = cfg.MODEL.BACKBONE.lower()\n        self.encoder = get_segmentation_backbone(self.backbone, self.norm_layer)\n\n    def base_forward(self, x):\n        """"""forwarding backbone network""""""\n        c1, c2, c3, c4 = self.encoder(x)\n        return c1, c2, c3, c4\n\n    def demo(self, x):\n        pred = self.forward(x)\n        if self.aux:\n            pred = pred[0]\n        return pred\n\n    def evaluate(self, image):\n        """"""evaluating network with inputs and targets""""""\n        scales = cfg.TEST.SCALES\n        flip = cfg.TEST.FLIP\n        crop_size = _to_tuple(cfg.TEST.CROP_SIZE) if cfg.TEST.CROP_SIZE else None\n        batch, _, h, w = image.shape\n        base_size = max(h, w)\n        # scores = torch.zeros((batch, self.nclass, h, w)).to(image.device)\n        scores = None\n        for scale in scales:\n            long_size = int(math.ceil(base_size * scale))\n            if h > w:\n                height = long_size\n                width = int(1.0 * w * long_size / h + 0.5)\n            else:\n                width = long_size\n                height = int(1.0 * h * long_size / w + 0.5)\n\n            # resize image to current size\n            cur_img = _resize_image(image, height, width)\n            if crop_size is not None:\n                assert crop_size[0] >= h and crop_size[1] >= w\n                crop_size_scaled = (int(math.ceil(crop_size[0] * scale)),\n                                    int(math.ceil(crop_size[1] * scale)))\n                cur_img = _pad_image(cur_img, crop_size_scaled)\n            outputs = self.forward(cur_img)[0][..., :height, :width]\n            if flip:\n                outputs += _flip_image(self.forward(_flip_image(cur_img))[0])[..., :height, :width]\n\n            score = _resize_image(outputs, h, w)\n\n            if scores is None:\n                scores = score\n            else:\n                scores += score\n        return scores\n\n\ndef _resize_image(img, h, w):\n    return F.interpolate(img, size=[h, w], mode=\'bilinear\', align_corners=True)\n\n\ndef _pad_image(img, crop_size):\n    b, c, h, w = img.shape\n    assert(c == 3)\n    padh = crop_size[0] - h if h < crop_size[0] else 0\n    padw = crop_size[1] - w if w < crop_size[1] else 0\n    if padh == 0 and padw == 0:\n        return img\n    img_pad = F.pad(img, (0, padh, 0, padw))\n\n    # TODO clean this code\n    # mean = cfg.DATASET.MEAN\n    # std = cfg.DATASET.STD\n    # pad_values = -np.array(mean) / np.array(std)\n    # img_pad = torch.zeros((b, c, h + padh, w + padw)).to(img.device)\n    # for i in range(c):\n    #     # print(img[:, i, :, :].unsqueeze(1).shape)\n    #     img_pad[:, i, :, :] = torch.squeeze(\n    #         F.pad(img[:, i, :, :].unsqueeze(1), (0, padh, 0, padw),\n    #               \'constant\', value=pad_values[i]), 1)\n    # assert(img_pad.shape[2] >= crop_size[0] and img_pad.shape[3] >= crop_size[1])\n\n    return img_pad\n\n\ndef _crop_image(img, h0, h1, w0, w1):\n    return img[:, :, h0:h1, w0:w1]\n\n\ndef _flip_image(img):\n    assert(img.ndim == 4)\n    return img.flip((3))\n\n\ndef _to_tuple(size):\n    if isinstance(size, (list, tuple)):\n        assert len(size), \'Expect eval crop size contains two element, \' \\\n                          \'but received {}\'.format(len(size))\n        return tuple(size)\n    elif isinstance(size, numbers.Number):\n        return tuple((size, size))\n    else:\n        raise ValueError(\'Unsupport datatype: {}\'.format(type(size)))\n'"
segmentron/models/unet.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .segbase import SegBaseModel\nfrom .model_zoo import MODEL_REGISTRY\nfrom ..modules import _FCNHead\nfrom ..config import cfg\n\n__all__ = [\'UNet\']\n\n\n@MODEL_REGISTRY.register()\nclass UNet(SegBaseModel):\n\n    def __init__(self):\n        super(UNet, self).__init__(need_backbone=False)\n        self.inc = DoubleConv(3, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        self.down4 = Down(512, 512)\n        self.head = _UNetHead(self.nclass)\n\n        self.__setattr__(\'decoder\', [\'head\', \'auxlayer\'] if self.aux else [\'head\'])\n\n    def forward(self, x):\n        size = x.size()[2:]\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n\n        outputs = list()\n        x = self.head(x1, x2, x3, x4, x5)\n        x = F.interpolate(x, size, mode=\'bilinear\', align_corners=True)\n\n        outputs.append(x)\n        return tuple(outputs)\n\n\nclass _UNetHead(nn.Module):\n    def __init__(self, nclass, norm_layer=nn.BatchNorm2d):\n        super(_UNetHead, self).__init__()\n        bilinear = True\n        self.up1 = Up(1024, 256, bilinear)\n        self.up2 = Up(512, 128, bilinear)\n        self.up3 = Up(256, 64, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, nclass)\n\n    def forward(self, x1, x2, x3, x4, x5):\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n\n        logits = self.outc(x)\n        return logits\n\n\nclass DoubleConv(nn.Module):\n    """"""(convolution => [BN] => ReLU) * 2""""""\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    """"""Downscaling with maxpool then double conv""""""\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    """"""Upscaling then double conv""""""\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode=\'bilinear\', align_corners=True)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n\n        self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        # if you have padding issues, see\n        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)\n'"
segmentron/modules/__init__.py,0,"b'""""""Seg NN Modules""""""\n\nfrom .basic import *\nfrom .module import *\nfrom .batch_norm import get_norm'"
segmentron/modules/basic.py,2,"b'""""""Basic Module for Semantic Segmentation""""""\nimport torch\nimport torch.nn as nn\n\nfrom collections import OrderedDict\n\n__all__ = [\'_ConvBNPReLU\', \'_ConvBN\', \'_BNPReLU\', \'_ConvBNReLU\', \'_DepthwiseConv\', \'InvertedResidual\',\n           \'SeparableConv2d\']\n\n_USE_FIXED_PAD = False\n\n\ndef _pytorch_padding(kernel_size, stride=1, dilation=1, **_):\n    if _USE_FIXED_PAD:\n        return 0  # FIXME remove once verified\n    else:\n        padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2\n\n        # FIXME remove once verified\n        fp = _fixed_padding(kernel_size, dilation)\n        assert all(padding == p for p in fp)\n\n        return padding\n\n\ndef _fixed_padding(kernel_size, dilation):\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    return [pad_beg, pad_end, pad_beg, pad_end]\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size=3, stride=1, dilation=1, relu_first=True,\n                 bias=False, norm_layer=nn.BatchNorm2d):\n        super().__init__()\n        depthwise = nn.Conv2d(inplanes, inplanes, kernel_size,\n                              stride=stride, padding=dilation,\n                              dilation=dilation, groups=inplanes, bias=bias)\n        bn_depth = norm_layer(inplanes)\n        pointwise = nn.Conv2d(inplanes, planes, 1, bias=bias)\n        bn_point = norm_layer(planes)\n\n        if relu_first:\n            self.block = nn.Sequential(OrderedDict([(\'relu\', nn.ReLU()),\n                                                    (\'depthwise\', depthwise),\n                                                    (\'bn_depth\', bn_depth),\n                                                    (\'pointwise\', pointwise),\n                                                    (\'bn_point\', bn_point)\n                                                    ]))\n        else:\n            self.block = nn.Sequential(OrderedDict([(\'depthwise\', depthwise),\n                                                    (\'bn_depth\', bn_depth),\n                                                    (\'relu1\', nn.ReLU(inplace=True)),\n                                                    (\'pointwise\', pointwise),\n                                                    (\'bn_point\', bn_point),\n                                                    (\'relu2\', nn.ReLU(inplace=True))\n                                                    ]))\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass _ConvBNReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n                 dilation=1, groups=1, relu6=False, norm_layer=nn.BatchNorm2d):\n        super(_ConvBNReLU, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias=False)\n        self.bn = norm_layer(out_channels)\n        self.relu = nn.ReLU6(True) if relu6 else nn.ReLU(True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass _ConvBNPReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n                 dilation=1, groups=1, norm_layer=nn.BatchNorm2d):\n        super(_ConvBNPReLU, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias=False)\n        self.bn = norm_layer(out_channels)\n        self.prelu = nn.PReLU(out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.prelu(x)\n        return x\n\n\nclass _ConvBN(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n                 dilation=1, groups=1, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_ConvBN, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias=False)\n        self.bn = norm_layer(out_channels)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass _BNPReLU(nn.Module):\n    def __init__(self, out_channels, norm_layer=nn.BatchNorm2d):\n        super(_BNPReLU, self).__init__()\n        self.bn = norm_layer(out_channels)\n        self.prelu = nn.PReLU(out_channels)\n\n    def forward(self, x):\n        x = self.bn(x)\n        x = self.prelu(x)\n        return x\n\n\n# -----------------------------------------------------------------\n#                      For MobileNet\n# -----------------------------------------------------------------\nclass _DepthwiseConv(nn.Module):\n    """"""conv_dw in MobileNet""""""\n\n    def __init__(self, in_channels, out_channels, stride, norm_layer=nn.BatchNorm2d, **kwargs):\n        super(_DepthwiseConv, self).__init__()\n        self.conv = nn.Sequential(\n            _ConvBNReLU(in_channels, in_channels, 3, stride, 1, groups=in_channels, norm_layer=norm_layer),\n            _ConvBNReLU(in_channels, out_channels, 1, norm_layer=norm_layer))\n\n    def forward(self, x):\n        return self.conv(x)\n\n\n# -----------------------------------------------------------------\n#                      For MobileNetV2\n# -----------------------------------------------------------------\nclass InvertedResidual(nn.Module):\n    def __init__(self, in_channels, out_channels, stride, expand_ratio, dilation=1, norm_layer=nn.BatchNorm2d):\n        super(InvertedResidual, self).__init__()\n        assert stride in [1, 2]\n        self.use_res_connect = stride == 1 and in_channels == out_channels\n\n        layers = list()\n        inter_channels = int(round(in_channels * expand_ratio))\n        if expand_ratio != 1:\n            # pw\n            layers.append(_ConvBNReLU(in_channels, inter_channels, 1, relu6=True, norm_layer=norm_layer))\n        layers.extend([\n            # dw\n            _ConvBNReLU(inter_channels, inter_channels, 3, stride, dilation, dilation,\n                        groups=inter_channels, relu6=True, norm_layer=norm_layer),\n            # pw-linear\n            nn.Conv2d(inter_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels)])\n        self.conv = nn.Sequential(*layers)\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nif __name__ == \'__main__\':\n    x = torch.randn(1, 32, 64, 64)\n    model = InvertedResidual(32, 64, 2, 1)\n    out = model(x)\n'"
segmentron/modules/batch_norm.py,18,"b'# this code heavily based on detectron2\nimport logging\nimport torch\nimport torch.distributed as dist\nfrom torch import nn\nfrom torch.autograd.function import Function\nfrom ..utils.distributed import get_world_size\n\n\nclass FrozenBatchNorm2d(nn.Module):\n    """"""\n    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n\n    It contains non-trainable buffers called\n    ""weight"" and ""bias"", ""running_mean"", ""running_var"",\n    initialized to perform identity transformation.\n\n    The pre-trained backbone models from Caffe2 only contain ""weight"" and ""bias"",\n    which are computed from the original four parameters of BN.\n    The affine transform `x * weight + bias` will perform the equivalent\n    computation of `(x - running_mean) / sqrt(running_var) * weight + bias`.\n    When loading a backbone model from Caffe2, ""running_mean"" and ""running_var""\n    will be left unchanged as identity transformation.\n\n    Other pre-trained backbone models may contain all 4 parameters.\n\n    The forward is implemented by `F.batch_norm(..., training=False)`.\n    """"""\n\n    _version = 3\n\n    def __init__(self, num_features, eps=1e-5):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.register_buffer(""weight"", torch.ones(num_features))\n        self.register_buffer(""bias"", torch.zeros(num_features))\n        self.register_buffer(""running_mean"", torch.zeros(num_features))\n        self.register_buffer(""running_var"", torch.ones(num_features) - eps)\n\n    def forward(self, x):\n        scale = self.weight * (self.running_var + self.eps).rsqrt()\n        bias = self.bias - self.running_mean * scale\n        scale = scale.reshape(1, -1, 1, 1)\n        bias = bias.reshape(1, -1, 1, 1)\n        return x * scale + bias\n\n    def _load_from_state_dict(\n        self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n    ):\n        version = local_metadata.get(""version"", None)\n\n        if version is None or version < 2:\n            # No running_mean/var in early versions\n            # This will silent the warnings\n            if prefix + ""running_mean"" not in state_dict:\n                state_dict[prefix + ""running_mean""] = torch.zeros_like(self.running_mean)\n            if prefix + ""running_var"" not in state_dict:\n                state_dict[prefix + ""running_var""] = torch.ones_like(self.running_var)\n\n        if version is not None and version < 3:\n            # logger = logging.getLogger(__name__)\n            logging.info(""FrozenBatchNorm {} is upgraded to version 3."".format(prefix.rstrip(""."")))\n            # In version < 3, running_var are used without +eps.\n            state_dict[prefix + ""running_var""] -= self.eps\n\n        super()._load_from_state_dict(\n            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n        )\n\n    def __repr__(self):\n        return ""FrozenBatchNorm2d(num_features={}, eps={})"".format(self.num_features, self.eps)\n\n    @classmethod\n    def convert_frozen_batchnorm(cls, module):\n        """"""\n        Convert BatchNorm/SyncBatchNorm in module into FrozenBatchNorm.\n\n        Args:\n            module (torch.nn.Module):\n\n        Returns:\n            If module is BatchNorm/SyncBatchNorm, returns a new module.\n            Otherwise, in-place convert module and return it.\n\n        Similar to convert_sync_batchnorm in\n        https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py\n        """"""\n        bn_module = nn.modules.batchnorm\n        bn_module = (bn_module.BatchNorm2d, bn_module.SyncBatchNorm)\n        res = module\n        if isinstance(module, bn_module):\n            res = cls(module.num_features)\n            if module.affine:\n                res.weight.data = module.weight.data.clone().detach()\n                res.bias.data = module.bias.data.clone().detach()\n            res.running_mean.data = module.running_mean.data\n            res.running_var.data = module.running_var.data + module.eps\n        else:\n            for name, child in module.named_children():\n                new_child = cls.convert_frozen_batchnorm(child)\n                if new_child is not child:\n                    res.add_module(name, new_child)\n        return res\n\n\ndef groupNorm(num_channels, eps=1e-5, momentum=0.1, affine=True):\n    return nn.GroupNorm(min(32, num_channels), num_channels, eps=eps, affine=affine)\n\n\ndef get_norm(norm):\n    """"""\n    Args:\n        norm (str or callable):\n\n    Returns:\n        nn.Module or None: the normalization layer\n    """"""\n    support_norm_type = [\'BN\', \'SyncBN\', \'FrozenBN\', \'GN\', \'nnSyncBN\']\n    assert norm in support_norm_type, \'Unknown norm type {}, support norm types are {}\'.format(\n                                                                        norm, support_norm_type)\n    if isinstance(norm, str):\n        if len(norm) == 0:\n            return None\n        norm = {\n            ""BN"": nn.BatchNorm2d,\n            ""SyncBN"": NaiveSyncBatchNorm,\n            ""FrozenBN"": FrozenBatchNorm2d,\n            ""GN"": groupNorm,\n            ""nnSyncBN"": nn.SyncBatchNorm,  # keep for debugging\n        }[norm]\n    return norm\n\n\nclass AllReduce(Function):\n    @staticmethod\n    def forward(ctx, input):\n        input_list = [torch.zeros_like(input) for k in range(dist.get_world_size())]\n        # Use allgather instead of allreduce since I don\'t trust in-place operations ..\n        dist.all_gather(input_list, input, async_op=False)\n        inputs = torch.stack(input_list, dim=0)\n        return torch.sum(inputs, dim=0)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        dist.all_reduce(grad_output, async_op=False)\n        return grad_output\n\n\nclass NaiveSyncBatchNorm(nn.BatchNorm2d):\n    """"""\n    `torch.nn.SyncBatchNorm` has known unknown bugs.\n    It produces significantly worse AP (and sometimes goes NaN)\n    when the batch size on each worker is quite different\n    (e.g., when scale augmentation is used, or when it is applied to mask head).\n\n    Use this implementation before `nn.SyncBatchNorm` is fixed.\n    It is slower than `nn.SyncBatchNorm`.\n    """"""\n\n    def forward(self, input):\n        if get_world_size() == 1 or not self.training:\n            return super().forward(input)\n\n        assert input.shape[0] > 0, ""SyncBatchNorm does not support empty inputs""\n        C = input.shape[1]\n        mean = torch.mean(input, dim=[0, 2, 3])\n        meansqr = torch.mean(input * input, dim=[0, 2, 3])\n\n        vec = torch.cat([mean, meansqr], dim=0)\n        vec = AllReduce.apply(vec) * (1.0 / dist.get_world_size())\n\n        mean, meansqr = torch.split(vec, C)\n        var = meansqr - mean * mean\n        self.running_mean += self.momentum * (mean.detach() - self.running_mean)\n        self.running_var += self.momentum * (var.detach() - self.running_var)\n\n        invstd = torch.rsqrt(var + self.eps)\n        scale = self.weight * invstd\n        bias = self.bias - mean * scale\n        scale = scale.reshape(1, -1, 1, 1)\n        bias = bias.reshape(1, -1, 1, 1)\n        return input * scale + bias\n'"
segmentron/modules/cc_attention.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd.function import once_differentiable\nfrom segmentron import _C\n\n__all__ = [\'CrissCrossAttention\', \'ca_weight\', \'ca_map\']\n\n\nclass _CAWeight(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, t, f):\n        weight = _C.ca_forward(t, f)\n\n        ctx.save_for_backward(t, f)\n\n        return weight\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dw):\n        t, f = ctx.saved_tensors\n\n        dt, df = _C.ca_backward(dw, t, f)\n        return dt, df\n\n\nclass _CAMap(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, weight, g):\n        out = _C.ca_map_forward(weight, g)\n\n        ctx.save_for_backward(weight, g)\n\n        return out\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dout):\n        weight, g = ctx.saved_tensors\n\n        dw, dg = _C.ca_map_backward(dout, weight, g)\n\n        return dw, dg\n\n\nca_weight = _CAWeight.apply\nca_map = _CAMap.apply\n\n\nclass CrissCrossAttention(nn.Module):\n    """"""Criss-Cross Attention Module""""""\n\n    def __init__(self, in_channels):\n        super(CrissCrossAttention, self).__init__()\n        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, 1)\n        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, 1)\n        self.value_conv = nn.Conv2d(in_channels, in_channels, 1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        proj_query = self.query_conv(x)\n        proj_key = self.key_conv(x)\n        proj_value = self.value_conv(x)\n\n        energy = ca_weight(proj_query, proj_key)\n        attention = F.softmax(energy, 1)\n        out = ca_map(attention, proj_value)\n        out = self.gamma * out + x\n\n        return out\n'"
segmentron/modules/module.py,12,"b'""""""Basic Module for Semantic Segmentation""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom collections import OrderedDict\nfrom .basic import _ConvBNReLU, SeparableConv2d, _ConvBN, _BNPReLU, _ConvBNPReLU\nfrom ..config import cfg\n\n__all__ = [\'_FCNHead\', \'_ASPP\', \'PyramidPooling\', \'PAM_Module\', \'CAM_Module\', \'EESP\']\n\n\nclass _FCNHead(nn.Module):\n    def __init__(self, in_channels, channels, norm_layer=nn.BatchNorm2d):\n        super(_FCNHead, self).__init__()\n        inter_channels = in_channels // 4\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n            norm_layer(inter_channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Conv2d(inter_channels, channels, 1)\n        )\n\n    def forward(self, x):\n        return self.block(x)\n\n\n# -----------------------------------------------------------------\n#                      For deeplab\n# -----------------------------------------------------------------\nclass _ASPP(nn.Module):\n    def __init__(self, in_channels=2048, out_channels=256):\n        super().__init__()\n        output_stride = cfg.MODEL.OUTPUT_STRIDE\n        if output_stride == 16:\n            dilations = [6, 12, 18]\n        elif output_stride == 8:\n            dilations = [12, 24, 36]\n        elif output_stride == 32:\n            dilations = [6, 12, 18]\n        else:\n            raise NotImplementedError\n\n        self.aspp0 = nn.Sequential(OrderedDict([(\'conv\', nn.Conv2d(in_channels, out_channels, 1, bias=False)),\n                                                (\'bn\', nn.BatchNorm2d(out_channels)),\n                                                (\'relu\', nn.ReLU(inplace=True))]))\n        self.aspp1 = SeparableConv2d(in_channels, out_channels, dilation=dilations[0], relu_first=False)\n        self.aspp2 = SeparableConv2d(in_channels, out_channels, dilation=dilations[1], relu_first=False)\n        self.aspp3 = SeparableConv2d(in_channels, out_channels, dilation=dilations[2], relu_first=False)\n\n        self.image_pooling = nn.Sequential(OrderedDict([(\'gap\', nn.AdaptiveAvgPool2d((1, 1))),\n                                                        (\'conv\', nn.Conv2d(in_channels, out_channels, 1, bias=False)),\n                                                        (\'bn\', nn.BatchNorm2d(out_channels)),\n                                                        (\'relu\', nn.ReLU(inplace=True))]))\n\n        self.conv = nn.Conv2d(out_channels*5, out_channels, 1, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout2d(p=0.1)\n\n    def forward(self, x):\n        pool = self.image_pooling(x)\n        pool = F.interpolate(pool, size=x.shape[2:], mode=\'bilinear\', align_corners=True)\n\n        x0 = self.aspp0(x)\n        x1 = self.aspp1(x)\n        x2 = self.aspp2(x)\n        x3 = self.aspp3(x)\n        x = torch.cat((pool, x0, x1, x2, x3), dim=1)\n\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n\n        return x\n\n# -----------------------------------------------------------------\n#                 For PSPNet, fast_scnn\n# -----------------------------------------------------------------\nclass PyramidPooling(nn.Module):\n    def __init__(self, in_channels, sizes=(1, 2, 3, 6), norm_layer=nn.BatchNorm2d, **kwargs):\n        super(PyramidPooling, self).__init__()\n        out_channels = int(in_channels / 4)\n        self.avgpools = nn.ModuleList()\n        self.convs = nn.ModuleList()\n        for size in sizes:\n            self.avgpools.append(nn.AdaptiveAvgPool2d(size))\n            self.convs.append(_ConvBNReLU(in_channels, out_channels, 1, norm_layer=norm_layer, **kwargs))\n\n    def forward(self, x):\n        size = x.size()[2:]\n        feats = [x]\n        for (avgpool, conv) in zip(self.avgpools, self.convs):\n            feats.append(F.interpolate(conv(avgpool(x)), size, mode=\'bilinear\', align_corners=True))\n        return torch.cat(feats, dim=1)\n\n\nclass PAM_Module(nn.Module):\n    """""" Position attention module""""""\n    def __init__(self, in_dim):\n        super(PAM_Module, self).__init__()\n        self.chanel_in = in_dim\n\n        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        """"""\n            inputs :\n                x : input feature maps( B X C X H X W)\n            returns :\n                out : attention value + input feature\n                attention: B X (HxW) X (HxW)\n        """"""\n        m_batchsize, C, height, width = x.size()\n        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n        energy = torch.bmm(proj_query, proj_key)\n        attention = self.softmax(energy)\n        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n        out = out.view(m_batchsize, C, height, width)\n\n        out = self.gamma*out + x\n        return out\n\n\nclass CAM_Module(nn.Module):\n    """""" Channel attention module""""""\n    def __init__(self, in_dim):\n        super(CAM_Module, self).__init__()\n        self.chanel_in = in_dim\n        self.gamma = nn.Parameter(torch.zeros(1))\n        self.softmax  = nn.Softmax(dim=-1)\n\n    def forward(self,x):\n        """"""\n            inputs :\n                x : input feature maps( B X C X H X W)\n            returns :\n                out : attention value + input feature\n                attention: B X C X C\n        """"""\n        m_batchsize, C, height, width = x.size()\n        proj_query = x.view(m_batchsize, C, -1)\n        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n        energy = torch.bmm(proj_query, proj_key)\n        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy\n        attention = self.softmax(energy_new)\n        proj_value = x.view(m_batchsize, C, -1)\n\n        out = torch.bmm(attention, proj_value)\n        out = out.view(m_batchsize, C, height, width)\n\n        out = self.gamma*out + x\n        return out\n\n\nclass EESP(nn.Module):\n\n    def __init__(self, in_channels, out_channels, stride=1, k=4, r_lim=7, down_method=\'esp\', norm_layer=nn.BatchNorm2d):\n        super(EESP, self).__init__()\n        self.stride = stride\n        n = int(out_channels / k)\n        n1 = out_channels - (k - 1) * n\n        assert down_method in [\'avg\', \'esp\'], \'One of these is suppported (avg or esp)\'\n        assert n == n1, ""n(={}) and n1(={}) should be equal for Depth-wise Convolution "".format(n, n1)\n        self.proj_1x1 = _ConvBNPReLU(in_channels, n, 1, stride=1, groups=k, norm_layer=norm_layer)\n\n        map_receptive_ksize = {3: 1, 5: 2, 7: 3, 9: 4, 11: 5, 13: 6, 15: 7, 17: 8}\n        self.k_sizes = list()\n        for i in range(k):\n            ksize = int(3 + 2 * i)\n            ksize = ksize if ksize <= r_lim else 3\n            self.k_sizes.append(ksize)\n        self.k_sizes.sort()\n        self.spp_dw = nn.ModuleList()\n        for i in range(k):\n            dilation = map_receptive_ksize[self.k_sizes[i]]\n            self.spp_dw.append(nn.Conv2d(n, n, 3, stride, dilation, dilation=dilation, groups=n, bias=False))\n        self.conv_1x1_exp = _ConvBN(out_channels, out_channels, 1, 1, groups=k, norm_layer=norm_layer)\n        self.br_after_cat = _BNPReLU(out_channels, norm_layer)\n        self.module_act = nn.PReLU(out_channels)\n        self.downAvg = True if down_method == \'avg\' else False\n\n    def forward(self, x):\n        output1 = self.proj_1x1(x)\n        output = [self.spp_dw[0](output1)]\n        for k in range(1, len(self.spp_dw)):\n            out_k = self.spp_dw[k](output1)\n            out_k = out_k + output[k - 1]\n            output.append(out_k)\n        expanded = self.conv_1x1_exp(self.br_after_cat(torch.cat(output, 1)))\n        del output\n        if self.stride == 2 and self.downAvg:\n            return expanded\n\n        if expanded.size() == x.size():\n            expanded = expanded + x\n\n        return self.module_act(expanded)'"
segmentron/solver/__init__.py,0,b''
segmentron/solver/loss.py,15,"b'""""""Custom losses.""""""\nimport logging\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\nfrom .lovasz_losses import lovasz_softmax\nfrom ..models.pointrend import point_sample\nfrom ..data.dataloader import datasets\nfrom ..config import cfg\n\n__all__ = [\'get_segmentation_loss\']\n\n\nclass MixSoftmaxCrossEntropyLoss(nn.CrossEntropyLoss):\n    def __init__(self, aux=True, aux_weight=0.2, ignore_index=-1, **kwargs):\n        super(MixSoftmaxCrossEntropyLoss, self).__init__(ignore_index=ignore_index)\n        self.aux = aux\n        self.aux_weight = aux_weight\n\n    def _aux_forward(self, *inputs, **kwargs):\n        *preds, target = tuple(inputs)\n\n        loss = super(MixSoftmaxCrossEntropyLoss, self).forward(preds[0], target)\n        for i in range(1, len(preds)):\n            aux_loss = super(MixSoftmaxCrossEntropyLoss, self).forward(preds[i], target)\n            loss += self.aux_weight * aux_loss\n        return loss\n\n    def _multiple_forward(self, *inputs):\n        *preds, target = tuple(inputs)\n        loss = super(MixSoftmaxCrossEntropyLoss, self).forward(preds[0], target)\n        for i in range(1, len(preds)):\n            loss += super(MixSoftmaxCrossEntropyLoss, self).forward(preds[i], target)\n        return loss\n\n    def forward(self, *inputs, **kwargs):\n        preds, target = tuple(inputs)\n        inputs = tuple(list(preds) + [target])\n        if self.aux:\n            return dict(loss=self._aux_forward(*inputs))\n        elif len(preds) > 1:\n            return dict(loss=self._multiple_forward(*inputs))\n        else:\n            return dict(loss=super(MixSoftmaxCrossEntropyLoss, self).forward(*inputs))\n\n\nclass ICNetLoss(nn.CrossEntropyLoss):\n    """"""Cross Entropy Loss for ICNet""""""\n    def __init__(self, aux_weight=0.4, ignore_index=-1, **kwargs):\n        super(ICNetLoss, self).__init__(ignore_index=ignore_index)\n        self.aux_weight = aux_weight\n\n    def forward(self, *inputs):\n        preds, target = tuple(inputs)\n        inputs = tuple(list(preds) + [target])\n\n        pred, pred_sub4, pred_sub8, pred_sub16, target = tuple(inputs)\n        # [batch, W, H] -> [batch, 1, W, H]\n        target = target.unsqueeze(1).float()\n        target_sub4 = F.interpolate(target, pred_sub4.size()[2:], mode=\'bilinear\', align_corners=True).squeeze(1).long()\n        target_sub8 = F.interpolate(target, pred_sub8.size()[2:], mode=\'bilinear\', align_corners=True).squeeze(1).long()\n        target_sub16 = F.interpolate(target, pred_sub16.size()[2:], mode=\'bilinear\', align_corners=True).squeeze(\n            1).long()\n        loss1 = super(ICNetLoss, self).forward(pred_sub4, target_sub4)\n        loss2 = super(ICNetLoss, self).forward(pred_sub8, target_sub8)\n        loss3 = super(ICNetLoss, self).forward(pred_sub16, target_sub16)\n        return dict(loss=loss1 + loss2 * self.aux_weight + loss3 * self.aux_weight)\n\n\nclass OhemCrossEntropy2d(nn.Module):\n    def __init__(self, ignore_index=-1, thresh=0.7, min_kept=100000, use_weight=True, **kwargs):\n        super(OhemCrossEntropy2d, self).__init__()\n        self.ignore_index = ignore_index\n        self.thresh = float(thresh)\n        self.min_kept = int(min_kept)\n        if use_weight:\n            weight = torch.FloatTensor([0.8373, 0.918, 0.866, 1.0345, 1.0166, 0.9969, 0.9754,\n                                        1.0489, 0.8786, 1.0023, 0.9539, 0.9843, 1.1116, 0.9037, 1.0865, 1.0955,\n                                        1.0865, 1.1529, 1.0507])\n            self.criterion = torch.nn.CrossEntropyLoss(weight=weight, ignore_index=ignore_index)\n        else:\n            self.criterion = torch.nn.CrossEntropyLoss(ignore_index=ignore_index)\n\n    def forward(self, pred, target):\n        n, c, h, w = pred.size()\n        target = target.view(-1)\n        valid_mask = target.ne(self.ignore_index)\n        target = target * valid_mask.long()\n        num_valid = valid_mask.sum()\n\n        prob = F.softmax(pred, dim=1)\n        prob = prob.transpose(0, 1).reshape(c, -1)\n\n        if self.min_kept > num_valid:\n            print(""Lables: {}"".format(num_valid))\n        elif num_valid > 0:\n            # prob = prob.masked_fill_(1 - valid_mask, 1)\n            prob = prob.masked_fill_(~valid_mask, 1)\n            mask_prob = prob[target, torch.arange(len(target), dtype=torch.long)]\n            threshold = self.thresh\n            if self.min_kept > 0:\n                index = mask_prob.argsort()\n                threshold_index = index[min(len(index), self.min_kept) - 1]\n                if mask_prob[threshold_index] > self.thresh:\n                    threshold = mask_prob[threshold_index]\n            kept_mask = mask_prob.le(threshold)\n            valid_mask = valid_mask * kept_mask\n            target = target * kept_mask.long()\n\n        # target = target.masked_fill_(1 - valid_mask, self.ignore_index)\n        target = target.masked_fill_(~valid_mask, self.ignore_index)\n        target = target.view(n, h, w)\n\n        return self.criterion(pred, target)\n\n\nclass EncNetLoss(nn.CrossEntropyLoss):\n    """"""2D Cross Entropy Loss with SE Loss""""""\n\n    def __init__(self, aux=False, aux_weight=0.4, weight=None, ignore_index=-1, **kwargs):\n        super(EncNetLoss, self).__init__(weight, None, ignore_index)\n        self.se_loss = cfg.MODEL.ENCNET.SE_LOSS\n        self.se_weight = cfg.MODEL.ENCNET.SE_WEIGHT\n        self.nclass = datasets[cfg.DATASET.NAME].NUM_CLASS\n        self.aux = aux\n        self.aux_weight = aux_weight\n        self.bceloss = nn.BCELoss(weight)\n\n    def forward(self, *inputs):\n        preds, target = tuple(inputs)\n        inputs = tuple(list(preds) + [target])\n        if not self.se_loss and not self.aux:\n            return super(EncNetLoss, self).forward(*inputs)\n        elif not self.se_loss:\n            pred1, pred2, target = tuple(inputs)\n            loss1 = super(EncNetLoss, self).forward(pred1, target)\n            loss2 = super(EncNetLoss, self).forward(pred2, target)\n            return dict(loss=loss1 + self.aux_weight * loss2)\n        elif not self.aux:\n            pred, se_pred, target = tuple(inputs)\n            se_target = self._get_batch_label_vector(target, nclass=self.nclass).type_as(pred)\n            loss1 = super(EncNetLoss, self).forward(pred, target)\n            loss2 = self.bceloss(torch.sigmoid(se_pred), se_target)\n            return dict(loss=loss1 + self.se_weight * loss2)\n        else:\n            pred1, se_pred, pred2, target = tuple(inputs)\n            se_target = self._get_batch_label_vector(target, nclass=self.nclass).type_as(pred1)\n            loss1 = super(EncNetLoss, self).forward(pred1, target)\n            loss2 = super(EncNetLoss, self).forward(pred2, target)\n            loss3 = self.bceloss(torch.sigmoid(se_pred), se_target)\n            return dict(loss=loss1 + self.aux_weight * loss2 + self.se_weight * loss3)\n\n    @staticmethod\n    def _get_batch_label_vector(target, nclass):\n        # target is a 3D Variable BxHxW, output is 2D BxnClass\n        batch = target.size(0)\n        tvect = Variable(torch.zeros(batch, nclass))\n        for i in range(batch):\n            hist = torch.histc(target[i].cpu().data.float(),\n                               bins=nclass, min=0,\n                               max=nclass - 1)\n            vect = hist > 0\n            tvect[i] = vect\n        return tvect\n\n\nclass MixSoftmaxCrossEntropyOHEMLoss(OhemCrossEntropy2d):\n    def __init__(self, aux=False, aux_weight=0.4, weight=None, ignore_index=-1, **kwargs):\n        super(MixSoftmaxCrossEntropyOHEMLoss, self).__init__(ignore_index=ignore_index)\n        self.aux = aux\n        self.aux_weight = aux_weight\n        self.bceloss = nn.BCELoss(weight)\n\n    def _aux_forward(self, *inputs, **kwargs):\n        *preds, target = tuple(inputs)\n\n        loss = super(MixSoftmaxCrossEntropyOHEMLoss, self).forward(preds[0], target)\n        for i in range(1, len(preds)):\n            aux_loss = super(MixSoftmaxCrossEntropyOHEMLoss, self).forward(preds[i], target)\n            loss += self.aux_weight * aux_loss\n        return loss\n\n    def forward(self, *inputs):\n        preds, target = tuple(inputs)\n        inputs = tuple(list(preds) + [target])\n        if self.aux:\n            return dict(loss=self._aux_forward(*inputs))\n        else:\n            return dict(loss=super(MixSoftmaxCrossEntropyOHEMLoss, self).forward(*inputs))\n\n\nclass LovaszSoftmax(nn.Module):\n    def __init__(self, aux=True, aux_weight=0.2, ignore_index=-1, **kwargs):\n        super(LovaszSoftmax, self).__init__()\n        self.aux = aux\n        self.aux_weight = aux_weight\n        self.ignore_index = ignore_index\n\n    def _aux_forward(self, *inputs, **kwargs):\n        *preds, target = tuple(inputs)\n\n        loss = lovasz_softmax(F.softmax(preds[0], dim=1), target, ignore=self.ignore_index)\n        for i in range(1, len(preds)):\n            aux_loss = lovasz_softmax(F.softmax(preds[i], dim=1), target, ignore=self.ignore_index)\n            loss += self.aux_weight * aux_loss\n        return loss\n\n    def _multiple_forward(self, *inputs):\n        *preds, target = tuple(inputs)\n        loss = super(MixSoftmaxCrossEntropyLoss, self).forward(preds[0], target)\n        for i in range(1, len(preds)):\n            loss += super(MixSoftmaxCrossEntropyLoss, self).forward(preds[i], target)\n        return loss\n\n    def forward(self, *inputs, **kwargs):\n        preds, target = tuple(inputs)\n        inputs = tuple(list(preds) + [target])\n        if self.aux:\n            return dict(loss=self._aux_forward(*inputs))\n        elif len(preds) > 1:\n            return dict(loss=self._multiple_forward(*inputs))\n        else:\n            return dict(loss=super(MixSoftmaxCrossEntropyLoss, self).forward(*inputs))\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.5, gamma=2, weight=None, aux=True, aux_weight=0.2, ignore_index=-1,\n                 size_average=True):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.weight = weight\n        self.ignore_index = ignore_index\n        self.aux = aux\n        self.aux_weight = aux_weight\n        self.size_average = size_average\n        self.ce_fn = nn.CrossEntropyLoss(weight=self.weight, ignore_index=self.ignore_index)\n\n    def _aux_forward(self, *inputs, **kwargs):\n        *preds, target = tuple(inputs)\n\n        loss = self._base_forward(preds[0], target)\n        for i in range(1, len(preds)):\n            aux_loss = self._base_forward(preds[i], target)\n            loss += self.aux_weight * aux_loss\n        return loss\n\n    def _base_forward(self, output, target):\n\n        if output.dim() > 2:\n            output = output.contiguous().view(output.size(0), output.size(1), -1)\n            output = output.transpose(1, 2)\n            output = output.contiguous().view(-1, output.size(2)).squeeze()\n        if target.dim() == 4:\n            target = target.contiguous().view(target.size(0), target.size(1), -1)\n            target = target.transpose(1, 2)\n            target = target.contiguous().view(-1, target.size(2)).squeeze()\n        elif target.dim() == 3:\n            target = target.view(-1)\n        else:\n            target = target.view(-1, 1)\n\n        logpt = self.ce_fn(output, target)\n        pt = torch.exp(-logpt)\n        loss = ((1 - pt) ** self.gamma) * self.alpha * logpt\n        if self.size_average:\n            return loss.mean()\n        else:\n            return loss.sum()\n\n    def forward(self, *inputs, **kwargs):\n        preds, target = tuple(inputs)\n        inputs = tuple(list(preds) + [target])\n        return dict(loss=self._aux_forward(*inputs))\n\n\nclass BinaryDiceLoss(nn.Module):\n    """"""Dice loss of binary class\n    Args:\n        smooth: A float number to smooth loss, and avoid NaN error, default: 1\n        p: Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2\n        predict: A tensor of shape [N, *]\n        target: A tensor of shape same with predict\n        reduction: Reduction method to apply, return mean over batch if \'mean\',\n            return sum if \'sum\', return a tensor of shape [N,] if \'none\'\n    Returns:\n        Loss tensor according to arg reduction\n    Raise:\n        Exception if unexpected reduction\n    """"""\n    def __init__(self, smooth=1, p=2, reduction=\'mean\'):\n        super(BinaryDiceLoss, self).__init__()\n        self.smooth = smooth\n        self.p = p\n        self.reduction = reduction\n\n    def forward(self, predict, target, valid_mask):\n        assert predict.shape[0] == target.shape[0], ""predict & target batch size don\'t match""\n        predict = predict.contiguous().view(predict.shape[0], -1)\n        target = target.contiguous().view(target.shape[0], -1)\n        valid_mask = valid_mask.contiguous().view(valid_mask.shape[0], -1)\n\n        num = torch.sum(torch.mul(predict, target) * valid_mask, dim=1) * 2 + self.smooth\n        den = torch.sum((predict.pow(self.p) + target.pow(self.p)) * valid_mask, dim=1) + self.smooth\n\n        loss = 1 - num / den\n\n        if self.reduction == \'mean\':\n            return loss.mean()\n        elif self.reduction == \'sum\':\n            return loss.sum()\n        elif self.reduction == \'none\':\n            return loss\n        else:\n            raise Exception(\'Unexpected reduction {}\'.format(self.reduction))\n\n\nclass DiceLoss(nn.Module):\n    """"""Dice loss, need one hot encode input""""""\n\n    def __init__(self, weight=None, aux=True, aux_weight=0.4, ignore_index=-1, **kwargs):\n        super(DiceLoss, self).__init__()\n        self.kwargs = kwargs\n        self.weight = weight\n        self.ignore_index = ignore_index\n        self.aux = aux\n        self.aux_weight = aux_weight\n\n    def _base_forward(self, predict, target, valid_mask):\n\n        dice = BinaryDiceLoss(**self.kwargs)\n        total_loss = 0\n        predict = F.softmax(predict, dim=1)\n\n        for i in range(target.shape[-1]):\n            if i != self.ignore_index:\n                dice_loss = dice(predict[:, i], target[..., i], valid_mask)\n                if self.weight is not None:\n                    assert self.weight.shape[0] == target.shape[1], \\\n                        \'Expect weight shape [{}], get[{}]\'.format(target.shape[1], self.weight.shape[0])\n                    dice_loss *= self.weights[i]\n                total_loss += dice_loss\n\n        return total_loss / target.shape[-1]\n\n    def _aux_forward(self, *inputs, **kwargs):\n        *preds, target = tuple(inputs)\n        valid_mask = (target != self.ignore_index).long()\n        target_one_hot = F.one_hot(torch.clamp_min(target, 0))\n        loss = self._base_forward(preds[0], target_one_hot, valid_mask)\n        for i in range(1, len(preds)):\n            aux_loss = self._base_forward(preds[i], target_one_hot, valid_mask)\n            loss += self.aux_weight * aux_loss\n        return loss\n\n    def forward(self, *inputs):\n        preds, target = tuple(inputs)\n        inputs = tuple(list(preds) + [target])\n        return dict(loss=self._aux_forward(*inputs))\n\n\nclass PointRendLoss(nn.CrossEntropyLoss):\n    def __init__(self, aux=True, aux_weight=0.2, ignore_index=-1, **kwargs):\n        super(PointRendLoss, self).__init__(ignore_index=ignore_index)\n        self.aux = aux\n        self.aux_weight = aux_weight\n        self.ignore_index = ignore_index\n\n    def forward(self, *inputs, **kwargs):\n        result, gt = tuple(inputs)\n        \n        pred = F.interpolate(result[""coarse""], gt.shape[-2:], mode=""bilinear"", align_corners=True)\n        seg_loss = F.cross_entropy(pred, gt, ignore_index=self.ignore_index)\n\n        gt_points = point_sample(\n            gt.float().unsqueeze(1),\n            result[""points""],\n            mode=""nearest"",\n            align_corners=False\n        ).squeeze_(1).long()\n        points_loss = F.cross_entropy(result[""rend""], gt_points, ignore_index=self.ignore_index)\n\n        loss = seg_loss + points_loss\n\n        return dict(loss=loss)\n\n\ndef get_segmentation_loss(model, use_ohem=False, **kwargs):\n    if use_ohem:\n        return MixSoftmaxCrossEntropyOHEMLoss(**kwargs)\n    elif cfg.SOLVER.LOSS_NAME == \'lovasz\':\n        logging.info(\'Use lovasz loss!\')\n        return LovaszSoftmax(**kwargs)\n    elif cfg.SOLVER.LOSS_NAME == \'focal\':\n        logging.info(\'Use focal loss!\')\n        return FocalLoss(**kwargs)\n    elif cfg.SOLVER.LOSS_NAME == \'dice\':\n        logging.info(\'Use dice loss!\')\n        return DiceLoss(**kwargs)\n\n    model = model.lower()\n    if model == \'icnet\':\n        return ICNetLoss(**kwargs)\n    elif model == \'encnet\':\n        return EncNetLoss(**kwargs)\n    elif model == \'pointrend\':\n        logging.info(\'Use pointrend loss!\')\n        return PointRendLoss(**kwargs)\n    else:\n        return MixSoftmaxCrossEntropyLoss(**kwargs)\n'"
segmentron/solver/lovasz_losses.py,7,"b'""""""\nLovasz-Softmax and Jaccard hinge loss in PyTorch\nMaxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\nhttps://github.com/bermanmaxim/LovaszSoftmax/blob/master/pytorch/lovasz_losses.py\n""""""\n\nfrom __future__ import print_function, division\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport numpy as np\n\ntry:\n    from itertools import ifilterfalse\nexcept ImportError:  # py3k\n    from itertools import filterfalse as ifilterfalse\n\n\ndef lovasz_grad(gt_sorted):\n    """"""\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    """"""\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.float().cumsum(0)\n    union = gts + (1 - gt_sorted).float().cumsum(0)\n    jaccard = 1. - intersection / union\n    if p > 1:  # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\n\ndef iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n    """"""\n    IoU for foreground class\n    binary: 1 foreground, 0 background\n    """"""\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        intersection = ((label == 1) & (pred == 1)).sum()\n        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n        if not union:\n            iou = EMPTY\n        else:\n            iou = float(intersection) / float(union)\n        ious.append(iou)\n    iou = mean(ious)  # mean accross images if per_image\n    return 100 * iou\n\n\ndef iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n    """"""\n    Array of IoU for each (non ignored) class\n    """"""\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        iou = []\n        for i in range(C):\n            if i != ignore:  # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n                intersection = ((label == i) & (pred == i)).sum()\n                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n                if not union:\n                    iou.append(EMPTY)\n                else:\n                    iou.append(float(intersection) / float(union))\n        ious.append(iou)\n    ious = [mean(iou) for iou in zip(*ious)]  # mean accross images if per_image\n    return 100 * np.array(ious)\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    """"""\n    if per_image:\n        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n                    for log, lab in zip(logits, labels))\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat(logits, labels):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    """"""\n    if len(labels) == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum() * 0.\n    signs = 2. * labels.float() - 1.\n    errors = (1. - logits * Variable(signs))\n    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n    perm = perm.data\n    gt_sorted = labels[perm]\n    grad = lovasz_grad(gt_sorted)\n    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to \'ignore\'\n    """"""\n    scores = scores.view(-1)\n    labels = labels.view(-1)\n    if ignore is None:\n        return scores, labels\n    valid = (labels != ignore)\n    vscores = scores[valid]\n    vlabels = labels[valid]\n    return vscores, vlabels\n\n\nclass StableBCELoss(torch.nn.modules.Module):\n    def __init__(self):\n        super(StableBCELoss, self).__init__()\n\n    def forward(self, input, target):\n        neg_abs = - input.abs()\n        loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n        return loss.mean()\n\n\ndef binary_xloss(logits, labels, ignore=None):\n    """"""\n    Binary Cross entropy loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      ignore: void class id\n    """"""\n    logits, labels = flatten_binary_scores(logits, labels, ignore)\n    loss = StableBCELoss()(logits, Variable(labels.float()))\n    return loss\n\n\n# --------------------------- MULTICLASS LOSSES ---------------------------\n\n\ndef lovasz_softmax(probas, labels, classes=\'present\', per_image=False, ignore=None):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1).\n              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n      classes: \'all\' for all, \'present\' for classes present in labels, or a list of classes to average.\n      per_image: compute the loss per image instead of per batch\n      ignore: void class labels\n    """"""\n    if per_image:\n        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes)\n                    for prob, lab in zip(probas, labels))\n    else:\n        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), classes=classes)\n    return loss\n\n\ndef lovasz_softmax_flat(probas, labels, classes=\'present\'):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n      classes: \'all\' for all, \'present\' for classes present in labels, or a list of classes to average.\n    """"""\n    if probas.numel() == 0:\n        # only void pixels, the gradients should be 0\n        return probas * 0.\n    C = probas.size(1)\n    losses = []\n    class_to_sum = list(range(C)) if classes in [\'all\', \'present\'] else classes\n    for c in class_to_sum:\n        fg = (labels == c).float()  # foreground for class c\n        if classes == \'present\' and fg.sum() == 0:\n            continue\n        if C == 1:\n            if len(classes) > 1:\n                raise ValueError(\'Sigmoid output possible only with 1 class\')\n            class_pred = probas[:, 0]\n        else:\n            class_pred = probas[:, c]\n        errors = (Variable(fg) - class_pred).abs()\n        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n        perm = perm.data\n        fg_sorted = fg[perm]\n        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n    return mean(losses)\n\n\ndef flatten_probas(probas, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch\n    """"""\n    if probas.dim() == 3:\n        # assumes output of a sigmoid layer\n        B, H, W = probas.size()\n        probas = probas.view(B, 1, H, W)\n    B, C, H, W = probas.size()\n    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n    labels = labels.view(-1)\n    if ignore is None:\n        return probas, labels\n    valid = (labels != ignore)\n    vprobas = probas[valid.nonzero().squeeze()]\n    vlabels = labels[valid]\n    return vprobas, vlabels\n\n\ndef xloss(logits, labels, ignore=None):\n    """"""\n    Cross entropy loss\n    """"""\n    return F.cross_entropy(logits, Variable(labels), ignore_index=255)\n\n\n# --------------------------- HELPER FUNCTIONS ---------------------------\ndef isnan(x):\n    return x != x\n\n\ndef mean(l, ignore_nan=False, empty=0):\n    """"""\n    nanmean compatible with generators.\n    """"""\n    l = iter(l)\n    if ignore_nan:\n        l = ifilterfalse(isnan, l)\n    try:\n        n = 1\n        acc = next(l)\n    except StopIteration:\n        if empty == \'raise\':\n            raise ValueError(\'Empty mean\')\n        return empty\n    for n, v in enumerate(l, 2):\n        acc += v\n    if n == 1:\n        return acc\n    return acc / n\n'"
segmentron/solver/lr_scheduler.py,5,"b'# this code heavily reference: detectron2\nfrom __future__ import division\nimport math\nimport torch\n\nfrom typing import List\nfrom bisect import bisect_right\nfrom segmentron.config import cfg\n\n__all__ = [\'get_scheduler\']\n\n\nclass WarmupPolyLR(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, target_lr=0, max_iters=0, power=0.9, warmup_factor=1.0 / 3,\n                 warmup_iters=500, warmup_method=\'linear\', last_epoch=-1):\n        if warmup_method not in (""constant"", ""linear""):\n            raise ValueError(\n                ""Only \'constant\' or \'linear\' warmup_method accepted ""\n                ""got {}"".format(warmup_method))\n\n        self.target_lr = target_lr\n        self.max_iters = max_iters\n        self.power = power\n        self.warmup_factor = warmup_factor\n        self.warmup_iters = warmup_iters\n        self.warmup_method = warmup_method\n\n        super(WarmupPolyLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        N = self.max_iters - self.warmup_iters\n        T = self.last_epoch - self.warmup_iters\n        if self.last_epoch < self.warmup_iters:\n            if self.warmup_method == \'constant\':\n                warmup_factor = self.warmup_factor\n            elif self.warmup_method == \'linear\':\n                alpha = float(self.last_epoch) / self.warmup_iters\n                warmup_factor = self.warmup_factor * (1 - alpha) + alpha\n            else:\n                raise ValueError(""Unknown warmup type."")\n            return [self.target_lr + (base_lr - self.target_lr) * warmup_factor for base_lr in self.base_lrs]\n        factor = pow(1 - T / N, self.power)\n        return [self.target_lr + (base_lr - self.target_lr) * factor for base_lr in self.base_lrs]\n\n\nclass WarmupMultiStepLR(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(\n        self,\n        optimizer: torch.optim.Optimizer,\n        milestones: List[int],\n        gamma: float = 0.1,\n        warmup_factor: float = 0.001,\n        warmup_iters: int = 1000,\n        warmup_method: str = ""linear"",\n        last_epoch: int = -1,\n    ):\n        if not list(milestones) == sorted(milestones):\n            raise ValueError(\n                ""Milestones should be a list of"" "" increasing integers. Got {}"", milestones\n            )\n        self.milestones = milestones\n        self.gamma = gamma\n        self.warmup_factor = warmup_factor\n        self.warmup_iters = warmup_iters\n        self.warmup_method = warmup_method\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self) -> List[float]:\n        warmup_factor = _get_warmup_factor_at_iter(\n            self.warmup_method, self.last_epoch, self.warmup_iters, self.warmup_factor\n        )\n        return [\n            base_lr * warmup_factor * self.gamma ** bisect_right(self.milestones, self.last_epoch)\n            for base_lr in self.base_lrs\n        ]\n\n    def _compute_values(self) -> List[float]:\n        # The new interface\n        return self.get_lr()\n\n\nclass WarmupCosineLR(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(\n        self,\n        optimizer: torch.optim.Optimizer,\n        max_iters: int,\n        warmup_factor: float = 0.001,\n        warmup_iters: int = 1000,\n        warmup_method: str = ""linear"",\n        last_epoch: int = -1,\n    ):\n        self.max_iters = max_iters\n        self.warmup_factor = warmup_factor\n        self.warmup_iters = warmup_iters\n        self.warmup_method = warmup_method\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self) -> List[float]:\n        warmup_factor = _get_warmup_factor_at_iter(\n            self.warmup_method, self.last_epoch, self.warmup_iters, self.warmup_factor\n        )\n        # Different definitions of half-cosine with warmup are possible. For\n        # simplicity we multiply the standard half-cosine schedule by the warmup\n        # factor. An alternative is to start the period of the cosine at warmup_iters\n        # instead of at 0. In the case that warmup_iters << max_iters the two are\n        # very close to each other.\n        return [\n            base_lr\n            * warmup_factor\n            * 0.5\n            * (1.0 + math.cos(math.pi * self.last_epoch / self.max_iters))\n            for base_lr in self.base_lrs\n        ]\n\n    def _compute_values(self) -> List[float]:\n        # The new interface\n        return self.get_lr()\n\n\ndef _get_warmup_factor_at_iter(\n    method: str, iter: int, warmup_iters: int, warmup_factor: float\n) -> float:\n    """"""\n    Return the learning rate warmup factor at a specific iteration.\n    See https://arxiv.org/abs/1706.02677 for more details.\n\n    Args:\n        method (str): warmup method; either ""constant"" or ""linear"".\n        iter (int): iteration at which to calculate the warmup factor.\n        warmup_iters (int): the number of warmup iterations.\n        warmup_factor (float): the base warmup factor (the meaning changes according\n            to the method used).\n\n    Returns:\n        float: the effective warmup factor at the given iteration.\n    """"""\n    if iter >= warmup_iters:\n        return 1.0\n\n    if method == ""constant"":\n        return warmup_factor\n    elif method == ""linear"":\n        alpha = iter / warmup_iters\n        return warmup_factor * (1 - alpha) + alpha\n    else:\n        raise ValueError(""Unknown warmup method: {}"".format(method))\n\n\ndef get_scheduler(optimizer, max_iters, iters_per_epoch):\n    mode = cfg.SOLVER.LR_SCHEDULER.lower()\n    warm_up_iters = iters_per_epoch * cfg.SOLVER.WARMUP.EPOCHS\n    if mode == \'poly\':\n        return WarmupPolyLR(optimizer, max_iters=max_iters, power=cfg.SOLVER.POLY.POWER,\n                            warmup_factor=cfg.SOLVER.WARMUP.FACTOR, warmup_iters=warm_up_iters,\n                            warmup_method=cfg.SOLVER.WARMUP.METHOD)\n    elif mode == \'cosine\':\n        return WarmupCosineLR(optimizer, max_iters=max_iters, warmup_factor=cfg.SOLVER.WARMUP.FACTOR,\n                              warmup_iters=warm_up_iters, warmup_method=cfg.SOLVER.WARMUP.METHOD)\n    elif mode == \'step\':\n        milestones = [x * iters_per_epoch for x in cfg.SOLVER.STEP.DECAY_EPOCH]\n        return WarmupMultiStepLR(optimizer, milestones=milestones, gamma=cfg.SOLVER.STEP.GAMMA,\n                                 warmup_factor=cfg.SOLVER.WARMUP.FACTOR, warmup_iters=warm_up_iters,\n                                 warmup_method=cfg.SOLVER.WARMUP.METHOD)\n    else:\n        raise ValueError(""not support lr scheduler method!"")\n\n'"
segmentron/solver/optimizer.py,1,"b'import logging\nimport torch.nn as nn\n\nfrom torch import optim\nfrom segmentron.config import cfg\n\n\ndef _set_batch_norm_attr(named_modules, attr, value):\n    for m in named_modules:\n        if isinstance(m[1], (nn.BatchNorm2d, nn.SyncBatchNorm)):\n            setattr(m[1], attr, value)\n\n\ndef _get_paramters(model):\n    params_list = list()\n    if hasattr(model, \'encoder\') and model.encoder is not None and hasattr(model, \'decoder\'):\n        params_list.append({\'params\': model.encoder.parameters(), \'lr\': cfg.SOLVER.LR})\n        if cfg.MODEL.BN_EPS_FOR_ENCODER:\n            logging.info(\'Set bn custom eps for bn in encoder: {}\'.format(cfg.MODEL.BN_EPS_FOR_ENCODER))\n            _set_batch_norm_attr(model.encoder.named_modules(), \'eps\', cfg.MODEL.BN_EPS_FOR_ENCODER)\n\n        for module in model.decoder:\n            params_list.append({\'params\': getattr(model, module).parameters(),\n                                \'lr\': cfg.SOLVER.LR * cfg.SOLVER.DECODER_LR_FACTOR})\n\n        if cfg.MODEL.BN_EPS_FOR_DECODER:\n            logging.info(\'Set bn custom eps for bn in decoder: {}\'.format(cfg.MODEL.BN_EPS_FOR_DECODER))\n            for module in model.decoder:\n                _set_batch_norm_attr(getattr(model, module).named_modules(), \'eps\',\n                                         cfg.MODEL.BN_EPS_FOR_DECODER)\n    else:\n        logging.info(\'Model do not have encoder or decoder, params list was from model.parameters(), \'\n                     \'and arguments BN_EPS_FOR_ENCODER, BN_EPS_FOR_DECODER, DECODER_LR_FACTOR not used!\')\n        params_list = model.parameters()\n\n    if cfg.MODEL.BN_MOMENTUM and cfg.MODEL.BN_TYPE in [\'BN\']:\n        logging.info(\'Set bn custom momentum: {}\'.format(cfg.MODEL.BN_MOMENTUM))\n        _set_batch_norm_attr(model.named_modules(), \'momentum\', cfg.MODEL.BN_MOMENTUM)\n    elif cfg.MODEL.BN_MOMENTUM and cfg.MODEL.BN_TYPE not in [\'BN\']:\n        logging.info(\'Batch norm type is {}, custom bn momentum is not effective!\'.format(cfg.MODEL.BN_TYPE))\n\n    return params_list\n\n\ndef get_optimizer(model):\n    parameters = _get_paramters(model)\n    opt_lower = cfg.SOLVER.OPTIMIZER.lower()\n\n    if opt_lower == \'sgd\':\n        optimizer = optim.SGD(\n            parameters, lr=cfg.SOLVER.LR, momentum=cfg.SOLVER.MOMENTUM, weight_decay=cfg.SOLVER.WEIGHT_DECAY)\n    elif opt_lower == \'adam\':\n        optimizer = optim.Adam(\n            parameters, lr=cfg.SOLVER.LR, eps=cfg.SOLVER.EPSILON, weight_decay=cfg.SOLVER.WEIGHT_DECAY)\n    elif opt_lower == \'adadelta\':\n        optimizer = optim.Adadelta(\n            parameters, lr=cfg.SOLVER.LR, eps=cfg.SOLVER.EPSILON, weight_decay=cfg.SOLVER.WEIGHT_DECAY)\n    elif opt_lower == \'rmsprop\':\n        optimizer = optim.RMSprop(\n            parameters, lr=cfg.SOLVER.LR, alpha=0.9, eps=cfg.SOLVER.EPSILON,\n            momentum=cfg.SOLVER.MOMENTUM, weight_decay=cfg.SOLVER.WEIGHT_DECAY)\n    else:\n        raise ValueError(""Expected optimizer method in [sgd, adam, adadelta, rmsprop], but received ""\n                         ""{}"".format(opt_lower))\n\n    return optimizer\n'"
segmentron/utils/__init__.py,0,"b'""""""Utility functions.""""""\nfrom __future__ import absolute_import\n\nfrom .download import download, check_sha1\nfrom .filesystem import makedirs\n'"
segmentron/utils/default_setup.py,4,"b'import os\nimport logging\nimport json\nimport torch\n\nfrom .distributed import get_rank, synchronize\nfrom .logger import setup_logger\nfrom .env import seed_all_rng\nfrom ..config import cfg\n\ndef default_setup(args):\n    num_gpus = int(os.environ[""WORLD_SIZE""]) if ""WORLD_SIZE"" in os.environ else 1\n    args.num_gpus = num_gpus\n    args.distributed = num_gpus > 1\n\n    if not args.no_cuda and torch.cuda.is_available():\n        # cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n        args.device = ""cuda""\n    else:\n        args.distributed = False\n        args.device = ""cpu""\n    if args.distributed:\n        torch.cuda.set_device(args.local_rank)\n        torch.distributed.init_process_group(backend=""nccl"", init_method=""env://"")\n        synchronize()\n\n    # TODO\n    # if args.save_pred:\n    #     outdir = \'../runs/pred_pic/{}_{}_{}\'.format(args.model, args.backbone, args.dataset)\n    #     if not os.path.exists(outdir):\n    #         os.makedirs(outdir)\n\n    save_dir = cfg.TRAIN.LOG_SAVE_DIR if cfg.PHASE == \'train\' else None\n    setup_logger(""Segmentron"", save_dir, get_rank(), filename=\'{}_{}_{}_{}_log.txt\'.format(\n        cfg.MODEL.MODEL_NAME, cfg.MODEL.BACKBONE, cfg.DATASET.NAME, cfg.TIME_STAMP))\n\n    logging.info(""Using {} GPUs"".format(num_gpus))\n    logging.info(args)\n    logging.info(json.dumps(cfg, indent=8))\n\n    seed_all_rng(None if cfg.SEED < 0 else cfg.SEED + get_rank())'"
segmentron/utils/distributed.py,18,"b'""""""\ncode is heavily based on https://github.com/facebookresearch/maskrcnn-benchmark\n""""""\nimport math\nimport pickle\nimport torch\nimport torch.utils.data as data\nimport torch.distributed as dist\n\nfrom torch.utils.data.sampler import Sampler, BatchSampler\n\n__all__ = [\'get_world_size\', \'get_rank\', \'synchronize\', \'is_main_process\',\n           \'all_gather\', \'make_data_sampler\', \'make_batch_data_sampler\',\n           \'reduce_dict\', \'reduce_loss_dict\']\n\n\ndef get_world_size():\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank():\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_main_process():\n    return get_rank() == 0\n\n\ndef synchronize():\n    """"""\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    """"""\n    if not dist.is_available():\n        return\n    if not dist.is_initialized():\n        return\n    world_size = dist.get_world_size()\n    if world_size == 1:\n        return\n    dist.barrier()\n\n\ndef all_gather(data):\n    """"""\n    Run all_gather on arbitrary picklable data (not necessarily tensors)\n    Args:\n        data: any picklable object\n    Returns:\n        list[data]: list of data gathered from each rank\n    """"""\n    world_size = get_world_size()\n    if world_size == 1:\n        return [data]\n\n    # serialized to a Tensor\n    buffer = pickle.dumps(data)\n    storage = torch.ByteStorage.from_buffer(buffer)\n    tensor = torch.ByteTensor(storage).to(""cuda"")\n\n    # obtain Tensor size of each rank\n    local_size = torch.IntTensor([tensor.numel()]).to(""cuda"")\n    size_list = [torch.IntTensor([0]).to(""cuda"") for _ in range(world_size)]\n    dist.all_gather(size_list, local_size)\n    size_list = [int(size.item()) for size in size_list]\n    max_size = max(size_list)\n\n    # receiving Tensor from all ranks\n    # we pad the tensor because torch all_gather does not support\n    # gathering tensors of different shapes\n    tensor_list = []\n    for _ in size_list:\n        tensor_list.append(torch.ByteTensor(size=(max_size,)).to(""cuda""))\n    if local_size != max_size:\n        padding = torch.ByteTensor(size=(max_size - local_size,)).to(""cuda"")\n        tensor = torch.cat((tensor, padding), dim=0)\n    dist.all_gather(tensor_list, tensor)\n\n    data_list = []\n    for size, tensor in zip(size_list, tensor_list):\n        buffer = tensor.cpu().numpy().tobytes()[:size]\n        data_list.append(pickle.loads(buffer))\n\n    return data_list\n\n\ndef reduce_dict(input_dict, average=True):\n    """"""\n    Args:\n        input_dict (dict): all the values will be reduced\n        average (bool): whether to do average or sum\n    Reduce the values in the dictionary from all processes so that process with rank\n    0 has the averaged results. Returns a dict with the same fields as\n    input_dict, after reduction.\n    """"""\n    world_size = get_world_size()\n    if world_size < 2:\n        return input_dict\n    with torch.no_grad():\n        names = []\n        values = []\n        # sort the keys so that they are consistent across processes\n        for k in sorted(input_dict.keys()):\n            names.append(k)\n            values.append(input_dict[k])\n        values = torch.stack(values, dim=0)\n        dist.reduce(values, dst=0)\n        if dist.get_rank() == 0 and average:\n            # only main process gets accumulated, so only divide by\n            # world_size in this case\n            values /= world_size\n        reduced_dict = {k: v for k, v in zip(names, values)}\n    return reduced_dict\n\n\ndef reduce_loss_dict(loss_dict):\n    """"""\n    Reduce the loss dictionary from all processes so that process with rank\n    0 has the averaged results. Returns a dict with the same fields as\n    loss_dict, after reduction.\n    """"""\n    world_size = get_world_size()\n    if world_size < 2:\n        return loss_dict\n    with torch.no_grad():\n        loss_names = []\n        all_losses = []\n        for k in sorted(loss_dict.keys()):\n            loss_names.append(k)\n            all_losses.append(loss_dict[k])\n        all_losses = torch.stack(all_losses, dim=0)\n        dist.reduce(all_losses, dst=0)\n        if dist.get_rank() == 0:\n            # only main process gets accumulated, so only divide by\n            # world_size in this case\n            all_losses /= world_size\n        reduced_losses = {k: v for k, v in zip(loss_names, all_losses)}\n    return reduced_losses\n\n\ndef make_data_sampler(dataset, shuffle, distributed):\n    if distributed:\n        return DistributedSampler(dataset, shuffle=shuffle)\n    if shuffle:\n        sampler = data.sampler.RandomSampler(dataset)\n    else:\n        sampler = data.sampler.SequentialSampler(dataset)\n    return sampler\n\n\ndef make_batch_data_sampler(sampler, images_per_batch, num_iters=None, start_iter=0, drop_last=True):\n    batch_sampler = data.sampler.BatchSampler(sampler, images_per_batch, drop_last=drop_last)\n    if num_iters is not None:\n        batch_sampler = IterationBasedBatchSampler(batch_sampler, num_iters, start_iter)\n    return batch_sampler\n\n\nclass DistributedSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        if self.shuffle:\n            # deterministically shuffle based on epoch\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n        else:\n            indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        indices += indices[: (self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset: offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n\n\nclass IterationBasedBatchSampler(BatchSampler):\n    """"""\n    Wraps a BatchSampler, resampling from it until\n    a specified number of iterations have been sampled\n    """"""\n\n    def __init__(self, batch_sampler, num_iterations, start_iter=0):\n        self.batch_sampler = batch_sampler\n        self.num_iterations = num_iterations\n        self.start_iter = start_iter\n\n    def __iter__(self):\n        iteration = self.start_iter\n        while iteration <= self.num_iterations:\n            # if the underlying sampler has a set_epoch method, like\n            # DistributedSampler, used for making each process see\n            # a different split of the dataset, then set it\n            if hasattr(self.batch_sampler.sampler, ""set_epoch""):\n                self.batch_sampler.sampler.set_epoch(iteration)\n            for batch in self.batch_sampler:\n                iteration += 1\n                if iteration > self.num_iterations:\n                    break\n                yield batch\n\n    def __len__(self):\n        return self.num_iterations\n'"
segmentron/utils/download.py,0,"b'import os\nimport hashlib\nimport requests\nfrom tqdm import tqdm\n\ndef check_sha1(filename, sha1_hash):\n    """"""Check whether the sha1 hash of the file content matches the expected hash.\n    Parameters\n    ----------\n    filename : str\n        Path to the file.\n    sha1_hash : str\n        Expected sha1 hash in hexadecimal digits.\n    Returns\n    -------\n    bool\n        Whether the file content matches the expected hash.\n    """"""\n    sha1 = hashlib.sha1()\n    with open(filename, \'rb\') as f:\n        while True:\n            data = f.read(1048576)\n            if not data:\n                break\n            sha1.update(data)\n\n    sha1_file = sha1.hexdigest()\n    l = min(len(sha1_file), len(sha1_hash))\n    return sha1.hexdigest()[0:l] == sha1_hash[0:l]\n\ndef download(url, path=None, overwrite=False, sha1_hash=None):\n    """"""Download an given URL\n    Parameters\n    ----------\n    url : str\n        URL to download\n    path : str, optional\n        Destination path to store downloaded file. By default stores to the\n        current directory with same name as in url.\n    overwrite : bool, optional\n        Whether to overwrite destination file if already exists.\n    sha1_hash : str, optional\n        Expected sha1 hash in hexadecimal digits. Will ignore existing file when hash is specified\n        but doesn\'t match.\n    Returns\n    -------\n    str\n        The file path of the downloaded file.\n    """"""\n    if path is None:\n        fname = url.split(\'/\')[-1]\n    else:\n        path = os.path.expanduser(path)\n        if os.path.isdir(path):\n            fname = os.path.join(path, url.split(\'/\')[-1])\n        else:\n            fname = path\n\n    if overwrite or not os.path.exists(fname) or (sha1_hash and not check_sha1(fname, sha1_hash)):\n        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n        print(\'Downloading %s from %s...\'%(fname, url))\n        r = requests.get(url, stream=True)\n        if r.status_code != 200:\n            raise RuntimeError(""Failed downloading url %s""%url)\n        total_length = r.headers.get(\'content-length\')\n        with open(fname, \'wb\') as f:\n            if total_length is None: # no content length header\n                for chunk in r.iter_content(chunk_size=1024):\n                    if chunk: # filter out keep-alive new chunks\n                        f.write(chunk)\n            else:\n                total_length = int(total_length)\n                for chunk in tqdm(r.iter_content(chunk_size=1024),\n                                  total=int(total_length / 1024. + 0.5),\n                                  unit=\'KB\', unit_scale=False, dynamic_ncols=True):\n                    f.write(chunk)\n\n        if sha1_hash and not check_sha1(fname, sha1_hash):\n            raise UserWarning(\'File {} is downloaded but the content hash does not match. \' \\\n                              \'The repo may be outdated or download may be incomplete. \' \\\n                              \'If the ""repo_url"" is overridden, consider switching to \' \\\n                              \'the default repo.\'.format(fname))\n\n    return fname'"
segmentron/utils/env.py,1,"b'# this code heavily based on detectron2\n\nimport logging\nimport numpy as np\nimport os\nimport random\nfrom datetime import datetime\nimport torch\n\n__all__ = [""seed_all_rng""]\n\n\ndef seed_all_rng(seed=None):\n    """"""\n    Set the random seed for the RNG in torch, numpy and python.\n\n    Args:\n        seed (int): if None, will use a strong random seed.\n    """"""\n    if seed is None:\n        seed = (\n            os.getpid()\n            + int(datetime.now().strftime(""%S%f""))\n            + int.from_bytes(os.urandom(2), ""big"")\n        )\n        logger = logging.getLogger(__name__)\n        logger.info(""Using a generated random seed {}"".format(seed))\n    np.random.seed(seed)\n    torch.set_rng_state(torch.manual_seed(seed).get_state())\n    random.seed(seed)\n'"
segmentron/utils/filesystem.py,2,"b'""""""Filesystem utility functions.""""""\nfrom __future__ import absolute_import\nimport os\nimport errno\nimport torch\nimport logging\n\nfrom ..config import cfg\n\ndef save_checkpoint(model, epoch, optimizer=None, lr_scheduler=None, is_best=False):\n    """"""Save Checkpoint""""""\n    directory = os.path.expanduser(cfg.TRAIN.MODEL_SAVE_DIR)\n    directory = os.path.join(directory, \'{}_{}_{}_{}\'.format(cfg.MODEL.MODEL_NAME, cfg.MODEL.BACKBONE,\n                                                             cfg.DATASET.NAME, cfg.TIME_STAMP))\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    filename = \'{}.pth\'.format(str(epoch))\n    filename = os.path.join(directory, filename)\n    model_state_dict = model.module.state_dict() if hasattr(model, \'module\') else model.state_dict()\n    if is_best:\n        best_filename = \'best_model.pth\'\n        best_filename = os.path.join(directory, best_filename)\n        torch.save(model_state_dict, best_filename)\n    else:\n        save_state = {\n            \'epoch\': epoch,\n            \'state_dict\': model_state_dict,\n            \'optimizer\': optimizer.state_dict(),\n            \'lr_scheduler\': lr_scheduler.state_dict()\n        }\n        if not os.path.exists(filename):\n            torch.save(save_state, filename)\n            logging.info(\'Epoch {} model saved in: {}\'.format(epoch, filename))\n\n        # remove last epoch\n        pre_filename = \'{}.pth\'.format(str(epoch - 1))\n        pre_filename = os.path.join(directory, pre_filename)\n        try:\n            if os.path.exists(pre_filename):\n                os.remove(pre_filename)\n        except OSError as e:\n            logging.info(e)\n\ndef makedirs(path):\n    """"""Create directory recursively if not exists.\n    Similar to `makedir -p`, you can skip checking existence before this function.\n    Parameters\n    ----------\n    path : str\n        Path of the desired dir\n    """"""\n    try:\n        os.makedirs(path)\n    except OSError as exc:\n        if exc.errno != errno.EEXIST:\n            raise\n\n'"
segmentron/utils/logger.py,0,"b'import logging\nimport os\nimport sys\n\n__all__ = [\'setup_logger\']\n\n\ndef setup_logger(name, save_dir, distributed_rank, filename=""log.txt"", mode=\'w\'):\n    if distributed_rank > 0:\n        return\n\n    logging.root.name = name\n    logging.root.setLevel(logging.INFO)\n    # don\'t log results for the non-master process\n    ch = logging.StreamHandler(stream=sys.stdout)\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(""%(asctime)s %(name)s %(levelname)s: %(message)s"")\n    ch.setFormatter(formatter)\n    logging.root.addHandler(ch)\n\n    if save_dir:\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        fh = logging.FileHandler(os.path.join(save_dir, filename), mode=mode)  # \'a+\' for add, \'w\' for overwrite\n        fh.setLevel(logging.DEBUG)\n        fh.setFormatter(formatter)\n        logging.root.addHandler(fh)\n'"
segmentron/utils/options.py,0,"b'import argparse\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Segmentron\')\n    parser.add_argument(\'--config-file\', metavar=""FILE"",\n                        help=\'config file path\')\n    # cuda setting\n    parser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                        help=\'disables CUDA training\')\n    parser.add_argument(\'--local_rank\', type=int, default=0)\n    # checkpoint and log\n    parser.add_argument(\'--resume\', type=str, default=None,\n                        help=\'put the path to resuming file if needed\')\n    parser.add_argument(\'--log-iter\', type=int, default=10,\n                        help=\'print log every log-iter\')\n    # for evaluation\n    parser.add_argument(\'--val-epoch\', type=int, default=1,\n                        help=\'run validation every val-epoch\')\n    parser.add_argument(\'--skip-val\', action=\'store_true\', default=False,\n                        help=\'skip validation during training\')\n    # for visual\n    parser.add_argument(\'--input-img\', type=str, default=\'tools/demo_vis.png\',\n                        help=\'path to the input image or a directory of images\')\n    # config options\n    parser.add_argument(\'opts\', help=\'See config for all options\',\n                        default=None, nargs=argparse.REMAINDER)\n    args = parser.parse_args()\n\n    return args'"
segmentron/utils/parallel.py,11,"b'""""""Utils for Semantic Segmentation""""""\nimport threading\nimport torch\nimport torch.cuda.comm as comm\nfrom torch.nn.parallel.data_parallel import DataParallel\nfrom torch.nn.parallel._functions import Broadcast\nfrom torch.autograd import Function\n\n__all__ = [\'DataParallelModel\', \'DataParallelCriterion\']\n\n\nclass Reduce(Function):\n    @staticmethod\n    def forward(ctx, *inputs):\n        ctx.target_gpus = [inputs[i].get_device() for i in range(len(inputs))]\n        inputs = sorted(inputs, key=lambda i: i.get_device())\n        return comm.reduce_add(inputs)\n\n    @staticmethod\n    def backward(ctx, gradOutputs):\n        return Broadcast.apply(ctx.target_gpus, gradOutputs)\n\n\nclass DataParallelModel(DataParallel):\n    """"""Data parallelism\n\n    Hide the difference of single/multiple GPUs to the user.\n    In the forward pass, the module is replicated on each device,\n    and each replica handles a portion of the input. During the backwards\n    pass, gradients from each replica are summed into the original module.\n\n    The batch size should be larger than the number of GPUs used.\n\n    Parameters\n    ----------\n    module : object\n        Network to be parallelized.\n    sync : bool\n        enable synchronization (default: False).\n    Inputs:\n        - **inputs**: list of input\n    Outputs:\n        - **outputs**: list of output\n    Example::\n        >>> net = DataParallelModel(model, device_ids=[0, 1, 2])\n        >>> output = net(input_var)  # input_var can be on any device, including CPU\n    """"""\n\n    def gather(self, outputs, output_device):\n        return outputs\n\n    def replicate(self, module, device_ids):\n        modules = super(DataParallelModel, self).replicate(module, device_ids)\n        return modules\n\n\n# Reference: https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/parallel.py\nclass DataParallelCriterion(DataParallel):\n    """"""\n    Calculate loss in multiple-GPUs, which balance the memory usage for\n    Semantic Segmentation.\n\n    The targets are splitted across the specified devices by chunking in\n    the batch dimension. Please use together with :class:`encoding.parallel.DataParallelModel`.\n\n    Example::\n        >>> net = DataParallelModel(model, device_ids=[0, 1, 2])\n        >>> criterion = DataParallelCriterion(criterion, device_ids=[0, 1, 2])\n        >>> y = net(x)\n        >>> loss = criterion(y, target)\n    """"""\n\n    def forward(self, inputs, *targets, **kwargs):\n        # the inputs should be the outputs of DataParallelModel\n        if not self.device_ids:\n            return self.module(inputs, *targets, **kwargs)\n        targets, kwargs = self.scatter(targets, kwargs, self.device_ids)\n        if len(self.device_ids) == 1:\n            return self.module(inputs, *targets[0], **kwargs[0])\n        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n        outputs = criterion_parallel_apply(replicas, inputs, targets, kwargs)\n        return Reduce.apply(*outputs) / len(outputs)\n\n\ndef get_a_var(obj):\n    if isinstance(obj, torch.Tensor):\n        return obj\n\n    if isinstance(obj, list) or isinstance(obj, tuple):\n        for result in map(get_a_var, obj):\n            if isinstance(result, torch.Tensor):\n                return result\n\n    if isinstance(obj, dict):\n        for result in map(get_a_var, obj.items()):\n            if isinstance(result, torch.Tensor):\n                return result\n    return None\n\n\ndef criterion_parallel_apply(modules, inputs, targets, kwargs_tup=None, devices=None):\n    r""""""Applies each `module` in :attr:`modules` in parallel on arguments\n    contained in :attr:`inputs` (positional), attr:\'targets\' (positional) and :attr:`kwargs_tup` (keyword)\n    on each of :attr:`devices`.\n\n    Args:\n        modules (Module): modules to be parallelized\n        inputs (tensor): inputs to the modules\n        targets (tensor): targets to the modules\n        devices (list of int or torch.device): CUDA devices\n    :attr:`modules`, :attr:`inputs`, :attr:\'targets\' :attr:`kwargs_tup` (if given), and\n    :attr:`devices` (if given) should all have same length. Moreover, each\n    element of :attr:`inputs` can either be a single object as the only argument\n    to a module, or a collection of positional arguments.\n    """"""\n    assert len(modules) == len(inputs)\n    assert len(targets) == len(inputs)\n    if kwargs_tup is not None:\n        assert len(modules) == len(kwargs_tup)\n    else:\n        kwargs_tup = ({},) * len(modules)\n    if devices is not None:\n        assert len(modules) == len(devices)\n    else:\n        devices = [None] * len(modules)\n    lock = threading.Lock()\n    results = {}\n    grad_enabled = torch.is_grad_enabled()\n\n    def _worker(i, module, input, target, kwargs, device=None):\n        torch.set_grad_enabled(grad_enabled)\n        if device is None:\n            device = get_a_var(input).get_device()\n        try:\n            with torch.cuda.device(device):\n                output = module(*(list(input) + target), **kwargs)\n            with lock:\n                results[i] = output\n        except Exception as e:\n            with lock:\n                results[i] = e\n\n    if len(modules) > 1:\n        threads = [threading.Thread(target=_worker,\n                                    args=(i, module, input, target, kwargs, device))\n                   for i, (module, input, target, kwargs, device) in\n                   enumerate(zip(modules, inputs, targets, kwargs_tup, devices))]\n\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n    else:\n        _worker(0, modules[0], inputs[0], targets[0], kwargs_tup[0], devices[0])\n\n    outputs = []\n    for i in range(len(inputs)):\n        output = results[i]\n        if isinstance(output, Exception):\n            raise output\n        outputs.append(output)\n    return outputs\n'"
segmentron/utils/registry.py,0,"b'# this code heavily based on detectron2\n\nimport logging\nimport torch\n\nfrom ..config import cfg\n\nclass Registry(object):\n    """"""\n    The registry that provides name -> object mapping, to support third-party users\' custom modules.\n\n    To create a registry (inside segmentron):\n\n    .. code-block:: python\n\n        BACKBONE_REGISTRY = Registry(\'BACKBONE\')\n\n    To register an object:\n\n    .. code-block:: python\n\n        @BACKBONE_REGISTRY.register()\n        class MyBackbone():\n            ...\n\n    Or:\n\n    .. code-block:: python\n\n        BACKBONE_REGISTRY.register(MyBackbone)\n    """"""\n\n    def __init__(self, name):\n        """"""\n        Args:\n            name (str): the name of this registry\n        """"""\n        self._name = name\n\n        self._obj_map = {}\n\n    def _do_register(self, name, obj):\n        assert (\n            name not in self._obj_map\n        ), ""An object named \'{}\' was already registered in \'{}\' registry!"".format(name, self._name)\n        self._obj_map[name] = obj\n\n    def register(self, obj=None, name=None):\n        """"""\n        Register the given object under the the name `obj.__name__`.\n        Can be used as either a decorator or not. See docstring of this class for usage.\n        """"""\n        if obj is None:\n            # used as a decorator\n            def deco(func_or_class, name=name):\n                if name is None:\n                    name = func_or_class.__name__\n                self._do_register(name, func_or_class)\n                return func_or_class\n\n            return deco\n\n        # used as a function call\n        if name is None:\n            name = obj.__name__\n        self._do_register(name, obj)\n\n\n\n    def get(self, name):\n        ret = self._obj_map.get(name)\n        if ret is None:\n            raise KeyError(""No object named \'{}\' found in \'{}\' registry!"".format(name, self._name))\n\n        return ret\n\n    def get_list(self):\n        return list(self._obj_map.keys())\n'"
segmentron/utils/score.py,12,"b'""""""Evaluation Metrics for Semantic Segmentation""""""\nimport torch\nimport numpy as np\nfrom torch import distributed as dist\nimport copy\n\n__all__ = [\'SegmentationMetric\', \'batch_pix_accuracy\', \'batch_intersection_union\',\n           \'pixelAccuracy\', \'intersectionAndUnion\', \'hist_info\', \'compute_score\']\n\n\nclass SegmentationMetric(object):\n    """"""Computes pixAcc and mIoU metric scores\n    """"""\n\n    def __init__(self, nclass, distributed):\n        super(SegmentationMetric, self).__init__()\n        self.nclass = nclass\n        self.distributed = distributed\n        self.reset()\n\n    def update(self, preds, labels):\n        """"""Updates the internal evaluation result.\n\n        Parameters\n        ----------\n        labels : \'NumpyArray\' or list of `NumpyArray`\n            The labels of the data.\n        preds : \'NumpyArray\' or list of `NumpyArray`\n            Predicted values.\n        """"""\n\n        def reduce_tensor(tensor):\n            rt = tensor.clone()\n            dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n            return rt\n\n        def evaluate_worker(self, pred, label):\n            correct, labeled = batch_pix_accuracy(pred, label)\n            inter, union = batch_intersection_union(pred, label, self.nclass)\n            if self.distributed:\n                correct = reduce_tensor(correct)\n                labeled = reduce_tensor(labeled)\n                inter = reduce_tensor(inter.cuda())\n                union = reduce_tensor(union.cuda())\n            torch.cuda.synchronize()\n            self.total_correct += correct.item()\n            self.total_label += labeled.item()\n            if self.total_inter.device != inter.device:\n                self.total_inter = self.total_inter.to(inter.device)\n                self.total_union = self.total_union.to(union.device)\n            self.total_inter += inter\n            self.total_union += union\n\n        if isinstance(preds, torch.Tensor):\n            evaluate_worker(self, preds, labels)\n        elif isinstance(preds, (list, tuple)):\n            for (pred, label) in zip(preds, labels):\n                evaluate_worker(self, pred, label)\n\n    def get(self, return_category_iou=False):\n        """"""Gets the current evaluation result.\n\n        Returns\n        -------\n        metrics : tuple of float\n            pixAcc and mIoU\n        """"""\n        pixAcc = 1.0 * self.total_correct / (2.220446049250313e-16 + self.total_label)  # remove np.spacing(1)\n        IoU = 1.0 * self.total_inter / (2.220446049250313e-16 + self.total_union)\n        mIoU = IoU.mean().item()\n        if return_category_iou:\n            return pixAcc, mIoU, IoU.cpu().numpy()\n        return pixAcc, mIoU\n\n    def reset(self):\n        """"""Resets the internal evaluation result to initial state.""""""\n        self.total_inter = torch.zeros(self.nclass)\n        self.total_union = torch.zeros(self.nclass)\n        self.total_correct = 0\n        self.total_label = 0\n\n\ndef batch_pix_accuracy(output, target):\n    """"""PixAcc""""""\n    # inputs are numpy array, output 4D, target 3D\n    predict = torch.argmax(output.long(), 1) + 1\n    target = target.long() + 1\n\n    pixel_labeled = torch.sum(target > 0)#.item()\n    pixel_correct = torch.sum((predict == target) * (target > 0))#.item()\n    assert pixel_correct <= pixel_labeled, ""Correct area should be smaller than Labeled""\n    return pixel_correct, pixel_labeled\n\n\ndef batch_intersection_union(output, target, nclass):\n    """"""mIoU""""""\n    # inputs are numpy array, output 4D, target 3D\n    mini = 1\n    maxi = nclass\n    nbins = nclass\n    predict = torch.argmax(output, 1) + 1\n    target = target.float() + 1\n\n    predict = predict.float() * (target > 0).float()\n    intersection = predict * (predict == target).float()\n    # areas of intersection and union\n    # element 0 in intersection occur the main difference from np.bincount. set boundary to -1 is necessary.\n    area_inter = torch.histc(intersection.cpu(), bins=nbins, min=mini, max=maxi)\n    area_pred = torch.histc(predict.cpu(), bins=nbins, min=mini, max=maxi)\n    area_lab = torch.histc(target.cpu(), bins=nbins, min=mini, max=maxi)\n    area_union = area_pred + area_lab - area_inter\n    assert torch.sum(area_inter > area_union).item() == 0, ""Intersection area should be smaller than Union area""\n    return area_inter.float(), area_union.float()\n\n\ndef pixelAccuracy(imPred, imLab):\n    """"""\n    This function takes the prediction and label of a single image, returns pixel-wise accuracy\n    To compute over many images do:\n    for i = range(Nimages):\n         (pixel_accuracy[i], pixel_correct[i], pixel_labeled[i]) = \\\n            pixelAccuracy(imPred[i], imLab[i])\n    mean_pixel_accuracy = 1.0 * np.sum(pixel_correct) / (np.spacing(1) + np.sum(pixel_labeled))\n    """"""\n    # Remove classes from unlabeled pixels in gt image.\n    # We should not penalize detections in unlabeled portions of the image.\n    pixel_labeled = np.sum(imLab >= 0)\n    pixel_correct = np.sum((imPred == imLab) * (imLab >= 0))\n    pixel_accuracy = 1.0 * pixel_correct / pixel_labeled\n    return (pixel_accuracy, pixel_correct, pixel_labeled)\n\n\ndef intersectionAndUnion(imPred, imLab, numClass):\n    """"""\n    This function takes the prediction and label of a single image,\n    returns intersection and union areas for each class\n    To compute over many images do:\n    for i in range(Nimages):\n        (area_intersection[:,i], area_union[:,i]) = intersectionAndUnion(imPred[i], imLab[i])\n    IoU = 1.0 * np.sum(area_intersection, axis=1) / np.sum(np.spacing(1)+area_union, axis=1)\n    """"""\n    # Remove classes from unlabeled pixels in gt image.\n    # We should not penalize detections in unlabeled portions of the image.\n    imPred = imPred * (imLab >= 0)\n\n    # Compute area intersection:\n    intersection = imPred * (imPred == imLab)\n    (area_intersection, _) = np.histogram(intersection, bins=numClass, range=(1, numClass))\n\n    # Compute area union:\n    (area_pred, _) = np.histogram(imPred, bins=numClass, range=(1, numClass))\n    (area_lab, _) = np.histogram(imLab, bins=numClass, range=(1, numClass))\n    area_union = area_pred + area_lab - area_intersection\n    return (area_intersection, area_union)\n\n\ndef hist_info(pred, label, num_cls):\n    assert pred.shape == label.shape\n    k = (label >= 0) & (label < num_cls)\n    labeled = np.sum(k)\n    correct = np.sum((pred[k] == label[k]))\n\n    return np.bincount(num_cls * label[k].astype(int) + pred[k], minlength=num_cls ** 2).reshape(num_cls,\n                                                                                                 num_cls), labeled, correct\n\n\ndef compute_score(hist, correct, labeled):\n    iu = np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n    mean_IU = np.nanmean(iu)\n    mean_IU_no_back = np.nanmean(iu[1:])\n    freq = hist.sum(1) / hist.sum()\n    # freq_IU = (iu[freq > 0] * freq[freq > 0]).sum()\n    mean_pixel_acc = correct / labeled\n\n    return iu, mean_IU, mean_IU_no_back, mean_pixel_acc\n'"
segmentron/utils/visualize.py,2,"b'import os\nimport logging\nimport numpy as np\nimport torch\n\nfrom PIL import Image\n#from torchsummary import summary\nfrom thop import profile\n\n__all__ = [\'get_color_pallete\', \'print_iou\', \'set_img_color\',\n           \'show_prediction\', \'show_colorful_images\', \'save_colorful_images\']\n\n\ndef print_iou(iu, mean_pixel_acc, class_names=None, show_no_back=False):\n    n = iu.size\n    lines = []\n    for i in range(n):\n        if class_names is None:\n            cls = \'Class %d:\' % (i + 1)\n        else:\n            cls = \'%d %s\' % (i + 1, class_names[i])\n        # lines.append(\'%-8s: %.3f%%\' % (cls, iu[i] * 100))\n    mean_IU = np.nanmean(iu)\n    mean_IU_no_back = np.nanmean(iu[1:])\n    if show_no_back:\n        lines.append(\'mean_IU: %.3f%% || mean_IU_no_back: %.3f%% || mean_pixel_acc: %.3f%%\' % (\n            mean_IU * 100, mean_IU_no_back * 100, mean_pixel_acc * 100))\n    else:\n        lines.append(\'mean_IU: %.3f%% || mean_pixel_acc: %.3f%%\' % (mean_IU * 100, mean_pixel_acc * 100))\n    lines.append(\'=================================================\')\n    line = ""\\n"".join(lines)\n\n    print(line)\n\n\n@torch.no_grad()\ndef show_flops_params(model, device, input_shape=[1, 3, 1024, 2048]):\n    #summary(model, tuple(input_shape[1:]), device=device)\n    input = torch.randn(*input_shape).to(torch.device(device))\n    flops, params = profile(model, inputs=(input,), verbose=False)\n\n    logging.info(\'{} flops: {:.3f}G input shape is {}, params: {:.3f}M\'.format(\n        model.__class__.__name__, flops / 1000000000, input_shape[1:], params / 1000000))\n\n\ndef set_img_color(img, label, colors, background=0, show255=False):\n    for i in range(len(colors)):\n        if i != background:\n            img[np.where(label == i)] = colors[i]\n    if show255:\n        img[np.where(label == 255)] = 255\n\n    return img\n\n\ndef show_prediction(img, pred, colors, background=0):\n    im = np.array(img, np.uint8)\n    set_img_color(im, pred, colors, background)\n    out = np.array(im)\n\n    return out\n\n\ndef show_colorful_images(prediction, palettes):\n    im = Image.fromarray(palettes[prediction.astype(\'uint8\').squeeze()])\n    im.show()\n\n\ndef save_colorful_images(prediction, filename, output_dir, palettes):\n    \'\'\'\n    :param prediction: [B, H, W, C]\n    \'\'\'\n    im = Image.fromarray(palettes[prediction.astype(\'uint8\').squeeze()])\n    fn = os.path.join(output_dir, filename)\n    out_dir = os.path.split(fn)[0]\n    if not os.path.exists(out_dir):\n        os.mkdir(out_dir)\n    im.save(fn)\n\n\ndef get_color_pallete(npimg, dataset=\'cityscape\'):\n    """"""Visualize image.\n\n    Parameters\n    ----------\n    npimg : numpy.ndarray\n        Single channel image with shape `H, W, 1`.\n    dataset : str, default: \'pascal_voc\'\n        The dataset that model pretrained on. (\'pascal_voc\', \'ade20k\')\n    Returns\n    -------\n    out_img : PIL.Image\n        Image with color pallete\n    """"""\n    # recovery boundary\n    if dataset in (\'pascal_voc\', \'pascal_aug\'):\n        npimg[npimg == -1] = 255\n    # put colormap\n    if dataset == \'ade20k\':\n        npimg = npimg + 1\n        out_img = Image.fromarray(npimg.astype(\'uint8\'))\n        out_img.putpalette(adepallete)\n        return out_img\n    elif dataset == \'cityscape\':\n        out_img = Image.fromarray(npimg.astype(\'uint8\'))\n        out_img.putpalette(cityscapepallete)\n        return out_img\n    out_img = Image.fromarray(npimg.astype(\'uint8\'))\n    out_img.putpalette(vocpallete)\n    return out_img\n\n\ndef _getvocpallete(num_cls):\n    n = num_cls\n    pallete = [0] * (n * 3)\n    for j in range(0, n):\n        lab = j\n        pallete[j * 3 + 0] = 0\n        pallete[j * 3 + 1] = 0\n        pallete[j * 3 + 2] = 0\n        i = 0\n        while (lab > 0):\n            pallete[j * 3 + 0] |= (((lab >> 0) & 1) << (7 - i))\n            pallete[j * 3 + 1] |= (((lab >> 1) & 1) << (7 - i))\n            pallete[j * 3 + 2] |= (((lab >> 2) & 1) << (7 - i))\n            i = i + 1\n            lab >>= 3\n    return pallete\n\n\nvocpallete = _getvocpallete(256)\n\nadepallete = [\n    0, 0, 0, 120, 120, 120, 180, 120, 120, 6, 230, 230, 80, 50, 50, 4, 200, 3, 120, 120, 80, 140, 140, 140, 204,\n    5, 255, 230, 230, 230, 4, 250, 7, 224, 5, 255, 235, 255, 7, 150, 5, 61, 120, 120, 70, 8, 255, 51, 255, 6, 82,\n    143, 255, 140, 204, 255, 4, 255, 51, 7, 204, 70, 3, 0, 102, 200, 61, 230, 250, 255, 6, 51, 11, 102, 255, 255,\n    7, 71, 255, 9, 224, 9, 7, 230, 220, 220, 220, 255, 9, 92, 112, 9, 255, 8, 255, 214, 7, 255, 224, 255, 184, 6,\n    10, 255, 71, 255, 41, 10, 7, 255, 255, 224, 255, 8, 102, 8, 255, 255, 61, 6, 255, 194, 7, 255, 122, 8, 0, 255,\n    20, 255, 8, 41, 255, 5, 153, 6, 51, 255, 235, 12, 255, 160, 150, 20, 0, 163, 255, 140, 140, 140, 250, 10, 15,\n    20, 255, 0, 31, 255, 0, 255, 31, 0, 255, 224, 0, 153, 255, 0, 0, 0, 255, 255, 71, 0, 0, 235, 255, 0, 173, 255,\n    31, 0, 255, 11, 200, 200, 255, 82, 0, 0, 255, 245, 0, 61, 255, 0, 255, 112, 0, 255, 133, 255, 0, 0, 255, 163,\n    0, 255, 102, 0, 194, 255, 0, 0, 143, 255, 51, 255, 0, 0, 82, 255, 0, 255, 41, 0, 255, 173, 10, 0, 255, 173, 255,\n    0, 0, 255, 153, 255, 92, 0, 255, 0, 255, 255, 0, 245, 255, 0, 102, 255, 173, 0, 255, 0, 20, 255, 184, 184, 0,\n    31, 255, 0, 255, 61, 0, 71, 255, 255, 0, 204, 0, 255, 194, 0, 255, 82, 0, 10, 255, 0, 112, 255, 51, 0, 255, 0,\n    194, 255, 0, 122, 255, 0, 255, 163, 255, 153, 0, 0, 255, 10, 255, 112, 0, 143, 255, 0, 82, 0, 255, 163, 255,\n    0, 255, 235, 0, 8, 184, 170, 133, 0, 255, 0, 255, 92, 184, 0, 255, 255, 0, 31, 0, 184, 255, 0, 214, 255, 255,\n    0, 112, 92, 255, 0, 0, 224, 255, 112, 224, 255, 70, 184, 160, 163, 0, 255, 153, 0, 255, 71, 255, 0, 255, 0,\n    163, 255, 204, 0, 255, 0, 143, 0, 255, 235, 133, 255, 0, 255, 0, 235, 245, 0, 255, 255, 0, 122, 255, 245, 0,\n    10, 190, 212, 214, 255, 0, 0, 204, 255, 20, 0, 255, 255, 255, 0, 0, 153, 255, 0, 41, 255, 0, 255, 204, 41, 0,\n    255, 41, 255, 0, 173, 0, 255, 0, 245, 255, 71, 0, 255, 122, 0, 255, 0, 255, 184, 0, 92, 255, 184, 255, 0, 0,\n    133, 255, 255, 214, 0, 25, 194, 194, 102, 255, 0, 92, 0, 255]\n\ncityscapepallete = [\n    128, 64, 128,\n    244, 35, 232,\n    70, 70, 70,\n    102, 102, 156,\n    190, 153, 153,\n    153, 153, 153,\n    250, 170, 30,\n    220, 220, 0,\n    107, 142, 35,\n    152, 251, 152,\n    0, 130, 180,\n    220, 20, 60,\n    255, 0, 0,\n    0, 0, 142,\n    0, 0, 70,\n    0, 60, 100,\n    0, 80, 100,\n    0, 0, 230,\n    119, 11, 32,\n]\n'"
segmentron/data/dataloader/__init__.py,0,"b'""""""\nThis module provides data loaders and transformers for popular vision datasets.\n""""""\nfrom .mscoco import COCOSegmentation\nfrom .cityscapes import CitySegmentation\nfrom .ade import ADE20KSegmentation\nfrom .pascal_voc import VOCSegmentation\nfrom .pascal_aug import VOCAugSegmentation\nfrom .sbu_shadow import SBUSegmentation\n\ndatasets = {\n    \'ade20k\': ADE20KSegmentation,\n    \'pascal_voc\': VOCSegmentation,\n    \'pascal_aug\': VOCAugSegmentation,\n    \'coco\': COCOSegmentation,\n    \'cityscape\': CitySegmentation,\n    \'sbu\': SBUSegmentation,\n}\n\n\ndef get_segmentation_dataset(name, **kwargs):\n    """"""Segmentation Datasets""""""\n    return datasets[name.lower()](**kwargs)\n'"
segmentron/data/dataloader/ade.py,2,"b'""""""Pascal ADE20K Semantic Segmentation Dataset.""""""\nimport os\nimport logging\nimport torch\nimport numpy as np\n\nfrom PIL import Image\nfrom .seg_data_base import SegmentationDataset\n\n\nclass ADE20KSegmentation(SegmentationDataset):\n    """"""ADE20K Semantic Segmentation Dataset.\n\n    Parameters\n    ----------\n    root : string\n        Path to ADE20K folder. Default is \'./datasets/ade\'\n    split: string\n        \'train\', \'val\' or \'test\'\n    transform : callable, optional\n        A function that transforms the image\n    Examples\n    --------\n    >>> from torchvision import transforms\n    >>> import torch.utils.data as data\n    >>> # Transforms for Normalization\n    >>> input_transform = transforms.Compose([\n    >>>     transforms.ToTensor(),\n    >>>     transforms.Normalize((.485, .456, .406), (.229, .224, .225)),\n    >>> ])\n    >>> # Create Dataset\n    >>> trainset = ADE20KSegmentation(split=\'train\', transform=input_transform)\n    >>> # Create Training Loader\n    >>> train_data = data.DataLoader(\n    >>>     trainset, 4, shuffle=True,\n    >>>     num_workers=4)\n    """"""\n    BASE_DIR = \'ADEChallengeData2016\'\n    NUM_CLASS = 150\n\n    def __init__(self, root=\'datasets/ade\', split=\'test\', mode=None, transform=None, **kwargs):\n        super(ADE20KSegmentation, self).__init__(root, split, mode, transform, **kwargs)\n        root = os.path.join(self.root, self.BASE_DIR)\n        assert os.path.exists(root), ""Please put the data in {SEG_ROOT}/datasets/ade""\n        self.images, self.masks = _get_ade20k_pairs(root, split)\n        assert (len(self.images) == len(self.masks))\n        if len(self.images) == 0:\n            raise RuntimeError(""Found 0 images in subfolders of:"" + root + ""\\n"")\n        logging.info(\'Found {} images in the folder {}\'.format(len(self.images), root))\n\n    def __getitem__(self, index):\n        img = Image.open(self.images[index]).convert(\'RGB\')\n        if self.mode == \'test\':\n            img = self._img_transform(img)\n            if self.transform is not None:\n                img = self.transform(img)\n            return img, os.path.basename(self.images[index])\n        mask = Image.open(self.masks[index])\n        # synchrosized transform\n        if self.mode == \'train\':\n            img, mask = self._sync_transform(img, mask)\n        elif self.mode == \'val\':\n            img, mask = self._val_sync_transform(img, mask)\n        else:\n            assert self.mode == \'testval\'\n            img, mask = self._img_transform(img), self._mask_transform(mask)\n        # general resize, normalize and to Tensor\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, mask, os.path.basename(self.images[index])\n\n    def _mask_transform(self, mask):\n        return torch.LongTensor(np.array(mask).astype(\'int32\') - 1)\n\n    def __len__(self):\n        return len(self.images)\n\n    @property\n    def pred_offset(self):\n        return 1\n\n    @property\n    def classes(self):\n        """"""Category names.""""""\n        return (""wall"", ""building, edifice"", ""sky"", ""floor, flooring"", ""tree"",\n                ""ceiling"", ""road, route"", ""bed"", ""windowpane, window"", ""grass"",\n                ""cabinet"", ""sidewalk, pavement"",\n                ""person, individual, someone, somebody, mortal, soul"",\n                ""earth, ground"", ""door, double door"", ""table"", ""mountain, mount"",\n                ""plant, flora, plant life"", ""curtain, drape, drapery, mantle, pall"",\n                ""chair"", ""car, auto, automobile, machine, motorcar"",\n                ""water"", ""painting, picture"", ""sofa, couch, lounge"", ""shelf"",\n                ""house"", ""sea"", ""mirror"", ""rug, carpet, carpeting"", ""field"", ""armchair"",\n                ""seat"", ""fence, fencing"", ""desk"", ""rock, stone"", ""wardrobe, closet, press"",\n                ""lamp"", ""bathtub, bathing tub, bath, tub"", ""railing, rail"", ""cushion"",\n                ""base, pedestal, stand"", ""box"", ""column, pillar"", ""signboard, sign"",\n                ""chest of drawers, chest, bureau, dresser"", ""counter"", ""sand"", ""sink"",\n                ""skyscraper"", ""fireplace, hearth, open fireplace"", ""refrigerator, icebox"",\n                ""grandstand, covered stand"", ""path"", ""stairs, steps"", ""runway"",\n                ""case, display case, showcase, vitrine"",\n                ""pool table, billiard table, snooker table"", ""pillow"",\n                ""screen door, screen"", ""stairway, staircase"", ""river"", ""bridge, span"",\n                ""bookcase"", ""blind, screen"", ""coffee table, cocktail table"",\n                ""toilet, can, commode, crapper, pot, potty, stool, throne"",\n                ""flower"", ""book"", ""hill"", ""bench"", ""countertop"",\n                ""stove, kitchen stove, range, kitchen range, cooking stove"",\n                ""palm, palm tree"", ""kitchen island"",\n                ""computer, computing machine, computing device, data processor, ""\n                ""electronic computer, information processing system"",\n                ""swivel chair"", ""boat"", ""bar"", ""arcade machine"",\n                ""hovel, hut, hutch, shack, shanty"",\n                ""bus, autobus, coach, charabanc, double-decker, jitney, motorbus, ""\n                ""motorcoach, omnibus, passenger vehicle"",\n                ""towel"", ""light, light source"", ""truck, motortruck"", ""tower"",\n                ""chandelier, pendant, pendent"", ""awning, sunshade, sunblind"",\n                ""streetlight, street lamp"", ""booth, cubicle, stall, kiosk"",\n                ""television receiver, television, television set, tv, tv set, idiot ""\n                ""box, boob tube, telly, goggle box"",\n                ""airplane, aeroplane, plane"", ""dirt track"",\n                ""apparel, wearing apparel, dress, clothes"",\n                ""pole"", ""land, ground, soil"",\n                ""bannister, banister, balustrade, balusters, handrail"",\n                ""escalator, moving staircase, moving stairway"",\n                ""ottoman, pouf, pouffe, puff, hassock"",\n                ""bottle"", ""buffet, counter, sideboard"",\n                ""poster, posting, placard, notice, bill, card"",\n                ""stage"", ""van"", ""ship"", ""fountain"",\n                ""conveyer belt, conveyor belt, conveyer, conveyor, transporter"",\n                ""canopy"", ""washer, automatic washer, washing machine"",\n                ""plaything, toy"", ""swimming pool, swimming bath, natatorium"",\n                ""stool"", ""barrel, cask"", ""basket, handbasket"", ""waterfall, falls"",\n                ""tent, collapsible shelter"", ""bag"", ""minibike, motorbike"", ""cradle"",\n                ""oven"", ""ball"", ""food, solid food"", ""step, stair"", ""tank, storage tank"",\n                ""trade name, brand name, brand, marque"", ""microwave, microwave oven"",\n                ""pot, flowerpot"", ""animal, animate being, beast, brute, creature, fauna"",\n                ""bicycle, bike, wheel, cycle"", ""lake"",\n                ""dishwasher, dish washer, dishwashing machine"",\n                ""screen, silver screen, projection screen"",\n                ""blanket, cover"", ""sculpture"", ""hood, exhaust hood"", ""sconce"", ""vase"",\n                ""traffic light, traffic signal, stoplight"", ""tray"",\n                ""ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, ""\n                ""dustbin, trash barrel, trash bin"",\n                ""fan"", ""pier, wharf, wharfage, dock"", ""crt screen"",\n                ""plate"", ""monitor, monitoring device"", ""bulletin board, notice board"",\n                ""shower"", ""radiator"", ""glass, drinking glass"", ""clock"", ""flag"")\n\n\ndef _get_ade20k_pairs(folder, mode=\'train\'):\n    img_paths = []\n    mask_paths = []\n    if mode == \'train\':\n        img_folder = os.path.join(folder, \'images/training\')\n        mask_folder = os.path.join(folder, \'annotations/training\')\n    else:\n        img_folder = os.path.join(folder, \'images/validation\')\n        mask_folder = os.path.join(folder, \'annotations/validation\')\n    for filename in os.listdir(img_folder):\n        basename, _ = os.path.splitext(filename)\n        if filename.endswith("".jpg""):\n            imgpath = os.path.join(img_folder, filename)\n            maskname = basename + \'.png\'\n            maskpath = os.path.join(mask_folder, maskname)\n            if os.path.isfile(maskpath):\n                img_paths.append(imgpath)\n                mask_paths.append(maskpath)\n            else:\n                logging.info(\'cannot find the mask:\', maskpath)\n\n    return img_paths, mask_paths\n\n\nif __name__ == \'__main__\':\n    train_dataset = ADE20KSegmentation()\n'"
segmentron/data/dataloader/cityscapes.py,2,"b'""""""Prepare Cityscapes dataset""""""\nimport os\nimport torch\nimport numpy as np\nimport logging\n\nfrom PIL import Image\nfrom .seg_data_base import SegmentationDataset\n\n\nclass CitySegmentation(SegmentationDataset):\n    """"""Cityscapes Semantic Segmentation Dataset.\n\n    Parameters\n    ----------\n    root : string\n        Path to Cityscapes folder. Default is \'./datasets/cityscapes\'\n    split: string\n        \'train\', \'val\' or \'test\'\n    transform : callable, optional\n        A function that transforms the image\n    Examples\n    --------\n    >>> from torchvision import transforms\n    >>> import torch.utils.data as data\n    >>> # Transforms for Normalization\n    >>> input_transform = transforms.Compose([\n    >>>     transforms.ToTensor(),\n    >>>     transforms.Normalize((.485, .456, .406), (.229, .224, .225)),\n    >>> ])\n    >>> # Create Dataset\n    >>> trainset = CitySegmentation(split=\'train\', transform=input_transform)\n    >>> # Create Training Loader\n    >>> train_data = data.DataLoader(\n    >>>     trainset, 4, shuffle=True,\n    >>>     num_workers=4)\n    """"""\n    BASE_DIR = \'cityscapes\'\n    NUM_CLASS = 19\n\n    def __init__(self, root=\'datasets/cityscapes\', split=\'train\', mode=None, transform=None, **kwargs):\n        super(CitySegmentation, self).__init__(root, split, mode, transform, **kwargs)\n        # self.root = os.path.join(root, self.BASE_DIR)\n        assert os.path.exists(self.root), ""Please put dataset in {SEG_ROOT}/datasets/cityscapes""\n        self.images, self.mask_paths = _get_city_pairs(self.root, self.split)\n        assert (len(self.images) == len(self.mask_paths))\n        if len(self.images) == 0:\n            raise RuntimeError(""Found 0 images in subfolders of:"" + root + ""\\n"")\n        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22,\n                              23, 24, 25, 26, 27, 28, 31, 32, 33]\n        self._key = np.array([-1, -1, -1, -1, -1, -1,\n                              -1, -1, 0, 1, -1, -1,\n                              2, 3, 4, -1, -1, -1,\n                              5, -1, 6, 7, 8, 9,\n                              10, 11, 12, 13, 14, 15,\n                              -1, -1, 16, 17, 18])\n        self._mapping = np.array(range(-1, len(self._key) - 1)).astype(\'int32\')\n\n    def _class_to_index(self, mask):\n        # assert the value\n        values = np.unique(mask)\n        for value in values:\n            assert (value in self._mapping)\n        index = np.digitize(mask.ravel(), self._mapping, right=True)\n        return self._key[index].reshape(mask.shape)\n\n    def __getitem__(self, index):\n        img = Image.open(self.images[index]).convert(\'RGB\')\n        if self.mode == \'test\':\n            if self.transform is not None:\n                img = self.transform(img)\n            return img, os.path.basename(self.images[index])\n        mask = Image.open(self.mask_paths[index])\n        # synchrosized transform\n        if self.mode == \'train\':\n            img, mask = self._sync_transform(img, mask)\n        elif self.mode == \'val\':\n            img, mask = self._val_sync_transform(img, mask)\n        else:\n            assert self.mode == \'testval\'\n            img, mask = self._img_transform(img), self._mask_transform(mask)\n        # general resize, normalize and toTensor\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, mask, os.path.basename(self.images[index])\n\n    def _mask_transform(self, mask):\n        target = self._class_to_index(np.array(mask).astype(\'int32\'))\n        return torch.LongTensor(np.array(target).astype(\'int32\'))\n\n    def __len__(self):\n        return len(self.images)\n\n    @property\n    def pred_offset(self):\n        return 0\n\n    @property\n    def classes(self):\n        """"""Category names.""""""\n        return (\'road\', \'sidewalk\', \'building\', \'wall\', \'fence\', \'pole\', \'traffic light\',\n                \'traffic sign\', \'vegetation\', \'terrain\', \'sky\', \'person\', \'rider\', \'car\',\n                \'truck\', \'bus\', \'train\', \'motorcycle\', \'bicycle\')\n\n\ndef _get_city_pairs(folder, split=\'train\'):\n    def get_path_pairs(img_folder, mask_folder):\n        img_paths = []\n        mask_paths = []\n        for root, _, files in os.walk(img_folder):\n            for filename in files:\n                if filename.startswith(\'._\'):\n                    continue\n                if filename.endswith(\'.png\'):\n                    imgpath = os.path.join(root, filename)\n                    foldername = os.path.basename(os.path.dirname(imgpath))\n                    maskname = filename.replace(\'leftImg8bit\', \'gtFine_labelIds\')\n                    maskpath = os.path.join(mask_folder, foldername, maskname)\n                    if os.path.isfile(imgpath) and os.path.isfile(maskpath):\n                        img_paths.append(imgpath)\n                        mask_paths.append(maskpath)\n                    else:\n                        logging.info(\'cannot find the mask or image:\', imgpath, maskpath)\n        logging.info(\'Found {} images in the folder {}\'.format(len(img_paths), img_folder))\n        return img_paths, mask_paths\n\n    if split in (\'train\', \'val\'):\n        img_folder = os.path.join(folder, \'leftImg8bit/\' + split)\n        mask_folder = os.path.join(folder, \'gtFine/\' + split)\n        img_paths, mask_paths = get_path_pairs(img_folder, mask_folder)\n        return img_paths, mask_paths\n    else:\n        assert split == \'trainval\'\n        logging.info(\'trainval set\')\n        train_img_folder = os.path.join(folder, \'leftImg8bit/train\')\n        train_mask_folder = os.path.join(folder, \'gtFine/train\')\n        val_img_folder = os.path.join(folder, \'leftImg8bit/val\')\n        val_mask_folder = os.path.join(folder, \'gtFine/val\')\n        train_img_paths, train_mask_paths = get_path_pairs(train_img_folder, train_mask_folder)\n        val_img_paths, val_mask_paths = get_path_pairs(val_img_folder, val_mask_folder)\n        img_paths = train_img_paths + val_img_paths\n        mask_paths = train_mask_paths + val_mask_paths\n    return img_paths, mask_paths\n\n\nif __name__ == \'__main__\':\n    dataset = CitySegmentation()\n'"
segmentron/data/dataloader/lip_parsing.py,1,"b'""""""Look into Person Dataset""""""\nimport os\nimport torch\nimport numpy as np\n\nfrom PIL import Image\nfrom segmentron.data.dataloader.seg_data_base import SegmentationDataset\n\n\nclass LIPSegmentation(SegmentationDataset):\n    """"""Look into person parsing dataset """"""\n\n    BASE_DIR = \'LIP\'\n    NUM_CLASS = 20\n\n    def __init__(self, root=\'datasets/LIP\', split=\'train\', mode=None, transform=None, **kwargs):\n        super(LIPSegmentation, self).__init__(root, split, mode, transform, **kwargs)\n        _trainval_image_dir = os.path.join(root, \'TrainVal_images\')\n        _testing_image_dir = os.path.join(root, \'Testing_images\')\n        _trainval_mask_dir = os.path.join(root, \'TrainVal_parsing_annotations\')\n        if split == \'train\':\n            _image_dir = os.path.join(_trainval_image_dir, \'train_images\')\n            _mask_dir = os.path.join(_trainval_mask_dir, \'train_segmentations\')\n            _split_f = os.path.join(_trainval_image_dir, \'train_id.txt\')\n        elif split == \'val\':\n            _image_dir = os.path.join(_trainval_image_dir, \'val_images\')\n            _mask_dir = os.path.join(_trainval_mask_dir, \'val_segmentations\')\n            _split_f = os.path.join(_trainval_image_dir, \'val_id.txt\')\n        elif split == \'test\':\n            _image_dir = os.path.join(_testing_image_dir, \'testing_images\')\n            _split_f = os.path.join(_testing_image_dir, \'test_id.txt\')\n        else:\n            raise RuntimeError(\'Unknown dataset split.\')\n\n        self.images = []\n        self.masks = []\n        with open(os.path.join(_split_f), \'r\') as lines:\n            for line in lines:\n                _image = os.path.join(_image_dir, line.rstrip(\'\\n\') + \'.jpg\')\n                assert os.path.isfile(_image)\n                self.images.append(_image)\n                if split != \'test\':\n                    _mask = os.path.join(_mask_dir, line.rstrip(\'\\n\') + \'.png\')\n                    assert os.path.isfile(_mask)\n                    self.masks.append(_mask)\n\n        if split != \'test\':\n            assert (len(self.images) == len(self.masks))\n        print(\'Found {} {} images in the folder {}\'.format(len(self.images), split, root))\n\n    def __getitem__(self, index):\n        img = Image.open(self.images[index]).convert(\'RGB\')\n        if self.mode == \'test\':\n            img = self._img_transform(img)\n            if self.transform is not None:\n                img = self.transform(img)\n            return img, os.path.basename(self.images[index])\n        mask = Image.open(self.masks[index])\n        # synchronized transform\n        if self.mode == \'train\':\n            img, mask = self._sync_transform(img, mask)\n        elif self.mode == \'val\':\n            img, mask = self._val_sync_transform(img, mask)\n        else:\n            assert self.mode == \'testval\'\n            img, mask = self._img_transform(img), self._mask_transform(mask)\n        # general resize, normalize and toTensor\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img, mask, os.path.basename(self.images[index])\n\n    def __len__(self):\n        return len(self.images)\n\n    def _mask_transform(self, mask):\n        target = np.array(mask).astype(\'int32\')\n        return torch.from_numpy(target).long()\n\n    @property\n    def classes(self):\n        """"""Category name.""""""\n        return (\'background\', \'hat\', \'hair\', \'glove\', \'sunglasses\', \'upperclothes\',\n                \'dress\', \'coat\', \'socks\', \'pants\', \'jumpsuits\', \'scarf\', \'skirt\',\n                \'face\', \'leftArm\', \'rightArm\', \'leftLeg\', \'rightLeg\', \'leftShoe\',\n                \'rightShoe\')\n\n\nif __name__ == \'__main__\':\n    dataset = LIPSegmentation(base_size=280, crop_size=256)'"
segmentron/data/dataloader/mscoco.py,2,"b'""""""MSCOCO Semantic Segmentation pretraining for VOC.""""""\nimport os\nimport pickle\nimport torch\nimport numpy as np\n\nfrom tqdm import trange\nfrom PIL import Image\nfrom .seg_data_base import SegmentationDataset\n\n\nclass COCOSegmentation(SegmentationDataset):\n    """"""COCO Semantic Segmentation Dataset for VOC Pre-training.\n\n    Parameters\n    ----------\n    root : string\n        Path to ADE20K folder. Default is \'./datasets/coco\'\n    split: string\n        \'train\', \'val\' or \'test\'\n    transform : callable, optional\n        A function that transforms the image\n    Examples\n    --------\n    >>> from torchvision import transforms\n    >>> import torch.utils.data as data\n    >>> # Transforms for Normalization\n    >>> input_transform = transforms.Compose([\n    >>>     transforms.ToTensor(),\n    >>>     transforms.Normalize((.485, .456, .406), (.229, .224, .225)),\n    >>> ])\n    >>> # Create Dataset\n    >>> trainset = COCOSegmentation(split=\'train\', transform=input_transform)\n    >>> # Create Training Loader\n    >>> train_data = data.DataLoader(\n    >>>     trainset, 4, shuffle=True,\n    >>>     num_workers=4)\n    """"""\n    CAT_LIST = [0, 5, 2, 16, 9, 44, 6, 3, 17, 62, 21, 67, 18, 19, 4,\n                1, 64, 20, 63, 7, 72]\n    NUM_CLASS = 21\n\n    def __init__(self, root=\'datasets/coco\', split=\'train\', mode=None, transform=None, **kwargs):\n        super(COCOSegmentation, self).__init__(root, split, mode, transform, **kwargs)\n        # lazy import pycocotools\n        from pycocotools.coco import COCO\n        from pycocotools import mask\n        if split == \'train\':\n            print(\'train set\')\n            ann_file = os.path.join(root, \'annotations/instances_train2017.json\')\n            ids_file = os.path.join(root, \'annotations/train_ids.pkl\')\n            self.root = os.path.join(root, \'train2017\')\n        else:\n            print(\'val set\')\n            ann_file = os.path.join(root, \'annotations/instances_val2017.json\')\n            ids_file = os.path.join(root, \'annotations/val_ids.pkl\')\n            self.root = os.path.join(root, \'val2017\')\n        self.coco = COCO(ann_file)\n        self.coco_mask = mask\n        if os.path.exists(ids_file):\n            with open(ids_file, \'rb\') as f:\n                self.ids = pickle.load(f)\n        else:\n            ids = list(self.coco.imgs.keys())\n            self.ids = self._preprocess(ids, ids_file)\n        self.transform = transform\n\n    def __getitem__(self, index):\n        coco = self.coco\n        img_id = self.ids[index]\n        img_metadata = coco.loadImgs(img_id)[0]\n        path = img_metadata[\'file_name\']\n        img = Image.open(os.path.join(self.root, path)).convert(\'RGB\')\n        cocotarget = coco.loadAnns(coco.getAnnIds(imgIds=img_id))\n        mask = Image.fromarray(self._gen_seg_mask(\n            cocotarget, img_metadata[\'height\'], img_metadata[\'width\']))\n        # synchrosized transform\n        if self.mode == \'train\':\n            img, mask = self._sync_transform(img, mask)\n        elif self.mode == \'val\':\n            img, mask = self._val_sync_transform(img, mask)\n        else:\n            assert self.mode == \'testval\'\n            img, mask = self._img_transform(img), self._mask_transform(mask)\n        # general resize, normalize and toTensor\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, mask, os.path.basename(path)\n\n    def __len__(self):\n        return len(self.ids)\n\n    def _mask_transform(self, mask):\n        return torch.LongTensor(np.array(mask).astype(\'int32\'))\n\n    def _gen_seg_mask(self, target, h, w):\n        mask = np.zeros((h, w), dtype=np.uint8)\n        coco_mask = self.coco_mask\n        for instance in target:\n            rle = coco_mask.frPyObjects(instance[\'segmentation\'], h, w)\n            m = coco_mask.decode(rle)\n            cat = instance[\'category_id\']\n            if cat in self.CAT_LIST:\n                c = self.CAT_LIST.index(cat)\n            else:\n                continue\n            if len(m.shape) < 3:\n                mask[:, :] += (mask == 0) * (m * c)\n            else:\n                mask[:, :] += (mask == 0) * (((np.sum(m, axis=2)) > 0) * c).astype(np.uint8)\n        return mask\n\n    def _preprocess(self, ids, ids_file):\n        print(""Preprocessing mask, this will take a while."" + \\\n              ""But don\'t worry, it only run once for each split."")\n        tbar = trange(len(ids))\n        new_ids = []\n        for i in tbar:\n            img_id = ids[i]\n            cocotarget = self.coco.loadAnns(self.coco.getAnnIds(imgIds=img_id))\n            img_metadata = self.coco.loadImgs(img_id)[0]\n            mask = self._gen_seg_mask(cocotarget, img_metadata[\'height\'], img_metadata[\'width\'])\n            # more than 1k pixels\n            if (mask > 0).sum() > 1000:\n                new_ids.append(img_id)\n            tbar.set_description(\'Doing: {}/{}, got {} qualified images\'. \\\n                                 format(i, len(ids), len(new_ids)))\n        print(\'Found number of qualified images: \', len(new_ids))\n        with open(ids_file, \'wb\') as f:\n            pickle.dump(new_ids, f)\n        return new_ids\n\n    @property\n    def classes(self):\n        """"""Category names.""""""\n        return (\'background\', \'airplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\',\n                \'bus\', \'car\', \'cat\', \'chair\', \'cow\', \'diningtable\', \'dog\', \'horse\',\n                \'motorcycle\', \'person\', \'potted-plant\', \'sheep\', \'sofa\', \'train\',\n                \'tv\')\n'"
segmentron/data/dataloader/pascal_aug.py,2,"b'""""""Pascal Augmented VOC Semantic Segmentation Dataset.""""""\nimport os\nimport torch\nimport scipy.io as sio\nimport numpy as np\n\nfrom PIL import Image\nfrom .seg_data_base import SegmentationDataset\n\n\nclass VOCAugSegmentation(SegmentationDataset):\n    """"""Pascal VOC Augmented Semantic Segmentation Dataset.\n\n    Parameters\n    ----------\n    root : string\n        Path to VOCdevkit folder. Default is \'./datasets/voc\'\n    split: string\n        \'train\', \'val\' or \'test\'\n    transform : callable, optional\n        A function that transforms the image\n    Examples\n    --------\n    >>> from torchvision import transforms\n    >>> import torch.utils.data as data\n    >>> # Transforms for Normalization\n    >>> input_transform = transforms.Compose([\n    >>>     transforms.ToTensor(),\n    >>>     transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n    >>> ])\n    >>> # Create Dataset\n    >>> trainset = VOCAugSegmentation(split=\'train\', transform=input_transform)\n    >>> # Create Training Loader\n    >>> train_data = data.DataLoader(\n    >>>     trainset, 4, shuffle=True,\n    >>>     num_workers=4)\n    """"""\n    BASE_DIR = \'VOCaug/dataset/\'\n    NUM_CLASS = 21\n\n    def __init__(self, root=\'datasets/voc\', split=\'train\', mode=None, transform=None, **kwargs):\n        super(VOCAugSegmentation, self).__init__(root, split, mode, transform, **kwargs)\n        # train/val/test splits are pre-cut\n        _voc_root = os.path.join(root, self.BASE_DIR)\n        _mask_dir = os.path.join(_voc_root, \'cls\')\n        _image_dir = os.path.join(_voc_root, \'img\')\n        if split == \'train\':\n            _split_f = os.path.join(_voc_root, \'trainval.txt\')\n        elif split == \'val\':\n            _split_f = os.path.join(_voc_root, \'val.txt\')\n        else:\n            raise RuntimeError(\'Unknown dataset split: {}\'.format(split))\n\n        self.images = []\n        self.masks = []\n        with open(os.path.join(_split_f), ""r"") as lines:\n            for line in lines:\n                _image = os.path.join(_image_dir, line.rstrip(\'\\n\') + "".jpg"")\n                assert os.path.isfile(_image)\n                self.images.append(_image)\n                _mask = os.path.join(_mask_dir, line.rstrip(\'\\n\') + "".mat"")\n                assert os.path.isfile(_mask)\n                self.masks.append(_mask)\n\n        assert (len(self.images) == len(self.masks))\n        print(\'Found {} images in the folder {}\'.format(len(self.images), _voc_root))\n\n    def __getitem__(self, index):\n        img = Image.open(self.images[index]).convert(\'RGB\')\n        target = self._load_mat(self.masks[index])\n        # synchrosized transform\n        if self.mode == \'train\':\n            img, target = self._sync_transform(img, target)\n        elif self.mode == \'val\':\n            img, target = self._val_sync_transform(img, target)\n        elif self.mode == \'testval\':\n            logging.warn(""Use mode of testval, you should set batch size=1"")\n            img, target = self._img_transform(img), self._mask_transform(target)\n        else:\n            raise RuntimeError(\'unknown mode for dataloader: {}\'.format(self.mode))\n        # general resize, normalize and toTensor\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, target, os.path.basename(self.images[index])\n\n    def _mask_transform(self, mask):\n        return torch.LongTensor(np.array(mask).astype(\'int32\'))\n\n    def _load_mat(self, filename):\n        mat = sio.loadmat(filename, mat_dtype=True, squeeze_me=True, struct_as_record=False)\n        mask = mat[\'GTcls\'].Segmentation\n        return Image.fromarray(mask)\n\n    def __len__(self):\n        return len(self.images)\n\n    @property\n    def classes(self):\n        """"""Category names.""""""\n        return (\'background\', \'airplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\',\n                \'bus\', \'car\', \'cat\', \'chair\', \'cow\', \'diningtable\', \'dog\', \'horse\',\n                \'motorcycle\', \'person\', \'potted-plant\', \'sheep\', \'sofa\', \'train\',\n                \'tv\')\n\n\nif __name__ == \'__main__\':\n    dataset = VOCAugSegmentation()'"
segmentron/data/dataloader/pascal_voc.py,2,"b'""""""Pascal VOC Semantic Segmentation Dataset.""""""\nimport os\nimport torch\nimport numpy as np\n\nfrom PIL import Image\nfrom .seg_data_base import SegmentationDataset\n\n\nclass VOCSegmentation(SegmentationDataset):\n    """"""Pascal VOC Semantic Segmentation Dataset.\n\n    Parameters\n    ----------\n    root : string\n        Path to VOCdevkit folder. Default is \'./datasets/VOCdevkit\'\n    split: string\n        \'train\', \'val\' or \'test\'\n    transform : callable, optional\n        A function that transforms the image\n    Examples\n    --------\n    >>> from torchvision import transforms\n    >>> import torch.utils.data as data\n    >>> # Transforms for Normalization\n    >>> input_transform = transforms.Compose([\n    >>>     transforms.ToTensor(),\n    >>>     transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n    >>> ])\n    >>> # Create Dataset\n    >>> trainset = VOCSegmentation(split=\'train\', transform=input_transform)\n    >>> # Create Training Loader\n    >>> train_data = data.DataLoader(\n    >>>     trainset, 4, shuffle=True,\n    >>>     num_workers=4)\n    """"""\n    BASE_DIR = \'VOC2012\'\n    NUM_CLASS = 21\n\n    def __init__(self, root=\'datasets/voc\', split=\'train\', mode=None, transform=None, **kwargs):\n        super(VOCSegmentation, self).__init__(root, split, mode, transform, **kwargs)\n        _voc_root = os.path.join(root, self.BASE_DIR)\n        _mask_dir = os.path.join(_voc_root, \'SegmentationClass\')\n        _image_dir = os.path.join(_voc_root, \'JPEGImages\')\n        # train/val/test splits are pre-cut\n        _splits_dir = os.path.join(_voc_root, \'ImageSets/Segmentation\')\n        if split == \'train\':\n            _split_f = os.path.join(_splits_dir, \'train.txt\')\n        elif split == \'val\':\n            _split_f = os.path.join(_splits_dir, \'val.txt\')\n        elif split == \'test\':\n            _split_f = os.path.join(_splits_dir, \'test.txt\')\n        else:\n            raise RuntimeError(\'Unknown dataset split.\')\n\n        self.images = []\n        self.masks = []\n        with open(os.path.join(_split_f), ""r"") as lines:\n            for line in lines:\n                _image = os.path.join(_image_dir, line.rstrip(\'\\n\') + "".jpg"")\n                assert os.path.isfile(_image)\n                self.images.append(_image)\n                if split != \'test\':\n                    _mask = os.path.join(_mask_dir, line.rstrip(\'\\n\') + "".png"")\n                    assert os.path.isfile(_mask)\n                    self.masks.append(_mask)\n\n        if split != \'test\':\n            assert (len(self.images) == len(self.masks))\n        print(\'Found {} images in the folder {}\'.format(len(self.images), _voc_root))\n\n    def __getitem__(self, index):\n        img = Image.open(self.images[index]).convert(\'RGB\')\n        if self.mode == \'test\':\n            img = self._img_transform(img)\n            if self.transform is not None:\n                img = self.transform(img)\n            return img, os.path.basename(self.images[index])\n        mask = Image.open(self.masks[index])\n        # synchronized transform\n        if self.mode == \'train\':\n            img, mask = self._sync_transform(img, mask)\n        elif self.mode == \'val\':\n            img, mask = self._val_sync_transform(img, mask)\n        else:\n            assert self.mode == \'testval\'\n            img, mask = self._img_transform(img), self._mask_transform(mask)\n        # general resize, normalize and toTensor\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img, mask, os.path.basename(self.images[index])\n\n    def __len__(self):\n        return len(self.images)\n\n    def _mask_transform(self, mask):\n        target = np.array(mask).astype(\'int32\')\n        target[target == 255] = -1\n        return torch.from_numpy(target).long()\n\n    @property\n    def classes(self):\n        """"""Category names.""""""\n        return (\'background\', \'airplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\',\n                \'bus\', \'car\', \'cat\', \'chair\', \'cow\', \'diningtable\', \'dog\', \'horse\',\n                \'motorcycle\', \'person\', \'potted-plant\', \'sheep\', \'sofa\', \'train\',\n                \'tv\')\n\n\nif __name__ == \'__main__\':\n    dataset = VOCSegmentation()'"
segmentron/data/dataloader/sbu_shadow.py,1,"b'""""""SBU Shadow  Segmentation Dataset.""""""\nimport os\nimport torch\nimport numpy as np\n\nfrom PIL import Image\nfrom .seg_data_base import SegmentationDataset\n\n\nclass SBUSegmentation(SegmentationDataset):\n    """"""SBU Shadow Segmentation Dataset\n    """"""\n    NUM_CLASS = 2\n\n    def __init__(self, root=\'datasets/sbu\', split=\'train\', mode=None, transform=None, **kwargs):\n        super(SBUSegmentation, self).__init__(root, split, mode, transform, **kwargs)\n        assert os.path.exists(self.root)\n        self.images, self.masks = _get_sbu_pairs(self.root, self.split)\n        assert (len(self.images) == len(self.masks))\n        if len(self.images) == 0:\n            raise RuntimeError(""Found 0 images in subfolders of:"" + root + ""\\n"")\n\n    def __getitem__(self, index):\n        img = Image.open(self.images[index]).convert(\'RGB\')\n        if self.mode == \'test\':\n            if self.transform is not None:\n                img = self.transform(img)\n            return img, os.path.basename(self.images[index])\n        mask = Image.open(self.masks[index])\n        # synchrosized transform\n        if self.mode == \'train\':\n            img, mask = self._sync_transform(img, mask)\n        elif self.mode == \'val\':\n            img, mask = self._val_sync_transform(img, mask)\n        else:\n            assert self.mode == \'testval\'\n            img, mask = self._img_transform(img), self._mask_transform(mask)\n        # general resize, normalize and toTensor\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, mask, os.path.basename(self.images[index])\n\n    def _mask_transform(self, mask):\n        target = np.array(mask).astype(\'int32\')\n        target[target > 0] = 1\n        return torch.from_numpy(target).long()\n\n    def __len__(self):\n        return len(self.images)\n\n    @property\n    def pred_offset(self):\n        return 0\n\n\ndef _get_sbu_pairs(folder, split=\'train\'):\n    def get_path_pairs(img_folder, mask_folder):\n        img_paths = []\n        mask_paths = []\n        for root, _, files in os.walk(img_folder):\n            print(root)\n            for filename in files:\n                if filename.endswith(\'.jpg\'):\n                    imgpath = os.path.join(root, filename)\n                    maskname = filename.replace(\'.jpg\', \'.png\')\n                    maskpath = os.path.join(mask_folder, maskname)\n                    if os.path.isfile(imgpath) and os.path.isfile(maskpath):\n                        img_paths.append(imgpath)\n                        mask_paths.append(maskpath)\n                    else:\n                        print(\'cannot find the mask or image:\', imgpath, maskpath)\n        print(\'Found {} images in the folder {}\'.format(len(img_paths), img_folder))\n        return img_paths, mask_paths\n\n    if split == \'train\':\n        img_folder = os.path.join(folder, \'SBUTrain4KRecoveredSmall/ShadowImages\')\n        mask_folder = os.path.join(folder, \'SBUTrain4KRecoveredSmall/ShadowMasks\')\n        img_paths, mask_paths = get_path_pairs(img_folder, mask_folder)\n    else:\n        assert split in (\'val\', \'test\')\n        img_folder = os.path.join(folder, \'SBU-Test/ShadowImages\')\n        mask_folder = os.path.join(folder, \'SBU-Test/ShadowMasks\')\n        img_paths, mask_paths = get_path_pairs(img_folder, mask_folder)\n    return img_paths, mask_paths\n\n\nif __name__ == \'__main__\':\n    dataset = SBUSegmentation(base_size=280, crop_size=256)'"
segmentron/data/dataloader/seg_data_base.py,0,"b'""""""Base segmentation dataset""""""\nimport os\nimport random\nimport numpy as np\nimport torchvision\n\nfrom PIL import Image, ImageOps, ImageFilter\nfrom ...config import cfg\n\n__all__ = [\'SegmentationDataset\']\n\n\nclass SegmentationDataset(object):\n    """"""Segmentation Base Dataset""""""\n\n    def __init__(self, root, split, mode, transform, base_size=520, crop_size=480):\n        super(SegmentationDataset, self).__init__()\n        self.root = os.path.join(cfg.ROOT_PATH, root)\n        self.transform = transform\n        self.split = split\n        self.mode = mode if mode is not None else split\n        self.base_size = base_size\n        self.crop_size = self.to_tuple(crop_size)\n        self.color_jitter = self._get_color_jitter()\n\n    def to_tuple(self, size):\n        if isinstance(size, (list, tuple)):\n            return tuple(size)\n        elif isinstance(size, (int, float)):\n            return tuple((size, size))\n        else:\n            raise ValueError(\'Unsupport datatype: {}\'.format(type(size)))\n\n    def _get_color_jitter(self):\n        color_jitter = cfg.AUG.COLOR_JITTER\n        if color_jitter is None:\n            return None\n        if isinstance(color_jitter, (list, tuple)):\n            # color jitter should be a 3-tuple/list if spec brightness/contrast/saturation\n            # or 4 if also augmenting hue\n            assert len(color_jitter) in (3, 4)\n        else:\n            # if it\'s a scalar, duplicate for brightness, contrast, and saturation, no hue\n            color_jitter = (float(color_jitter),) * 3\n        return torchvision.transforms.ColorJitter(*color_jitter)\n\n    def _val_sync_transform(self, img, mask):\n        outsize = self.crop_size\n        short_size = min(outsize)\n        w, h = img.size\n        if w > h:\n            oh = short_size\n            ow = int(1.0 * w * oh / h)\n        else:\n            ow = short_size\n            oh = int(1.0 * h * ow / w)\n        img = img.resize((ow, oh), Image.BILINEAR)\n        mask = mask.resize((ow, oh), Image.NEAREST)\n        # center crop\n        w, h = img.size\n        x1 = int(round((w - outsize[1]) / 2.))\n        y1 = int(round((h - outsize[0]) / 2.))\n        img = img.crop((x1, y1, x1 + outsize[1], y1 + outsize[0]))\n        mask = mask.crop((x1, y1, x1 + outsize[1], y1 + outsize[0]))\n\n        # final transform\n        img, mask = self._img_transform(img), self._mask_transform(mask)\n        return img, mask\n\n    def _sync_transform(self, img, mask):\n        # random mirror\n        if cfg.AUG.MIRROR and random.random() < 0.5:\n            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n        crop_size = self.crop_size\n        # random scale (short edge)\n        short_size = random.randint(int(self.base_size * 0.5), int(self.base_size * 2.0))\n        w, h = img.size\n        if h > w:\n            ow = short_size\n            oh = int(1.0 * h * ow / w)\n        else:\n            oh = short_size\n            ow = int(1.0 * w * oh / h)\n        img = img.resize((ow, oh), Image.BILINEAR)\n        mask = mask.resize((ow, oh), Image.NEAREST)\n        # pad crop\n        if short_size < min(crop_size):\n            padh = crop_size[0] - oh if oh < crop_size[0] else 0\n            padw = crop_size[1] - ow if ow < crop_size[1] else 0\n            img = ImageOps.expand(img, border=(0, 0, padw, padh), fill=0)\n            mask = ImageOps.expand(mask, border=(0, 0, padw, padh), fill=-1)\n        # random crop crop_size\n        w, h = img.size\n        x1 = random.randint(0, w - crop_size[1])\n        y1 = random.randint(0, h - crop_size[0])\n        img = img.crop((x1, y1, x1 + crop_size[1], y1 + crop_size[0]))\n        mask = mask.crop((x1, y1, x1 + crop_size[1], y1 + crop_size[0]))\n        # gaussian blur as in PSP\n        if cfg.AUG.BLUR_PROB > 0 and random.random() < cfg.AUG.BLUR_PROB:\n            radius = cfg.AUG.BLUR_RADIUS if cfg.AUG.BLUR_RADIUS > 0 else random.random()\n            img = img.filter(ImageFilter.GaussianBlur(radius=radius))\n        # color jitter\n        if self.color_jitter:\n            img = self.color_jitter(img)\n        # final transform\n        img, mask = self._img_transform(img), self._mask_transform(mask)\n        return img, mask\n\n    def _img_transform(self, img):\n        return np.array(img)\n\n    def _mask_transform(self, mask):\n        return np.array(mask).astype(\'int32\')\n\n    @property\n    def num_class(self):\n        """"""Number of categories.""""""\n        return self.NUM_CLASS\n\n    @property\n    def pred_offset(self):\n        return 0\n'"
segmentron/data/dataloader/utils.py,1,"b'import os\nimport hashlib\nimport errno\nimport tarfile\nfrom six.moves import urllib\nfrom torch.utils.model_zoo import tqdm\n\ndef gen_bar_updater():\n    pbar = tqdm(total=None)\n\n    def bar_update(count, block_size, total_size):\n        if pbar.total is None and total_size:\n            pbar.total = total_size\n        progress_bytes = count * block_size\n        pbar.update(progress_bytes - pbar.n)\n\n    return bar_update\n\ndef check_integrity(fpath, md5=None):\n    if md5 is None:\n        return True\n    if not os.path.isfile(fpath):\n        return False\n    md5o = hashlib.md5()\n    with open(fpath, \'rb\') as f:\n        # read in 1MB chunks\n        for chunk in iter(lambda: f.read(1024 * 1024), b\'\'):\n            md5o.update(chunk)\n    md5c = md5o.hexdigest()\n    if md5c != md5:\n        return False\n    return True\n\ndef makedir_exist_ok(dirpath):\n    try:\n        os.makedirs(dirpath)\n    except OSError as e:\n        if e.errno == errno.EEXIST:\n            pass\n        else:\n            pass\n\ndef download_url(url, root, filename=None, md5=None):\n    """"""Download a file from a url and place it in root.""""""\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = os.path.basename(url)\n    fpath = os.path.join(root, filename)\n\n    makedir_exist_ok(root)\n\n    # downloads file\n    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n        print(\'Using downloaded and verified file: \' + fpath)\n    else:\n        try:\n            print(\'Downloading \' + url + \' to \' + fpath)\n            urllib.request.urlretrieve(url, fpath, reporthook=gen_bar_updater())\n        except OSError:\n            if url[:5] == \'https\':\n                url = url.replace(\'https:\', \'http:\')\n                print(\'Failed download. Trying https -> http instead.\'\n                      \' Downloading \' + url + \' to \' + fpath)\n                urllib.request.urlretrieve(url, fpath, reporthook=gen_bar_updater())\n\ndef download_extract(url, root, filename, md5):\n    download_url(url, root, filename, md5)\n    with tarfile.open(os.path.join(root, filename), ""r"") as tar:\n        tar.extractall(path=root)'"
segmentron/data/downloader/__init__.py,0,b''
segmentron/data/downloader/ade20k.py,0,"b'""""""Prepare ADE20K dataset""""""\nimport os\nimport sys\n\ncur_path = os.path.abspath(os.path.dirname(__file__))\nroot_path = os.path.split(os.path.split(os.path.split(cur_path)[0])[0])[0]\nsys.path.append(root_path)\n\nimport argparse\nimport zipfile\nfrom segmentron.utils import download, makedirs\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Initialize ADE20K dataset.\',\n        epilog=\'Example: python setup_ade20k.py\',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--download-dir\', default=None, help=\'dataset directory on disk\')\n    args = parser.parse_args()\n    return args\n\ndef download_ade(path, overwrite=False):\n    _AUG_DOWNLOAD_URLS = [\n        (\'http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip\',\n         \'219e1696abb36c8ba3a3afe7fb2f4b4606a897c7\'),\n        (\'http://data.csail.mit.edu/places/ADEchallenge/release_test.zip\',\n         \'e05747892219d10e9243933371a497e905a4860c\'),\n    ]\n    download_dir = os.path.join(path, \'downloads\')\n    makedirs(download_dir)\n    for url, checksum in _AUG_DOWNLOAD_URLS:\n        filename = download(url, path=download_dir, overwrite=overwrite, sha1_hash=checksum)\n        # extract\n        with zipfile.ZipFile(filename,""r"") as zip_ref:\n            zip_ref.extractall(path=path)\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    default_dir = os.path.join(root_path, \'datasets/ade\')\n    if args.download_dir is not None:\n        _TARGET_DIR = args.download_dir\n    else:\n        _TARGET_DIR = default_dir\n    makedirs(_TARGET_DIR)\n\n    if os.path.exists(default_dir):\n        print(\'{} is already exist!\'.format(default_dir))\n    else:\n        try:\n            os.symlink(_TARGET_DIR, default_dir)\n        except Exception as e:\n            print(e)\n        download_ade(_TARGET_DIR, overwrite=False)\n'"
segmentron/data/downloader/cityscapes.py,0,"b'""""""Prepare Cityscapes dataset""""""\nimport os\nimport sys\nimport argparse\nimport zipfile\n\ncur_path = os.path.abspath(os.path.dirname(__file__))\nroot_path = os.path.split(os.path.split(os.path.split(cur_path)[0])[0])[0]\nsys.path.append(root_path)\n\nfrom segmentron.utils import makedirs, check_sha1\n\n_TARGET_DIR = os.path.expanduser(\'~/.torch/datasets/citys\')\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Initialize ADE20K dataset.\',\n        epilog=\'Example: python prepare_cityscapes.py\',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--download-dir\', default=None, help=\'dataset directory on disk\')\n    args = parser.parse_args()\n    return args\n\n\ndef download_city(path, overwrite=False):\n    _CITY_DOWNLOAD_URLS = [\n        (\'gtFine_trainvaltest.zip\', \'99f532cb1af174f5fcc4c5bc8feea8c66246ddbc\'),\n        (\'leftImg8bit_trainvaltest.zip\', \'2c0b77ce9933cc635adda307fbba5566f5d9d404\')]\n    download_dir = os.path.join(path, \'downloads\')\n    makedirs(download_dir)\n    for filename, checksum in _CITY_DOWNLOAD_URLS:\n        if not check_sha1(filename, checksum):\n            raise UserWarning(\'File {} is downloaded but the content hash does not match. \' \\\n                              \'The repo may be outdated or download may be incomplete. \' \\\n                              \'If the ""repo_url"" is overridden, consider switching to \' \\\n                              \'the default repo.\'.format(filename))\n        # extract\n        with zipfile.ZipFile(filename, ""r"") as zip_ref:\n            zip_ref.extractall(path=path)\n        print(""Extracted"", filename)\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    makedirs(os.path.expanduser(\'~/.torch/datasets\'))\n    if args.download_dir is not None:\n        if os.path.isdir(_TARGET_DIR):\n            os.remove(_TARGET_DIR)\n        # make symlink\n        os.symlink(args.download_dir, _TARGET_DIR)\n    else:\n        download_city(_TARGET_DIR, overwrite=False)\n'"
segmentron/data/downloader/mscoco.py,0,"b'""""""Prepare MS COCO datasets""""""\nimport os\nimport sys\nimport argparse\nimport zipfile\n\n# TODO: optim code\ncur_path = os.path.abspath(os.path.dirname(__file__))\nroot_path = os.path.split(os.path.split(os.path.split(cur_path)[0])[0])[0]\nsys.path.append(root_path)\n\nfrom segmentron.utils import download, makedirs\n\n_TARGET_DIR = os.path.expanduser(\'~/.torch/datasets/coco\')\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Initialize MS COCO dataset.\',\n        epilog=\'Example: python mscoco.py --download-dir ~/mscoco\',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--download-dir\', type=str, default=None, help=\'dataset directory on disk\')\n    parser.add_argument(\'--no-download\', action=\'store_true\', help=\'disable automatic download if set\')\n    parser.add_argument(\'--overwrite\', action=\'store_true\',\n                        help=\'overwrite downloaded files if set, in case they are corrupted\')\n    args = parser.parse_args()\n    return args\n\n\ndef download_coco(path, overwrite=False):\n    _DOWNLOAD_URLS = [\n        (\'http://images.cocodataset.org/zips/train2017.zip\',\n         \'10ad623668ab00c62c096f0ed636d6aff41faca5\'),\n        (\'http://images.cocodataset.org/annotations/annotations_trainval2017.zip\',\n         \'8551ee4bb5860311e79dace7e79cb91e432e78b3\'),\n        (\'http://images.cocodataset.org/zips/val2017.zip\',\n         \'4950dc9d00dbe1c933ee0170f5797584351d2a41\'),\n        # (\'http://images.cocodataset.org/annotations/stuff_annotations_trainval2017.zip\',\n        # \'46cdcf715b6b4f67e980b529534e79c2edffe084\'),\n        # test2017.zip, for those who want to attend the competition.\n        # (\'http://images.cocodataset.org/zips/test2017.zip\',\n        #  \'4e443f8a2eca6b1dac8a6c57641b67dd40621a49\'),\n    ]\n    makedirs(path)\n    for url, checksum in _DOWNLOAD_URLS:\n        filename = download(url, path=path, overwrite=overwrite, sha1_hash=checksum)\n        # extract\n        with zipfile.ZipFile(filename) as zf:\n            zf.extractall(path=path)\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    default_dir = os.path.join(root_path, \'datasets/coco\')\n    if args.download_dir is not None:\n        path = args.download_dir\n    else:\n        path = default_dir\n    if not os.path.isdir(path) or not os.path.isdir(os.path.join(path, \'train2017\')) \\\n            or not os.path.isdir(os.path.join(path, \'val2017\')) \\\n            or not os.path.isdir(os.path.join(path, \'annotations\')):\n        if args.no_download:\n            raise ValueError((\'{} is not a valid directory, make sure it is present.\'\n                              \' Or you should not disable ""--no-download"" to grab it\'.format(path)))\n        else:\n            download_coco(path, overwrite=args.overwrite)\n\n    # make symlink\n    try:\n        os.symlink(path, default_dir)\n    except Exception as e:\n        print(e)\n'"
segmentron/data/downloader/pascal_voc.py,0,"b'""""""Prepare PASCAL VOC datasets""""""\nimport os\nimport sys\n\ncur_path = os.path.abspath(os.path.dirname(__file__))\nroot_path = os.path.split(os.path.split(os.path.split(cur_path)[0])[0])[0]\nsys.path.append(root_path)\n\nimport argparse\nimport shutil\nimport tarfile\nfrom segmentron.utils import download, makedirs\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Initialize PASCAL VOC dataset.\',\n        epilog=\'Example: python pascal_voc.py --download-dir ~/VOCdevkit\',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--download-dir\', type=str, default=None, help=\'dataset directory on disk\')\n    parser.add_argument(\'--no-download\', action=\'store_true\', help=\'disable automatic download if set\')\n    parser.add_argument(\'--overwrite\', action=\'store_true\',\n                        help=\'overwrite downloaded files if set, in case they are corrupted\')\n    args = parser.parse_args()\n    return args\n\n\n#####################################################################################\n# Download and extract VOC datasets into ``path``\n\ndef download_voc(path, overwrite=False):\n    _DOWNLOAD_URLS = [\n        (\'http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\',\n         \'34ed68851bce2a36e2a223fa52c661d592c66b3c\'),\n        (\'http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\',\n         \'41a8d6e12baa5ab18ee7f8f8029b9e11805b4ef1\'),\n        (\'http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\',\n         \'4e443f8a2eca6b1dac8a6c57641b67dd40621a49\')]\n    makedirs(path)\n    for url, checksum in _DOWNLOAD_URLS:\n        filename = download(url, path=path, overwrite=overwrite, sha1_hash=checksum)\n        # extract\n        with tarfile.open(filename) as tar:\n            tar.extractall(path=path)\n\n\n#####################################################################################\n# Download and extract the VOC augmented segmentation dataset into ``path``\n\ndef download_aug(path, overwrite=False):\n    _AUG_DOWNLOAD_URLS = [\n        (\'http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz\',\n         \'7129e0a480c2d6afb02b517bb18ac54283bfaa35\')]\n    makedirs(path)\n    for url, checksum in _AUG_DOWNLOAD_URLS:\n        filename = download(url, path=path, overwrite=overwrite, sha1_hash=checksum)\n        # extract\n        with tarfile.open(filename) as tar:\n            tar.extractall(path=path)\n            shutil.move(os.path.join(path, \'benchmark_RELEASE\'),\n                        os.path.join(path, \'VOCaug\'))\n            filenames = [\'VOCaug/dataset/train.txt\', \'VOCaug/dataset/val.txt\']\n            # generate trainval.txt\n            with open(os.path.join(path, \'VOCaug/dataset/trainval.txt\'), \'w\') as outfile:\n                for fname in filenames:\n                    fname = os.path.join(path, fname)\n                    with open(fname) as infile:\n                        for line in infile:\n                            outfile.write(line)\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    default_dir = os.path.join(root_path, \'datasets/voc\')\n    if args.download_dir is not None:\n        path = args.download_dir\n    else:\n        path = default_dir\n    if not os.path.isfile(path) or not os.path.isdir(os.path.join(path, \'VOC2007\')) \\\n            or not os.path.isdir(os.path.join(path, \'VOC2012\')):\n        if args.no_download:\n            raise ValueError((\'{} is not a valid directory, make sure it is present.\'\n                              \' Or you should not disable ""--no-download"" to grab it\'.format(path)))\n        else:\n            download_voc(path, overwrite=args.overwrite)\n            shutil.move(os.path.join(path, \'VOCdevkit\', \'VOC2007\'), os.path.join(path, \'VOC2007\'))\n            shutil.move(os.path.join(path, \'VOCdevkit\', \'VOC2012\'), os.path.join(path, \'VOC2012\'))\n            shutil.rmtree(os.path.join(path, \'VOCdevkit\'))\n\n    if not os.path.isdir(os.path.join(path, \'VOCaug\')):\n        if args.no_download:\n            raise ValueError((\'{} is not a valid directory, make sure it is present.\'\n                              \' Or you should not disable ""--no-download"" to grab it\'.format(path)))\n        else:\n            download_aug(path, overwrite=args.overwrite)\n\n    try:\n        os.symlink(path, default_dir)\n    except Exception as e:\n        print(e)\n'"
segmentron/data/downloader/sbu_shadow.py,0,"b'""""""Prepare SBU Shadow datasets""""""\nimport os\nimport sys\nimport argparse\nimport zipfile\n\ncur_path = os.path.abspath(os.path.dirname(__file__))\nroot_path = os.path.split(os.path.split(os.path.split(cur_path)[0])[0])[0]\nsys.path.append(root_path)\n\nfrom segmentron.utils import download, makedirs\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Initialize SBU Shadow dataset.\',\n        epilog=\'Example: python sbu_shadow.py --download-dir ~/SBU-shadow\',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--download-dir\', type=str, default=None, help=\'dataset directory on disk\')\n    parser.add_argument(\'--no-download\', action=\'store_true\', help=\'disable automatic download if set\')\n    parser.add_argument(\'--overwrite\', action=\'store_true\',\n                        help=\'overwrite downloaded files if set, in case they are corrupted\')\n    args = parser.parse_args()\n    return args\n\n\n#####################################################################################\n# Download and extract SBU shadow datasets into ``path``\n\ndef download_sbu(path, overwrite=False):\n    _DOWNLOAD_URLS = [\n        (\'http://www3.cs.stonybrook.edu/~cvl/content/datasets/shadow_db/SBU-shadow.zip\'),\n    ]\n    download_dir = os.path.join(path, \'downloads\')\n    makedirs(download_dir)\n    for url in _DOWNLOAD_URLS:\n        filename = download(url, path=path, overwrite=overwrite)\n        # extract\n        with zipfile.ZipFile(filename, ""r"") as zf:\n            zf.extractall(path=path)\n        print(""Extracted"", filename)\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    default_dir = os.path.join(root_path, \'datasets/sbu\')\n    if args.download_dir is not None:\n        _TARGET_DIR = args.download_dir\n    else:\n        _TARGET_DIR = default_dir\n    makedirs(_TARGET_DIR)\n    if os.path.exists(default_dir):\n        print(\'{} is already exist!\'.format(default_dir))\n    else:\n        try:\n            os.symlink(_TARGET_DIR, default_dir)\n        except Exception as e:\n            print(e)\n        download_sbu(_TARGET_DIR, overwrite=False)\n'"
segmentron/models/backbones/__init__.py,0,"b'from .build import BACKBONE_REGISTRY, get_segmentation_backbone\nfrom .xception import *\nfrom .mobilenet import *\nfrom .resnet import *\nfrom .hrnet import *\nfrom .eespnet import *\n'"
segmentron/models/backbones/build.py,10,"b'import os\nimport torch\nimport logging\nimport torch.utils.model_zoo as model_zoo\n\nfrom ...utils.download import download\nfrom ...utils.registry import Registry\nfrom ...config import cfg\n\nBACKBONE_REGISTRY = Registry(""BACKBONE"")\nBACKBONE_REGISTRY.__doc__ = """"""\nRegistry for backbone, i.e. resnet.\n\nThe registered object will be called with `obj()`\nand expected to return a `nn.Module` object.\n""""""\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n    \'resnet50c\': \'https://github.com/LikeLy-Journey/SegmenTron/releases/download/v0.1.0/resnet50-25c4b509.pth\',\n    \'resnet101c\': \'https://github.com/LikeLy-Journey/SegmenTron/releases/download/v0.1.0/resnet101-2a57e44d.pth\',\n    \'resnet152c\': \'https://github.com/LikeLy-Journey/SegmenTron/releases/download/v0.1.0/resnet152-0d43d698.pth\',\n    \'xception65\': \'https://github.com/LikeLy-Journey/SegmenTron/releases/download/v0.1.0/tf-xception65-270e81cf.pth\',\n    \'hrnet_w18_small_v1\': \'https://github.com/LikeLy-Journey/SegmenTron/releases/download/v0.1.0/hrnet-w18-small-v1-08f8ae64.pth\',\n    \'mobilenet_v2\': \'https://github.com/LikeLy-Journey/SegmenTron/releases/download/v0.1.0/mobilenetV2-15498621.pth\',\n}\n\n\ndef load_backbone_pretrained(model, backbone):\n    if cfg.PHASE == \'train\' and cfg.TRAIN.BACKBONE_PRETRAINED and (not cfg.TRAIN.PRETRAINED_MODEL_PATH):\n        if os.path.isfile(cfg.TRAIN.BACKBONE_PRETRAINED_PATH):\n            logging.info(\'Load backbone pretrained model from {}\'.format(\n                cfg.TRAIN.BACKBONE_PRETRAINED_PATH\n            ))\n            msg = model.load_state_dict(torch.load(cfg.TRAIN.BACKBONE_PRETRAINED_PATH), strict=False)\n            logging.info(msg)\n        elif backbone not in model_urls:\n            logging.info(\'{} has no pretrained model\'.format(backbone))\n            return\n        else:\n            logging.info(\'load backbone pretrained model from url..\')\n            try:\n                msg = model.load_state_dict(model_zoo.load_url(model_urls[backbone]), strict=False)\n            except Exception as e:\n                logging.warning(e)\n                logging.info(\'Use torch download failed, try custom method!\')\n                \n                msg = model.load_state_dict(torch.load(download(model_urls[backbone], \n                        path=os.path.join(torch.hub._get_torch_home(), \'checkpoints\'))), strict=False)\n            logging.info(msg)\n\n\ndef get_segmentation_backbone(backbone, norm_layer=torch.nn.BatchNorm2d):\n    """"""\n    Built the backbone model, defined by `cfg.MODEL.BACKBONE`.\n    """"""\n    model = BACKBONE_REGISTRY.get(backbone)(norm_layer)\n    load_backbone_pretrained(model, backbone)\n    return model\n\n'"
segmentron/models/backbones/eespnet.py,4,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ...modules import _ConvBNPReLU, _ConvBN, _BNPReLU, EESP\nfrom .build import BACKBONE_REGISTRY\nfrom ...config import cfg\n\n__all__ = [\'EESPNet\', \'eespnet\']\n\n\nclass DownSampler(nn.Module):\n\n    def __init__(self, in_channels, out_channels, k=4, r_lim=9, reinf=True, inp_reinf=3, norm_layer=None):\n        super(DownSampler, self).__init__()\n        channels_diff = out_channels - in_channels\n        self.eesp = EESP(in_channels, channels_diff, stride=2, k=k,\n                         r_lim=r_lim, down_method=\'avg\', norm_layer=norm_layer)\n        self.avg = nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n        if reinf:\n            self.inp_reinf = nn.Sequential(\n                _ConvBNPReLU(inp_reinf, inp_reinf, 3, 1, 1),\n                _ConvBN(inp_reinf, out_channels, 1, 1))\n        self.act = nn.PReLU(out_channels)\n\n    def forward(self, x, x2=None):\n        avg_out = self.avg(x)\n        eesp_out = self.eesp(x)\n        output = torch.cat([avg_out, eesp_out], 1)\n        if x2 is not None:\n            w1 = avg_out.size(2)\n            while True:\n                x2 = F.avg_pool2d(x2, kernel_size=3, padding=1, stride=2)\n                w2 = x2.size(2)\n                if w2 == w1:\n                    break\n            output = output + self.inp_reinf(x2)\n\n        return self.act(output)\n\n\nclass EESPNet(nn.Module):\n    def __init__(self, num_classes=1000, scale=1, reinf=True, norm_layer=nn.BatchNorm2d):\n        super(EESPNet, self).__init__()\n        inp_reinf = 3 if reinf else None\n        reps = [0, 3, 7, 3]\n        r_lim = [13, 11, 9, 7, 5]\n        K = [4] * len(r_lim)\n\n        # set out_channels\n        base, levels, base_s = 32, 5, 0\n        out_channels = [base] * levels\n        for i in range(levels):\n            if i == 0:\n                base_s = int(base * scale)\n                base_s = math.ceil(base_s / K[0]) * K[0]\n                out_channels[i] = base if base_s > base else base_s\n            else:\n                out_channels[i] = base_s * pow(2, i)\n        if scale <= 1.5:\n            out_channels.append(1024)\n        elif scale in [1.5, 2]:\n            out_channels.append(1280)\n        else:\n            raise ValueError(""Unknown scale value."")\n\n        self.level1 = _ConvBNPReLU(3, out_channels[0], 3, 2, 1, norm_layer=norm_layer)\n\n        self.level2_0 = DownSampler(out_channels[0], out_channels[1], k=K[0], r_lim=r_lim[0],\n                                    reinf=reinf, inp_reinf=inp_reinf, norm_layer=norm_layer)\n\n        self.level3_0 = DownSampler(out_channels[1], out_channels[2], k=K[1], r_lim=r_lim[1],\n                                    reinf=reinf, inp_reinf=inp_reinf, norm_layer=norm_layer)\n        self.level3 = nn.ModuleList()\n        for i in range(reps[1]):\n            self.level3.append(EESP(out_channels[2], out_channels[2], k=K[2], r_lim=r_lim[2],\n                                    norm_layer=norm_layer))\n\n        self.level4_0 = DownSampler(out_channels[2], out_channels[3], k=K[2], r_lim=r_lim[2],\n                                    reinf=reinf, inp_reinf=inp_reinf, norm_layer=norm_layer)\n        self.level4 = nn.ModuleList()\n        for i in range(reps[2]):\n            self.level4.append(EESP(out_channels[3], out_channels[3], k=K[3], r_lim=r_lim[3],\n                                    norm_layer=norm_layer))\n\n        self.level5_0 = DownSampler(out_channels[3], out_channels[4], k=K[3], r_lim=r_lim[3],\n                                    reinf=reinf, inp_reinf=inp_reinf, norm_layer=norm_layer)\n        self.level5 = nn.ModuleList()\n        for i in range(reps[2]):\n            self.level5.append(EESP(out_channels[4], out_channels[4], k=K[4], r_lim=r_lim[4],\n                                    norm_layer=norm_layer))\n\n        self.level5.append(_ConvBNPReLU(out_channels[4], out_channels[4], 3, 1, 1,\n                                        groups=out_channels[4], norm_layer=norm_layer))\n        self.level5.append(_ConvBNPReLU(out_channels[4], out_channels[5], 1, 1, 0,\n                                        groups=K[4], norm_layer=norm_layer))\n\n        self.fc = nn.Linear(out_channels[5], num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, std=0.001)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x, seg=True):\n        out_l1 = self.level1(x)\n\n        out_l2 = self.level2_0(out_l1, x)\n\n        out_l3_0 = self.level3_0(out_l2, x)\n        for i, layer in enumerate(self.level3):\n            if i == 0:\n                out_l3 = layer(out_l3_0)\n            else:\n                out_l3 = layer(out_l3)\n\n        out_l4_0 = self.level4_0(out_l3, x)\n        for i, layer in enumerate(self.level4):\n            if i == 0:\n                out_l4 = layer(out_l4_0)\n            else:\n                out_l4 = layer(out_l4)\n\n        if not seg:\n            out_l5_0 = self.level5_0(out_l4)  # down-sampled\n            for i, layer in enumerate(self.level5):\n                if i == 0:\n                    out_l5 = layer(out_l5_0)\n                else:\n                    out_l5 = layer(out_l5)\n\n            output_g = F.adaptive_avg_pool2d(out_l5, output_size=1)\n            output_g = F.dropout(output_g, p=0.2, training=self.training)\n            output_1x1 = output_g.view(output_g.size(0), -1)\n\n            return self.fc(output_1x1)\n        return out_l1, out_l2, out_l3, out_l4\n\n\n@BACKBONE_REGISTRY.register()\ndef eespnet(norm_layer=nn.BatchNorm2d):\n    return EESPNet(norm_layer=norm_layer)\n\n# def eespnet(pretrained=False, **kwargs):\n#     model = EESPNet(**kwargs)\n#     if pretrained:\n#         raise ValueError(""Don\'t support pretrained"")\n#     return model\n\n\nif __name__ == \'__main__\':\n    img = torch.randn(1, 3, 224, 224)\n    model = eespnet()\n    out = model(img)\n'"
segmentron/models/backbones/hrnet.py,4,"b'# this code is heavily based on https://github.com/HRNet\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport logging\n\nimport torch\nimport torch.nn as nn\nimport torch._utils\nimport numpy as np\n\nfrom .build import BACKBONE_REGISTRY\nfrom ...config import cfg\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass HighResolutionModule(nn.Module):\n    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n                 num_channels, fuse_method, multi_scale_output=True):\n        super(HighResolutionModule, self).__init__()\n        self._check_branches(\n            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n\n        self.num_inchannels = num_inchannels\n        self.fuse_method = fuse_method\n        self.num_branches = num_branches\n\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(\n            num_branches, blocks, num_blocks, num_channels)\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU(False)\n\n    def _check_branches(self, num_branches, blocks, num_blocks,\n                        num_inchannels, num_channels):\n        if num_branches != len(num_blocks):\n            error_msg = \'NUM_BRANCHES({}) <> NUM_BLOCKS({})\'.format(\n                num_branches, len(num_blocks))\n            logging.error(error_msg)\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = \'NUM_BRANCHES({}) <> NUM_CHANNELS({})\'.format(\n                num_branches, len(num_channels))\n            logging.error(error_msg)\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_inchannels):\n            error_msg = \'NUM_BRANCHES({}) <> NUM_INCHANNELS({})\'.format(\n                num_branches, len(num_inchannels))\n            logging.error(error_msg)\n            raise ValueError(error_msg)\n\n    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n                         stride=1):\n        downsample = None\n        if stride != 1 or \\\n                self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.num_inchannels[branch_index],\n                          num_channels[branch_index] * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(num_channels[branch_index] * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.num_inchannels[branch_index],\n                            num_channels[branch_index], stride, downsample))\n        self.num_inchannels[branch_index] = \\\n            num_channels[branch_index] * block.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(block(self.num_inchannels[branch_index],\n                                num_channels[branch_index]))\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n        branches = []\n\n        for i in range(num_branches):\n            branches.append(\n                self._make_one_branch(i, block, num_blocks, num_channels))\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self):\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(nn.Sequential(\n                        nn.Conv2d(num_inchannels[j],\n                                  num_inchannels[i],\n                                  1,\n                                  1,\n                                  0,\n                                  bias=False),\n                        nn.BatchNorm2d(num_inchannels[i]),\n                        nn.Upsample(scale_factor=2 ** (j - i), mode=\'nearest\')))\n                elif j == i:\n                    fuse_layer.append(None)\n                else:\n                    conv3x3s = []\n                    for k in range(i - j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j],\n                                          num_outchannels_conv3x3,\n                                          3, 2, 1, bias=False),\n                                nn.BatchNorm2d(num_outchannels_conv3x3)))\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j],\n                                          num_outchannels_conv3x3,\n                                          3, 2, 1, bias=False),\n                                nn.BatchNorm2d(num_outchannels_conv3x3),\n                                nn.ReLU(False)))\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def get_num_inchannels(self):\n        return self.num_inchannels\n\n    def forward(self, x):\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = []\n        for i in range(len(self.fuse_layers)):\n            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n            for j in range(1, self.num_branches):\n                if i == j:\n                    y = y + x[j]\n                else:\n                    y = y + self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n\n        return x_fuse\n\n\nblocks_dict = {\n    \'BASIC\': BasicBlock,\n    \'BOTTLENECK\': Bottleneck\n}\n\n\nclass HighResolutionNet(nn.Module):\n\n    def __init__(self, norm_layer=nn.BatchNorm2d):\n        super(HighResolutionNet, self).__init__()\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn1 = norm_layer(64)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn2 = norm_layer(64)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.stage1_cfg = cfg.MODEL.HRNET.STAGE1\n        num_channels = self.stage1_cfg[\'NUM_CHANNELS\'][0]\n        block = blocks_dict[self.stage1_cfg[\'BLOCK\']]\n        num_blocks = self.stage1_cfg[\'NUM_BLOCKS\'][0]\n        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks, norm_layer=norm_layer)\n        stage1_out_channel = block.expansion * num_channels\n\n        self.stage2_cfg = cfg.MODEL.HRNET.STAGE2\n        num_channels = self.stage2_cfg[\'NUM_CHANNELS\']\n        block = blocks_dict[self.stage2_cfg[\'BLOCK\']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition1 = self._make_transition_layer(\n            [stage1_out_channel], num_channels, norm_layer=norm_layer)\n        self.stage2, pre_stage_channels = self._make_stage(\n            self.stage2_cfg, num_channels)\n\n        self.stage3_cfg = cfg.MODEL.HRNET.STAGE3\n        num_channels = self.stage3_cfg[\'NUM_CHANNELS\']\n        block = blocks_dict[self.stage3_cfg[\'BLOCK\']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition2 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage3, pre_stage_channels = self._make_stage(\n            self.stage3_cfg, num_channels)\n\n        self.stage4_cfg = cfg.MODEL.HRNET.STAGE4\n        num_channels = self.stage4_cfg[\'NUM_CHANNELS\']\n        block = blocks_dict[self.stage4_cfg[\'BLOCK\']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition3 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage4, pre_stage_channels = self._make_stage(\n            self.stage4_cfg, num_channels, multi_scale_output=True)\n\n        self.last_inp_channels = np.int(np.sum(pre_stage_channels))\n        # Classification Head\n        # self.incre_modules, self.downsamp_modules, \\\n        # self.final_layer = self._make_head(pre_stage_channels)\n        #\n        # self.classifier = nn.Linear(2048, 1000)\n\n    def _make_head(self, pre_stage_channels):\n        head_block = Bottleneck\n        head_channels = [32, 64, 128, 256]\n\n        # Increasing the #channels on each resolution\n        # from C, 2C, 4C, 8C to 128, 256, 512, 1024\n        incre_modules = []\n        for i, channels in enumerate(pre_stage_channels):\n            incre_module = self._make_layer(head_block,\n                                            channels,\n                                            head_channels[i],\n                                            1,\n                                            stride=1)\n            incre_modules.append(incre_module)\n        incre_modules = nn.ModuleList(incre_modules)\n\n        # downsampling modules\n        downsamp_modules = []\n        for i in range(len(pre_stage_channels) - 1):\n            in_channels = head_channels[i] * head_block.expansion\n            out_channels = head_channels[i + 1] * head_block.expansion\n\n            downsamp_module = nn.Sequential(\n                nn.Conv2d(in_channels=in_channels,\n                          out_channels=out_channels,\n                          kernel_size=3,\n                          stride=2,\n                          padding=1),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True)\n            )\n\n            downsamp_modules.append(downsamp_module)\n        downsamp_modules = nn.ModuleList(downsamp_modules)\n\n        final_layer = nn.Sequential(\n            nn.Conv2d(\n                in_channels=head_channels[3] * head_block.expansion,\n                out_channels=2048,\n                kernel_size=1,\n                stride=1,\n                padding=0\n            ),\n            nn.BatchNorm2d(2048),\n            nn.ReLU(inplace=True)\n        )\n\n        return incre_modules, downsamp_modules, final_layer\n\n    def _make_transition_layer(self, num_channels_pre_layer, num_channels_cur_layer,\n                               norm_layer=nn.BatchNorm2d):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(nn.Sequential(\n                        nn.Conv2d(num_channels_pre_layer[i],\n                                  num_channels_cur_layer[i],\n                                  3,\n                                  1,\n                                  1,\n                                  bias=False),\n                        norm_layer(\n                            num_channels_cur_layer[i]),\n                        nn.ReLU(inplace=True)))\n                else:\n                    transition_layers.append(None)\n            else:\n                conv3x3s = []\n                for j in range(i + 1 - num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = num_channels_cur_layer[i] \\\n                        if j == i - num_branches_pre else inchannels\n                    conv3x3s.append(nn.Sequential(\n                        nn.Conv2d(\n                            inchannels, outchannels, 3, 2, 1, bias=False),\n                        norm_layer(outchannels),\n                        nn.ReLU(inplace=True)))\n                transition_layers.append(nn.Sequential(*conv3x3s))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block, inplanes, planes, blocks, stride=1, norm_layer=nn.BatchNorm2d):\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(inplanes, planes, stride, downsample))\n        inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, layer_config, num_inchannels,\n                    multi_scale_output=True):\n        num_modules = layer_config[\'NUM_MODULES\']\n        num_branches = layer_config[\'NUM_BRANCHES\']\n        num_blocks = layer_config[\'NUM_BLOCKS\']\n        num_channels = layer_config[\'NUM_CHANNELS\']\n        block = blocks_dict[layer_config[\'BLOCK\']]\n        fuse_method = layer_config[\'FUSE_METHOD\']\n\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n\n            modules.append(\n                HighResolutionModule(num_branches,\n                                     block,\n                                     num_blocks,\n                                     num_inchannels,\n                                     num_channels,\n                                     fuse_method,\n                                     reset_multi_scale_output)\n            )\n            num_inchannels = modules[-1].get_num_inchannels()\n\n        return nn.Sequential(*modules), num_inchannels\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n\n        x_list = []\n        for i in range(self.stage2_cfg[\'NUM_BRANCHES\']):\n            if self.transition1[i] is not None:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.stage3_cfg[\'NUM_BRANCHES\']):\n            if self.transition2[i] is not None:\n                x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.stage4_cfg[\'NUM_BRANCHES\']):\n            if self.transition3[i] is not None:\n                x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage4(x_list)\n\n        # Classification Head\n        # y = self.incre_modules[0](y_list[0])\n        # for i in range(len(self.downsamp_modules)):\n        #     y = self.incre_modules[i + 1](y_list[i + 1]) + \\\n        #         self.downsamp_modules[i](y)\n        #\n        # y = self.final_layer(y)\n        #\n        # if torch._C._get_tracing_state():\n        #     y = y.flatten(start_dim=2).mean(dim=2)\n        # else:\n        #     y = F.avg_pool2d(y, kernel_size=y.size()\n        #     [2:]).view(y.size(0), -1)\n        #\n        # y = self.classifier(y)\n\n        return tuple(y_list)\n\n    def init_weights(self, pretrained=\'\', ):\n        logging.info(\'=> init weights from normal distribution\')\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(\n                    m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        if os.path.isfile(pretrained):\n            pretrained_dict = torch.load(pretrained)\n            logging.info(\'=> loading pretrained model {}\'.format(pretrained))\n            model_dict = self.state_dict()\n            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n                               if k in model_dict.keys()}\n            for k, _ in pretrained_dict.items():\n                logging.info(\n                    \'=> loading {} pretrained model {}\'.format(k, pretrained))\n            model_dict.update(pretrained_dict)\n            self.load_state_dict(model_dict)\n\n\n@BACKBONE_REGISTRY.register()\ndef hrnet_w18_small_v1(norm_layer=nn.BatchNorm2d):\n    return HighResolutionNet(norm_layer=norm_layer)\n'"
segmentron/models/backbones/mobilenet.py,1,"b'""""""MobileNet and MobileNetV2.""""""\nimport torch.nn as nn\n\nfrom .build import BACKBONE_REGISTRY\nfrom ...modules import _ConvBNReLU, _DepthwiseConv, InvertedResidual\nfrom ...config import cfg\n\n__all__ = [\'MobileNet\', \'MobileNetV2\']\n\n\nclass MobileNet(nn.Module):\n    def __init__(self, num_classes=1000, norm_layer=nn.BatchNorm2d):\n        super(MobileNet, self).__init__()\n        multiplier = cfg.MODEL.BACKBONE_SCALE\n        conv_dw_setting = [\n            [64, 1, 1],\n            [128, 2, 2],\n            [256, 2, 2],\n            [512, 6, 2],\n            [1024, 2, 2]]\n        input_channels = int(32 * multiplier) if multiplier > 1.0 else 32\n        features = [_ConvBNReLU(3, input_channels, 3, 2, 1, norm_layer=norm_layer)]\n\n        for c, n, s in conv_dw_setting:\n            out_channels = int(c * multiplier)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(_DepthwiseConv(input_channels, out_channels, stride, norm_layer))\n                input_channels = out_channels\n        self.last_inp_channels = int(1024 * multiplier)\n        features.append(nn.AdaptiveAvgPool2d(1))\n        self.features = nn.Sequential(*features)\n\n        self.classifier = nn.Linear(int(1024 * multiplier), num_classes)\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x.view(x.size(0), x.size(1)))\n        return x\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, num_classes=1000, norm_layer=nn.BatchNorm2d):\n        super(MobileNetV2, self).__init__()\n        output_stride = cfg.MODEL.OUTPUT_STRIDE\n        self.multiplier = cfg.MODEL.BACKBONE_SCALE\n        if output_stride == 32:\n            dilations = [1, 1]\n        elif output_stride == 16:\n            dilations = [1, 2]\n        elif output_stride == 8:\n            dilations = [2, 4]\n        else:\n            raise NotImplementedError\n        inverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1]]\n        # building first layer\n        input_channels = int(32 * self.multiplier) if self.multiplier > 1.0 else 32\n        # last_channels = int(1280 * multiplier) if multiplier > 1.0 else 1280\n        self.conv1 = _ConvBNReLU(3, input_channels, 3, 2, 1, relu6=True, norm_layer=norm_layer)\n\n        # building inverted residual blocks\n        self.planes = input_channels\n        self.block1 = self._make_layer(InvertedResidual, self.planes, inverted_residual_setting[0:1],\n                                       norm_layer=norm_layer)\n        self.block2 = self._make_layer(InvertedResidual, self.planes, inverted_residual_setting[1:2],\n                                       norm_layer=norm_layer)\n        self.block3 = self._make_layer(InvertedResidual, self.planes, inverted_residual_setting[2:3],\n                                       norm_layer=norm_layer)\n        self.block4 = self._make_layer(InvertedResidual, self.planes, inverted_residual_setting[3:5],\n                                       dilations[0], norm_layer=norm_layer)\n        self.block5 = self._make_layer(InvertedResidual, self.planes, inverted_residual_setting[5:],\n                                       dilations[1], norm_layer=norm_layer)\n        self.last_inp_channels = self.planes\n\n        # building last several layers\n        # features = list()\n        # features.append(_ConvBNReLU(input_channels, last_channels, 1, relu6=True, norm_layer=norm_layer))\n        # features.append(nn.AdaptiveAvgPool2d(1))\n        # self.features = nn.Sequential(*features)\n        #\n        # self.classifier = nn.Sequential(\n        #     nn.Dropout2d(0.2),\n        #     nn.Linear(last_channels, num_classes))\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n    def _make_layer(self, block, planes, inverted_residual_setting, dilation=1, norm_layer=nn.BatchNorm2d):\n        features = list()\n        for t, c, n, s in inverted_residual_setting:\n            out_channels = int(c * self.multiplier)\n            stride = s if dilation == 1 else 1\n            features.append(block(planes, out_channels, stride, t, dilation, norm_layer))\n            planes = out_channels\n            for i in range(n - 1):\n                features.append(block(planes, out_channels, 1, t, norm_layer=norm_layer))\n                planes = out_channels\n        self.planes = planes\n        return nn.Sequential(*features)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.block1(x)\n        c1 = self.block2(x)\n        c2 = self.block3(c1)\n        c3 = self.block4(c2)\n        c4 = self.block5(c3)\n\n        # x = self.features(x)\n        # x = self.classifier(x.view(x.size(0), x.size(1)))\n        return c1, c2, c3, c4\n\n\n@BACKBONE_REGISTRY.register()\ndef mobilenet_v1(norm_layer=nn.BatchNorm2d):\n    return MobileNet(norm_layer=norm_layer)\n\n\n@BACKBONE_REGISTRY.register()\ndef mobilenet_v2(norm_layer=nn.BatchNorm2d):\n    return MobileNetV2(norm_layer=norm_layer)\n\n'"
segmentron/models/backbones/resnet.py,1,"b'import torch.nn as nn\n\nfrom .build import BACKBONE_REGISTRY\nfrom ...config import cfg\n\n__all__ = [\'ResNetV1\']\n\n\nclass BasicBlockV1b(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None,\n                 previous_dilation=1, norm_layer=nn.BatchNorm2d):\n        super(BasicBlockV1b, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, 3, stride,\n                               dilation, dilation, bias=False)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(True)\n        self.conv2 = nn.Conv2d(planes, planes, 3, 1, previous_dilation,\n                               dilation=previous_dilation, bias=False)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass BottleneckV1b(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None,\n                 previous_dilation=1, norm_layer=nn.BatchNorm2d):\n        super(BottleneckV1b, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = norm_layer(planes)\n        self.conv2 = nn.Conv2d(planes, planes, 3, stride,\n                               dilation, dilation, bias=False)\n        self.bn2 = norm_layer(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNetV1(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, deep_stem=False,\n                 zero_init_residual=False, norm_layer=nn.BatchNorm2d):\n        output_stride = cfg.MODEL.OUTPUT_STRIDE\n        scale = cfg.MODEL.BACKBONE_SCALE\n        if output_stride == 32:\n            dilations = [1, 1]\n            strides = [2, 2]\n        elif output_stride == 16:\n            dilations = [1, 2]\n            strides = [2, 1]\n        elif output_stride == 8:\n            dilations = [2, 4]\n            strides = [1, 1]\n        else:\n            raise NotImplementedError\n        self.inplanes = int((128 if deep_stem else 64) * scale)\n        super(ResNetV1, self).__init__()\n        if deep_stem:\n            # resnet vc\n            mid_channel = int(64 * scale)\n            self.conv1 = nn.Sequential(\n                nn.Conv2d(3, mid_channel, 3, 2, 1, bias=False),\n                norm_layer(mid_channel),\n                nn.ReLU(True),\n                nn.Conv2d(mid_channel, mid_channel, 3, 1, 1, bias=False),\n                norm_layer(mid_channel),\n                nn.ReLU(True),\n                nn.Conv2d(mid_channel, self.inplanes, 3, 1, 1, bias=False)\n            )\n        else:\n            self.conv1 = nn.Conv2d(3, self.inplanes, 7, 2, 3, bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(True)\n        self.maxpool = nn.MaxPool2d(3, 2, 1)\n        self.layer1 = self._make_layer(block, int(64 * scale), layers[0], norm_layer=norm_layer)\n        self.layer2 = self._make_layer(block, int(128 * scale), layers[1], stride=2, norm_layer=norm_layer)\n\n        self.layer3 = self._make_layer(block, int(256 * scale), layers[2], stride=strides[0], dilation=dilations[0],\n                                       norm_layer=norm_layer)\n        self.layer4 = self._make_layer(block, int(512 * scale), layers[3], stride=strides[1], dilation=dilations[1],\n                                       norm_layer=norm_layer, multi_grid=cfg.MODEL.DANET.MULTI_GRID,\n                                       multi_dilation=cfg.MODEL.DANET.MULTI_DILATION)\n\n        self.last_inp_channels = int(512 * block.expansion * scale)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(int(512 * block.expansion * scale), num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, BottleneckV1b):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlockV1b):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=nn.BatchNorm2d,\n                    multi_grid=False, multi_dilation=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, 1, stride, bias=False),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        if not multi_grid:\n            if dilation in (1, 2):\n                layers.append(block(self.inplanes, planes, stride, dilation=1, downsample=downsample,\n                                    previous_dilation=dilation, norm_layer=norm_layer))\n            elif dilation == 4:\n                layers.append(block(self.inplanes, planes, stride, dilation=2, downsample=downsample,\n                                    previous_dilation=dilation, norm_layer=norm_layer))\n            else:\n                raise RuntimeError(""=> unknown dilation size: {}"".format(dilation))\n        else:\n            layers.append(block(self.inplanes, planes, stride, dilation=multi_dilation[0],\n                                downsample=downsample, previous_dilation=dilation, norm_layer=norm_layer))\n        self.inplanes = planes * block.expansion\n\n        if multi_grid:\n            div = len(multi_dilation)\n            for i in range(1, blocks):\n                layers.append(block(self.inplanes, planes, dilation=multi_dilation[i % div],\n                                    previous_dilation=dilation, norm_layer=norm_layer))\n        else:\n            for _ in range(1, blocks):\n                layers.append(block(self.inplanes, planes, dilation=dilation,\n                                    previous_dilation=dilation, norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        c1 = self.layer1(x)\n        c2 = self.layer2(c1)\n        c3 = self.layer3(c2)\n        c4 = self.layer4(c3)\n\n        # for classification\n        # x = self.avgpool(c4)\n        # x = x.view(x.size(0), -1)\n        # x = self.fc(x)\n\n        return c1, c2, c3, c4\n\n\n@BACKBONE_REGISTRY.register()\ndef resnet18(norm_layer=nn.BatchNorm2d):\n    num_block = [2, 2, 2, 2]\n    return ResNetV1(BasicBlockV1b, num_block, norm_layer=norm_layer)\n\n\n@BACKBONE_REGISTRY.register()\ndef resnet34(norm_layer=nn.BatchNorm2d):\n    num_block = [3, 4, 6, 3]\n    return ResNetV1(BasicBlockV1b, num_block, norm_layer=norm_layer)\n\n\n@BACKBONE_REGISTRY.register()\ndef resnet50(norm_layer=nn.BatchNorm2d):\n    num_block = [3, 4, 6, 3]\n    return ResNetV1(BottleneckV1b, num_block, norm_layer=norm_layer)\n\n\n@BACKBONE_REGISTRY.register()\ndef resnet101(norm_layer=nn.BatchNorm2d):\n    num_block = [3, 4, 23, 3]\n    return ResNetV1(BottleneckV1b, num_block, norm_layer=norm_layer)\n\n\n@BACKBONE_REGISTRY.register()\ndef resnet152(norm_layer=nn.BatchNorm2d):\n    num_block = [3, 8, 36, 3]\n    return ResNetV1(BottleneckV1b, num_block, norm_layer=norm_layer)\n\n\n@BACKBONE_REGISTRY.register()\ndef resnet50c(norm_layer=nn.BatchNorm2d):\n    num_block = [3, 4, 6, 3]\n    return ResNetV1(BottleneckV1b, num_block, norm_layer=norm_layer, deep_stem=True)\n\n\n@BACKBONE_REGISTRY.register()\ndef resnet101c(norm_layer=nn.BatchNorm2d):\n    num_block = [3, 4, 23, 3]\n    return ResNetV1(BottleneckV1b, num_block, norm_layer=norm_layer, deep_stem=True)\n\n\n@BACKBONE_REGISTRY.register()\ndef resnet152c(norm_layer=nn.BatchNorm2d):\n    num_block = [3, 8, 36, 3]\n    return ResNetV1(BottleneckV1b, num_block, norm_layer=norm_layer, deep_stem=True)\n\n'"
segmentron/models/backbones/xception.py,1,"b""import torch.nn as nn\n\nfrom ...modules import SeparableConv2d\nfrom .build import BACKBONE_REGISTRY\nfrom ...config import cfg\n\n__all__ = ['Xception65', 'Enc', 'FCAttention']\n\n\nclass XceptionBlock(nn.Module):\n    def __init__(self, channel_list, stride=1, dilation=1, skip_connection_type='conv', relu_first=True,\n                 low_feat=False, norm_layer=nn.BatchNorm2d):\n        super().__init__()\n\n        assert len(channel_list) == 4\n        self.skip_connection_type = skip_connection_type\n        self.relu_first = relu_first\n        self.low_feat = low_feat\n\n        if self.skip_connection_type == 'conv':\n            self.conv = nn.Conv2d(channel_list[0], channel_list[-1], 1, stride=stride, bias=False)\n            self.bn = norm_layer(channel_list[-1])\n\n        self.sep_conv1 = SeparableConv2d(channel_list[0], channel_list[1], dilation=dilation,\n                                         relu_first=relu_first, norm_layer=norm_layer)\n        self.sep_conv2 = SeparableConv2d(channel_list[1], channel_list[2], dilation=dilation,\n                                         relu_first=relu_first, norm_layer=norm_layer)\n        self.sep_conv3 = SeparableConv2d(channel_list[2], channel_list[3], dilation=dilation,\n                                         relu_first=relu_first, stride=stride, norm_layer=norm_layer)\n        self.last_inp_channels = channel_list[3]\n\n    def forward(self, inputs):\n        sc1 = self.sep_conv1(inputs)\n        sc2 = self.sep_conv2(sc1)\n        residual = self.sep_conv3(sc2)\n\n        if self.skip_connection_type == 'conv':\n            shortcut = self.conv(inputs)\n            shortcut = self.bn(shortcut)\n            outputs = residual + shortcut\n        elif self.skip_connection_type == 'sum':\n            outputs = residual + inputs\n        elif self.skip_connection_type == 'none':\n            outputs = residual\n        else:\n            raise ValueError('Unsupported skip connection type.')\n\n        if self.low_feat:\n            return outputs, sc2\n        else:\n            return outputs\n\n\nclass Xception65(nn.Module):\n    def __init__(self, norm_layer=nn.BatchNorm2d):\n        super().__init__()\n        output_stride = cfg.MODEL.OUTPUT_STRIDE\n        if output_stride == 32:\n            entry_block3_stride = 2\n            middle_block_dilation = 1\n            exit_block_dilations = (1, 1)\n            exit_block_stride = 2\n        elif output_stride == 16:\n            entry_block3_stride = 2\n            middle_block_dilation = 1\n            exit_block_dilations = (1, 2)\n            exit_block_stride = 1\n        elif output_stride == 8:\n            entry_block3_stride = 1\n            middle_block_dilation = 2\n            exit_block_dilations = (2, 4)\n            exit_block_stride = 1\n        else:\n            raise NotImplementedError\n\n        # Entry flow\n        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1, bias=False)\n        self.bn1 = norm_layer(32)\n        self.relu = nn.ReLU()\n\n        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1, bias=False)\n        self.bn2 = norm_layer(64)\n\n        self.block1 = XceptionBlock([64, 128, 128, 128], stride=2, norm_layer=norm_layer)\n        self.block2 = XceptionBlock([128, 256, 256, 256], stride=2, low_feat=True, norm_layer=norm_layer)\n        self.block3 = XceptionBlock([256, 728, 728, 728], stride=entry_block3_stride, low_feat=True,\n                                    norm_layer=norm_layer)\n\n        # Middle flow (16 units)\n        self.block4 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation,\n                                    skip_connection_type='sum', norm_layer=norm_layer)\n        self.block5 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation,\n                                    skip_connection_type='sum', norm_layer=norm_layer)\n        self.block6 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation,\n                                    skip_connection_type='sum', norm_layer=norm_layer)\n        self.block7 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation,\n                                    skip_connection_type='sum', norm_layer=norm_layer)\n        self.block8 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation,\n                                    skip_connection_type='sum', norm_layer=norm_layer)\n        self.block9 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation,\n                                    skip_connection_type='sum', norm_layer=norm_layer)\n        self.block10 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation,\n                                     skip_connection_type='sum', norm_layer=norm_layer)\n        self.block11 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation,\n                                     skip_connection_type='sum', norm_layer=norm_layer)\n        self.block12 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation,\n                                     skip_connection_type='sum', norm_layer=norm_layer)\n        self.block13 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation,\n                                     skip_connection_type='sum', norm_layer=norm_layer)\n        self.block14 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation,\n                                     skip_connection_type='sum', norm_layer=norm_layer)\n        self.block15 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation,\n                                     skip_connection_type='sum', norm_layer=norm_layer)\n        self.block16 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation,\n                                     skip_connection_type='sum', norm_layer=norm_layer)\n        self.block17 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation,\n                                     skip_connection_type='sum', norm_layer=norm_layer)\n        self.block18 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation,\n                                     skip_connection_type='sum', norm_layer=norm_layer)\n        self.block19 = XceptionBlock([728, 728, 728, 728], dilation=middle_block_dilation,\n                                     skip_connection_type='sum', norm_layer=norm_layer)\n\n        # Exit flow\n        self.block20 = XceptionBlock([728, 728, 1024, 1024], stride=exit_block_stride,\n                                     dilation=exit_block_dilations[0], norm_layer=norm_layer)\n        self.block21 = XceptionBlock([1024, 1536, 1536, 2048], dilation=exit_block_dilations[1],\n                                     skip_connection_type='none', relu_first=False, norm_layer=norm_layer)\n\n    def forward(self, x):\n        # Entry flow\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.block1(x)\n        x, c1 = self.block2(x)  # b, h//4, w//4, 256\n        x, c2 = self.block3(x)  # b, h//8, w//8, 728\n\n        # Middle flow\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        x = self.block7(x)\n        x = self.block8(x)\n        x = self.block9(x)\n        x = self.block10(x)\n        x = self.block11(x)\n        x = self.block12(x)\n        x = self.block13(x)\n        x = self.block14(x)\n        x = self.block15(x)\n        x = self.block16(x)\n        x = self.block17(x)\n        x = self.block18(x)\n        c3 = self.block19(x)\n\n        # Exit flow\n        x = self.block20(c3)\n        c4 = self.block21(x)\n\n        return c1, c2, c3, c4\n\n\n# -------------------------------------------------\n#                   For DFANet\n# -------------------------------------------------\nclass BlockA(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, dilation=1, norm_layer=None, start_with_relu=True):\n        super(BlockA, self).__init__()\n        if out_channels != in_channels or stride != 1:\n            self.skip = nn.Conv2d(in_channels, out_channels, 1, stride, bias=False)\n            self.skipbn = norm_layer(out_channels)\n        else:\n            self.skip = None\n        self.relu = nn.ReLU()\n        rep = list()\n        inter_channels = out_channels // 4\n\n        if start_with_relu:\n            rep.append(self.relu)\n        rep.append(SeparableConv2d(in_channels, inter_channels, 3, 1, dilation, norm_layer=norm_layer))\n        rep.append(norm_layer(inter_channels))\n\n        rep.append(self.relu)\n        rep.append(SeparableConv2d(inter_channels, inter_channels, 3, 1, dilation, norm_layer=norm_layer))\n        rep.append(norm_layer(inter_channels))\n\n        if stride != 1:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(inter_channels, out_channels, 3, stride, norm_layer=norm_layer))\n            rep.append(norm_layer(out_channels))\n        else:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(inter_channels, out_channels, 3, 1, norm_layer=norm_layer))\n            rep.append(norm_layer(out_channels))\n        self.rep = nn.Sequential(*rep)\n\n    def forward(self, x):\n        out = self.rep(x)\n        if self.skip is not None:\n            skip = self.skipbn(self.skip(x))\n        else:\n            skip = x\n        out = out + skip\n        return out\n\n\nclass Enc(nn.Module):\n    def __init__(self, in_channels, out_channels, blocks, norm_layer=nn.BatchNorm2d):\n        super(Enc, self).__init__()\n        block = list()\n        block.append(BlockA(in_channels, out_channels, 2, norm_layer=norm_layer))\n        for i in range(blocks - 1):\n            block.append(BlockA(out_channels, out_channels, 1, norm_layer=norm_layer))\n        self.block = nn.Sequential(*block)\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass FCAttention(nn.Module):\n    def __init__(self, in_channels, norm_layer=nn.BatchNorm2d):\n        super(FCAttention, self).__init__()\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(in_channels, 1000)\n        self.conv = nn.Sequential(\n            nn.Conv2d(1000, in_channels, 1, bias=False),\n            norm_layer(in_channels),\n            nn.ReLU(True))\n\n    def forward(self, x):\n        n, c, _, _ = x.size()\n        att = self.avgpool(x).view(n, c)\n        att = self.fc(att).view(n, 1000, 1, 1)\n        att = self.conv(att)\n        return x * att.expand_as(x)\n\n\nclass XceptionA(nn.Module):\n    def __init__(self, num_classes=1000, norm_layer=nn.BatchNorm2d):\n        super(XceptionA, self).__init__()\n        self.conv1 = nn.Sequential(nn.Conv2d(3, 8, 3, 2, 1, bias=False),\n                                   norm_layer(8),\n                                   nn.ReLU(True))\n\n        self.enc2 = Enc(8, 48, 4, norm_layer=norm_layer)\n        self.enc3 = Enc(48, 96, 6, norm_layer=norm_layer)\n        self.enc4 = Enc(96, 192, 4, norm_layer=norm_layer)\n\n        self.fca = FCAttention(192, norm_layer=norm_layer)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(192, num_classes)\n\n    def forward(self, x):\n        x = self.conv1(x)\n\n        x = self.enc2(x)\n        x = self.enc3(x)\n        x = self.enc4(x)\n        x = self.fca(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\n@BACKBONE_REGISTRY.register()\ndef xception_a(norm_layer=nn.BatchNorm2d):\n    model = XceptionA(norm_layer=norm_layer)\n    return model\n\n\n@BACKBONE_REGISTRY.register()\ndef xception65(norm_layer=nn.BatchNorm2d):\n    model = Xception65(norm_layer=norm_layer)\n    return model\n\n"""
segmentron/modules/sync_bn/syncbn.py,3,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n## Created by: Hang Zhang\n## ECE Department, Rutgers University\n## Email: zhang.hang@rutgers.edu\n## Copyright (c) 2017\n##\n## This source code is licensed under the MIT-style license found in the\n## LICENSE file in the root directory of this source tree\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n""""""Synchronized Cross-GPU Batch Normalization Module""""""\nimport warnings\nimport torch\n\nfrom torch.nn.modules.batchnorm import _BatchNorm\nfrom queue import Queue\nfrom .functions import *\n\n__all__ = [\'SyncBatchNorm\', \'BatchNorm1d\', \'BatchNorm2d\', \'BatchNorm3d\']\n\n\n# Adopt from https://github.com/zhanghang1989/PyTorch-Encoding/blob/master/encoding/nn/syncbn.py\nclass SyncBatchNorm(_BatchNorm):\n    """"""Cross-GPU Synchronized Batch normalization (SyncBN)\n\n    Parameters:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        sync: a boolean value that when set to ``True``, synchronize across\n            different gpus. Default: ``True``\n        activation : str\n            Name of the activation functions, one of: `leaky_relu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n\n    Shape:\n        - Input: :math:`(N, C, H, W)`\n        - Output: :math:`(N, C, H, W)` (same shape as input)\n    Reference:\n        .. [1] Ioffe, Sergey, and Christian Szegedy. ""Batch normalization: Accelerating deep network training by reducing internal covariate shift."" *ICML 2015*\n        .. [2] Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, and Amit Agrawal. ""Context Encoding for Semantic Segmentation."" *CVPR 2018*\n    Examples:\n        >>> m = SyncBatchNorm(100)\n        >>> net = torch.nn.DataParallel(m)\n        >>> output = net(input)\n    """"""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, sync=True, activation=\'none\', slope=0.01, inplace=True):\n        super(SyncBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=True)\n        self.activation = activation\n        self.inplace = False if activation == \'none\' else inplace\n        self.slope = slope\n        self.devices = list(range(torch.cuda.device_count()))\n        self.sync = sync if len(self.devices) > 1 else False\n        # Initialize queues\n        self.worker_ids = self.devices[1:]\n        self.master_queue = Queue(len(self.worker_ids))\n        self.worker_queues = [Queue(1) for _ in self.worker_ids]\n\n    def forward(self, x):\n        # resize the input to (B, C, -1)\n        input_shape = x.size()\n        x = x.view(input_shape[0], self.num_features, -1)\n        if x.get_device() == self.devices[0]:\n            # Master mode\n            extra = {\n                ""is_master"": True,\n                ""master_queue"": self.master_queue,\n                ""worker_queues"": self.worker_queues,\n                ""worker_ids"": self.worker_ids\n            }\n        else:\n            # Worker mode\n            extra = {\n                ""is_master"": False,\n                ""master_queue"": self.master_queue,\n                ""worker_queue"": self.worker_queues[self.worker_ids.index(x.get_device())]\n            }\n        if self.inplace:\n            return inp_syncbatchnorm(x, self.weight, self.bias, self.running_mean, self.running_var,\n                                     extra, self.sync, self.training, self.momentum, self.eps,\n                                     self.activation, self.slope).view(input_shape)\n        else:\n            return syncbatchnorm(x, self.weight, self.bias, self.running_mean, self.running_var,\n                                 extra, self.sync, self.training, self.momentum, self.eps,\n                                 self.activation, self.slope).view(input_shape)\n\n    def extra_repr(self):\n        if self.activation == \'none\':\n            return \'sync={}\'.format(self.sync)\n        else:\n            return \'sync={}, act={}, slope={}, inplace={}\'.format(\n                self.sync, self.activation, self.slope, self.inplace)\n\n\nclass BatchNorm1d(SyncBatchNorm):\n    """"""BatchNorm1d is deprecated in favor of :class:`core.nn.sync_bn.SyncBatchNorm`.""""""\n\n    def __init__(self, *args, **kwargs):\n        warnings.warn(""core.nn.sync_bn.{} is now deprecated in favor of core.nn.sync_bn.{}.""\n                      .format(\'BatchNorm1d\', SyncBatchNorm.__name__), DeprecationWarning)\n        super(BatchNorm1d, self).__init__(*args, **kwargs)\n\n\nclass BatchNorm2d(SyncBatchNorm):\n    """"""BatchNorm1d is deprecated in favor of :class:`core.nn.sync_bn.SyncBatchNorm`.""""""\n\n    def __init__(self, *args, **kwargs):\n        warnings.warn(""core.nn.sync_bn.{} is now deprecated in favor of core.nn.sync_bn.{}.""\n                      .format(\'BatchNorm2d\', SyncBatchNorm.__name__), DeprecationWarning)\n        super(BatchNorm2d, self).__init__(*args, **kwargs)\n\n\nclass BatchNorm3d(SyncBatchNorm):\n    """"""BatchNorm1d is deprecated in favor of :class:`core.nn.sync_bn.SyncBatchNorm`.""""""\n\n    def __init__(self, *args, **kwargs):\n        warnings.warn(""core.nn.sync_bn.{} is now deprecated in favor of core.nn.sync_bn.{}.""\n                      .format(\'BatchNorm3d\', SyncBatchNorm.__name__), DeprecationWarning)\n        super(BatchNorm3d, self).__init__(*args, **kwargs)\n'"
