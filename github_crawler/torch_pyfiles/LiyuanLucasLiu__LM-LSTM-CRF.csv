file_path,api_count,code
eval_w.py,7,"b'\r\nfrom __future__ import print_function\r\nimport datetime\r\nimport time\r\nimport torch\r\nimport torch.autograd as autograd\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport codecs\r\nfrom model.crf import *\r\nfrom model.lstm_crf import *\r\nimport model.utils as utils\r\nfrom model.evaluator import eval_w\r\n\r\nimport argparse\r\nimport json\r\nimport os\r\nimport sys\r\nfrom tqdm import tqdm\r\nimport itertools\r\nimport functools\r\n\r\nif __name__ == ""__main__"":\r\n    parser = argparse.ArgumentParser(description=\'Evaluating BLSTM-CRF\')\r\n    parser.add_argument(\'--load_arg\', default=\'./checkpoint/soa/check_wc_p_char_lstm_crf.json\', help=\'arg json file path\')\r\n    parser.add_argument(\'--load_check_point\', default=\'./checkpoint/soa/check_wc_p_char_lstm_crf.model\', help=\'checkpoint path\')\r\n    parser.add_argument(\'--gpu\',type=int, default=0, help=\'gpu id\')\r\n    parser.add_argument(\'--eva_matrix\', choices=[\'a\', \'fa\'], default=\'fa\', help=\'use f1 and accuracy or accuracy alone\')\r\n    parser.add_argument(\'--test_file\', default=\'\', help=\'path to test file, if set to none, would use test_file path in the checkpoint file\')\r\n    args = parser.parse_args()\r\n\r\n    with open(args.load_arg, \'r\') as f:\r\n        jd = json.load(f)\r\n    jd = jd[\'args\']\r\n\r\n    checkpoint_file = torch.load(args.load_check_point, map_location=lambda storage, loc: storage)\r\n    f_map = checkpoint_file[\'f_map\']\r\n    l_map = checkpoint_file[\'l_map\']\r\n    if args.gpu >= 0:\r\n        torch.cuda.set_device(args.gpu)\r\n\r\n\r\n    # load corpus\r\n\r\n    if args.test_file:\r\n        with codecs.open(args.test_file, \'r\', \'utf-8\') as f:\r\n            test_lines = f.readlines()\r\n    else:\r\n        with codecs.open(jd[\'test_file\'], \'r\', \'utf-8\') as f:\r\n            test_lines = f.readlines()\r\n\r\n    # converting format\r\n\r\n    test_features, test_labels = utils.read_corpus(test_lines)\r\n\r\n    # construct dataset\r\n    test_dataset = utils.construct_bucket_mean_vb(test_features, test_labels, f_map, l_map, jd[\'caseless\'])\r\n    \r\n    test_dataset_loader = [torch.utils.data.DataLoader(tup, 50, shuffle=False, drop_last=False) for tup in test_dataset]\r\n\r\n    # build model\r\n    ner_model = LSTM_CRF(len(f_map), len(l_map), jd[\'embedding_dim\'], jd[\'hidden\'], jd[\'layers\'], jd[\'drop_out\'], large_CRF=jd[\'small_crf\'])\r\n\r\n    ner_model.load_state_dict(checkpoint_file[\'state_dict\'])\r\n\r\n    if args.gpu >= 0:\r\n        if_cuda = True\r\n        torch.cuda.set_device(args.gpu)\r\n        ner_model.cuda()\r\n        packer = CRFRepack(len(l_map), True)\r\n    else:\r\n        if_cuda = False\r\n        packer = CRFRepack(len(l_map), False)\r\n\r\n    evaluator = eval_w(packer, l_map, args.eva_matrix)\r\n\r\n    if \'f\' in args.eva_matrix:\r\n\r\n        test_f1, test_pre, test_rec, test_acc = evaluator.calc_score(ner_model, test_dataset_loader)\r\n\r\n        print(jd[\'checkpoint\'] + \' test_f1: %.4f test_rec: %.4f test_pre: %.4f test_acc: %.4f\\n\' % (test_f1, test_rec, test_pre, test_acc))\r\n\r\n    else:\r\n\r\n        test_acc = evaluator.calc_score(ner_model, test_dataset_loader)\r\n\r\n        print(jd[\'checkpoint\'] + \' test_acc: %.4f\\n\' % (test_acc))\r\n'"
eval_wc.py,7,"b'\r\nfrom __future__ import print_function\r\nimport datetime\r\nimport time\r\nimport torch\r\nimport torch.autograd as autograd\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport codecs\r\nfrom model.crf import *\r\nfrom model.lm_lstm_crf import *\r\nimport model.utils as utils\r\nfrom model.evaluator import eval_wc\r\n\r\nimport argparse\r\nimport json\r\nimport os\r\nimport sys\r\nfrom tqdm import tqdm\r\nimport itertools\r\nimport functools\r\n\r\nif __name__ == ""__main__"":\r\n    parser = argparse.ArgumentParser(description=\'Evaluating LM-BLSTM-CRF\')\r\n    parser.add_argument(\'--load_arg\', default=\'./checkpoint/soa/check_wc_p_char_lstm_crf.json\', help=\'path to arg json\')\r\n    parser.add_argument(\'--load_check_point\', default=\'./checkpoint/soa/check_wc_p_char_lstm_crf.model\', help=\'path to model checkpoint file\')\r\n    parser.add_argument(\'--gpu\',type=int, default=0, help=\'gpu id\')\r\n    parser.add_argument(\'--eva_matrix\', choices=[\'a\', \'fa\'], default=\'fa\', help=\'use f1 and accuracy or f1 alone\')\r\n    parser.add_argument(\'--test_file\', default=\'\', help=\'path to test file, if set to none, would use test_file path in the checkpoint file\')\r\n    args = parser.parse_args()\r\n\r\n    with open(args.load_arg, \'r\') as f:\r\n        jd = json.load(f)\r\n    jd = jd[\'args\']\r\n\r\n    checkpoint_file = torch.load(args.load_check_point, map_location=lambda storage, loc: storage)\r\n    f_map = checkpoint_file[\'f_map\']\r\n    l_map = checkpoint_file[\'l_map\']\r\n    c_map = checkpoint_file[\'c_map\']\r\n    in_doc_words = checkpoint_file[\'in_doc_words\']\r\n    if args.gpu >= 0:\r\n        torch.cuda.set_device(args.gpu)\r\n\r\n\r\n    # load corpus\r\n    if args.test_file:\r\n        with codecs.open(args.test_file, \'r\', \'utf-8\') as f:\r\n            test_lines = f.readlines()\r\n    else:\r\n        with codecs.open(jd[\'test_file\'], \'r\', \'utf-8\') as f:\r\n            test_lines = f.readlines()\r\n\r\n    # converting format\r\n\r\n    test_features, test_labels = utils.read_corpus(test_lines)\r\n\r\n    # construct dataset\r\n    test_dataset, forw_test, back_test = utils.construct_bucket_mean_vb_wc(test_features, test_labels, l_map, c_map, f_map, jd[\'caseless\'])\r\n    \r\n    test_dataset_loader = [torch.utils.data.DataLoader(tup, 50, shuffle=False, drop_last=False) for tup in test_dataset]\r\n\r\n    # build model\r\n    ner_model = LM_LSTM_CRF(len(l_map), len(c_map), jd[\'char_dim\'], jd[\'char_hidden\'], jd[\'char_layers\'], jd[\'word_dim\'], jd[\'word_hidden\'], jd[\'word_layers\'], len(f_map), jd[\'drop_out\'], large_CRF=jd[\'small_crf\'], if_highway=jd[\'high_way\'], in_doc_words=in_doc_words, highway_layers = jd[\'highway_layers\'])\r\n\r\n    ner_model.load_state_dict(checkpoint_file[\'state_dict\'])\r\n\r\n    if args.gpu >= 0:\r\n        if_cuda = True\r\n        torch.cuda.set_device(args.gpu)\r\n        ner_model.cuda()\r\n        packer = CRFRepack_WC(len(l_map), True)\r\n    else:\r\n        if_cuda = False\r\n        packer = CRFRepack_WC(len(l_map), False)\r\n\r\n    evaluator = eval_wc(packer, l_map, args.eva_matrix)\r\n\r\n    print(\'start\')\r\n    if \'f\' in args.eva_matrix:\r\n\r\n        result = evaluator.calc_score(ner_model, test_dataset_loader)\r\n        for label, (test_f1, test_pre, test_rec, test_acc, msg) in result.items():\r\n            print(jd[\'checkpoint\'] +\' : %s : test_f1: %.4f test_rec: %.4f test_pre: %.4f test_acc: %.4f | %s\\n\' % (label, test_f1, test_rec, test_pre, test_acc, msg))\r\n\r\n    else:\r\n\r\n        test_acc = evaluator.calc_score(ner_model, test_dataset_loader)\r\n\r\n        print(jd[\'checkpoint\'] + \' test_acc: %.4f\\n\' % (test_acc))\r\n    print(\'end\')\r\n'"
seq_w.py,6,"b'from __future__ import print_function\nimport datetime\nimport time\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.optim as optim\nimport codecs\nfrom model.crf import *\nfrom model.lstm_crf import *\nimport model.utils as utils\nfrom model.predictor import predict_w\n\nimport argparse\nimport json\nimport os\nimport sys\nfrom tqdm import tqdm\nimport itertools\nimport functools\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Evaluating LM-BLSTM-CRF\')\n    parser.add_argument(\'--load_arg\', default=\'./checkpoint/ner/ner_4_cwlm_lstm_crf.json\', help=\'path to arg json\')\n    parser.add_argument(\'--load_check_point\', default=\'./checkpoint/ner/ner_4_cwlm_lstm_crf.model\', help=\'path to model checkpoint file\')\n    parser.add_argument(\'--gpu\',type=int, default=0, help=\'gpu id\')\n    parser.add_argument(\'--decode_type\', choices=[\'label\', \'string\'], default=\'string\', help=\'type of decode function, set `label` to couple label with text, or set `string` to insert label into test\')\n    parser.add_argument(\'--batch_size\', type=int, default=50, help=\'size of batch\')\n    parser.add_argument(\'--input_file\', default=\'data/ner2003/test.txt\', help=\'path to input un-annotated corpus\')\n    parser.add_argument(\'--output_file\', default=\'output.txt\', help=\'path to output file\')\n    args = parser.parse_args()\n\n    print(\'loading dictionary\')\n    with open(args.load_arg, \'r\') as f:\n        jd = json.load(f)\n    jd = jd[\'args\']\n\n    checkpoint_file = torch.load(args.load_check_point, map_location=lambda storage, loc: storage)\n    f_map = checkpoint_file[\'f_map\']\n    l_map = checkpoint_file[\'l_map\']\n    if args.gpu >= 0:\n        torch.cuda.set_device(args.gpu)\n\n    # loading corpus\n    print(\'loading corpus\')\n    with codecs.open(args.input_file, \'r\', \'utf-8\') as f:\n        lines = f.readlines()\n\n    # converting format\n    features = utils.read_features(lines)\n\n    # build model\n    print(\'loading model\')\n    ner_model = LSTM_CRF(len(f_map), len(l_map), jd[\'embedding_dim\'], jd[\'hidden\'], jd[\'layers\'], jd[\'drop_out\'], large_CRF=jd[\'small_crf\'])\n\n    ner_model.load_state_dict(checkpoint_file[\'state_dict\'])\n\n    if args.gpu >= 0:\n        if_cuda = True\n        torch.cuda.set_device(args.gpu)\n        ner_model.cuda()\n    else:\n        if_cuda = False\n\n    decode_label = (args.decode_type == \'label\')\n\n    predictor = predict_w(if_cuda, f_map, l_map, f_map[\'<eof>\'], l_map[\'<pad>\'], l_map[\'<start>\'], decode_label, args.batch_size, jd[\'caseless\'])\n\n    print(\'annotating\')\n    with open(args.output_file, \'w\') as fout:\n        predictor.output_batch(ner_model, features, fout)'"
seq_wc.py,6,"b'from __future__ import print_function\nimport datetime\nimport time\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.optim as optim\nimport codecs\nfrom model.crf import *\nfrom model.lm_lstm_crf import *\nimport model.utils as utils\nfrom model.predictor import predict_wc\n\nimport argparse\nimport json\nimport os\nimport sys\nfrom tqdm import tqdm\nimport itertools\nimport functools\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Evaluating LM-BLSTM-CRF\')\n    parser.add_argument(\'--load_arg\', default=\'./checkpoint/ner/ner_4_cwlm_lstm_crf.json\', help=\'path to arg json\')\n    parser.add_argument(\'--load_check_point\', default=\'./checkpoint/ner/ner_4_cwlm_lstm_crf.model\', help=\'path to model checkpoint file\')\n    parser.add_argument(\'--gpu\',type=int, default=0, help=\'gpu id\')\n    parser.add_argument(\'--decode_type\', choices=[\'label\', \'string\'], default=\'string\', help=\'type of decode function, set `label` to couple label with text, or set `string` to insert label into test\')\n    parser.add_argument(\'--batch_size\', type=int, default=50, help=\'size of batch\')\n    parser.add_argument(\'--input_file\', default=\'data/ner2003/test.txt\', help=\'path to input un-annotated corpus\')\n    parser.add_argument(\'--output_file\', default=\'output.txt\', help=\'path to output file\')\n    args = parser.parse_args()\n\n    print(\'loading dictionary\')\n    with open(args.load_arg, \'r\') as f:\n        jd = json.load(f)\n    jd = jd[\'args\']\n\n    checkpoint_file = torch.load(args.load_check_point, map_location=lambda storage, loc: storage)\n    f_map = checkpoint_file[\'f_map\']\n    l_map = checkpoint_file[\'l_map\']\n    c_map = checkpoint_file[\'c_map\']\n    in_doc_words = checkpoint_file[\'in_doc_words\']\n    if args.gpu >= 0:\n        torch.cuda.set_device(args.gpu)\n\n    # loading corpus\n    print(\'loading corpus\')\n    with codecs.open(args.input_file, \'r\', \'utf-8\') as f:\n        lines = f.readlines()\n\n    # converting format\n    features = utils.read_features(lines)\n\n    # build model\n    print(\'loading model\')\n    ner_model = LM_LSTM_CRF(len(l_map), len(c_map), jd[\'char_dim\'], jd[\'char_hidden\'], jd[\'char_layers\'], jd[\'word_dim\'], jd[\'word_hidden\'], jd[\'word_layers\'], len(f_map), jd[\'drop_out\'], large_CRF=jd[\'small_crf\'], if_highway=jd[\'high_way\'], in_doc_words=in_doc_words, highway_layers = jd[\'highway_layers\'])\n\n    ner_model.load_state_dict(checkpoint_file[\'state_dict\'])\n\n    if args.gpu >= 0:\n        if_cuda = True\n        torch.cuda.set_device(args.gpu)\n        ner_model.cuda()\n    else:\n        if_cuda = False\n\n    decode_label = (args.decode_type == \'label\')\n    predictor = predict_wc(if_cuda, f_map, c_map, l_map, f_map[\'<eof>\'], c_map[\'\\n\'], l_map[\'<pad>\'], l_map[\'<start>\'], decode_label, args.batch_size, jd[\'caseless\'])\n\n    print(\'annotating\')\n    with open(args.output_file, \'w\') as fout:\n        predictor.output_batch(ner_model, features, fout)'"
train_w.py,9,"b'from __future__ import print_function\r\nimport datetime\r\nimport time\r\nimport torch\r\nimport torch.autograd as autograd\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport codecs\r\nfrom model.crf import *\r\nfrom model.lstm_crf import *\r\nimport model.utils as utils\r\nfrom model.evaluator import eval_w\r\n\r\nimport argparse\r\nimport json\r\nimport os\r\nimport sys\r\nfrom tqdm import tqdm\r\nimport itertools\r\nimport functools\r\n\r\ndef eprint(*args, **kwargs):\r\n    print(*args, file=sys.stderr, **kwargs)\r\n\r\nif __name__ == ""__main__"":\r\n    parser = argparse.ArgumentParser(description=\'Learning with BLSTM-CRF\')\r\n    parser.add_argument(\'--rand_embedding\', action=\'store_true\', help=\'random initialize word embedding\')\r\n    parser.add_argument(\'--emb_file\', default=\'./embedding/glove.6B.100d.txt\', help=\'path to pre-trained embedding\')\r\n    parser.add_argument(\'--train_file\', default=\'./data/ner2003/eng.train.iobes\', help=\'path to training file\')\r\n    parser.add_argument(\'--dev_file\', default=\'./data/ner2003/eng.testa.iobes\', help=\'path to development file\')\r\n    parser.add_argument(\'--test_file\', default=\'./data/ner2003/eng.testb.iobes\', help=\'path to test file\')\r\n    parser.add_argument(\'--gpu\', type=int, default=0, help=\'gpu id, set to -1 if use cpu mode\')\r\n    parser.add_argument(\'--batch_size\', type=int, default=10, help=\'batch size (10)\')\r\n    parser.add_argument(\'--unk\', default=\'unk\', help=\'unknow-token in pre-trained embedding\')\r\n    parser.add_argument(\'--checkpoint\', default=\'./checkpoint/\', help=\'path to checkpoint prefix\')\r\n    parser.add_argument(\'--hidden\', type=int, default=100, help=\'hidden dimension\')\r\n    parser.add_argument(\'--drop_out\', type=float, default=0.55, help=\'dropout ratio\')\r\n    parser.add_argument(\'--epoch\', type=int, default=200, help=\'maximum epoch number\')\r\n    parser.add_argument(\'--start_epoch\', type=int, default=0, help=\'start epoch idx\')\r\n    parser.add_argument(\'--caseless\', action=\'store_true\', help=\'caseless or not\')\r\n    parser.add_argument(\'--embedding_dim\', type=int, default=100, help=\'dimension for word embedding\')\r\n    parser.add_argument(\'--layers\', type=int, default=1, help=\'number of lstm layers\')\r\n    parser.add_argument(\'--lr\', type=float, default=0.015, help=\'initial learning rate\')\r\n    parser.add_argument(\'--lr_decay\', type=float, default=0.05, help=\'decay ratio of learning rate\')\r\n    parser.add_argument(\'--fine_tune\', action=\'store_false\', help=\'fine tune pre-trained embedding dictionary\')\r\n    parser.add_argument(\'--load_check_point\', default=\'\', help=\'path of checkpoint\')\r\n    parser.add_argument(\'--load_opt\', action=\'store_true\', help=\'load optimizer from \')\r\n    parser.add_argument(\'--update\', choices=[\'sgd\', \'adam\'], default=\'sgd\', help=\'optimizer method\')\r\n    parser.add_argument(\'--momentum\', type=float, default=0.9, help=\'momentum for sgd\')\r\n    parser.add_argument(\'--clip_grad\', type=float, default=5.0, help=\'grad clip at\')\r\n    parser.add_argument(\'--small_crf\', action=\'store_false\', help=\'use small crf instead of large crf, refer model.crf module for more details\')\r\n    parser.add_argument(\'--mini_count\', type=float, default=5, help=\'thresholds to replace rare words with <unk>\')\r\n    parser.add_argument(\'--eva_matrix\', choices=[\'a\', \'fa\'], default=\'fa\', help=\'use f1 and accuracy or accuracy alone\')\r\n    parser.add_argument(\'--patience\', type=int, default=15, help=\'patience for early stop\')\r\n    parser.add_argument(\'--least_iters\', type=int, default=50, help=\'at least train how many epochs before stop\')\r\n    parser.add_argument(\'--shrink_embedding\', action=\'store_true\', help=\'shrink the embedding dictionary to corpus (open this if pre-trained embedding dictionary is too large, but disable this may yield better results on external corpus)\')\r\n    args = parser.parse_args()\r\n\r\n    if args.gpu >= 0:\r\n        torch.cuda.set_device(args.gpu)\r\n\r\n    print(\'setting:\')\r\n    print(args)\r\n\r\n    # load corpus\r\n    print(\'loading corpus\')\r\n    with codecs.open(args.train_file, \'r\', \'utf-8\') as f:\r\n        lines = f.readlines()\r\n    with codecs.open(args.dev_file, \'r\', \'utf-8\') as f:\r\n        dev_lines = f.readlines()\r\n    with codecs.open(args.test_file, \'r\', \'utf-8\') as f:\r\n        test_lines = f.readlines()\r\n\r\n    # converting format\r\n    dev_features, dev_labels = utils.read_corpus(dev_lines)\r\n    test_features, test_labels = utils.read_corpus(test_lines)\r\n\r\n    if args.load_check_point:\r\n        if os.path.isfile(args.load_check_point):\r\n            print(""loading checkpoint: \'{}\'"".format(args.load_check_point))\r\n            checkpoint_file = torch.load(args.load_check_point)\r\n            args.start_epoch = checkpoint_file[\'epoch\']\r\n            f_map = checkpoint_file[\'f_map\']\r\n            l_map = checkpoint_file[\'l_map\']\r\n            train_features, train_labels = utils.read_corpus(lines)\r\n        else:\r\n            print(""no checkpoint found at: \'{}\'"".format(args.load_check_point))\r\n    else:\r\n        print(\'constructing coding table\')\r\n\r\n        # converting format\r\n\r\n        train_features, train_labels, f_map, l_map = utils.generate_corpus(lines, if_shrink_feature=True, thresholds=0)\r\n        \r\n        f_set = {v for v in f_map}\r\n        f_map = utils.shrink_features(f_map, train_features, args.mini_count)\r\n\r\n        dt_f_set = functools.reduce(lambda x, y: x | y, map(lambda t: set(t), dev_features), f_set)\r\n        dt_f_set = functools.reduce(lambda x, y: x | y, map(lambda t: set(t), test_features), dt_f_set)\r\n\r\n        if not args.rand_embedding:\r\n            print(""feature size: \'{}\'"".format(len(f_map)))\r\n            print(\'loading embedding\')\r\n            if args.fine_tune:  # which means does not do fine-tune\r\n                f_map = {\'<eof>\': 0}\r\n            f_map, embedding_tensor, in_doc_words = utils.load_embedding_wlm(args.emb_file, \' \', f_map, dt_f_set,args.caseless,args.unk, args.embedding_dim, shrink_to_corpus=args.shrink_embedding)\r\n            print(""embedding size: \'{}\'"".format(len(f_map)))\r\n\r\n        l_set = functools.reduce(lambda x, y: x | y, map(lambda t: set(t), dev_labels))\r\n        l_set = functools.reduce(lambda x, y: x | y, map(lambda t: set(t), test_labels), l_set)\r\n        for label in l_set:\r\n            if label not in l_map:\r\n                l_map[label] = len(l_map)\r\n\r\n    # construct dataset\r\n    dataset = utils.construct_bucket_mean_vb(train_features, train_labels, f_map, l_map, args.caseless)\r\n    dev_dataset = utils.construct_bucket_mean_vb(dev_features, dev_labels, f_map, l_map, args.caseless)\r\n    test_dataset = utils.construct_bucket_mean_vb(test_features, test_labels, f_map, l_map, args.caseless)\r\n\r\n    dataset_loader = [torch.utils.data.DataLoader(tup, args.batch_size, shuffle=True, drop_last=False) for tup in dataset]\r\n    dev_dataset_loader = [torch.utils.data.DataLoader(tup, 50, shuffle=False, drop_last=False) for tup in dev_dataset]\r\n    test_dataset_loader = [torch.utils.data.DataLoader(tup, 50, shuffle=False, drop_last=False) for tup in test_dataset]\r\n\r\n    # build model\r\n    print(\'building model\')\r\n    ner_model = LSTM_CRF(len(f_map), len(l_map), args.embedding_dim, args.hidden, args.layers, args.drop_out, large_CRF=args.small_crf)\r\n\r\n    if args.load_check_point:\r\n            ner_model.load_state_dict(checkpoint_file[\'state_dict\'])\r\n    else:\r\n        if not args.rand_embedding:\r\n            ner_model.load_pretrained_embedding(embedding_tensor)\r\n        print(\'random initialization\')\r\n        ner_model.rand_init(init_embedding=args.rand_embedding)\r\n\r\n    if args.update == \'sgd\':\r\n        optimizer = optim.SGD(ner_model.parameters(), lr=args.lr, momentum=args.momentum)\r\n    elif args.update == \'adam\':\r\n        optimizer = optim.Adam(ner_model.parameters(), lr=args.lr)\r\n\r\n\r\n    if args.load_check_point and args.load_opt:\r\n        optimizer.load_state_dict(checkpoint_file[\'optimizer\'])\r\n\r\n    crit = CRFLoss_vb(len(l_map), l_map[\'<start>\'], l_map[\'<pad>\'])\r\n\r\n    if args.gpu >= 0:\r\n        if_cuda = True\r\n        print(\'device: \' + str(args.gpu))\r\n        torch.cuda.set_device(args.gpu)\r\n        crit.cuda()\r\n        ner_model.cuda()\r\n        packer = CRFRepack(len(l_map), True)\r\n    else:\r\n        if_cuda = False\r\n        packer = CRFRepack(len(l_map), False)\r\n\r\n    if args.load_check_point:\r\n        dev_f1, dev_acc = eval_batch(ner_model, dev_dataset_loader, pack, l_map)\r\n        test_f1, test_acc = eval_batch(ner_model, test_dataset_loader, pack, l_map)\r\n        print(\'(checkpoint: dev F1 = %.4f, dev acc = %.4f, F1 on test = %.4f, acc on test= %.4f)\' %\r\n              (dev_f1,\r\n               dev_acc,\r\n               test_f1,\r\n               test_acc))\r\n\r\n    tot_length = sum(map(lambda t: len(t), dataset_loader))\r\n    best_f1 = float(\'-inf\')\r\n    best_acc = float(\'-inf\')\r\n    track_list = list()\r\n    start_time = time.time()\r\n    epoch_list = range(args.start_epoch, args.start_epoch + args.epoch)\r\n    patience_count = 0\r\n\r\n    evaluator = eval_w(packer, l_map, args.eva_matrix)\r\n\r\n    for epoch_idx, args.start_epoch in enumerate(epoch_list):\r\n\r\n        epoch_loss = 0\r\n        ner_model.train()\r\n\r\n        for feature, tg, mask in tqdm(\r\n                itertools.chain.from_iterable(dataset_loader), mininterval=2,\r\n                desc=\' - Tot it %d (epoch %d)\' % (tot_length, args.start_epoch), leave=False, file=sys.stdout):\r\n\r\n            fea_v, tg_v, mask_v = packer.repack_vb(feature, tg, mask)\r\n            ner_model.zero_grad()\r\n            scores, hidden = ner_model.forward(fea_v)\r\n            loss = crit.forward(scores, tg_v, mask_v)\r\n            loss.backward()\r\n            nn.utils.clip_grad_norm_(ner_model.parameters(), args.clip_grad)\r\n            optimizer.step()\r\n            epoch_loss += utils.to_scalar(loss)\r\n\r\n        # update lr\r\n        utils.adjust_learning_rate(optimizer, args.lr / (1 + (args.start_epoch + 1) * args.lr_decay))\r\n\r\n        # average\r\n        epoch_loss /= tot_length\r\n\r\n        # eval & save check_point\r\n\r\n        if \'f\' in args.eva_matrix:\r\n            dev_result = evaluator.calc_score(ner_model, dev_dataset_loader)\r\n            for label, (dev_f1, dev_pre, dev_rec, dev_acc, msg) in dev_result.items():\r\n                print(\'DEV : %s : dev_f1: %.4f dev_rec: %.4f dev_pre: %.4f dev_acc: %.4f | %s\\n\' % (label, dev_f1, dev_pre, dev_rec, dev_acc, msg))\r\n            (dev_f1, dev_pre, dev_rec, dev_acc, msg) = dev_result[\'total\']\r\n\r\n            if dev_f1 > best_f1:\r\n                patience_count = 0\r\n                best_f1 = dev_f1\r\n\r\n                test_result = evaluator.calc_score(ner_model, test_dataset_loader)\r\n                for label, (test_f1, test_pre, test_rec, test_acc, msg) in test_result.items():\r\n                    print(\'TEST : %s : test_f1: %.4f test_rec: %.4f test_pre: %.4f test_acc: %.4f | %s\\n\' % (label, test_f1, test_rec, test_pre, test_acc, msg))\r\n                (test_f1, test_rec, test_pre, test_acc, msg) = test_result[\'total\']\r\n\r\n                track_list.append(\r\n                    {\'loss\': epoch_loss, \'dev_f1\': dev_f1, \'dev_acc\': dev_acc, \'test_f1\': test_f1,\r\n                     \'test_acc\': test_acc})\r\n\r\n                print(\r\n                    \'(loss: %.4f, epoch: %d, dev F1 = %.4f, dev acc = %.4f, F1 on test = %.4f, acc on test= %.4f), saving...\' %\r\n                    (epoch_loss,\r\n                     args.start_epoch,\r\n                     dev_f1,\r\n                     dev_acc,\r\n                     test_f1,\r\n                     test_acc))\r\n\r\n                try:\r\n                    utils.save_checkpoint({\r\n                        \'epoch\': args.start_epoch,\r\n                        \'state_dict\': ner_model.state_dict(),\r\n                        \'optimizer\': optimizer.state_dict(),\r\n                        \'f_map\': f_map,\r\n                        \'l_map\': l_map,\r\n                    }, {\'track_list\': track_list,\r\n                        \'args\': vars(args)\r\n                        }, args.checkpoint + \'lstm_crf\')\r\n                except Exception as inst:\r\n                    print(inst)\r\n\r\n            else:\r\n                patience_count += 1\r\n                print(\'(loss: %.4f, epoch: %d, dev F1 = %.4f, dev acc = %.4f)\' %\r\n                      (epoch_loss,\r\n                       args.start_epoch,\r\n                       dev_f1,\r\n                       dev_acc))\r\n                track_list.append({\'loss\': epoch_loss, \'dev_f1\': dev_f1, \'dev_acc\': dev_acc})\r\n\r\n        else:\r\n\r\n            dev_acc = evaluator.calc_score(ner_model, dev_dataset_loader)\r\n\r\n            if dev_acc > best_acc:\r\n                patience_count = 0\r\n                best_acc = dev_acc\r\n                \r\n                test_acc = evaluator.calc_score(ner_model, test_dataset_loader)\r\n\r\n                track_list.append(\r\n                    {\'loss\': epoch_loss, \'dev_acc\': dev_acc, \'test_acc\': test_acc})\r\n\r\n                print(\r\n                    \'(loss: %.4f, epoch: %d, dev acc = %.4f, acc on test= %.4f), saving...\' %\r\n                    (epoch_loss,\r\n                     args.start_epoch,\r\n                     dev_acc,\r\n                     test_acc))\r\n\r\n                try:\r\n                    utils.save_checkpoint({\r\n                        \'epoch\': args.start_epoch,\r\n                        \'state_dict\': ner_model.state_dict(),\r\n                        \'optimizer\': optimizer.state_dict(),\r\n                        \'f_map\': f_map,\r\n                        \'l_map\': l_map,\r\n                    }, {\'track_list\': track_list,\r\n                        \'args\': vars(args)\r\n                        }, args.checkpoint + \'lstm_crf\')\r\n                except Exception as inst:\r\n                    print(inst)\r\n\r\n            else:\r\n                patience_count += 1\r\n                print(\'(loss: %.4f, epoch: %d, dev acc = %.4f)\' %\r\n                      (epoch_loss,\r\n                       args.start_epoch,\r\n                       dev_acc))\r\n                track_list.append({\'loss\': epoch_loss, \'dev_acc\': dev_acc})\r\n\r\n        print(\'epoch: \' + str(args.start_epoch) + \'\\t in \' + str(args.epoch) + \' take: \' + str(\r\n            time.time() - start_time) + \' s\')\r\n\r\n        if patience_count >= args.patience and args.start_epoch >= args.least_iters:\r\n            break\r\n\r\n    #print best\r\n    if \'f\' in args.eva_matrix:\r\n        eprint(args.checkpoint + \' dev_f1: %.4f dev_rec: %.4f dev_pre: %.4f dev_acc: %.4f test_f1: %.4f test_rec: %.4f test_pre: %.4f test_acc: %.4f\\n\' % (dev_f1, dev_rec, dev_pre, dev_acc, test_f1, test_rec, test_pre, test_acc))\r\n    else:\r\n        eprint(args.checkpoint + \' dev_acc: %.4f test_acc: %.4f\\n\' % (dev_acc, test_acc))\r\n\r\n    # printing summary\r\n    print(\'setting:\')\r\n    print(args)\r\n'"
train_wc.py,9,"b'from __future__ import print_function\r\nimport datetime\r\nimport time\r\nimport torch\r\nimport torch.autograd as autograd\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport codecs\r\nfrom model.crf import *\r\nfrom model.lm_lstm_crf import *\r\nimport model.utils as utils\r\nfrom model.evaluator import eval_wc\r\n\r\nimport argparse\r\nimport json\r\nimport os\r\nimport sys\r\nfrom tqdm import tqdm\r\nimport itertools\r\nimport functools\r\n\r\ndef eprint(*args, **kwargs):\r\n    print(*args, file=sys.stderr, **kwargs)\r\n\r\nif __name__ == ""__main__"":\r\n    parser = argparse.ArgumentParser(description=\'Learning with LM-LSTM-CRF together with Language Model\')\r\n    parser.add_argument(\'--rand_embedding\', action=\'store_true\', help=\'random initialize word embedding\')\r\n    parser.add_argument(\'--emb_file\', default=\'./embedding/glove.6B.100d.txt\', help=\'path to pre-trained embedding\')\r\n    parser.add_argument(\'--train_file\', default=\'./data/ner/eng.train.iobes\', help=\'path to training file\')\r\n    parser.add_argument(\'--dev_file\', default=\'./data/ner/eng.testa.iobes\', help=\'path to development file\')\r\n    parser.add_argument(\'--test_file\', default=\'./data/ner/eng.testb.iobes\', help=\'path to test file\')\r\n    parser.add_argument(\'--gpu\', type=int, default=0, help=\'gpu id\')\r\n    parser.add_argument(\'--batch_size\', type=int, default=10, help=\'batch_size\')\r\n    parser.add_argument(\'--unk\', default=\'unk\', help=\'unknow-token in pre-trained embedding\')\r\n    parser.add_argument(\'--char_hidden\', type=int, default=300, help=\'dimension of char-level layers\')\r\n    parser.add_argument(\'--word_hidden\', type=int, default=300, help=\'dimension of word-level layers\')\r\n    parser.add_argument(\'--drop_out\', type=float, default=0.55, help=\'dropout ratio\')\r\n    parser.add_argument(\'--epoch\', type=int, default=200, help=\'maximum epoch number\')\r\n    parser.add_argument(\'--start_epoch\', type=int, default=0, help=\'start point of epoch\')\r\n    parser.add_argument(\'--checkpoint\', default=\'./checkpoint/\', help=\'checkpoint path\')\r\n    parser.add_argument(\'--caseless\', action=\'store_true\', help=\'caseless or not\')\r\n    parser.add_argument(\'--char_dim\', type=int, default=30, help=\'dimension of char embedding\')\r\n    parser.add_argument(\'--word_dim\', type=int, default=100, help=\'dimension of word embedding\')\r\n    parser.add_argument(\'--char_layers\', type=int, default=1, help=\'number of char level layers\')\r\n    parser.add_argument(\'--word_layers\', type=int, default=1, help=\'number of word level layers\')\r\n    parser.add_argument(\'--lr\', type=float, default=0.015, help=\'initial learning rate\')\r\n    parser.add_argument(\'--lr_decay\', type=float, default=0.05, help=\'decay ratio of learning rate\')\r\n    parser.add_argument(\'--fine_tune\', action=\'store_false\', help=\'fine tune the diction of word embedding or not\')\r\n    parser.add_argument(\'--load_check_point\', default=\'\', help=\'path previous checkpoint that want to be loaded\')\r\n    parser.add_argument(\'--load_opt\', action=\'store_true\', help=\'also load optimizer from the checkpoint\')\r\n    parser.add_argument(\'--update\', choices=[\'sgd\', \'adam\'], default=\'sgd\', help=\'optimizer choice\')\r\n    parser.add_argument(\'--momentum\', type=float, default=0.9, help=\'momentum for sgd\')\r\n    parser.add_argument(\'--clip_grad\', type=float, default=5.0, help=\'clip grad at\')\r\n    parser.add_argument(\'--small_crf\', action=\'store_false\', help=\'use small crf instead of large crf, refer model.crf module for more details\')\r\n    parser.add_argument(\'--mini_count\', type=float, default=5, help=\'thresholds to replace rare words with <unk>\')\r\n    parser.add_argument(\'--lambda0\', type=float, default=1, help=\'lambda0\')\r\n    parser.add_argument(\'--co_train\', action=\'store_true\', help=\'cotrain language model\')\r\n    parser.add_argument(\'--patience\', type=int, default=15, help=\'patience for early stop\')\r\n    parser.add_argument(\'--high_way\', action=\'store_true\', help=\'use highway layers\')\r\n    parser.add_argument(\'--highway_layers\', type=int, default=1, help=\'number of highway layers\')\r\n    parser.add_argument(\'--eva_matrix\', choices=[\'a\', \'fa\'], default=\'fa\', help=\'use f1 and accuracy or accuracy alone\')\r\n    parser.add_argument(\'--least_iters\', type=int, default=50, help=\'at least train how many epochs before stop\')\r\n    parser.add_argument(\'--shrink_embedding\', action=\'store_true\', help=\'shrink the embedding dictionary to corpus (open this if pre-trained embedding dictionary is too large, but disable this may yield better results on external corpus)\')\r\n    args = parser.parse_args()\r\n\r\n    if args.gpu >= 0:\r\n        torch.cuda.set_device(args.gpu)\r\n\r\n    print(\'setting:\')\r\n    print(args)\r\n\r\n    # load corpus\r\n    print(\'loading corpus\')\r\n    with codecs.open(args.train_file, \'r\', \'utf-8\') as f:\r\n        lines = f.readlines()\r\n    with codecs.open(args.dev_file, \'r\', \'utf-8\') as f:\r\n        dev_lines = f.readlines()\r\n    with codecs.open(args.test_file, \'r\', \'utf-8\') as f:\r\n        test_lines = f.readlines()\r\n\r\n    dev_features, dev_labels = utils.read_corpus(dev_lines)\r\n    test_features, test_labels = utils.read_corpus(test_lines)\r\n\r\n    if args.load_check_point:\r\n        if os.path.isfile(args.load_check_point):\r\n            print(""loading checkpoint: \'{}\'"".format(args.load_check_point))\r\n            checkpoint_file = torch.load(args.load_check_point)\r\n            args.start_epoch = checkpoint_file[\'epoch\']\r\n            f_map = checkpoint_file[\'f_map\']\r\n            l_map = checkpoint_file[\'l_map\']\r\n            c_map = checkpoint_file[\'c_map\']\r\n            in_doc_words = checkpoint_file[\'in_doc_words\']\r\n            train_features, train_labels = utils.read_corpus(lines)\r\n        else:\r\n            print(""no checkpoint found at: \'{}\'"".format(args.load_check_point))\r\n    else:\r\n        print(\'constructing coding table\')\r\n\r\n        # converting format\r\n        train_features, train_labels, f_map, l_map, c_map = utils.generate_corpus_char(lines, if_shrink_c_feature=True, c_thresholds=args.mini_count, if_shrink_w_feature=False)\r\n        \r\n        f_set = {v for v in f_map}\r\n        f_map = utils.shrink_features(f_map, train_features, args.mini_count)\r\n\r\n        if args.rand_embedding:\r\n            print(""embedding size: \'{}\'"".format(len(f_map)))\r\n            in_doc_words = len(f_map)\r\n        else:\r\n            dt_f_set = functools.reduce(lambda x, y: x | y, map(lambda t: set(t), dev_features), f_set)\r\n            dt_f_set = functools.reduce(lambda x, y: x | y, map(lambda t: set(t), test_features), dt_f_set)\r\n            print(""feature size: \'{}\'"".format(len(f_map)))\r\n            print(\'loading embedding\')\r\n            if args.fine_tune:  # which means does not do fine-tune\r\n                f_map = {\'<eof>\': 0}\r\n            f_map, embedding_tensor, in_doc_words = utils.load_embedding_wlm(args.emb_file, \' \', f_map, dt_f_set, args.caseless, args.unk, args.word_dim, shrink_to_corpus=args.shrink_embedding)\r\n            print(""embedding size: \'{}\'"".format(len(f_map)))\r\n\r\n        l_set = functools.reduce(lambda x, y: x | y, map(lambda t: set(t), dev_labels))\r\n        l_set = functools.reduce(lambda x, y: x | y, map(lambda t: set(t), test_labels), l_set)\r\n        for label in l_set:\r\n            if label not in l_map:\r\n                l_map[label] = len(l_map)\r\n    \r\n    print(\'constructing dataset\')\r\n    # construct dataset\r\n    dataset, forw_corp, back_corp = utils.construct_bucket_mean_vb_wc(train_features, train_labels, l_map, c_map, f_map, args.caseless)\r\n    dev_dataset, forw_dev, back_dev = utils.construct_bucket_mean_vb_wc(dev_features, dev_labels, l_map, c_map, f_map, args.caseless)\r\n    test_dataset, forw_test, back_test = utils.construct_bucket_mean_vb_wc(test_features, test_labels, l_map, c_map, f_map, args.caseless)\r\n    \r\n    dataset_loader = [torch.utils.data.DataLoader(tup, args.batch_size, shuffle=True, drop_last=False) for tup in dataset]\r\n    dev_dataset_loader = [torch.utils.data.DataLoader(tup, 50, shuffle=False, drop_last=False) for tup in dev_dataset]\r\n    test_dataset_loader = [torch.utils.data.DataLoader(tup, 50, shuffle=False, drop_last=False) for tup in test_dataset]\r\n\r\n    # build model\r\n    print(\'building model\')\r\n    ner_model = LM_LSTM_CRF(len(l_map), len(c_map), args.char_dim, args.char_hidden, args.char_layers, args.word_dim, args.word_hidden, args.word_layers, len(f_map), args.drop_out, large_CRF=args.small_crf, if_highway=args.high_way, in_doc_words=in_doc_words, highway_layers = args.highway_layers)\r\n\r\n    if args.load_check_point:\r\n        ner_model.load_state_dict(checkpoint_file[\'state_dict\'])\r\n    else:\r\n        if not args.rand_embedding:\r\n            ner_model.load_pretrained_word_embedding(embedding_tensor)\r\n        ner_model.rand_init(init_word_embedding=args.rand_embedding)\r\n\r\n    if args.update == \'sgd\':\r\n        optimizer = optim.SGD(ner_model.parameters(), lr=args.lr, momentum=args.momentum)\r\n    elif args.update == \'adam\':\r\n        optimizer = optim.Adam(ner_model.parameters(), lr=args.lr)\r\n\r\n    if args.load_check_point and args.load_opt:\r\n        optimizer.load_state_dict(checkpoint_file[\'optimizer\'])\r\n\r\n    crit_lm = nn.CrossEntropyLoss()\r\n    crit_ner = CRFLoss_vb(len(l_map), l_map[\'<start>\'], l_map[\'<pad>\'])\r\n\r\n    if args.gpu >= 0:\r\n        if_cuda = True\r\n        print(\'device: \' + str(args.gpu))\r\n        torch.cuda.set_device(args.gpu)\r\n        crit_ner.cuda()\r\n        crit_lm.cuda()\r\n        ner_model.cuda()\r\n        packer = CRFRepack_WC(len(l_map), True)\r\n    else:\r\n        if_cuda = False\r\n        packer = CRFRepack_WC(len(l_map), False)\r\n\r\n    tot_length = sum(map(lambda t: len(t), dataset_loader))\r\n\r\n    best_f1 = float(\'-inf\')\r\n    best_acc = float(\'-inf\')\r\n    track_list = list()\r\n    start_time = time.time()\r\n    epoch_list = range(args.start_epoch, args.start_epoch + args.epoch)\r\n    patience_count = 0\r\n\r\n    evaluator = eval_wc(packer, l_map, args.eva_matrix)\r\n\r\n    for epoch_idx, args.start_epoch in enumerate(epoch_list):\r\n\r\n        epoch_loss = 0\r\n        ner_model.train()\r\n        for f_f, f_p, b_f, b_p, w_f, tg_v, mask_v, len_v in tqdm(\r\n                itertools.chain.from_iterable(dataset_loader), mininterval=2,\r\n                desc=\' - Tot it %d (epoch %d)\' % (tot_length, args.start_epoch), leave=False, file=sys.stdout):\r\n            f_f, f_p, b_f, b_p, w_f, tg_v, mask_v = packer.repack_vb(f_f, f_p, b_f, b_p, w_f, tg_v, mask_v, len_v)\r\n            ner_model.zero_grad()\r\n            scores = ner_model(f_f, f_p, b_f, b_p, w_f)\r\n            loss = crit_ner(scores, tg_v, mask_v)\r\n            epoch_loss += utils.to_scalar(loss)\r\n            if args.co_train:\r\n                cf_p = f_p[0:-1, :].contiguous()\r\n                cb_p = b_p[1:, :].contiguous()\r\n                cf_y = w_f[1:, :].contiguous()\r\n                cb_y = w_f[0:-1, :].contiguous()\r\n                cfs, _ = ner_model.word_pre_train_forward(f_f, cf_p)\r\n                loss = loss + args.lambda0 * crit_lm(cfs, cf_y.view(-1))\r\n                cbs, _ = ner_model.word_pre_train_backward(b_f, cb_p)\r\n                loss = loss + args.lambda0 * crit_lm(cbs, cb_y.view(-1))\r\n            loss.backward()\r\n            nn.utils.clip_grad_norm_(ner_model.parameters(), args.clip_grad)\r\n            optimizer.step()\r\n        epoch_loss /= tot_length\r\n\r\n        # update lr\r\n        if args.update == \'sgd\':\r\n            utils.adjust_learning_rate(optimizer, args.lr / (1 + (args.start_epoch + 1) * args.lr_decay))\r\n\r\n        # eval & save check_point\r\n\r\n        if \'f\' in args.eva_matrix:\r\n            dev_result = evaluator.calc_score(ner_model, dev_dataset_loader)\r\n            for label, (dev_f1, dev_pre, dev_rec, dev_acc, msg) in dev_result.items():\r\n                print(\'DEV : %s : dev_f1: %.4f dev_rec: %.4f dev_pre: %.4f dev_acc: %.4f | %s\\n\' % (label, dev_f1, dev_rec, dev_pre, dev_acc, msg))\r\n            (dev_f1, dev_pre, dev_rec, dev_acc, msg) = dev_result[\'total\']\r\n\r\n            if dev_f1 > best_f1:\r\n                patience_count = 0\r\n                best_f1 = dev_f1\r\n\r\n                test_result = evaluator.calc_score(ner_model, test_dataset_loader)\r\n                for label, (test_f1, test_pre, test_rec, test_acc, msg) in test_result.items():\r\n                    print(\'TEST : %s : test_f1: %.4f test_rec: %.4f test_pre: %.4f test_acc: %.4f | %s\\n\' % (label, test_f1, test_rec, test_pre, test_acc, msg))\r\n                (test_f1, test_rec, test_pre, test_acc, msg) = test_result[\'total\']\r\n\r\n                track_list.append(\r\n                    {\'loss\': epoch_loss, \'dev_f1\': dev_f1, \'dev_acc\': dev_acc, \'test_f1\': test_f1,\r\n                     \'test_acc\': test_acc})\r\n\r\n                print(\r\n                    \'(loss: %.4f, epoch: %d, dev F1 = %.4f, dev acc = %.4f, F1 on test = %.4f, acc on test= %.4f), saving...\' %\r\n                    (epoch_loss,\r\n                     args.start_epoch,\r\n                     dev_f1,\r\n                     dev_acc,\r\n                     test_f1,\r\n                     test_acc))\r\n\r\n                try:\r\n                    utils.save_checkpoint({\r\n                        \'epoch\': args.start_epoch,\r\n                        \'state_dict\': ner_model.state_dict(),\r\n                        \'optimizer\': optimizer.state_dict(),\r\n                        \'f_map\': f_map,\r\n                        \'l_map\': l_map,\r\n                        \'c_map\': c_map,\r\n                        \'in_doc_words\': in_doc_words\r\n                    }, {\'track_list\': track_list,\r\n                        \'args\': vars(args)\r\n                        }, args.checkpoint + \'cwlm_lstm_crf\')\r\n                except Exception as inst:\r\n                    print(inst)\r\n\r\n            else:\r\n                patience_count += 1\r\n                print(\'(loss: %.4f, epoch: %d, dev F1 = %.4f, dev acc = %.4f)\' %\r\n                      (epoch_loss,\r\n                       args.start_epoch,\r\n                       dev_f1,\r\n                       dev_acc))\r\n                track_list.append({\'loss\': epoch_loss, \'dev_f1\': dev_f1, \'dev_acc\': dev_acc})\r\n\r\n        else:\r\n\r\n            dev_acc = evaluator.calc_score(ner_model, dev_dataset_loader)\r\n\r\n            if dev_acc > best_acc:\r\n                patience_count = 0\r\n                best_acc = dev_acc\r\n                \r\n                test_acc = evaluator.calc_score(ner_model, test_dataset_loader)\r\n\r\n                track_list.append(\r\n                    {\'loss\': epoch_loss, \'dev_acc\': dev_acc, \'test_acc\': test_acc})\r\n\r\n                print(\r\n                    \'(loss: %.4f, epoch: %d, dev acc = %.4f, acc on test= %.4f), saving...\' %\r\n                    (epoch_loss,\r\n                     args.start_epoch,\r\n                     dev_acc,\r\n                     test_acc))\r\n\r\n                try:\r\n                    utils.save_checkpoint({\r\n                        \'epoch\': args.start_epoch,\r\n                        \'state_dict\': ner_model.state_dict(),\r\n                        \'optimizer\': optimizer.state_dict(),\r\n                        \'f_map\': f_map,\r\n                        \'l_map\': l_map,\r\n                        \'c_map\': c_map,\r\n                        \'in_doc_words\': in_doc_words\r\n                    }, {\'track_list\': track_list,\r\n                        \'args\': vars(args)\r\n                        }, args.checkpoint + \'cwlm_lstm_crf\')\r\n                except Exception as inst:\r\n                    print(inst)\r\n\r\n            else:\r\n                patience_count += 1\r\n                print(\'(loss: %.4f, epoch: %d, dev acc = %.4f)\' %\r\n                      (epoch_loss,\r\n                       args.start_epoch,\r\n                       dev_acc))\r\n                track_list.append({\'loss\': epoch_loss, \'dev_acc\': dev_acc})\r\n\r\n        print(\'epoch: \' + str(args.start_epoch) + \'\\t in \' + str(args.epoch) + \' take: \' + str(\r\n            time.time() - start_time) + \' s\')\r\n\r\n        if patience_count >= args.patience and args.start_epoch >= args.least_iters:\r\n            break\r\n\r\n    #print best\r\n    if \'f\' in args.eva_matrix:\r\n        eprint(args.checkpoint + \' dev_f1: %.4f dev_rec: %.4f dev_pre: %.4f dev_acc: %.4f test_f1: %.4f test_rec: %.4f test_pre: %.4f test_acc: %.4f\\n\' % (dev_f1, dev_rec, dev_pre, dev_acc, test_f1, test_rec, test_pre, test_acc))\r\n    else:\r\n        eprint(args.checkpoint + \' dev_acc: %.4f test_acc: %.4f\\n\' % (dev_acc, test_acc))\r\n\r\n    # printing summary\r\n    print(\'setting:\')\r\n    print(args)\r\n'"
model/__init__.py,0,"b'__author__ = ""Liyuan Liu and Frank Xu""\r\n__credits__ = [""Liyuan Liu"", ""Frank Xu"", ""Jingbo Shang""]\r\n\r\n__license__ = ""Apache License 2.0""\r\n__maintainer__ = ""Liyuan Liu""\r\n__email__ = ""llychinalz@gmail.com""'"
model/crf.py,10,"b'""""""\r\n.. module:: crf\r\n    :synopsis: conditional random field\r\n\r\n.. moduleauthor:: Liyuan Liu\r\n""""""\r\n\r\nimport torch\r\nimport torch.autograd as autograd\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport torch.sparse as sparse\r\nimport model.utils as utils\r\n\r\n\r\nclass CRF_L(nn.Module):\r\n    """"""Conditional Random Field (CRF) layer. This version is used in Ma et al. 2016, has more parameters than CRF_S\r\n\r\n    args:\r\n        hidden_dim : input dim size\r\n        tagset_size: target_set_size\r\n        if_biase: whether allow bias in linear trans\r\n    """"""\r\n\r\n\r\n    def __init__(self, hidden_dim, tagset_size, if_bias=True):\r\n        super(CRF_L, self).__init__()\r\n        self.tagset_size = tagset_size\r\n        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size * self.tagset_size, bias=if_bias)\r\n\r\n    def rand_init(self):\r\n        """"""random initialization\r\n        """"""\r\n        utils.init_linear(self.hidden2tag)\r\n\r\n    def forward(self, feats):\r\n        """"""\r\n        args:\r\n            feats (batch_size, seq_len, hidden_dim) : input score from previous layers\r\n        return:\r\n            output from crf layer (batch_size, seq_len, tag_size, tag_size)\r\n        """"""\r\n        return self.hidden2tag(feats).view(-1, self.tagset_size, self.tagset_size)\r\n\r\n\r\nclass CRF_S(nn.Module):\r\n    """"""Conditional Random Field (CRF) layer. This version is used in Lample et al. 2016, has less parameters than CRF_L.\r\n\r\n    args:\r\n        hidden_dim: input dim size\r\n        tagset_size: target_set_size\r\n        if_biase: whether allow bias in linear trans\r\n\r\n    """"""\r\n\r\n    def __init__(self, hidden_dim, tagset_size, if_bias=True):\r\n        super(CRF_S, self).__init__()\r\n        self.tagset_size = tagset_size\r\n        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size, bias=if_bias)\r\n        self.transitions = nn.Parameter(torch.Tensor(self.tagset_size, self.tagset_size))\r\n\r\n    def rand_init(self):\r\n        """"""random initialization\r\n        """"""\r\n        utils.init_linear(self.hidden2tag)\r\n        self.transitions.data.zero_()\r\n\r\n    def forward(self, feats):\r\n        """"""\r\n        args:\r\n            feats (batch_size, seq_len, hidden_dim) : input score from previous layers\r\n        return:\r\n            output from crf layer ( (batch_size * seq_len), tag_size, tag_size)\r\n        """"""\r\n\r\n        scores = self.hidden2tag(feats).view(-1, self.tagset_size, 1)\r\n        ins_num = scores.size(0)\r\n        crf_scores = scores.expand(ins_num, self.tagset_size, self.tagset_size) + self.transitions.view(1, self.tagset_size, self.tagset_size).expand(ins_num, self.tagset_size, self.tagset_size)\r\n\r\n        return crf_scores\r\n\r\nclass CRFRepack:\r\n    """"""Packer for word level model\r\n\r\n    args:\r\n        tagset_size: target_set_size\r\n        if_cuda: whether use GPU\r\n    """"""\r\n\r\n    def __init__(self, tagset_size, if_cuda):\r\n\r\n        self.tagset_size = tagset_size\r\n        self.if_cuda = if_cuda\r\n\r\n    def repack_vb(self, feature, target, mask):\r\n        """"""packer for viterbi loss\r\n\r\n        args:\r\n            feature (Seq_len, Batch_size): input feature\r\n            target (Seq_len, Batch_size): output target\r\n            mask (Seq_len, Batch_size): padding mask\r\n        return:\r\n            feature (Seq_len, Batch_size), target (Seq_len, Batch_size), mask (Seq_len, Batch_size)\r\n        """"""\r\n\r\n        if self.if_cuda:\r\n            fea_v = feature.transpose(0, 1).cuda()\r\n            tg_v = target.transpose(0, 1).unsqueeze(2).cuda()\r\n            mask_v = mask.transpose(0, 1).cuda()\r\n        else:\r\n            fea_v = feature.transpose(0, 1)\r\n            tg_v = target.transpose(0, 1).contiguous().unsqueeze(2)\r\n            mask_v = mask.transpose(0, 1).contiguous()\r\n        return fea_v, tg_v, mask_v\r\n\r\n    def repack_gd(self, feature, target, current):\r\n        """"""packer for greedy loss\r\n\r\n        args:\r\n            feature (Seq_len, Batch_size): input feature\r\n            target (Seq_len, Batch_size): output target\r\n            current (Seq_len, Batch_size): current state\r\n        return:\r\n            feature (Seq_len, Batch_size), target (Seq_len * Batch_size), current (Seq_len * Batch_size, 1, 1)\r\n        """"""\r\n        if self.if_cuda:\r\n            fea_v = feature.transpose(0, 1).cuda()\r\n            ts_v = target.transpose(0, 1).cuda().view(-1)\r\n            cs_v = current.transpose(0, 1).cuda().view(-1, 1, 1)\r\n        else:\r\n            fea_v = feature.transpose(0, 1)\r\n            ts_v = target.transpose(0, 1).contiguous().view(-1)\r\n            cs_v = current.transpose(0, 1).contiguous().view(-1, 1, 1)\r\n        return fea_v, ts_v, cs_v\r\n\r\n    def convert_for_eval(self, target):\r\n        """"""convert target to original decoding\r\n\r\n        args:\r\n            target: input labels used in training\r\n        return:\r\n            output labels used in test\r\n        """"""\r\n        return target % self.tagset_size\r\n\r\n\r\nclass CRFRepack_WC:\r\n    """"""Packer for model with char-level and word-level\r\n\r\n    args:\r\n        tagset_size: target_set_size\r\n        if_cuda: whether use GPU\r\n\r\n    """"""\r\n\r\n    def __init__(self, tagset_size, if_cuda):\r\n\r\n        self.tagset_size = tagset_size\r\n        self.if_cuda = if_cuda\r\n\r\n    def repack_vb(self, fc_feature, fc_position, bc_feature, bc_position, word_feature, target, mask, batch_len):\r\n        """"""packer for viterbi loss\r\n\r\n        args:\r\n            fc_feature (Char_Seq_len, Batch_size) : forward_char input feature\r\n            fc_position (Word_Seq_len, Batch_size) : forward_char input position\r\n            bc_feature (Char_Seq_len, Batch_size) : backward_char input feature\r\n            bc_position (Word_Seq_len, Batch_size) : backward_char input position\r\n            word_feature (Word_Seq_len, Batch_size) : input word feature\r\n            target (Seq_len, Batch_size) : output target\r\n            mask (Word_Seq_len, Batch_size) : padding mask\r\n            batch_len (Batch_size, 2) : length of instances in one batch\r\n        return:\r\n            f_f (Char_Reduced_Seq_len, Batch_size), f_p (Word_Reduced_Seq_len, Batch_size), b_f (Char_Reduced_Seq_len, Batch_size), b_p (Word_Reduced_Seq_len, Batch_size), w_f (size Word_Seq_Len, Batch_size), target (Reduced_Seq_len, Batch_size), mask  (Word_Reduced_Seq_len, Batch_size)\r\n\r\n        """"""\r\n        mlen, _ = batch_len.max(0)\r\n        mlen = mlen.squeeze()\r\n        ocl = bc_feature.size(1)\r\n        if self.if_cuda:\r\n            fc_feature = fc_feature[:, 0:mlen[0]].transpose(0, 1).cuda()\r\n            fc_position = fc_position[:, 0:mlen[1]].transpose(0, 1).cuda()\r\n            bc_feature = bc_feature[:, -mlen[0]:].transpose(0, 1).cuda()\r\n            bc_position = (bc_position[:, 0:mlen[1]] - ocl + mlen[0]).transpose(0, 1).cuda()\r\n            word_feature = word_feature[:, 0:mlen[1]].transpose(0, 1).cuda()\r\n            tg_v = target[:, 0:mlen[1]].transpose(0, 1).unsqueeze(2).cuda()\r\n            mask_v = mask[:, 0:mlen[1]].transpose(0, 1).cuda()\r\n        else:\r\n            fc_feature = fc_feature[:, 0:mlen[0]].transpose(0, 1)\r\n            fc_position = fc_position[:, 0:mlen[1]].transpose(0, 1)\r\n            bc_feature = bc_feature[:, -mlen[0]:].transpose(0, 1)\r\n            bc_position = (bc_position[:, 0:mlen[1]] - ocl + mlen[0]).transpose(0, 1)\r\n            word_feature = word_feature[:, 0:mlen[1]].transpose(0, 1)\r\n            tg_v = target[:, 0:mlen[1]].transpose(0, 1).unsqueeze(2)\r\n            mask_v = mask[:, 0:mlen[1]].transpose(0, 1).contiguous()\r\n        return fc_feature, fc_position, bc_feature, bc_position, word_feature, tg_v, mask_v\r\n\r\n    def convert_for_eval(self, target):\r\n        """"""convert for eval\r\n\r\n        args:\r\n            target: input labels used in training\r\n        return:\r\n            output labels used in test\r\n        """"""\r\n        return target % self.tagset_size\r\n\r\n\r\nclass CRFLoss_gd(nn.Module):\r\n    """"""loss for greedy decode loss, i.e., although its for CRF Layer, we calculate the loss as\r\n\r\n    .. math::\r\n        \\sum_{j=1}^n \\log (p(\\hat{y}_{j+1}|z_{j+1}, \\hat{y}_{j}))\r\n\r\n    instead of\r\n\r\n    .. math::\r\n        \\sum_{j=1}^n \\log (\\phi(\\hat{y}_{j-1}, \\hat{y}_j, \\mathbf{z}_j)) - \\log (\\sum_{\\mathbf{y}\' \\in \\mathbf{Y}(\\mathbf{Z})} \\prod_{j=1}^n \\phi(y\'_{j-1}, y\'_j, \\mathbf{z}_j) )\r\n\r\n    args:\r\n        tagset_size: target_set_size\r\n        start_tag: ind for <start>\r\n        end_tag: ind for <pad>\r\n        average_batch: whether average the loss among batch\r\n\r\n    """"""\r\n\r\n    def __init__(self, tagset_size, start_tag, end_tag, average_batch=True):\r\n        super(CRFLoss_gd, self).__init__()\r\n        self.tagset_size = tagset_size\r\n        self.average_batch = average_batch\r\n        self.crit = nn.CrossEntropyLoss(size_average=self.average_batch)\r\n\r\n    def forward(self, scores, target, current):\r\n        """"""\r\n        args:\r\n            scores (Word_Seq_len, Batch_size, target_size_from, target_size_to): crf scores\r\n            target (Word_Seq_len, Batch_size): golden list\r\n            current (Word_Seq_len, Batch_size): current state\r\n        return:\r\n            crf greedy loss\r\n        """"""\r\n        ins_num = current.size(0)\r\n        current = current.expand(ins_num, 1, self.tagset_size)\r\n        scores = scores.view(ins_num, self.tagset_size, self.tagset_size)\r\n        current_score = torch.gather(scores, 1, current).squeeze()\r\n        return self.crit(current_score, target)\r\n\r\n\r\nclass CRFLoss_vb(nn.Module):\r\n    """"""loss for viterbi decode\r\n\r\n    .. math::\r\n        \\sum_{j=1}^n \\log (\\phi(\\hat{y}_{j-1}, \\hat{y}_j, \\mathbf{z}_j)) - \\log (\\sum_{\\mathbf{y}\' \\in \\mathbf{Y}(\\mathbf{Z})} \\prod_{j=1}^n \\phi(y\'_{j-1}, y\'_j, \\mathbf{z}_j) )\r\n\r\n    args:\r\n        tagset_size: target_set_size\r\n        start_tag: ind for <start>\r\n        end_tag: ind for <pad>\r\n        average_batch: whether average the loss among batch\r\n\r\n    """"""\r\n\r\n    def __init__(self, tagset_size, start_tag, end_tag, average_batch=True):\r\n        super(CRFLoss_vb, self).__init__()\r\n        self.tagset_size = tagset_size\r\n        self.start_tag = start_tag\r\n        self.end_tag = end_tag\r\n        self.average_batch = average_batch\r\n\r\n    def forward(self, scores, target, mask):\r\n        """"""\r\n        args:\r\n            scores (seq_len, bat_size, target_size_from, target_size_to) : crf scores\r\n            target (seq_len, bat_size, 1) : golden state\r\n            mask (size seq_len, bat_size) : mask for padding\r\n        return:\r\n            loss\r\n        """"""\r\n\r\n        # calculate batch size and seq len\r\n        seq_len = scores.size(0)\r\n        bat_size = scores.size(1)\r\n\r\n        # calculate sentence score\r\n        tg_energy = torch.gather(scores.view(seq_len, bat_size, -1), 2, target).view(seq_len, bat_size)  # seq_len * bat_size\r\n        tg_energy = tg_energy.masked_select(mask).sum()\r\n\r\n        # calculate forward partition score\r\n\r\n        # build iter\r\n        seq_iter = enumerate(scores)\r\n        # the first score should start with <start>\r\n        _, inivalues = seq_iter.__next__()  # bat_size * from_target_size * to_target_size\r\n        # only need start from start_tag\r\n        partition = inivalues[:, self.start_tag, :].clone()  # bat_size * to_target_size\r\n        # iter over last scores\r\n        for idx, cur_values in seq_iter:\r\n            # previous to_target is current from_target\r\n            # partition: previous results log(exp(from_target)), #(batch_size * from_target)\r\n            # cur_values: bat_size * from_target * to_target\r\n            cur_values = cur_values + partition.contiguous().view(bat_size, self.tagset_size, 1).expand(bat_size, self.tagset_size, self.tagset_size)\r\n            cur_partition = utils.log_sum_exp(cur_values, self.tagset_size)\r\n            # (bat_size * from_target * to_target) -> (bat_size * to_target)\r\n            # partition = utils.switch(partition, cur_partition, mask[idx].view(bat_size, 1).expand(bat_size, self.tagset_size)).view(bat_size, -1)\r\n            mask_idx = mask[idx, :].view(bat_size, 1).expand(bat_size, self.tagset_size)\r\n            partition.masked_scatter_(mask_idx, cur_partition.masked_select(mask_idx))  #0 for partition, 1 for cur_partition\r\n\r\n        #only need end at end_tag\r\n        partition = partition[:, self.end_tag].sum()\r\n        # average = mask.sum()\r\n\r\n        # average_batch\r\n        if self.average_batch:\r\n            loss = (partition - tg_energy) / bat_size\r\n        else:\r\n            loss = (partition - tg_energy)\r\n\r\n        return loss\r\n\r\nclass CRFDecode_vb():\r\n    """"""Batch-mode viterbi decode\r\n\r\n    args:\r\n        tagset_size: target_set_size\r\n        start_tag: ind for <start>\r\n        end_tag: ind for <pad>\r\n        average_batch: whether average the loss among batch\r\n\r\n    """"""\r\n\r\n    def __init__(self, tagset_size, start_tag, end_tag, average_batch=True):\r\n        self.tagset_size = tagset_size\r\n        self.start_tag = start_tag\r\n        self.end_tag = end_tag\r\n        self.average_batch = average_batch\r\n\r\n    def decode(self, scores, mask):\r\n        """"""Find the optimal path with viterbe decode\r\n\r\n        args:\r\n            scores (size seq_len, bat_size, target_size_from, target_size_to) : crf scores\r\n            mask (seq_len, bat_size) : mask for padding\r\n        return:\r\n            decoded sequence (size seq_len, bat_size)\r\n        """"""\r\n        # calculate batch size and seq len\r\n\r\n        seq_len = scores.size(0)\r\n        bat_size = scores.size(1)\r\n\r\n        mask = 1 - mask\r\n        decode_idx = torch.LongTensor(seq_len-1, bat_size)\r\n\r\n        # calculate forward score and checkpoint\r\n\r\n        # build iter\r\n        seq_iter = enumerate(scores)\r\n        # the first score should start with <start>\r\n        _, inivalues = seq_iter.__next__()  # bat_size * from_target_size * to_target_size\r\n        # only need start from start_tag\r\n        forscores = inivalues[:, self.start_tag, :]  # bat_size * to_target_size\r\n        back_points = list()\r\n        # iter over last scores\r\n        for idx, cur_values in seq_iter:\r\n            # previous to_target is current from_target\r\n            # partition: previous results log(exp(from_target)), #(batch_size * from_target)\r\n            # cur_values: bat_size * from_target * to_target\r\n            cur_values = cur_values + forscores.contiguous().view(bat_size, self.tagset_size, 1).expand(bat_size, self.tagset_size, self.tagset_size)\r\n\r\n            forscores, cur_bp = torch.max(cur_values, 1)\r\n            cur_bp.masked_fill_(mask[idx].view(bat_size, 1).expand(bat_size, self.tagset_size), self.end_tag)\r\n            back_points.append(cur_bp)\r\n\r\n        pointer = back_points[-1][:, self.end_tag]\r\n        decode_idx[-1] = pointer\r\n        for idx in range(len(back_points)-2, -1, -1):\r\n            back_point = back_points[idx]\r\n            index = pointer.contiguous().view(-1,1)\r\n            pointer = torch.gather(back_point, 1, index).view(-1)\r\n            decode_idx[idx] = pointer\r\n        return decode_idx\r\n'"
model/evaluator.py,5,"b'""""""\r\n.. module:: evaluator\r\n    :synopsis: evaluation method (f1 score and accuracy)\r\n\r\n.. moduleauthor:: Liyuan Liu, Frank Xu\r\n""""""\r\n\r\n\r\nimport torch\r\nimport numpy as np\r\nimport itertools\r\n\r\nimport model.utils as utils\r\nfrom torch.autograd import Variable\r\n\r\nfrom model.crf import CRFDecode_vb\r\n\r\nclass eval_batch:\r\n    """"""Base class for evaluation, provide method to calculate f1 score and accuracy\r\n\r\n    args:\r\n        packer: provide method to convert target into original space [TODO: need to improve]\r\n        l_map: dictionary for labels\r\n    """"""\r\n\r\n\r\n    def __init__(self, packer, l_map):\r\n        self.packer = packer\r\n        self.l_map = l_map\r\n        self.r_l_map = utils.revlut(l_map)\r\n        self.totalp_counts={}\r\n        self.truep_counts={}\r\n        self.fn_counts={}\r\n        self.fp_counts={}\r\n        self.f1={}\r\n\r\n    def reset(self):\r\n        """"""\r\n        re-set all states\r\n        """"""\r\n        self.correct_labels = 0\r\n        self.total_labels = 0\r\n        self.gold_count = 0\r\n        self.guess_count = 0\r\n        self.overlap_count = 0\r\n        self.totalp_counts={}\r\n        self.truep_counts={}\r\n        self.fn_counts={}\r\n        self.fp_counts={}\r\n        self.f1={}\r\n\r\n    def calc_f1_batch(self, decoded_data, target_data):\r\n        """"""\r\n        update statics for f1 score\r\n\r\n        args:\r\n            decoded_data (batch_size, seq_len): prediction sequence\r\n            target_data (batch_size, seq_len): ground-truth\r\n        """"""\r\n        batch_decoded = torch.unbind(decoded_data, 1)\r\n        batch_targets = torch.unbind(target_data, 0)\r\n\r\n        for decoded, target in zip(batch_decoded, batch_targets):\r\n            gold = self.packer.convert_for_eval(target)\r\n            # remove padding\r\n            length = utils.find_length_from_labels(gold, self.l_map)\r\n            gold = gold[:length]\r\n            best_path = decoded[:length]\r\n\r\n            correct_labels_i, total_labels_i, gold_count_i, guess_count_i, overlap_count_i = self.eval_instance(best_path.numpy(), gold.numpy())\r\n            self.correct_labels += correct_labels_i\r\n            self.total_labels += total_labels_i\r\n            self.gold_count += gold_count_i\r\n            self.guess_count += guess_count_i\r\n            self.overlap_count += overlap_count_i\r\n\r\n    def calc_acc_batch(self, decoded_data, target_data):\r\n        """"""\r\n        update statics for accuracy\r\n\r\n        args:\r\n            decoded_data (batch_size, seq_len): prediction sequence\r\n            target_data (batch_size, seq_len): ground-truth\r\n        """"""\r\n        batch_decoded = torch.unbind(decoded_data, 1)\r\n        batch_targets = torch.unbind(target_data, 0)\r\n\r\n        for decoded, target in zip(batch_decoded, batch_targets):\r\n            gold = self.packer.convert_for_eval(target)\r\n            # remove padding\r\n            length = utils.find_length_from_labels(gold, self.l_map)\r\n            gold = gold[:length].numpy()\r\n            best_path = decoded[:length].numpy()\r\n\r\n            self.total_labels += length\r\n            self.correct_labels += np.sum(np.equal(best_path, gold))\r\n\r\n    def f1_score(self):\r\n        """"""\r\n        calculate f1 score based on statics\r\n        """"""\r\n        if self.guess_count == 0:\r\n            return {\'total\': (0.0, 0.0, 0.0, 0.0, \'\')}\r\n        precision = self.overlap_count / float(self.guess_count)\r\n        recall = self.overlap_count / float(self.gold_count)\r\n        if precision == 0.0 or recall == 0.0:\r\n            return {\'total\': (0.0, 0.0, 0.0, 0.0, \'\')}\r\n        f = 2 * (precision * recall) / (precision + recall)\r\n        accuracy = float(self.correct_labels) / self.total_labels\r\n        message=""""\r\n        self.f1[\'total\'] = (f, precision, recall, accuracy, message)\r\n        for label in self.totalp_counts:\r\n            tp = self.truep_counts.get(label,1)\r\n            fn = sum(self.fn_counts.get(label,{}).values())\r\n            fp = sum(self.fp_counts.get(label,{}).values())\r\n            # print(label, str(tp), str(fp), str(fn), str(self.totalp_counts.get(label,0)))\r\n            precision = tp / float(tp+fp+1e-9)\r\n            recall = tp / float(tp+fn+1e-9)\r\n            f = 2 * (precision * recall) / (precision + recall+1e-9)\r\n            message = str(self.fn_counts.get(label, {}))\r\n            self.f1[label] = (f, precision, recall, 0, message)\r\n        return self.f1\r\n\r\n    def acc_score(self):\r\n        """"""\r\n        calculate accuracy score based on statics\r\n        """"""\r\n        if 0 == self.total_labels:\r\n            return 0.0\r\n        accuracy = float(self.correct_labels) / self.total_labels\r\n        return accuracy\r\n\r\n    def eval_instance(self, best_path, gold):\r\n        """"""\r\n        update statics for one instance\r\n\r\n        args:\r\n            best_path (seq_len): predicted\r\n            gold (seq_len): ground-truth\r\n        """"""\r\n\r\n        total_labels = len(best_path)\r\n        correct_labels = np.sum(np.equal(best_path, gold))\r\n        for i in range(total_labels):\r\n            gold_label = self.r_l_map[gold[i]]\r\n            guessed_label = self.r_l_map[best_path[i]]\r\n            self.totalp_counts[gold_label] = 1 + self.totalp_counts.get(gold_label,0)\r\n            if gold_label == guessed_label:\r\n                self.truep_counts[gold_label] = 1 + self.truep_counts.get(gold_label,0)\r\n            else:\r\n                val = self.fn_counts.get(gold_label,{})\r\n                val[guessed_label] = 1+ val.get(guessed_label,0)\r\n                self.fn_counts[gold_label]=val\r\n\r\n                val2 = self.fp_counts.get(guessed_label,{})\r\n                val2[gold_label] = 1+ val2.get(gold_label,0)\r\n                self.fp_counts[guessed_label] = val2\r\n\r\n        gold_chunks = utils.iobes_to_spans(gold, self.r_l_map)\r\n        gold_count = len(gold_chunks)\r\n\r\n        guess_chunks = utils.iobes_to_spans(best_path, self.r_l_map)\r\n        guess_count = len(guess_chunks)\r\n\r\n        overlap_chunks = gold_chunks & guess_chunks\r\n        overlap_count = len(overlap_chunks)\r\n\r\n        return correct_labels, total_labels, gold_count, guess_count, overlap_count\r\n\r\nclass eval_w(eval_batch):\r\n    """"""evaluation class for word level model (LSTM-CRF)\r\n\r\n    args:\r\n        packer: provide method to convert target into original space [TODO: need to improve]\r\n        l_map: dictionary for labels\r\n        score_type: use f1score with using \'f\'\r\n\r\n    """"""\r\n\r\n    def __init__(self, packer, l_map, score_type):\r\n        eval_batch.__init__(self, packer, l_map)\r\n\r\n        self.decoder = CRFDecode_vb(len(l_map), l_map[\'<start>\'], l_map[\'<pad>\'])\r\n\r\n        if \'f\' in score_type:\r\n            self.eval_b = self.calc_f1_batch\r\n            self.calc_s = self.f1_score\r\n        else:\r\n            self.eval_b = self.calc_acc_batch\r\n            self.calc_s = self.acc_score\r\n\r\n    def calc_score(self, ner_model, dataset_loader):\r\n        """"""\r\n        calculate score for pre-selected metrics\r\n\r\n        args:\r\n            ner_model: LSTM-CRF model\r\n            dataset_loader: loader class for test set\r\n        """"""\r\n        ner_model.eval()\r\n        self.reset()\r\n\r\n        for feature, tg, mask in itertools.chain.from_iterable(dataset_loader):\r\n            fea_v, _, mask_v = self.packer.repack_vb(feature, tg, mask)\r\n            scores, _ = ner_model(fea_v)\r\n            decoded = self.decoder.decode(scores.data, mask_v.data)\r\n            self.eval_b(decoded, tg)\r\n\r\n        return self.calc_s()\r\n\r\nclass eval_wc(eval_batch):\r\n    """"""evaluation class for LM-LSTM-CRF\r\n\r\n    args:\r\n        packer: provide method to convert target into original space [TODO: need to improve]\r\n        l_map: dictionary for labels\r\n        score_type: use f1score with using \'f\'\r\n\r\n    """"""\r\n\r\n    def __init__(self, packer, l_map, score_type):\r\n        eval_batch.__init__(self, packer, l_map)\r\n\r\n        self.decoder = CRFDecode_vb(len(l_map), l_map[\'<start>\'], l_map[\'<pad>\'])\r\n\r\n        if \'f\' in score_type:\r\n            self.eval_b = self.calc_f1_batch\r\n            self.calc_s = self.f1_score\r\n        else:\r\n            self.eval_b = self.calc_acc_batch\r\n            self.calc_s = self.acc_score\r\n\r\n    def calc_score(self, ner_model, dataset_loader):\r\n        """"""\r\n        calculate score for pre-selected metrics\r\n\r\n        args:\r\n            ner_model: LM-LSTM-CRF model\r\n            dataset_loader: loader class for test set\r\n        """"""\r\n        ner_model.eval()\r\n        self.reset()\r\n\r\n        for f_f, f_p, b_f, b_p, w_f, tg, mask_v, len_v in itertools.chain.from_iterable(dataset_loader):\r\n            f_f, f_p, b_f, b_p, w_f, _, mask_v = self.packer.repack_vb(f_f, f_p, b_f, b_p, w_f, tg, mask_v, len_v)\r\n            scores = ner_model(f_f, f_p, b_f, b_p, w_f)\r\n            decoded = self.decoder.decode(scores.data, mask_v.data)\r\n            self.eval_b(decoded, tg)\r\n\r\n        return self.calc_s()\r\n'"
model/highway.py,1,"b'""""""\r\n.. module:: highway\r\n    :synopsis: highway network\r\n \r\n.. moduleauthor:: Liyuan Liu\r\n""""""\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport model.utils as utils\r\n\r\nclass hw(nn.Module):\r\n    """"""Highway layers\r\n\r\n    args: \r\n        size: input and output dimension\r\n        dropout_ratio: dropout ratio\r\n    """"""\r\n   \r\n    def __init__(self, size, num_layers = 1, dropout_ratio = 0.5):\r\n        super(hw, self).__init__()\r\n        self.size = size\r\n        self.num_layers = num_layers\r\n        self.trans = nn.ModuleList()\r\n        self.gate = nn.ModuleList()\r\n        self.dropout = nn.Dropout(p=dropout_ratio)\r\n\r\n        for i in range(num_layers):\r\n            tmptrans = nn.Linear(size, size)\r\n            tmpgate = nn.Linear(size, size)\r\n            self.trans.append(tmptrans)\r\n            self.gate.append(tmpgate)\r\n\r\n    def rand_init(self):\r\n        """"""\r\n        random initialization\r\n        """"""\r\n        for i in range(self.num_layers):\r\n            utils.init_linear(self.trans[i])\r\n            utils.init_linear(self.gate[i])\r\n\r\n    def forward(self, x):\r\n        """"""\r\n        update statics for f1 score\r\n\r\n        args: \r\n            x (ins_num, hidden_dim): input tensor\r\n        return:\r\n            output tensor (ins_num, hidden_dim)\r\n        """"""\r\n        \r\n        \r\n        g = nn.functional.sigmoid(self.gate[0](x))\r\n        h = nn.functional.relu(self.trans[0](x))\r\n        x = g * h + (1 - g) * x\r\n\r\n        for i in range(1, self.num_layers):\r\n            x = self.dropout(x)\r\n            g = nn.functional.sigmoid(self.gate[i](x))\r\n            h = nn.functional.relu(self.trans[i](x))\r\n            x = g * h + (1 - g) * x\r\n\r\n        return x'"
model/lm_lstm_crf.py,9,"b'""""""\r\n.. module:: lm_lstm_crf\r\n    :synopsis: lm_lstm_crf\r\n\r\n.. moduleauthor:: Liyuan Liu\r\n""""""\r\n\r\nimport torch\r\nimport torch.autograd as autograd\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nimport numpy as np\r\nimport model.crf as crf\r\nimport model.utils as utils\r\nimport model.highway as highway\r\n\r\nclass LM_LSTM_CRF(nn.Module):\r\n    """"""LM_LSTM_CRF model\r\n\r\n    args:\r\n        tagset_size: size of label set\r\n        char_size: size of char dictionary\r\n        char_dim: size of char embedding\r\n        char_hidden_dim: size of char-level lstm hidden dim\r\n        char_rnn_layers: number of char-level lstm layers\r\n        embedding_dim: size of word embedding\r\n        word_hidden_dim: size of word-level blstm hidden dim\r\n        word_rnn_layers: number of word-level lstm layers\r\n        vocab_size: size of word dictionary\r\n        dropout_ratio: dropout ratio\r\n        large_CRF: use CRF_L or not, refer model.crf.CRF_L and model.crf.CRF_S for more details\r\n        if_highway: use highway layers or not\r\n        in_doc_words: number of words that occurred in the corpus (used for language model prediction)\r\n        highway_layers: number of highway layers\r\n    """"""\r\n\r\n    def __init__(self, tagset_size, char_size, char_dim, char_hidden_dim, char_rnn_layers, embedding_dim, word_hidden_dim, word_rnn_layers, vocab_size, dropout_ratio, large_CRF=True, if_highway = False, in_doc_words = 2, highway_layers = 1):\r\n\r\n        super(LM_LSTM_CRF, self).__init__()\r\n        self.char_dim = char_dim\r\n        self.char_hidden_dim = char_hidden_dim\r\n        self.char_size = char_size\r\n        self.word_dim = embedding_dim\r\n        self.word_hidden_dim = word_hidden_dim\r\n        self.word_size = vocab_size\r\n        self.if_highway = if_highway\r\n\r\n        self.char_embeds = nn.Embedding(char_size, char_dim)\r\n        self.forw_char_lstm = nn.LSTM(char_dim, char_hidden_dim, num_layers=char_rnn_layers, bidirectional=False, dropout=dropout_ratio)\r\n        self.back_char_lstm = nn.LSTM(char_dim, char_hidden_dim, num_layers=char_rnn_layers, bidirectional=False, dropout=dropout_ratio)\r\n        self.char_rnn_layers = char_rnn_layers\r\n\r\n        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\r\n\r\n        self.word_lstm = nn.LSTM(embedding_dim + char_hidden_dim * 2, word_hidden_dim // 2, num_layers=word_rnn_layers, bidirectional=True, dropout=dropout_ratio)\r\n\r\n        self.word_rnn_layers = word_rnn_layers\r\n\r\n        self.dropout = nn.Dropout(p=dropout_ratio)\r\n\r\n        self.tagset_size = tagset_size\r\n        if large_CRF:\r\n            self.crf = crf.CRF_L(word_hidden_dim, tagset_size)\r\n        else:\r\n            self.crf = crf.CRF_S(word_hidden_dim, tagset_size)\r\n\r\n        if if_highway:\r\n            self.forw2char = highway.hw(char_hidden_dim, num_layers=highway_layers, dropout_ratio=dropout_ratio)\r\n            self.back2char = highway.hw(char_hidden_dim, num_layers=highway_layers, dropout_ratio=dropout_ratio)\r\n            self.forw2word = highway.hw(char_hidden_dim, num_layers=highway_layers, dropout_ratio=dropout_ratio)\r\n            self.back2word = highway.hw(char_hidden_dim, num_layers=highway_layers, dropout_ratio=dropout_ratio)\r\n            self.fb2char = highway.hw(2 * char_hidden_dim, num_layers=highway_layers, dropout_ratio=dropout_ratio)\r\n\r\n        self.char_pre_train_out = nn.Linear(char_hidden_dim, char_size)\r\n        self.word_pre_train_out = nn.Linear(char_hidden_dim, in_doc_words)\r\n\r\n        self.batch_size = 1\r\n        self.word_seq_length = 1\r\n\r\n    def set_batch_size(self, bsize):\r\n        """"""\r\n        set batch size\r\n        """"""\r\n        self.batch_size = bsize\r\n\r\n    def set_batch_seq_size(self, sentence):\r\n        """"""\r\n        set batch size and sequence length\r\n        """"""\r\n        tmp = sentence.size()\r\n        self.word_seq_length = tmp[0]\r\n        self.batch_size = tmp[1]\r\n\r\n    def rand_init_embedding(self):\r\n        """"""\r\n        random initialize char-level embedding\r\n        """"""\r\n        utils.init_embedding(self.char_embeds.weight)\r\n\r\n    def load_pretrained_word_embedding(self, pre_word_embeddings):\r\n        """"""\r\n        load pre-trained word embedding\r\n\r\n        args:\r\n            pre_word_embeddings (self.word_size, self.word_dim) : pre-trained embedding\r\n        """"""\r\n        assert (pre_word_embeddings.size()[1] == self.word_dim)\r\n        self.word_embeds.weight = nn.Parameter(pre_word_embeddings)\r\n\r\n    def rand_init(self, init_char_embedding=True, init_word_embedding=False):\r\n        """"""\r\n        random initialization\r\n\r\n        args:\r\n            init_char_embedding: random initialize char embedding or not\r\n            init_word_embedding: random initialize word embedding or not\r\n        """"""\r\n\r\n        if init_char_embedding:\r\n            utils.init_embedding(self.char_embeds.weight)\r\n        if init_word_embedding:\r\n            utils.init_embedding(self.word_embeds.weight)\r\n        if self.if_highway:\r\n            self.forw2char.rand_init()\r\n            self.back2char.rand_init()\r\n            self.forw2word.rand_init()\r\n            self.back2word.rand_init()\r\n            self.fb2char.rand_init()\r\n        utils.init_lstm(self.forw_char_lstm)\r\n        utils.init_lstm(self.back_char_lstm)\r\n        utils.init_lstm(self.word_lstm)\r\n        utils.init_linear(self.char_pre_train_out)\r\n        utils.init_linear(self.word_pre_train_out)\r\n        self.crf.rand_init()\r\n\r\n    def word_pre_train_forward(self, sentence, position, hidden=None):\r\n        """"""\r\n        output of forward language model\r\n\r\n        args:\r\n            sentence (char_seq_len, batch_size): char-level representation of sentence\r\n            position (word_seq_len, batch_size): position of blank space in char-level representation of sentence\r\n            hidden: initial hidden state\r\n\r\n        return:\r\n            language model output (word_seq_len, in_doc_word), hidden\r\n        """"""\r\n\r\n        embeds = self.char_embeds(sentence)\r\n        d_embeds = self.dropout(embeds)\r\n        lstm_out, hidden = self.forw_char_lstm(d_embeds)\r\n\r\n        tmpsize = position.size()\r\n        position = position.unsqueeze(2).expand(tmpsize[0], tmpsize[1], self.char_hidden_dim)\r\n        select_lstm_out = torch.gather(lstm_out, 0, position)\r\n        d_lstm_out = self.dropout(select_lstm_out).view(-1, self.char_hidden_dim)\r\n\r\n        if self.if_highway:\r\n            char_out = self.forw2word(d_lstm_out)\r\n            d_char_out = self.dropout(char_out)\r\n        else:\r\n            d_char_out = d_lstm_out\r\n\r\n        pre_score = self.word_pre_train_out(d_char_out)\r\n        return pre_score, hidden\r\n\r\n    def word_pre_train_backward(self, sentence, position, hidden=None):\r\n        """"""\r\n        output of backward language model\r\n\r\n        args:\r\n            sentence (char_seq_len, batch_size): char-level representation of sentence (inverse order)\r\n            position (word_seq_len, batch_size): position of blank space in inversed char-level representation of sentence\r\n            hidden: initial hidden state\r\n\r\n        return:\r\n            language model output (word_seq_len, in_doc_word), hidden\r\n        """"""\r\n        embeds = self.char_embeds(sentence)\r\n        d_embeds = self.dropout(embeds)\r\n        lstm_out, hidden = self.back_char_lstm(d_embeds)\r\n\r\n        tmpsize = position.size()\r\n        position = position.unsqueeze(2).expand(tmpsize[0], tmpsize[1], self.char_hidden_dim)\r\n        select_lstm_out = torch.gather(lstm_out, 0, position)\r\n        d_lstm_out = self.dropout(select_lstm_out).view(-1, self.char_hidden_dim)\r\n\r\n        if self.if_highway:\r\n            char_out = self.back2word(d_lstm_out)\r\n            d_char_out = self.dropout(char_out)\r\n        else:\r\n            d_char_out = d_lstm_out\r\n\r\n        pre_score = self.word_pre_train_out(d_char_out)\r\n        return pre_score, hidden\r\n\r\n    def forward(self, forw_sentence, forw_position, back_sentence, back_position, word_seq, hidden=None):\r\n        \'\'\'\r\n        args:\r\n            forw_sentence (char_seq_len, batch_size) : char-level representation of sentence\r\n            forw_position (word_seq_len, batch_size) : position of blank space in char-level representation of sentence\r\n            back_sentence (char_seq_len, batch_size) : char-level representation of sentence (inverse order)\r\n            back_position (word_seq_len, batch_size) : position of blank space in inversed char-level representation of sentence\r\n            word_seq (word_seq_len, batch_size) : word-level representation of sentence\r\n            hidden: initial hidden state\r\n\r\n        return:\r\n            crf output (word_seq_len, batch_size, tag_size, tag_size), hidden\r\n        \'\'\'\r\n\r\n        self.set_batch_seq_size(forw_position)\r\n\r\n        #embedding layer\r\n        forw_emb = self.char_embeds(forw_sentence)\r\n        back_emb = self.char_embeds(back_sentence)\r\n\r\n        #dropout\r\n        d_f_emb = self.dropout(forw_emb)\r\n        d_b_emb = self.dropout(back_emb)\r\n\r\n        #forward the whole sequence\r\n        forw_lstm_out, _ = self.forw_char_lstm(d_f_emb)#seq_len_char * batch * char_hidden_dim\r\n\r\n        back_lstm_out, _ = self.back_char_lstm(d_b_emb)#seq_len_char * batch * char_hidden_dim\r\n\r\n        #select predict point\r\n        forw_position = forw_position.unsqueeze(2).expand(self.word_seq_length, self.batch_size, self.char_hidden_dim)\r\n        select_forw_lstm_out = torch.gather(forw_lstm_out, 0, forw_position)\r\n\r\n        back_position = back_position.unsqueeze(2).expand(self.word_seq_length, self.batch_size, self.char_hidden_dim)\r\n        select_back_lstm_out = torch.gather(back_lstm_out, 0, back_position)\r\n\r\n        fb_lstm_out = self.dropout(torch.cat((select_forw_lstm_out, select_back_lstm_out), dim=2))\r\n        if self.if_highway:\r\n            char_out = self.fb2char(fb_lstm_out)\r\n            d_char_out = self.dropout(char_out)\r\n        else:\r\n            d_char_out = fb_lstm_out\r\n\r\n        #word\r\n        word_emb = self.word_embeds(word_seq)\r\n        d_word_emb = self.dropout(word_emb)\r\n\r\n        #combine\r\n        word_input = torch.cat((d_word_emb, d_char_out), dim = 2)\r\n\r\n        #word level lstm\r\n        lstm_out, _ = self.word_lstm(word_input)\r\n        d_lstm_out = self.dropout(lstm_out)\r\n\r\n        #convert to crf\r\n        crf_out = self.crf(d_lstm_out)\r\n        crf_out = crf_out.view(self.word_seq_length, self.batch_size, self.tagset_size, self.tagset_size)\r\n\r\n        return crf_out'"
model/lstm_crf.py,4,"b'""""""\r\n.. module:: lstm_crf\r\n    :synopsis: lstm_crf\r\n \r\n.. moduleauthor:: Liyuan Liu\r\n""""""\r\n\r\nimport torch\r\nimport torch.autograd as autograd\r\nimport torch.nn as nn\r\nimport model.crf as crf\r\nimport model.utils as utils\r\n\r\n\r\nclass LSTM_CRF(nn.Module):\r\n    """"""LSTM_CRF model\r\n\r\n    args: \r\n        vocab_size: size of word dictionary\r\n        tagset_size: size of label set\r\n        embedding_dim: size of word embedding\r\n        hidden_dim: size of word-level blstm hidden dim\r\n        rnn_layers: number of word-level lstm layers\r\n        dropout_ratio: dropout ratio\r\n        large_CRF: use CRF_L or not, refer model.crf.CRF_L and model.crf.CRF_S for more details\r\n    """"""\r\n    \r\n    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, rnn_layers, dropout_ratio, large_CRF=True):\r\n        super(LSTM_CRF, self).__init__()\r\n        self.embedding_dim = embedding_dim\r\n        self.hidden_dim = hidden_dim\r\n        self.vocab_size = vocab_size\r\n\r\n        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\r\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\r\n                            num_layers=rnn_layers, bidirectional=True, dropout=dropout_ratio)\r\n        self.rnn_layers = rnn_layers\r\n    \r\n        self.dropout1 = nn.Dropout(p=dropout_ratio)\r\n        self.dropout2 = nn.Dropout(p=dropout_ratio)\r\n\r\n        self.tagset_size = tagset_size\r\n        if large_CRF:\r\n            self.crf = crf.CRF_L(hidden_dim, tagset_size)\r\n        else:\r\n            self.crf = crf.CRF_S(hidden_dim, tagset_size)\r\n\r\n        self.batch_size = 1\r\n        self.seq_length = 1\r\n\r\n    def rand_init_hidden(self):\r\n        """"""\r\n        random initialize hidden variable\r\n        """"""\r\n        return autograd.Variable(\r\n            torch.randn(2 * self.rnn_layers, self.batch_size, self.hidden_dim // 2)), autograd.Variable(\r\n            torch.randn(2 * self.rnn_layers, self.batch_size, self.hidden_dim // 2))\r\n\r\n    def set_batch_size(self, bsize):\r\n        """"""\r\n        set batch size\r\n        """"""\r\n        self.batch_size = bsize\r\n\r\n    def set_batch_seq_size(self, sentence):\r\n        """"""\r\n        set batch size and sequence length\r\n        """"""        \r\n        tmp = sentence.size()\r\n        self.seq_length = tmp[0]\r\n        self.batch_size = tmp[1]\r\n\r\n    def load_pretrained_embedding(self, pre_embeddings):\r\n        """"""\r\n        load pre-trained word embedding\r\n\r\n        args:\r\n            pre_word_embeddings (self.word_size, self.word_dim) : pre-trained embedding\r\n        """"""\r\n        assert (pre_embeddings.size()[1] == self.embedding_dim)\r\n        self.word_embeds.weight = nn.Parameter(pre_embeddings)\r\n\r\n    def rand_init_embedding(self):\r\n        utils.init_embedding(self.word_embeds.weight)\r\n\r\n    def rand_init(self, init_embedding=False):\r\n        """"""\r\n        random initialization\r\n\r\n        args:\r\n            init_embedding: random initialize embedding or not\r\n        """"""\r\n        if init_embedding:\r\n            utils.init_embedding(self.word_embeds.weight)\r\n        utils.init_lstm(self.lstm)\r\n        self.crf.rand_init()\r\n\r\n    def forward(self, sentence, hidden=None):\r\n        \'\'\'\r\n        args:\r\n            sentence (word_seq_len, batch_size) : word-level representation of sentence\r\n            hidden: initial hidden state\r\n\r\n        return:\r\n            crf output (word_seq_len, batch_size, tag_size, tag_size), hidden\r\n        \'\'\'\r\n        self.set_batch_seq_size(sentence)\r\n\r\n        embeds = self.word_embeds(sentence)\r\n        d_embeds = self.dropout1(embeds)\r\n\r\n        lstm_out, hidden = self.lstm(d_embeds, hidden)\r\n        lstm_out = lstm_out.view(-1, self.hidden_dim)\r\n\r\n        d_lstm_out = self.dropout2(lstm_out)\r\n\r\n        crf_out = self.crf(d_lstm_out)\r\n        crf_out = crf_out.view(self.seq_length, self.batch_size, self.tagset_size, self.tagset_size)\r\n        return crf_out, hidden'"
model/ner_dataset.py,1,"b'""""""\r\n.. module:: datasets\r\n    :synopsis: datasets\r\n \r\n.. moduleauthor:: Liyuan Liu\r\n""""""\r\n\r\nfrom torch.utils.data import Dataset\r\n\r\n\r\nclass CRFDataset(Dataset):\r\n    """"""Dataset Class for word-level model \r\n\r\n    args: \r\n        data_tensor (ins_num, seq_length): words \r\n        label_tensor (ins_num, seq_length): labels\r\n        mask_tensor (ins_num, seq_length): padding masks\r\n    """"""\r\n    def __init__(self, data_tensor, label_tensor, mask_tensor):\r\n        assert data_tensor.size(0) == label_tensor.size(0)\r\n        assert data_tensor.size(0) == mask_tensor.size(0)\r\n        self.data_tensor = data_tensor\r\n        self.label_tensor = label_tensor\r\n        self.mask_tensor = mask_tensor\r\n\r\n    def __getitem__(self, index):\r\n        return self.data_tensor[index], self.label_tensor[index], self.mask_tensor[index]\r\n\r\n    def __len__(self):\r\n        return self.data_tensor.size(0)\r\n\r\nclass CRFDataset_WC(Dataset):\r\n    """"""Dataset Class for char-aware model \r\n\r\n    args: \r\n        forw_tensor (ins_num, seq_length): forward chars\r\n        forw_index (ins_num, seq_length): index of forward chars\r\n        back_tensor (ins_num, seq_length): backward chars\r\n        back_index (ins_num, seq_length): index of backward chars\r\n        word_tensor (ins_num, seq_length): words\r\n        label_tensor (ins_num, seq_length): labels:\r\n        mask_tensor (ins_num, seq_length): padding masks\r\n        len_tensor (ins_num, 2): length of chars (dim0) and words (dim1)\r\n    """"""\r\n    def __init__(self, forw_tensor, forw_index, back_tensor, back_index, word_tensor, label_tensor, mask_tensor, len_tensor):\r\n        assert forw_tensor.size(0) == label_tensor.size(0)\r\n        assert forw_tensor.size(0) == mask_tensor.size(0)\r\n        assert forw_tensor.size(0) == forw_index.size(0)\r\n        assert forw_tensor.size(0) == back_tensor.size(0)\r\n        assert forw_tensor.size(0) == back_index.size(0)\r\n        assert forw_tensor.size(0) == word_tensor.size(0)\r\n        assert forw_tensor.size(0) == len_tensor.size(0)\r\n        self.forw_tensor = forw_tensor\r\n        self.forw_index = forw_index\r\n        self.back_tensor = back_tensor\r\n        self.back_index = back_index\r\n        self.word_tensor = word_tensor\r\n        self.label_tensor = label_tensor\r\n        self.mask_tensor = mask_tensor\r\n        self.len_tensor = len_tensor\r\n\r\n    def __getitem__(self, index):\r\n        return self.forw_tensor[index], self.forw_index[index], self.back_tensor[index], self.back_index[index], self.word_tensor[index], self.label_tensor[index], self.mask_tensor[index], self.len_tensor[index]\r\n\r\n    def __len__(self):\r\n        return self.forw_tensor.size(0)\r\n'"
model/predictor.py,10,"b'""""""\n.. module:: predictor\n    :synopsis: prediction method (for un-annotated text)\n \n.. moduleauthor:: Liyuan Liu\n""""""\n\nimport torch\nimport torch.autograd as autograd\nimport numpy as np\nimport itertools\nimport sys\nfrom tqdm import tqdm\n\nfrom model.crf import CRFDecode_vb\nfrom model.utils import *\n\nclass predict:\n    """"""Base class for prediction, provide method to calculate f1 score and accuracy \n\n    args: \n        if_cuda: if use cuda to speed up \n        l_map: dictionary for labels \n        label_seq: type of decode function, set `True` to couple label with text, or set \'False\' to insert label into test\n        batch_size: size of batch in decoding\n    """"""\n\n    def __init__(self, if_cuda, l_map, label_seq = True, batch_size = 50):\n        self.if_cuda = if_cuda\n        self.l_map = l_map\n        self.r_l_map = revlut(l_map)\n        self.batch_size = batch_size\n        if label_seq:\n            self.decode_str = self.decode_l\n        else:\n            self.decode_str = self.decode_s\n\n    def decode_l(self, feature, label):\n        """"""\n        decode a sentence coupled with label\n\n        args:\n            feature (list): words list\n            label (list): label list\n        """"""\n        return \'\\n\'.join(map(lambda t: t[0] + \' \'+ self.r_l_map[t[1].item()], zip(feature, label)))\n\n    def decode_s(self, feature, label):\n        """"""\n        decode a sentence in the format of <>\n\n        args:\n            feature (list): words list\n            label (list): label list\n        """"""\n        chunks = """"\n        current = None\n\n        for f, y in zip(feature, label):\n            label = self.r_l_map[y.item()]\n\n            if label.startswith(\'B-\'):\n\n                if current is not None:\n                    chunks += ""</""+current+""> ""\n                current = label[2:]\n                chunks += ""<""+current+""> "" + f + "" ""\n\n            elif label.startswith(\'S-\'):\n\n                if current is not None:\n                    chunks += "" </""+current+""> ""\n                current = label[2:]\n                chunks += ""<""+current+""> "" + f + "" </""+current+""> ""\n                current = None\n\n            elif label.startswith(\'I-\'):\n\n                if current is not None:\n                    base = label[2:]\n                    if base == current:\n                        chunks += f+"" ""\n                    else:\n                        chunks += ""</""+current+""> <""+base+""> "" + f + "" ""\n                        current = base\n                else:\n                    current = label[2:]\n                    chunks += ""<""+current+""> "" + f + "" ""\n\n            elif label.startswith(\'E-\'):\n\n                if current is not None:\n                    base = label[2:]\n                    if base == current:\n                        chunks += f + "" </""+base+""> ""\n                        current = None\n                    else:\n                        chunks += ""</""+current+""> <""+base+""> "" + f + "" </""+base+""> ""\n                        current = None\n\n                else:\n                    current = label[2:]\n                    chunks += ""<""+current+""> "" + f + "" </""+current+""> ""\n                    current = None\n\n            else:\n                if current is not None:\n                    chunks += ""</""+current+""> ""\n                chunks += f+"" ""\n                current = None\n\n        if current is not None:\n            chunks += ""</""+current+""> ""\n\n        return chunks\n\n    def output_batch(self, ner_model, documents, fout):\n        """"""\n        decode the whole corpus in the specific format by calling apply_model to fit specific models\n\n        args:\n            ner_model: sequence labeling model\n            feature (list): list of words list\n            fout: output file\n        """"""\n        ner_model.eval()\n\n        d_len = len(documents)\n        for d_ind in tqdm( range(0, d_len), mininterval=1,\n                desc=\' - Process\', leave=False, file=sys.stdout):\n            fout.write(\'-DOCSTART- -DOCSTART- -DOCSTART-\\n\\n\')\n            features = documents[d_ind]\n            f_len = len(features)\n            for ind in range(0, f_len, self.batch_size):\n                eind = min(f_len, ind + self.batch_size)\n                labels = self.apply_model(ner_model, features[ind: eind])\n                labels = torch.unbind(labels, 1)\n\n                for ind2 in range(ind, eind):\n                    f = features[ind2]\n                    l = labels[ind2 - ind][0: len(f) ]\n                    fout.write(self.decode_str(features[ind2], l) + \'\\n\\n\')\n\n    def apply_model(self, ner_model, features):\n        """"""\n        template function for apply_model\n\n        args:\n            ner_model: sequence labeling model\n            feature (list): list of words list\n        """"""\n        return None\n\nclass predict_w(predict):\n    """"""prediction class for word level model (LSTM-CRF)\n\n    args: \n        if_cuda: if use cuda to speed up \n        f_map: dictionary for words\n        l_map: dictionary for labels\n        pad_word: word padding\n        pad_label: label padding\n        start_label: start label \n        label_seq: type of decode function, set `True` to couple label with text, or set \'False\' to insert label into test\n        batch_size: size of batch in decoding\n        caseless: caseless or not\n    """"""\n   \n    def __init__(self, if_cuda, f_map, l_map, pad_word, pad_label, start_label, label_seq = True, batch_size = 50, caseless=True):\n        predict.__init__(self, if_cuda, l_map, label_seq, batch_size)\n        self.decoder = CRFDecode_vb(len(l_map), start_label, pad_label)\n        self.pad_word = pad_word\n        self.f_map = f_map\n        self.l_map = l_map\n        self.caseless = caseless\n        \n    def apply_model(self, ner_model, features):\n        """"""\n        apply_model function for LSTM-CRF\n\n        args:\n            ner_model: sequence labeling model\n            feature (list): list of words list\n        """"""\n        if self.caseless:\n            features = list(map(lambda t: list(map(lambda x: x.lower(), t)), features))\n        features = encode_safe(features, self.f_map, self.f_map[\'<unk>\'])\n        f_len = max(map(lambda t: len(t) + 1, features))\n\n        masks = torch.ByteTensor(list(map(lambda t: [1] * (len(t) + 1) + [0] * (f_len - len(t) - 1), features)))\n        word_features = torch.LongTensor(list(map(lambda t: t + [self.pad_word] * (f_len - len(t)), features)))\n\n        if self.if_cuda:\n            fea_v = autograd.Variable(word_features.transpose(0, 1)).cuda()\n            mask_v = masks.transpose(0, 1).cuda()\n        else:\n            fea_v = autograd.Variable(word_features.transpose(0, 1))\n            mask_v = masks.transpose(0, 1).contiguous()\n\n        scores, _ = ner_model(fea_v)\n        decoded = self.decoder.decode(scores.data, mask_v)\n\n        return decoded\n\nclass predict_wc(predict):\n    """"""prediction class for LM-LSTM-CRF\n\n    args: \n        if_cuda: if use cuda to speed up \n        f_map: dictionary for words\n        c_map: dictionary for chars\n        l_map: dictionary for labels\n        pad_word: word padding\n        pad_char: word padding\n        pad_label: label padding\n        start_label: start label \n        label_seq: type of decode function, set `True` to couple label with text, or set \'False\' to insert label into test\n        batch_size: size of batch in decoding\n        caseless: caseless or not\n    """"""\n   \n    def __init__(self, if_cuda, f_map, c_map, l_map, pad_word, pad_char, pad_label, start_label, label_seq = True, batch_size = 50, caseless=True):\n        predict.__init__(self, if_cuda, l_map, label_seq, batch_size)\n        self.decoder = CRFDecode_vb(len(l_map), start_label, pad_label)\n        self.pad_word = pad_word\n        self.pad_char = pad_char\n        self.f_map = f_map\n        self.c_map = c_map\n        self.l_map = l_map\n        self.caseless = caseless\n        \n    def apply_model(self, ner_model, features):\n        """"""\n        apply_model function for LM-LSTM-CRF\n\n        args:\n            ner_model: sequence labeling model\n            feature (list): list of words list\n        """"""\n        char_features = encode2char_safe(features, self.c_map)\n\n        if self.caseless:\n            word_features = encode_safe(list(map(lambda t: list(map(lambda x: x.lower(), t)), features)), self.f_map, self.f_map[\'<unk>\'])\n        else:\n            word_features = encode_safe(features, self.f_map, self.f_map[\'<unk>\'])\n\n        fea_len = [list( map( lambda t: len(t) + 1, f) ) for f in char_features]\n        forw_features = concatChar(char_features, self.c_map)\n\n        word_len = max(map(lambda t: len(t) + 1, word_features))\n        char_len = max(map(lambda t: len(t[0]) + word_len - len(t[1]), zip(forw_features, word_features)))\n        forw_t = list( map( lambda t: t + [self.pad_char] * ( char_len - len(t) ), forw_features ) )\n        back_t = torch.LongTensor( list( map( lambda t: t[::-1], forw_t ) ) )\n        forw_t = torch.LongTensor( forw_t )\n        forw_p = torch.LongTensor( list( map( lambda t: list(itertools.accumulate( t + [1] * (word_len - len(t) ) ) ), fea_len) ) )\n        back_p = torch.LongTensor( list( map( lambda t: [char_len - 1] + [ char_len - 1 - tup for tup in t[:-1] ], forw_p) ) )\n\n        masks = torch.ByteTensor(list(map(lambda t: [1] * (len(t) + 1) + [0] * (word_len - len(t) - 1), word_features)))\n        word_t = torch.LongTensor(list(map(lambda t: t + [self.pad_word] * (word_len - len(t)), word_features)))\n\n        if self.if_cuda:\n            f_f = autograd.Variable(forw_t.transpose(0, 1)).cuda()\n            f_p = autograd.Variable(forw_p.transpose(0, 1)).cuda()\n            b_f = autograd.Variable(back_t.transpose(0, 1)).cuda()\n            b_p = autograd.Variable(back_p.transpose(0, 1)).cuda()\n            w_f = autograd.Variable(word_t.transpose(0, 1)).cuda()\n            mask_v = masks.transpose(0, 1).cuda()\n        else:\n            f_f = autograd.Variable(forw_t.transpose(0, 1))\n            f_p = autograd.Variable(forw_p.transpose(0, 1))\n            b_f = autograd.Variable(back_t.transpose(0, 1))\n            b_p = autograd.Variable(back_p.transpose(0, 1))\n            w_f = autograd.Variable(word_t.transpose(0, 1))\n            mask_v = masks.transpose(0, 1)\n\n        scores = ner_model(f_f, f_p, b_f, b_p, w_f)\n        decoded = self.decoder.decode(scores.data, mask_v)\n\n        return decoded\n'"
model/utils.py,27,"b'""""""\r\n.. module:: utils\r\n    :synopsis: utility tools\r\n\r\n.. moduleauthor:: Liyuan Liu, Frank Xu\r\n""""""\r\n\r\nimport codecs\r\nimport csv\r\nimport itertools\r\nfrom functools import reduce\r\n\r\nimport numpy as np\r\nimport shutil\r\nimport torch\r\nimport json\r\n\r\nimport torch.nn as nn\r\nimport torch.nn.init\r\n\r\nfrom model.ner_dataset import *\r\n\r\nzip = getattr(itertools, \'izip\', zip)\r\n\r\n\r\ndef to_scalar(var):\r\n    """"""change the first element of a tensor to scalar\r\n    """"""\r\n    return var.view(-1).data.tolist()[0]\r\n\r\n\r\ndef argmax(vec):\r\n    """"""helper function to calculate argmax of input vector at dimension 1\r\n    """"""\r\n    _, idx = torch.max(vec, 1)\r\n    return to_scalar(idx)\r\n\r\n\r\ndef log_sum_exp(vec, m_size):\r\n    """"""\r\n    calculate log of exp sum\r\n\r\n    args:\r\n        vec (batch_size, vanishing_dim, hidden_dim) : input tensor\r\n        m_size : hidden_dim\r\n    return:\r\n        batch_size, hidden_dim\r\n    """"""\r\n    _, idx = torch.max(vec, 1)  # B * 1 * M\r\n    max_score = torch.gather(vec, 1, idx.view(-1, 1, m_size)).view(-1, 1, m_size)  # B * M\r\n\r\n    return max_score.view(-1, m_size) + torch.log(torch.sum(torch.exp(vec - max_score.expand_as(vec)), 1)).view(-1, m_size)  # B * M\r\n\r\n\r\ndef switch(vec1, vec2, mask):\r\n    """"""\r\n    switch function for pytorch\r\n\r\n    args:\r\n        vec1 (any size) : input tensor corresponding to 0\r\n        vec2 (same to vec1) : input tensor corresponding to 1\r\n        mask (same to vec1) : input tensor, each element equals to 0/1\r\n    return:\r\n        vec (*)\r\n    """"""\r\n    catvec = torch.cat([vec1.view(-1, 1), vec2.view(-1, 1)], dim=1)\r\n    switched_vec = torch.gather(catvec, 1, mask.long().view(-1, 1))\r\n    return switched_vec.view(-1)\r\n\r\n\r\ndef encode2char_safe(input_lines, char_dict):\r\n    """"""\r\n    get char representation of lines\r\n\r\n    args:\r\n        input_lines (list of strings) : input corpus\r\n        char_dict (dictionary) : char-level dictionary\r\n    return:\r\n        forw_lines\r\n    """"""\r\n    unk = char_dict[\'<u>\']\r\n    forw_lines = [list(map(lambda m: list(map(lambda t: char_dict.get(t, unk), m)), line)) for line in input_lines]\r\n    return forw_lines\r\n\r\n\r\ndef concatChar(input_lines, char_dict):\r\n    """"""\r\n    concat char into string\r\n\r\n    args:\r\n        input_lines (list of list of char) : input corpus\r\n        char_dict (dictionary) : char-level dictionary\r\n    return:\r\n        forw_lines\r\n    """"""\r\n    features = [[char_dict[\' \']] + list(reduce(lambda x, y: x + [char_dict[\' \']] + y, sentence)) + [char_dict[\'\\n\']] for sentence in input_lines]\r\n    return features\r\n\r\n\r\ndef encode_safe(input_lines, word_dict, unk):\r\n    """"""\r\n    encode list of strings into word-level representation with unk\r\n    """"""\r\n    lines = list(map(lambda t: list(map(lambda m: word_dict.get(m, unk), t)), input_lines))\r\n    return lines\r\n\r\n\r\ndef encode(input_lines, word_dict):\r\n    """"""\r\n    encode list of strings into word-level representation\r\n    """"""\r\n    lines = list(map(lambda t: list(map(lambda m: word_dict[m], t)), input_lines))\r\n    return lines\r\n\r\n\r\ndef encode2Tensor(input_lines, word_dict, unk):\r\n    """"""\r\n    encode list of strings into word-level representation (tensor) with unk\r\n    """"""\r\n    lines = list(map(lambda t: torch.LongTensor(list(map(lambda m: word_dict.get(m, unk), t))), input_lines))\r\n    return lines\r\n\r\n\r\ndef generate_corpus_char(lines, if_shrink_c_feature=False, c_thresholds=1, if_shrink_w_feature=False, w_thresholds=1):\r\n    """"""\r\n    generate label, feature, word dictionary, char dictionary and label dictionary\r\n\r\n    args:\r\n        lines : corpus\r\n        if_shrink_c_feature: whether shrink char-dictionary\r\n        c_threshold: threshold for shrinking char-dictionary\r\n        if_shrink_w_feature: whether shrink word-dictionary\r\n        w_threshold: threshold for shrinking word-dictionary\r\n\r\n    """"""\r\n    features, labels, feature_map, label_map = generate_corpus(lines, if_shrink_feature=if_shrink_w_feature, thresholds=w_thresholds)\r\n    char_count = dict()\r\n    for feature in features:\r\n        for word in feature:\r\n            for tup in word:\r\n                if tup not in char_count:\r\n                    char_count[tup] = 0\r\n                else:\r\n                    char_count[tup] += 1\r\n    if if_shrink_c_feature:\r\n        shrink_char_count = [k for (k, v) in iter(char_count.items()) if v >= c_thresholds]\r\n        char_map = {shrink_char_count[ind]: ind for ind in range(0, len(shrink_char_count))}\r\n    else:\r\n        char_map = {k: v for (v, k) in enumerate(char_count.keys())}\r\n    char_map[\'<u>\'] = len(char_map)  # unk for char\r\n    char_map[\' \'] = len(char_map)  # concat for char\r\n    char_map[\'\\n\'] = len(char_map)  # eof for char\r\n    return features, labels, feature_map, label_map, char_map\r\n\r\ndef shrink_features(feature_map, features, thresholds):\r\n    """"""\r\n    filter un-common features by threshold\r\n    """"""\r\n    feature_count = {k: 0 for (k, v) in iter(feature_map.items())}\r\n    for feature_list in features:\r\n        for feature in feature_list:\r\n            feature_count[feature] += 1\r\n    shrinked_feature_count = [k for (k, v) in iter(feature_count.items()) if v >= thresholds]\r\n    feature_map = {shrinked_feature_count[ind]: (ind + 1) for ind in range(0, len(shrinked_feature_count))}\r\n\r\n    #inserting unk to be 0 encoded\r\n    feature_map[\'<unk>\'] = 0\r\n    #inserting eof\r\n    feature_map[\'<eof>\'] = len(feature_map)\r\n    return feature_map\r\n\r\ndef generate_corpus(lines, if_shrink_feature=False, thresholds=1):\r\n    """"""\r\n    generate label, feature, word dictionary and label dictionary\r\n\r\n    args:\r\n        lines : corpus\r\n        if_shrink_feature: whether shrink word-dictionary\r\n        threshold: threshold for shrinking word-dictionary\r\n\r\n    """"""\r\n    features = list()\r\n    labels = list()\r\n    tmp_fl = list()\r\n    tmp_ll = list()\r\n    feature_map = dict()\r\n    label_map = dict()\r\n    for line in lines:\r\n        if not (line.isspace() or (len(line) > 10 and line[0:10] == \'-DOCSTART-\')):\r\n            line = line.rstrip(\'\\n\').split()\r\n            tmp_fl.append(line[0])\r\n            if line[0] not in feature_map:\r\n                feature_map[line[0]] = len(feature_map) + 1 #0 is for unk\r\n            tmp_ll.append(line[-1])\r\n            if line[-1] not in label_map:\r\n                label_map[line[-1]] = len(label_map)\r\n        elif len(tmp_fl) > 0:\r\n            features.append(tmp_fl)\r\n            labels.append(tmp_ll)\r\n            tmp_fl = list()\r\n            tmp_ll = list()\r\n    if len(tmp_fl) > 0:\r\n        features.append(tmp_fl)\r\n        labels.append(tmp_ll)\r\n    label_map[\'<start>\'] = len(label_map)\r\n    label_map[\'<pad>\'] = len(label_map)\r\n    if if_shrink_feature:\r\n        feature_map = shrink_features(feature_map, features, thresholds)\r\n    else:\r\n        #inserting unk to be 0 encoded\r\n        feature_map[\'<unk>\'] = 0\r\n        #inserting eof\r\n        feature_map[\'<eof>\'] = len(feature_map)\r\n\r\n    return features, labels, feature_map, label_map\r\n\r\n\r\ndef read_corpus(lines):\r\n    """"""\r\n    convert corpus into features and labels\r\n    """"""\r\n    features = list()\r\n    labels = list()\r\n    tmp_fl = list()\r\n    tmp_ll = list()\r\n    for line in lines:\r\n        if not (line.isspace() or (len(line) > 10 and line[0:10] == \'-DOCSTART-\')):\r\n            line = line.rstrip(\'\\n\').split()\r\n            tmp_fl.append(line[0])\r\n            tmp_ll.append(line[-1])\r\n        elif len(tmp_fl) > 0:\r\n            features.append(tmp_fl)\r\n            labels.append(tmp_ll)\r\n            tmp_fl = list()\r\n            tmp_ll = list()\r\n    if len(tmp_fl) > 0:\r\n        features.append(tmp_fl)\r\n        labels.append(tmp_ll)\r\n\r\n    return features, labels\r\n\r\ndef read_features(lines, multi_docs = True):\r\n    """"""\r\n    convert un-annotated corpus into features\r\n    """"""\r\n    if multi_docs:\r\n        documents = list()\r\n        features = list()\r\n        tmp_fl = list()\r\n        for line in lines:\r\n            if_doc_end = (len(line) > 10 and line[0:10] == \'-DOCSTART-\')\r\n            if not (line.isspace() or if_doc_end):\r\n                line = line.split()[0]\r\n                tmp_fl.append(line)\r\n            else:\r\n                if len(tmp_fl) > 0:\r\n                    features.append(tmp_fl)\r\n                    tmp_fl = list()\r\n                if if_doc_end and len(features) > 0:\r\n                    documents.append(features)\r\n                    features = list()\r\n        if len(tmp_fl) > 0:\r\n            features.append(tmp_fl)\r\n        if len(features) >0:\r\n            documents.append(features)\r\n        return documents\r\n    else:\r\n        features = list()\r\n        tmp_fl = list()\r\n        for line in lines:\r\n            if not (line.isspace() or (len(line) > 10 and line[0:10] == \'-DOCSTART-\')):\r\n                line = line.split()[0]\r\n                tmp_fl.append(line)\r\n            elif len(tmp_fl) > 0:\r\n                features.append(tmp_fl)\r\n                tmp_fl = list()\r\n        if len(tmp_fl) > 0:\r\n            features.append(tmp_fl)\r\n\r\n        return features\r\n\r\ndef shrink_embedding(feature_map, word_dict, word_embedding, caseless):\r\n    """"""\r\n    shrink embedding dictionary to in-doc words only\r\n    """"""\r\n    if caseless:\r\n        feature_map = set([k.lower() for k in feature_map.keys()])\r\n    new_word_list = [k for k in word_dict.keys() if (k in feature_map)]\r\n    new_word_dict = {k:v for (v, k) in enumerate(new_word_list)}\r\n    new_word_list_ind = torch.LongTensor([word_dict[k] for k in new_word_list])\r\n    new_embedding = word_embedding[new_word_list_ind]\r\n    return new_word_dict, new_embedding\r\n\r\ndef encode_corpus(lines, f_map, l_map, if_lower = False):\r\n    """"""\r\n    encode corpus into features and labels\r\n    """"""\r\n    tmp_fl = []\r\n    tmp_ll = []\r\n    features = []\r\n    labels = []\r\n    for line in lines:\r\n        if not (line.isspace() or (len(line) > 10 and line[0:10] == \'-DOCSTART-\')):\r\n            line = line.rstrip(\'\\n\').split()\r\n            tmp_fl.append(line[0])\r\n            tmp_ll.append(line[-1])\r\n        elif len(tmp_fl) > 0:\r\n            features.append(tmp_fl)\r\n            labels.append(tmp_ll)\r\n            tmp_fl = list()\r\n            tmp_ll = list()\r\n    if len(tmp_fl) > 0:\r\n        features.append(tmp_fl)\r\n        labels.append(tmp_ll)\r\n    if if_lower:\r\n        features = list(map(lambda t: list(map(lambda x: x.lower(), t)), features))\r\n    feature_e = encode_safe(features, f_map, f_map[\'<unk>\'])\r\n    label_e = encode(labels, l_map)\r\n    return feature_e, label_e\r\n\r\n\r\ndef encode_corpus_c(lines, f_map, l_map, c_map):\r\n    """"""\r\n    encode corpus into features (both word-level and char-level) and labels\r\n    """"""\r\n    tmp_fl = []\r\n    tmp_ll = []\r\n    features = []\r\n    labels = []\r\n    for line in lines:\r\n        if not (line.isspace() or (len(line) > 10 and line[0:10] == \'-DOCSTART-\')):\r\n            line = line.rstrip(\'\\n\').split()\r\n            tmp_fl.append(line[0])\r\n            tmp_ll.append(line[-1])\r\n        elif len(tmp_fl) > 0:\r\n            features.append(tmp_fl)\r\n            labels.append(tmp_ll)\r\n            tmp_fl = list()\r\n            tmp_ll = list()\r\n    if len(tmp_fl) > 0:\r\n        features.append(tmp_fl)\r\n        labels.append(tmp_ll)\r\n\r\n    feature_c = encode2char_safe(features, c_map)\r\n    feature_e = encode_safe(features, f_map, f_map[\'<unk>\'])\r\n    label_e = encode(labels, l_map)\r\n    return feature_c, feature_e, label_e\r\n\r\ndef load_embedding(emb_file, delimiter, feature_map, caseless, unk, shrink_to_train=False):\r\n    """"""\r\n    load embedding\r\n    """"""\r\n    if caseless:\r\n        feature_set = set([key.lower() for key in feature_map])\r\n    else:\r\n        feature_set = set([key for key in feature_map])\r\n\r\n    word_dict = dict()\r\n    embedding_array = list()\r\n    for line in open(emb_file, \'r\'):\r\n        line = line.split(delimiter)\r\n        vector = list(map(lambda t: float(t), filter(lambda n: n and not n.isspace(), line[1:])))\r\n        if shrink_to_train and line[0] not in feature_set:\r\n            continue\r\n        if line[0] == unk:\r\n            word_dict[\'<unk>\'] = len(word_dict)\r\n        else:\r\n            word_dict[line[0]] = len(word_dict)\r\n        embedding_array.append(vector)\r\n    embedding_tensor_1 = torch.FloatTensor(np.asarray(embedding_array))\r\n    emb_len = embedding_tensor_1.size(1)\r\n\r\n    rand_embedding_count = 0\r\n    for key in feature_map:\r\n        if caseless:\r\n            key = key.lower()\r\n        if key not in word_dict:\r\n            word_dict[key] = len(word_dict)\r\n            rand_embedding_count += 1\r\n\r\n    rand_embedding_tensor = torch.FloatTensor(rand_embedding_count, emb_len)\r\n    init_embedding(rand_embedding_tensor)\r\n\r\n    embedding_tensor = torch.cat((embedding_tensor_1, rand_embedding_tensor), 0)\r\n    return word_dict, embedding_tensor\r\n\r\ndef load_embedding_wlm(emb_file, delimiter, feature_map, full_feature_set, caseless, unk, emb_len, shrink_to_train=False, shrink_to_corpus=False):\r\n    """"""\r\n    load embedding, indoc words would be listed before outdoc words\r\n\r\n    args:\r\n        emb_file: path to embedding file\r\n        delimiter: delimiter of lines\r\n        feature_map: word dictionary\r\n        full_feature_set: all words in the corpus\r\n        caseless: convert into casesless style\r\n        unk: string for unknown token\r\n        emb_len: dimension of embedding vectors\r\n        shrink_to_train: whether to shrink out-of-training set or not\r\n        shrink_to_corpus: whether to shrink out-of-corpus or not\r\n    """"""\r\n    if caseless:\r\n        feature_set = set([key.lower() for key in feature_map])\r\n        full_feature_set = set([key.lower() for key in full_feature_set])\r\n    else:\r\n        feature_set = set([key for key in feature_map])\r\n        full_feature_set = set([key for key in full_feature_set])\r\n\r\n    #ensure <unk> is 0\r\n    word_dict = {v:(k+1) for (k,v) in enumerate(feature_set - set([\'<unk>\']))}\r\n    word_dict[\'<unk>\'] = 0\r\n\r\n    in_doc_freq_num = len(word_dict)\r\n    rand_embedding_tensor = torch.FloatTensor(in_doc_freq_num, emb_len)\r\n    init_embedding(rand_embedding_tensor)\r\n\r\n    indoc_embedding_array = list()\r\n    indoc_word_array = list()\r\n    outdoc_embedding_array = list()\r\n    outdoc_word_array = list()\r\n\r\n    for line in open(emb_file, \'r\'):\r\n        line = line.split(delimiter)\r\n        vector = list(map(lambda t: float(t), filter(lambda n: n and not n.isspace(), line[1:])))\r\n\r\n        if shrink_to_train and line[0] not in feature_set:\r\n            continue\r\n\r\n        if line[0] == unk:\r\n            rand_embedding_tensor[0] = torch.FloatTensor(vector) #unk is 0\r\n        elif line[0] in word_dict:\r\n            rand_embedding_tensor[word_dict[line[0]]] = torch.FloatTensor(vector)\r\n        elif line[0] in full_feature_set:\r\n            indoc_embedding_array.append(vector)\r\n            indoc_word_array.append(line[0])\r\n        elif not shrink_to_corpus:\r\n            outdoc_word_array.append(line[0])\r\n            outdoc_embedding_array.append(vector)\r\n\r\n    embedding_tensor_0 = torch.FloatTensor(np.asarray(indoc_embedding_array))\r\n\r\n    if not shrink_to_corpus:\r\n        embedding_tensor_1 = torch.FloatTensor(np.asarray(outdoc_embedding_array))\r\n        word_emb_len = embedding_tensor_0.size(1)\r\n        assert(word_emb_len == emb_len)\r\n\r\n    if shrink_to_corpus:\r\n        embedding_tensor = torch.cat([rand_embedding_tensor, embedding_tensor_0], 0)\r\n    else:\r\n        embedding_tensor = torch.cat([rand_embedding_tensor, embedding_tensor_0, embedding_tensor_1], 0)\r\n\r\n    for word in indoc_word_array:\r\n        word_dict[word] = len(word_dict)\r\n    in_doc_num = len(word_dict)\r\n    if  not shrink_to_corpus:\r\n        for word in outdoc_word_array:\r\n            word_dict[word] = len(word_dict)\r\n\r\n    return word_dict, embedding_tensor, in_doc_num\r\n\r\ndef calc_threshold_mean(features):\r\n    """"""\r\n    calculate the threshold for bucket by mean\r\n    """"""\r\n    lines_len = list(map(lambda t: len(t) + 1, features))\r\n    average = int(sum(lines_len) / len(lines_len))\r\n    lower_line = list(filter(lambda t: t < average, lines_len))\r\n    upper_line = list(filter(lambda t: t >= average, lines_len))\r\n    lower_average = int(sum(lower_line) / len(lower_line))\r\n    upper_average = int(sum(upper_line) / len(upper_line))\r\n    max_len = max(lines_len)\r\n    return [lower_average, average, upper_average, max_len]\r\n\r\n\r\ndef construct_bucket_mean_gd(input_features, input_label, word_dict, label_dict):\r\n    """"""\r\n    Construct bucket by mean for greedy decode, word-level only\r\n    """"""\r\n    # encode and padding\r\n    features = encode_safe(input_features, word_dict, word_dict[\'<unk>\'])\r\n    labels = encode(input_label, label_dict)\r\n    labels = list(map(lambda t: [label_dict[\'<start>\']] + list(t), labels))\r\n\r\n    thresholds = calc_threshold_mean(features)\r\n\r\n    return construct_bucket_gd(features, labels, thresholds, word_dict[\'<eof>\'], label_dict[\'<pad>\'])\r\n\r\n\r\ndef construct_bucket_mean_vb(input_features, input_label, word_dict, label_dict, caseless):\r\n    """"""\r\n    Construct bucket by mean for viterbi decode, word-level only\r\n    """"""\r\n    # encode and padding\r\n    if caseless:\r\n        input_features = list(map(lambda t: list(map(lambda x: x.lower(), t)), input_features))\r\n\r\n    features = encode_safe(input_features, word_dict, word_dict[\'<unk>\'])\r\n    labels = encode(input_label, label_dict)\r\n    labels = list(map(lambda t: [label_dict[\'<start>\']] + list(t), labels))\r\n\r\n    thresholds = calc_threshold_mean(features)\r\n\r\n    return construct_bucket_vb(features, labels, thresholds, word_dict[\'<eof>\'], label_dict[\'<pad>\'], len(label_dict))\r\n\r\ndef construct_bucket_mean_vb_wc(word_features, input_label, label_dict, char_dict, word_dict, caseless):\r\n    """"""\r\n    Construct bucket by mean for viterbi decode, word-level and char-level\r\n    """"""\r\n    # encode and padding\r\n    char_features = encode2char_safe(word_features, char_dict)\r\n    fea_len = [list(map(lambda t: len(t) + 1, f)) for f in char_features]\r\n    forw_features = concatChar(char_features, char_dict)\r\n\r\n    labels = encode(input_label, label_dict)\r\n    labels = list(map(lambda t: [label_dict[\'<start>\']] + list(t), labels))\r\n\r\n    thresholds = calc_threshold_mean(fea_len)\r\n\r\n    if caseless:\r\n        word_features = list(map(lambda t: list(map(lambda x: x.lower(), t)), word_features))\r\n    word_features = encode_safe(word_features, word_dict, word_dict[\'<unk>\'])\r\n\r\n    return construct_bucket_vb_wc(word_features, forw_features, fea_len, labels, thresholds, word_dict[\'<eof>\'], char_dict[\'\\n\'], label_dict[\'<pad>\'], len(label_dict))\r\n\r\ndef construct_bucket_vb_wc(word_features, forw_features, fea_len, input_labels, thresholds, pad_word_feature, pad_char_feature, pad_label, label_size):\r\n    """"""\r\n    Construct bucket by thresholds for viterbi decode, word-level and char-level\r\n    """"""\r\n    # construct corpus for language model pre-training\r\n    forw_corpus = [pad_char_feature] + list(reduce(lambda x, y: x + [pad_char_feature] + y, forw_features)) + [pad_char_feature]\r\n    back_corpus = forw_corpus[::-1]\r\n    # two way construct, first build the bucket, then calculate padding length, then do the padding\r\n    buckets = [[[], [], [], [], [], [], [], []] for ind in range(len(thresholds))]\r\n    # forw, forw_ind, back, back_in, label, mask\r\n    buckets_len = [0 for ind in range(len(thresholds))]\r\n\r\n    # thresholds is the padded length for fea\r\n    # buckets_len is the padded length for char\r\n    for f_f, f_l in zip(forw_features, fea_len):\r\n        cur_len_1 = len(f_l) + 1\r\n        idx = 0\r\n        while thresholds[idx] < cur_len_1:\r\n            idx += 1\r\n        tmp_concat_len = len(f_f) + thresholds[idx] - len(f_l)\r\n        if buckets_len[idx] < tmp_concat_len:\r\n            buckets_len[idx] = tmp_concat_len\r\n\r\n    # calc padding\r\n    for f_f, f_l, w_f, i_l in zip(forw_features, fea_len, word_features, input_labels):\r\n        cur_len = len(f_l)\r\n        idx = 0\r\n        cur_len_1 = cur_len + 1\r\n        while thresholds[idx] < cur_len_1:\r\n            idx += 1\r\n\r\n        padded_feature = f_f + [pad_char_feature] * (buckets_len[idx] - len(f_f))  # pad feature with <\'\\n\'>, at least one\r\n\r\n        padded_feature_len = f_l + [1] * (thresholds[idx] - len(f_l)) # pad feature length with <\'\\n\'>, at least one\r\n        padded_feature_len_cum = list(itertools.accumulate(padded_feature_len)) # start from 0, but the first is \' \', so the position need not to be -1\r\n        buckets[idx][0].append(padded_feature) # char\r\n        buckets[idx][1].append(padded_feature_len_cum)\r\n        buckets[idx][2].append(padded_feature[::-1])\r\n        buckets[idx][3].append([buckets_len[idx] - 1] + [buckets_len[idx] - 1 - tup for tup in padded_feature_len_cum[:-1]])\r\n        buckets[idx][4].append(w_f + [pad_word_feature] * (thresholds[idx] - cur_len)) #word\r\n        buckets[idx][5].append([i_l[ind] * label_size + i_l[ind + 1] for ind in range(0, cur_len)] + [i_l[cur_len] * label_size + pad_label] + [pad_label * label_size + pad_label] * (thresholds[idx] - cur_len_1))  # has additional start, label\r\n        buckets[idx][6].append([1] * cur_len_1 + [0] * (thresholds[idx] - cur_len_1))  # has additional start, mask\r\n        buckets[idx][7].append([len(f_f) + thresholds[idx] - len(f_l), cur_len_1])\r\n    bucket_dataset = [CRFDataset_WC(torch.LongTensor(bucket[0]), torch.LongTensor(bucket[1]),\r\n                                    torch.LongTensor(bucket[2]), torch.LongTensor(bucket[3]),\r\n                                    torch.LongTensor(bucket[4]), torch.LongTensor(bucket[5]),\r\n                                    torch.ByteTensor(bucket[6]), torch.LongTensor(bucket[7])) for bucket in buckets]\r\n    return bucket_dataset, forw_corpus, back_corpus\r\n\r\n\r\ndef construct_bucket_vb(input_features, input_labels, thresholds, pad_feature, pad_label, label_size):\r\n    """"""\r\n    Construct bucket by thresholds for viterbi decode, word-level only\r\n    """"""\r\n    buckets = [[[], [], []] for _ in range(len(thresholds))]\r\n    for feature, label in zip(input_features, input_labels):\r\n        cur_len = len(feature)\r\n        idx = 0\r\n        cur_len_1 = cur_len + 1\r\n        while thresholds[idx] < cur_len_1:\r\n            idx += 1\r\n        buckets[idx][0].append(feature + [pad_feature] * (thresholds[idx] - cur_len))\r\n        buckets[idx][1].append([label[ind] * label_size + label[ind + 1] for ind in range(0, cur_len)] + [\r\n            label[cur_len] * label_size + pad_label] + [pad_label * label_size + pad_label] * (\r\n                                       thresholds[idx] - cur_len_1))\r\n        buckets[idx][2].append([1] * cur_len_1 + [0] * (thresholds[idx] - cur_len_1))\r\n    bucket_dataset = [CRFDataset(torch.LongTensor(bucket[0]), torch.LongTensor(bucket[1]), torch.ByteTensor(bucket[2]))\r\n                      for bucket in buckets]\r\n    return bucket_dataset\r\n\r\n\r\ndef construct_bucket_gd(input_features, input_labels, thresholds, pad_feature, pad_label):\r\n    """"""\r\n    Construct bucket by thresholds for greedy decode, word-level only\r\n    """"""\r\n    buckets = [[[], [], []] for ind in range(len(thresholds))]\r\n    for feature, label in zip(input_features, input_labels):\r\n        cur_len = len(feature)\r\n        cur_len_1 = cur_len + 1\r\n        idx = 0\r\n        while thresholds[idx] < cur_len_1:\r\n            idx += 1\r\n        buckets[idx][0].append(feature + [pad_feature] * (thresholds[idx] - cur_len))\r\n        buckets[idx][1].append(label[1:] + [pad_label] * (thresholds[idx] - cur_len))\r\n        buckets[idx][2].append(label + [pad_label] * (thresholds[idx] - cur_len_1))\r\n    bucket_dataset = [CRFDataset(torch.LongTensor(bucket[0]), torch.LongTensor(bucket[1]), torch.LongTensor(bucket[2])) for bucket in buckets]\r\n    return bucket_dataset\r\n\r\n\r\ndef find_length_from_feats(feats, feat_to_ix):\r\n    """"""\r\n    find length of unpadded features based on feature\r\n    """"""\r\n    end_position = len(feats) - 1\r\n    for position, feat in enumerate(feats):\r\n        if feat.data[0] == feat_to_ix[\'<eof>\']:\r\n            end_position = position\r\n            break\r\n    return end_position + 1\r\n\r\n\r\ndef find_length_from_labels(labels, label_to_ix):\r\n    """"""\r\n    find length of unpadded features based on labels\r\n    """"""\r\n    end_position = len(labels) - 1\r\n    for position, label in enumerate(labels):\r\n        if label == label_to_ix[\'<pad>\']:\r\n            end_position = position\r\n            break\r\n    return end_position\r\n\r\n\r\ndef revlut(lut):\r\n    return {v: k for k, v in lut.items()}\r\n\r\n\r\n# Turn a sequence of IOB chunks into single tokens\r\ndef iob_to_spans(sequence, lut, strict_iob2=False):\r\n    """"""\r\n    convert to iob to span\r\n    """"""\r\n    iobtype = 2 if strict_iob2 else 1\r\n    chunks = []\r\n    current = None\r\n\r\n    for i, y in enumerate(sequence):\r\n        label = lut[y]\r\n\r\n        if label.startswith(\'B-\'):\r\n            if current is not None:\r\n                chunks.append(\'@\'.join(current))\r\n            current = [label.replace(\'B-\', \'\'), \'%d\' % i]\r\n\r\n        elif label.startswith(\'I-\'):\r\n\r\n            if current is not None:\r\n                base = label.replace(\'I-\', \'\')\r\n                if base == current[0]:\r\n                    current.append(\'%d\' % i)\r\n                else:\r\n                    chunks.append(\'@\'.join(current))\r\n                    if iobtype == 2:\r\n                        print(\'Warning, type=IOB2, unexpected format ([%s] follows other tag type [%s] @ %d)\' % (\r\n                            label, current[0], i))\r\n\r\n                    current = [base, \'%d\' % i]\r\n\r\n            else:\r\n                current = [label.replace(\'I-\', \'\'), \'%d\' % i]\r\n                if iobtype == 2:\r\n                    print(\'Warning, unexpected format (I before B @ %d) %s\' % (i, label))\r\n        else:\r\n            if current is not None:\r\n                chunks.append(\'@\'.join(current))\r\n            current = None\r\n\r\n    if current is not None:\r\n        chunks.append(\'@\'.join(current))\r\n\r\n    return set(chunks)\r\n\r\n# Turn a sequence of IOBES chunks into single tokens\r\ndef iobes_to_spans(sequence, lut, strict_iob2=False):\r\n    """"""\r\n    convert to iobes to span\r\n    """"""\r\n    iobtype = 2 if strict_iob2 else 1\r\n    chunks = []\r\n    current = None\r\n\r\n    for i, y in enumerate(sequence):\r\n        label = lut[y]\r\n\r\n        if label.startswith(\'B-\'):\r\n\r\n            if current is not None:\r\n                chunks.append(\'@\'.join(current))\r\n            current = [label.replace(\'B-\', \'\'), \'%d\' % i]\r\n\r\n        elif label.startswith(\'S-\'):\r\n\r\n            if current is not None:\r\n                chunks.append(\'@\'.join(current))\r\n                current = None\r\n            base = label.replace(\'S-\', \'\')\r\n            chunks.append(\'@\'.join([base, \'%d\' % i]))\r\n\r\n        elif label.startswith(\'I-\'):\r\n\r\n            if current is not None:\r\n                base = label.replace(\'I-\', \'\')\r\n                if base == current[0]:\r\n                    current.append(\'%d\' % i)\r\n                else:\r\n                    chunks.append(\'@\'.join(current))\r\n                    if iobtype == 2:\r\n                        print(\'Warning\')\r\n                    current = [base, \'%d\' % i]\r\n\r\n            else:\r\n                current = [label.replace(\'I-\', \'\'), \'%d\' % i]\r\n                if iobtype == 2:\r\n                    print(\'Warning\')\r\n\r\n        elif label.startswith(\'E-\'):\r\n\r\n            if current is not None:\r\n                base = label.replace(\'E-\', \'\')\r\n                if base == current[0]:\r\n                    current.append(\'%d\' % i)\r\n                    chunks.append(\'@\'.join(current))\r\n                    current = None\r\n                else:\r\n                    chunks.append(\'@\'.join(current))\r\n                    if iobtype == 2:\r\n                        print(\'Warning\')\r\n                    current = [base, \'%d\' % i]\r\n                    chunks.append(\'@\'.join(current))\r\n                    current = None\r\n\r\n            else:\r\n                current = [label.replace(\'E-\', \'\'), \'%d\' % i]\r\n                if iobtype == 2:\r\n                    print(\'Warning\')\r\n                chunks.append(\'@\'.join(current))\r\n                current = None\r\n        else:\r\n            if current is not None:\r\n                chunks.append(\'@\'.join(current))\r\n            current = None\r\n\r\n    if current is not None:\r\n        chunks.append(\'@\'.join(current))\r\n\r\n    return set(chunks)\r\n\r\n\r\ndef fill_y(nc, yidx):\r\n    """"""\r\n    fill y to dense matrix\r\n    """"""\r\n    batchsz = yidx.shape[0]\r\n    siglen = yidx.shape[1]\r\n    dense = np.zeros((batchsz, siglen, nc), dtype=np.int)\r\n    for i in range(batchsz):\r\n        for j in range(siglen):\r\n            idx = int(yidx[i, j])\r\n            if idx > 0:\r\n                dense[i, j, idx] = 1\r\n\r\n    return dense\r\n\r\ndef save_checkpoint(state, track_list, filename):\r\n    """"""\r\n    save checkpoint\r\n    """"""\r\n    with open(filename+\'.json\', \'w\') as f:\r\n        json.dump(track_list, f)\r\n    torch.save(state, filename+\'.model\')\r\n\r\ndef adjust_learning_rate(optimizer, lr):\r\n    """"""\r\n    shrink learning rate for pytorch\r\n    """"""\r\n    for param_group in optimizer.param_groups:\r\n        param_group[\'lr\'] = lr\r\n\r\ndef init_embedding(input_embedding):\r\n    """"""\r\n    Initialize embedding\r\n    """"""\r\n    bias = np.sqrt(3.0 / input_embedding.size(1))\r\n    nn.init.uniform_(input_embedding, -bias, bias)\r\n\r\ndef init_linear(input_linear):\r\n    """"""\r\n    Initialize linear transformation\r\n    """"""\r\n    bias = np.sqrt(6.0 / (input_linear.weight.size(0) + input_linear.weight.size(1)))\r\n    nn.init.uniform_(input_linear.weight, -bias, bias)\r\n    if input_linear.bias is not None:\r\n        input_linear.bias.data.zero_()\r\n\r\ndef init_lstm(input_lstm):\r\n    """"""\r\n    Initialize lstm\r\n    """"""\r\n    for ind in range(0, input_lstm.num_layers):\r\n        weight = eval(\'input_lstm.weight_ih_l\'+str(ind))\r\n        bias = np.sqrt(6.0 / (weight.size(0)/4 + weight.size(1)))\r\n        nn.init.uniform_(weight, -bias, bias)\r\n        weight = eval(\'input_lstm.weight_hh_l\'+str(ind))\r\n        bias = np.sqrt(6.0 / (weight.size(0)/4 + weight.size(1)))\r\n        nn.init.uniform_(weight, -bias, bias)\r\n\r\n    if input_lstm.bias:\r\n        for ind in range(0, input_lstm.num_layers):\r\n            weight = eval(\'input_lstm.bias_ih_l\'+str(ind))\r\n            weight.data.zero_()\r\n            weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\r\n            weight = eval(\'input_lstm.bias_hh_l\'+str(ind))\r\n            weight.data.zero_()\r\n            weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\r\n'"
docs/source/conf.py,0,"b'#!/usr/bin/env python3\r\n# -*- coding: utf-8 -*-\r\n#\r\n# LM-LSTM-CRF documentation build configuration file, created by\r\n# sphinx-quickstart on Thu Sep 14 03:49:01 2017.\r\n#\r\n# This file is execfile()d with the current directory set to its\r\n# containing dir.\r\n#\r\n# Note that not all possible configuration values are present in this\r\n# autogenerated file.\r\n#\r\n# All configuration values have a default; values that are commented out\r\n# serve to show the default.\r\n\r\n# If extensions (or modules to document with autodoc) are in another directory,\r\n# add these directories to sys.path here. If the directory is relative to the\r\n# documentation root, use os.path.abspath to make it absolute, like shown here.\r\n\r\nimport os\r\nimport sys\r\n\r\nsys.path.insert(0, os.path.abspath(\'../..\'))\r\n\r\n# -- General configuration ------------------------------------------------\r\n\r\n# If your documentation needs a minimal Sphinx version, state it here.\r\n#\r\n# needs_sphinx = \'1.0\'\r\n\r\n# Add any Sphinx extension module names here, as strings. They can be\r\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\r\n# ones.\r\nextensions = [\r\n    \'sphinx.ext.autodoc\',\r\n    \'sphinx.ext.autosummary\',\r\n    \'sphinx.ext.doctest\',\r\n    \'sphinx.ext.intersphinx\',\r\n    \'sphinx.ext.todo\',\r\n    \'sphinx.ext.coverage\',\r\n    \'sphinx.ext.mathjax\',\r\n    \'sphinx.ext.napoleon\',\r\n    \'sphinx.ext.viewcode\',\r\n]\r\n\r\nnapoleon_use_ivar = True\r\n\r\n# Add any paths that contain templates here, relative to this directory.\r\ntemplates_path = [\'_templates\']\r\n\r\n# The suffix(es) of source filenames.\r\n# You can specify multiple suffix as a list of string:\r\n#\r\n# source_suffix = [\'.rst\', \'.md\']\r\nsource_suffix = \'.rst\'\r\n\r\n# The master toctree document.\r\nmaster_doc = \'index\'\r\n\r\n# General information about the project.\r\nproject = \'LM-LSTM-CRF\'\r\ncopyright = \'2017, Liyuan Liu, Frank Xu, Jingbo Shang\'\r\nauthor = \'Liyuan Liu, Frank Xu, Jingbo Shang\'\r\n\r\n# The version info for the project you\'re documenting, acts as replacement for\r\n# |version| and |release|, also used in various other places throughout the\r\n# built documents.\r\n#\r\n# The short X.Y version.\r\nversion = \'\'\r\n# The full version, including alpha/beta/rc tags.\r\nrelease = \'\'\r\n\r\n# The language for content autogenerated by Sphinx. Refer to documentation\r\n# for a list of supported languages.\r\n#\r\n# This is also used if you do content translation via gettext catalogs.\r\n# Usually you set ""language"" from the command line for these cases.\r\nlanguage = None\r\n\r\n# List of patterns, relative to source directory, that match files and\r\n# directories to ignore when looking for source files.\r\n# This patterns also effect to html_static_path and html_extra_path\r\nexclude_patterns = []\r\n\r\n# The name of the Pygments (syntax highlighting) style to use.\r\npygments_style = \'sphinx\'\r\n\r\n# If true, `todo` and `todoList` produce output, else they produce nothing.\r\ntodo_include_todos = False\r\n\r\n# -- Options for HTML output ----------------------------------------------\r\n\r\n# The theme to use for HTML and HTML Help pages.  See the documentation for\r\n# a list of builtin themes.\r\n#\r\nhtml_theme = \'sphinx_rtd_theme\'\r\n\r\n# Theme options are theme-specific and customize the look and feel of a theme\r\n# further.  For a list of options available for each theme, see the\r\n# documentation.\r\n#\r\n# html_theme_options = {}\r\nhtml_theme_options = {\r\n    \'collapse_navigation\': False,\r\n    \'display_version\': True,\r\n}\r\n\r\n# Add any paths that contain custom static files (such as style sheets) here,\r\n# relative to this directory. They are copied after the builtin static files,\r\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\r\nhtml_static_path = [\'_static\']\r\n\r\n# Custom sidebar templates, must be a dictionary that maps document names\r\n# to template names.\r\n#\r\n# This is required for the alabaster theme\r\n# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\r\nhtml_sidebars = {\r\n    \'**\': [\r\n        \'about.html\',\r\n        \'navigation.html\',\r\n        \'relations.html\',  # needs \'show_related\': True theme option to display\r\n        \'searchbox.html\',\r\n        \'donate.html\',\r\n    ]\r\n}\r\n\r\n# -- Options for HTMLHelp output ------------------------------------------\r\n\r\n# Output file base name for HTML help builder.\r\nhtmlhelp_basename = \'LM-LSTM-CRFdoc\'\r\n\r\n# -- Options for LaTeX output ---------------------------------------------\r\n\r\nlatex_elements = {\r\n    # The paper size (\'letterpaper\' or \'a4paper\').\r\n    #\r\n    # \'papersize\': \'letterpaper\',\r\n\r\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\r\n    #\r\n    # \'pointsize\': \'10pt\',\r\n\r\n    # Additional stuff for the LaTeX preamble.\r\n    #\r\n    # \'preamble\': \'\',\r\n\r\n    # Latex figure (float) alignment\r\n    #\r\n    # \'figure_align\': \'htbp\',\r\n}\r\n\r\n# Grouping the document tree into LaTeX files. List of tuples\r\n# (source start file, target name, title,\r\n#  author, documentclass [howto, manual, or own class]).\r\nlatex_documents = [\r\n    (master_doc, \'LM-LSTM-CRF.tex\', \'LM-LSTM-CRF Documentation\',\r\n     \'Liyuan Liu, Frank Xu, Jingbo Shang\', \'manual\'),\r\n]\r\n\r\n# -- Options for manual page output ---------------------------------------\r\n\r\n# One entry per manual page. List of tuples\r\n# (source start file, name, description, authors, manual section).\r\nman_pages = [\r\n    (master_doc, \'lm-lstm-crf\', \'LM-LSTM-CRF Documentation\',\r\n     [author], 1)\r\n]\r\n\r\n# -- Options for Texinfo output -------------------------------------------\r\n\r\n# Grouping the document tree into Texinfo files. List of tuples\r\n# (source start file, target name, title, author,\r\n#  dir menu entry, description, category)\r\ntexinfo_documents = [\r\n    (master_doc, \'LM-LSTM-CRF\', \'LM-LSTM-CRF Documentation\',\r\n     author, \'LM-LSTM-CRF\', \'One line description of project.\',\r\n     \'Miscellaneous\'),\r\n]\r\n'"
