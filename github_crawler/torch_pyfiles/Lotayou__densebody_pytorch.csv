file_path,api_count,code
eval.py,0,b''
nohup_train.py,1,"b""from data_utils.densebody_dataset import DenseBodyDataset\nfrom data_utils import visualizer as vis\nfrom models import create_model\nfrom torch.utils.data import DataLoader\nimport sys \nfrom sys import platform\nimport os\nfrom tqdm import tqdm\nimport numpy as np\nfrom argparse import ArgumentParser\n\n# default options\ndef TrainOptions(debug=False):\n    parser = ArgumentParser()\n    \n    # dataset options    \n    # platform specific options\n    windows_root = 'D:/data' \n    linux_root = '/backup1/lingboyang/data'  # change to you dir\n    data_root = linux_root if platform == 'linux' else windows_root\n    num_threads = 4 if platform == 'linux' else 0\n    batch_size = 8 if platform == 'linux' else 4\n    \n    parser.add_argument('--data_root', type=str, default=data_root)\n    parser.add_argument('--checkpoints_dir', type=str, default='checkpoints')\n    parser.add_argument('--dataset', type=str, default='human36m',\n        choices=['human36m', 'surreal', 'up3d'])\n    parser.add_argument('--max_dataset_size', type=int, default=-1)\n    parser.add_argument('--im_size', type=int, default=256)\n    parser.add_argument('--batch_size', type=int, default=batch_size)\n    parser.add_argument('--name', type=str, default='densebody_resnet_h36m')\n    parser.add_argument('--uv_map', type=str, default='radvani', choices=['radvani', 'vbml_close', 'vbml_spaced', 'smpl_fbx'])\n    parser.add_argument('--num_threads', default=num_threads, type=int, help='# sthreads for loading data')\n    \n    # model options\n    parser.add_argument('--model', type=str, default='resnet', choices=['resnet', 'vggnet', 'mobilenet'])\n    parser.add_argument('--netD', type=str, default='convres', choices=['convres', 'conv-up'])\n    parser.add_argument('--nz', type=int, default=256, help='latent dims')\n    parser.add_argument('--ndown', type=int, default=6, help='downsample times')\n    parser.add_argument('--nchannels', type=int, default=64, help='conv channels')\n    parser.add_argument('--norm', type=str, default='batch', choices=['batch', 'instance', 'none'])\n    parser.add_argument('--nl', type=str, default='relu', choices=['relu', 'lrelu', 'elu'])\n    parser.add_argument('--init_type', type=str, default='xavier', choices=['xavier', 'normal', 'kaiming', 'orthogonal'])\n    \n    # training options\n    parser.add_argument('--phase', type=str, default='train', choices=['train', 'test'])\n    parser.add_argument('--continue_train', action='store_true')\n    parser.add_argument('--load_epoch', type=int, default=0)\n    parser.add_argument('--epoch_count', type=int, default=1)\n    parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n    parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero')    \n    parser.add_argument('--verbose', action='store_true', help='if specified, print more debugging information')\n    parser.add_argument('--save_result_freq', type=int, default=500, help='frequency of showing training results on screen')\n    parser.add_argument('--save_epoch_freq', type=int, default=5, help='frequency of saving checkpoints at the end of epochs')\n    \n    # optimization options\n    parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n    parser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate for adam')\n    parser.add_argument('--lr_policy', type=str, default='plateau', choices=['lambda', 'step', 'plateau'])\n    parser.add_argument('--lr_decay_iters', type=int, default=100, help='multiply by a gamma every lr_decay_iters iterations')\n    parser.add_argument('--tv_weight', type=float, default=10, help='toal variation loss weights')\n    opt = parser.parse_args()\n    \n    opt.uv_prefix = opt.uv_map + '_template'\n    opt.project_root = os.path.dirname(os.path.realpath(__file__))\n    \n    if debug:\n        opt.batch_size = 2\n        opt.save_result_freq = 2\n        opt.save_epoch_freq = 1\n        opt.max_dataset_size = 10\n        opt.num_threads = 0\n        opt.niter = 2\n        opt.niter_decay = 2\n    \n    return opt\n\n# \n# sys.path.append('{}/models'.format(project_root))\n# sys.path.append('{}/data_utils'.format(project_root))\n    \nif __name__ == '__main__':\n    # Change this to your gpu id.\n    # The program is fixed to run on a single GPU\n    if platform == 'linux':\n        os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n    \n    np.random.seed(9608)    \n    opt = TrainOptions(debug=False)\n    dataset = DenseBodyDataset(data_root=opt.data_root, uv_map=opt.uv_map, max_size=opt.max_dataset_size)\n    batchs_per_epoch = len(dataset) // opt.batch_size # drop last batch\n    print('#training images = %d' % len(dataset))\n\n    model = create_model(opt)\n    model.setup(opt)\n    visualizer = vis.Visualizer(opt)\n    total_steps = 0\n    \n    rand_perm = np.arange(batchs_per_epoch)\n    \n    # put something in txt file\n    file_log = open(os.path.join(opt.checkpoints_dir, opt.name, 'log.txt'), 'w')\n    for epoch in range(opt.load_epoch + 1, opt.niter + opt.niter_decay + 1):\n        # set loop information\n        print('Epoch %d: start training' % epoch)\n        np.random.shuffle(rand_perm)\n        loss_metrics = 0\n        for i in range(batchs_per_epoch):\n            data = dataset[rand_perm[i] * opt.batch_size: (rand_perm[i] + 1) * opt.batch_size]\n            loss_dict = model.train_one_batch(data)\n            loss_metrics = loss_dict['total']\n            # change tqdm info\n            tqdm_info = ''\n            for k,v in loss_dict.items():\n                tqdm_info += ' %s: %.6f' % (k, v)\n            \n            if (i + 1) % opt.save_result_freq == 0:\n                file_log.write('epoch {} iter {}: {}\\n'.format(epoch, i, tqdm_info))\n                file_log.flush()\n                visualizer.save_results(model.get_current_visuals(), epoch, i)\n        \n        if epoch % opt.save_epoch_freq == 0:\n            model.save_networks(epoch)\n        print('Epoch %d training finished' % epoch)\n        if epoch > opt.niter:\n            model.update_learning_rate(metrics=loss_metrics)\n            \n    file_log.close()\n\n"""
test.py,2,"b""from data_utils.densebody_dataset import DenseBodyDataset\r\nfrom data_utils import visualizer as vis\r\nimport torch\r\nfrom models import create_model\r\nfrom torch.utils.data import DataLoader\r\nimport sys \r\nfrom sys import platform\r\nimport os\r\nfrom tqdm import tqdm\r\nimport numpy as np\r\nfrom argparse import ArgumentParser\r\n\r\n# default options\r\ndef TestOptions(debug=False):\r\n    parser = ArgumentParser()\r\n    \r\n    # dataset options    \r\n    # platform specific options\r\n    windows_root = 'D:/data' \r\n    linux_root = '/backup1/lingboyang/data'  # change to you dir\r\n    data_root = linux_root if platform == 'linux' else windows_root\r\n    num_threads = 0\r\n    batch_size = 1\r\n    \r\n    parser.add_argument('--data_root', type=str, default=data_root)\r\n    parser.add_argument('--checkpoints_dir', type=str, default='checkpoints')\r\n    parser.add_argument('--dataset', type=str, default='human36m',\r\n        choices=['human36m', 'surreal', 'up3d', 'nturgbd'])\r\n    parser.add_argument('--max_dataset_size', type=int, default=-1)\r\n    parser.add_argument('--im_size', type=int, default=256)\r\n    parser.add_argument('--batch_size', type=int, default=batch_size)\r\n    parser.add_argument('--name', type=str, default='densebody_resnet_h36m')\r\n    parser.add_argument('--uv_map', type=str, default='radvani', choices=['radvani', 'vbml_close', 'vbml_spaced', 'smpl_fbx'])\r\n    parser.add_argument('--num_threads', default=num_threads, type=int, help='# sthreads for loading data')\r\n    \r\n    # model options\r\n    parser.add_argument('--model', type=str, default='resnet', choices=['resnet', 'vggnet', 'mobilenet'])\r\n    parser.add_argument('--netD', type=str, default='convres', choices=['convres', 'conv-up'])\r\n    parser.add_argument('--nz', type=int, default=256, help='latent dims')\r\n    parser.add_argument('--ndown', type=int, default=6, help='downsample times')\r\n    parser.add_argument('--nchannels', type=int, default=64, help='conv channels')\r\n    parser.add_argument('--norm', type=str, default='batch', choices=['batch', 'instance', 'none'])\r\n    parser.add_argument('--nl', type=str, default='relu', choices=['relu', 'lrelu', 'elu'])\r\n    parser.add_argument('--init_type', type=str, default='xavier', choices=['xavier', 'normal', 'kaiming', 'orthogonal'])\r\n    parser.add_argument('--verbose', action='store_true', help='if specified, print more debugging information')\r\n\r\n    # testing options\r\n    parser.add_argument('--results_dir', type=str, default='results')\r\n    parser.add_argument('--phase', type=str, default='test', choices=['test', 'in_the_wild'])\r\n    parser.add_argument('--continue_train', action='store_true')\r\n    parser.add_argument('--load_epoch', type=int, default=0)\r\n    \r\n    opt = parser.parse_args()\r\n    \r\n    opt.uv_prefix = opt.uv_map + '_template'\r\n    opt.project_root = os.path.dirname(os.path.realpath(__file__))\r\n    \r\n    return opt\r\n\r\nif __name__ == '__main__':\r\n    # Change this to your gpu id.\r\n    # The program is fixed to run on a single GPU\r\n    if platform == 'linux':\r\n        os.environ['CUDA_VISIBLE_DEVICES'] = '2'\r\n    \r\n    opt = TestOptions(debug=False)\r\n    dataset = DenseBodyDataset(data_root=opt.data_root, dataset_name=opt.dataset, \r\n            uv_map=opt.uv_map, max_size=opt.max_dataset_size, phase=opt.phase)\r\n    batchs_per_epoch = len(dataset) // opt.batch_size # drop last batch\r\n    print('#testing images = %d' % len(dataset))\r\n\r\n    model = create_model(opt)\r\n    model.setup(opt)\r\n    visualizer = vis.Visualizer(opt)\r\n    \r\n    loop = tqdm(range(opt.max_dataset_size), ncols=120)\r\n    for i in loop:\r\n        loop.set_description('Testing case %d' % i)\r\n        data = dataset[i:i+1]\r\n        model.set_input(data)\r\n        with torch.no_grad():\r\n            model.fake_UV = model.decoder(model.encoder(model.real_input))\r\n        \r\n        visualizer.save_results(model.get_current_visuals(), opt.load_epoch, i)\r\n"""
train.py,1,"b""from data_utils.densebody_dataset import DenseBodyDataset\nfrom data_utils import visualizer as vis\nfrom models import create_model\nfrom torch.utils.data import DataLoader\nimport sys \nfrom sys import platform\nimport os\nfrom tqdm import tqdm\nimport numpy as np\nfrom argparse import ArgumentParser\n\n# default options\ndef TrainOptions(debug=False):\n    parser = ArgumentParser()\n    \n    # dataset options    \n    # platform specific options\n    windows_root = 'D:/data' \n    linux_root = '/backup1/lingboyang/data'  # change to you dir\n    data_root = linux_root if platform == 'linux' else windows_root\n    num_threads = 4 if platform == 'linux' else 0\n    batch_size = 8 if platform == 'linux' else 4\n    \n    parser.add_argument('--data_root', type=str, default=data_root)\n    parser.add_argument('--checkpoints_dir', type=str, default='checkpoints')\n    parser.add_argument('--max_dataset_size', type=int, default=-1)\n    parser.add_argument('--im_size', type=int, default=512)\n    parser.add_argument('--batch_size', type=int, default=batch_size)\n    parser.add_argument('--name', type=str, default='densebody_resnet_h36m')\n    parser.add_argument('--uv_map', type=str, default='radvani', choices=['radvani', 'vbml_close', 'vbml_spaced', 'smpl_fbx'])\n    parser.add_argument('--num_threads', default=num_threads, type=int, help='# sthreads for loading data')\n    \n    # model options\n    parser.add_argument('--model', type=str, default='resnet', choices=['resnet', 'vggnet', 'mobilenet'])\n    parser.add_argument('--netD', type=str, default='convres', choices=['convres', 'conv-up'])\n    parser.add_argument('--nz', type=int, default=256, help='latent dims')\n    parser.add_argument('--ndown', type=int, default=6, help='downsample times')\n    parser.add_argument('--nchannels', type=int, default=64, help='conv channels')\n    parser.add_argument('--norm', type=str, default='batch', choices=['batch', 'instance', 'none'])\n    parser.add_argument('--nl', type=str, default='relu', choices=['relu', 'lrelu', 'elu'])\n    parser.add_argument('--init_type', type=str, default='xavier', choices=['xavier', 'normal', 'kaiming', 'orthogonal'])\n    \n    # training options\n    parser.add_argument('--phase', type=str, default='train', choices=['train', 'test'])\n    parser.add_argument('--continue_train', action='store_true')\n    parser.add_argument('--load_epoch', type=int, default=0)\n    parser.add_argument('--epoch_count', type=int, default=1)\n    parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n    parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero')    \n    parser.add_argument('--verbose', action='store_true', help='if specified, print more debugging information')\n    parser.add_argument('--save_result_freq', type=int, default=500, help='frequency of showing training results on screen')\n    parser.add_argument('--save_epoch_freq', type=int, default=5, help='frequency of saving checkpoints at the end of epochs')\n    \n    # optimization options\n    parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n    parser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate for adam')\n    parser.add_argument('--lr_policy', type=str, default='plateau', choices=['lambda', 'step', 'plateau'])\n    parser.add_argument('--lr_decay_iters', type=int, default=100, help='multiply by a gamma every lr_decay_iters iterations')\n    parser.add_argument('--tv_weight', type=float, default=10, help='toal variation loss weights')\n    opt = parser.parse_args()\n    \n    opt.uv_prefix = opt.uv_map + '_template'\n    opt.project_root = os.path.dirname(os.path.realpath(__file__))\n    \n    if debug:\n        opt.batch_size = 2\n        opt.save_result_freq = 2\n        opt.save_epoch_freq = 1\n        opt.max_dataset_size = 10\n        opt.num_threads = 0\n        opt.niter = 2\n        opt.niter_decay = 2\n    \n    return opt\n\n# \n# sys.path.append('{}/models'.format(project_root))\n# sys.path.append('{}/data_utils'.format(project_root))\n    \nif __name__ == '__main__':\n    # Change this to your gpu id.\n    # The program is fixed to run on a single GPU\n    if platform == 'linux':\n        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n    \n    np.random.seed(9608)    \n    opt = TrainOptions(debug=False)\n    dataset = DenseBodyDataset(data_root=opt.data_root, uv_map=opt.uv_map, max_size=opt.max_dataset_size)\n    batchs_per_epoch = len(dataset) // opt.batch_size # drop last batch\n    print('#training images = %d' % len(dataset))\n\n    model = create_model(opt)\n    model.setup(opt)\n    visualizer = vis.Visualizer(opt)\n    total_steps = 0\n    \n    rand_perm = np.arange(batchs_per_epoch)\n    \n    # put something in txt file\n    file_log = open(os.path.join(opt.checkpoints_dir, opt.name, 'log.txt'), 'w')\n    for epoch in range(opt.load_epoch + 1, opt.niter + opt.niter_decay + 1):\n        # set loop information\n        print('Epoch %d: start training' % epoch)\n        np.random.shuffle(rand_perm)\n        loop = tqdm(range(batchs_per_epoch), ncols=120)\n        loss_metrics = 0\n        for i in loop:\n            data = dataset[rand_perm[i] * opt.batch_size: (rand_perm[i] + 1) * opt.batch_size]\n            loss_dict = model.train_one_batch(data)\n            loss_metrics = loss_dict['total']\n            # change tqdm info\n            tqdm_info = ''\n            for k,v in loss_dict.items():\n                tqdm_info += ' %s: %.6f' % (k, v)\n            loop.set_description(tqdm_info)\n            \n            if (i + 1) % opt.save_result_freq == 0:\n                file_log.write('epoch {} iter {}: {}\\n'.format(epoch, i, tqdm_info))\n                file_log.flush()\n                visualizer.save_results(model.get_current_visuals(), epoch, i)\n        \n        if epoch % opt.save_epoch_freq == 0:\n            model.save_networks(epoch)\n        print('Epoch %d training finished' % epoch)\n        if epoch > opt.niter:\n            model.update_learning_rate(metrics=loss_metrics)\n            \n    file_log.close()\n\n"""
data_utils/__init__.py,0,b''
data_utils/batch_svd.py,1,"b'import torch\nimport torch_batch_svd\n\n\nclass BatchSVDFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(self, x):\n        U0, S, V0 = torch_batch_svd.batch_svd_forward(x, True, 1e-7, 100)\n        k = S.size(1)\n        U = U0[:, :, :k]\n        V = V0[:, :, :k]\n        self.save_for_backward(x, U, S, V)\n\n        return U, S, V\n\n    @staticmethod\n    def backward(self, grad_u, grad_s, grad_v):\n        x, U, S, V = self.saved_variables\n\n        grad_out = torch_batch_svd.batch_svd_backward(\n            [grad_u, grad_s, grad_v],\n            x, True, True, U, S, V\n        )\n\n        return grad_out\n\n\ndef batch_svd(x):\n    """"""\n    input:\n        x --- shape of [B, M, N]\n    return:\n        U, S, V = batch_svd(x) where x = USV^T\n    """"""\n    return BatchSVDFunction.apply(x)\n'"
data_utils/create_UV_maps.py,6,"b""from smpl_torch_batch import SMPLModel\nimport numpy as np\nimport pickle \nimport h5py\nimport torch\nfrom torch.nn import Module\nimport os\nimport shutil\nfrom tqdm import tqdm\nfrom time import time\nfrom sys import platform\nfrom torch.utils.data import Dataset, DataLoader\nfrom cv2 import imread, imwrite\nfrom skimage.draw import circle\n\nfrom procrustes import map_3d_to_2d\nfrom uv_map_generator import UV_Map_Generator\n\nclass Human36MWashedDataset(Dataset):\n    def __init__(self, smpl, max_item=312188, root_dir=None, \n            annotation='h36m.pickle', calc_mesh=False):\n        super(Human36MWashedDataset, self).__init__()\n        if root_dir is None:            \n            root_dir = '/backup1/lingboyang/data/human36m_washed' \\\n                if platform == 'linux' \\\n                else 'D:/data/human36m_washed'\n            \n        self.root_dir = root_dir\n        self.calc_mesh = calc_mesh\n        self.smpl = smpl\n        self.dtype = smpl.data_type\n        self.device = smpl.device\n        self.itemlist = []\n        '''\n        center\n        gt2d\n        gt3d\n        height\n        imagename\n        pose\n        shape\n        smpl_joint\n        width\n        '''\n        with open(root_dir + '/' + annotation, 'rb') as f:\n            tmp = pickle.load(f)        \n            for k in tmp.keys():\n                data = tmp[k][:max_item]\n                    \n                setattr(self, k, data)\n                self.itemlist.append(k)\n            \n        # flip gt2d y coordinates\n        # self.gt2d[:,:,1] = 256 - self.gt2d[:,:,1]\n        # # flip gt3d y & z coordinates\n        # self.gt3d[:,:,1:] *= -1\n        self.length = self.pose.shape[0]\n        \n    def __getitem__(self, index):\n        out_dict = {}\n        for item in self.itemlist:\n            if item == 'imagename':\n                out_dict[item] = getattr(self, item)[index]\n            else:\n                data_npy = getattr(self, item)[index]\n                out_dict[item] = torch.from_numpy(data_npy)\\\n                    .type(self.dtype).to(self.device)\n            \n        if self.calc_mesh:\n            _trans = torch.zeros((out_dict['pose'].shape[0], 3), \n                dtype=self.dtype, device=self.device)\n            meshes, lsp_joints = self.smpl(out_dict['shape'], out_dict['pose'], _trans)\n            out_dict['meshes'] = meshes\n            out_dict['lsp_joints'] = lsp_joints\n        \n        return out_dict\n        \n    def __len__(self):\n        return self.length\n    \ndef visualize(folder, imagenames, mesh_2d, joints_2d, root=None):\n    i = 0\n    for name, mesh, joints in zip(imagenames, mesh_2d, joints_2d):\n        print(name)\n        shutil.copyfile(root + name,\n            '/im_gt_{}.png'.format(folder, i)\n        )\n        im = imread(name)\n        shape = im.shape[0:2]\n        height = im.shape[0]\n        for p2d in mesh:\n            im[height - p2d[1], p2d[0]] = [127,127,127]\n            \n        for j2d in joints:\n            rr, cc = circle(height - j2d[1], j2d[0], 2, shape)\n            im[rr, cc] = [255, 0, 0]\n            \n        imwrite('{}/im_mask_{}.png'.format(folder, i), im)\n        i += 1\n\n    \ndef create_UV_maps(UV_label_root=None, uv_prefix = 'radvani_template'):\n    if platform == 'linux':\n        os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n    \n    data_type = torch.float32\n    device=torch.device('cuda')\n    pose_size = 72\n    beta_size = 10\n\n    np.random.seed(9608)\n    model = SMPLModel(\n            device=device,\n            model_path = './model_lsp.pkl',\n            data_type=data_type,\n        )\n    dataset = Human36MWashedDataset(model, calc_mesh=True)\n    \n    generator = UV_Map_Generator(\n        UV_height=256,\n        UV_pickle=uv_prefix+'.pickle'\n    )\n    \n    # create root folder for UV labels\n    if UV_label_root is None:\n        UV_label_root=dataset.root_dir.replace('_washed', \n                '_UV_map_{}'.format(uv_prefix[:-9]))\n    \n    if not os.path.isdir(UV_label_root):\n        os.makedirs(UV_label_root)\n        subs = [sub for sub in os.listdir(dataset.root_dir)\n            if os.path.isdir(dataset.root_dir + '/' + sub)]\n        for sub in subs:\n            os.makedirs(UV_label_root + '/' + sub)\n    else:\n        print('{} folder exists, process terminated...'.format(UV_label_root))\n        return\n    \n    # generate mesh, align with 14 point ground truth\n    batch_size = 64\n    total_batch_num = dataset.length // batch_size + 1\n    _loop = tqdm(range(total_batch_num), ncols=80)\n    \n    for batch_id in _loop:\n        data = dataset[batch_id * batch_size: (batch_id + 1) * batch_size]\n        meshes = data['meshes']\n        input = data['lsp_joints']\n        target_2d = data['gt2d']\n        target_3d = data['gt3d']\n        imagename = [UV_label_root + str for str in data['imagename']]\n        \n        transforms = map_3d_to_2d(input, target_2d, target_3d)\n        \n        # Important: mesh should be centered at the origin!\n        deformed_meshes = transforms(meshes)\n        mesh_3d = deformed_meshes.detach().cpu().numpy()\n        '''\n        test_folder = '_test_radvani'\n        if not os.path.isdir(test_folder):\n            os.makedirs(test_folder)\n        visualize(test_folder, data['imagename'], mesh_3d[:,:,:2].astype(np.int), \n           target_2d.detach().cpu().numpy().astype(np.int), dataset.root_dir)\n        '''\n        s=time()\n        for name, mesh in zip(imagename, mesh_3d):\n            UV_position_map, verts_backup = \\\n                generator.get_UV_map(mesh)\n            imwrite(name, (UV_position_map * 255).astype(np.uint8))\n            \n            # write colorized coordinates to ply\n            '''\n            model.write_obj(\n                mesh, \n                '{}/real_mesh_{}.obj'.format(test_folder, i)\n            )   # weird.\n            UV_scatter, _, _ = generator.render_point_cloud(\n                verts=mesh\n            )\n            \n            generator.write_ply(\n                '{}/colored_mesh_{}.ply'.format(test_folder, i), mesh\n            )\n            out = np.concatenate(\n                (UV_position_map, UV_scatter), axis=1\n            )\n            \n            imsave('{}/UV_position_map_{}.png'.format(test_folder, i), out)\n            \n            resampled_mesh = generator.resample(UV_position_map)\n            \n            model.write_obj(\n                resampled_mesh, \n                '{}/recon_mesh_{}.obj'.format(test_folder, i)\n            )\n            '''\n\nif __name__ == '__main__':\n    # Please make sure the prefix is the same as in train.py opt.uv_prefix\n    # prefix = 'radvani_template'\n    # prefix = 'vbml_close_template'\n    prefix = 'vbml_spaced_template'\n    create_UV_maps(uv_prefix=prefix)\n    # create_UV_maps(uv_prefix='radvani_new_template')\n    # create_UV_maps(uv_prefix='smpl_fbx_template')\n    \n"""
data_utils/data_washing.py,3,"b""import numpy as np\nimport pickle \nimport h5py\nimport torch\nfrom torch.nn import Module\nimport os\nimport shutil\nfrom sys import platform\n#from skimage.io import imread, imsave\nfrom cv2 import imread, imwrite\nfrom skimage.transform import resize\nfrom skimage.draw import circle\nfrom tqdm import tqdm\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass DataWasher():\n    def __init__(self, data_type=torch.float32, max_item=312188, root_dir=None, \n            annotation='annotation_large.h5', out_im_size=256):\n        super(DataWasher, self).__init__()\n        if root_dir is None:            \n            root_dir = '/backup1/lingboyang/data/human36m' \\\n                if platform == 'linux' \\\n                else 'D:/data/human36m'\n            \n        self.root_dir = root_dir\n        self.dtype = data_type\n        self.out_size = out_im_size\n        self.itemlist = []\n        fin = h5py.File(root_dir + '/' + annotation, 'r')\n        '''\n        center\n        gt2d\n        gt3d\n        height\n        imagename\n        pose\n        shape\n        smpl_joint\n        width\n        '''\n        for k in fin.keys():\n            data = fin[k][:max_item]\n            if k.startswith('gt'):  # reshape coords\n                data = data.reshape(-1, 14, 3)\n                if k == 'gt2d':  # remove confidence score\n                    data = data[:,:,:2]\n            elif k == 'imagename':\n                data = [b.decode() for b in data]\n                \n            setattr(self, k, data)\n            self.itemlist.append(k)\n        \n        # flip gt2d y coordinates\n        self.gt2d[:,:,1] = self.height[:, np.newaxis] - self.gt2d[:,:,1]\n        # flip gt3d y & z coordinates\n        self.gt3d[:,:,1:] *= -1\n        \n        fin.close()\n        self._strip_non_square_images()\n        \n    def _strip_non_square_images(self):\n        length = self.pose.shape[0]\n        is_valid = np.zeros(length)\n        for i in range(length):\n            if self.height[i] > self.out_size \\\n            and self.width[i] > self.out_size:\n                is_valid[i] = 1\n            #else:\n            #    print('Image {} invalid: h={}, w={}'\\\n            #        .format(i,self.height[i], self.width[i]))\n        \n        valid_indices = np.nonzero(is_valid)[0]\n        for k in self.itemlist:\n            if k == 'imagename':\n                self.imagename = [\n                    self.imagename[i]\n                    for i in valid_indices\n                ]\n            else:\n                data = getattr(self, k)\n                setattr(self, k, data[valid_indices])\n                \n        self.length = self.pose.shape[0]\n    \n    def _rand_crop_and_resize(self, img, i, crop_margin=40):\n        h = img.shape[0]\n        w = img.shape[1]\n        # random_crop and resize\n        x2d = self.gt2d[i, :, 0]\n        y2d = self.gt2d[i, :, 1]\n        w_span = x2d.max() - x2d.min() + crop_margin\n        h_span = y2d.max() - y2d.min() + crop_margin\n        \n        crop_size = np.random.randint(\n            low=max(h_span, w_span), \n            high=min(h, w)-6\n        )\n        top = (max(0, y2d.max() - crop_size + 20) + \n            min(y2d.min() - 20, h - crop_size)) // 2\n        \n        # top = np.random.randint(\n            # low=max(0, y2d.max() - crop_size + 20),\n            # high=min(y2d.min() - 20, h - crop_size)\n        # )\n        \n        left = (max(0, x2d.max() - crop_size + 20) + \n            min(x2d.min() - 20, w - crop_size) ) // 2\n        \n        # left = np.random.randint(\n            # low=max(0, x2d.max() - crop_size + 20),\n            # high=min(x2d.min() - 20, w - crop_size)\n        # )\n        \n        img = img[top:top+crop_size, left:left+crop_size, :]\n        img = resize(img, (self.out_size, self.out_size))\n        factor = self.out_size / crop_size\n\n        self.gt2d[i, :, 0] = (self.gt2d[i, :, 0] - left) * factor\n        self.gt2d[i, :, 1] = (self.gt2d[i, :, 1] + top - h + crop_size) * factor\n\n        return img\n    \n    def _lrflip(self, img, i):\n        img = img[:, ::-1, :]\n        self.gt2d[i, :, 0] = self.out_size - self.gt2d[i, :, 0]\n        self.gt3d[i, :, 0] *= -1.\n        return img\n    \n    def _visualize(self, img_name, im, joints):\n        shape = im.shape[0:2]\n        height = im.shape[0]\n        # for p2d in mesh:\n        #    im[height - p2d[1], p2d[0]] = [127,127,127]\n            \n        for j2d in joints:\n            rr, cc = circle(height - j2d[1], j2d[0], 2, shape)\n            im[rr, cc] = [1., 0., 0.]\n            \n        imsave(img_name, im)\n        \n    # add Gaussian noise\n    def _add_noise(self, img, sigma):\n        img += np.random.standard_normal(img.shape) * sigma\n        img = np.maximum(0., np.minimum(1., img))\n        return img\n        \n    def data_augmentation(self, target_folder=None, \n        annotation_pickle='h36m.pickle', sigma_noise=0.02):\n        # for each item, open and perform random augmentation\n        # then save the corrected image in new place.\n        if target_folder is None or target_folder == self.root_dir:\n            print('Warning: target_folder is invalid, using default name')\n            target_folder = self.root_dir + '_washed'\n        \n        if not os.path.isdir(target_folder):\n            os.makedirs(target_folder)\n            subs = [sub for sub in os.listdir(self.root_dir)\n                if os.path.isdir(self.root_dir + '/' + sub)]\n            for sub in subs:\n                os.makedirs(target_folder + '/' + sub)\n        \n        do_lrflip = np.random.rand(self.length)\n        _loop = tqdm(range(self.length), ncols=80)\n        for i in _loop:\n            name = self.imagename[i]\n            img = imread(self.root_dir + name)\n            img = img.astype(np.float) / 255.\n            img = self._rand_crop_and_resize(img, i)\n            if do_lrflip[i] > 0.5:\n                img = self._lrflip(img, i)\n            img = self._add_noise(img, sigma_noise)\n            '''\n            self._visualize(\n                '_test_washed/result_{}.png'.format(i),\n                img, self.gt2d[i, :, :2]\n            )\n            '''\n            #imsave(target_folder + name, img)\n            imwrite(target_folder + name, (img*255).astype(np.uint8))\n        \n        keep_list = ['gt2d', 'gt3d', 'imagename', 'pose', 'shape']\n        f = open(target_folder + '/' + annotation_pickle, 'wb')\n        keep_dict = {k: getattr(self, k) for k in keep_list}\n        pickle.dump(keep_dict, f)\n        f.close()\n        \nif __name__ == '__main__':\n    np.random.seed(9608)\n    root_dir = None  # Change this to your human36m dataset path\n    datawasher = DataWasher(root_dir=root_dir)\n    datawasher._strip_non_square_images()\n    datawasher.data_augmentation()\n"""
data_utils/densebody_dataset.py,10,"b'import numpy as np\nimport torch\nfrom cv2 import imread\nfrom torch.utils.data import Dataset\nfrom sys import platform\nimport pickle\nimport os\nfrom torchvision import transforms\nfrom PIL import Image\n\n# TODO: Change the global directory to where you normally hold the datasets.\n# I use both Windows PC and Linux Server for this project so I have two dirs.\n\nwindows_root = \'D:/data\'\nlinux_root = \'/backup1/lingboyang/data\'\ndata_root = linux_root if platform == \'linux\' else windows_root\n\nim_trans = transforms.Compose([\n    Image.fromarray,\n    transforms.ToTensor(),\n    transforms.Normalize([.5, .5, .5], [.5, .5, .5]),\n])\n\nIMAGE_EXTENSIONS = [\'jpg\', \'jpeg\', \'png\', \'bmp\', \'tiff\']\n\n\'\'\'\n    DenseBodyDataset: Return paired human image and UV position map, \n    along with ground truth 3D and 2D skeleton annotations.\n    \n    All human pics are stored in folder \'{path_to_your_dataset}/{dataset_name}_washed\'\n        along with a processed h5 file containing necessary annotations.\n    All UV maps are stored in folder \'{path_to_your_dataset}/{dataset_name}_UV_map\'\n    \n    Note: In the annotation pickle, all image paths are relative\n    \n    New [20190418] Adding a new mode ""in_the_wild"" for evaluating \n    trained models on images in the wild without pose annotations.\n    \n    Behavior:\n    --------------------------------------------------------------------\n    phase == \'train\' or \'test\': Load annotated dataset according to train_test_split\n    phase == \'in_the_wild\': Load all images under the given folder. Assume sizes are equal.\n    \n\'\'\'\nclass DenseBodyDataset(Dataset):\n    def __init__(self, data_root=data_root, uv_map=\'radvani\', dataset_name=\'human36m\', \n        annotation = \'h36m.pickle\', phase=\'train\', train_test_split=0.8, max_size=-1, device=None, transform=im_trans):\n        \n        super(DenseBodyDataset, self).__init__()\n        # Set image transforms and device\n        self.device = torch.device(\'cuda\') if device is None else device\n        self.transform = transform\n        self.phase = phase\n\n        self.im_root = \'{}/{}_washed\'.format(data_root, dataset_name)\n        if not os.path.isdir(self.im_root):\n            raise(FileNotFoundError(\'{} dataset not found, \'.format(dataset_name) + \n                \'please run ""data_washing.py"" first\'))\n        \n        # parse annotation\n        self.itemlist = []\n        if phase == \'in_the_wild\':\n            # TODO: crawl all image paths from folder, expect nested directory (depth <= 1)\n            im_names = self._get_image_names(self.im_root) # get all images\n            cur_length = len(im_names)\n            if 0 < max_size < cur_length:\n                im_names = im_names[:max_size]\n            self.im_names = im_names\n            self.length = len(self.im_names)\n            self.itemlist = [\'im_names\']\n        else:            \n            self.uv_root = \'{}/{}_UV_map_{}\'.format(data_root, dataset_name, uv_map)\n            if not os.path.isdir(self.uv_root):\n                raise(FileNotFoundError(\'{} uv map not found, \'.format(uv_map) + \n                    \'please run ""create_dataset.py"" first\'))\n        \n            with open(self.im_root + \'/\' + annotation, \'rb\') as f:\n                tmp = pickle.load(f)\n                \n            # Prepare train/test split\n            total_length = tmp[\'pose\'].shape[0]\n            split_point = int(train_test_split * total_length)\n            print(tmp.keys())\n            for k in tmp.keys():\n                data = tmp[k]\n                if phase == \'train\':\n                    data = data[0:split_point]\n                    cur_length = split_point\n                elif phase == \'test\':\n                    data = data[split_point:total_length]\n                    cur_length = total_length - split_point\n                \n                if 0 < max_size < cur_length:\n                    data = data[:max_size]\n                \n                if k == \'imagename\':\n                    self.im_names = [self.im_root + s for s in data]\n                    self.uv_names = [self.uv_root + s for s in data]\n                    self.itemlist.append(\'im_names\')\n                    self.itemlist.append(\'uv_names\')\n                else:\n                    setattr(self, k, data)\n                    self.itemlist.append(k)\n            \n            self.length = self.pose.shape[0]\n\n    @staticmethod\n    def _get_image_names(root):\n        # get all subdirectories\n        image_names = [root + \'/\' + name for name in os.listdir(root) \n            if name.split(\'.\')[-1].lower() in IMAGE_EXTENSIONS]\n        subs = [sub for sub in os.listdir(root) if os.path.isdir(sub)]\n        for sub in subs:\n            full_sub = root + \'/\' + sub\n            image_names += [\n                full_sub + \'/\' + name for name in os.listdir(full_sub)\n                if name.split(\'.\')[-1].lower() in IMAGE_EXTENSIONS\n            ]\n        return image_names\n    \n    def __getitem__(self, id):\n        out_dict = {}\n        for k in self.itemlist:\n            items = getattr(self, k)[id]\n            if k.endswith(\'names\'):\n                ims = [self.transform(imread(item)) for item in items]\n                out_dict[k.replace(\'names\',\'data\')] = torch.stack(ims).to(self.device)\n            else:\n                out_dict[k] = torch.from_numpy(items)\n        return out_dict    \n        \n    def __len__(self):\n        return self.length\n\nif __name__ == \'__main__\':\n    \'\'\'\n    Return sample:\n        gt2d cpu torch.int64 torch.Size([10, 14, 2]) tensor(225)\n        gt3d cpu torch.float64 torch.Size([10, 14, 3]) tensor(0.7802, dtype=torch.float64)\n        im_ cuda:0 torch.float32 torch.Size([10, 3, 256, 256]) tensor(1., device=\'cuda:0\')\n        uv_ cuda:0 torch.float32 torch.Size([10, 3, 256, 256]) tensor(1., device=\'cuda:0\')\n        pose cpu torch.float64 torch.Size([10, 72]) tensor(2.9966, dtype=torch.float64)\n        shape cpu torch.float64 torch.Size([10, 10]) tensor(2.1988, dtype=torch.float64)\n    \'\'\'\n    dataset = DenseBodyDataset()\n    print(len(dataset), dataset.itemlist)\n    data = dataset[0:10]\n    for k, v in data.items():\n        print(k, v.device, v.dtype, v.shape, v.max())\n        \n    wild_dataset = DenseBodyDataset(dataset_name=\'nturgbd\', phase=\'in_the_wild\')\n    print(len(wild_dataset), wild_dataset.itemlist)\n    data = wild_dataset[0:10]\n    for k, v in data.items():\n        print(k, v.device, v.dtype, v.shape, v.max())\n    \n    \n'"
data_utils/preprocess_smpl.py,0,"b'import numpy as np\nimport pickle\nimport sys\n\noutput_path = \'./model_lsp.pkl\'\n\nif __name__ == \'__main__\':\n  src_path = \'../../smpl_toy/res/neutral_smpl_with_cocoplus_reg.pkl\'\n  with open(src_path, \'rb\') as f:\n    src_data = pickle.load(f, encoding=""latin1"")\n  model = {\n    \'J_regressor\': src_data[\'J_regressor\'],\n    \'joint_regressor\': src_data[\'cocoplus_regressor\'][:14],\n    \'weights\': np.array(src_data[\'weights\']),\n    \'posedirs\': np.array(src_data[\'posedirs\']),\n    \'v_template\': np.array(src_data[\'v_template\']),\n    \'shapedirs\': np.array(src_data[\'shapedirs\']),\n    \'f\': np.array(src_data[\'f\']),\n    \'kintree_table\': src_data[\'kintree_table\'],\n  }\n  with open(output_path, \'wb\') as f:\n    pickle.dump(model, f)\n'"
data_utils/procrustes.py,18,"b""import numpy as np\nimport torch\nfrom torch.nn import Module\nimport os\nfrom sys import platform\n\nif platform == 'linux':\n    from batch_svd import batch_svd\n\n'''\n    procrustes_3d_to_2d: \n        Align 3D input keypoints to 2D/3D ground truth via Procrustes analysis\n        \n    Parameters:\n    ------------------------------------------------------------\n    J3d: [N * J * 3]: batch 3D input \n    gt3d: groung truth 3D input\n    gt2d: groung truth 2D input\n    \n    Output:\n    ------------------------------------------------------------\n    A transformation function that directly takes input N * None * 3 points\n    and align it with 2D annotations.\n    \n    Algorithm:\n    ------------------------------------------------------------\n    @ Using ground truth 3D to determine rotation\n    @ Using ground truth 2D to determine translation and scaling\n'''    \ndef map_3d_to_2d(J3d, gt2d, gt3d):\n    batch_size = J3d.shape[0]\n    \n    # Part 1: Align J3d with gt3d\n    \n    ### i. centering: must\n    G3d = gt3d.clone()\n    cent_J = torch.mean(J3d, dim=1, keepdim=True)\n    J3d -= cent_J\n    cent_G = torch.mean(G3d, dim=1, keepdim=True)\n    G3d -= cent_G\n    \n    ### ii. scaling: not necessary here.\n    \n    ### iii. rotation\n    M = torch.bmm(G3d.transpose(1,2), J3d) # [N, 3, 3]\n    if platform == 'linux':\n        U, D, V = batch_svd(M)\n        R = torch.bmm(V, U.transpose(1,2))\n    else:\n        R = [None] * batch_size\n        for i in range(batch_size):\n            U, D, V = torch.svd(M[i])\n            R[i] = torch.mm(V, U.transpose(0,1)) # transpose\n        R = torch.stack(R, dim=0)\n        \n    reg3d = torch.bmm(J3d, R)\n    ### eval stage I: rel error < 5%\n    '''\n    print(reg3d - G3d)\n    test_case = range(10)\n    for i in test_case:\n        #np.savetxt('_test_cache/2d_gt_{}.xyz'.format(i), gt2d[i].detach().cpu().numpy(), delimiter=' ')\n        np.savetxt('_test_cache/3d_in_{}.xyz'.format(i), J3d[i].detach().cpu().numpy(), delimiter=' ')\n        np.savetxt('_test_cache/3d_reg_{}.xyz'.format(i), reg3d[i].detach().cpu().numpy(), delimiter=' ')\n        np.savetxt('_test_cache/3d_gt_{}.xyz'.format(i), G3d[i].detach().cpu().numpy(), delimiter=' ')\n    '''\n    \n    # Part II: Align reg3d with gt2d\n    G2d = gt2d.clone()\n    reg2d = reg3d[:, :, :2]\n    ### i. translation\n    cent2d = torch.mean(G2d, dim=1, keepdim=True)\n    G2d -= cent2d\n    cent3d = torch.cat((cent2d,\n        torch.zeros((batch_size, 1, 1), dtype=cent2d.dtype, device=cent2d.device)\n    ), dim=2)\n    \n    ### ii. scaling\n    G2d = G2d.view(batch_size, -1)\n    r2d = reg2d.reshape(batch_size, -1)\n    s = torch.sum(r2d * G2d, dim=1) / torch.sum(r2d * r2d, dim=1)\n    s = s.unsqueeze(dim=1).unsqueeze(dim=2)\n    \n    ### eval: max 5 pixels error...\n    '''\n    reg2d = reg2d * s + cent2d\n    print(reg2d - gt2d)\n    # eval: make sure joint locations matching\n    # 2D not very accurate, consider 3D?\n    # gt3d and gt2d spatially aligned, roughly s * gt3d[:,:,:2] + t = gt2d\n    \n    gt2d = torch.cat((gt2d,\n        torch.zeros((batch_size, gt2d.shape[1], 1), dtype=gt2d.dtype, device=gt2d.device)\n    ), dim=2)\n    reg2d = torch.cat((reg2d,\n        torch.zeros((batch_size, reg2d.shape[1], 1), dtype=reg2d.dtype, device=reg2d.device)\n    ), dim=2)\n    \n    test_case = range(10)\n    for i in test_case:\n        np.savetxt('_test_cache/2d_gt_{}.xyz'.format(i), gt2d[i].detach().cpu().numpy(), delimiter=' ')\n        np.savetxt('_test_cache/2d_reg_{}.xyz'.format(i), reg2d[i].detach().cpu().numpy(), delimiter=' ')\n    '''\n    \n    # Wrap-up the result into a function, keep z component as a depth inidcator\n    def transform(x):\n        return torch.bmm(x - cent_J, R) * s + cent3d\n        \n    return transform\n"""
data_utils/smpl_torch_batch.py,50,"b'import numpy as np\nimport pickle\nimport torch\nfrom torch.nn import Module\nimport os\nfrom time import time\n\nclass SMPLModel(Module):\n  def __init__(self, device=None, model_path=\'./model.pkl\',\n                data_type=torch.float, simplify=False):\n    super(SMPLModel, self).__init__()\n    self.data_type = data_type\n    self.simplify = simplify\n    with open(model_path, \'rb\') as f:\n      params = pickle.load(f)\n    #print(params[\'J_regressor\'].nonzero())\n    self.J_regressor = torch.from_numpy(\n      np.array(params[\'J_regressor\'].todense())\n    ).type(self.data_type)\n    # 20190330: lsp 14 joint regressor\n    self.joint_regressor = torch.from_numpy(\n      np.array(params[\'joint_regressor\'].T.todense())\n    ).type(self.data_type)\n    self.weights = torch.from_numpy(params[\'weights\']).type(self.data_type)\n    self.posedirs = torch.from_numpy(params[\'posedirs\']).type(self.data_type)\n    self.v_template = torch.from_numpy(params[\'v_template\']).type(self.data_type)\n    self.shapedirs = torch.from_numpy(params[\'shapedirs\']).type(self.data_type)\n    self.kintree_table = params[\'kintree_table\']\n    id_to_col = {self.kintree_table[1, i]: i\n                 for i in range(self.kintree_table.shape[1])}\n    self.parent = {\n      i: id_to_col[self.kintree_table[0, i]]\n      for i in range(1, self.kintree_table.shape[1])\n    }\n    self.faces = params[\'f\']\n    self.device = device if device is not None else torch.device(\'cpu\')\n    \n    self.visualize_model_parameters()\n    for name in [\'J_regressor\', \'joint_regressor\', \'weights\', \'posedirs\', \'v_template\', \'shapedirs\']:\n      _tensor = getattr(self, name)\n      print(\' Tensor {} shape: \'.format(name), _tensor.shape)\n      setattr(self, name, _tensor.to(device))\n\n  @staticmethod\n  def rodrigues(r):\n    """"""\n    Rodrigues\' rotation formula that turns axis-angle tensor into rotation\n    matrix in a batch-ed manner.\n\n    Parameter:\n    ----------\n    r: Axis-angle rotation tensor of shape [batch_size * angle_num, 1, 3].\n\n    Return:\n    -------\n    Rotation matrix of shape [batch_size * angle_num, 3, 3].\n\n    """"""\n    eps = r.clone().normal_(std=1e-8)\n    theta = torch.norm(r + eps, dim=(1, 2), keepdim=True)  # dim cannot be tuple\n    theta_dim = theta.shape[0]\n    r_hat = r / theta\n    cos = torch.cos(theta)\n    z_stick = torch.zeros(theta_dim, dtype=r.dtype).to(r.device)\n    m = torch.stack(\n      (z_stick, -r_hat[:, 0, 2], r_hat[:, 0, 1], r_hat[:, 0, 2], z_stick,\n       -r_hat[:, 0, 0], -r_hat[:, 0, 1], r_hat[:, 0, 0], z_stick), dim=1)\n    m = torch.reshape(m, (-1, 3, 3))\n    i_cube = (torch.eye(3, dtype=r.dtype).unsqueeze(dim=0) \\\n             + torch.zeros((theta_dim, 3, 3), dtype=r.dtype)).to(r.device)\n    A = r_hat.permute(0, 2, 1)\n    dot = torch.matmul(A, r_hat)\n    R = cos * i_cube + (1 - cos) * dot + torch.sin(theta) * m\n    return R\n\n  @staticmethod\n  def with_zeros(x):\n    """"""\n    Append a [0, 0, 0, 1] tensor to a [3, 4] tensor.\n\n    Parameter:\n    ---------\n    x: Tensor to be appended.\n\n    Return:\n    ------\n    Tensor after appending of shape [4,4]\n\n    """"""\n    ones = torch.tensor(\n      [[[0.0, 0.0, 0.0, 1.0]]], dtype=x.dtype\n    ).expand(x.shape[0],-1,-1).to(x.device)\n    ret = torch.cat((x, ones), dim=1)\n    return ret\n\n  @staticmethod\n  def pack(x):\n    """"""\n    Append zero tensors of shape [4, 3] to a batch of [4, 1] shape tensor.\n\n    Parameter:\n    ----------\n    x: A tensor of shape [batch_size, 4, 1]\n\n    Return:\n    ------\n    A tensor of shape [batch_size, 4, 4] after appending.\n\n    """"""\n    zeros43 = torch.zeros(\n      (x.shape[0], x.shape[1], 4, 3), dtype=x.dtype).to(x.device)\n    ret = torch.cat((zeros43, x), dim=3)\n    return ret\n\n  def write_obj(self, verts, file_name):\n    with open(file_name, \'w\') as fp:\n      for v in verts:\n        fp.write(\'v %f %f %f\\n\' % (v[0], v[1], v[2]))\n\n      for f in self.faces + 1:\n        fp.write(\'f %d %d %d\\n\' % (f[0], f[1], f[2]))\n\n  def visualize_model_parameters(self):\n    self.write_obj(self.v_template, \'v_template.obj\')\n    \n  \'\'\'\n    _lR2G: Buildin function, calculating G terms for each vertex.\n  \'\'\'  \n  def _lR2G(self, lRs, J):\n    batch_num = lRs.shape[0]\n    results = []    # results correspond to G\' terms in original paper.\n    results.append(\n      self.with_zeros(torch.cat((lRs[:, 0], torch.reshape(J[:, 0, :], (-1, 3, 1))), dim=2))\n    )\n    for i in range(1, self.kintree_table.shape[1]):\n      results.append(\n        torch.matmul(\n          results[self.parent[i]],\n          self.with_zeros(\n            torch.cat(\n              (lRs[:, i], torch.reshape(J[:, i, :] - J[:, self.parent[i], :], (-1, 3, 1))),\n              dim=2\n            )\n          )\n        )\n      )\n    \n    stacked = torch.stack(results, dim=1)\n    deformed_joint = \\\n        torch.matmul(\n          stacked,\n          torch.reshape(\n            torch.cat((J, torch.zeros((batch_num, 24, 1), dtype=self.data_type).to(self.device)), dim=2),\n            (batch_num, 24, 4, 1)\n          )\n        ) \n    results = stacked - self.pack(deformed_joint)\n    return results, lRs\n    \n  def theta2G(self, thetas, J):\n    batch_num = thetas.shape[0]\n    lRs = self.rodrigues(thetas.view(-1, 1, 3)).reshape(batch_num, -1, 3, 3)\n    return self._lR2G(lRs, J)\n  \n  \'\'\'\n    gR2G: Calculate G terms from global rotation matrices.\n    --------------------------------------------------\n    Input: gR: global rotation matrices [N * 24 * 3 * 3]\n           J: shape blended template pose J(b)\n  \'\'\'    \n  def gR2G(self, gR, J):\n    # convert global R to local R\n    lRs = [gR[:, 0]]\n    for i in range(1, self.kintree_table.shape[1]):\n        # Solve the relative rotation matrix at current joint\n        # Apply inverse rotation for all subnodes of the tree rooted at current joint\n        # Update: Compute quick inverse for rotation matrices (actually the transpose)\n        lRs.append(torch.bmm(gR[:, self.parent[i]].transpose(1,2), gR[:, i]))\n        \n    lRs = torch.stack(lRs, dim=1)\n    return self._lR2G(lRs, J)\n        \n  \n  \n  def forward(self, betas, thetas, trans, gR=None):\n    \n    """"""\n          Construct a compute graph that takes in parameters and outputs a tensor as\n          model vertices. Face indices are also returned as a numpy ndarray.\n          \n          20190128: Add batch support.\n          20190322: Extending forward compatiability with SMPLModelv3\n          \n          Usage:\n          ---------\n          meshes, joints = forward(betas, thetas, trans): normal SMPL \n          meshes, joints = forward(betas, thetas, trans, gR=gR): \n                calling from SMPLModelv3, using gR to cache G terms, ignoring thetas\n\n          Parameters:\n          ---------\n          thetas: an [N, 24 * 3] tensor indicating child joint rotation\n          relative to parent joint. For root joint it\'s global orientation.\n          Represented in a axis-angle format.\n\n          betas: Parameter for model shape. A tensor of shape [N, 10] as coefficients of\n          PCA components. Only 10 components were released by SMPL author.\n\n          trans: Global translation tensor of shape [N, 3].\n          \n          G, R_cube_big: (Added on 0322) Fix compatible issue when calling from v3 objects\n            when calling this mode, theta must be set as None\n          \n          Return:\n          ------\n          A 3-D tensor of [N * 6890 * 3] for vertices,\n          and the corresponding [N * 24 * 3] joint positions.\n\n    """"""\n    batch_num = betas.shape[0]\n    \n    v_shaped = torch.tensordot(betas, self.shapedirs, dims=([1], [2])) + self.v_template\n    J = torch.matmul(self.J_regressor, v_shaped)\n    if gR is not None:\n        G, R_cube_big = self.gR2G(gR, J)\n    elif thetas is not None:\n        G, R_cube_big = self.theta2G(thetas, J)  # pre-calculate G terms for skinning\n    else:\n        raise(RuntimeError(\'Either thetas or gR should be specified, but detected two Nonetypes\'))\n         \n    # (1) Pose shape blending (SMPL formula(9))\n    if self.simplify:\n      v_posed = v_shaped\n    else:\n      R_cube = R_cube_big[:, 1:, :, :]\n      I_cube = (torch.eye(3, dtype=self.data_type).unsqueeze(dim=0) + \\\n        torch.zeros((batch_num, R_cube.shape[1], 3, 3), dtype=self.data_type)).to(self.device)\n      lrotmin = (R_cube - I_cube).reshape(batch_num, -1)\n      v_posed = v_shaped + torch.tensordot(lrotmin, self.posedirs, dims=([1], [2]))\n      \n    # (2) Skinning (W)\n    T = torch.tensordot(G, self.weights, dims=([1], [1])).permute(0, 3, 1, 2)\n    rest_shape_h = torch.cat(\n      (v_posed, torch.ones((batch_num, v_posed.shape[1], 1), dtype=self.data_type).to(self.device)), dim=2\n    )\n    v = torch.matmul(T, torch.reshape(rest_shape_h, (batch_num, -1, 4, 1)))\n    v = torch.reshape(v, (batch_num, -1, 4))[:, :, :3]\n    result = v + torch.reshape(trans, (batch_num, 1, 3))\n    \n    # estimate 3D joint locations\n    joints = torch.tensordot(result, self.joint_regressor, dims=([1], [0])).transpose(1, 2)\n    #joints = torch.tensordot(result, self.J_regressor.transpose(0, 1), dims=([1], [0])).transpose(1, 2)\n    return result, joints\n\n\ndef test_gpu(data_type=torch.float):\n  device=torch.device(\'cuda\')\n  pose_size = 72\n  beta_size = 10\n\n  np.random.seed(9608)\n  model = SMPLModel(\n                    device=device,\n                    model_path = \'./model.pkl\',\n                    data_type=data_type,\n                    simplify=True\n                    )\n  \n  pose = torch.from_numpy((np.random.rand(32, pose_size) - 0.5) * 1)\\\n          .type(data_type).to(device)\n  betas = torch.from_numpy(np.zeros((32, beta_size))) \\\n          .type(data_type).to(device)\n  trans = torch.from_numpy(np.zeros((32, 3))).type(data_type).to(device)\n  for i in range(10):\n    s = time()\n    result, joints = model(betas, pose, trans)\n    print(\'Time: {}s\'.format(time()-s))\n  \n  # outmesh_path = \'./24joint/smpl_torch_{}.obj\'\n  # outjoint_path = \'./24joint/smpl_torch_{}.xyz\'\n  # for i in range(result.shape[0]):\n      # model.write_obj(result[i].detach().cpu().numpy(), outmesh_path.format(i))\n      # np.savetxt(outjoint_path.format(i), joints[i].detach().cpu().numpy(), delimiter=\' \')\n  \n  \nif __name__ == \'__main__\':\n  test_gpu()\n'"
data_utils/triangulation.py,2,"b""from smpl_torch_batch import SMPLModel\nimport numpy as np\nimport torch\n\ndef import_verts(obj_file):\n    with open(obj_file, 'r') as f:\n        lines = [\n            line for line in f\n            if line.startswith('v ')\n        ]\n        \n    verts = np.zeros((6890, 3), dtype=np.float)\n    for (i, line) in enumerate(lines):\n        ls = line.split(' ')\n        verts[i,0] = float(ls[1])\n        verts[i,1] = float(ls[2])\n        verts[i,2] = float(ls[3])\n    return verts\n\ndef triangulation(quad_obj, out_obj, trifaces):\n    with open(quad_obj, 'r') as f:\n        all_lines = [line for line in f]\n        \n    is_face = [line.startswith('f ') for line in all_lines]\n    fid = is_face.index(True)\n    other_lines = all_lines[:fid]\n    face_lines = all_lines[fid:]\n        \n    # convert each quad into two triangles\n    # that matches two corresponding lines in trifaces\n    # I assume that SMPL faces are converted this way from fbx\n    triface_pointer = 0\n    tri_face_lines = []\n    for line in face_lines:\n        ls = [item.replace('\\n', '') for item in line.split(' ')[1:]] # weird\n        if len(ls) == 4: # a quad\n            vs = {int(triplet.split('/')[0]):triplet for triplet in ls}\n            triangle_1 = trifaces[triface_pointer]\n            # note that in SMPLModel objects vert index starts with 0 rather than 1\n            tri_line_1 = ' '.join(['f', vs[triangle_1[0]+1], vs[triangle_1[1]+1], vs[triangle_1[2]+1]])\n            tri_face_lines.append(tri_line_1 + '\\n')\n            \n            triangle_2 = trifaces[triface_pointer+1]\n            tri_line_2 = ' '.join(['f', vs[triangle_2[0]+1], vs[triangle_2[1]+1], vs[triangle_2[2]+1]])\n            tri_face_lines.append(tri_line_2 + '\\n')\n            \n            triface_pointer += 2\n        elif len(ls) == 3: # a triangle\n            tri_face_lines.append(line+'\\n')\n            triface_pointer += 1\n        else:\n            print('wtf?')\n    \n    print(len(tri_face_lines), triface_pointer)\n    assert(len(tri_face_lines) == trifaces.shape[0])\n    with open(out_obj, 'w') as f:\n        f.writelines(other_lines + tri_face_lines)\n       \nif __name__ == '__main__':\n    device=torch.device('cuda')\n    data_type=torch.float32\n    pose_size = 72\n    beta_size = 10\n\n    model = SMPLModel(\n                device=device,\n                model_path = './model_lsp.pkl',\n                data_type=data_type,\n                simplify=True\n            )\n            \n    quad_obj = 'untitled.obj'\n    out_obj = 'smpl_fbx_template.obj'\n    fbx_obj_verts = import_verts(quad_obj)\n    #model.write_obj(fbx_obj_verts, 'hybrid_fbx_verts_SMPL_face.obj')\n    triangulation(quad_obj, out_obj, model.faces)\n"""
data_utils/uv_map_generator.py,2,"b""import numpy as np\nimport pickle\nimport torch\nfrom torch.nn import Module\nimport os\nimport shutil\nfrom sys import platform\nfrom skimage.io import imread, imsave\nfrom skimage.draw import circle\nfrom skimage.draw import polygon_perimeter as pope\n\nfrom time import time\nfrom tqdm import tqdm\nfrom numpy.linalg import solve\nfrom scipy.interpolate import RectBivariateSpline as RBS\n\n'''\n    UV_Map_Generator: preparing UV position map labels \n    and resample 3D vertex coords from rendered UV maps.\n'''\nclass UV_Map_Generator():\n    def __init__(self, UV_height, UV_width=-1, \n        UV_pickle='radvani_template.pickle'):\n        self.h = UV_height\n        self.w = self.h if UV_width < 0 else UV_width\n        \n        ### Load UV texcoords and face mapping info\n        if not os.path.isfile(UV_pickle):\n            self._parse_obj(\n                UV_pickle.replace('pickle','obj'), UV_pickle\n            )\n        else:\n            with open(UV_pickle, 'rb') as f:\n                tmp = pickle.load(f)\n            for k in tmp.keys():\n                setattr(self, k, tmp[k])\n        \n        ### Load (or calcluate) barycentric info\n        self.bc_pickle = 'barycentric_h{:04d}_w{:04d}_{}'\\\n            .format(self.h, self.w, UV_pickle)\n        if os.path.isfile(self.bc_pickle):\n            print('Find cached pickle file...')\n            with open(self.bc_pickle, 'rb') as rf:\n                bary_info = pickle.load(rf)\n                self.bary_id = bary_info['face_id']\n                self.bary_weights = bary_info['bary_weights']\n                self.edge_dict = bary_info['edge_dict']\n                \n        else:\n            print('Bary info cache not found, start calculating...' \n                + '(This could take a few minutes)')\n            self.bary_id, self.bary_weights, self.edge_dict = \\\n                self._calc_bary_info(self.h, self.w, self.vt_faces.shape[0])\n    \n    #####################\n    # Private Functions #\n    #####################\n    def _parse_obj(self, obj_file, cache_file):\n        with open(obj_file, 'r') as fin:\n            lines = [l \n                for l in fin.readlines()\n                if len(l.split()) > 0\n                and not l.startswith('#')\n            ]\n        \n        # Load all vertices (v) and texcoords (vt)\n        vertices = []\n        texcoords = []\n        \n        for line in lines:\n            lsp = line.split()\n            if lsp[0] == 'v':\n                x = float(lsp[1])\n                y = float(lsp[2])\n                z = float(lsp[3])\n                vertices.append((x, y, z))\n            elif lsp[0] == 'vt':\n                u = float(lsp[1])\n                v = float(lsp[2])\n                texcoords.append((1 - v, u))\n                \n        # Stack these into an array\n        self.vertices = np.vstack(vertices).astype(np.float32)\n        self.texcoords = np.vstack(texcoords).astype(np.float32)\n        \n        # Load face data. All lines are of the form:\n        # f v1/vt1/vn1 v2/vt2/vn2 v3/vt3/vn3\n        #\n        # Store the texcoord faces and a mapping from texcoord faces\n        # to vertex faces\n        vt_faces = []\n        self.vt_to_v = {}\n        self.v_to_vt = [None] * self.vertices.shape[0]\n        for i in range(self.vertices.shape[0]):\n            self.v_to_vt[i] = set()\n        \n        for line in lines:\n            vs = line.split()\n            if vs[0] == 'f':\n                v0 = int(vs[1].split('/')[0]) - 1\n                v1 = int(vs[2].split('/')[0]) - 1\n                v2 = int(vs[3].split('/')[0]) - 1\n                vt0 = int(vs[1].split('/')[1]) - 1\n                vt1 = int(vs[2].split('/')[1]) - 1\n                vt2 = int(vs[3].split('/')[1]) - 1\n                vt_faces.append((vt0, vt1, vt2))\n                self.vt_to_v[vt0] = v0\n                self.vt_to_v[vt1] = v1\n                self.vt_to_v[vt2] = v2\n                self.v_to_vt[v0].add(vt0)\n                self.v_to_vt[v1].add(vt1)\n                self.v_to_vt[v2].add(vt2)\n                \n        self.vt_faces = np.vstack(vt_faces)\n        tmp_dict = {\n            'vertices': self.vertices,\n            'texcoords': self.texcoords,\n            'vt_faces': self.vt_faces,\n            'vt_to_v': self.vt_to_v,\n            'v_to_vt': self.v_to_vt\n        }\n        with open(cache_file, 'wb') as w:\n            pickle.dump(tmp_dict, w)\n            \n    '''\n    _calc_bary_info: for a given uv vertice position,\n    return the berycentric information for all pixels\n    \n    Parameters:\n    ------------------------------------------------------------\n    h, w: image size\n    faces: [F * 3], triangle pieces represented with UV vertices.\n    uvs: [N * 2] UV coordinates on texture map, scaled with h & w\n    \n    Output: \n    ------------------------------------------------------------\n    bary_dict: A dictionary containing three items, saved as pickle.\n    @ face_id: [H*W] int tensor where f[i,j] represents\n        which face the pixel [i,j] is in\n        if [i,j] doesn't belong to any face, f[i,j] = -1\n    @ bary_weights: [H*W*3] float tensor of barycentric coordinates \n        if f[i,j] == -1, then w[i,j] = [0,0,0]\n    @ edge_dict: {(u,v):n} dict where (u,v) indicates the pixel to \n        dilate, with n being non-zero neighbors within 8-neighborhood\n        \n        \n    Algorithm:\n    ------------------------------------------------------------\n    The barycentric coordinates are obtained by \n    solving the following linear equation:\n    \n            [ x1 x2 x3 ][w1]   [x]\n            [ y1 y2 y3 ][w2] = [y]\n            [ 1  1  1  ][w3]   [1]\n    \n    Note: This algorithm is not the fastest but can be batchlized.\n    It could take 8~10 minutes on a regular PC for 300*300 maps. \n    Luckily, for each experiment the bary_info only need to be \n    computed once, so I just stick to the current implementation.\n    '''\n    \n    def _calc_bary_info(self, h, w, F):\n        s = time()\n        face_id = np.zeros((h, w), dtype=np.int)\n        bary_weights = np.zeros((h, w, 3), dtype=np.float32)\n        \n        uvs = self.texcoords * np.array([[self.h - 1, self.w - 1]])\n        grids = np.ones((F, 3), dtype=np.float32)\n        anchors = np.concatenate((\n            uvs[self.vt_faces].transpose(0,2,1),\n            np.ones((F, 1, 3), dtype=uvs.dtype)\n        ), axis=1) # [F * 3 * 3]\n        \n        _loop = tqdm(np.arange(h*w), ncols=80)\n        for i in _loop:\n            r = i // w\n            c = i % w\n            grids[:, 0] = r\n            grids[:, 1] = c\n            \n            weights = solve(anchors, grids) # not enough accuracy?\n            inside = np.logical_and.reduce(weights.T > 1e-10)\n            index = np.where(inside == True)[0]\n            \n            if 0 == index.size:\n                face_id[r,c] = -1  # just assign random id with all zero weights.\n            #elif index.size > 1:\n            #    print('bad %d' %i)\n            else:\n                face_id[r,c] = index[0]\n                bary_weights[r,c] = weights[index[0]]\n\n        # calculate counter pixels for UV_map dilation\n        _mask = np.where(face_id == -1, 0, 1)\n        edge_dict = {}\n        _loop = _loop = tqdm(np.arange((h-2)*(w-2)), ncols=80)\n        for l in _loop:\n            i = l // (w-2) + 1 \n            j = l % (w-2) + 1\n            _neighbor = np.array([\n                _mask[i-1, j], _mask[i-1, j+1],\n                _mask[i, j+1], _mask[i+1, j+1],\n                _mask[i+1, j], _mask[i+1, j-1],\n                _mask[i, j-1], _mask[i-1, j-1],\n            ])\n            \n            if _mask[i,j] == 0 and _neighbor.min() != _neighbor.max():\n                edge_dict[(i, j)] = np.count_nonzero(_neighbor)\n                    \n            \n        print('Calculating finished. Time elapsed: {}s'.format(time()-s))\n        \n        bary_dict = {\n            'face_id': face_id,\n            'bary_weights': bary_weights,\n            'edge_dict': edge_dict\n        }\n        \n        with open(self.bc_pickle, 'wb') as wf:\n            pickle.dump(bary_dict, wf)\n            \n        return face_id, bary_weights, edge_dict\n        \n    '''\n        _dilate: perform dilate_like operation for initial\n        UV map, to avoid out-of-region error when resampling\n    '''\n    def _dilate(self, UV_map, pixels=1):\n        _UV_map = UV_map.copy()\n        for k, v in self.edge_dict.items():\n            i, j = k[0], k[1]\n            _UV_map[i, j] = np.sum(np.array([\n                UV_map[i-1, j], UV_map[i-1, j+1],\n                UV_map[i, j+1], UV_map[i+1, j+1],\n                UV_map[i+1, j], UV_map[i+1, j-1],\n                UV_map[i, j-1], UV_map[i-1, j-1],\n            ]), axis=0) / v\n    \n        return _UV_map\n        \n    ####################\n    # Render Functions #\n    ####################\n    \n    '''\n        Render a point cloud to an image of [self.h, self.w] size. This point cloud\n        approximates what the UV position map will look like: for each texture\n        coordinate, we render its corresponding _vertex_ by setting the \n        vertex's (normalized) XYZ to the RGB of the pixel corresponding to the\n        texture coordinate.\n        \n        verts: [N * 3] vertex coordinates\n        rgbs: [N * 3] RGB values in [0,1] float.\n            If vertex color is not defined, \n            the normalized XYZ coordinates will be used\n    '''\n    def render_point_cloud(self, img_name=None, verts=None, rgbs=None, eps=1e-8):\n        if verts is None:\n            verts = self.vertices\n        if rgbs is None:\n            #print('Warning: rgb not specified, use normalized 3d coords instead...')\n            v_min = np.amin(verts, axis=0, keepdims=True)\n            v_max = np.amax(verts, axis=0, keepdims=True)\n            rgbs = (verts - v_min) / np.maximum(eps, v_max - v_min)\n        \n        vt_id = [self.vt_to_v[i] for i in range(self.texcoords.shape[0])]\n        img = np.zeros((self.h, self.w, 3), dtype=rgbs.dtype)\n        uvs = (self.texcoords * np.array([[self.h - 1, self.w - 1]])).astype(np.int)\n        \n        img[uvs[:, 0], uvs[:, 1]] = rgbs[vt_id]\n        \n        if img_name is not None:\n            imsave(img_name, img)\n            \n        return img, verts, rgbs\n        \n    def render_UV_atlas(self, image_name, size=1024):\n        if self.vt_faces is None:\n            print('Cyka Blyat: Load an obj file first!')\n        \n        faces = (self.texcoords[self.vt_faces] * size).astype(np.int32)\n        img = np.zeros((size, size), dtype=np.uint8)\n        for f in faces:\n            rr, cc = pope(f[:,0], f[:,1], shape=(size, size))\n            img[rr, cc] = 255\n            \n        imsave(image_name, img)\n    \n    def write_ply(self, ply_name, verts, rgbs=None, eps=1e-8):\n        if rgbs is None:\n            #print('Warning: rgb not specified, use normalized 3d coords instead...')\n            v_min = np.amin(verts, axis=0, keepdims=True)\n            v_max = np.amax(verts, axis=0, keepdims=True)\n            rgbs = (verts - v_min) / np.maximum(eps, v_max - v_min)\n        if rgbs.max() < 1.001:\n            rgbs = (rgbs * 255.).astype(np.uint8)\n        \n        with open(ply_name, 'w') as f:\n            # headers\n            f.writelines([\n                'ply\\n'\n                'format ascii 1.0\\n',\n                'element vertex {}\\n'.format(verts.shape[0]),\n                'property float x\\n',\n                'property float y\\n',\n                'property float z\\n',\n                'property uchar red\\n',\n                'property uchar green\\n',\n                'property uchar blue\\n',\n                'end_header\\n',\n                ]\n            )\n            \n            for i in range(verts.shape[0]):\n                str = '{:10.6f} {:10.6f} {:10.6f} {:d} {:d} {:d}\\n'\\\n                    .format(verts[i,0], verts[i,1], verts[i,2],\n                        rgbs[i,0], rgbs[i,1], rgbs[i,2])\n                f.write(str)\n                \n        return verts, rgbs\n    \n    #####################\n    # UV Map Generation #\n    #####################\n    '''\n    UV_interp: barycentric interpolation from given\n    rgb values at non-integer UV vertices.\n    \n    Parameters:\n    ------------------------------------------------------------\n    rgbs: [N * 3] rgb colors at given uv vetices.\n    \n    Output: \n    ------------------------------------------------------------\n    UV_map: colored texture map with the same size as im\n    '''    \n    def UV_interp(self, rgbs):\n        face_num = self.vt_faces.shape[0]\n        vt_num = self.texcoords.shape[0]\n        assert(vt_num == rgbs.shape[0])\n        \n        uvs = self.texcoords * np.array([[self.h - 1, self.w - 1]])\n        \n        #print(np.max(rgbs), np.min(rgbs))\n        triangle_rgbs = rgbs[self.vt_faces][self.bary_id]\n        bw = self.bary_weights[:,:,np.newaxis,:]\n        #print(triangle_rgbs.shape, bw.shape)\n        im = np.matmul(bw, triangle_rgbs).squeeze(axis=2)\n        \n        '''\n        for i in range(height):\n            for j in range(width):\n                t0 = faces[fid[i,j]][0]\n                t1 = faces[fid[i,j]][1]\n                t2 = faces[fid[i,j]][2]\n                im[i,j] = (w[i,j,0] * rgbs[t0] + w[i,j,1] * rgbs[t1] + w[i,j,2] * rgbs[t2])\n        '''\n        \n        #print(im.shape, np.max(im), np.min(im))\n        im = np.minimum(np.maximum(im, 0.), 1.)\n        return im\n        \n    '''\n    get_UV_map: create UV position map from aligned mesh coordinates\n    Parameters:\n    ------------------------------------------------------------\n    verts: [V * 3], aligned mesh coordinates.\n    \n    Output: \n    ------------------------------------------------------------\n    UV_map: [H * W * 3] Interpolated UV map.\n    colored_verts: [H * W * 3] Scatter plot of colorized UV vertices\n    '''\n    def get_UV_map(self, verts, dilate=True):\n        # normalize all to [0,1]\n        _min = np.amin(verts, axis=0, keepdims=True)\n        _max = np.amax(verts, axis=0, keepdims=True)\n        verts = (verts - _min) / (_max - _min)\n        verts_backup = verts.copy()\n        \n        vt_to_v_index = np.array([\n            self.vt_to_v[i] for i in range(self.texcoords.shape[0])\n        ])\n        rgbs = verts[vt_to_v_index]\n        \n        uv_map = self.UV_interp(rgbs)\n        if dilate:\n            uv_map = self._dilate(uv_map)\n        return uv_map, verts_backup\n    \n    '''\n        TODO: make it torch.\n    '''\n    def resample(self, UV_map):\n        h, w, c = UV_map.shape\n        vts = np.floor(self.texcoords * np.array([[self.h - 1, self.w - 1]])).astype(np.int)\n        vt_3d = [None] * vts.shape[0]\n        \n        for i in range(vts.shape[0]):\n            \n            coords = [\n                (vts[i, 0], vts[i, 1]),\n                (vts[i, 0], vts[i, 1]+1),\n                (vts[i, 0]+1, vts[i, 1]),\n                (vts[i, 0]+1, vts[i, 1]+1),\n            ]\n            for coord in coords:\n                if UV_map[coord[0], coord[1]].max() > 0:\n                    vt_3d[i] = UV_map[coord[0], coord[1]]\n                    \n        vt_3d = np.stack(vt_3d)\n        # convert vt back to v (requires v_to_vt index)\n        cyka_v_3d = [None] * len(self.v_to_vt)\n        for i in range(len(self.v_to_vt)):\n            cyka_v_3d[i] = np.mean(vt_3d[list(self.v_to_vt[i])], axis=0)\n        \n        cyka_v_3d = np.array(cyka_v_3d)\n        return cyka_v_3d\n\n        \nif __name__ == '__main__':\n    # test render module\n    # change this to the same as in train.py opt.uv_prefix\n    # file_prefix = 'radvani_template'\n    #file_prefix = 'vbml_close_template'\n    #file_prefix = 'vbml_spaced_template'\n    file_prefix = 'smpl_fbx_template'\n    generator = UV_Map_Generator(\n        UV_height=512,\n        UV_pickle=file_prefix+'.pickle'\n    )\n    test_folder = 'smpl_512'\n    if not os.path.isdir(test_folder):\n        os.makedirs(test_folder)\n        \n    generator.render_UV_atlas('{}/{}_atlas.png'.format(test_folder, file_prefix))\n    img, verts, rgbs = generator.render_point_cloud('{}/{}.png'.format(test_folder, file_prefix))\n    verts, rgbs = generator.write_ply('{}/{}.ply'.format(test_folder, file_prefix), verts, rgbs)\n    uv, _ = generator.get_UV_map(verts, dilate=False)\n    uv = uv.max(axis=2)\n    print(uv.shape)\n    binary_mask = np.where(uv > 0, 1., 0.)\n    binary_mask = (binary_mask * 255).astype(np.uint8)\n    imsave('./{}_UV_mask.png'.format(file_prefix), binary_mask)\n"""
data_utils/uv_map_generator_unit_test.py,6,"b""from smpl_torch_batch import SMPLModel\nimport numpy as np\nimport pickle \nimport h5py\nimport torch\nfrom torch.nn import Module\nimport os\nimport shutil\nfrom time import time\nfrom sys import platform\nfrom torch.utils.data import Dataset, DataLoader\nfrom skimage.io import imread, imsave\nfrom skimage.draw import circle\n\nfrom procrustes import map_3d_to_2d\nfrom uv_map_generator import UV_Map_Generator\n#from uv_map_generator_dev import UV_Map_Generator\n\n\nclass Human36MDataset(Dataset):\n    def __init__(self, smpl, max_item=312188, root_dir=None, \n            annotation='annotation_large.h5', calc_mesh=False):\n        super(Human36MDataset, self).__init__()\n        if root_dir is None:            \n            root_dir = '/backup1/lingboyang/data/human36m' \\\n                if platform == 'linux' \\\n                else 'D:/data/human36m'\n            \n        self.root_dir = root_dir\n        self.calc_mesh = calc_mesh\n        self.smpl = smpl\n        self.dtype = smpl.data_type\n        self.device = smpl.device\n        fin = h5py.File(os.path.join(root_dir, annotation), 'r')\n        self.itemlist = []\n        '''\n        center\n        gt2d\n        gt3d\n        height\n        imagename\n        pose\n        shape\n        smpl_joint\n        width\n        '''\n        for k in fin.keys():\n            data = fin[k][:max_item]\n            if k.startswith('gt'):  # reshape coords\n                data = data.reshape(-1, 14, 3)\n                if k == 'gt2d':  # remove confidence score\n                    data = data[:,:,:2]\n                \n            setattr(self, k, data)\n            self.itemlist.append(k)\n        \n        # flip gt2d y coordinates\n        self.gt2d[:,:,1] = self.height[:, np.newaxis] - self.gt2d[:,:,1]\n        # flip gt3d y & z coordinates\n        self.gt3d[:,:,1:] *= -1\n        \n        fin.close()\n        self.length = self.pose.shape[0]\n        \n    def __getitem__(self, index):\n        out_dict = {}\n        for item in self.itemlist:\n            if item == 'imagename':\n                out_dict[item] = [self.root_dir + '/' + b.decode()\n                    for b in getattr(self, item)[index]]\n            else:\n                data_npy = getattr(self, item)[index]\n                out_dict[item] = torch.from_numpy(data_npy)\\\n                    .type(self.dtype).to(self.device)\n            \n        if self.calc_mesh:\n            _trans = torch.zeros((out_dict['pose'].shape[0], 3), \n                dtype=self.dtype, device=self.device)\n            meshes, lsp_joints = self.smpl(out_dict['shape'], out_dict['pose'], _trans)\n            out_dict['meshes'] = meshes\n            out_dict['lsp_joints'] = lsp_joints\n        \n        return out_dict\n        \n    def __len__(self):\n        return self.length\n    \ndef visualize(folder, imagenames, mesh_2d, joints_2d):\n    i = 0\n    for name, mesh, joints in zip(imagenames, mesh_2d, joints_2d):\n        shutil.copyfile(name,\n            '/im_gt_{}.png'.format(folder, i)\n        )\n        im = imread(name)\n        shape = im.shape[0:2]\n        height = im.shape[0]\n        for p2d in mesh:\n            im[height - p2d[1], p2d[0]] = [127,127,127]\n            \n        for j2d in joints:\n            rr, cc = circle(height - j2d[1], j2d[0], 2, shape)\n            im[rr, cc] = [255, 0, 0]\n            \n        imsave('{}/im_mask_{}.png'.format(folder, i), im)\n        i += 1\n\n    \ndef run_test():\n    if platform == 'linux':\n        os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n    \n    data_type = torch.float32\n    device=torch.device('cuda')\n    pose_size = 72\n    beta_size = 10\n\n    np.random.seed(9608)\n    model = SMPLModel(\n            device=device,\n            model_path = './model_lsp.pkl',\n            data_type=data_type,\n        )\n    dataset = Human36MDataset(model, max_item=100, calc_mesh=True)\n    \n    # generate mesh, align with 14 point ground truth\n    case_num = 10\n    data = dataset[:case_num]\n    meshes = data['meshes']\n    input = data['lsp_joints']\n    target_2d = data['gt2d']\n    target_3d = data['gt3d']\n    \n    transforms = map_3d_to_2d(input, target_2d, target_3d)\n    \n    # Important: mesh should be centered at the origin!\n    deformed_meshes = transforms(meshes)\n    mesh_3d = deformed_meshes.detach().cpu().numpy()\n    \n    file_prefix = 'smpl_fbx_template'\n    generator = UV_Map_Generator(\n        UV_height=256,\n        UV_pickle=file_prefix+'.pickle'\n    )\n    \n    test_folder = '_test_smpl_fbx'\n    if not os.path.isdir(test_folder):\n        os.makedirs(test_folder)\n        \n    visualize( test_folder, data['imagename'], mesh_3d[:,:,:2].astype(np.int), \n       target_2d.detach().cpu().numpy().astype(np.int))\n        \n    s=time()\n    for i, mesh in enumerate(mesh_3d):\n        model.write_obj(\n            mesh, \n            '{}/real_mesh_{}.obj'.format(test_folder, i)\n        )   # weird.\n        \n        UV_position_map, verts_backup = \\\n            generator.get_UV_map(mesh)\n        \n        # write colorized coordinates to ply\n        UV_scatter, _, _ = generator.render_point_cloud(\n            verts=mesh\n        )\n        generator.write_ply(\n            '{}/colored_mesh_{}.ply'.format(test_folder, i), mesh\n        )\n        \n        out = np.concatenate(\n            (UV_position_map, UV_scatter), axis=1\n        )\n        \n        imsave('{}/UV_position_map_{}.png'.format(test_folder, i), out)\n        \n        resampled_mesh = generator.resample(UV_position_map)\n        \n        model.write_obj(\n            resampled_mesh, \n            '{}/recon_mesh_{}.obj'.format(test_folder, i)\n        )\n    print('{} cases for {}s' .format(case_num, time()-s))\n\nif __name__ == '__main__':\n    run_test()\n    """
data_utils/visualizer.py,1,"b""from .smpl_torch_batch import SMPLModel\r\nfrom .uv_map_generator import UV_Map_Generator\r\nimport os\r\nfrom cv2 import imwrite\r\nimport torch\r\nimport numpy as np\r\n\r\nclass Visualizer():\r\n    def __init__(self, opt):\r\n        os.chdir(opt.project_root + '/data_utils')\r\n        self.UV_sampler = UV_Map_Generator(\r\n            UV_height=opt.im_size,\r\n            UV_pickle=opt.uv_prefix+'.pickle'\r\n        )\r\n        # Only use save obj \r\n        self.model = SMPLModel(\r\n            device=None,\r\n            model_path = './model_lsp.pkl',\r\n        )\r\n        os.chdir(opt.project_root)\r\n        if opt.phase == 'train':\r\n            self.save_root = '{}/{}/visuals/'.format(opt.checkpoints_dir, opt.name)\r\n        elif opt.phase == 'test':\r\n            self.save_root = '{}/{}/visuals/'.format(opt.results_dir, opt.name)\r\n        else:\r\n            self.save_root = '{}/{}/{}_in_the_wild/'.format(opt.results_dir, opt.name, opt.dataset)\r\n        if not os.path.isdir(self.save_root):\r\n            os.makedirs(self.save_root)\r\n    \r\n    @staticmethod\r\n    def tensor2im(tensor):\r\n        # input: cuda tensor (CHW) [-1,1]; output: numpy uint8 [0,255] (HWC)\r\n        return ((tensor.detach().cpu().numpy().transpose(1, 2, 0) + 1.) * 127.5).astype(np.uint8)\r\n    \r\n    @staticmethod    \r\n    def tensor2numpy(tensor):\r\n        # input: cuda tensor (CHW) [-1,1]; output: numpy float [0,1] (HWC)\r\n        return (tensor.detach().cpu().numpy().transpose(1, 2, 0) + 1.) / 2.\r\n    \r\n    def save_results(self, visual_dict, epoch, batch):\r\n        img_name = self.save_root + '{:03d}_{:05d}.png'.format(epoch, batch)\r\n        obj_name = self.save_root + '{:03d}_{:05d}.obj'.format(epoch, batch)\r\n        ply_name = self.save_root + '{:03d}_{:05d}.ply'.format(epoch, batch)\r\n        imwrite(img_name, \r\n            self.tensor2im(torch.cat([im for im in visual_dict.values()], dim=2))\r\n        )\r\n        fake_UV = visual_dict['fake_UV']\r\n        resampled_verts = self.UV_sampler.resample(self.tensor2numpy(fake_UV))\r\n        self.UV_sampler.write_ply(ply_name, resampled_verts)\r\n        self.model.write_obj(resampled_verts, obj_name)\r\n            \r\n"""
models/__init__.py,0,"b'import importlib\nfrom models.base_model import BaseModel\n\ndef find_model_using_name(model_name):\n    # Given the option --model [modelname],\n    # the file ""models/modelname_model.py""\n    # will be imported.\n    model_filename = ""models."" + model_name + ""_model""\n    modellib = importlib.import_module(model_filename)\n\n    # In the file, the class called ModelNameModel() will\n    # be instantiated. It has to be a subclass of BaseModel,\n    # and it is case-insensitive.\n    model = None\n    target_model_name = model_name.replace(\'_\', \'\') + \'model\'\n    for name, cls in modellib.__dict__.items():\n        if name.lower() == target_model_name.lower() \\\n           and issubclass(cls, BaseModel):\n            model = cls\n\n    if model is None:\n        print(""In %s.py, there should be a subclass of BaseModel with class name that matches %s in lowercase."" % (model_filename, target_model_name))\n        exit(0)\n\n    return model\n\ndef get_option_setter(model_name):\n    model_class = find_model_using_name(model_name)\n    return model_class.modify_commandline_options\n\ndef create_model(opt):\n    model = find_model_using_name(opt.model)\n    instance = model()\n    instance.initialize(opt)\n    print(""model [%s] was created"" % (instance.name()))\n    return instance\n'"
models/base_model.py,5,"b""import os\nimport torch\nfrom collections import OrderedDict\nfrom . import networks\n\nclass BaseModel():\n    def name(self):\n        return 'BaseModel'\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.isTrain = opt.phase == 'train'\n        self.device = torch.device('cuda' if torch.cuda.is_available() else torch.device('cpu'))\n        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n        if not os.path.isdir(self.save_dir):\n            os.makedirs(self.save_dir)\n        torch.backends.cudnn.benchmark = True\n        \n    def setup(self, opt):\n        if self.isTrain:\n            self.schedulers = [networks.get_scheduler(optimizer, opt) for optimizer in self.optimizers]\n\n        if not self.isTrain or opt.continue_train:\n            self.load_networks(opt.load_epoch)\n        self.print_networks(opt.verbose)\n\n    def set_input(self, input):\n        self.real_input = input['im_data']\n        if not self.opt.phase == 'in_the_wild':\n            self.real_UV = input['uv_data']\n\n    def forward(self):\n        pass\n\n    def is_train(self):\n        return True\n\n    def set_requires_grad(self, net, requires_grad=False):\n        if net is not None:\n            for param in net.parameters():\n                param.requires_grad = requires_grad  # to avoid computation\n\n    # used in test time, wrapping `forward` in no_grad() so we don't save\n    # intermediate steps for backprop\n    def test(self):\n        with torch.no_grad():\n            self.forward()\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n    def optimize_parameters(self):\n        pass\n\n    # update learning rate (called once every epoch)\n    def update_learning_rate(self, metrics):\n        for scheduler in self.schedulers:\n            if self.opt.lr_policy == 'plateau':\n                scheduler.step(metrics=metrics)\n            else:\n                scheduler.step()\n        lr = self.optimizers[0].param_groups[0]['lr']\n        print('learning rate = %.7f' % lr)\n\n    # make models eval mode during test time\n    def eval(self):\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, name)\n                net.eval()\n\n    # save models to the disk\n    def save_networks(self, epoch):\n        for name in self.model_names:\n            if isinstance(name, str):\n                save_filename = '%s_net_%s.pth' % (epoch, name)\n                save_path = os.path.join(self.save_dir, save_filename)\n                net = getattr(self, name)\n                torch.save(net.state_dict(), save_path)\n        print('Checkpoints saved at epoch %d' % epoch)\n\n    # load models from the disk\n    def load_networks(self, epoch):\n        for name in self.model_names:\n            if isinstance(name, str):\n                load_filename = '%s_net_%s.pth' % (epoch, name)\n                load_path = os.path.join(self.save_dir, load_filename)\n                state_dict = torch.load(load_path)\n                net = getattr(self, name)\n                net.load_state_dict(state_dict)\n        print('Checkpoints loaded. Resume training from epoch %d' % epoch)\n\n    # print network information\n    def print_networks(self, verbose):\n        print('---------- Networks initialized -------------')\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, name)\n                num_params = 0\n                for param in net.parameters():\n                    num_params += param.numel()\n                if verbose:\n                    print(net)\n                print('[Network %s] Total number of parameters : %.3f M' % (name, num_params / 1e6))\n        print('-----------------------------------------------')\n"""
models/create_model.py,0,b''
models/networks.py,9,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport functools\nfrom torch.optim import lr_scheduler\nimport os\nimport numpy as np\nfrom cv2 import imread, imwrite, connectedComponents\n\n#####################\n#   Initializers    #\n#####################\n\ndef init_weights(net, init_type=\'normal\', gain=0.02):\n    def init_func(m):\n        classname = m.__class__.__name__\n        if hasattr(m, \'weight\') and (classname.find(\'Conv\') != -1 or classname.find(\'Linear\') != -1):\n            if init_type == \'normal\':\n                init.normal_(m.weight.data, 0.0, gain)\n            elif init_type == \'xavier\':\n                init.xavier_normal_(m.weight.data, gain=gain)\n            elif init_type == \'kaiming\':\n                init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\')\n            elif init_type == \'orthogonal\':\n                init.orthogonal_(m.weight.data, gain=gain)\n            else:\n                raise NotImplementedError(\'initialization method [%s] is not implemented\' % init_type)\n            if hasattr(m, \'bias\') and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find(\'BatchNorm2d\') != -1:\n            init.normal_(m.weight.data, 1.0, gain)\n            init.constant_(m.bias.data, 0.0)\n\n    print(\'initialize network with %s\' % init_type)\n    net.apply(init_func)\n    \ndef init_net(net, init_type=\'normal\', device=torch.device(\'cuda\')):\n    init_weights(net, init_type)\n    return net.to(device)\n\ndef get_scheduler(optimizer, opt):\n    if opt.lr_policy == \'lambda\':\n        def lambda_rule(epoch):\n            lr_l = 1.0 - max(0, epoch - opt.niter) / float(opt.niter_decay + 1)\n            return lr_l\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n    elif opt.lr_policy == \'step\':\n        scheduler = lr_scheduler.StepLR(\n            optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n    elif opt.lr_policy == \'plateau\':\n        scheduler = lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode=\'min\', factor=0.2, threshold=0.01, patience=5)\n    else:\n        return NotImplementedError(\'learning rate policy [%s] is not implemented\', opt.lr_policy)\n    return scheduler\n\ndef get_norm_layer(layer_type=\'instance\'):\n    if layer_type == \'batch\':\n        #norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n        norm_layer = nn.BatchNorm2d\n    elif layer_type == \'instance\':\n        norm_layer = nn.InstanceNorm2d\n    elif layer_type == \'none\':\n        norm_layer = None\n    else:\n        raise NotImplementedError(\n            \'normalization layer [%s] is not found\' % layer_type)\n    return norm_layer\n    \ndef get_non_linearity(layer_type=\'relu\'):\n    if layer_type == \'relu\':\n        nl_layer = nn.ReLU(inplace=True)\n    elif layer_type == \'lrelu\':\n        nl_layer = nn.LeakyReLU(0.2, inplace=True)\n    elif layer_type == \'elu\':\n        nl_layer = nn.ELU(inplace=True)\n    else:\n        raise NotImplementedError(\n            \'nonlinearity activitation [%s] is not found\' % layer_type)\n    return nl_layer\n\ndef define_encoder(im_size, nz, nef, netE, ndown, norm=\'batch\', nl=\'lrelu\', init_type=\'xavier\', device=None):\n    net = None\n    norm_layer = get_norm_layer(layer_type=norm)\n    nl = \'lrelu\'  # use leaky relu for E\n    nl_layer = get_non_linearity(layer_type=nl)\n    if netE == \'resnet\':\n        net = ResNetEncoder(im_size, nz, nef, ndown, norm_layer, nl_layer)\n    elif netE == \'vggnet\':\n        net = VGGEncoder(im_size, nz, nef, ndown, norm_layer, nl_layer)\n    else:\n        raise NotImplementedError(\'Encoder model name [%s] is not recognized\' % netE)\n\n    return init_net(net, init_type, device)\n    \ndef define_decoder(im_size, nz, ndf, netD, nup, norm=\'batch\', nl=\'lrelu\', init_type=\'xavier\', device=None):\n    net = None\n    norm_layer = get_norm_layer(layer_type=norm)\n    nl_layer = get_non_linearity(layer_type=nl)\n    if netD == \'convres\':\n        net = ConvResDecoder(im_size, nz, ndf, nup=nup, norm_layer=norm_layer, nl_layer=nl_layer)\n    elif netD == \'conv-up\':\n        net = ConvUpSampleDecoder(im_size, nz, ndf, nup=nup, norm_layer=norm_layer, nl_layer=nl_layer)\n    else:\n        raise NotImplementedError(\'Decoder model name [%s] is not recognized\' % netD)\n\n    return init_net(net, init_type, device)\n    \n\n#####################\n#      Losses       #   \n#####################\n\ndef acquire_weights(UV_weight_npy):\n    if os.path.isfile(UV_weight_npy):\n        return np.load(UV_weight_npy)\n    else:\n        mask_name = UV_weight_npy.replace(\'weights.npy\', \'mask.png\')\n        print(mask_name)\n        UV_mask = imread(mask_name)\n        if UV_mask.ndim == 3:\n            UV_mask = UV_mask[:,:,0]\n        ret, labels = connectedComponents(UV_mask, connectivity=4)\n        unique, counts = np.unique(labels, return_counts=True)\n        print(unique, counts)\n        \n        UV_weights = np.zeros_like(UV_mask).astype(np.float32)\n        for id, count in zip(unique, counts):\n            if id == 0:\n                continue\n            indices = np.argwhere(labels == id)\n            UV_weights[indices[:,0], indices[:,1]] = 1 / count\n        \n        UV_weights *= np.prod(UV_mask.shape)   # adjust loss to [0,10] level.\n        np.save(UV_weight_npy, UV_weights)\n        return UV_weights\n        \n        \nclass WeightedL1Loss(nn.Module):\n    def __init__(self, uv_map, device):\n        super(WeightedL1Loss, self).__init__()\n        self.weight = torch.from_numpy(acquire_weights(\n            (\'data_utils/{}_UV_weights.npy\'.format(uv_map))\n        )).to(device)\n        print(self.weight.shape)\n        self.loss = nn.L1Loss()\n    \n    def __call__(self, input, target):\n        return self.loss(input * self.weight, target * self.weight)\n    \nclass TotalVariationLoss(nn.Module):\n    def __init__(self, uv_map, device):\n        super(TotalVariationLoss, self).__init__()\n        weight = torch.from_numpy(acquire_weights(\n            (\'data_utils/{}_UV_weights.npy\'.format(uv_map))\n        )).to(device)\n        self.weight = weight[0:-1, 0:-1]\n        self.factor = self.weight.shape[0] * self.weight.shape[1]\n        \n    def __call__(self, input):\n        tv = torch.abs(input[:,:,0:-1, 0:-1] - input[:,:,0:-1, 1:]) \\\n            + torch.abs(input[:,:,0:-1, 0:-1] - input[:,:,1:, 0:-1])\n        return torch.sum(tv * self.weight) / self.factor\n    \n#####################\n#      Networks     #\n#####################\n\n#####  ResNet  #####\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n# Use kernel size 4 to make sure deconv(conv(x)) has the same shape as x\n# TODO: replace convtrans to upsample to reduce checkerboard artifacts\n# not working well...\n# https://distill.pub/2016/deconv-checkerboard/\ndef deconv3x3(in_planes, out_planes, stride=1):\n    return nn.Sequential(\n        nn.Upsample(scale_factor=stride, mode=\'bilinear\'),\n        nn.ReflectionPad2d(1),\n        nn.Conv2d(in_planes, out_planes,\n                  kernel_size=3, stride=1, padding=0)\n    )\n    # return nn.ConvTranspose2d(in_planes, out_planes, kernel_size=4, stride=stride,\n    #                           padding=1, bias=False)\n\n# Basic resnet block:\n# x ---------------- shortcut ---------------x\n# \\___conv___norm____relu____conv____norm____/\nclass BasicResBlock(nn.Module):\n    def __init__(self, inplanes, norm_layer=nn.BatchNorm2d,\n                 activation_layer=nn.LeakyReLU(0.2, True)):\n        super(BasicResBlock, self).__init__()\n\n        self.norm_layer = norm_layer\n        self.activation_layer = activation_layer\n        self.inplanes = inplanes\n\n        layers = [\n            conv3x3(inplanes, inplanes),\n            norm_layer(inplanes),\n            activation_layer,\n            conv3x3(inplanes, inplanes),\n            norm_layer(inplanes)\n        ]\n        self.res = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.res(x) + x\n\n# ResBlock: A classic ResBlock with 2 conv layers and a up/downsample conv layer. (2+1)\n# x ---- BasicConvBlock ---- ReLU ---- conv/upconv ----\n# If direction is ""down"", we use nn.Conv2d with stride > 1, getting a smaller image\n# If direction is ""up"", we use nn.ConvTranspose2d with stride > 1, getting a larger image\nclass ConvResBlock(nn.Module):\n    def __init__(self, inplanes, planes, direction, stride=1,\n                 norm_layer=nn.BatchNorm2d, activation_layer=nn.LeakyReLU(0.2, True)):\n        super(ConvResBlock, self).__init__()\n        self.res = BasicResBlock(inplanes, norm_layer=norm_layer,\n                                 activation_layer=activation_layer)\n        self.activation = activation_layer\n\n        if stride == 1 and inplanes == planes:\n            conv = lambda x: x\n        else:\n            if direction == \'down\':\n                conv = conv3x3(inplanes, planes, stride=stride)\n            elif direction == \'up\':\n                conv = deconv3x3(inplanes, planes, stride=stride)\n            else:\n                raise (ValueError(\'Direction must be either ""down"" or ""up"", get %s instead.\' % direction))\n        self.conv = conv\n        self.inplanes = inplanes\n        self.planes = planes\n        self.stride = stride\n\n    def forward(self, x):\n        return self.conv(self.activation(self.res(x)))\n        \n        #im_size, nz, nef, norm_layer, nl_layer)\nclass ResNetEncoder(nn.Module):\n    def __init__(self, im_size, nz=256, ngf=64, ndown=6,\n        norm_layer=None, nl_layer=None):\n        super(ResNetEncoder, self).__init__()\n        self.ngf = ngf\n        fc_dim = 2 * nz\n\n        layers = [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(3, ngf, kernel_size=7, stride=1, padding=0),\n            norm_layer(ngf),\n            nl_layer,\n        ]\n        prev = 1\n        for i in range(ndown):\n            im_size //= 2\n            cur = min(8, prev*2)\n            layers.append(ConvResBlock(ngf * prev, ngf * cur, direction=\'down\', stride=2,\n                norm_layer=norm_layer, activation_layer=nl_layer))\n            prev = cur\n\n        self.conv = nn.Sequential(*layers)\n        self.fc = nn.Sequential(\n            nn.Linear(im_size * im_size * ngf * cur, fc_dim),\n            nn.BatchNorm1d(fc_dim),\n            nl_layer,\n            nn.Linear(fc_dim, nz)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.view(x.shape[0], -1)\n        return self.fc(x)\n        \n#####  VGGNet  #####\n\n\'\'\'\n    This is a replica of torchvision.models.vgg13_bn with modified input size\n\'\'\'\nclass VGGEncoder(nn.Module):\n    def __init__(self, im_size, nz=256, ngf=64, ndown=5,\n        norm_layer=None, nl_layer=None):\n        super(VGGEncoder, self).__init__()\n        cfg_parts = [\n            [1 * ngf, 1 * ngf, \'M\'], \n            [2 * ngf, 2 * ngf, \'M\'],\n            [4 * ngf, 4 * ngf, \'M\'],  # [4 * ngf, 4 * ngf, 4 * ngf, \'M\'],\n            [8 * ngf, 8 * ngf, \'M\'],  # [8 * ngf, 8 * ngf, 8 * ngf, \'M\'],\n        ]\n        custom_cfg = []\n        for i in range(ndown):\n            custom_cfg += cfg_parts[min(i, 3)]\n        fc_dim = 4 * nz\n        \n        self.features = self._make_layers(\n            cfg=custom_cfg,\n            batch_norm=True,\n            norm_layer=norm_layer,\n            nl_layer=nl_layer,\n        )\n        im_size = im_size // (2**ndown)\n        self.avgpool=nn.AdaptiveAvgPool2d((im_size, im_size))\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * im_size * im_size, fc_dim),\n            nl_layer,\n            nn.Dropout(),\n            nn.Linear(fc_dim, nz),\n        )\n    \n    def _make_layers(self, cfg, batch_norm=False, norm_layer=None, nl_layer=None):\n        layers = []\n        in_channels = 3\n        for v in cfg:\n            if v == \'M\':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n                if batch_norm:\n                    layers += [conv2d, norm_layer(v), nl_layer]\n                else:\n                    layers += [conv2d, nl_layer]\n                in_channels = v\n        return nn.Sequential(*layers)\n        \n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        return self.classifier(x)\n\n\n#####  Decoder #####\nclass ConvResDecoder(nn.Module):\n    \'\'\'\n        ConvResDecoder: Use convres block for upsampling\n    \'\'\'\n    def __init__(self, im_size, nz, ngf=64, nup=6,\n        norm_layer=None, nl_layer=None):\n        super(ConvResDecoder, self).__init__()\n        self.im_size = im_size // (2 ** nup)\n        fc_dim = 2 * nz\n        \n        layers = []\n        prev = 8\n        for i in range(nup-1, -1, -1):\n            cur = min(prev, 2**i)\n            layers.append(ConvResBlock(ngf * prev, ngf * cur, direction=\'up\', stride=2,\n                norm_layer=norm_layer, activation_layer=nl_layer))\n            prev = cur\n        \n        layers += [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(ngf, 3, kernel_size=7, stride=1, padding=0),\n            nn.Tanh(),\n        ]\n        self.conv = nn.Sequential(*layers)\n        self.fc = nn.Sequential(\n            nn.Linear(nz, fc_dim),\n            nn.BatchNorm1d(fc_dim),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(fc_dim, self.im_size * self.im_size * ngf * 8),\n        )\n        \n    def forward(self, x):\n        x = self.fc(x)\n        x = x.view(x.shape[0], -1, self.im_size, self.im_size)\n        return self.conv(x)\n        \nclass ConvUpSampleDecoder(nn.Module):\n    \'\'\'\n        SimpleDecoder\n    \'\'\'\n    def __init__(self, im_size, nz, ngf=64, nup=6,\n        norm_layer=None, nl_layer=None):\n        super(ConvUpSampleDecoder, self).__init__()\n        self.im_size = im_size // (2 ** nup)\n        fc_dim = 4 * nz\n        \n        layers = []\n        prev = 8\n        for i in range(nup-1, -1, -1):\n            cur = min(prev, 2**i)\n            layers.append(deconv3x3(ngf * prev, ngf * cur, stride=2))\n            prev = cur\n        \n        layers += [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(ngf, 3, kernel_size=7, stride=1, padding=0),\n            nn.Tanh(),\n        ]\n        self.conv = nn.Sequential(*layers)\n        self.fc = nn.Sequential(\n            nn.Linear(nz, fc_dim),\n            nl_layer,\n            nn.Dropout(),\n            nn.Linear(fc_dim, self.im_size * self.im_size * ngf * 8),\n        )\n        \n    def forward(self, x):\n        x = self.fc(x)\n        x = x.view(x.shape[0], -1, self.im_size, self.im_size)\n        return self.conv(x)\n    \nif __name__ == \'__main__\':\n    acquire_weights(\'../data_utils/smpl_fbx_template_UV_weights.npy\')\n    '"
models/resnet_model.py,2,"b""import torch\n\nfrom .base_model import BaseModel\nfrom . import networks\n\nclass ResNetModel(BaseModel):\n    def name(self):\n        return 'ResNetModel'\n        \n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        return parser\n        \n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n        self.loss_names = ['L1', 'TV']\n        self.model_names = ['encoder', 'decoder']\n        \n        self.encoder = networks.define_encoder(opt.im_size, opt.nz, opt.nchannels, netE=opt.model, ndown=opt.ndown,\n            norm = opt.norm, nl=opt.nl, init_type=opt.init_type, device=self.device)\n        \n        self.decoder = networks.define_decoder(opt.im_size, opt.nz, opt.nchannels, netD=opt.netD, nup=opt.ndown,\n            norm = opt.norm, nl=opt.nl, init_type=opt.init_type, device=self.device)\n        \n        if opt.phase == 'train':\n            self.L1_loss = networks.WeightedL1Loss(opt.uv_prefix, self.device)  # requires a weight npy\n            self.TV_loss = networks.TotalVariationLoss(opt.uv_prefix, self.device)\n            self.encoder.train()\n            self.decoder.train()\n            \n            self.optimizers = []\n            self.optimizer_enc = torch.optim.Adam(self.encoder.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizers.append(self.optimizer_enc)\n            \n            self.optimizer_dec = torch.optim.Adam(self.decoder.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizers.append(self.optimizer_dec)\n        else:\n            self.encoder.eval()\n            self.decoder.eval()\n    \n    '''\n        forward: Train one step, return loss calues\n    '''\n    def train_one_batch(self, data):\n        self.set_input(data)\n        self.fake_UV = self.decoder(self.encoder(self.real_input))\n        l1_loss = self.L1_loss(self.fake_UV, self.real_UV)\n        tv_loss = self.TV_loss(self.fake_UV)\n        total_loss = l1_loss #+ self.opt.tv_weight * tv_loss\n        \n        self.optimizer_enc.zero_grad()\n        self.optimizer_dec.zero_grad()\n        total_loss.backward()\n        self.optimizer_enc.step()\n        self.optimizer_dec.step()\n        \n        return {\n            'l1': l1_loss.item(),\n            'tv': tv_loss.item(),\n            'total': total_loss.item()\n        }\n    \n    def get_current_visuals(self):\n        # return: real image, real UV maps fake UV maps\n        visuals = {\n            'real_image': self.real_input[0],\n            'fake_UV': self.fake_UV[0]\n        }\n        if not self.opt.phase == 'in_the_wild':\n            visuals['real_UV'] = self.real_UV[0]\n        return visuals \n"""
