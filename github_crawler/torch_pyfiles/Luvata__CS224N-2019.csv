file_path,api_count,code
squad_final_project/args.py,0,"b'""""""Command-line arguments for setup.py, train.py, test.py.\n\nAuthor:\n    Chris Chute (chute@stanford.edu)\n""""""\n\nimport argparse\n\n\ndef get_setup_args():\n    """"""Get arguments needed in setup.py.""""""\n    parser = argparse.ArgumentParser(\'Download and pre-process SQuAD\')\n\n    add_common_args(parser)\n\n    parser.add_argument(\'--train_url\',\n                        type=str,\n                        default=\'https://github.com/chrischute/squad/data/train-v2.0.json\')\n    parser.add_argument(\'--dev_url\',\n                        type=str,\n                        default=\'https://github.com/chrischute/squad/data/dev-v2.0.json\')\n    parser.add_argument(\'--test_url\',\n                        type=str,\n                        default=\'https://github.com/chrischute/squad/data/test-v2.0.json\')\n    parser.add_argument(\'--glove_url\',\n                        type=str,\n                        default=\'http://nlp.stanford.edu/data/glove.840B.300d.zip\')\n    parser.add_argument(\'--dev_meta_file\',\n                        type=str,\n                        default=\'./data/dev_meta.json\')\n    parser.add_argument(\'--test_meta_file\',\n                        type=str,\n                        default=\'./data/test_meta.json\')\n    parser.add_argument(\'--word2idx_file\',\n                        type=str,\n                        default=\'./data/word2idx.json\')\n    parser.add_argument(\'--char2idx_file\',\n                        type=str,\n                        default=\'./data/char2idx.json\')\n    parser.add_argument(\'--answer_file\',\n                        type=str,\n                        default=\'./data/answer.json\')\n    parser.add_argument(\'--para_limit\',\n                        type=int,\n                        default=400,\n                        help=\'Max number of words in a paragraph\')\n    parser.add_argument(\'--ques_limit\',\n                        type=int,\n                        default=50,\n                        help=\'Max number of words to keep from a question\')\n    parser.add_argument(\'--test_para_limit\',\n                        type=int,\n                        default=1000,\n                        help=\'Max number of words in a paragraph at test time\')\n    parser.add_argument(\'--test_ques_limit\',\n                        type=int,\n                        default=100,\n                        help=\'Max number of words in a question at test time\')\n    parser.add_argument(\'--char_dim\',\n                        type=int,\n                        default=64,\n                        help=\'Size of char vectors (char-level embeddings)\')\n    parser.add_argument(\'--glove_dim\',\n                        type=int,\n                        default=300,\n                        help=\'Size of GloVe word vectors to use\')\n    parser.add_argument(\'--glove_num_vecs\',\n                        type=int,\n                        default=2196017,\n                        help=\'Number of GloVe vectors\')\n    parser.add_argument(\'--ans_limit\',\n                        type=int,\n                        default=30,\n                        help=\'Max number of words in a training example answer\')\n    parser.add_argument(\'--char_limit\',\n                        type=int,\n                        default=16,\n                        help=\'Max number of chars to keep from a word\')\n    parser.add_argument(\'--include_test_examples\',\n                        type=lambda s: s.lower().startswith(\'t\'),\n                        default=True,\n                        help=\'Process examples from the test set\')\n\n    args = parser.parse_args()\n\n    return args\n\n\ndef get_train_args():\n    """"""Get arguments needed in train.py.""""""\n    parser = argparse.ArgumentParser(\'Train a model on SQuAD\')\n\n    add_common_args(parser)\n    add_train_test_args(parser)\n\n    parser.add_argument(\'--eval_steps\',\n                        type=int,\n                        default=50000,\n                        help=\'Number of steps between successive evaluations.\')\n    parser.add_argument(\'--lr\',\n                        type=float,\n                        default=0.5,\n                        help=\'Learning rate.\')\n    parser.add_argument(\'--l2_wd\',\n                        type=float,\n                        default=0,\n                        help=\'L2 weight decay.\')\n    parser.add_argument(\'--num_epochs\',\n                        type=int,\n                        default=30,\n                        help=\'Number of epochs for which to train. Negative means forever.\')\n    parser.add_argument(\'--drop_prob\',\n                        type=float,\n                        default=0.2,\n                        help=\'Probability of zeroing an activation in dropout layers.\')\n    parser.add_argument(\'--metric_name\',\n                        type=str,\n                        default=\'F1\',\n                        choices=(\'NLL\', \'EM\', \'F1\'),\n                        help=\'Name of dev metric to determine best checkpoint.\')\n    parser.add_argument(\'--max_checkpoints\',\n                        type=int,\n                        default=5,\n                        help=\'Maximum number of checkpoints to keep on disk.\')\n    parser.add_argument(\'--max_grad_norm\',\n                        type=float,\n                        default=5.0,\n                        help=\'Maximum gradient norm for gradient clipping.\')\n    parser.add_argument(\'--seed\',\n                        type=int,\n                        default=224,\n                        help=\'Random seed for reproducibility.\')\n    parser.add_argument(\'--ema_decay\',\n                        type=float,\n                        default=0.999,\n                        help=\'Decay rate for exponential moving average of parameters.\')\n\n    args = parser.parse_args()\n\n    if args.metric_name == \'NLL\':\n        # Best checkpoint is the one that minimizes negative log-likelihood\n        args.maximize_metric = False\n    elif args.metric_name in (\'EM\', \'F1\'):\n        # Best checkpoint is the one that maximizes EM or F1\n        args.maximize_metric = True\n    else:\n        raise ValueError(\'Unrecognized metric name: ""{}""\'\n                         .format(args.metric_name))\n\n    return args\n\n\ndef get_test_args():\n    """"""Get arguments needed in test.py.""""""\n    parser = argparse.ArgumentParser(\'Test a trained model on SQuAD\')\n\n    add_common_args(parser)\n    add_train_test_args(parser)\n\n    parser.add_argument(\'--split\',\n                        type=str,\n                        default=\'dev\',\n                        choices=(\'train\', \'dev\', \'test\'),\n                        help=\'Split to use for testing.\')\n    parser.add_argument(\'--sub_file\',\n                        type=str,\n                        default=\'submission.csv\',\n                        help=\'Name for submission file.\')\n\n    # Require load_path for test.py\n    args = parser.parse_args()\n    if not args.load_path:\n        raise argparse.ArgumentError(\'Missing required argument --load_path\')\n\n    return args\n\n\ndef add_common_args(parser):\n    """"""Add arguments common to all 3 scripts: setup.py, train.py, test.py""""""\n    parser.add_argument(\'--train_record_file\',\n                        type=str,\n                        default=\'./data/train.npz\')\n    parser.add_argument(\'--dev_record_file\',\n                        type=str,\n                        default=\'./data/dev.npz\')\n    parser.add_argument(\'--test_record_file\',\n                        type=str,\n                        default=\'./data/test.npz\')\n    parser.add_argument(\'--word_emb_file\',\n                        type=str,\n                        default=\'./data/word_emb.json\')\n    parser.add_argument(\'--char_emb_file\',\n                        type=str,\n                        default=\'./data/char_emb.json\')\n    parser.add_argument(\'--train_eval_file\',\n                        type=str,\n                        default=\'./data/train_eval.json\')\n    parser.add_argument(\'--dev_eval_file\',\n                        type=str,\n                        default=\'./data/dev_eval.json\')\n    parser.add_argument(\'--test_eval_file\',\n                        type=str,\n                        default=\'./data/test_eval.json\')\n\n\ndef add_train_test_args(parser):\n    """"""Add arguments common to train.py and test.py""""""\n    parser.add_argument(\'--name\',\n                        \'-n\',\n                        type=str,\n                        required=True,\n                        help=\'Name to identify training or test run.\')\n    parser.add_argument(\'--max_ans_len\',\n                        type=int,\n                        default=15,\n                        help=\'Maximum length of a predicted answer.\')\n    parser.add_argument(\'--num_workers\',\n                        type=int,\n                        default=4,\n                        help=\'Number of sub-processes to use per data loader.\')\n    parser.add_argument(\'--save_dir\',\n                        type=str,\n                        default=\'./save/\',\n                        help=\'Base directory for saving information.\')\n    parser.add_argument(\'--batch_size\',\n                        type=int,\n                        default=64,\n                        help=\'Batch size per GPU. Scales automatically when \\\n                              multiple GPUs are available.\')\n    parser.add_argument(\'--use_squad_v2\',\n                        type=lambda s: s.lower().startswith(\'t\'),\n                        default=True,\n                        help=\'Whether to use SQuAD 2.0 (unanswerable) questions.\')\n    parser.add_argument(\'--hidden_size\',\n                        type=int,\n                        default=100,\n                        help=\'Number of features in encoder hidden layers.\')\n    parser.add_argument(\'--num_visuals\',\n                        type=int,\n                        default=10,\n                        help=\'Number of examples to visualize in TensorBoard.\')\n    parser.add_argument(\'--load_path\',\n                        type=str,\n                        default=None,\n                        help=\'Path to load as a model checkpoint.\')\n'"
squad_final_project/layers.py,15,"b'""""""Assortment of layers for use in models.py.\n\nAuthor:\n    Chris Chute (chute@stanford.edu)\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom util import masked_softmax\n\n\nclass Embedding(nn.Module):\n    """"""Embedding layer used by BiDAF, without the character-level component.\n\n    Word-level embeddings are further refined using a 2-layer Highway Encoder\n    (see `HighwayEncoder` class for details).\n\n    Args:\n        word_vectors (torch.Tensor): Pre-trained word vectors.\n        hidden_size (int): Size of hidden activations.\n        drop_prob (float): Probability of zero-ing out activations\n    """"""\n    def __init__(self, word_vectors, hidden_size, drop_prob):\n        super(Embedding, self).__init__()\n        self.drop_prob = drop_prob\n        self.embed = nn.Embedding.from_pretrained(word_vectors)\n        self.proj = nn.Linear(word_vectors.size(1), hidden_size, bias=False)\n        self.hwy = HighwayEncoder(2, hidden_size)\n\n    def forward(self, x):\n        emb = self.embed(x)   # (batch_size, seq_len, embed_size)\n        emb = F.dropout(emb, self.drop_prob, self.training)\n        emb = self.proj(emb)  # (batch_size, seq_len, hidden_size)\n        emb = self.hwy(emb)   # (batch_size, seq_len, hidden_size)\n\n        return emb\n\n\nclass HighwayEncoder(nn.Module):\n    """"""Encode an input sequence using a highway network.\n\n    Based on the paper:\n    ""Highway Networks""\n    by Rupesh Kumar Srivastava, Klaus Greff, J\xc3\xbcrgen Schmidhuber\n    (https://arxiv.org/abs/1505.00387).\n\n    Args:\n        num_layers (int): Number of layers in the highway encoder.\n        hidden_size (int): Size of hidden activations.\n    """"""\n    def __init__(self, num_layers, hidden_size):\n        super(HighwayEncoder, self).__init__()\n        self.transforms = nn.ModuleList([nn.Linear(hidden_size, hidden_size)\n                                         for _ in range(num_layers)])\n        self.gates = nn.ModuleList([nn.Linear(hidden_size, hidden_size)\n                                    for _ in range(num_layers)])\n\n    def forward(self, x):\n        for gate, transform in zip(self.gates, self.transforms):\n            # Shapes of g, t, and x are all (batch_size, seq_len, hidden_size)\n            g = torch.sigmoid(gate(x))\n            t = F.relu(transform(x))\n            x = g * t + (1 - g) * x\n\n        return x\n\n\nclass RNNEncoder(nn.Module):\n    """"""General-purpose layer for encoding a sequence using a bidirectional RNN.\n\n    Encoded output is the RNN\'s hidden state at each position, which\n    has shape `(batch_size, seq_len, hidden_size * 2)`.\n\n    Args:\n        input_size (int): Size of a single timestep in the input.\n        hidden_size (int): Size of the RNN hidden state.\n        num_layers (int): Number of layers of RNN cells to use.\n        drop_prob (float): Probability of zero-ing out activations.\n    """"""\n    def __init__(self,\n                 input_size,\n                 hidden_size,\n                 num_layers,\n                 drop_prob=0.):\n        super(RNNEncoder, self).__init__()\n        self.drop_prob = drop_prob\n        self.rnn = nn.LSTM(input_size, hidden_size, num_layers,\n                           batch_first=True,\n                           bidirectional=True,\n                           dropout=drop_prob if num_layers > 1 else 0.)\n\n    def forward(self, x, lengths):\n        # Save original padded length for use by pad_packed_sequence\n        orig_len = x.size(1)\n\n        # Sort by length and pack sequence for RNN\n        lengths, sort_idx = lengths.sort(0, descending=True)\n        x = x[sort_idx]     # (batch_size, seq_len, input_size)\n        x = pack_padded_sequence(x, lengths, batch_first=True)\n\n        # Apply RNN\n        x, _ = self.rnn(x)  # (batch_size, seq_len, 2 * hidden_size)\n\n        # Unpack and reverse sort\n        x, _ = pad_packed_sequence(x, batch_first=True, total_length=orig_len)\n        _, unsort_idx = sort_idx.sort(0)\n        x = x[unsort_idx]   # (batch_size, seq_len, 2 * hidden_size)\n\n        # Apply dropout (RNN applies dropout after all but the last layer)\n        x = F.dropout(x, self.drop_prob, self.training)\n\n        return x\n\n\nclass BiDAFAttention(nn.Module):\n    """"""Bidirectional attention originally used by BiDAF.\n\n    Bidirectional attention computes attention in two directions:\n    The context attends to the query and the query attends to the context.\n    The output of this layer is the concatenation of [context, c2q_attention,\n    context * c2q_attention, context * q2c_attention]. This concatenation allows\n    the attention vector at each timestep, along with the embeddings from\n    previous layers, to flow through the attention layer to the modeling layer.\n    The output has shape (batch_size, context_len, 8 * hidden_size).\n\n    Args:\n        hidden_size (int): Size of hidden activations.\n        drop_prob (float): Probability of zero-ing out activations.\n    """"""\n    def __init__(self, hidden_size, drop_prob=0.1):\n        super(BiDAFAttention, self).__init__()\n        self.drop_prob = drop_prob\n        self.c_weight = nn.Parameter(torch.zeros(hidden_size, 1))\n        self.q_weight = nn.Parameter(torch.zeros(hidden_size, 1))\n        self.cq_weight = nn.Parameter(torch.zeros(1, 1, hidden_size))\n        for weight in (self.c_weight, self.q_weight, self.cq_weight):\n            nn.init.xavier_uniform_(weight)\n        self.bias = nn.Parameter(torch.zeros(1))\n\n    def forward(self, c, q, c_mask, q_mask):\n        batch_size, c_len, _ = c.size()\n        q_len = q.size(1)\n        s = self.get_similarity_matrix(c, q)        # (batch_size, c_len, q_len)\n        c_mask = c_mask.view(batch_size, c_len, 1)  # (batch_size, c_len, 1)\n        q_mask = q_mask.view(batch_size, 1, q_len)  # (batch_size, 1, q_len)\n        s1 = masked_softmax(s, q_mask, dim=2)       # (batch_size, c_len, q_len)\n        s2 = masked_softmax(s, c_mask, dim=1)       # (batch_size, c_len, q_len)\n\n        # (bs, c_len, q_len) x (bs, q_len, hid_size) => (bs, c_len, hid_size)\n        a = torch.bmm(s1, q)\n        # (bs, c_len, c_len) x (bs, c_len, hid_size) => (bs, c_len, hid_size)\n        b = torch.bmm(torch.bmm(s1, s2.transpose(1, 2)), c)\n\n        x = torch.cat([c, a, c * a, c * b], dim=2)  # (bs, c_len, 4 * hid_size)\n\n        return x\n\n    def get_similarity_matrix(self, c, q):\n        """"""Get the ""similarity matrix"" between context and query (using the\n        terminology of the BiDAF paper).\n\n        A naive implementation as described in BiDAF would concatenate the\n        three vectors then project the result with a single weight matrix. This\n        method is a more memory-efficient implementation of the same operation.\n\n        See Also:\n            Equation 1 in https://arxiv.org/abs/1611.01603\n        """"""\n        c_len, q_len = c.size(1), q.size(1)\n        c = F.dropout(c, self.drop_prob, self.training)  # (bs, c_len, hid_size)\n        q = F.dropout(q, self.drop_prob, self.training)  # (bs, q_len, hid_size)\n\n        # Shapes: (batch_size, c_len, q_len)\n        s0 = torch.matmul(c, self.c_weight).expand([-1, -1, q_len])\n        s1 = torch.matmul(q, self.q_weight).transpose(1, 2)\\\n                                           .expand([-1, c_len, -1])\n        s2 = torch.matmul(c * self.cq_weight, q.transpose(1, 2))\n        s = s0 + s1 + s2 + self.bias\n\n        return s\n\n\nclass BiDAFOutput(nn.Module):\n    """"""Output layer used by BiDAF for question answering.\n\n    Computes a linear transformation of the attention and modeling\n    outputs, then takes the softmax of the result to get the start pointer.\n    A bidirectional LSTM is then applied the modeling output to produce `mod_2`.\n    A second linear+softmax of the attention output and `mod_2` is used\n    to get the end pointer.\n\n    Args:\n        hidden_size (int): Hidden size used in the BiDAF model.\n        drop_prob (float): Probability of zero-ing out activations.\n    """"""\n    def __init__(self, hidden_size, drop_prob):\n        super(BiDAFOutput, self).__init__()\n        self.att_linear_1 = nn.Linear(8 * hidden_size, 1)\n        self.mod_linear_1 = nn.Linear(2 * hidden_size, 1)\n\n        self.rnn = RNNEncoder(input_size=2 * hidden_size,\n                              hidden_size=hidden_size,\n                              num_layers=1,\n                              drop_prob=drop_prob)\n\n        self.att_linear_2 = nn.Linear(8 * hidden_size, 1)\n        self.mod_linear_2 = nn.Linear(2 * hidden_size, 1)\n\n    def forward(self, att, mod, mask):\n        # Shapes: (batch_size, seq_len, 1)\n        logits_1 = self.att_linear_1(att) + self.mod_linear_1(mod)\n        mod_2 = self.rnn(mod, mask.sum(-1))\n        logits_2 = self.att_linear_2(att) + self.mod_linear_2(mod_2)\n\n        # Shapes: (batch_size, seq_len)\n        log_p1 = masked_softmax(logits_1.squeeze(), mask, log_softmax=True)\n        log_p2 = masked_softmax(logits_2.squeeze(), mask, log_softmax=True)\n\n        return log_p1, log_p2\n'"
squad_final_project/models.py,4,"b'""""""Top-level model classes.\n\nAuthor:\n    Chris Chute (chute@stanford.edu)\n""""""\n\nimport layers\nimport torch\nimport torch.nn as nn\n\n\nclass BiDAF(nn.Module):\n    """"""Baseline BiDAF model for SQuAD.\n\n    Based on the paper:\n    ""Bidirectional Attention Flow for Machine Comprehension""\n    by Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi\n    (https://arxiv.org/abs/1611.01603).\n\n    Follows a high-level structure commonly found in SQuAD models:\n        - Embedding layer: Embed word indices to get word vectors.\n        - Encoder layer: Encode the embedded sequence.\n        - Attention layer: Apply an attention mechanism to the encoded sequence.\n        - Model encoder layer: Encode the sequence again.\n        - Output layer: Simple layer (e.g., fc + softmax) to get final outputs.\n\n    Args:\n        word_vectors (torch.Tensor): Pre-trained word vectors.\n        hidden_size (int): Number of features in the hidden state at each layer.\n        drop_prob (float): Dropout probability.\n    """"""\n    def __init__(self, word_vectors, hidden_size, drop_prob=0.):\n        super(BiDAF, self).__init__()\n        self.emb = layers.Embedding(word_vectors=word_vectors,\n                                    hidden_size=hidden_size,\n                                    drop_prob=drop_prob)\n\n        self.enc = layers.RNNEncoder(input_size=hidden_size,\n                                     hidden_size=hidden_size,\n                                     num_layers=1,\n                                     drop_prob=drop_prob)\n\n        self.att = layers.BiDAFAttention(hidden_size=2 * hidden_size,\n                                         drop_prob=drop_prob)\n\n        self.mod = layers.RNNEncoder(input_size=8 * hidden_size,\n                                     hidden_size=hidden_size,\n                                     num_layers=2,\n                                     drop_prob=drop_prob)\n\n        self.out = layers.BiDAFOutput(hidden_size=hidden_size,\n                                      drop_prob=drop_prob)\n\n    def forward(self, cw_idxs, qw_idxs):\n        c_mask = torch.zeros_like(cw_idxs) != cw_idxs\n        q_mask = torch.zeros_like(qw_idxs) != qw_idxs\n        c_len, q_len = c_mask.sum(-1), q_mask.sum(-1)\n\n        c_emb = self.emb(cw_idxs)         # (batch_size, c_len, hidden_size)\n        q_emb = self.emb(qw_idxs)         # (batch_size, q_len, hidden_size)\n\n        c_enc = self.enc(c_emb, c_len)    # (batch_size, c_len, 2 * hidden_size)\n        q_enc = self.enc(q_emb, q_len)    # (batch_size, q_len, 2 * hidden_size)\n\n        att = self.att(c_enc, q_enc,\n                       c_mask, q_mask)    # (batch_size, c_len, 8 * hidden_size)\n\n        mod = self.mod(att, c_len)        # (batch_size, c_len, 2 * hidden_size)\n\n        out = self.out(att, mod, c_mask)  # 2 tensors, each (batch_size, c_len)\n\n        return out\n'"
squad_final_project/setup.py,0,"b'""""""Download and pre-process SQuAD and GloVe.\n\nUsage:\n    > source activate squad\n    > python setup.py\n\nPre-processing code adapted from:\n    > https://github.com/HKUST-KnowComp/R-Net/blob/master/prepro.py\n\nAuthor:\n    Chris Chute (chute@stanford.edu)\n""""""\n\nimport numpy as np\nimport os\nimport spacy\nimport ujson as json\nimport urllib.request\n\nfrom args import get_setup_args\nfrom codecs import open\nfrom collections import Counter\nfrom subprocess import run\nfrom tqdm import tqdm\nfrom zipfile import ZipFile\n\n\ndef download_url(url, output_path, show_progress=True):\n    class DownloadProgressBar(tqdm):\n        def update_to(self, b=1, bsize=1, tsize=None):\n            if tsize is not None:\n                self.total = tsize\n            self.update(b * bsize - self.n)\n\n    if show_progress:\n        # Download with a progress bar\n        with DownloadProgressBar(unit=\'B\', unit_scale=True,\n                                 miniters=1, desc=url.split(\'/\')[-1]) as t:\n            urllib.request.urlretrieve(url,\n                                       filename=output_path,\n                                       reporthook=t.update_to)\n    else:\n        # Simple download with no progress bar\n        urllib.request.urlretrieve(url, output_path)\n\n\ndef url_to_data_path(url):\n    return os.path.join(\'./data/\', url.split(\'/\')[-1])\n\n\ndef download(args):\n    downloads = [\n        # Can add other downloads here (e.g., other word vectors)\n        (\'GloVe word vectors\', args.glove_url),\n    ]\n\n    for name, url in downloads:\n        output_path = url_to_data_path(url)\n        if not os.path.exists(output_path):\n            print(\'Downloading {}...\'.format(name))\n            download_url(url, output_path)\n\n        if os.path.exists(output_path) and output_path.endswith(\'.zip\'):\n            extracted_path = output_path.replace(\'.zip\', \'\')\n            if not os.path.exists(extracted_path):\n                print(\'Unzipping {}...\'.format(name))\n                with ZipFile(output_path, \'r\') as zip_fh:\n                    zip_fh.extractall(extracted_path)\n\n    print(\'Downloading spacy language model...\')\n    run([\'python\', \'-m\', \'spacy\', \'download\', \'en\'])\n\n\ndef word_tokenize(sent):\n    doc = nlp(sent)\n    return [token.text for token in doc]\n\n\ndef convert_idx(text, tokens):\n    current = 0\n    spans = []\n    for token in tokens:\n        current = text.find(token, current)\n        if current < 0:\n            print(""Token {} cannot be found"".format(token))\n            raise Exception()\n        spans.append((current, current + len(token)))\n        current += len(token)\n    return spans\n\n\ndef process_file(filename, data_type, word_counter, char_counter):\n    print(""Pre-processing {} examples..."".format(data_type))\n    examples = []\n    eval_examples = {}\n    total = 0\n    with open(filename, ""r"") as fh:\n        source = json.load(fh)\n        for article in tqdm(source[""data""]):\n            for para in article[""paragraphs""]:\n                context = para[""context""].replace(\n                    ""\'\'"", \'"" \').replace(""``"", \'"" \')\n                context_tokens = word_tokenize(context)\n                context_chars = [list(token) for token in context_tokens]\n                spans = convert_idx(context, context_tokens)\n                for token in context_tokens:\n                    word_counter[token] += len(para[""qas""])\n                    for char in token:\n                        char_counter[char] += len(para[""qas""])\n                for qa in para[""qas""]:\n                    total += 1\n                    ques = qa[""question""].replace(\n                        ""\'\'"", \'"" \').replace(""``"", \'"" \')\n                    ques_tokens = word_tokenize(ques)\n                    ques_chars = [list(token) for token in ques_tokens]\n                    for token in ques_tokens:\n                        word_counter[token] += 1\n                        for char in token:\n                            char_counter[char] += 1\n                    y1s, y2s = [], []\n                    answer_texts = []\n                    for answer in qa[""answers""]:\n                        answer_text = answer[""text""]\n                        answer_start = answer[\'answer_start\']\n                        answer_end = answer_start + len(answer_text)\n                        answer_texts.append(answer_text)\n                        answer_span = []\n                        for idx, span in enumerate(spans):\n                            if not (answer_end <= span[0] or answer_start >= span[1]):\n                                answer_span.append(idx)\n                        y1, y2 = answer_span[0], answer_span[-1]\n                        y1s.append(y1)\n                        y2s.append(y2)\n                    example = {""context_tokens"": context_tokens,\n                               ""context_chars"": context_chars,\n                               ""ques_tokens"": ques_tokens,\n                               ""ques_chars"": ques_chars,\n                               ""y1s"": y1s,\n                               ""y2s"": y2s,\n                               ""id"": total}\n                    examples.append(example)\n                    eval_examples[str(total)] = {""context"": context,\n                                                 ""question"": ques,\n                                                 ""spans"": spans,\n                                                 ""answers"": answer_texts,\n                                                 ""uuid"": qa[""id""]}\n        print(""{} questions in total"".format(len(examples)))\n    return examples, eval_examples\n\n\ndef get_embedding(counter, data_type, limit=-1, emb_file=None, vec_size=None, num_vectors=None):\n    print(""Pre-processing {} vectors..."".format(data_type))\n    embedding_dict = {}\n    filtered_elements = [k for k, v in counter.items() if v > limit]\n    if emb_file is not None:\n        assert vec_size is not None\n        with open(emb_file, ""r"", encoding=""utf-8"") as fh:\n            for line in tqdm(fh, total=num_vectors):\n                array = line.split()\n                word = """".join(array[0:-vec_size])\n                vector = list(map(float, array[-vec_size:]))\n                if word in counter and counter[word] > limit:\n                    embedding_dict[word] = vector\n        print(""{} / {} tokens have corresponding {} embedding vector"".format(\n            len(embedding_dict), len(filtered_elements), data_type))\n    else:\n        assert vec_size is not None\n        for token in filtered_elements:\n            embedding_dict[token] = [np.random.normal(\n                scale=0.1) for _ in range(vec_size)]\n        print(""{} tokens have corresponding {} embedding vector"".format(\n            len(filtered_elements), data_type))\n\n    NULL = ""--NULL--""\n    OOV = ""--OOV--""\n    token2idx_dict = {token: idx for idx, token in enumerate(embedding_dict.keys(), 2)}\n    token2idx_dict[NULL] = 0\n    token2idx_dict[OOV] = 1\n    embedding_dict[NULL] = [0. for _ in range(vec_size)]\n    embedding_dict[OOV] = [0. for _ in range(vec_size)]\n    idx2emb_dict = {idx: embedding_dict[token]\n                    for token, idx in token2idx_dict.items()}\n    emb_mat = [idx2emb_dict[idx] for idx in range(len(idx2emb_dict))]\n    return emb_mat, token2idx_dict\n\n\ndef convert_to_features(args, data, word2idx_dict, char2idx_dict, is_test):\n    example = {}\n    context, question = data\n    context = context.replace(""\'\'"", \'"" \').replace(""``"", \'"" \')\n    question = question.replace(""\'\'"", \'"" \').replace(""``"", \'"" \')\n    example[\'context_tokens\'] = word_tokenize(context)\n    example[\'ques_tokens\'] = word_tokenize(question)\n    example[\'context_chars\'] = [list(token) for token in example[\'context_tokens\']]\n    example[\'ques_chars\'] = [list(token) for token in example[\'ques_tokens\']]\n\n    para_limit = args.test_para_limit if is_test else args.para_limit\n    ques_limit = args.test_ques_limit if is_test else args.ques_limit\n    char_limit = args.char_limit\n\n    def filter_func(example):\n        return len(example[""context_tokens""]) > para_limit or \\\n               len(example[""ques_tokens""]) > ques_limit\n\n    if filter_func(example):\n        raise ValueError(""Context/Questions lengths are over the limit"")\n\n    context_idxs = np.zeros([para_limit], dtype=np.int32)\n    context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32)\n    ques_idxs = np.zeros([ques_limit], dtype=np.int32)\n    ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32)\n\n    def _get_word(word):\n        for each in (word, word.lower(), word.capitalize(), word.upper()):\n            if each in word2idx_dict:\n                return word2idx_dict[each]\n        return 1\n\n    def _get_char(char):\n        if char in char2idx_dict:\n            return char2idx_dict[char]\n        return 1\n\n    for i, token in enumerate(example[""context_tokens""]):\n        context_idxs[i] = _get_word(token)\n\n    for i, token in enumerate(example[""ques_tokens""]):\n        ques_idxs[i] = _get_word(token)\n\n    for i, token in enumerate(example[""context_chars""]):\n        for j, char in enumerate(token):\n            if j == char_limit:\n                break\n            context_char_idxs[i, j] = _get_char(char)\n\n    for i, token in enumerate(example[""ques_chars""]):\n        for j, char in enumerate(token):\n            if j == char_limit:\n                break\n            ques_char_idxs[i, j] = _get_char(char)\n\n    return context_idxs, context_char_idxs, ques_idxs, ques_char_idxs\n\n\ndef is_answerable(example):\n    return len(example[\'y2s\']) > 0 and len(example[\'y1s\']) > 0\n\n\ndef build_features(args, examples, data_type, out_file, word2idx_dict, char2idx_dict, is_test=False):\n    para_limit = args.test_para_limit if is_test else args.para_limit\n    ques_limit = args.test_ques_limit if is_test else args.ques_limit\n    ans_limit = args.ans_limit\n    char_limit = args.char_limit\n\n    def drop_example(ex, is_test_=False):\n        if is_test_:\n            drop = False\n        else:\n            drop = len(ex[""context_tokens""]) > para_limit or \\\n                   len(ex[""ques_tokens""]) > ques_limit or \\\n                   (is_answerable(ex) and\n                    ex[""y2s""][0] - ex[""y1s""][0] > ans_limit)\n\n        return drop\n\n    print(""Converting {} examples to indices..."".format(data_type))\n    total = 0\n    total_ = 0\n    meta = {}\n    context_idxs = []\n    context_char_idxs = []\n    ques_idxs = []\n    ques_char_idxs = []\n    y1s = []\n    y2s = []\n    ids = []\n    for n, example in tqdm(enumerate(examples)):\n        total_ += 1\n\n        if drop_example(example, is_test):\n            continue\n\n        total += 1\n\n        def _get_word(word):\n            for each in (word, word.lower(), word.capitalize(), word.upper()):\n                if each in word2idx_dict:\n                    return word2idx_dict[each]\n            return 1\n\n        def _get_char(char):\n            if char in char2idx_dict:\n                return char2idx_dict[char]\n            return 1\n\n        context_idx = np.zeros([para_limit], dtype=np.int32)\n        context_char_idx = np.zeros([para_limit, char_limit], dtype=np.int32)\n        ques_idx = np.zeros([ques_limit], dtype=np.int32)\n        ques_char_idx = np.zeros([ques_limit, char_limit], dtype=np.int32)\n\n        for i, token in enumerate(example[""context_tokens""]):\n            context_idx[i] = _get_word(token)\n        context_idxs.append(context_idx)\n\n        for i, token in enumerate(example[""ques_tokens""]):\n            ques_idx[i] = _get_word(token)\n        ques_idxs.append(ques_idx)\n\n        for i, token in enumerate(example[""context_chars""]):\n            for j, char in enumerate(token):\n                if j == char_limit:\n                    break\n                context_char_idx[i, j] = _get_char(char)\n        context_char_idxs.append(context_char_idx)\n\n        for i, token in enumerate(example[""ques_chars""]):\n            for j, char in enumerate(token):\n                if j == char_limit:\n                    break\n                ques_char_idx[i, j] = _get_char(char)\n        ques_char_idxs.append(ques_char_idx)\n\n        if is_answerable(example):\n            start, end = example[""y1s""][-1], example[""y2s""][-1]\n        else:\n            start, end = -1, -1\n\n        y1s.append(start)\n        y2s.append(end)\n        ids.append(example[""id""])\n\n    np.savez(out_file,\n             context_idxs=np.array(context_idxs),\n             context_char_idxs=np.array(context_char_idxs),\n             ques_idxs=np.array(ques_idxs),\n             ques_char_idxs=np.array(ques_char_idxs),\n             y1s=np.array(y1s),\n             y2s=np.array(y2s),\n             ids=np.array(ids))\n    print(""Built {} / {} instances of features in total"".format(total, total_))\n    meta[""total""] = total\n    return meta\n\n\ndef save(filename, obj, message=None):\n    if message is not None:\n        print(""Saving {}..."".format(message))\n        with open(filename, ""w"") as fh:\n            json.dump(obj, fh)\n\n\ndef pre_process(args):\n    # Process training set and use it to decide on the word/character vocabularies\n    word_counter, char_counter = Counter(), Counter()\n    train_examples, train_eval = process_file(args.train_file, ""train"", word_counter, char_counter)\n    word_emb_mat, word2idx_dict = get_embedding(\n        word_counter, \'word\', emb_file=args.glove_file, vec_size=args.glove_dim, num_vectors=args.glove_num_vecs)\n    char_emb_mat, char2idx_dict = get_embedding(\n        char_counter, \'char\', emb_file=None, vec_size=args.char_dim)\n\n    # Process dev and test sets\n    dev_examples, dev_eval = process_file(args.dev_file, ""dev"", word_counter, char_counter)\n    build_features(args, train_examples, ""train"", args.train_record_file, word2idx_dict, char2idx_dict)\n    dev_meta = build_features(args, dev_examples, ""dev"", args.dev_record_file, word2idx_dict, char2idx_dict)\n    if args.include_test_examples:\n        test_examples, test_eval = process_file(args.test_file, ""test"", word_counter, char_counter)\n        save(args.test_eval_file, test_eval, message=""test eval"")\n        test_meta = build_features(args, test_examples, ""test"",\n                                   args.test_record_file, word2idx_dict, char2idx_dict, is_test=True)\n        save(args.test_meta_file, test_meta, message=""test meta"")\n\n    save(args.word_emb_file, word_emb_mat, message=""word embedding"")\n    save(args.char_emb_file, char_emb_mat, message=""char embedding"")\n    save(args.train_eval_file, train_eval, message=""train eval"")\n    save(args.dev_eval_file, dev_eval, message=""dev eval"")\n    save(args.word2idx_file, word2idx_dict, message=""word dictionary"")\n    save(args.char2idx_file, char2idx_dict, message=""char dictionary"")\n    save(args.dev_meta_file, dev_meta, message=""dev meta"")\n\n\nif __name__ == \'__main__\':\n    # Get command-line args\n    args_ = get_setup_args()\n\n    # Download resources\n    download(args_)\n\n    # Import spacy language model\n    nlp = spacy.blank(""en"")\n\n    # Preprocess dataset\n    args_.train_file = url_to_data_path(args_.train_url)\n    args_.dev_file = url_to_data_path(args_.dev_url)\n    if args_.include_test_examples:\n        args_.test_file = url_to_data_path(args_.test_url)\n    glove_dir = url_to_data_path(args_.glove_url.replace(\'.zip\', \'\'))\n    glove_ext = \'.txt\' if glove_dir.endswith(\'d\') else \'.{}d.txt\'.format(args_.glove_dim)\n    args_.glove_file = os.path.join(glove_dir, os.path.basename(glove_dir) + glove_ext)\n    pre_process(args_)\n'"
squad_final_project/test.py,4,"b'""""""Test a model and generate submission CSV.\n\nUsage:\n    > python test.py --split SPLIT --load_path PATH --name NAME\n    where\n    > SPLIT is either ""dev"" or ""test""\n    > PATH is a path to a checkpoint (e.g., save/train/model-01/best.pth.tar)\n    > NAME is a name to identify the test run\n\nAuthor:\n    Chris Chute (chute@stanford.edu)\n""""""\n\nimport csv\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport util\n\nfrom args import get_test_args\nfrom collections import OrderedDict\nfrom json import dumps\nfrom models import BiDAF\nfrom os.path import join\nfrom tensorboardX import SummaryWriter\nfrom tqdm import tqdm\nfrom ujson import load as json_load\nfrom util import collate_fn, SQuAD\n\n\ndef main(args):\n    # Set up logging\n    args.save_dir = util.get_save_dir(args.save_dir, args.name, training=False)\n    log = util.get_logger(args.save_dir, args.name)\n    log.info(\'Args: {}\'.format(dumps(vars(args), indent=4, sort_keys=True)))\n    device, gpu_ids = util.get_available_devices()\n    args.batch_size *= max(1, len(gpu_ids))\n\n    # Get embeddings\n    log.info(\'Loading embeddings...\')\n    word_vectors = util.torch_from_json(args.word_emb_file)\n\n    # Get model\n    log.info(\'Building model...\')\n    model = BiDAF(word_vectors=word_vectors,\n                  hidden_size=args.hidden_size)\n    model = nn.DataParallel(model, gpu_ids)\n    log.info(\'Loading checkpoint from {}...\'.format(args.load_path))\n    model = util.load_model(model, args.load_path, gpu_ids, return_step=False)\n    model = model.to(device)\n    model.eval()\n\n    # Get data loader\n    log.info(\'Building dataset...\')\n    record_file = vars(args)[\'{}_record_file\'.format(args.split)]\n    dataset = SQuAD(record_file, args.use_squad_v2)\n    data_loader = data.DataLoader(dataset,\n                                  batch_size=args.batch_size,\n                                  shuffle=False,\n                                  num_workers=args.num_workers,\n                                  collate_fn=collate_fn)\n\n    # Evaluate\n    log.info(\'Evaluating on {} split...\'.format(args.split))\n    nll_meter = util.AverageMeter()\n    pred_dict = {}  # Predictions for TensorBoard\n    sub_dict = {}   # Predictions for submission\n    eval_file = vars(args)[\'{}_eval_file\'.format(args.split)]\n    with open(eval_file, \'r\') as fh:\n        gold_dict = json_load(fh)\n    with torch.no_grad(), \\\n            tqdm(total=len(dataset)) as progress_bar:\n        for cw_idxs, cc_idxs, qw_idxs, qc_idxs, y1, y2, ids in data_loader:\n            # Setup for forward\n            cw_idxs = cw_idxs.to(device)\n            qw_idxs = qw_idxs.to(device)\n            batch_size = cw_idxs.size(0)\n\n            # Forward\n            log_p1, log_p2 = model(cw_idxs, qw_idxs)\n            y1, y2 = y1.to(device), y2.to(device)\n            loss = F.nll_loss(log_p1, y1) + F.nll_loss(log_p2, y2)\n            nll_meter.update(loss.item(), batch_size)\n\n            # Get F1 and EM scores\n            p1, p2 = log_p1.exp(), log_p2.exp()\n            starts, ends = util.discretize(p1, p2, args.max_ans_len, args.use_squad_v2)\n\n            # Log info\n            progress_bar.update(batch_size)\n            if args.split != \'test\':\n                # No labels for the test set, so NLL would be invalid\n                progress_bar.set_postfix(NLL=nll_meter.avg)\n\n            idx2pred, uuid2pred = util.convert_tokens(gold_dict,\n                                                      ids.tolist(),\n                                                      starts.tolist(),\n                                                      ends.tolist(),\n                                                      args.use_squad_v2)\n            pred_dict.update(idx2pred)\n            sub_dict.update(uuid2pred)\n\n    # Log results (except for test set, since it does not come with labels)\n    if args.split != \'test\':\n        results = util.eval_dicts(gold_dict, pred_dict, args.use_squad_v2)\n        results_list = [(\'NLL\', nll_meter.avg),\n                        (\'F1\', results[\'F1\']),\n                        (\'EM\', results[\'EM\'])]\n        if args.use_squad_v2:\n            results_list.append((\'AvNA\', results[\'AvNA\']))\n        results = OrderedDict(results_list)\n\n        # Log to console\n        results_str = \', \'.join(\'{}: {:05.2f}\'.format(k, v)\n                                for k, v in results.items())\n        log.info(\'{} {}\'.format(args.split.title(), results_str))\n\n        # Log to TensorBoard\n        tbx = SummaryWriter(args.save_dir)\n        util.visualize(tbx,\n                       pred_dict=pred_dict,\n                       eval_path=eval_file,\n                       step=0,\n                       split=args.split,\n                       num_visuals=args.num_visuals)\n\n    # Write submission file\n    sub_path = join(args.save_dir, args.split + \'_\' + args.sub_file)\n    log.info(\'Writing submission file to {}...\'.format(sub_path))\n    with open(sub_path, \'w\', newline=\'\', encoding=\'utf-8\') as csv_fh:\n        csv_writer = csv.writer(csv_fh, delimiter=\',\')\n        csv_writer.writerow([\'Id\', \'Predicted\'])\n        for uuid in sorted(sub_dict):\n            csv_writer.writerow([uuid, sub_dict[uuid]])\n\n\nif __name__ == \'__main__\':\n    main(get_test_args())\n'"
squad_final_project/train.py,9,"b'""""""Train a model on SQuAD.\n\nAuthor:\n    Chris Chute (chute@stanford.edu)\n""""""\n\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as sched\nimport torch.utils.data as data\nimport util\n\nfrom args import get_train_args\nfrom collections import OrderedDict\nfrom json import dumps\nfrom models import BiDAF\nfrom tensorboardX import SummaryWriter\nfrom tqdm import tqdm\nfrom ujson import load as json_load\nfrom util import collate_fn, SQuAD\n\n\ndef main(args):\n    # Set up logging and devices\n    args.save_dir = util.get_save_dir(args.save_dir, args.name, training=True)\n    log = util.get_logger(args.save_dir, args.name)\n    tbx = SummaryWriter(args.save_dir)\n    device, args.gpu_ids = util.get_available_devices()\n    log.info(\'Args: {}\'.format(dumps(vars(args), indent=4, sort_keys=True)))\n    args.batch_size *= max(1, len(args.gpu_ids))\n\n    # Set random seed\n    log.info(\'Using random seed {}...\'.format(args.seed))\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n\n    # Get embeddings\n    log.info(\'Loading embeddings...\')\n    word_vectors = util.torch_from_json(args.word_emb_file)\n\n    # Get model\n    log.info(\'Building model...\')\n    model = BiDAF(word_vectors=word_vectors,\n                  hidden_size=args.hidden_size,\n                  drop_prob=args.drop_prob)\n    model = nn.DataParallel(model, args.gpu_ids)\n    if args.load_path:\n        log.info(\'Loading checkpoint from {}...\'.format(args.load_path))\n        model, step = util.load_model(model, args.load_path, args.gpu_ids)\n    else:\n        step = 0\n    model = model.to(device)\n    model.train()\n    ema = util.EMA(model, args.ema_decay)\n\n    # Get saver\n    saver = util.CheckpointSaver(args.save_dir,\n                                 max_checkpoints=args.max_checkpoints,\n                                 metric_name=args.metric_name,\n                                 maximize_metric=args.maximize_metric,\n                                 log=log)\n\n    # Get optimizer and scheduler\n    optimizer = optim.Adadelta(model.parameters(), args.lr,\n                               weight_decay=args.l2_wd)\n    scheduler = sched.LambdaLR(optimizer, lambda s: 1.)  # Constant LR\n\n    # Get data loader\n    log.info(\'Building dataset...\')\n    train_dataset = SQuAD(args.train_record_file, args.use_squad_v2)\n    train_loader = data.DataLoader(train_dataset,\n                                   batch_size=args.batch_size,\n                                   shuffle=True,\n                                   num_workers=args.num_workers,\n                                   collate_fn=collate_fn)\n    dev_dataset = SQuAD(args.dev_record_file, args.use_squad_v2)\n    dev_loader = data.DataLoader(dev_dataset,\n                                 batch_size=args.batch_size,\n                                 shuffle=False,\n                                 num_workers=args.num_workers,\n                                 collate_fn=collate_fn)\n\n    # Train\n    log.info(\'Training...\')\n    steps_till_eval = args.eval_steps\n    epoch = step // len(train_dataset)\n    while epoch != args.num_epochs:\n        epoch += 1\n        log.info(\'Starting epoch {}...\'.format(epoch))\n        with torch.enable_grad(), \\\n                tqdm(total=len(train_loader.dataset)) as progress_bar:\n            for cw_idxs, cc_idxs, qw_idxs, qc_idxs, y1, y2, ids in train_loader:\n                # Setup for forward\n                cw_idxs = cw_idxs.to(device)\n                qw_idxs = qw_idxs.to(device)\n                batch_size = cw_idxs.size(0)\n                optimizer.zero_grad()\n\n                # Forward\n                log_p1, log_p2 = model(cw_idxs, qw_idxs)\n                y1, y2 = y1.to(device), y2.to(device)\n                loss = F.nll_loss(log_p1, y1) + F.nll_loss(log_p2, y2)\n                loss_val = loss.item()\n\n                # Backward\n                loss.backward()\n                nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                optimizer.step()\n                scheduler.step(step // batch_size)\n                ema(model, step // batch_size)\n\n                # Log info\n                step += batch_size\n                progress_bar.update(batch_size)\n                progress_bar.set_postfix(epoch=epoch,\n                                         NLL=loss_val)\n                tbx.add_scalar(\'train/NLL\', loss_val, step)\n                tbx.add_scalar(\'train/LR\',\n                               optimizer.param_groups[0][\'lr\'],\n                               step)\n\n                steps_till_eval -= batch_size\n                if steps_till_eval <= 0:\n                    steps_till_eval = args.eval_steps\n\n                    # Evaluate and save checkpoint\n                    log.info(\'Evaluating at step {}...\'.format(step))\n                    ema.assign(model)\n                    results, pred_dict = evaluate(model, dev_loader, device,\n                                                  args.dev_eval_file,\n                                                  args.max_ans_len,\n                                                  args.use_squad_v2)\n                    saver.save(step, model, results[args.metric_name], device)\n                    ema.resume(model)\n\n                    # Log to console\n                    results_str = \', \'.join(\'{}: {:05.2f}\'.format(k, v)\n                                            for k, v in results.items())\n                    log.info(\'Dev {}\'.format(results_str))\n\n                    # Log to TensorBoard\n                    log.info(\'Visualizing in TensorBoard...\')\n                    for k, v in results.items():\n                        tbx.add_scalar(\'dev/{}\'.format(k), v, step)\n                    util.visualize(tbx,\n                                   pred_dict=pred_dict,\n                                   eval_path=args.dev_eval_file,\n                                   step=step,\n                                   split=\'dev\',\n                                   num_visuals=args.num_visuals)\n\n\ndef evaluate(model, data_loader, device, eval_file, max_len, use_squad_v2):\n    nll_meter = util.AverageMeter()\n\n    model.eval()\n    pred_dict = {}\n    with open(eval_file, \'r\') as fh:\n        gold_dict = json_load(fh)\n    with torch.no_grad(), \\\n            tqdm(total=len(data_loader.dataset)) as progress_bar:\n        for cw_idxs, cc_idxs, qw_idxs, qc_idxs, y1, y2, ids in data_loader:\n            # Setup for forward\n            cw_idxs = cw_idxs.to(device)\n            qw_idxs = qw_idxs.to(device)\n            batch_size = cw_idxs.size(0)\n\n            # Forward\n            log_p1, log_p2 = model(cw_idxs, qw_idxs)\n            y1, y2 = y1.to(device), y2.to(device)\n            loss = F.nll_loss(log_p1, y1) + F.nll_loss(log_p2, y2)\n            nll_meter.update(loss.item(), batch_size)\n\n            # Get F1 and EM scores\n            p1, p2 = log_p1.exp(), log_p2.exp()\n            starts, ends = util.discretize(p1, p2, max_len, use_squad_v2)\n\n            # Log info\n            progress_bar.update(batch_size)\n            progress_bar.set_postfix(NLL=nll_meter.avg)\n\n            preds, _ = util.convert_tokens(gold_dict,\n                                           ids.tolist(),\n                                           starts.tolist(),\n                                           ends.tolist(),\n                                           use_squad_v2)\n            pred_dict.update(preds)\n\n    model.train()\n\n    results = util.eval_dicts(gold_dict, pred_dict, use_squad_v2)\n    results_list = [(\'NLL\', nll_meter.avg),\n                    (\'F1\', results[\'F1\']),\n                    (\'EM\', results[\'EM\'])]\n    if use_squad_v2:\n        results_list.append((\'AvNA\', results[\'AvNA\']))\n    results = OrderedDict(results_list)\n\n    return results, pred_dict\n\n\nif __name__ == \'__main__\':\n    main(get_train_args())\n'"
squad_final_project/util.py,56,"b'""""""Utility classes and methods.\n\nAuthor:\n    Chris Chute (chute@stanford.edu)\n""""""\nimport logging\nimport os\nimport queue\nimport re\nimport shutil\nimport string\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport tqdm\nimport numpy as np\nimport ujson as json\n\nfrom collections import Counter\n\n\nclass SQuAD(data.Dataset):\n    """"""Stanford Question Answering Dataset (SQuAD).\n\n    Each item in the dataset is a tuple with the following entries (in order):\n        - context_idxs: Indices of the words in the context.\n            Shape (context_len,).\n        - context_char_idxs: Indices of the characters in the context.\n            Shape (context_len, max_word_len).\n        - question_idxs: Indices of the words in the question.\n            Shape (question_len,).\n        - question_char_idxs: Indices of the characters in the question.\n            Shape (question_len, max_word_len).\n        - y1: Index of word in the context where the answer begins.\n            -1 if no answer.\n        - y2: Index of word in the context where the answer ends.\n            -1 if no answer.\n        - id: ID of the example.\n\n    Args:\n        data_path (str): Path to .npz file containing pre-processed dataset.\n        use_v2 (bool): Whether to use SQuAD 2.0 questions. Otherwise only use SQuAD 1.1.\n    """"""\n    def __init__(self, data_path, use_v2=True):\n        super(SQuAD, self).__init__()\n\n        dataset = np.load(data_path)\n        self.context_idxs = torch.from_numpy(dataset[\'context_idxs\']).long()\n        self.context_char_idxs = torch.from_numpy(dataset[\'context_char_idxs\']).long()\n        self.question_idxs = torch.from_numpy(dataset[\'ques_idxs\']).long()\n        self.question_char_idxs = torch.from_numpy(dataset[\'ques_char_idxs\']).long()\n        self.y1s = torch.from_numpy(dataset[\'y1s\']).long()\n        self.y2s = torch.from_numpy(dataset[\'y2s\']).long()\n\n        if use_v2:\n            # SQuAD 2.0: Use index 0 for no-answer token (token 1 = OOV)\n            batch_size, c_len, w_len = self.context_char_idxs.size()\n            ones = torch.ones((batch_size, 1), dtype=torch.int64)\n            self.context_idxs = torch.cat((ones, self.context_idxs), dim=1)\n            self.question_idxs = torch.cat((ones, self.question_idxs), dim=1)\n\n            ones = torch.ones((batch_size, 1, w_len), dtype=torch.int64)\n            self.context_char_idxs = torch.cat((ones, self.context_char_idxs), dim=1)\n            self.question_char_idxs = torch.cat((ones, self.question_char_idxs), dim=1)\n\n            self.y1s += 1\n            self.y2s += 1\n\n        # SQuAD 1.1: Ignore no-answer examples\n        self.ids = torch.from_numpy(dataset[\'ids\']).long()\n        self.valid_idxs = [idx for idx in range(len(self.ids))\n                           if use_v2 or self.y1s[idx].item() >= 0]\n\n    def __getitem__(self, idx):\n        idx = self.valid_idxs[idx]\n        example = (self.context_idxs[idx],\n                   self.context_char_idxs[idx],\n                   self.question_idxs[idx],\n                   self.question_char_idxs[idx],\n                   self.y1s[idx],\n                   self.y2s[idx],\n                   self.ids[idx])\n\n        return example\n\n    def __len__(self):\n        return len(self.valid_idxs)\n\n\ndef collate_fn(examples):\n    """"""Create batch tensors from a list of individual examples returned\n    by `SQuAD.__getitem__`. Merge examples of different length by padding\n    all examples to the maximum length in the batch.\n\n    Args:\n        examples (list): List of tuples of the form (context_idxs, context_char_idxs,\n        question_idxs, question_char_idxs, y1s, y2s, ids).\n\n    Returns:\n        examples (tuple): Tuple of tensors (context_idxs, context_char_idxs, question_idxs,\n        question_char_idxs, y1s, y2s, ids). All of shape (batch_size, ...), where\n        the remaining dimensions are the maximum length of examples in the input.\n\n    Adapted from:\n        https://github.com/yunjey/seq2seq-dataloader\n    """"""\n    def merge_0d(scalars, dtype=torch.int64):\n        return torch.tensor(scalars, dtype=dtype)\n\n    def merge_1d(arrays, dtype=torch.int64, pad_value=0):\n        lengths = [(a != pad_value).sum() for a in arrays]\n        padded = torch.zeros(len(arrays), max(lengths), dtype=dtype)\n        for i, seq in enumerate(arrays):\n            end = lengths[i]\n            padded[i, :end] = seq[:end]\n        return padded\n\n    def merge_2d(matrices, dtype=torch.int64, pad_value=0):\n        heights = [(m.sum(1) != pad_value).sum() for m in matrices]\n        widths = [(m.sum(0) != pad_value).sum() for m in matrices]\n        padded = torch.zeros(len(matrices), max(heights), max(widths), dtype=dtype)\n        for i, seq in enumerate(matrices):\n            height, width = heights[i], widths[i]\n            padded[i, :height, :width] = seq[:height, :width]\n        return padded\n\n    # Group by tensor type\n    context_idxs, context_char_idxs, \\\n        question_idxs, question_char_idxs, \\\n        y1s, y2s, ids = zip(*examples)\n\n    # Merge into batch tensors\n    context_idxs = merge_1d(context_idxs)\n    context_char_idxs = merge_2d(context_char_idxs)\n    question_idxs = merge_1d(question_idxs)\n    question_char_idxs = merge_2d(question_char_idxs)\n    y1s = merge_0d(y1s)\n    y2s = merge_0d(y2s)\n    ids = merge_0d(ids)\n\n    return (context_idxs, context_char_idxs,\n            question_idxs, question_char_idxs,\n            y1s, y2s, ids)\n\n\nclass AverageMeter:\n    """"""Keep track of average values over time.\n\n    Adapted from:\n        > https://github.com/pytorch/examples/blob/master/imagenet/main.py\n    """"""\n    def __init__(self):\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def reset(self):\n        """"""Reset meter.""""""\n        self.__init__()\n\n    def update(self, val, num_samples=1):\n        """"""Update meter with new value `val`, the average of `num` samples.\n\n        Args:\n            val (float): Average value to update the meter with.\n            num_samples (int): Number of samples that were averaged to\n                produce `val`.\n        """"""\n        self.count += num_samples\n        self.sum += val * num_samples\n        self.avg = self.sum / self.count\n\n\nclass EMA:\n    """"""Exponential moving average of model parameters.\n    Args:\n        model (torch.nn.Module): Model with parameters whose EMA will be kept.\n        decay (float): Decay rate for exponential moving average.\n    """"""\n    def __init__(self, model, decay):\n        self.decay = decay\n        self.shadow = {}\n        self.original = {}\n\n        # Register model parameters\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.data.clone()\n\n    def __call__(self, model, num_updates):\n        decay = min(self.decay, (1.0 + num_updates) / (10.0 + num_updates))\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                assert name in self.shadow\n                new_average = \\\n                    (1.0 - decay) * param.data + decay * self.shadow[name]\n                self.shadow[name] = new_average.clone()\n\n    def assign(self, model):\n        """"""Assign exponential moving average of parameter values to the\n        respective parameters.\n        Args:\n            model (torch.nn.Module): Model to assign parameter values.\n        """"""\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                assert name in self.shadow\n                self.original[name] = param.data.clone()\n                param.data = self.shadow[name]\n\n    def resume(self, model):\n        """"""Restore original parameters to a model. That is, put back\n        the values that were in each parameter at the last call to `assign`.\n        Args:\n            model (torch.nn.Module): Model to assign parameter values.\n        """"""\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                assert name in self.shadow\n                param.data = self.original[name]\n\n\nclass CheckpointSaver:\n    """"""Class to save and load model checkpoints.\n\n    Save the best checkpoints as measured by a metric value passed into the\n    `save` method. Overwrite checkpoints with better checkpoints once\n    `max_checkpoints` have been saved.\n\n    Args:\n        save_dir (str): Directory to save checkpoints.\n        max_checkpoints (int): Maximum number of checkpoints to keep before\n            overwriting old ones.\n        metric_name (str): Name of metric used to determine best model.\n        maximize_metric (bool): If true, best checkpoint is that which maximizes\n            the metric value passed in via `save`. Otherwise, best checkpoint\n            minimizes the metric.\n        log (logging.Logger): Optional logger for printing information.\n    """"""\n    def __init__(self, save_dir, max_checkpoints, metric_name,\n                 maximize_metric=False, log=None):\n        super(CheckpointSaver, self).__init__()\n\n        self.save_dir = save_dir\n        self.max_checkpoints = max_checkpoints\n        self.metric_name = metric_name\n        self.maximize_metric = maximize_metric\n        self.best_val = None\n        self.ckpt_paths = queue.PriorityQueue()\n        self.log = log\n        self._print(\'Saver will {}imize {}...\'\n                    .format(\'max\' if maximize_metric else \'min\', metric_name))\n\n    def is_best(self, metric_val):\n        """"""Check whether `metric_val` is the best seen so far.\n\n        Args:\n            metric_val (float): Metric value to compare to prior checkpoints.\n        """"""\n        if metric_val is None:\n            # No metric reported\n            return False\n\n        if self.best_val is None:\n            # No checkpoint saved yet\n            return True\n\n        return ((self.maximize_metric and self.best_val < metric_val)\n                or (not self.maximize_metric and self.best_val > metric_val))\n\n    def _print(self, message):\n        """"""Print a message if logging is enabled.""""""\n        if self.log is not None:\n            self.log.info(message)\n\n    def save(self, step, model, metric_val, device):\n        """"""Save model parameters to disk.\n\n        Args:\n            step (int): Total number of examples seen during training so far.\n            model (torch.nn.DataParallel): Model to save.\n            metric_val (float): Determines whether checkpoint is best so far.\n            device (torch.device): Device where model resides.\n        """"""\n        ckpt_dict = {\n            \'model_name\': model.__class__.__name__,\n            \'model_state\': model.cpu().state_dict(),\n            \'step\': step\n        }\n        model.to(device)\n\n        checkpoint_path = os.path.join(self.save_dir,\n                                       \'step_{}.pth.tar\'.format(step))\n        torch.save(ckpt_dict, checkpoint_path)\n        self._print(\'Saved checkpoint: {}\'.format(checkpoint_path))\n\n        if self.is_best(metric_val):\n            # Save the best model\n            self.best_val = metric_val\n            best_path = os.path.join(self.save_dir, \'best.pth.tar\')\n            shutil.copy(checkpoint_path, best_path)\n            self._print(\'New best checkpoint at step {}...\'.format(step))\n\n        # Add checkpoint path to priority queue (lowest priority removed first)\n        if self.maximize_metric:\n            priority_order = metric_val\n        else:\n            priority_order = -metric_val\n\n        self.ckpt_paths.put((priority_order, checkpoint_path))\n\n        # Remove a checkpoint if more than max_checkpoints have been saved\n        if self.ckpt_paths.qsize() > self.max_checkpoints:\n            _, worst_ckpt = self.ckpt_paths.get()\n            try:\n                os.remove(worst_ckpt)\n                self._print(\'Removed checkpoint: {}\'.format(worst_ckpt))\n            except OSError:\n                # Avoid crashing if checkpoint has been removed or protected\n                pass\n\n\ndef load_model(model, checkpoint_path, gpu_ids, return_step=True):\n    """"""Load model parameters from disk.\n\n    Args:\n        model (torch.nn.DataParallel): Load parameters into this model.\n        checkpoint_path (str): Path to checkpoint to load.\n        gpu_ids (list): GPU IDs for DataParallel.\n        return_step (bool): Also return the step at which checkpoint was saved.\n\n    Returns:\n        model (torch.nn.DataParallel): Model loaded from checkpoint.\n        step (int): Step at which checkpoint was saved. Only if `return_step`.\n    """"""\n    device = \'cuda:{}\'.format(gpu_ids[0]) if gpu_ids else \'cpu\'\n    ckpt_dict = torch.load(checkpoint_path, map_location=device)\n\n    # Build model, load parameters\n    model.load_state_dict(ckpt_dict[\'model_state\'])\n\n    if return_step:\n        step = ckpt_dict[\'step\']\n        return model, step\n\n    return model\n\n\ndef get_available_devices():\n    """"""Get IDs of all available GPUs.\n\n    Returns:\n        device (torch.device): Main device (GPU 0 or CPU).\n        gpu_ids (list): List of IDs of all GPUs that are available.\n    """"""\n    gpu_ids = []\n    if torch.cuda.is_available():\n        gpu_ids += [gpu_id for gpu_id in range(torch.cuda.device_count())]\n        device = torch.device(\'cuda:{}\'.format(gpu_ids[0]))\n        torch.cuda.set_device(device)\n    else:\n        device = torch.device(\'cpu\')\n\n    return device, gpu_ids\n\n\ndef masked_softmax(logits, mask, dim=-1, log_softmax=False):\n    """"""Take the softmax of `logits` over given dimension, and set\n    entries to 0 wherever `mask` is 0.\n\n    Args:\n        logits (torch.Tensor): Inputs to the softmax function.\n        mask (torch.Tensor): Same shape as `logits`, with 0 indicating\n            positions that should be assigned 0 probability in the output.\n        dim (int): Dimension over which to take softmax.\n        log_softmax (bool): Take log-softmax rather than regular softmax.\n            E.g., some PyTorch functions such as `F.nll_loss` expect log-softmax.\n\n    Returns:\n        probs (torch.Tensor): Result of taking masked softmax over the logits.\n    """"""\n    mask = mask.type(torch.float32)\n    masked_logits = mask * logits + (1 - mask) * -1e30\n    softmax_fn = F.log_softmax if log_softmax else F.softmax\n    probs = softmax_fn(masked_logits, dim)\n\n    return probs\n\n\ndef visualize(tbx, pred_dict, eval_path, step, split, num_visuals):\n    """"""Visualize text examples to TensorBoard.\n\n    Args:\n        tbx (tensorboardX.SummaryWriter): Summary writer.\n        pred_dict (dict): dict of predictions of the form id -> pred.\n        eval_path (str): Path to eval JSON file.\n        step (int): Number of examples seen so far during training.\n        split (str): Name of data split being visualized.\n        num_visuals (int): Number of visuals to select at random from preds.\n    """"""\n    if num_visuals <= 0:\n        return\n    if num_visuals > len(pred_dict):\n        num_visuals = len(pred_dict)\n\n    visual_ids = np.random.choice(list(pred_dict), size=num_visuals, replace=False)\n\n    with open(eval_path, \'r\') as eval_file:\n        eval_dict = json.load(eval_file)\n    for i, id_ in enumerate(visual_ids):\n        pred = pred_dict[id_] or \'N/A\'\n        example = eval_dict[str(id_)]\n        question = example[\'question\']\n        context = example[\'context\']\n        answers = example[\'answers\']\n\n        gold = answers[0] if answers else \'N/A\'\n        tbl_fmt = (\'- **Question:** {}\\n\'\n                   + \'- **Context:** {}\\n\'\n                   + \'- **Answer:** {}\\n\'\n                   + \'- **Prediction:** {}\')\n        tbx.add_text(tag=\'{}/{}_of_{}\'.format(split, i + 1, num_visuals),\n                     text_string=tbl_fmt.format(question, context, gold, pred),\n                     global_step=step)\n\n\ndef save_preds(preds, save_dir, file_name=\'predictions.csv\'):\n    """"""Save predictions `preds` to a CSV file named `file_name` in `save_dir`.\n\n    Args:\n        preds (list): List of predictions each of the form (id, start, end),\n            where id is an example ID, and start/end are indices in the context.\n        save_dir (str): Directory in which to save the predictions file.\n        file_name (str): File name for the CSV file.\n\n    Returns:\n        save_path (str): Path where CSV file was saved.\n    """"""\n    # Validate format\n    if (not isinstance(preds, list)\n            or any(not isinstance(p, tuple) or len(p) != 3 for p in preds)):\n        raise ValueError(\'preds must be a list of tuples (id, start, end)\')\n\n    # Make sure predictions are sorted by ID\n    preds = sorted(preds, key=lambda p: p[0])\n\n    # Save to a CSV file\n    save_path = os.path.join(save_dir, file_name)\n    np.savetxt(save_path, np.array(preds), delimiter=\',\', fmt=\'%d\')\n\n    return save_path\n\n\ndef get_save_dir(base_dir, name, training, id_max=100):\n    """"""Get a unique save directory by appending the smallest positive integer\n    `id < id_max` that is not already taken (i.e., no dir exists with that id).\n\n    Args:\n        base_dir (str): Base directory in which to make save directories.\n        name (str): Name to identify this training run. Need not be unique.\n        training (bool): Save dir. is for training (determines subdirectory).\n        id_max (int): Maximum ID number before raising an exception.\n\n    Returns:\n        save_dir (str): Path to a new directory with a unique name.\n    """"""\n    for uid in range(1, id_max):\n        subdir = \'train\' if training else \'test\'\n        save_dir = os.path.join(base_dir, subdir, \'{}-{:02d}\'.format(name, uid))\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n            return save_dir\n\n    raise RuntimeError(\'Too many save directories created with the same name. \\\n                       Delete old save directories or use another name.\')\n\n\ndef get_logger(log_dir, name):\n    """"""Get a `logging.Logger` instance that prints to the console\n    and an auxiliary file.\n\n    Args:\n        log_dir (str): Directory in which to create the log file.\n        name (str): Name to identify the logs.\n\n    Returns:\n        logger (logging.Logger): Logger instance for logging events.\n    """"""\n    class StreamHandlerWithTQDM(logging.Handler):\n        """"""Let `logging` print without breaking `tqdm` progress bars.\n\n        See Also:\n            > https://stackoverflow.com/questions/38543506\n        """"""\n        def emit(self, record):\n            try:\n                msg = self.format(record)\n                tqdm.tqdm.write(msg)\n                self.flush()\n            except (KeyboardInterrupt, SystemExit):\n                raise\n            except:\n                self.handleError(record)\n\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n\n    # Log everything (i.e., DEBUG level and above) to a file\n    log_path = os.path.join(log_dir, \'log.txt\')\n    file_handler = logging.FileHandler(log_path)\n    file_handler.setLevel(logging.DEBUG)\n\n    # Log everything except DEBUG level (i.e., INFO level and above) to console\n    console_handler = StreamHandlerWithTQDM()\n    console_handler.setLevel(logging.INFO)\n\n    # Create format for the logs\n    file_formatter = logging.Formatter(\'[%(asctime)s] %(message)s\',\n                                       datefmt=\'%m.%d.%y %H:%M:%S\')\n    file_handler.setFormatter(file_formatter)\n    console_formatter = logging.Formatter(\'[%(asctime)s] %(message)s\',\n                                          datefmt=\'%m.%d.%y %H:%M:%S\')\n    console_handler.setFormatter(console_formatter)\n\n    # add the handlers to the logger\n    logger.addHandler(file_handler)\n    logger.addHandler(console_handler)\n\n    return logger\n\n\ndef torch_from_json(path, dtype=torch.float32):\n    """"""Load a PyTorch Tensor from a JSON file.\n\n    Args:\n        path (str): Path to the JSON file to load.\n        dtype (torch.dtype): Data type of loaded array.\n\n    Returns:\n        tensor (torch.Tensor): Tensor loaded from JSON file.\n    """"""\n    with open(path, \'r\') as fh:\n        array = np.array(json.load(fh))\n\n    tensor = torch.from_numpy(array).type(dtype)\n\n    return tensor\n\n\ndef discretize(p_start, p_end, max_len=15, no_answer=False):\n    """"""Discretize soft predictions to get start and end indices.\n\n    Choose the pair `(i, j)` of indices that maximizes `p1[i] * p2[j]`\n    subject to `i <= j` and `j - i + 1 <= max_len`.\n\n    Args:\n        p_start (torch.Tensor): Soft predictions for start index.\n            Shape (batch_size, context_len).\n        p_end (torch.Tensor): Soft predictions for end index.\n            Shape (batch_size, context_len).\n        max_len (int): Maximum length of the discretized prediction.\n            I.e., enforce that `preds[i, 1] - preds[i, 0] + 1 <= max_len`.\n        no_answer (bool): Treat 0-index as the no-answer prediction. Consider\n            a prediction no-answer if `preds[0, 0] * preds[0, 1]` is greater\n            than the probability assigned to the max-probability span.\n\n    Returns:\n        start_idxs (torch.Tensor): Hard predictions for start index.\n            Shape (batch_size,)\n        end_idxs (torch.Tensor): Hard predictions for end index.\n            Shape (batch_size,)\n    """"""\n    if p_start.min() < 0 or p_start.max() > 1 \\\n            or p_end.min() < 0 or p_end.max() > 1:\n        raise ValueError(\'Expected p_start and p_end to have values in [0, 1]\')\n\n    # Compute pairwise probabilities\n    p_start = p_start.unsqueeze(dim=2)\n    p_end = p_end.unsqueeze(dim=1)\n    p_joint = torch.matmul(p_start, p_end)  # (batch_size, c_len, c_len)\n\n    # Restrict to pairs (i, j) such that i <= j <= i + max_len - 1\n    c_len, device = p_start.size(1), p_start.device\n    is_legal_pair = torch.triu(torch.ones((c_len, c_len), device=device))\n    is_legal_pair -= torch.triu(torch.ones((c_len, c_len), device=device),\n                                diagonal=max_len)\n    if no_answer:\n        # Index 0 is no-answer\n        p_no_answer = p_joint[:, 0, 0].clone()\n        is_legal_pair[0, :] = 0\n        is_legal_pair[:, 0] = 0\n    else:\n        p_no_answer = None\n    p_joint *= is_legal_pair\n\n    # Take pair (i, j) that maximizes p_joint\n    max_in_row, _ = torch.max(p_joint, dim=2)\n    max_in_col, _ = torch.max(p_joint, dim=1)\n    start_idxs = torch.argmax(max_in_row, dim=-1)\n    end_idxs = torch.argmax(max_in_col, dim=-1)\n\n    if no_answer:\n        # Predict no-answer whenever p_no_answer > max_prob\n        max_prob, _ = torch.max(max_in_col, dim=-1)\n        start_idxs[p_no_answer > max_prob] = 0\n        end_idxs[p_no_answer > max_prob] = 0\n\n    return start_idxs, end_idxs\n\n\ndef convert_tokens(eval_dict, qa_id, y_start_list, y_end_list, no_answer):\n    """"""Convert predictions to tokens from the context.\n\n    Args:\n        eval_dict (dict): Dictionary with eval info for the dataset. This is\n            used to perform the mapping from IDs and indices to actual text.\n        qa_id (int): List of QA example IDs.\n        y_start_list (list): List of start predictions.\n        y_end_list (list): List of end predictions.\n        no_answer (bool): Questions can have no answer. E.g., SQuAD 2.0.\n\n    Returns:\n        pred_dict (dict): Dictionary index IDs -> predicted answer text.\n        sub_dict (dict): Dictionary UUIDs -> predicted answer text (submission).\n    """"""\n    pred_dict = {}\n    sub_dict = {}\n    for qid, y_start, y_end in zip(qa_id, y_start_list, y_end_list):\n        context = eval_dict[str(qid)][""context""]\n        spans = eval_dict[str(qid)][""spans""]\n        uuid = eval_dict[str(qid)][""uuid""]\n        if no_answer and (y_start == 0 or y_end == 0):\n            pred_dict[str(qid)] = \'\'\n            sub_dict[uuid] = \'\'\n        else:\n            if no_answer:\n                y_start, y_end = y_start - 1, y_end - 1\n            start_idx = spans[y_start][0]\n            end_idx = spans[y_end][1]\n            pred_dict[str(qid)] = context[start_idx: end_idx]\n            sub_dict[uuid] = context[start_idx: end_idx]\n    return pred_dict, sub_dict\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n    if not ground_truths:\n        return metric_fn(prediction, \'\')\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    return max(scores_for_ground_truths)\n\n\ndef eval_dicts(gold_dict, pred_dict, no_answer):\n    avna = f1 = em = total = 0\n    for key, value in pred_dict.items():\n        total += 1\n        ground_truths = gold_dict[key][\'answers\']\n        prediction = value\n        em += metric_max_over_ground_truths(compute_em, prediction, ground_truths)\n        f1 += metric_max_over_ground_truths(compute_f1, prediction, ground_truths)\n        if no_answer:\n            avna += compute_avna(prediction, ground_truths)\n\n    eval_dict = {\'EM\': 100. * em / total,\n                 \'F1\': 100. * f1 / total}\n\n    if no_answer:\n        eval_dict[\'AvNA\'] = 100. * avna / total\n\n    return eval_dict\n\n\ndef compute_avna(prediction, ground_truths):\n    """"""Compute answer vs. no-answer accuracy.""""""\n    return float(bool(prediction) == bool(ground_truths))\n\n\n# All methods below this line are from the official SQuAD 2.0 eval script\n# https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\ndef normalize_answer(s):\n    """"""Convert to lowercase and remove punctuation, articles and extra whitespace.""""""\n\n    def remove_articles(text):\n        regex = re.compile(r\'\\b(a|an|the)\\b\', re.UNICODE)\n        return re.sub(regex, \' \', text)\n\n    def white_space_fix(text):\n        return \' \'.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \'\'.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\n\ndef compute_em(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = Counter(gold_toks) & Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n'"
Assignment/a2/run.py,0,"b'#!/usr/bin/env python\n\nimport random\nimport numpy as np\nfrom utils.treebank import StanfordSentiment\nimport matplotlib\nmatplotlib.use(\'agg\')\nimport matplotlib.pyplot as plt\nimport time\n\nfrom word2vec import *\nfrom sgd import *\n\n# Check Python Version\nimport sys\nassert sys.version_info[0] == 3\nassert sys.version_info[1] >= 5\n\n# Reset the random seed to make sure that everyone gets the same results\nrandom.seed(314)\ndataset = StanfordSentiment()\ntokens = dataset.tokens()\nnWords = len(tokens)\n\n# We are going to train 10-dimensional vectors for this assignment\ndimVectors = 10\n\n# Context size\nC = 5\n\n# Reset the random seed to make sure that everyone gets the same results\nrandom.seed(31415)\nnp.random.seed(9265)\n\nstartTime=time.time()\nwordVectors = np.concatenate(\n    ((np.random.rand(nWords, dimVectors) - 0.5) /\n       dimVectors, np.zeros((nWords, dimVectors))),\n    axis=0)\nwordVectors = sgd(\n    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C,\n        negSamplingLossAndGradient),\n    wordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n# Note that normalization is not called here. This is not a bug,\n# normalizing during training loses the notion of length.\n\nprint(""sanity check: cost at convergence should be around or below 10"")\nprint(""training took %d seconds"" % (time.time() - startTime))\n\n# concatenate the input and output word vectors\nwordVectors = np.concatenate(\n    (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n    axis=0)\n\nvisualizeWords = [\n    ""great"", ""cool"", ""brilliant"", ""wonderful"", ""well"", ""amazing"",\n    ""worth"", ""sweet"", ""enjoyable"", ""boring"", ""bad"", ""dumb"",\n    ""annoying"", ""female"", ""male"", ""queen"", ""king"", ""man"", ""woman"", ""rain"", ""snow"",\n    ""hail"", ""coffee"", ""tea""]\n\nvisualizeIdx = [tokens[word] for word in visualizeWords]\nvisualizeVecs = wordVectors[visualizeIdx, :]\ntemp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\ncovariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\nU,S,V = np.linalg.svd(covariance)\ncoord = temp.dot(U[:,0:2])\n\nfor i in range(len(visualizeWords)):\n    plt.text(coord[i,0], coord[i,1], visualizeWords[i],\n        bbox=dict(facecolor=\'green\', alpha=0.1))\n\nplt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\nplt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n\nplt.savefig(\'word_vectors.png\')\n'"
Assignment/a2/sgd.py,0,"b'#!/usr/bin/env python\n\n# Save parameters every a few SGD iterations as fail-safe\nSAVE_PARAMS_EVERY = 5000\n\nimport pickle\nimport glob\nimport random\nimport numpy as np\nimport os.path as op\n\ndef load_saved_params():\n    """"""\n    A helper function that loads previously saved parameters and resets\n    iteration start.\n    """"""\n    st = 0\n    for f in glob.glob(""saved_params_*.npy""):\n        iter = int(op.splitext(op.basename(f))[0].split(""_"")[2])\n        if (iter > st):\n            st = iter\n\n    if st > 0:\n        params_file = ""saved_params_%d.npy"" % st\n        state_file = ""saved_state_%d.pickle"" % st\n        params = np.load(params_file)\n        with open(state_file, ""rb"") as f:\n            state = pickle.load(f)\n        return st, params, state\n    else:\n        return st, None, None\n\n\ndef save_params(iter, params):\n    params_file = ""saved_params_%d.npy"" % iter\n    np.save(params_file, params)\n    with open(""saved_state_%d.pickle"" % iter, ""wb"") as f:\n        pickle.dump(random.getstate(), f)\n\n\ndef sgd(f, x0, step, iterations, postprocessing=None, useSaved=False,\n        PRINT_EVERY=10):\n    """""" Stochastic Gradient Descent\n\n    Implement the stochastic gradient descent method in this function.\n\n    Arguments:\n    f -- the function to optimize, it should take a single\n         argument and yield two outputs, a loss and the gradient\n         with respect to the arguments\n    x0 -- the initial point to start SGD from\n    step -- the step size for SGD\n    iterations -- total iterations to run SGD for\n    postprocessing -- postprocessing function for the parameters\n                      if necessary. In the case of word2vec we will need to\n                      normalize the word vectors to have unit length.\n    PRINT_EVERY -- specifies how many iterations to output loss\n\n    Return:\n    x -- the parameter value after SGD finishes\n    """"""\n\n    # Anneal learning rate every several iterations\n    ANNEAL_EVERY = 20000\n\n    if useSaved:\n        start_iter, oldx, state = load_saved_params()\n        if start_iter > 0:\n            x0 = oldx\n            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n\n        if state:\n            random.setstate(state)\n    else:\n        start_iter = 0\n\n    x = x0\n\n    if not postprocessing:\n        postprocessing = lambda x: x\n\n    exploss = None\n\n    for iter in range(start_iter + 1, iterations + 1):\n        # You might want to print the progress every few iterations.\n\n        loss = None\n        ### YOUR CODE HERE\n        loss, grad = f(x)\n        x -= step * grad\n        ### END YOUR CODE\n\n        x = postprocessing(x)\n        if iter % PRINT_EVERY == 0:\n            if not exploss:\n                exploss = loss\n            else:\n                exploss = .95 * exploss + .05 * loss\n            print(""iter %d: %f"" % (iter, exploss))\n\n        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n            save_params(iter, x)\n\n        if iter % ANNEAL_EVERY == 0:\n            step *= 0.5\n\n    return x\n\n\ndef sanity_check():\n    quad = lambda x: (np.sum(x ** 2), x * 2)\n\n    print(""Running sanity checks..."")\n    t1 = sgd(quad, 0.5, 0.01, 1000, PRINT_EVERY=100)\n    print(""test 1 result:"", t1)\n    assert abs(t1) <= 1e-6\n\n    t2 = sgd(quad, 0.0, 0.01, 1000, PRINT_EVERY=100)\n    print(""test 2 result:"", t2)\n    assert abs(t2) <= 1e-6\n\n    t3 = sgd(quad, -1.5, 0.01, 1000, PRINT_EVERY=100)\n    print(""test 3 result:"", t3)\n    assert abs(t3) <= 1e-6\n\n    print(""-"" * 40)\n    print(""ALL TESTS PASSED"")\n    print(""-"" * 40)\n\n\nif __name__ == ""__main__"":\n    sanity_check()\n'"
Assignment/a2/word2vec.py,0,"b'#!/usr/bin/env python\n\nimport numpy as np\nimport random\n\nfrom utils.gradcheck import gradcheck_naive\nfrom utils.utils import normalizeRows, softmax\n\n\ndef sigmoid(x):\n    """"""\n    Compute the sigmoid function for the input here.\n    Arguments:\n    x -- A scalar or numpy array.\n    Return:\n    s -- sigmoid(x)\n    """"""\n\n    ### YOUR CODE HERE\n    s = 1./(1. + np.exp(-x))\n    ### END YOUR CODE\n\n    return s\n\n\ndef naiveSoftmaxLossAndGradient(\n    centerWordVec,\n    outsideWordIdx,\n    outsideVectors,\n    dataset\n):\n    """""" Naive Softmax loss & gradient function for word2vec models\n\n    Implement the naive softmax loss and gradients between a center word\'s \n    embedding and an outside word\'s embedding. This will be the building block\n    for our word2vec models.\n\n    Arguments:\n    centerWordVec -- numpy ndarray, center word\'s embedding\n                    (v_c in the pdf handout)\n    outsideWordIdx -- integer, the index of the outside word\n                    (o of u_o in the pdf handout)\n    outsideVectors -- outside vectors (rows of matrix) for all words in vocab\n                      (U in the pdf handout)\n    dataset -- needed for negative sampling, unused here.\n\n    Return:\n    loss -- naive softmax loss\n    gradCenterVec -- the gradient with respect to the center word vector\n                     (dJ / dv_c in the pdf handout)\n    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n                    (dJ / dU)\n    """"""\n\n    ### YOUR CODE HERE\n\n    ### Please use the provided softmax function (imported earlier in this file)\n    ### This numerically stable implementation helps you avoid issues pertaining\n    ### to integer overflow. \n    value = np.dot(outsideVectors, centerWordVec) # N x 1\n    y_hat  = softmax(value)\n    loss = - np.log(y_hat[outsideWordIdx])\n\n    d_value = y_hat\n    d_value[outsideWordIdx] -= 1 # y_hat - y, matrix shape (N, 1)\n    gradCenterVec   = outsideVectors.T.dot(d_value) # shape d x 1\n    gradOutsideVecs = d_value[:, np.newaxis].dot( np.array([centerWordVec]) ) # (N, 1) dot (1, d) -> (N, d)\n\n    ### END YOUR CODE\n\n    return loss, gradCenterVec, gradOutsideVecs\n\n\ndef getNegativeSamples(outsideWordIdx, dataset, K):\n    """""" Samples K indexes which are not the outsideWordIdx """"""\n\n    negSampleWordIndices = [None] * K\n    for k in range(K):\n        newidx = dataset.sampleTokenIdx()\n        while newidx == outsideWordIdx:\n            newidx = dataset.sampleTokenIdx()\n        negSampleWordIndices[k] = newidx\n    return negSampleWordIndices\n\n\ndef negSamplingLossAndGradient(\n    centerWordVec,\n    outsideWordIdx,\n    outsideVectors,\n    dataset,\n    K=10\n):\n    """""" Negative sampling loss function for word2vec models\n\n    Implement the negative sampling loss and gradients for a centerWordVec\n    and a outsideWordIdx word vector as a building block for word2vec\n    models. K is the number of negative samples to take.\n\n    Note: The same word may be negatively sampled multiple times. For\n    example if an outside word is sampled twice, you shall have to\n    double count the gradient with respect to this word. Thrice if\n    it was sampled three times, and so forth.\n\n    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient\n    """"""\n\n    # Negative sampling of words is done for you. Do not modify this if you\n    # wish to match the autograder and receive points!\n    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n    indices = [outsideWordIdx] + negSampleWordIndices\n\n    ### YOUR CODE HERE\n    o_vector = outsideVectors[outsideWordIdx]\n    neg_vector = outsideVectors[negSampleWordIndices]\n    ### Please use your implementation of sigmoid in here.\n    value_outside  = o_vector.dot(centerWordVec)\n    value_negative = neg_vector.dot(centerWordVec)\n\n    p_outside  = sigmoid(value_outside)\n    p_negative = sigmoid(- value_negative)\n\n    loss = - (np.log(p_outside) + np.sum(np.log(p_negative)) )\n    gradCenterVec = (p_outside - 1) * o_vector + np.sum((1 - p_negative)[:, np.newaxis] * neg_vector, axis = 0)\n\n    gradOutsideVecs = np.zeros_like(outsideVectors)\n    gradOutsideVecs[outsideWordIdx] = (p_outside - 1) * centerWordVec\n    for i, neg_index in enumerate(negSampleWordIndices):\n        gradOutsideVecs[neg_index] += (1 - p_negative[i]) * centerWordVec # remember negative can appear multiple times\n\n    ### END YOUR CODE\n\n    return loss, gradCenterVec, gradOutsideVecs\n\n\ndef skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n             centerWordVectors, outsideVectors, dataset,\n             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n    """""" Skip-gram model in word2vec\n\n    Implement the skip-gram model in this function.\n\n    Arguments:\n    currentCenterWord -- a string of the current center word\n    windowSize -- integer, context window size\n    outsideWords -- list of no more than 2*windowSize strings, the outside words\n    word2Ind -- a dictionary that maps words to their indices in\n              the word vector list\n    centerWordVectors -- center word vectors (as rows) for all words in vocab\n                        (V in pdf handout)\n    outsideVectors -- outside word vectors (as rows) for all words in vocab\n                    (U in pdf handout)\n    word2vecLossAndGradient -- the loss and gradient function for\n                               a prediction vector given the outsideWordIdx\n                               word vectors, could be one of the two\n                               loss functions you implemented above.\n\n    Return:\n    loss -- the loss function value for the skip-gram model\n            (J in the pdf handout)\n    gradCenterVecs -- the gradient with respect to the center word vectors\n            (dJ / dV in the pdf handout)\n    gradOutsideVectors -- the gradient with respect to the outside word vectors\n                        (dJ / dU in the pdf handout)\n    """"""\n\n    loss = 0.0\n    gradCenterVecs = np.zeros(centerWordVectors.shape)\n    gradOutsideVectors = np.zeros(outsideVectors.shape)\n\n    ### YOUR CODE HERE\n    centerWordIdx = word2Ind[currentCenterWord]\n    centerWordVec = centerWordVectors[centerWordIdx] \n    outsideWordIndices = [word2Ind[i] for i in outsideWords]\n\n    for outsideWordIdx in outsideWordIndices:\n        one_loss, one_gradCenter, one_gradOutside = \\\n            word2vecLossAndGradient(centerWordVec, outsideWordIdx, outsideVectors, dataset) \n        loss += one_loss\n        gradCenterVecs[centerWordIdx] += one_gradCenter\n        gradOutsideVectors += one_gradOutside\n    ### END YOUR CODE\n\n    return loss, gradCenterVecs, gradOutsideVectors\n\n#############################################\n# Testing functions below. DO NOT MODIFY!   #\n#############################################\n\ndef word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset, \n                         windowSize,\n                         word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n    batchsize = 50\n    loss = 0.0\n    grad = np.zeros(wordVectors.shape)\n    N = wordVectors.shape[0]\n    centerWordVectors = wordVectors[:int(N/2),:]\n    outsideVectors = wordVectors[int(N/2):,:]\n    for i in range(batchsize):\n        windowSize1 = random.randint(1, windowSize)\n        centerWord, context = dataset.getRandomContext(windowSize1)\n\n        c, gin, gout = word2vecModel(\n            centerWord, windowSize1, context, word2Ind, centerWordVectors,\n            outsideVectors, dataset, word2vecLossAndGradient\n        )\n        loss += c / batchsize\n        grad[:int(N/2), :] += gin / batchsize\n        grad[int(N/2):, :] += gout / batchsize\n\n    return loss, grad\n\n\ndef test_word2vec():\n    """""" Test the two word2vec implementations, before running on Stanford Sentiment Treebank """"""\n    dataset = type(\'dummy\', (), {})()\n    def dummySampleTokenIdx():\n        return random.randint(0, 4)\n\n    def getRandomContext(C):\n        tokens = [""a"", ""b"", ""c"", ""d"", ""e""]\n        return tokens[random.randint(0,4)], \\\n            [tokens[random.randint(0,4)] for i in range(2*C)]\n    dataset.sampleTokenIdx = dummySampleTokenIdx\n    dataset.getRandomContext = getRandomContext\n\n    random.seed(31415)\n    np.random.seed(9265)\n    dummy_vectors = normalizeRows(np.random.randn(10,3))\n    dummy_tokens = dict([(""a"",0), (""b"",1), (""c"",2),(""d"",3),(""e"",4)])\n\n    print(""==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ===="")\n    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n        skipgram, dummy_tokens, vec, dataset, 5, naiveSoftmaxLossAndGradient),\n        dummy_vectors, ""naiveSoftmaxLossAndGradient Gradient"")\n\n    print(""==== Gradient check for skip-gram with negSamplingLossAndGradient ===="")\n    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n        skipgram, dummy_tokens, vec, dataset, 5, negSamplingLossAndGradient),\n        dummy_vectors, ""negSamplingLossAndGradient Gradient"")\n\n    print(""\\n=== Results ==="")\n    print (""Skip-Gram with naiveSoftmaxLossAndGradient"")\n\n    print (""Your Result:"")\n    print(""Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\nGradient wrt Outside Vectors (dJ/dU):\\n {}\\n"".format(\n            *skipgram(""c"", 3, [""a"", ""b"", ""e"", ""d"", ""b"", ""c""],\n                dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset) \n        )\n    )\n\n    print (""Expected Result: Value should approximate these:"")\n    print(""""""Loss: 11.16610900153398\nGradient wrt Center Vectors (dJ/dV):\n [[ 0.          0.          0.        ]\n [ 0.          0.          0.        ]\n [-1.26947339 -1.36873189  2.45158957]\n [ 0.          0.          0.        ]\n [ 0.          0.          0.        ]]\nGradient wrt Outside Vectors (dJ/dU):\n [[-0.41045956  0.18834851  1.43272264]\n [ 0.38202831 -0.17530219 -1.33348241]\n [ 0.07009355 -0.03216399 -0.24466386]\n [ 0.09472154 -0.04346509 -0.33062865]\n [-0.13638384  0.06258276  0.47605228]]\n    """""")\n\n    print (""Skip-Gram with negSamplingLossAndGradient"")   \n    print (""Your Result:"")\n    print(""Loss: {}\\nGradient wrt Center Vectors (dJ/dV):\\n {}\\n Gradient wrt Outside Vectors (dJ/dU):\\n {}\\n"".format(\n        *skipgram(""c"", 1, [""a"", ""b""], dummy_tokens, dummy_vectors[:5,:],\n            dummy_vectors[5:,:], dataset, negSamplingLossAndGradient)\n        )\n    )\n    print (""Expected Result: Value should approximate these:"")\n    print(""""""Loss: 16.15119285363322\nGradient wrt Center Vectors (dJ/dV):\n [[ 0.          0.          0.        ]\n [ 0.          0.          0.        ]\n [-4.54650789 -1.85942252  0.76397441]\n [ 0.          0.          0.        ]\n [ 0.          0.          0.        ]]\n Gradient wrt Outside Vectors (dJ/dU):\n [[-0.69148188  0.31730185  2.41364029]\n [-0.22716495  0.10423969  0.79292674]\n [-0.45528438  0.20891737  1.58918512]\n [-0.31602611  0.14501561  1.10309954]\n [-0.80620296  0.36994417  2.81407799]]\n    """""")\n\nif __name__ == ""__main__"":\n    test_word2vec()\n'"
Assignment/a3/parser_model.py,15,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCS224N 2018-19: Homework 3\nparser_model.py: Feed-Forward Neural Network for Dependency Parsing\nSahil Chopra <schopra8@stanford.edu>\n""""""\nimport pickle\nimport os\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ParserModel(nn.Module):\n    """""" Feedforward neural network with an embedding layer and single hidden layer.\n    The ParserModel will predict which transition should be applied to a\n    given partial parse configuration.\n\n    PyTorch Notes:\n        - Note that ""ParserModel"" is a subclass of the ""nn.Module"" class. In PyTorch all neural networks\n            are a subclass of this ""nn.Module"".\n        - The ""__init__"" method is where you define all the layers and their respective parameters\n            (embedding layers, linear layers, dropout layers, etc.).\n        - ""__init__"" gets automatically called when you create a new instance of your class, e.g.\n            when you write ""m = ParserModel()"".\n        - Other methods of ParserModel can access variables that have ""self."" prefix. Thus,\n            you should add the ""self."" prefix layers, values, etc. that you want to utilize\n            in other ParserModel methods.\n        - For further documentation on ""nn.Module"" please see https://pytorch.org/docs/stable/nn.html.\n    """"""\n    def __init__(self, embeddings, n_features=36,\n        hidden_size=200, n_classes=3, dropout_prob=0.5):\n        """""" Initialize the parser model.\n\n        @param embeddings (Tensor): word embeddings (num_words, embedding_size)\n        @param n_features (int): number of input features\n        @param hidden_size (int): number of hidden units\n        @param n_classes (int): number of output classes\n        @param dropout_prob (float): dropout probability\n        """"""\n        super(ParserModel, self).__init__()\n        self.n_features = n_features\n        self.n_classes = n_classes\n        self.dropout_prob = dropout_prob\n        self.embed_size = embeddings.shape[1]\n        self.hidden_size = hidden_size\n        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n\n        ### YOUR CODE HERE (~5 Lines)\n        ### TODO:\n        ###     1) Construct `self.embed_to_hidden` linear layer, initializing the weight matrix\n        ###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)\n        ###     2) Construct `self.dropout` layer.\n        ###     3) Construct `self.hidden_to_logits` linear layer, initializing the weight matrix\n        ###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)\n        ###\n        ### Note: Here, we use Xavier Uniform Initialization for our Weight initialization.\n        ###         It has been shown empirically, that this provides better initial weights\n        ###         for training networks than random uniform initialization.\n        ###         For more details checkout this great blogpost:\n        ###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization \n        ### Hints:\n        ###     - After you create a linear layer you can access the weight\n        ###       matrix via:\n        ###         linear_layer.weight\n        ###\n        ### Please see the following docs for support:\n        ###     Linear Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n        ###     Xavier Init: https://pytorch.org/docs/stable/nn.html#torch.nn.init.xavier_uniform_\n        ###     Dropout: https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n        self.embed_to_hidden = nn.Linear(self.embed_size * self.n_features, self.hidden_size)\n        nn.init.xavier_uniform_(self.embed_to_hidden.weight, gain=1)\n        self.dropout = nn.Dropout(p = self.dropout_prob)\n        self.hidden_to_logits = nn.Linear(self.hidden_size, self.n_classes)\n        nn.init.xavier_uniform_(self.hidden_to_logits.weight, gain=1)\n\n        ### END YOUR CODE\n\n    def embedding_lookup(self, t):\n        """""" Utilize `self.pretrained_embeddings` to map input `t` from input tokens (integers)\n            to embedding vectors.\n\n            PyTorch Notes:\n                - `self.pretrained_embeddings` is a torch.nn.Embedding object that we defined in __init__\n                - Here `t` is a tensor where each row represents a list of features. Each feature is represented by an integer (input token).\n                - In PyTorch the Embedding object, e.g. `self.pretrained_embeddings`, allows you to\n                    go from an index to embedding. Please see the documentation (https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding)\n                    to learn how to use `self.pretrained_embeddings` to extract the embeddings for your tensor `t`.\n\n            @param t (Tensor): input tensor of tokens (batch_size, n_features)\n\n            @return x (Tensor): tensor of embeddings for words represented in t\n                                (batch_size, n_features * embed_size)\n        """"""\n        ### YOUR CODE HERE (~1-3 Lines)\n        ### TODO:\n        ###     1) Use `self.pretrained_embeddings` to lookup the embeddings for the input tokens in `t`.\n        ###     2) After you apply the embedding lookup, you will have a tensor shape (batch_size, n_features, embedding_size).\n        ###         Use the tensor `view` method to reshape the embeddings tensor to (batch_size, n_features * embedding_size)\n        ###\n        ### Note: In order to get batch_size, you may need use the tensor .size() function:\n        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size\n        ###\n        ###  Please see the following docs for support:\n        ###     Embedding Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n        ###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n        x = self.pretrained_embeddings(t)\n        x = x.view(x.size()[0], -1) # shape (batch_size, n_features * embedding_size)\n\n        ### END YOUR CODE\n        return x\n\n\n    def forward(self, t):\n        """""" Run the model forward.\n\n            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss\n\n            PyTorch Notes:\n                - Every nn.Module object (PyTorch model) has a `forward` function.\n                - When you apply your nn.Module to an input tensor `t` this function is applied to the tensor.\n                    For example, if you created an instance of your ParserModel and applied it to some `t` as follows,\n                    the `forward` function would called on `t` and the result would be stored in the `output` variable:\n                        model = ParserModel()\n                        output = model(t) # this calls the forward function\n                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n\n        @param t (Tensor): input tensor of tokens (batch_size, n_features)\n\n        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)\n                                 without applying softmax (batch_size, n_classes)\n        """"""\n        ###  YOUR CODE HERE (~3-5 lines)\n        ### TODO:\n        ###     1) Apply `self.embedding_lookup` to `t` to get the embeddings\n        ###     2) Apply `embed_to_hidden` linear layer to the embeddings\n        ###     3) Apply relu non-linearity to the output of step 2 to get the hidden units.\n        ###     4) Apply dropout layer to the output of step 3.\n        ###     5) Apply `hidden_to_logits` layer to the output of step 4 to get the logits.\n        ###\n        ### Note: We do not apply the softmax to the logits here, because\n        ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.\n        ###\n        ### Please see the following docs for support:\n        ###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu\n\n        x = self.embedding_lookup(t)\n        x = self.embed_to_hidden(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        logits = self.hidden_to_logits(x)\n\n        ### END YOUR CODE\n        return logits\n'"
Assignment/a3/parser_transitions.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCS224N 2018-19: Homework 3\nparser_transitions.py: Algorithms for completing partial parsess.\nSahil Chopra <schopra8@stanford.edu>\n""""""\n\nimport sys\n\nclass PartialParse(object):\n    def __init__(self, sentence):\n        """"""Initializes this partial parse.\n\n        @param sentence (list of str): The sentence to be parsed as a list of words.\n                                        Your code should not modify the sentence.\n        """"""\n        # The sentence being parsed is kept for bookkeeping purposes. Do not alter it in your code.\n        self.sentence = sentence\n\n        ### YOUR CODE HERE (3 Lines)\n        ### Your code should initialize the following fields:\n        ###     self.stack: The current stack represented as a list with the top of the stack as the\n        ###                 last element of the list.\n        ###     self.buffer: The current buffer represented as a list with the first item on the\n        ###                  buffer as the first item of the list\n        ###     self.dependencies: The list of dependencies produced so far. Represented as a list of\n        ###             tuples where each tuple is of the form (head, dependent).\n        ###             Order for this list doesn\'t matter.\n        ###\n        ### Note: The root token should be represented with the string ""ROOT""\n        ###\n        self.stack = [""ROOT""]\n        self.buffer = list(sentence)\n        self.dependencies = []\n\n        ### END YOUR CODE\n\n\n    def parse_step(self, transition):\n        """"""Performs a single parse step by applying the given transition to this partial parse\n\n        @param transition (str): A string that equals ""S"", ""LA"", or ""RA"" representing the shift,\n                                left-arc, and right-arc transitions. You can assume the provided\n                                transition is a legal transition.\n        """"""\n        ### YOUR CODE HERE (~7-10 Lines)\n        ### TODO:\n        ###     Implement a single parsing step, i.e. the logic for the following as\n        ###     described in the pdf handout:\n        ###         1. Shift\n        ###         2. Left Arc\n        ###         3. Right Arc\n\n        # Assume proved transition is legal transition\n        if transition == \'S\' and len(self.buffer) > 0:\n            word = self.buffer.pop(0)\n            self.stack.append(word)\n        if transition == \'LA\' :\n            head = self.stack[-1]\n            dependent = self.stack.pop(-2)\n            self.dependencies.append((head, dependent))\n        if transition == \'RA\':\n            head = self.stack[-2]\n            dependent = self.stack.pop()\n            self.dependencies.append((head, dependent))\n        ### END YOUR CODE\n\n    def parse(self, transitions):\n        """"""Applies the provided transitions to this PartialParse\n\n        @param transitions (list of str): The list of transitions in the order they should be applied\n\n        @return dsependencies (list of string tuples): The list of dependencies produced when\n                                                        parsing the sentence. Represented as a list of\n                                                        tuples where each tuple is of the form (head, dependent).\n        """"""\n        for transition in transitions:\n            self.parse_step(transition)\n        return self.dependencies\n\n\ndef minibatch_parse(sentences, model, batch_size):\n    """"""Parses a list of sentences in minibatches using a model.\n\n    @param sentences (list of list of str): A list of sentences to be parsed\n                                            (each sentence is a list of words and each word is of type string)\n    @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function\n                                model.predict(partial_parses) that takes in a list of PartialParses as input and\n                                returns a list of transitions predicted for each parse. That is, after calling\n                                    transitions = model.predict(partial_parses)\n                                transitions[i] will be the next transition to apply to partial_parses[i].\n    @param batch_size (int): The number of PartialParses to include in each minibatch\n\n\n    @return dependencies (list of dependency lists): A list where each element is the dependencies\n                                                    list for a parsed sentence. Ordering should be the\n                                                    same as in sentences (i.e., dependencies[i] should\n                                                    contain the parse for sentences[i]).\n    """"""\n    dependencies = []\n\n    ### YOUR CODE HERE (~8-10 Lines)\n    ### TODO:\n    ###     Implement the minibatch parse algorithm as described in the pdf handout\n    ###\n    ###     Note: A shallow copy (as denoted in the PDF) can be made with the ""="" sign in python, e.g.\n    ###                 unfinished_parses = partial_parses[:].\n    ###             Here `unfinished_parses` is a shallow copy of `partial_parses`.\n    ###             In Python, a shallow copied list like `unfinished_parses` does not contain new instances\n    ###             of the object stored in `partial_parses`. Rather both lists refer to the same objects.\n    ###             In our case, `partial_parses` contains a list of partial parses. `unfinished_parses`\n    ###             contains references to the same objects. Thus, you should NOT use the `del` operator\n    ###             to remove objects from the `unfinished_parses` list. This will free the underlying memory that\n    ###             is being accessed by `partial_parses` and may cause your code to crash.\n\n    num_sentences     = len(sentences)\n    partial_parses    = [PartialParse(sentence) for sentence in sentences]\n    unfinished_parses = partial_parses[:] # shallow copy of pp\n\n    while len(unfinished_parses) > 0:\n        parsers = unfinished_parses[:batch_size]\n        batch_transitions = model.predict(parsers)\n        for pp, transition in zip(parsers, batch_transitions):\n            pp.parse([transition])\n            if len(pp.buffer) == 0 and len(pp.stack) == 1:\n                unfinished_parses.remove(pp)\n\n    ### END YOUR CODE\n    dependencies = [pp.dependencies for pp in partial_parses]\n    return dependencies\n\n\ndef test_step(name, transition, stack, buf, deps,\n              ex_stack, ex_buf, ex_deps):\n    """"""Tests that a single parse step returns the expected output""""""\n    pp = PartialParse([])\n    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n\n    pp.parse_step(transition)\n    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n    assert stack == ex_stack, \\\n        ""{:} test resulted in stack {:}, expected {:}"".format(name, stack, ex_stack)\n    assert buf == ex_buf, \\\n        ""{:} test resulted in buffer {:}, expected {:}"".format(name, buf, ex_buf)\n    assert deps == ex_deps, \\\n        ""{:} test resulted in dependency list {:}, expected {:}"".format(name, deps, ex_deps)\n    print(""{:} test passed!"".format(name))\n\n\ndef test_parse_step():\n    """"""Simple tests for the PartialParse.parse_step function\n    Warning: these are not exhaustive\n    """"""\n    test_step(""SHIFT"", ""S"", [""ROOT"", ""the""], [""cat"", ""sat""], [],\n              (""ROOT"", ""the"", ""cat""), (""sat"",), ())\n    test_step(""LEFT-ARC"", ""LA"", [""ROOT"", ""the"", ""cat""], [""sat""], [],\n              (""ROOT"", ""cat"",), (""sat"",), ((""cat"", ""the""),))\n    test_step(""RIGHT-ARC"", ""RA"", [""ROOT"", ""run"", ""fast""], [], [],\n              (""ROOT"", ""run"",), (), ((""run"", ""fast""),))\n\n\ndef test_parse():\n    """"""Simple tests for the PartialParse.parse function\n    Warning: these are not exhaustive\n    """"""\n    sentence = [""parse"", ""this"", ""sentence""]\n    dependencies = PartialParse(sentence).parse([""S"", ""S"", ""S"", ""LA"", ""RA"", ""RA""])\n    dependencies = tuple(sorted(dependencies))\n    expected = ((\'ROOT\', \'parse\'), (\'parse\', \'sentence\'), (\'sentence\', \'this\'))\n    assert dependencies == expected,  \\\n        ""parse test resulted in dependencies {:}, expected {:}"".format(dependencies, expected)\n    assert tuple(sentence) == (""parse"", ""this"", ""sentence""), \\\n        ""parse test failed: the input sentence should not be modified""\n    print(""parse test passed!"")\n\n\nclass DummyModel(object):\n    """"""Dummy model for testing the minibatch_parse function\n    First shifts everything onto the stack and then does exclusively right arcs if the first word of\n    the sentence is ""right"", ""left"" if otherwise.\n    """"""\n    def predict(self, partial_parses):\n        return [(""RA"" if pp.stack[1] is ""right"" else ""LA"") if len(pp.buffer) == 0 else ""S""\n                for pp in partial_parses]\n\n\ndef test_dependencies(name, deps, ex_deps):\n    """"""Tests the provided dependencies match the expected dependencies""""""\n    deps = tuple(sorted(deps))\n    assert deps == ex_deps, \\\n        ""{:} test resulted in dependency list {:}, expected {:}"".format(name, deps, ex_deps)\n\n\ndef test_minibatch_parse():\n    """"""Simple tests for the minibatch_parse function\n    Warning: these are not exhaustive\n    """"""\n    sentences = [[""right"", ""arcs"", ""only""],\n                 [""right"", ""arcs"", ""only"", ""again""],\n                 [""left"", ""arcs"", ""only""],\n                 [""left"", ""arcs"", ""only"", ""again""]]\n    deps = minibatch_parse(sentences, DummyModel(), 2)\n    test_dependencies(""minibatch_parse"", deps[0],\n                      ((\'ROOT\', \'right\'), (\'arcs\', \'only\'), (\'right\', \'arcs\')))\n    test_dependencies(""minibatch_parse"", deps[1],\n                      ((\'ROOT\', \'right\'), (\'arcs\', \'only\'), (\'only\', \'again\'), (\'right\', \'arcs\')))\n    test_dependencies(""minibatch_parse"", deps[2],\n                      ((\'only\', \'ROOT\'), (\'only\', \'arcs\'), (\'only\', \'left\')))\n    test_dependencies(""minibatch_parse"", deps[3],\n                      ((\'again\', \'ROOT\'), (\'again\', \'arcs\'), (\'again\', \'left\'), (\'again\', \'only\')))\n    print(""minibatch_parse test passed!"")\n\n\nif __name__ == \'__main__\':\n    args = sys.argv\n    if len(args) != 2:\n        raise Exception(""You did not provide a valid keyword. Either provide \'part_c\' or \'part_d\', when executing this script"")\n    elif args[1] == ""part_c"":\n        test_parse_step()\n        test_parse()\n    elif args[1] == ""part_d"":\n        test_minibatch_parse()\n    else:\n        raise Exception(""You did not provide a valid keyword. Either provide \'part_c\' or \'part_d\', when executing this script"")\n'"
Assignment/a3/run.py,9,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCS224N 2018-19: Homework 3\nrun.py: Run the dependency parser.\nSahil Chopra <schopra8@stanford.edu>\n""""""\nfrom datetime import datetime\nimport os\nimport pickle\nimport math\nimport time\n\nfrom torch import nn, optim\nimport torch\nfrom tqdm import tqdm\n\nfrom parser_model import ParserModel\nfrom utils.parser_utils import minibatches, load_and_preprocess_data, AverageMeter\n\n# -----------------\n# Primary Functions\n# -----------------\ndef train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n    """""" Train the neural dependency parser.\n\n    @param parser (Parser): Neural Dependency Parser\n    @param train_data ():\n    @param dev_data ():\n    @param output_path (str): Path to which model weights and results are written.\n    @param batch_size (int): Number of examples in a single batch\n    @param n_epochs (int): Number of training epochs\n    @param lr (float): Learning rate\n    """"""\n    best_dev_UAS = 0\n\n\n    ### YOUR CODE HERE (~2-7 lines)\n    ### TODO:\n    ###      1) Construct Adam Optimizer in variable `optimizer`\n    ###      2) Construct the Cross Entropy Loss Function in variable `loss_func`\n    ###\n    ### Hint: Use `parser.model.parameters()` to pass optimizer\n    ###       necessary parameters to tune.\n    ### Please see the following docs for support:\n    ###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html\n    ###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n    optimizer = optim.Adam(model.parameters())\n    loss_func = torch.nn.CrossEntropyLoss()\n\n    ### END YOUR CODE\n\n    for epoch in range(n_epochs):\n        print(""Epoch {:} out of {:}"".format(epoch + 1, n_epochs))\n        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)\n        if dev_UAS > best_dev_UAS:\n            best_dev_UAS = dev_UAS\n            print(""New best dev UAS! Saving model."")\n            torch.save(parser.model.state_dict(), output_path)\n        print("""")\n\n\ndef train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n    """""" Train the neural dependency parser for single epoch.\n\n    Note: In PyTorch we can signify train versus test and automatically have\n    the Dropout Layer applied and removed, accordingly, by specifying\n    whether we are training, `model.train()`, or evaluating, `model.eval()`\n\n    @param parser (Parser): Neural Dependency Parser\n    @param train_data ():\n    @param dev_data ():\n    @param optimizer (nn.Optimizer): Adam Optimizer\n    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function\n    @param batch_size (int): batch size\n    @param lr (float): learning rate\n\n    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data\n    """"""\n    parser.model.train() # Places model in ""train"" mode, i.e. apply dropout layer\n    n_minibatches = math.ceil(len(train_data) / batch_size)\n    loss_meter = AverageMeter()\n\n    with tqdm(total=(n_minibatches)) as prog:\n        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n            optimizer.zero_grad()   # remove any baggage in the optimizer\n            loss = 0. # store loss for this batch here\n            train_x = torch.from_numpy(train_x).long()\n            train_y = torch.from_numpy(train_y.nonzero()[1]).long()\n\n            ### YOUR CODE HERE (~5-10 lines)\n            ### TODO:\n            ###      1) Run train_x forward through model to produce `logits`\n            ###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.\n            ###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss\n            ###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)\n            ###         are the predictions (y^ from the PDF).\n            ###      3) Backprop losses\n            ###      4) Take step with the optimizer\n            ### Please see the following docs for support:\n            ###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step\n            logits = model(train_x)\n\n            optimizer.zero_grad()\n            loss = loss_func(logits, train_y)\n            loss.backward()\n            optimizer.step()\n\n            ### END YOUR CODE\n            prog.update(1)\n            loss_meter.update(loss.item())\n\n    print (""Average Train Loss: {}"".format(loss_meter.avg))\n\n    print(""Evaluating on dev set"",)\n    parser.model.eval() # Places model in ""eval"" mode, i.e. don\'t apply dropout layer\n    dev_UAS, _ = parser.parse(dev_data)\n    print(""- dev UAS: {:.2f}"".format(dev_UAS * 100.0))\n    return dev_UAS\n\n\nif __name__ == ""__main__"":\n    # Note: Set debug to False, when training on entire corpus\n    # debug = True\n    debug = False\n\n    # assert(torch.__version__ == ""1.0.0""),  ""Please install torch version 1.0.0""\n\n    print(80 * ""="")\n    print(""INITIALIZING"")\n    print(80 * ""="")\n    parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(debug)\n\n    start = time.time()\n    model = ParserModel(embeddings)\n    parser.model = model\n    print(""took {:.2f} seconds\\n"".format(time.time() - start))\n\n    print(80 * ""="")\n    print(""TRAINING"")\n    print(80 * ""="")\n    output_dir = ""results/{:%Y%m%d_%H%M%S}/"".format(datetime.now())\n    output_path = output_dir + ""model.weights""\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005)\n\n    if not debug:\n        print(80 * ""="")\n        print(""TESTING"")\n        print(80 * ""="")\n        print(""Restoring the best model weights found on the dev set"")\n        parser.model.load_state_dict(torch.load(output_path))\n        print(""Final evaluation on test set"",)\n        parser.model.eval()\n        UAS, dependencies = parser.parse(test_data)\n        print(""- test UAS: {:.2f}"".format(UAS * 100.0))\n        print(""Done!"")\n'"
Assignment/a4/__init__.py,0,b''
Assignment/a4/model_embeddings.py,2,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCS224N 2018-19: Homework 4\nmodel_embeddings.py: Embeddings for the NMT model\nPencheng Yin <pcyin@cs.cmu.edu>\nSahil Chopra <schopra8@stanford.edu>\nAnand Dhoot <anandd@stanford.edu>\n""""""\n\nimport torch.nn as nn\n\nclass ModelEmbeddings(nn.Module): \n    """"""\n    Class that converts input words to their embeddings.\n    """"""\n    def __init__(self, embed_size, vocab):\n        """"""\n        Init the Embedding layers.\n\n        @param embed_size (int): Embedding size (dimensionality)\n        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n                              See vocab.py for documentation.\n        """"""\n        super(ModelEmbeddings, self).__init__()\n        self.embed_size = embed_size\n\n        # default values\n        self.source = None\n        self.target = None\n\n        src_pad_token_idx = vocab.src[\'<pad>\']\n        tgt_pad_token_idx = vocab.tgt[\'<pad>\']\n\n        ### YOUR CODE HERE (~2 Lines)\n        ### TODO - Initialize the following variables:\n        ###     self.source (Embedding Layer for source language)\n        ###     self.target (Embedding Layer for target langauge)\n        ###\n        ### Note:\n        ###     1. `vocab` object contains two vocabularies:\n        ###            `vocab.src` for source\n        ###            `vocab.tgt` for target\n        ###     2. You can get the length of a specific vocabulary by running:\n        ###             `len(vocab.<specific_vocabulary>)`\n        ###     3. Remember to include the padding token for the specific vocabulary\n        ###        when creating your Embedding.\n        ###\n        ### Use the following docs to properly initialize these variables:\n        ###     Embedding Layer:\n        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n\n        self.source = nn.Embedding(len(vocab.src), embed_size, padding_idx=src_pad_token_idx)\n        self.target = nn.Embedding(len(vocab.tgt), embed_size, padding_idx=tgt_pad_token_idx)\n\n        ### END YOUR CODE\n\n\n'"
Assignment/a4/nmt_model.py,60,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCS224N 2018-19: Homework 4\nnmt_model.py: NMT Model\nPencheng Yin <pcyin@cs.cmu.edu>\nSahil Chopra <schopra8@stanford.edu>\n""""""\nfrom collections import namedtuple\nimport sys\nfrom typing import List, Tuple, Dict, Set, Union\nimport torch\nimport torch.nn as nn\nimport torch.nn.utils\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n\nfrom model_embeddings import ModelEmbeddings\nHypothesis = namedtuple(\'Hypothesis\', [\'value\', \'score\'])\n\n\nclass NMT(nn.Module):\n    """""" Simple Neural Machine Translation Model:\n        - Bidrectional LSTM Encoder\n        - Unidirection LSTM Decoder\n        - Global Attention Model (Luong, et al. 2015)\n    """"""\n    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2):\n        """""" Init NMT Model.\n\n        @param embed_size (int): Embedding size (dimensionality)\n        @param hidden_size (int): Hidden Size (dimensionality)\n        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n                              See vocab.py for documentation.\n        @param dropout_rate (float): Dropout probability, for attention\n        """"""\n        super(NMT, self).__init__()\n        self.model_embeddings = ModelEmbeddings(embed_size, vocab)\n        self.hidden_size = hidden_size\n        self.dropout_rate = dropout_rate\n        self.vocab = vocab\n\n        # default values\n        self.encoder = None \n        self.decoder = None\n        self.h_projection = None\n        self.c_projection = None\n        self.att_projection = None\n        self.combined_output_projection = None\n        self.target_vocab_projection = None\n        self.dropout = None\n\n\n        ### YOUR CODE HERE (~8 Lines)\n        ### TODO - Initialize the following variables:\n        ###     self.encoder (Bidirectional LSTM with bias)\n        ###     self.decoder (LSTM Cell with bias)\n        ###     self.h_projection (Linear Layer with no bias), called W_{h} in the PDF.\n        ###     self.c_projection (Linear Layer with no bias), called W_{c} in the PDF.\n        ###     self.att_projection (Linear Layer with no bias), called W_{attProj} in the PDF.\n        ###     self.combined_output_projection (Linear Layer with no bias), called W_{u} in the PDF.\n        ###     self.target_vocab_projection (Linear Layer with no bias), called W_{vocab} in the PDF.\n        ###     self.dropout (Dropout Layer)\n        ###\n        ### Use the following docs to properly initialize these variables:\n        ###     LSTM:\n        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n        ###     LSTM Cell:\n        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell\n        ###     Linear Layer:\n        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n        ###     Dropout Layer:\n        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n\n        self.encoder = nn.LSTM(embed_size, hidden_size, bias=True, bidirectional=True)\n        self.decoder = nn.LSTMCell(embed_size + hidden_size, hidden_size, bias=True)\n        self.h_projection = nn.Linear(hidden_size * 2, hidden_size, bias=False) # prj output of last h_state of encode (R^2h) to R^h\n        self.c_projection = nn.Linear(hidden_size * 2, hidden_size, bias=False)\n        self.att_projection = nn.Linear(hidden_size * 2, hidden_size, bias=False) # 1 x 2h (h_encode_i) * 2h x h (W) * h * 1 (h_decode_t) = 1 x 1 = e_t,i\n        self.combined_output_projection = nn.Linear(hidden_size * 3, hidden_size, bias=False) # use after combined attention output and h_decode\n        self.target_vocab_projection    = nn.Linear(hidden_size, len(vocab.tgt), bias=False) # for softmax of last\n        self.dropout = nn.Dropout(self.dropout_rate)\n        ### END YOUR CODE\n\n\n    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:\n        """""" Take a mini-batch of source and target sentences, compute the log-likelihood of\n        target sentences under the language models learned by the NMT system.\n\n        @param source (List[List[str]]): list of source sentence tokens\n        @param target (List[List[str]]): list of target sentence tokens, wrapped by `<s>` and `</s>`\n\n        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the\n                                    log-likelihood of generating the gold-standard target sentence for\n                                    each example in the input batch. Here b = batch size.\n        """"""\n        # Compute sentence lengths\n        source_lengths = [len(s) for s in source]\n\n        # Convert list of lists into tensors\n        source_padded = self.vocab.src.to_input_tensor(source, device=self.device)   # Tensor: (src_len, b)\n        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)   # Tensor: (tgt_len, b)\n\n        ###     Run the network forward:\n        ###     1. Apply the encoder to `source_padded` by calling `self.encode()`\n        ###     2. Generate sentence masks for `source_padded` by calling `self.generate_sent_masks()`\n        ###     3. Apply the decoder to compute combined-output by calling `self.decode()`\n        ###     4. Compute log probability distribution over the target vocabulary using the\n        ###        combined_outputs returned by the `self.decode()` function.\n\n        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)\n        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n        combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n        P = F.log_softmax(self.target_vocab_projection(combined_outputs), dim=-1)\n\n        # Zero out, probabilities for which we have nothing in the target text\n        target_masks = (target_padded != self.vocab.tgt[\'<pad>\']).float()\n        \n        # Compute log probability of generating true target words\n        target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[1:]\n        scores = target_gold_words_log_prob.sum(dim=0)\n        return scores\n\n\n    def encode(self, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        """""" Apply the encoder to source sentences to obtain encoder hidden states.\n            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n\n        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where\n                                        b = batch_size, src_len = maximum source sentence length. Note that \n                                       these have already been sorted in order of longest to shortest sentence.\n        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch\n        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where\n                                        b = batch size, src_len = maximum source sentence length, h = hidden size.\n        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder\'s initial\n                                                hidden state and cell.\n        """"""\n        enc_hiddens, dec_init_state = None, None\n\n        ### YOUR CODE HERE (~ 8 Lines)\n        ### TODO:\n        ###     1. Construct Tensor `X` of source sentences with shape (src_len, b, e) using the source model embeddings.\n        ###         src_len = maximum source sentence length, b = batch size, e = embedding size. Note\n        ###         that there is no initial hidden state or cell for the decoder.\n        ###     2. Compute `enc_hiddens`, `last_hidden`, `last_cell` by applying the encoder to `X`.\n        ###         - Before you can apply the encoder, you need to apply the `pack_padded_sequence` function to X.\n        ###         - After you apply the encoder, you need to apply the `pad_packed_sequence` function to enc_hiddens.\n        ###         - Note that the shape of the tensor returned by the encoder is (src_len b, h*2) and we want to\n        ###           return a tensor of shape (b, src_len, h*2) as `enc_hiddens`.\n        ###     3. Compute `dec_init_state` = (init_decoder_hidden, init_decoder_cell):\n        ###         - `init_decoder_hidden`:\n        ###             `last_hidden` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.\n        ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).\n        ###             Apply the h_projection layer to this in order to compute init_decoder_hidden.\n        ###             This is h_0^{dec} in the PDF. Here b = batch size, h = hidden size\n        ###         - `init_decoder_cell`:\n        ###             `last_cell` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.\n        ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).\n        ###             Apply the c_projection layer to this in order to compute init_decoder_cell.\n        ###             This is c_0^{dec} in the PDF. Here b = batch size, h = hidden size\n        ###\n        ### See the following docs, as you may need to use some of the following functions in your implementation:\n        ###     Pack the padded sequence X before passing to the encoder:\n        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence\n        ###     Pad the packed sequence, enc_hiddens, returned by the encoder:\n        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_packed_sequence\n        ###     Tensor Concatenation:\n        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n        ###     Tensor Permute:\n        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute\n        X = self.model_embeddings.source(source_padded) # (src_len, b, e)\n        X = pack_padded_sequence(X, lengths=source_lengths) # if feed pack to RNN, it will not calculate output for pad element\n        # pack_padded_sequence and pad_packed_sequence example:\n        # https://github.com/HarshTrivedi/packing-unpacking-pytorch-minimal-tutorial\n        # PackedSequence: Named Tuple with 2 attribute data & batch_size\n        # data: shape (batch_sum_len x embed_dim)\n        # batch_size: each columns when feed to lstm (max = batch_size (start word of all sentence), min = 1 (only one word in this column))\n\n        # After feed PackedSequence to LSTM, return PackedSequence with the same attributes : data & batch_size\n        enc_hiddens, (last_hidden, last_cell) = self.encoder(X)\n        # pad_packed_sequence will unpack PackedSequence, which transform (data & batch_size) -> (max_len, b, h * 2)\n        # padded indice will be 0s\n        enc_hiddens, _ = pad_packed_sequence(enc_hiddens)\n        enc_hiddens = enc_hiddens.transpose(0, 1) # (b, max_len, h* 2)\n\n        last_hidden = torch.cat((last_hidden[0], last_hidden[1]), 1)# (2, b, h) -> (b, h * 2)\n        init_decoder_hidden = self.h_projection(last_hidden)\n\n        last_cell   = torch.cat((last_cell[0], last_cell[1]), 1)\n        init_decoder_cell   = self.c_projection(last_cell)\n\n        dec_init_state = (init_decoder_hidden, init_decoder_cell)\n        ### END YOUR CODE\n\n        return enc_hiddens, dec_init_state\n\n\n    def decode(self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,\n                dec_init_state: Tuple[torch.Tensor, torch.Tensor], target_padded: torch.Tensor) -> torch.Tensor:\n        """"""Compute combined output vectors for a batch.\n\n        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where\n                                     b = batch size, src_len = maximum source sentence length, h = hidden size.\n        @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where\n                                     b = batch size, src_len = maximum source sentence length.\n        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder\n        @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b), where\n                                       tgt_len = maximum target sentence length, b = batch size. \n\n        @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where\n                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size\n        """"""\n        # Chop of the <END> token for max length sentences.\n        target_padded = target_padded[:-1]\n\n        # Initialize the decoder state (hidden and cell)\n        dec_state = dec_init_state\n\n        # Initialize previous combined output vector o_{t-1} as zero\n        batch_size = enc_hiddens.size(0)\n        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)\n\n        # Initialize a list we will use to collect the combined output o_t on each step\n        combined_outputs = []\n\n        ### YOUR CODE HERE (~9 Lines)\n        ### TODO:\n        ###     1. Apply the attention projection layer to `enc_hiddens` to obtain `enc_hiddens_proj`,\n        ###         which should be shape (b, src_len, h),\n        ###         where b = batch size, src_len = maximum source length, h = hidden size.\n        ###         This is applying W_{attProj} to h^enc, as described in the PDF.\n        ###     2. Construct tensor `Y` of target sentences with shape (tgt_len, b, e) using the target model embeddings.\n        ###         where tgt_len = maximum target sentence length, b = batch size, e = embedding size.\n        ###     3. Use the torch.split function to iterate over the time dimension of Y.\n        ###         Within the loop, this will give you Y_t of shape (1, b, e) where b = batch size, e = embedding size.\n        ###             - Squeeze Y_t into a tensor of dimension (b, e). \n        ###             - Construct Ybar_t by concatenating Y_t with o_prev.\n        ###             - Use the step function to compute the the Decoder\'s next (cell, state) values\n        ###               as well as the new combined output o_t.\n        ###             - Append o_t to combined_outputs\n        ###             - Update o_prev to the new o_t.\n        ###     4. Use torch.stack to convert combined_outputs from a list length tgt_len of\n        ###         tensors shape (b, h), to a single tensor shape (tgt_len, b, h)\n        ###         where tgt_len = maximum target sentence length, b = batch size, h = hidden size.\n        ###\n        ### Note:\n        ###    - When using the squeeze() function make sure to specify the dimension you want to squeeze\n        ###      over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.\n        ###   \n        ### Use the following docs to implement this functionality:\n        ###     Zeros Tensor:\n        ###         https://pytorch.org/docs/stable/torch.html#torch.zeros\n        ###     Tensor Splitting (iteration):\n        ###         https://pytorch.org/docs/stable/torch.html#torch.split\n        ###     Tensor Dimension Squeezing:\n        ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze\n        ###     Tensor Concatenation:\n        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n        ###     Tensor Stacking:\n        ###         https://pytorch.org/docs/stable/torch.html#torch.stack\n\n        # 1,\n        enc_hiddens_proj = self.att_projection(enc_hiddens) # enc_hiddens: (b, l, h * 2)  dot (h * 2, h) -> b, l, h\n        # 2,\n        Y = self.model_embeddings.target(target_padded) # (tgt_len, b, h)\n        # 3,\n        for Y_t in torch.split(Y, 1, dim=0):\n            squeezed = torch.squeeze(Y_t) # shape (b, e)\n            Ybar_t = torch.cat((squeezed, o_prev), dim=1) # shape (b, e + h)\n            dec_state, o_t, _ = self.step(Ybar_t, dec_state, enc_hiddens, enc_hiddens_proj, enc_masks)\n            combined_outputs.append(o_t)\n            o_prev = o_t\n        # 4,\n        combined_outputs = torch.stack(combined_outputs, dim=0)\n        ### END YOUR CODE\n\n        return combined_outputs\n\n\n    def step(self, Ybar_t: torch.Tensor,\n            dec_state: Tuple[torch.Tensor, torch.Tensor],\n            enc_hiddens: torch.Tensor,\n            enc_hiddens_proj: torch.Tensor,\n            enc_masks: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n        """""" Compute one forward step of the LSTM decoder, including the attention computation.\n\n        @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,\n                                where b = batch size, e = embedding size, h = hidden size.\n        @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.\n                First tensor is decoder\'s prev hidden state, second tensor is decoder\'s prev cell.\n        @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,\n                                    src_len = maximum source length, h = hidden size.\n        @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),\n                                    where b = batch size, src_len = maximum source length, h = hidden size.\n        @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),\n                                    where b = batch size, src_len is maximum source length. \n\n        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.\n                First tensor is decoder\'s new hidden state, second tensor is decoder\'s new cell.\n        @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.\n        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.\n                                Note: You will not use this outside of this function.\n                                      We are simply returning this value so that we can sanity check\n                                      your implementation.\n        """"""\n\n        combined_output = None\n\n        ### YOUR CODE HERE (~3 Lines)\n        ### TODO:\n        ###     1. Apply the decoder to `Ybar_t` and `dec_state`to obtain the new dec_state.\n        ###     2. Split dec_state into its two parts (dec_hidden, dec_cell)\n        ###     3. Compute the attention scores e_t, a Tensor shape (b, src_len). \n        ###        Note: b = batch_size, src_len = maximum source length, h = hidden size.\n        ###\n        ###       Hints:\n        ###         - dec_hidden is shape (b, h) and corresponds to h^dec_t in the PDF (batched)\n        ###         - enc_hiddens_proj is shape (b, src_len, h) and corresponds to W_{attProj} h^enc (batched).\n        ###         - Use batched matrix multiplication (torch.bmm) to compute e_t.\n        ###         - To get the tensors into the right shapes for bmm, you will need to do some squeezing and unsqueezing.\n        ###         - When using the squeeze() function make sure to specify the dimension you want to squeeze\n        ###             over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.\n        ###\n        ### Use the following docs to implement this functionality:\n        ###     Batch Multiplication:\n        ###        https://pytorch.org/docs/stable/torch.html#torch.bmm\n        ###     Tensor Unsqueeze:\n        ###         https://pytorch.org/docs/stable/torch.html#torch.unsqueeze\n        ###     Tensor Squeeze:\n        ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze\n\n        # 1,\n        dec_state = self.decoder(Ybar_t, dec_state)\n        (dec_hidden, dec_cell) = dec_state\n        # 3, (b, src_len, h) .dot(b, h, 1) -> (b, src_len, 1) -> (b, src_len)\n        e_t = enc_hiddens_proj.bmm(dec_hidden.unsqueeze(2)).squeeze(2)\n        ### END YOUR CODE\n\n        # Set e_t to -inf where enc_masks has 1\n        if enc_masks is not None:\n            e_t.data.masked_fill_(enc_masks.byte(), -float(\'inf\')) # mask the 0s with -inf, so e^x = 0\n\n        ### YOUR CODE HERE (~6 Lines)\n        ### TODO:\n        ###     1. Apply softmax to e_t to yield alpha_t\n        ###     2. Use batched matrix multiplication between alpha_t and enc_hiddens to obtain the\n        ###         attention output vector, a_t.\n        #$$     Hints:\n        ###           - alpha_t is shape (b, src_len)\n        ###           - enc_hiddens is shape (b, src_len, 2h)\n        ###           - a_t should be shape (b, 2h)\n        ###           - You will need to do some squeezing and unsqueezing.\n        ###     Note: b = batch size, src_len = maximum source length, h = hidden size.\n        ###\n        ###     3. Concatenate dec_hidden with a_t to compute tensor U_t\n        ###     4. Apply the combined output projection layer to U_t to compute tensor V_t\n        ###     5. Compute tensor O_t by first applying the Tanh function and then the dropout layer.\n        ###\n        ### Use the following docs to implement this functionality:\n        ###     Softmax:\n        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.functional.softmax\n        ###     Batch Multiplication:\n        ###        https://pytorch.org/docs/stable/torch.html#torch.bmm\n        ###     Tensor View:\n        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n        ###     Tensor Concatenation:\n        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n        ###     Tanh:\n        ###         https://pytorch.org/docs/stable/torch.html#torch.tanh\n\n        # 1, apply softmax to e_t\n        alpha_t = F.softmax(e_t, dim=1) # (b, src_len)\n        # 2, (b, 1, src_len) x (b, src_len, 2h) = (b, 1, 2h) -> (b, 2h)\n        # a_t = e_t.unsqueeze(1).bmm(enc_hiddens).squeeze(1)\n        att_view = (alpha_t.size(0), 1, alpha_t.size(1))\n        a_t = torch.bmm(alpha_t.view(*att_view), enc_hiddens).squeeze(1)\n\n        # 3, concate a_t (b, 2h) and dec_hidden (b, h) to U_t (b, 3h)\n        U_t = torch.cat((a_t, dec_hidden), dim=1)\n        # 4, apply combined output to U_T -> V_t, shape (b, h)\n        V_t = self.combined_output_projection(U_t)\n        O_t = self.dropout(torch.tanh(V_t))\n\n        ### END YOUR CODE\n\n        combined_output = O_t\n        return dec_state, combined_output, e_t\n\n    def generate_sent_masks(self, enc_hiddens: torch.Tensor, source_lengths: List[int]) -> torch.Tensor:\n        """""" Generate sentence masks for encoder hidden states.\n\n        @param enc_hiddens (Tensor): encodings of shape (b, src_len, 2*h), where b = batch size,\n                                     src_len = max source length, h = hidden size. \n        @param source_lengths (List[int]): List of actual lengths for each of the sentences in the batch.\n        \n        @returns enc_masks (Tensor): Tensor of sentence masks of shape (b, src_len),\n                                    where src_len = max source length, h = hidden size.\n        """"""\n        enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float)\n        for e_id, src_len in enumerate(source_lengths):\n            enc_masks[e_id, src_len:] = 1\n        return enc_masks.to(self.device)\n\n\n    def beam_search(self, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:\n        """""" Given a single source sentence, perform beam search, yielding translations in the target language.\n        @param src_sent (List[str]): a single source sentence (words)\n        @param beam_size (int): beam size\n        @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n        @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n                value: List[str]: the decoded target sentence, represented as a list of words\n                score: float: the log-likelihood of the target sentence\n        """"""\n        src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device)\n\n        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n        src_encodings_att_linear = self.att_projection(src_encodings)\n\n        h_tm1 = dec_init_vec\n        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n\n        eos_id = self.vocab.tgt[\'</s>\']\n\n        hypotheses = [[\'<s>\']]\n        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n        completed_hypotheses = []\n\n        t = 0\n        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n            t += 1\n            hyp_num = len(hypotheses)\n\n            exp_src_encodings = src_encodings.expand(hyp_num,\n                                                     src_encodings.size(1),\n                                                     src_encodings.size(2))\n\n            exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,\n                                                                           src_encodings_att_linear.size(1),\n                                                                           src_encodings_att_linear.size(2))\n\n            y_tm1 = torch.tensor([self.vocab.tgt[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=self.device)\n            y_t_embed = self.model_embeddings.target(y_tm1)\n\n            x = torch.cat([y_t_embed, att_tm1], dim=-1)\n\n            (h_t, cell_t), att_t, _  = self.step(x, h_tm1,\n                                                      exp_src_encodings, exp_src_encodings_att_linear, enc_masks=None)\n\n            # log probabilities over target words\n            log_p_t = F.log_softmax(self.target_vocab_projection(att_t), dim=-1)\n\n            live_hyp_num = beam_size - len(completed_hypotheses)\n            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n\n            prev_hyp_ids = top_cand_hyp_pos / len(self.vocab.tgt)\n            hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n\n            new_hypotheses = []\n            live_hyp_ids = []\n            new_hyp_scores = []\n\n            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n                prev_hyp_id = prev_hyp_id.item()\n                hyp_word_id = hyp_word_id.item()\n                cand_new_hyp_score = cand_new_hyp_score.item()\n\n                hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n                if hyp_word == \'</s>\':\n                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n                                                           score=cand_new_hyp_score))\n                else:\n                    new_hypotheses.append(new_hyp_sent)\n                    live_hyp_ids.append(prev_hyp_id)\n                    new_hyp_scores.append(cand_new_hyp_score)\n\n            if len(completed_hypotheses) == beam_size:\n                break\n\n            live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n            h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n            att_tm1 = att_t[live_hyp_ids]\n\n            hypotheses = new_hypotheses\n            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n\n        if len(completed_hypotheses) == 0:\n            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n                                                   score=hyp_scores[0].item()))\n\n        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n\n        return completed_hypotheses\n\n    @property\n    def device(self) -> torch.device:\n        """""" Determine which device to place the Tensors upon, CPU or GPU.\n        """"""\n        return self.model_embeddings.source.weight.device\n\n    @staticmethod\n    def load(model_path: str):\n        """""" Load the model from a file.\n        @param model_path (str): path to model\n        """"""\n        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n        args = params[\'args\']\n        model = NMT(vocab=params[\'vocab\'], **args)\n        model.load_state_dict(params[\'state_dict\'])\n\n        return model\n\n    def save(self, path: str):\n        """""" Save the odel to a file.\n        @param path (str): path to the model\n        """"""\n        print(\'save model parameters to [%s]\' % path, file=sys.stderr)\n\n        params = {\n            \'args\': dict(embed_size=self.model_embeddings.embed_size, hidden_size=self.hidden_size, dropout_rate=self.dropout_rate),\n            \'vocab\': self.vocab,\n            \'state_dict\': self.state_dict()\n        }\n\n        torch.save(params, path)\n'"
Assignment/a4/run.py,14,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCS224N 2018-19: Homework 4\nrun.py: Run Script for Simple NMT Model\nPencheng Yin <pcyin@cs.cmu.edu>\nSahil Chopra <schopra8@stanford.edu>\n\nUsage:\n    run.py train --train-src=<file> --train-tgt=<file> --dev-src=<file> --dev-tgt=<file> --vocab=<file> [options]\n    run.py decode [options] MODEL_PATH TEST_SOURCE_FILE OUTPUT_FILE\n    run.py decode [options] MODEL_PATH TEST_SOURCE_FILE TEST_TARGET_FILE OUTPUT_FILE\n\nOptions:\n    -h --help                               show this screen.\n    --cuda                                  use GPU\n    --train-src=<file>                      train source file\n    --train-tgt=<file>                      train target file\n    --dev-src=<file>                        dev source file\n    --dev-tgt=<file>                        dev target file\n    --vocab=<file>                          vocab file\n    --seed=<int>                            seed [default: 0]\n    --batch-size=<int>                      batch size [default: 32]\n    --embed-size=<int>                      embedding size [default: 256]\n    --hidden-size=<int>                     hidden size [default: 256]\n    --clip-grad=<float>                     gradient clipping [default: 5.0]\n    --log-every=<int>                       log every [default: 10]\n    --max-epoch=<int>                       max epoch [default: 30]\n    --input-feed                            use input feeding\n    --patience=<int>                        wait for how many iterations to decay learning rate [default: 5]\n    --max-num-trial=<int>                   terminate training after how many trials [default: 5]\n    --lr-decay=<float>                      learning rate decay [default: 0.5]\n    --beam-size=<int>                       beam size [default: 5]\n    --sample-size=<int>                     sample size [default: 5]\n    --lr=<float>                            learning rate [default: 0.001]\n    --uniform-init=<float>                  uniformly initialize all parameters [default: 0.1]\n    --save-to=<file>                        model save path [default: model.bin]\n    --valid-niter=<int>                     perform validation after how many iterations [default: 2000]\n    --dropout=<float>                       dropout [default: 0.3]\n    --max-decoding-time-step=<int>          maximum number of decoding time steps [default: 70]\n""""""\nimport math\nimport sys\nimport pickle\nimport time\n\n\nfrom docopt import docopt\nfrom nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\nfrom nmt_model import Hypothesis, NMT\nimport numpy as np\nfrom typing import List, Tuple, Dict, Set, Union\nfrom tqdm import tqdm\nfrom utils import read_corpus, batch_iter\nfrom vocab import Vocab, VocabEntry\n\nimport torch\nimport torch.nn.utils\n\n\ndef evaluate_ppl(model, dev_data, batch_size=32):\n    """""" Evaluate perplexity on dev sentences\n    @param model (NMT): NMT Model\n    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n    @param batch_size (batch size)\n    @returns ppl (perplixty on dev sentences)\n    """"""\n    was_training = model.training\n    model.eval()\n\n    cum_loss = 0.\n    cum_tgt_words = 0.\n\n    # no_grad() signals backend to throw away all gradients\n    with torch.no_grad():\n        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n            loss = -model(src_sents, tgt_sents).sum()\n\n            cum_loss += loss.item()\n            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n            cum_tgt_words += tgt_word_num_to_predict\n\n        ppl = np.exp(cum_loss / cum_tgt_words)\n\n    if was_training:\n        model.train()\n\n    return ppl\n\n\ndef compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n    """""" Given decoding results and reference sentences, compute corpus-level BLEU score.\n    @param references (List[List[str]]): a list of gold-standard reference target sentences\n    @param hypotheses (List[Hypothesis]): a list of hypotheses, one for each reference\n    @returns bleu_score: corpus-level BLEU score\n    """"""\n    if references[0][0] == \'<s>\':\n        references = [ref[1:-1] for ref in references]\n    bleu_score = corpus_bleu([[ref] for ref in references],\n                             [hyp.value for hyp in hypotheses])\n    return bleu_score\n\n\ndef train(args: Dict):\n    """""" Train the NMT Model.\n    @param args (Dict): args from cmd line\n    """"""\n    train_data_src = read_corpus(args[\'--train-src\'], source=\'src\')\n    train_data_tgt = read_corpus(args[\'--train-tgt\'], source=\'tgt\')\n\n    dev_data_src = read_corpus(args[\'--dev-src\'], source=\'src\')\n    dev_data_tgt = read_corpus(args[\'--dev-tgt\'], source=\'tgt\')\n\n    train_data = list(zip(train_data_src, train_data_tgt))\n    dev_data = list(zip(dev_data_src, dev_data_tgt))\n\n    train_batch_size = int(args[\'--batch-size\'])\n    clip_grad = float(args[\'--clip-grad\'])\n    valid_niter = int(args[\'--valid-niter\'])\n    log_every = int(args[\'--log-every\'])\n    model_save_path = args[\'--save-to\']\n\n    vocab = Vocab.load(args[\'--vocab\'])\n\n    model = NMT(embed_size=int(args[\'--embed-size\']),\n                hidden_size=int(args[\'--hidden-size\']),\n                dropout_rate=float(args[\'--dropout\']),\n                vocab=vocab)\n    model.train()\n\n    uniform_init = float(args[\'--uniform-init\'])\n    if np.abs(uniform_init) > 0.:\n        print(\'uniformly initialize parameters [-%f, +%f]\' % (uniform_init, uniform_init), file=sys.stderr)\n        for p in model.parameters():\n            p.data.uniform_(-uniform_init, uniform_init)\n\n    vocab_mask = torch.ones(len(vocab.tgt))\n    vocab_mask[vocab.tgt[\'<pad>\']] = 0\n\n    device = torch.device(""cuda:0"" if args[\'--cuda\'] else ""cpu"")\n    print(\'use device: %s\' % device, file=sys.stderr)\n\n    model = model.to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=float(args[\'--lr\']))\n\n    num_trial = 0\n    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n    cum_examples = report_examples = epoch = valid_num = 0\n    hist_valid_scores = []\n    train_time = begin_time = time.time()\n    print(\'begin Maximum Likelihood training\')\n\n    while True:\n        epoch += 1\n\n        for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n            train_iter += 1\n\n            optimizer.zero_grad()\n\n            batch_size = len(src_sents)\n\n            example_losses = -model(src_sents, tgt_sents) # (batch_size,)\n            batch_loss = example_losses.sum()\n            loss = batch_loss / batch_size\n\n            loss.backward()\n\n            # clip gradient\n            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n\n            optimizer.step()\n\n            batch_losses_val = batch_loss.item()\n            report_loss += batch_losses_val\n            cum_loss += batch_losses_val\n\n            tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n            report_tgt_words += tgt_words_num_to_predict\n            cum_tgt_words += tgt_words_num_to_predict\n            report_examples += batch_size\n            cum_examples += batch_size\n\n            if train_iter % log_every == 0:\n                print(\'epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f \' \\\n                      \'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec\' % (epoch, train_iter,\n                                                                                         report_loss / report_examples,\n                                                                                         math.exp(report_loss / report_tgt_words),\n                                                                                         cum_examples,\n                                                                                         report_tgt_words / (time.time() - train_time),\n                                                                                         time.time() - begin_time), file=sys.stderr)\n\n                train_time = time.time()\n                report_loss = report_tgt_words = report_examples = 0.\n\n            # perform validation\n            if train_iter % valid_niter == 0:\n                print(\'epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d\' % (epoch, train_iter,\n                                                                                         cum_loss / cum_examples,\n                                                                                         np.exp(cum_loss / cum_tgt_words),\n                                                                                         cum_examples), file=sys.stderr)\n\n                cum_loss = cum_examples = cum_tgt_words = 0.\n                valid_num += 1\n\n                print(\'begin validation ...\', file=sys.stderr)\n\n                # compute dev. ppl and bleu\n                dev_ppl = evaluate_ppl(model, dev_data, batch_size=128)   # dev batch size can be a bit larger\n                valid_metric = -dev_ppl\n\n                print(\'validation: iter %d, dev. ppl %f\' % (train_iter, dev_ppl), file=sys.stderr)\n\n                is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n                hist_valid_scores.append(valid_metric)\n\n                if is_better:\n                    patience = 0\n                    print(\'save currently the best model to [%s]\' % model_save_path, file=sys.stderr)\n                    model.save(model_save_path)\n\n                    # also save the optimizers\' state\n                    torch.save(optimizer.state_dict(), model_save_path + \'.optim\')\n                elif patience < int(args[\'--patience\']):\n                    patience += 1\n                    print(\'hit patience %d\' % patience, file=sys.stderr)\n\n                    if patience == int(args[\'--patience\']):\n                        num_trial += 1\n                        print(\'hit #%d trial\' % num_trial, file=sys.stderr)\n                        if num_trial == int(args[\'--max-num-trial\']):\n                            print(\'early stop!\', file=sys.stderr)\n                            exit(0)\n\n                        # decay lr, and restore from previously best checkpoint\n                        lr = optimizer.param_groups[0][\'lr\'] * float(args[\'--lr-decay\'])\n                        print(\'load previously best model and decay learning rate to %f\' % lr, file=sys.stderr)\n\n                        # load model\n                        params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n                        model.load_state_dict(params[\'state_dict\'])\n                        model = model.to(device)\n\n                        print(\'restore parameters of the optimizers\', file=sys.stderr)\n                        optimizer.load_state_dict(torch.load(model_save_path + \'.optim\'))\n\n                        # set new lr\n                        for param_group in optimizer.param_groups:\n                            param_group[\'lr\'] = lr\n\n                        # reset patience\n                        patience = 0\n\n                if epoch == int(args[\'--max-epoch\']):\n                    print(\'reached maximum number of epochs!\', file=sys.stderr)\n                    exit(0)\n\n\ndef decode(args: Dict[str, str]):\n    """""" Performs decoding on a test set, and save the best-scoring decoding results.\n    If the target gold-standard sentences are given, the function also computes\n    corpus-level BLEU score.\n    @param args (Dict): args from cmd line\n    """"""\n\n    print(""load test source sentences from [{}]"".format(args[\'TEST_SOURCE_FILE\']), file=sys.stderr)\n    test_data_src = read_corpus(args[\'TEST_SOURCE_FILE\'], source=\'src\')\n    if args[\'TEST_TARGET_FILE\']:\n        print(""load test target sentences from [{}]"".format(args[\'TEST_TARGET_FILE\']), file=sys.stderr)\n        test_data_tgt = read_corpus(args[\'TEST_TARGET_FILE\'], source=\'tgt\')\n\n    print(""load model from {}"".format(args[\'MODEL_PATH\']), file=sys.stderr)\n    model = NMT.load(args[\'MODEL_PATH\'])\n\n    if args[\'--cuda\']:\n        model = model.to(torch.device(""cuda:0""))\n\n    hypotheses = beam_search(model, test_data_src,\n                             beam_size=int(args[\'--beam-size\']),\n                             max_decoding_time_step=int(args[\'--max-decoding-time-step\']))\n\n    if args[\'TEST_TARGET_FILE\']:\n        top_hypotheses = [hyps[0] for hyps in hypotheses]\n        bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n        print(\'Corpus BLEU: {}\'.format(bleu_score * 100), file=sys.stderr)\n\n    with open(args[\'OUTPUT_FILE\'], \'w\') as f:\n        for src_sent, hyps in zip(test_data_src, hypotheses):\n            top_hyp = hyps[0]\n            hyp_sent = \' \'.join(top_hyp.value)\n            f.write(hyp_sent + \'\\n\')\n\n\ndef beam_search(model: NMT, test_data_src: List[List[str]], beam_size: int, max_decoding_time_step: int) -> List[List[Hypothesis]]:\n    """""" Run beam search to construct hypotheses for a list of src-language sentences.\n    @param model (NMT): NMT Model\n    @param test_data_src (List[List[str]]): List of sentences (words) in source language, from test set.\n    @param beam_size (int): beam_size (# of hypotheses to hold for a translation at every step)\n    @param max_decoding_time_step (int): maximum sentence length that Beam search can produce\n    @returns hypotheses (List[List[Hypothesis]]): List of Hypothesis translations for every source sentence.\n    """"""\n    was_training = model.training\n    model.eval()\n\n    hypotheses = []\n    with torch.no_grad():\n        for src_sent in tqdm(test_data_src, desc=\'Decoding\', file=sys.stdout):\n            example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n\n            hypotheses.append(example_hyps)\n\n    if was_training: model.train(was_training)\n\n    return hypotheses\n\n\ndef main():\n    """""" Main func.\n    """"""\n    args = docopt(__doc__)\n\n    # Check pytorch version\n    # assert(torch.__version__ == ""1.0.0""), ""Please update your installation of PyTorch. You have {} and you should have version 1.0.0"".format(torch.__version__)\n\n    # seed the random number generators\n    seed = int(args[\'--seed\'])\n    torch.manual_seed(seed)\n    if args[\'--cuda\']:\n        torch.cuda.manual_seed(seed)\n    np.random.seed(seed * 13 // 7)\n\n    if args[\'train\']:\n        train(args)\n    elif args[\'decode\']:\n        decode(args)\n    else:\n        raise RuntimeError(\'invalid run mode\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
Assignment/a4/sanity_check.py,31,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCS224N 2018-19: Homework 4\nsanity_check.py: sanity checks for assignment 4\nSahil Chopra <schopra8@stanford.edu>\nMichael Hahn <>\n\nUsage:\n    sanity_check.py 1d\n    sanity_check.py 1e\n    sanity_check.py 1f\n\n""""""\nimport math\nimport sys\nimport pickle\nimport time\n\nimport numpy as np\n\nfrom docopt import docopt\nfrom typing import List, Tuple, Dict, Set, Union\nfrom tqdm import tqdm\nfrom utils import read_corpus, batch_iter\nfrom vocab import Vocab, VocabEntry\n\nfrom nmt_model import NMT\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.utils\n\n#----------\n# CONSTANTS\n#----------\nBATCH_SIZE = 5\nEMBED_SIZE = 3\nHIDDEN_SIZE = 3\nDROPOUT_RATE = 0.0\n\ndef reinitialize_layers(model):\n    """""" Reinitialize the Layer Weights for Sanity Checks.\n    """"""\n    def init_weights(m):\n        if type(m) == nn.Linear:\n            m.weight.data.fill_(0.3)\n            if m.bias is not None:\n                m.bias.data.fill_(0.1)\n        elif type(m) == nn.Embedding:\n            m.weight.data.fill_(0.15)\n        elif type(m) == nn.Dropout:\n            nn.Dropout(DROPOUT_RATE)\n    with torch.no_grad():\n        model.apply(init_weights)\n\n\ndef generate_outputs(model, source, target, vocab):\n    """""" Generate outputs.\n    """"""\n    print (""-""*80)\n    print(""Generating Comparison Outputs"")\n    reinitialize_layers(model)\n\n    # Compute sentence lengths\n    source_lengths = [len(s) for s in source]\n\n    # Convert list of lists into tensors\n    source_padded = model.vocab.src.to_input_tensor(source, device=model.device)\n    target_padded = model.vocab.tgt.to_input_tensor(target, device=model.device)\n\n    # Run the model forward\n    with torch.no_grad():\n        enc_hiddens, dec_init_state = model.encode(source_padded, source_lengths)\n        enc_masks = model.generate_sent_masks(enc_hiddens, source_lengths)\n        combined_outputs = model.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n\n    # Save Tensors to disk\n    torch.save(enc_hiddens, \'./sanity_check_en_es_data/enc_hiddens.pkl\')\n    torch.save(dec_init_state, \'./sanity_check_en_es_data/dec_init_state.pkl\') \n    torch.save(enc_masks, \'./sanity_check_en_es_data/enc_masks.pkl\')\n    torch.save(combined_outputs, \'./sanity_check_en_es_data/combined_outputs.pkl\')\n\n\ndef question_1d_sanity_check(model, src_sents, tgt_sents, vocab):\n    """""" Sanity check for question 1d. \n        Compares student output to that of model with dummy data.\n    """"""\n    print(""Running Sanity Check for Question 1d: Encode"")\n    print (""-""*80)\n\n    # Configure for Testing\n    reinitialize_layers(model)\n    source_lengths = [len(s) for s in src_sents]\n    source_padded = model.vocab.src.to_input_tensor(src_sents, device=model.device)\n\n    # Load Outputs\n    enc_hiddens_target = torch.load(\'./sanity_check_en_es_data/enc_hiddens.pkl\')\n    dec_init_state_target = torch.load(\'./sanity_check_en_es_data/dec_init_state.pkl\')\n\n    # Test\n    with torch.no_grad():\n        enc_hiddens_pred, dec_init_state_pred = model.encode(source_padded, source_lengths)\n    assert(np.allclose(enc_hiddens_target.numpy(), enc_hiddens_pred.numpy())), ""enc_hiddens is incorrect: it should be:\\n {} but is:\\n{}"".format(enc_hiddens_target, enc_hiddens_pred)\n    print(""enc_hiddens Sanity Checks Passed!"")\n    assert(np.allclose(dec_init_state_target[0].numpy(), dec_init_state_pred[0].numpy())), ""dec_init_state[0] is incorrect: it should be:\\n {} but is:\\n{}"".format(dec_init_state_target[0], dec_init_state_pred[0])\n    print(""dec_init_state[0] Sanity Checks Passed!"")\n    assert(np.allclose(dec_init_state_target[1].numpy(), dec_init_state_pred[1].numpy())), ""dec_init_state[1] is incorrect: it should be:\\n {} but is:\\n{}"".format(dec_init_state_target[1], dec_init_state_pred[1])\n    print(""dec_init_state[1] Sanity Checks Passed!"")\n    print (""-""*80)\n    print(""All Sanity Checks Passed for Question 1d: Encode!"")\n    print (""-""*80)\n\n\ndef question_1e_sanity_check(model, src_sents, tgt_sents, vocab):\n    """""" Sanity check for question 1e. \n        Compares student output to that of model with dummy data.\n    """"""\n    print (""-""*80)\n    print(""Running Sanity Check for Question 1e: Decode"")\n    print (""-""*80)\n\n    # Load Inputs\n    dec_init_state = torch.load(\'./sanity_check_en_es_data/dec_init_state.pkl\')\n    enc_hiddens = torch.load(\'./sanity_check_en_es_data/enc_hiddens.pkl\')\n    enc_masks = torch.load(\'./sanity_check_en_es_data/enc_masks.pkl\')\n    target_padded = torch.load(\'./sanity_check_en_es_data/target_padded.pkl\')\n\n    # Load Outputs\n    combined_outputs_target = torch.load(\'./sanity_check_en_es_data/combined_outputs.pkl\')\n\n    # Configure for Testing\n    reinitialize_layers(model)\n    COUNTER = [0]\n    def stepFunction(Ybar_t, dec_state, enc_hiddens, enc_hiddens_proj, enc_masks):\n       dec_state = torch.load(\'./sanity_check_en_es_data/step_dec_state_{}.pkl\'.format(COUNTER[0]))\n       o_t = torch.load(\'./sanity_check_en_es_data/step_o_t_{}.pkl\'.format(COUNTER[0]))\n       COUNTER[0]+=1\n       return dec_state, o_t, None\n    model.step = stepFunction\n\n    # Run Tests\n    with torch.no_grad():\n        combined_outputs_pred = model.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n    assert(np.allclose(combined_outputs_pred.numpy(), combined_outputs_target.numpy())), ""combined_outputs is incorrect: it should be:\\n {} but is:\\n{}"".format(combined_outputs_target, combined_outputs_pred)\n    print(""combined_outputs Sanity Checks Passed!"")\n    print (""-""*80)\n    print(""All Sanity Checks Passed for Question 1e: Decode!"")\n    print (""-""*80)\n\ndef question_1f_sanity_check(model, src_sents, tgt_sents, vocab):\n    """""" Sanity check for question 1f. \n        Compares student output to that of model with dummy data.\n    """"""\n    print (""-""*80)\n    print(""Running Sanity Check for Question 1f: Step"")\n    print (""-""*80)\n    reinitialize_layers(model)\n\n    # Inputs\n    Ybar_t = torch.load(\'./sanity_check_en_es_data/Ybar_t.pkl\')\n    dec_init_state = torch.load(\'./sanity_check_en_es_data/dec_init_state.pkl\')\n    enc_hiddens = torch.load(\'./sanity_check_en_es_data/enc_hiddens.pkl\')\n    enc_masks = torch.load(\'./sanity_check_en_es_data/enc_masks.pkl\')\n    enc_hiddens_proj = torch.load(\'./sanity_check_en_es_data/enc_hiddens_proj.pkl\')\n\n    # Output\n    dec_state_target = torch.load(\'./sanity_check_en_es_data/dec_state.pkl\')\n    o_t_target = torch.load(\'./sanity_check_en_es_data/o_t.pkl\')\n    e_t_target = torch.load(\'./sanity_check_en_es_data/e_t.pkl\')\n\n    # Run Tests\n    with torch.no_grad():\n        dec_state_pred, o_t_pred, e_t_pred= model.step(Ybar_t, dec_init_state, enc_hiddens, enc_hiddens_proj, enc_masks)\n    assert(np.allclose(dec_state_target[0].numpy(), dec_state_pred[0].numpy())), ""decoder_state[0] is incorrect: it should be:\\n {} but is:\\n{}"".format(dec_state_target[0], dec_state_pred[0])\n    print(""dec_state[0] Sanity Checks Passed!"")\n    assert(np.allclose(dec_state_target[1].numpy(), dec_state_pred[1].numpy())), ""decoder_state[1] is incorrect: it should be:\\n {} but is:\\n{}"".format(dec_state_target[1], dec_state_pred[1])\n    print(""dec_state[1] Sanity Checks Passed!"")\n    assert(np.allclose(o_t_target.numpy(), o_t_pred.numpy())), ""combined_output is incorrect: it should be:\\n {} but is:\\n{}"".format(o_t_target, o_t_pred)\n    print(""combined_output  Sanity Checks Passed!"")\n    assert(np.allclose(e_t_target.numpy(), e_t_pred.numpy())), ""e_t is incorrect: it should be:\\n {} but is:\\n{}"".format(e_t_target, e_t_pred)\n    print(""e_t Sanity Checks Passed!"")\n    print (""-""*80)    \n    print(""All Sanity Checks Passed for Question 1f: Step!"")\n    print (""-""*80)\n\n\ndef main():\n    """""" Main func.\n    """"""\n    args = docopt(__doc__)\n\n    # Check Python & PyTorch Versions\n    assert (sys.version_info >= (3, 5)), ""Please update your installation of Python to version >= 3.5""\n    # assert(torch.__version__ == ""1.0.0""), ""Please update your installation of PyTorch. You have {} and you should have version 1.0.0"".format(torch.__version__)\n\n    # Seed the Random Number Generators\n    seed = 1234\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed * 13 // 7)\n\n    # Load training data & vocabulary\n    train_data_src = read_corpus(\'./sanity_check_en_es_data/train_sanity_check.es\', \'src\')\n    train_data_tgt = read_corpus(\'./sanity_check_en_es_data/train_sanity_check.en\', \'tgt\')\n    train_data = list(zip(train_data_src, train_data_tgt))\n\n    for src_sents, tgt_sents in batch_iter(train_data, batch_size=BATCH_SIZE, shuffle=True):\n        src_sents = src_sents\n        tgt_sents = tgt_sents\n        break\n    vocab = Vocab.load(\'./sanity_check_en_es_data/vocab_sanity_check.json\') \n\n    # Create NMT Model\n    model = NMT(\n        embed_size=EMBED_SIZE,\n        hidden_size=HIDDEN_SIZE,\n        dropout_rate=DROPOUT_RATE,\n        vocab=vocab)\n\n    if args[\'1d\']:\n        question_1d_sanity_check(model, src_sents, tgt_sents, vocab)\n    elif args[\'1e\']:\n        question_1e_sanity_check(model, src_sents, tgt_sents, vocab)\n    elif args[\'1f\']:\n       # generate_outputs(model, src_sents, tgt_sents, vocab)\n        question_1f_sanity_check(model, src_sents, tgt_sents, vocab)\n    else:\n        raise RuntimeError(\'invalid run mode\')\n\n\nif __name__ == \'__main__\':\n    main()\n    \n'"
Assignment/a4/utils.py,2,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCS224N 2018-19: Homework 4\nnmt.py: NMT Model\nPencheng Yin <pcyin@cs.cmu.edu>\nSahil Chopra <schopra8@stanford.edu>\n""""""\n\nimport math\nfrom typing import List\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef pad_sents(sents, pad_token):\n    """""" Pad list of sentences according to the longest sentence in the batch.\n    @param sents (list[list[str]]): list of sentences, where each sentence\n                                    is represented as a list of words\n    @param pad_token (str): padding token\n    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter\n        than the max length sentence are padded out with the pad_token, such that\n        each sentences in the batch now has equal length.\n    """"""\n    sents_padded = []\n\n    ### YOUR CODE HERE (~6 Lines)\n    corpus_size = len(sents)\n    lens = [len(i) for i in sents] # cache it for later use\n    max_lens = max(lens)\n    sents_padded = [sents[i] + [pad_token] * (max_lens - lens[i]) for i in range(corpus_size)] # shape N x max_lens\n\n    ### END YOUR CODE\n\n    return sents_padded\n\n\n\ndef read_corpus(file_path, source):\n    """""" Read file, where each sentence is dilineated by a `\\n`.\n    @param file_path (str): path to file containing corpus\n    @param source (str): ""tgt"" or ""src"" indicating whether text\n        is of the source language or target language\n    """"""\n    data = []\n    for line in open(file_path):\n        sent = line.strip().split(\' \')\n        # only append <s> and </s> to the target sentence\n        if source == \'tgt\':\n            sent = [\'<s>\'] + sent + [\'</s>\']\n        data.append(sent)\n\n    return data\n\n\ndef batch_iter(data, batch_size, shuffle=False):\n    """""" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n    @param batch_size (int): batch size\n    @param shuffle (boolean): whether to randomly shuffle the dataset\n    """"""\n    batch_num = math.ceil(len(data) / batch_size)\n    index_array = list(range(len(data)))\n\n    if shuffle:\n        np.random.shuffle(index_array)\n\n    for i in range(batch_num):\n        indices = index_array[i * batch_size: (i + 1) * batch_size]\n        examples = [data[idx] for idx in indices]\n\n        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n        src_sents = [e[0] for e in examples]\n        tgt_sents = [e[1] for e in examples]\n\n        yield src_sents, tgt_sents\n\n'"
Assignment/a4/vocab.py,3,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCS224N 2018-19: Homework 4\nvocab.py: Vocabulary Generation\nPencheng Yin <pcyin@cs.cmu.edu>\nSahil Chopra <schopra8@stanford.edu>\n\nUsage:\n    vocab.py --train-src=<file> --train-tgt=<file> [options] VOCAB_FILE\n\nOptions:\n    -h --help                  Show this screen.\n    --train-src=<file>         File of training source sentences\n    --train-tgt=<file>         File of training target sentences\n    --size=<int>               vocab size [default: 50000]\n    --freq-cutoff=<int>        frequency cutoff [default: 2]\n""""""\n\nfrom collections import Counter\nfrom docopt import docopt\nfrom itertools import chain\nimport json\nimport torch\nfrom typing import List\nfrom utils import read_corpus, pad_sents\n\n\nclass VocabEntry(object):\n    """""" Vocabulary Entry, i.e. structure containing either\n    src or tgt language terms.\n    """"""\n    def __init__(self, word2id=None):\n        """""" Init VocabEntry Instance.\n        @param word2id (dict): dictionary mapping words 2 indices\n        """"""\n        if word2id:\n            self.word2id = word2id\n        else:\n            self.word2id = dict()\n            self.word2id[\'<pad>\'] = 0   # Pad Token\n            self.word2id[\'<s>\'] = 1 # Start Token\n            self.word2id[\'</s>\'] = 2    # End Token\n            self.word2id[\'<unk>\'] = 3   # Unknown Token\n        self.unk_id = self.word2id[\'<unk>\']\n        self.id2word = {v: k for k, v in self.word2id.items()}\n\n    def __getitem__(self, word):\n        """""" Retrieve word\'s index. Return the index for the unk\n        token if the word is out of vocabulary.\n        @param word (str): word to look up.\n        @returns index (int): index of word \n        """"""\n        return self.word2id.get(word, self.unk_id)\n\n    def __contains__(self, word):\n        """""" Check if word is captured by VocabEntry.\n        @param word (str): word to look up\n        @returns contains (bool): whether word is contained    \n        """"""\n        return word in self.word2id\n\n    def __setitem__(self, key, value):\n        """""" Raise error, if one tries to edit the VocabEntry.\n        """"""\n        raise ValueError(\'vocabulary is readonly\')\n\n    def __len__(self):\n        """""" Compute number of words in VocabEntry.\n        @returns len (int): number of words in VocabEntry\n        """"""\n        return len(self.word2id)\n\n    def __repr__(self):\n        """""" Representation of VocabEntry to be used\n        when printing the object.\n        """"""\n        return \'Vocabulary[size=%d]\' % len(self)\n\n    def id2word(self, wid):\n        """""" Return mapping of index to word.\n        @param wid (int): word index\n        @returns word (str): word corresponding to index\n        """"""\n        return self.id2word[wid]\n\n    def add(self, word):\n        """""" Add word to VocabEntry, if it is previously unseen.\n        @param word (str): word to add to VocabEntry\n        @return index (int): index that the word has been assigned\n        """"""\n        if word not in self:\n            wid = self.word2id[word] = len(self)\n            self.id2word[wid] = word\n            return wid\n        else:\n            return self[word]\n\n    def words2indices(self, sents):\n        """""" Convert list of words or list of sentences of words\n        into list or list of list of indices.\n        @param sents (list[str] or list[list[str]]): sentence(s) in words\n        @return word_ids (list[int] or list[list[int]]): sentence(s) in indices\n        """"""\n        if type(sents[0]) == list:\n            return [[self[w] for w in s] for s in sents]\n        else:\n            return [self[w] for w in sents]\n\n    def indices2words(self, word_ids):\n        """""" Convert list of indices into words.\n        @param word_ids (list[int]): list of word ids\n        @return sents (list[str]): list of words\n        """"""\n        return [self.id2word[w_id] for w_id in word_ids]\n\n    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n        """""" Convert list of sentences (words) into tensor with necessary padding for \n        shorter sentences.\n\n        @param sents (List[List[str]]): list of sentences (words)\n        @param device: device on which to load the tesnor, i.e. CPU or GPU\n\n        @returns sents_var: tensor of (max_sentence_length, batch_size)\n        """"""\n        word_ids = self.words2indices(sents)\n        sents_t = pad_sents(word_ids, self[\'<pad>\']) # num_sent x max_lens\n        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n        return torch.t(sents_var) # transpose, shape\n\n    @staticmethod\n    def from_corpus(corpus, size, freq_cutoff=2):\n        """""" Given a corpus construct a Vocab Entry.\n        @param corpus (list[str]): corpus of text produced by read_corpus function\n        @param size (int): # of words in vocabulary\n        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n        """"""\n        vocab_entry = VocabEntry()\n        word_freq = Counter(chain(*corpus))\n        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n        print(\'number of word types: {}, number of word types w/ frequency >= {}: {}\'\n              .format(len(word_freq), freq_cutoff, len(valid_words)))\n        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n        for word in top_k_words:\n            vocab_entry.add(word)\n        return vocab_entry\n\n\nclass Vocab(object):\n    """""" Vocab encapsulating src and target langauges.\n    """"""\n    def __init__(self, src_vocab: VocabEntry, tgt_vocab: VocabEntry):\n        """""" Init Vocab.\n        @param src_vocab (VocabEntry): VocabEntry for source language\n        @param tgt_vocab (VocabEntry): VocabEntry for target language\n        """"""\n        self.src = src_vocab\n        self.tgt = tgt_vocab\n\n    @staticmethod\n    def build(src_sents, tgt_sents, vocab_size, freq_cutoff) -> \'Vocab\':\n        """""" Build Vocabulary.\n        @param src_sents (list[str]): Source sentences provided by read_corpus() function\n        @param tgt_sents (list[str]): Target sentences provided by read_corpus() function\n        @param vocab_size (int): Size of vocabulary for both source and target languages\n        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word.\n        """"""\n        assert len(src_sents) == len(tgt_sents)\n\n        print(\'initialize source vocabulary ..\')\n        src = VocabEntry.from_corpus(src_sents, vocab_size, freq_cutoff)\n\n        print(\'initialize target vocabulary ..\')\n        tgt = VocabEntry.from_corpus(tgt_sents, vocab_size, freq_cutoff)\n\n        return Vocab(src, tgt)\n\n    def save(self, file_path):\n        """""" Save Vocab to file as JSON dump.\n        @param file_path (str): file path to vocab file\n        """"""\n        json.dump(dict(src_word2id=self.src.word2id, tgt_word2id=self.tgt.word2id), open(file_path, \'w\'), indent=2)\n\n    @staticmethod\n    def load(file_path):\n        """""" Load vocabulary from JSON dump.\n        @param file_path (str): file path to vocab file\n        @returns Vocab object loaded from JSON dump\n        """"""\n        entry = json.load(open(file_path, \'r\'))\n        src_word2id = entry[\'src_word2id\']\n        tgt_word2id = entry[\'tgt_word2id\']\n\n        return Vocab(VocabEntry(src_word2id), VocabEntry(tgt_word2id))\n\n    def __repr__(self):\n        """""" Representation of Vocab to be used\n        when printing the object.\n        """"""\n        return \'Vocab(source %d words, target %d words)\' % (len(self.src), len(self.tgt))\n\n\n\nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n\n    print(\'read in source sentences: %s\' % args[\'--train-src\'])\n    print(\'read in target sentences: %s\' % args[\'--train-tgt\'])\n\n    src_sents = read_corpus(args[\'--train-src\'], source=\'src\')\n    tgt_sents = read_corpus(args[\'--train-tgt\'], source=\'tgt\')\n\n    vocab = Vocab.build(src_sents, tgt_sents, int(args[\'--size\']), int(args[\'--freq-cutoff\']))\n    print(\'generated vocabulary, source %d words, target %d words\' % (len(vocab.src), len(vocab.tgt)))\n\n    vocab.save(args[\'VOCAB_FILE\'])\n    print(\'vocabulary saved to %s\' % args[\'VOCAB_FILE\'])\n'"
Assignment/a5-v1.2/__init__.py,0,b''
Assignment/a5-v1.2/char_decoder.py,5,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCS224N 2018-19: Homework 5\n""""""\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\n\nclass CharDecoder(nn.Module):\n    def __init__(self, hidden_size, char_embedding_size=50, target_vocab=None):\n        """""" Init Character Decoder.\n\n        @param hidden_size (int): Hidden size of the decoder LSTM\n        @param char_embedding_size (int): dimensionality of character embeddings\n        @param target_vocab (VocabEntry): vocabulary for the target language. See vocab.py for documentation.\n        """"""\n        ### YOUR CODE HERE for part 2a\n        ### TODO - Initialize as an nn.Module.\n        ###      - Initialize the following variables:\n        ###        self.charDecoder: LSTM. Please use nn.LSTM() to construct this.\n        ###        self.char_output_projection: Linear layer, called W_{dec} and b_{dec} in the PDF\n        ###        self.decoderCharEmb: Embedding matrix of character embeddings\n        ###        self.target_vocab: vocabulary for the target language\n        ###\n        ### Hint: - Use target_vocab.char2id to access the character vocabulary for the target language.\n        ###       - Set the padding_idx argument of the embedding matrix.\n        ###       - Create a new Embedding layer. Do not reuse embeddings created in Part 1 of this assignment.\n\n        super(CharDecoder, self).__init__()\n        V = len(target_vocab.char2id)\n        self.charDecoder            = nn.LSTM(char_embedding_size, hidden_size) # uni directional\n        self.char_output_projection = nn.Linear(hidden_size, V, bias=True)\n        self.decoderCharEmb         = nn.Embedding(V, char_embedding_size, padding_idx=target_vocab.char2id[\'<pad>\'])\n        self.target_vocab           = target_vocab\n\n        ### END YOUR CODE\n\n\n    \n    def forward(self, input, dec_hidden=None):\n        """""" Forward pass of character decoder.\n\n        @param input: tensor of integers, shape (length, batch)\n        @param dec_hidden: internal state of the LSTM before reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)\n\n        @returns scores: called s_t in the PDF, shape (length, batch, self.vocab_size)\n        @returns dec_hidden: internal state of the LSTM after reading the input characters. A tuple of two tensors of shape (1, batch, hidden_size)\n        """"""\n        ### YOUR CODE HERE for part 2b\n        ### TODO - Implement the forward pass of the character decoder.\n        char_embedding = self.decoderCharEmb(input) # len, batch, char_embed_size\n        # h_ts  shape (len, b, hidden_size)\n        hiddens, dec_hidden= self.charDecoder(char_embedding, dec_hidden)\n        # score shape : (len, b, V)\n        score = self.char_output_projection(hiddens)\n        return score, dec_hidden\n        ### END YOUR CODE \n\n\n    def train_forward(self, char_sequence, dec_hidden=None):\n        """""" Forward computation during training.\n\n        @param char_sequence: tensor of integers, shape (length, batch). Note that ""length"" here and in forward() need not be the same.\n        @param dec_hidden: initial internal state of the LSTM, obtained from the output of the word-level decoder. A tuple of two tensors of shape (1, batch, hidden_size)\n\n        @returns The cross-entropy loss, computed as the *sum* of cross-entropy losses of all the words in the batch.\n        """"""\n        ### YOUR CODE HERE for part 2c\n        ### TODO - Implement training forward pass.\n        ###\n        ### Hint: - Make sure padding characters do not contribute to the cross-entropy loss.\n        ###       - char_sequence corresponds to the sequence x_1 ... x_{n+1} from the handout (e.g., <START>,m,u,s,i,c,<END>).\n\n        # TODO: Check loss implementation\n\n        input  = char_sequence[:-1] # not get last character\n        score, dec_hidden = self.forward(input, dec_hidden) # shape (len, b, V)\n\n        target = char_sequence[1:].contiguous().view(-1) # not get first character\n        score  = score.view(-1, score.shape[-1])\n\n        loss   = nn.CrossEntropyLoss(\n            reduction= ""sum"", # Equation #15: When compute loss_char_dec, we take the sum, not average\n            ignore_index=self.target_vocab.char2id[\'<pad>\'] # not take into account pad character when compute loss\n        )\n\n        # take input : (N, C), target (N)\n        return loss(score, target)\n\n        ### END YOUR CODE\n\n    def decode_greedy(self, initialStates, device, max_length=21):\n        """""" Greedy decoding\n        @param initialStates: initial internal state of the LSTM, a tuple of two tensors of size (1, batch, hidden_size)\n        @param device: torch.device (indicates whether the model is on CPU or GPU)\n        @param max_length: maximum length of words to decode\n\n        @returns decodedWords: a list (of length batch) of strings, each of which has length <= max_length.\n                              The decoded strings should NOT contain the start-of-word and end-of-word characters.\n        """"""\n\n        ### YOUR CODE HERE for part 2d\n        ### TODO - Implement greedy decoding.\n        ### Hints:\n        ###      - Use target_vocab.char2id and target_vocab.id2char to convert between integers and characters\n        ###      - Use torch.tensor(..., device=device) to turn a list of character indices into a tensor.\n        ###      - We use curly brackets as start-of-word and end-of-word characters. That is, use the character \'{\' for <START> and \'}\' for <END>.\n        ###        Their indices are self.target_vocab.start_of_word and self.target_vocab.end_of_word, respectively.\n        b = initialStates[0].shape[1]\n        dec_hidden = initialStates\n\n        start_index = self.target_vocab.start_of_word\n        end_index   = self.target_vocab.end_of_word\n\n        input = torch.tensor([start_index for _ in range(b)], device=device).unsqueeze(0)\n        decodeTuple = [["""", False] for _ in range(b)]\n\n        for step in range(max_length):\n            score, dec_hidden = self.forward(input, dec_hidden) # score shape (1, b, V)\n            input = score.argmax(dim=2) # (1, b)\n\n            for str_index, char_index in enumerate(input.detach().squeeze(0)):\n                # if not reach end index:\n                if not decodeTuple[str_index][1]:\n                    if char_index != end_index:\n                        decodeTuple[str_index][0] += self.target_vocab.id2char[char_index.item()]\n                    else:\n                        decodeTuple[str_index][1] = True\n\n        decodedWords = [i[0]for i in decodeTuple]\n        return decodedWords\n\n        ### END YOUR CODE\n\n'"
Assignment/a5-v1.2/cnn.py,2,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCS224N 2018-19: Homework 5\n""""""\n\n### YOUR CODE HERE for part 1i\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass CNN(nn.Module):\n    def __init__(self, char_embed_size, num_filters, max_word_length, kernel_size=5 ):\n        """"""\n        Init the character CNN module\n        :param char_embed_size (int): embedding size (dimensionality) of each character in a word\n        :param num_filters (int): a.k.a number of output channels or word_embed_size\n        :param max_word_length (int): maximum length of a word\n        :param kernel_size (int): length convolve of each kernel\n        """"""\n        # input  : batch, char_embed  , max_word_length\n        # output : batch, num_filters , convolution_length\n        super(CNN, self).__init__()\n        self.char_embed_size = char_embed_size # num input channels\n        self.num_filters     = num_filters     # num output channels\n        self.kernel_size     = kernel_size\n        self.max_word_length = max_word_length\n\n        self.conv1d = nn.Conv1d(\n            in_channels=char_embed_size,\n            out_channels=num_filters,\n            kernel_size=kernel_size,\n            bias=True\n        )\n\n        self.max_pool_1d = nn.MaxPool1d(max_word_length - kernel_size + 1)\n        # self.conv1d.w\n\n    def forward(self, input):\n        """"""\n        Take a mini batch of character embedding of each word, compute word embedding\n        :param input (Tensor): shape (batch_size, char_embed_size, max_word_length)\n        :return (Tensor): shape (batch_size, word_embed_size), word embedding of each word in batch\n        """"""\n        x = self.conv1d(input) # (batch_size, word_embed_size, max_word_length - kernel_size + 1)\n        x = self.max_pool_1d(F.relu_(x)).squeeze()  # (batch_size, word_embed_size)\n        return x\n\n\n### END YOUR CODE\n\n'"
Assignment/a5-v1.2/highway.py,7,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCS224N 2018-19: Homework 5\n""""""\n\n### YOUR CODE HERE for part 1h\n\nimport torch\nimport torch.nn as nn\nimport torch.functional as F\n\nclass Highway(nn.Module):\n    """"""\n    Class that map x_conv to an embedding vector\n    """"""\n    def __init__(self, word_embed_size):\n        """"""\n        Init the Highway module\n        @param word_embed_size (int): Embedding size (dimensionality) for both the input (conv_output) and output\n        """"""\n        super(Highway, self).__init__()\n        self.word_embed_size   = word_embed_size\n        self.proj = nn.Linear(self.word_embed_size, self.word_embed_size)\n        self.gate = nn.Linear(self.word_embed_size, self.word_embed_size)\n\n    def forward(self, x_conv: torch.Tensor) -> torch.Tensor:\n        """"""\n        Take a mini batch of convolution output, compute\n        :param x_conv (torch.Tensor), shape batch_size x word_embed_size\n        :return: word_embedding (torch.Tensor), shape batch_size x word_embed_size\n        """"""\n        x_proj = torch.relu_(self.proj(x_conv))\n        x_gate = torch.sigmoid(self.gate(x_conv))\n\n        x = x_gate * x_proj + (1 - x_gate) * x_conv\n        return x\n\n### END YOUR CODE \n\n'"
Assignment/a5-v1.2/model_embeddings.py,1,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCS224N 2018-19: Homework 5\nmodel_embeddings.py: Embeddings for the NMT model\nPencheng Yin <pcyin@cs.cmu.edu>\nSahil Chopra <schopra8@stanford.edu>\nAnand Dhoot <anandd@stanford.edu>\nMichael Hahn <mhahn2@stanford.edu>\n""""""\n\nimport torch.nn as nn\n\n# Do not change these imports; your module names should be\n#   `CNN` in the file `cnn.py`\n#   `Highway` in the file `highway.py`\n# Uncomment the following two imports once you\'re ready to run part 1(j)\n\nfrom cnn import CNN\nfrom highway import Highway\n\n# End ""do not change""\n\nclass ModelEmbeddings(nn.Module): \n    """"""\n    Class that converts input words to their CNN-based embeddings.\n    """"""\n    def __init__(self, embed_size, vocab):\n        """"""\n        Init the Embedding layer for one language\n        @param embed_size (int): Embedding size (dimensionality) for the output \n        @param vocab (VocabEntry): VocabEntry object. See vocab.py for documentation.\n        """"""\n        super(ModelEmbeddings, self).__init__()\n\n        ## A4 code\n        # pad_token_idx = vocab.src[\'<pad>\']\n        # self.embeddings = nn.Embedding(len(vocab.src), embed_size, padding_idx=pad_token_idx)\n        ## End A4 code\n\n        self.embed_size = embed_size\n        self.char_embed_size = 50\n        self.max_word_len    = 21\n        self.dropout_rate    = 0.3\n        self.vocab           = vocab\n\n        self.char_embedding  = nn.Embedding(\n            num_embeddings =len(vocab.char2id),\n            embedding_dim  =self.char_embed_size,\n            padding_idx    =vocab.char2id[\'<pad>\'],\n\n        )\n\n        self.CNN = CNN(\n            char_embed_size=self.char_embed_size,\n            num_filters=embed_size,\n            max_word_length=self.max_word_len,\n        )\n\n        self.Highway = Highway(word_embed_size=embed_size)\n        self.dropout = nn.Dropout(p=self.dropout_rate)\n        ### YOUR CODE HERE for part 1j\n\n\n        ### END YOUR CODE\n\n    def forward(self, input):\n        """"""\n        Looks up character-based CNN embeddings for the words in a batch of sentences.\n        @param input: Tensor of integers of shape (sentence_length, batch_size, max_word_length) where\n            each integer is an index into the character vocabulary\n\n        @param output: Tensor of shape (sentence_length, batch_size, embed_size), containing the \n            CNN-based embeddings for each word of the sentences in the batch\n        """"""\n        ## A4 code\n        # output = self.embeddings(input)\n        # return output\n        ## End A4 code\n\n        char_embeddings = self.char_embedding(input) # sentence_length, batch_size, max_word_length,\n        sent_len, batch_size, max_word, _ = char_embeddings.shape\n        view_shape = (sent_len * batch_size, max_word, self.char_embed_size)\n        # bb = sent_len * batch_size\n        # bb, char_embed, max_word because 1D CNN only convolve in last dimension\n        char_embeddings = char_embeddings.view(view_shape).transpose(1, 2)\n\n        x_conv    = self.CNN(char_embeddings) # bb, word_embed_size\n        x_highway = self.Highway(x_conv)\n        output    = self.dropout(x_highway) # bb, word_embed\n        output    = output.view(sent_len, batch_size, self.embed_size)\n        return output\n        ### YOUR CODE HERE for part 1j\n\n\n        ### END YOUR CODE\n\n'"
Assignment/a5-v1.2/nmt_model.py,39,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCS224N 2018-19: Homework 5\nnmt_model.py: NMT Model\nPencheng Yin <pcyin@cs.cmu.edu>\nSahil Chopra <schopra8@stanford.edu>\n""""""\nfrom collections import namedtuple\nimport sys\nfrom typing import List, Tuple, Dict, Set, Union\nimport torch\nimport torch.nn as nn\nimport torch.nn.utils\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n\nfrom model_embeddings import ModelEmbeddings\nfrom char_decoder import CharDecoder\n\nHypothesis = namedtuple(\'Hypothesis\', [\'value\', \'score\'])\n\nimport random\n\nclass NMT(nn.Module):\n    """""" Simple Neural Machine Translation Model:\n        - Bidrectional LSTM Encoder\n        - Unidirection LSTM Decoder\n        - Global Attention Model (Luong, et al. 2015)\n    """"""\n    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2, no_char_decoder=False):\n        """""" Init NMT Model.\n\n        @param embed_size (int): Embedding size (dimensionality)\n        @param hidden_size (int): Hidden Size (dimensionality)\n        @param vocab (VocabEntry): Vocabulary object containing src and tgt languages\n                              See vocab.py for documentation.\n        @param dropout_rate (float): Dropout probability, for attention\n        """"""\n        super(NMT, self).__init__()\n\n        self.model_embeddings_source = ModelEmbeddings(embed_size, vocab.src)\n        self.model_embeddings_target = ModelEmbeddings(embed_size, vocab.tgt)\n\n        self.hidden_size = hidden_size\n        self.dropout_rate = dropout_rate\n        self.vocab = vocab\n\n        self.encoder = nn.LSTM(embed_size, hidden_size, bidirectional=True)\n        self.decoder = nn.LSTMCell(embed_size + hidden_size, hidden_size)\n\n        self.h_projection = nn.Linear(hidden_size * 2, hidden_size, bias=False)\n        self.c_projection = nn.Linear(hidden_size * 2, hidden_size, bias=False)\n        self.att_projection = nn.Linear(hidden_size * 2, hidden_size, bias=False)    \n        self.combined_output_projection = nn.Linear(hidden_size * 2 + hidden_size, hidden_size, bias=False)        \n        self.target_vocab_projection = nn.Linear(hidden_size, len(vocab.tgt), bias=False)\n        self.dropout = nn.Dropout(self.dropout_rate)\n\n        if not no_char_decoder:\n           self.charDecoder = CharDecoder(hidden_size, target_vocab=vocab.tgt) \n        else:\n           self.charDecoder = None\n\n    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:\n        """""" Take a mini-batch of source and target sentences, compute the log-likelihood of\n        target sentences under the language models learned by the NMT system.\n\n        @param source (List[List[str]]): list of source sentence tokens\n        @param target (List[List[str]]): list of target sentence tokens, wrapped by `<s>` and `</s>`\n\n        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the\n                                    log-likelihood of generating the gold-standard target sentence for\n                                    each example in the input batch. Here b = batch size.\n        """"""\n        # Compute sentence lengths\n        source_lengths = [len(s) for s in source]\n\n        # Convert list of lists into tensors\n\n        ## A4 code\n        # source_padded = self.vocab.src.to_input_tensor(source, device=self.device)   # Tensor: (src_len, b)\n        # target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)   # Tensor: (tgt_len, b)\n \n        # enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)\n        # enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n        # combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n        ## End A4 code\n        \n        ### YOUR CODE HERE for part 1k\n        ### TODO: \n        ###     Modify the code lines above as needed to fetch the character-level tensor \n        ###     to feed into encode() and decode(). You should:\n        ###     - Keep `target_padded` from A4 code above for predictions -> It\'s a list of word indices\n        ###     - Add `source_padded_chars` for character level padded encodings for source -> Char padded indices\n        ###     - Add `target_padded_chars` for character level padded encodings for target\n        ###     - Modify calls to encode() and decode() to use the character level encodings\n\n        source_padded_chars = self.vocab.src.to_input_tensor_char(source, device=self.device) # (src_len, b, max_w_len)\n        target_padded_chars = self.vocab.tgt.to_input_tensor_char(target, device=self.device) # (tgt_len, b, max_w_len)\n        target_padded       =  self.vocab.tgt.to_input_tensor(target, device=self.device) # (tgt_len, b)\n\n        enc_hiddens, dec_init_state = self.encode(source_padded_chars, source_lengths)\n        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n        combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded_chars)\n\n        ### END YOUR CODE\n\n        P = F.log_softmax(self.target_vocab_projection(combined_outputs), dim=-1)\n\n        # Zero out, probabilities for which we have nothing in the target text\n        target_masks = (target_padded != self.vocab.tgt[\'<pad>\']).float()\n\n        # Compute log probability of generating true target words\n        target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(-1) * target_masks[1:]\n        scores = target_gold_words_log_prob.sum() # mhahn2 Small modification from A4 code.\n\n\n\n        if self.charDecoder is not None:\n            max_word_len = target_padded_chars.shape[-1]\n            # remove start of word character ?\n            target_words = target_padded[1:].contiguous().view(-1)\n            # view : (l, b, max_w_len) -> (l * b, max_w_len)\n            target_chars = target_padded_chars[1:].contiguous().view(-1, max_word_len)\n            target_outputs = combined_outputs.view(-1, 256)\n    \n            target_chars_oov = target_chars #torch.index_select(target_chars, dim=0, index=oovIndices)\n            rnn_states_oov = target_outputs #torch.index_select(target_outputs, dim=0, index=oovIndices)\n            oovs_losses = self.charDecoder.train_forward(target_chars_oov.t(), (rnn_states_oov.unsqueeze(0), rnn_states_oov.unsqueeze(0)))\n            scores = scores - oovs_losses\n    \n        return scores\n\n\n    def encode(self, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        """""" Apply the encoder to source sentences to obtain encoder hidden states.\n            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where\n                                        b = batch_size, src_len = maximum source sentence length. Note that \n                                       these have already been sorted in order of longest to shortest sentence.\n        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch\n        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where\n                                        b = batch size, src_len = maximum source sentence length, h = hidden size.\n        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder\'s initial\n                                                hidden state and cell.\n        """"""\n        enc_hiddens, dec_init_state = None, None\n\n        X = self.model_embeddings_source(source_padded)\n        X_packed = pack_padded_sequence(X, source_lengths)\n        enc_hiddens, (last_hidden, last_cell) = self.encoder(X_packed)\n        (enc_hiddens, _) = pad_packed_sequence(enc_hiddens)\n        enc_hiddens = enc_hiddens.permute(1, 0, 2)\n\n        init_decoder_hidden = self.h_projection(torch.cat((last_hidden[0], last_hidden[1]), dim=1))\n        init_decoder_cell = self.c_projection(torch.cat((last_cell[0], last_cell[1]), dim=1))\n        dec_init_state = (init_decoder_hidden, init_decoder_cell)\n\n        return enc_hiddens, dec_init_state\n\n\n    def decode(self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,\n                dec_init_state: Tuple[torch.Tensor, torch.Tensor], target_padded: torch.Tensor) -> torch.Tensor:\n        """"""Compute combined output vectors for a batch.\n        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where\n                                     b = batch size, src_len = maximum source sentence length, h = hidden size.\n        @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where\n                                     b = batch size, src_len = maximum source sentence length.\n        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder\n        @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b), where\n                                       tgt_len = maximum target sentence length, b = batch size. \n        @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where\n                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size\n        """"""\n        # Chop of the <END> token for max length sentences.\n        target_padded = target_padded[:-1]\n\n        # Initialize the decoder state (hidden and cell)\n        dec_state = dec_init_state\n\n        # Initialize previous combined output vector o_{t-1} as zero\n        batch_size = enc_hiddens.size(0)\n        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)\n\n        # Initialize a list we will use to collect the combined output o_t on each step\n        combined_outputs = []\n\n        enc_hiddens_proj = self.att_projection(enc_hiddens)\n        Y = self.model_embeddings_target(target_padded)\n\n        for Y_t in torch.split(Y, split_size_or_sections=1):\n            Y_t = Y_t.squeeze(0)\n            Ybar_t = torch.cat([Y_t, o_prev], dim=-1)\n            dec_state, o_t, _ = self.step(Ybar_t, dec_state, enc_hiddens, enc_hiddens_proj, enc_masks)\n            combined_outputs.append(o_t)\n            o_prev = o_t\n\n        combined_outputs = torch.stack(combined_outputs)\n\n        return combined_outputs\n\n\n    def step(self, Ybar_t: torch.Tensor,\n            dec_state: Tuple[torch.Tensor, torch.Tensor],\n            enc_hiddens: torch.Tensor,\n            enc_hiddens_proj: torch.Tensor,\n            enc_masks: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n        """""" Compute one forward step of the LSTM decoder, including the attention computation.\n        @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,\n                                where b = batch size, e = embedding size, h = hidden size.\n        @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.\n                First tensor is decoder\'s prev hidden state, second tensor is decoder\'s prev cell.\n        @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,\n                                    src_len = maximum source length, h = hidden size.\n        @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),\n                                    where b = batch size, src_len = maximum source length, h = hidden size.\n        @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),\n                                    where b = batch size, src_len is maximum source length. \n        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.\n                First tensor is decoder\'s new hidden state, second tensor is decoder\'s new cell.\n        @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.\n        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.\n                                Note: You will not use this outside of this function.\n                                      We are simply returning this value so that we can sanity check\n                                      your implementation.\n        """"""\n\n        combined_output = None\n\n        dec_state = self.decoder(Ybar_t, dec_state)\n        (dec_hidden, dec_cell) = dec_state\n        e_t = torch.bmm(enc_hiddens_proj, dec_hidden.unsqueeze(2)).squeeze(2)\n\n\n        # Set e_t to -inf where enc_masks has 1\n        if enc_masks is not None:\n            e_t.data.masked_fill_(enc_masks.byte(), -float(\'inf\'))\n\n        alpha_t = F.softmax(e_t, dim=-1)\n        alpha_t_view = (alpha_t.size(0), 1, alpha_t.size(1))\n        a_t = torch.bmm(alpha_t.view(*alpha_t_view), enc_hiddens).squeeze(1)\n        U_t = torch.cat([dec_hidden, a_t], 1)\n        V_t = self.combined_output_projection(U_t)\n        O_t = self.dropout(torch.tanh(V_t))\n\n        combined_output = O_t\n        return dec_state, combined_output, e_t\n\n    def generate_sent_masks(self, enc_hiddens: torch.Tensor, source_lengths: List[int]) -> torch.Tensor:\n        """""" Generate sentence masks for encoder hidden states.\n\n        @param enc_hiddens (Tensor): encodings of shape (b, src_len, 2*h), where b = batch size,\n                                     src_len = max source length, h = hidden size. \n        @param source_lengths (List[int]): List of actual lengths for each of the sentences in the batch.\n        \n        @returns enc_masks (Tensor): Tensor of sentence masks of shape (b, src_len),\n                                    where src_len = max source length, h = hidden size.\n        """"""\n        enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float)\n        for e_id, src_len in enumerate(source_lengths):\n            enc_masks[e_id, src_len:] = 1\n        return enc_masks.to(self.device)\n\n\n    def beam_search(self, src_sent: List[str], beam_size: int=5, max_decoding_time_step: int=70) -> List[Hypothesis]:\n        """""" Given a single source sentence, perform beam search, yielding translations in the target language.\n        @param src_sent (List[str]): a single source sentence (words)\n        @param beam_size (int): beam size\n        @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n        @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n                value: List[str]: the decoded target sentence, represented as a list of words\n                score: float: the log-likelihood of the target sentence\n        """"""\n        ## A4 code\n        # src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device)\n        ## End A4 code\n\n        src_sents_var = self.vocab.src.to_input_tensor_char([src_sent], self.device)\n\n        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n        src_encodings_att_linear = self.att_projection(src_encodings)\n\n        h_tm1 = dec_init_vec\n        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n\n        eos_id = self.vocab.tgt[\'</s>\']\n\n        hypotheses = [[\'<s>\']]\n        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n        completed_hypotheses = []\n\n\n        t = 0\n        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n            t += 1\n            hyp_num = len(hypotheses)\n\n            exp_src_encodings = src_encodings.expand(hyp_num,\n                                                     src_encodings.size(1),\n                                                     src_encodings.size(2))\n\n            exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,\n                                                                           src_encodings_att_linear.size(1),\n                                                                           src_encodings_att_linear.size(2))\n\t\t\t\n            ## A4 code\n            # y_tm1 = self.vocab.tgt.to_input_tensor(list([hyp[-1]] for hyp in hypotheses), device=self.device)\n            # y_t_embed = self.model_embeddings_target(y_tm1)\n            ## End A4 code\n\n            y_tm1 = self.vocab.tgt.to_input_tensor_char(list([hyp[-1]] for hyp in hypotheses), device=self.device)\n            y_t_embed = self.model_embeddings_target(y_tm1)\n            y_t_embed = torch.squeeze(y_t_embed, dim=0)\n\n\n            x = torch.cat([y_t_embed, att_tm1], dim=-1)\n\n            (h_t, cell_t), att_t, _  = self.step(x, h_tm1,\n                                                      exp_src_encodings, exp_src_encodings_att_linear, enc_masks=None)\n\n            # log probabilities over target words\n            log_p_t = F.log_softmax(self.target_vocab_projection(att_t), dim=-1)\n\n            live_hyp_num = beam_size - len(completed_hypotheses)\n            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n\n            prev_hyp_ids = top_cand_hyp_pos / len(self.vocab.tgt)\n            hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n\n            new_hypotheses = []\n            live_hyp_ids = []\n            new_hyp_scores = []\n\n            decoderStatesForUNKsHere = []\n            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n                prev_hyp_id = prev_hyp_id.item()\n                hyp_word_id = hyp_word_id.item()\n                cand_new_hyp_score = cand_new_hyp_score.item()\n\n                hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n\n                # Record output layer in case UNK was generated\n                if hyp_word == ""<unk>"":\n                   hyp_word = ""<unk>""+str(len(decoderStatesForUNKsHere))\n                   decoderStatesForUNKsHere.append(att_t[prev_hyp_id])\n\n                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n                if hyp_word == \'</s>\':\n                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n                                                           score=cand_new_hyp_score))\n                else:\n                    new_hypotheses.append(new_hyp_sent)\n                    live_hyp_ids.append(prev_hyp_id)\n                    new_hyp_scores.append(cand_new_hyp_score)\n\n            if len(decoderStatesForUNKsHere) > 0 and self.charDecoder is not None: # decode UNKs\n                decoderStatesForUNKsHere = torch.stack(decoderStatesForUNKsHere, dim=0)\n                decodedWords = self.charDecoder.decode_greedy((decoderStatesForUNKsHere.unsqueeze(0), decoderStatesForUNKsHere.unsqueeze(0)), max_length=21, device=self.device)\n                assert len(decodedWords) == decoderStatesForUNKsHere.size()[0], ""Incorrect number of decoded words"" \n                for hyp in new_hypotheses:\n                  if hyp[-1].startswith(""<unk>""):\n                        hyp[-1] = decodedWords[int(hyp[-1][5:])]#[:-1]\n\n            if len(completed_hypotheses) == beam_size:\n                break\n\n            live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n            h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n            att_tm1 = att_t[live_hyp_ids]\n\n            hypotheses = new_hypotheses\n            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n\n        if len(completed_hypotheses) == 0:\n            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n                                                   score=hyp_scores[0].item()))\n\n        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n        return completed_hypotheses\n\n    @property\n    def device(self) -> torch.device:\n        """""" Determine which device to place the Tensors upon, CPU or GPU.\n        """"""\n        return self.att_projection.weight.device\n\n    @staticmethod\n    def load(model_path: str, no_char_decoder=False):\n        """""" Load the model from a file.\n        @param model_path (str): path to model\n        """"""\n        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n        args = params[\'args\']\n        model = NMT(vocab=params[\'vocab\'], no_char_decoder=no_char_decoder, **args)\n        model.load_state_dict(params[\'state_dict\'])\n\n        return model\n\n    def save(self, path: str):\n        """""" Save the odel to a file.\n        @param path (str): path to the model\n        """"""\n        print(\'save model parameters to [%s]\' % path, file=sys.stderr)\n\n        params = {\n            \'args\': dict(embed_size=self.model_embeddings_source.embed_size, hidden_size=self.hidden_size, dropout_rate=self.dropout_rate),\n            \'vocab\': self.vocab,\n            \'state_dict\': self.state_dict()\n        }\n\n        torch.save(params, path)\n'"
Assignment/a5-v1.2/run.py,14,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCS224N 2018-19: Homework 5\nrun.py: Run Script for Simple NMT Model\nPencheng Yin <pcyin@cs.cmu.edu>\nSahil Chopra <schopra8@stanford.edu>\n\nUsage:\n    run.py train --train-src=<file> --train-tgt=<file> --dev-src=<file> --dev-tgt=<file> --vocab=<file> [options]\n    run.py decode [options] MODEL_PATH TEST_SOURCE_FILE OUTPUT_FILE\n    run.py decode [options] MODEL_PATH TEST_SOURCE_FILE TEST_TARGET_FILE OUTPUT_FILE\n\nOptions:\n    -h --help                               show this screen.\n    --cuda                                  use GPU\n    --train-src=<file>                      train source file\n    --train-tgt=<file>                      train target file\n    --dev-src=<file>                        dev source file\n    --dev-tgt=<file>                        dev target file\n    --vocab=<file>                          vocab file\n    --seed=<int>                            seed [default: 0]\n    --batch-size=<int>                      batch size [default: 32]\n    --embed-size=<int>                      embedding size [default: 256]\n    --hidden-size=<int>                     hidden size [default: 256]\n    --clip-grad=<float>                     gradient clipping [default: 5.0]\n    --log-every=<int>                       log every [default: 10]\n    --max-epoch=<int>                       max epoch [default: 30]\n    --input-feed                            use input feeding\n    --patience=<int>                        wait for how many iterations to decay learning rate [default: 5]\n    --max-num-trial=<int>                   terminate training after how many trials [default: 5]\n    --lr-decay=<float>                      learning rate decay [default: 0.5]\n    --beam-size=<int>                       beam size [default: 5]\n    --sample-size=<int>                     sample size [default: 5]\n    --lr=<float>                            learning rate [default: 0.001]\n    --uniform-init=<float>                  uniformly initialize all parameters [default: 0.1]\n    --save-to=<file>                        model save path [default: model.bin]\n    --valid-niter=<int>                     perform validation after how many iterations [default: 2000]\n    --dropout=<float>                       dropout [default: 0.3]\n    --max-decoding-time-step=<int>          maximum number of decoding time steps [default: 70]\n    --no-char-decoder                       do not use the character decoder\n""""""\nimport math\nimport sys\nimport pickle\nimport time\n\n\nfrom docopt import docopt\nfrom nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\nfrom nmt_model import Hypothesis, NMT\nimport numpy as np\nfrom typing import List, Tuple, Dict, Set, Union\nfrom tqdm import tqdm\nfrom utils import read_corpus, batch_iter\nfrom vocab import Vocab, VocabEntry\n\nimport torch\nimport torch.nn.utils\n\n\ndef evaluate_ppl(model, dev_data, batch_size=32):\n    """""" Evaluate perplexity on dev sentences\n    @param model (NMT): NMT Model\n    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n    @param batch_size (batch size)\n    @returns ppl (perplixty on dev sentences)\n    """"""\n    was_training = model.training\n    model.eval()\n\n    cum_loss = 0.\n    cum_tgt_words = 0.\n\n    # no_grad() signals backend to throw away all gradients\n    with torch.no_grad():\n        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n            loss = -model(src_sents, tgt_sents).sum()\n\n            cum_loss += loss.item()\n            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n            cum_tgt_words += tgt_word_num_to_predict\n\n        ppl = np.exp(cum_loss / cum_tgt_words)\n\n    if was_training:\n        model.train()\n\n    return ppl\n\n\ndef compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n    """""" Given decoding results and reference sentences, compute corpus-level BLEU score.\n    @param references (List[List[str]]): a list of gold-standard reference target sentences\n    @param hypotheses (List[Hypothesis]): a list of hypotheses, one for each reference\n    @returns bleu_score: corpus-level BLEU score\n    """"""\n    if references[0][0] == \'<s>\':\n        references = [ref[1:-1] for ref in references]\n    bleu_score = corpus_bleu([[ref] for ref in references],\n                             [hyp.value for hyp in hypotheses])\n    return bleu_score\n\n\ndef train(args: Dict):\n    """""" Train the NMT Model.\n    @param args (Dict): args from cmd line\n    """"""\n    train_data_src = read_corpus(args[\'--train-src\'], source=\'src\')\n    train_data_tgt = read_corpus(args[\'--train-tgt\'], source=\'tgt\')\n\n    dev_data_src = read_corpus(args[\'--dev-src\'], source=\'src\')\n    dev_data_tgt = read_corpus(args[\'--dev-tgt\'], source=\'tgt\')\n\n    train_data = list(zip(train_data_src, train_data_tgt))\n    dev_data = list(zip(dev_data_src, dev_data_tgt))\n\n    train_batch_size = int(args[\'--batch-size\'])\n\n    clip_grad = float(args[\'--clip-grad\'])\n    valid_niter = int(args[\'--valid-niter\'])\n    log_every = int(args[\'--log-every\'])\n    model_save_path = args[\'--save-to\']\n\n    vocab = Vocab.load(args[\'--vocab\'])\n\n    model = NMT(embed_size=int(args[\'--embed-size\']),\n                hidden_size=int(args[\'--hidden-size\']),\n                dropout_rate=float(args[\'--dropout\']),\n                vocab=vocab, no_char_decoder=args[\'--no-char-decoder\'])\n    model.train()\n\n    uniform_init = float(args[\'--uniform-init\'])\n    if np.abs(uniform_init) > 0.:\n        print(\'uniformly initialize parameters [-%f, +%f]\' % (uniform_init, uniform_init), file=sys.stderr)\n        for p in model.parameters():\n            p.data.uniform_(-uniform_init, uniform_init)\n\n    vocab_mask = torch.ones(len(vocab.tgt))\n    vocab_mask[vocab.tgt[\'<pad>\']] = 0\n\n    device = torch.device(""cuda:0"" if args[\'--cuda\'] else ""cpu"")\n    print(\'use device: %s\' % device, file=sys.stderr)\n\n    model = model.to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=float(args[\'--lr\']))\n\n    num_trial = 0\n    train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n    cum_examples = report_examples = epoch = valid_num = 0\n    hist_valid_scores = []\n    train_time = begin_time = time.time()\n    print(\'begin Maximum Likelihood training\')\n\n    while True:\n        epoch += 1\n\n        for src_sents, tgt_sents in batch_iter(train_data, batch_size=train_batch_size, shuffle=True):\n            train_iter += 1\n\n            optimizer.zero_grad()\n\n            batch_size = len(src_sents)\n\n            example_losses = -model(src_sents, tgt_sents) # (batch_size,)\n            batch_loss = example_losses.sum()\n            loss = batch_loss / batch_size\n\n            loss.backward()\n\n            # clip gradient\n            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n\n            optimizer.step()\n\n            batch_losses_val = batch_loss.item()\n            report_loss += batch_losses_val\n            cum_loss += batch_losses_val\n\n            tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n            report_tgt_words += tgt_words_num_to_predict\n            cum_tgt_words += tgt_words_num_to_predict\n            report_examples += batch_size\n            cum_examples += batch_size\n\n            if train_iter % log_every == 0:\n                print(\'epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f \' \\\n                      \'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec\' % (epoch, train_iter,\n                                                                                         report_loss / report_examples,\n                                                                                         math.exp(report_loss / report_tgt_words),\n                                                                                         cum_examples,\n                                                                                         report_tgt_words / (time.time() - train_time),\n                                                                                         time.time() - begin_time), file=sys.stderr)\n\n                train_time = time.time()\n                report_loss = report_tgt_words = report_examples = 0.\n\n            # perform validation\n            if train_iter % valid_niter == 0:\n                print(\'epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d\' % (epoch, train_iter,\n                                                                                         cum_loss / cum_examples,\n                                                                                         np.exp(cum_loss / cum_tgt_words),\n                                                                                         cum_examples), file=sys.stderr)\n\n                cum_loss = cum_examples = cum_tgt_words = 0.\n                valid_num += 1\n\n                print(\'begin validation ...\', file=sys.stderr)\n\n                # compute dev. ppl and bleu\n                dev_ppl = evaluate_ppl(model, dev_data, batch_size=128)   # dev batch size can be a bit larger\n                valid_metric = -dev_ppl\n\n                print(\'validation: iter %d, dev. ppl %f\' % (train_iter, dev_ppl), file=sys.stderr)\n\n                is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n                hist_valid_scores.append(valid_metric)\n\n                if is_better:\n                    patience = 0\n                    print(\'save currently the best model to [%s]\' % model_save_path, file=sys.stderr)\n                    model.save(model_save_path)\n\n                    # also save the optimizers\' state\n                    torch.save(optimizer.state_dict(), model_save_path + \'.optim\')\n                elif patience < int(args[\'--patience\']):\n                    patience += 1\n                    print(\'hit patience %d\' % patience, file=sys.stderr)\n\n                    if patience == int(args[\'--patience\']):\n                        num_trial += 1\n                        print(\'hit #%d trial\' % num_trial, file=sys.stderr)\n                        if num_trial == int(args[\'--max-num-trial\']):\n                            print(\'early stop!\', file=sys.stderr)\n                            exit(0)\n\n                        # decay lr, and restore from previously best checkpoint\n                        lr = optimizer.param_groups[0][\'lr\'] * float(args[\'--lr-decay\'])\n                        print(\'load previously best model and decay learning rate to %f\' % lr, file=sys.stderr)\n\n                        # load model\n                        params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n                        model.load_state_dict(params[\'state_dict\'])\n                        model = model.to(device)\n\n                        print(\'restore parameters of the optimizers\', file=sys.stderr)\n                        optimizer.load_state_dict(torch.load(model_save_path + \'.optim\'))\n\n                        # set new lr\n                        for param_group in optimizer.param_groups:\n                            param_group[\'lr\'] = lr\n\n                        # reset patience\n                        patience = 0\n\n            if epoch == int(args[\'--max-epoch\']):\n                print(\'reached maximum number of epochs!\', file=sys.stderr)\n                exit(0)\n\n\ndef decode(args: Dict[str, str]):\n    """""" Performs decoding on a test set, and save the best-scoring decoding results.\n    If the target gold-standard sentences are given, the function also computes\n    corpus-level BLEU score.\n    @param args (Dict): args from cmd line\n    """"""\n\n    print(""load test source sentences from [{}]"".format(args[\'TEST_SOURCE_FILE\']), file=sys.stderr)\n    test_data_src = read_corpus(args[\'TEST_SOURCE_FILE\'], source=\'src\')\n    if args[\'TEST_TARGET_FILE\']:\n        print(""load test target sentences from [{}]"".format(args[\'TEST_TARGET_FILE\']), file=sys.stderr)\n        test_data_tgt = read_corpus(args[\'TEST_TARGET_FILE\'], source=\'tgt\')\n\n    print(""load model from {}"".format(args[\'MODEL_PATH\']), file=sys.stderr)\n    model = NMT.load(args[\'MODEL_PATH\'], no_char_decoder=args[\'--no-char-decoder\'])\n\n    if args[\'--cuda\']:\n        model = model.to(torch.device(""cuda:0""))\n\n    hypotheses = beam_search(model, test_data_src,\n                             beam_size=int(args[\'--beam-size\']),\n                             max_decoding_time_step=int(args[\'--max-decoding-time-step\']))\n\n    if args[\'TEST_TARGET_FILE\']:\n        top_hypotheses = [hyps[0] for hyps in hypotheses]\n        bleu_score = compute_corpus_level_bleu_score(test_data_tgt, top_hypotheses)\n        print(\'Corpus BLEU: {}\'.format(bleu_score * 100), file=sys.stderr)\n\n    with open(args[\'OUTPUT_FILE\'], \'w\') as f:\n        for src_sent, hyps in zip(test_data_src, hypotheses):\n            top_hyp = hyps[0]\n            hyp_sent = \' \'.join(top_hyp.value)\n            f.write(hyp_sent + \'\\n\')\n\n\ndef beam_search(model: NMT, test_data_src: List[List[str]], beam_size: int, max_decoding_time_step: int) -> List[List[Hypothesis]]:\n    """""" Run beam search to construct hypotheses for a list of src-language sentences.\n    @param model (NMT): NMT Model\n    @param test_data_src (List[List[str]]): List of sentences (words) in source language, from test set.\n    @param beam_size (int): beam_size (# of hypotheses to hold for a translation at every step)\n    @param max_decoding_time_step (int): maximum sentence length that Beam search can produce\n    @returns hypotheses (List[List[Hypothesis]]): List of Hypothesis translations for every source sentence.\n    """"""\n    was_training = model.training\n    model.eval()\n\n    hypotheses = []\n    with torch.no_grad():\n        for src_sent in tqdm(test_data_src, desc=\'Decoding\', file=sys.stdout):\n            example_hyps = model.beam_search(src_sent, beam_size=beam_size, max_decoding_time_step=max_decoding_time_step)\n\n            hypotheses.append(example_hyps)\n\n    if was_training: model.train(was_training)\n\n    return hypotheses\n\n\ndef main():\n    """""" Main func.\n    """"""\n    args = docopt(__doc__)\n\n    # Check pytorch version\n    # assert(torch.__version__ == ""1.0.0""), ""Please update your installation of PyTorch. You have {} and you should have version 1.0.0"".format(torch.__version__)\n\n    # seed the random number generators\n    seed = int(args[\'--seed\'])\n    torch.manual_seed(seed)\n    if args[\'--cuda\']:\n        torch.cuda.manual_seed(seed)\n    np.random.seed(seed * 13 // 7)\n\n    if args[\'train\']:\n        train(args)\n    elif args[\'decode\']:\n        decode(args)\n    else:\n        raise RuntimeError(\'invalid run mode\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
Assignment/a5-v1.2/sanity_check.py,10,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCS224N 2018-19: Homework 5\nsanity_check.py: sanity checks for assignment 5\nUsage:\n    sanity_check.py 1e\n    sanity_check.py 1f\n    sanity_check.py 1j\n    sanity_check.py 2a\n    sanity_check.py 2b\n    sanity_check.py 2c\n    sanity_check.py 2d\n""""""\nimport json\nimport math\nimport pickle\nimport sys\nimport time\n\nimport numpy as np\n\nfrom docopt import docopt\nfrom typing import List, Tuple, Dict, Set, Union\nfrom tqdm import tqdm\nfrom utils import pad_sents_char, read_corpus, batch_iter\nfrom vocab import Vocab, VocabEntry\n\nfrom char_decoder import CharDecoder\nfrom nmt_model import NMT\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.utils\n\n#----------\n# CONSTANTS\n#----------\nBATCH_SIZE = 5\nEMBED_SIZE = 3\nHIDDEN_SIZE = 3\nDROPOUT_RATE = 0.0\n\n\nclass DummyVocab():\n    def __init__(self):\n        self.char2id = json.load(open(\'./sanity_check_en_es_data/char_vocab_sanity_check.json\', \'r\'))\n        self.id2char = {id: char for char, id in self.char2id.items()}\n        self.char_unk = self.char2id[\'<unk>\']\n        self.start_of_word = self.char2id[""{""]\n        self.end_of_word = self.char2id[""}""]\n\ndef question_1e_sanity_check():\n    """""" Sanity check for words2charindices function. \n    """"""\n    print (""-""*80)\n    print(""Running Sanity Check for Question 1e: words2charindices()"")\n    print (""-""*80)\n    vocab = VocabEntry()\n\n    print(\'Running test on small list of sentences\')\n    sentences = [[""a"", ""b"", ""c?""], [""~d~"", ""c"", ""b"", ""a""]]\n    small_ind = vocab.words2charindices(sentences)\n    small_ind_gold = [[[1, 30, 2], [1, 31, 2], [1, 32, 70, 2]], [[1, 85, 33, 85, 2], [1, 32, 2], [1, 31, 2], [1, 30, 2]]]\n    assert(small_ind == small_ind_gold), \\\n        ""small test resulted in indices list {:}, expected {:}"".format(small_ind, small_ind_gold)\n\n    print(\'Running test on large list of sentences\')\n    tgt_sents = [[\'<s>\', ""Let\'s"", \'start\', \'by\', \'thinking\', \'about\', \'the\', \'member\', \'countries\', \'of\', \'the\', \'OECD,\', \'or\', \'the\', \'Organization\', \'of\', \'Economic\', \'Cooperation\', \'and\', \'Development.\', \'</s>\'], [\'<s>\', \'In\', \'the\', \'case\', \'of\', \'gun\', \'control,\', \'we\', \'really\', \'underestimated\', \'our\', \'opponents.\', \'</s>\'], [\'<s>\', \'Let\', \'me\', \'share\', \'with\', \'those\', \'of\', \'you\', \'here\', \'in\', \'the\', \'first\', \'row.\', \'</s>\'], [\'<s>\', \'It\', \'suggests\', \'that\', \'we\', \'care\', \'about\', \'the\', \'fight,\', \'about\', \'the\', \'challenge.\', \'</s>\'], [\'<s>\', \'A\', \'lot\', \'of\', \'numbers\', \'there.\', \'A\', \'lot\', \'of\', \'numbers.\', \'</s>\']]\n    tgt_ind = vocab.words2charindices(tgt_sents)\n    tgt_ind_gold = pickle.load(open(\'./sanity_check_en_es_data/1e_tgt.pkl\', \'rb\'))\n    assert(tgt_ind == tgt_ind_gold), ""target vocab test resulted in indices list {:}, expected {:}"".format(tgt_ind, tgt_ind_gold)\n\n    print(""All Sanity Checks Passed for Question 1e: words2charindices()!"")\n    print (""-""*80)\n\ndef question_1f_sanity_check():\n    """""" Sanity check for pad_sents_char() function. \n    """"""\n    print (""-""*80)\n    print(""Running Sanity Check for Question 1f: Padding"")\n    print (""-""*80)\n    vocab = VocabEntry()\n\n    print(""Running test on a list of sentences"")\n    sentences = [[\'Human:\', \'What\', \'do\', \'we\', \'want?\'], [\'Computer:\', \'Natural\', \'language\', \'processing!\'], [\'Human:\', \'When\', \'do\', \'we\', \'want\', \'it?\'], [\'Computer:\', \'When\', \'do\', \'we\', \'want\', \'what?\']]\n    word_ids = vocab.words2charindices(sentences)\n\n    padded_sentences = pad_sents_char(word_ids, 0)\n    gold_padded_sentences = torch.load(\'./sanity_check_en_es_data/gold_padded_sentences.pkl\')\n    assert padded_sentences == gold_padded_sentences, ""Sentence padding is incorrect: it should be:\\n {} but is:\\n{}"".format(gold_padded_sentences, padded_sentences)\n\n    print(""Sanity Check Passed for Question 1f: Padding!"")\n    print(""-""*80)\n\n\ndef question_1j_sanity_check(model):\n\t"""""" Sanity check for model_embeddings.py \n\t\tbasic shape check\n\t""""""\n\tprint (""-""*80)\n\tprint(""Running Sanity Check for Question 1j: Model Embedding"")\n\tprint (""-""*80)\n\tsentence_length = 10\n\tmax_word_length = 21\n\tinpt = torch.zeros(sentence_length, BATCH_SIZE, max_word_length, dtype=torch.long)\n\tME_source = model.model_embeddings_source\n\toutput = ME_source.forward(inpt)\n\toutput_expected_size = [sentence_length, BATCH_SIZE, EMBED_SIZE]\n\tassert(list(output.size()) == output_expected_size), ""output shape is incorrect: it should be:\\n {} but is:\\n{}"".format(output_expected_size, list(output.size()))\n\tprint(""Sanity Check Passed for Question 1j: Model Embedding!"")\n\tprint(""-""*80)\n\ndef question_2a_sanity_check(decoder, char_vocab):\n    """""" Sanity check for CharDecoder.__init__()\n        basic shape check\n    """"""\n    print (""-""*80)\n    print(""Running Sanity Check for Question 2a: CharDecoder.__init__()"")\n    print (""-""*80)\n    assert(decoder.charDecoder.input_size == EMBED_SIZE), ""Input dimension is incorrect:\\n it should be {} but is: {}"".format(EMBED_SIZE, decoder.charDecoder.input_size)\n    assert(decoder.charDecoder.hidden_size == HIDDEN_SIZE), ""Hidden dimension is incorrect:\\n it should be {} but is: {}"".format(HIDDEN_SIZE, decoder.charDecoder.hidden_size)\n    assert(decoder.char_output_projection.in_features == HIDDEN_SIZE), ""Input dimension is incorrect:\\n it should be {} but is: {}"".format(HIDDEN_SIZE, decoder.char_output_projection.in_features)\n    assert(decoder.char_output_projection.out_features == len(char_vocab.char2id)), ""Output dimension is incorrect:\\n it should be {} but is: {}"".format(len(char_vocab.char2id), decoder.char_output_projection.out_features)\n    assert(decoder.decoderCharEmb.num_embeddings == len(char_vocab.char2id)), ""Number of embeddings is incorrect:\\n it should be {} but is: {}"".format(len(char_vocab.char2id), decoder.decoderCharEmb.num_embeddings)\n    assert(decoder.decoderCharEmb.embedding_dim == EMBED_SIZE), ""Embedding dimension is incorrect:\\n it should be {} but is: {}"".format(EMBED_SIZE, decoder.decoderCharEmb.embedding_dim)\n    print(""Sanity Check Passed for Question 2a: CharDecoder.__init__()!"")\n    print(""-""*80)\n\ndef question_2b_sanity_check(decoder, char_vocab):\n    """""" Sanity check for CharDecoder.forward()\n        basic shape check\n    """"""\n    print (""-""*80)\n    print(""Running Sanity Check for Question 2b: CharDecoder.forward()"")\n    print (""-""*80)\n    sequence_length = 4\n    inpt = torch.zeros(sequence_length, BATCH_SIZE, dtype=torch.long)\n    logits, (dec_hidden1, dec_hidden2) = decoder.forward(inpt)\n    logits_expected_size = [sequence_length, BATCH_SIZE, len(char_vocab.char2id)]\n    dec_hidden_expected_size = [1, BATCH_SIZE, HIDDEN_SIZE]\n    assert(list(logits.size()) == logits_expected_size), ""Logits shape is incorrect:\\n it should be {} but is:\\n{}"".format(logits_expected_size, list(logits.size()))\n    assert(list(dec_hidden1.size()) == dec_hidden_expected_size), ""Decoder hidden state shape is incorrect:\\n it should be {} but is: {}"".format(dec_hidden_expected_size, list(dec_hidden1.size()))\n    assert(list(dec_hidden2.size()) == dec_hidden_expected_size), ""Decoder hidden state shape is incorrect:\\n it should be {} but is: {}"".format(dec_hidden_expected_size, list(dec_hidden2.size()))\n    print(""Sanity Check Passed for Question 2b: CharDecoder.forward()!"")\n    print(""-""*80)\n\ndef question_2c_sanity_check(decoder):\n    """""" Sanity check for CharDecoder.train_forward()\n        basic shape check\n    """"""\n    print (""-""*80)\n    print(""Running Sanity Check for Question 2c: CharDecoder.train_forward()"")\n    print (""-""*80)\n    sequence_length = 4\n    inpt = torch.zeros(sequence_length, BATCH_SIZE, dtype=torch.long)\n    loss = decoder.train_forward(inpt)\n    assert(list(loss.size()) == []), ""Loss should be a scalar but its shape is: {}"".format(list(loss.size()))\n    print(""Sanity Check Passed for Question 2c: CharDecoder.train_forward()!"")\n    print(""-""*80)\n\ndef question_2d_sanity_check(decoder):\n    """""" Sanity check for CharDecoder.decode_greedy()\n        basic shape check\n    """"""\n    print (""-""*80)\n    print(""Running Sanity Check for Question 2d: CharDecoder.decode_greedy()"")\n    print (""-""*80)\n    sequence_length = 4\n    inpt = torch.zeros(1, BATCH_SIZE, HIDDEN_SIZE, dtype=torch.float)\n    initialStates = (inpt, inpt)\n    device = decoder.char_output_projection.weight.device\n    decodedWords = decoder.decode_greedy(initialStates, device)\n    assert(len(decodedWords) == BATCH_SIZE), ""Length of decodedWords should be {} but is: {}"".format(BATCH_SIZE, len(decodedWords))\n    print(""Sanity Check Passed for Question 2d: CharDecoder.decode_greedy()!"")\n    print(""-""*80)\n\ndef main():\n    """""" Main func.\n    """"""\n    args = docopt(__doc__)\n\n    # Check Python & PyTorch Versions\n    assert (sys.version_info >= (3, 5)), ""Please update your installation of Python to version >= 3.5""\n    # assert(torch.__version__ == ""1.0.0""), ""Please update your installation of PyTorch. You have {} and you should have version 1.0.0"".format(torch.__version__)\n\n    # Seed the Random Number Generators\n    seed = 1234\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed * 13 // 7)\n\n    vocab = Vocab.load(\'./sanity_check_en_es_data/vocab_sanity_check.json\') \n\n    # Create NMT Model\n    model = NMT(\n        embed_size=EMBED_SIZE,\n        hidden_size=HIDDEN_SIZE,\n        dropout_rate=DROPOUT_RATE,\n        vocab=vocab)\n\n    char_vocab = DummyVocab()\n\n    # Initialize CharDecoder\n    decoder = CharDecoder(\n        hidden_size=HIDDEN_SIZE,\n        char_embedding_size=EMBED_SIZE,\n        target_vocab=char_vocab)\n\n    if args[\'1e\']:\n        question_1e_sanity_check()\n    elif args[\'1f\']:\n        question_1f_sanity_check()\n    elif args[\'1j\']:\n        question_1j_sanity_check(model)\n    elif args[\'2a\']:\n        question_2a_sanity_check(decoder, char_vocab)\n    elif args[\'2b\']:\n        question_2b_sanity_check(decoder, char_vocab)\n    elif args[\'2c\']:\n        question_2c_sanity_check(decoder)\n    elif args[\'2d\']:\n        question_2d_sanity_check(decoder)\n    else:\n        raise RuntimeError(\'invalid run mode\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
Assignment/a5-v1.2/sanity_check_handmade.py,7,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCS224N 2018-19: Homework 5\nsanity_check_handmade.py: Handmade sanity checks for implementation problems\n\nUsage:\n    sanity_check_handmade.py generate\n    sanity_check_handmade.py highway\n    sanity_check_handmade.py cnn\n\nOptions:\n    -h --help       Show this screen.\n""""""\n\nfrom docopt import docopt\n\nfrom cnn import CNN\nfrom sanity_check import DummyVocab\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom nmt_model import NMT\n\nfrom highway import Highway\nfrom char_decoder import CharDecoder\nfrom vocab import Vocab\n\nBATCH_SIZE    = 5\nEMBED_SIZE    = 3\nHIDDEN_SIZE   = 3\nDROPOUT_RATE  = 0.0\nNUM_FILTER    = 4\nKERNEl_SIZE   = 3\nMAX_WORD_LEN  = 8\n\n\ndef reinitialize_layers(model):\n    """""" Reinitialize the Layer Weights for Sanity Checks.\n    """"""\n    def init_weights(m):\n        if type(m) == nn.Linear:\n            m.weight.data.fill_(0.3)\n            if m.bias is not None:\n                m.bias.data.fill_(0.1)\n        elif type(m) == nn.Embedding:\n            m.weight.data.fill_(0.15)\n        elif type(m) == nn.Dropout:\n            nn.Dropout(DROPOUT_RATE)\n    with torch.no_grad():\n        model.apply(init_weights)\n\ndef question_1h_generate_data():\n    """"""\n    Unit test data generator for Highway\n    """"""\n    conv_input = np.random.rand(BATCH_SIZE, EMBED_SIZE)\n    W_proj     = np.ones((EMBED_SIZE, EMBED_SIZE)) * 0.3\n    b_proj     = np.ones(EMBED_SIZE) * 0.1\n\n    W_gate     = np.ones((EMBED_SIZE, EMBED_SIZE)) * 0.3\n    b_gate     = np.ones(EMBED_SIZE) * 0.1\n\n    def relu(inpt):\n        return np.maximum(inpt, 0)\n\n    def sigmoid(inpt):\n        return 1. / (1 + np.exp(-inpt))\n\n    x_proj    = relu(conv_input.dot(W_proj) + b_proj)\n    x_gate = sigmoid(conv_input.dot(W_gate) + b_gate)\n    x_highway = x_gate * x_proj + (1 - x_gate) * conv_input\n\n    np.save(\'sanity_check_handmade_data/highway_conv_input.npy\', conv_input)\n    np.save(\'sanity_check_handmade_data/highway_output.npy\', x_highway)\n\n\ndef question_1h_sanity_check(highway):\n    """"""\n    Sanity check for highway.py, basic shape check and forward pass check\n    """"""\n    reinitialize_layers(highway) #\n    # print(""Running shape check"")\n\n    inpt = torch.from_numpy(np.load(\'sanity_check_handmade_data/highway_conv_input.npy\').astype(np.float32))\n    outp_expected = torch.from_numpy(np.load(\'sanity_check_handmade_data/highway_output.npy\').astype(np.float32))\n\n    with torch.no_grad():\n        outp = highway(inpt)\n\n    outp_expected_size = (BATCH_SIZE, EMBED_SIZE)\n    assert (outp.numpy().shape == outp_expected_size), \\\n      ""Highway output shape is incorrect it should be:\\n{} but is:\\n{}"".format(outp.numpy().shape, outp_expected_size)\n    assert (np.allclose(outp.numpy(), outp_expected.numpy())), \\\n        ""Highway output is incorrect: it should be:\\n {} but is:\\n{}"".format(outp_expected, outp)\n    # print(""Passed all tests :D"")\n\ndef question_1g_generate_data():\n    # pytorch uses cross relation, not convolution for 1d conv\n\n    pass\n\n\ndef question_1g_sanity_check(CNN):\n\n    pass\n\ndef main():\n    """""" Main func.\n    """"""\n    args = docopt(__doc__)\n    seed = 1234\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed * 13 // 7)\n\n    vocab = Vocab.load(\'./sanity_check_en_es_data/vocab_sanity_check.json\')\n\n    # Create NMT Model\n    model = NMT(\n        embed_size=EMBED_SIZE,\n        hidden_size=HIDDEN_SIZE,\n        dropout_rate=DROPOUT_RATE,\n        vocab=vocab)\n\n    char_vocab = DummyVocab()\n\n    # Initialize Highway\n    highway = Highway(\n        word_embed_size=EMBED_SIZE,\n        dropout_rate=DROPOUT_RATE\n    )\n\n    # Initialize CharDecoder\n    decoder = CharDecoder(\n        hidden_size=HIDDEN_SIZE,\n        char_embedding_size=EMBED_SIZE,\n        target_vocab=char_vocab)\n\n    cnn = CNN(\n        char_embed_size=EMBED_SIZE,\n        num_filters=NUM_FILTER,\n        max_word_length=MAX_WORD_LEN,\n        kernel_size=KERNEl_SIZE\n    )\n\n    if args[\'highway\']:\n        question_1h_sanity_check(highway)\n    elif args[\'cnn\']:\n        question_1g_sanity_check(cnn)\n    elif args[\'generate\']:\n        question_1h_generate_data()\n        question_1g_generate_data()\n    else:\n        raise RuntimeError(\'invalid run mode\')\n\nif __name__ == \'__main__\':\n    main()'"
Assignment/a5-v1.2/utils.py,2,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCS224N 2018-19: Homework 5\nnmt.py: NMT Model\nPencheng Yin <pcyin@cs.cmu.edu>\nSahil Chopra <schopra8@stanford.edu>\n""""""\n\nimport math\nfrom typing import List\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef pad_sents_char(sents, char_pad_token):\n    """""" Pad list of sentences according to the longest sentence in the batch and max_word_length.\n    @param sents (list[list[list[int]]]): list of sentences, result of `words2charindices()` \n        from `vocab.py`\n    @param char_pad_token (int): index of the character-padding token\n    @returns sents_padded (list[list[list[int]]]): list of sentences where sentences/words shorter\n        than the max length sentence/word are padded out with the appropriate pad token, such that\n        each sentence in the batch now has same number of words and each word has an equal \n        number of characters\n        Output shape: (batch_size, max_sentence_length, max_word_length)\n    """"""\n    # Words longer than 21 characters should be truncated\n    max_word_length = 21 \n    max_sentence_length = max([len(s) for s in sents]) # len\n    ### YOUR CODE HERE for part 1f\n    ### TODO:\n    ###     Perform necessary padding to the sentences in the batch similar to the pad_sents() \n    ###     method below using the padding character from the arguments. You should ensure all \n    ###     sentences have the same number of words and each word has the same number of \n    ###     characters. \n    ###     Set padding words to a `max_word_length` sized vector of padding characters.  \n    ###\n    ###     You should NOT use the method `pad_sents()` below because of the way it handles \n    ###     padding and unknown words.\n    def sent_to_vec(sent):\n        # sent: list[list[int]] list of word, each word is a list of int\n        sent_padded = []\n        for word in sent[:max_sentence_length]:\n            word_padded = word[:max_word_length]\n            word_padded = word_padded + [char_pad_token] * (max_word_length - len(word_padded))\n            sent_padded.append(word_padded)\n        # pad sentence to have same number of word\n        sent_padded += [[char_pad_token] * max_word_length] * (max_sentence_length - len(sent_padded))\n        return sent_padded\n    sents_padded = [sent_to_vec(i) for i in sents]\n\n    return sents_padded\n\n\ndef pad_sents(sents, pad_token):\n    """""" Pad list of sentences according to the longest sentence in the batch.\n    @param sents (list[list[int]]): list of sentences, where each sentence\n                                    is represented as a list of words\n    @param pad_token (int): padding token\n    @returns sents_padded (list[list[int]]): list of sentences where sentences shorter\n        than the max length sentence are padded out with the pad_token, such that\n        each sentences in the batch now has equal length.\n        Output shape: (batch_size, max_sentence_length)\n    """"""\n    sents_padded = []\n\n    max_len = max(len(s) for s in sents)\n    batch_size = len(sents)\n\n    for s in sents:\n        padded = [pad_token] * max_len\n        padded[:len(s)] = s\n        sents_padded.append(padded)\n\n    return sents_padded\n\n\n\ndef read_corpus(file_path, source):\n    """""" Read file, where each sentence is dilineated by a `\\n`.\n    @param file_path (str): path to file containing corpus\n    @param source (str): ""tgt"" or ""src"" indicating whether text\n        is of the source language or target language\n    """"""\n    data = []\n    for line in open(file_path):\n        sent = line.strip().split(\' \')\n        # only append <s> and </s> to the target sentence\n        if source == \'tgt\':\n            sent = [\'<s>\'] + sent + [\'</s>\']\n        data.append(sent)\n\n    return data\n\n\ndef batch_iter(data, batch_size, shuffle=False):\n    """""" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n    @param batch_size (int): batch size\n    @param shuffle (boolean): whether to randomly shuffle the dataset\n    """"""\n    batch_num = math.ceil(len(data) / batch_size)\n    index_array = list(range(len(data)))\n\n    if shuffle:\n        np.random.shuffle(index_array)\n\n    for i in range(batch_num):\n        indices = index_array[i * batch_size: (i + 1) * batch_size]\n        examples = [data[idx] for idx in indices]\n\n        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n        src_sents = [e[0] for e in examples]\n        tgt_sents = [e[1] for e in examples]\n\n        yield src_sents, tgt_sents\n\n'"
Assignment/a5-v1.2/vocab.py,5,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""\nCS224N 2018-19: Homework 5\nvocab.py: Vocabulary Generation\nPencheng Yin <pcyin@cs.cmu.edu>\nSahil Chopra <schopra8@stanford.edu>\n\nUsage:\n    vocab.py --train-src=<file> --train-tgt=<file> [options] VOCAB_FILE\n\nOptions:\n    -h --help                  Show this screen.\n    --train-src=<file>         File of training source sentences\n    --train-tgt=<file>         File of training target sentences\n    --size=<int>               vocab size [default: 50000]\n    --freq-cutoff=<int>        frequency cutoff [default: 2]\n""""""\n\nfrom collections import Counter\nfrom docopt import docopt\nfrom itertools import chain\nimport json\nimport torch\nfrom typing import List\nfrom utils import read_corpus, pad_sents, pad_sents_char\n\nclass VocabEntry(object):\n    """""" Vocabulary Entry, i.e. structure containing either\n    src or tgt language terms.\n    """"""\n    def __init__(self, word2id=None):\n        """""" Init VocabEntry Instance.\n        @param word2id (dict): dictionary mapping words 2 indices\n        """"""\n        if word2id:\n            self.word2id = word2id\n        else:\n            self.word2id = dict()\n            self.word2id[\'<pad>\'] = 0   # Pad Token\n            self.word2id[\'<s>\'] = 1 # Start Token\n            self.word2id[\'</s>\'] = 2    # End Token\n            self.word2id[\'<unk>\'] = 3   # Unknown Token\n        self.unk_id = self.word2id[\'<unk>\']\n        self.id2word = {v: k for k, v in self.word2id.items()}\n        \n        ## Additions to the A4 code:\n        self.char_list = list(""""""ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789,;.!?:\'\\""/\\\\|_@#$%^&*~`+-=<>()[]"""""")\n\n        self.char2id = dict() # Converts characters to integers\n        self.char2id[\'<pad>\'] = 0\n        self.char2id[\'{\'] = 1\n        self.char2id[\'}\'] = 2\n        self.char2id[\'<unk>\'] = 3\n        for i, c in enumerate(self.char_list):\n            self.char2id[c] = len(self.char2id)\n        self.char_unk = self.char2id[\'<unk>\']\n        self.start_of_word = self.char2id[""{""]\n        self.end_of_word = self.char2id[""}""]\n        assert self.start_of_word+1 == self.end_of_word\n\n        self.id2char = {v: k for k, v in self.char2id.items()} # Converts integers to characters\n        ## End additions to the A4 code\n\n    def __getitem__(self, word):\n        """""" Retrieve word\'s index. Return the index for the unk\n        token if the word is out of vocabulary.\n        @param word (str): word to look up.\n        @returns index (int): index of word \n        """"""\n        return self.word2id.get(word, self.unk_id)\n\n    def __contains__(self, word):\n        """""" Check if word is captured by VocabEntry.\n        @param word (str): word to look up\n        @returns contains (bool): whether word is contained    \n        """"""\n        return word in self.word2id\n\n    def __setitem__(self, key, value):\n        """""" Raise error, if one tries to edit the VocabEntry.\n        """"""\n        raise ValueError(\'vocabulary is readonly\')\n\n    def __len__(self):\n        """""" Compute number of words in VocabEntry.\n        @returns len (int): number of words in VocabEntry\n        """"""\n        return len(self.word2id)\n\n    def __repr__(self):\n        """""" Representation of VocabEntry to be used\n        when printing the object.\n        """"""\n        return \'Vocabulary[size=%d]\' % len(self)\n\n    def id2word(self, wid):\n        """""" Return mapping of index to word.\n        @param wid (int): word index\n        @returns word (str): word corresponding to index\n        """"""\n        return self.id2word[wid]\n\n    def add(self, word):\n        """""" Add word to VocabEntry, if it is previously unseen.\n        @param word (str): word to add to VocabEntry\n        @return index (int): index that the word has been assigned\n        """"""\n        if word not in self:\n            wid = self.word2id[word] = len(self)\n            self.id2word[wid] = word\n            return wid\n        else:\n            return self[word]\n\n    def words2charindices(self, sents):\n        """""" Convert list of sentences of words into list of list of list of character indices.\n        @param sents (list[list[str]]): sentence(s) in words\n        @return word_ids (list[list[list[int]]]): sentence(s) in indices\n        """"""\n        ### YOUR CODE HERE for part 1e\n        ### TODO: \n        ###     This method should convert characters in the input sentences into their \n        ###     corresponding character indices using the character vocabulary char2id \n        ###     defined above.\n        ###\n        ###     You must prepend each word with the `start_of_word` character and append \n        ###     with the `end_of_word` character.\n        sents_word_ids = []\n        for sent in sents:\n            char_sent_idx = []\n            for word in sent:\n                char_word_idx = [self.start_of_word] + [self.char2id[char] for char in word] + [self.end_of_word]\n                char_sent_idx.append(char_word_idx)\n            sents_word_ids.append(char_sent_idx)\n        return sents_word_ids\n\n\n        ### END YOUR CODE\n\n    def words2indices(self, sents):\n        """""" Convert list of sentences of words into list of list of indices.\n        @param sents (list[list[str]]): sentence(s) in words\n        @return word_ids (list[list[int]]): sentence(s) in indices\n        """"""\n        return [[self[w] for w in s] for s in sents]\n\n    def indices2words(self, word_ids):\n        """""" Convert list of indices into words.\n        @param word_ids (list[int]): list of word ids\n        @return sents (list[str]): list of words\n        """"""\n        return [self.id2word[w_id] for w_id in word_ids]\n\n    def to_input_tensor_char(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n        """""" Convert list of sentences (words) into tensor with necessary padding for \n        shorter sentences.\n\n        @param sents (List[List[str]]): list of sentences (words)\n        @param device: device on which to load the tensor, i.e. CPU or GPU\n\n        @returns sents_var: tensor of (max_sentence_length, batch_size, max_word_length)\n        """"""\n        ### YOUR CODE HERE for part 1g\n        ### TODO: \n        ###     Connect `words2charindices()` and `pad_sents_char()` which you\'ve defined in \n        ###     previous parts\n\n        indices   = pad_sents_char(self.words2charindices(sents), self[\'<pad>\']) # (batch_size, max_sentence_length, max_word_length)\n        sents_var = torch.LongTensor(indices).permute(1, 0, 2).to(device) # (max_sent_length, batch_size, max_word_len)\n        return sents_var\n        ### END YOUR CODE\n\n    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n        """""" Convert list of sentences (words) into tensor with necessary padding for \n        shorter sentences.\n\n        @param sents (List[List[str]]): list of sentences (words)\n        @param device: device on which to load the tesnor, i.e. CPU or GPU\n\n        @returns sents_var: tensor of (max_sentence_length, batch_size)\n        """"""\n        word_ids = self.words2indices(sents)\n        sents_t = pad_sents(word_ids, self[\'<pad>\'])\n        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n        return torch.t(sents_var)\n\n    @staticmethod\n    def from_corpus(corpus, size, freq_cutoff=2):\n        """""" Given a corpus construct a Vocab Entry.\n        @param corpus (list[str]): corpus of text produced by read_corpus function\n        @param size (int): # of words in vocabulary\n        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n        """"""\n        vocab_entry = VocabEntry()\n        word_freq = Counter(chain(*corpus))\n        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n        print(\'number of word types: {}, number of word types w/ frequency >= {}: {}\'\n              .format(len(word_freq), freq_cutoff, len(valid_words)))\n        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n        for word in top_k_words:\n            vocab_entry.add(word)\n        return vocab_entry\n\n\nclass Vocab(object):\n    """""" Vocab encapsulating src and target langauges.\n    """"""\n    def __init__(self, src_vocab: VocabEntry, tgt_vocab: VocabEntry):\n        """""" Init Vocab.\n        @param src_vocab (VocabEntry): VocabEntry for source language\n        @param tgt_vocab (VocabEntry): VocabEntry for target language\n        """"""\n        self.src = src_vocab\n        self.tgt = tgt_vocab\n\n    @staticmethod\n    def build(src_sents, tgt_sents, vocab_size, freq_cutoff) -> \'Vocab\':\n        """""" Build Vocabulary.\n        @param src_sents (list[str]): Source sentences provided by read_corpus() function\n        @param tgt_sents (list[str]): Target sentences provided by read_corpus() function\n        @param vocab_size (int): Size of vocabulary for both source and target languages\n        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word.\n        """"""\n        assert len(src_sents) == len(tgt_sents)\n\n        print(\'initialize source vocabulary ..\')\n        src = VocabEntry.from_corpus(src_sents, vocab_size, freq_cutoff)\n\n        print(\'initialize target vocabulary ..\')\n        tgt = VocabEntry.from_corpus(tgt_sents, vocab_size, freq_cutoff)\n\n        return Vocab(src, tgt)\n\n    def save(self, file_path):\n        """""" Save Vocab to file as JSON dump.\n        @param file_path (str): file path to vocab file\n        """"""\n        json.dump(dict(src_word2id=self.src.word2id, tgt_word2id=self.tgt.word2id), open(file_path, \'w\'), indent=2)\n\n    @staticmethod\n    def load(file_path):\n        """""" Load vocabulary from JSON dump.\n        @param file_path (str): file path to vocab file\n        @returns Vocab object loaded from JSON dump\n        """"""\n        entry = json.load(open(file_path, \'r\'))\n        src_word2id = entry[\'src_word2id\']\n        tgt_word2id = entry[\'tgt_word2id\']\n\n        return Vocab(VocabEntry(src_word2id), VocabEntry(tgt_word2id))\n\n    def __repr__(self):\n        """""" Representation of Vocab to be used\n        when printing the object.\n        """"""\n        return \'Vocab(source %d words, target %d words)\' % (len(self.src), len(self.tgt))\n\n\n\nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n\n    print(\'read in source sentences: %s\' % args[\'--train-src\'])\n    print(\'read in target sentences: %s\' % args[\'--train-tgt\'])\n\n    src_sents = read_corpus(args[\'--train-src\'], source=\'src\')\n    tgt_sents = read_corpus(args[\'--train-tgt\'], source=\'tgt\')\n\n    vocab = Vocab.build(src_sents, tgt_sents, int(args[\'--size\']), int(args[\'--freq-cutoff\']))\n    print(\'generated vocabulary, source %d words, target %d words\' % (len(vocab.src), len(vocab.tgt)))\n\n    vocab.save(args[\'VOCAB_FILE\'])\n    print(\'vocabulary saved to %s\' % args[\'VOCAB_FILE\'])\n'"
Assignment/a2/utils/__init__.py,0,b''
Assignment/a2/utils/gradcheck.py,0,"b'#!/usr/bin/env python\n\nimport numpy as np\nimport random\n\n\n# First implement a gradient checker by filling in the following functions\ndef gradcheck_naive(f, x, gradientText):\n    """""" Gradient check for a function f.\n    Arguments:\n    f -- a function that takes a single argument and outputs the\n         loss and its gradients\n    x -- the point (numpy array) to check the gradient at\n    gradientText -- a string detailing some context about the gradient computation\n    """"""\n\n    rndstate = random.getstate()\n    random.setstate(rndstate)\n    fx, grad = f(x) # Evaluate function value at original point\n    h = 1e-4        # Do not change this!\n\n    # Iterate over all indexes ix in x to check the gradient.\n    it = np.nditer(x, flags=[\'multi_index\'], op_flags=[\'readwrite\'])\n    while not it.finished:\n        ix = it.multi_index\n\n        x[ix] += h # increment by h\n        random.setstate(rndstate)\n        fxh, _ = f(x) # evalute f(x + h)\n        x[ix] -= 2 * h # restore to previous value (very important!)\n        random.setstate(rndstate)\n        fxnh, _ = f(x)\n        x[ix] += h\n        numgrad = (fxh - fxnh) / 2 / h\n\n        # Compare gradients\n        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n        if reldiff > 1e-5:\n            print(""Gradient check failed for %s."" % gradientText)\n            print(""First gradient error found at index %s in the vector of gradients"" % str(ix))\n            print(""Your gradient: %f \\t Numerical gradient: %f"" % (\n                grad[ix], numgrad))\n            return\n\n        it.iternext() # Step to next dimension\n\n    print(""Gradient check passed!"")\n'"
Assignment/a2/utils/treebank.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport pickle\nimport numpy as np\nimport os\nimport random\n\nclass StanfordSentiment:\n    def __init__(self, path=None, tablesize = 1000000):\n        if not path:\n            path = ""utils/datasets/stanfordSentimentTreebank""\n\n        self.path = path\n        self.tablesize = tablesize\n\n    def tokens(self):\n        if hasattr(self, ""_tokens"") and self._tokens:\n            return self._tokens\n\n        tokens = dict()\n        tokenfreq = dict()\n        wordcount = 0\n        revtokens = []\n        idx = 0\n\n        for sentence in self.sentences():\n            for w in sentence:\n                wordcount += 1\n                if not w in tokens:\n                    tokens[w] = idx\n                    revtokens += [w]\n                    tokenfreq[w] = 1\n                    idx += 1\n                else:\n                    tokenfreq[w] += 1\n\n        tokens[""UNK""] = idx\n        revtokens += [""UNK""]\n        tokenfreq[""UNK""] = 1\n        wordcount += 1\n\n        self._tokens = tokens\n        self._tokenfreq = tokenfreq\n        self._wordcount = wordcount\n        self._revtokens = revtokens\n        return self._tokens\n\n    def sentences(self):\n        if hasattr(self, ""_sentences"") and self._sentences:\n            return self._sentences\n\n        sentences = []\n        with open(self.path + ""/datasetSentences.txt"", ""r"") as f:\n            first = True\n            for line in f:\n                if first:\n                    first = False\n                    continue\n\n                splitted = line.strip().split()[1:]\n                # Deal with some peculiar encoding issues with this file\n                sentences += [[w.lower() for w in splitted]]\n\n        self._sentences = sentences\n        self._sentlengths = np.array([len(s) for s in sentences])\n        self._cumsentlen = np.cumsum(self._sentlengths)\n\n        return self._sentences\n\n    def numSentences(self):\n        if hasattr(self, ""_numSentences"") and self._numSentences:\n            return self._numSentences\n        else:\n            self._numSentences = len(self.sentences())\n            return self._numSentences\n\n    def allSentences(self):\n        if hasattr(self, ""_allsentences"") and self._allsentences:\n            return self._allsentences\n\n        sentences = self.sentences()\n        rejectProb = self.rejectProb()\n        tokens = self.tokens()\n        allsentences = [[w for w in s\n            if 0 >= rejectProb[tokens[w]] or random.random() >= rejectProb[tokens[w]]]\n            for s in sentences * 30]\n\n        allsentences = [s for s in allsentences if len(s) > 1]\n\n        self._allsentences = allsentences\n\n        return self._allsentences\n\n    def getRandomContext(self, C=5):\n        allsent = self.allSentences()\n        sentID = random.randint(0, len(allsent) - 1)\n        sent = allsent[sentID]\n        wordID = random.randint(0, len(sent) - 1)\n\n        context = sent[max(0, wordID - C):wordID]\n        if wordID+1 < len(sent):\n            context += sent[wordID+1:min(len(sent), wordID + C + 1)]\n\n        centerword = sent[wordID]\n        context = [w for w in context if w != centerword]\n\n        if len(context) > 0:\n            return centerword, context\n        else:\n            return self.getRandomContext(C)\n\n    def sent_labels(self):\n        if hasattr(self, ""_sent_labels"") and self._sent_labels:\n            return self._sent_labels\n\n        dictionary = dict()\n        phrases = 0\n        with open(self.path + ""/dictionary.txt"", ""r"") as f:\n            for line in f:\n                line = line.strip()\n                if not line: continue\n                splitted = line.split(""|"")\n                dictionary[splitted[0].lower()] = int(splitted[1])\n                phrases += 1\n\n        labels = [0.0] * phrases\n        with open(self.path + ""/sentiment_labels.txt"", ""r"") as f:\n            first = True\n            for line in f:\n                if first:\n                    first = False\n                    continue\n\n                line = line.strip()\n                if not line: continue\n                splitted = line.split(""|"")\n                labels[int(splitted[0])] = float(splitted[1])\n\n        sent_labels = [0.0] * self.numSentences()\n        sentences = self.sentences()\n        for i in range(self.numSentences()):\n            sentence = sentences[i]\n            full_sent = "" "".join(sentence).replace(\'-lrb-\', \'(\').replace(\'-rrb-\', \')\')\n            sent_labels[i] = labels[dictionary[full_sent]]\n\n        self._sent_labels = sent_labels\n        return self._sent_labels\n\n    def dataset_split(self):\n        if hasattr(self, ""_split"") and self._split:\n            return self._split\n\n        split = [[] for i in range(3)]\n        with open(self.path + ""/datasetSplit.txt"", ""r"") as f:\n            first = True\n            for line in f:\n                if first:\n                    first = False\n                    continue\n\n                splitted = line.strip().split("","")\n                split[int(splitted[1]) - 1] += [int(splitted[0]) - 1]\n\n        self._split = split\n        return self._split\n\n    def getRandomTrainSentence(self):\n        split = self.dataset_split()\n        sentId = split[0][random.randint(0, len(split[0]) - 1)]\n        return self.sentences()[sentId], self.categorify(self.sent_labels()[sentId])\n\n    def categorify(self, label):\n        if label <= 0.2:\n            return 0\n        elif label <= 0.4:\n            return 1\n        elif label <= 0.6:\n            return 2\n        elif label <= 0.8:\n            return 3\n        else:\n            return 4\n\n    def getDevSentences(self):\n        return self.getSplitSentences(2)\n\n    def getTestSentences(self):\n        return self.getSplitSentences(1)\n\n    def getTrainSentences(self):\n        return self.getSplitSentences(0)\n\n    def getSplitSentences(self, split=0):\n        ds_split = self.dataset_split()\n        return [(self.sentences()[i], self.categorify(self.sent_labels()[i])) for i in ds_split[split]]\n\n    def sampleTable(self):\n        if hasattr(self, \'_sampleTable\') and self._sampleTable is not None:\n            return self._sampleTable\n\n        nTokens = len(self.tokens())\n        samplingFreq = np.zeros((nTokens,))\n        self.allSentences()\n        i = 0\n        for w in range(nTokens):\n            w = self._revtokens[i]\n            if w in self._tokenfreq:\n                freq = 1.0 * self._tokenfreq[w]\n                # Reweigh\n                freq = freq ** 0.75\n            else:\n                freq = 0.0\n            samplingFreq[i] = freq\n            i += 1\n\n        samplingFreq /= np.sum(samplingFreq)\n        samplingFreq = np.cumsum(samplingFreq) * self.tablesize\n\n        self._sampleTable = [0] * self.tablesize\n\n        j = 0\n        for i in range(self.tablesize):\n            while i > samplingFreq[j]:\n                j += 1\n            self._sampleTable[i] = j\n\n        return self._sampleTable\n\n    def rejectProb(self):\n        if hasattr(self, \'_rejectProb\') and self._rejectProb is not None:\n            return self._rejectProb\n\n        threshold = 1e-5 * self._wordcount\n\n        nTokens = len(self.tokens())\n        rejectProb = np.zeros((nTokens,))\n        for i in range(nTokens):\n            w = self._revtokens[i]\n            freq = 1.0 * self._tokenfreq[w]\n            # Reweigh\n            rejectProb[i] = max(0, 1 - np.sqrt(threshold / freq))\n\n        self._rejectProb = rejectProb\n        return self._rejectProb\n\n    def sampleTokenIdx(self):\n        return self.sampleTable()[random.randint(0, self.tablesize - 1)]'"
Assignment/a2/utils/utils.py,0,"b'#!/usr/bin/env python\n\nimport numpy as np\n\ndef normalizeRows(x):\n    """""" Row normalization function\n\n    Implement a function that normalizes each row of a matrix to have\n    unit length.\n    """"""\n    N = x.shape[0]\n    x /= np.sqrt(np.sum(x**2, axis=1)).reshape((N,1)) + 1e-30\n    return x\n\ndef softmax(x):\n    """"""Compute the softmax function for each row of the input x.\n    It is crucial that this function is optimized for speed because\n    it will be used frequently in later code. \n\n    Arguments:\n    x -- A D dimensional vector or N x D dimensional numpy matrix.\n    Return:\n    x -- You are allowed to modify x in-place\n    """"""\n    orig_shape = x.shape\n\n    if len(x.shape) > 1:\n        # Matrix\n        tmp = np.max(x, axis=1)\n        x -= tmp.reshape((x.shape[0], 1))\n        x = np.exp(x)\n        tmp = np.sum(x, axis=1)\n        x /= tmp.reshape((x.shape[0], 1))\n    else:\n        # Vector\n        tmp = np.max(x)\n        x -= tmp\n        x = np.exp(x)\n        tmp = np.sum(x)\n        x /= tmp\n\n    assert x.shape == orig_shape\n    return x'"
Assignment/a3/utils/__init__.py,0,b''
Assignment/a3/utils/general_utils.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCS224N 2018-19: Homework 3\ngeneral_utils.py: General purpose utilities.\nSahil Chopra <schopra8@stanford.edu>\n""""""\n\nimport sys\nimport time\nimport numpy as np\n\n\ndef get_minibatches(data, minibatch_size, shuffle=True):\n    """"""\n    Iterates through the provided data one minibatch at at time. You can use this function to\n    iterate through data in minibatches as follows:\n\n        for inputs_minibatch in get_minibatches(inputs, minibatch_size):\n            ...\n\n    Or with multiple data sources:\n\n        for inputs_minibatch, labels_minibatch in get_minibatches([inputs, labels], minibatch_size):\n            ...\n\n    Args:\n        data: there are two possible values:\n            - a list or numpy array\n            - a list where each element is either a list or numpy array\n        minibatch_size: the maximum number of items in a minibatch\n        shuffle: whether to randomize the order of returned data\n    Returns:\n        minibatches: the return value depends on data:\n            - If data is a list/array it yields the next minibatch of data.\n            - If data a list of lists/arrays it returns the next minibatch of each element in the\n              list. This can be used to iterate through multiple data sources\n              (e.g., features and labels) at the same time.\n\n    """"""\n    list_data = type(data) is list and (type(data[0]) is list or type(data[0]) is np.ndarray)\n    data_size = len(data[0]) if list_data else len(data)\n    indices = np.arange(data_size)\n    if shuffle:\n        np.random.shuffle(indices)\n    for minibatch_start in np.arange(0, data_size, minibatch_size):\n        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n        yield [_minibatch(d, minibatch_indices) for d in data] if list_data \\\n            else _minibatch(data, minibatch_indices)\n\n\ndef _minibatch(data, minibatch_idx):\n    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n\n\ndef test_all_close(name, actual, expected):\n    if actual.shape != expected.shape:\n        raise ValueError(""{:} failed, expected output to have shape {:} but has shape {:}""\n                         .format(name, expected.shape, actual.shape))\n    if np.amax(np.fabs(actual - expected)) > 1e-6:\n        raise ValueError(""{:} failed, expected {:} but value is {:}"".format(name, expected, actual))\n    else:\n        print(name, ""passed!"")\n'"
Assignment/a3/utils/parser_utils.py,1,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCS224N 2018-19: Homework 3\nparser_utils.py: Utilities for training the dependency parser.\nSahil Chopra <schopra8@stanford.edu>\n""""""\n\nimport time\nimport os\nimport logging\nfrom collections import Counter\nfrom . general_utils import get_minibatches\nfrom parser_transitions import minibatch_parse\n\nfrom tqdm import tqdm\nimport torch\nimport numpy as np\n\nP_PREFIX = \'<p>:\'\nL_PREFIX = \'<l>:\'\nUNK = \'<UNK>\'\nNULL = \'<NULL>\'\nROOT = \'<ROOT>\'\n\n\nclass Config(object):\n    language = \'english\'\n    with_punct = True\n    unlabeled = True\n    lowercase = True\n    use_pos = True\n    use_dep = True\n    use_dep = use_dep and (not unlabeled)\n    data_path = \'./data\'\n    train_file = \'train.conll\'\n    dev_file = \'dev.conll\'\n    test_file = \'test.conll\'\n    embedding_file = \'./data/en-cw.txt\'\n\n\nclass Parser(object):\n    """"""Contains everything needed for transition-based dependency parsing except for the model""""""\n\n    def __init__(self, dataset):\n        # read all label (dependency) with head = 0 : root_label\n        root_labels = list([l for ex in dataset\n                           for (h, l) in zip(ex[\'head\'], ex[\'label\']) if h == 0])\n        counter = Counter(root_labels)\n        if len(counter) > 1:\n            logging.info(\'Warning: more than one root label\')\n            logging.info(counter)\n        self.root_label = counter.most_common()[0][0]\n        # dependencies\n        deprel = [self.root_label] + list(set([w for ex in dataset\n                                               for w in ex[\'label\']\n                                               if w != self.root_label]))\n        tok2id = {L_PREFIX + l: i for (i, l) in enumerate(deprel)} # token to index of dependency relation\n        tok2id[L_PREFIX + NULL] = self.L_NULL = len(tok2id)\n\n        config = Config()\n        self.unlabeled = config.unlabeled\n        self.with_punct = config.with_punct\n        self.use_pos = config.use_pos\n        self.use_dep = config.use_dep\n        self.language = config.language\n\n        if self.unlabeled:\n            trans = [\'L\', \'R\', \'S\']\n            self.n_deprel = 1\n        else:\n            trans = [\'L-\' + l for l in deprel] + [\'R-\' + l for l in deprel] + [\'S\']\n            self.n_deprel = len(deprel)\n\n        self.n_trans = len(trans)\n        self.tran2id = {t: i for (i, t) in enumerate(trans)}\n        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n\n        # logging.info(\'Build dictionary for part-of-speech tags.\')\n        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex[\'pos\']],\n                                  offset=len(tok2id)))\n        # Add 3 new POS tags : UNK, NULL, ROOT\n        tok2id[P_PREFIX + UNK] = self.P_UNK = len(tok2id)\n        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n\n        # logging.info(\'Build dictionary for words.\')\n        tok2id.update(build_dict([w for ex in dataset for w in ex[\'word\']],\n                                  offset=len(tok2id)))\n        # Add 3 new word\n        tok2id[UNK] = self.UNK = len(tok2id)\n        tok2id[NULL] = self.NULL = len(tok2id)\n        tok2id[ROOT] = self.ROOT = len(tok2id)\n\n        self.tok2id = tok2id # a dictionary with dependency relation, POS_tag and word\n        self.id2tok = {v: k for (k, v) in tok2id.items()}\n\n        ## 18 features = 6 + 8 + 4 (if no word, insert NULL)\n        ##      - top 3 words on the stack and buffer (6)\n        ##      - first and second leftmost / right most children of the top two words on the stack (4 children * 2 words = 8 )\n        ##      - The left most of left most / right most of right most children of the top two words on the stack ( 2 children (ll, rr) * 2 words = 4 )\n\n        self.n_features = 18 + (18 if config.use_pos else 0) + (12 if config.use_dep else 0)\n        # if take into account pos tag, we have 18 more features of POS tag for 18 words above\n        # same for dependency relation\n\n        self.n_tokens = len(tok2id)\n\n    def vectorize(self, examples):\n        vec_examples = []\n        for ex in examples:\n            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n                                  else self.UNK for w in ex[\'word\']]\n            pos = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n                                   else self.P_UNK for w in ex[\'pos\']]\n            head = [-1] + ex[\'head\']\n            label = [-1] + [self.tok2id[L_PREFIX + w] if L_PREFIX + w in self.tok2id\n                            else -1 for w in ex[\'label\']]\n            vec_examples.append({\'word\': word, \'pos\': pos,\n                                 \'head\': head, \'label\': label})\n        return vec_examples\n\n    def extract_features(self, stack, buf, arcs, ex):\n        if stack[0] == ""ROOT"":\n            stack[0] = 0\n\n        def get_lc(k):\n            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n\n        def get_rc(k):\n            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n                          reverse=True)\n\n        p_features = []\n        l_features = []\n        features = [self.NULL] * (3 - len(stack)) + [ex[\'word\'][x] for x in stack[-3:]]\n        features += [ex[\'word\'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n        if self.use_pos:\n            p_features = [self.P_NULL] * (3 - len(stack)) + [ex[\'pos\'][x] for x in stack[-3:]]\n            p_features += [ex[\'pos\'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n\n        for i in range(2):\n            if i < len(stack):\n                k = stack[-i-1]\n                lc = get_lc(k)\n                rc = get_rc(k)\n                llc = get_lc(lc[0]) if len(lc) > 0 else []\n                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n\n                features.append(ex[\'word\'][lc[0]] if len(lc) > 0 else self.NULL)\n                features.append(ex[\'word\'][rc[0]] if len(rc) > 0 else self.NULL)\n                features.append(ex[\'word\'][lc[1]] if len(lc) > 1 else self.NULL)\n                features.append(ex[\'word\'][rc[1]] if len(rc) > 1 else self.NULL)\n                features.append(ex[\'word\'][llc[0]] if len(llc) > 0 else self.NULL)\n                features.append(ex[\'word\'][rrc[0]] if len(rrc) > 0 else self.NULL)\n\n                if self.use_pos:\n                    p_features.append(ex[\'pos\'][lc[0]] if len(lc) > 0 else self.P_NULL)\n                    p_features.append(ex[\'pos\'][rc[0]] if len(rc) > 0 else self.P_NULL)\n                    p_features.append(ex[\'pos\'][lc[1]] if len(lc) > 1 else self.P_NULL)\n                    p_features.append(ex[\'pos\'][rc[1]] if len(rc) > 1 else self.P_NULL)\n                    p_features.append(ex[\'pos\'][llc[0]] if len(llc) > 0 else self.P_NULL)\n                    p_features.append(ex[\'pos\'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n\n                if self.use_dep:\n                    l_features.append(ex[\'label\'][lc[0]] if len(lc) > 0 else self.L_NULL)\n                    l_features.append(ex[\'label\'][rc[0]] if len(rc) > 0 else self.L_NULL)\n                    l_features.append(ex[\'label\'][lc[1]] if len(lc) > 1 else self.L_NULL)\n                    l_features.append(ex[\'label\'][rc[1]] if len(rc) > 1 else self.L_NULL)\n                    l_features.append(ex[\'label\'][llc[0]] if len(llc) > 0 else self.L_NULL)\n                    l_features.append(ex[\'label\'][rrc[0]] if len(rrc) > 0 else self.L_NULL)\n            else:\n                features += [self.NULL] * 6\n                if self.use_pos:\n                    p_features += [self.P_NULL] * 6\n                if self.use_dep:\n                    l_features += [self.L_NULL] * 6\n\n        features += p_features + l_features\n        assert len(features) == self.n_features\n        return features\n\n    def get_oracle(self, stack, buf, ex):\n        if len(stack) < 2:\n            return self.n_trans - 1\n\n        i0 = stack[-1]\n        i1 = stack[-2]\n        h0 = ex[\'head\'][i0]\n        h1 = ex[\'head\'][i1]\n        l0 = ex[\'label\'][i0]\n        l1 = ex[\'label\'][i1]\n\n        if self.unlabeled:\n            if (i1 > 0) and (h1 == i0):\n                return 0\n            elif (i1 >= 0) and (h0 == i1) and \\\n                 (not any([x for x in buf if ex[\'head\'][x] == i0])):\n                return 1\n            else:\n                return None if len(buf) == 0 else 2\n        else:\n            if (i1 > 0) and (h1 == i0):\n                return l1 if (l1 >= 0) and (l1 < self.n_deprel) else None\n            elif (i1 >= 0) and (h0 == i1) and \\\n                 (not any([x for x in buf if ex[\'head\'][x] == i0])):\n                return l0 + self.n_deprel if (l0 >= 0) and (l0 < self.n_deprel) else None\n            else:\n                return None if len(buf) == 0 else self.n_trans - 1\n\n    def create_instances(self, examples):\n        all_instances = []\n        succ = 0\n        for id, ex in enumerate(examples):\n            n_words = len(ex[\'word\']) - 1\n\n            # arcs = {(h, t, label)}\n            stack = [0]\n            buf = [i + 1 for i in range(n_words)]\n            arcs = []\n            instances = []\n            for i in range(n_words * 2):\n                gold_t = self.get_oracle(stack, buf, ex)\n                if gold_t is None:\n                    break\n                legal_labels = self.legal_labels(stack, buf)\n                assert legal_labels[gold_t] == 1\n                instances.append((self.extract_features(stack, buf, arcs, ex),\n                                  legal_labels, gold_t))\n                if gold_t == self.n_trans - 1:\n                    stack.append(buf[0])\n                    buf = buf[1:]\n                elif gold_t < self.n_deprel:\n                    arcs.append((stack[-1], stack[-2], gold_t))\n                    stack = stack[:-2] + [stack[-1]]\n                else:\n                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n                    stack = stack[:-1]\n            else:\n                succ += 1\n                all_instances += instances\n\n        return all_instances\n\n    def legal_labels(self, stack, buf):\n        labels = ([1] if len(stack) > 2 else [0]) * self.n_deprel\n        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel\n        labels += [1] if len(buf) > 0 else [0]\n        return labels\n\n    def parse(self, dataset, eval_batch_size=5000):\n        sentences = []\n        sentence_id_to_idx = {}\n        for i, example in enumerate(dataset):\n            n_words = len(example[\'word\']) - 1\n            sentence = [j + 1 for j in range(n_words)]\n            sentences.append(sentence)\n            sentence_id_to_idx[id(sentence)] = i\n\n        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n\n        UAS = all_tokens = 0.0\n        with tqdm(total=len(dataset)) as prog:\n            for i, ex in enumerate(dataset):\n                head = [-1] * len(ex[\'word\'])\n                for h, t, in dependencies[i]:\n                    head[t] = h\n                for pred_h, gold_h, gold_l, pos in \\\n                        zip(head[1:], ex[\'head\'][1:], ex[\'label\'][1:], ex[\'pos\'][1:]):\n                        assert self.id2tok[pos].startswith(P_PREFIX)\n                        pos_str = self.id2tok[pos][len(P_PREFIX):]\n                        if (self.with_punct) or (not punct(self.language, pos_str)):\n                            UAS += 1 if pred_h == gold_h else 0\n                            all_tokens += 1\n                prog.update(i + 1)\n        UAS /= all_tokens\n        return UAS, dependencies\n\n\nclass ModelWrapper(object):\n    def __init__(self, parser, dataset, sentence_id_to_idx):\n        self.parser = parser\n        self.dataset = dataset\n        self.sentence_id_to_idx = sentence_id_to_idx\n\n    def predict(self, partial_parses):\n        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dependencies,\n                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n                for p in partial_parses]\n        mb_x = np.array(mb_x).astype(\'int32\')\n        mb_x = torch.from_numpy(mb_x).long()\n        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n\n        pred = self.parser.model(mb_x)\n        pred = pred.detach().numpy()\n        pred = np.argmax(pred + 10000 * np.array(mb_l).astype(\'float32\'), 1)\n        pred = [""S"" if p == 2 else (""LA"" if p == 0 else ""RA"") for p in pred]\n        return pred\n\n\ndef read_conll(in_file, lowercase=False, max_example=None):\n    """""" Read .conll file, return list of dict {word: [list_word in sentence], pos: [list_pos], head : [list_index], label: [list_label]}\n    """"""\n    examples = []\n    with open(in_file) as f:\n        word, pos, head, label = [], [], [], []\n        for line in f.readlines():\n            sp = line.strip().split(\'\\t\')\n            if len(sp) == 10:\n                if \'-\' not in sp[0]:\n                    word.append(sp[1].lower() if lowercase else sp[1])\n                    pos.append(sp[4])\n                    head.append(int(sp[6]))\n                    label.append(sp[7])\n            elif len(word) > 0:\n                examples.append({\'word\': word, \'pos\': pos, \'head\': head, \'label\': label})\n                word, pos, head, label = [], [], [], []\n                if (max_example is not None) and (len(examples) == max_example):\n                    break\n        if len(word) > 0:\n            examples.append({\'word\': word, \'pos\': pos, \'head\': head, \'label\': label})\n    return examples\n\n\ndef build_dict(keys, n_max=None, offset=0):\n    count = Counter()\n    for key in keys:\n        count[key] += 1\n    ls = count.most_common() if n_max is None \\\n        else count.most_common(n_max)\n\n    return {w[0]: index + offset for (index, w) in enumerate(ls)}\n\n\ndef punct(language, pos):\n    if language == \'english\':\n        return pos in [""\'\'"", "","", ""."", "":"", ""``"", ""-LRB-"", ""-RRB-""]\n    elif language == \'chinese\':\n        return pos == \'PU\'\n    elif language == \'french\':\n        return pos == \'PUNC\'\n    elif language == \'german\':\n        return pos in [""$."", ""$,"", ""$[""]\n    elif language == \'spanish\':\n        # http://nlp.stanford.edu/software/spanish-faq.shtml\n        return pos in [""f0"", ""faa"", ""fat"", ""fc"", ""fd"", ""fe"", ""fg"", ""fh"",\n                       ""fia"", ""fit"", ""fp"", ""fpa"", ""fpt"", ""fs"", ""ft"",\n                       ""fx"", ""fz""]\n    elif language == \'universal\':\n        return pos == \'PUNCT\'\n    else:\n        raise ValueError(\'language: %s is not supported.\' % language)\n\n\ndef minibatches(data, batch_size):\n    x = np.array([d[0] for d in data])\n    y = np.array([d[2] for d in data])\n    one_hot = np.zeros((y.size, 3))\n    one_hot[np.arange(y.size), y] = 1\n    return get_minibatches([x, one_hot], batch_size)\n\n\ndef load_and_preprocess_data(reduced=True):\n    config = Config()\n\n    print(""Loading data..."",)\n    start = time.time()\n    train_set = read_conll(os.path.join(config.data_path, config.train_file),\n                           lowercase=config.lowercase)\n    dev_set = read_conll(os.path.join(config.data_path, config.dev_file),\n                         lowercase=config.lowercase)\n    test_set = read_conll(os.path.join(config.data_path, config.test_file),\n                          lowercase=config.lowercase)\n    if reduced:\n        train_set = train_set[:1000]\n        dev_set = dev_set[:500]\n        test_set = test_set[:500]\n    print(""took {:.2f} seconds"".format(time.time() - start))\n\n    print(""Building parser..."",)\n    start = time.time()\n    parser = Parser(train_set)\n    print(""took {:.2f} seconds"".format(time.time() - start))\n\n    print(""Loading pretrained embeddings..."",)\n    start = time.time()\n    word_vectors = {}\n    for line in open(config.embedding_file).readlines():\n        sp = line.strip().split()\n        word_vectors[sp[0]] = [float(x) for x in sp[1:]]\n    embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype=\'float32\')\n\n    for token in parser.tok2id:\n        i = parser.tok2id[token]\n        if token in word_vectors:\n            embeddings_matrix[i] = word_vectors[token]\n        elif token.lower() in word_vectors:\n            embeddings_matrix[i] = word_vectors[token.lower()]\n    print(""took {:.2f} seconds"".format(time.time() - start))\n\n    print(""Vectorizing data..."",)\n    start = time.time()\n    train_set = parser.vectorize(train_set)\n    dev_set = parser.vectorize(dev_set)\n    test_set = parser.vectorize(test_set)\n    print(""took {:.2f} seconds"".format(time.time() - start))\n\n    print(""Preprocessing training data..."",)\n    start = time.time()\n    train_examples = parser.create_instances(train_set)\n    print(""took {:.2f} seconds"".format(time.time() - start))\n\n    return parser, embeddings_matrix, train_examples, dev_set, test_set,\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nif __name__ == \'__main__\':\n    pass\n'"
