file_path,api_count,code
cifar100data.py,1,"b""import matplotlib.pyplot as plt\nimport numpy as np\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\nfrom utils import calc_dataset_stats\n\n\n# Example DataLoader on CIFAR-100\n\nclass CIFAR100Data:\n    def __init__(self, args):\n        mean, std = calc_dataset_stats(torchvision.datasets.CIFAR100(root='./data', train=True,\n                                                                     download=args.download_dataset).train_data,\n                                       axis=(0, 1, 2))\n\n        train_transform = transforms.Compose(\n            [transforms.RandomCrop(args.img_height),\n             transforms.RandomHorizontalFlip(),\n             transforms.ColorJitter(0.3, 0.3, 0.3),\n             transforms.ToTensor(),\n             transforms.Normalize(mean=mean, std=std)])\n\n        test_transform = transforms.Compose(\n            [transforms.ToTensor(),\n             transforms.Normalize(mean=mean, std=std)])\n\n        self.trainloader = DataLoader(torchvision.datasets.CIFAR100(root='./data', train=True,\n                                                                    download=args.download_dataset,\n                                                                    transform=train_transform),\n                                      batch_size=args.batch_size,\n                                      shuffle=args.shuffle, num_workers=args.dataloader_workers,\n                                      pin_memory=args.pin_memory)\n\n        self.testloader = DataLoader(torchvision.datasets.CIFAR100(root='./data', train=False,\n                                                                   download=args.download_dataset,\n                                                                   transform=test_transform),\n                                     batch_size=args.batch_size,\n                                     shuffle=False, num_workers=args.dataloader_workers,\n                                     pin_memory=args.pin_memory)\n\n    def plot_random_sample(self):\n        # Get some random training images\n        dataiter = iter(self.trainloader)\n        images, labels = dataiter.next()\n        print(images[0])\n        exit(1)\n        # Show images\n        grid = torchvision.utils.make_grid(images)\n        img = grid / 2 + 0.5\n        npimg = img.numpy()\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n        plt.show()\n        # Print labels\n        print(' '.join('%5s' % CIFAR100_LABELS_LIST[labels[j]] for j in range(len(labels))))\n\n\nCIFAR100_LABELS_LIST = [\n    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',\n    'worm'\n]\n"""
cifar10data.py,1,"b""import matplotlib.pyplot as plt\nimport numpy as np\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\nfrom utils import calc_dataset_stats\n\n\n# Example DataLoader on CIFAR-10\n\nclass CIFAR10Data:\n    def __init__(self, args):\n        mean, std = calc_dataset_stats(torchvision.datasets.CIFAR10(root='./data', train=True,\n                                                                    download=args.download_dataset).train_data,\n                                       axis=(0, 1, 2))\n\n        train_transform = transforms.Compose(\n            [transforms.RandomCrop(args.img_height),\n             transforms.RandomHorizontalFlip(),\n             transforms.ColorJitter(0.3, 0.3, 0.3),\n             transforms.ToTensor(),\n             transforms.Normalize(mean=mean, std=std)])\n\n        test_transform = transforms.Compose(\n            [transforms.ToTensor(),\n             transforms.Normalize(mean=mean, std=std)])\n\n        self.trainloader = DataLoader(torchvision.datasets.CIFAR10(root='./data', train=True,\n                                                                   download=args.download_dataset,\n                                                                   transform=train_transform),\n                                      batch_size=args.batch_size,\n                                      shuffle=args.shuffle, num_workers=args.dataloader_workers,\n                                      pin_memory=args.pin_memory)\n\n        self.testloader = DataLoader(torchvision.datasets.CIFAR10(root='./data', train=False,\n                                                                  download=args.download_dataset,\n                                                                  transform=test_transform),\n                                     batch_size=args.batch_size,\n                                     shuffle=False, num_workers=args.dataloader_workers,\n                                     pin_memory=args.pin_memory)\n\n\nCIFAR10_LABELS_LIST = [\n    'airplane',\n    'automobile',\n    'bird',\n    'cat',\n    'deer',\n    'dog',\n    'frog',\n    'horse',\n    'ship',\n    'truck'\n]\n"""
layers.py,1,"b'import torch.nn as nn\n\n\nclass InvertedResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, expansion_factor=6, kernel_size=3, stride=2):\n        super(InvertedResidualBlock, self).__init__()\n\n        if stride != 1 and stride != 2:\n            raise ValueError(""Stride should be 1 or 2"")\n\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels * expansion_factor, 1, bias=False),\n            nn.BatchNorm2d(in_channels * expansion_factor),\n            nn.ReLU6(inplace=True),\n\n            nn.Conv2d(in_channels * expansion_factor, in_channels * expansion_factor,\n                      kernel_size, stride, 1,\n                      groups=in_channels * expansion_factor, bias=False),\n            nn.BatchNorm2d(in_channels * expansion_factor),\n            nn.ReLU6(inplace=True),\n\n            nn.Conv2d(in_channels * expansion_factor, out_channels, 1,\n                      bias=False),\n            nn.BatchNorm2d(out_channels))\n\n        self.is_residual = True if stride == 1 else False\n        self.is_conv_res = False if in_channels == out_channels else True\n\n        # Assumption based on previous ResNet papers: If the number of filters doesn\'t match,\n        # there should be a conv1x1 operation.\n        if stride == 1 and self.is_conv_res:\n            self.conv_res = nn.Sequential(nn.Conv2d(in_channels, out_channels, 1, bias=False),\n                                          nn.BatchNorm2d(out_channels))\n\n    def forward(self, x):\n        block = self.block(x)\n        if self.is_residual:\n            if self.is_conv_res:\n                return self.conv_res(x) + block\n            return x + block\n        return block\n\n\ndef inverted_residual_sequence(in_channels, out_channels, num_units, expansion_factor=6,\n                               kernel_size=3,\n                               initial_stride=2):\n    bottleneck_arr = [\n        InvertedResidualBlock(in_channels, out_channels, expansion_factor, kernel_size,\n                              initial_stride)]\n\n    for i in range(num_units - 1):\n        bottleneck_arr.append(\n            InvertedResidualBlock(out_channels, out_channels, expansion_factor, kernel_size, 1))\n\n    return bottleneck_arr\n\n\ndef conv2d_bn_relu6(in_channels, out_channels, kernel_size=3, stride=2, dropout_prob=0.0):\n    # To preserve the equation of padding. (k=1 maps to pad 0, k=3 maps to pad 1, k=5 maps to pad 2, etc.)\n    padding = (kernel_size + 1) // 2 - 1\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n        nn.BatchNorm2d(out_channels),\n        # For efficiency, Dropout is placed before Relu.\n        nn.Dropout2d(dropout_prob, inplace=True),\n        # Assumption: Relu6 is used everywhere.\n        nn.ReLU6(inplace=True)\n    )\n'"
main.py,1,"b'import torch.backends.cudnn as cudnn\n\nfrom cifar10data import CIFAR10Data\nfrom model import MobileNetV2\nfrom train import Train\nfrom utils import parse_args, create_experiment_dirs\n\n\ndef main():\n    # Parse the JSON arguments\n    config_args = parse_args()\n\n    # Create the experiment directories\n    _, config_args.summary_dir, config_args.checkpoint_dir = create_experiment_dirs(\n        config_args.experiment_dir)\n\n    model = MobileNetV2(config_args)\n\n    if config_args.cuda:\n        model.cuda()\n        cudnn.enabled = True\n        cudnn.benchmark = True\n\n    print(""Loading Data..."")\n    data = CIFAR10Data(config_args)\n    print(""Data loaded successfully\\n"")\n\n    trainer = Train(model, data.trainloader, data.testloader, config_args)\n\n    if config_args.to_train:\n        try:\n            print(""Training..."")\n            trainer.train()\n            print(""Training Finished\\n"")\n        except KeyboardInterrupt:\n            pass\n\n    if config_args.to_test:\n        print(""Testing..."")\n        trainer.test(data.testloader)\n        print(""Testing Finished\\n"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
model.py,1,"b'import torch.nn as nn\n\nfrom layers import inverted_residual_sequence, conv2d_bn_relu6\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, args):\n        super(MobileNetV2, self).__init__()\n\n        s1, s2 = 2, 2\n        if args.downsampling == 16:\n            s1, s2 = 2, 1\n        elif args.downsampling == 8:\n            s1, s2 = 1, 1\n\n        # Network is created here, then will be unpacked into nn.sequential\n        self.network_settings = [{\'t\': -1, \'c\': 32, \'n\': 1, \'s\': s1},\n                                 {\'t\': 1, \'c\': 16, \'n\': 1, \'s\': 1},\n                                 {\'t\': 6, \'c\': 24, \'n\': 2, \'s\': s2},\n                                 {\'t\': 6, \'c\': 32, \'n\': 3, \'s\': 2},\n                                 {\'t\': 6, \'c\': 64, \'n\': 4, \'s\': 2},\n                                 {\'t\': 6, \'c\': 96, \'n\': 3, \'s\': 1},\n                                 {\'t\': 6, \'c\': 160, \'n\': 3, \'s\': 2},\n                                 {\'t\': 6, \'c\': 320, \'n\': 1, \'s\': 1},\n                                 {\'t\': None, \'c\': 1280, \'n\': 1, \'s\': 1}]\n        self.num_classes = args.num_classes\n\n        ###############################################################################################################\n\n        # Feature Extraction part\n        # Layer 0\n        self.network = [\n            conv2d_bn_relu6(args.num_channels,\n                            int(self.network_settings[0][\'c\'] * args.width_multiplier),\n                            args.kernel_size,\n                            self.network_settings[0][\'s\'], args.dropout_prob)]\n\n        # Layers from 1 to 7\n        for i in range(1, 8):\n            self.network.extend(\n                inverted_residual_sequence(\n                    int(self.network_settings[i - 1][\'c\'] * args.width_multiplier),\n                    int(self.network_settings[i][\'c\'] * args.width_multiplier),\n                    self.network_settings[i][\'n\'], self.network_settings[i][\'t\'],\n                    args.kernel_size, self.network_settings[i][\'s\']))\n\n        # Last layer before flattening\n        self.network.append(\n            conv2d_bn_relu6(int(self.network_settings[7][\'c\'] * args.width_multiplier),\n                            int(self.network_settings[8][\'c\'] * args.width_multiplier), 1,\n                            self.network_settings[8][\'s\'],\n                            args.dropout_prob))\n\n        ###############################################################################################################\n\n        # Classification part\n        self.network.append(nn.Dropout2d(args.dropout_prob, inplace=True))\n        self.network.append(nn.AvgPool2d(\n            (args.img_height // args.downsampling, args.img_width // args.downsampling)))\n        self.network.append(nn.Dropout2d(args.dropout_prob, inplace=True))\n        self.network.append(\n            nn.Conv2d(int(self.network_settings[8][\'c\'] * args.width_multiplier), self.num_classes,\n                      1, bias=True))\n\n        self.network = nn.Sequential(*self.network)\n\n        self.initialize()\n\n    def forward(self, x):\n        # Debugging mode\n        # for op in self.network:\n        #     x = op(x)\n        #     print(x.shape)\n        x = self.network(x)\n        x = x.view(-1, self.num_classes)\n        return x\n\n    def initialize(self):\n        """"""Initializes the model parameters""""""\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                nn.init.xavier_normal(m.weight)\n                if m.bias is not None:\n                    nn.init.constant(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant(m.weight, 1)\n                nn.init.constant(m.bias, 0)\n'"
train.py,7,"b'import shutil\n\nimport torch.nn as nn\nimport torch.optim\nfrom tensorboardX import SummaryWriter\nfrom torch.autograd import Variable\nfrom torch.optim.rmsprop import RMSprop\nfrom tqdm import tqdm\n\nfrom utils import AverageTracker\n\n\nclass Train:\n    def __init__(self, model, trainloader, valloader, args):\n        self.model = model\n        self.trainloader = trainloader\n        self.valloader = valloader\n        self.args = args\n        self.start_epoch = 0\n        self.best_top1 = 0.0\n\n        # Loss function and Optimizer\n        self.loss = None\n        self.optimizer = None\n        self.create_optimization()\n\n        # Model Loading\n        self.load_pretrained_model()\n        self.load_checkpoint(self.args.resume_from)\n\n        # Tensorboard Writer\n        self.summary_writer = SummaryWriter(log_dir=args.summary_dir)\n\n    def train(self):\n        for cur_epoch in range(self.start_epoch, self.args.num_epochs):\n\n            # Initialize tqdm\n            tqdm_batch = tqdm(self.trainloader,\n                              desc=""Epoch-"" + str(cur_epoch) + ""-"")\n\n            # Learning rate adjustment\n            self.adjust_learning_rate(self.optimizer, cur_epoch)\n\n            # Meters for tracking the average values\n            loss, top1, top5 = AverageTracker(), AverageTracker(), AverageTracker()\n\n            # Set the model to be in training mode (for dropout and batchnorm)\n            self.model.train()\n\n            for data, target in tqdm_batch:\n\n                if self.args.cuda:\n                    data, target = data.cuda(async=self.args.async_loading), target.cuda(\n                        async=self.args.async_loading)\n                data_var, target_var = Variable(data), Variable(target)\n\n                # Forward pass\n                output = self.model(data_var)\n                cur_loss = self.loss(output, target_var)\n\n                # Optimization step\n                self.optimizer.zero_grad()\n                cur_loss.backward()\n                self.optimizer.step()\n\n                # Top-1 and Top-5 Accuracy Calculation\n                cur_acc1, cur_acc5 = self.compute_accuracy(output.data, target, topk=(1, 5))\n                loss.update(cur_loss.data[0])\n                top1.update(cur_acc1[0])\n                top5.update(cur_acc5[0])\n\n            # Summary Writing\n            self.summary_writer.add_scalar(""epoch-loss"", loss.avg, cur_epoch)\n            self.summary_writer.add_scalar(""epoch-top-1-acc"", top1.avg, cur_epoch)\n            self.summary_writer.add_scalar(""epoch-top-5-acc"", top5.avg, cur_epoch)\n\n            # Print in console\n            tqdm_batch.close()\n            print(""Epoch-"" + str(cur_epoch) + "" | "" + ""loss: "" + str(\n                loss.avg) + "" - acc-top1: "" + str(\n                top1.avg)[:7] + ""- acc-top5: "" + str(top5.avg)[:7])\n\n            # Evaluate on Validation Set\n            if cur_epoch % self.args.test_every == 0 and self.valloader:\n                self.test(self.valloader, cur_epoch)\n\n            # Checkpointing\n            is_best = top1.avg > self.best_top1\n            self.best_top1 = max(top1.avg, self.best_top1)\n            self.save_checkpoint({\n                \'epoch\': cur_epoch + 1,\n                \'state_dict\': self.model.state_dict(),\n                \'best_top1\': self.best_top1,\n                \'optimizer\': self.optimizer.state_dict(),\n            }, is_best)\n\n    def test(self, testloader, cur_epoch=-1):\n        loss, top1, top5 = AverageTracker(), AverageTracker(), AverageTracker()\n\n        # Set the model to be in testing mode (for dropout and batchnorm)\n        self.model.eval()\n\n        for data, target in testloader:\n            if self.args.cuda:\n                data, target = data.cuda(async=self.args.async_loading), target.cuda(\n                    async=self.args.async_loading)\n            data_var, target_var = Variable(data, volatile=True), Variable(target, volatile=True)\n\n            # Forward pass\n            output = self.model(data_var)\n            cur_loss = self.loss(output, target_var)\n\n            # Top-1 and Top-5 Accuracy Calculation\n            cur_acc1, cur_acc5 = self.compute_accuracy(output.data, target, topk=(1, 5))\n            loss.update(cur_loss.data[0])\n            top1.update(cur_acc1[0])\n            top5.update(cur_acc5[0])\n\n        if cur_epoch != -1:\n            # Summary Writing\n            self.summary_writer.add_scalar(""test-loss"", loss.avg, cur_epoch)\n            self.summary_writer.add_scalar(""test-top-1-acc"", top1.avg, cur_epoch)\n            self.summary_writer.add_scalar(""test-top-5-acc"", top5.avg, cur_epoch)\n\n        print(""Test Results"" + "" | "" + ""loss: "" + str(loss.avg) + "" - acc-top1: "" + str(\n            top1.avg)[:7] + ""- acc-top5: "" + str(top5.avg)[:7])\n\n    def save_checkpoint(self, state, is_best, filename=\'checkpoint.pth.tar\'):\n        torch.save(state, self.args.checkpoint_dir + filename)\n        if is_best:\n            shutil.copyfile(self.args.checkpoint_dir + filename,\n                            self.args.checkpoint_dir + \'model_best.pth.tar\')\n\n    def compute_accuracy(self, output, target, topk=(1,)):\n        """"""Computes the accuracy@k for the specified values of k""""""\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, idx = output.topk(maxk, 1, True, True)\n        idx = idx.t()\n        correct = idx.eq(target.view(1, -1).expand_as(idx))\n\n        acc_arr = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            acc_arr.append(correct_k.mul_(1.0 / batch_size))\n        return acc_arr\n\n    def adjust_learning_rate(self, optimizer, epoch):\n        """"""Sets the learning rate to the initial LR multiplied by 0.98 every epoch""""""\n        learning_rate = self.args.learning_rate * (self.args.learning_rate_decay ** epoch)\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = learning_rate\n\n    def create_optimization(self):\n        self.loss = nn.CrossEntropyLoss()\n\n        if self.args.cuda:\n            self.loss.cuda()\n\n        self.optimizer = RMSprop(self.model.parameters(), self.args.learning_rate,\n                                 momentum=self.args.momentum,\n                                 weight_decay=self.args.weight_decay)\n\n    def load_pretrained_model(self):\n        try:\n            print(""Loading ImageNet pretrained weights..."")\n            pretrained_dict = torch.load(self.args.pretrained_path)\n            self.model.load_state_dict(pretrained_dict)\n            print(""ImageNet pretrained weights loaded successfully.\\n"")\n        except:\n            print(""No ImageNet pretrained weights exist. Skipping...\\n"")\n\n    def load_checkpoint(self, filename):\n        filename = self.args.checkpoint_dir + filename\n        try:\n            print(""Loading checkpoint \'{}\'"".format(filename))\n            checkpoint = torch.load(filename)\n            self.start_epoch = checkpoint[\'epoch\']\n            self.best_top1 = checkpoint[\'best_top1\']\n            self.model.load_state_dict(checkpoint[\'state_dict\'])\n            self.optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print(""Checkpoint loaded successfully from \'{}\' at (epoch {})\\n""\n                  .format(self.args.checkpoint_dir, checkpoint[\'epoch\']))\n        except:\n            print(""No checkpoint exists from \'{}\'. Skipping...\\n"".format(self.args.checkpoint_dir))\n'"
utils.py,0,"b'import argparse\nimport json\nimport os\nimport sys\nfrom pprint import pprint\n\nimport numpy as np\nfrom easydict import EasyDict as edict\n\n\ndef parse_args():\n    """"""\n    Parse the arguments of the program\n    :return: (config_args)\n    :rtype: tuple\n    """"""\n    # Create a parser\n    parser = argparse.ArgumentParser(description=""MobileNet-V2 PyTorch Implementation"")\n    parser.add_argument(\'--version\', action=\'version\', version=\'%(prog)s 0.0.1\')\n    parser.add_argument(\'--config\', default=None, type=str, help=\'Configuration file\')\n\n    # Parse the arguments\n    args = parser.parse_args()\n\n    # Parse the configurations from the config json file provided\n    try:\n        if args.config is not None:\n            with open(args.config, \'r\') as config_file:\n                config_args_dict = json.load(config_file)\n        else:\n            print(""Add a config file using \\\'--config file_name.json\\\'"", file=sys.stderr)\n            exit(1)\n\n    except FileNotFoundError:\n        print(""ERROR: Config file not found: {}"".format(args.config), file=sys.stderr)\n        exit(1)\n    except json.decoder.JSONDecodeError:\n        print(""ERROR: Config file is not a proper JSON file!"", file=sys.stderr)\n        exit(1)\n\n    config_args = edict(config_args_dict)\n\n    pprint(config_args)\n    print(""\\n"")\n\n    return config_args\n\n\ndef create_experiment_dirs(exp_dir):\n    """"""\n    Create Directories of a regular tensorflow experiment directory\n    :param exp_dir:\n    :return summary_dir, checkpoint_dir:\n    """"""\n    experiment_dir = os.path.realpath(\n        os.path.join(os.path.dirname(__file__))) + ""/experiments/"" + exp_dir + ""/""\n    summary_dir = experiment_dir + \'summaries/\'\n    checkpoint_dir = experiment_dir + \'checkpoints/\'\n\n    dirs = [summary_dir, checkpoint_dir]\n    try:\n        for dir_ in dirs:\n            if not os.path.exists(dir_):\n                os.makedirs(dir_)\n        print(""Experiment directories created!"")\n        # return experiment_dir, summary_dir, checkpoint_dir\n        return experiment_dir, summary_dir, checkpoint_dir\n    except Exception as err:\n        print(""Creating directories error: {0}"".format(err))\n        exit(-1)\n\n\ndef calc_dataset_stats(dataset, axis=0, ep=1e-7):\n    return (np.mean(dataset, axis=axis) / 255.0).tolist(), (\n            np.std(dataset + ep, axis=axis) / 255.0).tolist()\n\n\nclass AverageTracker:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n'"
