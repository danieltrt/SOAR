file_path,api_count,code
setup.py,0,"b'import os\nimport re\n\nfrom setuptools import setup, find_packages\n\n\ndef resolve_requirements(file):\n    requirements = []\n    with open(file) as f:\n        req = f.read().splitlines()\n        for r in req:\n            if r.startswith(""-r""):\n                requirements += resolve_requirements(os.path.join(os.path.dirname(file), r.split("" "")[1]))\n            else:\n                requirements.append(r)\n    return requirements\n\n\ndef read_file(file):\n    with open(file) as f:\n        content = f.read()\n    return content\n\n\ndef find_version(file):\n    content = read_file(file)\n    version_match = re.search(r""^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]"", content, re.M)\n    if version_match:\n        return version_match.group(1)\n    raise RuntimeError(""Unable to find version string."")\n\n\nrequired = resolve_requirements(os.path.join(os.path.dirname(__file__), \'requirements_full.txt\'))\nreadme = read_file(os.path.join(os.path.dirname(__file__), ""Readme.md""))\n#license = read_file(os.path.join(os.path.dirname(__file__), ""LICENSE""))\nversion = find_version(os.path.join(os.path.dirname(__file__), ""trixi"", ""__init__.py""))\n\n\nsetup(name=\'trixi\',\n      version=version,\n      description=\'Manage your machine learning experiments with trixi - modular, reproducible, high fashion\',\n      long_description=readme,\n      long_description_content_type=""text/markdown"",\n      url=\'https://github.com/MIC-DKFZ/trixi\',\n      author=\'Medical Image Computing Group, DKFZ\',\n      author_email=\'mic@dkfz-heidelberg.de\',\n      license=""MIT"",\n      packages=find_packages(),\n      install_requires=required,\n      zip_safe=True,\n      entry_points={\n          \'console_scripts\': [\'trixi-browser=trixi.experiment_browser.browser:start_browser\'],\n      },\n      include_package_data=True\n      )\n'"
doc/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# TRIXI documentation build configuration file, created by\n# sphinx-quickstart on Fri Apr 13 12:45:46 2018.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport re\nimport sys\nsys.path.insert(0, os.path.abspath(\'../..\'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.inheritance_diagram\'\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'trixi\'\ncopyright = \'2018, MIC\'\nauthor = \'MIC\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'0.1\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'0.1\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [""_api/_build/*""]\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    ""collapse_navigation"": False,\n    ""logo_only"": True\n}\n\nhtml_logo = ""_static/logo/trixi-100w.png""\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\nhtml_sidebars = {\n    \'**\': [\n        \'relations.html\',  # needs \'show_related\': True theme option to display\n        \'searchbox.html\',\n    ]\n}\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'TRIXIdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'trixi.tex\', \'trixi Documentation\',\n     \'MIC\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'trixi\', \'trixi Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'trixi\', \'trixi Documentation\',\n     author, \'trixi\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\'https://docs.python.org/\': None}\n\n\n# Mock all the things!\n# (If we actually try to install from our requirements file, ReadTheDocs will\n# kill the process because of ""excessive memory consumption""...)\n# Would be preferable to read automatically from requirements, but some packages\n# have different PyPI names so it\'s cleaner this way\nautodoc_mock_imports = [\n    ""colorlover"",\n    ""flask"",\n    ""graphviz"",\n    ""imageio"",\n    ""matplotlib"",\n    ""numpy"",\n    ""seaborn"",\n    ""scipy"",\n    ""sklearn"",\n    ""telegram"",\n    ""torch"",\n    ""torchvision"",\n    ""pathos"",\n    ""portalocker"",\n    ""plotly"",\n    ""PIL"",\n    ""umap"",\n    ""visdom"",\n    ""slack"",\n    ""slackclient"",\n    ""tensorboardX""\n]\n\n# We use the following to automatically run sphinx-apidoc, whenever we run make html.\n# The output is ignored (see exclude_patterns above) and just created for convenience,\n# so that we can compare _build with the existing rst files and see what we need to update.\n\n\ndef run_apidoc(_):\n    from sphinx.apidoc import main\n    parentFolder = os.path.join(os.path.dirname(__file__), \'..\')\n    cur_dir = os.path.abspath(os.path.dirname(__file__))\n    sys.path.append(parentFolder)\n    # change ""backend"" to your module name\n    module = os.path.join(parentFolder, \'trixi\')\n    output_path = os.path.join(cur_dir, \'_api/_build\')\n    main([\'-e\', \'-f\', \'-o\', output_path, module, ""-d"", ""1""])\n\n\ndef setup(app):\n    # trigger the run_apidoc\n    app.connect(\'builder-inited\', run_apidoc)\n    app.add_stylesheet(""css/custom.css"")\n'"
examples/train_net.py,4,"b'import torch\nimport torchvision\nfrom torch.autograd import Variable\n\nimport trixi\nimport trixi.logger as logger\n\n###############################################\n#\n# Basic Training script for a neural network. Does not really train, and the images look fancy, but only for\n# trixi demo (including error plots, image plots, model checkpoint storing).\n#\n################################################\n\n### Get Params\nparam = dict(\n    name=""AlexNet"",\n    output_folder=""AlexNet/"",\n    n_epoch=100,\n    batch_size=32\n)\n\n### Init stuff\nvizLog = logger.PytorchVisdomLogger(name=param[""name""], port=8080)\nexpLog = logger.PytorchExperimentLogger(experiment_name=param[""name""], base_dir=param[""output_folder""])\ncombiLog = logger.CombinedLogger((vizLog, 1), (expLog, 10))\n\nexpLog.print(expLog.base_dir)\nexpLog.text_logger.log_to(param, ""config"")\n\n### Get Dataset\ndataset = torchvision.datasets.MNIST(root=""data/"", download=True, transform=torchvision.transforms.ToTensor())\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=param[""batch_size""])\n\n### Models\nalexNet = torchvision.models.alexnet(pretrained=False, num_classes=10)\nnets = dict(alexNet=alexNet)\n\nif ""load_path"" in param:\n    nets = expLog.load_last_checkpoint(dir=param[""load_path""], **nets)\n\nexpLog.text_logger.log_to(nets, ""nets"")\n\n### Criterion\ncriterion = torch.nn.CrossEntropyLoss()\n\n### Optimizers\noptimizer = torch.optim.Adam(alexNet.parameters(), lr=1e-10)\n\n### Store stuff\nstore_checkpoint_fn = expLog.get_save_checkpoint_fn(optimizer=optimizer, **nets)\nexpLog.save_at_exit(optimizer=optimizer, **nets)\n\n### Fitting model\nfor epoch in range(param[""n_epoch""]):\n\n    for batch_idx, data_batch in enumerate(dataloader):\n\n        #######################\n        #    Update network   #\n        #######################\n\n        alexNet.zero_grad()\n\n        images, labels = data_batch\n\n        current_batch_size = images.size(0)\n        images = images.repeat(current_batch_size, 3, 28, 28).resize_(current_batch_size, 3, 224, 224)\n\n        inpt = Variable(images)\n        labels = Variable(labels)\n\n        pred = alexNet(inpt)\n        err = criterion(pred, labels)\n\n        err.backward()\n        optimizer.step()\n\n        #######################\n        #      Log results    #\n        #######################\n        combiLog.show_value(value=err.item(), name=\'err\')\n\n        log_text = \'[%d/%d][%d/%d] Loss: %.4f \' % (epoch, param[""n_epoch""], batch_idx, len(dataset), err.item())\n\n        expLog.print(log_text)\n\n        if batch_idx % 2 == 0:\n            vizLog.show_text(log_text, name=""log"")\n            vizLog.show_progress(epoch, param[""n_epoch""])\n\n            combiLog.show_image_grid(images, name=""xd"", title=""Samples"", n_iter=batch_idx)\n\n    if epoch % 20 == 0:\n        store_checkpoint_fn(n_iter=epoch)\n'"
examples/train_net_pytorchexperiment.py,8,"b'import os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision.transforms import transforms\n\nfrom trixi.experiment import PytorchExperiment\nfrom trixi.util import Config\n\n\n### Get Params\n\n###############################################\n#\n# Basic Training script for a neural network. Does not really train, and the images look fancy, but only for\n# trixi demo (including error plots, image plots, model checkpoint storing).\n#\n################################################\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return x\n\n\nc = Config(\n    name=""MNIST-Example"",\n    base_dir=""MNIST_experiment/"",\n    n_epochs=5,\n    batch_size=64,\n    log_interval=100,\n    lr=1e-5,\n    use_cuda=True,\n\n)\n\n\nclass MNISTExperiment(PytorchExperiment):\n\n    def setup(self):\n\n        self.elog.print(self.config)\n\n        self.device = torch.device(""cuda"" if self.config.use_cuda else ""cpu"")\n\n        ### Get Dataset\n        transf = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n        self.dataset_train = torchvision.datasets.MNIST(root=self.config.base_dir + ""data/"", download=True,\n                                                        transform=transf, train=True)\n        self.dataset_test = torchvision.datasets.MNIST(root=self.config.base_dir + ""data/"", download=True,\n                                                       transform=transf, train=False)\n\n        data_loader_kwargs = {\'num_workers\': 1, \'pin_memory\': True} if self.config.use_cuda else {}\n        self.train_loader = torch.utils.data.DataLoader(self.dataset_train, batch_size=self.config.batch_size,\n                                                        shuffle=True, **data_loader_kwargs)\n        self.test_loader = torch.utils.data.DataLoader(self.dataset_test, batch_size=self.config.batch_size,\n                                                       shuffle=True, **data_loader_kwargs)\n\n        ### Models\n        self.model = Net()\n        self.model.to(self.device)\n\n        ### Optimizers\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.lr)\n\n        if ""load_path"" in self.config:\n            self.load_checkpoint(path=self.config.load_path, name="""")\n\n        ### Criterion\n        self.criterion = torch.nn.CrossEntropyLoss()\n\n    def train(self, epoch):\n        self.model.train()\n        for batch_idx, (data, target) in enumerate(self.train_loader):\n            data, target = data.to(self.device), target.to(self.device)\n\n            self.optimizer.zero_grad()\n            output = self.model(data)\n\n            loss = self.criterion(output, target)\n            loss.backward()\n            self.optimizer.step()\n\n            if batch_idx % self.config.log_interval == 0:\n                print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                    epoch, batch_idx * len(data), len(self.train_loader.dataset),\n                           100. * batch_idx / len(self.train_loader), loss.item()))\n\n    def validate(self, epoch):\n        self.model.eval()\n        val_loss = 0\n        correct = 0\n        with torch.no_grad():\n            for data, target in self.test_loader:\n                data, target = data.to(self.device), target.to(self.device)\n\n                output = self.model(data)\n                val_loss += self.criterion(output, target).item()  # sum up batch loss\n\n                pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n                correct += pred.eq(target.view_as(pred)).sum().item()\n\n        val_loss /= len(self.test_loader.dataset)\n        print(\'\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\'.format(\n            val_loss, correct, len(self.test_loader.dataset),\n            100. * correct / len(self.test_loader.dataset)))\n\n\nif __name__ == \'__main__\':\n    mnist_exp = MNISTExperiment(config=c, globs=globals())\n    mnist_exp.run()\n\n    load_path = os.path.join(mnist_exp.elog.checkpoint_dir, ""checkpoint_last.pth.tar"")\n    c.load_path = load_path\n\n    mnist_exp_continued = MNISTExperiment(config=c, globs=globals())\n    mnist_exp_continued.run()\n'"
test/__init__.py,0,b''
test/test_experimentlogger.py,2,"b'import os\nimport tempfile\nimport unittest\nimport shutil\nimport time\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom imageio import imread\n\nfrom trixi.logger.experiment.experimentlogger import ExperimentLogger\nfrom trixi.util.config import Config\n\ntest_dir = ""test_dir""\n\n\nclass TestExperimentLogger(unittest.TestCase):\n\n    def setUp(self):\n        self.test_dir = tempfile.gettempdir()\n        self.experimentLogger = ExperimentLogger(exp_name=""test"",\n                                                 base_dir=self.test_dir,\n                                                 folder_format=""{experiment_name}"")\n\n    def tearDown(self):\n        if os.path.exists(self.test_dir):\n            shutil.rmtree(self.test_dir, ignore_errors=True)\n\n    def test_create_folders(self):\n        exp_dir = os.path.join(self.test_dir, ""test"")\n        self.assertTrue(os.path.exists(exp_dir), ""Experiment dir not created"")\n        self.assertTrue(os.path.exists(os.path.join(exp_dir, ""checkpoint"")), ""Config dir not created"")\n        self.assertTrue(os.path.exists(os.path.join(exp_dir, ""config"")), ""Config dir not created"")\n        self.assertTrue(os.path.exists(os.path.join(exp_dir, ""img"")), ""Config dir not created"")\n        self.assertTrue(os.path.exists(os.path.join(exp_dir, ""log"")), ""Config dir not created"")\n        self.assertTrue(os.path.exists(os.path.join(exp_dir, ""config"")), ""Config dir not created"")\n        self.assertTrue(os.path.exists(os.path.join(exp_dir, ""plot"")), ""Config dir not created"")\n        self.assertTrue(os.path.exists(os.path.join(exp_dir, ""result"")), ""Config dir not created"")\n        self.assertTrue(os.path.exists(os.path.join(exp_dir, ""save"")), ""Config dir not created"")\n\n    def test_two_experiment_loggers_same_test_dir_no_run_number_throws_error(self):\n        logger = ExperimentLogger(exp_name=""test"",\n                                  base_dir=self.test_dir,\n                                  folder_format=""{experiment_name}"")\n        self.assertTrue(os.path.isdir(logger.base_dir), ""Experiment directory not created"")\n\n    def test_show_image(self):\n        image = np.random.random_sample((3, 128, 128))\n        self.experimentLogger.show_image(image, ""image"")\n        self.assertTrue(os.path.exists(os.path.join(self.experimentLogger.img_dir, ""image.png"")),\n                        ""Show image could not create image"")\n\n    def test_show_barplot(self):\n        array = np.random.random_sample(5)\n        self.experimentLogger.show_barplot(array, ""barplot"")\n        self.assertTrue(os.path.exists(os.path.join(self.experimentLogger.plot_dir, ""barplot.png"")),\n                        ""Show barplot could not create barplot"")\n\n    def test_show_lineplot(self):\n        x = [0, 1, 2, 3, 4, 5]\n        y = np.random.random_sample(6)\n        self.experimentLogger.show_lineplot(y, x, name=""lineplot1"")\n        self.assertTrue(os.path.exists(os.path.join(self.experimentLogger.plot_dir, ""lineplot1.png"")),\n                        ""Show lineplot could not create lineplot"")\n        self.experimentLogger.show_lineplot(y, name=""lineplot2"")\n        self.assertTrue(os.path.exists(os.path.join(self.experimentLogger.plot_dir, ""lineplot2.png"")),\n                        ""Show lineplot could not create lineplot without x vals"")\n\n    def test_show_piechart(self):\n        array = np.random.random_sample(5)\n        self.experimentLogger.show_piechart(array, ""piechart"")\n        self.assertTrue(os.path.exists(os.path.join(self.experimentLogger.plot_dir, ""piechart.png"")),\n                        ""Show piechart could not create piechart"")\n\n    def test_show_scatterplot(self):\n        array = np.random.random_sample((5, 2))\n        self.experimentLogger.show_scatterplot(array, ""scatterplot"")\n        self.assertTrue(os.path.exists(os.path.join(self.experimentLogger.plot_dir, ""scatterplot.png"")),\n                        ""Show scatterplot could not create scatterplot"")\n\n    def test_show_value(self):\n        val = np.random.random_sample(1)\n        self.experimentLogger.show_value(val, ""value"")\n        plt1_content = imread(os.path.join(self.experimentLogger.plot_dir, ""value.png""))\n\n        val = np.random.random_sample(1)\n        self.experimentLogger.show_value(val, ""value"")\n        plt2_content = imread(os.path.join(self.experimentLogger.plot_dir, ""value.png""))\n        self.assertFalse(np.array_equal(plt1_content, plt2_content), ""Show value did not update the plot"")\n\n        val = np.random.random_sample(1)\n        self.experimentLogger.show_value(val, ""value"", counter=4)\n        plt3_content = imread(os.path.join(self.experimentLogger.plot_dir, ""value.png""))\n        self.assertFalse(np.array_equal(plt2_content, plt3_content), ""Show value did not update the plot"")\n\n    def test_show_text(self):\n        text = ""\\nTest 4 fun: zD ;-D 0o""\n        self.experimentLogger.show_text(text)\n        log_text = """"\n        with open(os.path.join(self.experimentLogger.log_dir, ""default.log""), \'r\') as log_file:\n            log_text = log_file.read()\n        self.assertTrue(text in log_text)\n\n\n\n    def test_save_and_load_config(self):\n        c = Config()\n        c.text = ""0o""\n        c.nmbr = 4\n        c.cls = str\n        c.lst = [1, 2, 3, 4]\n\n        self.experimentLogger.save_config(c, ""config"")\n        config_path = os.path.join(self.experimentLogger.config_dir, ""config.json"")\n        self.assertTrue(os.path.exists(config_path),\n                        ""Config could not be saved"")\n\n        c2 = self.experimentLogger.load_config(""config"")\n        self.assertTrue(""text"" in c2 and c2.text == c.text, ""Text in config could not be restored"")\n        self.assertTrue(""nmbr"" in c2 and c2.nmbr == c.nmbr, ""Number in config could not be restored"")\n        self.assertTrue(""cls"" in c2 and c2.cls == c.cls, ""Text in config could not be restored"")\n        self.assertTrue(""lst"" in c2 and c2.lst == c.lst, ""List in config could not be restored"")\n\n    def test_save_results(self):\n        d = dict()\n        d[\'best_val\'] = 3.1415926\n        d[\'worst_val\'] = 2.718281\n\n        self.experimentLogger.save_result(d, ""result"")\n        result_path = os.path.join(self.experimentLogger.result_dir, ""result.json"")\n        self.assertTrue(os.path.exists(result_path),\n                        ""Result could not be saved"")\n        result_text = """"\n        with open(result_path, \'r\') as log_file:\n            result_text = log_file.read()\n        self.assertTrue(""best_val"" in result_text, ""Saved vals could not be found in result"")\n        self.assertTrue(""3.1415926"" in result_text, ""Saved vals could not be found in result"")\n        self.assertTrue(""worst_val"" in result_text, ""Saved vals could not be found in result"")\n        self.assertTrue(""2.718281"" in result_text, ""Saved vals could not be found in result"")\n\n    def test_save_and_load_dict(self):\n        d = dict()\n        d[\'text\'] = ""0o""\n        d[\'nmbr\'] = 4\n        d[\'tuple\'] = (1, 2, 3, 4)\n\n        self.experimentLogger.save_dict(d, ""dict"")\n        dict_path = os.path.join(self.experimentLogger.save_dir, ""dict.json"")\n        self.assertTrue(os.path.exists(dict_path),\n                        ""dict could not be saved"")\n\n        d2 = self.experimentLogger.load_dict(""dict"")\n        self.assertTrue(""text"" in d2 and d2[\'text\'] == d[\'text\'], ""Text in dict could not be restored"")\n        self.assertTrue(""nmbr"" in d2 and d2[\'nmbr\'] == d[\'nmbr\'], ""Number in dict could not be restored"")\n        self.assertTrue(""tuple"" in d2 and d2[\'tuple\'] == d[\'tuple\'], ""Tuple in dict could not be restored"")\n\n\n    def test_save_and_load_numpy_data(self):\n        np_array = np.random.random_sample((3, 128, 128))\n\n        self.experimentLogger.save_numpy_data(np_array, ""array"")\n        np_path = os.path.join(self.experimentLogger.save_dir, ""array.npy"")\n        self.assertTrue(os.path.exists(np_path),\n                        ""Np Array could not be saved"")\n\n        array2 = self.experimentLogger.load_numpy_data(""array"")\n        self.assertTrue(np.array_equal(np_array, array2), ""Numpy array could not be loaded"")\n\n    def test_save_and_load_pickle(self):\n        data1 = np.random.random_sample((3, 128, 128))\n\n        self.experimentLogger.save_pickle(data1, ""pickle.pkl"")\n        np_path = os.path.join(self.experimentLogger.save_dir, ""pickle.pkl"")\n        self.assertTrue(os.path.exists(np_path),\n                        ""Pickle file could not be saved"")\n\n        data2 = self.experimentLogger.load_pickle(""pickle.pkl"")\n        self.assertTrue(np.array_equal(data1, data2), ""Pickle file could not be loaded"")\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
test/test_numpyseabornimageplotlogger.py,2,"b'import os\nimport tempfile\nimport unittest\nimport shutil\nimport time\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom trixi.logger.plt.numpyseabornimageplotlogger import NumpySeabornImagePlotLogger\nfrom trixi.logger.visdom import PytorchVisdomLogger\nfrom trixi.logger.visdom.numpyvisdomlogger import start_visdom\n\n\nclass TestNumpySearbonImagePlotLogger(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        super(TestNumpySearbonImagePlotLogger, cls).setUpClass()\n\n\n    def setUp(self):\n        self.logger = NumpySeabornImagePlotLogger()\n\n    def test_show_image(self):\n        image = np.random.random_sample((3, 128, 128))\n        figure_image = self.logger.show_image(image, ""image"")\n        self.assertTrue(isinstance(figure_image, np.ndarray))\n\n    def test_show_barplot(self):\n        tensor = np.random.random_sample(5)\n        figure_image = self.logger.show_barplot(tensor, name=""barplot"")\n        self.assertTrue(isinstance(figure_image, np.ndarray))\n\n    def test_show_lineplot(self):\n        x = [0, 1, 2, 3, 4, 5]\n        y = np.random.random_sample(6)\n        figure_image = self.logger.show_lineplot(y, x, name=""lineplot1"")\n        self.assertTrue(isinstance(figure_image, np.ndarray))\n\n    def test_show_piechart(self):\n        array = np.random.random_sample(5)\n        figure_image = self.logger.show_piechart(array, name=""piechart"")\n        self.assertTrue(isinstance(figure_image, np.ndarray))\n\n    def test_show_scatterplot(self):\n        array = np.random.random_sample((5, 2))\n        figure_image = self.logger.show_scatterplot(array, name=""scatterplot"")\n        self.assertTrue(isinstance(figure_image, np.ndarray))\n\n    def test_show_value(self):\n        val = np.random.random_sample(1)\n        figure_image = self.logger.show_value(val, ""value"")\n        self.assertTrue(isinstance(figure_image, np.ndarray))\n\n        val = np.random.random_sample(1)\n        figure_image = self.logger.show_value(val, ""value"")\n        self.assertTrue(isinstance(figure_image, np.ndarray))\n\n        val = np.random.random_sample(1)\n        figure_image = self.logger.show_value(val, ""value"", counter=4)\n        self.assertTrue(isinstance(figure_image, np.ndarray))\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
test/test_pytorchexperiment.py,8,"b'import json\nimport os\nimport tempfile\nimport unittest\nimport shutil\nimport time\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom trixi.experiment import PytorchExperiment\nfrom trixi.util import ResultLogDict\n\n\nclass TestPytorchExperiment(unittest.TestCase):\n\n    def setUp(self):\n        self.test_dir = tempfile.gettempdir()\n        self.experiment = PytorchExperiment(name=""test_experiment"", base_dir=self.test_dir, n_epochs=10)\n\n    def tearDown(self):\n        self.experiment._exp_state = ""Ended""\n        if os.path.exists(self.test_dir):\n            shutil.rmtree(self.test_dir, ignore_errors=True)\n\n    def test_train(self):\n        self.cntr = []\n        self.experiment.train = lambda epoch: self.cntr.append(0)\n        self.experiment.run()\n        self.assertTrue(len(self.cntr) == 10, ""Did not call train for the right number of epochs"")\n\n    def test_validate(self):\n        self.cntr = []\n        self.experiment.validate = lambda epoch: self.cntr.append(0)\n        self.experiment.run()\n        self.assertTrue(len(self.cntr) == 10, ""Did not call train for the right number of epochs"")\n\n    def test_update_attributes(self):\n        results2 = ResultLogDict(""results-log.json"", base_dir=self.test_dir)\n        results2[""test""] = 0\n        self.experiment.test_var = ""test""\n\n        self.experiment.update_attributes({""results"": results2, ""test_var"": ""test2""})\n\n        self.assertTrue(self.experiment.test_var == ""test2"", ""Could not update simple var in update_attributes"")\n        self.assertTrue(self.experiment.results[\'test\'] == 0, ""Could not result dict var in update_attributes"")\n\n    def test_get_pytorch_modules(self):\n        module = Net()\n        self.experiment.net = module\n\n        pyt_module = self.experiment.get_pytorch_modules()\n\n        self.assertTrue(""net"" in pyt_module, ""Name for module could not be correctly returned in get_pytorch_modules"")\n        self.assertTrue(pyt_module[\'net\'] == module, ""Module could not be correctly returned in  get_pytorch_modules"")\n\n    def test_get_pytorch_optimizers(self):\n        optimizer = torch.optim.Adam(params=[torch.tensor(1)])\n        self.experiment.optim = optimizer\n\n        pyt_optim = self.experiment.get_pytorch_optimizers()\n\n        self.assertTrue(""optim"" in pyt_optim, ""Name for optimizer could not be correctly returned in ""\n                                              ""get_pytorch_optimizers"")\n        self.assertTrue(pyt_optim[\'optim\'] == optimizer, ""Optimizer could not be correctly returned in  ""\n                                                         ""get_pytorch_optimizers"")\n\n    def test_get_simple_vars(self):\n        self.experiment.a = ""test""\n        self.experiment.b = 1\n        self.experiment.c = True\n        self.experiment.d = (1, 2, 3, 4)\n\n        var = self.experiment.get_simple_variables()\n\n        self.assertTrue(""a"" in var and ""b"" in var and ""c"" in var and ""d"" in var,\n                        ""Names for attributes could not be correctly returned in get_simple_variables"")\n        self.assertTrue(var[\'a\'] == ""test"" and var[\'b\'] == 1 and var[\'c\'] == True and var[\'d\'] == (1, 2, 3, 4),\n                        ""variables could not be correctly returned in get_simple_variables"")\n\n    def test_get_pytorch_vars(self):\n        var = torch.tensor(1)\n        self.experiment.pyt_var = var\n\n        pyt_vars = self.experiment.get_pytorch_variables()\n\n        self.assertTrue(""pyt_var"" in pyt_vars, ""Name for variable could not be correctly returned in ""\n                                               ""get_pytorch_variables"")\n        self.assertTrue(pyt_vars[\'pyt_var\'] == var, ""Variable could not be correctly returned in  ""\n                                                    ""get_pytorch_variables"")\n\n    def test_save_results(self):\n        self.experiment.add_result(0, ""test"")\n        self.experiment.save_results(""results-test.json"")\n\n        self.assertTrue(os.path.exists(os.path.join(self.experiment.elog.result_dir, ""results-test.json"")),\n                        ""result file could not be stored"")\n\n        with open(os.path.join(self.experiment.elog.result_dir, ""results-test.json""), ""r"") as f:\n            content = f.read()\n            self.assertTrue(""test"" in content and ""0"" in content, ""results content not sucessfully saved"")\n\n    def test_save_and_load_checkpoints(self):\n        net = Net()\n        optim = torch.optim.Adam(params=net.parameters())\n        ptvar = torch.tensor(1)\n        svar = ""test""\n        lvar = (1, 2, 3, 4)\n\n        self.experiment.net = net\n        self.experiment.optim = optim\n        self.experiment.ptvar = ptvar\n        self.experiment.svar = svar\n        self.experiment.lvar = lvar\n        self.experiment.results[""test""] = 1\n\n        self.experiment.save_checkpoint(name=""test_checkpoint"")\n\n        self.experiment.net = Net()\n        self.experiment.optim = torch.optim.Adam(params=Net().parameters(), lr=1)\n        self.experiment.ptvar = torch.tensor(0)\n        self.experiment.svar = ""test2""\n        self.experiment.lvar = (-1, -2)\n        self.experiment.results[""test""] = 0\n\n        self.experiment.load_checkpoint(name=""test_checkpoint"")\n\n        self.assertTrue((list(self.experiment.net.parameters())[0] - list(net.parameters())[0]).sum().item() < 0.00001,\n                        ""Net could not be restored from checkpoint"")\n        self.assertTrue(\n            self.experiment.optim.state_dict()[\'param_groups\'][0][\'lr\'] == optim.state_dict()[\'param_groups\'][0][\'lr\'],\n            ""optim could not be restored from checkpoint"")\n        self.assertTrue(self.experiment.ptvar == ptvar.item(), ""ptvar could not be restored from checkpoint"")\n        self.assertTrue(self.experiment.svar == svar, ""svar could not be restored from checkpoint"")\n        self.assertTrue(self.experiment.lvar == lvar, ""lvar could not be restored from checkpoint"")\n        self.assertTrue(self.experiment.results[""test""] == 1, ""Results could not be restored from checkpoint"")\n\n    def test_add_results(self):\n        self.experiment.add_result(name=""test"", value=1)\n\n        self.assertTrue(self.experiment.results[""test""] == 1,\n                        ""Result was not added"")\n        self.assertTrue(self.experiment.get_result(""test"") == 1,\n                        ""Result was not added"")\n\n        self.assertTrue(os.path.exists(os.path.join(self.experiment.elog.result_dir, ""results-log.json"")),\n                        ""result file could not be stored"")\n\n        with open(os.path.join(self.experiment.elog.result_dir, ""results-log.json""), ""r"") as f:\n            content = f.read()\n        self.assertTrue(""test"" in content and ""1"" in content, ""results content not sucessfully saved"")\n\n    def test_save_tmp_results(self):\n        self.experiment.add_result(name=""test"", value=1)\n        self.experiment.run()\n\n        self.assertTrue(os.path.exists(os.path.join(self.experiment.elog.result_dir, ""results.json"")),\n                        ""result file could not be stored"")\n\n        with open(os.path.join(self.experiment.elog.result_dir, ""results.json""), ""r"") as f:\n            content = f.read()\n        self.assertTrue(""test"" in content and ""1"" in content, ""results content not sucessfully temporarily saved"")\n\n    def test_save_tmp_checkpoint(self):\n        self.experiment.test_var = ""test""\n        self.experiment.run()\n\n        time.sleep(5)\n\n        self.assertTrue(os.path.exists(os.path.join(self.experiment.elog.checkpoint_dir, ""checkpoint_current.pth.tar"")),\n                        ""Temp Checkpoint file could not be stored"")\n        self.assertTrue(os.path.exists(os.path.join(self.experiment.elog.checkpoint_dir, ""checkpoint_last.pth.tar"")),\n                        ""Last Checkpoint file could not be stored"")\n\n        self.experiment.test_var = ""test2""\n        self.experiment.load_checkpoint(""checkpoint_current"")\n        self.assertTrue(self.experiment.test_var == ""test"",\n                        ""Temp Checkpoint file loading could not sucessful"")\n        self.experiment.test_var = ""test2""\n        self.experiment.load_checkpoint(""checkpoint_last"")\n        self.assertTrue(self.experiment.test_var == ""test"",\n                        ""Temp Checkpoint file loading could not sucessful"")\n\n    def test_save_exp_info(self):\n        self.experiment.run()\n\n        with open(os.path.join(self.experiment.elog.config_dir, ""exp.json""), ""r"") as f:\n            exp_info = json.load(f)\n\n        self.assertTrue(exp_info[\'epoch\'] == 10, ""Epoch not sucessfully stored in exp info"")\n        self.assertTrue(exp_info[\'name\'] == \'test_experiment\', ""Name not sucessfully stored in exp info"")\n        self.assertTrue(exp_info[\'state\'] == \'Trained\', ""State not sucessfully stored as \'Trained\' in exp info"")\n\n    def test_print(self):\n        self.experiment.print(""0o zD 0o"")\n        with open(os.path.join(self.experiment.elog.log_dir, ""default.log""), ""r"") as f:\n            content = f.read()\n        self.assertTrue(""0o zD 0o"" in content, ""Print not sucessfully saved"")\n\n    def test_resume(self):\n        # TODO\n\n        self.cntr = []\n        self.experiment.train = lambda epoch: self.cntr.append(0)\n        self.experiment.end = lambda: time.sleep(2)\n        self.experiment.run()\n        self.assertTrue(len(self.cntr) == 10, ""Did not call train for the right number of epochs"")\n\n        exp2 = PytorchExperiment(name=""test-exp2"", base_dir=self.test_dir, resume=self.experiment.elog.work_dir)\n        exp3 = PytorchExperiment(name=""test-exp2"", base_dir=self.test_dir, resume=self.experiment.elog.work_dir,\n                                 resume_reset_epochs=False)\n\n        exp2.prepare_resume()\n        exp3.prepare_resume()\n\n        self.assertTrue(exp2.exp_name == \'test_experiment\', ""Did not restore exp_name"")\n        self.assertTrue(exp2._epoch_idx == 0, ""Did not reset epochs"")\n        self.assertTrue(exp3._epoch_idx == 10, ""Did reset epochs"")\n\n        exp2.train = lambda epoch: self.cntr.append(0)\n        exp3.train = lambda epoch: self.cntr.append(0)\n        exp2.run()\n        self.assertTrue(len(self.cntr) == 20, ""Did call not train for exp2"")\n        exp3.run()\n        self.assertTrue(len(self.cntr) == 20, ""Did call train for exp3"")\n\n        exp2._exp_state = ""Ended""\n        exp3._exp_state = ""Ended""\n\n\n\nclass Net(nn.Module):\n    """"""\n    Small network to test save/load functionality\n    """"""\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
test/test_pytorchexperimentlogger.py,13,"b'import os\nimport tempfile\nimport unittest\nimport shutil\nimport time\nimport matplotlib\n\nfrom trixi.util.metrics import get_roc_curve, get_pr_curve, get_classification_metrics\n\nmatplotlib.use(""Agg"")\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom trixi.logger.experiment import PytorchExperimentLogger\n\n\nclass TestPytorchExperimentLogger(unittest.TestCase):\n\n    def setUp(self):\n        self.test_dir = tempfile.gettempdir()\n        self.experimentLogger = PytorchExperimentLogger(exp_name=""test"",\n                                                        base_dir=self.test_dir,\n                                                        folder_format=""{experiment_name}"")\n\n    def tearDown(self):\n        if os.path.exists(self.test_dir):\n            shutil.rmtree(self.test_dir, ignore_errors=True)\n\n    def test_show_image(self):\n        image = np.random.random_sample((3, 128, 128))\n        tensor = torch.from_numpy(image)\n        self.experimentLogger.show_image(tensor, ""image"")\n\n        time.sleep(1)\n        self.assertTrue(os.path.exists(os.path.join(self.experimentLogger.img_dir, ""image.png"")),\n                        ""Show image could not create image"")\n\n    def test_show_images(self):\n        images = np.random.random_sample((4, 3, 128, 128))\n        tensors = torch.from_numpy(images)\n        self.experimentLogger.show_images(tensors, ""image"")\n\n        time.sleep(1)\n        self.assertTrue(len(os.listdir(self.experimentLogger.img_dir)) > 3,\n                        ""Show images could not create multiple images"")\n\n    def test_show_image_grid(self):\n        images = np.random.random_sample((4, 3, 128, 128))\n        tensor = torch.from_numpy(images)\n        self.experimentLogger.show_image_grid(tensor, ""image_grid"")\n\n        time.sleep(1)\n        self.assertTrue(os.path.exists(os.path.join(self.experimentLogger.img_dir, ""image_grid.png"")),\n                        ""Show image grid could not create image grid from tensor"")\n\n    def test_show_image_grid_heatmap(self):\n        images = np.random.random_sample((4, 3, 128, 128))\n        tensor = torch.from_numpy(images)\n        self.experimentLogger.show_image_grid_heatmap(tensor, name=""image_grid_heatmap"")\n\n        time.sleep(1)\n        self.assertTrue(os.path.exists(os.path.join(self.experimentLogger.img_dir, ""image_grid_heatmap.png"")),\n                        ""Show image grid could not create image grid from tensor"")\n\n    def test_show_barplot(self):\n        tensor = torch.from_numpy(np.random.random_sample(5))\n        self.experimentLogger.show_barplot(tensor, ""barplot"")\n        self.assertTrue(os.path.exists(os.path.join(self.experimentLogger.plot_dir, ""barplot.png"")),\n                        ""Show barplot could not create barplot"")\n\n    def test_net_save_and_load_model(self):\n        net = Net()\n        self.experimentLogger.save_model(name=""model"", model=net)\n        # give some time before loading, so saving has finished\n        time.sleep(1)\n        net2 = Net()\n        diff_conv1 = np.abs(np.sum(net.conv1.bias.detach().numpy() - net2.conv1.bias.detach().numpy()))\n        self.assertTrue(diff_conv1 > 1e-3, ""conv1 bias values have not been initialized differently"")\n        self.experimentLogger.load_model(name=""model"", model=net2)\n        time.sleep(2)\n\n        np.testing.assert_allclose(net.conv1.bias.detach().numpy(), net2.conv1.bias.detach().numpy(),\n                                   err_msg=""loading model did not restore model values"")\n\n    def test_net_save_and_load_checkpoint(self):\n        net = Net()\n        self.experimentLogger.save_checkpoint(name=""checkpoint"", net=net)\n        # give some time before loading, so saving has finished\n        time.sleep(1)\n        net2 = Net()\n        diff_conv1 = np.abs(np.sum(net.conv1.bias.detach().numpy() - net2.conv1.bias.detach().numpy()))\n        self.assertTrue(diff_conv1 > 1e-3, ""conv1 bias values have not been initialized differently"")\n        self.experimentLogger.load_last_checkpoint(dir=self.experimentLogger.checkpoint_dir, net=net2)\n        np.testing.assert_allclose(net.conv1.bias.detach().numpy(), net2.conv1.bias.detach().numpy(),\n                                   err_msg=""loading checkpoint did not restore model values"")\n\n    def test_net_save_and_load_checkpoint_with_optimizer(self):\n        self._test_load_save_checkpoint(test_with_cuda=False)\n\n    def test_net_save_and_load_checkpoint_with_optimizer_with_cuda(self):\n        if torch.cuda.is_available():\n            self._test_load_save_checkpoint(test_with_cuda=True)\n\n    def _test_load_save_checkpoint(self, test_with_cuda):\n        # small testing net\n        net = Net()\n        # fake values for fake step\n        random_input = torch.from_numpy(np.random.randn(28 * 28).reshape((1, 1, 28, 28))).float()\n        fake_labels = torch.from_numpy(np.array([2])).long()\n\n        if test_with_cuda:\n            net.cuda()\n            random_input, fake_labels = random_input.cuda(), fake_labels.cuda()\n\n        criterion = torch.nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(net.parameters(), lr=1e-1, eps=1e-7, weight_decay=1e-4, betas=(0.3, 0.4))\n\n        # do a fake forward/backward/step pass\n        pred = net(random_input)\n        err = criterion(pred, fake_labels)\n        err.backward()\n        optimizer.step()\n\n        # save and load network and optimizer\n        self.experimentLogger.save_checkpoint(name=""checkpoint"", net=net, optimizer=optimizer)\n        # give some time before loading, so saving has finished\n        time.sleep(1)\n\n        loaded_network = Net()\n        if test_with_cuda:\n            loaded_network.cuda()\n\n        loaded_optimizer = torch.optim.Adam(loaded_network.parameters(), lr=1e-1)\n        self.experimentLogger.load_last_checkpoint(dir=self.experimentLogger.checkpoint_dir, net=loaded_network,\n                                                   optimizer=loaded_optimizer)\n\n        saved_pg = optimizer.param_groups[0]\n        loaded_pg = loaded_optimizer.param_groups[0]\n\n        # assert optimizer values have been correctly saved/loaded\n        self.assertAlmostEqual(saved_pg[""lr""], loaded_pg[""lr""], ""learning rate not correctly saved"")\n        self.assertAlmostEqual(saved_pg[""eps""], loaded_pg[""eps""], ""eps not correctly saved"")\n        self.assertAlmostEqual(saved_pg[""weight_decay""], loaded_pg[""weight_decay""], ""weight_decay not correctly saved"")\n        self.assertAlmostEqual(saved_pg[""betas""][0], loaded_pg[""betas""][0], ""beta 0 not correctly saved"")\n        self.assertAlmostEqual(saved_pg[""betas""][1], loaded_pg[""betas""][1], ""beta 1 not correctly saved"")\n        for saved_parameter, loaded_parameter in zip(saved_pg[""params""], loaded_pg[""params""]):\n            np.testing.assert_allclose(saved_parameter.detach().cpu().numpy(), loaded_parameter.detach().cpu().numpy(),\n                                       err_msg=""loading checkpoint did not restore optimizer param values"")\n\n        # do a fake forward/backward/step pass with loaded stuff\n        pred = loaded_network(random_input)\n        err = criterion(pred, fake_labels)\n        err.backward()\n        loaded_optimizer.step()  # this could raise an error, which is why there is no assert but just the execution\n\n    def test_print(self):\n        text = ""\\nTest 4 fun: zD ;-D 0o""\n        self.experimentLogger.print(text)\n        log_text = """"\n        with open(os.path.join(self.experimentLogger.log_dir, ""default.log""), \'r\') as log_file:\n            log_text = log_file.read()\n        self.assertTrue(text in log_text)\n\n    def test_get_roc_curve(self):\n\n        try:\n            import sklearn\n\n            array = np.random.random_sample(100)\n            labels = np.random.choice((0, 1), 100)\n\n            tpr, fpr = get_roc_curve(array, labels)\n            self.assertTrue(np.all(tpr >= 0) and np.all(tpr <= 1) and np.all(fpr >= 0) and np.all(fpr <= 1),\n                            ""Got an invalid tpr, fpr"")\n        except:\n            pass\n\n    def test_get_pr_curve(self):\n\n        try:\n            import sklearn\n\n            array = np.random.random_sample(100)\n            labels = np.random.choice((0, 1), 100)\n\n            precision, recall = get_pr_curve(array, labels)\n            self.assertTrue(np.all(precision >= 0) and np.all(precision <= 1)\n                            and np.all(recall >= 0) and np.all(recall <= 1),\n                            ""Got an invalid precision, recall"")\n        except:\n            pass\n\n    def test_get_classification_metric(self):\n\n        try:\n            import sklearn\n\n            array = np.random.random_sample(100)\n            labels = np.random.choice((0, 1), 100)\n\n            ret_dict = get_classification_metrics(array, labels, metric=(""roc-auc"", ""pr-score""))\n\n            vals = list(ret_dict.values())\n\n            self.assertTrue(""roc-auc"" in ret_dict and ""pr-score"" in ret_dict, ""Did not get all classification metrics"")\n            self.assertTrue(vals[0] >= 0 and vals[0] <= 1\n                            and vals[1] >= 0 and vals[1] <= 1,\n                            ""Got an invalid classification metric values"")\n        except:\n            pass\n\n\nclass Net(nn.Module):\n    """"""\n    Small network to test save/load functionality\n    """"""\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
test/test_pytorchtensorboardxlogger.py,4,"b'import os\nimport tempfile\nimport unittest\nimport shutil\nimport time\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom scipy import misc\n\nfrom trixi.logger.experiment.experimentlogger import ExperimentLogger\nfrom trixi.logger.tensorboard.pytorchtensorboardlogger import PytorchTensorboardLogger\nfrom trixi.util.config import Config\n\n\nclass TestPytorchTensorboardXLogger(unittest.TestCase):\n\n    def setUp(self):\n        self.test_dir = tempfile.gettempdir()\n        self.logger = PytorchTensorboardLogger(self.test_dir)\n\n    def tearDown(self):\n        self.logger.close()\n        if os.path.exists(self.test_dir):\n            shutil.rmtree(self.test_dir, ignore_errors=True)\n\n    def test_show_text(self):\n        self.logger.show_text(""Test"")\n\n    def test_show_image(self):\n        image = np.random.random_sample((3, 128, 128))\n        self.logger.show_image(image)\n\n    def test_show_images(self):\n        images = np.random.random_sample((16, 3, 128, 128))\n        self.logger.show_images(images)\n\n    def test_show_image_grid(self):\n        images = np.random.random_sample((16, 3, 128, 128))\n        self.logger.show_image_grid(images)\n\n    def test_show_barplot(self):\n        tensor = np.random.random_sample(5)\n        self.logger.show_barplot(tensor)\n\n    def test_show_lineplot(self):\n        x = [0, 1, 2, 3, 4, 5]\n        y = np.random.random_sample(6)\n        self.logger.show_lineplot(y, x, name=""lineplot"")\n\n    def test_show_piechart(self):\n        array = np.random.random_sample(5)\n        self.logger.show_piechart(array, name=""piechart"")\n\n    def test_show_scatterplot(self):\n        array = np.random.random_sample((5, 2))\n        self.logger.show_scatterplot(array, name=""scatterplot"")\n\n    def test_show_value(self):\n        val = np.random.random_sample(1)\n        self.logger.show_value(val, ""value"")\n        val = np.random.random_sample(1)\n        self.logger.show_value(val, ""value"")\n        val = np.random.random_sample(1)\n        self.logger.show_value(val, ""value"", counter=4)\n        \n        val = np.random.random_sample(1)\n        self.logger.show_value(val, ""value1"", tag=""xD"")\n        val = np.random.random_sample(1)\n        self.logger.show_value(val, ""value2"", tag=""xD"")\n        val = np.random.random_sample(1)\n        self.logger.show_value(val, ""value1"", counter=4, tag=""xD"")\n\n    def test_show_pr_curve(self):\n        self.logger.show_pr_curve(np.random.rand(100), np.random.randint(2, size=100))\n\n    # def test_show_embedding(self):\n    #     import torch\n    #     label_img = torch.rand(100, 3, 10, 32)\n    #     for i in range(100):\n    #         label_img[i]*=i/100.0\n    #     self.logger.show_embedding(torch.randn(100, 5), label_img=label_img)\n\n    def test_show_histogram(self):\n        self.logger.show_histogram(np.random.rand(100))\n        \n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
test/test_pytorchvisdomlogger.py,15,"b'import os\nimport tempfile\nimport unittest\nimport shutil\nimport time\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom trixi.logger.visdom import PytorchVisdomLogger\nfrom trixi.logger.visdom.numpyvisdomlogger import start_visdom\n\n\nclass TestPytorchVisdomLogger(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        super(TestPytorchVisdomLogger, cls).setUpClass()\n        try:\n            start_visdom()\n        except:\n            print(""Could not start visdom, it might be already running."")\n\n    def setUp(self):\n        self.visdomLogger = PytorchVisdomLogger()\n\n    def test_show_image(self):\n        image = np.random.random_sample((3, 128, 128))\n        tensor = torch.from_numpy(image)\n        self.visdomLogger.show_image(tensor.numpy(), title=\'image\')\n\n    def test_show_images(self):\n        images = np.random.random_sample((4, 3, 128, 128))\n        tensors = torch.from_numpy(images)\n        self.visdomLogger.show_images(tensors.numpy(), title=\'images\')\n\n    def test_show_image_grid(self):\n        images = np.random.random_sample((4, 3, 128, 128))\n        tensor = torch.from_numpy(images)\n        self.visdomLogger.show_image_grid(tensor, title=""image_grid"")\n\n    def test_show_image_grid_heatmap(self):\n        images = np.random.random_sample((4, 3, 128, 128))\n        self.visdomLogger.show_image_grid_heatmap(images, title=""image_grid_heatmap"")\n\n    def test_show_barplot(self):\n        tensor = torch.from_numpy(np.random.random_sample(5))\n        self.visdomLogger.show_barplot(tensor, title=""barplot"")\n\n    def test_show_lineplot(self):\n        x = [0, 1, 2, 3, 4, 5]\n        y = np.random.random_sample(6)\n        self.visdomLogger.show_lineplot(y, x, title=""lineplot1"")\n\n    def test_show_piechart(self):\n        array = torch.from_numpy(np.random.random_sample(5))\n        self.visdomLogger.show_piechart(array, title=""piechart"")\n\n    def test_show_scatterplot(self):\n        array = torch.from_numpy(np.random.random_sample((5, 2)))\n        self.visdomLogger.show_scatterplot(array, title=""scatterplot"")\n\n    def test_show_value(self):\n        val = torch.from_numpy(np.random.random_sample(1))\n        self.visdomLogger.show_value(val, title=""value"")\n\n        val = torch.from_numpy(np.random.random_sample(1))\n        self.visdomLogger.show_value(val, title=""value"")\n\n        val = torch.from_numpy(np.random.random_sample(1))\n        self.visdomLogger.show_value(val, title=""value"", counter=4)\n\n    def test_show_text(self):\n        text = ""\\nTest 4 fun: zD ;-D 0o""\n        self.visdomLogger.show_text(text, title=\'text\')\n\n    def test_get_roc_curve(self):\n        array = np.random.random_sample(100)\n        labels = np.random.choice((0, 1), 100)\n\n        self.visdomLogger.show_roc_curve(array, labels, name=""roc"")\n\n    def test_get_pr_curve(self):\n        array = np.random.random_sample(100)\n        labels = np.random.choice((0, 1), 100)\n\n        self.visdomLogger.show_roc_curve(array, labels, name=""pr"")\n\n    def test_get_classification_metric(self):\n        array = np.random.random_sample(100)\n        labels = np.random.choice((0, 1), 100)\n\n        self.visdomLogger.show_classification_metrics(array, labels, metric=(""roc-auc"", ""pr-score""),\n                                                      name=""classification-metrics"")\n\n    def test_show_image_gradient(self):\n        net = Net()\n        random_input = torch.from_numpy(np.random.randn(28 * 28).reshape((1, 1, 28, 28))).float()\n        fake_labels = torch.from_numpy(np.array([2])).long()\n        criterion = torch.nn.CrossEntropyLoss()\n\n        def err_fn(x):\n            x = net(x)\n            return criterion(x, fake_labels)\n\n        self.visdomLogger.show_image_gradient(name=""grads-vanilla"", model=net, inpt=random_input, err_fn=err_fn,\n                                              grad_type=""vanilla"")\n        time.sleep(1)\n\n        self.visdomLogger.show_image_gradient(name=""grads-svanilla"", model=net, inpt=random_input, err_fn=err_fn,\n                                              grad_type=""smooth-vanilla"")\n        time.sleep(1)\n\n        self.visdomLogger.show_image_gradient(name=""grads-guided"", model=net, inpt=random_input, err_fn=err_fn,\n                                              grad_type=""guided"")\n        time.sleep(1)\n\n        self.visdomLogger.show_image_gradient(name=""grads-sguided"", model=net, inpt=random_input, err_fn=err_fn,\n                                              grad_type=""smooth-guided"")\n        time.sleep(1)\n\n    def test_plot_model_structure(self):\n        net = Net()\n        self.visdomLogger.plot_model_structure(net, [(1, 1, 28, 28)])\n\n\n    def test_plot_model_statistics(self):\n        net = Net()\n        self.visdomLogger.plot_model_statistics(net, plot_grad=False)\n        self.visdomLogger.plot_model_statistics(net, plot_grad=True)\n\n    def test_show_embedding(self):\n        array = torch.from_numpy(np.random.random_sample((100, 100)))\n        self.visdomLogger.show_embedding(array, method=""tsne"")\n        self.visdomLogger.show_embedding(array, method=""umap"")\n\n\nclass Net(nn.Module):\n    """"""\n    Small network to test save/load functionality\n    """"""\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
trixi/__init__.py,0,"b'use_agg = False\nimport matplotlib\nif use_agg: matplotlib.use(""Agg"", warn=False)\n\n__version__ = ""0.1.2.2""\n'"
trixi/browser.py,0,"b'from trixi.experiment_browser.browser import start_browser\n\nif __name__ == ""__main__"":\n    start_browser()\n'"
trixi/experiment/__init__.py,0,"b'from trixi.experiment.experiment import Experiment\n\ntry:\n    from trixi.experiment.pytorchexperiment import PytorchExperiment\nexcept ImportError as e:\n    import warnings\n    warnings.warn(ImportWarning(""Could not import Pytorch related modules:\\n%s""\n        % e.msg))\n'"
trixi/experiment/experiment.py,0,"b'import time\n\n\nclass Experiment(object):\n    """"""\n    An abstract Experiment which can be run for a number of epochs.\n\n    The basic life cycle of an experiment is::\n\n        setup()\n        prepare()\n\n        while epoch < n_epochs:\n            train()\n            validate()\n            epoch += 1\n\n        end()\n\n    If you want to use another criterion than number of epochs, e.g. stopping based on validation loss,\n    you can implement that in your validation method and just call .stop() at some point to break the loop.\n    Just set your n_epochs to a high number or np.inf.\n\n    The reason there is both :meth:`.setup` and :meth:`.prepare` is that internally there is also\n    a :meth:`._setup_internal` method for hidden magic in classes that inherit from this. For\n    example, the :class:`trixi.experiment.pytorchexperiment.PytorchExperiment` uses this to restore checkpoints. Think\n    of :meth:`.setup` as an :meth:`.__init__` that is only called when the Experiment is actually\n    asked to do anything. Then use :meth:`.prepare` to modify the fully instantiated Experiment if\n    you need to.\n\n    To write a new Experiment simply inherit the Experiment class and overwrite the methods.\n    You can then start your Experiment calling :meth:`.run`\n\n    In Addition the Experiment also has a test function. If you call the :meth:`.run_test` method it\n    will call the :meth:`.test` and :meth:`.end_test` method internally (and if you give the\n    parameter setup = True in run_test is will again call :meth:`.setup` and :meth:`.prepare` ).\n\n    Each Experiment also has its current state in  :attr:`_exp_state`, its start time in\n    :attr:`_time_start`, its end time in :attr:`_time_end` and the current epoch index in\n    :attr:`_epoch_idx`\n\n    Args:\n        n_epochs (int): The number of epochs in the Experiment (how often the train and validate\n            method will be called)\n\n    """"""\n\n    def __init__(self, n_epochs=0):\n\n        self.n_epochs = n_epochs\n        self._exp_state = ""Preparing""\n        self._time_start = """"\n        self._time_end = """"\n        self._epoch_idx = 0\n        self.__stop = False\n\n    def run(self, setup=True):\n        """"""\n        This method runs the Experiment. It runs through the basic lifecycle of an Experiment::\n\n            setup()\n            prepare()\n\n            while epoch < n_epochs:\n                train()\n                validate()\n                epoch += 1\n\n            end()\n\n        """"""\n\n        try:\n            self._time_start = time.strftime(""%y-%m-%d_%H:%M:%S"", time.localtime(time.time()))\n            self._time_end = """"\n\n            if setup:\n                self.setup()\n                self._setup_internal()\n\n            self.prepare()\n\n            self._exp_state = ""Started""\n            self._start_internal()\n            print(""Experiment started."")\n\n            self.__stop = False\n            while self._epoch_idx < self.n_epochs and not self.__stop:\n                self.train(epoch=self._epoch_idx)\n                self.validate(epoch=self._epoch_idx)\n                self._end_epoch_internal(epoch=self._epoch_idx)\n                self._epoch_idx += 1\n\n            self._exp_state = ""Trained""\n            print(""Training complete."")\n\n            self.end()\n            self._end_internal()\n            self._exp_state = ""Ended""\n            print(""Experiment ended."")\n\n            self._time_end = time.strftime(""%y-%m-%d_%H:%M:%S"", time.localtime(time.time()))\n\n        except Exception as e:\n\n            self._exp_state = ""Error""\n            self._time_end = time.strftime(""%y-%m-%d_%H:%M:%S"", time.localtime(time.time()))\n            self.process_err(e)\n\n    def run_test(self, setup=True):\n        """"""\n        This method runs the Experiment.\n\n        The test consist of an optional setup and then calls the :meth:`.test` and :meth:`.end_test`.\n\n        Args:\n            setup: If True it will execute the :meth:`.setup` and :meth:`.prepare` function similar to the run method\n                before calling :meth:`.test`.\n\n        """"""\n\n        try:\n\n            if setup:\n                self.setup()\n                self._setup_internal()\n\n            self.prepare()\n\n            self._exp_state = ""Testing""\n            print(""Start test."")\n\n            self.test()\n\n            self.end_test()\n            self._end_test_internal()\n\n            self._exp_state = ""Tested""\n            print(""Testing complete."")\n\n        except Exception as e:\n\n            self._exp_state = ""Error""\n            self.process_err(e)\n\n    @property\n    def epoch(self):\n        """"""Convenience access property for self._epoch_idx""""""\n        return self._epoch_idx\n\n    def setup(self):\n        """"""Is called at the beginning of each Experiment run to setup the basic components needed for a run.""""""\n        pass\n\n    def train(self, epoch):\n        """"""\n        The training part of the Experiment, it is called once for each epoch\n\n        Args:\n            epoch (int): The current epoch the train method is called in\n\n        """"""\n        pass\n\n    def validate(self, epoch):\n        """"""\n        The evaluation/validation part of the Experiment, it is called once for each epoch (after the training\n        part)\n\n        Args:\n            epoch (int):  The current epoch the validate method is called in\n\n        """"""\n        pass\n\n    def test(self):\n        """"""The testing part of the Experiment""""""\n        pass\n\n    def stop(self):\n        """"""If called the Experiment will stop after that epoch and not continue training""""""\n        self.__stop = True\n\n    def process_err(self, e):\n        """"""\n        This method is called if an error occurs during the execution of an experiment. Will just raise by default.\n\n        Args:\n            e (Exception): The exception which was raised during the experiment life cycle\n\n        """"""\n        raise e\n\n    def _setup_internal(self):\n        pass\n\n    def _end_epoch_internal(self, epoch):\n        pass\n\n    def end(self):\n        """"""Is called at the end of each experiment""""""\n        pass\n\n    def end_test(self):\n        """"""Is called at the end of each experiment test""""""\n        pass\n\n    def prepare(self):\n        """"""This method is called directly before the experiment training starts""""""\n        pass\n\n    def _start_internal(self):\n        pass\n\n    def _end_internal(self):\n        pass\n\n    def _end_test_internal(self):\n        pass\n'"
trixi/experiment/pytorchexperiment.py,11,"b'import atexit\nimport fnmatch\nimport json\nimport os\nimport numpy as np\nimport sys\nimport re\nimport random\nimport shutil\nimport string\nimport time\nimport traceback\nimport warnings\n\nimport torch\n\nfrom trixi.experiment.experiment import Experiment\nfrom trixi.logger import CombinedLogger, PytorchExperimentLogger, PytorchVisdomLogger\n\nfrom trixi.logger.tensorboard.pytorchtensorboardlogger import PytorchTensorboardLogger\nfrom trixi.util import Config, ResultElement, ResultLogDict, SourcePacker, name_and_iter_to_filename\nfrom trixi.util.config import update_from_sys_argv\nfrom trixi.util.pytorchutils import set_seed\nfrom trixi.util.util import is_picklable\n\nlogger_lookup_dict = dict(\n    visdom=PytorchVisdomLogger,\n    tensorboard=PytorchTensorboardLogger,\n)\n\ntry:\n    from trixi.logger.message.slackmessagelogger import SlackMessageLogger\n    logger_lookup_dict[""slack""] = SlackMessageLogger\nexcept:\n    pass\n\ntry:\n    from trixi.logger import TelegramMessageLogger\n    logger_lookup_dict[""telegram""] = TelegramMessageLogger\nexcept:\n    pass\n\n\nclass PytorchExperiment(Experiment):\n    """"""\n    A PytorchExperiment extends the basic\n    functionality of the :class:`.Experiment` class with\n    convenience features for PyTorch (and general logging) such as creating a folder structure,\n    saving, plotting results and checkpointing your experiment.\n\n    The basic life cycle of a PytorchExperiment is the same as\n    :class:`.Experiment`::\n\n        setup()\n        prepare()\n\n        for epoch in n_epochs:\n            train()\n            validate()\n\n        end()\n\n    where the distinction between the first two is that between them\n    PytorchExperiment will automatically restore checkpoints and save the\n    :attr:`_config_raw` in :meth:`._setup_internal`. Please see below for more\n    information on this.\n    To get your own experiment simply inherit from the PytorchExperiment and\n    overwrite the :meth:`.setup`, :meth:`.prepare`, :meth:`.train`,\n    :meth:`.validate` method (or you can use the `very` experimental decorator\n    :func:`.experimentify` to convert your class into a experiment).\n    Then you can run your own experiment by calling the :meth:`.run` method.\n\n    Internally PytorchExperiment will provide a number of member variables which\n    you can access.\n\n        - n_epochs\n            Number of epochs.\n        - exp_name\n            Name of your experiment.\n        - config\n            The (initialized) :class:`.Config` of your experiment. You can\n            access the uninitialized one via :attr:`_config_raw`.\n        - result\n            A dict in which you can store your result values. If a\n            :class:`.PytorchExperimentLogger` is used, results will be a\n            :class:`.ResultLogDict` that directly automatically writes to a file\n            and also stores the N last entries for each key for quick access\n            (e.g. to quickly get the running mean).\n        - elog (if base_dir is given)\n            A :class:`.PytorchExperimentLogger` instance which can log your\n            results to a given folder. Will automatically be created if a base_dir is available.\n        - loggers\n            Contains all loggers you provide, including the experiment logger, accessible by the\n            names you provide.\n        - clog\n            A :class:`.CombinedLogger` instance which logs to all loggers with\n            different frequencies (specified with the last entry in the tuple you provide for each\n            logger where 1 means every time and N means every Nth time,\n            e.g. if you only want to send stuff to Visdom every 10th time).\n\n    The most important attribute is certainly :attr:`.config`, which is the\n    initialized :class:`.Config` for the experiment. To understand how it needs\n    to be structured to allow for automatic instantiation of types, please refer\n    to its documentation. If you decide not to use this functionality,\n    :attr:`config` and :attr:`_config_raw` are identical. **Beware however that\n    by default the Pytorchexperiment only saves the raw config** after\n    :meth:`.setup`. If you modify :attr:`config` during setup, make sure\n    to implement :meth:`._setup_internal` yourself should you want the modified\n    config to be saved::\n\n        def _setup_internal(self):\n\n            super(YourExperiment, self)._setup_internal() # calls .prepare_resume()\n            self.elog.save_config(self.config, ""config"")\n\n    Args:\n        config (dict or Config): Configures your experiment. If :attr:`name`,\n            :attr:`n_epochs`, :attr:`seed`, :attr:`base_dir` are given in the\n            config, it will automatically\n            overwrite the other args/kwargs with the values from the config.\n            In addition (defined by :attr:`parse_config_sys_argv`) the config\n            automatically parses the argv arguments and updates its values if a\n            key matches a console argument.\n        name (str):\n            The name of the PytorchExperiment.\n        n_epochs (int): The number of epochs (number of times the training\n            cycle will be executed).\n        seed (int): A random seed (which will set the random, numpy and\n            torch seed).\n        base_dir (str): A base directory in which the experiment result folder\n            will be created. A :class:`.PytorchExperimentLogger` instance will be created if\n            this is given.\n        globs: The :func:`globals` of the script which is run. This is necessary\n            to get and save the executed files in the experiment folder.\n        resume (str or PytorchExperiment): Another PytorchExperiment or path to\n            the result dir from another PytorchExperiment from which it will\n            load the PyTorch modules and other member variables and resume\n            the experiment.\n        ignore_resume_config (bool): If :obj:`True` it will not resume with the\n            config from the resume experiment but take the current/own config.\n        resume_save_types (list or tuple): A list which can define which values\n            to restore when resuming. Choices are:\n\n                - ""model"" <-- Pytorch models\n                - ""optimizer"" <-- Optimizers\n                - ""simple"" <-- Simple python variables (basic types and lists/tuples\n                - ""th_vars"" <-- torch tensors/variables\n                - ""results"" <-- The result dict\n\n        resume_reset_epochs (bool): Set epoch to zero if you resume an existing experiment.\n        parse_sys_argv (bool): Parsing the console arguments (argv) to get a\n            :attr:`config path` and/or :attr:`resume_path`.\n        parse_config_sys_argv (bool): Parse argv to update the config\n            (if the keys match).\n        checkpoint_to_cpu (bool): When checkpointing, transfer all tensors to\n            the CPU beforehand.\n        save_checkpoint_every_epoch (int): Determines after how many epochs a\n            checkpoint is stored.\n        explogger_kwargs (dict): Keyword arguments for :attr:`elog`\n            instantiation.\n        explogger_freq (int): The frequency x (meaning one in x) with which\n            the :attr:`clog` will call the :attr:`elog`.\n        loggers (dict): Specify additional loggers.\n            Entries should have one of these formats::\n\n                ""name"": ""identifier"" (will default to a frequency of 10)\n                ""name"": (""identifier""(, kwargs, frequency)) (last two are optional)\n\n            ""identifier"" is one of ""telegram"", ""tensorboard"", ""visdom"", ""slack"".\n        append_rnd_to_name (bool): If :obj:`True`, will append a random six\n            digit string to the experiment name.\n        save_checkpoints_default (bool): By default save the current and the last checkpoint or not.\n\n     """"""\n\n    def __init__(self,\n                 config=None,\n                 name=None,\n                 n_epochs=None,\n                 seed=None,\n                 base_dir=None,\n                 globs=None,\n                 resume=None,\n                 ignore_resume_config=False,\n                 resume_save_types=(""model"", ""optimizer"", ""simple"", ""th_vars"", ""results""),\n                 resume_reset_epochs=True,\n                 parse_sys_argv=False,\n                 checkpoint_to_cpu=True,\n                 save_checkpoint_every_epoch=1,\n                 explogger_kwargs=None,\n                 explogger_freq=1,\n                 loggers=None,\n                 append_rnd_to_name=False,\n                 default_save_types=(""model"", ""optimizer"", ""simple"", ""th_vars"", ""results""),\n                 save_checkpoints_default=True):\n\n        # super(PytorchExperiment, self).__init__()\n        Experiment.__init__(self)\n\n        # check for command line inputs for config_path and resume_path,\n        # will be prioritized over config and resume!\n        config_path_from_argv = None\n        if parse_sys_argv:\n            config_path_from_argv, resume_path_from_argv = get_vars_from_sys_argv()\n            if resume_path_from_argv:\n                resume = resume_path_from_argv\n\n        # construct _config_raw\n        if config_path_from_argv is None:\n            self._config_raw = self._config_raw_from_input(config, name, n_epochs, seed, append_rnd_to_name)\n        else:\n            self._config_raw = Config(file_=config_path_from_argv)\n        update_from_sys_argv(self._config_raw)\n\n        # set a few experiment attributes\n        self.n_epochs = self._config_raw[""n_epochs""]\n        self._seed = self._config_raw[\'seed\']\n        set_seed(self._seed)\n        self.exp_name = self._config_raw[""name""]\n        self._checkpoint_to_cpu = checkpoint_to_cpu\n        self._save_checkpoint_every_epoch = save_checkpoint_every_epoch\n        self._default_save_types = default_save_types\n        self._save_checkpoint_default = save_checkpoints_default\n        self.results = dict()\n\n        # get base_dir from _config_raw or store there\n        if base_dir is not None:\n            self._config_raw[""base_dir""] = base_dir\n        base_dir = self._config_raw[""base_dir""]\n\n        # Construct experiment logger (automatically activated if base_dir is there)\n        self.loggers = {}\n        logger_list = []\n        if base_dir is not None:\n            if explogger_kwargs is None:\n                explogger_kwargs = {}\n            self.elog = PytorchExperimentLogger(base_dir=base_dir,\n                                                exp_name=self.exp_name,\n                                                **explogger_kwargs)\n            if explogger_freq is not None and explogger_freq > 0:\n                logger_list.append((self.elog, explogger_freq))\n            self.results = ResultLogDict(""results-log.json"", base_dir=self.elog.result_dir)\n        else:\n            self.elog = None\n\n        # Construct other loggers\n        if loggers is not None:\n            for logger_name, logger_cfg in loggers.items():\n                _logger, log_freq = self._make_logger(logger_name, logger_cfg)\n                self.loggers[logger_name] = _logger\n                if log_freq is not None and log_freq > 0:\n                    logger_list.append((_logger, log_freq))\n\n        self.clog = CombinedLogger(*logger_list)\n\n        # Set resume attributes and update _config_raw,\n        # actual resuming is done automatically after setup in _setup_internal\n        self._resume_path = None\n        self._resume_save_types = resume_save_types\n        self._ignore_resume_config = ignore_resume_config\n        self._resume_reset_epochs = resume_reset_epochs\n        if resume is not None:\n            if isinstance(resume, str):\n                if resume == ""last"":\n                    if base_dir is None:\n                        raise ValueError(""resume=\'last\' requires base_dir."")\n                    self._resume_path = os.path.join(base_dir, sorted(os.listdir(base_dir))[-1])\n                else:\n                    self._resume_path = resume\n            elif isinstance(resume, PytorchExperiment):\n                self._resume_path = resume.elog.base_dir\n        if self._resume_path is not None and not self._ignore_resume_config:\n            self._config_raw.update(Config(file_=os.path.join(self._resume_path, ""config"", ""config.json"")),\n                                    ignore=list(map(lambda x: re.sub(""^-+"", """", x), sys.argv)))\n\n        # Save everything we need to reproduce experiment\n        if globs is not None and self.elog is not None:\n            zip_name = os.path.join(self.elog.save_dir, ""sources.zip"")\n            SourcePacker.zip_sources(globs, zip_name)\n\n        # Init objects in config\n        self.config = Config.init_objects(self._config_raw)\n\n        atexit.register(self.at_exit_func)\n\n    def _config_raw_from_input(self,\n                               config=None,\n                               name=None,\n                               n_epochs=None,\n                               seed=None,\n                               append_rnd_to_name=False):\n        """"""Construct _config_raw from input.""""""\n\n        _config_raw = None\n        if isinstance(config, str):\n            _config_raw = Config(file_=config)\n        elif isinstance(config, (Config, dict)):\n            _config_raw = Config(config=config)\n        else:\n            _config_raw = Config()\n\n        if n_epochs is None and _config_raw.get(""n_epochs"") is not None:\n            n_epochs = _config_raw[""n_epochs""]\n        elif n_epochs is None and _config_raw.get(""n_epochs"") is None:\n            n_epochs = 0\n        _config_raw[""n_epochs""] = n_epochs\n\n        if seed is None and _config_raw.get(\'seed\') is not None:\n            seed = _config_raw[\'seed\']\n        elif seed is None and _config_raw.get(\'seed\') is None:\n            random_data = os.urandom(4)\n            seed = int.from_bytes(random_data, byteorder=""big"")\n        _config_raw[\'seed\'] = seed\n\n        if name is None and _config_raw.get(""name"") is not None:\n            name = _config_raw[""name""]\n        elif name is None and _config_raw.get(""name"") is None:\n            name = ""experiment""\n        if append_rnd_to_name:\n            rnd_str = \'\'.join(random.choice(string.ascii_letters + string.digits) for _ in range(5))\n            name += ""_"" + rnd_str\n        _config_raw[""name""] = name\n\n        return _config_raw\n\n    def _make_logger(self, logger_name, logger_cfg):\n\n        if isinstance(logger_cfg, (list, tuple)):\n            log_type = logger_cfg[0]\n            log_params = logger_cfg[1] if len(logger_cfg) > 1 else {}\n            log_freq = logger_cfg[2] if len(logger_cfg) > 2 else 10\n        else:\n            assert isinstance(logger_cfg, str), ""The specified logger has to either be a string or a list with "" \\\n                                                ""name, parameters, clog_frequency""\n            log_type = logger_cfg\n            log_params = {}\n            log_freq = 10\n\n        if ""exp_name"" not in log_params:\n            log_params[""exp_name""] = self.exp_name\n\n        if log_type == ""tensorboard"":\n            if ""target_dir"" not in log_params or log_params[""target_dir""] is None:\n                if self.elog is not None:\n                    log_params[""target_dir""] = os.path.join(self.elog.save_dir, ""tensorboard"")\n                else:\n                    raise AttributeError(""TensorboardLogger requires a target_dir or an ExperimentLogger instance."")\n            elif self.elog is not None:\n                log_params[""target_dir""] = os.path.join(log_params[""target_dir""], self.elog.folder_name)\n\n        log_type = logger_lookup_dict[log_type]\n        _logger = log_type(**log_params)\n\n        return _logger, log_freq\n\n    @property\n    def vlog(self):\n        if ""visdom"" in self.loggers:\n            return self.loggers[""visdom""]\n        elif ""v"" in self.loggers:\n            return self.loggers[""v""]\n        else:\n            return None\n\n    @property\n    def tlog(self):\n        if ""telegram"" in self.loggers:\n            return self.loggers[""telegram""]\n        elif ""t"" in self.loggers:\n            return self.loggers[""t""]\n        else:\n            return None\n\n    @property\n    def txlog(self):\n        return self.tblog\n\n    @property\n    def tblog(self):\n        if ""tensorboard"" in self.loggers:\n            return self.loggers[""tensorboard""]\n        if ""tensorboardx"" in self.loggers:\n            return self.loggers[""tensorboardx""]\n        elif ""tx"" in self.loggers:\n            return self.loggers[""tx""]\n        else:\n            return None\n\n    @property\n    def slog(self):\n        if ""slack"" in self.loggers:\n            return self.loggers[""slack""]\n        elif ""s"" in self.loggers:\n            return self.loggers[""s""]\n        else:\n            return None\n\n    def process_err(self, e):\n        if self.elog is not None:\n            self.elog.text_logger.log_to(""\\n"".join(traceback.format_tb(e.__traceback__)), ""err"")\n            self.elog.text_logger.log_to(repr(e), ""err"")\n        raise e\n\n    def update_attributes(self, var_dict, ignore=()):\n        """"""\n        Updates the member attributes with the attributes given in the var_dict\n\n        Args:\n            var_dict (dict): dict in which the update values stored. If a key matches a member attribute name\n                the member attribute will be updated\n            ignore (list or tuple): iterable of keys to ignore\n\n        """"""\n        for key, val in var_dict.items():\n            if key == ""results"":\n                self.results.load(val)\n                continue\n            if key in ignore:\n                continue\n            if hasattr(self, key):\n                setattr(self, key, val)\n\n    def get_pytorch_modules(self, from_config=True):\n        """"""\n        Returns all torch.nn.Modules stored in the experiment in a dict (even child dicts are stored).\n\n        Args:\n            from_config (bool): Also get modules that are stored in the :attr:`.config` attribute.\n\n        Returns:\n            dict: Dictionary of PyTorch modules\n\n        """"""\n\n        def parse_torchmodules_recursive(input, output):\n            if isinstance(input, dict):\n                for key, value in input.items():\n                    if isinstance(value, dict):\n                        parse_torchmodules_recursive(value, output)\n                    elif isinstance(value, torch.nn.Module):\n                        output[key] = value\n\n        pyth_modules = dict()\n        parse_torchmodules_recursive(self.__dict__, pyth_modules)\n        # for key, val in self.__dict__.items():\n        #     if isinstance(val, torch.nn.Module):\n        #         pyth_modules[key] = val\n        if from_config:\n            for key, val in self.config.items():\n                if isinstance(val, torch.nn.Module):\n                    if type(key) == str:\n                        key = ""config."" + key\n                    pyth_modules[key] = val\n        return pyth_modules\n\n    def get_pytorch_optimizers(self, from_config=True):\n        """"""\n        Returns all torch.optim.Optimizers stored in the experiment in a dict.\n\n        Args:\n            from_config (bool): Also get optimizers that are stored in the :attr:`.config`\n                attribute.\n\n        Returns:\n            dict: Dictionary of PyTorch optimizers\n\n        """"""\n\n        pyth_optimizers = dict()\n        for key, val in self.__dict__.items():\n            if isinstance(val, torch.optim.Optimizer):\n                pyth_optimizers[key] = val\n        if from_config:\n            for key, val in self.config.items():\n                if isinstance(val, torch.optim.Optimizer):\n                    if type(key) == str:\n                        key = ""config."" + key\n                    pyth_optimizers[key] = val\n        return pyth_optimizers\n\n    def get_simple_variables(self, ignore=()):\n        """"""\n        Returns all standard variables in the experiment in a dict.\n        Specifically, this looks for types :class:`int`, :class:`float`, :class:`bytes`,\n        :class:`bool`, :class:`str`, :class:`set`, :class:`list`, :class:`tuple`.\n\n        Args:\n            ignore (list or tuple): Iterable of names which will be ignored\n\n        Returns:\n            dict: Dictionary of variables\n\n        """"""\n\n        simple_vars = dict()\n        for key, val in self.__dict__.items():\n            if key in ignore:\n                continue\n            if isinstance(val, (int, float, bytes, bool, str, set, list, tuple)):\n                if is_picklable(val):\n                    simple_vars[key] = val\n        return simple_vars\n\n    def get_pytorch_tensors(self, ignore=()):\n        """"""\n        Returns all torch.tensors in the experiment in a dict.\n\n        Args:\n            ignore (list or tuple): Iterable of names which will be ignored\n\n        Returns:\n            dict: Dictionary of PyTorch tensor\n\n        """"""\n\n        pytorch_vars = dict()\n        for key, val in self.__dict__.items():\n            if key in ignore:\n                continue\n            if torch.is_tensor(val):\n                pytorch_vars[key] = val\n        return pytorch_vars\n\n    def get_pytorch_variables(self, ignore=()):\n        """"""Same as :meth:`.get_pytorch_tensors`.""""""\n        return self.get_pytorch_tensors(ignore)\n\n    def save_results(self, name=""results.json""):\n        """"""\n        Saves the result dict as a json file in the result dir of the experiment logger.\n\n        Args:\n            name (str): The name of the json file in which the results are written.\n\n        """"""\n        if self.elog is None:\n            return\n        with open(os.path.join(self.elog.result_dir, name), ""w"") as file_:\n            json.dump(self.results, file_, indent=4)\n\n    def save_pytorch_models(self):\n        """"""Saves all torch.nn.Modules as model files in the experiment checkpoint folder.""""""\n\n        if self.elog is None:\n            return\n\n        pyth_modules = self.get_pytorch_modules()\n        for key, val in pyth_modules.items():\n            self.elog.save_model(val, key)\n\n    def load_pytorch_models(self):\n        """"""Loads all model files from the experiment checkpoint folder.""""""\n\n        if self.elog is None:\n            return\n        pyth_modules = self.get_pytorch_modules()\n        for key, val in pyth_modules.items():\n            self.elog.load_model(val, key)\n\n    def log_simple_vars(self):\n        """"""\n        Logs all simple python member variables as a json file in the experiment log folder.\n        The file will be names \'simple_vars.json\'.\n        """"""\n\n        if self.elog is None:\n            return\n        simple_vars = self.get_simple_variables()\n        with open(os.path.join(self.elog.log_dir, ""simple_vars.json""), ""w"") as file_:\n            json.dump(simple_vars, file_)\n\n    def load_simple_vars(self):\n        """"""\n        Restores all simple python member variables from the \'simple_vars.json\' file in the log\n        folder.\n        """"""\n\n        if self.elog is None:\n            return\n        simple_vars = {}\n        with open(os.path.join(self.elog.log_dir, ""simple_vars.json""), ""r"") as file_:\n            simple_vars = json.load(file_)\n        self.update_attributes(simple_vars)\n\n    def save_checkpoint(self,\n                        name=""checkpoint"",\n                        save_types=(""model"", ""optimizer"", ""simple"", ""th_vars"", ""results""),\n                        n_iter=None,\n                        iter_format=""{:05d}"",\n                        prefix=False):\n        """"""\n        Saves a current model checkpoint from the experiment.\n\n        Args:\n            name (str): The name of the checkpoint file\n            save_types (list or tuple): What kind of member variables should be stored? Choices are:\n                ""model"" <-- Pytorch models,\n                ""optimizer"" <-- Optimizers,\n                ""simple"" <-- Simple python variables (basic types and lists/tuples),\n                ""th_vars"" <-- torch tensors,\n                ""results"" <-- The result dict\n            n_iter (int): Number of iterations. Together with the name, defined by the iter_format,\n                a file name will be created.\n            iter_format (str): Defines how the name and the n_iter will be combined.\n            prefix (bool): If True, the formatted n_iter will be prepended, otherwise appended.\n\n        """"""\n\n        if self.elog is None:\n            return\n\n        model_dict = {}\n        optimizer_dict = {}\n        simple_dict = {}\n        th_vars_dict = {}\n        results_dict = {}\n\n        if ""model"" in save_types:\n            model_dict = self.get_pytorch_modules()\n        if ""optimizer"" in save_types:\n            optimizer_dict = self.get_pytorch_optimizers()\n        if ""simple"" in save_types:\n            simple_dict = self.get_simple_variables()\n        if ""th_vars"" in save_types:\n            th_vars_dict = self.get_pytorch_variables()\n        if ""results"" in save_types:\n            results_dict = {""results"": self.results}\n\n        checkpoint_dict = {**model_dict, **optimizer_dict, **simple_dict, **th_vars_dict, **results_dict}\n\n        self.elog.save_checkpoint(name=name, n_iter=n_iter, iter_format=iter_format, prefix=prefix,\n                                  move_to_cpu=self._checkpoint_to_cpu, **checkpoint_dict)\n\n    def load_checkpoint(self,\n                        name=""checkpoint"",\n                        save_types=(""model"", ""optimizer"", ""simple"", ""th_vars"", ""results""),\n                        n_iter=None,\n                        iter_format=""{:05d}"",\n                        prefix=False,\n                        path=None):\n        """"""\n        Loads a checkpoint and restores the experiment.\n\n        Make sure you have your torch stuff already on the right devices beforehand,\n        otherwise this could lead to errors e.g. when making a optimizer step\n        (and for some reason the Adam states are not already on the GPU:\n        https://discuss.pytorch.org/t/loading-a-saved-model-for-continue-training/17244/3 )\n\n        Args:\n            name (str): The name of the checkpoint file\n            save_types (list or tuple): What kind of member variables should be loaded? Choices are:\n                ""model"" <-- Pytorch models,\n                ""optimizer"" <-- Optimizers,\n                ""simple"" <-- Simple python variables (basic types and lists/tuples),\n                ""th_vars"" <-- torch tensors,\n                ""results"" <-- The result dict\n            n_iter (int): Number of iterations. Together with the name, defined by the iter_format,\n                a file name will be created and searched for.\n            iter_format (str): Defines how the name and the n_iter will be combined.\n            prefix (bool): If True, the formatted n_iter will be prepended, otherwise appended.\n            path (str): If no path is given then it will take the current experiment dir and formatted\n                name, otherwise it will simply use the path and the formatted name to define the\n                checkpoint file.\n\n        """"""\n        if self.elog is None:\n            return\n\n        model_dict = {}\n        optimizer_dict = {}\n        simple_dict = {}\n        th_vars_dict = {}\n        results_dict = {}\n\n        if ""model"" in save_types:\n            model_dict = self.get_pytorch_modules()\n        if ""optimizer"" in save_types:\n            optimizer_dict = self.get_pytorch_optimizers()\n        if ""simple"" in save_types:\n            simple_dict = self.get_simple_variables()\n        if ""th_vars"" in save_types:\n            th_vars_dict = self.get_pytorch_variables()\n        if ""results"" in save_types:\n            results_dict = {""results"": self.results}\n\n        checkpoint_dict = {**model_dict, **optimizer_dict, **simple_dict, **th_vars_dict, **results_dict}\n\n        if n_iter is not None:\n            name = name_and_iter_to_filename(name,\n                                             n_iter,\n                                             "".pth.tar"",\n                                             iter_format=iter_format,\n                                             prefix=prefix)\n\n        if path is None:\n            restore_dict = self.elog.load_checkpoint(name=name, **checkpoint_dict)\n        else:\n            checkpoint_path = os.path.join(path, name)\n            if checkpoint_path.endswith(os.sep):\n                checkpoint_path = os.path.dirname(checkpoint_path)\n            restore_dict = self.elog.load_checkpoint_static(checkpoint_file=checkpoint_path, **checkpoint_dict)\n\n        self.update_attributes(restore_dict)\n\n    def _end_internal(self):\n        """"""Ends the experiment and stores the final results/checkpoint""""""\n        if isinstance(self.results, ResultLogDict):\n            self.results.close()\n        self.save_results()\n        self.save_end_checkpoint()\n        self._save_exp_config()\n        self.print(""Experiment ended. Checkpoints stored =)"")\n\n    def _end_test_internal(self):\n        """"""Ends the experiment after test and stores the final results and config""""""\n        self.save_results()\n        self._save_exp_config()\n        self.print(""Testing ended. Results stored =)"")\n\n    def at_exit_func(self):\n        """"""\n        Stores the results and checkpoint at the end (if not already stored).\n        This method is also called if an error occurs.\n        """"""\n\n        if self._exp_state not in (""Ended"", ""Tested""):\n            if isinstance(self.results, ResultLogDict):\n                self.results.print_to_file(""]"")\n            self.save_checkpoint(name=""checkpoint_exit-"" + self._exp_state, save_types=self._default_save_types)\n            self.save_results()\n            self._save_exp_config()\n            self.print(""Experiment exited. Checkpoints stored =)"")\n        time.sleep(2)  # allow checkpoint saving to finish. We need a better solution for this :D\n\n    def _setup_internal(self):\n        self.prepare_resume()\n\n        if self.elog is not None:\n            self.elog.save_config(self._config_raw, ""config"")\n        self._save_exp_config()\n\n    def _start_internal(self):\n        self._save_exp_config()\n\n    def prepare_resume(self):\n        """"""Tries to resume the experiment by using the defined resume path or PytorchExperiment.""""""\n\n        checkpoint_file = """"\n        base_dir = """"\n\n        reset_epochs = self._resume_reset_epochs\n\n        if self._resume_path is not None:\n            if isinstance(self._resume_path, str):\n                if self._resume_path.endswith("".pth.tar""):\n                    checkpoint_file = self._resume_path\n                    base_dir = os.path.dirname(os.path.dirname(checkpoint_file))\n                elif self._resume_path.endswith(""checkpoint"") or self._resume_path.endswith(""checkpoint/""):\n                    checkpoint_file = get_last_file(self._resume_path)\n                    base_dir = os.path.dirname(os.path.dirname(checkpoint_file))\n                elif ""checkpoint"" in os.listdir(self._resume_path) and ""config"" in os.listdir(self._resume_path):\n                    checkpoint_file = get_last_file(self._resume_path)\n                    base_dir = self._resume_path\n                else:\n                    warnings.warn(""You have not selected a valid experiment folder, will search all sub folders"",\n                                  UserWarning)\n                    if self.elog is not None:\n                        self.elog.text_logger.log_to(""You have not selected a valid experiment folder, will search all ""\n                                                     ""sub folders"", ""warnings"")\n                    checkpoint_file = get_last_file(self._resume_path)\n                    base_dir = os.path.dirname(os.path.dirname(checkpoint_file))\n\n        # if base_dir:\n        #     if not self._ignore_resume_config:\n        #         load_config = Config()\n        #         load_config.load(os.path.join(base_dir, ""config/config.json""))\n        #         self._config_raw = load_config\n        #         self.config = Config.init_objects(self._config_raw)\n        #         self.print(""Loaded existing config from:"", base_dir)\n        #         if self.n_epochs is None:\n        #             self.n_epochs = self._config_raw.get(""n_epochs"")\n\n        if checkpoint_file:\n            self.load_checkpoint(name="""", path=checkpoint_file, save_types=self._resume_save_types)\n            self._resume_path = checkpoint_file\n            shutil.copyfile(checkpoint_file, os.path.join(self.elog.checkpoint_dir, ""0_checkpoint.pth.tar""))\n            self.print(""Loaded existing checkpoint from:"", checkpoint_file)\n\n            self._resume_reset_epochs = reset_epochs\n            if self._resume_reset_epochs:\n                self._epoch_idx = 0\n\n    def _end_epoch_internal(self, epoch):\n        self.save_results()\n        if self._save_checkpoint_every_epoch is not None and self._save_checkpoint_every_epoch > 0 and epoch % \\\n                self._save_checkpoint_every_epoch == 0:\n            self.save_temp_checkpoint()\n        self._save_exp_config()\n\n    def _save_exp_config(self):\n\n        if self.elog is not None:\n            cur_time = time.strftime(""%y-%m-%d_%H:%M:%S"", time.localtime(time.time()))\n            self.elog.save_config(Config(**{\'name\': self.exp_name,\n                                            \'time\': self._time_start,\n                                            \'state\': self._exp_state,\n                                            \'current_time\': cur_time,\n                                            \'epoch\': self._epoch_idx\n                                            }),\n                                  ""exp"")\n\n    def save_temp_checkpoint(self):\n        """"""Saves the current checkpoint as checkpoint_current.""""""\n        if self._save_checkpoint_default:\n            self.save_checkpoint(name=""checkpoint_current"", save_types=self._default_save_types)\n\n    def save_end_checkpoint(self):\n        """"""Saves the current checkpoint as checkpoint_last.""""""\n        if self._save_checkpoint_default:\n            self.save_checkpoint(name=""checkpoint_last"", save_types=self._default_save_types)\n\n    def add_result(self, value, name, counter=None, tag=None, label=None, plot_result=True, plot_running_mean=False):\n        """"""\n        Saves a results and add it to the result dict, this is similar to results[key] = val,\n        but in addition also logs the value to the combined logger\n        (it also stores in the results-logs file).\n\n        **This should be your preferred method to log your numeric values**\n\n        Args:\n            value: The value of your variable\n            name (str): The name/key of your variable\n            counter (int or float): A counter which can be seen as the x-axis of your value.\n                Normally you would just use the current epoch for this.\n            tag (str): A label/tag which can group similar values and will plot values with the same\n                label in the same plot\n            label: deprecated label\n            plot_result (bool): By default True, will also log all your values to the combined\n                logger (with show_value).\n\n        """"""\n\n        if label is not None:\n            warnings.warn(""label in add_result is deprecated, please use tag instead"")\n\n            if tag is None:\n                tag = label\n\n        tag_name = tag\n        if tag_name is None:\n            tag_name = name\n\n        r_elem = ResultElement(data=value, label=tag_name, epoch=self._epoch_idx, counter=counter)\n\n        self.results[name] = r_elem\n\n        if plot_result:\n            if tag is None:\n                legend = False\n            else:\n                legend = True\n            if plot_running_mean:\n                value = np.mean(self.results.running_mean_dict[name])\n            self.clog.show_value(value=value, name=name, tag=tag, counter=counter, show_legend=legend)\n\n    def get_result(self, name):\n        """"""\n        Similar to result[key] this will return the values in the results dictionary with the given\n        name/key.\n\n        Args:\n            name (str): the name/key for which a value is stored.\n\n        Returns:\n            The value with the key \'name\' in the results dict.\n\n        """"""\n        return self.results.get(name)\n\n    def add_result_without_epoch(self, val, name):\n        """"""\n        A faster method to store your results, has less overhead and does not call the combined\n        logger. Will only store to the results dictionary.\n\n        Args:\n            val: the value you want to add.\n            name (str): the name/key of your value.\n\n        """"""\n        self.results[name] = val\n\n    def get_result_without_epoch(self, name):\n        """"""\n        Similar to result[key] this will return the values in result with the given name/key.\n\n        Args:\n            name (str): the name/ key for which a value is stores.\n\n        Returns:\n            The value with the key \'name\' in the results dict.\n\n        """"""\n        return self.results.get(name)\n\n    def print(self, *args):\n        """"""\n        Calls \'print\' on the experiment logger or uses builtin \'print\' if former is not\n        available.\n        """"""\n\n        if self.elog is None:\n            print(*args)\n        else:\n            self.elog.print(*args)\n\n\ndef get_last_file(dir_, name=None):\n    """"""\n    Returns the most recently created file in the folder which matches the name supplied\n\n    Args:\n        dir_: The base directory to start the search in\n        name: The name pattern to match with the files\n\n    Returns:\n        str: the path to the most recent file\n\n    """"""\n    if name is None:\n        name = ""*checkpoint*.pth.tar""\n\n    dir_files = []\n\n    for root, dirs, files in os.walk(dir_):\n        for filename in fnmatch.filter(files, name):\n            # if \'last\' in filename:\n            #    return os.path.join(root, filename)\n            checkpoint_file = os.path.join(root, filename)\n            dir_files.append(checkpoint_file)\n\n    if len(dir_files) == 0:\n        return """"\n\n    last_file = max(dir_files, key=os.path.getctime)\n\n    return last_file\n\n\ndef get_vars_from_sys_argv():\n    """"""\n    Parses the command line args (argv) and looks for --config_path and --resume_path and returns them if found.\n\n    Returns:\n        tuple: a tuple of (config_path, resume_path ) , None if it is not found\n\n    """"""\n    import sys\n    import argparse\n\n    if len(sys.argv) > 1:\n\n        parser = argparse.ArgumentParser()\n\n        # parse just config keys\n        parser.add_argument(""config_path"", type=str)\n        parser.add_argument(""resume_path"", type=str)\n\n        # parse args\n        param, unknown = parser.parse_known_args()\n\n        if len(unknown) > 0:\n            warnings.warn(""Called with unknown arguments: %s"" % unknown, RuntimeWarning)\n\n        # update dict\n        return param.config_path, param.resume_path\n\n\ndef experimentify(setup_fn=""setup"", train_fn=""train"", validate_fn=""validate"", end_fn=""end"", test_fn=""test"", **decoargs):\n    """"""\n    Experimental decorator which monkey patches your class into a PytorchExperiment.\n    You can then call run on your new :class:`.PytorchExperiment` class.\n\n    Args:\n        setup_fn: The name of your setup() function\n        train_fn: The name of your train() function\n        validate_fn: The name of your validate() function\n        end_fn: The name of your end() function\n        test_fn: The name of your test() function\n\n    """"""\n\n    def wrap(cls):\n\n        ### Initilaize both Classes (as original class)\n        prev_init = cls.__init__\n\n        def new_init(*args, **kwargs):\n            prev_init(*args, **kwargs)\n            kwargs.update(decoargs)\n            PytorchExperiment.__init__(*args, **kwargs)\n\n        cls.__init__ = new_init\n\n        ### Set new Experiment methods\n        if not hasattr(cls, ""setup"") and hasattr(cls, setup_fn):\n            setattr(cls, ""setup"", getattr(cls, setup_fn))\n        elif hasattr(cls, ""setup"") and setup_fn != ""setup"":\n            warnings.warn(""Found already exisiting setup function in class, so will use the exisiting one"")\n\n        if not hasattr(cls, ""train"") and hasattr(cls, train_fn):\n            setattr(cls, ""train"", getattr(cls, train_fn))\n        elif hasattr(cls, ""train"") and setup_fn != ""train"":\n            warnings.warn(""Found already exisiting train function in class, so will use the exisiting one"")\n\n        if not hasattr(cls, ""validate"") and hasattr(cls, validate_fn):\n            setattr(cls, ""validate"", getattr(cls, validate_fn))\n        elif hasattr(cls, ""validate"") and setup_fn != ""validate"":\n            warnings.warn(""Found already exisiting validate function in class, so will use the exisiting one"")\n\n        if not hasattr(cls, ""end"") and hasattr(cls, end_fn):\n            setattr(cls, ""end"", getattr(cls, end_fn))\n        elif hasattr(cls, ""end"") and end_fn != ""end"":\n            warnings.warn(""Found already exisiting end function in class, so will use the exisiting one"")\n\n        if not hasattr(cls, ""test"") and hasattr(cls, test_fn):\n            setattr(cls, ""test"", getattr(cls, test_fn))\n        elif hasattr(cls, ""test"") and test_fn != ""test"":\n            warnings.warn(""Found already exisiting test function in class, so will use the exisiting one"")\n\n        ### Copy methods from PytorchExperiment into the original class\n        for elem in dir(PytorchExperiment):\n            if not hasattr(cls, elem):\n                trans_fn = getattr(PytorchExperiment, elem)\n                setattr(cls, elem, trans_fn)\n\n        return cls\n\n    return wrap\n'"
trixi/experiment_browser/__init__.py,0,b'from trixi.experiment_browser.experimentreader import ExperimentReader\n'
trixi/experiment_browser/browser.py,0,"b'import argparse\nimport json\nimport os\nimport sys\nfrom collections import OrderedDict\n\nimport colorlover as cl\nfrom flask import Blueprint, Flask, abort, render_template, request\n\nfrom trixi.experiment_browser.dataprocessing import group_images, make_graphs, merge_results, process_base_dir\nfrom trixi.experiment_browser.experimentreader import ExperimentReader, CombiExperimentReader, group_experiments_by\nfrom trixi.util import Config\n\n# These keys will be ignored when in a config file\nIGNORE_KEYS = (""name"",\n               ""experiment_dir"",\n               ""work_dir"",\n               ""config_dir"",\n               ""log_dir"",\n               ""checkpoint_dir"",\n               ""img_dir"",\n               ""plot_dir"",\n               ""save_dir"",\n               ""result_dir"",\n               ""time"",\n               ""state"")\n\n# Set the color palette for plots\nCOLORMAP = cl.scales[""8""][""qual""][""Dark2""]\n\n\ndef parse_args():\n    # Read in base directory\n\n    if len(sys.argv) > 1:\n\n        parser = argparse.ArgumentParser()\n        parser.add_argument(""base_directory"",\n                            help=""Give the path to the base directory of your project files"",\n                            type=str, default=""."")\n        parser.add_argument(""-d"", ""--debug"", action=""store_true"",\n                            help=""Turn debug mode on, eg. for live reloading."")\n        parser.add_argument(""-x"", ""--expose"", action=""store_true"",\n                            help=""Make server externally visible"")\n        parser.add_argument(""-p"", ""--port"", default=5000, type=int,\n                            help=""Port to start the server on (5000 by default)"")\n        args = parser.parse_args()\n        base_dir = args.base_directory\n        if base_dir[-1] == os.sep:\n            base_dir = base_dir[:-1]\n\n    else:\n        args = type(\'\', (), {})()\n        args.port = 5000\n        args.expose = False\n        args.debug = False\n        args.base_directory = "".""\n        base_dir = "".""\n\n    return args, base_dir\n\n\ndef create_flask_app(base_dir):\n    # The actual flask app lives in the package directory. The blueprint allows us\n    # to specify an additional static folder and we use that to give access to the\n    # experiment files\n    app = Flask(__name__, static_folder=os.path.join(os.path.dirname(__file__), ""static""))\n    blueprint = Blueprint(""data"", __name__, static_url_path=base_dir, static_folder=base_dir)\n    app.register_blueprint(blueprint)\n\n    return app\n\n\ndef register_url_routes(app, base_dir):\n    app.add_url_rule(""/"", ""overview"", lambda: overview(base_dir), methods=[""GET""])\n    app.add_url_rule(""/overview"", ""overview_"", lambda: overview_(base_dir), methods=[""GET""])\n    app.add_url_rule(\'/experiment\', ""experiment"", lambda: experiment(base_dir), methods=[\'GET\'])\n    app.add_url_rule(\'/combine\', ""combine"", lambda: combine(base_dir), methods=[\'GET\'])\n    app.add_url_rule(\'/experiment_log\', ""experiment_log"", lambda: experiment_log(base_dir), methods=[\'GET\'])\n    app.add_url_rule(\'/experiment_plots\', ""experiment_plots"", lambda: experiment_plots(base_dir), methods=[\'GET\'])\n    app.add_url_rule(\'/experiment_remove\', ""experiment_remove"", lambda: experiment_remove(base_dir), methods=[\'GET\'])\n    app.add_url_rule(\'/experiment_star\', ""experiment_star"", lambda: experiment_star(base_dir), methods=[\'GET\'])\n    app.add_url_rule(\'/experiment_rename\', ""experiment_rename"", lambda: experiment_rename(base_dir), methods=[\'GET\'])\n\n\ndef start_browser():\n    args, base_dir = parse_args()\n\n    base_dir = os.path.abspath(base_dir)\n\n    app = create_flask_app(base_dir)\n    register_url_routes(app, base_dir)\n\n    host = ""0.0.0.0"" if args.expose else ""localhost""\n    app.run(debug=args.debug, host=host, port=args.port)\n\n\ndef overview(base_dir):\n    try:\n        base_info = process_base_dir(base_dir, ignore_keys=IGNORE_KEYS)\n        base_info[""title""] = base_dir\n        return render_template(""overview.html"", **base_info)\n    except Exception as e:\n        print(e.__repr__())\n        raise e\n        abort(500)\n\n\ndef overview_(base_dir):\n    dir_ = request.args.get(""dir"")\n    try:\n        base_info = process_base_dir(base_dir, dir_, ignore_keys=IGNORE_KEYS)\n        base_info[""title""] = dir_\n        return render_template(""overview.html"", **base_info)\n    except Exception as e:\n        print(e.__repr__())\n        raise e\n        abort(500)\n\n\ndef experiment(base_dir):\n    experiment_paths = request.args.getlist(\'exp\')\n    name = request.args.get(\'name\', """")\n    do_save = request.args.get(\'save\', """")\n    combi = request.args.get(\'combi\', \'false\')\n\n    experiments = []\n\n    if combi == ""true"":\n        combi_exp = CombiExperimentReader(base_dir, experiment_paths, name=name)\n        if do_save == ""true"":\n            combi_exp.save()\n        experiments = [combi_exp]\n    else:\n        # Get all Experiments\n        for experiment_path in sorted(experiment_paths):\n            exp = ExperimentReader(base_dir, experiment_path)\n            experiments.append(exp)\n\n    # Assign unique names\n    exp_names = [exp.exp_name for exp in experiments]\n    if len(exp_names) > len(set(exp_names)):\n        for i, exp in enumerate(experiments):\n            exp.exp_name += str(i)\n    exp_names = [exp.exp_name for exp in experiments]\n\n    # Site Content\n    content = {}\n\n    # Get config\n    default_val = ""-""\n    combi_config = {}\n    exp_configs = [exp.config.flat(False) for exp in experiments]\n    diff_config_keys = list(Config.difference_config_static(*exp_configs).flat().keys())\n\n    config_keys = set([k for c in exp_configs for k in c.keys()])\n    for k in sorted(config_keys):\n        combi_config[k] = []\n        for conf in exp_configs:\n            combi_config[k].append(conf.get(k, default_val))\n    config_keys = list(sorted(list(config_keys)))\n\n    # Get results\n    default_val = ""-""\n    combi_results = {}\n    exp_results = [exp.get_results() for exp in experiments]\n    result_keys = set([k for r in exp_results for k in r.keys()])\n    for k in sorted(result_keys):\n        combi_results[k] = []\n        for res in exp_results:\n            combi_results[k].append(res.get(k, default_val))\n    result_keys = list(sorted(list(result_keys)))\n\n    # Get images\n    images = OrderedDict({})\n    image_keys = set()\n    image_path = {}\n    for exp in experiments:\n        exp_images = exp.get_images()\n        img_groups = group_images(exp_images)\n        images[exp.exp_name] = img_groups\n        image_path[exp.exp_name] = exp.img_dir\n        image_keys.update(list(img_groups.keys()))\n    image_keys = list(image_keys)\n    image_keys.sort()\n\n    # Get Plots\n    plots = OrderedDict({})\n    for exp in experiments:\n        exp_plots = exp.get_plots()\n        plots[exp.exp_name] = exp_plots\n\n    # Get logs\n    logs_dict = OrderedDict({})\n    for exp in experiments:\n        exp_logs = [(os.path.basename(l), exp.exp_dir) for l in exp.get_logs()]\n        logs_dict[exp.exp_name] = exp_logs\n\n    content[""title""] = experiments\n    content[""images""] = {""img_path"": image_path, ""imgs"": images, ""img_keys"": image_keys}\n    content[""plots""] = {""plots"": plots}\n    content[""config""] = {""exps"": experiments, ""configs"": combi_config, ""keys"": config_keys,\n                         ""diff_keys"": diff_config_keys}\n    content[""results""] = {""exps"": exp_names, ""results"": combi_results, ""keys"": result_keys}\n    content[""logs""] = {""logs_dict"": logs_dict}\n\n    return render_template(\'experiment.html\', **content)\n\n\ndef experiment_log(base_dir):\n    experiment_path = request.args.get(\'exp\')\n    log_name = request.args.get(\'log\')\n\n    exp = ExperimentReader(base_dir, experiment_path)\n    content = exp.get_log_file_content(log_name)\n\n    print(experiment_path, log_name)\n\n    return content\n\n\ndef experiment_remove(base_dir):\n    experiment_paths = request.args.getlist(\'exp\')\n\n    # Get all Experiments\n    for experiment_path in sorted(experiment_paths):\n        exp = ExperimentReader(base_dir, experiment_path)\n        exp.ignore_experiment()\n\n    return """"\n\n\ndef experiment_plots(base_dir):\n    experiment_paths = request.args.getlist(\'exp\')\n    combi = request.args.get(\'combi\', \'false\')\n    experiments = []\n\n    if combi == ""true"":\n        combi_exp = CombiExperimentReader(base_dir, experiment_paths)\n        # combi_exp.save()\n        experiments = [combi_exp]\n    else:\n        # Get all Experiments\n        for experiment_path in sorted(experiment_paths):\n            exp = ExperimentReader(base_dir, experiment_path)\n            experiments.append(exp)\n\n    # Assign unique names\n    exp_names = [exp.exp_name for exp in experiments]\n    if len(exp_names) > len(set(exp_names)):\n        for i, exp in enumerate(experiments):\n            exp.exp_name += str(i)\n    exp_names = [exp.exp_name for exp in experiments]\n\n    results = []\n    for exp in experiments:\n        results.append(exp.get_results_log())\n    results = merge_results(exp_names, results)\n\n    graphs, traces = make_graphs(results, color_map=COLORMAP)\n    graphs = [str(g) for g in graphs]\n\n    return json.dumps({""graphs"": graphs, ""traces"": traces})\n\n\ndef experiment_star(base_dir):\n    experiment_path = request.args.get(\'exp\')\n    star_val = request.args.get(\'star\')\n\n    star_val = bool(int(star_val))\n\n    print(experiment_path, star_val)\n\n    exp = ExperimentReader(base_dir, experiment_path)\n    exp.update_meta_info(star=star_val)\n\n    return """"\n\n\ndef experiment_rename(base_dir):\n    experiment_path = request.args.get(\'exp\')\n    new_name = request.args.get(\'name\')\n\n    print(experiment_path, new_name)\n\n    exp = ExperimentReader(base_dir, experiment_path)\n    exp.update_meta_info(name=new_name)\n\n    return """"\n\n\ndef combine(base_dir):\n    experiment_paths = request.args.getlist(\'exp\')\n    group_by_list = request.args.getlist(\'group\')\n    name = request.args.get(\'name\', """")\n\n    try:\n        exps = []\n        for ep in experiment_paths:\n            expr = ExperimentReader(os.path.join(base_dir, ep))\n            if expr.exp_info.get(""state"", ""Combined"") != ""Combined"":\n                exps.append(expr)\n\n        exp_groups = group_experiments_by(exps, group_by_list)\n\n        combis = []\n        for group in exp_groups:\n            expc = CombiExperimentReader(base_dir="""", exp_dirs=[e.work_dir for e in group], name=name)\n            combis.append(expc)\n\n        for expc in combis:\n            expc.save()\n\n        return ""1""\n    except Exception as e:\n        return ""0""\n\n    return ""0""\n\n\nif __name__ == ""__main__"":\n    start_browser()\n'"
trixi/experiment_browser/dataprocessing.py,0,"b'import os\nfrom collections import defaultdict\nimport re\n\nimport colorlover as cl\nimport numpy as np\nimport plotly.graph_objs as go\nfrom flask import Markup\nfrom plotly.offline import plot\nfrom scipy.signal import savgol_filter\n\nfrom trixi.experiment_browser.experimentreader import ExperimentReader\n\n# These keys will be ignored when in a config file\nfrom trixi.util import Config\n\nIGNORE_KEYS = (""name"",\n               ""experiment_dir"",\n               ""work_dir"",\n               ""config_dir"",\n               ""log_dir"",\n               ""checkpoint_dir"",\n               ""img_dir"",\n               ""plot_dir"",\n               ""save_dir"",\n               ""result_dir"",\n               ""time"",\n               ""state"")\n\n# Set the color palette for plots\nCOLORMAP = cl.scales[""8""][""qual""][""Dark2""]\n\n\ndef process_base_dir(base_dir, view_dir="""", default_val=""-"", short_len=25, ignore_keys=IGNORE_KEYS):\n    """"""Create an overview table of all experiments in the given directory.\n\n    Args:\n        directory (str): A directory containing experiment folders.\n        default_val (str): Default value if an entry is missing.\n        short_len (int): Cut strings to this length. Full string in alt-text.\n\n    Returns:\n        dict: {""ccols"": Columns for config entries,\n               ""rcols"": Columns for result entries,\n               ""rows"": The actual data}\n\n    """"""\n\n    full_dir = os.path.join(base_dir, view_dir)\n\n    config_keys = set()\n    result_keys = set()\n    exps = []\n    non_exps = []\n\n    ### Load Experiments with keys / different param values\n    for sub_dir in sorted(os.listdir(full_dir)):\n        dir_path = os.path.join(full_dir, sub_dir)\n        if os.path.isdir(dir_path):\n            try:\n                exp = ExperimentReader(full_dir, sub_dir)\n                if exp.ignore:\n                    continue\n                config_keys.update(list(exp.config.flat().keys()))\n                result_keys.update(list(exp.get_results().keys()))\n                exps.append(exp)\n            except Exception as e:\n                print(""Could not load experiment: "", dir_path)\n                print(e)\n                print(""-"" * 20)\n                non_exps.append(os.path.join(view_dir, sub_dir))\n\n    ### Get not common val keys\n    diff_keys = list(Config.difference_config_static(*[xp.config for xp in exps]).flat())\n\n    ### Remove unwanted keys\n    config_keys -= set(ignore_keys)\n    result_keys -= set(ignore_keys)\n\n    ### Generate table rows\n    sorted_c_keys1 = sorted([c for c in config_keys if c in diff_keys], key=lambda x: str(x).lower())\n    sorted_c_keys2 = sorted([c for c in config_keys if c not in diff_keys], key=lambda x: str(x).lower())\n    sorted_r_keys = sorted(result_keys, key=lambda x: str(x).lower())\n\n    rows = []\n    for exp in exps:\n        config_row = []\n        for key in sorted_c_keys1:\n            attr_strng = str(exp.config.flat().get(key, default_val))\n            config_row.append((attr_strng, attr_strng[:short_len]))\n        for key in sorted_c_keys2:\n            attr_strng = str(exp.config.flat().get(key, default_val))\n            config_row.append((attr_strng, attr_strng[:short_len]))\n        result_row = []\n        for key in sorted_r_keys:\n            attr_strng = str(exp.get_results().get(key, default_val))\n            result_row.append((attr_strng, attr_strng[:short_len]))\n\n        name = exp.exp_name\n        time = exp.exp_info.get(""time"", default_val) if ""time"" in exp.exp_info else exp.config.get(""time"", default_val)\n        state = exp.exp_info.get(""state"", default_val) if ""state"" in exp.exp_info else exp.config.get(""state"",\n                                                                                                      default_val)\n        epoch = exp.exp_info.get(""epoch"", default_val) if ""epoch"" in exp.exp_info else exp.config.get(""epoch"",\n                                                                                                      default_val)\n\n        rows.append((os.path.relpath(exp.work_dir, base_dir),\n                     exp.star,\n                     str(name),\n                     str(time),\n                     str(state),\n                     str(epoch),\n                     config_row, result_row))\n\n    return {""ccols1"": sorted_c_keys1, ""ccols2"": sorted_c_keys2, ""rcols"": sorted_r_keys, ""rows"": rows, ""noexp"": non_exps}\n\n\ndef group_images(images):\n    images.sort()\n    group_dict = defaultdict(list)\n\n    for img in images:\n        filename = img.split(os.sep + ""img"" + os.sep)[1]\n        base_name = os.path.splitext(filename)[0]\n        number_groups = re.findall(""\\d+\\.\\d+"", base_name)\n        if len(number_groups) == 0:\n            base_name = \'\'.join(e for e in base_name if e.isalpha())\n        else:\n            base_name = base_name.replace(number_groups[0], """")\n\n        group_dict[base_name].append(filename)\n\n    return group_dict\n\n\ndef make_graphs(results, trace_options=None, layout_options=None, color_map=COLORMAP):\n    """"""Create plot markups.\n\n    This converts results into plotly plots in markup form. Results in a common\n    group will be placed in the same plot.\n\n    Args:\n        results (dict): Dictionary\n\n    """"""\n\n    if trace_options is None:\n        trace_options = {}\n    if layout_options is None:\n        layout_options = {\n            ""legend"": dict(\n                orientation=""v"",\n                xanchor=""left"",\n                x=0,\n                yanchor=""top"",\n                y=-0.1,\n                font=dict(\n                    size=8,\n                )\n            )\n        }\n\n    graphs = []\n    trace_counters = []\n\n    for group in sorted(results):\n\n        layout = go.Layout(title=group, **layout_options)\n        traces = []\n\n        for r, result in enumerate(sorted(results[group])):\n\n            y = np.array(results[group][result][""data""])\n            x = np.array(results[group][result][""counter""])\n\n            do_filter = len(y) >= 1000\n            opacity = 0.2 if do_filter else 1.\n\n            if ""min"" in results[group][result] and ""max"" in results[group][result]:\n                min_ = np.array(results[group][result][""min""])\n                max_ = np.array(results[group][result][""max""])\n                fill_color = color_map[r % len(color_map)][:3] + ""a"" + color_map[r % len(color_map)][3:-1] + "",0.1)""\n                upper_bound = go.Scatter(x=x, y=max_, name=result, legendgroup=result, showlegend=False,\n                                         mode=\'lines\', line=dict(width=0), hoverinfo=\'none\',\n                                         fillcolor=fill_color, **trace_options)\n                lower_bound = go.Scatter(x=x, y=min_, name=result, legendgroup=result, showlegend=False,\n                                         mode=\'lines\', fill=""tonexty"", line=dict(width=0), hoverinfo=\'none\',\n                                         fillcolor=fill_color, **trace_options)\n                traces.append(upper_bound)\n                traces.append(lower_bound)\n                traces.append(go.Scatter(x=x, y=y, opacity=opacity, name=result, legendgroup=result,\n                                         line=dict(color=color_map[r % len(color_map)]), **trace_options))\n            elif do_filter:\n                def filter_(x):\n                    return savgol_filter(x, max(5, 2 * (len(y) // 50) + 1), 3)\n\n                traces.append(go.Scatter(x=x, y=y, opacity=opacity, name=result, legendgroup=result, showlegend=False,\n                                         line=dict(color=color_map[r % len(color_map)]), **trace_options))\n                traces.append(go.Scatter(x=x, y=filter_(y), name=result, legendgroup=result,\n                                         line=dict(color=color_map[r % len(color_map)]), **trace_options))\n            else:\n                traces.append(go.Scatter(x=x, y=y, opacity=opacity, name=result, legendgroup=result,\n                                         line=dict(color=color_map[r % len(color_map)]), **trace_options))\n\n        trace_counters.append(len(results[group]))\n        graphs.append(Markup(plot({""data"": traces, ""layout"": layout},\n                                  output_type=""div"",\n                                  include_plotlyjs=False,\n                                  show_link=False)))\n\n    return graphs, trace_counters\n\n\ndef merge_results(experiment_names, result_list):\n    merged_results = {}\n\n    for r, result in enumerate(result_list):\n        for label in result.keys():\n            if label not in merged_results:\n                merged_results[label] = {}\n            for key in result[label].keys():\n                new_key = ""_"".join([experiment_names[r], key])\n                merged_results[label][new_key] = result[label][key]\n\n    return merged_results\n'"
trixi/experiment_browser/experimentreader.py,0,"b'import copy\nimport itertools\nimport json\nimport os\nimport time\nimport warnings\nfrom collections import defaultdict\n\nimport numpy as np\n\nfrom trixi.logger import ExperimentLogger\nfrom trixi.util import Config\nfrom trixi.util.util import StringMultiTypeDecoder\n\n\nclass ExperimentReader(object):\n    """"""Reader class to read out experiments created by :class:`trixi.experimentlogger.ExperimentLogger`.\n\n    Args:\n        work_dir (str): Directory with the structure defined by\n                        :class:`trixi.experimentlogger.ExperimentLogger`.\n        name (str): Optional name for the experiment. If None, will try\n                    to read name from experiment config.\n\n    """"""\n\n    def __init__(self, base_dir, exp_dir="""", name=None, decode_config_clean_str=True):\n\n        super(ExperimentReader, self).__init__()\n\n        self.base_dir = base_dir\n        self.exp_dir = exp_dir\n        self.work_dir = os.path.abspath(os.path.join(self.base_dir, self.exp_dir))\n        self.config_dir = os.path.join(self.work_dir, ""config"")\n        self.log_dir = os.path.join(self.work_dir, ""log"")\n        self.checkpoint_dir = os.path.join(self.work_dir, ""checkpoint"")\n        self.img_dir = os.path.join(self.work_dir, ""img"")\n        self.plot_dir = os.path.join(self.work_dir, ""plot"")\n        self.save_dir = os.path.join(self.work_dir, ""save"")\n        self.result_dir = os.path.join(self.work_dir, ""result"")\n\n        self.config = Config()\n        if decode_config_clean_str:\n            self.config.load(os.path.join(self.config_dir, ""config.json""), decoder_cls_=StringMultiTypeDecoder)\n        else:\n            self.config.load(os.path.join(self.config_dir, ""config.json""), decoder_cls_=None)\n\n        self.exp_info = Config()\n        exp_info_file = os.path.join(self.config_dir, ""exp.json"")\n        if os.path.exists(exp_info_file):\n            self.exp_info.load(exp_info_file)\n\n        self.__results_dict = None\n\n        self.meta_name = None\n        self.meta_star = False\n        self.meta_ignore = False\n        self.read_meta_info()\n\n        if name is not None:\n            self.exp_name = name\n        elif self.meta_name is not None:\n            self.exp_name = self.meta_name\n        elif ""name"" in self.exp_info:\n            self.exp_name = self.exp_info[\'name\']\n        elif ""exp_name"" in self.config:\n            self.exp_name = self.config[\'exp_name\']\n        else:\n            self.exp_name = ""experiments""\n\n        self.ignore = self.meta_ignore\n        self.star = self.meta_star\n\n    @staticmethod\n    def get_file_contents(folder, include_subdirs=False):\n        """"""Get all files in a folder.\n\n        Returns:\n            list: All files joined with folder path.\n        """"""\n\n        if os.path.isdir(folder):\n            list_ = list(map(lambda x: os.path.join(folder, x), sorted(os.listdir(folder))))\n            files = list(filter(lambda x: os.path.isfile(x), list_))\n            dirs = list(filter(lambda x: os.path.isdir(x), list_))\n            if include_subdirs:\n                for d in dirs:\n                    files += ExperimentReader.get_file_contents(d, True)\n            return files\n        else:\n            return []\n\n    def get_images(self):\n        return ExperimentReader.get_file_contents(self.img_dir, True)\n\n    def get_plots(self):\n        return ExperimentReader.get_file_contents(self.plot_dir, True)\n\n    def get_checkpoints(self):\n        return ExperimentReader.get_file_contents(self.checkpoint_dir)\n\n    def get_logs(self):\n        return ExperimentReader.get_file_contents(self.log_dir)\n\n    def get_log_file_content(self, file_name):\n        """"""Read out log file and HTMLify.\n\n        Args:\n            file_name (str): Name of the log file.\n\n        Returns:\n            str: Log file contents as HTML ready string.\n        """"""\n\n        content = """"\n        log_file = os.path.join(self.log_dir, file_name)\n\n        if os.path.exists(log_file):\n            with open(log_file, \'r\') as f:\n                content = f.read()\n                content = content.replace(""\\n"", ""<br>"")\n\n        return content\n\n    def get_results_log(self):\n        """"""Build result dictionary.\n\n        During the experiment result items are\n        written out as a stream of quasi-atomic units. This reads the stream and\n        builds arrays of corresponding items.\n        The resulting dict looks like this::\n\n            {\n                ""result group"": {\n                    ""result"": {\n                        ""counter"": x-array,\n                        ""data"": y-array\n                    }\n                }\n            }\n\n        Returns:\n            dict: Result dictionary.\n\n        """"""\n\n        results_merged = {}\n\n        results = []\n        try:\n            with open(os.path.join(self.result_dir, ""results-log.json""), ""r"") as results_file:\n                results = json.load(results_file)\n        except Exception as e:\n            try:\n                with open(os.path.join(self.result_dir, ""results-log.json""), ""r"") as results_file:\n                    results_str = results_file.readlines()\n                    if ""]"" in ""]"" in """".join(results_str):\n                        results_str = [rs.replace(""]"", "","") for rs in results_str]\n                    results_str.append(""{}]"")\n                    results_json = """".join(results_str)\n                    results = json.loads(results_json)\n            except Exception as ee:\n                print(""Could not load result log from"", self.result_dir)\n                print(ee)\n\n        for result in results:\n            for key in result.keys():\n                counter = result[key][""counter""]\n                data = result[key][""data""]\n                label = str(result[key][""label""])\n                if label not in results_merged:\n                    results_merged[label] = {}\n                if key not in results_merged[label]:\n                    results_merged[label][key] = defaultdict(list)\n                results_merged[label][key][""data""].append(data)\n                results_merged[label][key][""counter""].append(counter)\n                if ""max"" in result[key] and ""min"" in result[key]:\n                    results_merged[label][key][""max""].append(result[key][""max""])\n                    results_merged[label][key][""min""].append(result[key][""min""])\n\n        return results_merged\n\n    def get_results(self):\n        """"""Get the last result item.\n\n        Returns:\n            dict: The last result item in the experiment.\n\n        """"""\n\n        if self.__results_dict is None:\n\n            self.__results_dict = {}\n            results_file = os.path.join(self.result_dir, ""results.json"")\n\n            if os.path.exists(results_file):\n                try:\n                    with open(results_file, ""r"") as f:\n                        self.__results_dict = json.load(f)\n                except Exception as e:\n                    pass\n\n        return self.__results_dict\n\n    def ignore_experiment(self):\n        """"""Create a flag file, so the browser ignores this experiment.""""""\n        self.update_meta_info(ignore=True)\n\n    def read_meta_info(self):\n        """"""Reads the meta info of the experiment i.e. new name, stared or ignored""""""\n        meta_dict = {}\n        meta_file = os.path.join(self.work_dir, "".exp_info"")\n        if os.path.exists(meta_file):\n            with open(meta_file, ""r"") as mf:\n                meta_dict = json.load(mf)\n            self.meta_name = meta_dict.get(""name"")\n            self.meta_star = meta_dict.get(""star"", False)\n            self.meta_ignore = meta_dict.get(""ignore"", False)\n\n    def update_meta_info(self, name=None, star=None, ignore=None):\n        """"""\n        Updates the meta info i.e. new name, stared or ignored and saves it in the experiment folder\n\n        Args:\n            name (str): New name of the experiment\n            star (bool): Flag, if experiment is starred/ favorited\n            ignore (boll): Flag, if experiment should be ignored\n        """"""\n\n        if name is not None:\n            self.meta_name = name\n        if star is not None:\n            self.meta_star = star\n        if ignore is not None:\n            self.meta_ignore = ignore\n\n        meta_dict = {\n            ""name"": self.meta_name,\n            ""star"": self.meta_star,\n            ""ignore"": self.meta_ignore\n        }\n        meta_file = os.path.join(self.work_dir, "".exp_info"")\n        with open(meta_file, ""w"") as mf:\n            json.dump(meta_dict, mf)\n\n\nclass CombiExperimentReader(ExperimentReader):\n\n    def __init__(self, base_dir, exp_dirs=(), name=None, decode_config_clean_str=True):\n\n        self.base_dir = base_dir\n\n        if name is None or name == """":\n            self.exp_name = ""combi-experiments""\n        else:\n            self.exp_name = name\n\n        self.experiments = []\n        for exp_dir in exp_dirs:\n            self.experiments.append(ExperimentReader(base_dir=base_dir, exp_dir=exp_dir,\n                                                     decode_config_clean_str=decode_config_clean_str))\n\n        exp_base_dirs = os.path.commonpath([os.path.dirname(e.work_dir) for e in self.experiments])\n        if exp_base_dirs != """":\n            self.base_dir = exp_base_dirs\n\n        self.exp_info = Config()\n\n        self.exp_info[""epoch""] = -1\n        self.exp_info[""name""] = self.exp_name\n        self.exp_info[""state""] = ""Combined""\n        self.exp_info[""time""] = time.strftime(""%y-%m-%d_%H:%M:%S"", time.localtime(time.time()))\n\n        self.__results_dict = None\n\n        self.work_dir = None\n        self.config_dir = ""not_saved_yet""\n        self.log_dir = ""not_saved_yet""\n        self.checkpoint_dir = ""not_saved_yet""\n        self.img_dir = ""not_saved_yet""\n        self.plot_dir = ""not_saved_yet""\n        self.save_dir = ""not_saved_yet""\n        self.result_dir = ""not_saved_yet""\n        self.exp_dir = ""not_saved_yet""\n\n        self.meta_name = None\n        self.meta_star = False\n        self.meta_ignore = False\n\n        self.config = self.get_config()\n\n        self.elog = None\n\n    def get_config(self):\n        combi_config = copy.deepcopy(self.experiments[0].config)\n        config_diff = Config.difference_config_static(*[e.config for e in self.experiments], only_set=True)\n        combi_config.update(config_diff)\n\n        return combi_config\n\n    def get_results_log(self):\n        exp_results_logs = [e.get_results_log() for e in self.experiments]\n\n        log_tags = set()\n        for res in exp_results_logs:\n            log_tags.update(res.keys())\n\n        res_keys = defaultdict(set)\n        for tag in log_tags:\n            for exp_res in exp_results_logs:\n                if tag not in exp_res:\n                    continue\n                res_keys[tag].update(exp_res[tag].keys())\n\n        combi_res_log = dict()\n\n        for tag, keys in res_keys.items():\n            key_result_dict = {}\n            for s_key in keys:\n                skey_result = defaultdict(list)\n\n                for exp_res in exp_results_logs:\n                    if tag not in exp_res:\n                        continue\n                    if s_key not in exp_res[tag]:\n                        continue\n\n                    for c, val in zip(exp_res[tag][s_key][""counter""], exp_res[tag][s_key][""data""]):\n                        skey_result[c].append(val)\n\n                key_result_dict[s_key] = skey_result\n            combi_res_log[tag] = key_result_dict\n\n        final_results_log = {}\n\n        for tag, key_result_dict in combi_res_log.items():\n            final_results_log[tag] = {}\n            for s_key, s_key_result_dict in key_result_dict.items():\n                final_results_log[tag][s_key] = defaultdict(list)\n                cnts = sorted(s_key_result_dict.keys())\n                for cnt in cnts:\n                    val_list = s_key_result_dict[cnt]\n                    final_results_log[tag][s_key][""counter""].append(cnt)\n                    final_results_log[tag][s_key][""data""].append(np.nanmedian(val_list))\n                    final_results_log[tag][s_key][""label""].append(tag)\n                    final_results_log[tag][s_key][""mean""].append(np.nanmean(val_list))\n                    final_results_log[tag][s_key][""median""].append(np.nanmedian(val_list))\n                    final_results_log[tag][s_key][""max""].append(np.nanmax(val_list))\n                    final_results_log[tag][s_key][""min""].append(np.nanmin(val_list))\n                    final_results_log[tag][s_key][""std""].append(np.nanstd(val_list))\n\n        return final_results_log\n\n    def get_results_full(self):\n        exp_results = [e.get_results() for e in self.experiments]\n        result_keys = set()\n        for res in exp_results:\n            result_keys.update(res.keys())\n\n        res_collect = defaultdict(list)\n\n        for key in result_keys:\n            for exp_res in exp_results:\n                if key not in exp_res:\n                    continue\n                res_collect[key].append(exp_res[key])\n\n        results_dict = {}\n        results_aux_dict = defaultdict(dict)\n\n        for key, val_list in res_collect.items():\n            results_dict[key] = np.nanmedian(val_list)\n            results_aux_dict[key][""mean""] = np.nanmean(val_list)\n            results_aux_dict[key][""median""] = np.nanmedian(val_list)\n            results_aux_dict[key][""max""] = np.nanmax(val_list)\n            results_aux_dict[key][""min""] = np.nanmin(val_list)\n            results_aux_dict[key][""std""] = np.nanstd(val_list)\n            results_aux_dict[key][""all""] = val_list\n\n        return results_dict, results_aux_dict\n\n    def get_results(self):\n        results_dict, _ = self.get_results_full()\n        return results_dict\n\n    def get_result_log_dict(self):\n\n        results_dict = self.get_results_log()\n\n        res_list = []\n\n        for tag, key_result_dict in results_dict.items():\n            for s_key, s_key_result_dict in key_result_dict.items():\n                for cnt, val, max_, min_ in zip(s_key_result_dict[""counter""], s_key_result_dict[""data""],\n                                              s_key_result_dict[""max""], s_key_result_dict[""min""]):\n                    res_list.append({s_key: dict(data=val, counter=cnt, epoch=-1, label=tag, max=max_, min=min_)})\n\n        return res_list\n\n    def ignore_experiment(self):\n        """"""Create a flag file, so the browser ignores this experiment.""""""\n        if self.work_dir is None:\n            warnings.warn(""Can only be called for a combined experiment which is saved"")\n            return\n        super(CombiExperimentReader, self).ignore_experiment()\n\n    def read_meta_info(self):\n        """"""Reads the meta info of the experiment i.e. new name, stared or ignored""""""\n        if self.work_dir is None:\n            warnings.warn(""Can only be called for a combined experiment which is saved"")\n            return\n        super(CombiExperimentReader, self).read_meta_info()\n\n    def update_meta_info(self, name=None, star=None, ignore=None):\n        if self.work_dir is None:\n            warnings.warn(""Can only be called for a combined experiment which is saved"")\n            return\n        super(CombiExperimentReader, self).update_meta_info(name, star, ignore)\n\n    def save(self, target_dir=None):\n\n        if target_dir is None:\n            target_dir = self.base_dir\n\n        self.elog = ExperimentLogger(experiment_name=self.exp_name, base_dir=target_dir)\n\n        self.exp_dir = self.elog.folder_name\n        self.work_dir = self.elog.work_dir\n        self.config_dir = os.path.join(self.work_dir, ""config"")\n        self.log_dir = os.path.join(self.work_dir, ""log"")\n        self.checkpoint_dir = os.path.join(self.work_dir, ""checkpoint"")\n        self.img_dir = os.path.join(self.work_dir, ""img"")\n        self.plot_dir = os.path.join(self.work_dir, ""plot"")\n        self.save_dir = os.path.join(self.work_dir, ""save"")\n        self.result_dir = os.path.join(self.work_dir, ""result"")\n\n        def __default(o):\n            if isinstance(o, np.int64): return int(o)\n            raise TypeError\n\n        results, results_combi = self.get_results_full()\n        self.config.dump(os.path.join(self.elog.config_dir, ""config.json""))\n        self.elog.save_config(self.exp_info, ""exp"")\n        self.elog.save_result(results, ""results"", encoder_cls=None, default=__default)\n        self.elog.save_result(results_combi, ""results-combi"", encoder_cls=None, default=__default)\n        self.elog.save_result(self.get_result_log_dict(), ""results-log"", encoder_cls=None, default=__default)\n        self.elog.text_logger.log_to(""\\n"".join([""\\n""]+[e.work_dir for e in self.experiments]), ""combi_exps"")\n\n\n\ndef group_experiments_by(exps, group_by_list):\n    configs_flat = [e.config.flat() for e in exps]  ### Exclude already combined experiments\n    config_diff = Config.difference_config_static(*configs_flat)\n\n    group_diff_key_list = []\n    group_diff_val_list = []\n\n    for diff_key in group_by_list:\n        if diff_key in config_diff:\n            group_diff_key_list.append(diff_key)\n            group_diff_val_list.append(set(config_diff[diff_key]))\n\n    val_combis = itertools.product(*group_diff_val_list)\n\n    group_dict = defaultdict(list)\n\n    for val_combi in val_combis:\n        for e in exps:\n            e_config = e.config.flat()\n            is_match = True\n\n            for key, val in zip(group_diff_key_list, val_combi):\n                if e_config[key] != val:\n                    is_match = False\n\n            if is_match:\n                group_dict[val_combi].append(e)\n\n    return list(group_dict.values())\n'"
trixi/logger/__init__.py,0,"b'from trixi.logger.abstractlogger import AbstractLogger\nfrom trixi.logger.combinedlogger import CombinedLogger\nfrom trixi.logger.plt.numpyseabornplotlogger import NumpySeabornPlotLogger\nfrom trixi.logger.file.numpyplotfilelogger import NumpyPlotFileLogger\nfrom trixi.logger.file.textfilelogger import TextFileLogger\nfrom trixi.logger.visdom.numpyvisdomlogger import NumpyVisdomLogger\nfrom trixi.logger.experiment.experimentlogger import ExperimentLogger\n\ntry:\n    from trixi.logger.experiment.pytorchexperimentlogger import PytorchExperimentLogger\n    from trixi.logger.file.pytorchplotfilelogger import PytorchPlotFileLogger\n    from trixi.logger.visdom.pytorchvisdomlogger import PytorchVisdomLogger\nexcept ImportError as e:\n    import warnings\n    warnings.warn(ImportWarning(""Could not import Pytorch related modules:\\n%s""\n        % e.msg))\n\ntry:\n    from trixi.logger.message.telegrammessagelogger import TelegramMessageLogger\nexcept ImportError as e:\n    import warnings\n    warnings.warn(ImportWarning(""Could not import Telegram related modules:\\n%s""\n        % e.msg))\n'"
trixi/logger/abstractlogger.py,0,"b'from abc import ABCMeta, abstractmethod\nimport _thread\nfrom functools import wraps\n\n\ndef convert_params(f):\n    """"""Decorator to call the process_params method of the class.""""""\n\n    @wraps(f)\n    def wrapper(self, *args, **kwargs):\n        return self.process_params(f, *args, **kwargs)\n\n    return wrapper\n\n\ndef threaded(f):\n    """"""Decorator to run the process in an extra thread.""""""\n\n    def wrapper(*args, **kwargs):\n        return _thread.start_new(f, args, kwargs)\n\n    return wrapper\n\n\nclass AbstractLogger(object):\n    """"""Abstract interface for visual logger.""""""\n\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def process_params(self, f, *args, **kwargs):\n        """"""\n        Implement this to handle data conversions in your logger.\n\n        Example: Implement logger for numpy data, then implement torch logger as child of numpy logger and just use\n        the process_params method to convert from torch to numpy.\n        """"""\n\n        return f(self, *args, **kwargs)\n\n    @abstractmethod\n    @convert_params\n    def show_image(self, *args, **kwargs):\n        """"""Abstract method which should handle and somehow log/ store an image""""""\n        raise NotImplementedError()\n\n    @abstractmethod\n    @convert_params\n    def show_value(self, *args, **kwargs):\n        """"""Abstract method which should handle and somehow log/ store a value""""""\n        raise NotImplementedError()\n\n    @abstractmethod\n    @convert_params\n    def show_text(self, *args, **kwargs):\n        """"""Abstract method which should handle and somehow log/ store a text""""""\n        raise NotImplementedError()\n\n    @abstractmethod\n    @convert_params\n    def show_barplot(self, *args, **kwargs):\n        """"""Abstract method which should handle and somehow log/ store a barplot""""""\n        raise NotImplementedError()\n\n    @abstractmethod\n    @convert_params\n    def show_lineplot(self, *args, **kwargs):\n        """"""Abstract method which should handle and somehow log/ store a lineplot""""""\n        raise NotImplementedError()\n\n    @abstractmethod\n    @convert_params\n    def show_scatterplot(self, *args, **kwargs):\n        """"""Abstract method which should handle and somehow log/ store a scatterplot""""""\n        raise NotImplementedError()\n\n    @abstractmethod\n    @convert_params\n    def show_piechart(self, *args, **kwargs):\n        """"""Abstract method which should handle and somehow log/ store a piechart""""""\n        raise NotImplementedError()\n'"
trixi/logger/combinedlogger.py,0,"b'import sys\nimport traceback\nfrom collections import defaultdict\n\nfrom trixi.logger.abstractlogger import AbstractLogger\n\n\ndef create_function(self, sub_methods):\n    def surrogate_fn(*args, **kwargs):\n\n        for sub_method in sub_methods:\n\n            try:\n\n                method_cntr = self.log_methods_cntr[sub_method]\n                method_freq = self.log_methods_freq[sub_method]\n\n                if method_freq is None or method_freq == 0:\n                    continue\n\n                name_id = None\n                if ""name"" in kwargs and not (""ignore_name_in_args"" in kwargs and kwargs[""ignore_name_in_args""] is True):\n                    name_id = kwargs[""name""]\n                    if ""tag"" in kwargs and kwargs[""tag""] is not None:\n                        name_id = name_id + kwargs[""tag""]\n                    method_cntr = self.log_methods_name_cntr[sub_method][name_id]\n\n                if ""log_all"" in kwargs and kwargs[""log_all""] is True:\n                    sub_method(*args, **kwargs)\n                    continue\n\n                if method_cntr % method_freq == 0:\n                    sub_method(*args, **kwargs)\n\n                elif ""same_as_last"" in kwargs and kwargs[""same_as_last""] is True:\n                    if method_cntr % method_freq == 1:\n                        sub_method(*args, **kwargs)\n                    kwargs[""do_not_increase""] = True\n\n                if name_id is not None:\n                    self.log_methods_name_cntr[sub_method][name_id] += 1\n                elif ""do_not_increase"" not in kwargs or \\\n                        (""do_not_increase"" in kwargs and kwargs[""do_not_increase""] is False):\n                    self.log_methods_cntr[sub_method] += 1\n\n            except Exception as e:\n                print(""a combi logger method failed: "", str(sub_method))\n                error = sys.exc_info()[0]\n                msg = traceback.format_exc()\n                print(""Error {}: {}"".format(error, msg))\n\n    return surrogate_fn\n\n\nclass CombinedLogger(object):\n    """"""\n    A Logger which can combine all other logger and if called calls all the sub loggers\n\n    """"""\n\n    def __init__(self, *loggers):\n        """"""\n        Initializes a new combined logger with a list of logger and their logging frequencies.\n\n        By default it initalized a counter for each logger method, and on each call it increases the counter (and if the\n        counter is a multiple of the frequency it logs to the given logger).\n\n        Furthermore you can use the same_as_last argument in all methods to repeat the frequency_counter from the\n        previous call. If you set the do_not_increase attribute to True, it wont increase the counter, it has been\n        called 10 times and the frequency is 10, it will log, and also log in the next round, since the counter is\n        still 10 and was not increase. Use can use the log_all attribute, it you want to ignore the frequencies and\n        simply log to all loggers. By default, if your method has a name attribute, there is a counter for each\n        method called with a unique name (so one counter for plot(name=""1"") and a different counter for plot(name=""2"")),\n        but if you want one counter for the method (despite having a name attribute), you can set the\n        ignore_name_in_args to True,\n\n        Args:\n            *loggers (list): a list of tuples where each tuple conisist of (logger, frequencies), where logger is\n            a given logger and frequencies is the frequency (or rather log each on in frequencies, e.g. if it is 10,\n            it logs every ten_th call).\n        """"""\n\n        self.loggers, self.frequencies = zip(*loggers)\n\n        for logger in self.loggers:\n            if not isinstance(logger, AbstractLogger):\n                raise TypeError(""All logger must be subclasses of the abstract visual logger."")\n        for freq in self.frequencies:\n            if freq is None:\n                continue\n            elif freq < 0:\n                raise ValueError(""All frequencies must be positive."")\n\n        self.logger_methods = defaultdict(list)\n        self.log_methods_cntr = defaultdict(dict)\n        self.log_methods_freq = defaultdict(int)\n        self.log_methods_name_cntr = defaultdict(int)\n\n        for logger, freq in zip(self.loggers, self.frequencies):\n            if freq is None:\n                continue\n\n            logger_vars = [i for i in dir(logger) if not i.startswith(""__"")]\n\n            for el in logger_vars:\n                if hasattr(logger, el) and callable(getattr(logger, el)):\n                    self.logger_methods[el].append(getattr(logger, el))\n                    self.log_methods_cntr[getattr(logger, el)] = 0\n                    self.log_methods_freq[getattr(logger, el)] = freq\n                    self.log_methods_name_cntr[getattr(logger, el)] = defaultdict(int)\n\n        for method_name, sub_methods in self.logger_methods.items():\n            setattr(self, method_name, create_function(self, sub_methods))\n'"
trixi/util/__init__.py,0,"b'from trixi.util.util import (\n    CustomJSONEncoder,\n    CustomJSONDecoder,\n    MultiTypeEncoder,\n    MultiTypeDecoder,\n    ModuleMultiTypeEncoder,\n    ModuleMultiTypeDecoder,\n    Singleton,\n    savefig_and_close,\n    random_string,\n    create_folder,\n    name_and_iter_to_filename,\n    SafeDict,\n    PyLock,\n    LogDict,\n    ResultLogDict,\n    ResultElement\n)\nfrom trixi.util.config import Config\nfrom trixi.util.extravisdom import ExtraVisdom\nfrom trixi.util.sourcepacker import SourcePacker\nfrom trixi.util.gridsearch import GridSearch\n'"
trixi/util/config.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport inspect\nimport json\nfrom copy import deepcopy\nfrom functools import wraps\n\nfrom trixi.util.util import ModuleMultiTypeDecoder, ModuleMultiTypeEncoder\n\n\nclass Config(dict):\n    """"""\n    Config is the main object used to store configurations. As a rule of thumb, anything you might\n    want to change in your experiment should go into the Config. It\'s basically a :class:`dict`,\n    but vastly more powerful. Key features are\n\n        - Access keys as attributes\n            Config[""a""][""b""][""c""] is the same as Config.a.b.c.\n            Can also be used for setting if the second to last key exists. Only works for keys that\n            conform with Python syntax (Config.myattr-1 is not allowed).\n        - Advanced de-/serialization\n            Using specialized JSON encoders and decoders, almost anything can be serialized and\n            deserialized. This includes types, functions (except lambdas) and modules. For example,\n            you could have something like::\n\n                c = Config(model=MyModel)\n                c.dump(""somewhere"")\n\n            and end up with a JSON file that looks like this::\n\n                {\n                    ""model"": ""__type__(your.model.module.MyModel)""\n                }\n\n            and vice versa. We use double underscores and parentheses for serialization,\n            so it\'s probably a good idea to not use this pattern for other stuff!\n        - Automatic CLI exposure\n            If desired, the Config will create an ArgumentParser that contains all keys in the\n            Config as arguments in the form ""- - key"", so you can run your experiment from the command\n            line and manually overwrite certain values. Deeper levels are also accessible via dot\n            notation ""- - key_with_dict_value.inner_key"".\n        - Comparison\n            Compare any number of Configs and get a new Config containing only the values that\n            differ among input Configs.\n\n    Args:\n        file_ (str): Load Config from this file.\n        config (Config): Update with values from this Config (can be combined with :attr:`file_`).\n            Will by default only make shallow copies, see :attr:`deep`.\n        update_from_argv (bool): Update values from argv. Will automatically expose keys to the\n            CLI as \'- - key\'.\n        deep (bool): Make deep copies if :attr:`config` is given.\n\n    """"""\n\n    def __init__(self, file_=None, config=None, update_from_argv=False, deep=False, **kwargs):\n\n        super(Config, self).__init__()\n\n        # the following allows us to access keys as attributes if syntax permits\n        # config[""a""] = 1 -> config.a -> 1\n        # config[""a-b""] = 2 -> config.a-b (not possible)\n        # this is purely for convenience\n        self.__dict__ = self\n\n        if file_ is not None:\n            self.load(file_)\n\n        if config is not None:\n            if deep:\n                # convert config to Config (in case it\'s just a dict)\n                # and get deepcopy\n                config = Config(config=config, update_from_argv=False, deep=False)\n                config = config.deepcopy()\n            self.update(config, deep=False)\n\n        if len(kwargs) >= 1:\n            if deep:\n                kwargs = Config(config=kwargs, update_from_argv=False, deep=False)\n                kwargs = kwargs.deepcopy()\n            self.update(kwargs, deep=False)\n\n        if update_from_argv:\n            update_from_sys_argv(self)\n\n    def update(self, dict_like, deep=False, ignore=None, allow_dict_overwrite=True):\n        """"""Update entries in the Config.\n\n        Args:\n            dict_like (dict or derivative thereof): Update source.\n            deep (bool): Make deep copies of all references in the source.\n            ignore (iterable): Iterable of keys to ignore in update.\n            allow_dict_overwrite (bool): Allow overwriting with dict.\n                Regular dicts only update on the highest level while we recurse\n                and merge Configs. This flag decides whether it is possible to\n                overwrite a \'regular\' value with a dict/Config at lower levels.\n                See examples for an illustration of the difference\n\n\n        Examples:\n            The following illustrates the update behaviour if\n            :obj:allow_dict_overwrite is active. If it isn\'t, an AttributeError\n            would be raised, originating from trying to update ""string""::\n\n                config1 = Config(config={\n                    ""lvl0"": {\n                        ""lvl1"": ""string"",\n                        ""something"": ""else""\n                    }\n                })\n\n                config2 = Config(config={\n                    ""lvl0"": {\n                        ""lvl1"": {\n                            ""lvl2"": ""string""\n                        }\n                    }\n                })\n\n                config1.update(config2, allow_dict_overwrite=True)\n\n                >>>config1\n                {\n                    ""lvl0"": {\n                        ""lvl1"": {\n                            ""lvl2"": ""string""\n                        },\n                        ""something"": ""else""\n                    }\n                }\n\n        """"""\n\n        if ignore is None:\n            ignore = ()\n\n        if deep:\n            update_config = Config(config=dict_like, deep=True)\n            self.update(update_config, deep=False, ignore=ignore, allow_dict_overwrite=allow_dict_overwrite)\n\n        else:\n            for key, value in dict_like.items():\n                if key in ignore:\n                    continue\n                if key in self and isinstance(value, dict):\n                    try:\n                        self[key].update(value, deep=False, ignore=ignore, allow_dict_overwrite=allow_dict_overwrite)\n                    except AttributeError as ae:\n                        if allow_dict_overwrite:\n                            self[key] = value\n                        else:\n                            raise ae\n                else:\n                    self[key] = value\n\n    def deepupdate(self, dict_like, ignore=None, allow_dict_overwrite=True):\n        """"""Identical to :meth:`update` with `deep=True`.\n\n        Args:\n            dict_like (dict or derivative thereof): Update source.\n            ignore (iterable): Iterable of keys to ignore in update.\n            allow_dict_overwrite (bool): Allow overwriting with dict.\n                Regular dicts only update on the highest level while we recurse\n                and merge Configs. This flag decides whether it is possible to\n                overwrite a \'regular\' value with a dict/Config at lower levels.\n                See examples for an illustration of the difference\n\n        """"""\n        self.update(dict_like, deep=True, ignore=ignore, allow_dict_overwrite=allow_dict_overwrite)\n\n    def __setattr__(self, key, value):\n        """"""Modified to automatically convert `dict` to Config.""""""\n\n        if type(value) == dict:\n            new_config = Config()\n            new_config.update(value, deep=False)\n            super(Config, self).__setattr__(key, new_config)\n        else:\n            super(Config, self).__setattr__(key, value)\n\n    def __getitem__(self, key):\n        """"""Allows convenience access to deeper levels using dots to separate\n        levels, for example `config[""a.b.c""]`.\n        """"""\n\n        if key == """":\n            if len(self.keys()) == 1:\n                key = list(self.keys())[0]\n            else:\n                raise KeyError(""Empty string only works for single element Configs."")\n\n        if type(key) == str and ""."" in key:\n            superkey = key.split(""."")[0]\n            subkeys = ""."".join(key.split(""."")[1:])\n            if superkey not in self:\n                # this part enables ints in the access chain, e.g. a.1.b\n                try:\n                    intkey = int(superkey)\n                    if intkey in self:\n                        superkey = intkey\n                except ValueError:\n                    # if we can\'t convert to int, just continue so a KeyError will be raised\n                    pass\n            if type(self[superkey]) in (list, tuple):\n                try:\n                    subkeys = int(subkeys)\n                except ValueError:\n                    pass\n            return self[superkey][subkeys]\n        else:\n            return super(Config, self).__getitem__(key)\n\n    def __setitem__(self, key, value):\n        """"""Allows convenience access to deeper levels using dots to separate\n        levels, for example `config[""a.b.c""]`.\n        """"""\n\n        if key == """":\n            if len(self.keys()) == 1:\n                key = list(self.keys())[0]\n            else:\n                raise KeyError(""Empty string only works for single element Configs."")\n\n        if type(key) == str and ""."" in key:\n            superkey = key.split(""."")[0]\n            subkeys = ""."".join(key.split(""."")[1:])\n            if superkey != """" and superkey not in self:\n                self[superkey] = Config()\n            if type(self[superkey]) == list:\n                try:\n                    subkeys = int(subkeys)\n                except ValueError:\n                    pass\n            self[superkey][subkeys] = value\n        elif type(value) == dict:\n            super(Config, self).__setitem__(key, Config(config=value))\n        else:\n            super(Config, self).__setitem__(key, value)\n\n    def __delitem__(self, key):\n        """"""Allows convenience access to deeper levels using dots to separate\n        levels, for example `config[""a.b.c""]`.\n        """"""\n\n        if type(key) == str and ""."" in key and key not in self:\n            superkey = key.split(""."")[0]\n            subkeys = ""."".join(key.split(""."")[1:])\n            if superkey not in self:\n                raise KeyError(superkey + "" not found."")\n            else:\n                self[superkey].__delitem__(subkeys)\n        else:\n            super().__delitem__(key)\n\n    def set_with_decode(self, key, value, stringify_value=False):\n        """"""Set single value, using :class:`.ModuleMultiTypeDecoder` to interpret\n        key and value strings by creating a temporary JSON string.\n\n        Args:\n            key (str): Config key.\n            value (str): New value key will map to.\n            stringify_value (bool): If `True`, will insert the value into the\n                temporary JSON as a real string. See examples!\n\n        Examples:\n            Example for when you need to set `stringify_value=True`::\n\n                config.set_with_decode(""key"", ""__type__(trixi.util.config.Config)"", stringify_value=True)\n\n            Example for when you need to set `stringify_value=False`::\n\n                config.set_with_decode(""key"", ""[1, 2, 3]"")\n\n        """"""\n\n        if type(key) != str:\n            # We could encode the key if it\'s not a string, but for now raise\n            raise TypeError(""set_with_decode requires string as key."")\n        if type(value) != str:\n            raise TypeError(""set_with_decode requires string as value."")\n\n        dict_str = """"\n        depth = 0\n        key_split = key.split(""."")\n\n        for k in key_split:\n            dict_str += ""{""\n            dict_str += \'""{}"":\'.format(k)\n            depth += 1\n\n        if stringify_value:\n            dict_str += \'""{}""\'.format(value)\n        else:\n            dict_str += ""{}"".format(value)\n\n        for _ in range(depth):\n            dict_str += ""}""\n\n        self.loads(dict_str)\n\n    def set_from_string(self, str_, stringify_value=False):\n        """"""Set a value from a single string, separated with ""="".\n        Uses :meth:\xc2\xb4set_with_decode\xc2\xb4.\n\n        Args:\n            str_ (str): String that looks like ""key=value"".\n\n        """"""\n\n        key, value = str_.split(""="")\n        self.set_with_decode(key, value, stringify_value)\n\n    def update_missing(self, dict_like, deep=False, ignore=None):\n        """"""Recursively insert values that do not yet exist.\n\n        Args:\n            dict_like (dict or derivative thereof): Update source.\n            deep (bool): Make deep copies of all references in the source.\n            ignore (iterable): Iterable of keys to ignore in update.\n\n        """"""\n\n        for key, value in dict_like.items():\n\n            if key not in self:\n                if type(value) == Config:\n                    if deep:\n                        self[key] = value.deepcopy()\n                    else:\n                        self[key] = value\n                else:\n                    if deep:\n                        self[key] = deepcopy(value)\n                    else:\n                        self[key] = value\n            else:\n                if isinstance(value, dict) and isinstance(self[key], dict):\n                    self[key].update_missing(Config(config=value, deep=deep))\n\n    def dump(self, file_, indent=4, separators=("","", "": ""), **kwargs):\n        """"""Write config to file using :meth:`json.dump`.\n\n        Args:\n            file_ (str or File): Write to this location.\n            indent (int): Formatting option.\n            separators (iterable): Formatting option.\n            **kwargs: Will be passed to :meth:`json.dump`.\n\n        """"""\n\n        if hasattr(file_, ""write""):\n            json.dump(self, file_, cls=ModuleMultiTypeEncoder, indent=indent, separators=separators, **kwargs)\n        else:\n            with open(file_, ""w"") as file_object:\n                json.dump(self, file_object, cls=ModuleMultiTypeEncoder, indent=indent, separators=separators, **kwargs)\n\n    def dumps(self, indent=4, separators=("","", "": ""), **kwargs):\n        """"""Get string representation using :meth:`json.dumps`.\n\n        Args:\n            indent (int): Formatting option.\n            separators (iterable): Formatting option.\n            **kwargs: Will be passed to :meth:`json.dumps`.\n\n        """"""\n        return json.dumps(self, cls=ModuleMultiTypeEncoder, indent=indent, separators=separators, **kwargs)\n\n    def load(self, file_, raise_=True, decoder_cls_=ModuleMultiTypeDecoder, **kwargs):\n        """"""Load config from file using :meth:`json.load`.\n\n        Args:\n            file_ (str or File): Read from this location.\n            raise (bool): Raise errors.\n            decoder_cls_ (type): Class that is used to decode JSON string.\n            **kwargs: Will be passed to :meth:`json.load`.\n\n        """"""\n\n        try:\n            if hasattr(file_, ""read""):\n                new_dict = json.load(file_, cls=decoder_cls_, **kwargs)\n            else:\n                with open(file_, ""r"") as file_object:\n                    new_dict = json.load(file_object, cls=decoder_cls_, **kwargs)\n        except Exception as e:\n            if raise_:\n                raise e\n\n        self.update(new_dict)\n\n    def loads(self, json_str, decoder_cls_=ModuleMultiTypeDecoder, **kwargs):\n        """"""Load config from JSON string using :meth:`json.loads`.\n\n        Args:\n            json_str (str): Interpret this string.\n            decoder_cls_ (type): Class that is used to decode JSON string.\n            **kwargs: Will be passed to :meth:`json.loads`.\n\n        """"""\n\n        if not json_str.startswith(""{""):\n            json_str = ""{"" + json_str\n        if not json_str.endswith(""}""):\n            json_str = json_str + ""}""\n        new_dict = json.loads(json_str, cls=decoder_cls_, **kwargs)\n        self.update(new_dict)\n\n    def hasattr_not_none(self, key):\n        try:\n            result = self[key]\n            return result is not None\n        except KeyError as ke:\n            return False\n\n    def contains(self, dict_like):\n        """"""Check whether all items in a dictionary-like object match the ones in\n        this Config.\n\n        Args:\n            dict_like (dict or derivative thereof): Returns True if this is\n                contained in this Config.\n\n        Returns:\n            bool: True if dict_like is contained in self, otherwise False.\n\n        """"""\n\n        dict_like_config = Config(config=dict_like)\n\n        for key, val in dict_like_config.items():\n\n            if key not in self:\n                return False\n            else:\n                if isinstance(val, dict):\n                    if not self[key].contains(val):\n                        return False\n                else:\n                    if not self[key] == val:\n                        return False\n\n        return True\n\n    def deepcopy(self):\n        """"""Get a deep copy of this Config.\n\n        Returns:\n            Config: A deep copy of self.\n\n        """"""\n\n        def _deepcopy(source, target):\n            for key, val in source.items():\n                if not isinstance(val, dict):\n                    try:\n                        target[key] = deepcopy(val)\n                    except TypeError as e:\n                        target[key] = val\n                else:\n                    target[key] = Config()\n                    _deepcopy(source[key], target[key])\n\n        new_config = Config()\n        _deepcopy(self, new_config)\n\n        return new_config\n\n    @staticmethod\n    def init_objects(config):\n        """"""Returns a new Config with types converted to instances.\n\n        Any value that is a Config and contains a type key will be converted to\n        an instance of that type::\n\n            {\n                ""stuff"": ""also_stuff"",\n                ""convert_me"": {\n                    type: {\n                        ""param"": 1,\n                        ""other_param"": 2\n                    },\n                    ""something_else"": ""hopefully_useless""\n                }\n            }\n\n        becomes::\n\n            {\n                ""stuff"": ""also_stuff"",\n                ""convert_me"": type(param=1, other_param=2)\n            }\n\n        Note that additional entries can be lost as shown above.\n\n        Args:\n            config (Config): New Config will be built from this one\n\n        Returns:\n            Config: A new config with instances made from type entries.\n        """"""\n\n        def init_sub_objects(objs):\n            if isinstance(objs, dict):\n                ret_dict = Config()\n                for key, val in objs.items():\n                    if isinstance(key, type):\n                        init_param = init_sub_objects(val)\n                        if isinstance(init_param, dict):\n                            init_obj = key(**init_param)\n                        elif isinstance(init_param, (list, tuple, set)):\n                            init_obj = key(*init_param)\n                        else:\n                            init_obj = key()\n                        return init_obj\n                    elif isinstance(val, (dict, list, tuple, set)):\n                        ret_dict[key] = init_sub_objects(val)\n                    else:\n                        ret_dict[key] = val\n                return ret_dict\n            elif isinstance(objs, (list, tuple, set)):\n                orig_type = type(objs)\n                ret_list = []\n                for el in objs:\n                    ret_list.append(init_sub_objects(el))\n                return orig_type(ret_list)\n            else:\n                return objs\n\n        return init_sub_objects(config)\n\n    def __str__(self):\n        return self.dumps(sort_keys=True)\n\n    def difference_config(self, *other_configs):\n        """"""Get the difference of this and any number of other configs.\n        See :meth:`difference_config_static` for more information.\n\n        Args:\n            *other_configs (Config): Compare these configs and self.\n\n        Returns:\n            Config: Difference of self and the other configs.\n\n        """"""\n        return self.difference_config_static(self, *other_configs)\n\n    @staticmethod\n    def difference_config_static(*configs, only_set=False, encode=True):\n        """"""Make a Config of all elements that differ between N configs.\n\n        The resulting Config looks like this::\n\n            {\n                key: (config1[key], config2[key], ...)\n            }\n\n        If the key is missing, None will be inserted. The inputs will not be\n        modified.\n\n        Args:\n            configs (Config): Any number of Configs\n            only_set (bool): If only the set of different values hould be returned or for each config the\n            corresponding one\n            encode (bool): If True, values will be encoded the same way as they are when exported to disk (e.g.""__type__(MyClass)"")\n\n        Returns:\n            Config: Possibly empty Config\n        """"""\n\n        difference = dict()\n        mmte = ModuleMultiTypeEncoder()\n\n        all_keys = set()\n        for config in configs:\n            all_keys.update(set(config.keys()))\n\n        for key in all_keys:\n\n            current_values = []\n            all_equal = True\n            all_configs = True\n\n            for config in configs:\n\n                if key not in config:\n                    all_equal = False\n                    all_configs = False\n                    current_values.append(None)\n                else:\n                    if encode:\n                        current_values.append(mmte._encode(config[key]))\n                    else:\n                        current_values.append(config[key])\n\n                if len(current_values) >= 2:\n                    if current_values[-1] != current_values[-2]:\n                        all_equal = False\n\n                if type(current_values[-1]) != Config:\n                    all_configs = False\n\n            if not all_equal:\n\n                if not all_configs:\n                    if not only_set:\n                        difference[key] = tuple(current_values)\n                    else:\n                        difference[key] = tuple(set(current_values))\n                else:\n                    difference[key] = Config.difference_config_static(*current_values, only_set=only_set)\n\n        return Config(config=difference)\n\n    def flat(self, keep_lists=True, max_split_size=10, flatten_int=False):\n        """"""Returns a flattened version of the Config as dict.\n\n        Nested Configs and lists will be replaced by concatenated keys like so::\n\n            {\n                ""a"": 1,\n                ""b"": [2, 3],\n                ""c"": {\n                    ""x"": 4,\n                    ""y"": {\n                        ""z"": 5\n                    }\n                },\n                ""d"": (6, 7)\n            }\n\n        Becomes::\n\n            {\n                ""a"": 1,\n                ""b"": [2, 3], # if keep_lists is True\n                ""b.0"": 2,\n                ""b.1"": 3,\n                ""c.x"": 4,\n                ""c.y.z"": 5,\n                ""d"": (6, 7)\n            }\n\n        We return a dict because dots are disallowed within Config keys.\n\n        Args:\n            keep_lists: Keeps list along with unpacked values\n            max_split_size: List longer than this will not be unpacked\n            flatten_int: Integer keys will be treated as strings\n\n        Returns:\n            dict: A flattened version of self\n        """"""\n\n        def flat_(obj):\n            def items():\n                for key, val in obj.items():\n                    if isinstance(val, dict) and (isinstance(key, str) or (isinstance(key, int) and flatten_int)):\n                        intermediate_dict = {}\n                        for subkey, subval in flat_(val).items():\n                            if isinstance(subkey, str):\n                                yield str(key) + ""."" + subkey, subval\n                            elif isinstance(subkey, int) and flatten_int:\n                                yield str(key) + ""."" + str(subkey), subval\n                            else:\n                                intermediate_dict[subkey] = subval\n                        if len(intermediate_dict) > 0:\n                            yield str(key), intermediate_dict\n                    elif isinstance(val, (list, tuple)):\n                        keep_this = (\n                            keep_lists or not isinstance(key, (str, int)) or (isinstance(key, int) and not flatten_int)\n                        )\n                        if max_split_size not in (None, False) and len(val) > max_split_size:\n                            keep_this = True\n                        if keep_this:\n                            yield key, val\n                        else:\n                            for i, subval in enumerate(val):\n                                yield str(key) + ""."" + str(i), subval\n                    else:\n                        yield key, val\n\n            return dict(items())\n\n        return flat_(self)\n\n    def to_cmd_args_str(self):\n        """"""Create a string representing what one would need to pass to the\n        command line. Does not yet use JSON encoding!\n\n        Returns:\n            str: Command line string\n\n        """"""\n\n        c_flat = self.flat()\n\n        str_list = []\n        for key, val in c_flat.items():\n\n            if isinstance(val, (list, tuple)):\n                vals = [str(v) for v in val]\n                val_str = "" "".join(vals)\n            else:\n                val_str = str(val)\n            str_list.append(""--{} {}"".format(key, val_str))\n\n        return ""  "".join(str_list)\n\n\ndef update_from_sys_argv(config, warn=False):\n    """"""Updates Config with the arguments passed as args when running the\n    program. Keys will be converted to command line options, then matching\n    options in `sys.argv` will be used to update the Config.\n\n    Args:\n        config (Config): Update this Config.\n        warn (bool): Raise warnings if there are unknown options. Turn this on\n            if you don\'t use any :class:`argparse.ArgumentParser` after to\n            check for possible errors.\n\n    """"""\n\n    import sys\n    import argparse\n    import warnings\n\n    def str2bool(v):\n        if v.lower() in (""yes"", ""true"", ""t"", ""y"", ""1""):\n            return True\n        elif v.lower() in (""no"", ""false"", ""f"", ""n"", ""0""):\n            return False\n        else:\n            raise argparse.ArgumentTypeError(""Boolean value expected."")\n\n    if len(sys.argv) > 1:\n\n        parser = argparse.ArgumentParser(allow_abbrev=False)\n        encoder = ModuleMultiTypeEncoder()\n        decoder = ModuleMultiTypeDecoder()\n\n        config_flat = config.flat()\n        for key, val in config_flat.items():\n            name = ""--{}"".format(key)\n            if val is None:\n                parser.add_argument(name)\n            else:\n                if type(val) == bool:\n                    parser.add_argument(name, type=str2bool, default=val)\n                elif isinstance(val, (list, tuple)):\n                    if len(val) > 0 and type(val[0]) != type:\n                        parser.add_argument(name, nargs=""+"", type=type(val[0]), default=val)\n                    else:\n                        parser.add_argument(name, nargs=""+"", default=val)\n                else:\n                    if type(val) == type:\n                        val = encoder._encode(val)\n                    parser.add_argument(name, type=type(val), default=val)\n\n        # parse args\n        param, unknown = parser.parse_known_args()\n        param = vars(param)\n\n        if len(unknown) > 0 and warn:\n            warnings.warn(""Called with unknown arguments: {}"".format(unknown), RuntimeWarning)\n\n        # calc diff between configs\n        diff_keys = list(Config.difference_config_static(param, config_flat).flat().keys())\n\n        # convert type args\n        ignore_ = []\n        for key, val in param.items():\n            if val in (""none"", ""None""):\n                param[key] = None\n            if type(config_flat[key]) == type:\n                if isinstance(val, str):\n                    val = val.replace(""\'"", """")\n                    val = val.replace(\'""\', """")\n                param[key] = decoder._decode(val)\n            try:\n                key_split = key.split(""."")\n                list_object, _ = ""."".join(key_split[:-1]), int(key_split[-1])\n                if ""--"" + list_object in sys.argv:\n                    ignore_.append(key)\n            except ValueError as ve:\n                pass\n        for i in ignore_:\n            del param[i]\n\n        ### Delete not changed entries\n        param_keys = list(param.keys())\n        for i in param_keys:\n            if i not in diff_keys and i in param:\n                del param[i]\n\n        # update dict\n        config.update(param)\n\n\ndef monkey_patch_fn_args_as_config(f):\n    """"""Decorator: Monkey patches, aka addes a variable \'fn_args_as_config\' to globals, \n    so that it can be accessed by the decorated function.\n    Adds all function parameters to a dict \'fn_args_as_config\', which can be accessed by the method. \n    Be careful using it!\n\n    """"""\n    sig = inspect.signature(f)\n\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        bound_arguments = sig.bind(*args, **kwargs)\n        bound_arguments.apply_defaults()\n        c = Config(config=bound_arguments.arguments)\n        if ""self"" in c:\n            c[""self""] = c[""self""].__class__\n\n        g = f.__globals__\n        g[""fn_args_as_config""] = c\n\n        try:\n            res = f(*args, **kwargs)\n        finally:\n            del g[""fn_args_as_config""]\n        return res\n\n    return wrapper\n'"
trixi/util/extravisdom.py,0,"b'import numpy as np\nfrom visdom import Visdom, _assert_opts, _opts2layout\n\n\nclass ExtraVisdom(Visdom):\n    def histogram_3d(self, X, win=None, env=None, opts=None):\n        """"""\n        Given an array it plots the histrograms of the entries.\n\n        Args:\n            X : An array of at least 2 dimensions, where the first dimensions gives the number of histograms.\n            win: Window name.\n            env: Env name.\n            opts: dict with options, especially opts[\'numbins\'] (number of histogram bins) and opts[\'mutiplier\']\n        ( factor to stretch / queeze the values on the x axis) should be considered.\n\n        Returns:\n            The send result.\n        """"""\n\n        opts = {} if opts is None else opts\n        _assert_opts(opts)\n\n        X = np.asarray(X)\n        assert X.ndim >= 2, \'X must have atleast 2 dimensions\'\n\n        opts[\'numbins\'] = opts.get(\'numbins\', min(30, len(X[0])))\n        opts[\'mutiplier\'] = opts.get(\'numbins\', 100)\n\n        traces = []\n        for i, array in enumerate(X):\n            array = array.flatten()\n            bins, intervals = np.histogram(array, bins=opts[\'numbins\'])\n\n            x = []\n            y = []\n            z = []\n            prev_interv = 0.\n            for b, iv in zip(bins, intervals):\n                interval_middle = float((prev_interv + iv) / 2.) * opts[\'mutiplier\']\n                z.append([float(b), float(b)])\n                y.append([interval_middle, interval_middle])\n                x.append([i * 2, i * 2 + 0.5])\n                prev_interv = float(iv)\n            traces.append(dict(\n                z=z,\n                x=x,\n                y=y,\n                # colorscale=[[i, \'rgb(%d,%d,255)\' % (ci, ci)] for i in np.arange(0, 1.1, 0.1)],\n                # autocolorscale=True,\n                showscale=False,\n                type=\'surface\',\n            ))\n\n        return self._send({\n            \'data\': traces,\n            \'win\': win,\n            \'eid\': env,\n            \'layout\': _opts2layout(opts),\n            \'opts\': opts,\n        })\n'"
trixi/util/gridsearch.py,0,"b'import json\nfrom trixi.util import ModuleMultiTypeDecoder\n\n\nclass GridSearch(dict):\n\n    def all_combinations(self):\n\n        if len(self) == 0:\n            return []\n\n        combinations = []\n        zero_key = sorted(self.keys())[0]\n        inner_dict = self.copy()\n        del inner_dict[zero_key]\n\n        for val in self[zero_key]:\n\n            if len(inner_dict) == 0:\n                combinations.append({zero_key: val})\n            else:\n                for combination in GridSearch(inner_dict):\n                    combination[zero_key] = val\n                    combinations.append(combination)\n\n        return combinations\n\n    def __iter__(self):\n\n        self._counter = 0\n        self._all_combinations = self.all_combinations()\n        self._max = len(self._all_combinations)\n        return self\n\n    def __next__(self):\n\n        if self._counter >= self._max:\n            raise StopIteration\n\n        self._counter += 1\n        return self._all_combinations[self._counter - 1]\n\n    def read(self, file_, raise_=True, decoder_cls_=ModuleMultiTypeDecoder, **kwargs):\n\n        try:\n            if hasattr(file_, ""read""):\n                new_dict = json.load(file_, cls=decoder_cls_, **kwargs)\n            else:\n                with open(file_, ""r"") as file_object:\n                    new_dict = json.load(file_object, cls=decoder_cls_, **kwargs)\n        except Exception as e:\n            if raise_:\n                raise e\n\n        self.update(new_dict)\n\n        return self\n'"
trixi/util/metrics.py,0,"b'import atexit\nimport warnings\nfrom collections import OrderedDict\nfrom multiprocessing import Process\n\nimport numpy as np\nfrom sklearn import metrics\n\n\ndef get_roc_curve(tensor, labels, reduce_to_n_samples=None, use_sub_process=False, results_fn=lambda\n        x, *y, **z: None):\n    """"""\n    Displays a roc curve given a tensor with scores and the coresponding labels\n\n    Args:\n        tensor: Tensor with scores (e.g class probability )\n        labels: Labels of the samples to which the scores match\n        reduce_to_n_samples: Reduce/ downsample to to n samples for fewer data points\n        use_sub_process: Use a sub process to do the processing, if true nothing is returned\n        results_fn: function which is called with the results/ return values. Expected f(tpr, fpr)\n\n    """"""\n\n    def __get_roc_curve(tensor, labels, reduce_to_n_samples=None, results_fn=lambda x, *y, **z: None):\n\n        if not isinstance(labels, list):\n            labels = labels.flatten()\n        if not isinstance(tensor, list):\n            tensor = tensor.flatten()\n\n        fpr, tpr, thresholds = metrics.roc_curve(labels, tensor)\n        if reduce_to_n_samples is not None:\n            fpr = [np.mean(x) for x in np.array_split(fpr, reduce_to_n_samples)]\n            tpr = [np.mean(x) for x in np.array_split(tpr, reduce_to_n_samples)]\n        results_fn(tpr, fpr)\n\n        return tpr, fpr\n        # self.show_lineplot(tpr, fpr, name=name, opts={""fillarea"": True, ""webgl"": True})\n        # self.add_to_graph(x_vals=np.arange(0, 1.1, 0.1), y_vals=np.arange(0, 1.1, 0.1), name=name, append=True)\n\n    if use_sub_process:\n        p = Process(target=__get_roc_curve, kwargs=dict(tensor=tensor,\n                                                        labels=labels,\n                                                        reduce_to_n_samples=reduce_to_n_samples,\n                                                        results_fn=results_fn\n                                                        ))\n        atexit.register(p.terminate)\n        p.start()\n    else:\n        try:\n            return __get_roc_curve(tensor=tensor,\n                                   labels=labels,\n                                   reduce_to_n_samples=reduce_to_n_samples,\n                                   results_fn=results_fn\n                                   )\n        except Exception as e:\n            warnings.warn(""Sth went wrong with calculating the roc curve"")\n\n\ndef get_pr_curve(tensor, labels, reduce_to_n_samples=None, use_sub_process=False,\n                 results_fn=lambda x, *y, **z: None):\n    """"""\n    Displays a precision recall curve given a tensor with scores and the coresponding labels\n\n    Args:\n        tensor: Tensor with scores (e.g class probability )\n        labels: Labels of the samples to which the scores match\n        reduce_to_n_samples: Reduce/ downsample to to n samples for fewer data points\n        use_sub_process: Use a sub process to do the processing, if true nothing is returned\n        results_fn: function which is called with the results/ return values. Expected f(precision, recall)\n\n    """"""\n\n    def __get_pr_curve(tensor, labels, reduce_to_n_samples=None, results_fn=lambda x, *y, **z: None):\n\n        if not isinstance(labels, list):\n            labels = labels.flatten()\n        if not isinstance(tensor, list):\n            tensor = tensor.flatten()\n\n        precision, recall, thresholds = metrics.precision_recall_curve(labels, tensor)\n        if reduce_to_n_samples is not None:\n            precision = [np.mean(x) for x in np.array_split(precision, reduce_to_n_samples)]\n            recall = [np.mean(x) for x in np.array_split(recall, reduce_to_n_samples)]\n        results_fn(precision, recall)\n\n        return precision, recall\n        # self.show_lineplot(precision, recall, name=name, opts={""fillarea"": True, ""webgl"": True})\n        # self.add_to_graph(x_vals=np.arange(0, 1.1, 0.1), y_vals=np.arange(0, 1.1, 0.1), name=name, append=True)\n\n    if use_sub_process:\n        p = Process(target=__get_pr_curve, kwargs=dict(tensor=tensor,\n                                                       labels=labels,\n                                                       reduce_to_n_samples=reduce_to_n_samples,\n                                                       results_fn=results_fn\n                                                       ))\n        atexit.register(p.terminate)\n        p.start()\n    else:\n        try:\n            return __get_pr_curve(tensor=tensor,\n                                  labels=labels,\n                                  reduce_to_n_samples=reduce_to_n_samples,\n                                  results_fn=results_fn\n                                  )\n        except Exception as e:\n            warnings.warn(""Sth went wrong with calculating the pr curve"")\n\n\ndef _get_classification_metrics(tensor, labels, metric=""roc-auc""):\n    """"""\n   Calculates a metric given the predicted values and the given/correct labels.\n\n    Args:\n        tensor: Tensor with scores (e.g class probability )\n        labels: Labels of the samples to which the scores match\n        metric: List of metrics to calculate. Options are: roc-auc, pr-auc, pr-score, mcc, f1\n\n    Returns:\n        The metric value\n\n    """"""\n\n    if not isinstance(labels, list):\n        labels = labels.flatten()\n    if not isinstance(tensor, list):\n        tensor = tensor.flatten()\n\n    metric_value = 0.0\n    if ""roc-auc"" == metric:\n        metric_value = metrics.roc_auc_score(labels, tensor)\n    if ""pr-auc"" == metric:\n        precision, recall, thresholds = metrics.precision_recall_curve(labels, tensor)\n        metric_value = metrics.auc(recall, precision)\n    if ""pr-score"" == metric:\n        metric_value = metrics.average_precision_score(labels, tensor)\n    if ""mcc"" == metric:\n        metric_value = metrics.matthews_corrcoef(labels, tensor)\n    if ""f1"" == metric:\n        metric_value = metrics.f1_score(labels, tensor)\n\n    return metric_value\n\n\ndef get_classification_metrics(tensor, labels, name="""", metric=(""roc-auc"", ""pr-score""), use_sub_process=False,\n                               tag_name=None, results_fn=lambda x, *y, **z: None):\n    """"""\n    Displays some classification metrics as line plots in a graph (similar to show value (also uses show value\n    for the caluclated values))\n\n    Args:\n        tensor: Tensor with scores (e.g class probability )\n        labels: Labels of the samples to which the scores match\n        name: The name of the window\n        metric: List of metrics to calculate. Options are: roc-auc, pr-auc, pr-score, mcc, f1\n        tag_name: Name for the tag, if no given use name\n        use_sub_process: Use a sub process to do the processing, if true nothing is returned\n        results_fn: function which is called with the results/ return values. Expected f(val, name, tag)\n\n    Returns:\n\n    """"""\n\n    def __get_classification_metrics(tensor, labels, name="""", metric=(""roc-auc"", ""pr-score""),\n                                     tag_name=None, results_fn=lambda x, *y, **z: None):\n\n        if not isinstance(labels, list):\n            labels = labels.flatten()\n        if not isinstance(tensor, list):\n            tensor = tensor.flatten()\n\n        res_dict = OrderedDict()\n\n        for m in metric:\n            res_dict[m] = _get_classification_metrics(tensor, labels, m)\n\n        for tag, val in res_dict.items():\n            results_fn(val, name=tag + ""-"" + name, tag=tag_name)\n\n        return res_dict\n\n    if use_sub_process:\n        p = Process(target=__get_classification_metrics, kwargs=dict(tensor=tensor,\n                                                                     labels=labels,\n                                                                     name=name,\n                                                                     metric=metric,\n                                                                     tag_name=tag_name,\n                                                                     results_fn=results_fn\n                                                                     ))\n        atexit.register(p.terminate)\n        p.start()\n    else:\n        try:\n            return __get_classification_metrics(tensor=tensor,\n                                                labels=labels,\n                                                name=name,\n                                                metric=metric,\n                                                tag_name=tag_name,\n                                                results_fn=results_fn\n                                                )\n\n        except Exception as e:\n            warnings.warn(""Sth went wrong with calculating the classification metrics"")\n            ret = {m: 0.0 for m in metric}\n            return ret\n\n\ndef get_classification_metric(tensor, labels, metric=""roc-auc"", use_sub_process=False):\n    """"""\n   Calculates a metric given the predicted values and the given/correct labels.\n\n    Args:\n        tensor: Tensor with scores (e.g class probability )\n        labels: Labels of the samples to which the scores match\n        metric: List of metrics to calculate. Options are: roc-auc, pr-auc, pr-score, mcc, f1\n        use_sub_process: Use a sub process to do the processing, if true nothing is returned\n\n    Returns:\n        The metric value\n\n    """"""\n\n    if use_sub_process:\n        p = Process(target=_get_classification_metrics, kwargs=dict(tensor=tensor,\n                                                                    labels=labels,\n                                                                    metric=metric,\n                                                                    ))\n        atexit.register(p.terminate)\n        p.start()\n    else:\n        try:\n            return _get_classification_metrics(tensor=tensor,\n                                               labels=labels,\n                                               metric=metric,\n                                               )\n\n        except Exception as e:\n            warnings.warn(""Sth went wrong with calculating the classification metrics"")\n            return 0.0\n'"
trixi/util/pytorchexperimentstub.py,0,"b'import json\nimport os\nimport time\nimport warnings\nfrom unittest.mock import Mock\n\nimport numpy as np\n\nfrom trixi.logger import PytorchExperimentLogger, PytorchVisdomLogger, TelegramMessageLogger\nfrom trixi.logger.message.slackmessagelogger import SlackMessageLogger\nfrom trixi.logger.tensorboard import PytorchTensorboardLogger\nfrom trixi.util import ResultElement, ResultLogDict, Config\n\nlogger_lookup_dict = dict(\n    visdom=PytorchVisdomLogger,\n    tensorboard=PytorchTensorboardLogger,\n    telegram=TelegramMessageLogger,\n    slack=SlackMessageLogger,\n)\n\n\nclass PytorchExperimentStub:\n    def __init__(self, base_dir=None, name=None, config=None, loggers=None):\n        super(PytorchExperimentStub, self).__init__()\n\n        if config is None:\n            config = Config()\n        if loggers is None:\n            loggers = {}\n\n        # assert base_dir is not None or ""base_dir"" in config, ""A base dir has to be given, either directly or via config""\n\n        if name is None and ""name"" in config:\n            self.name = config[""name""]\n        elif name is None:\n            self.name = ""experiment""\n        else:\n            self.name = name\n\n        if base_dir is not None:\n            self.base_dir = base_dir\n        else:\n            self.base_dir = config.get(""base_dir"")\n\n        self.config = config\n\n        if base_dir is not None:\n            self.elog = PytorchExperimentLogger(base_dir=self.base_dir, exp_name=self.name)\n\n            self.results = ResultLogDict(""results-log.json"", base_dir=self.elog.result_dir)\n        else:\n            warnings.warn(""PytorchExperimentStub will not save to drive"")\n            self.elog = Mock()\n            self.results = dict()\n\n        self.loggers = {}\n        for logger_name, logger_cfg in loggers.items():\n            _logger = self._make_logger(logger_name, logger_cfg)\n            self.loggers[logger_name] = _logger\n\n        self._save_exp_config()\n        self.elog.save_config(self.config, ""config"")\n\n    def _make_logger(self, logger_name, logger_cfg):\n\n        if isinstance(logger_cfg, (list, tuple)):\n            log_type = logger_cfg[0]\n            log_params = logger_cfg[1] if len(logger_cfg) > 1 else {}\n            log_freq = logger_cfg[2] if len(logger_cfg) > 2 else 10\n        else:\n            assert isinstance(logger_cfg, str), (\n                ""The specified logger has to either be a string or a list with "" ""name, parameters, clog_frequency""\n            )\n            log_type = logger_cfg\n            log_params = {}\n            log_freq = 10\n\n        if ""exp_name"" not in log_params:\n            log_params[""exp_name""] = self.name\n\n        if log_type == ""tensorboard"":\n            if ""target_dir"" not in log_params or log_params[""target_dir""] is None:\n                if self.elog is not None and not isinstance(self.elog, Mock):\n                    log_params[""target_dir""] = os.path.join(self.elog.save_dir, ""tensorboard"")\n                else:\n                    raise AttributeError(""TensorboardLogger requires a target_dir or an ExperimentLogger instance."")\n            elif self.elog is not None and not isinstance(self.elog, Mock):\n                log_params[""target_dir""] = os.path.join(log_params[""target_dir""], self.elog.folder_name)\n\n        log_type = logger_lookup_dict[log_type]\n        _logger = log_type(**log_params)\n\n        return _logger\n\n    @property\n    def vlog(self):\n        if ""visdom"" in self.loggers:\n            return self.loggers[""visdom""]\n        elif ""v"" in self.loggers:\n            return self.loggers[""v""]\n        else:\n            return None\n\n    @property\n    def tlog(self):\n        if ""tensorboard"" in self.loggers:\n            return self.loggers[""tensorboard""]\n        if ""tensorboardx"" in self.loggers:\n            return self.loggers[""tensorboardx""]\n        elif ""tx"" in self.loggers:\n            return self.loggers[""tx""]\n        else:\n            return None\n\n    @property\n    def slog(self):\n        if ""slack"" in self.loggers:\n            return self.loggers[""slack""]\n        elif ""s"" in self.loggers:\n            return self.loggers[""s""]\n        else:\n            return None\n\n    @property\n    def l(self):\n        return self.loggers\n\n    def _save_exp_config(self):\n\n        if self.elog is not None and not isinstance(self.elog, Mock):\n            cur_time = time.strftime(""%y-%m-%d_%H:%M:%S"", time.localtime(time.time()))\n            self.elog.save_config(\n                Config(**{""name"": self.name, ""time"": cur_time, ""state"": ""Stub"", ""current_time"": cur_time, ""epoch"": 0}),\n                ""exp"",\n            )\n\n    def add_result(self, value, name, counter=None, tag=None, label=None, plot_result=True, plot_running_mean=False):\n        """"""\n        Saves a results and add it to the result dict, this is similar to results[key] = val,\n        but in addition also logs the value to the combined logger\n        (it also stores in the results-logs file).\n\n        **This should be your preferred method to log your numeric values**\n\n        Args:\n            value: The value of your variable\n            name (str): The name/key of your variable\n            counter (int or float): A counter which can be seen as the x-axis of your value.\n                Normally you would just use the current epoch for this.\n            tag (str): A label/tag which can group similar values and will plot values with the same\n                label in the same plot\n            label: deprecated label\n            plot_result (bool): By default True, will also log all your values to the combined\n                logger (with show_value).\n\n        """"""\n\n        if label is not None:\n            warnings.warn(""label in add_result is deprecated, please use tag instead"")\n\n            if tag is None:\n                tag = label\n\n        tag_name = tag\n        if tag_name is None:\n            tag_name = name\n\n        r_elem = ResultElement(data=value, label=tag_name, epoch=0, counter=counter)\n\n        self.results[name] = r_elem\n\n        if plot_result:\n            if tag is None:\n                legend = False\n            else:\n                legend = True\n            if plot_running_mean:\n                value = np.mean(self.results.running_mean_dict[name])\n            self.elog.show_value(value=value, name=name, tag=tag_name, counter=counter, show_legend=legend)\n            for log_name, logger_ in self.loggers.items():\n                logger_.show_value(value=value, name=name, tag=tag_name, counter=counter, show_legend=legend)\n\n    def get_result(self, name):\n        """"""\n        Similar to result[key] this will return the values in the results dictionary with the given\n        name/key.\n\n        Args:\n            name (str): the name/key for which a value is stored.\n\n        Returns:\n            The value with the key \'name\' in the results dict.\n\n        """"""\n        return self.results.get(name)\n\n    def add_result_without_epoch(self, val, name):\n        """"""\n        A faster method to store your results, has less overhead and does not call the combined\n        logger. Will only store to the results dictionary.\n\n        Args:\n            val: the value you want to add.\n            name (str): the name/key of your value.\n\n        """"""\n        self.results[name] = val\n\n    def add_res(self, **kwargs):\n        """"""\n                A faster method to store your results, has less overhead and does not call the combined\n                logger. Will only store to the results dictionary.\n\n                Args:\n                    kwargs: dict with name/keys of your values the values you want to add.\n                """"""\n        for key, val in kwargs.items():\n            self.results[key] = val\n\n    def get_result_without_epoch(self, name):\n        """"""\n        Similar to result[key] this will return the values in result with the given name/key.\n\n        Args:\n            name (str): the name/ key for which a value is stores.\n\n        Returns:\n            The value with the key \'name\' in the results dict.\n\n        """"""\n        return self.results.get(name)\n\n    def print(self, *args):\n        """"""\n        Calls \'print\' on the experiment logger or uses builtin \'print\' if former is not\n        available.\n        """"""\n\n        if self.elog is None or isinstance(self.elog, Mock):\n            print(*args)\n        else:\n            self.elog.print(*args)\n\n    def save_results(self, name=""results.json""):\n        """"""\n        Saves the result dict as a json file in the result dir of the experiment logger.\n\n        Args:\n            name (str): The name of the json file in which the results are written.\n\n        """"""\n        if self.elog is None or isinstance(self.elog, Mock):\n            return\n        with open(os.path.join(self.elog.result_dir, name), ""w"") as file_:\n            json.dump(self.results, file_, indent=4)\n\n    def close_tmp_results(self):\n        """"""\n        Closes the tmp results file (i.e. add a \'}\' )\n\n        """"""\n        if isinstance(self.results, ResultLogDict):\n            self.results.close()\n\n    def save_model(self, model, name, n_iter=None, iter_format=""{:05d}"", prefix=False):\n        """"""\n        Saves a pytorch model in the model directory of the experiment folder\n\n        Args:\n            model: The model to be stored\n            name: The file name of the model file\n            n_iter: The iteration number, formatted with the iter_format and added to the model name (if not None)\n            iter_format: The format string, which indicates how n_iter will be formated as a string\n            prefix: If True, the formated n_iter will be appended as a prefix, otherwise as a suffix\n        """"""\n        self.elog.save_model(model, name, n_iter, iter_format, prefix)\n\n    def save_checkpoint(self, name, n_iter=None, iter_format=""{:05d}"", prefix=False, **kwargs):\n        """"""\n        Saves a checkpoint in the checkpoint directory of the experiment folder\n\n        Args:\n            name: The file name of the checkpoint file\n            n_iter: The iteration number, formatted with the iter_format and added to the checkpoint name (if not None)\n            iter_format: The format string, which indicates how n_iter will be formated as a string\n            prefix: If True, the formated n_iter will be appended as a prefix, otherwise as a suffix\n            **kwargs:  dict which is actually saved (key=name, value=variable to be stored)\n\n        """"""\n        self.elog.save_checkpoint(name, n_iter, iter_format, prefix, **kwargs)\n'"
trixi/util/pytorchutils.py,14,"b'import random\nimport warnings\nfrom functools import lru_cache\n\nimport numpy as np\nimport torch\n\n\n#@lru_cache(maxsize=32)\ndef get_vanilla_image_gradient(model, inpt, err_fn, abs=False):\n    if isinstance(model, torch.nn.Module):\n        model.zero_grad()\n    inpt = inpt.detach()\n    inpt.requires_grad = True\n\n    # output = model(inpt)\n\n    err = err_fn(inpt)\n    err.backward()\n\n    grad = inpt.grad.detach()\n\n    if isinstance(model, torch.nn.Module):\n        model.zero_grad()\n\n    if abs:\n        grad = torch.abs(grad)\n    return grad.detach()\n\n\n#@lru_cache(maxsize=32)\ndef get_guided_image_gradient(model: torch.nn.Module, inpt, err_fn, abs=False):\n    def guided_relu_hook_function(module, grad_in, grad_out):\n        if isinstance(module, (torch.nn.ReLU, torch.nn.LeakyReLU)):\n            return (torch.clamp(grad_in[0], min=0.0),)\n\n    model.zero_grad()\n\n    ### Apply hooks\n    hook_ids = []\n    for mod in model.modules():\n        hook_id = mod.register_backward_hook(guided_relu_hook_function)\n        hook_ids.append(hook_id)\n\n    inpt = inpt.detach()\n    inpt.requires_grad = True\n\n    # output = model(inpt)\n\n    err = err_fn(inpt)\n    err.backward()\n\n    grad = inpt.grad.detach()\n\n    model.zero_grad()\n    for hooks in hook_ids:\n        hooks.remove()\n\n    if abs:\n        grad = torch.abs(grad)\n    return grad.detach()\n\n\n#@lru_cache(maxsize=32)\ndef get_smooth_image_gradient(model, inpt, err_fn, abs=True, n_runs=20, eps=0.1,  grad_type=""vanilla""):\n    grads = []\n    for i in range(n_runs):\n        inpt = inpt + torch.randn(inpt.size()).to(inpt.device) * eps\n        if grad_type == ""vanilla"":\n            single_grad = get_vanilla_image_gradient(model, inpt, err_fn, abs=abs)\n        elif grad_type == ""guided"":\n            single_grad = get_guided_image_gradient(model, inpt, err_fn, abs=abs)\n        else:\n            warnings.warn(""This grad_type is not implemented yet"")\n            single_grad = torch.zeros_like(inpt)\n        grads.append(single_grad)\n\n    grad = torch.mean(torch.stack(grads), dim=0)\n    return grad.detach()\n\n\ndef get_input_gradient(model, inpt, err_fn, grad_type=""vanilla"", n_runs=20, eps=0.1,\n                       abs=False, results_fn=lambda x, *y, **z: None):\n    """"""\n    Given a model creates calculates the error and backpropagates it to the image and saves it (saliency map).\n\n    Args:\n        model: The model to be evaluated\n        inpt: Input to the model\n        err_fn: The error function the evaluate the output of the model on\n        grad_type: Gradient calculation method, currently supports (vanilla, vanilla-smooth, guided,\n        guided-smooth) ( the guided backprob can lead to segfaults -.-)\n        n_runs: Number of runs for the smooth variants\n        eps: noise scaling to be applied on the input image (noise is drawn from N(0,1))\n        abs (bool): Flag, if the gradient should be a absolute value\n        results_fn: function which is called with the results/ return values. Expected f(grads)\n\n    """"""\n    model.zero_grad()\n\n    if grad_type == ""vanilla"":\n        grad = get_vanilla_image_gradient(model, inpt, err_fn, abs)\n    elif grad_type == ""guided"":\n        grad = get_guided_image_gradient(model, inpt, err_fn, abs)\n    elif grad_type == ""smooth-vanilla"":\n        grad = get_smooth_image_gradient(model, inpt, err_fn, abs, n_runs, eps, grad_type=""vanilla"")\n    elif grad_type == ""smooth-guided"":\n        grad = get_smooth_image_gradient(model, inpt, err_fn, abs, n_runs, eps, grad_type=""guided"")\n    else:\n        warnings.warn(""This grad_type is not implemented yet"")\n        grad = torch.zeros_like(inpt)\n    model.zero_grad()\n\n    results_fn(grad)\n\n    return grad\n\ndef update_model(original_model, update_dict, exclude_layers=(), do_warnings=True):\n    # also allow loading of partially pretrained net\n    model_dict = original_model.state_dict()\n\n    # 1. Give warnings for unused update values\n    unused = set(update_dict.keys()) - set(exclude_layers) - set(model_dict.keys())\n    not_updated = set(model_dict.keys()) - set(exclude_layers) - set(update_dict.keys())\n    if do_warnings:\n        for item in unused:\n            warnings.warn(""Update layer {} not used."".format(item))\n        for item in not_updated:\n            warnings.warn(""{} layer not updated."".format(item))\n\n    # 2. filter out unnecessary keys\n    update_dict = {k: v for k, v in update_dict.items() if\n                   k in model_dict and k not in exclude_layers}\n\n    # 3. overwrite entries in the existing state dict\n    model_dict.update(update_dict)\n\n    # 4. load the new state dict\n    original_model.load_state_dict(model_dict)\n\n\ndef set_seed(seed):\n    """"""Sets the seed""""""\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    # if torch.cuda.is_available():\n    #     torch.cuda.manual_seed_all(seed)\n'"
trixi/util/sourcepacker.py,0,"b'import os.path\nimport re\nimport subprocess\nimport subprocess as subp\nimport sys\nimport zipfile\n\n\nclass SourcePacker(object):\n    """"""\n    Inspired by https://github.com/IDSIA/sacred\n    """"""\n\n    @staticmethod\n    def join_paths(*parts):\n        """"""Join different parts together to a valid dotted path.""""""\n        return \'.\'.join(str(p).strip(\'.\') for p in parts if p)\n\n    @staticmethod\n    def iter_prefixes(path):\n        """"""\n        Iterate through all (non-empty) prefixes of a dotted path.\n        Example:\n        >>> list(iter_prefixes(\'foo.bar.baz\'))\n        [\'foo\', \'foo.bar\', \'foo.bar.baz\']\n        """"""\n        split_path = path.split(\'.\')\n        for i in range(1, len(split_path) + 1):\n            yield SourcePacker.join_paths(*split_path[:i])\n\n    @staticmethod\n    def create_source_or_dep(mod, sources):\n        filename = \'\'\n        if mod is not None and hasattr(mod, \'__file__\'):\n            filename = os.path.abspath(mod.__file__)\n\n        ### To source or dependency\n        if filename and filename not in sources and SourcePacker.is_source(filename):\n            sources.add(filename)\n\n    @staticmethod\n    def is_source(filename):\n        if "".virtualenvs"" in filename or ""site-packages"" in filename or re.search(""python[0-9]\\.[0-9]"", filename) is not \\\n                None:\n            return False\n        else:\n            return True\n\n    @staticmethod\n    def git_info(file_):\n\n        old_dir = os.getcwd()\n        file_path = os.path.abspath(file_)\n        os.chdir(os.path.dirname(file_path))\n\n        try:\n            commit = subp.check_output([""git"", ""rev-parse"", ""HEAD""]).decode(""ascii"")[:-1]\n            branch = subp.check_output([""git"", ""rev-parse"", ""--abbrev-ref"", ""HEAD""]).decode(""ascii"")[:-1]\n            repo = subp.check_output([""git"", ""remote"", ""-vv""]).decode(""ascii"")\n            repo = re.findall(""(?<=origin[\\s\\t])(http.+|ssh.+|git.+)(?=[\\s\\t]\\(fetch)"", repo)[0]\n            result = (repo, branch, commit)\n        except Exception as e:\n            print(""Could not find git info for {}"".format(file_path))\n            print(e)\n            result = (None, None, None)\n\n        os.chdir(old_dir)\n        return result\n\n    @staticmethod\n    def gather_sources_and_dependencies(globs):\n        py_str = ""python {}"".format(sys.version)\n        dependencies = subprocess.check_output([sys.executable, \'-m\', \'pip\', \'freeze\']).decode(""utf-8"").split(""\\n"")\n\n        filename = globs.get(\'__file__\')\n\n        if filename is None:\n            sources = set()\n        else:\n            sources = set()\n            sources.add(filename)\n        for glob in globs.values():\n            if isinstance(glob, type(sys)):\n                mod_path = glob.__name__\n            elif hasattr(glob, \'__module__\'):\n                mod_path = glob.__module__\n            else:\n                continue\n\n            if not mod_path:\n                continue\n\n            for modname in SourcePacker.iter_prefixes(mod_path):\n                mod = sys.modules.get(modname)\n                SourcePacker.create_source_or_dep(mod, sources)\n\n        return py_str, sources, dependencies\n\n    @staticmethod\n    def zip_sources(globs, filename):\n\n        py_str, sources, dependencies = SourcePacker.gather_sources_and_dependencies(globs=globs)\n        repo, branch, commit = SourcePacker.git_info(globs.get(""__file__""))\n        cmd = "" "".join(sys.argv)\n\n        with zipfile.ZipFile(filename, mode=\'w\') as zf:\n            for source in sources:\n                zf.write(source)\n\n            zf.writestr(""python_version.txt"", py_str)\n            dep_str = ""\\n"".join(dependencies)\n            zf.writestr(""modules.txt"", dep_str)\n            git_str = ""Repository: {}\\nBranch: {}\\nCommit: {}"".format(repo, branch, commit)\n            zf.writestr(""git_info.txt"", git_str)\n            zf.writestr(""command.txt"", cmd)\n'"
trixi/util/util.py,1,"b'import ast\nimport importlib\nimport io\nimport json\nimport logging\nimport math\nimport pickle\n\nimport numpy as np\nimport os\nimport random\nimport string\nimport matplotlib.pyplot as plt\nimport time\nimport traceback\nimport warnings\nfrom collections import defaultdict, deque\nfrom hashlib import sha256\nfrom tempfile import gettempdir\nfrom types import FunctionType, ModuleType\n\nimport numpy as np\nimport portalocker\nfrom imageio import imwrite\n\ntry:\n    import torch\nexcept ImportError as e:\n    import warnings\n\n    warnings.warn(ImportWarning(""Could not import Pytorch related modules:\\n%s""\n                                % e.msg))\n\n\n    class torch:\n        dtype = None\n\n\nclass CustomJSONEncoder(json.JSONEncoder):\n\n    def _encode(self, obj):\n        raise NotImplementedError\n\n    def _encode_switch(self, obj):\n        if isinstance(obj, list):\n            return [self._encode_switch(item) for item in obj]\n        elif isinstance(obj, dict):\n            return {self._encode_key(key): self._encode_switch(val) for key, val in obj.items()}\n        else:\n            return self._encode(obj)\n\n    def _encode_key(self, obj):\n        return self._encode(obj)\n\n    def encode(self, obj):\n        return super(CustomJSONEncoder, self).encode(self._encode_switch(obj))\n\n    def iterencode(self, obj, *args, **kwargs):\n        return super(CustomJSONEncoder, self).iterencode(self._encode_switch(obj), *args, **kwargs)\n\n\nclass MultiTypeEncoder(CustomJSONEncoder):\n\n    def _encode_key(self, obj):\n        if isinstance(obj, int):\n            return ""__int__({})"".format(obj)\n        elif isinstance(obj, float):\n            return ""__float__({})"".format(obj)\n        else:\n            return self._encode(obj)\n\n    def _encode(self, obj):\n        if isinstance(obj, tuple):\n            return ""__tuple__({})"".format(obj)\n        elif isinstance(obj, np.integer):\n            return ""__int__({})"".format(obj)\n        elif isinstance(obj, np.floating):\n            return ""__float__({})"".format(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        else:\n            return obj\n\n\nclass ModuleMultiTypeEncoder(MultiTypeEncoder):\n\n    def _encode(self, obj, strict=False):\n        if type(obj) == type:\n            return ""__type__({}.{})"".format(obj.__module__, obj.__name__)\n        elif type(obj) == torch.dtype:\n            return ""__type__({})"".format(str(obj))\n        elif isinstance(obj, FunctionType):\n            return ""__function__({}.{})"".format(obj.__module__, obj.__name__)\n        elif isinstance(obj, ModuleType):\n            return ""__module__({})"".format(obj.__name__)\n        else:\n            try:\n                return super(ModuleMultiTypeEncoder, self)._encode(obj)\n            except Exception as e:\n                if strict:\n                    raise e\n                else:\n                    message = ""Could not pickle object of type {}\\n"".format(type(obj))\n                    message += traceback.format_exc()\n                    warnings.warn(message)\n                    return repr(obj)\n\n\nclass CustomJSONDecoder(json.JSONDecoder):\n\n    def _decode(self, obj):\n        raise NotImplementedError\n\n    def _decode_switch(self, obj):\n        if isinstance(obj, list):\n            return [self._decode_switch(item) for item in obj]\n        elif isinstance(obj, dict):\n            return {self._decode_key(key): self._decode_switch(val) for key, val in obj.items()}\n        else:\n            return self._decode(obj)\n\n    def _decode_key(self, obj):\n        return self._decode(obj)\n\n    def decode(self, obj):\n        return self._decode_switch(super(CustomJSONDecoder, self).decode(obj))\n\n\nclass MultiTypeDecoder(CustomJSONDecoder):\n\n    def _decode(self, obj):\n        if isinstance(obj, str):\n            if obj.startswith(""__int__""):\n                return int(obj[8:-1])\n            elif obj.startswith(""__float__""):\n                return float(obj[10:-1])\n            elif obj.startswith(""__tuple__""):\n                return tuple(ast.literal_eval(obj[10:-1]))\n        return obj\n\n\nclass ModuleMultiTypeDecoder(MultiTypeDecoder):\n\n    def _decode(self, obj):\n        if isinstance(obj, str):\n            if obj.startswith(""__type__""):\n                str_ = obj[9:-1]\n                module_ = ""."".join(str_.split(""."")[:-1])\n                name_ = str_.split(""."")[-1]\n                type_ = str_\n                try:\n                    type_ = getattr(importlib.import_module(module_), name_)\n                except Exception as e:\n                    warnings.warn(""Could not load {}"".format(str_))\n                return type_\n            elif obj.startswith(""__function__""):\n                str_ = obj[13:-1]\n                module_ = ""."".join(str_.split(""."")[:-1])\n                name_ = str_.split(""."")[-1]\n                type_ = str_\n                try:\n                    type_ = getattr(importlib.import_module(module_), name_)\n                except Exception as e:\n                    warnings.warn(""Could not load {}"".format(str_))\n                return type_\n            elif obj.startswith(""__module__""):\n                str_ = obj[11:-1]\n                type_ = str_\n                try:\n                    type_ = importlib.import_module(str_)\n                except Exception as e:\n                    warnings.warn(""Could not load {}"".format(str_))\n                return type_\n        return super(ModuleMultiTypeDecoder, self)._decode(obj)\n\n\nclass StringMultiTypeDecoder(CustomJSONDecoder):\n\n    def _decode(self, obj):\n        if isinstance(obj, str):\n            if obj.startswith(""__int__""):\n                return obj[8:-1]\n            elif obj.startswith(""__float__""):\n                return obj[10:-1]\n            elif obj.startswith(""__tuple__""):\n                return obj[10:-1]\n            elif obj.startswith(""__type__""):\n                return obj[9:-1]\n            elif obj.startswith(""__function__""):\n                return obj[13:-1]\n            elif obj.startswith(""__module__""):\n                return obj[11:-1]\n        return obj\n\n\nclass Singleton:\n    """"""\n    A non-thread-safe helper class to ease implementing singletons.\n    This should be used as a decorator -- not a metaclass -- to the\n    class that should be a singleton.\n\n    The decorated class can define one `__init__` function that\n    takes only the `self` argument. Also, the decorated class cannot be\n    inherited from. Other than that, there are no restrictions that apply\n    to the decorated class.\n\n    To get the singleton instance, use the `Instance` method. Trying\n    to use `__call__` will result in a `TypeError` being raised.\n\n    """"""\n\n    _instance = None\n\n    def __init__(self, decorated):\n        self._decorated = decorated\n\n    def get_instance(self, **kwargs):\n        """"""\n        Returns the singleton instance. Upon its first call, it creates a\n        new instance of the decorated class and calls its `__init__` method.\n        On all subsequent calls, the already created instance is returned.\n\n        """"""\n        if not self._instance:\n            self._instance = self._decorated(**kwargs)\n            return self._instance\n        else:\n            return self._instance\n\n    def __call__(self):\n        raise TypeError(\'Singletons must be accessed through `get_instance()`.\')\n        # return self.get_instance()\n\n    def __instancecheck__(self, inst):\n        return isinstance(inst, self._decorated)\n\n\ndef get_image_as_buffered_file(image_array):\n    """"""\n    Returns a images as file pointer in a buffer\n\n    Args:\n        image_array: (C,W,H) To be returned as a file pointer\n\n    Returns:\n        Buffer file-pointer object containing the image file\n    """"""\n    buf = io.BytesIO()\n    imwrite(buf, image_array.transpose((1, 2, 0)), format=""png"")\n    buf.seek(0)\n\n    return buf\n\n\ndef figure_to_image(figures, close=True):\n    """"""Render matplotlib figure to numpy format.\n\n    Note that this requires the ``matplotlib`` package.\n    (https://tensorboardx.readthedocs.io/en/latest/_modules/tensorboardX/utils.html#figure_to_image)\n\n    Args:\n        figure (matplotlib.pyplot.figure) or list of figures: figure or a list of figures\n        close (bool): Flag to automatically close the figure\n\n    Returns:\n        numpy.array: image in [CHW] order\n    """"""\n    try:\n        import matplotlib.pyplot as plt\n        import matplotlib.backends.backend_agg as plt_backend_agg\n    except ModuleNotFoundError:\n        print(\'please install matplotlib\')\n\n    def render_to_rgb(figure):\n        canvas = plt_backend_agg.FigureCanvasAgg(figure)\n        canvas.draw()\n        data = np.frombuffer(canvas.buffer_rgba(), dtype=np.uint8)\n        w, h = figure.canvas.get_width_height()\n        image_hwc = data.reshape([h, w, 4])[:, :, 0:3]\n        image_chw = np.moveaxis(image_hwc, source=2, destination=0)\n        if close:\n            plt.close(figure)\n        return image_chw\n\n    if isinstance(figures, list):\n        images = [render_to_rgb(figure) for figure in figures]\n        return np.stack(images)\n    else:\n        image = render_to_rgb(figures)\n        return image\n\n\ndef savefig_and_close(figure, filename, close=True):\n    fig_img = figure_to_image(figure, close=close)\n    imwrite(filename, np.transpose(fig_img, (1, 2, 0)))\n\n\ndef random_string(length):\n    random.seed()\n    return """".join(random.choice(string.ascii_letters + string.digits) for _ in range(length))\n\n\ndef create_folder(path):\n    """"""\n    Creates a folder if not already exists\n    Args:\n        :param path: The folder to be created\n    Returns\n        :return: True if folder was newly created, false if folder already exists\n    """"""\n\n    if not os.path.exists(path):\n        os.makedirs(path)\n        return True\n    else:\n        return False\n\n\ndef name_and_iter_to_filename(name, n_iter, ending, iter_format=""{:05d}"", prefix=False):\n    iter_str = iter_format.format(n_iter)\n    if prefix:\n        name = iter_str + ""_"" + name + ending\n    else:\n        name = name + ""_"" + iter_str + ending\n\n    return name\n\n\nclass SafeDict(dict):\n    def __missing__(self, key):\n        return ""{"" + key + ""}""\n\n\nclass PyLock(object):\n    def __init__(self, name, timeout, check_interval=0.25):\n        self._timeout = timeout\n        self._check_interval = check_interval\n\n        lock_directory = gettempdir()\n        unique_token = sha256(name.encode()).hexdigest()\n        self._filepath = os.path.join(lock_directory, \'ilock-\' + unique_token + \'.lock\')\n\n    def __enter__(self):\n\n        current_time = call_time = time.time()\n        while call_time + self._timeout > current_time:\n            self._lockfile = open(self._filepath, \'w\')\n            try:\n                portalocker.lock(self._lockfile, portalocker.constants.LOCK_NB | portalocker.constants.LOCK_EX)\n                return self\n            except portalocker.exceptions.LockException:\n                pass\n\n            current_time = time.time()\n            check_interval = self._check_interval if self._timeout > self._check_interval else self._timeout\n            time.sleep(check_interval)\n\n        raise RuntimeError(\'Timeout was reached\')\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        portalocker.unlock(self._lockfile)\n        self._lockfile.close()\n\n\nclass LogDict(dict):\n    def __init__(self, file_name, base_dir=None, to_console=False, mode=""a""):\n        """"""Initializes a new Dict which can log to a given target file.""""""\n\n        super(LogDict, self).__init__()\n\n        self.file_name = file_name\n        if base_dir is not None:\n            self.file_name = os.path.join(base_dir, file_name)\n\n        self.logging_identifier = random_string(15)\n        self.logger = logging.getLogger(""logdict-"" + self.logging_identifier)\n        self.logger.setLevel(logging.INFO)\n        file_handler_formatter = logging.Formatter(\'\')\n\n        self.file_handler = logging.FileHandler(self.file_name, mode=mode)\n        self.file_handler.setFormatter(file_handler_formatter)\n        self.logger.addHandler(self.file_handler)\n        self.logger.propagate = to_console\n\n    def __setitem__(self, key, item):\n        super(LogDict, self).__setitem__(key, item)\n\n    def log_complete_content(self):\n        """"""Logs the current content of the dict to the output file as a whole.""""""\n        self.logger.info(str(self))\n\n\nclass ResultLogDict(LogDict):\n    def __init__(self, file_name, base_dir=None, running_mean_length=10, **kwargs):\n        """"""Initializes a new Dict which directly logs value changes to a given target_file.""""""\n        super(ResultLogDict, self).__init__(file_name=file_name, base_dir=base_dir, **kwargs)\n\n        self.is_init = False\n        self.running_mean_dict = defaultdict(lambda: deque(maxlen=running_mean_length))\n\n        self.__cntr_dict = defaultdict(float)\n\n        if self.file_handler.mode == ""w"" or os.stat(self.file_handler.baseFilename).st_size == 0:\n            self.print_to_file(""["")\n\n        self.is_init = True\n\n    def __setitem__(self, key, item):\n\n        if key == ""__cntr_dict"":\n            raise ValueError(""In ResultLogDict you can not add an item with key \'__cntr_dict\'"")\n\n        data = item\n        if isinstance(item, dict) and ""data"" in item and ""label"" in item and ""epoch"" in item:\n\n            data = item[""data""]\n            if ""counter"" in item and item[""counter""] is not None:\n                self.__cntr_dict[key] = item[""counter""]\n            json_dict = {key: ResultElement(data=data, label=item[""label""], epoch=item[""epoch""],\n                                            counter=self.__cntr_dict[key])}\n        else:\n            json_dict = {key: ResultElement(data=data, counter=self.__cntr_dict[key])}\n        self.__cntr_dict[key] += 1\n        self.logger.info(json.dumps(json_dict) + "","")\n\n        self.running_mean_dict[key].append(data)\n\n        super(ResultLogDict, self).__setitem__(key, data)\n\n    def print_to_file(self, text):\n        self.logger.info(text)\n\n    def load(self, reload_dict):\n        for key, item in reload_dict.items():\n\n            if isinstance(item, dict) and ""data"" in item and ""label"" in item and ""epoch"" in item:\n                data = item[""data""]\n                if ""counter"" in item and item[""counter""] is not None:\n                    self.__cntr_dict[key] = item[""counter""]\n            else:\n                data = item\n            self.__cntr_dict[key] += 1\n\n            super(ResultLogDict, self).__setitem__(key, data)\n\n    def close(self):\n\n        self.file_handler.close()\n        # Remove trailing comma, unless we\'ve only written ""["".\n        # This approach (fixed offset) sometimes fails upon errors and the like,\n        # we could alternatively read the whole file,\n        # parse to only keep ""clean"" rows and rewrite.\n        with open(self.file_handler.baseFilename, ""rb+"") as handle:\n            if os.stat(self.file_handler.baseFilename).st_size > 2:\n                handle.seek(-2, os.SEEK_END)\n                handle.truncate()\n        with open(self.file_handler.baseFilename, ""a"") as handle:\n            handle.write(""\\n]"")\n\n\nclass ResultElement(dict):\n    def __init__(self, data=None, label=None, epoch=None, counter=None):\n        super(ResultElement, self).__init__()\n\n        if data is not None:\n            if isinstance(data, np.floating):\n                data = float(data)\n            if isinstance(data, np.integer):\n                data = int(data)\n            self[""data""] = data\n        if label is not None:\n            self[""label""] = label\n        if epoch is not None:\n            self[""epoch""] = epoch\n        if counter is not None:\n            self[""counter""] = counter\n\n\ndef chw_to_hwc(np_array):\n    if len(np_array.shape) != 3:\n        return np_array\n    elif np_array.shape[0] != 1 and np_array.shape[0] != 3:\n        return np_array\n    elif np_array.shape[2] == 1 or np_array.shape[2] == 3:\n        return np_array\n    else:\n        np_array = np.transpose(np_array, (1, 2, 0))\n        return np_array\n\n\ndef np_make_grid(np_array, nrow=8, padding=2,\n                 normalize=False, range=None, scale_each=False, pad_value=0, to_int=False, standardize=False):\n    """"""Make a grid of images.\n\n    Args:\n        np_array (numpy array): 4D mini-batch Tensor of shape (B x C x H x W)\n            or a list of images all of the same size.\n        nrow (int, optional): Number of images displayed in each row of the grid.\n            The Final grid size is (B / nrow, nrow). Default is 8.\n        padding (int, optional): amount of padding. Default is 2.\n        normalize (bool, optional): If True, shift the image to the range (0, 1),\n            by subtracting the minimum and dividing by the maximum pixel value.\n        range (tuple, optional): tuple (min, max) where min and max are numbers,\n            then these numbers are used to normalize the image. By default, min and max\n            are computed from the tensor.\n        scale_each (bool, optional): If True, scale each image in the batch of\n            images separately rather than the (min, max) over all images.\n        pad_value (float, optional): Value for the padded pixels.\n        to_int (bool): Transforms the np array to a unit8 array with min 0 and max 255\n\n    Example:\n        See this notebook `here <https://gist.github.com/anonymous/bf16430f7750c023141c562f3e9f2a91>`_\n\n    """"""\n    if not (isinstance(np_array, np.ndarray) or\n            (isinstance(np_array, list) and all(isinstance(a, np.ndarray) for a in np_array))):\n        raise TypeError(\'Numpy array or list of tensors expected, got {}\'.format(type(np_array)))\n\n    # if list of tensors, convert to a 4D mini-batch Tensor\n    if isinstance(np_array, list):\n        np_array = np.stack(np_array, axis=0)\n\n    if len(np_array.shape) == 2:  # single image H x W\n        np_array = np_array.reshape((1, np_array.shape[0], np_array.shape[1]))\n    if len(np_array.shape) == 3:  # single image\n        if np_array.shape[0] == 1:  # if single-channel, convert to 3-channel\n            np_array = np.concatenate((np_array, np_array, np_array), 0)\n        np_array = np_array.reshape((1, np_array.shape[0], np_array.shape[1], np_array.shape[2]))\n\n    if len(np_array.shape) == 3 == 4 and np_array.shape[1] == 1:  # single-channel images\n        np_array = np.concatenate((np_array, np_array, np_array), 1)\n\n    if standardize is True:\n        np_array = np.copy(np_array)  # avoid modifying tensor in-place\n\n        def standardize_array_(img):\n            img = (img - np.mean(img)) / (np.std(img) + 1e-5)\n            return img\n\n        if scale_each is True:\n            for i in np.arange(np_array.shape[0]):  # loop over mini-batch dimension\n                np_array[i] = standardize_array_(np_array[i])\n        else:\n            np_array = standardize_array_(np_array)\n\n    if normalize is True:\n        np_array = np.copy(np_array)  # avoid modifying tensor in-place\n        if range is not None:\n            assert isinstance(range, tuple), \\\n                ""range has to be a tuple (min, max) if specified. min and max are numbers""\n\n        def norm_ip(img, min_, max_):\n            img = np.clip(img, a_min=min_, a_max=max_)\n            img = (img - min_) / (max_ - min_ + 1e-5)\n            return img\n\n        def norm_range(t, range_=None):\n            if range_ is not None:\n                t = norm_ip(t, range_[0], range_[1])\n            else:\n                t = norm_ip(t, np.min(t), np.max(t))\n            return t\n\n        if scale_each is True:\n            for i in np.arange(np_array.shape[0]):  # loop over mini-batch dimension\n                np_array[i] = norm_range(np_array[i], range)\n        else:\n            np_array = norm_range(np_array, range)\n\n\n\n\n\n    if np_array.shape[0] == 1:\n        return np_array.squeeze(0)\n\n    # make the mini-batch of images into a grid\n    nmaps = np_array.shape[0]\n    xmaps = min(nrow, nmaps)\n    ymaps = int(math.ceil(float(nmaps) / xmaps))\n    height, width = int(np_array.shape[2] + padding), int(np_array.shape[3] + padding)\n    grid = np.zeros((3, height * ymaps + padding, width * xmaps + padding))\n    grid += pad_value\n    k = 0\n    for y in np.arange(ymaps):\n        for x in np.arange(xmaps):\n            if k >= nmaps:\n                break\n            grid[:,\n            y * height + padding: y * height + padding + height - padding,\n            x * width + padding: x * width + padding + width - padding] = np_array[k]\n            k = k + 1\n\n    if to_int:\n        grid = np.clip(grid * 255, a_min=0, a_max=255)\n        grid = grid.astype(np.uint8)\n\n    return grid\n\n\ndef get_tensor_embedding(tensor, method=""tsne"", n_dims=2, n_neigh=30, **meth_args):\n    """"""\n    Return a embedding of a tensor (in a lower dimensional space, e.g. t-SNE)\n\n    Args:\n       tensor: Tensor to be embedded\n       method: Method used for embedding, options are: tsne, standard, ltsa, hessian, modified, isomap, mds,\n       spectral, umap\n       n_dims: dimensions to embed the data into\n       n_neigh: Neighbour parameter to kind of determin the embedding (see t-SNE for more information)\n       **meth_args: Further arguments which can be passed to the embedding method\n\n    Returns:\n        The embedded tensor\n\n    """"""\n    from sklearn import manifold\n    import umap\n\n    linears = [\'standard\', \'ltsa\', \'hessian\', \'modified\']\n    if method in linears:\n\n        loclin = manifold.LocallyLinearEmbedding(n_neigh, n_dims, method=method, **meth_args)\n        emb_data = loclin.fit_transform(tensor)\n\n    elif method == ""isomap"":\n        iso = manifold.Isomap(n_neigh, n_dims, **meth_args)\n        emb_data = iso.fit_transform(tensor)\n\n    elif method == ""mds"":\n        mds = manifold.MDS(n_dims, **meth_args)\n        emb_data = mds.fit_transform(tensor)\n\n    elif method == ""spectral"":\n        se = manifold.SpectralEmbedding(n_components=n_dims, n_neighbors=n_neigh, **meth_args)\n        emb_data = se.fit_transform(tensor)\n\n    elif method == ""tsne"":\n        tsne = manifold.TSNE(n_components=n_dims, perplexity=n_neigh, **meth_args)\n        emb_data = tsne.fit_transform(tensor)\n\n    elif method == ""umap"":\n        um = umap.UMAP(n_components=n_dims, n_neighbors=n_neigh, **meth_args)\n        emb_data = um.fit_transform(tensor)\n\n    else:\n        emb_data = tensor\n\n    return emb_data\n\n\ndef is_picklable(obj):\n    try:\n        pickle.dumps(obj)\n\n    except pickle.PicklingError:\n        return False\n    return True\n'"
trixi/logger/experiment/__init__.py,0,"b'from trixi.logger.experiment.experimentlogger import ExperimentLogger\ntry:\n    from trixi.logger.experiment.pytorchexperimentlogger import PytorchExperimentLogger\nexcept ImportError as e:\n    import warnings\n    warnings.warn(ImportWarning(""Could not import Pytorch related modules:\\n%s""\n        % e.msg))\n'"
trixi/logger/experiment/experimentlogger.py,0,"b'from __future__ import print_function\n\nimport datetime\nimport json\nimport os\nimport re\nimport shutil\nimport warnings\n\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\n\nimport numpy as np\n\nfrom trixi.logger.abstractlogger import AbstractLogger\nfrom trixi.logger.file.textfilelogger import TextFileLogger\nfrom trixi.logger.file.numpyplotfilelogger import NumpyPlotFileLogger\nfrom trixi.util import create_folder, MultiTypeEncoder, MultiTypeDecoder, Config\n\n\nREPLACEMENTS = [(""%Y"", 4), (""%m"", 2), (""%d"", 2), (""%H"", 2), (""%M"", 2), (""%S"", 2),\n                (""%w"", 1), (""%y"", 2), (""%I"", 2), (""%f"", 6), (""%j"", 3), (""%U"", 2),\n                (""%W"", 2)]\n\n\nclass ExperimentLogger(AbstractLogger):\n    """"""A single class for logging your experiments to file.\n\n    It creates a experiment folder in your base folder and a folder structure to store your experiment files.\n    The folder structure is::\n\n        base_dir/\n            new_experiment_folder/\n                checkpoint/\n                config/\n                img/\n                log/\n                plot/\n                result/\n                save/\n\n\n    """"""\n\n    def __init__(self,\n                 exp_name,\n                 base_dir,\n                 folder_format=""%Y%m%d-%H%M%S_{experiment_name}"",\n                 resume=False,\n                 text_logger_args=None,\n                 plot_logger_args=None,\n                 **kwargs):\n        """"""\n        Initializes the Experiment logger and creates the experiment folder structure\n\n        Args:\n            exp_name (str): The name of the experiment\n            base_dir (str): The base directory in which the experiment folder will be created\n            folder_format (str): The format for the naming of the experiment folder\n            resume (bool): if True use the given folder and do not create new ones\n            text_logger_args: Parameters for the TextFileLogger initialization\n            plot_logger_args: Parameters for the NumpyPlotFileLogger initialization\n        """"""\n\n        super(ExperimentLogger, self).__init__(**kwargs)\n\n        self.experiment_name = exp_name\n        self.base_dir = base_dir\n        self.folder_format = folder_format\n\n        self.init_time = datetime.datetime.today()\n\n        # try to make folder until successful or until counter runs out\n        # this is necessary when multiple ExperimentLoggers start at the same time\n        makedir_success = False\n        makedir_counter = 100\n        makedir_exception = None\n        while (not makedir_success and makedir_counter > 0):\n            try:\n                self.folder_name = self.resolve_format(folder_format, resume)\n                self.work_dir = os.path.join(base_dir, self.folder_name)\n                if not resume:\n                    create_folder(self.work_dir)\n                makedir_success = True\n            except FileExistsError as file_error:\n                makedir_counter -= 1\n            except Exception as e:\n                makedir_exception = e\n                makedir_counter -= 1\n        if makedir_exception is not None:\n            warnings.warn(""Last exception encountered in makedir process:\\n"" +\n                                 ""{}\\n"".format(makedir_exception) +\n                                 ""There may or may not be a folder for the experiment to run in."", RuntimeWarning)\n\n        self.config_dir = os.path.join(self.work_dir, ""config"")\n        self.log_dir = os.path.join(self.work_dir, ""log"")\n        self.checkpoint_dir = os.path.join(self.work_dir, ""checkpoint"")\n        self.img_dir = os.path.join(self.work_dir, ""img"")\n        self.plot_dir = os.path.join(self.work_dir, ""plot"")\n        self.save_dir = os.path.join(self.work_dir, ""save"")\n        self.result_dir = os.path.join(self.work_dir, ""result"")\n\n        if not resume:\n            create_folder(self.config_dir)\n            create_folder(self.log_dir)\n            create_folder(self.checkpoint_dir)\n            create_folder(self.img_dir)\n            create_folder(self.plot_dir)\n            create_folder(self.save_dir)\n            create_folder(self.result_dir)\n\n        if text_logger_args is None:\n            text_logger_args = {}\n        if plot_logger_args is None:\n            plot_logger_args = {}\n\n        self.text_logger = TextFileLogger(self.log_dir, **text_logger_args)\n        self.plot_logger = NumpyPlotFileLogger(\n            self.img_dir, self.plot_dir, **plot_logger_args)\n\n    def show_image(self, image, name, file_format="".png"", **kwargs):\n        """"""\n        This function saves an image in the experiment img folder.\n\n        Args:\n            image(np.ndarray): image to be shown\n            name(str): image title\n            file_format (str): file format of the image\n        """"""\n        self.plot_logger.show_image(image, name, file_format="".png"", **kwargs)\n\n    def show_barplot(self, array, name, file_format="".png"", **kwargs):\n        """"""\n        This function saves a barplot in the experiment plot folder.\n\n        Args:\n            array(np.ndarray): array to be plotted\n            name(str): image title\n            file_format (str): file format of the image\n        """"""\n        self.plot_logger.show_barplot(\n            array, name, file_format="".png"", **kwargs)\n\n    def show_lineplot(self, y_vals, x_vals=None, name=""lineplot"", file_format="".png"", **kwargs):\n        """"""\n        This function saves a line plot in the experiment plot folder.\n\n        Args:\n            x_vals: x values of the line\n            y_vals: y values of the line\n            name(str): image title\n            file_format (str): file format of the image\n        """"""\n        self.plot_logger.show_lineplot(\n            y_vals, x_vals, name, file_format="".png"", **kwargs)\n\n    def show_piechart(self, array, name, file_format="".png"", **kwargs):\n        """"""\n        This function saves a piechart in the experiment plot folder.\n\n        Args:\n            array(np.ndarray): array to be plotted\n            name(str): image title\n            file_format (str): file format of the image\n        """"""\n        self.plot_logger.show_piechart(\n            array, name, file_format="".png"", **kwargs)\n\n    def show_scatterplot(self, array, name, file_format="".png"", **kwargs):\n        """"""\n        This function saves a scatterplot in the experiment plot folder.\n\n        Args:\n            array(np.ndarray): array to be plotted\n            name(str): image title\n            file_format (str): file format of the image\n        """"""\n        self.plot_logger.show_scatterplot(\n            array, name, file_format="".png"", **kwargs)\n\n    def show_value(self, value, name=None, counter=None, tag=None, file_format="".png"", **kwargs):\n        """"""\n        This function saves a value as a consequtive line plot.\n\n        Args:\n            value(np.ndarray): value to be plotted\n            name(str): image title\n            counter: y-value of the image (if not suppled simply increases for each call)\n            tag: group/label for the value. Values with the same tag will be plotted in the same plot\n            file_format (str): file format of the image\n        """"""\n        self.plot_logger.show_value(value, name, counter, tag, file_format, **kwargs)\n\n    def show_text(self, text, name=None, logger=""default"", **kwargs):\n        """"""\n        Logs a text to a log file.\n\n        Args:\n            text: The text to be logged\n            name: Name of the text\n            logger: log file (in the experiment log folder) in which the text will be logged.\n            **kwargs:\n\n        """"""\n        self.text_logger.show_text(text, name, logger, **kwargs)\n\n    def show_boxplot(self, array, name, file_format="".png"", **kwargs):\n        """"""\n        This function saves a boxplot in the experiment plot folder.\n\n        Args:\n            array(np.ndarray): array to be plotted\n            name(str): image title\n            file_format (str): file format of the image\n        """"""\n        self.plot_logger.show_boxplot(\n            array, name, file_format="".png"", **kwargs)\n\n    def show_matplot_plt(self, figure, name, file_format="".png"", *args, **kwargs):\n        """"""\n        This function saves a custom matplotlib figure in the experiment plot folder.\n\n        Args:\n            figure(matplotlib.figure.Figure): figure to be plotted\n            name(str): image title\n            file_format (str): file format of the image\n        """"""\n        self.plot_logger.show_matplot_plt(figure, name, file_format="".png"", *args, **kwargs)\n\n    def save_model(self):\n        raise NotImplementedError\n\n    def load_model(self):\n        raise NotImplementedError\n\n    def save_config(self, data, name, **kwargs):\n        """"""\n        Saves a config as a json file in the experiment config dir\n\n        Args:\n            data: The data to be stored as config json\n            name: The name of the json file in which the data will be stored\n\n        """"""\n\n        if not name.endswith("".json""):\n            name += "".json""\n        data.dump(os.path.join(self.config_dir, name), **kwargs)\n\n    def load_config(self, name, **kwargs):\n        """"""\n        Loads a config from a json file from the experiment config dir\n\n        Args:\n            name: the name of the config file\n\n        Returns: A Config/ dict filled with the json file content\n\n        """"""\n\n        if not name.endswith("".json""):\n            name += "".json""\n        c = Config()\n        c.load(os.path.join(self.config_dir, name), **kwargs)\n        return c\n\n    def save_checkpoint(self):\n        raise NotImplementedError\n\n    def load_checkpoint(self):\n        raise NotImplementedError\n\n    def save_result(self, data, name, indent=4, separators=("","", "": ""), encoder_cls=MultiTypeEncoder, **kwargs):\n        """"""\n        Saves data as a json file in the experiment result dir\n\n        Args:\n            data: The data to be stored as result json\n            name: name of the result json file\n            indent: Indent for the json file\n            separators: Separators for the json file\n            encoder_cls: Encoder Class for the encoding to json\n\n        """"""\n\n        if not name.endswith("".json""):\n            name += "".json""\n        name = os.path.join(self.result_dir, name)\n        create_folder(os.path.dirname(name))\n        with open(name, ""w"") as jf:\n            json.dump(data, jf,\n                      cls=encoder_cls,\n                      indent=indent,\n                      separators=separators,\n                      **kwargs)\n\n    def save_dict(self, data, path, indent=4, separators=("","", "": ""), encoder_cls=MultiTypeEncoder, **kwargs):\n        """"""\n        Saves a dict as a json file in the experiment save dir\n\n        Args:\n            data: The data to be stored as save file\n            path: sub path in the save folder (or simply filename)\n            indent: Indent for the json file\n            separators: Separators for the json file\n            encoder_cls: Encoder Class for the encoding to json\n        """"""\n\n        if not path.endswith("".json""):\n            path += "".json""\n        path = os.path.join(self.save_dir, path)\n        create_folder(os.path.dirname(path))\n        with open(path, ""w"") as jf:\n            json.dump(data, jf,\n                      cls=encoder_cls,\n                      indent=indent,\n                      separators=separators,\n                      **kwargs)\n\n    def load_dict(self, path):\n        """"""\n        Loads a json file as dict from a sub path in the experiment save dir\n\n        Args:\n            path: sub path to the file (starting from the experiment save dir)\n\n        Returns: The restored data as a dict\n        """"""\n\n        if not path.endswith("".json""):\n            path += "".json""\n        path = os.path.join(self.save_dir, path)\n        ret_val = dict()\n        with open(path, ""r"") as df:\n            ret_val = json.load(df, cls=MultiTypeDecoder)\n        return ret_val\n\n    def save_numpy_data(self, data, path):\n        """"""\n            Saves a numpy array in the experiment save dir\n\n            Args:\n                data: The array to be stored as a save file\n                path: sub path in the save folder (or simply filename)\n        """"""\n\n        if not path.endswith("".npy""):\n            path += "".npy""\n        path = os.path.join(self.save_dir, path)\n        create_folder(os.path.dirname(path))\n        np.save(path, data)\n\n    def load_numpy_data(self, path):\n        """"""\n        Loads a numpy file from a sub path in the experiment save dir\n\n        Args:\n            path: sub path to the file (starting from the experiment save dir)\n\n        Returns: The restored numpy array\n        """"""\n\n        if not path.endswith("".npy""):\n            path += "".npy""\n        path = os.path.join(self.save_dir, path)\n        return np.load(path)\n\n    def save_pickle(self, data, path):\n        """"""\n            Saves a object data in the experiment save dir via pickle\n\n            Args:\n                data: The data to be stored as a save file\n                path: sub path in the save folder (or simply filename)\n        """"""\n\n        path = os.path.join(self.save_dir, path)\n        create_folder(os.path.dirname(path))\n        with open(path, ""wb"") as out:\n            pickle.dump(data, out)\n\n    def load_pickle(self, path):\n        """"""\n        Loads a object via pickle from a sub path in the experiment save dir\n\n        Args:\n            path: sub path to the file (starting from the experiment save dir)\n\n        Returns: The restored object\n        """"""\n\n        path = os.path.join(self.save_dir, path)\n        with open(path, ""rb"") as in_:\n            return pickle.load(in_)\n\n    def save_file(self, filepath, path=None):\n        """"""\n        Copies a file to the experiment save dir\n\n        Args:\n            filepath: Path to the file to be copied to the experiment save dir\n            path: sub path to the target file (starting from the experiment save dir, does not have to exist yet)\n\n        """"""\n\n        if path is None:\n            target_dir = self.save_dir\n        else:\n            target_dir = os.path.join(self.save_dir, path)\n        filename = os.path.basename(filepath)\n        shutil.copy(filepath, os.path.join(target_dir, filename))\n\n    def resolve_format(self, input_, resume):\n        """"""\n        Given some input pattern, tries to find the best matching folder name by resolving the format. Options are:\n         - Run-number: {run_number}\n         - Time: ""%Y%m%d-%H%M%S\n         - Member variables (e.g experiment_name) : {variable_name} (e.g. {experiment_name})\n\n\n        Args:\n            input_: The format to be resolved\n            resume: Flag if folder should be resumed\n\n        Returns: The resolved folder name\n\n        """"""\n\n        if resume:\n\n            pattern = input_[:]\n\n            for find in re.findall(""{[\\w\\:]+}"", pattern):\n                run_match = re.search(""(?<=\\{run_number\\:0)\\d+(?=d\\})"", find)\n                if find == ""{run_number}"":\n                    pattern = pattern.replace(""{run_number}"", ""\\d+"")\n                elif run_match:\n                    length = int(run_match.group(0))\n                    pattern = re.sub(""\\{run_number\\:\\d+d\\}"",\n                                     ""\\\\d{"" + str(length) + ""}"", pattern)\n                else:\n                    if find[1:-1] in self.__dict__:\n                        pattern = pattern.replace(\n                            find, self.__dict__[find[1:-1]])\n\n            for r in REPLACEMENTS:\n                pattern = pattern.replace(r[0], ""\\\\d{"" + str(r[1]) + ""}"")\n\n            return list(filter(lambda x: re.match(pattern, x),\n                               sorted(os.listdir(self.base_dir))))[-1]\n\n        if ""%"" in input_:\n            input_ = self.init_time.strftime(input_)\n\n        if ""{"" not in input_:\n            return input_\n\n        run_number = 1\n        while os.path.exists(os.path.join(self.base_dir,\n                                          input_.format(run_number=run_number,\n                                                        **self.__dict__))):\n            # if the folder already exists, for example if two jobs are launched\n            # at the same time, append run_number\n            if ""{run_number}"" not in input_:\n                input_ = input_ + ""_{run_number}""\n\n            run_number += 1\n\n        return input_.format(run_number=run_number, **self.__dict__)\n'"
trixi/logger/experiment/pytorchexperimentlogger.py,7,"b'from __future__ import print_function\n\nimport atexit\nimport fnmatch\nimport os\nimport warnings\nfrom multiprocessing import Process\n# import cv2\nfrom PIL import Image\n\nimport torch\nimport numpy as np\n\nfrom trixi.logger.abstractlogger import threaded\nfrom trixi.logger.experiment import ExperimentLogger\nfrom trixi.logger.file.pytorchplotfilelogger import PytorchPlotFileLogger\nfrom trixi.util import name_and_iter_to_filename\nfrom trixi.util.metrics import get_classification_metrics, get_pr_curve, get_roc_curve\nfrom trixi.util.pytorchutils import update_model, get_vanilla_image_gradient, get_guided_image_gradient, \\\n    get_smooth_image_gradient, get_input_gradient\n\n\nclass PytorchExperimentLogger(ExperimentLogger):\n    """"""\n    A single class for logging your pytorch experiments to file.\n    Extends the ExperimentLogger also also creates a experiment folder with a file structure:\n\n    The folder structure is :\n        base_dir/\n            new_experiment_folder/\n                checkpoint/\n                config/\n                img/\n                log/\n                plot/\n                result/\n                save/\n\n\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        """"""Initializes the PytorchExperimentLogger and parses the arguments to the ExperimentLogger""""""\n\n        super(PytorchExperimentLogger, self).__init__(*args, **kwargs)\n        self.plot_logger = PytorchPlotFileLogger(self.img_dir, self.plot_dir)\n\n    def show_images(self, images, name, **kwargs):\n        """"""\n        Saves images in the img folder\n\n        Args:\n            images: The images to be saved\n            name: file name of the new image file\n\n        """"""\n        self.plot_logger.show_images(images, name, **kwargs)\n\n    def show_image_grid(self, image, name, **kwargs):\n        """"""\n        Saves images in the img folder as a image grid\n\n        Args:\n            images: The images to be saved\n            name: file name of the new image file\n\n        """"""\n        self.plot_logger.show_image_grid(image, name, **kwargs)\n\n    def show_image_grid_heatmap(self, heatmap, background=None, name=""heatmap"", **kwargs):\n        """"""\n        Saves images in the img folder as a image grid\n\n        Args:\n            heatmap: The images to be converted to a heatmap\n            background: Context of the heatmap (to be underlayed)\n            name: file name of the new image file\n\n        """"""\n        self.plot_logger.show_image_grid_heatmap(heatmap=heatmap, background=background, name=name, **kwargs)\n\n    def show_video(self, frame_list=None, name=""video"", dim=""LxHxWxC"", scale=1.0, fps=25,\n                   extension="".mp4"", codec=""THEO""):\n        """"""\n        Saves video in the img folder. Should be a list of arrays with dimension HxWxC.\n\n        Args:\n            frame_list: The list of image tensors/arrays to be saved as a video\n            name: Filename of the video\n            dim: Dimension of the tensor - should be either LxHxWxC or LxCxHxW\n            fps: FPS of the video\n            extension: File extension - should be mp4, ogc, avi or webm\n        """"""\n        # TODO: trixi browser currently can\'t show videos, so using GIF instead - work in progress\n        self.show_gif(frame_list, name=name, scale=scale, fps=fps)\n        """"""\n        tensor = np.array(frame_list)\n        assert tensor.ndim == 4, ""video should be a 4d tensor""\n        assert dim == ""LxHxWxC"" or  dim == ""LxCxHxW"", ""dimension argument should be LxHxWxC or LxCxHxW""\n        if dim == ""LxCxHxW"":\n            tensor = tensor.transpose([0, 2, 3, 1])\n        filename = os.path.join(self.img_dir, name + extension)\n        fourcc = cv2.VideoWriter_fourcc(*codec)\n        writer = cv2.VideoWriter(filename, fourcc, fps, (tensor.shape[2], tensor.shape[1]))\n        assert writer.isOpened(), ""video writer could not be opened""\n        for i in range(tensor.shape[0]):\n            writer.write(tensor[i, :, :, :])\n        writer.release()\n        writer = None\n        """"""\n\n    def show_gif(self, frame_list=None, name=""frames"", scale=1.0, fps=25):\n        """"""\n        Saves gif in the img folder. Should be a list of arrays with dimension HxWxC.\n\n        Args:\n            frame_list: The list of image tensors/arrays to be saved as a gif\n            name: Filename of the gif\n            scale: Scaling factor of the individual frames\n            fps: FPS of the gif\n        """"""\n        w, h = Image.fromarray(np.uint8(frame_list[0])).size\n        image_list = []\n        for i in range(len(frame_list)):\n            image_list.append(Image.fromarray(np.uint8(frame_list[i])).resize((w * int(scale), h * int(scale))))\n        filename = os.path.join(self.img_dir, name + "".gif"")\n        image_list[0].save(filename, save_all=True, append_images=image_list[1:], duration=int(1e3 / fps), loop=0)\n\n    @staticmethod\n    @threaded\n    def save_model_static(model, model_dir, name):\n        """"""\n        Saves a pytorch model in a given directory (using pytorch)\n\n        Args:\n            model: The model to be stored\n            model_dir: The directory in which the model file should be written\n            name: The file name of the model file\n\n        """"""\n\n        model_file = os.path.join(model_dir, name)\n        torch.save(model.state_dict(), model_file)\n\n    def save_model(self, model, name, n_iter=None, iter_format=""{:05d}"", prefix=False):\n        """"""\n        Saves a pytorch model in the model directory of the experiment folder\n\n        Args:\n            model: The model to be stored\n            name: The file name of the model file\n            n_iter: The iteration number, formatted with the iter_format and added to the model name (if not None)\n            iter_format: The format string, which indicates how n_iter will be formated as a string\n            prefix: If True, the formated n_iter will be appended as a prefix, otherwise as a suffix\n\n        """"""\n\n        if n_iter is not None:\n            name = name_and_iter_to_filename(name,\n                                             n_iter,\n                                             "".pth"",\n                                             iter_format=iter_format,\n                                             prefix=prefix)\n\n        if not name.endswith("".pth""):\n            name += "".pth""\n\n        self.save_model_static(model=model,\n                               model_dir=self.checkpoint_dir,\n                               name=name)\n\n    @staticmethod\n    @threaded\n    def load_model_static(model, model_file, exclude_layers=(), warnings=True):\n        """"""\n        Loads a pytorch model from a given directory (using pytorch)\n\n\n        Args:\n            model: The model to be loaded (whose parameters should be restored)\n            model_file: The file from which the model parameters should be loaded\n            exclude_layers: List of layer names which should be excluded from restoring\n            warnings (bool): Flag which indicates if method should warn if not everything went perfectly\n\n        """"""\n\n        if os.path.exists(model_file):\n\n            pretrained_dict = torch.load(model_file, map_location=lambda storage, loc: storage)\n            update_model(model, pretrained_dict, exclude_layers, warnings)\n            return model\n\n        else:\n\n            raise IOError(""Model file does not exist!"")\n\n    def load_model(self, model, name, exclude_layers=(), warnings=True):\n        """"""\n        Loads a pytorch model from the model directory of the experiment folder\n\n\n        Args:\n            model: The model to be loaded (whose parameters should be restored)\n            name: The file name of the model file\n            exclude_layers: List of layer names which should be excluded from restoring\n            warnings: Flag which indicates if method should warn if not everything went perfectlys\n\n\n        """"""\n\n        if not name.endswith("".pth""):\n            name += "".pth""\n\n        self.load_model_static(model=model,\n                               model_file=os.path.join(self.checkpoint_dir, name),\n                               exclude_layers=exclude_layers,\n                               warnings=warnings)\n\n    @staticmethod\n    @threaded\n    def save_checkpoint_static(checkpoint_dir, name, move_to_cpu=False, **kwargs):\n        """"""\n        Saves a checkpoint/dict in a given directory (using pytorch)\n\n        Args:\n            checkpoint_dir: The directory in which the checkpoint file should be written\n            name: The file name of the checkpoint file\n            move_to_cpu (bool): Flag, if all pytorch tensors should be moved to cpu before storing\n            **kwargs: dict which is actually saved\n\n        """"""\n        for key, value in kwargs.items():\n            if isinstance(value, torch.nn.Module) or isinstance(value, torch.optim.Optimizer):\n                kwargs[key] = value.state_dict()\n\n        checkpoint_file = os.path.join(checkpoint_dir, name)\n\n        def to_cpu(obj):\n            if hasattr(obj, ""cpu""):\n                return obj.cpu()\n            elif isinstance(obj, dict):\n                return {key: to_cpu(val) for key, val in obj.items()}\n            else:\n                return obj\n\n        if move_to_cpu:\n            torch.save(to_cpu(kwargs), checkpoint_file)\n        else:\n            torch.save(kwargs, checkpoint_file)\n\n    def save_checkpoint(self, name, n_iter=None, iter_format=""{:05d}"", prefix=False, **kwargs):\n        """"""\n        Saves a checkpoint in the checkpoint directory of the experiment folder\n\n        Args:\n            name: The file name of the checkpoint file\n            n_iter: The iteration number, formatted with the iter_format and added to the checkpoint name (if not None)\n            iter_format: The format string, which indicates how n_iter will be formated as a string\n            prefix: If True, the formated n_iter will be appended as a prefix, otherwise as a suffix\n            **kwargs:  dict which is actually saved (key=name, value=variable to be stored)\n\n        """"""\n\n        if n_iter is not None:\n            name = name_and_iter_to_filename(name,\n                                             n_iter,\n                                             "".pth.tar"",\n                                             iter_format=iter_format,\n                                             prefix=prefix)\n\n        if not name.endswith("".pth.tar""):\n            name += "".pth.tar""\n\n        self.save_checkpoint_static(self.checkpoint_dir, name=name, **kwargs)\n\n    @staticmethod\n    def load_checkpoint_static(checkpoint_file, exclude_layer_dict=None, warnings=True, **kwargs):\n        """"""\n        Loads a checkpoint/dict in a given directory (using pytorch)\n\n        Args:\n            checkpoint_file: The checkpoint from which the checkpoint/dict should be loaded\n            exclude_layer_dict: A dict with key \'model_name\' and a list of all layers of \'model_name\' which should\n            not be restored\n            warnings: Flag which indicates if method should warn if not everything went perfectlys\n            **kwargs: dict which is actually loaded (key=name (used to save the checkpoint) , value=variable to be\n            loaded/ overwritten)\n\n        Returns: The kwargs dict with the loaded/ overwritten values\n\n        """"""\n\n        if exclude_layer_dict is None:\n            exclude_layer_dict = {}\n\n        checkpoint = torch.load(checkpoint_file, map_location=lambda storage, loc: storage)\n\n        for key, value in kwargs.items():\n            if key in checkpoint:\n                if isinstance(value, torch.nn.Module) or isinstance(value, torch.optim.Optimizer):\n                    exclude_layers = exclude_layer_dict.get(key, [])\n                    update_model(value, checkpoint[key], exclude_layers, warnings)\n                else:\n                    kwargs[key] = checkpoint[key]\n\n        return kwargs\n\n    def load_checkpoint(self, name, exclude_layer_dict=None, warnings=True, **kwargs):\n        """"""\n        Loads a checkpoint from the checkpoint directory of the experiment folder\n\n        Args:\n            name: The name of the checkpoint file\n            exclude_layer_dict: A dict with key \'model_name\' and a list of all layers of \'model_name\' which should\n            not be restored\n            warnings: Flag which indicates if method should warn if not everything went perfectlys\n            **kwargs: dict which is actually loaded (key=name (used to save the checkpoint) , value=variable to be\n            loaded/ overwritten)\n\n        Returns: The kwargs dict with the loaded/ overwritten values\n\n        """"""\n\n        if not name.endswith("".pth.tar""):\n            name += "".pth.tar""\n\n        checkpoint_file = os.path.join(self.checkpoint_dir, name)\n        return self.load_checkpoint_static(checkpoint_file=checkpoint_file,\n                                           exclude_layer_dict=exclude_layer_dict,\n                                           warnings=warnings,\n                                           **kwargs)\n\n    def save_at_exit(self, name=""checkpoint_end"", **kwargs):\n        """"""\n        Saves a dict as checkpoint if the program exits (not garanteed to work 100%)\n\n        Args:\n            name: Name of the checkpoint file\n            **kwargs: dict which is actually saved (key=name, value=variable to be stored)\n\n        """"""\n\n        if not name.endswith("".pth.tar""):\n            name += "".pth.tar""\n\n        def save_fnc():\n            self.save_checkpoint(name, **kwargs)\n            print(""Checkpoint saved securely... =)"")\n\n        atexit.register(save_fnc)\n\n    def get_save_checkpoint_fn(self, name=""checkpoint"", **kwargs):\n        """"""\n        A function which returns a function which takes n_iter as arguments and saves the current values of the\n        variables given as kwargs as a checkpoint file.\n\n\n        Args:\n            name: Base-name of the checkpoint file\n            **kwargs:  dict which is actually saved, when the returned function is called\n\n        Returns: Function which takes n_iter as arguments and saves a checkpoint file\n        """"""\n\n        def save_fnc(n_iter, iter_format=""{:05d}"", prefix=False):\n            self.save_checkpoint(name=name,\n                                 n_iter=n_iter,\n                                 iter_format=iter_format,\n                                 prefix=prefix,\n                                 **kwargs)\n\n        return save_fnc\n\n    @staticmethod\n    def load_last_checkpoint_static(dir_, name=None, **kwargs):\n        """"""\n        Loads the (alphabetically) last checkpoint file in a given directory\n\n        Args:\n            dir_: The directory to look for the (alphabetically) last checkpoint\n            name: String pattern which indicates the files to look form\n            **kwargs: dict which is actually loaded (key=name (used to save the checkpoint) , value=variable to be\n            loaded/ overwritten)\n\n        Returns:  The kwargs dict with the loaded/ overwritten values\n\n        """"""\n\n        if name is None:\n            name = ""*checkpoint*.pth.tar""\n\n        checkpoint_files = []\n\n        for root, dirs, files in os.walk(dir_):\n            for filename in fnmatch.filter(files, name):\n                checkpoint_file = os.path.join(root, filename)\n                checkpoint_files.append(checkpoint_file)\n\n        if len(checkpoint_files) == 0:\n            return None\n\n        last_file = sorted(checkpoint_files, reverse=True)[0]\n\n        return PytorchExperimentLogger.load_checkpoint_static(last_file, **kwargs)\n\n    def load_last_checkpoint(self, **kwargs):\n        """"""\n                Loads the (alphabetically) last checkpoint file in the checkpoint directory in the experiment folder\n\n                Args:\n                    **kwargs: dict which is actually loaded (key=name (used to save the checkpoint) , value=variable to be\n                    loaded/ overwritten)\n\n                Returns:  The kwargs dict with the loaded/ overwritten values\n\n                """"""\n        return self.load_last_checkpoint_static(self.checkpoint_dir, **kwargs)\n\n    def print(self, *args):\n        """"""\n        Prints the given arguments using the text logger print function\n\n        Args:\n            *args: Things to be printed\n\n        """"""\n        self.text_logger.print(*args)\n\n    @staticmethod\n    def get_roc_curve(tensor, labels, reduce_to_n_samples=None, use_sub_process=False, results_fn=lambda\n            x, *y, **z: None):\n        """"""\n        Displays a roc curve given a tensor with scores and the coresponding labels\n\n        Args:\n            tensor: Tensor with scores (e.g class probability )\n            labels: Labels of the samples to which the scores match\n            reduce_to_n_samples: Reduce/ downsample to to n samples for fewer data points\n            use_sub_process: Use a sub process to do the processing, if true nothing is returned\n            results_fn: function which is called with the results/ return values. Expected f(tpr, fpr)\n\n        """"""\n        warnings.warn(""This method is deprecated !!! Please use the util.metrics method"")\n        return get_roc_curve(tensor, labels, reduce_to_n_samples, use_sub_process, results_fn)\n\n    @staticmethod\n    def get_pr_curve(tensor, labels, reduce_to_n_samples=None, use_sub_process=False,\n                     results_fn=lambda x, *y, **z: None):\n        """"""\n        Displays a precision recall curve given a tensor with scores and the coresponding labels\n\n        Args:\n            tensor: Tensor with scores (e.g class probability )\n            labels: Labels of the samples to which the scores match\n            reduce_to_n_samples: Reduce/ downsample to to n samples for fewer data points\n            use_sub_process: Use a sub process to do the processing, if true nothing is returned\n            results_fn: function which is called with the results/ return values. Expected f(precision, recall)\n\n        """"""\n        warnings.warn(""This method is deprecated !!! Please use the util.metrics method"")\n        return get_pr_curve(tensor, labels, reduce_to_n_samples, use_sub_process, results_fn)\n\n    @staticmethod\n    def get_classification_metrics(tensor, labels, name="""", metric=(""roc-auc"", ""pr-score""), use_sub_process=False,\n                                   tag_name=None, results_fn=lambda x, *y, **z: None):\n        """"""\n        Displays some classification metrics as line plots in a graph (similar to show value (also uses show value\n        for the caluclated values))\n\n        Args:\n            tensor: Tensor with scores (e.g class probability )\n            labels: Labels of the samples to which the scores match\n            name: The name of the window\n            metric: List of metrics to calculate. Options are: roc-auc, pr-auc, pr-score, mcc, f1\n            tag_name: Name for the tag, if no given use name\n            use_sub_process: Use a sub process to do the processing, if true nothing is returned\n            results_fn: function which is called with the results/ return values. Expected f(val, name, tag)\n\n        Returns:\n\n        """"""\n        warnings.warn(""This method is deprecated !!! Please use the util.metrics method"")\n        return get_classification_metrics(tensor, labels, name, metric, use_sub_process, tag_name, results_fn)\n\n    @staticmethod\n    def get_input_gradient(model, inpt, err_fn, grad_type=""vanilla"", n_runs=20, eps=0.1,\n                           abs=False, results_fn=lambda x, *y, **z: None):\n        """"""\n        Given a model creates calculates the error and backpropagates it to the image and saves it (saliency map).\n\n        Args:\n            model: The model to be evaluated\n            inpt: Input to the model\n            err_fn: The error function the evaluate the output of the model on\n            grad_type: Gradient calculation method, currently supports (vanilla, vanilla-smooth, guided,\n            guided-smooth) ( the guided backprob can lead to segfaults -.-)\n            n_runs: Number of runs for the smooth variants\n            eps: noise scaling to be applied on the input image (noise is drawn from N(0,1))\n            abs (bool): Flag, if the gradient should be a absolute value\n            results_fn: function which is called with the results/ return values. Expected f(grads)\n\n        """"""\n        warnings.warn(""This method is deprecated !!! Please use the util.pytorchutils method"")\n        return get_input_gradient(model, inpt, err_fn, grad_type, n_runs, eps, abs, results_fn)\n\n    def show_image_gradient(self, name, *args, **kwargs):\n        """"""\n        Given a model creates calculates the error and backpropagates it to the image and saves it.\n\n        Args:\n            name: Name of the file\n            model: The model to be evaluated\n            inpt: Input to the model\n            err_fn: The error function the evaluate the output of the model on\n            grad_type: Gradient calculation method, currently supports (vanilla, vanilla-smooth, guided,\n            guided-smooth) ( the guided backprob can lead to segfaults -.-)\n            n_runs: Number of runs for the smooth variants\n            eps: noise scaling to be applied on the input image (noise is drawn from N(0,1))\n            abs (bool): Flag, if the gradient should be a absolute value\n\n\n        """"""\n        grad = self.get_input_gradient(*args, **kwargs)\n        self.show_image_grid(grad, name)\n'"
trixi/logger/file/__init__.py,0,"b'from trixi.logger.file.numpyplotfilelogger import NumpyPlotFileLogger\nfrom trixi.logger.file.textfilelogger import TextFileLogger\n\ntry:\n    from trixi.logger.file.pytorchplotfilelogger import PytorchPlotFileLogger\nexcept ImportError as e:\n    import warnings\n    warnings.warn(ImportWarning(""Could not import Pytorch related modules:\\n%s""\n        % e.msg))\n'"
trixi/logger/file/numpyplotfilelogger.py,0,"b'import os\n\nfrom trixi.logger.abstractlogger import convert_params\nfrom trixi.logger.plt.numpyseabornplotlogger import NumpySeabornPlotLogger\nfrom trixi.util import savefig_and_close\n\n\n# this is just to turn threaded into non-threaded\ndef threaded(func):\n    return func\n\n\nclass NumpyPlotFileLogger(NumpySeabornPlotLogger):\n    """"""\n    NumpyPlotFileLogger is a logger, which can plot/ interpret numpy array as different types (images, lineplots, ...)\n    into an image and plot directory. For the plotting it builds up on the NumpySeabornPlotLogger.\n\n    """"""\n\n    def __init__(self, img_dir, plot_dir, **kwargs):\n        """"""\n        Initializes a numpy plot file logger to plot images and plots into an image and plot directory\n\n        Args:\n            img_dir: The directory to store images in\n            plot_dir: The directory to store plots in\n        """"""\n        super(NumpyPlotFileLogger, self).__init__(**kwargs)\n        self.img_dir = img_dir\n        self.plot_dir = plot_dir\n\n    @convert_params\n    def show_image(self, image, name, file_format="".png"", *args, **kwargs):\n        """"""\n        Method which stores an image as a image file\n\n        Args:\n            image: Numpy array-image\n            name: file-name\n            file_format: output-image file format\n\n        """"""\n        figure = NumpySeabornPlotLogger.show_image(self, image, name, show=False)\n        outname = os.path.join(self.img_dir, name) + file_format\n        os.makedirs(os.path.dirname(outname), exist_ok=True)\n        threaded(savefig_and_close)(figure, outname)\n\n    @convert_params\n    def show_value(self, value, name, counter=None, tag=None, file_format="".png"", *args, **kwargs):\n        """"""\n        Method which logs a value as a line plot\n\n        Args:\n            value: Value (y-axis value) you want to display/ plot/ store\n            name: Name of the value (will also be the filename if no tag is given)\n            counter: counter, which tells the number of the sample (with the same name --> filename) (x-axis value)\n            tag: Tag, grouping similar values. Values with the same tag will be plotted in the same plot\n            file_format: output-image file format\n\n        Returns:\n\n        """"""\n        figure = NumpySeabornPlotLogger.show_value(self, value, name, counter, tag, show=False)\n        if tag is None:\n            outname = os.path.join(self.plot_dir, name) + file_format\n        else:\n            outname = os.path.join(self.plot_dir, tag) + file_format\n        os.makedirs(os.path.dirname(outname), exist_ok=True)\n        threaded(savefig_and_close)(figure, outname)\n\n    @convert_params\n    def show_barplot(self, array, name, file_format="".png"", *args, **kwargs):\n        """"""\n        Method which creates and stores a barplot\n\n        Args:\n            array: Array of values you want to plot\n            name: file-name\n            file_format: output-image (plot) file format\n        """"""\n        figure = NumpySeabornPlotLogger.show_barplot(self, array, name, show=False)\n        outname = os.path.join(self.plot_dir, name) + file_format\n        os.makedirs(os.path.dirname(outname), exist_ok=True)\n        threaded(savefig_and_close)(figure, outname)\n\n    @convert_params\n    def show_boxplot(self, array, name, file_format="".png"", *args, **kwargs):\n        """"""\n        Method which creates and stores a boxplot\n\n        Args:\n            array: Array of values you want to plot\n            name: file-name\n            file_format: output-image (plot) file format\n        """"""\n        figure = NumpySeabornPlotLogger.show_boxplot(self, array, name, show=False, **kwargs)\n        outname = os.path.join(self.plot_dir, name) + file_format\n        os.makedirs(os.path.dirname(outname), exist_ok=True)\n        threaded(savefig_and_close)(figure, outname)\n\n    @convert_params\n    def show_lineplot(self, y_vals, x_vals, name, file_format="".png"", *args, **kwargs):\n        """"""\n        Method which creates and stores a lineplot\n\n        Args:\n            y_vals: Array of y values\n            x_vals: Array of corresponding x-values\n            name: file-name\n            file_format: output-image (plot) file format\n        """"""\n        figure = NumpySeabornPlotLogger.show_lineplot(self, y_vals, x_vals, name, show=False)\n        outname = os.path.join(self.plot_dir, name) + file_format\n        os.makedirs(os.path.dirname(outname), exist_ok=True)\n        threaded(savefig_and_close)(figure, outname)\n\n    @convert_params\n    def show_scatterplot(self, array, name, file_format="".png"", *args, **kwargs):\n        """"""\n        Method which creates and stores a scatter\n\n        Args:\n            array: Array of values you want to plot\n            name: file-name\n            file_format: output-image (plot) file format\n        """"""\n        figure = NumpySeabornPlotLogger.show_scatterplot(self, array, name, show=False)\n        outname = os.path.join(self.plot_dir, name) + file_format\n        os.makedirs(os.path.dirname(outname), exist_ok=True)\n        threaded(savefig_and_close)(figure, outname)\n\n    @convert_params\n    def show_piechart(self, array, name, file_format="".png"", *args, **kwargs):\n        """"""\n        Method which creates and stores a piechart\n\n        Args:\n            array: Array of values you want to plot\n            name: file-name\n            file_format: output-image (plot) file format\n        """"""\n        figure = NumpySeabornPlotLogger.show_piechart(self, array, name, show=False)\n        outname = os.path.join(self.plot_dir, name) + file_format\n        os.makedirs(os.path.dirname(outname), exist_ok=True)\n        threaded(savefig_and_close)(figure, outname)\n\n    def show_matplot_plt(self, figure, name, file_format="".png"", *args, **kwargs):\n        """"""\n        Method to save a custom matplotlib figure\n\n        Args:\n            figure: Figure you want to plot\n            name: file name\n            file_format: output image (plot) file format\n        """"""\n\n        outname = os.path.join(self.plot_dir, name) + file_format\n        os.makedirs(os.path.dirname(outname), exist_ok=True)\n        threaded(savefig_and_close)(figure, outname)\n'"
trixi/logger/file/pytorchplotfilelogger.py,5,"b'import os\nimport warnings\n\nimport torch\nfrom PIL import Image\nimport numpy as np\nfrom matplotlib import cm\nfrom imageio import imwrite\nfrom torch.autograd import Variable\nfrom torchvision.utils import save_image as tv_save_image\nfrom trixi.util.util import np_make_grid\n\nfrom trixi.logger.abstractlogger import threaded, convert_params\nfrom trixi.logger.file.numpyplotfilelogger import NumpyPlotFileLogger\nfrom trixi.util import name_and_iter_to_filename\nfrom trixi.util.pytorchutils import get_guided_image_gradient, get_smooth_image_gradient, get_vanilla_image_gradient\n\n\nclass PytorchPlotFileLogger(NumpyPlotFileLogger):\n    """"""\n    Visual logger, inherits the NumpyPlotLogger and plots/ logs pytorch tensors and variables as files on the local\n    file system.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        """"""\n        Initializes a PytorchPlotFileLogger to plot images, plots, ... into an image and plot directory\n\n        Args:\n            img_dir: The directory to store images in\n            plot_dir: The directory to store plots in\n        """"""\n        super(PytorchPlotFileLogger, self).__init__(*args, **kwargs)\n\n    def process_params(self, f, *args, **kwargs):\n        """"""\n        Inherited ""decorator"": convert Pytorch variables and Tensors to numpy arrays\n        """"""\n\n        ### convert args\n        args = (a.detach().cpu().numpy() if torch.is_tensor(a) else a for a in args)\n\n        ### convert kwargs\n        for key, data in kwargs.items():\n            if torch.is_tensor(data):\n                kwargs[key] = data.detach().cpu().numpy()\n\n        return f(self, *args, **kwargs)\n\n    @staticmethod\n    @threaded\n    def save_image_static(image_dir, tensor, name, n_iter=None, iter_format=""{:05d}"", prefix=False, image_args=None):\n        """"""\n        Saves an image tensor in an image directory\n\n        Args:\n            image_dir: Directory to save the image in\n            tensor: Tensor containing the image\n            name: file-name of the image file\n            n_iter: The iteration number, formatted with the iter_format and added to the model name (if not None)\n            iter_format: The format string, which indicates how n_iter will be formated as a string\n            prefix: If True, the formated n_iter will be appended as a prefix, otherwise as a suffix\n            image_args: Arguments for the tensorvision save image method\n        """"""\n\n        if isinstance(tensor, np.ndarray):\n            tensor = torch.from_numpy(tensor)\n\n        if image_args is None:\n            image_args = {}\n\n        if n_iter is not None:\n            name = name_and_iter_to_filename(name=name, n_iter=n_iter, ending="".png"", iter_format=iter_format,\n                                             prefix=prefix)\n        elif not name.endswith("".png""):\n            name = name + "".png""\n\n        img_file = os.path.join(image_dir, name)\n        os.makedirs(os.path.dirname(img_file), exist_ok=True)\n        tv_save_image(tensor, img_file, **image_args)\n\n    def save_image(self, tensor, name, n_iter=None, iter_format=""{:05d}"", prefix=False, image_args=None):\n        """"""\n        Saves an image into the image directory of the PytorchPlotFileLogger\n\n        Args:\n            tensor: Tensor containing the image\n            name: file-name of the image file\n            n_iter: The iteration number, formatted with the iter_format and added to the model name (if not None)\n            iter_format: The format string, which indicates how n_iter will be formated as a string\n            prefix: If True, the formated n_iter will be appended as a prefix, otherwise as a suffix\n            image_args: Arguments for the tensorvision save image method\n\n        """"""\n\n        if image_args is None:\n            image_args = {}\n\n        self.save_image_static(image_dir=self.img_dir, tensor=tensor, name=name, n_iter=n_iter, iter_format=iter_format,\n                               prefix=prefix, image_args=image_args)\n\n    @staticmethod\n    @threaded\n    def save_images_static(image_dir, tensors, n_iter=None, iter_format=""{:05d}"", prefix=False, image_args=None):\n        """"""\n        Saves an image tensors in an image directory\n\n        Args:\n            image_dir: Directory to save the image in\n            tensors: A dict with file-name-> tensor to plot as image\n            n_iter: The iteration number, formatted with the iter_format and added to the model name (if not None)\n            iter_format: The format string, which indicates how n_iter will be formated as a string\n            prefix: If True, the formated n_iter will be appended as a prefix, otherwise as a suffix\n            image_args: Arguments for the tensorvision save image method\n        """"""\n\n        assert isinstance(tensors, dict)\n        if image_args is None:\n            image_args = {}\n\n        for name, tensor in tensors.items():\n            PytorchPlotFileLogger.save_image_static(image_dir=image_dir, tensor=tensor, name=name, n_iter=n_iter,\n                                                    iter_format=iter_format, prefix=prefix, image_args=image_args)\n\n    def save_images(self, tensors, n_iter=None, iter_format=""{:05d}"", prefix=False, image_args=None):\n        """"""\n        Saves an image tensors into the image directory of the PytorchPlotFileLogger\n\n        Args:\n            tensors: A dict with file-name-> tensor to plot as image\n            n_iter: The iteration number, formatted with the iter_format and added to the model name (if not None)\n            iter_format: The format string, which indicates how n_iter will be formated as a string\n            prefix: If True, the formated n_iter will be appended as a prefix, otherwise as a suffix\n            image_args: Arguments for the tensorvision save image method\n        """"""\n\n        assert isinstance(tensors, dict)\n        if image_args is None:\n            image_args = {}\n\n        self.save_images_static(image_dir=self.img_dir, tensors=tensors, n_iter=n_iter, iter_format=iter_format,\n                                prefix=prefix, image_args=image_args)\n\n    @staticmethod\n    @threaded\n    def save_image_grid_static(image_dir, tensor, name, n_iter=None, prefix=False, iter_format=""{:05d}"",\n                               image_args=None):\n        """"""\n        Saves images of a 4d- tensor (N, C, H, W) as a image grid into an image file in a given directory\n\n        Args:\n            image_dir: Directory to save the image in\n            tensor: 4d- tensor (N, C, H, W)\n            name: file-name of the image file\n            n_iter: The iteration number, formatted with the iter_format and added to the model name (if not None)\n            iter_format: The format string, which indicates how n_iter will be formated as a string\n            prefix: If True, the formated n_iter will be appended as a prefix, otherwise as a suffix\n            image_args: Arguments for the tensorvision save image method\n\n        """"""\n\n        if isinstance(tensor, np.ndarray):\n            tensor = torch.tensor(tensor)\n\n        if not (tensor.size(1) == 1 or tensor.size(1) == 3):\n            warnings.warn(""The 1. dimension (channel) has to be either 1 (gray) or 3 (rgb), taking the first ""\n                          ""dimension now !!!"")\n            tensor = tensor[:, 0:1, ]\n\n        if n_iter is not None:\n            name = name_and_iter_to_filename(name=name, n_iter=n_iter, ending="".png"", iter_format=iter_format,\n                                             prefix=prefix)\n        elif not name.endswith("".png""):\n            name += "".png""\n\n        img_file = os.path.join(image_dir, name)\n\n        if image_args is None:\n            image_args = {}\n\n        os.makedirs(os.path.dirname(img_file), exist_ok=True)\n\n        tv_save_image(tensor, img_file, **image_args)\n\n    def save_image_grid(self, tensor, name, n_iter=None, prefix=False, iter_format=""{:05d}"", image_args=None):\n        """"""\n        Saves images of a 4d- tensor (N, C, H, W) as a image grid into an image file in the image directory of the\n        PytorchPlotFileLogger\n\n        Args:\n            tensor: 4d- tensor (N, C, H, W)\n            name: file-name of the image file\n            n_iter: The iteration number, formatted with the iter_format and added to the model name (if not None)\n            iter_format: The format string, which indicates how n_iter will be formated as a string\n            prefix: If True, the formated n_iter will be appended as a prefix, otherwise as a suffix\n            image_args: Arguments for the tensorvision save image method\n\n        """"""\n\n        if image_args is None:\n            image_args = {}\n\n        self.save_image_grid_static(image_dir=self.img_dir, tensor=tensor, name=name, n_iter=n_iter, prefix=prefix,\n                                    iter_format=iter_format, image_args=image_args)\n\n    def show_image(self, image, name, n_iter=None, iter_format=""{:05d}"", prefix=False, image_args=None, **kwargs):\n        """"""\n        Calls the save image method (for abstract logger combatibility)\n\n        Args:\n            image: Tensor containing the image\n            name: file-name of the image file\n            n_iter: The iteration number, formatted with the iter_format and added to the model name (if not None)\n            iter_format: The format string, which indicates how n_iter will be formated as a string\n            prefix: If True, the formated n_iter will be appended as a prefix, otherwise as a suffix\n            image_args: Arguments for the tensorvision save image method\n\n\n        """"""\n        self.save_image(tensor=image, name=name, n_iter=n_iter, iter_format=iter_format, image_args=image_args,\n                        prefix=prefix)\n\n    def show_images(self, images, name, n_iter=None, iter_format=""{:05d}"", prefix=False, image_args=None, **kwargs):\n        """"""\n        Calls the save images method (for abstract logger combatibility)\n\n        Args:\n            images: List of Tensors\n            name: List of file names (corresponding to the images list)\n            n_iter: The iteration number, formatted with the iter_format and added to the model name (if not None)\n            iter_format: The format string, which indicates how n_iter will be formated as a string\n            prefix: If True, the formated n_iter will be appended as a prefix, otherwise as a suffix\n            image_args: Arguments for the tensorvision save image method\n\n        """"""\n        tensors = {}\n        for i, img in enumerate(images):\n            tensors[name + ""_"" + str(i)] = img\n\n        self.save_images(tensors=tensors, n_iter=n_iter, iter_format=iter_format, prefix=prefix, image_args=image_args)\n\n    def show_image_grid(self, images, name, n_iter=None, prefix=False, iter_format=""{:05d}"", image_args=None,\n                        **kwargs):\n        """"""\n        Calls the save image grid method (for abstract logger combatibility)\n\n        Args:\n            images: 4d- tensor (N, C, H, W)\n            name: file-name of the image file\n            n_iter: The iteration number, formatted with the iter_format and added to the model name (if not None)\n            iter_format: The format string, which indicates how n_iter will be formated as a string\n            prefix: If True, the formated n_iter will be appended as a prefix, otherwise as a suffix\n            image_args: Arguments for the tensorvision save image method\n\n\n        """"""\n\n        self.save_image_grid(tensor=images, name=name, n_iter=n_iter, prefix=prefix, iter_format=iter_format,\n                             image_args=image_args)\n\n    @convert_params\n    def show_image_grid_heatmap(self, heatmap, background=None, ratio=0.3, normalize=True,\n                                colormap=cm.jet, name=""heatmap"", n_iter=None,\n                                prefix=False, iter_format=""{:05d}"", image_args=None, **kwargs):\n        """"""\n        Creates heat map from the given map and if given combines it with the background and then\n        displays results with as image grid.\n\n        Args:\n           heatmap:  4d- tensor (N, C, H, W) to be converted to a heatmap\n           background: 4d- tensor (N, C, H, W) background/ context of the heatmap (to be underlayed)\n           name: The name of the window\n           ratio: The ratio to mix the map with the background (0 = only background, 1 = only map)\n           n_iter: The iteration number, formatted with the iter_format and added to the model name (if not None)\n           iter_format: The format string, which indicates how n_iter will be formated as a string\n           prefix: If True, the formated n_iter will be appended as a prefix, otherwise as a suffix\n           image_args: Arguments for the tensorvision save image method\n\n        """"""\n\n        if image_args is None:\n            image_args = {}\n        if ""normalize"" not in image_args:\n            image_args[""normalize""] = normalize\n\n        if n_iter is not None:\n            name = name_and_iter_to_filename(name=name, n_iter=n_iter, ending="".png"", iter_format=iter_format,\n                                             prefix=prefix)\n        elif not name.endswith("".png""):\n            name += "".png""\n\n        file_name = os.path.join(self.img_dir, name)\n\n        map_grid = np_make_grid(heatmap, normalize=normalize)  # map_grid.shape is (3, X, Y)\n        if heatmap.shape[1] != 3:\n            map_ = colormap(map_grid[0])[..., :-1].transpose(2, 0, 1)\n        else:  # heatmap was already RGB, so don\'t apply colormap\n            map_ = map_grid\n\n        if background is not None:\n            img_grid = np_make_grid(background, **image_args)\n            fuse_img = (1.0 - ratio) * img_grid + ratio * map_\n        else:\n            fuse_img = map_\n\n        fuse_img = np.clip(fuse_img * 255, a_min=0, a_max=255).astype(np.uint8)\n\n        imwrite(file_name, fuse_img.transpose(1, 2, 0))\n'"
trixi/logger/file/textfilelogger.py,0,"b'from __future__ import print_function\n\nimport datetime\nimport logging\nimport os\nimport sys\n\nfrom trixi.logger.abstractlogger import AbstractLogger\nfrom trixi.util import random_string\n\n\nclass TextFileLogger(AbstractLogger):\n    """"""\n    A Logger for logging text into different text files and output streams (using the python logging framework)\n    """"""\n\n    def __init__(self, base_dir=None, logging_level=logging.DEBUG, logging_stream=sys.stdout,\n                 default_stream_handler=True, **kwargs):\n        """"""\n        Initializes a TextFileLogger and a default logger\n\n        Args:\n            base_dir: Directory to save the log files in\n            logging_level: Default logging level for the default log\n            logging_stream: default logging level\n            default_stream_handler: Falg, if a default stream handler should be added\n        """"""\n\n        super(TextFileLogger, self).__init__(**kwargs)\n\n        self.base_dir = base_dir\n        self.logging_level = logging_level\n        self.logging_stream = logging_stream\n        self.loggers = dict()\n        self.stream_handler_formatter = logging.Formatter(\'%(message)s\')\n        self.file_handler_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\')\n\n        self.init_time = datetime.datetime.today()\n\n        # set up logging\n        self.logging_identifier = random_string(10)\n        self.add_logger(""default"", stream_handler=default_stream_handler)\n\n    def add_logger(self, name, logging_level=None, file_handler=True, stream_handler=True):\n        """"""\n        Adds a new logger\n\n        Args:\n            name: Name of the new logger\n            logging_level: Logging level of the new logger\n            file_handler: Flag, if it should use a file_handler, if yes creates a new file with the given name in the\n                logging directory\n            stream_handler: Flag, if the logger should also log to the default stream\n\n        Returns:\n\n        """"""\n\n        self.loggers[name] = logging.getLogger(name + ""-"" + self.logging_identifier)\n\n        if logging_level is None:\n            self.loggers[name].setLevel(self.logging_level)\n        else:\n            self.loggers[name].setLevel(logging_level)\n\n        if file_handler is True:\n            self.add_file_handler(name, name)\n        elif isinstance(file_handler, (list, tuple, set)):\n            for fh in file_handler:\n                if isinstance(fh, logging.FileHandler):\n                    self.add_handler(fh, name)\n                else:\n                    self.add_file_handler(fh, name)\n        elif isinstance(file_handler, logging.FileHandler):\n            self.add_handler(file_handler, name)\n        else:\n            pass\n\n        if stream_handler:\n            self.add_stream_handler(name)\n\n    def add_handler(self, handler, logger=""default""):\n        """"""\n        Adds an additional handler to a logger with a name\n        Args:\n            handler: Logging handler to be added to a given logger\n            logger: Name of the logger to add the hander to\n\n        """"""\n        self.loggers[logger].addHandler(handler)\n\n    def add_file_handler(self, name, logger=""default""):\n        """"""\n        Adds a file handler to a logger, thus the logger will log into a log file with a given name\n\n        Args:\n            name: File name of the log file (in which the logger will log now)\n            logger: Name of the logger to add the file-hander/ logging file to\n\n        """"""\n        file_handler = logging.FileHandler(os.path.join(self.base_dir, name + "".log""))\n        file_handler.setFormatter(self.file_handler_formatter)\n        self.add_handler(file_handler, logger)\n\n    def add_stream_handler(self, logger=""default""):\n        """"""\n        Adds a stream handler to a logger, thus the logger will log into the default logging stream\n\n        Args:\n            logger: Name of the logger to add the stream-hander to\n\n        """"""\n        stream_handler = logging.StreamHandler(self.logging_stream)\n        stream_handler.setFormatter(self.stream_handler_formatter)\n        self.add_handler(stream_handler, logger)\n\n    def print(self, *args, logger=""default""):\n        """"""\n        Prints and logs objects\n\n        Args:\n            *args: Object to print/ log\n            logger: Logger which should log\n\n        """"""\n        self.log("" "".join(map(str, args)), logger)\n\n    def log(self, msg, logger=""default""):\n        """"""\n        Prints and logs a message with the level info\n\n        Args:\n            msg: Message to print/ log\n            logger: Logger which should log\n\n        """"""\n        self.loggers[logger].info(msg)\n\n    def info(self, msg, logger=""default""):\n        """"""\n        Prints and logs a message with the level info\n\n        Args:\n            msg: Message to print/ log\n            logger: Logger which should log\n\n        """"""\n        self.loggers[logger].info(msg)\n\n    def debug(self, msg, logger=""default""):\n        """"""\n        Prints and logs a message with the level debug\n\n        Args:\n            msg: Message to print/ log\n            logger: Logger which should log\n\n        """"""\n        self.loggers[logger].debug(msg)\n\n    def error(self, msg, logger=""default""):\n        """"""\n        Prints and logs a message with the level error\n\n        Args:\n            msg: Message to print/ log\n            logger: Logger which should log\n\n        """"""\n        self.loggers[logger].error(msg)\n\n    def log_to(self, msg, name, log_to_default=False):\n        """"""\n        Logs to an existing logger or if logger does not exists creates new one\n\n        Args:\n            msg: Message to be logged\n            name: Name of the logger to log to (usually also the logfile-name)\n            log_to_default: Flag if it should in addition to the logger given by name, log to the default logger\n\n        """"""\n\n        if name not in self.loggers:\n            self.add_logger(name)\n        self.log(msg, name)\n        if log_to_default:\n            self.log(msg, ""default"")\n\n    def show_text(self, text, name=None, logger=""default"", **kwargs):\n        """"""\n        Logs a text. Calls the log function (for combatibility reasons with AbstractLogger)\n\n        Args:\n            text: Text to be logged\n            name: Some identifier for the text (will be added infront of the text)\n            logger: Name of the Logger to log to\n\n        """"""\n        if name is not None:\n            self.log(""{}: {}"".format(name, text), logger)\n        else:\n            self.log(text, logger)\n\n    def show_value(self, value, name=None, logger=""default"", **kwargs):\n        """"""\n        Logs a Value. Calls the log function (for combatibility reasons with AbstractLogger)\n\n        Args:\n            value: Value to be logged\n            name: Some identifier for the text (will be added infront of the text)\n            logger: Name of the Logger to log to\n\n        """"""\n        if name is not None:\n            self.log(""{}: {}"".format(name, value), logger)\n        else:\n            self.log(value, logger)\n'"
trixi/logger/message/__init__.py,0,"b'try:\n    from trixi.logger.message.telegrammessagelogger import TelegramMessageLogger\nexcept ImportError as e:\n    import warnings\n    warnings.warn(ImportWarning(""Could not import telegram related modules:\\n%s""\n        % e.msg))\n'"
trixi/logger/message/slackmessagelogger.py,4,"b'import numpy as np\nimport torch\nimport torchvision\nfrom slack import WebClient\n\nfrom trixi.util.util import get_image_as_buffered_file\n\nfrom trixi.logger.plt.numpyseabornimageplotlogger import NumpySeabornImagePlotLogger\n\n\nclass SlackMessageLogger(NumpySeabornImagePlotLogger):\n    """"""\n    Slack logger, inherits the NumpySeabornImagePlotLogger and sends plots/logs to a chat via a Slack bot.\n    """"""\n\n    @staticmethod\n    def find_uid_for_email(slack_client, email):\n        """"""\n        Returns the slack user id for a given email\n\n        Args:\n            slack_client: Slack client (already authorized)\n            email: Workspace email address to get the user id for\n\n        Returns:\n            Slack workspace user id\n\n        """"""\n\n        user_list = slack_client.users_list().get(\'members\', [])\n        user_id = """"\n\n        for user in user_list:\n            if user.get(\'profile\', {}).get(\'email\', \'\') == email:\n                user_id = user[\'id\']\n                break\n\n        return user_id\n\n    @staticmethod\n    def find_cid_for_user(slack_client, uid):\n        """"""\n        Returns the chat/channel id for a direct message of the bot with the given user\n\n        Args:\n            slack_client: Slack client (already authorized)\n            uid: User id of the user\n\n        Returns:\n            chat/channel id for a direct message\n        """"""\n\n        direct_conv = slack_client.conversations_open(\n            users=[uid]\n        )\n\n        c_id = direct_conv.get(\'channel\', {}).get(\'id\', None)\n\n        return c_id\n\n    def __init__(self, token, user_email, exp_name=None, *args, **kwargs):\n        """"""\n        Creates a new NumpySeabornImagePlotLogger object.\n\n        Args:\n            token (str): The Bot User OAuth Access Token of the slack bot used.\n            user_email (str): The user email in the workspace for the chat between the user and the slack bot.\n            exp_name: name of the experiment, always used as message prefix\n        """"""\n        super(SlackMessageLogger, self).__init__(**kwargs)\n\n        self.token = token\n        self.user_email = user_email\n        self.slack_client = WebClient(token)\n        self.uid = self.find_uid_for_email(self.slack_client, self.user_email)\n        self.cid = self.find_cid_for_user(self.slack_client, self.uid)\n        self.exp_name = exp_name\n\n        self.ts_dict = {}\n\n    def process_params(self, f, *args, **kwargs):\n        """"""\n        Inherited ""decorator"": convert PyTorch variables and Tensors to numpy arrays\n        """"""\n\n        # convert args\n        args = (a.detach().cpu().numpy() if torch.is_tensor(a) else a for a in args)\n\n        # convert kwargs\n        for key, data in kwargs.items():\n            if torch.is_tensor(data):\n                kwargs[key] = data.detach().cpu().numpy()\n\n        return f(self, *args, **kwargs)\n\n    def send_message(self, message="""", file=None):\n        """"""\n        Sends a message and a file if one is given\n\n        Args:\n            message: Message to be sent\n            file:  File to be sent\n\n        Returns:\n            The timestamp (ts) of the message\n\n        """"""\n        if file is None:\n            ret_val = self.slack_client.chat_postMessage(\n                channel=self.cid,\n                text=message\n            )\n            return ret_val.get(\'ts\', \'\')\n        else:\n            ret_val = self.slack_client.files_upload(\n                file=file,\n                channels=self.cid,\n                title=message\n            )\n            try:\n                ts = list(ret_val[\'file\'][\'shares\'][\'private\'].values())[0][0][\'ts\']\n            except Exception as e:\n                ts = """"\n            return ts\n\n    def delete_message(self, ts):\n        """"""\n        Deletes a direct message from the bot with the given timestamp (ts)\n\n        Args:\n            ts: Time stamp the message was sent\n\n        """"""\n        self.slack_client.chat_delete(\n          channel=self.cid,\n          ts=ts\n        )\n\n    def show_text(self, text, *args, **kwargs):\n        """"""\n        Sends a text to a chat using an existing slack bot.\n\n        Args:\n            text (str): Text message to be sent to the bot.\n        """"""\n        if self.exp_name is not None:\n            text = self.exp_name + "":\\n"" + text\n        try:\n            self.send_message(message=text)\n        except Exception as e:\n            print(""Could not send text to slack"")\n\n    def show_image(self, image, *args,  **kwargs):\n        """"""\n        Sends an image file to a chat using an existing slack bot.\n\n        Args:\n            image (str or np array): Path to the image file to be sent to the chat.\n        """"""\n        try:\n            if isinstance(image, str):\n                with open(image, \'rb\') as img_file:\n                    self.send_message(message=self.exp_name, file=img_file)\n            elif isinstance(image, np.ndarray):\n                buf = get_image_as_buffered_file(image)\n                self.send_message(message=self.exp_name, file=buf)\n\n        except Exception as e:\n            print(""Could not send image to slack"")\n\n    def show_image_grid(self, image_array, name=None, nrow=8, padding=2,\n                        normalize=False, range=None, scale_each=False, pad_value=0, delete_last=True,\n                        *args, **kwargs):\n        """"""\n        Sends an array of images to a chat using  an existing Slack bot. (Requires torch and torchvision)\n\n\n        Args:\n            image_array (np.narray / torch.tensor): Image array/tensor which will be sent as an image grid\n            make_grid_kargs: Key word arguments for the torchvision make grid method\n            delete_last: If a message with the same name was sent, delete it beforehand\n        """"""\n\n        caption = """"\n        if self.exp_name is not None:\n            caption += self.exp_name + ""  ""\n        if name is not None:\n            caption += name + ""  ""\n\n        if delete_last and name in self.ts_dict:\n            self.delete_message(self.ts_dict[name])\n\n        if isinstance(image_array, np.ndarray):\n            image_array = torch.from_numpy(image_array)\n\n        image_array = image_array.cpu()\n        grid = torchvision.utils.make_grid(image_array, nrow=nrow, padding=padding, pad_value=pad_value,\n                                           normalize=normalize, range=range, scale_each=scale_each)\n        ndarr = grid.mul(255).clamp(0, 255).byte().numpy()\n        buf = get_image_as_buffered_file(ndarr)\n        try:\n            ts = self.send_message(message=caption, file=buf)\n            self.ts_dict[name] = ts\n        except Exception as e:\n            print(""Could not send image_grid to slack"")\n\n    def show_value(self, value, name, counter=None, tag=None, delete_last=True, *args, **kwargs):\n        """"""\n        Sends a value to a chat using an existing slack bot.\n\n        Args:\n            value: Value to be plotted sent to the chat.\n            name: Name for the plot.\n            counter: Optional counter to be sent in conjunction with the value.\n            tag: Tag to be used as a label for the plot.\n            delete_last: If a message with the same name was sent, delete it beforehand\n        """"""\n\n        caption = """"\n        if self.exp_name is not None:\n            caption += self.exp_name + ""  ""\n        if name is not None:\n            caption += name + ""  ""\n\n        if delete_last and name in self.ts_dict:\n            self.delete_message(self.ts_dict[name])\n\n        figure = super().show_value(value=value, name=name, counter=counter, tag=tag)\n        buf = get_image_as_buffered_file(figure)\n        try:\n            ts = self.send_message(message=caption, file=buf)\n            self.ts_dict[name] = ts\n        except Exception as e:\n            print(""Could not send plot to slack"")\n\n    def show_barplot(self, array, name=""barplot"", delete_last=True, *args, **kwargs):\n        """"""\n        Sends a barplot to a chat using an existing slack bot.\n\n        Args:\n            array: array of shape NxM where N is the number of rows and M is the number of elements in the row.\n            name: The name of the figure\n            delete_last: If a message with the same name was sent, delete it beforehand\n\n        """"""\n\n        caption = """"\n        if self.exp_name is not None:\n            caption += self.exp_name + ""  ""\n        if name is not None:\n            caption += name + ""  ""\n\n        if delete_last and name in self.ts_dict:\n            self.delete_message(self.ts_dict[name])\n\n        figure = super().show_barplot(array, name, *args, **kwargs)\n        buf = get_image_as_buffered_file(figure)\n        try:\n            ts = self.send_message(message=caption, file=buf)\n            self.ts_dict[name] = ts\n        except Exception as e:\n            print(""Could not send plot to slack"")\n\n    def show_lineplot(self, y_vals, x_vals=None, name=""lineplot"", delete_last=True, *args, **kwargs):\n        """"""\n        Sends a lineplot to a chat using an existing slack bot.\n\n        Args:\n            y_vals: Array of shape MxN , where M is the number of points and N is the number of different line\n            x_vals: Has to have the same shape as Y: MxN. For each point in Y it gives the corresponding X value (if\n                not set the points are assumed to be equally distributed in the interval [0, 1])\n            name: The name of the figure\n            delete_last: If a message with the same name was sent, delete it beforehand\n\n        """"""\n\n        caption = """"\n        if self.exp_name is not None:\n            caption += self.exp_name + ""  ""\n        if name is not None:\n            caption += name + ""  ""\n\n        if delete_last and name in self.ts_dict:\n            self.delete_message(self.ts_dict[name])\n\n        figure = super().show_lineplot(y_vals, x_vals, name, *args, **kwargs)\n        buf = get_image_as_buffered_file(figure)\n        try:\n            ts = self.send_message(message=caption, file=buf)\n            self.ts_dict[name] = ts\n        except Exception as e:\n            print(""Could not send plot to slack"")\n\n    def show_scatterplot(self, array, name=""scatterplot"", delete_last=True, *args, **kwargs):\n        """"""\n        Sends a scatterplot to a chat using an existing slack bot.\n\n        Args:\n            array: An array with size N x dim, where each element i \\in N at X[i] results in a 2D\n                (if dim = 2) or 3D (if dim = 3) point.\n            name: The name of the figure\n            delete_last: If a message with the same name was sent, delete it beforehand\n\n        """"""\n\n        caption = """"\n        if self.exp_name is not None:\n            caption += self.exp_name + ""  ""\n        if name is not None:\n            caption += name + ""  ""\n\n        if delete_last and name in self.ts_dict:\n            self.delete_message(self.ts_dict[name])\n\n        figure = super().show_scatterplot(array, name, *args, **kwargs)\n        buf = get_image_as_buffered_file(figure)\n\n        try:\n            ts = self.send_message(message=caption, file=buf)\n            self.ts_dict[name] = ts\n        except Exception as e:\n            print(""Could not send plot to slack"")\n\n    def show_piechart(self, array, name=""piechart"", delete_last=True, *args, **kwargs):\n        """"""\n        Sends a piechart to a chat using an existing slack bot.\n\n        Args:\n            array: Array of positive integers. Each integer will be presented as a part of the pie (with the total\n                as the sum of all integers)\n            name: The name of the figure\n            delete_last: If a message with the same name was sent, delete it beforehand\n\n        """"""\n\n        caption = """"\n        if self.exp_name is not None:\n            caption += self.exp_name + ""  ""\n        if name is not None:\n            caption += name + ""  ""\n\n        if delete_last and name in self.ts_dict:\n            self.delete_message(self.ts_dict[name])\n\n        figure = super().show_piechart(array, name, *args, **kwargs)\n        buf = get_image_as_buffered_file(figure)\n\n        try:\n            ts = self.send_message(message=caption, file=buf)\n            self.ts_dict[name] = ts\n        except Exception as e:\n            print(""Could not send plot to slack"")\n\n    def print(self, text, *args, **kwargs):\n        """"""Just calls :meth:`.show_text`""""""\n        self.show_text(text, *args, **kwargs)\n'"
trixi/logger/message/telegrammessagelogger.py,4,"b'import numpy as np\nimport telegram\nimport torch\nimport torchvision\nfrom trixi.util.util import get_image_as_buffered_file\n\nfrom trixi.logger.plt.numpyseabornimageplotlogger import NumpySeabornImagePlotLogger\n\n\nclass TelegramMessageLogger(NumpySeabornImagePlotLogger):\n    """"""\n    Telegram logger, inherits the NumpySeabornImagePlotLogger and sends plots/logs to a chat via a Telegram bot.\n    """"""\n\n    def __init__(self, token, chat_id, exp_name=None, *args, **kwargs):\n        """"""\n        Creates a new TelegramMessageLogger object.\n\n        Args:\n            token (str): The token of the Telegram bot used.\n            chat_id (str): The chat ID for the chat between the user and the Telegram bot.\n        """"""\n        super(TelegramMessageLogger, self).__init__(**kwargs)\n\n        self.token = token\n        self.chat_id = chat_id\n        self.bot = telegram.Bot(token=self.token)\n        self.exp_name = exp_name\n\n    def process_params(self, f, *args, **kwargs):\n        """"""\n        Inherited ""decorator"": convert PyTorch variables and Tensors to numpy arrays\n        """"""\n\n        # convert args\n        args = (a.detach().cpu().numpy() if torch.is_tensor(a) else a for a in args)\n\n        # convert kwargs\n        for key, data in kwargs.items():\n            if torch.is_tensor(data):\n                kwargs[key] = data.detach().cpu().numpy()\n\n        return f(self, *args, **kwargs)\n\n    def show_text(self, text, *args, **kwargs):\n        """"""\n        Sends a text to a chat using an existing Telegram bot.\n\n        Args:\n            text (str): Text message to be sent to the bot.\n        """"""\n        if self.exp_name is not None:\n            text = self.exp_name + "":\\n"" + text\n        try:\n            self.bot.send_message(chat_id=self.chat_id, text=text)\n        except Exception as e:\n            print(""Could not send text to telegram"")\n\n    def show_image(self, image, *args, **kwargs):\n        """"""\n        Sends an image file to a chat using an existing Telegram bot.\n\n        Args:\n            image (str or np array): Path to the image file to be sent to the chat.\n        """"""\n        try:\n            if isinstance(image, str):\n                with open(image, \'rb\') as img_file:\n                    self.bot.send_photo(chat_id=self.chat_id, photo=img_file)\n            elif isinstance(image, np.ndarray):\n                buf = get_image_as_buffered_file(image)\n\n                self.bot.send_photo(chat_id=self.chat_id, photo=buf)\n\n        except Exception as e:\n            print(""Could not send image to telegram"")\n\n    def show_image_grid(self, image_array, name=None, nrow=8, padding=2,\n                        normalize=False, range=None, scale_each=False, pad_value=0, *args, **kwargs):\n        """"""\n        Sends an array of images to a chat using  an existing Telegram bot. (Requires torch and torchvision)\n\n\n        Args:\n            image_array (np.narray / torch.tensor): Image array/ tensor which will be sent as an image grid\n            make_grid_kargs: Key word arguments for the torchvision make grid method\n        """"""\n\n        caption = """"\n        if self.exp_name is not None:\n            caption += self.exp_name + ""  ""\n        if name is not None:\n            caption += name + ""  ""\n\n        if isinstance(image_array, np.ndarray):\n            image_array = torch.from_numpy(image_array)\n\n        image_array = image_array.cpu()\n        grid = torchvision.utils.make_grid(image_array, nrow=nrow, padding=padding, pad_value=pad_value,\n                                           normalize=normalize, range=range, scale_each=scale_each)\n        ndarr = grid.mul(255).clamp(0, 255).byte().permute(1, 2, 0).numpy()\n        buf = get_image_as_buffered_file(ndarr)\n\n        try:\n            self.bot.send_photo(chat_id=self.chat_id, photo=buf, caption=caption)\n        except Exception as e:\n            print(""Could not send image_grid to telegram"")\n\n    def show_value(self, value, name, counter=None, tag=None, *args, **kwargs):\n        """"""\n        Sends a value to a chat using an existing Telegram bot.\n\n        Args:\n            value: Value to be plotted sent to the chat.\n            name: Name for the plot.\n            counter: Optional counter to be sent in conjunction with the value.\n            tag: Tag to be used as a label for the plot.\n        """"""\n\n        caption = """"\n        if self.exp_name is not None:\n            caption += self.exp_name + ""  ""\n        if name is not None:\n            caption += name + ""  ""\n\n        figure = super().show_value(value, name, counter, tag, show=False)\n        buf = get_image_as_buffered_file(figure)\n\n        try:\n            self.bot.send_photo(chat_id=self.chat_id, photo=buf, caption=caption)\n        except Exception as e:\n            print(""Could not send plot to telegram"")\n\n    def show_barplot(self, array, name=None, *args, **kwargs):\n        """"""\n        Sends a barplot to a chat using an existing Telegram bot.\n\n        Args:\n            array: array of shape NxM where N is the number of rows and M is the number of elements in the row.\n            name: The name of the figure\n\n        """"""\n\n        caption = """"\n        if self.exp_name is not None:\n            caption += self.exp_name + ""  ""\n        if name is not None:\n            caption += name + ""  ""\n\n        figure = super().show_barplot(array, name, *args, **kwargs)\n        buf = get_image_as_buffered_file(figure)\n\n        try:\n            self.bot.send_photo(chat_id=self.chat_id, photo=buf, caption=caption)\n        except Exception as e:\n            print(""Could not send plot to telegram"")\n\n    def show_lineplot(self, y_vals, x_vals=None, name=None, *args, **kwargs):\n        """"""\n        Sends a lineplot to a chat using an existing Telegram bot.\n\n        Args:\n            y_vals: Array of shape MxN , where M is the number of points and N is the number of different line\n            x_vals: Has to have the same shape as Y: MxN. For each point in Y it gives the corresponding X value (if\n            not set the points are assumed to be equally distributed in the interval [0, 1] )\n            name: The name of the figure\n\n        """"""\n\n        caption = """"\n        if self.exp_name is not None:\n            caption += self.exp_name + ""  ""\n        if name is not None:\n            caption += name + ""  ""\n\n        figure = super().show_lineplot(y_vals, x_vals, name, show=False, *args, **kwargs)\n        buf = get_image_as_buffered_file(figure)\n\n        try:\n            self.bot.send_photo(chat_id=self.chat_id, photo=buf, caption=caption)\n        except Exception as e:\n            print(""Could not send plot to telegram"")\n\n    def show_scatterplot(self, array, name=None, *args, **kwargs):\n        """"""\n        Sends a scatterplot to a chat using an existing Telegram bot.\n\n        Args:\n            array: A 2d array with size N x dim, where each element i \\in N at X[i] results in a a 2d (if dim = 2)/\n            3d (if dim = 3) point.\n            name: The name of the figure\n\n        """"""\n\n        caption = """"\n        if self.exp_name is not None:\n            caption += self.exp_name + ""  ""\n        if name is not None:\n            caption += name + ""  ""\n\n        figure = super().show_scatterplot(array, name, *args, **kwargs)\n        buf = get_image_as_buffered_file(figure)\n\n        try:\n            self.bot.send_photo(chat_id=self.chat_id, photo=buf, caption=caption)\n        except Exception as e:\n            print(""Could not send plot to telegram"")\n\n    def show_piechart(self, array, name=None, *args, **kwargs):\n        """"""\n        Sends a piechart to a chat using an existing Telegram bot.\n\n        Args:\n            array: Array of positive integers. Each integer will be presented as a part of the pie (with the total\n            as the sum of all integers)\n            name: The name of the figure\n\n        """"""\n\n        caption = """"\n        if self.exp_name is not None:\n            caption += self.exp_name + ""  ""\n        if name is not None:\n            caption += name + ""  ""\n\n        figure = super().show_piechart(array, name,  *args, **kwargs)\n        buf = get_image_as_buffered_file(figure)\n\n        try:\n            self.bot.send_photo(chat_id=self.chat_id, photo=buf, caption=caption)\n        except Exception as e:\n            print(""Could not send plot to telegram"")\n\n    def print(self, text, *args, **kwargs):\n        """"""Just calls show_text()""""""\n        self.show_text(text, *args, **kwargs)\n'"
trixi/logger/plt/__init__.py,0,b'from trixi.logger.plt.numpyseabornplotlogger import NumpySeabornPlotLogger\n'
trixi/logger/plt/numpyseabornimageplotlogger.py,0,"b'from collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn\nfrom trixi.logger.plt.numpyseabornplotlogger import NumpySeabornPlotLogger\n\nfrom trixi.logger.abstractlogger import AbstractLogger, convert_params\nfrom trixi.util.util import chw_to_hwc, figure_to_image\n\n\nclass NumpySeabornImagePlotLogger(NumpySeabornPlotLogger):\n    """"""\n    Wrapper around :class:`.NumpySeabornPlotLogger` that renders figures into numpy arrays.\n    """"""\n\n    def __init__(self, **kwargs):\n        """"""\n        Initializes a new NumpySeabornPlotLogger\n        """"""\n        super(NumpySeabornImagePlotLogger, self).__init__(**kwargs)\n\n    @convert_params\n    def show_image(self, image, name=None, *args, **kwargs):\n        """"""\n        Create an image figure\n\n        Args:\n            image: The image array to be displayed\n            name: The name of the image window\n\n        Returns:\n            A numpy array image of the figure\n\n        """"""\n        # figure = super().show_image(image, name, show=False, *args, **kwargs)\n        # figure_image = figure_to_image(figure)\n\n        return image\n\n    @convert_params\n    def show_value(self, value, name, counter=None, tag=None, *args, **kwargs):\n        """"""\n       Creates a line plot that is automatically appended with new values and returns it as a figure.\n\n       Args:\n           value: Value to be plotted / appended to the graph (y-axis value)\n           name: The name of the window\n           counter: counter, which tells the number of the sample (with the same name) (x-axis value)\n           tag: Tag, grouping similar values. Values with the same tag will be plotted in the same plot\n\n        Returns:\n            A numpy array image of the figure\n\n        """"""\n        figure = super().show_value(value, name, show=False, counter=counter, tag=tag, *args, **kwargs)\n        figure_image = figure_to_image(figure)\n\n        return figure_image\n\n    @convert_params\n    def show_barplot(self, array, name=None, *args, **kwargs):\n        """"""\n        Creates a bar plot figure from an array\n\n        Args:\n            array: array of shape NxM where N is the number of rows and M is the number of elements in the row.\n            name: The name of the figure\n\n        Returns:\n            A numpy array image of the figure\n        """"""\n\n        figure = super().show_barplot(array, name, show=False, *args, **kwargs)\n        figure_image = figure_to_image(figure)\n\n        return figure_image\n\n    @convert_params\n    def show_lineplot(self, y_vals, x_vals=None, name=None, *args, **kwargs):\n        """"""\n        Creates a line plot figure with (multiple) lines plot, given values Y (and optional the corresponding X values)\n\n        Args:\n            y_vals: Array of shape MxN , where M is the number of points and N is the number of different line\n            x_vals: Has to have the same shape as Y: MxN. For each point in Y it gives the corresponding X value (if\n            not set the points are assumed to be equally distributed in the interval [0, 1] )\n            name: The name of the figure\n\n        Returns:\n            A numpy array image of the figure\n        """"""\n\n        figure = super().show_lineplot(y_vals=y_vals, x_vals=x_vals, name=name, show=False, *args, **kwargs)\n        figure_image = figure_to_image(figure)\n\n        return figure_image\n\n    @convert_params\n    def show_scatterplot(self, array, name=None, *args, **kwargs):\n        """"""\n        Creates a scatter plot figure with the points given in array\n\n        Args:\n            array: A 2d array with size N x dim, where each element i \\in N at X[i] results in a a 2d (if dim = 2)/\n            3d (if dim = 3) point.\n            name: The name of the figure\n\n         Returns:\n            A numpy array image of the figure\n        """"""\n\n        figure = super().show_scatterplot(array, name, show=False, *args, **kwargs)\n        figure_image = figure_to_image(figure)\n\n        return figure_image\n\n    @convert_params\n    def show_piechart(self, array, name=None, *args, **kwargs):\n        """"""\n        Creates a scatter plot figure\n\n        Args:\n            array: Array of positive integers. Each integer will be presented as a part of the pie (with the total\n            as the sum of all integers)\n            name: The name of the figure\n\n        Returns:\n            A numpy array image of the figure\n        """"""\n\n        figure = super().show_piechart(array, name, show=False, *args, **kwargs)\n        figure_image = figure_to_image(figure)\n\n        return figure_image\n'"
trixi/logger/plt/numpyseabornplotlogger.py,0,"b'from collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn\n\nfrom trixi.logger.abstractlogger import AbstractLogger, convert_params\nfrom trixi.util.util import chw_to_hwc\n\n\nclass NumpySeabornPlotLogger(AbstractLogger):\n    """"""\n    Visual logger, inherits the AbstractLogger and plots/ logs numpy arrays/ values as matplotlib / seaborn plots.\n    """"""\n\n    def __init__(self, **kwargs):\n        """"""\n        Initializes a new NumpySeabornPlotLogger\n        """"""\n        super(NumpySeabornPlotLogger, self).__init__(**kwargs)\n\n        self.values = defaultdict(lambda: defaultdict(list))\n        self.max_values = defaultdict(int)\n\n    @convert_params\n    def show_image(self, image, name=None, show=True, *args, **kwargs):\n        """"""\n        Create an image figure\n\n        Args:\n            image: The image array to be displayed\n            name: The name of the image window\n            show: Flag if it should also display the figure (result might also depend on the matplotlib backend )\n\n        Returns:\n            A matplotlib figure\n\n        """"""\n        figure = self.get_figure(name)\n        plt.clf()\n\n        image = chw_to_hwc(image)\n\n        plt.imshow(image)\n        plt.axis(""off"")\n        if show:\n            plt.show(block=False)\n            plt.pause(0.01)\n\n        return figure\n\n    @convert_params\n    def show_value(self, value, name, counter=None, tag=None, show=True, *args, **kwargs):\n        """"""\n       Creates a line plot that is automatically appended with new values and returns it as a figure.\n\n       Args:\n           value: Value to be plotted / appended to the graph (y-axis value)\n           name: The name of the window\n           counter: counter, which tells the number of the sample (with the same name) (x-axis value)\n           tag: Tag, grouping similar values. Values with the same tag will be plotted in the same plot\n           show: Flag if it should also display the figure (result might also depend on the matplotlib backend )\n\n        Returns:\n            A matplotlib figure\n\n               """"""\n        figure = self.get_figure(tag)\n        plt.clf()\n\n        seaborn.set_style(""whitegrid"")\n\n        if tag is None:\n            tag = name\n\n        if counter is None:\n            counter = len(self.values[tag][name]) + 1\n\n        max_val = max(self.max_values[tag], counter)\n        self.max_values[tag] = max_val\n        self.values[tag][name].append((value, max_val))\n\n        for y_tag in self.values[tag]:\n\n            y, x = zip(*self.values[tag][y_tag])\n            plt.plot(x, y, label=y_tag)\n\n        if show:\n            plt.show(block=False)\n            plt.pause(0.01)\n\n        plt.legend()\n        plt.ylabel(name)\n\n        return figure\n\n    @convert_params\n    def show_barplot(self, array, name=None, show=True, *args, **kwargs):\n        """"""\n        Creates a bar plot figure from an array\n\n        Args:\n            array: array of shape NxM where N is the number of rows and M is the number of elements in the row.\n            name: The name of the figure\n            show: Flag if it should also display the figure (result might also depend on the matplotlib backend )\n\n        Returns:\n            A matplotlib figure\n        """"""\n\n        figure = self.get_figure(name)\n\n        y = array\n        x = list(range(1, len(y) + 1))\n\n        seaborn.set_style(""whitegrid"")\n\n        ax = seaborn.barplot(y=y, x=x)\n\n        if show:\n            plt.show(block=False)\n            plt.pause(0.01)\n\n        return figure\n\n    @convert_params\n    def show_boxplot(self, array, name, show=True, *args, **kwargs):\n        """"""\n        Creates a box plot figure from an array\n\n        Args:\n            array: array of shape NxM where N is the number of rows and M is the number of elements in the row.\n            name: The name of the figure\n            show: Flag if it should also display the figure (result might also depend on the matplotlib backend )\n\n        Returns:\n            A matplotlib figure\n        """"""\n\n        figure = self.get_figure(name)\n        seaborn.set_style(""whitegrid"")\n\n        ax = seaborn.boxplot(data=array)\n\n        handles, _ = ax.get_legend_handles_labels()\n        try:\n            legend = kwargs[\'opts\'][\'legend\']\n            ax.legend(handles, legend)\n        except KeyError: # if no legend is defined\n            pass\n        if show:\n            plt.show(block=False)\n            plt.pause(0.01)\n\n        return figure\n\n    @convert_params\n    def show_lineplot(self, y_vals, x_vals=None, name=None, show=True, *args, **kwargs):\n        """"""\n        Creates a line plot figure with (multiple) lines plot, given values Y (and optional the corresponding X values)\n\n        Args:\n            y_vals: Array of shape MxN , where M is the number of points and N is the number of different line\n            x_vals: Has to have the same shape as Y: MxN. For each point in Y it gives the corresponding X value (if\n            not set the points are assumed to be equally distributed in the interval [0, 1] )\n            name: The name of the figure\n            show: Flag if it should also display the figure (result might also depend on the matplotlib backend )\n\n        Returns:\n            A matplotlib figure\n        """"""\n\n        figure = self.get_figure(name)\n        plt.clf()\n\n        seaborn.set_style(""whitegrid"")\n\n        if x_vals is None:\n            x_vals = list(range(len(y_vals)))\n\n        plt.plot(x_vals, y_vals)\n\n        if show:\n            plt.show(block=False)\n            plt.pause(0.01)\n\n        return figure\n\n    @convert_params\n    def show_scatterplot(self, array, name=None, show=True, *args, **kwargs):\n        """"""\n        Creates a scatter plot figure with the points given in array\n\n        Args:\n            array: A 2d array with size N x dim, where each element i \\in N at X[i] results in a a 2d (if dim = 2)/\n            3d (if dim = 3) point.\n            name: The name of the figure\n            show: Flag if it should also display the figure (result might also depend on the matplotlib backend )\n\n        Returns:\n            A matplotlib figure\n        """"""\n\n        if not isinstance(array, np.ndarray):\n            raise TypeError(""Array must be numpy arrays (this class is called NUMPY seaborn logger, and seaborn""\n                            "" can only handle numpy arrays -.- .__. )"")\n        if len(array.shape) != 2:\n            raise ValueError(""Array must be 2D for scatterplot"")\n        if array.shape[1] != 2:\n            raise ValueError(""Array must be 2D and have x,y pairs in the 2nd dim for scatterplot"")\n\n        x, y = zip(*array)\n        x, y = np.asarray(x), np.asarray(y)\n\n        figure = self.get_figure(name)\n        plt.clf()\n\n        ax = seaborn.regplot(x=x, y=y, fit_reg=False)\n\n        if show:\n            plt.show(block=False)\n            plt.pause(0.01)\n\n        return figure\n\n    @convert_params\n    def show_piechart(self, array, name=None, show=True, *args, **kwargs):\n        """"""\n        Creates a scatter plot figure\n\n        Args:\n            array: Array of positive integers. Each integer will be presented as a part of the pie (with the total\n            as the sum of all integers)\n            name: The name of the figure\n            show: Flag if it should also display the figure (result might also depend on the matplotlib backend )\n\n        Returns:\n            A matplotlib figure\n        """"""\n\n        figure = self.get_figure(name)\n        plt.clf()\n\n        plt.pie(array)\n\n        if show:\n            plt.show(block=False)\n            plt.pause(0.01)\n\n        return figure\n\n    def get_figure(self, name):\n        """"""\n        Returns a figure with a given name as identifier.\n\n        If no figure yet exists with the name a new one is created. Otherwise the existing one is returned\n\n        Args:\n            name: Name of the figure\n\n        Returns:\n            A figure with the given name\n        """"""\n\n        return plt.figure(name)\n'"
trixi/logger/tensorboard/__init__.py,0,"b'from trixi.logger.tensorboard.tensorboardlogger import TensorboardLogger\ntry:\n    from trixi.logger.tensorboard.pytorchtensorboardlogger import PytorchTensorboardLogger\nexcept ImportError as e:\n    import warnings\n    warnings.warn(ImportWarning(""Could not import Pytorch related modules:\\n%s""\n        % e.msg))\n'"
trixi/logger/tensorboard/pytorchtensorboardlogger.py,3,"b'import torch\nfrom trixi.logger.tensorboard.tensorboardlogger import TensorboardLogger\n\n\nclass PytorchTensorboardLogger(TensorboardLogger):\n    """"""Abstract interface for visual logger.""""""\n\n    def process_params(self, f, *args, **kwargs):\n        """"""\n        Inherited ""decorator"": convert PyTorch variables and Tensors to numpy arrays\n        """"""\n\n        # convert args\n        args = (a.detach().cpu().numpy() if torch.is_tensor(a) else a for a in args)\n\n        # convert kwargs\n        for key, data in kwargs.items():\n            if torch.is_tensor(data):\n                kwargs[key] = data.detach().cpu().numpy()\n\n        return f(self, *args, **kwargs)\n\n    def __init__(self, *args, **kwargs):\n        super(PytorchTensorboardLogger, self).__init__(*args, **kwargs)\n\n    def plot_model_structure(self, model, input_size):\n        """"""\n         Plots the model structure/ model graph of a pytorch module.\n\n         Args:\n             model: The graph of this model will be plotted.\n             input_size: Input size of the model (with batch dim).\n         """"""\n        inpt_vars = [torch.randn(i_s) for i_s in input_size]\n\n        self.writer.add_graph(model=model, input_to_model=inpt_vars)\n'"
trixi/logger/tensorboard/tensorboardlogger.py,15,"b'import atexit\nimport os\nfrom collections import defaultdict\n\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom trixi.logger.plt.numpyseabornplotlogger import NumpySeabornPlotLogger\nfrom trixi.logger.abstractlogger import convert_params\nfrom trixi.util.util import np_make_grid\n\n\nclass TensorboardLogger(NumpySeabornPlotLogger):\n    """"""Logger that uses tensorboard to log to Tensorboard.""""""\n\n    def __init__(self, target_dir, *args, **kwargs):\n\n        super(TensorboardLogger, self).__init__(*args, **kwargs)\n\n        os.makedirs(target_dir, exist_ok=True)\n\n        self.writer = SummaryWriter(target_dir)\n        self.val_dict = defaultdict(int)\n\n        atexit.register(self.writer.close)\n\n    def show_image(self, image, name=""Image"", counter=None, **kwargs):\n        """"""\n        Sends an image to tensorboard.\n\n        Args:\n            image (np.narray/torch.tensor): Image array/tensor which will be sent\n            name (str): Identifier for the image\n            counter (int): Global step value\n        """"""\n\n        if counter is not None:\n            self.val_dict[""{}-image"".format(name)] = counter\n        else:\n            self.val_dict[""{}-image"".format(name)] += 1\n\n        self.writer.add_image(name, image, global_step=self.val_dict[""{}-image"".format(name)])\n\n    def show_images(self, images, name=""Images"", counter=None, **kwargs):\n        """"""\n        Sends multiple images to tensorboard.\n\n        Args:\n            image (np.narray/torch.tensor): Image array/tensor which will be sent (NxCxHxW)\n            name (str): Identifier for the images\n            counter (int): Global step value\n        """"""\n\n        if counter is not None:\n            self.val_dict[""{}-image"".format(name)] = counter\n        else:\n            self.val_dict[""{}-image"".format(name)] += 1\n\n        self.writer.add_images(name, images, global_step=self.val_dict[""{}-image"".format(name)])\n\n    @convert_params\n    def show_value(self, value, name=""Value"", counter=None, tag=None, **kwargs):\n        """"""\n        Sends a scalar value to tensorboard.\n\n        Args:\n            value (numeric): Value to be sent\n            name (str): Identifier for the value\n            counter (int): Global step value\n            tag (str): Identifier for the frame (values with the same tag will be shown in the same graph)\n        """"""\n\n        if tag is None:\n            key = name + ""-"" + name\n        else:\n            key = tag + ""-"" + name\n\n        if counter is not None and isinstance(counter, int):\n            self.val_dict[f""{key}-image""] = counter\n        else:\n            self.val_dict[f""{key}-image""] += 1\n\n        if tag is not None:\n            self.writer.add_scalars(tag, {name: value}, global_step=self.val_dict[f""{key}-image""])\n            self.writer.scalar_dict = {}\n        else:\n            self.writer.add_scalar(name, value, global_step=self.val_dict[f""{key}-image""])\n\n    def show_text(self, text, name=""Text"", counter=None, **kwargs):\n        """"""\n        Sends text to tensorboard.\n\n        Args:\n            text (str): Text to be sent\n            name (str): Identifier for the text\n            counter (int): Global step value\n        """"""\n\n        if counter is not None:\n            self.val_dict[""{}-text"".format(name)] = counter\n        else:\n            self.val_dict[""{}-text"".format(name)] += 1\n\n        self.writer.add_text(name, text, global_step=self.val_dict[""{}-text"".format(name)])\n\n    @convert_params\n    def show_image_grid(\n        self,\n        image_array,\n        name=""Image-Grid"",\n        counter=None,\n        nrow=8,\n        padding=2,\n        normalize=False,\n        range=None,\n        scale_each=False,\n        pad_value=0,\n        *args,\n        **kwargs,\n    ):\n        """"""\n        Sends an array of images to tensorboard as a grid. Like :meth:`.show_image`, but generates\n        image grid before.\n\n        Args:\n            image_array (np.narray/torch.tensor): Image array/tensor which will be sent as an image grid\n            name (str): Identifier for the image grid\n            counter (int): Global step value\n            nrow (int): Items per row in grid\n            padding (int): Padding between images in grid\n            normalize (bool): Normalize images in grid\n            range (tuple): Tuple (min, max), so images will be normalized to this range\n            scale_each (bool): If True, each image will be normalized separately instead of using\n                min and max of whole tensor\n            pad_value (float): Fill padding with this value\n        """"""\n\n        image_args = dict(\n            nrow=nrow, padding=padding, normalize=normalize, range=range, scale_each=scale_each, pad_value=pad_value\n        )\n\n        if counter is not None:\n            self.val_dict[""{}-image"".format(name)] = counter\n        else:\n            self.val_dict[""{}-image"".format(name)] += 1\n\n        grid = np_make_grid(image_array, **image_args)\n        self.writer.add_image(tag=name, img_tensor=grid, global_step=self.val_dict[""{}-image"".format(name)])\n        self.val_dict[name] += 1\n\n    @convert_params\n    def show_barplot(self, array, name=""barplot"", counter=None, *args, **kwargs):\n        """"""\n        Sends a barplot to tensorboard.\n\n        Args:\n            array (np.array/torch.tensor): array of shape NxM where N is the number of rows and M is the number of elements in the row.\n            name (str): The name of the figure\n            counter (int): Global step value to record\n\n        """"""\n\n        if counter is not None:\n            self.val_dict[""{}-figure"".format(name)] = counter\n        else:\n            self.val_dict[""{}-figure"".format(name)] += 1\n\n        figure = super().show_barplot(array, name, *args, **kwargs)\n        self.writer.add_figure(tag=name, figure=figure, global_step=self.val_dict[""{}-figure"".format(name)])\n\n    @convert_params\n    def show_lineplot(self, y_vals, x_vals=None, name=""lineplot"", counter=None, *args, **kwargs):\n        """"""\n        Sends a lineplot to tensorboard.\n\n        Args:\n            y_vals (np.array/torch.tensor): Array of shape MxN , where M is the number of points and N is the number of different line\n            x_vals (np.array/torch.tensor): Has to have the same shape as Y: MxN. For each point in Y it gives the corresponding X value (if\n                not set the points are assumed to be equally distributed in the interval [0, 1])\n            name (str): The name of the figure\n            counter (int): Global step value to record\n\n        """"""\n\n        if counter is not None:\n            self.val_dict[""{}-figure"".format(name)] = counter\n        else:\n            self.val_dict[""{}-figure"".format(name)] += 1\n\n        figure = super().show_lineplot(y_vals, x_vals, name, *args, **kwargs)\n        self.writer.add_figure(tag=name, figure=figure, global_step=self.val_dict[""{}-figure"".format(name)])\n\n    @convert_params\n    def show_scatterplot(self, array, name=""scatterplot"", counter=None, *args, **kwargs):\n        """"""\n        Sends a scatterplot to tensorboard.\n\n        Args:\n            array (np.array/torch.tensor): An array with size N x dim, where each element i \\in N` at X[i] results in a 2D\n                (if dim = 2) or 3D (if dim = 3) point.\n            name (str): The name of the figure\n            counter (int): Global step value to record\n\n        """"""\n\n        if counter is not None:\n            self.val_dict[""{}-figure"".format(name)] = counter\n        else:\n            self.val_dict[""{}-figure"".format(name)] += 1\n\n        figure = super().show_scatterplot(array, name, *args, **kwargs)\n        self.writer.add_figure(tag=name, figure=figure, global_step=self.val_dict[""{}-figure"".format(name)])\n\n    @convert_params\n    def show_piechart(self, array, name=""piechart"", counter=None, *args, **kwargs):\n        """"""\n        Sends a piechart tensorboard.\n\n        Args:\n            array (np.array/torch.tensor): Array of positive integers. Each integer will be\n                presented as a part of the pie (with the total as the sum of all integers)\n            name (str): The name of the figure\n            counter (int): Global step value to record\n\n        """"""\n\n        if counter is not None:\n            self.val_dict[""{}-figure"".format(name)] = counter\n        else:\n            self.val_dict[""{}-figure"".format(name)] += 1\n\n        figure = super().show_piechart(array, name, *args, **kwargs)\n        self.writer.add_figure(tag=name, figure=figure, global_step=self.val_dict[""{}-figure"".format(name)])\n\n    def show_embedding(self, tensor, labels=None, name=""default"", label_img=None, counter=None, *args, **kwargs):\n        """"""\n        Displays an embedding of a tensor (for more details see tensorboard)\n\n        Args:\n            tensor (torch.tensor/np.array): Tensor to be embedded and then displayed\n            labels (list): List of labels, each element will be converted to string\n            name (str): The name for the embedding\n            label_img (torch.tensor): Images to be displayed at the embedding points\n            counter (int):  Global step value to record\n\n        """"""\n\n        if counter is not None:\n            self.val_dict[""{}-embedding"".format(name)] = counter\n        else:\n            self.val_dict[""{}-embedding"".format(name)] += 1\n\n        self.writer.add_embedding(\n            mat=tensor,\n            metadata=labels,\n            label_img=label_img,\n            tag=name,\n            global_step=self.val_dict[""{}-embedding"".format(name)],\n        )\n\n    def show_histogram(self, array, name=""Histogram"", counter=None, *args, **kwargs):\n        """"""\n        Plots a histogram in the tensorboard histrogram plugin\n\n        Args:\n            array (torch.tensor/np.array): Values to build histogram\n            name (str): Data identifier\n            counter (int):  Global step value to record\n\n        """"""\n\n        if counter is not None:\n            self.val_dict[""{}-histogram"".format(name)] = counter\n        else:\n            self.val_dict[""{}-histogram"".format(name)] += 1\n\n        self.writer.add_histogram(tag=name, values=array, global_step=self.val_dict[""{}-histogram"".format(name)])\n\n    def show_pr_curve(self, tensor, labels, name=""pr-curve"", counter=None, *args, **kwargs):\n        """"""\n        Displays a precision recall curve given a tensor with scores and the corresponding labels\n\n        Args:\n            tensor (torch.tensor/np.array): Tensor with scores (e.g class probabilities)\n            labels (list): Labels of the samples to which the scores match\n            name (str): The name of the plot\n            counter (int): Global step value\n        """"""\n\n        if counter is not None:\n            self.val_dict[""{}-pr-curve"".format(name)] = counter\n        else:\n            self.val_dict[""{}-pr-curve"".format(name)] += 1\n\n        self.writer.add_pr_curve(\n            tag=name, labels=labels, predictions=tensor, global_step=self.val_dict[""{}-pr-curve"".format(name)]\n        )\n\n    # adapted from torch.utils.tensorboard.add_hparams\n    def show_hparams(self, hparam_dict=None, metric_dict=None, counter=None, *args, **kwargs):\n        """"""\n        When using tensorboard\'s hparam-view, it can be filled with this function.\n        It is usually called once per experiment. The values can later be updated by calling show_value with the same\n        name (and WITHOUT tag) as used in metric_dict. It is recommended to choose that name such that it does not\n        coincide with other quantities your logging, because it will also appear in the scalars view\n        (e.g. `hp/loss\' instead of `loss`)\n        Args:\n            hparam_dict: Each key-value pair in the dictionary is the name of the hyper parameter and it\xe2\x80\x99s corresponding value.\n            metric_dict: Each key-value pair in the dictionary is the name of the metric and it\xe2\x80\x99s corresponding value.\n             Note that the key used here should be unique in the tensorboard record. Otherwise the value you added by\n             show_value will be displayed in the hparam plugin. In most cases, this is unwanted.\n             counter (int): Global step value\n        """"""\n        if counter is not None:\n            self.val_dict[""hparams""] = counter\n        else:\n            self.val_dict[""hparams""] += 1\n\n        if type(hparam_dict) is not dict or type(metric_dict) is not dict:\n            raise TypeError(\'hparam_dict and metric_dict should be dictionary.\')\n\n        from torch.utils.tensorboard.summary import hparams\n        exp, ssi, sei = hparams(hparam_dict, metric_dict)\n\n        self.writer.file_writer.add_summary(exp)\n        self.writer.file_writer.add_summary(ssi)\n        self.writer.file_writer.add_summary(sei)\n        for k, v in metric_dict.items():\n            self.writer.add_scalar(k, v, counter)\n\n    def close(self):\n        self.writer.close()\n'"
trixi/logger/visdom/__init__.py,0,"b'from trixi.logger.visdom.numpyvisdomlogger import NumpyVisdomLogger\ntry:\n    from trixi.logger.visdom.pytorchvisdomlogger import PytorchVisdomLogger\nexcept ImportError as e:\n    import warnings\n    warnings.warn(ImportWarning(""Could not import Pytorch related modules:\\n%s""\n        % e.msg))\n'"
trixi/logger/visdom/numpyvisdomlogger.py,0,"b'from __future__ import division, print_function\n\nimport atexit\nfrom collections import defaultdict\nimport inspect\n\nimport os\n\nif os.name == ""nt"":\n    IS_WINDOWS = True\n    from queue import Queue\n    from threading import Thread as Process\nelse:\n    IS_WINDOWS = False\n    from pathos.helpers import mp\n    Queue = mp.Queue\n    Process = mp.Process\nimport sys\nimport traceback\n\nimport numpy as np\n\nfrom trixi.logger.abstractlogger import AbstractLogger, convert_params\nfrom trixi.util import ExtraVisdom\n\n\ndef add_to_queue(func):\n    def wrapper(self, *args, **kwargs):\n        tpl = (func, args, kwargs)\n        self._queue.put_nowait(tpl)\n\n    return wrapper\n\n\nclass NumpyVisdomLogger(AbstractLogger):\n    """"""\n    Visual logger, inherits the AbstractLogger and plots/ logs numpy arrays/ values on a Visdom server.\n    """"""\n\n    def __init__(self, exp_name=""main"", server=""http://localhost"", port=8080, auto_close=True, auto_start=False,\n                 auto_start_ports=(8080, 8000), **kwargs):\n        """"""\n        Creates a new NumpyVisdomLogger object.\n\n        Args:\n            name: The name of the visdom environment\n            server: The address of the visdom server\n            port: The port of the visdom server\n            auto_close: Close all objects and kill process at the end of the python script\n            auto_start: Flag, if it should try to start a visdom server on the given ports\n            auto_start_ports: Ordered list of ports, to try to start a visdom server on (only on the first available\n            port)\n        """"""\n        super(NumpyVisdomLogger, self).__init__(**kwargs)\n\n        if auto_start:\n            auto_port = start_visdom(auto_start_ports)\n            if auto_port != -1:\n                port = auto_port\n                server = ""http://localhost""\n\n        self.name = exp_name\n        self.server = server\n        self.port = port\n\n        self.vis = ExtraVisdom(env=self.name, server=self.server, port=self.port)\n\n        self._value_counter = defaultdict(dict)\n        self._3d_histograms = dict()\n\n        self._queue = Queue()\n        self._process = Process(target=self.__show, args=(self._queue,))\n\n        if auto_close:\n            # atexit.register(self.close_all)\n            if not IS_WINDOWS:\n                atexit.register(self.exit)\n            atexit.register(self.save_vis)\n\n        self._process.start()\n\n    def __show(self, queue):\n        """"""\n        Loop for the internal process to process all visualization tasks\n\n        Args:\n            queue: queue with all visualization tasks\n        """"""\n\n        while True:\n\n            func, args, kwargs = queue.get()\n\n            try:\n\n                func(self, *args, **kwargs)\n\n            except Exception as e:\n\n                error = sys.exc_info()[0]\n                msg = traceback.format_exc()\n                print(""Error {}: {}"".format(error, msg))\n\n    @convert_params\n    @add_to_queue\n    def show_image(self, image, name=None, title=None, caption=None, env_appendix="""", opts=None, **kwargs):\n        """"""\n        Displays an image in a window/pane at the visdom server\n\n        Args:\n            image: The image array to be displayed\n            name: The name of the image window\n            title: The title of the image window\n            caption: The of the image, displayed in the window\n            env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n            opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n        """"""\n\n        if opts is None:\n            opts = {}\n        opts = opts.copy()\n        opts.update(dict(\n            title=title,\n            caption=caption\n        ))\n\n        win = self.vis.image(\n            img=image,\n            win=name,\n            env=self.name + env_appendix,\n            opts=opts\n        )\n\n        return win\n\n    @convert_params\n    @add_to_queue\n    def show_images(self, images, name=None, title=None, caption=None, env_appendix="""", opts=None, **kwargs):\n        """"""\n        Displays multiple images in a window/pane at a visdom server\n\n        Args:\n            images: The image array to be displayed\n            name: The name of the window\n            title: The title of the image window\n            caption: The of the image, displayed in the window\n            env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n            opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n        """"""\n        if opts is None:\n            opts = {}\n        else:\n            if \'nrow\' in opts.keys():\n                nrow = opts[\'nrow\']\n            else:\n                nrow = 8  # as defined in function\n        opts = opts.copy()\n        opts.update(dict(\n            title=title,\n            caption=caption\n        ))\n\n        win = self.vis.images(\n            tensor=images,\n            win=name,\n            env=self.name + env_appendix,\n            opts=opts,\n            # nrow=nrow\n        )\n\n        return win\n\n    @convert_params\n    @add_to_queue\n    def show_value(self, value, name=None, counter=None, tag=None, show_legend=True, env_appendix="""", opts=None,\n                     **kwargs):\n        """"""\n        Creates a line plot that is automatically appended with new values and plots it to a visdom server.\n\n        Args:\n            value: Value to be plotted / appended to the graph (y-axis value)\n            name: The name of the window\n            counter: counter, which tells the number of the sample (with the same name) (x-axis value)\n            tag: Tag, grouping similar values. Values with the same tag will be plotted in the same plot\n            show_legend (bool): Flag, if it should display a legend\n            env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n            opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n        """"""\n\n        if opts is None:\n            opts = {}\n        value_dim = 1\n        if isinstance(value, (list, tuple, np.ndarray)):\n            value_dim = len(value)\n            value = np.asarray([value])\n        else:\n            value = np.asarray([value])\n        if tag is None:\n            tag = """"\n            if name is not None:\n                tag = name\n        if ""showlegend"" not in opts and show_legend:\n            opts[""showlegend""] = True\n\n        up_str = None\n        if tag is not None and tag in self._value_counter:\n            up_str = ""append""\n\n        if tag is not None and tag in self._value_counter and name in self._value_counter[tag]:\n            if counter is None:\n                self._value_counter[tag][name] += 1\n            else:\n                self._value_counter[tag][name] += counter - self._value_counter[tag][name]\n        else:\n            if counter is None:\n                counter = 0\n            if value_dim == 1:\n                self._value_counter[tag][name] = np.array([counter])\n            else:\n                self._value_counter[tag][name] = np.array([[counter] * value_dim])\n\n        opts = opts.copy()\n        opts.update(dict(\n            title=tag,\n        ))\n\n        display_name = tag\n        if value_dim != 1:\n            display_name = None\n\n        win = self.vis.line(\n            Y=value,\n            X=self._value_counter[tag][name],\n            win=tag,\n            name=name,\n            update=up_str,\n            env=self.name + env_appendix,\n            opts=opts\n        )\n\n        return win\n\n    @convert_params\n    @add_to_queue\n    def show_text(self, text, name=None, env_appendix="""", opts=None, **kwargs):\n        """"""\n        Displays a text in a visdom window\n\n        Args:\n             text: The text to be displayed\n             name: The name of the window\n             env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n             opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n        """"""\n\n        text = text.replace(""\\n"", ""<br>"")\n\n        if opts is None:\n            opts = {}\n        win = self.vis.text(\n            text=text,\n            win=name,\n            env=self.name + env_appendix,\n            opts=opts\n        )\n\n        return win\n\n    @convert_params\n    @add_to_queue\n    def show_progress(self, num, total=None, name=None, env_appendix="""", opts=None, **kwargs):\n        """"""\n        Shows the progress as a pie chart.\n\n        Args:\n            num: Current progress. Either a relative value (0 <= num <= 1) or a absolute value if total is given (\n            but has to be smaller than total)\n            total: This if the total number of iterations.\n            name: The name of the window\n            env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n            opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n        """"""\n\n        if opts is None:\n            opts = {}\n        if total is None:\n            if 0 <= num <= 1:\n                total = 1000.\n                num = 1000. * num\n            else:\n                raise AttributeError(""Either num has to be a ratio between 0 and 1 or give a valid total number"")\n        else:\n            if num > total:\n                raise AttributeError(""Num has to be smaller than total"")\n\n        if name is None:\n            name = ""progress""\n\n        x = np.asarray([num, total - num])\n\n        opts = opts.copy()\n        opts.update(dict(\n            legend=[\'Done\', \'To-Do\'],\n            title=""Progress"")\n        )\n\n        win = self.vis.pie(\n            X=x,\n            win=name,\n            env=self.name + env_appendix,\n            opts=opts\n        )\n\n        return win\n\n    @convert_params\n    @add_to_queue\n    def show_histogram(self, array, name=None, bins=30, env_appendix="""", opts=None, **kwargs):\n        """"""\n        Displays the histogramm of an array.\n\n        Args:\n            array: The array the histogram is calculated of\n            name: The name of the window\n            bins: Number of bins (== bars) in the histogram\n            env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n            opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n        """"""\n        if opts is None:\n            opts = {}\n        opts = opts.copy()\n        opts.update(dict(\n            title=name,\n            numbins=bins\n        ))\n\n        win = self.vis.histogram(\n            X=array,\n            win=name,\n            env=self.name + env_appendix,\n            opts=opts\n        )\n\n        return win\n\n    @convert_params\n    @add_to_queue\n    def __show_histogram_3d(self, array, name, bins=50, env_appendix="""", opts=None, **kwargs):\n        """"""\n        Displays a history of histograms as consequtive lines in a 3d space (similar to tensorflow)\n\n        Args:\n            array: New sample of the array that should be plotted (i.e. results in one line of the 3d histogramm)\n            name: The name of the window\n            bins: Number of bins in the histogram\n            env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n            opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n        """"""\n\n        if opts is None:\n            opts = {}\n        if name not in self._3d_histograms:\n            self._3d_histograms[name] = []\n\n        self._3d_histograms[name].append(array)\n\n        if len(self._3d_histograms[name]) > 50:\n\n            n_histo = len(self._3d_histograms[name]) - 20\n            every_n = n_histo // 30 + 1\n\n            x = self._3d_histograms[name][:-20][0::every_n] + self._3d_histograms[name][-20:]\n        else:\n            x = self._3d_histograms[name]\n\n        opts = opts.copy()\n        opts.update(dict(\n            title=name,\n            numbins=bins\n        ))\n\n        win = self.vis.histogram_3d(\n            X=x,\n            win=name,\n            env=self.name + env_appendix,\n            opts=opts\n        )\n\n        return win\n\n    @convert_params\n    @add_to_queue\n    def show_barplot(self, array, legend=None, rownames=None, name=None, env_appendix="""", opts=None, **kwargs):\n        """"""\n        Displays a bar plot from an array\n\n        Args:\n            array: array of shape NxM where N is the nomber of rows and M is the number of elements in the row.\n            legend: list of legend names. Has to have M elements.\n            rownames: list of row names. Has to have N elements.\n            name: The name of the window\n            env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n            opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n        """"""\n\n        if opts is None:\n            opts = {}\n        opts = opts.copy()\n        opts.update(dict(\n            stacked=False,\n            legend=legend,\n            rownames=rownames,\n            title=name\n        ))\n\n        win = self.vis.bar(\n            X=array,\n            win=name,\n            env=self.name + env_appendix,\n            opts=opts\n        )\n\n        return win\n\n    @convert_params\n    @add_to_queue\n    def show_lineplot(self, y_vals, x_vals=None, name=None, env_appendix="""", show_legend=True, opts=None, **kwargs):\n        """"""\n        Displays (multiple) lines plot, given values Y (and optional the corresponding X values)\n\n        Args:\n            y_vals: Array of shape MxN , where M is the number of points and N is the number of different line\n            x_vals: Has to have the same shape as Y: MxN. For each point in Y it gives the corresponding X value (if\n            not set the points are assumed to be equally distributed in the interval [0, 1] )\n            name: The name of the window\n            env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n            opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n        """"""\n\n        if opts is None:\n            opts = {}\n        opts = opts.copy()\n        opts.update(dict(\n            title=name,\n        ))\n\n        if ""showlegend"" not in opts and show_legend:\n            opts[""showlegend""] = True\n\n        win = self.vis.line(\n            X=x_vals,\n            Y=y_vals,\n            win=name,\n            env=self.name + env_appendix,\n            opts=opts\n        )\n\n        return win\n\n    @convert_params\n    @add_to_queue\n    def show_boxplot(self, x_vals, name=None, env_appendix="""", opts=None, **kwargs):\n        """"""\n        Displays a box plot, given values X\n\n        Args:\n            x_vals: Array of shape MxN , where M is the number of points and N are the number of groups\n            name: The name of the window\n            env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n            opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n        """"""\n\n        if opts is None:\n            opts = {}\n        opts = opts.copy()\n        opts.update(dict(\n            title=name,\n        ))\n\n        win = self.vis.boxplot(\n            X=x_vals,\n            win=name,\n            env=self.name + env_appendix,\n            opts=opts\n        )\n\n        return win\n\n    @convert_params\n    @add_to_queue\n    def show_surfaceplot(self, x_vals, name=None, env_appendix="""", opts=None, **kwargs):\n        """"""\n        Displays a surface plot given values X\n\n        Args:\n            x_vals: Array of shape MxN\n            name: The name of the window\n            env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n            opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n        """"""\n\n        if opts is None:\n            opts = {}\n            opts.update(dict(\n                title=name,\n                colormap=\'Viridis\',\n                xlabel=\'X\',\n                ylabel=\'Y\'\n            ))\n        opts = opts.copy()\n\n        win = self.vis.surf(X=x_vals,\n                            win=name,\n                            env=self.name + env_appendix,\n                            opts=opts, )\n\n        return win\n\n    @convert_params\n    @add_to_queue\n    def show_contourplot(self, x_vals, name=None, env_appendix="""", opts=None, **kwargs):\n        """"""\n        Displays a contour plot\n\n        Args:\n            x_vals: Array of shape MxN\n            name: The name of the window\n            env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n            opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n        """"""\n\n        if opts is None:\n            opts = {}\n            opts.update(dict(\n                title=name,\n                colormap=\'Viridis\',\n                xlabel=\'feature\',\n                ylabel=\'batch\'\n            ))\n        opts = opts.copy()\n\n        win = self.vis.contour(X=x_vals,\n                               win=name,\n                               env=self.name + env_appendix,\n                               opts=opts, )\n\n        return win\n\n    @convert_params\n    @add_to_queue\n    def show_scatterplot(self, array, labels=None, name=None, env_appendix="""", opts=None, **kwargs):\n        """"""\n        Displays a scatter plots, with the points given in array\n\n        Args:\n            array: A 2d array with size N x dim, where each element i \\in N at X[i] results in a a 2d (if dim = 2)/\n            3d (if dim = 3) point.\n            labels: For each point a int label (starting from 1) can be given. Has to be an array of size N.\n            name: The name of the window\n            env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n            opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n        """"""\n\n        if opts is None:\n            opts = {}\n        opts.update(dict(\n            title=name,\n        ))\n\n        win = self.vis.scatter(\n            X=array,\n            Y=labels,\n            win=name,\n            env=self.name + env_appendix,\n            opts=opts\n        )\n\n        return win\n\n    @convert_params\n    @add_to_queue\n    def show_piechart(self, array, name=None, env_appendix="""", opts=None, **kwargs):\n        """"""\n        Displays a pie chart.\n\n        Args:\n            array: Array of positive integers. Each integer will be presented as a part of the pie (with the total\n            as the sum of all integers)\n            name: The name of the window\n            env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n            opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n        """"""\n\n        if opts is None:\n            opts = {}\n        opts.update(dict(\n            title=name\n        ))\n\n        win = self.vis.pie(\n            X=array,\n            win=name,\n            env=self.name + env_appendix,\n            opts=opts\n        )\n\n        return win\n\n    @convert_params\n    @add_to_queue\n    def show_svg(self, svg, name=None, env_appendix="""", opts=None, **kwargs):\n        """"""\n        Displays a svg file.\n\n        Args:\n            svg: Filename of the svg file which should be displayed\n            name: The name of the window\n            env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n            opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n        """"""\n\n        if opts is None:\n            opts = {}\n        with open(svg, \'r\') as fileobj:\n            svgstr = fileobj.read()\n\n        opts.update(dict(\n            title=name\n        ))\n\n        win = self.vis.svg(\n            svgstr=svgstr,\n            win=name,\n            env=self.name + env_appendix,\n            opts=opts\n        )\n\n        return win\n\n    def show_values(self, val_dict):\n        """"""A util function for multiple values. Simple plots all values in a dict, there the window name is the key\n        in the dict and the plotted value is the value of a dict (Simply calls the show_value function).\n\n        Args:\n            val_dict: Dict with key, values pairs which will be plotted\n        """"""\n\n        for name, value in val_dict:\n            self.show_value(value, name)\n\n    @convert_params\n    @add_to_queue\n    def add_to_graph(self, y_vals, x_vals=None, name=None, legend_name=None, append=True, env_appendix="""", opts=None,\n                       **kwargs):\n        """"""\n        Displays (multiple) lines plot, given values Y (and optional the corresponding X values)\n\n        Args:\n            y_vals: Array of shape MxN , where M is the number of points and N is the number of different line\n            x_vals: Has to have the same shape as Y: MxN. For each point in Y it gives the corresponding X value (if\n            not set the points are assumed to be equally distributed in the interval [0, 1] )\n            name: The name of the window\n            env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n            opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n        """"""\n\n        if opts is None:\n            opts = {}\n        opts.update(dict(\n            title=name,\n        ))\n\n        win = self.vis.updateTrace(\n            X=x_vals,\n            Y=y_vals,\n            win=name,\n            name=legend_name,\n            append=append,\n            env=self.name + env_appendix,\n            opts=opts\n        )\n\n        return win\n\n    @convert_params\n    @add_to_queue\n    def show_matplot_plt(self, plt, name=None, title=None, caption=None, env_appendix="""", opts=None, **kwargs):\n        """"""\n        Displays an matplotlib figure in a window/pane at the visdom server\n\n        Args:\n            plt: The matplotlib figure/plot to be displayed\n            name: The name of the image window\n            title: The title of the image window\n            caption: The of the image, displayed in the window\n            env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n            opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n        """"""\n\n        if opts is None:\n            opts = {}\n        opts = opts.copy()\n        opts.update(dict(\n            title=title,\n            caption=caption\n        ))\n\n        win = self.vis.matplot(\n            plot=plt,\n            win=name,\n            env=self.name + env_appendix,\n            opts=opts\n        )\n\n        return win\n\n    @convert_params\n    @add_to_queue\n    def show_plotly_plt(self, figure, name=None, env_appendix="""", **kwargs):\n        """"""\n        Displays an plotly figure in a window/pane at the visdom server\n\n        Args:\n            figure: The plotly figure/plot to be displayed\n            name: The name of the image window\n            title: The title of the image window\n            caption: The of the image, displayed in the window\n            env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n        """"""\n\n        win = self.vis.plotlyplot(\n            figure=figure,\n            win=name,\n            env=self.name + env_appendix,\n        )\n\n        return win\n\n    @convert_params\n    @add_to_queue\n    def send_data(self, data, name=None, layout=None, endpoint=\'events\', append=False, **kwargs):\n        """"""\n        Sends data to a visdom server.\n\n        Args:\n            data: The data to be send (has to be in the plotly / visdom format, see the offical visdom github page)\n            name: The name of the window\n            layout: Layout of the data\n            endpoint: Endpoint to recieve the data (or at least to which visdom endpoint to send it to)\n            append: Flag, if data should be appended\n        """"""\n\n        if layout is None:\n            layout = {}\n\n        if not isinstance(data, (list, type)):\n            data = [data]\n\n        win = self.vis._send({\'data\': data,\n                              \'layout\': layout,\n                              \'win\': name,\n                              \'append\': append}, endpoint=endpoint)\n\n        return win\n\n    def close_all(self):\n        """"""Closes all visdom windows.""""""\n        self.vis.close()\n\n    def save_vis(self):\n        self.vis.save([self.name])\n\n    def exit(self):\n        """"""Kills the internal process.""""""\n        if self._process is not None:\n            self._process.terminate()\n\n\n\ndef start_visdom(port_list=(8080, 8000)):\n    """"""\n    Starts a visdom server on a given port\n\n    Args:\n        port_list: Priority list of port. Will start a visdom server on the first available port.\n\n    """"""\n    import time\n    from multiprocessing import Process\n    import visdom.server\n\n    from trixi.util import PyLock\n\n    lock_id = ""visdom_lock""\n\n    def is_port_available(port, verbose=False):\n\n        import socket\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        try:\n            s.bind((""127.0.0.1"", port))\n        except socket.error as e:\n            if e.errno == 98:\n                if verbose:\n                    print(""Port {} is already in use"".format(port))\n            else:\n                if verbose:\n                    print(e)\n            s.close()\n            return False\n        s.close()\n        return True\n\n    def _start_visdom(port):\n        p = Process(target=visdom.server.start_server, kwargs={""port"": port, ""base_url"": \'\'})\n        p.start()\n        atexit.register(p.terminate)\n        time.sleep(20)\n        return True\n\n    lock = PyLock(lock_id, timeout=10)\n\n    i = 0\n    while i < len(port_list):\n        port = port_list[i]\n\n        try:\n            lock.__enter__()\n            if is_port_available(port):\n                if _start_visdom(port):\n                    lock.__exit__(0, 0, 0)\n                    print(""Started Visdom on Port:"", port)\n                    return port\n            else:\n                i += 1\n        except Exception:\n            pass\n\n    return -1\n'"
trixi/logger/visdom/pytorchvisdomlogger.py,12,"b'import atexit\nimport tempfile\nimport warnings\nfrom multiprocessing import Process\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport numpy as np\nimport torch\n\nfrom torch.autograd import Variable\nfrom torchvision.utils import make_grid\n\nfrom trixi.util.util import np_make_grid, get_tensor_embedding\nfrom trixi.logger.experiment.pytorchexperimentlogger import PytorchExperimentLogger\nfrom trixi.logger.visdom.numpyvisdomlogger import NumpyVisdomLogger, add_to_queue\nfrom trixi.logger.abstractlogger import convert_params\nfrom trixi.util.pytorchutils import get_guided_image_gradient, get_smooth_image_gradient, get_vanilla_image_gradient\n\n\nfrom functools import wraps\n\n\ndef move_to_cpu(fn):\n    """"""Decorator to call the process_params method of the class.""""""\n\n    def __process_params(*args, **kwargs):\n        ### convert args\n        args = (a.detach().cpu() if torch.is_tensor(a) else a for a in args)\n        ### convert kwargs\n        for key, data in kwargs.items():\n            if torch.is_tensor(data):\n                kwargs[key] = data.detach().cpu()\n        return fn(*args, **kwargs)\n\n    # # @wraps(f)\n    # def wrapper(*args, **kwargs):\n    #     return __process_params(fn, *args, **kwargs)\n\n    return __process_params\n\nclass PytorchVisdomLogger(NumpyVisdomLogger):\n    """"""\n    Visual logger, inherits the NumpyVisdomLogger and plots/ logs pytorch tensors and variables on a Visdom server.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(PytorchVisdomLogger, self).__init__(*args, **kwargs)\n\n    def process_params(self, f, *args, **kwargs):\n        """"""\n        Inherited ""decorator"": convert Pytorch variables and Tensors to numpy arrays.\n        """"""\n\n        ### convert args\n        args = (a.detach().cpu().numpy() if torch.is_tensor(a) else a for a in args)\n        # args = (a.data.cpu().numpy() if isinstance(a, Variable) else a for a in args)\n\n        ### convert kwargs\n        for key, data in kwargs.items():\n            # if isinstance(data, Variable):\n            #     kwargs[key] = data.detach().cpu().numpy()\n            if torch.is_tensor(data):\n                kwargs[key] = data.detach().cpu().numpy()\n\n        return f(self, *args, **kwargs)\n\n\n    @move_to_cpu\n    def plot_model_statistics(self, model, env_appendix="""", model_name="""", plot_grad=False, **kwargs):\n        """"""\n        Plots statstics (mean, std, abs(max)) of the weights or the corresponding gradients of a model as a barplot.\n\n        Args:\n            model: Model with the weights.\n            env_appendix: Visdom environment name appendix\n            model_name: Name of the model (is used as window name).\n            plot_grad: If false plots weight statistics, if true plot the gradients of the weights.\n        """"""\n\n        means = []\n        stds = []\n        maxmin = []\n        legendary = []\n        for i, (m_param_name, m_param) in enumerate(model.named_parameters()):\n            win_name = ""%s_params"" % model_name\n            if plot_grad:\n                m_param = m_param.grad\n                win_name = ""%s_grad"" % model_name\n\n            if m_param is not None:\n                param_mean = m_param.detach().mean().item()\n                param_std = m_param.detach().std().item()\n\n                if np.isnan(param_std):\n                    param_std = 0\n\n                means.append(param_mean)\n                stds.append(param_std)\n                maxmin.append(torch.max(torch.abs(m_param)).item())\n                legendary.append(""%s-%s"" % (model_name, m_param_name))\n\n        self.show_barplot(name=win_name, array=np.asarray([means, stds, maxmin]), legend=legendary,\n                          rownames=[""mean"", ""std"", ""max""], env_appendix=env_appendix)\n\n    def plot_model_statistics_weights(self, model, env_appendix="""", model_name="""", **kwargs):\n        """"""\n        Plots statstics (mean, std, abs(max)) of the weights of a model as a barplot (uses plot model statistics with plot_grad=False).\n\n        Args:\n            model: Model with the weights.\n            env_appendix: Visdom environment name appendix\n            model_name: Name of the model (is used as window name).\n        """"""\n        self.plot_model_statistics(model=model, env_appendix=env_appendix, model_name=model_name, plot_grad=False)\n\n    def plot_model_statistics_grads(self, model, env_appendix="""", model_name="""", **kwargs):\n        """"""\n        Plots statstics (mean, std, abs(max)) of the gradients of a model as a barplot (uses plot model statistics with plot_grad=True).\n\n        Args:\n            model: Model with the weights and the corresponding gradients (have to calculated previously).\n            env_appendix: Visdom environment name appendix\n            model_name: Name of the model (is used as window name).\n        """"""\n        self.plot_model_statistics(model=model, env_appendix=env_appendix, model_name=model_name, plot_grad=True)\n\n    def plot_model_gradient_flow(self, model, name=""model"", title=None):\n        """"""\n        Plots statstics (mean, std, abs(max)) of the weights or the corresponding gradients of a model as a barplot.\n\n        Args:\n            model: Model with the weights.\n            env_appendix: Visdom environment name appendix, if none is given, it uses ""-histogram"".\n            model_name: Name of the model (is used as window name).\n            plot_grad: If false plots weight statistics, if true plot the gradients of the weights.\n        """"""\n        ave_grads = []\n        layers = []\n\n        named_parameters = model.named_parameters()\n        for n, p in named_parameters:\n            if (p.requires_grad) and (""bias"" not in n):\n                layers.append(n)\n                ave_grads.append(p.grad.abs().mean())\n\n        plt.figure()\n        plt.plot(ave_grads, alpha=0.3, color=""b"")\n        plt.hlines(0, 0, len(ave_grads) + 1, linewidth=1, color=""k"")\n        plt.xticks(range(0, len(ave_grads), 1), layers, rotation=""vertical"")\n        plt.xlim(xmin=0, xmax=len(ave_grads))\n        plt.xlabel(""Layers"")\n        plt.ylabel(""average gradient {}"".format(name))\n        plt.title(""Gradient flow"")\n        plt.grid(True)\n\n        self.show_matplot_plt(plt.gcf(), name=name, title=title)\n\n    def plot_mutliple_models_statistics_weights(self, model_dict, env_appendix=None, **kwargs):\n        """"""\n        Given models in a dict, plots the weight statistics of the models.\n\n        Args:\n            model_dict: Dict with models, the key is assumed to be the name, while the value is the model.\n            env_appendix: Visdom environment name appendix\n        """"""\n        for model_name, model in model_dict.items():\n            self.plot_model_statistics_weights(model=model, env_appendix=env_appendix, model_name=model_name)\n\n    def plot_mutliple_models_statistics_grads(self, model_dict, env_appendix="""", **kwargs):\n        """"""\n        Given models in a dict, plots the gradient statistics of the models.\n\n        Args:\n            model_dict: Dict with models, the key is assumed to be the name, while the value is the model.\n            env_appendix: Visdom environment name appendix\n        """"""\n        for model_name, model in model_dict.items():\n            self.plot_model_statistics_grads(model=model, env_appendix=env_appendix, model_name=model_name)\n\n    def plot_model_structure(self, model, input_size, name=""model_structure"", use_cuda=True, delete_tmp_on_close=False, forward_kwargs=None, **kwargs):\n        """"""\n        Plots the model structure/ model graph of a pytorch module (this only works correctly with pytorch 0.2.0).\n\n        Args:\n            model: The graph of this model will be plotted.\n            input_size: Input size of the model (with batch dim).\n            name: The name of the window in the visdom env.\n            use_cuda: Perform model dimension calculations on the gpu (cuda).\n            delete_tmp_on_close: Determines if the tmp file will be deleted on close. If set true, can cause problems due to the multi threadded plotting.\n        """"""\n\n        if not hasattr(input_size[0], ""__iter__""):\n            input_size = [input_size, ]\n\n        if not torch.cuda.is_available():\n            use_cuda = False\n\n        if forward_kwargs is None:\n            forward_kwargs = {}\n\n        def make_dot(output_var, state_dict=None):\n            """"""\n            Produces Graphviz representation of Pytorch autograd graph.\n            Blue nodes are the Variables that require grad, orange are Tensors\n            saved for backward in torch.autograd.Function.\n\n            Args:\n                output_var: output Variable\n                state_dict: dict of (name, parameter) to add names to node that require grad\n            """"""\n            from graphviz import Digraph\n\n            if state_dict is not None:\n                # assert isinstance(params.values()[0], Variable)\n                param_map = {id(v): k for k, v in state_dict.items()}\n\n            node_attr = dict(style=\'filled\',\n                             shape=\'box\',\n                             align=\'left\',\n                             fontsize=\'12\',\n                             ranksep=\'0.1\',\n                             height=\'0.2\')\n            dot = Digraph(node_attr=node_attr, graph_attr=dict(size=""12,12""), format=""svg"")\n            seen = set()\n\n            def size_to_str(size):\n                return \'(\' + (\', \').join([\'%d\' % v for v in size]) + \')\'\n\n            def add_nodes(var):\n                if var not in seen:\n                    if torch.is_tensor(var):\n                        dot.node(str(id(var)), size_to_str(var.size()), fillcolor=\'orange\')\n                    elif hasattr(var, \'variable\'):\n                        u = var.variable\n                        if state_dict is not None and id(u.data) in param_map:\n                            node_name = param_map[id(u.data)]\n                        else:\n                            node_name = """"\n                        node_name = \'%s\\n %s\' % (node_name, size_to_str(u.size()))\n                        dot.node(str(id(var)), node_name, fillcolor=\'lightblue\')\n                    else:\n                        node_name = str(type(var).__name__)\n                        if node_name.endswith(""Backward""):\n                            node_name = node_name[:-8]\n                        dot.node(str(id(var)), node_name)\n                    seen.add(var)\n                    if hasattr(var, \'next_functions\'):\n                        for u in var.next_functions:\n                            if u[0] is not None:\n                                dot.edge(str(id(u[0])), str(id(var)))\n                                add_nodes(u[0])\n                    if hasattr(var, \'saved_tensors\'):\n                        for t in var.saved_tensors:\n                            dot.edge(str(id(t)), str(id(var)))\n                            add_nodes(t)\n\n            add_nodes(output_var.grad_fn)\n            return dot\n\n        # Create input\n        inpt_vars = [torch.randn(i_s) for i_s in input_size]\n        if use_cuda:\n            if next(model.parameters()).is_cuda:\n                device = next(model.parameters()).device.index\n            else:\n                device = None\n            inpt_vars = [i_v.cuda(device) for i_v in inpt_vars]\n            model = model.cuda(device)\n\n        # get output\n        output = model(*inpt_vars, **forward_kwargs)\n\n        # get temp file to store svg in\n        fp = tempfile.NamedTemporaryFile(suffix="".svg"", delete=delete_tmp_on_close)\n        g = make_dot(output, model.state_dict())\n\n        try:\n\n            # Create model graph and store it as svg\n            x = g.render(fp.name[:-4], cleanup=True)\n\n            # Display model graph in visdom\n            self.show_svg(svg=x, name=name)\n        except Exception as e:\n            warnings.warn(""Could not render model, make sure the Graphviz executables are on your system."")\n\n    @move_to_cpu\n    @add_to_queue\n    def show_image_grid(self, tensor, name=None, caption=None, env_appendix="""", opts=None,\n                          image_args=None, **kwargs):\n        """"""\n        Calls the save image grid method (for abstract logger combatibility)\n\n        Args:\n           images: 4d- tensor (N, C, H, W)\n           name: The name of the window\n           caption: Caption of the generated image grid\n           env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n           opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n           image_args: Arguments for the tensorvision save image method\n\n\n        """"""\n\n        if opts is None: opts = {}\n        if image_args is None: image_args = {}\n\n        if isinstance(tensor, Variable):\n\n            tensor = tensor.detach()\n\n        if torch.is_tensor(tensor):\n            assert torch.is_tensor(tensor), ""tensor has to be a pytorch tensor or variable""\n            assert tensor.dim() == 4, ""tensor has to have 4 dimensions""\n            if not (tensor.size(1) == 1 or tensor.size(1) == 3):\n                warnings.warn(""The 1. dimension (channel) has to be either 1 (gray) or 3 (rgb), taking the first ""\n                              ""dimension now !!!"")\n                tensor = tensor[:, 0:1, ]\n\n            tensor_np = tensor.numpy()\n            grid = np_make_grid(tensor_np, **image_args)\n            image = np.clip(grid * 255, a_min=0, a_max=255)\n            image = image.astype(np.uint8)\n            # grid = make_grid(tensor, **image_args)\n            # image = grid.mul(255).clamp(0, 255).byte().numpy()\n        elif isinstance(tensor, np.ndarray):\n            grid = np_make_grid(tensor, **image_args)\n            image = np.clip(grid * 255, a_min=0, a_max=255)\n            image = image.astype(np.uint8)\n\n        else:\n            raise ValueError(""Tensor has to be a torch tensor or a numpy array"")\n\n        opts = opts.copy()\n        opts.update(dict(\n            title=name,\n            caption=caption\n        ))\n\n        win = self.vis.image(\n            img=image,\n            win=name,\n            env=self.name + env_appendix,\n            opts=opts\n        )\n\n        return win\n\n\n    @convert_params\n    @add_to_queue\n    def show_image_grid_heatmap(self, heatmap, background=None, ratio=0.3, colormap=cm.jet,\n                                  normalize=True, name=""heatmap"", caption=None,\n                                  env_appendix="""", opts=None, image_args=None, **kwargs):\n        """"""\n        Creates heat map from the given map and if given combines it with the background and then\n        displays results with as image grid.\n\n        Args:\n           heatmap:  4d- tensor (N, C, H, W), if C = 3, colormap won\'t be applied.\n           background: 4d- tensor (N, C, H, W)\n           name: The name of the window\n           ratio: The ratio to mix the map with the background (0 = only background, 1 = only map)\n           caption: Caption of the generated image grid\n           env_appendix: appendix to the environment name, if used the new env is env+env_appendix\n           opts: opts dict for the ploty/ visdom plot, i.e. can set window size, en/disable ticks,...\n           image_args: Arguments for the tensorvision save image method\n\n        """"""\n\n        if opts is None:\n            opts = {}\n        if image_args is None:\n            image_args = {}\n        if ""normalize"" not in image_args:\n            image_args[""normalize""] = normalize\n\n        # if len(heatmap.shape) != 4:\n        #     raise IndexError(""\'heatmap\' must have dimensions BxCxHxW!"")\n\n        map_grid = np_make_grid(heatmap, normalize=normalize)  # map_grid.shape is (3, X, Y)\n        if heatmap.shape[1] != 3:\n            map_ = colormap(map_grid[0])[..., :-1].transpose(2, 0, 1)\n        else:  # heatmap was already RGB, so don\'t apply colormap\n            map_ = map_grid\n\n        if background is not None:\n            img_grid = np_make_grid(background, **image_args)\n            fuse_img = (1.0 - ratio) * img_grid + ratio * map_\n        else:\n            fuse_img = map_\n\n        fuse_img = np.clip(fuse_img * 255, a_min=0, a_max=255).astype(np.uint8)\n\n        opts = opts.copy()\n        opts.update(dict(\n            title=name,\n            caption=caption\n        ))\n\n        win = self.vis.image(\n            img=fuse_img,\n            win=name,\n            env=self.name + env_appendix,\n            opts=opts\n        )\n\n        return win\n\n\n    @convert_params\n    def show_embedding(self, tensor, labels=None, name=None, method=""tsne"", n_dims=2, n_neigh=30, meth_args=None,\n                       *args, **kwargs):\n        """"""\n        Displays a tensor a an embedding\n\n        Args:\n            tensor: Tensor to be embedded an then displayed\n            labels: Labels of the entries in the tensor (first dimension)\n            name: The name of the window\n            method: Method used for embedding, options are: tsne, standard, ltsa, hessian, modified, isomap, mds,\n            spectral, umap\n            n_dims: dimensions to embed the data into\n            n_neigh: Neighbour parameter to kind of determin the embedding (see t-SNE for more information)\n            meth_args: Further arguments which can be passed to the embedding method\n\n        """"""\n\n        if meth_args is None:\n            meth_args = {}\n\n        def __show_embedding(queue, tensor, labels=None, name=None, method=""tsne"", n_dims=2, n_neigh=30, **meth_args):\n            emb_data = get_tensor_embedding(tensor, method=method, n_dims=n_dims, n_neigh=n_neigh, **meth_args)\n\n            vis_task = {\n                ""type"": ""scatterplot"",\n                ""array"": emb_data,\n                ""labels"": labels,\n                ""name"": name,\n                ""env_appendix"": """",\n                ""opts"": {}\n            }\n            queue.put_nowait(vis_task)\n\n        p = Process(target=__show_embedding, kwargs=dict(queue=self._queue,\n                                                         tensor=tensor,\n                                                         labels=labels,\n                                                         name=name,\n                                                         method=method,\n                                                         n_dims=n_dims,\n                                                         n_neigh=n_neigh,\n                                                         **meth_args\n                                                         ))\n        atexit.register(p.terminate)\n        p.start()\n\n    @convert_params\n    def show_roc_curve(self, tensor, labels, name, reduce_to_n_samples=None, use_sub_process=False):\n        """"""\n        Displays a roc curve given a tensor with scores and the coresponding labels\n\n        Args:\n            tensor: Tensor with scores (e.g class probability )\n            labels: Labels of the samples to which the scores match\n            name: The name of the window\n            reduce_to_n_samples: Reduce/ downsample to to n samples for fewer data points\n            use_sub_process: Use a sub process to do the processing\n\n        """"""\n\n        res_fn = lambda tpr, fpr: self.show_lineplot(tpr, fpr, name=name, opts={""fillarea"": True,\n                                                                                ""webgl"": True})\n        PytorchExperimentLogger.get_roc_curve(tensor=tensor, labels=labels, reduce_to_n_samples=reduce_to_n_samples,\n                                              use_sub_process=use_sub_process, results_fn=res_fn)\n\n    @convert_params\n    def show_pr_curve(self, tensor, labels, name, reduce_to_n_samples=None, use_sub_process=False):\n        """"""\n        Displays a precision recall curve given a tensor with scores and the coresponding labels\n\n        Args:\n            tensor: Tensor with scores (e.g class probability )\n            labels: Labels of the samples to which the scores match\n            name: The name of the window\n            reduce_to_n_samples: Reduce/ downsample to to n samples for fewer data points\n            use_sub_process: Use a sub process to do the processing\n        """"""\n        res_fn = lambda precision, recall: self.show_lineplot(precision, recall, name=name, opts={""fillarea"": True,\n                                                                                                  ""webgl"": True})\n        PytorchExperimentLogger.get_pr_curve(tensor=tensor, labels=labels, reduce_to_n_samples=reduce_to_n_samples,\n                                             use_sub_process=use_sub_process, results_fn=res_fn)\n\n    @convert_params\n    def show_classification_metrics(self, tensor, labels, name, metric=(""roc-auc"", ""pr-score""),\n                                    use_sub_process=False, tag_name=None):\n        """"""\n        Displays some classification metrics as line plots in a graph (similar to show value (also uses show value\n        for the caluclated values))\n\n        Args:\n            tensor: Tensor with scores (e.g class probability )\n            labels: Labels of the samples to which the scores match\n            name: The name of the window\n            metric: List of metrics to calculate. Options are: roc-auc, pr-auc, pr-score, mcc, f1\n\n        Returns:\n\n        """"""\n\n        res_fn = lambda val, name, tag: self.show_value(val, name=name, tag=tag)\n        PytorchExperimentLogger.get_classification_metrics(tensor=tensor, labels=labels, name=name, metric=metric,\n                                                           use_sub_process=use_sub_process, tag_name=tag_name,\n                                                           results_fn=res_fn)\n\n    def show_image_gradient(self, model, inpt, err_fn, grad_type=""vanilla"", n_runs=20, eps=0.1,\n                            abs=False, **image_grid_params):\n        """"""\n        Given a model creates calculates the error and backpropagates it to the image and saves it (saliency map).\n\n        Args:\n            model: The model to be evaluated\n            inpt: Input to the model\n            err_fn: The error function the evaluate the output of the model on\n            grad_type: Gradient calculation method, currently supports (vanilla, vanilla-smooth, guided,\n            guided-smooth) ( the guided backprob can lead to segfaults -.-)\n            n_runs: Number of runs for the smooth variants\n            eps: noise scaling to be applied on the input image (noise is drawn from N(0,1))\n            abs (bool): Flag, if the gradient should be a absolute value\n            **image_grid_params: Params for make image grid.\n\n        """"""\n        grad = PytorchExperimentLogger.get_input_gradient(model=model, inpt=inpt, err_fn=err_fn, grad_type=grad_type,\n                                                          n_runs=n_runs, eps=eps, abs=abs)\n        self.show_image_grid(grad, **image_grid_params)\n\n    def show_video(self, frame_list=None, name=""frames"", dim=""LxHxWxC"", scale=1.0, fps=25):\n        self.vis.video(tensor=np.array(frame_list), dim=dim, opts={\'fps\': fps})\n'"
