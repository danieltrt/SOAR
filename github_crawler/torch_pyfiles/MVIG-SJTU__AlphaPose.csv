file_path,api_count,code
setup.py,1,"b'import os\nimport platform\nimport subprocess\nimport time\n\nimport numpy as np\nfrom Cython.Build import cythonize\nfrom setuptools import Extension, find_packages, setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nMAJOR = 0\nMINOR = 3\nPATCH = 0\nSUFFIX = \'\'\nSHORT_VERSION = \'{}.{}.{}{}\'.format(MAJOR, MINOR, PATCH, SUFFIX)\n\nversion_file = \'alphapose/version.py\'\n\n\ndef readme():\n    with open(\'README.md\') as f:\n        content = f.read()\n    return content\n\n\ndef get_git_hash():\n\n    def _minimal_ext_cmd(cmd):\n        # construct minimal environment\n        env = {}\n        for k in [\'SYSTEMROOT\', \'PATH\', \'HOME\']:\n            v = os.environ.get(k)\n            if v is not None:\n                env[k] = v\n        # LANGUAGE is used on win32\n        env[\'LANGUAGE\'] = \'C\'\n        env[\'LANG\'] = \'C\'\n        env[\'LC_ALL\'] = \'C\'\n        out = subprocess.Popen(\n            cmd, stdout=subprocess.PIPE, env=env).communicate()[0]\n        return out\n\n    try:\n        out = _minimal_ext_cmd([\'git\', \'rev-parse\', \'HEAD\'])\n        sha = out.strip().decode(\'ascii\')\n    except OSError:\n        sha = \'unknown\'\n\n    return sha\n\n\ndef get_hash():\n    if os.path.exists(\'.git\'):\n        sha = get_git_hash()[:7]\n    elif os.path.exists(version_file):\n        try:\n            from alphapose.version import __version__\n            sha = __version__.split(\'+\')[-1]\n        except ImportError:\n            raise ImportError(\'Unable to get git version\')\n    else:\n        sha = \'unknown\'\n\n    return sha\n\n\ndef write_version_py():\n    content = """"""# GENERATED VERSION FILE\n# TIME: {}\n\n__version__ = \'{}\'\nshort_version = \'{}\'\n""""""\n    sha = get_hash()\n    VERSION = SHORT_VERSION + \'+\' + sha\n\n    with open(version_file, \'w\') as f:\n        f.write(content.format(time.asctime(), VERSION, SHORT_VERSION))\n\n\ndef get_version():\n    with open(version_file, \'r\') as f:\n        exec(compile(f.read(), version_file, \'exec\'))\n    return locals()[\'__version__\']\n\n\ndef make_cython_ext(name, module, sources):\n    extra_compile_args = None\n    if platform.system() != \'Windows\':\n        extra_compile_args = {\n            \'cxx\': [\'-Wno-unused-function\', \'-Wno-write-strings\']\n        }\n\n    extension = Extension(\n        \'{}.{}\'.format(module, name),\n        [os.path.join(*module.split(\'.\'), p) for p in sources],\n        include_dirs=[np.get_include()],\n        language=\'c++\',\n        extra_compile_args=extra_compile_args)\n    extension, = cythonize(extension)\n    return extension\n\n\ndef make_cuda_ext(name, module, sources):\n\n    return CUDAExtension(\n        name=\'{}.{}\'.format(module, name),\n        sources=[os.path.join(*module.split(\'.\'), p) for p in sources],\n        extra_compile_args={\n            \'cxx\': [],\n            \'nvcc\': [\n                \'-D__CUDA_NO_HALF_OPERATORS__\',\n                \'-D__CUDA_NO_HALF_CONVERSIONS__\',\n                \'-D__CUDA_NO_HALF2_OPERATORS__\',\n            ]\n        })\n\n\ndef get_ext_modules():\n    ext_modules = []\n    # only windows visual studio 2013+ support compile c/cuda extensions\n    # If you force to compile extension on Windows and ensure appropriate visual studio\n    # is intalled, you can try to use these ext_modules.\n    ext_modules = [\n        make_cython_ext(\n            name=\'soft_nms_cpu\',\n            module=\'detector.nms\',\n            sources=[\'src/soft_nms_cpu.pyx\']),\n        make_cuda_ext(\n            name=\'nms_cpu\',\n            module=\'detector.nms\',\n            sources=[\'src/nms_cpu.cpp\']),\n        make_cuda_ext(\n            name=\'nms_cuda\',\n            module=\'detector.nms\',\n            sources=[\'src/nms_cuda.cpp\', \'src/nms_kernel.cu\']),\n        make_cuda_ext(\n            name=\'roi_align_cuda\',\n            module=\'alphapose.utils.roi_align\',\n            sources=[\'src/roi_align_cuda.cpp\', \'src/roi_align_kernel.cu\']),\n        make_cuda_ext(\n            name=\'deform_conv_cuda\',\n            module=\'alphapose.models.layers.dcn\',\n            sources=[\n                \'src/deform_conv_cuda.cpp\',\n                \'src/deform_conv_cuda_kernel.cu\'\n            ]),\n        make_cuda_ext(\n            name=\'deform_pool_cuda\',\n            module=\'alphapose.models.layers.dcn\',\n            sources=[\n                \'src/deform_pool_cuda.cpp\',\n                \'src/deform_pool_cuda_kernel.cu\'\n            ]),\n    ]\n    return ext_modules\n\n\ndef get_install_requires():\n    install_requires = [\n        \'six\', \'terminaltables\', \'scipy==1.1.0\',\n        \'opencv-python\', \'matplotlib\', \'visdom\',\n        \'tqdm\', \'tensorboardx\', \'easydict\',\n        \'pyyaml\',\n        \'torch>=1.1.0\', \'torchvision>=0.3.0\',\n        \'munkres\', \'timm\'\n    ]\n    # official pycocotools doesn\'t support Windows, we will install it by third-party git repository later\n    if platform.system() != \'Windows\':\n        install_requires.append(\'pycocotools\')\n    return install_requires\n\n\ndef is_installed(package_name):\n    from pip._internal.utils.misc import get_installed_distributions\n    for p in get_installed_distributions():\n        if package_name in p.egg_name():\n            return True\n    return False\n\n\nif __name__ == \'__main__\':\n    write_version_py()\n    setup(\n        name=\'alphapose\',\n        version=get_version(),\n        description=\'Code for AlphaPose\',\n        long_description=readme(),\n        keywords=\'computer vision, human pose estimation\',\n        url=\'https://github.com/MVIG-SJTU/AlphaPose\',\n        packages=find_packages(exclude=(\'data\', \'exp\',)),\n        package_data={\'\': [\'*.json\', \'*.txt\']},\n        classifiers=[\n            \'Development Status :: 4 - Beta\',\n            \'License :: OSI Approved :: Apache Software License\',\n            \'Operating System :: OS Independent\',\n            \'Programming Language :: Python :: 2\',\n            \'Programming Language :: Python :: 2.7\',\n            \'Programming Language :: Python :: 3\',\n            \'Programming Language :: Python :: 3.4\',\n            \'Programming Language :: Python :: 3.5\',\n            \'Programming Language :: Python :: 3.6\',\n        ],\n        license=\'GPLv3\',\n        python_requires="">=3"",\n        setup_requires=[\'pytest-runner\', \'numpy\', \'cython\'],\n        tests_require=[\'pytest\'],\n        install_requires=get_install_requires(),\n        ext_modules=get_ext_modules(),\n        cmdclass={\'build_ext\': BuildExtension},\n        zip_safe=False)\n    # Windows need pycocotools here: https://github.com/philferriere/cocoapi#subdirectory=PythonAPI\n    if platform.system() == \'Windows\' and not is_installed(\'pycocotools\'):\n        print(""\\nInstall third-party pycocotools for Windows..."")\n        cmd = \'python -m pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI\'\n        os.system(cmd)\n    if not is_installed(\'cython_bbox\'):\n        print(""\\nInstall `cython_bbox`..."")\n        cmd = \'python -m pip install git+https://github.com/yanfengliu/cython_bbox.git\'\n        os.system(cmd)\n'"
PoseFlow/matching.py,0,"b'# coding: utf-8\n\'\'\'\nFile: matching.py\nProject: AlphaPose\nFile Created: Monday, 1st October 2018 12:53:12 pm\nAuthor: Yuliang Xiu (yuliangxiu@sjtu.edu.cn)\nCopyright 2018 - 2018 Shanghai Jiao Tong University, Machine Vision and Intelligence Group\n\'\'\'\n\n\nimport os\nimport cv2\nfrom tqdm import tqdm\nimport numpy as np\nimport time\nimport argparse\n\ndef generate_fake_cor(img, out_path):\n    print(""Generate fake correspondence files...%s""%out_path)\n    fd = open(out_path,""w"")\n    height, width, channels = img.shape\n\n    for x in range(width):\n        for y in range(height):\n            ret = fd.write(""%d %d %d %d %f \\n""%(x, y, x, y, 1.0))\n    fd.close()\n\n\ndef orb_matching(img1_path, img2_path, vidname, img1_id, img2_id):\n    \n    out_path = ""%s/%s_%s_orb.txt""%(vidname, img1_id, img2_id)\n    # print(out_path)\n    \n    if isinstance(img1_path, str):\n        img1 = cv2.cvtColor(cv2.imread(img1_path), cv2.COLOR_BGR2RGB)\n    else:\n        img1 = cv2.cvtColor(img1_path, cv2.COLOR_BGR2RGB)\n    if isinstance(img2_path, str):\n        img2 = cv2.cvtColor(cv2.imread(img2_path), cv2.COLOR_BGR2RGB)\n    else:\n        img2 = cv2.cvtColor(img2_path, cv2.COLOR_BGR2RGB)\n    \n    # Initiate ORB detector\n    orb = cv2.ORB_create(nfeatures=10000, scoreType=cv2.ORB_FAST_SCORE)\n\n    # find the keypoints and descriptors with ORB\n    kp1, des1 = orb.detectAndCompute(img1,None)\n    kp2, des2 = orb.detectAndCompute(img2,None)\n\n    if len(kp1)*len(kp2) < 400:\n        generate_fake_cor(img1, out_path)\n        return\n\n    # FLANN parameters\n    FLANN_INDEX_LSH = 6\n    index_params= dict(algorithm = FLANN_INDEX_LSH,\n                       table_number = 12, # 12\n                       key_size = 12,     # 20\n                       multi_probe_level = 2) #2\n\n    search_params = dict(checks=100)   # or pass empty dictionary\n\n    flann = cv2.FlannBasedMatcher(index_params,search_params)\n\n    matches = flann.knnMatch(des1, des2, k=2)\n    \n    # Open file\n    fd = open(out_path,""w"")\n\n    # ratio test as per Lowe\'s paper\n    for i, m_n in enumerate(matches):\n        if len(m_n) != 2:\n            continue\n        elif m_n[0].distance < 0.80*m_n[1].distance:\n            ret = fd.write(""%d %d %d %d %f \\n""%(kp1[m_n[0].queryIdx].pt[0], kp1[m_n[0].queryIdx].pt[1], kp2[m_n[0].trainIdx].pt[0], kp2[m_n[0].trainIdx].pt[1], m_n[0].distance))\n    \n    # Close opened file\n    fd.close()\n\n    # print(os.stat(out_path).st_size)\n\n    if os.stat(out_path).st_size<1000:\n        generate_fake_cor(img1, out_path)\n\nif __name__ == \'__main__\':\n    \n    parser = argparse.ArgumentParser(description=\'FoseFlow Matching\')\n    parser.add_argument(\'--orb\', type=int, default=1)\n    args = parser.parse_args()\n\n    image_dir = ""posetrack_data/images""\n    imgnames = []\n    vidnames = []\n\n    for a,b,c in os.walk(image_dir):\n        if len(a.split(""/"")) == 4:\n            vidnames.append(a)\n\n    for vidname in tqdm(sorted(vidnames)):\n        for a,b,c in os.walk(vidname):\n            c=[item for item in c if ""jpg"" in item]\n            imgnames = sorted(c)\n            break\n        for imgname in imgnames[:-1]:\n            if \'crop\' in imgname:\n                continue\n            img1 = os.path.join(vidname,imgname)\n            len_name = len(imgname.split(""."")[0])\n            if len_name == 5:\n                img2 = os.path.join(vidname,""%05d.jpg""%(int(imgname.split(""."")[0])+1))\n            else:\n                img2 = os.path.join(vidname,""%08d.jpg""%(int(imgname.split(""."")[0])+1))\n            if not os.path.exists(img2):\n                continue\n            img1_id = img1.split(""."")[0].split(""/"")[-1]\n            img2_id = img2.split(""."")[0].split(""/"")[-1]\n            if args.orb:\n                cor_file = ""%s/%s_%s_orb.txt""%(vidname,img1_id,img2_id)\n            else:\n                cor_file = ""%s/%s_%s.txt""%(vidname,img1_id,img2_id)\n            if not os.path.exists(cor_file) or os.stat(cor_file).st_size<1000:\n                if args.orb:\n                    # calc orb matching\n                    orb_matching(img1,img2,vidname,img1_id,img2_id)\n                else:\n                    # calc deep matching\n                    cmd = ""./deepmatching/deepmatching %s %s -nt 10 -downscale 3 -out %s/%s_%s.txt > cache""%(img1,img2,vidname,img1_id,img2_id)\n                    os.system(cmd)\n'"
PoseFlow/parallel_process.py,0,"b'# adapted from http://danshiebler.com/2016-09-14-parallel-progress-bar/\nfrom tqdm import tqdm\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\n\ndef parallel_process(array, function, n_jobs=16, use_kwargs=False, front_num=3):\n    """"""\n        A parallel version of the map function with a progress bar. \n\n        Args:\n            array (array-like): An array to iterate over.\n            function (function): A python function to apply to the elements of array\n            n_jobs (int, default=16): The number of cores to use\n            use_kwargs (boolean, default=False): Whether to consider the elements of array as dictionaries of \n                keyword arguments to function \n            front_num (int, default=3): The number of iterations to run serially before kicking off the parallel job. \n                Useful for catching bugs\n        Returns:\n            [function(array[0]), function(array[1]), ...]\n    """"""\n    #We run the first few iterations serially to catch bugs\n    if front_num > 0:\n        front = [function(**a) if use_kwargs else function(*a) for a in array[:front_num]]\n    #If we set n_jobs to 1, just run a list comprehension. This is useful for benchmarking and debugging.\n    if n_jobs==1:\n        return front + [function(**a) if use_kwargs else function(*a) for a in tqdm(array[front_num:])]\n    #Assemble the workers\n    with ProcessPoolExecutor(max_workers=n_jobs) as pool:\n        #Pass the elements of array into function\n        if use_kwargs:\n            futures = [pool.submit(function, **a) for a in array[front_num:]]\n        else:\n            futures = [pool.submit(function, *a) for a in array[front_num:]]\n        kwargs = {\n            \'total\': len(futures),\n            \'unit\': \'it\',\n            \'unit_scale\': True,\n            \'leave\': True\n        }\n        #Print out the progress as tasks complete\n        for f in tqdm(as_completed(futures), **kwargs):\n            pass\n    out = []\n    #Get the results from the futures. \n    for i, future in enumerate(futures):\n        try:\n            out.append(future.result())\n        except Exception as e:\n            out.append(e)\n    return front + out'"
PoseFlow/poseflow_infer.py,1,"b'# -*- coding: utf-8 -*-\n# @Author: Chao Xu\n# @Email: xuchao.19962007@sjtu.edu.cn\n# @Date:   2019-10-09 17:42:10\n# @Last Modified by:   Chao Xu\n# @Last Modified time: 2019-10-27 20:20:45\n\nimport os\nimport numpy as np\n\nfrom .matching import orb_matching\nfrom .utils import expand_bbox, stack_all_pids, best_matching_hungarian\n\ndef get_box(pose, img_height, img_width):\n\n    pose = np.array(pose).reshape(-1,3)\n    xmin = np.min(pose[:,0])\n    xmax = np.max(pose[:,0])\n    ymin = np.min(pose[:,1])\n    ymax = np.max(pose[:,1])\n\n    return expand_bbox(xmin, xmax, ymin, ymax, img_width, img_height)\n\n#The wrapper of PoseFlow algorithm to be embedded in alphapose inference\nclass PoseFlowWrapper():\n    def __init__(self, link=100, drop=2.0, num=7,\n                 mag=30, match=0.2, save_path=\'.tmp/poseflow\', pool_size=5):\n        # super parameters\n        # 1. look-ahead LINK_LEN frames to find tracked human bbox\n        # 2. bbox_IoU(deepmatching), bbox_IoU(general), pose_IoU(deepmatching), pose_IoU(general), box1_score, box2_score\n        # 3. bbox_IoU(deepmatching), bbox_IoU(general), pose_IoU(deepmatching), pose_IoU(general), box1_score, box2_score(Non DeepMatching)\n        # 4. drop low-score(<DROP) keypoints\n        # 5. pick high-score(top NUM) keypoints when computing pose_IOU\n        # 6. box width/height around keypoint for computing pose IoU\n        # 7. match threshold in Hungarian Matching\n        self.link_len = link\n        self.weights = [1,2,1,2,0,0] \n        self.weights_fff = [0,1,0,1,0,0]\n        self.drop = drop\n        self.num = num\n        self.mag = mag\n        self.match_thres = match\n        self.notrack = {}\n        self.track = {}\n        self.save_path = save_path\n        self.save_match_path = os.path.join(save_path,\'matching\')\n        self.pool_size = pool_size\n\n        if not os.path.exists(save_path):\n            os.mkdir(save_path)\n\n        #init local variables\n        self.max_pid_id = 0\n        self.prev_img = None\n        print(""Start pose tracking...\\n"")\n\n    def convert_results_to_no_track(self, alphapose_results):\n        # INPUT:\n        #   alphapose_results: the results of pose detection given by pose_nms,\n        #   not the final version for saving. Data array\'s format is torch.FloatTensor.\n        #   format: {""imgname"": str, ""result"": [{\'keypoints\': [17,2], \'kp_score\': [17,], \'proposal_score\': float},...]}\n        # OUTPUT:\n        #   notrack: data array\'s format is list.\n        #   format: {""(str)$imgid"": [{\'keypoints\': [17*3], \'scores\': float},...]}\n        imgname = os.path.basename(alphapose_results[""imgname""])\n        alphapose_results = alphapose_results[""result""]\n        notrack = {}\n        notrack[imgname] = []\n        for human in alphapose_results:\n            keypoints = []\n            kp_preds = human[\'keypoints\']\n            kp_scores = human[\'kp_score\']\n            pro_scores = human[\'proposal_score\']\n            for n in range(kp_scores.shape[0]):\n                keypoints.append(float(kp_preds[n, 0]))\n                keypoints.append(float(kp_preds[n, 1]))\n                keypoints.append(float(kp_scores[n]))\n            notrack[imgname].append({\'keypoints\': keypoints, \'scores\': pro_scores})\n        return notrack\n\n    def convert_notrack_to_track(self, notrack, img_height, img_width):\n        # INPUT:\n        #   notrack: data array\'s format is list.\n        #   - format: {""(str)$imgid"": [{\'keypoints\': [17*3], \'scores\': float},...]}\n        #   img_height: int\n        #   img_width: int\n        # OUTPUT:\n        #   track: tracked human poses\n        #   - format: {\'num_boxes\': int, \'$1-indexed human id\': {\'box_score\': float, \'box_pos\': [4], \n        #              \'box_pose_pos\': [17,2], \'box_pose_score\': [17,1]}, ...}\n        track = {}\n        for img_name in sorted(notrack.keys()):\n            track[img_name] = {\'num_boxes\':len(notrack[img_name])}\n            for bid in range(len(notrack[img_name])):\n                track[img_name][bid+1] = {}\n                track[img_name][bid+1][\'box_score\'] = notrack[img_name][bid][\'scores\']\n                track[img_name][bid+1][\'box_pos\'] = get_box(notrack[img_name][bid][\'keypoints\'], img_height, img_width)\n                track[img_name][bid+1][\'box_pose_pos\'] = np.array(notrack[img_name][bid][\'keypoints\']).reshape(-1,3)[:,0:2]\n                track[img_name][bid+1][\'box_pose_score\'] = np.array(notrack[img_name][bid][\'keypoints\']).reshape(-1,3)[:,-1]\n        return track\n\n    def step(self, img, alphapose_results):\n        frame_name = os.path.basename(alphapose_results[""imgname""])\n        frame_id = frame_name.split(""."")[0]\n        #load track information\n        _notrack = self.convert_results_to_no_track(alphapose_results)\n        self.notrack.update(_notrack)\n        img_height, img_width, _ = img.shape\n        _track = self.convert_notrack_to_track(_notrack, img_height, img_width)\n        self.track.update(_track)\n\n        #track\n        # init tracking info of the first frame in one video\n        if len(self.track.keys()) == 1:\n            for pid in range(1, self.track[frame_name][\'num_boxes\']+1):\n                self.track[frame_name][pid][\'new_pid\'] = pid\n                self.track[frame_name][pid][\'match_score\'] = 0\n            #make directory to store matching files\n            if not os.path.exists(self.save_match_path):\n                os.mkdir(self.save_match_path)\n            self.prev_img = img.copy()\n            return self.final_result_by_name(frame_name)\n\n        frame_id_list = sorted([(int(os.path.splitext(i)[0]), os.path.splitext(i)[1]) for i in self.track.keys()])\n        frame_list = [ """".join([str(i[0]), i[1]]) for i in frame_id_list]\n        prev_frame_name = frame_list[-2]\n        prev_frame_id = prev_frame_name.split(""."")[0]\n        frame_new_pids = []\n\n        self.max_pid_id = max(self.max_pid_id, self.track[prev_frame_name][\'num_boxes\'])\n        cor_file = os.path.join(self.save_match_path, """".join([prev_frame_id, \'_\', frame_id, \'_orb.txt\']))\n        orb_matching(self.prev_img, img, self.save_match_path, prev_frame_id, frame_id)\n        all_cors = np.loadtxt(cor_file)\n\n        if self.track[frame_name][\'num_boxes\'] == 0:\n            self.track[frame_name] = copy.deepcopy(self.track[prev_frame_name])\n            self.prev_img = img.copy()\n            return self.final_result_by_name(frame_name)\n\n        cur_all_pids, cur_all_pids_fff = stack_all_pids(self.track, frame_list, len(frame_list)-2, self.max_pid_id, self.link_len)\n        match_indexes, match_scores = best_matching_hungarian(\n            all_cors, cur_all_pids, cur_all_pids_fff, self.track[frame_name], self.weights, self.weights_fff, self.num, self.mag, pool_size=self.pool_size)\n\n        for pid1, pid2 in match_indexes:\n            if match_scores[pid1][pid2] > self.match_thres:\n                self.track[frame_name][pid2+1][\'new_pid\'] = cur_all_pids[pid1][\'new_pid\']\n                self.max_pid_id = max(self.max_pid_id, self.track[frame_name][pid2+1][\'new_pid\'])\n                self.track[frame_name][pid2+1][\'match_score\'] = match_scores[pid1][pid2]\n\n        # add the untracked new person\n        for next_pid in range(1, self.track[frame_name][\'num_boxes\'] + 1):\n            if \'new_pid\' not in self.track[frame_name][next_pid]:\n                self.max_pid_id += 1\n                self.track[frame_name][next_pid][\'new_pid\'] = self.max_pid_id\n                self.track[frame_name][next_pid][\'match_score\'] = 0\n\n        self.prev_img = img.copy()\n        return self.final_result_by_name(frame_name)\n\n    @property\n    def num_persons(self):\n        # calculate number of people\n        num_persons = 0\n        frame_list = sorted(list(self.track.keys()))\n        for fid, frame_name in enumerate(frame_list):\n            for pid in range(1, self.track[frame_name][\'num_boxes\']+1):\n                num_persons = max(num_persons, self.track[frame_name][pid][\'new_pid\'])\n        return num_persons\n\n    @property\n    def final_results(self):\n        # export tracking result into notrack json data\n        frame_list = sorted(list(self.track.keys()))\n        for fid, frame_name in enumerate(frame_list):\n            for pid in range(self.track[frame_name][\'num_boxes\']):\n                self.notrack[frame_name][pid][\'idx\'] = self.track[frame_name][pid+1][\'new_pid\']\n        return self.notrack\n\n    def final_result_by_name(self, frame_name):\n        # export tracking result into notrack json data by frame name\n        for pid in range(self.track[frame_name][\'num_boxes\']):\n            self.notrack[frame_name][pid][\'idx\'] = self.track[frame_name][pid+1][\'new_pid\']\n        return self.notrack[frame_name]\n\n\n\n\n\n\n\n        '"
PoseFlow/tracker-baseline.py,0,"b'# coding: utf-8\n\n\'\'\'\nFile: tracker-baseline.py\nProject: AlphaPose\nFile Created: Thursday, 1st March 2018 6:12:23 pm\nAuthor: Yuliang Xiu (yuliangxiu@sjtu.edu.cn)\n-----\nLast Modified: Monday, 1st October 2018 12:53:12 pm\nModified By: Yuliang Xiu (yuliangxiu@sjtu.edu.cn>)\n-----\nCopyright 2018 - 2018 Shanghai Jiao Tong University, Machine Vision and Intelligence Group\n\'\'\'\n\nimport numpy as np\nimport os\nimport json\nimport copy\nimport heapq\nfrom munkres import Munkres, print_matrix\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom utils import *\nfrom matching import orb_matching\nimport argparse\n\n\n# posetrack dataset path\nimage_dir = ""./posetrack_data""\n\nif __name__ == \'__main__\':\n    \n    parser = argparse.ArgumentParser(description=\'FoseFlow Tracker\')\n    parser.add_argument(\'--link\', type=int, default=100)\n    parser.add_argument(\'--drop\', type=float, default=2.0)\n    parser.add_argument(\'--num\', type=int, default=7)\n    parser.add_argument(\'--mag\', type=int, default=30)\n    parser.add_argument(\'--match\', type=float, default=0.2)\n    parser.add_argument(\'--dataset\', type=str, default=\'val\')\n    parser.add_argument(\'--orb\', type=int, default=0)\n\n    args = parser.parse_args()\n\n    # super parameters\n    # 1. look-ahead LINK_LEN frames to find tracked human bbox\n    # 2. bbox_IoU(deepmatching), bbox_IoU(general), pose_IoU(deepmatching), pose_IoU(general), box1_score, box2_score\n    # 3. bbox_IoU(deepmatching), bbox_IoU(general), pose_IoU(deepmatching), pose_IoU(general), box1_score, box2_score(Non DeepMatching)\n    # 4. drop low-score(<DROP) keypoints\n    # 5. pick high-score(top NUM) keypoints when computing pose_IOU\n    # 6. box width/height around keypoint for computing pose IoU\n    # 7. match threshold in Hungarian Matching\n    # 8. dataset = \'test\' or \'val\'\n    # 9. use orb matching or not\n\n    link_len = args.link\n    weights = [1,2,1,2,0,0] \n    weights_fff = [0,1,0,1,0,0]\n    drop = args.drop\n    num = args.num\n    mag = args.mag\n    match_thres = args.match\n    dataset = args.dataset\n    use_orb = args.orb\n            \n    anno_dir = ""./posetrack_data/annotations/{}"".format(dataset)\n    notrack_json = ""alpha-pose-results-{}.json"".format(dataset) \n    track_dir = ""{}-predict"".format(dataset) # results dir name\n       \n    if not os.path.exists(track_dir):\n            os.mkdir(track_dir)\n\n    track = {}\n    cur_vname = """"\n    num_persons = 0\n\n    # load json file without tracking information\n    # Note: time is a little long, so it is better to uncomment the following save operation at first time\n    with open(notrack_json,\'r\') as f:\n        notrack = json.load(f)\n        for imgpath in tqdm(sorted(notrack.keys())):\n            \n            if \'crop\' in imgpath:\n                vname,fname = imgpath[:-18],imgpath[-17:]\n                print(imgpath,vname,fname)\n                continue\n\n            vname,fname = imgpath[:-13],imgpath[-12:]\n            if vname != cur_vname:\n                cur_vname = vname\n                track[vname] = {}\n            \n            track[vname][fname] = {\'num_boxes\':len(notrack[imgpath])}\n            for bid in range(len(notrack[imgpath])):\n                track[vname][fname][bid+1] = {}\n                track[vname][fname][bid+1][\'box_score\'] = notrack[imgpath][bid][\'score\']\n                track[vname][fname][bid+1][\'box_pos\'] = get_box(notrack[imgpath][bid][\'keypoints\'], os.path.join(image_dir,imgpath))\n                track[vname][fname][bid+1][\'box_pose_pos\'] = np.array(notrack[imgpath][bid][\'keypoints\']).reshape(-1,3)[:,0:2]\n                track[vname][fname][bid+1][\'box_pose_score\'] = np.array(notrack[imgpath][bid][\'keypoints\']).reshape(-1,3)[:,-1]\n   \n    np.save(\'notrack-{}.npy\'.format(dataset),track)\n    track = np.load(\'notrack-{}.npy\'.format(dataset)).item()\n\n    # tracking process\n    for video_name in tqdm(track.keys()):\n\n        max_pid_id = 0\n        frame_list = sorted(list(track[video_name].keys()))\n\n        for idx, frame_name in enumerate(frame_list[:-1]):\n            frame_new_pids = []\n            frame_id = frame_name.split(""."")[0]\n\n            next_frame_name = frame_list[idx+1]\n            next_frame_id = next_frame_name.split(""."")[0]\n            \n            # deal with image file whose name ended with \'__crop\'\n            if \'crop\' in next_frame_name:\n                track[video_name][next_frame_name] = copy.deepcopy(track[video_name][frame_name])\n                continue\n            \n            # init tracking info of the first frame in one video\n            if idx == 0:\n                for pid in range(1, track[video_name][frame_name][\'num_boxes\']+1):\n                        track[video_name][frame_name][pid][\'new_pid\'] = pid\n                        track[video_name][frame_name][pid][\'match_score\'] = 0\n\n            max_pid_id = max(max_pid_id, track[video_name][frame_name][\'num_boxes\'])\n            if use_orb:\n                cor_file = os.path.join(image_dir, video_name, """".join([frame_id, \'_\', next_frame_id, \'_orb.txt\']))\n            else:\n                cor_file = os.path.join(image_dir, video_name, """".join([frame_id, \'_\', next_frame_id, \'.txt\']))\n\n            # regenerate the missed pair-matching txt\n            if not os.path.exists(cor_file) or os.stat(cor_file).st_size<200:\n                \n                dm = ""/home/yuliang/code/PoseTrack-CVPR2017/external/deepmatching/deepmatching""\n                img1_path = os.path.join(image_dir,video_name,frame_name)\n                img2_path = os.path.join(image_dir,video_name,next_frame_name)\n\n                if use_orb:\n                    orb_matching(img1_path,img2_path, os.path.join(image_dir, video_name), frame_id, next_frame_id)\n                else:\n                    cmd = ""%s %s %s -nt 20 -downscale 2 -out %s""%(dm,img1_path,img2_path,cor_file)\n                    os.system(cmd)\n\n            all_cors = np.loadtxt(cor_file)\n\n            # if there is no people in this frame, then copy the info from former frame\n            if track[video_name][next_frame_name][\'num_boxes\'] == 0:\n                track[video_name][next_frame_name] = copy.deepcopy(track[video_name][frame_name])\n                continue\n            cur_all_pids, cur_all_pids_fff = stack_all_pids(track[video_name], frame_list[:-1], idx, max_pid_id, link_len)\n            match_indexes, match_scores = best_matching_hungarian(\n                all_cors, cur_all_pids, cur_all_pids_fff, track[video_name][next_frame_name], weights, weights_fff, num, mag)\n        \n            for pid1, pid2 in match_indexes:\n                if match_scores[pid1][pid2] > match_thres:\n                    track[video_name][next_frame_name][pid2+1][\'new_pid\'] = cur_all_pids[pid1][\'new_pid\']\n                    max_pid_id = max(max_pid_id, track[video_name][next_frame_name][pid2+1][\'new_pid\'])\n                    track[video_name][next_frame_name][pid2+1][\'match_score\'] = match_scores[pid1][pid2]\n\n            # add the untracked new person\n            for next_pid in range(1, track[video_name][next_frame_name][\'num_boxes\'] + 1):\n                if \'new_pid\' not in track[video_name][next_frame_name][next_pid]:\n                    max_pid_id += 1\n                    track[video_name][next_frame_name][next_pid][\'new_pid\'] = max_pid_id\n                    track[video_name][next_frame_name][next_pid][\'match_score\'] = 0\n            \n            # deal with unconsecutive frames caused by this fucking terrible dataset\n            gap = int(next_frame_id)-int(frame_id)\n            if gap>1:\n                for i in range(gap):\n                    if i>0:\n                        new_frame_name = ""%08d.jpg""%(int(frame_id)+i)\n                        track[video_name][new_frame_name] = copy.deepcopy(track[video_name][frame_name])\n            \n    rmpe_part_ids = [0, 1, 2, 3, 4, 5, 10, 11, 12, 13, 14, 15, 8, 9]\n\n    for video_name in tqdm(track.keys()):\n        num_persons = 0\n        frame_list = sorted(list(track[video_name].keys()))\n        for fid, frame_name in enumerate(frame_list):\n            for pid in range(1, track[video_name][frame_name][\'num_boxes\']+1):\n                new_score = copy.deepcopy(track[video_name][frame_name][pid][\'box_pose_score\'])\n                new_pose = copy.deepcopy(track[video_name][frame_name][pid][\'box_pose_pos\'])\n                track[video_name][frame_name][pid][\'box_pose_score\'] = new_score[rmpe_part_ids]\n                track[video_name][frame_name][pid][\'box_pose_pos\'] = new_pose[rmpe_part_ids,:]\n                num_persons = max(num_persons, track[video_name][frame_name][pid][\'new_pid\'])\n        track[video_name][\'num_persons\'] = num_persons\n\n    np.save(\'track-{}.npy\'.format(dataset),track)\n    track = np.load(\'track-{}.npy\'.format(dataset)).item()\n\n    for a,b,c in os.walk(anno_dir):\n        val_jsons = [item for item in c if \'json\' in item]\n        break\n\n    # export tracking result into json files\n    for video_name in tqdm(track.keys()):\n        if dataset == \'val\':\n            name = [item for item in val_jsons if video_name.split(""/"")[-1] in item]\n            if len(name) == 0:\n                name = [item for item in val_jsons if video_name.split(""/"")[-1][1:] in item]\n            name = name[0]\n        else:\n            # FUCK the dirty PoseTrack dataset\n            name = [item for item in val_jsons if video_name.split(""/"")[-1].split(""_"")[0] == item.split(""_"")[0]]\n            if video_name.split(""/"")[-1].split(""_"")[0] == ""000044"":\n                if video_name.split(""/"")[-2]==\'mpii_5sec\':\n                    name = [""00044_mpii_step1_relpath_5sec_testsub.json""]\n                elif video_name.split(""/"")[-2]==\'bonn_5sec\':\n                    name = [""000044_mpii_relpath_5sec_testsub.json""]\n                    \n            if video_name.split(""/"")[-1].split(""_"")[0] == ""002279"":\n                if video_name.split(""/"")[-2]==\'mpii_5sec\':\n                    name = [""02279_mpii_step2_relpath_5sec_testsub.json""]\n                elif video_name.split(""/"")[-2]==\'bonn_mpii_test_v2_5sec\':\n                    name = [""02279_mpii_relpath_5sec_testsub.json""]\n                    \n            if video_name.split(""/"")[-1].split(""_"")[0] == ""019980"":\n                if video_name.split(""/"")[-2]==\'bonn_5sec\':\n                    name = [""019980_mpii_relpath_5sec_testsub.json""]\n                elif video_name.split(""/"")[-2]==\'mpii_5sec\':\n                    name = [""19980_mpii_step1_relpath_5sec_testsub.json""]\n            \n            if video_name.split(""/"")[-1].split(""_"")[0] == ""09611"":\n                name = [""09611_mpii_relpath_5sec_testsub.json""]\n            if video_name.split(""/"")[-1].split(""_"")[0] == ""009611"":\n                name = [""09611_mpii_step2_relpath_5sec_testsub.json""]\n\n            if video_name.split(""/"")[-1].split(""_"")[0][:-1] == \'00000\':\n                name = [item for item in val_jsons if video_name.split(""/"")[-1].split(""_"")[0][1:] == item.split(""_"")[0]]\n            if len(name)==0:\n                name = [item for item in val_jsons if video_name.split(""/"")[-1].split(""_"")[0][1:] == item.split(""_"")[0]]\n            name = name[0]\n\n        final = {\'annolist\':[]}\n        frame_list = list(track[video_name].keys())\n        frame_list.remove(\'num_persons\')\n        frame_list = sorted(frame_list)\n        \n        with open(os.path.join(anno_dir,name)) as f:\n                annot = json.load(f)\n\n        imgs = []\n        for img in annot[\'annolist\']:\n            imgs.append(img[\'image\'][0][\'name\'])\n                \n        for fid, frame_name in enumerate(frame_list):\n            if os.path.join(video_name,frame_name) not in imgs:\n                continue\n            final[\'annolist\'].append({""image"":[{""name"":os.path.join(video_name,frame_name)}],""annorect"":[]})\n            for pid in range(1, track[video_name][frame_name][\'num_boxes\']+1):\n                pid_info = track[video_name][frame_name][pid]\n                box_pos = pid_info[\'box_pos\']\n                box_score = pid_info[\'box_score\']\n                pose_pos = pid_info[\'box_pose_pos\']\n                pose_score = pid_info[\'box_pose_score\']\n                pose_pos = add_nose(pose_pos)\n                pose_score = add_nose(pose_score)\n                new_pid = pid_info[\'new_pid\']\n                \n                point_struct = []\n                for idx,pose in enumerate(pose_pos):\n                    if pose_score[idx]>drop:\n                        point_struct.append({""id"":[idx],""x"":[pose[0]],""y"":[pose[1]],""score"":[pose_score[idx]]})\n                final[\'annolist\'][fid][\'annorect\'].append({""x1"":[box_pos[0]],\\\n                                                            ""x2"":[box_pos[1]],\\\n                                                            ""y1"":[box_pos[2]],\\\n                                                            ""y2"":[box_pos[3]],\\\n                                                            ""score"":[box_score],\\\n                                                            ""track_id"":[new_pid-1],\\\n                                                            ""annopoints"":[{""point"":point_struct}]})\n                \n        for rest_name in enumerate(remove_list(imgs,video_name,frame_list)):\n            final[\'annolist\'].append({""image"":[{""name"":rest_name}],""annorect"":[]}) \n        with open(""%s/%s""%(track_dir,name),\'w\') as json_file:\n            json_file.write(json.dumps(final))\n'"
PoseFlow/tracker-general.py,0,"b'# coding: utf-8\n\n\'\'\'\nFile: tracker-general.py\nProject: AlphaPose\nFile Created: Tuesday, 18st Dec 2018 14:55:41 pm\n-----\nLast Modified: Thursday, 20st Dec 2018 23:24:47 pm\nModified By: Yuliang Xiu (yuliangxiu@sjtu.edu.cn>)\n-----\nAuthor: Yuliang Xiu (yuliangxiu@sjtu.edu.cn)\nCopyright 2018 - 2018 Shanghai Jiao Tong University, Machine Vision and Intelligence Group\n\'\'\'\n\nimport numpy as np\nimport os\nimport json\nimport copy\nimport heapq\nfrom munkres import Munkres, print_matrix\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom utils import *\nfrom matching import orb_matching\nimport argparse\nimport multiprocessing\nfrom parallel_process import parallel_process\n\n# visualization\ndef display_pose(imgdir, visdir, tracked, cmap):\n\n    print(""Start visualization...\\n"")\n    for imgname in tqdm(tracked.keys()):\n        img = Image.open(os.path.join(imgdir,imgname))\n        width, height = img.size\n        fig = plt.figure(figsize=(width/10,height/10),dpi=10)\n        plt.imshow(img)\n        for pid in range(len(tracked[imgname])):\n            pose = np.array(tracked[imgname][pid][\'keypoints\']).reshape(-1,3)[:,:3]\n            tracked_id = tracked[imgname][pid][\'idx\']\n\n            # keypoint scores of torch version and pytorch version are different\n            if np.mean(pose[:,2]) <1 :\n                alpha_ratio = 1.0\n            else:\n                alpha_ratio = 5.0\n\n            if pose.shape[0] == 16:\n                mpii_part_names = [\'RAnkle\',\'RKnee\',\'RHip\',\'LHip\',\'LKnee\',\'LAnkle\',\'Pelv\',\'Thrx\',\'Neck\',\'Head\',\'RWrist\',\'RElbow\',\'RShoulder\',\'LShoulder\',\'LElbow\',\'LWrist\']\n                colors = [\'m\', \'b\', \'b\', \'r\', \'r\', \'b\', \'b\', \'r\', \'r\', \'m\', \'m\', \'m\', \'r\', \'r\',\'b\',\'b\']\n                pairs = [[8,9],[11,12],[11,10],[2,1],[1,0],[13,14],[14,15],[3,4],[4,5],[8,7],[7,6],[6,2],[6,3],[8,12],[8,13]]\n                for idx_c, color in enumerate(colors):\n                    plt.plot(np.clip(pose[idx_c,0],0,width), np.clip(pose[idx_c,1],0,height), marker=\'o\', \n                            color=color, ms=80/alpha_ratio*np.mean(pose[idx_c,2]), markerfacecolor=(1, 1, 0, 0.7/alpha_ratio*pose[idx_c,2]))\n                for idx in range(len(pairs)):\n                    plt.plot(np.clip(pose[pairs[idx],0],0,width),np.clip(pose[pairs[idx],1],0,height), \'r-\',\n                            color=cmap(tracked_id), linewidth=60/alpha_ratio*np.mean(pose[pairs[idx],2]),  alpha=0.6/alpha_ratio*np.mean(pose[pairs[idx],2]))\n            elif pose.shape[0] == 17:\n                coco_part_names = [\'Nose\',\'LEye\',\'REye\',\'LEar\',\'REar\',\'LShoulder\',\'RShoulder\',\'LElbow\',\'RElbow\',\'LWrist\',\'RWrist\',\'LHip\',\'RHip\',\'LKnee\',\'RKnee\',\'LAnkle\',\'RAnkle\']\n                colors = [\'r\', \'r\', \'r\', \'r\', \'r\', \'y\', \'y\', \'y\', \'y\', \'y\', \'y\', \'g\', \'g\', \'g\',\'g\',\'g\',\'g\']\n                pairs = [[0,1],[0,2],[1,3],[2,4],[5,6],[5,7],[7,9],[6,8],[8,10],[11,12],[11,13],[13,15],[12,14],[14,16],[6,12],[5,11]]\n                for idx_c, color in enumerate(colors):\n                    plt.plot(np.clip(pose[idx_c,0],0,width), np.clip(pose[idx_c,1],0,height), marker=\'o\', \n                            color=color, ms=80/alpha_ratio*np.mean(pose[idx_c,2]), markerfacecolor=(1, 1, 0, 0.7/alpha_ratio*pose[idx_c,2]))\n                for idx in range(len(pairs)):\n                    plt.plot(np.clip(pose[pairs[idx],0],0,width),np.clip(pose[pairs[idx],1],0,height),\'r-\',\n                            color=cmap(tracked_id), linewidth=60/alpha_ratio*np.mean(pose[pairs[idx],2]), alpha=0.6/alpha_ratio*np.mean(pose[pairs[idx],2]))\n        plt.axis(\'off\')\n        ax = plt.gca()\n        ax.set_xlim([0,width])\n        ax.set_ylim([height,0])\n        extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n        if not os.path.exists(visdir): \n            os.mkdir(visdir)\n        fig.savefig(os.path.join(visdir,imgname.split()[0]+"".png""), pad_inches = 0.0, bbox_inches=extent, dpi=13)\n        plt.close()\n\n\nif __name__ == \'__main__\':\n    \n    parser = argparse.ArgumentParser(description=\'FoseFlow Tracker\')\n    parser.add_argument(\'--imgdir\', type=str, required=True, help=""Must input the images dir"")\n    parser.add_argument(\'--in_json\', type=str, required=True, help=""result json predicted by AlphaPose"")\n    parser.add_argument(\'--out_json\', type=str, required=True, help=""output path of tracked json"")\n    parser.add_argument(\'--visdir\', type=str, default="""", help=""visulization tracked results of video sequences"")\n\n    parser.add_argument(\'--link\', type=int, default=100)\n    parser.add_argument(\'--drop\', type=float, default=2.0)\n    parser.add_argument(\'--num\', type=int, default=7)\n    parser.add_argument(\'--mag\', type=int, default=30)\n    parser.add_argument(\'--match\', type=float, default=0.2)\n\n    args = parser.parse_args()\n\n    # super parameters\n    # 1. look-ahead LINK_LEN frames to find tracked human bbox\n    # 2. bbox_IoU(deepmatching), bbox_IoU(general), pose_IoU(deepmatching), pose_IoU(general), box1_score, box2_score\n    # 3. bbox_IoU(deepmatching), bbox_IoU(general), pose_IoU(deepmatching), pose_IoU(general), box1_score, box2_score(Non DeepMatching)\n    # 4. drop low-score(<DROP) keypoints\n    # 5. pick high-score(top NUM) keypoints when computing pose_IOU\n    # 6. box width/height around keypoint for computing pose IoU\n    # 7. match threshold in Hungarian Matching\n\n    link_len = args.link\n    weights = [1,2,1,2,0,0] \n    weights_fff = [0,1,0,1,0,0]\n    drop = args.drop\n    num = args.num\n    mag = args.mag\n    match_thres = args.match\n            \n    notrack_json = args.in_json\n    tracked_json = args.out_json\n    image_dir = args.imgdir\n    vis_dir = args.visdir\n\n    # if json format is differnt from ""alphapose-forvis.json"" (pytorch version)\n    if ""forvis"" not in notrack_json:\n        results_forvis = {}\n        last_image_name = \' \'\n\n        with open(notrack_json) as f:\n            results = json.load(f)\n            for i in range(len(results)):\n                imgpath = results[i][\'image_id\']\n                if last_image_name != imgpath:\n                    results_forvis[imgpath] = []\n                    results_forvis[imgpath].append({\'keypoints\':results[i][\'keypoints\'],\'scores\':results[i][\'score\']})\n                else:\n                    results_forvis[imgpath].append({\'keypoints\':results[i][\'keypoints\'],\'scores\':results[i][\'score\']})\n                last_image_name = imgpath\n        notrack_json = os.path.join(os.path.dirname(notrack_json), ""alphapose-results-forvis.json"")\n        with open(notrack_json,\'w\') as json_file:\n                json_file.write(json.dumps(results_forvis))\n        \n    notrack = {}\n    track = {}\n    num_persons = 0\n\n    def load_pose_boxes(img_name):\n        out = {\'num_boxes\':len(notrack[img_name])}\n        for bid in range(len(notrack[img_name])):\n            out[bid+1] = {}\n            out[bid+1][\'box_score\'] = notrack[img_name][bid][\'scores\']\n            out[bid+1][\'box_pos\'] = get_box(notrack[img_name][bid][\'keypoints\'], os.path.join(image_dir,img_name))\n            out[bid+1][\'box_pose_pos\'] = np.array(notrack[img_name][bid][\'keypoints\']).reshape(-1,3)[:,0:2]\n            out[bid+1][\'box_pose_score\'] = np.array(notrack[img_name][bid][\'keypoints\']).reshape(-1,3)[:,-1]\n        return out\n\n    print(""Start loading json file...\\n"")\n    # load json file without tracking information\n    with open(notrack_json,\'r\') as f:\n        notrack = json.load(f)\n        pose_boxes = parallel_process([(k,) for k in sorted(notrack.keys())], load_pose_boxes, n_jobs=8)\n        track.update(zip(sorted(notrack.keys()), pose_boxes) )\n   \n    np.save(\'notrack-bl.npy\',track)\n    # track = np.load(\'notrack-bl.npy\').item()\n\n    frame_list = sorted(list(track.keys()))\n\n    print(""ORB matching frame pairs ...\\n"")\n    tasks = []\n    for idx, frame_name in enumerate(frame_list[:-1]):\n        frame_id = frame_name.split(""."")[0]\n        next_frame_name = frame_list[idx+1]\n        next_frame_id = next_frame_name.split(""."")[0]\n        cor_file = os.path.join(image_dir, """".join([frame_id, \'_\', next_frame_id, \'_orb.txt\']))\n       \n        # regenerate the missed pair-matching txt\n        if not os.path.exists(cor_file) or os.stat(cor_file).st_size<200:\n            img1_path = os.path.join(image_dir, frame_name)\n            img2_path = os.path.join(image_dir, next_frame_name)\n            tasks.append((img1_path,img2_path, image_dir, frame_id, next_frame_id))\n    \n    # do the matching parallel\n    parallel_process(tasks, orb_matching, n_jobs=16)\n\n    print(""Start pose tracking...\\n"")\n    # tracking process\n    max_pid_id = 0\n    for idx, frame_name in enumerate(tqdm(frame_list[:-1])):\n        frame_new_pids = []\n        frame_id = frame_name.split(""."")[0]\n\n        next_frame_name = frame_list[idx+1]\n        next_frame_id = next_frame_name.split(""."")[0]\n\n        # init tracking info of the first frame in one video\n        if idx == 0:\n            for pid in range(1, track[frame_name][\'num_boxes\']+1):\n                    track[frame_name][pid][\'new_pid\'] = pid\n                    track[frame_name][pid][\'match_score\'] = 0\n\n        max_pid_id = max(max_pid_id, track[frame_name][\'num_boxes\'])\n        cor_file = os.path.join(image_dir, """".join([frame_id, \'_\', next_frame_id, \'_orb.txt\']))\n        all_cors = np.loadtxt(cor_file)\n\n        # if there is no people in this frame, then copy the info from former frame\n        if track[next_frame_name][\'num_boxes\'] == 0:\n            track[next_frame_name] = copy.deepcopy(track[frame_name])\n            continue\n        cur_all_pids, cur_all_pids_fff = stack_all_pids(track, frame_list[:-1], idx, max_pid_id, link_len)\n        match_indexes, match_scores = best_matching_hungarian(\n            all_cors, cur_all_pids, cur_all_pids_fff, track[next_frame_name], weights, weights_fff, num, mag)\n    \n        for pid1, pid2 in match_indexes:\n            if match_scores[pid1][pid2] > match_thres:\n                track[next_frame_name][pid2+1][\'new_pid\'] = cur_all_pids[pid1][\'new_pid\']\n                max_pid_id = max(max_pid_id, track[next_frame_name][pid2+1][\'new_pid\'])\n                track[next_frame_name][pid2+1][\'match_score\'] = match_scores[pid1][pid2]\n\n        # add the untracked new person\n        for next_pid in range(1, track[next_frame_name][\'num_boxes\'] + 1):\n            if \'new_pid\' not in track[next_frame_name][next_pid]:\n                max_pid_id += 1\n                track[next_frame_name][next_pid][\'new_pid\'] = max_pid_id\n                track[next_frame_name][next_pid][\'match_score\'] = 0\n\n    np.save(\'track-bl.npy\',track)\n    # track = np.load(\'track-bl.npy\').item()\n    \n    # calculate number of people\n    num_persons = 0\n    for fid, frame_name in enumerate(frame_list):\n        for pid in range(1, track[frame_name][\'num_boxes\']+1):\n            num_persons = max(num_persons, track[frame_name][pid][\'new_pid\'])\n    print(""This video contains %d people.""%(num_persons))\n\n    # export tracking result into notrack json files\n    print(""Export tracking results to json...\\n"")\n    for fid, frame_name in enumerate(tqdm(frame_list)):\n        for pid in range(track[frame_name][\'num_boxes\']):\n            notrack[frame_name][pid][\'idx\'] = track[frame_name][pid+1][\'new_pid\']\n\n    with open(tracked_json,\'w\') as json_file:\n        json_file.write(json.dumps(notrack))\n\n    if len(args.visdir)>0:\n        cmap = plt.cm.get_cmap(""hsv"", num_persons)\n        display_pose(image_dir, vis_dir, notrack, cmap)\n'"
PoseFlow/utils.py,0,"b""# coding: utf-8\n\n'''\nFile: utils.py\nProject: AlphaPose\nFile Created: Thursday, 1st March 2018 5:32:34 pm\nAuthor: Yuliang Xiu (yuliangxiu@sjtu.edu.cn)\n-----\nLast Modified: Thursday, 20th March 2018 1:18:17 am\nModified By: Yuliang Xiu (yuliangxiu@sjtu.edu.cn>)\n-----\nCopyright 2018 - 2018 Shanghai Jiao Tong University, Machine Vision and Intelligence Group\n'''\n\nimport numpy as np\nimport cv2 as cv\nimport os\nimport json\nimport copy\nimport heapq\nfrom concurrent.futures import ProcessPoolExecutor\nfrom munkres import Munkres, print_matrix\nfrom PIL import Image\nfrom tqdm import tqdm\n\n\n# keypoint penalty weight\ndelta = 2*np.array([0.01388152, 0.01515228, 0.01057665, 0.01417709, 0.01497891, 0.01402144, \\\n                    0.03909642, 0.03686941, 0.01981803, 0.03843971, 0.03412318, 0.02415081, \\\n                    0.01291456, 0.01236173,0.01291456, 0.01236173])\n\n\n# get expand bbox surrounding single person's keypoints\ndef get_box(pose, imgpath):\n\n    pose = np.array(pose).reshape(-1,3)\n    xmin = np.min(pose[:,0])\n    xmax = np.max(pose[:,0])\n    ymin = np.min(pose[:,1])\n    ymax = np.max(pose[:,1])\n    \n    img_height, img_width, _ = cv.imread(imgpath).shape\n\n    return expand_bbox(xmin, xmax, ymin, ymax, img_width, img_height)\n\n# expand bbox for containing more background\ndef expand_bbox(left, right, top, bottom, img_width, img_height):\n\n    width = right - left\n    height = bottom - top\n    ratio = 0.1 # expand ratio\n    new_left = np.clip(left - ratio * width, 0, img_width)\n    new_right = np.clip(right + ratio * width, 0, img_width)\n    new_top = np.clip(top - ratio * height, 0, img_height)\n    new_bottom = np.clip(bottom + ratio * height, 0, img_height)\n\n    return [int(new_left), int(new_right), int(new_top), int(new_bottom)]\n\n# calculate final matching grade\ndef cal_grade(l, w):\n    return sum(np.array(l)*np.array(w))\n\n# calculate IoU of two boxes(thanks @ZongweiZhou1)\ndef cal_bbox_iou(boxA, boxB): \n\n    xA = max(boxA[0], boxB[0]) #xmin\n    yA = max(boxA[2], boxB[2]) #ymin\n    xB = min(boxA[1], boxB[1]) #xmax\n    yB = min(boxA[3], boxB[3]) #ymax\n\n    if xA < xB and yA < yB: \n        interArea = (xB - xA + 1) * (yB - yA + 1) \n        boxAArea = (boxA[1] - boxA[0] + 1) * (boxA[3] - boxA[2] + 1) \n        boxBArea = (boxB[1] - boxB[0] + 1) * (boxB[3] - boxB[2] + 1) \n        iou = interArea / float(boxAArea + boxBArea - interArea+0.00001) \n    else: \n        iou=0.0\n\n    return iou\n\n# calculate OKS between two single poses\ndef compute_oks(anno, predict, delta):\n    \n    xmax = np.max(np.vstack((anno[:, 0], predict[:, 0])))\n    xmin = np.min(np.vstack((anno[:, 0], predict[:, 0])))\n    ymax = np.max(np.vstack((anno[:, 1], predict[:, 1])))\n    ymin = np.min(np.vstack((anno[:, 1], predict[:, 1])))\n    scale = (xmax - xmin) * (ymax - ymin)\n    dis = np.sum((anno - predict)**2, axis=1)\n    oks = np.mean(np.exp(-dis / 2 / delta**2 / scale))\n\n    return oks\n\n# stack all already tracked people's info together(thanks @ZongweiZhou1)\ndef stack_all_pids(track_vid, frame_list, idxs, max_pid_id, link_len):\n    \n    #track_vid contains track_vid[<=idx]\n    all_pids_info = []\n    all_pids_fff = [] # boolean list, 'fff' means From Former Frame\n    all_pids_ids = [(item+1) for item in range(max_pid_id)]\n    \n    for idx in np.arange(idxs,max(idxs-link_len,-1),-1):\n        for pid in range(1, track_vid[frame_list[idx]]['num_boxes']+1):\n            if len(all_pids_ids) == 0:\n                return all_pids_info, all_pids_fff\n            elif track_vid[frame_list[idx]][pid]['new_pid'] in all_pids_ids:\n                all_pids_ids.remove(track_vid[frame_list[idx]][pid]['new_pid'])\n                all_pids_info.append(track_vid[frame_list[idx]][pid])\n                if idx == idxs:\n                    all_pids_fff.append(True)\n                else:\n                    all_pids_fff.append(False)\n    return all_pids_info, all_pids_fff\n\n# calculate DeepMatching Pose IoU given two boxes\ndef find_two_pose_box_iou(pose1_box, pose2_box, all_cors):\n    \n    x1, y1, x2, y2 = [all_cors[:, col] for col in range(4)]\n    x_min, x_max, y_min, y_max = pose1_box\n    x1_region_ids = set(np.where((x1 >= x_min) & (x1 <= x_max))[0].tolist())\n    y1_region_ids = set(np.where((y1 >= y_min) & (y1 <= y_max))[0].tolist())\n    region_ids1 = x1_region_ids & y1_region_ids\n    x_min, x_max, y_min, y_max = pose2_box\n    x2_region_ids = set(np.where((x2 >= x_min) & (x2 <= x_max))[0].tolist())\n    y2_region_ids = set(np.where((y2 >= y_min) & (y2 <= y_max))[0].tolist())\n    region_ids2 = x2_region_ids & y2_region_ids\n    inter = region_ids1 & region_ids2\n    union = region_ids1 | region_ids2\n    pose_box_iou = len(inter) / (len(union) + 0.00001)\n\n    return pose_box_iou\n\n# calculate general Pose IoU(only consider top NUM matched keypoints)\ndef cal_pose_iou(pose1_box,pose2_box, num,mag):\n    \n    pose_iou = []\n    for row in range(len(pose1_box)):\n        x1,y1 = pose1_box[row]\n        x2,y2 = pose2_box[row]\n        box1 = [x1-mag,x1+mag,y1-mag,y1+mag]\n        box2 = [x2-mag,x2+mag,y2-mag,y2+mag]\n        pose_iou.append(cal_bbox_iou(box1,box2))\n\n    return np.mean(heapq.nlargest(num, pose_iou))\n\n# calculate DeepMatching based Pose IoU(only consider top NUM matched keypoints)\ndef cal_pose_iou_dm(all_cors,pose1,pose2,num,mag):\n    \n    poses_iou = []\n    for ids in range(len(pose1)):\n        pose1_box = [pose1[ids][0]-mag,pose1[ids][0]+mag,pose1[ids][1]-mag,pose1[ids][1]+mag]\n        pose2_box = [pose2[ids][0]-mag,pose2[ids][0]+mag,pose2[ids][1]-mag,pose2[ids][1]+mag]\n        poses_iou.append(find_two_pose_box_iou(pose1_box, pose2_box, all_cors))\n\n    return np.mean(heapq.nlargest(num, poses_iou))\n        \n# hungarian matching algorithm(thanks @ZongweiZhou1)\ndef _best_matching_hungarian(all_cors, all_pids_info, all_pids_fff, track_vid_next_fid, weights, weights_fff, num, mag):\n    \n    x1, y1, x2, y2 = [all_cors[:, col] for col in range(4)]\n    all_grades_details = []\n    all_grades = []\n    \n    box1_num = len(all_pids_info)\n    box2_num = track_vid_next_fid['num_boxes']\n    cost_matrix = np.zeros((box1_num, box2_num))\n\n    for pid1 in range(box1_num):\n        box1_pos = all_pids_info[pid1]['box_pos']\n        box1_region_ids = find_region_cors_last(box1_pos, all_cors)\n        box1_score = all_pids_info[pid1]['box_score']\n        box1_pose = all_pids_info[pid1]['box_pose_pos']\n        box1_fff = all_pids_fff[pid1]\n\n        for pid2 in range(1, track_vid_next_fid['num_boxes'] + 1):\n            box2_pos = track_vid_next_fid[pid2]['box_pos']\n            box2_region_ids = find_region_cors_next(box2_pos, all_cors)\n            box2_score = track_vid_next_fid[pid2]['box_score']\n            box2_pose = track_vid_next_fid[pid2]['box_pose_pos']\n                        \n            inter = box1_region_ids & box2_region_ids\n            union = box1_region_ids | box2_region_ids\n            dm_iou = len(inter) / (len(union) + 0.00001)\n            box_iou = cal_bbox_iou(box1_pos, box2_pos)\n            pose_iou_dm = cal_pose_iou_dm(all_cors, box1_pose, box2_pose, num,mag)\n            pose_iou = cal_pose_iou(box1_pose, box2_pose,num,mag)\n            if box1_fff:\n                grade = cal_grade([dm_iou, box_iou, pose_iou_dm, pose_iou, box1_score, box2_score], weights)\n            else:\n                grade = cal_grade([dm_iou, box_iou, pose_iou_dm, pose_iou, box1_score, box2_score], weights_fff)\n                \n            cost_matrix[pid1, pid2 - 1] = grade\n    m = Munkres()\n    indexes = m.compute((-np.array(cost_matrix)).tolist())\n\n    return indexes, cost_matrix\n\n# multiprocessing version of hungarian matching algorithm\ndef best_matching_hungarian(all_cors, all_pids_info, all_pids_fff, track_vid_next_fid, weights, weights_fff, num, mag, pool_size=5):\n    x1, y1, x2, y2 = [all_cors[:, col] for col in range(4)]\n    all_grades_details = []\n    all_grades = []\n    \n    box1_num = len(all_pids_info)\n    box2_num = track_vid_next_fid['num_boxes']\n    cost_matrix = np.zeros((box1_num, box2_num))\n\n    qsize = box1_num * track_vid_next_fid['num_boxes']\n    pool = ProcessPoolExecutor(max_workers=pool_size)\n    futures = []\n    for pid1 in range(box1_num):\n        box1_pos = all_pids_info[pid1]['box_pos']\n        box1_region_ids = find_region_cors_last(box1_pos, all_cors)\n        box1_score = all_pids_info[pid1]['box_score']\n        box1_pose = all_pids_info[pid1]['box_pose_pos']\n        box1_fff = all_pids_fff[pid1]\n\n        for pid2 in range(1, track_vid_next_fid['num_boxes'] + 1):\n            future = pool.submit(best_matching_hungarian_kernel, pid1, pid2, all_cors, track_vid_next_fid, weights, weights_fff, num, mag, box1_pos, box1_region_ids, box1_score, box1_pose, box1_fff)\n            futures.append(future)\n\n    pool.shutdown(True)\n    for future in futures:\n        pid1, pid2, grade = future.result()\n        cost_matrix[pid1, pid2 - 1] = grade\n    m = Munkres()\n    indexes = m.compute((-np.array(cost_matrix)).tolist())\n\n    return indexes, cost_matrix\n\n# one iteration of hungarian matching algorithm\ndef best_matching_hungarian_kernel(pid1, pid2, all_cors, track_vid_next_fid, weights, weights_fff, num, mag, box1_pos, box1_region_ids, box1_score, box1_pose, box1_fff):\n    box2_pos = track_vid_next_fid[pid2]['box_pos']\n    box2_region_ids = find_region_cors_next(box2_pos, all_cors)\n    box2_score = track_vid_next_fid[pid2]['box_score']\n    box2_pose = track_vid_next_fid[pid2]['box_pose_pos']\n                        \n    inter = box1_region_ids & box2_region_ids\n    union = box1_region_ids | box2_region_ids\n    dm_iou = len(inter) / (len(union) + 0.00001)\n    box_iou = cal_bbox_iou(box1_pos, box2_pos)\n    pose_iou_dm = cal_pose_iou_dm(all_cors, box1_pose, box2_pose, num,mag)\n    pose_iou = cal_pose_iou(box1_pose, box2_pose,num,mag)\n    if box1_fff:\n        grade = cal_grade([dm_iou, box_iou, pose_iou_dm, pose_iou, box1_score, box2_score], weights)\n    else:\n        grade = cal_grade([dm_iou, box_iou, pose_iou_dm, pose_iou, box1_score, box2_score], weights_fff)  \n    return (pid1, pid2, grade)\n\n# calculate number of matching points in one box from last frame\ndef find_region_cors_last(box_pos, all_cors):\n    \n    x1, y1, x2, y2 = [all_cors[:, col] for col in range(4)]\n    x_min, x_max, y_min, y_max = box_pos\n    x1_region_ids = set(np.where((x1 >= x_min) & (x1 <= x_max))[0].tolist())\n    y1_region_ids = set(np.where((y1 >= y_min) & (y1 <= y_max))[0].tolist())\n    region_ids = x1_region_ids & y1_region_ids\n\n    return region_ids\n\n# calculate number of matching points in one box from next frame\ndef find_region_cors_next(box_pos, all_cors):\n    \n    x1, y1, x2, y2 = [all_cors[:, col] for col in range(4)]\n    x_min, x_max, y_min, y_max = box_pos\n    x2_region_ids = set(np.where((x2 >= x_min) & (x2 <= x_max))[0].tolist())\n    y2_region_ids = set(np.where((y2 >= y_min) & (y2 <= y_max))[0].tolist())\n    region_ids = x2_region_ids & y2_region_ids\n\n    return region_ids\n\n# fill the nose keypoint by averaging head and neck\ndef add_nose(array):\n    \n    if min(array.shape) == 2:\n        head = array[-1,:]\n        neck = array[-2,:]\n    else:\n        head = array[-1]\n        neck = array[-2]\n    nose = (head+neck)/2.0\n\n    return np.insert(array,-1,nose,axis=0)\n\n# list remove operation\ndef remove_list(l1,vname,l2):\n    \n    for item in l2:\n        l1.remove(os.path.join(vname,item))\n        \n    return l1\n"""
alphapose/__init__.py,0,"b""from .version import __version__, short_version\n\n__all__ = ['__version__', 'short_version']\n"""
alphapose/opt.py,2,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\nimport argparse\nimport logging\nimport os\nfrom types import MethodType\n\nimport torch\n\nfrom .utils.config import update_config\n\nparser = argparse.ArgumentParser(description=\'AlphaPose Training\')\n\n""----------------------------- Experiment options -----------------------------""\nparser.add_argument(\'--cfg\',\n                    help=\'experiment configure file name\',\n                    required=True,\n                    type=str)\nparser.add_argument(\'--exp-id\', default=\'default\', type=str,\n                    help=\'Experiment ID\')\n\n""----------------------------- General options -----------------------------""\nparser.add_argument(\'--nThreads\', default=60, type=int,\n                    help=\'Number of data loading threads\')\nparser.add_argument(\'--snapshot\', default=2, type=int,\n                    help=\'How often to take a snapshot of the model (0 = never)\')\n\nparser.add_argument(\'--rank\', default=-1, type=int,\n                    help=\'node rank for distributed training\')\nparser.add_argument(\'--dist-url\', default=\'tcp://192.168.1.214:23345\', type=str,\n                    help=\'url used to set up distributed training\')\nparser.add_argument(\'--dist-backend\', default=\'nccl\', type=str,\n                    help=\'distributed backend\')\nparser.add_argument(\'--launcher\', choices=[\'none\', \'pytorch\', \'slurm\', \'mpi\'], default=\'none\',\n                    help=\'job launcher\')\n\n""----------------------------- Training options -----------------------------""\nparser.add_argument(\'--sync\', default=False, dest=\'sync\',\n                    help=\'Use Sync Batchnorm\', action=\'store_true\')\nparser.add_argument(\'--detector\', dest=\'detector\',\n                    help=\'detector name\', default=""yolo"")\n\n""----------------------------- Log options -----------------------------""\nparser.add_argument(\'--board\', default=True, dest=\'board\',\n                    help=\'Logging with tensorboard\', action=\'store_true\')\nparser.add_argument(\'--debug\', default=False, dest=\'debug\',\n                    help=\'Visualization debug\', action=\'store_true\')\nparser.add_argument(\'--map\', default=True, dest=\'map\',\n                    help=\'Evaluate mAP per epoch\', action=\'store_true\')\n\n\nopt = parser.parse_args()\ncfg_file_name = os.path.basename(opt.cfg)\ncfg = update_config(opt.cfg)\n\ncfg[\'FILE_NAME\'] = cfg_file_name\ncfg.TRAIN.DPG_STEP = [i - cfg.TRAIN.DPG_MILESTONE for i in cfg.TRAIN.DPG_STEP]\nopt.world_size = cfg.TRAIN.WORLD_SIZE\nopt.work_dir = \'./exp/{}-{}/\'.format(opt.exp_id, cfg_file_name)\nopt.gpus = [i for i in range(torch.cuda.device_count())]\nopt.device = torch.device(""cuda:"" + str(opt.gpus[0]) if opt.gpus[0] >= 0 else ""cpu"")\n\nif not os.path.exists(""./exp/{}-{}"".format(opt.exp_id, cfg_file_name)):\n    os.makedirs(""./exp/{}-{}"".format(opt.exp_id, cfg_file_name))\n\nfilehandler = logging.FileHandler(\n    \'./exp/{}-{}/training.log\'.format(opt.exp_id, cfg_file_name))\nstreamhandler = logging.StreamHandler()\n\nlogger = logging.getLogger(\'\')\nlogger.setLevel(logging.INFO)\nlogger.addHandler(filehandler)\nlogger.addHandler(streamhandler)\n\n\ndef epochInfo(self, set, idx, loss, acc):\n    self.info(\'{set}-{idx:d} epoch | loss:{loss:.8f} | acc:{acc:.4f}\'.format(\n        set=set,\n        idx=idx,\n        loss=loss,\n        acc=acc\n    ))\n\n\nlogger.epochInfo = MethodType(epochInfo, logger)\n'"
alphapose/version.py,0,"b""# GENERATED VERSION FILE\n# TIME: Wed May 27 17:09:47 2020\n\n__version__ = '0.3.0+3873077'\nshort_version = '0.3.0'\n"""
detector/apis.py,0,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Chao Xu (xuchao.19962007@sjtu.edu.cn)\n# -----------------------------------------------------\n\n""""""API of detector""""""\nfrom abc import ABC, abstractmethod\n\n\ndef get_detector(opt=None):\n    if opt.detector == \'yolo\':\n        from detector.yolo_api import YOLODetector\n        from detector.yolo_cfg import cfg\n        return YOLODetector(cfg, opt)\n    elif opt.detector == \'tracker\':\n        from detector.tracker_api import Tracker\n        from detector.tracker_cfg import cfg\n        return Tracker(cfg, opt)\n    elif opt.detector.startswith(\'efficientdet_d\'):\n        from detector.effdet_api import EffDetDetector\n        from detector.effdet_cfg import cfg\n        return EffDetDetector(cfg, opt)\n    else:\n        raise NotImplementedError\n\n\nclass BaseDetector(ABC):\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def image_preprocess(self, img_name):\n        pass\n\n    @abstractmethod\n    def images_detection(self, imgs, orig_dim_list):\n        pass\n\n    @abstractmethod\n    def detect_one_img(self, img_name):\n        pass\n'"
detector/effdet_api.py,27,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Haoshu Fang (fhaoshu@gmail.com)\n# -----------------------------------------------------\n\n""""""API of efficientdet detector""""""\nimport os\nimport sys\nsys.path.insert(0, os.path.dirname(__file__))\nfrom abc import ABC, abstractmethod\nimport platform\n\nimport torch\nimport numpy as np\n\nfrom efficientdet.utils import unique, prep_image, prep_frame, bbox_iou\nfrom efficientdet.effdet import EfficientDet, get_efficientdet_config, DetBenchEval, load_checkpoint\n\nfrom detector.apis import BaseDetector\n\ntry:\n    from apex import amp\n    has_amp = True\nexcept ImportError:\n    has_amp = False\n\n#only windows visual studio 2013 ~2017 support compile c/cuda extensions\n#If you force to compile extension on Windows and ensure appropriate visual studio\n#is intalled, you can try to use these ext_modules.\nif platform.system() != \'Windows\':\n    from detector.nms import nms_wrapper\n\n\nclass EffDetDetector(BaseDetector):\n    def __init__(self, cfg, opt=None):\n        super(EffDetDetector, self).__init__()\n\n        self.detector_cfg = cfg\n        self.detector_opt = opt\n        self.model_cfg = get_efficientdet_config(opt.detector)\n        self.model_weights = \'detector/efficientdet/weights/\'+opt.detector+\'.pth\'\n        #Input image dimension, uses model default if empty\n        self.inp_dim = cfg.get(\'INP_DIM\', None) if cfg.get(\'INP_DIM\', None) is not None else self.model_cfg.image_size\n        self.nms_thres = cfg.get(\'NMS_THRES\', 0.6)\n        self.confidence = cfg.get(\'CONFIDENCE\', 0.05)\n        self.num_classes = cfg.get(\'NUM_CLASSES\', 80)\n        self.max_dets = cfg.get(\'MAX_DETECTIONS\', 100)\n        self.model = None\n\n\n    def load_model(self):\n        args = self.detector_opt\n\n        net = EfficientDet(self.model_cfg)\n        load_checkpoint(net, self.model_weights)\n        self.model = DetBenchEval(net, self.model_cfg, nms_thres=self.nms_thres, max_dets=self.max_dets)\n\n        if args:\n            if len(args.gpus) > 1:\n                if has_amp:\n                    print(\'Using AMP mixed precision.\')\n                    self.model = amp.initialize(self.model, opt_level=\'O1\')\n                else:\n                    print(\'AMP not installed, running network in FP32.\')\n\n                self.model = torch.nn.DataParallel(self.model, device_ids=args.gpus).to(args.device)\n            else:\n                self.model.to(args.device)\n        else:\n            if has_amp:\n                print(\'Using AMP mixed precision.\')\n                self.model = amp.initialize(self.model, opt_level=\'O1\')\n            else:\n                print(\'AMP not installed, running network in FP32.\')\n            self.model.cuda()\n\n        net.eval()\n\n    def image_preprocess(self, img_source):\n        """"""\n        Pre-process the img before fed to the object detection network\n        Input: image name(str) or raw image data(ndarray or torch.Tensor,channel GBR)\n        Output: pre-processed image data(torch.FloatTensor,(1,3,h,w))\n        """"""\n        if isinstance(img_source, str):\n            img, orig_img, im_dim_list = prep_image(img_source, self.inp_dim)\n        elif isinstance(img_source, torch.Tensor) or isinstance(img_source, np.ndarray):\n            img, orig_img, im_dim_list = prep_frame(img_source, self.inp_dim)\n        else:\n            raise IOError(\'Unknown image source type: {}\'.format(type(img_source)))\n\n        return img\n\n    def images_detection(self, imgs, orig_dim_list):\n        """"""\n        Feed the img data into object detection network and \n        collect bbox w.r.t original image size\n        Input: imgs(torch.FloatTensor,(b,3,h,w)): pre-processed mini-batch image input\n               orig_dim_list(torch.FloatTensor, (b,(w,h,w,h))): original mini-batch image size\n        Output: dets(torch.cuda.FloatTensor,(n,(batch_idx,x1,y1,x2,y2,c,s,idx of cls))): human detection results\n        """"""\n        args = self.detector_opt\n        if not self.model:\n            self.load_model()\n        with torch.no_grad():\n            imgs = imgs.to(args.device) if args else imgs.cuda()\n            scaling_factors = torch.FloatTensor([1./min(self.inp_dim / orig_dim[0], self.inp_dim / orig_dim[1]) for orig_dim in orig_dim_list]).view(-1, 1)\n            scaling_factors = scaling_factors.to(args.device) if args else scaling_factors.cuda()\n            prediction = self.model(imgs, scaling_factors) \n            #change the pred format to alphapose (nms has already been done in effdeteval model)\n            prediction = prediction.cpu()\n            write = False\n            for index, sample in enumerate(prediction):\n                for det in sample:\n                    score = float(det[4])\n                    if score < .001:  # stop when below this threshold, scores in descending order\n                        break\n                    if int(det[5]) != 1 or score < self.confidence:\n                        continue\n                    det_new = prediction.new(1,8)\n                    det_new[0,0] = index    #index of img\n                    det_new[0,1:3] = det[0:2]  # bbox x1,y1\n                    det_new[0,3:5] = det[0:2] + det[2:4] # bbox x2,y2\n                    det_new[0,6:7] = det[4]  # cls conf\n                    det_new[0,7] = det[5]   # cls idx\n                    if not write:\n                        dets = det_new\n                        write = True\n                    else:\n                        dets = torch.cat((dets, det_new))    \n            if not write:\n                return 0\n\n            orig_dim_list = torch.index_select(orig_dim_list, 0, dets[:, 0].long())\n            for i in range(dets.shape[0]):\n                dets[i, [1, 3]] = torch.clamp(dets[i, [1, 3]], 0.0, orig_dim_list[i, 0])\n                dets[i, [2, 4]] = torch.clamp(dets[i, [2, 4]], 0.0, orig_dim_list[i, 1])\n\n            return dets\n\n    def detect_one_img(self, img_name):\n        """"""\n        Detect bboxs in one image\n        Input: \'str\', full path of image\n        Output: \'[{""category_id"":1,""score"":float,""bbox"":[x,y,w,h],""image_id"":str},...]\',\n        The output results are similar with coco results type, except that image_id uses full path str\n        instead of coco %012d id for generalization. \n        """"""\n        args = self.detector_opt\n        _CUDA = True\n        if args:\n            if args.gpus[0] < 0:\n                _CUDA = False\n        if not self.model:\n            self.load_model()\n        if isinstance(self.model, torch.nn.DataParallel):\n            self.model = self.model.module\n        dets_results = []\n        #pre-process(scale, normalize, ...) the image\n        img, orig_img, img_dim_list = prep_image(img_name, self.inp_dim)\n        with torch.no_grad():\n            img_dim_list = torch.FloatTensor([img_dim_list]).repeat(1, 2)\n            img = img.to(args.device) if args else img.cuda()\n            scaling_factor = torch.FloatTensor([1/min(self.inp_dim / orig_dim[0], self.inp_dim / orig_dim[1]) for orig_dim in img_dim_list]).view(-1, 1)\n            scaling_factor = scaling_factor.to(args.device) if args else scaling_factor.cuda()\n            prediction = self.model(img, scaling_factor) \n            #change the pred format to alphapose (nms has already been done in effdeteval model)\n            prediction = prediction.cpu()\n            write = False\n            for index, sample in enumerate(prediction):\n                for det in sample:\n                    score = float(det[4])\n                    if score < .001:  # stop when below this threshold, scores in descending order\n                        break\n                    if int(det[5]) != 1 or score < self.confidence:\n                        continue\n                    det_new = prediction.new(1,8)\n                    det_new[0,0] = index    #index of img\n                    det_new[0,1:3] = det[0:2]  # bbox x1,y1\n                    det_new[0,3:5] = det[0:2] + det[2:4] # bbox x2,y2\n                    det_new[0,6:7] = det[4]  # cls conf\n                    det_new[0,7] = det[5]   # cls idx\n                    if not write:\n                        dets = det_new\n                        write = True\n                    else:\n                        dets = torch.cat((dets, det_new))          \n            if not write:\n                return None\n\n            img_dim_list = torch.index_select(img_dim_list, 0, dets[:, 0].long())\n            for i in range(dets.shape[0]):\n                dets[i, [1, 3]] = torch.clamp(dets[i, [1, 3]], 0.0, img_dim_list[i, 0])\n                dets[i, [2, 4]] = torch.clamp(dets[i, [2, 4]], 0.0, img_dim_list[i, 1])\n\n                #write results\n                det_dict = {}\n                x = float(dets[i, 1])\n                y = float(dets[i, 2])\n                w = float(dets[i, 3] - dets[i, 1])\n                h = float(dets[i, 4] - dets[i, 2])\n                det_dict[""category_id""] = 1\n                det_dict[""score""] = float(dets[i, 5])\n                det_dict[""bbox""] = [x, y, w, h]\n                det_dict[""image_id""] = int(os.path.basename(img_name).split(\'.\')[0])\n                dets_results.append(det_dict)\n\n            return dets_results\n\n\n    def check_detector(self, img_name):\n        """"""\n        Detect bboxs in one image\n        Input: \'str\', full path of image\n        Output: \'[{""category_id"":1,""score"":float,""bbox"":[x,y,w,h],""image_id"":str},...]\',\n        The output results are similar with coco results type, except that image_id uses full path str\n        instead of coco %012d id for generalization. \n        """"""\n        args = self.detector_opt\n        _CUDA = True\n        if args:\n            if args.gpus[0] < 0:\n                _CUDA = False\n        if not self.model:\n            self.load_model()\n        if isinstance(self.model, torch.nn.DataParallel):\n            self.model = self.model.module\n        dets_results = []\n        #pre-process(scale, normalize, ...) the image\n        img, orig_img, img_dim_list = prep_image(img_name, self.inp_dim)\n        with torch.no_grad():\n            img_dim_list = torch.FloatTensor([img_dim_list]).repeat(1, 2)\n            img = img.to(args.device) if args else img.cuda()\n            scaling_factor = torch.FloatTensor([1/min(self.inp_dim / orig_dim[0], self.inp_dim / orig_dim[1]) for orig_dim in img_dim_list]).view(-1, 1)\n            scaling_factor = scaling_factor.to(args.device) if args else scaling_factor.cuda()\n            output = self.model(img, scaling_factor) \n\n            output = output.cpu()\n            for index, sample in enumerate(output):\n                image_id = int(os.path.basename(img_name).split(\'.\')[0])\n                for det in sample:\n                    score = float(det[4])\n                    if score < .001:  # stop when below this threshold, scores in descending order\n                        break\n                    #### uncomment it for only human detection\n                    # if int(det[5]) != 1 or score < self.confidence:\n                    #     continue\n                    coco_det = dict(\n                        image_id=image_id,\n                        bbox=det[0:4].tolist(),\n                        score=score,\n                        category_id=int(det[5]))\n                    dets_results.append(coco_det)\n\n        return dets_results\n\nif __name__ == ""__main__"":\n#run with python detector/effdet_api.py /DATA1/Benchmark/coco/ efficientdet_d0\n        from pycocotools.coco import COCO\n        from pycocotools.cocoeval import COCOeval\n        from easydict import EasyDict as edict\n        from apis import get_detector\n        from tqdm import tqdm\n        import json\n\n        opt = edict()\n        _coco = COCO(sys.argv[1]+\'/annotations/instances_val2017.json\')\n        # _coco = COCO(sys.argv[1]+\'/annotations/person_keypoints_val2017.json\')\n        opt.detector = sys.argv[2]\n        opt.gpus = [0] if torch.cuda.device_count() >= 1 else [-1]\n        opt.device = torch.device(""cuda:"" + str(opt.gpus[0]) if opt.gpus[0] >= 0 else ""cpu"")\n        image_ids = sorted(_coco.getImgIds())\n        det_model = get_detector(opt)\n        dets = []\n        for entry in tqdm(_coco.loadImgs(image_ids)):\n            abs_path = os.path.join(\n                sys.argv[1], \'val2017\', entry[\'file_name\'])\n            det = det_model.check_detector(abs_path)\n            if det:\n                dets += det\n        result_file = \'results.json\'\n        json.dump(dets, open(result_file, \'w\'))\n\n        coco_results = _coco.loadRes(result_file)\n        coco_eval = COCOeval(_coco, coco_results, \'bbox\')\n        coco_eval.params.imgIds = image_ids  # score only ids we\'ve used\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        coco_eval.summarize()\n'"
detector/effdet_cfg.py,0,b'from easydict import EasyDict as edict\r\n\r\ncfg = edict()\r\n\r\ncfg.NMS_THRES =  0.6  # 0.6(0.713) 0.5(0.707)\r\ncfg.CONFIDENCE = 0.2  # 0.15       0.1\r\ncfg.NUM_CLASSES = 80\r\ncfg.MAX_DETECTIONS = 200  # 100\r\n'
detector/tracker_api.py,11,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Haoshu Fang (fhaoshu@gmail.com)\n# -----------------------------------------------------\n\n""""""API of yolo tracker""""""\nimport os\nimport sys\nsys.path.insert(0, os.path.dirname(__file__))\nfrom abc import ABC, abstractmethod\nimport platform\n\nimport torch\nimport numpy as np\n\nfrom tracker.tracker.multitracker import STrack, joint_stracks, sub_stracks, remove_duplicate_stracks\n\nfrom tracker.preprocess import prep_image, prep_frame\nfrom tracker.utils.kalman_filter import KalmanFilter\nfrom tracker.utils.utils import non_max_suppression, scale_coords\nfrom tracker.utils.log import logger\nfrom tracker.tracker import matching\nfrom tracker.tracker.basetrack import BaseTrack, TrackState\nfrom tracker.models import Darknet\n\nfrom detector.apis import BaseDetector\n\n\nclass Tracker(BaseDetector):\n    def __init__(self, cfg, opt=None):\n        super(Tracker, self).__init__()\n\n        self.tracker_opt = opt\n        self.model_cfg = cfg.get(\'CONFIG\', \'detector/tracker/cfg/yolov3.cfg\')\n        self.model_weights = cfg.get(\'WEIGHTS\', \'detector/tracker/data/jde.1088x608.uncertainty.pt\')\n        self.img_size = cfg.get(\'IMG_SIZE\', (1088, 608))\n        self.nms_thres = cfg.get(\'NMS_THRES\', 0.6)\n        self.confidence = cfg.get(\'CONFIDENCE\', 0.05)\n        self.max_time_lost = cfg.get(\'BUFFER_SIZE\', 30) # buffer\n        self.model = None\n        \n        self.tracked_stracks = []  # type: list[STrack]\n        self.lost_stracks = []  # type: list[STrack]\n        self.removed_stracks = []  # type: list[STrack]\n\n        self.frame_id = 0\n        self.emb_dim = None\n        \n\n        self.kalman_filter = KalmanFilter()\n\n    def load_model(self):\n        print(\'Loading tracking model..\')\n        self.model = Darknet(self.model_cfg, self.img_size, nID=14455)\n        # load_darknet_weights(self.model, args.weights)\n        self.model.load_state_dict(torch.load(self.model_weights, map_location=\'cpu\')[\'model\'], strict=False)\n        self.emb_dim = self.model.emb_dim\n\n        if self.tracker_opt:\n            if len(self.tracker_opt.gpus) > 1:\n                self.model = torch.nn.DataParallel(self.model, device_ids=self.tracker_opt.gpus).to(self.tracker_opt.device)\n            else:\n                self.model.to(self.tracker_opt.device)\n        else:\n            self.model.cuda()\n        self.model.eval()\n        print(""Network successfully loaded"")\n\n        \n\n    def image_preprocess(self, img_source):\n        """"""\n        Pre-process the img before fed to the object detection network\n        Input: image name(str) or raw image data(ndarray or torch.Tensor,channel GBR)\n        Output: pre-processed image data(torch.FloatTensor,(1,3,h,w))\n        """"""\n        if isinstance(img_source, str):\n            img, orig_img, im_dim_list = prep_image(img_source, self.img_size)\n        elif isinstance(img_source, torch.Tensor) or isinstance(img_source, np.ndarray):\n            img, orig_img, im_dim_list = prep_frame(img_source, self.img_size)\n        else:\n            raise IOError(\'Unknown image source type: {}\'.format(type(img_source)))\n\n        return img\n\n    def images_detection(self, imgs, orig_dim_list):\n        """"""\n        Feed the img data into object detection network and \n        collect bbox w.r.t original image size\n        Input: imgs(torch.FloatTensor,(b,3,h,w)): pre-processed mini-batch image input\n               orig_dim_list(torch.FloatTensor, (b,(w,h,w,h))): original mini-batch image size\n        Output: dets(torch.cuda.FloatTensor,(n,(batch_idx,x1,y1,x2,y2,c,s,idx of cls))): human detection results\n        """"""\n        args = self.tracker_opt\n        _CUDA = True\n        if args:\n            if args.gpus[0] < 0:\n                _CUDA = False\n        if not self.model:\n            self.load_model()\n        \n        \n        activated_starcks = []\n        refind_stracks = []\n        lost_stracks = []\n        removed_stracks = []\n\n        \'\'\' Step 1: Network forward, get detections & embeddings\'\'\'\n        with torch.no_grad():\n            imgs = imgs.to(args.device) if args else imgs.cuda()\n            pred = self.model(imgs)\n\n        if len(pred) > 0:\n            dets = non_max_suppression(pred, self.confidence, self.nms_thres)\n\n        output_stracks = []\n        for image_i in range(len(imgs)):\n            self.frame_id += 1\n            if dets[image_i] is not None:\n                det_i = scale_coords(self.img_size, dets[image_i], orig_dim_list[image_i])\n                \'\'\'Detections\'\'\'\n                detections = [STrack(STrack.tlbr_to_tlwh(tlbrs[:4]), tlbrs[4], f.numpy(), 30) for\n                              (tlbrs, f) in zip(det_i[:, :5], det_i[:, -self.emb_dim:])]\n            else:\n                detections = []\n\n\n            \'\'\' Add newly detected tracklets to tracked_stracks\'\'\'\n            unconfirmed = []\n            tracked_stracks = []  # type: list[STrack]\n            for track in self.tracked_stracks:\n                if not track.is_activated:\n                    unconfirmed.append(track)\n                else:\n                    tracked_stracks.append(track)\n\n            \'\'\' Step 2: First association, with embedding\'\'\'\n            strack_pool = joint_stracks(tracked_stracks, self.lost_stracks)\n            # Predict the current location with KF\n            for strack in strack_pool:\n                strack.predict()\n\n            dists = matching.embedding_distance(strack_pool, detections)\n            dists = matching.gate_cost_matrix(self.kalman_filter, dists, strack_pool, detections)\n            matches, u_track, u_detection = matching.linear_assignment(dists, thresh=0.7)\n\n            for itracked, idet in matches:\n                track = strack_pool[itracked]\n                det = detections[idet]\n                if track.state == TrackState.Tracked:\n                    track.update(detections[idet], self.frame_id)\n                    activated_starcks.append(track)\n                else:\n                    track.re_activate(det, self.frame_id, new_id=False)\n                    refind_stracks.append(track)\n\n            \'\'\' Step 3: Second association, with IOU\'\'\'\n            detections = [detections[i] for i in u_detection]\n            r_tracked_stracks = [strack_pool[i] for i in u_track if strack_pool[i].state==TrackState.Tracked ]\n            dists = matching.iou_distance(r_tracked_stracks, detections)\n            matches, u_track, u_detection = matching.linear_assignment(dists, thresh=0.5)\n            \n            for itracked, idet in matches:\n                track = r_tracked_stracks[itracked]\n                det = detections[idet]\n                if track.state == TrackState.Tracked:\n                    track.update(det, self.frame_id)\n                    activated_starcks.append(track)\n                else:\n                    track.re_activate(det, self.frame_id, new_id=False)\n                    refind_stracks.append(track)\n\n            for it in u_track:\n                track = r_tracked_stracks[it]\n                if not track.state == TrackState.Lost:\n                    track.mark_lost()\n                    lost_stracks.append(track)\n\n            \'\'\'Deal with unconfirmed tracks, usually tracks with only one beginning frame\'\'\'\n            detections = [detections[i] for i in u_detection]\n            dists = matching.iou_distance(unconfirmed, detections)\n            matches, u_unconfirmed, u_detection = matching.linear_assignment(dists, thresh=0.7)\n            for itracked, idet in matches:\n                unconfirmed[itracked].update(detections[idet], self.frame_id)\n                activated_starcks.append(unconfirmed[itracked])\n            for it in u_unconfirmed:\n                track = unconfirmed[it]\n                track.mark_removed()\n                removed_stracks.append(track)\n\n            """""" Step 4: Init new stracks""""""\n            for inew in u_detection:\n                track = detections[inew]\n                if track.score < self.confidence:\n                    continue\n                track.activate(self.kalman_filter, self.frame_id)\n                activated_starcks.append(track)\n\n            """""" Step 5: Update state""""""\n            for track in self.lost_stracks:\n                if self.frame_id - track.end_frame > self.max_time_lost:\n                    track.mark_removed()\n                    removed_stracks.append(track)\n\n            self.tracked_stracks = [t for t in self.tracked_stracks if t.state == TrackState.Tracked]\n            self.tracked_stracks = joint_stracks(self.tracked_stracks, activated_starcks)\n            self.tracked_stracks = joint_stracks(self.tracked_stracks, refind_stracks)\n            # self.lost_stracks = [t for t in self.lost_stracks if t.state == TrackState.Lost]  # type: list[STrack]\n            self.lost_stracks = sub_stracks(self.lost_stracks, self.tracked_stracks)\n            self.lost_stracks.extend(lost_stracks)\n            self.lost_stracks = sub_stracks(self.lost_stracks, self.removed_stracks)\n            self.removed_stracks.extend(removed_stracks)\n            self.tracked_stracks, self.lost_stracks = remove_duplicate_stracks(self.tracked_stracks, self.lost_stracks)\n\n            if self.tracker_opt.debug:\n                logger.debug(\'===========Frame {}==========\'.format(self.frame_id))\n                logger.debug(\'Activated: {}\'.format([track.track_id for track in activated_starcks]))\n                logger.debug(\'Refind: {}\'.format([track.track_id for track in refind_stracks]))\n                logger.debug(\'Lost: {}\'.format([track.track_id for track in lost_stracks]))\n                logger.debug(\'Removed: {}\'.format([track.track_id for track in removed_stracks]))\n\n            # Add tracks to outputs\n            for t in self.tracked_stracks:\n                tlwh = t.tlwh\n                tid = t.track_id\n                tlbr = t.tlbr\n                ts = t.score\n                if tlwh[2] * tlwh[3] > self.tracker_opt.min_box_area:\n                    res = torch.tensor([image_i, tlbr[0], tlbr[1], tlbr[2], tlbr[3], ts, tid])\n                output_stracks.append(res)\n\n        if len(output_stracks) == 0:\n            return 0\n            \n        return torch.stack(output_stracks)\n\n    def detect_one_img(self, img_name):\n        pass'"
detector/tracker_cfg.py,0,"b""from easydict import EasyDict as edict\r\n\r\ncfg = edict()\r\ncfg.CONFIG = 'detector/tracker/cfg/yolov3.cfg'\r\ncfg.WEIGHTS = 'detector/tracker/data/jde.1088x608.uncertainty.pt'\r\ncfg.IMG_SIZE =  (1088, 608)\r\ncfg.NMS_THRES =  0.6\r\ncfg.CONFIDENCE = 0.4\r\ncfg.BUFFER_SIZE = 30 # frame buffer"""
detector/yolo_api.py,29,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Chao Xu (xuchao.19962007@sjtu.edu.cn)\n# -----------------------------------------------------\n\n""""""API of yolo detector""""""\nimport os\nimport sys\nsys.path.insert(0, os.path.dirname(__file__))\nfrom abc import ABC, abstractmethod\nimport platform\n\nimport torch\nimport numpy as np\n\nfrom yolo.preprocess import prep_image, prep_frame\nfrom yolo.darknet import Darknet\nfrom yolo.util import unique\nfrom yolo.bbox import bbox_iou\n\nfrom detector.apis import BaseDetector\n\n#only windows visual studio 2013 ~2017 support compile c/cuda extensions\n#If you force to compile extension on Windows and ensure appropriate visual studio\n#is intalled, you can try to use these ext_modules.\nif platform.system() != \'Windows\':\n    from detector.nms import nms_wrapper\n\n\nclass YOLODetector(BaseDetector):\n    def __init__(self, cfg, opt=None):\n        super(YOLODetector, self).__init__()\n\n        self.detector_cfg = cfg\n        self.detector_opt = opt\n        self.model_cfg = cfg.get(\'CONFIG\', \'detector/yolo/cfg/yolov3-spp.cfg\')\n        self.model_weights = cfg.get(\'WEIGHTS\', \'detector/yolo/data/yolov3-spp.weights\')\n        self.inp_dim = cfg.get(\'INP_DIM\', 608)\n        self.nms_thres = cfg.get(\'NMS_THRES\', 0.6)\n        self.confidence = cfg.get(\'CONFIDENCE\', 0.05)\n        self.num_classes = cfg.get(\'NUM_CLASSES\', 80)\n        self.model = None\n\n    def load_model(self):\n        args = self.detector_opt\n\n        print(\'Loading YOLO model..\')\n        self.model = Darknet(self.model_cfg)\n        self.model.load_weights(self.model_weights)\n        self.model.net_info[\'height\'] = self.inp_dim\n        print(""Network successfully loaded"")\n\n        if args:\n            if len(args.gpus) > 1:\n                self.model = torch.nn.DataParallel(self.model, device_ids=args.gpus).to(args.device)\n            else:\n                self.model.to(args.device)\n        else:\n            self.model.cuda()\n        self.model.eval()\n\n    def image_preprocess(self, img_source):\n        """"""\n        Pre-process the img before fed to the object detection network\n        Input: image name(str) or raw image data(ndarray or torch.Tensor,channel GBR)\n        Output: pre-processed image data(torch.FloatTensor,(1,3,h,w))\n        """"""\n        if isinstance(img_source, str):\n            img, orig_img, im_dim_list = prep_image(img_source, self.inp_dim)\n        elif isinstance(img_source, torch.Tensor) or isinstance(img_source, np.ndarray):\n            img, orig_img, im_dim_list = prep_frame(img_source, self.inp_dim)\n        else:\n            raise IOError(\'Unknown image source type: {}\'.format(type(img_source)))\n\n        return img\n\n    def images_detection(self, imgs, orig_dim_list):\n        """"""\n        Feed the img data into object detection network and \n        collect bbox w.r.t original image size\n        Input: imgs(torch.FloatTensor,(b,3,h,w)): pre-processed mini-batch image input\n               orig_dim_list(torch.FloatTensor, (b,(w,h,w,h))): original mini-batch image size\n        Output: dets(torch.cuda.FloatTensor,(n,(batch_idx,x1,y1,x2,y2,c,s,idx of cls))): human detection results\n        """"""\n        args = self.detector_opt\n        _CUDA = True\n        if args:\n            if args.gpus[0] < 0:\n                _CUDA = False\n        if not self.model:\n            self.load_model()\n        with torch.no_grad():\n            imgs = imgs.to(args.device) if args else imgs.cuda()\n            prediction = self.model(imgs, args=args) \n            #do nms to the detection results, only human category is left\n            dets = self.dynamic_write_results(prediction, self.confidence, \n                                              self.num_classes, nms=True, \n                                              nms_conf=self.nms_thres)\n            if isinstance(dets, int) or dets.shape[0] == 0:\n                return 0\n            dets = dets.cpu()\n\n            orig_dim_list = torch.index_select(orig_dim_list, 0, dets[:, 0].long())\n            scaling_factor = torch.min(self.inp_dim / orig_dim_list, 1)[0].view(-1, 1)\n            dets[:, [1, 3]] -= (self.inp_dim - scaling_factor * orig_dim_list[:, 0].view(-1, 1)) / 2\n            dets[:, [2, 4]] -= (self.inp_dim - scaling_factor * orig_dim_list[:, 1].view(-1, 1)) / 2\n            dets[:, 1:5] /= scaling_factor\n            for i in range(dets.shape[0]):\n                dets[i, [1, 3]] = torch.clamp(dets[i, [1, 3]], 0.0, orig_dim_list[i, 0])\n                dets[i, [2, 4]] = torch.clamp(dets[i, [2, 4]], 0.0, orig_dim_list[i, 1])\n\n            return dets\n\n    def dynamic_write_results(self, prediction, confidence, num_classes, nms=True, nms_conf=0.4):\n        prediction_bak = prediction.clone()\n        dets = self.write_results(prediction.clone(), confidence, num_classes, nms, nms_conf)\n        if isinstance(dets, int):\n            return dets\n\n        if dets.shape[0] > 100:\n            nms_conf -= 0.05\n            dets = self.write_results(prediction_bak.clone(), confidence, num_classes, nms, nms_conf)\n\n        return dets\n\n    def write_results(self, prediction, confidence, num_classes, nms=True, nms_conf=0.4):\n        args = self.detector_opt\n        #prediction: (batchsize, num of objects, (xc,yc,w,h,box confidence, 80 class scores))\n        conf_mask = (prediction[:, :, 4] > confidence).float().float().unsqueeze(2)\n        prediction = prediction * conf_mask\n\n        try:\n            ind_nz = torch.nonzero(prediction[:,:,4]).transpose(0,1).contiguous()\n        except:\n            return 0\n\n        #the 3rd channel of prediction: (xc,yc,w,h)->(x1,y1,x2,y2)\n        box_a = prediction.new(prediction.shape)\n        box_a[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)\n        box_a[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)\n        box_a[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2) \n        box_a[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)\n        prediction[:,:,:4] = box_a[:,:,:4]\n\n        batch_size = prediction.size(0)\n\n        output = prediction.new(1, prediction.size(2) + 1)\n        write = False\n        num = 0\n        for ind in range(batch_size):\n            #select the image from the batch\n            image_pred = prediction[ind]\n\n            #Get the class having maximum score, and the index of that class\n            #Get rid of num_classes softmax scores \n            #Add the class index and the class score of class having maximum score\n            max_conf, max_conf_score = torch.max(image_pred[:,5:5+ num_classes], 1)\n            max_conf = max_conf.float().unsqueeze(1)\n            max_conf_score = max_conf_score.float().unsqueeze(1)\n            seq = (image_pred[:,:5], max_conf, max_conf_score)\n            #image_pred:(n,(x1,y1,x2,y2,c,s,idx of cls))\n            image_pred = torch.cat(seq, 1)\n\n            #Get rid of the zero entries\n            non_zero_ind =  (torch.nonzero(image_pred[:,4]))\n\n            image_pred_ = image_pred[non_zero_ind.squeeze(),:].view(-1,7)\n\n            #Get the various classes detected in the image\n            try:\n                img_classes = unique(image_pred_[:,-1])\n            except:\n                continue\n\n            #WE will do NMS classwise\n            #print(img_classes)\n            for cls in img_classes:\n                if cls != 0:\n                    continue\n                #get the detections with one particular class\n                cls_mask = image_pred_*(image_pred_[:,-1] == cls).float().unsqueeze(1)\n                class_mask_ind = torch.nonzero(cls_mask[:,-2]).squeeze()\n\n                image_pred_class = image_pred_[class_mask_ind].view(-1,7)\n\n                #sort the detections such that the entry with the maximum objectness\n                #confidence is at the top\n                conf_sort_index = torch.sort(image_pred_class[:,4], descending = True )[1]\n                image_pred_class = image_pred_class[conf_sort_index]\n                idx = image_pred_class.size(0)\n\n                #if nms has to be done\n                if nms:\n                    if platform.system() != \'Windows\':\n                        #We use faster rcnn implementation of nms (soft nms is optional)\n                        nms_op = getattr(nms_wrapper, \'nms\')\n                        #nms_op input:(n,(x1,y1,x2,y2,c))\n                        #nms_op output: input[inds,:], inds\n                        _, inds = nms_op(image_pred_class[:,:5], nms_conf)\n\n                        image_pred_class = image_pred_class[inds]\n                    else:\n                        # Perform non-maximum suppression\n                        max_detections = []\n                        while image_pred_class.size(0):\n                            # Get detection with highest confidence and save as max detection\n                            max_detections.append(image_pred_class[0].unsqueeze(0))\n                            # Stop if we\'re at the last detection\n                            if len(image_pred_class) == 1:\n                                break\n                            # Get the IOUs for all boxes with lower confidence\n                            ious = bbox_iou(max_detections[-1], image_pred_class[1:], args)\n                            # Remove detections with IoU >= NMS threshold\n                            image_pred_class = image_pred_class[1:][ious < nms_conf]\n\n                        image_pred_class = torch.cat(max_detections).data\n\n                #Concatenate the batch_id of the image to the detection\n                #this helps us identify which image does the detection correspond to \n                #We use a linear straucture to hold ALL the detections from the batch\n                #the batch_dim is flattened\n                #batch is identified by extra batch column\n\n                batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind)\n                seq = batch_ind, image_pred_class\n                if not write:\n                    output = torch.cat(seq,1)\n                    write = True\n                else:\n                    out = torch.cat(seq,1)\n                    output = torch.cat((output,out))\n                num += 1\n    \n        if not num:\n            return 0\n        #output:(n,(batch_ind,x1,y1,x2,y2,c,s,idx of cls))\n        return output\n\n    def detect_one_img(self, img_name):\n        """"""\n        Detect bboxs in one image\n        Input: \'str\', full path of image\n        Output: \'[{""category_id"":1,""score"":float,""bbox"":[x,y,w,h],""image_id"":str},...]\',\n        The output results are similar with coco results type, except that image_id uses full path str\n        instead of coco %012d id for generalization. \n        """"""\n        args = self.detector_opt\n        _CUDA = True\n        if args:\n            if args.gpus[0] < 0:\n                _CUDA = False\n        if not self.model:\n            self.load_model()\n        if isinstance(self.model, torch.nn.DataParallel):\n            self.model = self.model.module\n        dets_results = []\n        #pre-process(scale, normalize, ...) the image\n        img, orig_img, img_dim_list = prep_image(img_name, self.inp_dim)\n        with torch.no_grad():\n            img_dim_list = torch.FloatTensor([img_dim_list]).repeat(1, 2)\n            img = img.to(args.device) if args else img.cuda()\n            prediction = self.model(img, args=args)\n            #do nms to the detection results, only human category is left\n            dets = self.dynamic_write_results(prediction, self.confidence,\n                                              self.num_classes, nms=True,\n                                              nms_conf=self.nms_thres)\n            if isinstance(dets, int) or dets.shape[0] == 0:\n                return None\n            dets = dets.cpu()\n\n            img_dim_list = torch.index_select(img_dim_list, 0, dets[:, 0].long())\n            scaling_factor = torch.min(self.inp_dim / img_dim_list, 1)[0].view(-1, 1)\n            dets[:, [1, 3]] -= (self.inp_dim - scaling_factor * img_dim_list[:, 0].view(-1, 1)) / 2\n            dets[:, [2, 4]] -= (self.inp_dim - scaling_factor * img_dim_list[:, 1].view(-1, 1)) / 2\n            dets[:, 1:5] /= scaling_factor\n            for i in range(dets.shape[0]):\n                dets[i, [1, 3]] = torch.clamp(dets[i, [1, 3]], 0.0, img_dim_list[i, 0])\n                dets[i, [2, 4]] = torch.clamp(dets[i, [2, 4]], 0.0, img_dim_list[i, 1])\n\n                #write results\n                det_dict = {}\n                x = float(dets[i, 1])\n                y = float(dets[i, 2])\n                w = float(dets[i, 3] - dets[i, 1])\n                h = float(dets[i, 4] - dets[i, 2])\n                det_dict[""category_id""] = 1\n                det_dict[""score""] = float(dets[i, 5])\n                det_dict[""bbox""] = [x, y, w, h]\n                det_dict[""image_id""] = int(os.path.basename(img_name).split(\'.\')[0])\n                dets_results.append(det_dict)\n\n            return dets_results\n'"
detector/yolo_cfg.py,0,"b""from easydict import EasyDict as edict\r\n\r\ncfg = edict()\r\ncfg.CONFIG = 'detector/yolo/cfg/yolov3-spp.cfg'\r\ncfg.WEIGHTS = 'detector/yolo/data/yolov3-spp.weights'\r\ncfg.INP_DIM =  608\r\ncfg.NMS_THRES =  0.6\r\ncfg.CONFIDENCE = 0.05\r\ncfg.NUM_CLASSES = 80"""
scripts/demo_inference.py,9,"b'""""""Script for single-gpu/multi-gpu demo.""""""\nimport argparse\nimport os\nimport platform\nimport sys\nimport time\n\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\nfrom detector.apis import get_detector\nfrom alphapose.models import builder\nfrom alphapose.utils.config import update_config\nfrom alphapose.utils.detector import DetectionLoader\nfrom alphapose.utils.pPose_nms import write_json\nfrom alphapose.utils.transforms import flip, flip_heatmap\nfrom alphapose.utils.vis import getTime\nfrom alphapose.utils.webcam_detector import WebCamDetectionLoader\nfrom alphapose.utils.writer import DataWriter\n\n""""""----------------------------- Demo options -----------------------------""""""\nparser = argparse.ArgumentParser(description=\'AlphaPose Demo\')\nparser.add_argument(\'--cfg\', type=str, required=True,\n                    help=\'experiment configure file name\')\nparser.add_argument(\'--checkpoint\', type=str, required=True,\n                    help=\'checkpoint file name\')\nparser.add_argument(\'--sp\', default=False, action=\'store_true\',\n                    help=\'Use single process for pytorch\')\nparser.add_argument(\'--detector\', dest=\'detector\',\n                    help=\'detector name\', default=""yolo"")\nparser.add_argument(\'--detfile\', dest=\'detfile\',\n                    help=\'detection result file\', default="""")\nparser.add_argument(\'--indir\', dest=\'inputpath\',\n                    help=\'image-directory\', default="""")\nparser.add_argument(\'--list\', dest=\'inputlist\',\n                    help=\'image-list\', default="""")\nparser.add_argument(\'--image\', dest=\'inputimg\',\n                    help=\'image-name\', default="""")\nparser.add_argument(\'--outdir\', dest=\'outputpath\',\n                    help=\'output-directory\', default=""examples/res/"")\nparser.add_argument(\'--save_img\', default=False, action=\'store_true\',\n                    help=\'save result as image\')\nparser.add_argument(\'--vis\', default=False, action=\'store_true\',\n                    help=\'visualize image\')\nparser.add_argument(\'--showbox\', default=False, action=\'store_true\',\n                    help=\'visualize human bbox\')\nparser.add_argument(\'--profile\', default=False, action=\'store_true\',\n                    help=\'add speed profiling at screen output\')\nparser.add_argument(\'--format\', type=str,\n                    help=\'save in the format of cmu or coco or openpose, option: coco/cmu/open\')\nparser.add_argument(\'--min_box_area\', type=int, default=0,\n                    help=\'min box area to filter out\')\nparser.add_argument(\'--detbatch\', type=int, default=5,\n                    help=\'detection batch size PER GPU\')\nparser.add_argument(\'--posebatch\', type=int, default=80,\n                    help=\'pose estimation maximum batch size PER GPU\')\nparser.add_argument(\'--eval\', dest=\'eval\', default=False, action=\'store_true\',\n                    help=\'save the result json as coco format, using image index(int) instead of image name(str)\')\nparser.add_argument(\'--gpus\', type=str, dest=\'gpus\', default=""0"",\n                    help=\'choose which cuda device to use by index and input comma to use multi gpus, e.g. 0,1,2,3. (input -1 for cpu only)\')\nparser.add_argument(\'--qsize\', type=int, dest=\'qsize\', default=1024,\n                    help=\'the length of result buffer, where reducing it will lower requirement of cpu memory\')\nparser.add_argument(\'--flip\', default=False, action=\'store_true\',\n                    help=\'enable flip testing\')\nparser.add_argument(\'--debug\', default=False, action=\'store_true\',\n                    help=\'print detail information\')\n""""""----------------------------- Video options -----------------------------""""""\nparser.add_argument(\'--video\', dest=\'video\',\n                    help=\'video-name\', default="""")\nparser.add_argument(\'--webcam\', dest=\'webcam\', type=int,\n                    help=\'webcam number\', default=-1)\nparser.add_argument(\'--save_video\', dest=\'save_video\',\n                    help=\'whether to save rendered video\', default=False, action=\'store_true\')\nparser.add_argument(\'--vis_fast\', dest=\'vis_fast\',\n                    help=\'use fast rendering\', action=\'store_true\', default=False)\nparser.add_argument(\'--pose_track\', dest=\'pose_track\',\n                    help=\'track humans in video\', action=\'store_true\', default=False)\n\nargs = parser.parse_args()\ncfg = update_config(args.cfg)\n\nif platform.system() == \'Windows\':\n    args.sp = True\n\nargs.gpus = [int(i) for i in args.gpus.split(\',\')] if torch.cuda.device_count() >= 1 else [-1]\nargs.device = torch.device(""cuda:"" + str(args.gpus[0]) if args.gpus[0] >= 0 else ""cpu"")\nargs.detbatch = args.detbatch * len(args.gpus)\nargs.posebatch = args.posebatch * len(args.gpus)\nargs.tracking = (args.detector == \'tracker\')\n\nif not args.sp:\n    torch.multiprocessing.set_start_method(\'forkserver\', force=True)\n    torch.multiprocessing.set_sharing_strategy(\'file_system\')\n\n\ndef check_input():\n    # for wecam\n    if args.webcam != -1:\n        args.detbatch = 1\n        return \'webcam\', int(args.webcam)\n\n    # for video\n    if len(args.video):\n        if os.path.isfile(args.video):\n            videofile = args.video\n            return \'video\', videofile\n        else:\n            raise IOError(\'Error: --video must refer to a video file, not directory.\')\n\n    # for detection results\n    if len(args.detfile):\n        if os.path.isfile(args.detfile):\n            detfile = args.detfile\n            return \'detfile\', detfile\n        else:\n            raise IOError(\'Error: --detfile must refer to a detection json file, not directory.\')\n\n    # for images\n    if len(args.inputpath) or len(args.inputlist) or len(args.inputimg):\n        inputpath = args.inputpath\n        inputlist = args.inputlist\n        inputimg = args.inputimg\n\n        if len(inputlist):\n            im_names = open(inputlist, \'r\').readlines()\n        elif len(inputpath) and inputpath != \'/\':\n            for root, dirs, files in os.walk(inputpath):\n                im_names = files\n        elif len(inputimg):\n            im_names = [inputimg]\n\n        return \'image\', im_names\n\n    else:\n        raise NotImplementedError\n\n\ndef print_finish_info():\n    print(\'===========================> Finish Model Running.\')\n    if (args.save_img or args.save_video) and not args.vis_fast:\n        print(\'===========================> Rendering remaining images in the queue...\')\n        print(\'===========================> If this step takes too long, you can enable the --vis_fast flag to use fast rendering (real-time).\')\n\n\ndef loop():\n    n = 0\n    while True:\n        yield n\n        n += 1\n\n\nif __name__ == ""__main__"":\n    mode, input_source = check_input()\n\n    if not os.path.exists(args.outputpath):\n        os.makedirs(args.outputpath)\n\n    # Load detection loader\n    if mode == \'webcam\':\n        det_loader = WebCamDetectionLoader(input_source, get_detector(args), cfg, args)\n        det_worker = det_loader.start()\n    elif mode == \'detfile\':\n        det_loader = FileDetectionLoader(input_source, cfg, args)\n        det_worker = det_loader.start()\n    else:\n        det_loader = DetectionLoader(input_source, get_detector(args), cfg, args, batchSize=args.detbatch, mode=mode, queueSize=args.qsize)\n        det_worker = det_loader.start()\n\n    # Load pose model\n    pose_model = builder.build_sppe(cfg.MODEL, preset_cfg=cfg.DATA_PRESET)\n\n    print(f\'Loading pose model from {args.checkpoint}...\')\n    pose_model.load_state_dict(torch.load(args.checkpoint, map_location=args.device))\n\n    if len(args.gpus) > 1:\n        pose_model = torch.nn.DataParallel(pose_model, device_ids=args.gpus).to(args.device)\n    else:\n        pose_model.to(args.device)\n    pose_model.eval()\n\n    runtime_profile = {\n        \'dt\': [],\n        \'pt\': [],\n        \'pn\': []\n    }\n\n    # Init data writer\n    queueSize = 2 if mode == \'webcam\' else args.qsize\n    if args.save_video and mode != \'image\':\n        from alphapose.utils.writer import DEFAULT_VIDEO_SAVE_OPT as video_save_opt\n        if mode == \'video\':\n            video_save_opt[\'savepath\'] = os.path.join(args.outputpath, \'AlphaPose_\' + os.path.basename(input_source))\n        else:\n            video_save_opt[\'savepath\'] = os.path.join(args.outputpath, \'AlphaPose_webcam\' + str(input_source) + \'.mp4\')\n        video_save_opt.update(det_loader.videoinfo)\n        writer = DataWriter(cfg, args, save_video=True, video_save_opt=video_save_opt, queueSize=queueSize).start()\n    else:\n        writer = DataWriter(cfg, args, save_video=False, queueSize=queueSize).start()\n\n    if mode == \'webcam\':\n        print(\'Starting webcam demo, press Ctrl + C to terminate...\')\n        sys.stdout.flush()\n        im_names_desc = tqdm(loop())\n    else:\n        data_len = det_loader.length\n        im_names_desc = tqdm(range(data_len), dynamic_ncols=True)\n\n    batchSize = args.posebatch\n    if args.flip:\n        batchSize = int(batchSize / 2)\n    try:\n        for i in im_names_desc:\n            start_time = getTime()\n            with torch.no_grad():\n                (inps, orig_img, im_name, boxes, scores, ids, cropped_boxes) = det_loader.read()\n                if orig_img is None:\n                    break\n                if boxes is None or boxes.nelement() == 0:\n                    writer.save(None, None, None, None, None, orig_img, os.path.basename(im_name))\n                    continue\n                if args.profile:\n                    ckpt_time, det_time = getTime(start_time)\n                    runtime_profile[\'dt\'].append(det_time)\n                # Pose Estimation\n                inps = inps.to(args.device)\n                datalen = inps.size(0)\n                leftover = 0\n                if (datalen) % batchSize:\n                    leftover = 1\n                num_batches = datalen // batchSize + leftover\n                hm = []\n                for j in range(num_batches):\n                    inps_j = inps[j * batchSize:min((j + 1) * batchSize, datalen)]\n                    if args.flip:\n                        inps_j = torch.cat((inps_j, flip(inps_j)))\n                    hm_j = pose_model(inps_j)\n                    if args.flip:\n                        hm_j_flip = flip_heatmap(hm_j[int(len(hm_j) / 2):], det_loader.joint_pairs, shift=True)\n                        hm_j = (hm_j[0:int(len(hm_j) / 2)] + hm_j_flip) / 2\n                    hm.append(hm_j)\n                hm = torch.cat(hm)\n                if args.profile:\n                    ckpt_time, pose_time = getTime(ckpt_time)\n                    runtime_profile[\'pt\'].append(pose_time)\n                hm = hm.cpu()\n                writer.save(boxes, scores, ids, hm, cropped_boxes, orig_img, os.path.basename(im_name))\n\n                if args.profile:\n                    ckpt_time, post_time = getTime(ckpt_time)\n                    runtime_profile[\'pn\'].append(post_time)\n\n            if args.profile:\n                # TQDM\n                im_names_desc.set_description(\n                    \'det time: {dt:.4f} | pose time: {pt:.4f} | post processing: {pn:.4f}\'.format(\n                        dt=np.mean(runtime_profile[\'dt\']), pt=np.mean(runtime_profile[\'pt\']), pn=np.mean(runtime_profile[\'pn\']))\n                )\n        print_finish_info()\n        while(writer.running()):\n            time.sleep(1)\n            print(\'===========================> Rendering remaining \' + str(writer.count()) + \' images in the queue...\')\n        writer.stop()\n        det_loader.stop()\n    except Exception as e:\n        print(repr(e))\n        print(\'An error as above occurs when processing the images, please check it\')\n        pass\n    except KeyboardInterrupt:\n        print_finish_info()\n        # Thread won\'t be killed when press Ctrl+C\n        if args.sp:\n            det_loader.terminate()\n            while(writer.running()):\n                time.sleep(1)\n                print(\'===========================> Rendering remaining \' + str(writer.count()) + \' images in the queue...\')\n            writer.stop()\n        else:\n            # subprocesses are killed, manually clear queues\n            for p in det_worker:\n                p.terminate()\n            writer.commit()\n            writer.clear_queues()\n            # det_loader.clear_queues()\n    final_result = writer.results()\n    write_json(final_result, args.outputpath, form=args.format, for_eval=args.eval)\n    print(""Results have been written to json."")\n'"
scripts/train.py,18,"b'""""""Script for multi-gpu training.""""""\nimport json\nimport os\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nfrom tensorboardX import SummaryWriter\nfrom tqdm import tqdm\n\nfrom alphapose.models import builder\nfrom alphapose.opt import cfg, logger, opt\nfrom alphapose.utils.logger import board_writing, debug_writing\nfrom alphapose.utils.metrics import DataLogger, calc_accuracy, evaluate_mAP\nfrom alphapose.utils.transforms import get_func_heatmap_to_coord\n\nnum_gpu = torch.cuda.device_count()\nvalid_batch = 1 * num_gpu\nif opt.sync:\n    norm_layer = nn.SyncBatchNorm\nelse:\n    norm_layer = nn.BatchNorm2d\n\n\ndef train(opt, train_loader, m, criterion, optimizer, writer):\n    loss_logger = DataLogger()\n    acc_logger = DataLogger()\n    m.train()\n\n    train_loader = tqdm(train_loader, dynamic_ncols=True)\n\n    for i, (inps, labels, label_masks, _, bboxes) in enumerate(train_loader):\n        if isinstance(inps, list):\n            inps = [inp.cuda().requires_grad_() for inp in inps]\n        else:\n            inps = inps.cuda().requires_grad_()\n        labels = labels.cuda()\n        label_masks = label_masks.cuda()\n\n        output = m(inps)\n\n        loss = 0.5 * criterion(output.mul(label_masks), labels.mul(label_masks))\n        acc = calc_accuracy(output.mul(label_masks), labels.mul(label_masks))\n\n        if isinstance(inps, list):\n            batch_size = inps[0].size(0)\n        else:\n            batch_size = inps.size(0)\n\n        loss_logger.update(loss.item(), batch_size)\n        acc_logger.update(acc, batch_size)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        opt.trainIters += 1\n        # Tensorboard\n        if opt.board:\n            board_writing(writer, loss_logger.avg, acc_logger.avg, opt.trainIters, \'Train\')\n\n        # Debug\n        if opt.debug and not i % 10:\n            debug_writing(writer, output, labels, inps, opt.trainIters)\n\n        # TQDM\n        train_loader.set_description(\n            \'loss: {loss:.8f} | acc: {acc:.4f}\'.format(\n                loss=loss_logger.avg,\n                acc=acc_logger.avg)\n        )\n\n    train_loader.close()\n\n    return loss_logger.avg, acc_logger.avg\n\n\ndef validate(m, opt, heatmap_to_coord, batch_size=20):\n    det_dataset = builder.build_dataset(cfg.DATASET.TEST, preset_cfg=cfg.DATA_PRESET, train=False, opt=opt)\n    det_loader = torch.utils.data.DataLoader(\n        det_dataset, batch_size=batch_size, shuffle=False, num_workers=20, drop_last=False)\n    kpt_json = []\n    eval_joints = det_dataset.EVAL_JOINTS\n\n    m.eval()\n\n    for inps, crop_bboxes, bboxes, img_ids, scores, imghts, imgwds in tqdm(det_loader, dynamic_ncols=True):\n        if isinstance(inps, list):\n            inps = [inp.cuda() for inp in inps]\n        else:\n            inps = inps.cuda()\n        output = m(inps)\n\n        pred = output.cpu().data.numpy()\n        assert pred.ndim == 4\n        pred = pred[:, eval_joints, :, :]\n\n        for i in range(output.shape[0]):\n            bbox = crop_bboxes[i].tolist()\n            pose_coords, pose_scores = heatmap_to_coord(pred[i][det_dataset.EVAL_JOINTS], bbox)\n\n            keypoints = np.concatenate((pose_coords, pose_scores), axis=1)\n            keypoints = keypoints.reshape(-1).tolist()\n\n            data = dict()\n            data[\'bbox\'] = bboxes[i, 0].tolist()\n            data[\'image_id\'] = int(img_ids[i])\n            data[\'score\'] = float(scores[i] + np.mean(pose_scores) + np.max(pose_scores))\n            data[\'category_id\'] = 1\n            data[\'keypoints\'] = keypoints\n\n            kpt_json.append(data)\n\n    with open(os.path.join(opt.work_dir, \'test_kpt.json\'), \'w\') as fid:\n        json.dump(kpt_json, fid)\n    res = evaluate_mAP(os.path.join(opt.work_dir, \'test_kpt.json\'), ann_type=\'keypoints\')\n    return res[\'AP\']\n\n\ndef validate_gt(m, opt, cfg, heatmap_to_coord, batch_size=20):\n    gt_val_dataset = builder.build_dataset(cfg.DATASET.VAL, preset_cfg=cfg.DATA_PRESET, train=False)\n    eval_joints = gt_val_dataset.EVAL_JOINTS\n\n    gt_val_loader = torch.utils.data.DataLoader(\n        gt_val_dataset, batch_size=batch_size, shuffle=False, num_workers=20, drop_last=False)\n    kpt_json = []\n    m.eval()\n\n    for inps, labels, label_masks, img_ids, bboxes in tqdm(gt_val_loader, dynamic_ncols=True):\n        if isinstance(inps, list):\n            inps = [inp.cuda() for inp in inps]\n        else:\n            inps = inps.cuda()\n        output = m(inps)\n\n        pred = output.cpu().data.numpy()\n        assert pred.ndim == 4\n        pred = pred[:, eval_joints, :, :]\n\n        for i in range(output.shape[0]):\n            bbox = bboxes[i].tolist()\n            pose_coords, pose_scores = heatmap_to_coord(pred[i][gt_val_dataset.EVAL_JOINTS], bbox)\n\n            keypoints = np.concatenate((pose_coords, pose_scores), axis=1)\n            keypoints = keypoints.reshape(-1).tolist()\n\n            data = dict()\n            data[\'bbox\'] = bboxes[i].tolist()\n            data[\'image_id\'] = int(img_ids[i])\n            data[\'score\'] = float(np.mean(pose_scores) + np.max(pose_scores))\n            data[\'category_id\'] = 1\n            data[\'keypoints\'] = keypoints\n\n            kpt_json.append(data)\n\n    with open(os.path.join(opt.work_dir, \'test_gt_kpt.json\'), \'w\') as fid:\n        json.dump(kpt_json, fid)\n    res = evaluate_mAP(os.path.join(opt.work_dir, \'test_gt_kpt.json\'), ann_type=\'keypoints\')\n    return res[\'AP\']\n\n\ndef main():\n    logger.info(\'******************************\')\n    logger.info(opt)\n    logger.info(\'******************************\')\n    logger.info(cfg)\n    logger.info(\'******************************\')\n\n    # Model Initialize\n    m = preset_model(cfg)\n    m = nn.DataParallel(m).cuda()\n\n    criterion = torch.nn.MSELoss().cuda()\n    if cfg.TRAIN.OPTIMIZER == \'adam\':\n        optimizer = torch.optim.Adam(m.parameters(), lr=cfg.TRAIN.LR)\n    elif cfg.TRAIN.OPTIMIZER == \'rmsprop\':\n        optimizer = torch.optim.RMSprop(m.parameters(), lr=cfg.TRAIN.LR)\n\n    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n        optimizer, milestones=cfg.TRAIN.LR_STEP, gamma=cfg.TRAIN.LR_FACTOR)\n\n    writer = SummaryWriter(\'.tensorboard/{}-{}\'.format(opt.exp_id, cfg.FILE_NAME))\n\n    train_dataset = builder.build_dataset(cfg.DATASET.TRAIN, preset_cfg=cfg.DATA_PRESET, train=True)\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=cfg.TRAIN.BATCH_SIZE * num_gpu, shuffle=True, num_workers=opt.nThreads)\n\n    heatmap_to_coord = get_func_heatmap_to_coord(cfg)\n\n    opt.trainIters = 0\n\n    for i in range(cfg.TRAIN.BEGIN_EPOCH, cfg.TRAIN.END_EPOCH):\n        opt.epoch = i\n        current_lr = optimizer.state_dict()[\'param_groups\'][0][\'lr\']\n\n        logger.info(f\'############# Starting Epoch {opt.epoch} | LR: {current_lr} #############\')\n\n        # Training\n        loss, miou = train(opt, train_loader, m, criterion, optimizer, writer)\n        logger.epochInfo(\'Train\', opt.epoch, loss, miou)\n\n        lr_scheduler.step()\n\n        if (i + 1) % opt.snapshot == 0:\n            # Save checkpoint\n            torch.save(m.module.state_dict(), \'./exp/{}-{}/model_{}.pth\'.format(opt.exp_id, cfg.FILE_NAME, opt.epoch))\n            # Prediction Test\n            with torch.no_grad():\n                gt_AP = validate_gt(m.module, opt, cfg, heatmap_to_coord)\n                rcnn_AP = validate(m.module, opt, heatmap_to_coord)\n                logger.info(f\'##### Epoch {opt.epoch} | gt mAP: {gt_AP} | rcnn mAP: {rcnn_AP} #####\')\n\n        # Time to add DPG\n        if i == cfg.TRAIN.DPG_MILESTONE:\n            torch.save(m.module.state_dict(), \'./exp/{}-{}/final.pth\'.format(opt.exp_id, cfg.FILE_NAME))\n            # Adjust learning rate\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] = cfg.TRAIN.LR\n            lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=cfg.TRAIN.DPG_STEP, gamma=0.1)\n            # Reset dataset\n            train_dataset = builder.build_dataset(cfg.DATASET.TRAIN, preset_cfg=cfg.DATA_PRESET, train=True, dpg=True)\n            train_loader = torch.utils.data.DataLoader(\n                train_dataset, batch_size=cfg.TRAIN.BATCH_SIZE * num_gpu, shuffle=True, num_workers=opt.nThreads)\n\n    torch.save(m.module.state_dict(), \'./exp/{}-{}/final_DPG.pth\'.format(opt.exp_id, cfg.FILE_NAME))\n\n\ndef preset_model(cfg):\n    model = builder.build_sppe(cfg.MODEL, preset_cfg=cfg.DATA_PRESET)\n\n    if cfg.MODEL.PRETRAINED:\n        logger.info(f\'Loading model from {cfg.MODEL.PRETRAINED}...\')\n        model.load_state_dict(torch.load(cfg.MODEL.PRETRAINED))\n    elif cfg.MODEL.TRY_LOAD:\n        logger.info(f\'Loading model from {cfg.MODEL.TRY_LOAD}...\')\n        pretrained_state = torch.load(cfg.MODEL.TRY_LOAD)\n        model_state = model.state_dict()\n        pretrained_state = {k: v for k, v in pretrained_state.items()\n                            if k in model_state and v.size() == model_state[k].size()}\n\n        model_state.update(pretrained_state)\n        model.load_state_dict(model_state)\n    else:\n        logger.info(\'Create new model\')\n        logger.info(\'=> init weights\')\n        model._initialize()\n\n    return model\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/validate.py,6,"b'""""""Validation script.""""""\nimport argparse\nimport json\n\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\nfrom alphapose.models import builder\nfrom alphapose.utils.config import update_config\nfrom alphapose.utils.metrics import evaluate_mAP\nfrom alphapose.utils.transforms import (flip, flip_heatmap,\n                                        get_func_heatmap_to_coord)\n\nparser = argparse.ArgumentParser(description=\'AlphaPose Validate\')\nparser.add_argument(\'--cfg\',\n                    help=\'experiment configure file name\',\n                    required=True,\n                    type=str)\nparser.add_argument(\'--checkpoint\',\n                    help=\'checkpoint file name\',\n                    required=True,\n                    type=str)\nparser.add_argument(\'--gpus\',\n                    help=\'gpus\',\n                    type=str)\nparser.add_argument(\'--batch\',\n                    help=\'validation batch size\',\n                    type=int)\nparser.add_argument(\'--flip-test\',\n                    default=False,\n                    dest=\'flip_test\',\n                    help=\'flip test\',\n                    action=\'store_true\')\nparser.add_argument(\'--detector\', dest=\'detector\',\n                    help=\'detector name\', default=""yolo"")\n\nopt = parser.parse_args()\ncfg = update_config(opt.cfg)\n\ngpus = [int(i) for i in opt.gpus.split(\',\')]\nopt.gpus = [gpus[0]]\nopt.device = torch.device(""cuda:"" + str(opt.gpus[0]) if opt.gpus[0] >= 0 else ""cpu"")\n\n\ndef validate(m, heatmap_to_coord, batch_size=20):\n    det_dataset = builder.build_dataset(cfg.DATASET.TEST, preset_cfg=cfg.DATA_PRESET, train=False, opt=opt)\n    eval_joints = det_dataset.EVAL_JOINTS\n\n    det_loader = torch.utils.data.DataLoader(\n        det_dataset, batch_size=batch_size, shuffle=False, num_workers=20, drop_last=False)\n    kpt_json = []\n    m.eval()\n\n    for inps, crop_bboxes, bboxes, img_ids, scores, imghts, imgwds in tqdm(det_loader, dynamic_ncols=True):\n        if isinstance(inps, list):\n            inps = [inp.cuda() for inp in inps]\n        else:\n            inps = inps.cuda()\n        output = m(inps)\n        if opt.flip_test:\n            if isinstance(inps, list):\n                inps_flip = [flip(inp) for inp in inps]\n            else:\n                inps_flip = flip(inps)\n            output_flip = flip_heatmap(m(inps_flip), det_dataset.joint_pairs, shift=True)\n            output = (output + output_flip) / 2\n\n        pred = output.cpu().data.numpy()\n        assert pred.ndim == 4\n        pred = pred[:, eval_joints, :, :]\n\n        for i in range(output.shape[0]):\n            bbox = crop_bboxes[i].tolist()\n            pose_coords, pose_scores = heatmap_to_coord(pred[i][det_dataset.EVAL_JOINTS], bbox)\n\n            keypoints = np.concatenate((pose_coords, pose_scores), axis=1)\n            keypoints = keypoints.reshape(-1).tolist()\n\n            data = dict()\n            data[\'bbox\'] = bboxes[i, 0].tolist()\n            data[\'image_id\'] = int(img_ids[i])\n            data[\'score\'] = float(scores[i] + np.mean(pose_scores) + np.max(pose_scores))\n            data[\'category_id\'] = 1\n            data[\'keypoints\'] = keypoints\n\n            kpt_json.append(data)\n\n    with open(\'./exp/json/validate_rcnn_kpt.json\', \'w\') as fid:\n        json.dump(kpt_json, fid)\n    res = evaluate_mAP(\'./exp/json/validate_rcnn_kpt.json\', ann_type=\'keypoints\')\n    return res[\'AP\']\n\n\ndef validate_gt(m, cfg, heatmap_to_coord, batch_size=20):\n    gt_val_dataset = builder.build_dataset(cfg.DATASET.VAL, preset_cfg=cfg.DATA_PRESET, train=False)\n    eval_joints = gt_val_dataset.EVAL_JOINTS\n\n    gt_val_loader = torch.utils.data.DataLoader(\n        gt_val_dataset, batch_size=batch_size, shuffle=False, num_workers=20, drop_last=False)\n    kpt_json = []\n    m.eval()\n\n    for inps, labels, label_masks, img_ids, bboxes in tqdm(gt_val_loader, dynamic_ncols=True):\n        if isinstance(inps, list):\n            inps = [inp.cuda() for inp in inps]\n        else:\n            inps = inps.cuda()\n        output = m(inps)\n        if opt.flip_test:\n            if isinstance(inps, list):\n                inps_flip = [flip(inp) for inp in inps]\n            else:\n                inps_flip = flip(inps)\n            output_flip = flip_heatmap(m(inps_flip), gt_val_dataset.joint_pairs, shift=True)\n            output = (output + output_flip) / 2\n\n        pred = output.cpu().data.numpy()\n        assert pred.ndim == 4\n        pred = pred[:, eval_joints, :, :]\n\n        for i in range(output.shape[0]):\n            bbox = bboxes[i].tolist()\n            pose_coords, pose_scores = heatmap_to_coord(pred[i][gt_val_dataset.EVAL_JOINTS], bbox)\n\n            keypoints = np.concatenate((pose_coords, pose_scores), axis=1)\n            keypoints = keypoints.reshape(-1).tolist()\n\n            data = dict()\n            data[\'bbox\'] = bboxes[i].tolist()\n            data[\'image_id\'] = int(img_ids[i])\n            data[\'score\'] = float(np.mean(pose_scores) + np.max(pose_scores))\n            data[\'category_id\'] = 1\n            data[\'keypoints\'] = keypoints\n\n            kpt_json.append(data)\n\n    with open(\'./exp/json/validate_gt_kpt.json\', \'w\') as fid:\n        json.dump(kpt_json, fid)\n    res = evaluate_mAP(\'./exp/json/validate_gt_kpt.json\', ann_type=\'keypoints\')\n    return res[\'AP\']\n\n\nif __name__ == ""__main__"":\n    m = builder.build_sppe(cfg.MODEL, preset_cfg=cfg.DATA_PRESET)\n\n    print(f\'Loading model from {opt.checkpoint}...\')\n    m.load_state_dict(torch.load(opt.checkpoint))\n\n    m = torch.nn.DataParallel(m, device_ids=gpus).cuda()\n    heatmap_to_coord = get_func_heatmap_to_coord(cfg)\n\n    with torch.no_grad():\n        gt_AP = validate_gt(m, cfg, heatmap_to_coord, opt.batch)\n        detbox_AP = validate(m, heatmap_to_coord, opt.batch)\n    print(\'##### gt box: {} mAP | det box: {} mAP #####\'.format(gt_AP, detbox_AP))\n'"
alphapose/datasets/__init__.py,0,"b""from .coco_det import Mscoco_det\nfrom .concat_dataset import ConcatDataset\nfrom .custom import CustomDataset\nfrom .mscoco import Mscoco\nfrom .mpii import Mpii\n\n__all__ = ['CustomDataset', 'Mscoco', 'Mscoco_det', 'Mpii', 'ConcatDataset']\n"""
alphapose/datasets/coco_det.py,2,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\n""""""MS COCO Human Detection Box dataset.""""""\nimport json\nimport os\n\nimport scipy.misc\nimport torch\nimport torch.utils.data as data\nfrom tqdm import tqdm\n\nfrom alphapose.utils.presets import SimpleTransform\nfrom detector.apis import get_detector\nfrom alphapose.models.builder import DATASET\n\n\n@DATASET.register_module\nclass Mscoco_det(data.Dataset):\n    """""" COCO human detection box dataset.\n\n    """"""\n    EVAL_JOINTS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n\n    def __init__(self,\n                 det_file=None,\n                 opt=None,\n                 **cfg):\n\n        self._cfg = cfg\n        self._opt = opt\n        self._preset_cfg = cfg[\'PRESET\']\n        self._root = cfg[\'ROOT\']\n        self._img_prefix = cfg[\'IMG_PREFIX\']\n        if not det_file:\n            det_file = cfg[\'DET_FILE\']\n        self._ann_file = os.path.join(self._root, cfg[\'ANN\'])\n\n        if os.path.exists(det_file):\n            print(""Detection results exist, will use it"")\n        else:\n            print(""Will create detection results to {}"".format(det_file))\n            self.write_coco_json(det_file)\n\n        assert os.path.exists(det_file), ""Error: no detection results found""\n        with open(det_file, \'r\') as fid:\n            self._det_json = json.load(fid)\n\n        self._input_size = self._preset_cfg[\'IMAGE_SIZE\']\n        self._output_size = self._preset_cfg[\'HEATMAP_SIZE\']\n\n        self._sigma = self._preset_cfg[\'SIGMA\']\n\n        if self._preset_cfg[\'TYPE\'] == \'simple\':\n            self.transformation = SimpleTransform(\n                self, scale_factor=0,\n                input_size=self._input_size,\n                output_size=self._output_size,\n                rot=0, sigma=self._sigma,\n                train=False, add_dpg=False)\n\n    def __getitem__(self, index):\n        det_res = self._det_json[index]\n        if not isinstance(det_res[\'image_id\'], int):\n            img_id, _ = os.path.splitext(os.path.basename(det_res[\'image_id\']))\n            img_id = int(img_id)\n        else:\n            img_id = det_res[\'image_id\']\n        img_path = \'./data/coco/val2017/%012d.jpg\' % img_id\n\n        # Load image\n        image = scipy.misc.imread(img_path, mode=\'RGB\')\n\n        imght, imgwidth = image.shape[1], image.shape[2]\n        x1, y1, w, h = det_res[\'bbox\']\n        bbox = [x1, y1, x1 + w, y1 + h]\n        inp, bbox = self.transformation.test_transform(image, bbox)\n        return inp, torch.Tensor(bbox), torch.Tensor([det_res[\'bbox\']]), torch.Tensor([det_res[\'image_id\']]), torch.Tensor([det_res[\'score\']]), torch.Tensor([imght]), torch.Tensor([imgwidth])\n\n    def __len__(self):\n        return len(self._det_json)\n\n    def write_coco_json(self, det_file):\n        from pycocotools.coco import COCO\n        import pathlib\n\n        _coco = COCO(self._ann_file)\n        image_ids = sorted(_coco.getImgIds())\n        det_model = get_detector(self._opt)\n        dets = []\n        for entry in tqdm(_coco.loadImgs(image_ids)):\n            abs_path = os.path.join(\n                self._root, self._img_prefix, entry[\'file_name\'])\n            det = det_model.detect_one_img(abs_path)\n            if det:\n                dets += det\n        pathlib.Path(os.path.split(det_file)[0]).mkdir(parents=True, exist_ok=True)\n        json.dump(dets, open(det_file, \'w\'))\n\n    @property\n    def joint_pairs(self):\n        """"""Joint pairs which defines the pairs of joint to be swapped\n        when the image is flipped horizontally.""""""\n        return [[1, 2], [3, 4], [5, 6], [7, 8],\n                [9, 10], [11, 12], [13, 14], [15, 16]]\n'"
alphapose/datasets/concat_dataset.py,3,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\nimport bisect\n\nimport torch\nimport torch.utils.data as data\n\nfrom alphapose.models.builder import DATASET, build_dataset\n\n\n@DATASET.register_module\nclass ConcatDataset(data.Dataset):\n    """"""Custom Concat dataset.\n    Annotation file must be in `coco` format.\n\n    Parameters\n    ----------\n    train: bool, default is True\n        If true, will set as training mode.\n    dpg: bool, default is False\n        If true, will activate `dpg` for data augmentation.\n    skip_empty: bool, default is False\n        Whether skip entire image if no valid label is found.\n    cfg: dict, dataset configuration.\n    """"""\n\n    def __init__(self,\n                 train=True,\n                 dpg=False,\n                 skip_empty=True,\n                 **cfg):\n\n        self._cfg = cfg\n        self._subset_cfg_list = cfg[\'SET_LIST\']\n        self._preset_cfg = cfg[\'PRESET\']\n        self._mask_id = [item[\'MASK_ID\'] for item in self._subset_cfg_list]\n\n        self.num_joints = self._preset_cfg[\'NUM_JOINTS\']\n\n        self._subsets = []\n        self._subset_size = [0]\n        for _subset_cfg in self._subset_cfg_list:\n            subset = build_dataset(_subset_cfg, preset_cfg=self._preset_cfg, train=train)\n            self._subsets.append(subset)\n            self._subset_size.append(len(subset))\n        self.cumulative_sizes = self.cumsum(self._subset_size)\n\n    def __getitem__(self, idx):\n        assert idx >= 0\n        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n        dataset_idx -= 1\n        sample_idx = idx - self.cumulative_sizes[dataset_idx]\n\n        sample = self._subsets[dataset_idx][sample_idx]\n        img, label, label_mask, img_id, bbox = sample\n\n        K = label.shape[0]  # num_joints from `_subsets[dataset_idx]`\n        expend_label = torch.zeros((self.num_joints, *label.shape[1:]), dtype=label.dtype)\n        expend_label_mask = torch.zeros((self.num_joints, *label_mask.shape[1:]), dtype=label_mask.dtype)\n        expend_label[self._mask_id[dataset_idx]:self._mask_id[dataset_idx] + K] = label\n        expend_label_mask[self._mask_id[dataset_idx]:self._mask_id[dataset_idx] + K] = label_mask\n\n        return img, expend_label, expend_label_mask, img_id, bbox\n\n    def __len__(self):\n        return self.cumulative_sizes[-1]\n\n    @staticmethod\n    def cumsum(sequence):\n        r, s = [], 0\n        for e in sequence:\n            r.append(e + s)\n            s += e\n        return r\n'"
alphapose/datasets/custom.py,1,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\n""""""Custum training dataset.""""""\nimport copy\nimport os\nimport pickle as pk\nfrom abc import abstractmethod, abstractproperty\n\nimport scipy.misc\nimport torch.utils.data as data\nfrom pycocotools.coco import COCO\n\nfrom alphapose.utils.presets import SimpleTransform\n\n\nclass CustomDataset(data.Dataset):\n    """"""Custom dataset.\n    Annotation file must be in `coco` format.\n\n    Parameters\n    ----------\n    train: bool, default is True\n        If true, will set as training mode.\n    dpg: bool, default is False\n        If true, will activate `dpg` for data augmentation.\n    skip_empty: bool, default is False\n        Whether skip entire image if no valid label is found.\n    cfg: dict, dataset configuration.\n    """"""\n\n    CLASSES = None\n\n    def __init__(self,\n                 train=True,\n                 dpg=False,\n                 skip_empty=True,\n                 lazy_import=False,\n                 **cfg):\n\n        self._cfg = cfg\n        self._preset_cfg = cfg[\'PRESET\']\n        self._root = cfg[\'ROOT\']\n        self._img_prefix = cfg[\'IMG_PREFIX\']\n        self._ann_file = os.path.join(self._root, cfg[\'ANN\'])\n\n        self._lazy_import = lazy_import\n        self._skip_empty = skip_empty\n        self._train = train\n        self._dpg = dpg\n\n        if \'AUG\' in cfg.keys():\n            self._scale_factor = cfg[\'AUG\'][\'SCALE_FACTOR\']\n            self._rot = cfg[\'AUG\'][\'ROT_FACTOR\']\n            self.num_joints_half_body = cfg[\'AUG\'][\'NUM_JOINTS_HALF_BODY\']\n            self.prob_half_body = cfg[\'AUG\'][\'PROB_HALF_BODY\']\n        else:\n            self._scale_factor = 0\n            self._rot = 0\n            self.num_joints_half_body = -1\n            self.prob_half_body = -1\n\n        self._input_size = self._preset_cfg[\'IMAGE_SIZE\']\n        self._output_size = self._preset_cfg[\'HEATMAP_SIZE\']\n\n        self._sigma = self._preset_cfg[\'SIGMA\']\n\n        self._check_centers = False\n\n        self.num_class = len(self.CLASSES)\n\n        self.upper_body_ids = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n        self.lower_body_ids = (11, 12, 13, 14, 15, 16)\n\n        if self._preset_cfg[\'TYPE\'] == \'simple\':\n            self.transformation = SimpleTransform(\n                self, scale_factor=self._scale_factor,\n                input_size=self._input_size,\n                output_size=self._output_size,\n                rot=self._rot, sigma=self._sigma,\n                train=self._train, add_dpg=self._dpg)\n        else:\n            raise NotImplementedError\n\n        self._items, self._labels = self._lazy_load_json()\n\n    def __getitem__(self, idx):\n        # get image id\n        img_path = self._items[idx]\n        img_id = int(os.path.splitext(os.path.basename(img_path))[0])\n\n        # load ground truth, including bbox, keypoints, image size\n        label = copy.deepcopy(self._labels[idx])\n        img = scipy.misc.imread(img_path, mode=\'RGB\')\n\n        # transform ground truth into training label and apply data augmentation\n        img, label, label_mask, bbox = self.transformation(img, label)\n        return img, label, label_mask, img_id, bbox\n\n    def __len__(self):\n        return len(self._items)\n\n    def _lazy_load_ann_file(self):\n        if os.path.exists(self._ann_file + \'.pkl\') and self._lazy_import:\n            print(\'Lazy load json...\')\n            with open(self._ann_file + \'.pkl\', \'rb\') as fid:\n                return pk.load(fid)\n        else:\n            _database = COCO(self._ann_file)\n            if os.access(self._ann_file + \'.pkl\', os.W_OK):\n                with open(self._ann_file + \'.pkl\', \'wb\') as fid:\n                    pk.dump(_database, fid, pk.HIGHEST_PROTOCOL)\n            return _database\n\n    def _lazy_load_json(self):\n        if os.path.exists(self._ann_file + \'_annot_keypoint.pkl\') and self._lazy_import:\n            print(\'Lazy load annot...\')\n            with open(self._ann_file + \'_annot_keypoint.pkl\', \'rb\') as fid:\n                items, labels = pk.load(fid)\n        else:\n            items, labels = self._load_jsons()\n            if os.access(self._ann_file + \'_annot_keypoint.pkl\', os.W_OK):\n                with open(self._ann_file + \'_annot_keypoint.pkl\', \'wb\') as fid:\n                    pk.dump((items, labels), fid, pk.HIGHEST_PROTOCOL)\n\n        return items, labels\n\n    @abstractmethod\n    def _load_jsons(self):\n        pass\n\n    @abstractproperty\n    def CLASSES(self):\n        return None\n\n    @abstractproperty\n    def num_joints(self):\n        return None\n\n    @abstractproperty\n    def joint_pairs(self):\n        """"""Joint pairs which defines the pairs of joint to be swapped\n        when the image is flipped horizontally.""""""\n        return None\n'"
alphapose/datasets/mpii.py,0,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\n""""""MPII Human Pose Dataset.""""""\nimport os\n\nimport numpy as np\n\nfrom alphapose.models.builder import DATASET\nfrom alphapose.utils.bbox import bbox_clip_xyxy, bbox_xywh_to_xyxy\n\nfrom .custom import CustomDataset\n\n\n@DATASET.register_module\nclass Mpii(CustomDataset):\n    """""" MPII Human Pose Dataset.\n\n    Parameters\n    ----------\n    root: str, default \'./data/mpii\'\n        Path to the mpii dataset.\n    train: bool, default is True\n        If true, will set as training mode.\n    dpg: bool, default is False\n        If true, will activate `dpg` for data augmentation.\n    """"""\n\n    CLASSES = [\'person\']\n    num_joints = 16\n\n    @property\n    def joint_pairs(self):\n        """"""Joint pairs which defines the pairs of joint to be swapped\n        when the image is flipped horizontally.""""""\n        return [[0, 5], [1, 4], [2, 3],\n                [10, 15], [11, 14], [12, 13]]\n\n    def _load_jsons(self):\n        """"""Load all image paths and labels from annotation files into buffer.""""""\n        items = []\n        labels = []\n\n        _mpii = self._lazy_load_ann_file()\n        classes = [c[\'name\'] for c in _mpii.loadCats(_mpii.getCatIds())]\n        assert classes == self.CLASSES, ""Incompatible category names with MPII. ""\n\n        # iterate through the annotations\n        image_ids = sorted(_mpii.getImgIds())\n        for entry in _mpii.loadImgs(image_ids):\n            filename = entry[\'file_name\']\n            abs_path = os.path.join(self._root, self._img_prefix, filename)\n            if not os.path.exists(abs_path):\n                raise IOError(\'Image: {} not exists.\'.format(abs_path))\n            label = self._check_load_keypoints(_mpii, entry)\n            if not label:\n                continue\n\n            # num of items are relative to person, not image\n            for obj in label:\n                items.append(abs_path)\n                labels.append(obj)\n\n        return items, labels\n\n    def _check_load_keypoints(self, _mpii, entry):\n        """"""Check and load ground-truth keypoints""""""\n        ann_ids = _mpii.getAnnIds(imgIds=entry[\'id\'], iscrowd=False)\n        objs = _mpii.loadAnns(ann_ids)\n        # check valid bboxes\n        valid_objs = []\n        width = entry[\'width\']\n        height = entry[\'height\']\n\n        for obj in objs:\n            if max(obj[\'keypoints\']) == 0:\n                continue\n            # convert from (x, y, w, h) to (xmin, ymin, xmax, ymax) and clip bound\n            xmin, ymin, xmax, ymax = bbox_clip_xyxy(bbox_xywh_to_xyxy(obj[\'bbox\']), width, height)\n            # require non-zero box area\n            if xmax <= xmin or ymax <= ymin:\n                continue\n            if obj[\'num_keypoints\'] == 0:\n                continue\n            # joints 3d: (num_joints, 3, 2); 3 is for x, y, z; 2 is for position, visibility\n            joints_3d = np.zeros((self.num_joints, 3, 2), dtype=np.float32)\n            for i in range(self.num_joints):\n                joints_3d[i, 0, 0] = obj[\'keypoints\'][i * 3 + 0]\n                joints_3d[i, 1, 0] = obj[\'keypoints\'][i * 3 + 1]\n                # joints_3d[i, 2, 0] = 0\n                visible = min(1, obj[\'keypoints\'][i * 3 + 2])\n                joints_3d[i, :2, 1] = visible\n                # joints_3d[i, 2, 1] = 0\n\n            if np.sum(joints_3d[:, 0, 1]) < 1:\n                # no visible keypoint\n                continue\n\n            if self._check_centers and self._train:\n                bbox_center, bbox_area = self._get_box_center_area((xmin, ymin, xmax, ymax))\n                kp_center, num_vis = self._get_keypoints_center_count(joints_3d)\n                ks = np.exp(-2 * np.sum(np.square(bbox_center - kp_center)) / bbox_area)\n                if (num_vis / 80.0 + 47 / 80.0) > ks:\n                    continue\n\n            valid_objs.append({\n                \'bbox\': (xmin, ymin, xmax, ymax),\n                \'width\': width,\n                \'height\': height,\n                \'joints_3d\': joints_3d\n            })\n\n        if not valid_objs:\n            if not self._skip_empty:\n                # dummy invalid labels if no valid objects are found\n                valid_objs.append({\n                    \'bbox\': np.array([-1, -1, 0, 0]),\n                    \'width\': width,\n                    \'height\': height,\n                    \'joints_3d\': np.zeros((self.num_joints, 2, 2), dtype=np.float32)\n                })\n        return valid_objs\n'"
alphapose/datasets/mscoco.py,0,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\n""""""MS COCO Human keypoint dataset.""""""\nimport os\n\nimport numpy as np\n\nfrom alphapose.models.builder import DATASET\nfrom alphapose.utils.bbox import bbox_clip_xyxy, bbox_xywh_to_xyxy\n\nfrom .custom import CustomDataset\n\n\n@DATASET.register_module\nclass Mscoco(CustomDataset):\n    """""" COCO Person dataset.\n\n    Parameters\n    ----------\n    train: bool, default is True\n        If true, will set as training mode.\n    skip_empty: bool, default is False\n        Whether skip entire image if no valid label is found. Use `False` if this dataset is\n        for validation to avoid COCO metric error.\n    dpg: bool, default is False\n        If true, will activate `dpg` for data augmentation.\n    """"""\n    CLASSES = [\'person\']\n    EVAL_JOINTS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n    num_joints = 17\n\n    @property\n    def joint_pairs(self):\n        """"""Joint pairs which defines the pairs of joint to be swapped\n        when the image is flipped horizontally.""""""\n        return [[1, 2], [3, 4], [5, 6], [7, 8],\n                [9, 10], [11, 12], [13, 14], [15, 16]]\n\n    def _load_jsons(self):\n        """"""Load all image paths and labels from JSON annotation files into buffer.""""""\n        items = []\n        labels = []\n\n        _coco = self._lazy_load_ann_file()\n        classes = [c[\'name\'] for c in _coco.loadCats(_coco.getCatIds())]\n        assert classes == self.CLASSES, ""Incompatible category names with COCO. ""\n\n        self.json_id_to_contiguous = {\n            v: k for k, v in enumerate(_coco.getCatIds())}\n\n        # iterate through the annotations\n        image_ids = sorted(_coco.getImgIds())\n        for entry in _coco.loadImgs(image_ids):\n            dirname, filename = entry[\'coco_url\'].split(\'/\')[-2:]\n            abs_path = os.path.join(self._root, dirname, filename)\n            if not os.path.exists(abs_path):\n                raise IOError(\'Image: {} not exists.\'.format(abs_path))\n            label = self._check_load_keypoints(_coco, entry)\n            if not label:\n                continue\n\n            # num of items are relative to person, not image\n            for obj in label:\n                items.append(abs_path)\n                labels.append(obj)\n\n        return items, labels\n\n    def _check_load_keypoints(self, coco, entry):\n        """"""Check and load ground-truth keypoints""""""\n        ann_ids = coco.getAnnIds(imgIds=entry[\'id\'], iscrowd=False)\n        objs = coco.loadAnns(ann_ids)\n        # check valid bboxes\n        valid_objs = []\n        width = entry[\'width\']\n        height = entry[\'height\']\n\n        for obj in objs:\n            contiguous_cid = self.json_id_to_contiguous[obj[\'category_id\']]\n            if contiguous_cid >= self.num_class:\n                # not class of interest\n                continue\n            if max(obj[\'keypoints\']) == 0:\n                continue\n            # convert from (x, y, w, h) to (xmin, ymin, xmax, ymax) and clip bound\n            xmin, ymin, xmax, ymax = bbox_clip_xyxy(bbox_xywh_to_xyxy(obj[\'bbox\']), width, height)\n            # require non-zero box area\n            if obj[\'area\'] <= 0 or xmax <= xmin or ymax <= ymin:\n                continue\n            if obj[\'num_keypoints\'] == 0:\n                continue\n            # joints 3d: (num_joints, 3, 2); 3 is for x, y, z; 2 is for position, visibility\n            joints_3d = np.zeros((self.num_joints, 3, 2), dtype=np.float32)\n            for i in range(self.num_joints):\n                joints_3d[i, 0, 0] = obj[\'keypoints\'][i * 3 + 0]\n                joints_3d[i, 1, 0] = obj[\'keypoints\'][i * 3 + 1]\n                # joints_3d[i, 2, 0] = 0\n                visible = min(1, obj[\'keypoints\'][i * 3 + 2])\n                joints_3d[i, :2, 1] = visible\n                # joints_3d[i, 2, 1] = 0\n\n            if np.sum(joints_3d[:, 0, 1]) < 1:\n                # no visible keypoint\n                continue\n\n            if self._check_centers and self._train:\n                bbox_center, bbox_area = self._get_box_center_area((xmin, ymin, xmax, ymax))\n                kp_center, num_vis = self._get_keypoints_center_count(joints_3d)\n                ks = np.exp(-2 * np.sum(np.square(bbox_center - kp_center)) / bbox_area)\n                if (num_vis / 80.0 + 47 / 80.0) > ks:\n                    continue\n\n            valid_objs.append({\n                \'bbox\': (xmin, ymin, xmax, ymax),\n                \'width\': width,\n                \'height\': height,\n                \'joints_3d\': joints_3d\n            })\n\n        if not valid_objs:\n            if not self._skip_empty:\n                # dummy invalid labels if no valid objects are found\n                valid_objs.append({\n                    \'bbox\': np.array([-1, -1, 0, 0]),\n                    \'width\': width,\n                    \'height\': height,\n                    \'joints_3d\': np.zeros((self.num_joints, 2, 2), dtype=np.float32)\n                })\n        return valid_objs\n\n    def _get_box_center_area(self, bbox):\n        """"""Get bbox center""""""\n        c = np.array([(bbox[0] + bbox[2]) / 2.0, (bbox[1] + bbox[3]) / 2.0])\n        area = (bbox[3] - bbox[1]) * (bbox[2] - bbox[0])\n        return c, area\n\n    def _get_keypoints_center_count(self, keypoints):\n        """"""Get geometric center of all keypoints""""""\n        keypoint_x = np.sum(keypoints[:, 0, 0] * (keypoints[:, 0, 1] > 0))\n        keypoint_y = np.sum(keypoints[:, 1, 0] * (keypoints[:, 1, 1] > 0))\n        num = float(np.sum(keypoints[:, 0, 1]))\n        return np.array([keypoint_x / num, keypoint_y / num]), num\n'"
alphapose/models/__init__.py,0,"b""from .fastpose import FastPose\nfrom .fastpose_duc import FastPose_DUC\nfrom .hrnet import PoseHighResolutionNet\nfrom .simplepose import SimplePose\nfrom .fastpose_duc_dense import FastPose_DUC_Dense\n\n__all__ = ['FastPose', 'SimplePose', 'PoseHighResolutionNet', 'FastPose_DUC','FastPose_DUC_Dense']\n"""
alphapose/models/builder.py,0,"b""from torch import nn\n\nfrom alphapose.utils import Registry, build_from_cfg\n\n\nSPPE = Registry('sppe')\nLOSS = Registry('loss')\nDATASET = Registry('dataset')\n\n\ndef build(cfg, registry, default_args=None):\n    if isinstance(cfg, list):\n        modules = [\n            build_from_cfg(cfg_, registry, default_args) for cfg_ in cfg\n        ]\n        return nn.Sequential(*modules)\n    else:\n        return build_from_cfg(cfg, registry, default_args)\n\n\ndef build_sppe(cfg, preset_cfg, **kwargs):\n    default_args = {\n        'PRESET': preset_cfg,\n    }\n    for key, value in kwargs.items():\n        default_args[key] = value\n    return build(cfg, SPPE, default_args=default_args)\n\n\ndef build_loss(cfg):\n    return build(cfg, LOSS)\n\n\ndef build_dataset(cfg, preset_cfg, **kwargs):\n    exec(f'from ..datasets import {cfg.TYPE}')\n    default_args = {\n        'PRESET': preset_cfg,\n    }\n    for key, value in kwargs.items():\n        default_args[key] = value\n    return build(cfg, DATASET, default_args=default_args)\n"""
alphapose/models/fastpose.py,1,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\nimport torch.nn as nn\n\nfrom .builder import SPPE\nfrom .layers.DUC import DUC\nfrom .layers.SE_Resnet import SEResnet\n\n\n@SPPE.register_module\nclass FastPose(nn.Module):\n    conv_dim = 128\n\n    def __init__(self, norm_layer=nn.BatchNorm2d, **cfg):\n        super(FastPose, self).__init__()\n        self._preset_cfg = cfg[\'PRESET\']\n\n        if \'DCN\' in cfg.keys():\n            stage_with_dcn = cfg[\'STAGE_WITH_DCN\']\n            dcn = cfg[\'DCN\']\n            self.preact = SEResnet(\n                f""resnet{cfg[\'NUM_LAYERS\']}"", dcn=dcn, stage_with_dcn=stage_with_dcn)\n        else:\n            self.preact = SEResnet(f""resnet{cfg[\'NUM_LAYERS\']}"")\n\n        # Imagenet pretrain model\n        import torchvision.models as tm   # noqa: F401,F403\n        assert cfg[\'NUM_LAYERS\'] in [18, 34, 50, 101, 152]\n        x = eval(f""tm.resnet{cfg[\'NUM_LAYERS\']}(pretrained=True)"")\n\n        model_state = self.preact.state_dict()\n        state = {k: v for k, v in x.state_dict().items()\n                 if k in self.preact.state_dict() and v.size() == self.preact.state_dict()[k].size()}\n        model_state.update(state)\n        self.preact.load_state_dict(model_state)\n\n        self.suffle1 = nn.PixelShuffle(2)\n        self.duc1 = DUC(512, 1024, upscale_factor=2, norm_layer=norm_layer)\n        self.duc2 = DUC(256, 512, upscale_factor=2, norm_layer=norm_layer)\n\n        self.conv_out = nn.Conv2d(\n            self.conv_dim, self._preset_cfg[\'NUM_JOINTS\'], kernel_size=3, stride=1, padding=1)\n\n    def forward(self, x):\n        out = self.preact(x)\n        out = self.suffle1(out)\n        out = self.duc1(out)\n        out = self.duc2(out)\n\n        out = self.conv_out(out)\n        return out\n\n    def _initialize(self):\n        for m in self.conv_out.modules():\n            if isinstance(m, nn.Conv2d):\n                # nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                # logger.info(\'=> init {}.weight as normal(0, 0.001)\'.format(name))\n                # logger.info(\'=> init {}.bias as 0\'.format(name))\n                nn.init.normal_(m.weight, std=0.001)\n                nn.init.constant_(m.bias, 0)\n'"
alphapose/models/fastpose_duc.py,1,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\nimport torch.nn as nn\n\nfrom .builder import SPPE\nfrom .layers.Resnet import ResNet\nfrom .layers.SE_Resnet import SEResnet\nfrom .layers.ShuffleResnet import ShuffleResnet\n\n\n@SPPE.register_module\nclass FastPose_DUC(nn.Module):\n    conv_dim = 256\n\n    def __init__(self, norm_layer=nn.BatchNorm2d, **cfg):\n        super(FastPose_DUC, self).__init__()\n        self._preset_cfg = cfg[\'PRESET\']\n        if cfg[\'BACKBONE\'] == \'shuffle\':\n            print(\'Load shuffle backbone...\')\n            backbone = ShuffleResnet\n        elif cfg[\'BACKBONE\'] == \'se-resnet\':\n            print(\'Load SE Resnet...\')\n            backbone = SEResnet\n        else:\n            print(\'Load Resnet...\')\n            backbone = ResNet\n\n        if \'DCN\' in cfg.keys():\n            stage_with_dcn = cfg[\'STAGE_WITH_DCN\']\n            dcn = cfg[\'DCN\']\n            self.preact = backbone(\n                f""resnet{cfg[\'NUM_LAYERS\']}"", dcn=dcn, stage_with_dcn=stage_with_dcn)\n        else:\n            self.preact = backbone(f""resnet{cfg[\'NUM_LAYERS\']}"")\n\n        # Imagenet pretrain model\n        import torchvision.models as tm   # noqa: F401,F403\n        assert cfg[\'NUM_LAYERS\'] in [18, 34, 50, 101, 152]\n        x = eval(f""tm.resnet{cfg[\'NUM_LAYERS\']}(pretrained=True)"")\n\n        model_state = self.preact.state_dict()\n        state = {k: v for k, v in x.state_dict().items()\n                 if k in self.preact.state_dict() and v.size() == self.preact.state_dict()[k].size()}\n        model_state.update(state)\n        self.preact.load_state_dict(model_state)\n        self.norm_layer = norm_layer\n\n        stage1_cfg = cfg[\'STAGE1\']\n        stage2_cfg = cfg[\'STAGE2\']\n        stage3_cfg = cfg[\'STAGE3\']\n\n        self.duc1 = self._make_duc_stage(stage1_cfg, 2048, 1024)\n        self.duc2 = self._make_duc_stage(stage2_cfg, 1024, 512)\n        self.duc3 = self._make_duc_stage(stage3_cfg, 512, self.conv_dim)\n\n        self.conv_out = nn.Conv2d(\n            self.conv_dim, self._preset_cfg[\'NUM_JOINTS\'], kernel_size=3, stride=1, padding=1)\n\n    def forward(self, x):\n        out = self.preact(x)\n        out = self.duc1(out)\n        out = self.duc2(out)\n        out = self.duc3(out)\n\n        out = self.conv_out(out)\n        return out\n\n    def _make_duc_stage(self, layer_config, inplanes, outplanes):\n        layers = []\n\n        shuffle = nn.PixelShuffle(2)\n        inplanes //= 4\n        layers.append(shuffle)\n        for i in range(layer_config.NUM_CONV - 1):\n            conv = nn.Conv2d(inplanes, inplanes, kernel_size=3,\n                             padding=1, bias=False)\n            norm_layer = self.norm_layer(inplanes, momentum=0.1)\n            relu = nn.ReLU(inplace=True)\n            layers += [conv, norm_layer, relu]\n        conv = nn.Conv2d(inplanes, outplanes, kernel_size=3,\n                         padding=1, bias=False)\n        norm_layer = self.norm_layer(outplanes, momentum=0.1)\n        relu = nn.ReLU(inplace=True)\n        layers += [conv, norm_layer, relu]\n        return nn.Sequential(*layers)\n\n    def _initialize(self):\n        for m in self.conv_out.modules():\n            if isinstance(m, nn.Conv2d):\n                # nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                # logger.info(\'=> init {}.weight as normal(0, 0.001)\'.format(name))\n                # logger.info(\'=> init {}.bias as 0\'.format(name))\n                nn.init.normal_(m.weight, std=0.001)\n                nn.init.constant_(m.bias, 0)\n'"
alphapose/models/fastpose_duc_dense.py,2,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nfrom .builder import SPPE\nfrom .layers.Resnet import ResNet\nfrom .layers.SE_Resnet import SEResnet\nfrom .layers.ShuffleResnet import ShuffleResnet\n\n@SPPE.register_module\nclass FastPose_DUC_Dense(nn.Module):\n    conv_dim = 256\n\n    def __init__(self,norm_layer=nn.BatchNorm2d,**cfg):\n        super(FastPose_DUC_Dense, self).__init__()\n        self._preset_cfg = cfg[\'PRESET\']\n        if cfg[\'BACKBONE\'] == \'shuffle\':\n            print(\'Load shuffle backbone...\')\n            backbone = ShuffleResnet\n        elif cfg[\'BACKBONE\'] == \'se-resnet\':\n            print(\'Load SE Resnet...\')\n            backbone = SEResnet\n        else:\n            print(\'Load Resnet...\')\n            backbone = ResNet\n\n        if \'DCN\' in cfg.keys():\n            stage_with_dcn = cfg[\'STAGE_WITH_DCN\']\n            dcn = cfg[\'DCN\']\n            self.preact = backbone(\n                f""resnet{cfg[\'NUM_LAYERS\']}"", dcn=dcn, stage_with_dcn=stage_with_dcn)\n        else:\n            self.preact = backbone(f""resnet{cfg[\'NUM_LAYERS\']}"")\n\n        # Init Backbone\n        for m in self.preact.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, std=0.001)\n                for name, _ in m.named_parameters():\n                    if name in [\'bias\']:\n                        nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                # nn.init.constant_(m.weight, 1)\n                nn.init.uniform_(m.weight, 0, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Imagenet pretrain model\n        import torchvision.models as tm\n        if cfg[\'NUM_LAYERS\'] == 152:\n            \'\'\' Load pretrained model \'\'\'\n            x = tm.resnet152(pretrained=True)\n        elif cfg[\'NUM_LAYERS\'] == 101:\n            \'\'\' Load pretrained model \'\'\'\n            x = tm.resnet101(pretrained=True)\n        elif cfg[\'NUM_LAYERS\'] == 50:\n            x = tm.resnet50(pretrained=True)\n        elif cfg[\'NUM_LAYERS\'] == 18:\n            x = tm.resnet18(pretrained=True)\n        else:\n            raise NotImplementedError\n        model_state = self.preact.state_dict()\n        state = {k: v for k, v in x.state_dict().items()\n                 if k in self.preact.state_dict() and v.size() == self.preact.state_dict()[k].size()}\n        model_state.update(state)\n        self.preact.load_state_dict(model_state)\n        self.norm_layer = norm_layer\n\n        stage1_cfg = cfg[\'STAGE1\']\n        stage2_cfg = cfg[\'STAGE2\']\n        stage3_cfg = cfg[\'STAGE3\']\n\n        duc1 = self._make_duc_stage(stage1_cfg, 2048, 1024)\n        duc2 = self._make_duc_stage(stage2_cfg, 1024, 512)\n        duc3 = self._make_duc_stage(stage3_cfg, 512, self.conv_dim)\n\n        self.duc = nn.Sequential(duc1, duc2, duc3)\n\n        duc1_dense = self._make_duc_stage(stage1_cfg,2048,1024)\n        duc2_dense = self._make_duc_stage(stage2_cfg,1024,512)\n        duc3_dense = self._make_duc_stage(stage3_cfg,512,self.conv_dim)\n\n        self.duc_dense = nn.Sequential(duc1_dense,duc2_dense,duc3_dense)\n\n        self.conv_out = nn.Conv2d(\n            self.conv_dim, self._preset_cfg[\'NUM_JOINTS\'], kernel_size=3, stride=1, padding=1)\n\n        self.conv_out_dense = nn.Conv2d(\n            self.conv_dim,(self._preset_cfg[\'NUM_JOINTS_DENSE\']-self._preset_cfg[\'NUM_JOINTS\']),kernel_size=3,stride=1,padding=1)\n        for params in self.preact.parameters():\n            params.requires_grad = False\n        for params in self.duc.parameters():\n            params.requires_grad = False\n\n    def forward(self, x):\n        bk_out = self.preact(x)\n        out = self.duc(bk_out)\n        out_dense = self.duc_dense(bk_out)\n        out = self.conv_out(out)\n        out_dense = self.conv_out_dense(out_dense)\n        out = torch.cat((out,out_dense),1)\n        return out\n\n    def _make_duc_stage(self, layer_config, inplanes, outplanes):\n        layers = []\n\n        shuffle = nn.PixelShuffle(2)\n        inplanes //= 4\n        layers.append(shuffle)\n        for i in range(layer_config.NUM_CONV - 1):\n            conv = nn.Conv2d(inplanes, inplanes, kernel_size=3,\n                             padding=1, bias=False)\n            norm_layer = self.norm_layer(inplanes, momentum=0.1)\n            relu = nn.ReLU(inplace=True)\n            layers += [conv, norm_layer, relu]\n        conv = nn.Conv2d(inplanes, outplanes, kernel_size=3,\n                         padding=1, bias=False)\n        norm_layer = self.norm_layer(outplanes, momentum=0.1)\n        relu = nn.ReLU(inplace=True)\n        layers += [conv, norm_layer, relu]\n        return nn.Sequential(*layers)\n\n    def _initialize(self):\n        for m in self.duc.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, std=0.001)\n                for name, _ in m.named_parameters():\n                    if name in [\'bias\']:\n                        nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                # nn.init.constant_(m.weight, 1)\n                nn.init.uniform_(m.weight, 0, 1)\n                nn.init.constant_(m.bias, 0)\n        for m in self.conv_out.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, std=0.001)\n                nn.init.constant_(m.bias, 0)\n\n        #init dense-branch\n        for m in self.duc_dense.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, std=0.001)\n                for name, _ in m.named_parameters():\n                    if name in [\'bias\']:\n                        nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                # nn.init.constant_(m.weight, 1)\n                nn.init.uniform_(m.weight, 0, 1)\n                nn.init.constant_(m.bias, 0)\n        for m in self.conv_out_dense.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, std=0.001)\n                nn.init.constant_(m.bias, 0)\n'"
alphapose/models/hrnet.py,2,"b'# ------------------------------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under the MIT License.\n# Written by Bin Xiao (Bin.Xiao@microsoft.com)\n# ------------------------------------------------------------------------------\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport torch\nimport torch.nn as nn\n\nfrom .builder import SPPE\n\nBN_MOMENTUM = 0.1\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion,\n                                  momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass HighResolutionModule(nn.Module):\n    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n                 num_channels, fuse_method, multi_scale_output=True):\n        super(HighResolutionModule, self).__init__()\n        self._check_branches(\n            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n\n        self.num_inchannels = num_inchannels\n        self.fuse_method = fuse_method\n        self.num_branches = num_branches\n\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(\n            num_branches, blocks, num_blocks, num_channels)\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU(True)\n\n    def _check_branches(self, num_branches, blocks, num_blocks,\n                        num_inchannels, num_channels):\n        if num_branches != len(num_blocks):\n            error_msg = \'NUM_BRANCHES({}) <> NUM_BLOCKS({})\'.format(\n                num_branches, len(num_blocks))\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = \'NUM_BRANCHES({}) <> NUM_CHANNELS({})\'.format(\n                num_branches, len(num_channels))\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_inchannels):\n            error_msg = \'NUM_BRANCHES({}) <> NUM_INCHANNELS({})\'.format(\n                num_branches, len(num_inchannels))\n            raise ValueError(error_msg)\n\n    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n                         stride=1):\n        downsample = None\n        if stride != 1 or \\\n           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.num_inchannels[branch_index],\n                    num_channels[branch_index] * block.expansion,\n                    kernel_size=1, stride=stride, bias=False\n                ),\n                nn.BatchNorm2d(\n                    num_channels[branch_index] * block.expansion,\n                    momentum=BN_MOMENTUM\n                ),\n            )\n\n        layers = []\n        layers.append(\n            block(\n                self.num_inchannels[branch_index],\n                num_channels[branch_index],\n                stride,\n                downsample\n            )\n        )\n        self.num_inchannels[branch_index] = \\\n            num_channels[branch_index] * block.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(\n                block(\n                    self.num_inchannels[branch_index],\n                    num_channels[branch_index]\n                )\n            )\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n        branches = []\n\n        for i in range(num_branches):\n            branches.append(\n                self._make_one_branch(i, block, num_blocks, num_channels)\n            )\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self):\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(\n                        nn.Sequential(\n                            nn.Conv2d(\n                                num_inchannels[j],\n                                num_inchannels[i],\n                                1, 1, 0, bias=False\n                            ),\n                            nn.BatchNorm2d(num_inchannels[i]),\n                            nn.Upsample(scale_factor=2 **\n                                        (j - i), mode=\'nearest\')\n                        )\n                    )\n                elif j == i:\n                    fuse_layer.append(None)\n                else:\n                    conv3x3s = []\n                    for k in range(i - j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(\n                                nn.Sequential(\n                                    nn.Conv2d(\n                                        num_inchannels[j],\n                                        num_outchannels_conv3x3,\n                                        3, 2, 1, bias=False\n                                    ),\n                                    nn.BatchNorm2d(num_outchannels_conv3x3)\n                                )\n                            )\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(\n                                nn.Sequential(\n                                    nn.Conv2d(\n                                        num_inchannels[j],\n                                        num_outchannels_conv3x3,\n                                        3, 2, 1, bias=False\n                                    ),\n                                    nn.BatchNorm2d(num_outchannels_conv3x3),\n                                    nn.ReLU(True)\n                                )\n                            )\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def get_num_inchannels(self):\n        return self.num_inchannels\n\n    def forward(self, x):\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = []\n\n        for i in range(len(self.fuse_layers)):\n            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n            for j in range(1, self.num_branches):\n                if i == j:\n                    y = y + x[j]\n                else:\n                    y = y + self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n\n        return x_fuse\n\n\nblocks_dict = {\n    \'BASIC\': BasicBlock,\n    \'BOTTLENECK\': Bottleneck\n}\n\n\n@SPPE.register_module\nclass PoseHighResolutionNet(nn.Module):\n\n    def __init__(self, **cfg):\n        self.inplanes = 64\n        super(PoseHighResolutionNet, self).__init__()\n        self._preset_cfg = cfg[\'PRESET\']\n\n        # stem net\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(Bottleneck, 64, 4)\n\n        self.stage2_cfg = cfg[\'STAGE2\']\n        num_channels = self.stage2_cfg[\'NUM_CHANNELS\']\n        block = blocks_dict[self.stage2_cfg[\'BLOCK\']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))\n        ]\n        self.transition1 = self._make_transition_layer([256], num_channels)\n        self.stage2, pre_stage_channels = self._make_stage(\n            self.stage2_cfg, num_channels)\n\n        self.stage3_cfg = cfg[\'STAGE3\']\n        num_channels = self.stage3_cfg[\'NUM_CHANNELS\']\n        block = blocks_dict[self.stage3_cfg[\'BLOCK\']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))\n        ]\n        self.transition2 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage3, pre_stage_channels = self._make_stage(\n            self.stage3_cfg, num_channels)\n\n        self.stage4_cfg = cfg[\'STAGE4\']\n        num_channels = self.stage4_cfg[\'NUM_CHANNELS\']\n        block = blocks_dict[self.stage4_cfg[\'BLOCK\']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))\n        ]\n        self.transition3 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage4, pre_stage_channels = self._make_stage(\n            self.stage4_cfg, num_channels, multi_scale_output=False)\n\n        self.final_layer = nn.Conv2d(\n            in_channels=pre_stage_channels[0],\n            out_channels=self._preset_cfg[\'NUM_JOINTS\'],\n            kernel_size=cfg[\'FINAL_CONV_KERNEL\'],\n            stride=1,\n            padding=1 if cfg[\'FINAL_CONV_KERNEL\'] == 3 else 0\n        )\n\n        self.pretrained_layers = cfg[\'PRETRAINED_LAYERS\']\n\n    def _make_transition_layer(\n            self, num_channels_pre_layer, num_channels_cur_layer):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(\n                        nn.Sequential(\n                            nn.Conv2d(\n                                num_channels_pre_layer[i],\n                                num_channels_cur_layer[i],\n                                3, 1, 1, bias=False\n                            ),\n                            nn.BatchNorm2d(num_channels_cur_layer[i]),\n                            nn.ReLU(inplace=True)\n                        )\n                    )\n                else:\n                    transition_layers.append(None)\n            else:\n                conv3x3s = []\n                for j in range(i + 1 - num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = num_channels_cur_layer[i] \\\n                        if j == i - num_branches_pre else inchannels\n                    conv3x3s.append(\n                        nn.Sequential(\n                            nn.Conv2d(\n                                inchannels, outchannels, 3, 2, 1, bias=False\n                            ),\n                            nn.BatchNorm2d(outchannels),\n                            nn.ReLU(inplace=True)\n                        )\n                    )\n                transition_layers.append(nn.Sequential(*conv3x3s))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.inplanes, planes * block.expansion,\n                    kernel_size=1, stride=stride, bias=False\n                ),\n                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, layer_config, num_inchannels,\n                    multi_scale_output=True):\n        num_modules = layer_config[\'NUM_MODULES\']\n        num_branches = layer_config[\'NUM_BRANCHES\']\n        num_blocks = layer_config[\'NUM_BLOCKS\']\n        num_channels = layer_config[\'NUM_CHANNELS\']\n        block = blocks_dict[layer_config[\'BLOCK\']]\n        fuse_method = layer_config[\'FUSE_METHOD\']\n\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n\n            modules.append(\n                HighResolutionModule(\n                    num_branches,\n                    block,\n                    num_blocks,\n                    num_inchannels,\n                    num_channels,\n                    fuse_method,\n                    reset_multi_scale_output\n                )\n            )\n            num_inchannels = modules[-1].get_num_inchannels()\n\n        return nn.Sequential(*modules), num_inchannels\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n\n        x_list = []\n        for i in range(self.stage2_cfg[\'NUM_BRANCHES\']):\n            if self.transition1[i] is not None:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.stage3_cfg[\'NUM_BRANCHES\']):\n            if self.transition2[i] is not None:\n                x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.stage4_cfg[\'NUM_BRANCHES\']):\n            if self.transition3[i] is not None:\n                x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage4(x_list)\n\n        x = self.final_layer(y_list[0])\n\n        return x\n\n    def _initialize(self, pretrained=\'\'):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                nn.init.normal_(m.weight, std=0.001)\n                for name, _ in m.named_parameters():\n                    if name in [\'bias\']:\n                        nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.ConvTranspose2d):\n                nn.init.normal_(m.weight, std=0.001)\n                for name, _ in m.named_parameters():\n                    if name in [\'bias\']:\n                        nn.init.constant_(m.bias, 0)\n\n        if os.path.isfile(pretrained):\n            pretrained_state_dict = torch.load(pretrained)\n\n            need_init_state_dict = {}\n            for name, m in pretrained_state_dict.items():\n                if name.split(\'.\')[0] in self.pretrained_layers \\\n                   or self.pretrained_layers[0] == \'*\':\n                    need_init_state_dict[name] = m\n            self.load_state_dict(need_init_state_dict, strict=False)\n        elif pretrained:\n            raise ValueError(\'{} is not exist!\'.format(pretrained))\n\n\ndef get_pose_net(cfg, is_train, **kwargs):\n    model = PoseHighResolutionNet(cfg, **kwargs)\n\n    if is_train and cfg.MODEL.INIT_WEIGHTS:\n        model._initialize(cfg.MODEL.PRETRAINED)\n\n    return model\n'"
alphapose/models/simplepose.py,1,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\nimport torch.nn as nn\n\nfrom .builder import SPPE\nfrom .layers.Resnet import ResNet\n\n\n@SPPE.register_module\nclass SimplePose(nn.Module):\n    def __init__(self, norm_layer=nn.BatchNorm2d, **cfg):\n        super(SimplePose, self).__init__()\n        self._preset_cfg = cfg[\'PRESET\']\n        self.deconv_dim = cfg[\'NUM_DECONV_FILTERS\']\n        self._norm_layer = norm_layer\n\n        self.preact = ResNet(f""resnet{cfg[\'NUM_LAYERS\']}"")\n\n        # Imagenet pretrain model\n        import torchvision.models as tm   # noqa: F401,F403\n        assert cfg[\'NUM_LAYERS\'] in [18, 34, 50, 101, 152]\n        x = eval(f""tm.resnet{cfg[\'NUM_LAYERS\']}(pretrained=True)"")\n\n        model_state = self.preact.state_dict()\n        state = {k: v for k, v in x.state_dict().items()\n                 if k in self.preact.state_dict() and v.size() == self.preact.state_dict()[k].size()}\n        model_state.update(state)\n        self.preact.load_state_dict(model_state)\n\n        self.deconv_layers = self._make_deconv_layer()\n        self.final_layer = nn.Conv2d(\n            self.deconv_dim[2], self._preset_cfg[\'NUM_JOINTS\'], kernel_size=1, stride=1, padding=0)\n\n    def _make_deconv_layer(self):\n        deconv_layers = []\n        deconv1 = nn.ConvTranspose2d(\n            2048, self.deconv_dim[0], kernel_size=4, stride=2, padding=int(4 / 2) - 1, bias=False)\n        bn1 = self._norm_layer(self.deconv_dim[0])\n        deconv2 = nn.ConvTranspose2d(\n            self.deconv_dim[0], self.deconv_dim[1], kernel_size=4, stride=2, padding=int(4 / 2) - 1, bias=False)\n        bn2 = self._norm_layer(self.deconv_dim[1])\n        deconv3 = nn.ConvTranspose2d(\n            self.deconv_dim[1], self.deconv_dim[2], kernel_size=4, stride=2, padding=int(4 / 2) - 1, bias=False)\n        bn3 = self._norm_layer(self.deconv_dim[2])\n\n        deconv_layers.append(deconv1)\n        deconv_layers.append(bn1)\n        deconv_layers.append(nn.ReLU(inplace=True))\n        deconv_layers.append(deconv2)\n        deconv_layers.append(bn2)\n        deconv_layers.append(nn.ReLU(inplace=True))\n        deconv_layers.append(deconv3)\n        deconv_layers.append(bn3)\n        deconv_layers.append(nn.ReLU(inplace=True))\n\n        return nn.Sequential(*deconv_layers)\n\n    def _initialize(self):\n        for name, m in self.deconv_layers.named_modules():\n            if isinstance(m, nn.ConvTranspose2d):\n                # logger.info(\'=> init {}.weight as normal(0, 0.001)\'.format(name))\n                # logger.info(\'=> init {}.bias as 0\'.format(name))\n                nn.init.normal_(m.weight, std=0.001)\n                # if self.deconv_with_bias:\n                #     nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                # logger.info(\'=> init {}.weight as 1\'.format(name))\n                # logger.info(\'=> init {}.bias as 0\'.format(name))\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        for m in self.final_layer.modules():\n            if isinstance(m, nn.Conv2d):\n                # nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                # logger.info(\'=> init {}.weight as normal(0, 0.001)\'.format(name))\n                # logger.info(\'=> init {}.bias as 0\'.format(name))\n                nn.init.normal_(m.weight, std=0.001)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        out = self.preact(x)\n        out = self.deconv_layers(out)\n        out = self.final_layer(out)\n        return out\n'"
alphapose/utils/__init__.py,0,"b""from .registry import Registry, build_from_cfg\n\n__all__ = [\n    'Registry', 'build_from_cfg'\n]\n"""
alphapose/utils/bbox.py,5,"b'from __future__ import division\n\nimport torch\nimport numpy as np\n\n\ndef bbox_iou(bbox_a, bbox_b, offset=0):\n    """"""Calculate Intersection-Over-Union(IOU) of two bounding boxes.\n\n    Parameters\n    ----------\n    bbox_a : numpy.ndarray\n        An ndarray with shape :math:`(N, 4)`.\n    bbox_b : numpy.ndarray\n        An ndarray with shape :math:`(M, 4)`.\n    offset : float or int, default is 0\n        The ``offset`` is used to control the whether the width(or height) is computed as\n        (right - left + ``offset``).\n        Note that the offset must be 0 for normalized bboxes, whose ranges are in ``[0, 1]``.\n\n    Returns\n    -------\n    numpy.ndarray\n        An ndarray with shape :math:`(N, M)` indicates IOU between each pairs of\n        bounding boxes in `bbox_a` and `bbox_b`.\n\n    """"""\n    if bbox_a.shape[1] < 4 or bbox_b.shape[1] < 4:\n        raise IndexError(""Bounding boxes axis 1 must have at least length 4"")\n\n    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n    br = np.minimum(bbox_a[:, None, 2:4], bbox_b[:, 2:4])\n\n    area_i = np.prod(br - tl + offset, axis=2) * (tl < br).all(axis=2)\n    area_a = np.prod(bbox_a[:, 2:4] - bbox_a[:, :2] + offset, axis=1)\n    area_b = np.prod(bbox_b[:, 2:4] - bbox_b[:, :2] + offset, axis=1)\n    return area_i / (area_a[:, None] + area_b - area_i)\n\n\ndef bbox_xywh_to_xyxy(xywh):\n    """"""Convert bounding boxes from format (x, y, w, h) to (xmin, ymin, xmax, ymax)\n\n    Parameters\n    ----------\n    xywh : list, tuple or numpy.ndarray\n        The bbox in format (x, y, w, h).\n        If numpy.ndarray is provided, we expect multiple bounding boxes with\n        shape `(N, 4)`.\n\n    Returns\n    -------\n    tuple or numpy.ndarray\n        The converted bboxes in format (xmin, ymin, xmax, ymax).\n        If input is numpy.ndarray, return is numpy.ndarray correspondingly.\n\n    """"""\n    if isinstance(xywh, (tuple, list)):\n        if not len(xywh) == 4:\n            raise IndexError(\n                ""Bounding boxes must have 4 elements, given {}"".format(len(xywh)))\n        w, h = np.maximum(xywh[2] - 1, 0), np.maximum(xywh[3] - 1, 0)\n        return (xywh[0], xywh[1], xywh[0] + w, xywh[1] + h)\n    elif isinstance(xywh, np.ndarray):\n        if not xywh.size % 4 == 0:\n            raise IndexError(\n                ""Bounding boxes must have n * 4 elements, given {}"".format(xywh.shape))\n        xyxy = np.hstack((xywh[:, :2], xywh[:, :2] + np.maximum(0, xywh[:, 2:4] - 1)))\n        return xyxy\n    else:\n        raise TypeError(\n            \'Expect input xywh a list, tuple or numpy.ndarray, given {}\'.format(type(xywh)))\n\n\ndef bbox_xyxy_to_xywh(xyxy):\n    """"""Convert bounding boxes from format (xmin, ymin, xmax, ymax) to (x, y, w, h).\n\n    Parameters\n    ----------\n    xyxy : list, tuple or numpy.ndarray\n        The bbox in format (xmin, ymin, xmax, ymax).\n        If numpy.ndarray is provided, we expect multiple bounding boxes with\n        shape `(N, 4)`.\n\n    Returns\n    -------\n    tuple or numpy.ndarray\n        The converted bboxes in format (x, y, w, h).\n        If input is numpy.ndarray, return is numpy.ndarray correspondingly.\n\n    """"""\n    if isinstance(xyxy, (tuple, list)):\n        if not len(xyxy) == 4:\n            raise IndexError(\n                ""Bounding boxes must have 4 elements, given {}"".format(len(xyxy)))\n        x1, y1 = xyxy[0], xyxy[1]\n        w, h = xyxy[2] - x1 + 1, xyxy[3] - y1 + 1\n        return (x1, y1, w, h)\n    elif isinstance(xyxy, np.ndarray):\n        if not xyxy.size % 4 == 0:\n            raise IndexError(\n                ""Bounding boxes must have n * 4 elements, given {}"".format(xyxy.shape))\n        return np.hstack((xyxy[:, :2], xyxy[:, 2:4] - xyxy[:, :2] + 1))\n    else:\n        raise TypeError(\n            \'Expect input xywh a list, tuple or numpy.ndarray, given {}\'.format(type(xyxy)))\n\n\ndef bbox_clip_xyxy(xyxy, width, height):\n    """"""Clip bounding box with format (xmin, ymin, xmax, ymax) to specified boundary.\n\n    All bounding boxes will be clipped to the new region `(0, 0, width, height)`.\n\n    Parameters\n    ----------\n    xyxy : list, tuple or numpy.ndarray\n        The bbox in format (xmin, ymin, xmax, ymax).\n        If numpy.ndarray is provided, we expect multiple bounding boxes with\n        shape `(N, 4)`.\n    width : int or float\n        Boundary width.\n    height : int or float\n        Boundary height.\n\n    Returns\n    -------\n    type\n        Description of returned object.\n\n    """"""\n    if isinstance(xyxy, (tuple, list)):\n        if not len(xyxy) == 4:\n            raise IndexError(\n                ""Bounding boxes must have 4 elements, given {}"".format(len(xyxy)))\n        x1 = np.minimum(width - 1, np.maximum(0, xyxy[0]))\n        y1 = np.minimum(height - 1, np.maximum(0, xyxy[1]))\n        x2 = np.minimum(width - 1, np.maximum(0, xyxy[2]))\n        y2 = np.minimum(height - 1, np.maximum(0, xyxy[3]))\n        return (x1, y1, x2, y2)\n    elif isinstance(xyxy, np.ndarray):\n        if not xyxy.size % 4 == 0:\n            raise IndexError(\n                ""Bounding boxes must have n * 4 elements, given {}"".format(xyxy.shape))\n        x1 = np.minimum(width - 1, np.maximum(0, xyxy[:, 0]))\n        y1 = np.minimum(height - 1, np.maximum(0, xyxy[:, 1]))\n        x2 = np.minimum(width - 1, np.maximum(0, xyxy[:, 2]))\n        y2 = np.minimum(height - 1, np.maximum(0, xyxy[:, 3]))\n        return np.hstack((x1, y1, x2, y2))\n    else:\n        raise TypeError(\n            \'Expect input xywh a list, tuple or numpy.ndarray, given {}\'.format(type(xyxy)))\n\n\ndef transformBox(pt, bbox, input_size, output_size):\n    inpH, inpW = input_size\n    resH, _ = output_size\n\n    center = torch.zeros(2)\n    center[0] = (bbox[2] - 1 - bbox[0]) / 2\n    center[1] = (bbox[3] - 1 - bbox[1]) / 2\n\n    lenH = max(bbox[3] - bbox[1], (bbox[2] - bbox[0]) * inpH / inpW)\n    lenW = lenH * inpW / inpH\n\n    _pt = torch.zeros(2)\n    _pt[0] = pt[0] - bbox[0]\n    _pt[1] = pt[1] - bbox[1]\n    # Move to center\n    _pt[0] = _pt[0] + max(0, (lenW - 1) / 2 - center[0])\n    _pt[1] = _pt[1] + max(0, (lenH - 1) / 2 - center[1])\n    pt = (_pt * resH) / lenH\n    pt[0] = round(float(pt[0]))\n    pt[1] = round(float(pt[1]))\n    return pt.int()\n\n\ndef transformBoxInvert(pt, bbox, resH, resW):\n    center = torch.zeros(2)\n    center[0] = (bbox[2] - 1 - bbox[0]) / 2\n    center[1] = (bbox[3] - 1 - bbox[1]) / 2\n\n    lenH = max(bbox[3] - bbox[1], (bbox[2] - bbox[0]) * resH / resW)\n    lenW = lenH * resW / resH\n\n    _pt = (pt * lenH) / resH\n\n    if bool(((lenW - 1) / 2 - center[0]) > 0):\n        _pt[0] = _pt[0] - ((lenW - 1) / 2 - center[0]).item()\n    if bool(((lenH - 1) / 2 - center[1]) > 0):\n        _pt[1] = _pt[1] - ((lenH - 1) / 2 - center[1]).item()\n\n    new_point = torch.zeros(2)\n    new_point[0] = _pt[0] + bbox[0]\n    new_point[1] = _pt[1] + bbox[1]\n    return new_point\n\n\ndef _box_to_center_scale(x, y, w, h, aspect_ratio=1.0, scale_mult=1.25):\n    """"""Convert box coordinates to center and scale.\n    adapted from https://github.com/Microsoft/human-pose-estimation.pytorch\n    """"""\n    pixel_std = 1\n    center = np.zeros((2), dtype=np.float32)\n    center[0] = x + w * 0.5\n    center[1] = y + h * 0.5\n\n    if w > aspect_ratio * h:\n        h = w / aspect_ratio\n    elif w < aspect_ratio * h:\n        w = h * aspect_ratio\n    scale = np.array(\n        [w * 1.0 / pixel_std, h * 1.0 / pixel_std], dtype=np.float32)\n    if center[0] != -1:\n        scale = scale * scale_mult\n    return center, scale\n\n\ndef _center_scale_to_box(center, scale):\n    pixel_std = 1.0\n    w = scale[0] * pixel_std\n    h = scale[1] * pixel_std\n    xmin = center[0] - w * 0.5\n    ymin = center[1] - h * 0.5\n    xmax = xmin + w\n    ymax = ymin + h\n    bbox = [xmin, ymin, xmax, ymax]\n    return bbox\n\n\ndef _clip_aspect_ratio(boxes, aspect_ratio=1.0):\n    xmin, ymin = boxes[:, 0], boxes[:, 1]\n    xmax, ymax = boxes[:, 2], boxes[:, 3]\n\n    w = xmax - xmin\n    h = ymax - ymin\n\n    c_x = xmin + w * 0.5\n    c_y = ymin + h * 0.5\n\n    idx = w > (aspect_ratio * h)\n    h[idx] = w[idx] / aspect_ratio\n\n    idx = w < (aspect_ratio * h)\n    w[idx] = h[idx] * aspect_ratio\n\n    new_boxes = torch.zeros(boxes.shape[0], 5)\n    new_boxes[:, 1] = c_x - w * 0.5\n    new_boxes[:, 2] = c_y - h * 0.5\n    new_boxes[:, 3] = c_x + w * 0.5\n    new_boxes[:, 4] = c_y + h * 0.5\n\n    return new_boxes\n'"
alphapose/utils/config.py,0,"b'import yaml\nfrom easydict import EasyDict as edict\n\n\ndef update_config(config_file):\n    with open(config_file) as f:\n        config = edict(yaml.load(f, Loader=yaml.FullLoader))\n        return config\n'"
alphapose/utils/detector.py,21,"b'import os\nimport sys\nfrom threading import Thread\nfrom queue import Queue\n\nimport cv2\nimport scipy.misc\nimport numpy as np\n\nimport torch\nimport torch.multiprocessing as mp\n\nfrom alphapose.utils.presets import SimpleTransform\n\n\nclass DetectionLoader():\n    def __init__(self, input_source, detector, cfg, opt, mode=\'image\', batchSize=1, queueSize=128):\n        self.cfg = cfg\n        self.opt = opt\n        self.mode = mode\n        self.device = opt.device\n\n        if mode == \'image\':\n            self.img_dir = opt.inputpath\n            self.imglist = [os.path.join(self.img_dir, im_name.rstrip(\'\\n\').rstrip(\'\\r\')) for im_name in input_source]\n            self.datalen = len(input_source)\n        elif mode == \'video\':\n            stream = cv2.VideoCapture(input_source)\n            assert stream.isOpened(), \'Cannot capture source\'\n            self.path = input_source\n            self.datalen = int(stream.get(cv2.CAP_PROP_FRAME_COUNT))\n            self.fourcc = int(stream.get(cv2.CAP_PROP_FOURCC))\n            self.fps = stream.get(cv2.CAP_PROP_FPS)\n            self.frameSize = (int(stream.get(cv2.CAP_PROP_FRAME_WIDTH)), int(stream.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n            self.videoinfo = {\'fourcc\': self.fourcc, \'fps\': self.fps, \'frameSize\': self.frameSize}\n            stream.release()\n\n        self.detector = detector\n        self.batchSize = batchSize\n        leftover = 0\n        if (self.datalen) % batchSize:\n            leftover = 1\n        self.num_batches = self.datalen // batchSize + leftover\n\n        self._input_size = cfg.DATA_PRESET.IMAGE_SIZE\n        self._output_size = cfg.DATA_PRESET.HEATMAP_SIZE\n\n        self._sigma = cfg.DATA_PRESET.SIGMA\n\n        if cfg.DATA_PRESET.TYPE == \'simple\':\n            self.transformation = SimpleTransform(\n                self, scale_factor=0,\n                input_size=self._input_size,\n                output_size=self._output_size,\n                rot=0, sigma=self._sigma,\n                train=False, add_dpg=False, gpu_device=self.device)\n\n        # initialize the queue used to store data\n        """"""\n        image_queue: the buffer storing pre-processed images for object detection\n        det_queue: the buffer storing human detection results\n        pose_queue: the buffer storing post-processed cropped human image for pose estimation\n        """"""\n        if opt.sp:\n            self._stopped = False\n            self.image_queue = Queue(maxsize=queueSize)\n            self.det_queue = Queue(maxsize=10 * queueSize)\n            self.pose_queue = Queue(maxsize=10 * queueSize)\n        else:\n            self._stopped = mp.Value(\'b\', False)\n            self.image_queue = mp.Queue(maxsize=queueSize)\n            self.det_queue = mp.Queue(maxsize=10 * queueSize)\n            self.pose_queue = mp.Queue(maxsize=10 * queueSize)\n\n    def start_worker(self, target):\n        if self.opt.sp:\n            p = Thread(target=target, args=())\n        else:\n            p = mp.Process(target=target, args=())\n        # p.daemon = True\n        p.start()\n        return p\n\n    def start(self):\n        # start a thread to pre process images for object detection\n        if self.mode == \'image\':\n            image_preprocess_worker = self.start_worker(self.image_preprocess)\n        elif self.mode == \'video\':\n            image_preprocess_worker = self.start_worker(self.frame_preprocess)\n        # start a thread to detect human in images\n        image_detection_worker = self.start_worker(self.image_detection)\n        # start a thread to post process cropped human image for pose estimation\n        image_postprocess_worker = self.start_worker(self.image_postprocess)\n\n        return [image_preprocess_worker, image_detection_worker, image_postprocess_worker]\n\n    def stop(self):\n        # clear queues\n        self.clear_queues()\n\n    def terminate(self):\n        if self.opt.sp:\n            self._stopped = True\n        else:\n            self._stopped.value = True\n        self.stop()\n\n    def clear_queues(self):\n        self.clear(self.image_queue)\n        self.clear(self.det_queue)\n        self.clear(self.pose_queue)\n\n    def clear(self, queue):\n        while not queue.empty():\n            queue.get()\n\n    def wait_and_put(self, queue, item):\n        queue.put(item)\n\n    def wait_and_get(self, queue):\n        return queue.get()\n\n    def image_preprocess(self):\n        for i in range(self.num_batches):\n            imgs = []\n            orig_imgs = []\n            im_names = []\n            im_dim_list = []\n            for k in range(i * self.batchSize, min((i + 1) * self.batchSize, self.datalen)):\n                if self.stopped:\n                    self.wait_and_put(self.image_queue, (None, None, None, None))\n                    return\n                im_name_k = self.imglist[k]\n\n                # expected image shape like (1,3,h,w) or (3,h,w)\n                img_k = self.detector.image_preprocess(im_name_k)\n                if isinstance(img_k, np.ndarray):\n                    img_k = torch.from_numpy(img_k)\n                # add one dimension at the front for batch if image shape (3,h,w)\n                if img_k.dim() == 3:\n                    img_k = img_k.unsqueeze(0)\n                orig_img_k = scipy.misc.imread(im_name_k, mode=\'RGB\')\n                im_dim_list_k = orig_img_k.shape[1], orig_img_k.shape[0]\n\n                imgs.append(img_k)\n                orig_imgs.append(orig_img_k)\n                im_names.append(im_name_k)\n                im_dim_list.append(im_dim_list_k)\n\n            with torch.no_grad():\n                # Human Detection\n                imgs = torch.cat(imgs)\n                im_dim_list = torch.FloatTensor(im_dim_list).repeat(1, 2)\n                # im_dim_list_ = im_dim_list\n\n            self.wait_and_put(self.image_queue, (imgs, orig_imgs, im_names, im_dim_list))\n\n    def frame_preprocess(self):\n        stream = cv2.VideoCapture(self.path)\n        assert stream.isOpened(), \'Cannot capture source\'\n\n        for i in range(self.num_batches):\n            imgs = []\n            orig_imgs = []\n            im_names = []\n            im_dim_list = []\n            for k in range(i * self.batchSize, min((i + 1) * self.batchSize, self.datalen)):\n                (grabbed, frame) = stream.read()\n                # if the `grabbed` boolean is `False`, then we have\n                # reached the end of the video file\n                if not grabbed or self.stopped:\n                    # put the rest pre-processed data to the queue\n                    if len(imgs) > 0:\n                        with torch.no_grad():\n                            # Record original image resolution\n                            imgs = torch.cat(imgs)\n                            im_dim_list = torch.FloatTensor(im_dim_list).repeat(1, 2)\n                        self.wait_and_put(self.image_queue, (imgs, orig_imgs, im_names, im_dim_list))\n                    self.wait_and_put(self.image_queue, (None, None, None, None))\n                    print(\'===========================> This video get \' + str(k) + \' frames in total.\')\n                    sys.stdout.flush()\n                    stream.release()\n                    return\n\n                # expected frame shape like (1,3,h,w) or (3,h,w)\n                img_k = self.detector.image_preprocess(frame)\n\n                if isinstance(img_k, np.ndarray):\n                    img_k = torch.from_numpy(img_k)\n                # add one dimension at the front for batch if image shape (3,h,w)\n                if img_k.dim() == 3:\n                    img_k = img_k.unsqueeze(0)\n\n                im_dim_list_k = frame.shape[1], frame.shape[0]\n\n                imgs.append(img_k)\n                orig_imgs.append(frame[:, :, ::-1])\n                im_names.append(str(k) + \'.jpg\')\n                im_dim_list.append(im_dim_list_k)\n\n            with torch.no_grad():\n                # Record original image resolution\n                imgs = torch.cat(imgs)\n                im_dim_list = torch.FloatTensor(im_dim_list).repeat(1, 2)\n                # im_dim_list_ = im_dim_list\n\n            self.wait_and_put(self.image_queue, (imgs, orig_imgs, im_names, im_dim_list))\n        stream.release()\n\n    def image_detection(self):\n        for i in range(self.num_batches):\n            imgs, orig_imgs, im_names, im_dim_list = self.wait_and_get(self.image_queue)\n            if imgs is None or self.stopped:\n                self.wait_and_put(self.det_queue, (None, None, None, None, None, None, None))\n                return\n\n            with torch.no_grad():\n                # pad useless images to fill a batch, else there will be a bug\n                for pad_i in range(self.batchSize - len(imgs)):\n                    imgs = torch.cat((imgs, torch.unsqueeze(imgs[0], dim=0)), 0)\n                    im_dim_list = torch.cat((im_dim_list, torch.unsqueeze(im_dim_list[0], dim=0)), 0)\n\n                dets = self.detector.images_detection(imgs, im_dim_list)\n                if isinstance(dets, int) or dets.shape[0] == 0:\n                    for k in range(len(orig_imgs)):\n                        self.wait_and_put(self.det_queue, (orig_imgs[k], im_names[k], None, None, None, None, None))\n                    continue\n                if isinstance(dets, np.ndarray):\n                    dets = torch.from_numpy(dets)\n                dets = dets.cpu()\n                boxes = dets[:, 1:5]\n                scores = dets[:, 5:6]\n                if self.opt.tracking:\n                    ids = dets[:, 6:7]\n                else:\n                    ids = torch.zeros(scores.shape)\n\n            for k in range(len(orig_imgs)):\n                boxes_k = boxes[dets[:, 0] == k]\n                if isinstance(boxes_k, int) or boxes_k.shape[0] == 0:\n                    self.wait_and_put(self.det_queue, (orig_imgs[k], im_names[k], None, None, None, None, None))\n                    continue\n                inps = torch.zeros(boxes_k.size(0), 3, *self._input_size)\n                cropped_boxes = torch.zeros(boxes_k.size(0), 4)\n\n                self.wait_and_put(self.det_queue, (orig_imgs[k], im_names[k], boxes_k, scores[dets[:, 0] == k], ids[dets[:, 0] == k], inps, cropped_boxes))\n\n    def image_postprocess(self):\n        for i in range(self.datalen):\n            with torch.no_grad():\n                (orig_img, im_name, boxes, scores, ids, inps, cropped_boxes) = self.wait_and_get(self.det_queue)\n                if orig_img is None or self.stopped:\n                    self.wait_and_put(self.pose_queue, (None, None, None, None, None, None, None))\n                    return\n                if boxes is None or boxes.nelement() == 0:\n                    self.wait_and_put(self.pose_queue, (None, orig_img, im_name, boxes, scores, ids, None))\n                    continue\n                # imght = orig_img.shape[0]\n                # imgwidth = orig_img.shape[1]\n                for i, box in enumerate(boxes):\n                    inps[i], cropped_box = self.transformation.test_transform(orig_img, box)\n                    cropped_boxes[i] = torch.FloatTensor(cropped_box)\n\n                # inps, cropped_boxes = self.transformation.align_transform(orig_img, boxes)\n\n                self.wait_and_put(self.pose_queue, (inps, orig_img, im_name, boxes, scores, ids, cropped_boxes))\n\n    def read(self):\n        return self.wait_and_get(self.pose_queue)\n\n    @property\n    def stopped(self):\n        if self.opt.sp:\n            return self._stopped\n        else:\n            return self._stopped.value\n\n    @property\n    def length(self):\n        return self.datalen\n\n    @property\n    def joint_pairs(self):\n        """"""Joint pairs which defines the pairs of joint to be swapped\n        when the image is flipped horizontally.""""""\n        return [[1, 2], [3, 4], [5, 6], [7, 8],\n                [9, 10], [11, 12], [13, 14], [15, 16]]\n'"
alphapose/utils/env.py,5,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\nimport os\nimport torch\nimport torch.distributed as dist\n\n\ndef init_dist(opt):\n    """"""Initialize distributed computing environment.""""""\n    opt.ngpus_per_node = torch.cuda.device_count()\n\n    torch.cuda.set_device(opt.gpu)\n\n    if opt.launcher == \'pytorch\':\n        _init_dist_pytorch(opt)\n    elif opt.launcher == \'mpi\':\n        _init_dist_mpi(opt)\n    elif opt.launcher == \'slurm\':\n        _init_dist_slurm(opt)\n    else:\n        raise ValueError(\'Invalid launcher type: {}\'.format(opt.launcher))\n\n\ndef _init_dist_pytorch(opt, **kwargs):\n    """"""Set up environment.""""""\n    # TODO: use local_rank instead of rank % num_gpus\n    opt.rank = opt.rank * opt.ngpus_per_node + opt.gpu\n    opt.world_size = opt.world_size\n    dist.init_process_group(backend=opt.dist_backend, init_method=opt.dist_url,\n                            world_size=opt.world_size, rank=opt.rank)\n    print(f""{opt.dist_url}, ws:{opt.world_size}, rank:{opt.rank}"")\n\n    if opt.rank % opt.ngpus_per_node == 0:\n        opt.log = True\n    else:\n        opt.log = False\n\n\ndef _init_dist_slurm(opt, port=23348, **kwargs):\n    """"""Set up slurm environment.""""""\n    proc_id = int(os.environ[\'SLURM_PROCID\'])\n    ntasks = int(os.environ[\'SLURM_NTASKS\'])\n    node_list = os.environ[\'SLURM_NODELIST\']\n    num_gpus = torch.cuda.device_count()\n    torch.cuda.set_device(proc_id % num_gpus)\n    if \'[\' in node_list:\n        beg = node_list.find(\'[\')\n        pos1 = node_list.find(\'-\', beg)\n        if pos1 < 0:\n            pos1 = 1000\n        pos2 = node_list.find(\',\', beg)\n        if pos2 < 0:\n            pos2 = 1000\n        node_list = node_list[:min(pos1, pos2)].replace(\'[\', \'\')\n    addr = node_list[8:].replace(\'-\', \'.\')\n    os.environ[\'MASTER_PORT\'] = str(port)\n    os.environ[\'MASTER_ADDR\'] = addr\n    os.environ[\'WORLD_SIZE\'] = str(ntasks)\n    os.environ[\'RANK\'] = str(proc_id)\n\n    opt.ngpus_per_node = num_gpus\n    opt.rank = int(proc_id)\n    opt.rank = proc_id * num_gpus + opt.gpu\n    opt.world_size = int(ntasks) * num_gpus\n\n    print(f""tcp://{node_list}:{port}, ws:{opt.world_size}, rank:{opt.rank}, proc_id:{proc_id}"")\n    dist.init_process_group(backend=opt.dist_backend,\n                            init_method=f\'tcp://{node_list}:{port}\',\n                            world_size=opt.world_size,\n                            rank=opt.rank)\n    if opt.rank == 0:\n        opt.log = True\n    else:\n        opt.log = False\n\n\ndef _init_dist_mpi(backend, **kwargs):\n    raise NotImplementedError\n'"
alphapose/utils/file_detector.py,7,"b'from itertools import count\nfrom threading import Thread\nfrom queue import Queue\nimport json\n\nimport cv2\nimport scipy.misc\nimport numpy as np\n\nimport torch\nimport torch.multiprocessing as mp\n\nfrom alphapose.utils.presets import SimpleTransform\n\n\nclass FileDetectionLoader():\n    def __init__(self, input_source, cfg, opt, queueSize=128):\n        self.cfg = cfg\n        self.opt = opt\n        self.bbox_file = input_source\n\n        self._input_size = cfg.DATA_PRESET.IMAGE_SIZE\n        self._output_size = cfg.DATA_PRESET.HEATMAP_SIZE\n\n        self._sigma = cfg.DATA_PRESET.SIGMA\n\n        if cfg.DATA_PRESET.TYPE == \'simple\':\n            self.transformation = SimpleTransform(\n                self, scale_factor=0,\n                input_size=self._input_size,\n                output_size=self._output_size,\n                rot=0, sigma=self._sigma,\n                train=False, add_dpg=False)\n\n        # initialize the det file list\n        boxes = None\n        with open(self.bbox_file, \'r\') as f:\n            boxes = json.load(f)\n        assert boxes is not None, \'Load %s fail!\' % self.bbox_file\n\n        self.all_imgs = []\n        self.all_boxes = {}\n        self.all_scores = {}\n        self.all_ids = {}\n        num_boxes = 0\n        for k_img in range(0, len(boxes)):\n            det_res = boxes[k_img]\n            img_name = det_res[\'image_id\']\n            if img_name not in self.all_imgs:\n                self.all_imgs.append(img_name)\n                self.all_boxes[img_name] = []\n                self.all_scores[img_name] = []\n                self.all_ids[img_name] = []\n            x1, y1, w, h = det_res[\'bbox\']\n            bbox = [x1, y1, x1 + w, y1 + h]\n            score = det_res[\'score\']\n            self.all_boxes[img_name].append(bbox)\n            self.all_scores[img_name].append(score)\n            if \'idx\' in det_res.keys():\n                self.all_ids[img_name].append(int(det_res[\'idx\']))\n            else:\n                self.all_ids[img_name].append(0)\n\n        # initialize the queue used to store data\n        """"""\n        pose_queue: the buffer storing post-processed cropped human image for pose estimation\n        """"""\n        if opt.sp:\n            self._stopped = False\n            self.pose_queue = Queue(maxsize=queueSize)\n        else:\n            self._stopped = mp.Value(\'b\', False)\n            self.pose_queue = mp.Queue(maxsize=queueSize)\n\n    def start_worker(self, target):\n        if self.opt.sp:\n            p = Thread(target=target, args=())\n        else:\n            p = mp.Process(target=target, args=())\n        # p.daemon = True\n        p.start()\n        return p\n\n    def start(self):\n        # start a thread to pre process images for object detection\n        image_preprocess_worker = self.start_worker(self.get_detection)\n        return [image_preprocess_worker]\n\n    def stop(self):\n        # clear queues\n        self.clear_queues()\n\n    def terminate(self):\n        if self.opt.sp:\n            self._stopped = True\n        else:\n            self._stopped.value = True\n        self.stop()\n\n    def clear_queues(self):\n        self.clear(self.pose_queue)\n\n    def clear(self, queue):\n        while not queue.empty():\n            queue.get()\n\n    def wait_and_put(self, queue, item):\n        if not self.stopped:\n            queue.put(item)\n\n    def wait_and_get(self, queue):\n        if not self.stopped:\n            return queue.get()\n\n    def get_detection(self):\n        \n        for im_name_k in self.all_imgs:\n            boxes = torch.from_numpy(np.array(self.all_boxes[im_name_k]))\n            scores = torch.from_numpy(np.array(self.all_scores[im_name_k]))\n            ids = torch.from_numpy(np.array(self.all_ids[im_name_k]))\n            orig_img_k = scipy.misc.imread(im_name_k, mode=\'RGB\')\n\n\n            inps = torch.zeros(boxes.size(0), 3, *self._input_size)\n            cropped_boxes = torch.zeros(boxes.size(0), 4)\n            for i, box in enumerate(boxes):\n                inps[i], cropped_box = self.transformation.test_transform(orig_img_k, box)\n                cropped_boxes[i] = torch.FloatTensor(cropped_box)\n\n            \n            self.wait_and_put(self.pose_queue, (inps, orig_img_k, im_name_k, boxes, scores, ids, cropped_boxes))\n        \n        self.wait_and_put(self.pose_queue, (None, None, None, None, None, None, None))\n        return\n        \n    def read(self):\n        return self.wait_and_get(self.pose_queue)\n\n    @property\n    def stopped(self):\n        if self.opt.sp:\n            return self._stopped\n        else:\n            return self._stopped.value\n\n    @property\n    def joint_pairs(self):\n        """"""Joint pairs which defines the pairs of joint to be swapped\n        when the image is flipped horizontally.""""""\n        return [[1, 2], [3, 4], [5, 6], [7, 8],\n                [9, 10], [11, 12], [13, 14], [15, 16]]\n'"
alphapose/utils/logger.py,4,"b""# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\nimport torch\nimport torch.nn.functional as F\n\n\ndef board_writing(writer, loss, acc, iterations, dataset='Train'):\n    writer.add_scalar(\n        '{}/Loss'.format(dataset), loss, iterations)\n    writer.add_scalar(\n        '{}/acc'.format(dataset), acc, iterations)\n\n\ndef debug_writing(writer, outputs, labels, inputs, iterations):\n    tmp_tar = torch.unsqueeze(labels.cpu().data[0], dim=1)\n    # tmp_out = torch.unsqueeze(outputs.cpu().data[0], dim=1)\n\n    tmp_inp = inputs.cpu().data[0]\n    tmp_inp[0] += 0.406\n    tmp_inp[1] += 0.457\n    tmp_inp[2] += 0.480\n\n    tmp_inp[0] += torch.sum(F.interpolate(tmp_tar, scale_factor=4, mode='bilinear'), dim=0)[0]\n    tmp_inp.clamp_(0, 1)\n\n    writer.add_image('Data/input', tmp_inp, iterations)\n"""
alphapose/utils/metrics.py,3,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\nimport os\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom .transforms import get_max_pred_batch\n\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\n\nclass DataLogger(object):\n    """"""Average data logger.""""""\n    def __init__(self):\n        self.clear()\n\n    def clear(self):\n        self.value = 0\n        self.sum = 0\n        self.cnt = 0\n        self.avg = 0\n\n    def update(self, value, n=1):\n        self.value = value\n        self.sum += value * n\n        self.cnt += n\n        self._cal_avg()\n\n    def _cal_avg(self):\n        self.avg = self.sum / self.cnt\n\n\ndef calc_iou(pred, target):\n    """"""Calculate mask iou""""""\n    if isinstance(pred, torch.Tensor):\n        pred = pred.cpu().data.numpy()\n    if isinstance(target, torch.Tensor):\n        target = target.cpu().data.numpy()\n\n    pred = pred >= 0.5\n    target = target >= 0.5\n\n    intersect = (pred == target) * pred * target\n    union = np.maximum(pred, target)\n\n    if pred.ndim == 2:\n        iou = np.sum(intersect) / np.sum(union)\n    elif pred.ndim == 3 or pred.ndim == 4:\n        n_samples = pred.shape[0]\n        intersect = intersect.reshape(n_samples, -1)\n        union = union.reshape(n_samples, -1)\n\n        iou = np.mean(np.sum(intersect, axis=1) / np.sum(union, axis=1))\n\n    return iou\n\n\ndef mask_cross_entropy(pred, target):\n    return F.binary_cross_entropy_with_logits(\n        pred, target, reduction=\'mean\')[None]\n\n\ndef evaluate_mAP(res_file, ann_type=\'bbox\', ann_file=\'person_keypoints_val2017.json\', silence=True):\n    """"""Evaluate mAP result for coco dataset.\n\n    Parameters\n    ----------\n    res_file: str\n        Path to result json file.\n    ann_type: str\n        annotation type, including: `bbox`, `segm`, `keypoints`.\n    ann_file: str\n        Path to groundtruth file.\n    silence: bool\n        True: disable running log.\n\n    """"""\n    class NullWriter(object):\n        def write(self, arg):\n            pass\n\n    ann_file = os.path.join(\'./data/coco/annotations/\', ann_file)\n\n    if silence:\n        nullwrite = NullWriter()\n        oldstdout = sys.stdout\n        sys.stdout = nullwrite  # disable output\n\n    cocoGt = COCO(ann_file)\n    cocoDt = cocoGt.loadRes(res_file)\n\n    cocoEval = COCOeval(cocoGt, cocoDt, ann_type)\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    cocoEval.summarize()\n\n    if silence:\n        sys.stdout = oldstdout  # enable output\n\n    stats_names = [\'AP\', \'Ap .5\', \'AP .75\', \'AP (M)\', \'AP (L)\',\n                   \'AR\', \'AR .5\', \'AR .75\', \'AR (M)\', \'AR (L)\']\n    info_str = {}\n    for ind, name in enumerate(stats_names):\n        info_str[name] = cocoEval.stats[ind]\n\n    return info_str\n\n\ndef calc_accuracy(preds, labels):\n    """"""Calculate heatmap accuracy.""""""\n    preds = preds.cpu().data.numpy()\n    labels = labels.cpu().data.numpy()\n\n    num_joints = preds.shape[1]\n\n    norm = 1.0\n    hm_h = preds.shape[2]\n    hm_w = preds.shape[3]\n\n    preds, _ = get_max_pred_batch(preds)\n    labels, _ = get_max_pred_batch(labels)\n    norm = np.ones((preds.shape[0], 2)) * np.array([hm_w, hm_h]) / 10\n\n    dists = calc_dist(preds, labels, norm)\n\n    acc = 0\n    sum_acc = 0\n    cnt = 0\n    for i in range(num_joints):\n        acc = dist_acc(dists[i])\n        if acc >= 0:\n            sum_acc += acc\n            cnt += 1\n\n    if cnt > 0:\n        return sum_acc / cnt\n    else:\n        return 0\n\n\ndef calc_dist(preds, target, normalize):\n    """"""Calculate normalized distances""""""\n    preds = preds.astype(np.float32)\n    target = target.astype(np.float32)\n    dists = np.zeros((preds.shape[1], preds.shape[0]))\n\n    for n in range(preds.shape[0]):\n        for c in range(preds.shape[1]):\n            if target[n, c, 0] > 1 and target[n, c, 1] > 1:\n                normed_preds = preds[n, c, :] / normalize[n]\n                normed_targets = target[n, c, :] / normalize[n]\n                dists[c, n] = np.linalg.norm(normed_preds - normed_targets)\n            else:\n                dists[c, n] = -1\n\n    return dists\n\n\ndef dist_acc(dists, thr=0.5):\n    """"""Calculate accuracy with given input distance.""""""\n    dist_cal = np.not_equal(dists, -1)\n    num_dist_cal = dist_cal.sum()\n    if num_dist_cal > 0:\n        return np.less(dists[dist_cal], thr).sum() * 1.0 / num_dist_cal\n    else:\n        return -1\n'"
alphapose/utils/pPose_nms.py,35,"b'# -*- coding: utf-8 -*-\nimport json\nimport os\nimport zipfile\nimport time\nfrom multiprocessing.dummy import Pool as ThreadPool\n\nimport torch\nimport numpy as np\n\n\'\'\' Constant Configuration \'\'\'\ndelta1 = 1\nmu = 1.7\ndelta2 = 2.65\ngamma = 22.48\nscoreThreds = 0.3\nmatchThreds = 5\nalpha = 0.1\n#pool = ThreadPool(4)\n\n\ndef pose_nms(bboxes, bbox_scores, bbox_ids, pose_preds, pose_scores, areaThres=0):\n    \'\'\'\n    Parametric Pose NMS algorithm\n    bboxes:         bbox locations list (n, 4)\n    bbox_scores:    bbox scores list (n, 1)\n    bbox_ids:       bbox tracking ids list (n, 1)\n    pose_preds:     pose locations list (n, kp_num, 2)\n    pose_scores:    pose scores list    (n, kp_num, 1)\n    \'\'\'\n    #global ori_pose_preds, ori_pose_scores, ref_dists\n\n    pose_scores[pose_scores == 0] = 1e-5\n    kp_nums = pose_preds.size()[1]\n    final_result = []\n    \n    ori_bboxes = bboxes.clone()\n    ori_bbox_scores = bbox_scores.clone()\n    ori_bbox_ids = bbox_ids.clone()\n    ori_pose_preds = pose_preds.clone()\n    ori_pose_scores = pose_scores.clone()\n\n    xmax = bboxes[:, 2]\n    xmin = bboxes[:, 0]\n    ymax = bboxes[:, 3]\n    ymin = bboxes[:, 1]\n\n    widths = xmax - xmin\n    heights = ymax - ymin\n    ref_dists = alpha * np.maximum(widths, heights)\n\n    nsamples = bboxes.shape[0]\n    human_scores = pose_scores.mean(dim=1)\n\n    human_ids = np.arange(nsamples)\n    # Do pPose-NMS\n    pick = []\n    merge_ids = []\n    while(human_scores.shape[0] != 0):\n        # Pick the one with highest score\n        pick_id = torch.argmax(human_scores)\n        pick.append(human_ids[pick_id])\n        # num_visPart = torch.sum(pose_scores[pick_id] > 0.2)\n\n        # Get numbers of match keypoints by calling PCK_match\n        ref_dist = ref_dists[human_ids[pick_id]]\n        simi = get_parametric_distance(pick_id, pose_preds, pose_scores, ref_dist)\n        num_match_keypoints = PCK_match(pose_preds[pick_id], pose_preds, ref_dist)\n\n        # Delete humans who have more than matchThreds keypoints overlap and high similarity\n        delete_ids = torch.from_numpy(np.arange(human_scores.shape[0]))[((simi > gamma) | (num_match_keypoints >= matchThreds))]\n\n        if delete_ids.shape[0] == 0:\n            delete_ids = pick_id\n        #else:\n        #    delete_ids = torch.from_numpy(delete_ids)\n\n        merge_ids.append(human_ids[delete_ids])\n        pose_preds = np.delete(pose_preds, delete_ids, axis=0)\n        pose_scores = np.delete(pose_scores, delete_ids, axis=0)\n        human_ids = np.delete(human_ids, delete_ids)\n        human_scores = np.delete(human_scores, delete_ids, axis=0)\n        bbox_scores = np.delete(bbox_scores, delete_ids, axis=0)\n        bbox_ids = np.delete(bbox_ids, delete_ids, axis=0)\n\n    assert len(merge_ids) == len(pick)\n    preds_pick = ori_pose_preds[pick]\n    scores_pick = ori_pose_scores[pick]\n    bbox_scores_pick = ori_bbox_scores[pick]\n    bboxes_pick = ori_bboxes[pick]\n    bbox_ids_pick = ori_bbox_ids[pick]\n    #final_result = pool.map(filter_result, zip(scores_pick, merge_ids, preds_pick, pick, bbox_scores_pick))\n    #final_result = [item for item in final_result if item is not None]\n\n    for j in range(len(pick)):\n        ids = np.arange(kp_nums)\n        max_score = torch.max(scores_pick[j, ids, 0])\n\n        if max_score < scoreThreds:\n            continue\n\n        # Merge poses\n        merge_id = merge_ids[j]\n        merge_pose, merge_score = p_merge_fast(\n            preds_pick[j], ori_pose_preds[merge_id], ori_pose_scores[merge_id], ref_dists[pick[j]])\n\n        max_score = torch.max(merge_score[ids])\n        if max_score < scoreThreds:\n            continue\n\n        xmax = max(merge_pose[:, 0])\n        xmin = min(merge_pose[:, 0])\n        ymax = max(merge_pose[:, 1])\n        ymin = min(merge_pose[:, 1])\n        bbox = bboxes_pick[j].cpu().tolist()\n\n        if (1.5 ** 2 * (xmax - xmin) * (ymax - ymin) < areaThres):\n            continue\n\n        final_result.append({\n            \'box\': [bbox[0], bbox[1], bbox[2]-bbox[0],bbox[3]-bbox[1]],\n            \'keypoints\': merge_pose - 0.3,\n            \'kp_score\': merge_score,\n            \'proposal_score\': torch.mean(merge_score) + bbox_scores_pick[j] + 1.25 * max(merge_score),\n            \'idx\' : ori_bbox_ids[merge_id].tolist()\n        })\n\n    return final_result\n\n\ndef filter_result(args):\n    score_pick, merge_id, pred_pick, pick, bbox_score_pick = args\n    global ori_pose_preds, ori_pose_scores, ref_dists\n    kp_nums = ori_pose_preds.size()[1]\n    ids = np.arange(kp_nums)\n    max_score = torch.max(score_pick[ids, 0])\n\n    if max_score < scoreThreds:\n        return None\n\n    # Merge poses\n    merge_pose, merge_score = p_merge_fast(\n        pred_pick, ori_pose_preds[merge_id], ori_pose_scores[merge_id], ref_dists[pick])\n\n    max_score = torch.max(merge_score[ids])\n    if max_score < scoreThreds:\n        return None\n\n    xmax = max(merge_pose[:, 0])\n    xmin = min(merge_pose[:, 0])\n    ymax = max(merge_pose[:, 1])\n    ymin = min(merge_pose[:, 1])\n\n    if (1.5 ** 2 * (xmax - xmin) * (ymax - ymin) < 40 * 40.5):\n        return None\n\n    return {\n        \'keypoints\': merge_pose - 0.3,\n        \'kp_score\': merge_score,\n        \'proposal_score\': torch.mean(merge_score) + bbox_score_pick + 1.25 * max(merge_score)\n    }\n\n\ndef p_merge(ref_pose, cluster_preds, cluster_scores, ref_dist):\n    \'\'\'\n    Score-weighted pose merging\n    INPUT:\n        ref_pose:       reference pose          -- [kp_num, 2]\n        cluster_preds:  redundant poses         -- [n, kp_num, 2]\n        cluster_scores: redundant poses score   -- [n, kp_num, 1]\n        ref_dist:       reference scale         -- Constant\n    OUTPUT:\n        final_pose:     merged pose             -- [kp_num, 2]\n        final_score:    merged score            -- [kp_num]\n    \'\'\'\n    dist = torch.sqrt(torch.sum(\n        torch.pow(ref_pose[np.newaxis, :] - cluster_preds, 2),\n        dim=2\n    ))  # [n, kp_num]\n\n    kp_num = ref_pose.size()[0]\n    ref_dist = min(ref_dist, 15)\n\n    mask = (dist <= ref_dist)\n    final_pose = torch.zeros(kp_num, 2)\n    final_score = torch.zeros(kp_num)\n\n    if cluster_preds.dim() == 2:\n        cluster_preds.unsqueeze_(0)\n        cluster_scores.unsqueeze_(0)\n    if mask.dim() == 1:\n        mask.unsqueeze_(0)\n\n    for i in range(kp_num):\n        cluster_joint_scores = cluster_scores[:, i][mask[:, i]]  # [k, 1]\n        cluster_joint_location = cluster_preds[:, i, :][mask[:, i].unsqueeze(\n            -1).repeat(1, 2)].view((torch.sum(mask[:, i]), -1))\n\n        # Get an normalized score\n        normed_scores = cluster_joint_scores / torch.sum(cluster_joint_scores)\n\n        # Merge poses by a weighted sum\n        final_pose[i, 0] = torch.dot(cluster_joint_location[:, 0], normed_scores.squeeze(-1))\n        final_pose[i, 1] = torch.dot(cluster_joint_location[:, 1], normed_scores.squeeze(-1))\n\n        final_score[i] = torch.dot(cluster_joint_scores.transpose(0, 1).squeeze(0), normed_scores.squeeze(-1))\n\n    return final_pose, final_score\n\n\ndef p_merge_fast(ref_pose, cluster_preds, cluster_scores, ref_dist):\n    \'\'\'\n    Score-weighted pose merging\n    INPUT:\n        ref_pose:       reference pose          -- [kp_num, 2]\n        cluster_preds:  redundant poses         -- [n, kp_num, 2]\n        cluster_scores: redundant poses score   -- [n, kp_num, 1]\n        ref_dist:       reference scale         -- Constant\n    OUTPUT:\n        final_pose:     merged pose             -- [kp_num, 2]\n        final_score:    merged score            -- [kp_num]\n    \'\'\'\n    dist = torch.sqrt(torch.sum(\n        torch.pow(ref_pose[np.newaxis, :] - cluster_preds, 2),\n        dim=2\n    ))\n\n    kp_num = ref_pose.size()[0]\n    ref_dist = min(ref_dist, 15)\n\n    mask = (dist <= ref_dist)\n    final_pose = torch.zeros(kp_num, 2)\n    final_score = torch.zeros(kp_num)\n\n    if cluster_preds.dim() == 2:\n        cluster_preds.unsqueeze_(0)\n        cluster_scores.unsqueeze_(0)\n    if mask.dim() == 1:\n        mask.unsqueeze_(0)\n\n    # Weighted Merge\n    masked_scores = cluster_scores.mul(mask.float().unsqueeze(-1))\n    normed_scores = masked_scores / torch.sum(masked_scores, dim=0)\n\n    final_pose = torch.mul(cluster_preds, normed_scores.repeat(1, 1, 2)).sum(dim=0)\n    final_score = torch.mul(masked_scores, normed_scores).sum(dim=0)\n    return final_pose, final_score\n\n\ndef get_parametric_distance(i, all_preds, keypoint_scores, ref_dist):\n    pick_preds = all_preds[i]\n    pred_scores = keypoint_scores[i]\n    dist = torch.sqrt(torch.sum(\n        torch.pow(pick_preds[np.newaxis, :] - all_preds, 2),\n        dim=2\n    ))\n    mask = (dist <= 1)\n\n    kp_nums = all_preds.size()[1]\n    # Define a keypoints distance\n    score_dists = torch.zeros(all_preds.shape[0], kp_nums)\n    keypoint_scores.squeeze_()\n    if keypoint_scores.dim() == 1:\n        keypoint_scores.unsqueeze_(0)\n    if pred_scores.dim() == 1:\n        pred_scores.unsqueeze_(1)\n    # The predicted scores are repeated up to do broadcast\n    pred_scores = pred_scores.repeat(1, all_preds.shape[0]).transpose(0, 1)\n\n    score_dists[mask] = torch.tanh(pred_scores[mask] / delta1) * torch.tanh(keypoint_scores[mask] / delta1)\n\n    point_dist = torch.exp((-1) * dist / delta2)\n    final_dist = torch.sum(score_dists, dim=1) + mu * torch.sum(point_dist, dim=1)\n\n    return final_dist\n\n\ndef PCK_match(pick_pred, all_preds, ref_dist):\n    dist = torch.sqrt(torch.sum(\n        torch.pow(pick_pred[np.newaxis, :] - all_preds, 2),\n        dim=2\n    ))\n    ref_dist = min(ref_dist, 7)\n    num_match_keypoints = torch.sum(\n        dist / ref_dist <= 1,\n        dim=1\n    )\n\n    return num_match_keypoints\n\n\ndef write_json(all_results, outputpath, form=None, for_eval=False):\n    \'\'\'\n    all_result: result dict of predictions\n    outputpath: output directory\n    \'\'\'\n    json_results = []\n    json_results_cmu = {}\n    for im_res in all_results:\n        im_name = im_res[\'imgname\']\n        for human in im_res[\'result\']:\n            keypoints = []\n            result = {}\n            if for_eval:\n                result[\'image_id\'] = int(os.path.basename(im_name).split(\'.\')[0].split(\'_\')[-1])\n            else:\n                result[\'image_id\'] = os.path.basename(im_name)\n            result[\'category_id\'] = 1\n\n            kp_preds = human[\'keypoints\']\n            kp_scores = human[\'kp_score\']\n            pro_scores = human[\'proposal_score\']\n            for n in range(kp_scores.shape[0]):\n                keypoints.append(float(kp_preds[n, 0]))\n                keypoints.append(float(kp_preds[n, 1]))\n                keypoints.append(float(kp_scores[n]))\n            result[\'keypoints\'] = keypoints\n            result[\'score\'] = float(pro_scores)\n            result[\'box\'] = human[\'box\']\n            #pose track results by PoseFlow\n            if \'idx\' in human.keys():\n                result[\'idx\'] = human[\'idx\']\n\n            if form == \'cmu\': # the form of CMU-Pose\n                if result[\'image_id\'] not in json_results_cmu.keys():\n                    json_results_cmu[result[\'image_id\']]={}\n                    json_results_cmu[result[\'image_id\']][\'version\']=""AlphaPose v0.3""\n                    json_results_cmu[result[\'image_id\']][\'bodies\']=[]\n                tmp={\'joints\':[]}\n                result[\'keypoints\'].append((result[\'keypoints\'][15]+result[\'keypoints\'][18])/2)\n                result[\'keypoints\'].append((result[\'keypoints\'][16]+result[\'keypoints\'][19])/2)\n                result[\'keypoints\'].append((result[\'keypoints\'][17]+result[\'keypoints\'][20])/2)\n                indexarr=[0,51,18,24,30,15,21,27,36,42,48,33,39,45,6,3,12,9]\n                for i in indexarr:\n                    tmp[\'joints\'].append(result[\'keypoints\'][i])\n                    tmp[\'joints\'].append(result[\'keypoints\'][i+1])\n                    tmp[\'joints\'].append(result[\'keypoints\'][i+2])\n                json_results_cmu[result[\'image_id\']][\'bodies\'].append(tmp)\n            elif form == \'open\': # the form of OpenPose\n                if result[\'image_id\'] not in json_results_cmu.keys():\n                    json_results_cmu[result[\'image_id\']]={}\n                    json_results_cmu[result[\'image_id\']][\'version\']=""AlphaPose v0.3""\n                    json_results_cmu[result[\'image_id\']][\'people\']=[]\n                tmp={\'pose_keypoints_2d\':[]}\n                result[\'keypoints\'].append((result[\'keypoints\'][15]+result[\'keypoints\'][18])/2)\n                result[\'keypoints\'].append((result[\'keypoints\'][16]+result[\'keypoints\'][19])/2)\n                result[\'keypoints\'].append((result[\'keypoints\'][17]+result[\'keypoints\'][20])/2)\n                indexarr=[0,51,18,24,30,15,21,27,36,42,48,33,39,45,6,3,12,9]\n                for i in indexarr:\n                    tmp[\'pose_keypoints_2d\'].append(result[\'keypoints\'][i])\n                    tmp[\'pose_keypoints_2d\'].append(result[\'keypoints\'][i+1])\n                    tmp[\'pose_keypoints_2d\'].append(result[\'keypoints\'][i+2])\n                json_results_cmu[result[\'image_id\']][\'people\'].append(tmp)\n            else:\n                json_results.append(result)\n\n    if form == \'cmu\': # the form of CMU-Pose\n        with open(os.path.join(outputpath,\'alphapose-results.json\'), \'w\') as json_file:\n            json_file.write(json.dumps(json_results_cmu))\n            if not os.path.exists(os.path.join(outputpath,\'sep-json\')):\n                os.mkdir(os.path.join(outputpath,\'sep-json\'))\n            for name in json_results_cmu.keys():\n                with open(os.path.join(outputpath,\'sep-json\',name.split(\'.\')[0]+\'.json\'),\'w\') as json_file:\n                    json_file.write(json.dumps(json_results_cmu[name]))\n    elif form == \'open\': # the form of OpenPose\n        with open(os.path.join(outputpath,\'alphapose-results.json\'), \'w\') as json_file:\n            json_file.write(json.dumps(json_results_cmu))\n            if not os.path.exists(os.path.join(outputpath,\'sep-json\')):\n                os.mkdir(os.path.join(outputpath,\'sep-json\'))\n            for name in json_results_cmu.keys():\n                with open(os.path.join(outputpath,\'sep-json\',name.split(\'.\')[0]+\'.json\'),\'w\') as json_file:\n                    json_file.write(json.dumps(json_results_cmu[name]))\n    else:\n        with open(os.path.join(outputpath,\'alphapose-results.json\'), \'w\') as json_file:\n            json_file.write(json.dumps(json_results))\n\n'"
alphapose/utils/registry.py,0,"b'import inspect\n\n\nclass Registry(object):\n\n    def __init__(self, name):\n        self._name = name\n        self._module_dict = dict()\n\n    def __repr__(self):\n        format_str = self.__class__.__name__ + \'(name={}, items={})\'.format(\n            self._name, list(self._module_dict.keys()))\n        return format_str\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def module_dict(self):\n        return self._module_dict\n\n    def get(self, key):\n        return self._module_dict.get(key, None)\n\n    def _register_module(self, module_class):\n        """"""Register a module.\n\n        Args:\n            module (:obj:`nn.Module`): Module to be registered.\n        """"""\n        if not inspect.isclass(module_class):\n            raise TypeError(\'module must be a class, but got {}\'.format(\n                type(module_class)))\n        module_name = module_class.__name__\n        if module_name in self._module_dict:\n            raise KeyError(\'{} is already registered in {}\'.format(\n                module_name, self.name))\n        self._module_dict[module_name] = module_class\n\n    def register_module(self, cls):\n        self._register_module(cls)\n        return cls\n\n\ndef build_from_cfg(cfg, registry, default_args=None):\n    """"""Build a module from config dict.\n\n    Args:\n        cfg (dict): Config dict. It should at least contain the key ""type"".\n        registry (:obj:`Registry`): The registry to search the type from.\n        default_args (dict, optional): Default initialization arguments.\n\n    Returns:\n        obj: The constructed object.\n    """"""\n    assert isinstance(cfg, dict) and \'TYPE\' in cfg\n    assert isinstance(default_args, dict) or default_args is None\n    args = cfg.copy()\n    obj_type = args.pop(\'TYPE\')\n\n    if isinstance(obj_type, str):\n        obj_cls = registry.get(obj_type)\n        if obj_cls is None:\n            raise KeyError(\'{} is not in the {} registry\'.format(\n                obj_type, registry.name))\n    elif inspect.isclass(obj_type):\n        obj_cls = obj_type\n    else:\n        raise TypeError(\'type must be a str or valid type, but got {}\'.format(\n            type(obj_type)))\n    if default_args is not None:\n        for name, value in default_args.items():\n            args.setdefault(name, value)\n    return obj_cls(**args)\n'"
alphapose/utils/transforms.py,27,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\n""""""Pose related transforrmation functions.""""""\n\nimport torch\nimport cv2\nimport scipy.misc\nimport numpy as np\nimport random\n\n\ndef rnd(x):\n    return max(-2 * x, min(2 * x, np.random.randn(1)[0] * x))\n\n\ndef box_transform(bbox, sf, imgwidth, imght, train):\n    """"""Random scaling.""""""\n    width = bbox[2] - bbox[0]\n    ht = bbox[3] - bbox[1]\n    if train:\n        scaleRate = 0.25 * np.clip(np.random.randn() * sf, - sf, sf)\n\n        bbox[0] = max(0, bbox[0] - width * scaleRate / 2)\n        bbox[1] = max(0, bbox[1] - ht * scaleRate / 2)\n        bbox[2] = min(imgwidth, bbox[2] + width * scaleRate / 2)\n        bbox[3] = min(imght, bbox[3] + ht * scaleRate / 2)\n    else:\n        scaleRate = 0.25\n\n        bbox[0] = max(0, bbox[0] - width * scaleRate / 2)\n        bbox[1] = max(0, bbox[1] - ht * scaleRate / 2)\n        bbox[2] = min(imgwidth, max(bbox[2] + width * scaleRate / 2, bbox[0] + 5))\n        bbox[3] = min(imght, max(bbox[3] + ht * scaleRate / 2, bbox[1] + 5))\n\n    return bbox\n\n\ndef addDPG(bbox, imgwidth, imght):\n    """"""Add dpg for data augmentation, including random crop and random sample.""""""\n    PatchScale = random.uniform(0, 1)\n    width = bbox[2] - bbox[0]\n    ht = bbox[3] - bbox[1]\n\n    if PatchScale > 0.85:\n        ratio = ht / width\n        if (width < ht):\n            patchWidth = PatchScale * width\n            patchHt = patchWidth * ratio\n        else:\n            patchHt = PatchScale * ht\n            patchWidth = patchHt / ratio\n\n        xmin = bbox[0] + random.uniform(0, 1) * (width - patchWidth)\n        ymin = bbox[1] + random.uniform(0, 1) * (ht - patchHt)\n        xmax = xmin + patchWidth + 1\n        ymax = ymin + patchHt + 1\n    else:\n        xmin = max(1, min(bbox[0] + np.random.normal(-0.0142, 0.1158) * width, imgwidth - 3))\n        ymin = max(1, min(bbox[1] + np.random.normal(0.0043, 0.068) * ht, imght - 3))\n        xmax = min(max(xmin + 2, bbox[2] + np.random.normal(0.0154, 0.1337) * width), imgwidth - 3)\n        ymax = min(max(ymin + 2, bbox[3] + np.random.normal(-0.0013, 0.0711) * ht), imght - 3)\n\n    bbox[0] = xmin\n    bbox[1] = ymin\n    bbox[2] = xmax\n    bbox[3] = ymax\n\n    return bbox\n\n\ndef im_to_torch(img):\n    """"""Transform ndarray image to torch tensor.\n\n    Parameters\n    ----------\n    img: numpy.ndarray\n        An ndarray with shape: `(H, W, 3)`.\n\n    Returns\n    -------\n    torch.Tensor\n        A tensor with shape: `(3, H, W)`.\n\n    """"""\n    img = np.transpose(img, (2, 0, 1))  # C*H*W\n    img = to_torch(img).float()\n    if img.max() > 1:\n        img /= 255\n    return img\n\n\ndef torch_to_im(img):\n    """"""Transform torch tensor to ndarray image.\n\n    Parameters\n    ----------\n    img: torch.Tensor\n        A tensor with shape: `(3, H, W)`.\n\n    Returns\n    -------\n    numpy.ndarray\n        An ndarray with shape: `(H, W, 3)`.\n\n    """"""\n    img = to_numpy(img)\n    img = np.transpose(img, (1, 2, 0))  # C*H*W\n    return img\n\n\ndef load_image(img_path):\n    # H x W x C => C x H x W\n    return im_to_torch(scipy.misc.imread(img_path, mode=\'RGB\'))\n\n\ndef to_numpy(tensor):\n    # torch.Tensor => numpy.ndarray\n    if torch.is_tensor(tensor):\n        return tensor.cpu().numpy()\n    elif type(tensor).__module__ != \'numpy\':\n        raise ValueError(""Cannot convert {} to numpy array""\n                         .format(type(tensor)))\n    return tensor\n\n\ndef to_torch(ndarray):\n    # numpy.ndarray => torch.Tensor\n    if type(ndarray).__module__ == \'numpy\':\n        return torch.from_numpy(ndarray)\n    elif not torch.is_tensor(ndarray):\n        raise ValueError(""Cannot convert {} to torch tensor""\n                         .format(type(ndarray)))\n    return ndarray\n\n\ndef cv_cropBox(img, bbox, input_size):\n    """"""Crop bbox from image by Affinetransform.\n\n    Parameters\n    ----------\n    img: torch.Tensor\n        A tensor with shape: `(3, H, W)`.\n    bbox: list or tuple\n        [xmin, ymin, xmax, ymax].\n    input_size: tuple\n        Resulting image size, as (height, width).\n\n    Returns\n    -------\n    torch.Tensor\n        A tensor with shape: `(3, height, width)`.\n\n    """"""\n    xmin, ymin, xmax, ymax = bbox\n    xmax -= 1\n    ymax -= 1\n    resH, resW = input_size\n\n    lenH = max((ymax - ymin), (xmax - xmin) * resH / resW)\n    lenW = lenH * resW / resH\n    if img.dim() == 2:\n        img = img[np.newaxis, :, :]\n\n    box_shape = [ymax - ymin, xmax - xmin]\n    pad_size = [(lenH - box_shape[0]) // 2, (lenW - box_shape[1]) // 2]\n    # Padding Zeros\n    img[:, :ymin, :], img[:, :, :xmin] = 0, 0\n    img[:, ymax + 1:, :], img[:, :, xmax + 1:] = 0, 0\n\n    src = np.zeros((3, 2), dtype=np.float32)\n    dst = np.zeros((3, 2), dtype=np.float32)\n\n    src[0, :] = np.array([xmin - pad_size[1], ymin - pad_size[0]], np.float32)\n    src[1, :] = np.array([xmax + pad_size[1], ymax + pad_size[0]], np.float32)\n    dst[0, :] = 0\n    dst[1, :] = np.array([resW - 1, resH - 1], np.float32)\n\n    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\n    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\n\n    trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n    dst_img = cv2.warpAffine(torch_to_im(img), trans,\n                             (resW, resH), flags=cv2.INTER_LINEAR)\n    if dst_img.ndim == 2:\n        dst_img = dst_img[:, :, np.newaxis]\n\n    return im_to_torch(torch.Tensor(dst_img))\n\n\ndef cv_cropBox_rot(img, bbox, input_size, rot):\n    """"""Crop bbox from image by Affinetransform.\n\n    Parameters\n    ----------\n    img: torch.Tensor\n        A tensor with shape: `(3, H, W)`.\n    bbox: list or tuple\n        [xmin, ymin, xmax, ymax].\n    input_size: tuple\n        Resulting image size, as (height, width).\n\n    Returns\n    -------\n    torch.Tensor\n        A tensor with shape: `(3, height, width)`.\n\n    """"""\n    xmin, ymin, xmax, ymax = bbox\n    xmax -= 1\n    ymax -= 1\n    resH, resW = input_size\n    rot_rad = np.pi * rot / 180\n\n    if img.dim() == 2:\n        img = img[np.newaxis, :, :]\n\n    src = np.zeros((3, 2), dtype=np.float32)\n    dst = np.zeros((3, 2), dtype=np.float32)\n    center = np.array([(xmax + xmin) / 2, (ymax + ymin) / 2])\n\n    src_dir = get_dir([0, (ymax - ymin) * -0.5], rot_rad)\n    dst_dir = np.array([0, (resH - 1) * -0.5], np.float32)\n\n    src = np.zeros((3, 2), dtype=np.float32)\n    dst = np.zeros((3, 2), dtype=np.float32)\n\n    src[0, :] = center\n    src[1, :] = center + src_dir\n    dst[0, :] = [(resW - 1) * 0.5, (resH - 1) * 0.5]\n    dst[1, :] = np.array([(resW - 1) * 0.5, (resH - 1) * 0.5]) + dst_dir\n\n    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\n    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\n\n    trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n    dst_img = cv2.warpAffine(torch_to_im(img), trans,\n                             (resW, resH), flags=cv2.INTER_LINEAR)\n    if dst_img.ndim == 2:\n        dst_img = dst_img[:, :, np.newaxis]\n\n    return im_to_torch(torch.Tensor(dst_img))\n\n\ndef fix_cropBox(img, bbox, input_size):\n    """"""Crop bbox from image by Affinetransform.\n\n    Parameters\n    ----------\n    img: torch.Tensor\n        A tensor with shape: `(3, H, W)`.\n    bbox: list or tuple\n        [xmin, ymin, xmax, ymax].\n    input_size: tuple\n        Resulting image size, as (height, width).\n\n    Returns\n    -------\n    torch.Tensor\n        A tensor with shape: `(3, height, width)`.\n\n    """"""\n    xmin, ymin, xmax, ymax = bbox\n    input_ratio = input_size[0] / input_size[1]\n    bbox_ratio = (ymax - ymin) / (xmax - xmin)\n    if bbox_ratio > input_ratio:\n        # expand width\n        cx = (xmax + xmin) / 2\n        h = ymax - ymin\n        w = h / input_ratio\n        xmin = cx - w / 2\n        xmax = cx + w / 2\n    elif bbox_ratio < input_ratio:\n        # expand height\n        cy = (ymax + ymin) / 2\n        w = xmax - xmin\n        h = w * input_ratio\n        ymin = cy - h / 2\n        ymax = cy + h / 2\n    bbox = [int(x) for x in [xmin, ymin, xmax, ymax]]\n\n    return cv_cropBox(img, bbox, input_size), bbox\n\n\ndef fix_cropBox_rot(img, bbox, input_size, rot):\n    """"""Crop bbox from image by Affinetransform.\n\n    Parameters\n    ----------\n    img: torch.Tensor\n        A tensor with shape: `(3, H, W)`.\n    bbox: list or tuple\n        [xmin, ymin, xmax, ymax].\n    input_size: tuple\n        Resulting image size, as (height, width).\n\n    Returns\n    -------\n    torch.Tensor\n        A tensor with shape: `(3, height, width)`.\n\n    """"""\n    xmin, ymin, xmax, ymax = bbox\n    input_ratio = input_size[0] / input_size[1]\n    bbox_ratio = (ymax - ymin) / (xmax - xmin)\n    if bbox_ratio > input_ratio:\n        # expand width\n        cx = (xmax + xmin) / 2\n        h = ymax - ymin\n        w = h / input_ratio\n        xmin = cx - w / 2\n        xmax = cx + w / 2\n    elif bbox_ratio < input_ratio:\n        # expand height\n        cy = (ymax + ymin) / 2\n        w = xmax - xmin\n        h = w * input_ratio\n        ymin = cy - h / 2\n        ymax = cy + h / 2\n    bbox = [int(x) for x in [xmin, ymin, xmax, ymax]]\n\n    return cv_cropBox_rot(img, bbox, input_size, rot), bbox\n\n\ndef get_3rd_point(a, b):\n    """"""Return vector c that perpendicular to (a - b).""""""\n    direct = a - b\n    return b + np.array([-direct[1], direct[0]], dtype=np.float32)\n\n\ndef get_dir(src_point, rot_rad):\n    """"""Rotate the point by `rot_rad` degree.""""""\n    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n\n    src_result = [0, 0]\n    src_result[0] = src_point[0] * cs - src_point[1] * sn\n    src_result[1] = src_point[0] * sn + src_point[1] * cs\n\n    return src_result\n\n\ndef cv_cropBoxInverse(inp, bbox, img_size, output_size):\n    """"""Paste the cropped bbox to the original image.\n\n    Parameters\n    ----------\n    inp: torch.Tensor\n        A tensor with shape: `(3, height, width)`.\n    bbox: list or tuple\n        [xmin, ymin, xmax, ymax].\n    img_size: tuple\n        Original image size, as (img_H, img_W).\n    output_size: tuple\n        Cropped input size, as (height, width).\n    Returns\n    -------\n    torch.Tensor\n        A tensor with shape: `(3, img_H, img_W)`.\n\n    """"""\n    xmin, ymin, xmax, ymax = bbox\n    xmax -= 1\n    ymax -= 1\n    resH, resW = output_size\n    imgH, imgW = img_size\n\n    lenH = max((ymax - ymin), (xmax - xmin) * resH / resW)\n    lenW = lenH * resW / resH\n    if inp.dim() == 2:\n        inp = inp[np.newaxis, :, :]\n\n    box_shape = [ymax - ymin, xmax - xmin]\n    pad_size = [(lenH - box_shape[0]) // 2, (lenW - box_shape[1]) // 2]\n\n    src = np.zeros((3, 2), dtype=np.float32)\n    dst = np.zeros((3, 2), dtype=np.float32)\n\n    src[0, :] = 0\n    src[1, :] = np.array([resW - 1, resH - 1], np.float32)\n    dst[0, :] = np.array([xmin - pad_size[1], ymin - pad_size[0]], np.float32)\n    dst[1, :] = np.array([xmax + pad_size[1], ymax + pad_size[0]], np.float32)\n\n    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\n    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\n\n    trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n    dst_img = cv2.warpAffine(torch_to_im(inp), trans,\n                             (imgW, imgH), flags=cv2.INTER_LINEAR)\n    if dst_img.ndim == 3 and dst_img.shape[2] == 1:\n        dst_img = dst_img[:, :, 0]\n        return dst_img\n    elif dst_img.ndim == 2:\n        return dst_img\n    else:\n        return im_to_torch(torch.Tensor(dst_img))\n\n\ndef cv_rotate(img, rot, input_size):\n    """"""Rotate image by Affinetransform.\n\n    Parameters\n    ----------\n    img: torch.Tensor\n        A tensor with shape: `(3, H, W)`.\n    rot: int\n        Rotation degree.\n    input_size: tuple\n        Resulting image size, as (height, width).\n\n    Returns\n    -------\n    torch.Tensor\n        A tensor with shape: `(3, height, width)`.\n\n    """"""\n    resH, resW = input_size\n    center = np.array((resW - 1, resH - 1)) / 2\n    rot_rad = np.pi * rot / 180\n\n    src_dir = get_dir([0, (resH - 1) * -0.5], rot_rad)\n    dst_dir = np.array([0, (resH - 1) * -0.5], np.float32)\n\n    src = np.zeros((3, 2), dtype=np.float32)\n    dst = np.zeros((3, 2), dtype=np.float32)\n\n    src[0, :] = center\n    src[1, :] = center + src_dir\n    dst[0, :] = [(resW - 1) * 0.5, (resH - 1) * 0.5]\n    dst[1, :] = np.array([(resW - 1) * 0.5, (resH - 1) * 0.5]) + dst_dir\n\n    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\n    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\n\n    trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n\n    dst_img = cv2.warpAffine(torch_to_im(img), trans,\n                             (resW, resH), flags=cv2.INTER_LINEAR)\n    if dst_img.ndim == 2:\n        dst_img = dst_img[:, :, np.newaxis]\n\n    return im_to_torch(torch.Tensor(dst_img))\n\n\ndef count_visible(bbox, joints_3d):\n    """"""Count number of visible joints given bound box.""""""\n    vis = np.logical_and.reduce((\n        joints_3d[:, 0, 0] > 0,\n        joints_3d[:, 0, 0] > bbox[0],\n        joints_3d[:, 0, 0] < bbox[2],\n        joints_3d[:, 1, 0] > 0,\n        joints_3d[:, 1, 0] > bbox[1],\n        joints_3d[:, 1, 0] < bbox[3],\n        joints_3d[:, 0, 1] > 0,\n        joints_3d[:, 1, 1] > 0\n    ))\n    return np.sum(vis), vis\n\n\ndef drawGaussian(img, pt, sigma):\n    """"""Draw 2d gaussian on input image.\n\n    Parameters\n    ----------\n    img: torch.Tensor\n        A tensor with shape: `(3, H, W)`.\n    pt: list or tuple\n        A point: (x, y).\n    sigma: int\n        Sigma of gaussian distribution.\n\n    Returns\n    -------\n    torch.Tensor\n        A tensor with shape: `(3, H, W)`.\n\n    """"""\n    img = to_numpy(img)\n    tmpSize = 3 * sigma\n    # Check that any part of the gaussian is in-bounds\n    ul = [int(pt[0] - tmpSize), int(pt[1] - tmpSize)]\n    br = [int(pt[0] + tmpSize + 1), int(pt[1] + tmpSize + 1)]\n\n    if (ul[0] >= img.shape[1] or ul[1] >= img.shape[0] or br[0] < 0 or br[1] < 0):\n        # If not, just return the image as is\n        return to_torch(img)\n\n    # Generate gaussian\n    size = 2 * tmpSize + 1\n    x = np.arange(0, size, 1, float)\n    y = x[:, np.newaxis]\n    x0 = y0 = size // 2\n    # The gaussian is not normalized, we want the center value to equal 1\n    g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n\n    # Usable gaussian range\n    g_x = max(0, -ul[0]), min(br[0], img.shape[1]) - ul[0]\n    g_y = max(0, -ul[1]), min(br[1], img.shape[0]) - ul[1]\n    # Image range\n    img_x = max(0, ul[0]), min(br[0], img.shape[1])\n    img_y = max(0, ul[1]), min(br[1], img.shape[0])\n\n    img[img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n    return to_torch(img)\n\n\ndef flip(x):\n    assert (x.dim() == 3 or x.dim() == 4)\n    dim = x.dim() - 1\n\n    return x.flip(dims=(dim,))\n\n\ndef flip_heatmap(heatmap, joint_pairs, shift=False):\n    """"""Flip pose heatmap according to joint pairs.\n\n    Parameters\n    ----------\n    heatmap : numpy.ndarray\n        Heatmap of joints.\n    joint_pairs : list\n        List of joint pairs.\n    shift : bool\n        Whether to shift the output.\n\n    Returns\n    -------\n    numpy.ndarray\n        Flipped heatmap.\n\n    """"""\n    assert (heatmap.dim() == 3 or heatmap.dim() == 4)\n    out = flip(heatmap)\n\n    for pair in joint_pairs:\n        dim0, dim1 = pair\n        idx = torch.Tensor((dim0, dim1)).long()\n        inv_idx = torch.Tensor((dim1, dim0)).long()\n        if out.dim() == 4:\n            out[:, idx] = out[:, inv_idx]\n        else:\n            out[idx] = out[inv_idx]\n\n    if shift:\n        if out.dim() == 3:\n            out[:, :, 1:] = out[:, :, 0:-1]\n        else:\n            out[:, :, :, 1:] = out[:, :, :, 0:-1]\n    return out\n\n\ndef flip_joints_3d(joints_3d, width, joint_pairs):\n    """"""Flip 3d joints.\n\n    Parameters\n    ----------\n    joints_3d : numpy.ndarray\n        Joints in shape (num_joints, 3, 2)\n    width : int\n        Image width.\n    joint_pairs : list\n        List of joint pairs.\n\n    Returns\n    -------\n    numpy.ndarray\n        Flipped 3d joints with shape (num_joints, 3, 2)\n\n    """"""\n    joints = joints_3d.copy()\n    # flip horizontally\n    joints[:, 0, 0] = width - joints[:, 0, 0] - 1\n    # change left-right parts\n    for pair in joint_pairs:\n        joints[pair[0], :, 0], joints[pair[1], :, 0] = \\\n            joints[pair[1], :, 0], joints[pair[0], :, 0].copy()\n        joints[pair[0], :, 1], joints[pair[1], :, 1] = \\\n            joints[pair[1], :, 1], joints[pair[0], :, 1].copy()\n\n    joints[:, :, 0] *= joints[:, :, 1]\n    return joints\n\n\ndef heatmap_to_coord_simple(hms, bbox):\n    coords, maxvals = get_max_pred(hms)\n\n    hm_h = hms.shape[1]\n    hm_w = hms.shape[2]\n\n    # post-processing\n    for p in range(coords.shape[0]):\n        hm = hms[p]\n        px = int(round(float(coords[p][0])))\n        py = int(round(float(coords[p][1])))\n        if 1 < px < hm_w - 1 and 1 < py < hm_h - 1:\n            diff = np.array((hm[py][px + 1] - hm[py][px - 1],\n                             hm[py + 1][px] - hm[py - 1][px]))\n            coords[p] += np.sign(diff) * .25\n\n    preds = np.zeros_like(coords)\n\n    # transform bbox to scale\n    xmin, ymin, xmax, ymax = bbox\n    w = xmax - xmin\n    h = ymax - ymin\n    center = np.array([xmin + w * 0.5, ymin + h * 0.5])\n    scale = np.array([w, h])\n    # Transform back\n    for i in range(coords.shape[0]):\n        preds[i] = transform_preds(coords[i], center, scale,\n                                   [hm_w, hm_h])\n\n    return preds, maxvals\n\n\ndef transform_preds(coords, center, scale, output_size):\n    target_coords = np.zeros(coords.shape)\n    trans = get_affine_transform(center, scale, 0, output_size, inv=1)\n    target_coords[0:2] = affine_transform(coords[0:2], trans)\n    return target_coords\n\n\ndef get_max_pred(heatmaps):\n    num_joints = heatmaps.shape[0]\n    width = heatmaps.shape[2]\n    heatmaps_reshaped = heatmaps.reshape((num_joints, -1))\n    idx = np.argmax(heatmaps_reshaped, 1)\n    maxvals = np.max(heatmaps_reshaped, 1)\n\n    maxvals = maxvals.reshape((num_joints, 1))\n    idx = idx.reshape((num_joints, 1))\n\n    preds = np.tile(idx, (1, 2)).astype(np.float32)\n\n    preds[:, 0] = (preds[:, 0]) % width\n    preds[:, 1] = np.floor((preds[:, 1]) / width)\n\n    pred_mask = np.tile(np.greater(maxvals, 0.0), (1, 2))\n    pred_mask = pred_mask.astype(np.float32)\n\n    preds *= pred_mask\n    return preds, maxvals\n\n\ndef get_max_pred_batch(batch_heatmaps):\n    batch_size = batch_heatmaps.shape[0]\n    num_joints = batch_heatmaps.shape[1]\n    width = batch_heatmaps.shape[3]\n    heatmaps_reshaped = batch_heatmaps.reshape((batch_size, num_joints, -1))\n    idx = np.argmax(heatmaps_reshaped, 2)\n    maxvals = np.max(heatmaps_reshaped, 2)\n\n    maxvals = maxvals.reshape((batch_size, num_joints, 1))\n    idx = idx.reshape((batch_size, num_joints, 1))\n\n    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\n\n    preds[:, :, 0] = (preds[:, :, 0]) % width\n    preds[:, :, 1] = np.floor((preds[:, :, 1]) / width)\n\n    pred_mask = np.tile(np.greater(maxvals, 0.0), (1, 1, 2))\n    pred_mask = pred_mask.astype(np.float32)\n\n    preds *= pred_mask\n    return preds, maxvals\n\n\ndef get_affine_transform(center,\n                         scale,\n                         rot,\n                         output_size,\n                         shift=np.array([0, 0], dtype=np.float32),\n                         inv=0):\n    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\n        scale = np.array([scale, scale])\n\n    scale_tmp = scale\n    src_w = scale_tmp[0]\n    dst_w = output_size[0]\n    dst_h = output_size[1]\n\n    rot_rad = np.pi * rot / 180\n    src_dir = get_dir([0, src_w * -0.5], rot_rad)\n    dst_dir = np.array([0, dst_w * -0.5], np.float32)\n\n    src = np.zeros((3, 2), dtype=np.float32)\n    dst = np.zeros((3, 2), dtype=np.float32)\n    src[0, :] = center + scale_tmp * shift\n    src[1, :] = center + src_dir + scale_tmp * shift\n    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]\n    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir\n\n    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\n    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\n\n    if inv:\n        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n    else:\n        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n\n    return trans\n\n\ndef affine_transform(pt, t):\n    new_pt = np.array([pt[0], pt[1], 1.]).T\n    new_pt = np.dot(t, new_pt)\n    return new_pt[:2]\n\n\ndef get_func_heatmap_to_coord(cfg):\n    if cfg.DATA_PRESET.TYPE == \'simple\':\n        return heatmap_to_coord_simple\n    else:\n        raise NotImplementedError\n'"
alphapose/utils/vis.py,6,"b""import math\nimport time\n\nimport cv2\nimport numpy as np\nimport torch\n\nRED = (0, 0, 255)\nGREEN = (0, 255, 0)\nBLUE = (255, 0, 0)\nCYAN = (255, 255, 0)\nYELLOW = (0, 255, 255)\nORANGE = (0, 165, 255)\nPURPLE = (255, 0, 255)\nWHITE = (255, 255, 255)\nBLACK = (0, 0, 0)\n\nDEFAULT_FONT = cv2.FONT_HERSHEY_SIMPLEX\n\n\ndef get_color(idx):\n    idx = idx * 3\n    color = ((37 * idx) % 255, (17 * idx) % 255, (29 * idx) % 255)\n\n    return color\n\n\ndef get_color_fast(idx):\n    color_pool = [RED, GREEN, BLUE, CYAN, YELLOW, ORANGE, PURPLE, WHITE]\n    color = color_pool[idx % 8]\n\n    return color\n\n\ndef vis_frame_dense(frame, im_res, add_bbox=False, format='coco'):\n    '''\n    frame: frame image\n    im_res: im_res of predictions\n    format: coco or mpii\n\n    return rendered image\n    '''\n    l_pair = [\n        (0, 1), (0, 2), (1, 3), (2, 4),  # Head\n        (5, 19), (19, 21), (21, 17), (17, 22), (22, 20), (20, 6), # Shoulder\n        (6, 28), (28, 30), (30, 8), (8, 32), (32, 34), (34, 10),  # right arm\n        (5, 27), (27, 29), (29, 7), (7, 31), (31, 33), (33, 9),  #left arm\n        (17, 47), (47, 48), (48, 18), #spine\n        (20, 24), (24, 26), (26, 12), #right spine\n        (19, 23), (23, 25), (25, 11), #left spine\n        (12, 36), (36, 38), (38, 18), (18, 37), (37, 35), (35, 11),#row\n        (12, 40), (40, 42), (42, 14), (14, 44), (44, 46), (46, 16),#right leg\n        (11, 39), (39, 41), (41, 13), (13, 43), (43, 45), (45, 15)#left leg\n    ]\n    p_color = [(0, 255, 255), (0, 191, 255), (0, 255, 102), (0, 77, 255), (0, 255, 0),  # Nose, LEye, REye, LEar, REar\n               (77, 255, 255), (77, 255, 204), (77, 204, 255), (191, 255, 77), (77, 191, 255), (191, 255, 77),  # LShoulder, RShoulder, LElbow, RElbow, LWrist, RWrist\n               (204, 77, 255), (77, 255, 204), (191, 77, 255), (77, 255, 191), (127, 77, 255), (77, 255, 127), (0, 255, 255),  # LHip, RHip, LKnee, Rknee, LAnkle, RAnkle, Neck\n               (0, 255, 255), (0, 191, 255), (0, 255, 102), (0, 77, 255), (0, 255, 0),  # unkonw\n               (77, 255, 255), (77, 255, 204), (77, 204, 255), (191, 255, 77), (77, 191, 255), (191, 255, 77),  # unknown\n               (204, 77, 255), (77, 255, 204), (191, 77, 255), (77, 255, 191), (127, 77, 255), (77, 255, 127), (0, 255, 255), #unknown\n               (0, 255, 255), (0, 191, 255), (0, 255, 102), (0, 77, 255), (0, 255, 0),  # unkonw\n               (77, 255, 255), (77, 255, 204), (77, 204, 255), (191, 255, 77), (77, 191, 255), (191, 255, 77),  # unknown\n               (204, 77, 255), (77, 255, 204), (191, 77, 255), (77, 255, 191), (127, 77, 255), (77, 255, 127), (0, 255, 255)] #unknown\n\n    line_color = [(0, 215, 255), (0, 255, 204), (0, 134, 255), (0, 255, 50),\n                  (77, 255, 222), (77, 255, 222), (77, 255, 222), (77, 196, 255), (77, 196, 255), (77, 196, 255),  # Shoulder\n                  (77, 135, 255), (77, 135, 255), (77, 135, 255), (191, 255, 77), (191, 255, 77), (191, 255, 77), \n                  (77, 255, 77), (77, 255, 77), (77, 255, 77), (77, 222, 255), (77, 222, 255), (77, 222, 255),\n                  (255, 156, 127), (255, 156, 127), (255, 156, 127),\n                  (255, 156, 127), (255, 156, 127), (255, 156, 127),\n                  (255, 156, 127), (255, 156, 127), (255, 156, 127),\n                  (255, 156, 127), (255, 156, 127), (255, 156, 127), (255, 156, 127), (255, 156, 127), (255, 156, 127),\n                  (0, 127, 255), (0, 127, 255), (0, 127, 255), (255, 127, 77), (255, 127, 77), (255, 127, 77), \n                  (0, 77, 255), (0, 77, 255), (0, 77, 255), (255, 77, 36), (255, 77, 36),  (255, 77, 36)]\n\n\n    # im_name = os.path.basename(im_res['imgname'])\n    img = frame.copy()\n    height, width = img.shape[:2]\n    for human in im_res['result']:\n        part_line = {}\n        kp_preds = human['keypoints']\n        kp_scores = human['kp_score']\n        kp_preds = torch.cat((kp_preds, torch.unsqueeze((kp_preds[5, :] + kp_preds[6, :]) / 2, 0)))\n        kp_scores = torch.cat((kp_scores, torch.unsqueeze((kp_scores[5, :] + kp_scores[6, :]) / 2, 0)))\n        # Draw bboxes\n        if add_bbox:\n            from PoseFlow.poseflow_infer import get_box\n            keypoints = []\n            for n in range(kp_scores.shape[0]):\n                keypoints.append(float(kp_preds[n, 0]))\n                keypoints.append(float(kp_preds[n, 1]))\n                keypoints.append(float(kp_scores[n]))\n            bbox = get_box(keypoints, height, width)\n            # color = get_color_fast(int(abs(human['idx'][0])))\n            cv2.rectangle(img, (int(bbox[0]), int(bbox[2])), (int(bbox[1]), int(bbox[3])), BLUE, 2)\n            # Draw indexes of humans\n            if 'idx' in human.keys():\n                cv2.putText(img, ''.join(str(human['idx'])), (int(bbox[0]), int((bbox[2] + 26))), DEFAULT_FONT, 1, BLACK, 2)\n        # Draw keypoints\n        for n in range(kp_scores.shape[0]):\n            if kp_scores[n] <= 0.35:\n                continue\n            cor_x, cor_y = int(kp_preds[n, 0]), int(kp_preds[n, 1])\n            part_line[n] = (cor_x, cor_y)\n            cv2.circle(img, (cor_x, cor_y), 3, p_color[n], -1)\n        # Draw limbs\n        for i, (start_p, end_p) in enumerate(l_pair):\n            if start_p in part_line and end_p in part_line:\n                start_xy = part_line[start_p]\n                end_xy = part_line[end_p]\n                cv2.line(img, start_xy, end_xy, line_color[i], (kp_scores[start_p] + kp_scores[end_p]) + 1)\n    return img\n\n\ndef vis_frame_fast(frame, im_res, add_bbox=False, format='coco'):\n    '''\n    frame: frame image\n    im_res: im_res of predictions\n    format: coco or mpii\n\n    return rendered image\n    '''\n    if format == 'coco':\n        l_pair = [\n            (0, 1), (0, 2), (1, 3), (2, 4),  # Head\n            (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),\n            (17, 11), (17, 12),  # Body\n            (11, 13), (12, 14), (13, 15), (14, 16)\n        ]\n        p_color = [(0, 255, 255), (0, 191, 255), (0, 255, 102), (0, 77, 255), (0, 255, 0),  # Nose, LEye, REye, LEar, REar\n                   (77, 255, 255), (77, 255, 204), (77, 204, 255), (191, 255, 77), (77, 191, 255), (191, 255, 77),  # LShoulder, RShoulder, LElbow, RElbow, LWrist, RWrist\n                   (204, 77, 255), (77, 255, 204), (191, 77, 255), (77, 255, 191), (127, 77, 255), (77, 255, 127), (0, 255, 255)]  # LHip, RHip, LKnee, Rknee, LAnkle, RAnkle, Neck\n        line_color = [(0, 215, 255), (0, 255, 204), (0, 134, 255), (0, 255, 50),\n                      (77, 255, 222), (77, 196, 255), (77, 135, 255), (191, 255, 77), (77, 255, 77),\n                      (77, 222, 255), (255, 156, 127),\n                      (0, 127, 255), (255, 127, 77), (0, 77, 255), (255, 77, 36)]\n    elif format == 'mpii':\n        l_pair = [\n            (8, 9), (11, 12), (11, 10), (2, 1), (1, 0),\n            (13, 14), (14, 15), (3, 4), (4, 5),\n            (8, 7), (7, 6), (6, 2), (6, 3), (8, 12), (8, 13)\n        ]\n        p_color = [PURPLE, BLUE, BLUE, RED, RED, BLUE, BLUE, RED, RED, PURPLE, PURPLE, PURPLE, RED, RED, BLUE, BLUE]\n    else:\n        NotImplementedError\n\n    # im_name = os.path.basename(im_res['imgname'])\n    img = frame.copy()\n    height, width = img.shape[:2]\n    for human in im_res['result']:\n        part_line = {}\n        kp_preds = human['keypoints']\n        kp_scores = human['kp_score']\n        kp_preds = torch.cat((kp_preds, torch.unsqueeze((kp_preds[5, :] + kp_preds[6, :]) / 2, 0)))\n        kp_scores = torch.cat((kp_scores, torch.unsqueeze((kp_scores[5, :] + kp_scores[6, :]) / 2, 0)))\n        # Draw bboxes\n        if add_bbox:\n            if 'box' in human.keys():\n                bbox = human['box']\n            else:\n                from PoseFlow.poseflow_infer import get_box\n                keypoints = []\n                for n in range(kp_scores.shape[0]):\n                    keypoints.append(float(kp_preds[n, 0]))\n                    keypoints.append(float(kp_preds[n, 1]))\n                    keypoints.append(float(kp_scores[n]))\n                bbox = get_box(keypoints, height, width)\n            # color = get_color_fast(int(abs(human['idx'][0])))\n            cv2.rectangle(img, (int(bbox[0]), int(bbox[2])), (int(bbox[1]), int(bbox[3])), BLUE, 2)\n            # Draw indexes of humans\n            if 'idx' in human.keys():\n                cv2.putText(img, ''.join(str(human['idx'])), (int(bbox[0]), int((bbox[2] + 26))), DEFAULT_FONT, 1, BLACK, 2)\n        # Draw keypoints\n        for n in range(kp_scores.shape[0]):\n            if kp_scores[n] <= 0.35:\n                continue\n            cor_x, cor_y = int(kp_preds[n, 0]), int(kp_preds[n, 1])\n            part_line[n] = (cor_x, cor_y)\n            cv2.circle(img, (cor_x, cor_y), 3, p_color[n], -1)\n        # Draw limbs\n        for i, (start_p, end_p) in enumerate(l_pair):\n            if start_p in part_line and end_p in part_line:\n                start_xy = part_line[start_p]\n                end_xy = part_line[end_p]\n                cv2.line(img, start_xy, end_xy, line_color[i], 2 * (kp_scores[start_p] + kp_scores[end_p]) + 1)\n    return img\n\n\ndef vis_frame(frame, im_res, add_bbox=False, format='coco'):\n    '''\n    frame: frame image\n    im_res: im_res of predictions\n    format: coco or mpii\n\n    return rendered image\n    '''\n    if format == 'coco':\n        l_pair = [\n            (0, 1), (0, 2), (1, 3), (2, 4),  # Head\n            (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),\n            (17, 11), (17, 12),  # Body\n            (11, 13), (12, 14), (13, 15), (14, 16)\n        ]\n\n        p_color = [(0, 255, 255), (0, 191, 255), (0, 255, 102), (0, 77, 255), (0, 255, 0),  # Nose, LEye, REye, LEar, REar\n                   (77, 255, 255), (77, 255, 204), (77, 204, 255), (191, 255, 77), (77, 191, 255), (191, 255, 77),  # LShoulder, RShoulder, LElbow, RElbow, LWrist, RWrist\n                   (204, 77, 255), (77, 255, 204), (191, 77, 255), (77, 255, 191), (127, 77, 255), (77, 255, 127), (0, 255, 255)]  # LHip, RHip, LKnee, Rknee, LAnkle, RAnkle, Neck\n        line_color = [(0, 215, 255), (0, 255, 204), (0, 134, 255), (0, 255, 50),\n                      (77, 255, 222), (77, 196, 255), (77, 135, 255), (191, 255, 77), (77, 255, 77),\n                      (77, 222, 255), (255, 156, 127),\n                      (0, 127, 255), (255, 127, 77), (0, 77, 255), (255, 77, 36)]\n    elif format == 'mpii':\n        l_pair = [\n            (8, 9), (11, 12), (11, 10), (2, 1), (1, 0),\n            (13, 14), (14, 15), (3, 4), (4, 5),\n            (8, 7), (7, 6), (6, 2), (6, 3), (8, 12), (8, 13)\n        ]\n        p_color = [PURPLE, BLUE, BLUE, RED, RED, BLUE, BLUE, RED, RED, PURPLE, PURPLE, PURPLE, RED, RED, BLUE, BLUE]\n        line_color = [PURPLE, BLUE, BLUE, RED, RED, BLUE, BLUE, RED, RED, PURPLE, PURPLE, RED, RED, BLUE, BLUE]\n    else:\n        raise NotImplementedError\n\n    # im_name = os.path.basename(im_res['imgname'])\n    img = frame.copy()\n    height, width = img.shape[:2]\n    img = cv2.resize(img, (int(width / 2), int(height / 2)))\n    for human in im_res['result']:\n        part_line = {}\n        kp_preds = human['keypoints']\n        kp_scores = human['kp_score']\n        kp_preds = torch.cat((kp_preds, torch.unsqueeze((kp_preds[5, :] + kp_preds[6, :]) / 2, 0)))\n        kp_scores = torch.cat((kp_scores, torch.unsqueeze((kp_scores[5, :] + kp_scores[6, :]) / 2, 0)))\n        # Draw bboxes\n        if add_bbox:\n            if 'box' in human.keys():\n                bbox = human['box']\n                bbox = [bbox[0], bbox[0]+bbox[2], bbox[1], bbox[1]+bbox[3]]#xmin,xmax,ymin,ymax\n            else:\n                from PoseFlow.poseflow_infer import get_box\n                keypoints = []\n                for n in range(kp_scores.shape[0]):\n                    keypoints.append(float(kp_preds[n, 0]))\n                    keypoints.append(float(kp_preds[n, 1]))\n                    keypoints.append(float(kp_scores[n]))\n                bbox = get_box(keypoints, height, width)\n            bg = img.copy()\n            # color = get_color_fast(int(abs(human['idx'][0][0])))\n            cv2.rectangle(bg, (int(bbox[0]/2), int(bbox[2]/2)), (int(bbox[1]/2),int(bbox[3]/2)), BLUE, 1)\n            transparency = 0.8\n            img = cv2.addWeighted(bg, transparency, img, 1 - transparency, 0)\n            # Draw indexes of humans\n            if 'idx' in human.keys():\n                bg = img.copy()\n                cv2.putText(bg, ''.join(str(human['idx'])), (int(bbox[0] / 2), int((bbox[2] + 26) / 2)), DEFAULT_FONT, 0.5, BLACK, 1)\n                transparency = 0.8\n                img = cv2.addWeighted(bg, transparency, img, 1 - transparency, 0)\n        # Draw keypoints\n        for n in range(kp_scores.shape[0]):\n            if kp_scores[n] <= 0.35:\n                continue\n            cor_x, cor_y = int(kp_preds[n, 0]), int(kp_preds[n, 1])\n            part_line[n] = (int(cor_x / 2), int(cor_y / 2))\n            bg = img.copy()\n            cv2.circle(bg, (int(cor_x / 2), int(cor_y / 2)), 2, p_color[n], -1)\n            # Now create a mask of logo and create its inverse mask also\n            transparency = max(0, min(1, kp_scores[n]))\n            img = cv2.addWeighted(bg, transparency, img, 1 - transparency, 0)\n        # Draw limbs\n        for i, (start_p, end_p) in enumerate(l_pair):\n            if start_p in part_line and end_p in part_line:\n                start_xy = part_line[start_p]\n                end_xy = part_line[end_p]\n                bg = img.copy()\n\n                X = (start_xy[0], end_xy[0])\n                Y = (start_xy[1], end_xy[1])\n                mX = np.mean(X)\n                mY = np.mean(Y)\n                length = ((Y[0] - Y[1]) ** 2 + (X[0] - X[1]) ** 2) ** 0.5\n                angle = math.degrees(math.atan2(Y[0] - Y[1], X[0] - X[1]))\n                stickwidth = (kp_scores[start_p] + kp_scores[end_p]) + 1\n                polygon = cv2.ellipse2Poly((int(mX), int(mY)), (int(length / 2), stickwidth), int(angle), 0, 360, 1)\n                cv2.fillConvexPoly(bg, polygon, line_color[i])\n                # cv2.line(bg, start_xy, end_xy, line_color[i], (2 * (kp_scores[start_p] + kp_scores[end_p])) + 1)\n                transparency = max(0, min(1, 0.5 * (kp_scores[start_p] + kp_scores[end_p])))\n                img = cv2.addWeighted(bg, transparency, img, 1 - transparency, 0)\n    img = cv2.resize(img, (width, height), interpolation=cv2.INTER_CUBIC)\n    return img\n\n\ndef getTime(time1=0):\n    if not time1:\n        return time.time()\n    else:\n        interval = time.time() - time1\n        return time.time(), interval\n"""
alphapose/utils/webcam_detector.py,11,"b'from itertools import count\nfrom threading import Thread\nfrom queue import Queue\n\nimport cv2\nimport numpy as np\n\nimport torch\nimport torch.multiprocessing as mp\n\nfrom alphapose.utils.presets import SimpleTransform\n\n\nclass WebCamDetectionLoader():\n    def __init__(self, input_source, detector, cfg, opt, queueSize=1):\n        self.cfg = cfg\n        self.opt = opt\n\n        stream = cv2.VideoCapture(int(input_source))\n        assert stream.isOpened(), \'Cannot capture source\'\n        self.path = input_source\n        self.fourcc = int(stream.get(cv2.CAP_PROP_FOURCC))\n        self.fps = stream.get(cv2.CAP_PROP_FPS)\n        self.frameSize = (int(stream.get(cv2.CAP_PROP_FRAME_WIDTH)), int(stream.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n        self.videoinfo = {\'fourcc\': self.fourcc, \'fps\': self.fps, \'frameSize\': self.frameSize}\n        stream.release()\n\n        self.detector = detector\n\n        self._input_size = cfg.DATA_PRESET.IMAGE_SIZE\n        self._output_size = cfg.DATA_PRESET.HEATMAP_SIZE\n\n        self._sigma = cfg.DATA_PRESET.SIGMA\n\n        if cfg.DATA_PRESET.TYPE == \'simple\':\n            self.transformation = SimpleTransform(\n                self, scale_factor=0,\n                input_size=self._input_size,\n                output_size=self._output_size,\n                rot=0, sigma=self._sigma,\n                train=False, add_dpg=False)\n\n        # initialize the queue used to store data\n        """"""\n        pose_queue: the buffer storing post-processed cropped human image for pose estimation\n        """"""\n        if opt.sp:\n            self._stopped = False\n            self.pose_queue = Queue(maxsize=queueSize)\n        else:\n            self._stopped = mp.Value(\'b\', False)\n            self.pose_queue = mp.Queue(maxsize=queueSize)\n\n    def start_worker(self, target):\n        if self.opt.sp:\n            p = Thread(target=target, args=())\n        else:\n            p = mp.Process(target=target, args=())\n        # p.daemon = True\n        p.start()\n        return p\n\n    def start(self):\n        # start a thread to pre process images for object detection\n        image_preprocess_worker = self.start_worker(self.frame_preprocess)\n        return [image_preprocess_worker]\n\n    def stop(self):\n        # clear queues\n        self.clear_queues()\n\n    def terminate(self):\n        if self.opt.sp:\n            self._stopped = True\n        else:\n            self._stopped.value = True\n        self.stop()\n\n    def clear_queues(self):\n        self.clear(self.pose_queue)\n\n    def clear(self, queue):\n        while not queue.empty():\n            queue.get()\n\n    def wait_and_put(self, queue, item):\n        if not self.stopped:\n            queue.put(item)\n\n    def wait_and_get(self, queue):\n        if not self.stopped:\n            return queue.get()\n\n    def frame_preprocess(self):\n        stream = cv2.VideoCapture(self.path)\n        assert stream.isOpened(), \'Cannot capture source\'\n\n        # keep looping infinitely\n        for i in count():\n            if self.stopped:\n                stream.release()\n                return\n            if not self.pose_queue.full():\n                # otherwise, ensure the queue has room in it\n                (grabbed, frame) = stream.read()\n                # if the `grabbed` boolean is `False`, then we have\n                # reached the end of the video file\n                if not grabbed:\n                    self.wait_and_put(self.pose_queue, (None, None, None, None, None, None, None))\n                    stream.release()\n                    return\n\n                # expected frame shape like (1,3,h,w) or (3,h,w)\n                img_k = self.detector.image_preprocess(frame)\n\n                if isinstance(img_k, np.ndarray):\n                    img_k = torch.from_numpy(img_k)\n                # add one dimension at the front for batch if image shape (3,h,w)\n                if img_k.dim() == 3:\n                    img_k = img_k.unsqueeze(0)\n\n                im_dim_list_k = frame.shape[1], frame.shape[0]\n\n                orig_img = frame[:, :, ::-1]\n                im_name = str(i) + \'.jpg\'\n                # im_dim_list = im_dim_list_k\n\n                with torch.no_grad():\n                    # Record original image resolution\n                    im_dim_list_k = torch.FloatTensor(im_dim_list_k).repeat(1, 2)\n                img_det = self.image_detection((img_k, orig_img, im_name, im_dim_list_k))\n                self.image_postprocess(img_det)\n\n    def image_detection(self, inputs):\n        img, orig_img, im_name, im_dim_list = inputs\n        if img is None or self.stopped:\n            return (None, None, None, None, None, None, None)\n\n        with torch.no_grad():\n            dets = self.detector.images_detection(img, im_dim_list)\n            if isinstance(dets, int) or dets.shape[0] == 0:\n                return (orig_img, im_name, None, None, None, None, None)\n            if isinstance(dets, np.ndarray):\n                dets = torch.from_numpy(dets)\n            dets = dets.cpu()\n            boxes = dets[:, 1:5]\n            scores = dets[:, 5:6]\n            if self.opt.tracking:\n                ids = dets[:, 6:7]\n            else:\n                ids = torch.zeros(scores.shape)\n\n        boxes_k = boxes[dets[:, 0] == 0]\n        if isinstance(boxes_k, int) or boxes_k.shape[0] == 0:\n            return (orig_img, im_name, None, None, None, None, None)\n        inps = torch.zeros(boxes_k.size(0), 3, *self._input_size)\n        cropped_boxes = torch.zeros(boxes_k.size(0), 4)\n        return (orig_img, im_name, boxes_k, scores[dets[:, 0] == 0], ids[dets[:, 0] == 0], inps, cropped_boxes)\n\n    def image_postprocess(self, inputs):\n        with torch.no_grad():\n            (orig_img, im_name, boxes, scores, ids, inps, cropped_boxes) = inputs\n            if orig_img is None or self.stopped:\n                self.wait_and_put(self.pose_queue, (None, None, None, None, None, None, None))\n                return\n            if boxes is None or boxes.nelement() == 0:\n                self.wait_and_put(self.pose_queue, (None, orig_img, im_name, boxes, scores, ids, None))\n                return\n            # imght = orig_img.shape[0]\n            # imgwidth = orig_img.shape[1]\n            for i, box in enumerate(boxes):\n                inps[i], cropped_box = self.transformation.test_transform(orig_img, box)\n                cropped_boxes[i] = torch.FloatTensor(cropped_box)\n\n            # inps, cropped_boxes = self.transformation.align_transform(orig_img, boxes)\n\n            self.wait_and_put(self.pose_queue, (inps, orig_img, im_name, boxes, scores, ids, cropped_boxes))\n\n    def read(self):\n        return self.wait_and_get(self.pose_queue)\n\n    @property\n    def stopped(self):\n        if self.opt.sp:\n            return self._stopped\n        else:\n            return self._stopped.value\n\n    @property\n    def joint_pairs(self):\n        """"""Joint pairs which defines the pairs of joint to be swapped\n        when the image is flipped horizontally.""""""\n        return [[1, 2], [3, 4], [5, 6], [7, 8],\n                [9, 10], [11, 12], [13, 14], [15, 16]]\n'"
alphapose/utils/writer.py,5,"b'import os\nimport time\nfrom threading import Thread\nfrom queue import Queue\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.multiprocessing as mp\n\nfrom alphapose.utils.transforms import get_func_heatmap_to_coord\nfrom alphapose.utils.pPose_nms import pose_nms\n\nDEFAULT_VIDEO_SAVE_OPT = {\n    \'savepath\': \'examples/res/1.mp4\',\n    \'fourcc\': cv2.VideoWriter_fourcc(*\'mp4v\'),\n    \'fps\': 25,\n    \'frameSize\': (640, 480)\n}\n\nEVAL_JOINTS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n\n\nclass DataWriter():\n    def __init__(self, cfg, opt, save_video=False,\n                 video_save_opt=DEFAULT_VIDEO_SAVE_OPT,\n                 queueSize=1024):\n        self.cfg = cfg\n        self.opt = opt\n        self.video_save_opt = video_save_opt\n\n        self.eval_joints = EVAL_JOINTS\n        self.save_video = save_video\n        self.final_result = []\n        self.heatmap_to_coord = get_func_heatmap_to_coord(cfg)\n        # initialize the queue used to store frames read from\n        # the video file\n        if opt.sp:\n            self.result_queue = Queue(maxsize=queueSize)\n            self.final_result_queue = Queue(maxsize=queueSize)\n        else:\n            self.result_queue = mp.Queue(maxsize=queueSize)\n            self.final_result_queue = mp.Queue(maxsize=queueSize)\n\n        if opt.save_img:\n            if not os.path.exists(opt.outputpath + \'/vis\'):\n                os.mkdir(opt.outputpath + \'/vis\')\n\n        if opt.pose_track:\n            from PoseFlow.poseflow_infer import PoseFlowWrapper\n            self.pose_flow_wrapper = PoseFlowWrapper(save_path=os.path.join(opt.outputpath, \'poseflow\'))\n\n    def start_worker(self, target):\n        if self.opt.sp:\n            p = Thread(target=target, args=())\n        else:\n            p = mp.Process(target=target, args=())\n        # p.daemon = True\n        p.start()\n        return p\n\n    def start(self):\n        # start a thread to read pose estimation results per frame\n        self.result_worker = self.start_worker(self.update)\n        return self\n\n    def update(self):\n        if self.save_video:\n            # initialize the file video stream, adapt ouput video resolution to original video\n            stream = cv2.VideoWriter(*[self.video_save_opt[k] for k in [\'savepath\', \'fourcc\', \'fps\', \'frameSize\']])\n            if not stream.isOpened():\n                print(""Try to use other video encoders..."")\n                ext = self.video_save_opt[\'savepath\'].split(\'.\')[-1]\n                fourcc, _ext = self.recognize_video_ext(ext)\n                self.video_save_opt[\'fourcc\'] = fourcc\n                self.video_save_opt[\'savepath\'] = self.video_save_opt[\'savepath\'][:-4] + _ext\n                stream = cv2.VideoWriter(*[self.video_save_opt[k] for k in [\'savepath\', \'fourcc\', \'fps\', \'frameSize\']])\n            assert stream.isOpened(), \'Cannot open video for writing\'\n        # keep looping infinitelyd\n        while True:\n            # ensure the queue is not empty and get item\n            (boxes, scores, ids, hm_data, cropped_boxes, orig_img, im_name) = self.wait_and_get(self.result_queue)\n            if orig_img is None:\n                # if the thread indicator variable is set (img is None), stop the thread\n                self.wait_and_put(self.final_result_queue, None)\n                if self.save_video:\n                    stream.release()\n                return\n            # image channel RGB->BGR\n            orig_img = np.array(orig_img, dtype=np.uint8)[:, :, ::-1]\n            if boxes is None:\n                if self.opt.save_img or self.save_video or self.opt.vis:\n                    self.write_image(orig_img, im_name, stream=stream if self.save_video else None)\n            else:\n                # location prediction (n, kp, 2) | score prediction (n, kp, 1)\n                pred = hm_data.cpu().data.numpy()\n                assert pred.ndim == 4\n\n                if hm_data.size()[1] == 49:\n                    self.eval_joints = [*range(0,49)]\n                pose_coords = []\n                pose_scores = []\n                for i in range(hm_data.shape[0]):\n                    bbox = cropped_boxes[i].tolist()\n                    pose_coord, pose_score = self.heatmap_to_coord(pred[i][self.eval_joints], bbox)\n                    pose_coords.append(torch.from_numpy(pose_coord).unsqueeze(0))\n                    pose_scores.append(torch.from_numpy(pose_score).unsqueeze(0))\n                preds_img = torch.cat(pose_coords)\n                preds_scores = torch.cat(pose_scores)\n                result = pose_nms(boxes, scores, ids, preds_img, preds_scores, self.opt.min_box_area)\n                result = {\n                    \'imgname\': im_name,\n                    \'result\': result\n                }\n                if self.opt.pose_track:\n                    poseflow_result = self.pose_flow_wrapper.step(orig_img, result)\n                    for i in range(len(poseflow_result)):\n                        result[\'result\'][i][\'idx\'] = poseflow_result[i][\'idx\']\n                self.wait_and_put(self.final_result_queue, result)\n                if self.opt.save_img or self.save_video or self.opt.vis:\n                    if hm_data.size()[1] == 49:\n                        from alphapose.utils.vis import vis_frame_dense as vis_frame\n                    elif self.opt.vis_fast:\n                        from alphapose.utils.vis import vis_frame_fast as vis_frame\n                    else:\n                        from alphapose.utils.vis import vis_frame\n                    img = vis_frame(orig_img, result, add_bbox=(self.opt.pose_track | self.opt.tracking | self.opt.showbox))\n                    self.write_image(img, im_name, stream=stream if self.save_video else None)\n\n    def write_image(self, img, im_name, stream=None):\n        if self.opt.vis:\n            cv2.imshow(""AlphaPose Demo"", img)\n            cv2.waitKey(30)\n        if self.opt.save_img:\n            cv2.imwrite(os.path.join(self.opt.outputpath, \'vis\', im_name), img)\n        if self.save_video:\n            stream.write(img)\n\n    def wait_and_put(self, queue, item):\n        queue.put(item)\n\n    def wait_and_get(self, queue):\n        return queue.get()\n\n    def save(self, boxes, scores, ids, hm_data, cropped_boxes, orig_img, im_name):\n        self.commit()\n        # save next frame in the queue\n        self.wait_and_put(self.result_queue, (boxes, scores, ids, hm_data, cropped_boxes, orig_img, im_name))\n\n    def running(self):\n        # indicate that the thread is still running\n        time.sleep(0.2)\n        self.commit()\n        return not self.result_queue.empty()\n\n    def count(self):\n        # indicate the remaining images\n        return self.result_queue.qsize()\n\n    def stop(self):\n        # indicate that the thread should be stopped\n        self.save(None, None, None, None, None, None, None)\n        while True:\n            final_res = self.wait_and_get(self.final_result_queue)\n            if final_res:\n                self.final_result.append(final_res)\n            else:\n                break\n        self.result_worker.join()\n\n    def clear_queues(self):\n        self.clear(self.result_queue)\n        self.clear(self.final_result_queue)\n        \n    def clear(self, queue):\n        while not queue.empty():\n            queue.get()\n\n    def commit(self):\n        # commit finished final results to main process\n        while not self.final_result_queue.empty():\n            self.final_result.append(self.wait_and_get(self.final_result_queue))\n\n    def results(self):\n        # return final result\n        return self.final_result\n\n    def recognize_video_ext(self, ext=\'\'):\n        if ext == \'mp4\':\n            return cv2.VideoWriter_fourcc(*\'mp4v\'), \'.\' + ext\n        elif ext == \'avi\':\n            return cv2.VideoWriter_fourcc(*\'XVID\'), \'.\' + ext\n        elif ext == \'mov\':\n            return cv2.VideoWriter_fourcc(*\'XVID\'), \'.\' + ext\n        else:\n            print(""Unknow video format {}, will use .mp4 instead of it"".format(ext))\n            return cv2.VideoWriter_fourcc(*\'mp4v\'), \'.mp4\'\n\n'"
detector/efficientdet/utils.py,13,"b'import os\nimport cv2\nimport numpy as np\nimport torch\n\nclass AverageMeter:\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef get_outdir(path, *paths, inc=False):\n    outdir = os.path.join(path, *paths)\n    if not os.path.exists(outdir):\n        os.makedirs(outdir)\n    elif inc:\n        count = 1\n        outdir_inc = outdir + \'-\' + str(count)\n        while os.path.exists(outdir_inc):\n            count = count + 1\n            outdir_inc = outdir + \'-\' + str(count)\n            assert count < 100\n        outdir = outdir_inc\n        os.makedirs(outdir)\n    return outdir\n\ndef unique(tensor):\n    tensor_np = tensor.cpu().numpy()\n    unique_np = np.unique(tensor_np)\n    unique_tensor = torch.from_numpy(unique_np)\n\n    tensor_res = tensor.new(unique_tensor.shape)\n    tensor_res.copy_(unique_tensor)\n    return tensor_res\n\n\ndef letterbox_image(img, inp_dim):\n    \'\'\'resize image with unchanged aspect ratio using padding\'\'\'\n    img_w, img_h = img.shape[1], img.shape[0]\n    w, h = inp_dim\n    new_w = int(img_w * min(w / img_w, h / img_h))\n    new_h = int(img_h * min(w / img_w, h / img_h))\n    resized_image = cv2.resize(img, (new_w, new_h))#default is INTER_LINEAR, interpolation=cv2.INTER_CUBIC)\n\n    canvas = np.full((inp_dim[1], inp_dim[0], 3), 0)\n\n    canvas[0:new_h, 0:new_w, :] = resized_image\n\n    return canvas\n\ndef prep_image(img, inp_dim):\n    """"""\n    Prepare image for inputting to the neural network.\n\n    Returns a Variable\n    """"""\n\n    orig_im = cv2.imread(img)\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n    img_ = img[:, :, ::-1].transpose((2, 0, 1)).copy()\n    img_ = torch.from_numpy(img_).float().unsqueeze(0)\n\n    mean = torch.tensor([x for x in (0.485, 0.456, 0.406)]).float().view(1, 3, 1, 1)\n    std = torch.tensor([x for x in (0.229, 0.224, 0.225)]).float().view(1, 3, 1, 1)\n    img_ = img_.div_(255).sub_(mean).div_(std)\n\n    return img_, orig_im, dim\n\n\ndef prep_frame(img, inp_dim):\n    """"""\n    Prepare image for inputting to the neural network.\n\n    Returns a Variable\n    """"""\n\n    orig_im = img\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n    img_ = img[:, :, ::-1].transpose((2, 0, 1)).copy()\n    img_ = torch.from_numpy(img_).float().unsqueeze(0)\n\n    mean = torch.tensor([x for x in (0.485, 0.456, 0.406)]).float().view(1, 3, 1, 1)\n    std = torch.tensor([x for x in (0.229, 0.224, 0.225)]).float().view(1, 3, 1, 1)\n    img_ = img_.div_(255).sub_(mean).div_(std)\n\n    return img_, orig_im, dim\n\ndef bbox_iou(box1, box2, args=None):\n    """"""\n    Returns the IoU of two bounding boxes \n    \n    \n    """"""\n    #Get the coordinates of bounding boxes\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]\n\n    #get the corrdinates of the intersection rectangle\n    inter_rect_x1 =  torch.max(b1_x1, b2_x1)\n    inter_rect_y1 =  torch.max(b1_y1, b2_y1)\n    inter_rect_x2 =  torch.min(b1_x2, b2_x2)\n    inter_rect_y2 =  torch.min(b1_y2, b2_y2)\n\n    #Intersection area\n    if not args:\n        inter_area = torch.max(inter_rect_x2 - inter_rect_x1 + 1,torch.zeros(inter_rect_x2.shape).cuda())*torch.max(inter_rect_y2 - inter_rect_y1 + 1, torch.zeros(inter_rect_x2.shape).cuda())\n    else:\n        inter_area = torch.max(inter_rect_x2 - inter_rect_x1 + 1,torch.zeros(inter_rect_x2.shape).to(args.device))*torch.max(inter_rect_y2 - inter_rect_y1 + 1, torch.zeros(inter_rect_x2.shape).to(args.device))\n    #Union Area\n    b1_area = (b1_x2 - b1_x1 + 1)*(b1_y2 - b1_y1 + 1)\n    b2_area = (b2_x2 - b2_x1 + 1)*(b2_y2 - b2_y1 + 1)\n\n    iou = inter_area / (b1_area + b2_area - inter_area)\n\n    return iou\n'"
detector/nms/__init__.py,0,"b""from .nms_wrapper import nms, soft_nms\n\n__all__ = ['nms', 'soft_nms']\n"""
detector/nms/nms_wrapper.py,6,"b'import numpy as np\nimport torch\n\nfrom . import nms_cpu, nms_cuda\nfrom .soft_nms_cpu import soft_nms_cpu\n\n\ndef nms(dets, iou_thr, device_id=None):\n    """"""Dispatch to either CPU or GPU NMS implementations.\n\n    The input can be either a torch tensor or numpy array. GPU NMS will be used\n    if the input is a gpu tensor or device_id is specified, otherwise CPU NMS\n    will be used. The returned type will always be the same as inputs.\n\n    Arguments:\n        dets (torch.Tensor or np.ndarray): bboxes with scores.\n        iou_thr (float): IoU threshold for NMS.\n        device_id (int, optional): when `dets` is a numpy array, if `device_id`\n            is None, then cpu nms is used, otherwise gpu_nms will be used.\n\n    Returns:\n        tuple: kept bboxes and indice, which is always the same data type as\n            the input.\n    """"""\n    # convert dets (tensor or numpy array) to tensor\n    if isinstance(dets, torch.Tensor):\n        is_numpy = False\n        dets_th = dets.to(\'cpu\')\n    elif isinstance(dets, np.ndarray):\n        is_numpy = True\n        device = \'cpu\' if device_id is None else \'cuda:{}\'.format(device_id)\n        dets_th = torch.from_numpy(dets).to(device)\n    else:\n        raise TypeError(\n            \'dets must be either a Tensor or numpy array, but got {}\'.format(\n                type(dets)))\n\n    # execute cpu or cuda nms\n    if dets_th.shape[0] == 0:\n        inds = dets_th.new_zeros(0, dtype=torch.long)\n    else:\n        if dets_th.is_cuda:\n            inds = nms_cuda.nms(dets_th, iou_thr)\n        else:\n            inds = nms_cpu.nms(dets_th, iou_thr)\n\n    if is_numpy:\n        inds = inds.cpu().numpy()\n    return dets[inds, :], inds\n\n\ndef soft_nms(dets, iou_thr, method=\'linear\', sigma=0.5, min_score=1e-3):\n    if isinstance(dets, torch.Tensor):\n        is_tensor = True\n        dets_np = dets.detach().cpu().numpy()\n    elif isinstance(dets, np.ndarray):\n        is_tensor = False\n        dets_np = dets\n    else:\n        raise TypeError(\n            \'dets must be either a Tensor or numpy array, but got {}\'.format(\n                type(dets)))\n\n    method_codes = {\'linear\': 1, \'gaussian\': 2}\n    if method not in method_codes:\n        raise ValueError(\'Invalid method for SoftNMS: {}\'.format(method))\n    new_dets, inds = soft_nms_cpu(\n        dets_np,\n        iou_thr,\n        method=method_codes[method],\n        sigma=sigma,\n        min_score=min_score)\n\n    if is_tensor:\n        return dets.new_tensor(new_dets), dets.new_tensor(\n            inds, dtype=torch.long)\n    else:\n        return new_dets.astype(np.float32), inds.astype(np.int64)\n'"
detector/tracker/__init__.py,0,b''
detector/tracker/models.py,27,"b'import os\nfrom collections import defaultdict,OrderedDict\n\nimport torch.nn as nn\n\nfrom tracker.utils.parse_config import *\nfrom tracker.utils.utils import *\n# from utils.syncbn import SyncBN\nimport time\nimport math\n\nbatch_norm=nn.BatchNorm2d\n\ndef create_modules(module_defs):\n    """"""\n    Constructs module list of layer blocks from module configuration in module_defs\n    """"""\n    hyperparams = module_defs.pop(0)\n    output_filters = [int(hyperparams[\'channels\'])]\n    module_list = nn.ModuleList()\n    yolo_layer_count = 0\n    for i, module_def in enumerate(module_defs):\n        modules = nn.Sequential()\n\n        if module_def[\'type\'] == \'convolutional\':\n            bn = int(module_def[\'batch_normalize\'])\n            filters = int(module_def[\'filters\'])\n            kernel_size = int(module_def[\'size\'])\n            pad = (kernel_size - 1) // 2 if int(module_def[\'pad\']) else 0\n            modules.add_module(\'conv_%d\' % i, nn.Conv2d(in_channels=output_filters[-1],\n                                                        out_channels=filters,\n                                                        kernel_size=kernel_size,\n                                                        stride=int(module_def[\'stride\']),\n                                                        padding=pad,\n                                                        bias=not bn))\n            if bn:\n                modules.add_module(\'batch_norm_%d\' % i, batch_norm(filters))\n            if module_def[\'activation\'] == \'leaky\':\n                modules.add_module(\'leaky_%d\' % i, nn.LeakyReLU(0.1))\n\n        elif module_def[\'type\'] == \'maxpool\':\n            kernel_size = int(module_def[\'size\'])\n            stride = int(module_def[\'stride\'])\n            if kernel_size == 2 and stride == 1:\n                modules.add_module(\'_debug_padding_%d\' % i, nn.ZeroPad2d((0, 1, 0, 1)))\n            maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=int((kernel_size - 1) // 2))\n            modules.add_module(\'maxpool_%d\' % i, maxpool)\n\n        elif module_def[\'type\'] == \'upsample\':\n            upsample = Upsample(scale_factor=int(module_def[\'stride\']))\n            modules.add_module(\'upsample_%d\' % i, upsample)\n\n        elif module_def[\'type\'] == \'route\':\n            layers = [int(x) for x in module_def[\'layers\'].split(\',\')]\n            filters = sum([output_filters[i + 1 if i > 0 else i] for i in layers])\n            modules.add_module(\'route_%d\' % i, EmptyLayer())\n\n        elif module_def[\'type\'] == \'shortcut\':\n            filters = output_filters[int(module_def[\'from\'])]\n            modules.add_module(\'shortcut_%d\' % i, EmptyLayer())\n\n        elif module_def[\'type\'] == \'yolo\':\n            anchor_idxs = [int(x) for x in module_def[\'mask\'].split(\',\')]\n            # Extract anchors\n            anchors = [float(x) for x in module_def[\'anchors\'].split(\',\')]\n            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n            anchors = [anchors[i] for i in anchor_idxs]\n            nC = int(module_def[\'classes\'])  # number of classes\n            img_size = (int(hyperparams[\'width\']),int(hyperparams[\'height\']))\n            # Define detection layer\n            yolo_layer = YOLOLayer(anchors, nC, hyperparams[\'nID\'], img_size, yolo_layer_count, cfg=hyperparams[\'cfg\'])\n            modules.add_module(\'yolo_%d\' % i, yolo_layer)\n            yolo_layer_count += 1\n\n        # Register module list and number of output filters\n        module_list.append(modules)\n        output_filters.append(filters)\n\n    return hyperparams, module_list\n\n\nclass EmptyLayer(nn.Module):\n    """"""Placeholder for \'route\' and \'shortcut\' layers""""""\n\n    def __init__(self):\n        super(EmptyLayer, self).__init__()\n\n    def forward(self, x):\n        return x\n\n\nclass Upsample(nn.Module):\n    # Custom Upsample layer (nn.Upsample gives deprecated warning message)\n\n    def __init__(self, scale_factor=1, mode=\'nearest\'):\n        super(Upsample, self).__init__()\n        self.scale_factor = scale_factor\n        self.mode = mode\n\n    def forward(self, x):\n        return F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n\n\nclass YOLOLayer(nn.Module):\n    def __init__(self, anchors, nC, nID, img_size, yolo_layer, cfg):\n        super(YOLOLayer, self).__init__()\n        self.layer = yolo_layer\n        nA = len(anchors)\n        self.anchors = torch.FloatTensor(anchors)\n        self.nA = nA  # number of anchors (3)\n        self.nC = nC  # number of classes (80)\n        self.nID = nID # number of identities\n        self.img_size = 0\n        self.emb_dim = 512\n\n        self.SmoothL1Loss  = nn.SmoothL1Loss()\n        self.SoftmaxLoss = nn.CrossEntropyLoss(ignore_index=-1)\n        self.CrossEntropyLoss = nn.CrossEntropyLoss()\n        self.IDLoss = nn.CrossEntropyLoss(ignore_index=-1)\n        self.s_c = nn.Parameter(-4.15*torch.ones(1))  # -4.15\n        self.s_r = nn.Parameter(-4.85*torch.ones(1))  # -4.85\n        self.s_id = nn.Parameter(-2.3*torch.ones(1))  # -2.3\n        self.emb_scale = math.sqrt(2) * math.log(self.nID-1)\n        \n\n    def forward(self, p_cat,  img_size, targets=None, classifier=None, test_emb=False):\n        p, p_emb = p_cat[:, :24, ...], p_cat[:, 24:, ...]\n        nB, nGh, nGw = p.shape[0], p.shape[-2], p.shape[-1]\n\n        if self.img_size != img_size:\n            create_grids(self, img_size, nGh, nGw)\n\n            self.grid_xy = self.grid_xy.to(p)\n            self.anchor_wh = self.anchor_wh.to(p)\n\n        p = p.view(nB, self.nA, self.nC + 5, nGh, nGw).permute(0, 1, 3, 4, 2).contiguous()  # prediction\n        \n        p_emb = p_emb.permute(0,2,3,1).contiguous()\n        p_box = p[..., :4]\n        p_conf = p[..., 4:6].permute(0, 4, 1, 2, 3)  # Conf\n\n        # Training\n        if targets is not None:\n            if test_emb:\n                tconf, tbox, tids = build_targets_max(targets, self.anchor_vec.cuda(), self.nA, self.nC, nGh, nGw)\n            else:\n                tconf, tbox, tids = build_targets_thres(targets, self.anchor_vec.cuda(), self.nA, self.nC, nGh, nGw)\n            tconf, tbox, tids = tconf.cuda(), tbox.cuda(), tids.cuda()\n            mask = tconf > 0\n\n            # Compute losses\n            nT = sum([len(x) for x in targets])  # number of targets\n            nM = mask.sum().float()  # number of anchors (assigned to targets)\n            nP = torch.ones_like(mask).sum().float()\n            if nM > 0:\n                lbox = self.SmoothL1Loss(p_box[mask], tbox[mask])\n            else:\n                FT = torch.cuda.FloatTensor if p_conf.is_cuda else torch.FloatTensor\n                lbox, lconf =  FT([0]), FT([0])\n            lconf =  self.SoftmaxLoss(p_conf, tconf)\n            lid = torch.Tensor(1).fill_(0).squeeze().cuda()\n            emb_mask,_ = mask.max(1)\n            \n            # For convenience we use max(1) to decide the id, TODO: more reseanable strategy\n            tids,_ = tids.max(1) \n            tids = tids[emb_mask]\n            embedding = p_emb[emb_mask].contiguous()\n            embedding = self.emb_scale * F.normalize(embedding)\n            nI = emb_mask.sum().float()\n            \n            if  test_emb:\n                if np.prod(embedding.shape)==0  or np.prod(tids.shape) == 0:\n                    return torch.zeros(0, self. emb_dim+1).cuda()\n                emb_and_gt = torch.cat([embedding, tids.float()], dim=1)\n                return emb_and_gt\n            \n            if len(embedding) > 1:\n                logits = classifier(embedding).contiguous()\n                lid =  self.IDLoss(logits, tids.squeeze())\n\n            # Sum loss components\n            loss = torch.exp(-self.s_r)*lbox + torch.exp(-self.s_c)*lconf + torch.exp(-self.s_id)*lid + \\\n                   (self.s_r + self.s_c + self.s_id)\n            loss *= 0.5\n\n            return loss, loss.item(), lbox.item(), lconf.item(), lid.item(), nT\n\n        else:\n            p_conf = torch.softmax(p_conf, dim=1)[:,1,...].unsqueeze(-1)\n            p_emb = p_emb.unsqueeze(1).repeat(1,self.nA,1,1,1).contiguous()\n            p_cls = torch.zeros(nB,self.nA,nGh,nGw,1).to(p)              # Temp\n            p = torch.cat([p_box, p_conf, p_cls, p_emb], dim=-1)\n            p[..., :4] = decode_delta_map(p[..., :4], self.anchor_vec.to(p))\n            p[..., :4] *= self.stride\n\n            return p.view(nB, -1, p.shape[-1])\n\n\nclass Darknet(nn.Module):\n    """"""YOLOv3 object detection model""""""\n\n    def __init__(self, cfg_path, img_size=(1088, 608), nID=1591, test_emb=False):\n        super(Darknet, self).__init__()\n\n        self.module_defs = parse_model_cfg(cfg_path)\n        self.module_defs[0][\'cfg\'] = cfg_path\n        self.module_defs[0][\'nID\'] = nID\n        self.hyperparams, self.module_list = create_modules(self.module_defs)\n        self.img_size = img_size\n        self.loss_names = [\'loss\', \'box\', \'conf\', \'id\', \'nT\']\n        self.losses = OrderedDict()\n        for ln in self.loss_names:\n            self.losses[ln] = 0\n        self.emb_dim = 512\n        self.classifier = nn.Linear(self.emb_dim, nID)\n        self.test_emb=test_emb\n\n\n    def forward(self, x, targets=None, targets_len=None):\n        self.losses = OrderedDict()\n        for ln in self.loss_names:\n            self.losses[ln] = 0\n        is_training = (targets is not None) and (not self.test_emb)\n        #img_size = x.shape[-1]\n        layer_outputs = []\n        output = []\n\n        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n            mtype = module_def[\'type\']\n            if mtype in [\'convolutional\', \'upsample\', \'maxpool\']:\n                x = module(x)\n            elif mtype == \'route\':\n                layer_i = [int(x) for x in module_def[\'layers\'].split(\',\')]\n                if len(layer_i) == 1:\n                    x = layer_outputs[layer_i[0]]\n                else:\n                    x = torch.cat([layer_outputs[i] for i in layer_i], 1)\n            elif mtype == \'shortcut\':\n                layer_i = int(module_def[\'from\'])\n                x = layer_outputs[-1] + layer_outputs[layer_i]\n            elif mtype == \'yolo\':\n                if is_training:  # get loss\n                    targets = [targets[i][:int(l)] for i,l in enumerate(targets_len)]\n                    x, *losses = module[0](x, self.img_size, targets, self.classifier)\n                    for name, loss in zip(self.loss_names, losses):\n                        self.losses[name] += loss\n                elif self.test_emb:\n                    targets = [targets[i][:int(l)] for i,l in enumerate(targets_len)]\n                    x = module[0](x, self.img_size, targets, self.classifier, self.test_emb)\n                else:  # get detections\n                    x = module[0](x, self.img_size)\n                output.append(x)\n            layer_outputs.append(x)\n\n        if is_training:\n            self.losses[\'nT\'] /= 3 \n            output = [o.squeeze() for o in output]\n            return sum(output), torch.Tensor(list(self.losses.values())).cuda()\n        elif self.test_emb:\n            return torch.cat(output, 0)\n        return torch.cat(output, 1)\n\n\ndef create_grids(self, img_size, nGh, nGw):\n    self.stride = img_size[0]/nGw\n    assert self.stride == img_size[1] / nGh\n\n    # build xy offsets\n    grid_x = torch.arange(nGw).repeat((nGh, 1)).view((1, 1, nGh, nGw)).float()\n    grid_y = torch.arange(nGh).repeat((nGw, 1)).transpose(0,1).view((1, 1, nGh, nGw)).float()\n    #grid_y = grid_x.permute(0, 1, 3, 2)\n    self.grid_xy = torch.stack((grid_x, grid_y), 4)\n\n    # build wh gains\n    self.anchor_vec = self.anchors / self.stride\n    self.anchor_wh = self.anchor_vec.view(1, self.nA, 1, 1, 2)\n\n\ndef load_darknet_weights(self, weights, cutoff=-1):\n    # Parses and loads the weights stored in \'weights\'\n    # cutoff: save layers between 0 and cutoff (if cutoff = -1 all are saved)\n    weights_file = weights.split(os.sep)[-1]\n\n    # Try to download weights if not available locally\n    if not os.path.isfile(weights):\n        try:\n            os.system(\'wget https://pjreddie.com/media/files/\' + weights_file + \' -O \' + weights)\n        except IOError:\n            print(weights + \' not found\')\n\n    # Establish cutoffs\n    if weights_file == \'darknet53.conv.74\':\n        cutoff = 75\n    elif weights_file == \'yolov3-tiny.conv.15\':\n        cutoff = 15\n\n    # Open the weights file\n    fp = open(weights, \'rb\')\n    header = np.fromfile(fp, dtype=np.int32, count=5)  # First five are header values\n\n    # Needed to write header when saving weights\n    self.header_info = header\n\n    self.seen = header[3]  # number of images seen during training\n    weights = np.fromfile(fp, dtype=np.float32)  # The rest are weights\n    fp.close()\n\n    ptr = 0\n    for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n        if module_def[\'type\'] == \'convolutional\':\n            conv_layer = module[0]\n            if module_def[\'batch_normalize\']:\n                # Load BN bias, weights, running mean and running variance\n                bn_layer = module[1]\n                num_b = bn_layer.bias.numel()  # Number of biases\n                # Bias\n                bn_b = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.bias)\n                bn_layer.bias.data.copy_(bn_b)\n                ptr += num_b\n                # Weight\n                bn_w = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.weight)\n                bn_layer.weight.data.copy_(bn_w)\n                ptr += num_b\n                # Running Mean\n                bn_rm = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.running_mean)\n                bn_layer.running_mean.data.copy_(bn_rm)\n                ptr += num_b\n                # Running Var\n                bn_rv = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.running_var)\n                bn_layer.running_var.data.copy_(bn_rv)\n                ptr += num_b\n            else:\n                # Load conv. bias\n                num_b = conv_layer.bias.numel()\n                conv_b = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(conv_layer.bias)\n                conv_layer.bias.data.copy_(conv_b)\n                ptr += num_b\n            # Load conv. weights\n            num_w = conv_layer.weight.numel()\n            conv_w = torch.from_numpy(weights[ptr:ptr + num_w]).view_as(conv_layer.weight)\n            conv_layer.weight.data.copy_(conv_w)\n            ptr += num_w\n\n\n""""""\n    @:param path    - path of the new weights file\n    @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)\n""""""\n\n\ndef save_weights(self, path, cutoff=-1):\n    fp = open(path, \'wb\')\n    self.header_info[3] = self.seen  # number of images seen during training\n    self.header_info.tofile(fp)\n\n    # Iterate through layers\n    for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n        if module_def[\'type\'] == \'convolutional\':\n            conv_layer = module[0]\n            # If batch norm, load bn first\n            if module_def[\'batch_normalize\']:\n                bn_layer = module[1]\n                bn_layer.bias.data.cpu().numpy().tofile(fp)\n                bn_layer.weight.data.cpu().numpy().tofile(fp)\n                bn_layer.running_mean.data.cpu().numpy().tofile(fp)\n                bn_layer.running_var.data.cpu().numpy().tofile(fp)\n            # Load conv bias\n            else:\n                conv_layer.bias.data.cpu().numpy().tofile(fp)\n            # Load conv weights\n            conv_layer.weight.data.cpu().numpy().tofile(fp)\n\n    fp.close()\n'"
detector/tracker/preprocess.py,5,"b'from __future__ import division\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ntry:\n    from util import count_parameters as count\n    from util import convert2cpu as cpu\nexcept ImportError:\n    from yolo.util import count_parameters as count\n    from yolo.util import convert2cpu as cpu\nfrom PIL import Image, ImageDraw\n\n\ndef letterbox_image(img, img_size=(1088, 608), color=(127.5, 127.5, 127.5)):  \n    # resize a rectangular image to a padded rectangular \n    height=img_size[1]\n    width=img_size[0]\n    shape = img.shape[:2]  # shape = [height, width]\n    ratio = min(float(height)/shape[0], float(width)/shape[1])\n    new_shape = (round(shape[1] * ratio), round(shape[0] * ratio)) # new_shape = [width, height]\n    dw = (width - new_shape[0]) / 2  # width padding\n    dh = (height - new_shape[1]) / 2  # height padding\n    top, bottom = round(dh - 0.1), round(dh + 0.1)\n    left, right = round(dw - 0.1), round(dw + 0.1)\n    img = cv2.resize(img, new_shape, interpolation=cv2.INTER_AREA)  # resized, no border\n    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # padded rectangular\n    return img\n\n\ndef prep_image(img, img_size=(1088, 608)):\n    """"""\n    Prepare image for inputting to the neural network.\n\n    Returns a Variable\n    """"""\n\n    orig_im = cv2.imread(img)\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = (letterbox_image(orig_im, img_size))\n    img_ = img[:, :, ::-1].transpose((2, 0, 1)).copy()\n    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n    return img_, orig_im, dim\n\n\ndef prep_frame(img, img_size=(1088, 608)):\n    """"""\n    Prepare image for inputting to the neural network.\n\n    Returns a Variable\n    """"""\n\n    orig_im = img\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = (letterbox_image(orig_im, img_size))\n    img_ = img[:, :, ::-1].transpose((2, 0, 1)).copy()\n    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n    return img_, orig_im, dim\n\n'"
detector/yolo/__init__.py,0,b''
detector/yolo/bbox.py,10,"b'from __future__ import division\n\nimport torch \nimport random\n\nimport numpy as np\nimport cv2\n\ndef confidence_filter(result, confidence):\n    conf_mask = (result[:,:,4] > confidence).float().unsqueeze(2)\n    result = result*conf_mask    \n    \n    return result\n\ndef confidence_filter_cls(result, confidence):\n    max_scores = torch.max(result[:,:,5:25], 2)[0]\n    res = torch.cat((result, max_scores),2)\n    print(res.shape)\n    \n    \n    cond_1 = (res[:,:,4] > confidence).float()\n    cond_2 = (res[:,:,25] > 0.995).float()\n    \n    conf = cond_1 + cond_2\n    conf = torch.clamp(conf, 0.0, 1.0)\n    conf = conf.unsqueeze(2)\n    result = result*conf   \n    return result\n\n\n\ndef get_abs_coord(box):\n    box[2], box[3] = abs(box[2]), abs(box[3])\n    x1 = (box[0] - box[2]/2) - 1 \n    y1 = (box[1] - box[3]/2) - 1 \n    x2 = (box[0] + box[2]/2) - 1 \n    y2 = (box[1] + box[3]/2) - 1\n    return x1, y1, x2, y2\n    \n\n\ndef sanity_fix(box):\n    if (box[0] > box[2]):\n        box[0], box[2] = box[2], box[0]\n    \n    if (box[1] >  box[3]):\n        box[1], box[3] = box[3], box[1]\n        \n    return box\n\ndef bbox_iou(box1, box2, args=None):\n    """"""\n    Returns the IoU of two bounding boxes \n    \n    \n    """"""\n    #Get the coordinates of bounding boxes\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]\n    \n    #get the corrdinates of the intersection rectangle\n    inter_rect_x1 =  torch.max(b1_x1, b2_x1)\n    inter_rect_y1 =  torch.max(b1_y1, b2_y1)\n    inter_rect_x2 =  torch.min(b1_x2, b2_x2)\n    inter_rect_y2 =  torch.min(b1_y2, b2_y2)\n    \n    #Intersection area\n    if not args:\n        inter_area = torch.max(inter_rect_x2 - inter_rect_x1 + 1,torch.zeros(inter_rect_x2.shape).cuda())*torch.max(inter_rect_y2 - inter_rect_y1 + 1, torch.zeros(inter_rect_x2.shape).cuda())\n    else:\n        inter_area = torch.max(inter_rect_x2 - inter_rect_x1 + 1,torch.zeros(inter_rect_x2.shape).to(args.device))*torch.max(inter_rect_y2 - inter_rect_y1 + 1, torch.zeros(inter_rect_x2.shape).to(args.device))\n    #Union Area\n    b1_area = (b1_x2 - b1_x1 + 1)*(b1_y2 - b1_y1 + 1)\n    b2_area = (b2_x2 - b2_x1 + 1)*(b2_y2 - b2_y1 + 1)\n    \n    iou = inter_area / (b1_area + b2_area - inter_area)\n    \n    return iou\n\n\ndef pred_corner_coord(prediction):\n    #Get indices of non-zero confidence bboxes\n    ind_nz = torch.nonzero(prediction[:,:,4]).transpose(0,1).contiguous()\n    \n    box = prediction[ind_nz[0], ind_nz[1]]\n    \n    \n    box_a = box.new(box.shape)\n    box_a[:,0] = (box[:,0] - box[:,2]/2)\n    box_a[:,1] = (box[:,1] - box[:,3]/2)\n    box_a[:,2] = (box[:,0] + box[:,2]/2) \n    box_a[:,3] = (box[:,1] + box[:,3]/2)\n    box[:,:4] = box_a[:,:4]\n    \n    prediction[ind_nz[0], ind_nz[1]] = box\n    \n    return prediction\n\n\n\n\ndef write(x, batches, results, colors, classes):\n    c1 = tuple(x[1:3].int())\n    c2 = tuple(x[3:5].int())\n    img = results[int(x[0])]\n    cls = int(x[-1])\n    label = ""{0}"".format(classes[cls])\n    color = random.choice(colors)\n    cv2.rectangle(img, c1, c2,color, 1)\n    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n    cv2.rectangle(img, c1, c2,color, -1)\n    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n    return img\n'"
detector/yolo/cam_demo.py,7,"b'from __future__ import division\nimport time\nimport torch \nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nfrom util import *\nfrom darknet import Darknet\nfrom preprocess import prep_image, inp_to_image\nimport pandas as pd\nimport random \nimport argparse\nimport pickle as pkl\n\ndef get_test_input(input_dim, CUDA):\n    img = cv2.imread(""imgs/messi.jpg"")\n    img = cv2.resize(img, (input_dim, input_dim)) \n    img_ =  img[:,:,::-1].transpose((2,0,1))\n    img_ = img_[np.newaxis,:,:,:]/255.0\n    img_ = torch.from_numpy(img_).float()\n    img_ = Variable(img_)\n    \n    if CUDA:\n        img_ = img_.cuda()\n    \n    return img_\n\ndef prep_image(img, inp_dim):\n    """"""\n    Prepare image for inputting to the neural network. \n    \n    Returns a Variable \n    """"""\n\n    orig_im = img\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = cv2.resize(orig_im, (inp_dim, inp_dim))\n    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n    return img_, orig_im, dim\n\ndef write(x, img):\n    c1 = tuple(x[1:3].int())\n    c2 = tuple(x[3:5].int())\n    cls = int(x[-1])\n    label = ""{0}"".format(classes[cls])\n    color = random.choice(colors)\n    cv2.rectangle(img, c1, c2,color, 1)\n    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n    cv2.rectangle(img, c1, c2,color, -1)\n    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n    return img\n\ndef arg_parse():\n    """"""\n    Parse arguements to the detect module\n    \n    """"""\n    \n    \n    parser = argparse.ArgumentParser(description=\'YOLO v3 Cam Demo\')\n    parser.add_argument(""--confidence"", dest = ""confidence"", help = ""Object Confidence to filter predictions"", default = 0.25)\n    parser.add_argument(""--nms_thresh"", dest = ""nms_thresh"", help = ""NMS Threshhold"", default = 0.4)\n    parser.add_argument(""--reso"", dest = \'reso\', help = \n                        ""Input resolution of the network. Increase to increase accuracy. Decrease to increase speed"",\n                        default = ""160"", type = str)\n    return parser.parse_args()\n\n\n\nif __name__ == \'__main__\':\n    cfgfile = ""cfg/yolov3-spp.cfg""\n    weightsfile = ""yolov3-spp.weights""\n    num_classes = 80\n\n    args = arg_parse()\n    confidence = float(args.confidence)\n    nms_thesh = float(args.nms_thresh)\n    start = 0\n    CUDA = torch.cuda.is_available()\n    \n\n    \n    \n    num_classes = 80\n    bbox_attrs = 5 + num_classes\n    \n    model = Darknet(cfgfile)\n    model.load_weights(weightsfile)\n    \n    model.net_info[""height""] = args.reso\n    inp_dim = int(model.net_info[""height""])\n    \n    assert inp_dim % 32 == 0 \n    assert inp_dim > 32\n\n    if CUDA:\n        model.cuda()\n            \n    model.eval()\n    \n    videofile = \'video.avi\'\n    \n    cap = cv2.VideoCapture(0)\n    \n    assert cap.isOpened(), \'Cannot capture source\'\n    \n    frames = 0\n    start = time.time()    \n    while cap.isOpened():\n        \n        ret, frame = cap.read()\n        if ret:\n            \n            img, orig_im, dim = prep_image(frame, inp_dim)\n            \n#            im_dim = torch.FloatTensor(dim).repeat(1,2)                        \n            \n            \n            if CUDA:\n                im_dim = im_dim.cuda()\n                img = img.cuda()\n            \n            \n            output = model(Variable(img), CUDA)\n            output = write_results(output, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n\n            if type(output) == int:\n                frames += 1\n                print(""FPS of the video is {:5.2f}"".format( frames / (time.time() - start)))\n                cv2.imshow(""frame"", orig_im)\n                key = cv2.waitKey(1)\n                if key & 0xFF == ord(\'q\'):\n                    break\n                continue\n            \n\n        \n            output[:,1:5] = torch.clamp(output[:,1:5], 0.0, float(inp_dim))/inp_dim\n            \n#            im_dim = im_dim.repeat(output.size(0), 1)\n            output[:,[1,3]] *= frame.shape[1]\n            output[:,[2,4]] *= frame.shape[0]\n\n            \n            classes = load_classes(\'data/coco.names\')\n            colors = pkl.load(open(""pallete"", ""rb""))\n            \n            list(map(lambda x: write(x, orig_im), output))\n            \n            \n            cv2.imshow(""frame"", orig_im)\n            key = cv2.waitKey(1)\n            if key & 0xFF == ord(\'q\'):\n                break\n            frames += 1\n            print(""FPS of the video is {:5.2f}"".format( frames / (time.time() - start)))\n\n            \n        else:\n            break\n    \n\n    \n    \n\n'"
detector/yolo/darknet.py,15,"b'from __future__ import division\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F \nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nimport matplotlib.pyplot as plt\ntry:\n    from util import count_parameters as count\n    from util import convert2cpu as cpu\n    from util import predict_transform\nexcept ImportError:\n    from yolo.util import count_parameters as count\n    from yolo.util import convert2cpu as cpu\n    from yolo.util import predict_transform\n\nclass test_net(nn.Module):\n    def __init__(self, num_layers, input_size):\n        super(test_net, self).__init__()\n        self.num_layers= num_layers\n        self.linear_1 = nn.Linear(input_size, 5)\n        self.middle = nn.ModuleList([nn.Linear(5,5) for x in range(num_layers)])\n        self.output = nn.Linear(5,2)\n    \n    def forward(self, x):\n        x = x.view(-1)\n        fwd = nn.Sequential(self.linear_1, *self.middle, self.output)\n        return fwd(x)\n        \ndef get_test_input():\n    img = cv2.imread(""dog-cycle-car.png"")\n    img = cv2.resize(img, (416,416)) \n    img_ =  img[:,:,::-1].transpose((2,0,1))\n    img_ = img_[np.newaxis,:,:,:]/255.0\n    img_ = torch.from_numpy(img_).float()\n    img_ = Variable(img_)\n    return img_\n\n\ndef parse_cfg(cfgfile):\n    """"""\n    Takes a configuration file\n    \n    Returns a list of blocks. Each blocks describes a block in the neural\n    network to be built. Block is represented as a dictionary in the list\n    \n    """"""\n    file = open(cfgfile, \'r\')\n    lines = file.read().split(\'\\n\')     #store the lines in a list\n    lines = [x for x in lines if len(x) > 0] #get read of the empty lines \n    lines = [x for x in lines if x[0] != \'#\']  \n    lines = [x.rstrip().lstrip() for x in lines]\n\n    \n    block = {}\n    blocks = []\n    \n    for line in lines:\n        if line[0] == ""["":               #This marks the start of a new block\n            if len(block) != 0:\n                blocks.append(block)\n                block = {}\n            block[""type""] = line[1:-1].rstrip()\n        else:\n            key,value = line.split(""="")\n            block[key.rstrip()] = value.lstrip()\n    blocks.append(block)\n\n    return blocks\n#    print(\'\\n\\n\'.join([repr(x) for x in blocks]))\n\nimport pickle as pkl\n\nclass MaxPoolStride1(nn.Module):\n    def __init__(self, kernel_size):\n        super(MaxPoolStride1, self).__init__()\n        self.kernel_size = kernel_size\n        self.pad = kernel_size - 1\n\n    def forward(self, x):\n        padding = int(self.pad / 2)\n        #padded_x = F.pad(x, (0,self.pad,0,self.pad), mode=""replicate"")\n        #pooled_x = nn.MaxPool2d(self.kernel_size, self.pad)(padded_x)\n        #padded_x = F.pad(x, (0, self.pad, 0, self.pad), mode=""replicate"")\n        padded_x = F.pad(x, (padding, padding, padding, padding), mode=""constant"", value=0)\n        pooled_x = nn.MaxPool2d(self.kernel_size, 1)(padded_x)\n        return pooled_x\n\n\nclass EmptyLayer(nn.Module):\n    def __init__(self):\n        super(EmptyLayer, self).__init__()\n        \n\nclass DetectionLayer(nn.Module):\n    def __init__(self, anchors):\n        super(DetectionLayer, self).__init__()\n        self.anchors = anchors\n    \n    def forward(self, x, inp_dim, num_classes, confidence):\n        x = x.data\n        global args\n        prediction = x.to(args.device)\n        prediction = predict_transform(prediction, inp_dim, self.anchors, num_classes, confidence, args)\n        return prediction\n        \n\n        \n\n\nclass Upsample(nn.Module):\n    def __init__(self, stride=2):\n        super(Upsample, self).__init__()\n        self.stride = stride\n        \n    def forward(self, x):\n        stride = self.stride\n        assert(x.data.dim() == 4)\n        B = x.data.size(0)\n        C = x.data.size(1)\n        H = x.data.size(2)\n        W = x.data.size(3)\n        ws = stride\n        hs = stride\n        x = x.view(B, C, H, 1, W, 1).expand(B, C, H, stride, W, stride).contiguous().view(B, C, H*stride, W*stride)\n        return x\n#       \n        \nclass ReOrgLayer(nn.Module):\n    def __init__(self, stride = 2):\n        super(ReOrgLayer, self).__init__()\n        self.stride= stride\n        \n    def forward(self,x):\n        assert(x.data.dim() == 4)\n        B,C,H,W = x.data.shape\n        hs = self.stride\n        ws = self.stride\n        assert(H % hs == 0),  ""The stride "" + str(self.stride) + "" is not a proper divisor of height "" + str(H)\n        assert(W % ws == 0),  ""The stride "" + str(self.stride) + "" is not a proper divisor of height "" + str(W)\n        x = x.view(B,C, H // hs, hs, W // ws, ws).transpose(-2,-3).contiguous()\n        x = x.view(B,C, H // hs * W // ws, hs, ws)\n        x = x.view(B,C, H // hs * W // ws, hs*ws).transpose(-1,-2).contiguous()\n        x = x.view(B, C, ws*hs, H // ws, W // ws).transpose(1,2).contiguous()\n        x = x.view(B, C*ws*hs, H // ws, W // ws)\n        return x\n\n\ndef create_modules(blocks):\n    net_info = blocks[0]     #Captures the information about the input and pre-processing    \n    \n    module_list = nn.ModuleList()\n    \n    index = 0    #indexing blocks helps with implementing route  layers (skip connections)\n\n    \n    prev_filters = 3\n    \n    output_filters = []\n    \n    for x in blocks:\n        module = nn.Sequential()\n        \n        if (x[""type""] == ""net""):\n            continue\n        \n        #If it\'s a convolutional layer\n        if (x[""type""] == ""convolutional""):\n            #Get the info about the layer\n            activation = x[""activation""]\n            try:\n                batch_normalize = int(x[""batch_normalize""])\n                bias = False\n            except:\n                batch_normalize = 0\n                bias = True\n                \n            filters= int(x[""filters""])\n            padding = int(x[""pad""])\n            kernel_size = int(x[""size""])\n            stride = int(x[""stride""])\n            \n            if padding:\n                pad = (kernel_size - 1) // 2\n            else:\n                pad = 0\n                \n            #Add the convolutional layer\n            conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias=bias)\n            module.add_module(""conv_{0}"".format(index), conv)\n            \n            #Add the Batch Norm Layer\n            if batch_normalize:\n                bn = nn.BatchNorm2d(filters)\n                module.add_module(""batch_norm_{0}"".format(index), bn)\n            \n            #Check the activation. \n            #It is either Linear or a Leaky ReLU for YOLO\n            if activation == ""leaky"":\n                activn = nn.LeakyReLU(0.1, inplace = True)\n                module.add_module(""leaky_{0}"".format(index), activn)\n            \n            \n            \n        #If it\'s an upsampling layer\n        #We use Bilinear2dUpsampling\n        \n        elif (x[""type""] == ""upsample""):\n            stride = int(x[""stride""])\n#            upsample = Upsample(stride)\n            upsample = nn.Upsample(scale_factor = 2, mode = ""nearest"")\n            module.add_module(""upsample_{}"".format(index), upsample)\n        \n        #If it is a route layer\n        elif (x[""type""] == ""route""):\n            x[""layers""] = x[""layers""].split(\',\')\n            \n            #Start  of a route\n            start = int(x[""layers""][0])\n            if len(x[""layers""]) <= 2:\n                #end, if there exists one.\n                try:\n                    end = int(x[""layers""][1])\n                except:\n                    end = 0\n\n                #Positive anotation\n                if start > 0: \n                    start = start - index\n                \n                if end > 0:\n                    end = end - index\n\n                \n                route = EmptyLayer()\n                module.add_module(""route_{0}"".format(index), route)\n                \n                \n                \n                if end < 0:\n                    filters = output_filters[index + start] + output_filters[index + end]\n                else:\n                    filters= output_filters[index + start]\n            else:  #SPP-route\n                assert len(x[""layers""]) == 4\n\n                round = EmptyLayer()\n                module.add_module(""route_{0}"".format(index), route)\n\n                filters = output_filters[index + start] + output_filters[index + int(x[""layers""][1])] \\\n                          + output_filters[index + int(x[""layers""][2])] + output_filters[index + int(x[""layers""][3])]\n        \n        #shortcut corresponds to skip connection\n        elif x[""type""] == ""shortcut"":\n            from_ = int(x[""from""])\n            shortcut = EmptyLayer()\n            module.add_module(""shortcut_{}"".format(index), shortcut)\n            \n            \n        elif x[""type""] == ""maxpool"":\n            stride = int(x[""stride""])\n            size = int(x[""size""])\n            if stride != 1:\n                maxpool = nn.MaxPool2d(size, stride)\n            else:\n                maxpool = MaxPoolStride1(size)\n                #maxpool = nn.MaxPool2d(size, stride=1, padding=size-1)\n\n            module.add_module(""maxpool_{}"".format(index), maxpool)\n        \n        #Yolo is the detection layer\n        elif x[""type""] == ""yolo"":\n            mask = x[""mask""].split("","")\n            mask = [int(x) for x in mask]\n            \n            \n            anchors = x[""anchors""].split("","")\n            anchors = [int(a) for a in anchors]\n            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n            anchors = [anchors[i] for i in mask]\n            \n            detection = DetectionLayer(anchors)\n            module.add_module(""Detection_{}"".format(index), detection)\n        \n            \n            \n        else:\n            print(""Something I dunno"")\n            assert False\n\n        module_list.append(module)\n        prev_filters = filters\n        output_filters.append(filters)\n        index += 1\n        \n    \n    return (net_info, module_list)\n\n\n\nclass Darknet(nn.Module):\n    def __init__(self, cfgfile):\n        super(Darknet, self).__init__()\n        self.blocks = parse_cfg(cfgfile)\n        self.net_info, self.module_list = create_modules(self.blocks)\n        self.header = torch.IntTensor([0,0,0,0])\n        self.seen = 0\n\n        \n        \n    def get_blocks(self):\n        return self.blocks\n    \n    def get_module_list(self):\n        return self.module_list\n\n                \n    def forward(self, x, args):\n        detections = []\n        modules = self.blocks[1:]\n        outputs = {}   #We cache the outputs for the route layer\n        \n        \n        write = 0\n        for i in range(len(modules)):        \n            \n            module_type = (modules[i][""type""])\n            if module_type == ""convolutional"" or module_type == ""upsample"" or module_type == ""maxpool"":\n                \n                x = self.module_list[i](x)\n                outputs[i] = x\n\n                \n            elif module_type == ""route"":\n                layers = modules[i][""layers""]\n                layers = [int(a) for a in layers]\n                \n                if (layers[0]) > 0:\n                    layers[0] = layers[0] - i\n\n                if len(layers) == 1:\n                    x = outputs[i + (layers[0])]\n\n                elif len(layers) == 2:\n                    if (layers[1]) > 0:\n                        layers[1] = layers[1] - i\n                        \n                    map1 = outputs[i + layers[0]]\n                    map2 = outputs[i + layers[1]]\n\n                    x = torch.cat((map1, map2), 1)\n                elif len(layers) == 4:  # SPP\n                    map1 = outputs[i + layers[0]]\n                    map2 = outputs[i + layers[1]]\n                    map3 = outputs[i + layers[2]]\n                    map4 = outputs[i + layers[3]]\n\n                    x = torch.cat((map1, map2, map3, map4), 1)\n                outputs[i] = x\n            \n            elif  module_type == ""shortcut"":\n                from_ = int(modules[i][""from""])\n                x = outputs[i-1] + outputs[i+from_]\n                outputs[i] = x\n                \n            \n            \n            elif module_type == \'yolo\':        \n                \n                anchors = self.module_list[i][0].anchors\n                #Get the input dimensions\n                inp_dim = int (self.net_info[""height""])\n                \n                #Get the number of classes\n                num_classes = int (modules[i][""classes""])\n                \n                #Output the result\n                x = x.data.to(args.device)\n                x = predict_transform(x, inp_dim, anchors, num_classes, args)\n                \n                if type(x) == int:\n                    continue\n\n                \n                if not write:\n                    detections = x\n                    write = 1\n                \n                else:\n                    detections = torch.cat((detections, x), 1)\n                \n                outputs[i] = outputs[i-1]\n                \n        \n        \n        try:\n            return detections\n        except:\n            return 0\n\n            \n    def load_weights(self, weightfile):\n        \n        #Open the weights file\n        fp = open(weightfile, ""rb"")\n\n        #The first 4 values are header information \n        # 1. Major version number\n        # 2. Minor Version Number\n        # 3. Subversion number \n        # 4. IMages seen \n        header = np.fromfile(fp, dtype = np.int32, count = 5)\n        self.header = torch.from_numpy(header)\n        self.seen = self.header[3]\n        \n        #The rest of the values are the weights\n        # Let\'s load them up\n        weights = np.fromfile(fp, dtype = np.float32)\n        \n        ptr = 0\n        for i in range(len(self.module_list)):\n            module_type = self.blocks[i + 1][""type""]\n            \n            if module_type == ""convolutional"":\n                model = self.module_list[i]\n                try:\n                    batch_normalize = int(self.blocks[i+1][""batch_normalize""])\n                except:\n                    batch_normalize = 0\n                \n                conv = model[0]\n                \n                if (batch_normalize):\n                    bn = model[1]\n                    \n                    #Get the number of weights of Batch Norm Layer\n                    num_bn_biases = bn.bias.numel()\n                    \n                    #Load the weights\n                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n                    ptr += num_bn_biases\n                    \n                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n                    ptr  += num_bn_biases\n                    \n                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n                    ptr  += num_bn_biases\n                    \n                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n                    ptr  += num_bn_biases\n                    \n                    #Cast the loaded weights into dims of model weights. \n                    bn_biases = bn_biases.view_as(bn.bias.data)\n                    bn_weights = bn_weights.view_as(bn.weight.data)\n                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n                    bn_running_var = bn_running_var.view_as(bn.running_var)\n\n                    #Copy the data to model\n                    bn.bias.data.copy_(bn_biases)\n                    bn.weight.data.copy_(bn_weights)\n                    bn.running_mean.copy_(bn_running_mean)\n                    bn.running_var.copy_(bn_running_var)\n                \n                else:\n                    #Number of biases\n                    num_biases = conv.bias.numel()\n                \n                    #Load the weights\n                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n                    ptr = ptr + num_biases\n                    \n                    #reshape the loaded weights according to the dims of the model weights\n                    conv_biases = conv_biases.view_as(conv.bias.data)\n                    \n                    #Finally copy the data\n                    conv.bias.data.copy_(conv_biases)\n                    \n                    \n                #Let us load the weights for the Convolutional layers\n                num_weights = conv.weight.numel()\n                \n                #Do the same as above for weights\n                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n                ptr = ptr + num_weights\n\n                conv_weights = conv_weights.view_as(conv.weight.data)\n                conv.weight.data.copy_(conv_weights)\n                \n    def save_weights(self, savedfile, cutoff = 0):\n            \n        if cutoff <= 0:\n            cutoff = len(self.blocks) - 1\n        \n        fp = open(savedfile, \'wb\')\n        \n        # Attach the header at the top of the file\n        self.header[3] = self.seen\n        header = self.header\n\n        header = header.numpy()\n        header.tofile(fp)\n        \n        # Now, let us save the weights \n        for i in range(len(self.module_list)):\n            module_type = self.blocks[i+1][""type""]\n            \n            if (module_type) == ""convolutional"":\n                model = self.module_list[i]\n                try:\n                    batch_normalize = int(self.blocks[i+1][""batch_normalize""])\n                except:\n                    batch_normalize = 0\n                    \n                conv = model[0]\n\n                if (batch_normalize):\n                    bn = model[1]\n                \n                    #If the parameters are on GPU, convert them back to CPU\n                    #We don\'t convert the parameter to GPU\n                    #Instead. we copy the parameter and then convert it to CPU\n                    #This is done as weight are need to be saved during training\n                    cpu(bn.bias.data).numpy().tofile(fp)\n                    cpu(bn.weight.data).numpy().tofile(fp)\n                    cpu(bn.running_mean).numpy().tofile(fp)\n                    cpu(bn.running_var).numpy().tofile(fp)\n                \n            \n                else:\n                    cpu(conv.bias.data).numpy().tofile(fp)\n                \n                \n                #Let us save the weights for the Convolutional layers\n                cpu(conv.weight.data).numpy().tofile(fp)\n               \n\n\n\n\n#\n#dn = Darknet(\'cfg/yolov3.cfg\')\n#dn.load_weights(""yolov3.weights"")\n#inp = get_test_input()\n#a, interms = dn(inp)\n#dn.eval()\n#a_i, interms_i = dn(inp)\n'"
detector/yolo/detect.py,10,"b'from __future__ import division\nimport time\nimport torch \nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nfrom util import *\nimport argparse\nimport os \nimport os.path as osp\nfrom darknet import Darknet\nfrom preprocess import prep_image, inp_to_image\nimport pandas as pd\nimport random \nimport pickle as pkl\nimport itertools\n\n\nif __name__ == \'__main__\':\n\n    scales = ""1,2,3""\n    images = ""imgs/messi.jpg""\n    batch_size = 1\n    confidence = 0.5\n    nms_thesh = 0.4\n\n    CUDA = torch.cuda.is_available()\n\n    num_classes = 80\n    classes = load_classes(\'data/coco.names\') \n\n    #Set up the neural network\n    print(""Loading network....."")\n    model = Darknet(""cfg/yolov3-spp.cfg"")\n    model.load_weights(""yolov3-spp.weights"")\n    print(""Network successfully loaded"")\n\n    model.net_info[""height""] = ""608""\n    inp_dim = int(model.net_info[""height""])\n    assert inp_dim % 32 == 0\n    assert inp_dim > 32\n\n    #If there\'s a GPU availible, put the model on GPU\n    if CUDA:\n        model.cuda()\n\n    #Set the model in evaluation mode\n    model.eval()\n\n    #Detection phase\n    try:\n        imlist = []\n        imlist.append(osp.join(osp.realpath(\'.\'), images))\n    except FileNotFoundError:\n        print (""No file or directory with the name {}"".format(images))\n        exit()\n\n    batches = list(map(prep_image, imlist, [inp_dim for x in range(len(imlist))]))\n    im_batches = [x[0] for x in batches]\n    orig_ims = [x[1] for x in batches]\n    im_dim_list = [x[2] for x in batches]\n    im_dim_list = torch.FloatTensor(im_dim_list).repeat(1, 2)\n\n    if CUDA:\n        im_dim_list = im_dim_list.cuda()\n\n\n    for batch in im_batches:\n        #load the image\n        if CUDA:\n            batch = batch.cuda()\n        with torch.no_grad():\n            prediction = model(Variable(batch), CUDA)\n\n        prediction = write_results(prediction, confidence, num_classes, nms=True, nms_conf=nms_thesh)\n        output = prediction\n\n        if CUDA:\n            torch.cuda.synchronize()\n\n    try:\n        output\n    except NameError:\n        print(""No detections were made"")\n        exit()\n    print(im_dim_list.shape)\n    im_dim_list = torch.index_select(im_dim_list, 0, output[:,0].long())\n\n    scaling_factor = torch.min(inp_dim/im_dim_list,1)[0].view(-1,1)\n\n\n    output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim_list[:,0].view(-1,1))/2\n    output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim_list[:,1].view(-1,1))/2\n\n    output[:,1:5] /= scaling_factor\n\n    for i in range(output.shape[0]):\n        output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim_list[i,0])\n        output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim_list[i,1])\n\n    print(output)\n    print(output.shape)\n'"
detector/yolo/preprocess.py,6,"b'from __future__ import division\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\ntry:\n    from util import count_parameters as count\n    from util import convert2cpu as cpu\nexcept ImportError:\n    from yolo.util import count_parameters as count\n    from yolo.util import convert2cpu as cpu\nfrom PIL import Image, ImageDraw\n\n\ndef letterbox_image(img, inp_dim):\n    \'\'\'resize image with unchanged aspect ratio using padding\'\'\'\n    img_w, img_h = img.shape[1], img.shape[0]\n    w, h = inp_dim\n    new_w = int(img_w * min(w / img_w, h / img_h))\n    new_h = int(img_h * min(w / img_w, h / img_h))\n    resized_image = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n\n    canvas = np.full((inp_dim[1], inp_dim[0], 3), 128)\n\n    canvas[(h - new_h) // 2:(h - new_h) // 2 + new_h, (w - new_w) // 2:(w - new_w) // 2 + new_w, :] = resized_image\n\n    return canvas\n\n\ndef prep_image(img, inp_dim):\n    """"""\n    Prepare image for inputting to the neural network.\n\n    Returns a Variable\n    """"""\n\n    orig_im = cv2.imread(img)\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n    img_ = img[:, :, ::-1].transpose((2, 0, 1)).copy()\n    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n    return img_, orig_im, dim\n\n\ndef prep_frame(img, inp_dim):\n    """"""\n    Prepare image for inputting to the neural network.\n\n    Returns a Variable\n    """"""\n\n    orig_im = img\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n    img_ = img[:, :, ::-1].transpose((2, 0, 1)).copy()\n    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n    return img_, orig_im, dim\n\n\ndef prep_image_pil(img, network_dim):\n    orig_im = Image.open(img)\n    img = orig_im.convert(\'RGB\')\n    dim = img.size\n    img = img.resize(network_dim)\n    img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes()))\n    img = img.view(*network_dim, 3).transpose(0, 1).transpose(0, 2).contiguous()\n    img = img.view(1, 3, *network_dim)\n    img = img.float().div(255.0)\n    return (img, orig_im, dim)\n\n\ndef inp_to_image(inp):\n    inp = inp.cpu().squeeze()\n    inp = inp * 255\n    try:\n        inp = inp.data.numpy()\n    except RuntimeError:\n        inp = inp.numpy()\n    inp = inp.transpose(1, 2, 0)\n\n    inp = inp[:, :, ::-1]\n    return inp\n'"
detector/yolo/util.py,42,"b'\nfrom __future__ import division\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F \nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nimport matplotlib.pyplot as plt\ntry:\n    from bbox import bbox_iou\nexcept ImportError:\n    from yolo.bbox import bbox_iou\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters())\n\ndef count_learnable_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef convert2cpu(matrix):\n    if matrix.is_cuda:\n        return torch.FloatTensor(matrix.size()).copy_(matrix)\n    else:\n        return matrix\n\ndef predict_transform(prediction, inp_dim, anchors, num_classes, args):\n    batch_size = prediction.size(0)\n    stride =  inp_dim // prediction.size(2)\n    grid_size = inp_dim // stride\n    bbox_attrs = 5 + num_classes\n    num_anchors = len(anchors)\n    \n    anchors = [(a[0]/stride, a[1]/stride) for a in anchors]\n\n\n\n    prediction = prediction.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size)\n    prediction = prediction.transpose(1,2).contiguous()\n    prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs)\n\n\n    #Sigmoid the  centre_X, centre_Y. and object confidencce\n    prediction[:,:,0] = torch.sigmoid(prediction[:,:,0])\n    prediction[:,:,1] = torch.sigmoid(prediction[:,:,1])\n    prediction[:,:,4] = torch.sigmoid(prediction[:,:,4])\n    \n\n    \n    #Add the center offsets\n    grid_len = np.arange(grid_size)\n    a,b = np.meshgrid(grid_len, grid_len)\n    \n    x_offset = torch.FloatTensor(a).view(-1,1)\n    y_offset = torch.FloatTensor(b).view(-1,1)\n    \n    if args:\n        x_offset = x_offset.to(args.device)\n        y_offset = y_offset.to(args.device)\n    else:\n        x_offset = x_offset.cuda()\n        y_offset = y_offset.cuda()\n    \n    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1,num_anchors).view(-1,2).unsqueeze(0)\n    \n    prediction[:,:,:2] += x_y_offset\n      \n    #log space transform height and the width\n    anchors = torch.FloatTensor(anchors)\n    \n    if args:\n        anchors = anchors.to(args.device)\n    else:\n        anchors = anchors.cuda()\n    \n    anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)\n    prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors\n\n    #Softmax the class scores\n    prediction[:,:,5: 5 + num_classes] = torch.sigmoid((prediction[:,:, 5 : 5 + num_classes]))\n\n    prediction[:,:,:4] *= stride\n   \n    \n    return prediction\n\ndef load_classes(namesfile):\n    fp = open(namesfile, ""r"")\n    names = fp.read().split(""\\n"")[:-1]\n    return names\n\ndef get_im_dim(im):\n    im = cv2.imread(im)\n    w,h = im.shape[1], im.shape[0]\n    return w,h\n\ndef unique(tensor):\n    tensor_np = tensor.cpu().numpy()\n    unique_np = np.unique(tensor_np)\n    unique_tensor = torch.from_numpy(unique_np)\n\n    tensor_res = tensor.new(unique_tensor.shape)\n    tensor_res.copy_(unique_tensor)\n    return tensor_res\n\n\ndef dynamic_write_results(prediction, confidence, num_classes, nms=True, nms_conf=0.4):\n    prediction_bak = prediction.clone()\n    dets = write_results(prediction.clone(), confidence, num_classes, nms, nms_conf)\n    if isinstance(dets, int):\n        return dets\n\n    if dets.shape[0] > 100:\n        nms_conf -= 0.05\n        dets = write_results(prediction_bak.clone(), confidence, num_classes, nms, nms_conf)\n\n    return dets\n\n\ndef write_results(prediction, confidence, num_classes, nms=True, nms_conf=0.4):\n    conf_mask = (prediction[:, :, 4] > confidence).float().float().unsqueeze(2)\n    prediction = prediction * conf_mask\n\n    try:\n        ind_nz = torch.nonzero(prediction[:,:,4]).transpose(0,1).contiguous()\n    except:\n        return 0\n\n    box_a = prediction.new(prediction.shape)\n    box_a[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)\n    box_a[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)\n    box_a[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2) \n    box_a[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)\n    prediction[:,:,:4] = box_a[:,:,:4]\n\n    batch_size = prediction.size(0)\n\n    output = prediction.new(1, prediction.size(2) + 1)\n    write = False\n    num = 0\n    for ind in range(batch_size):\n        #select the image from the batch\n        image_pred = prediction[ind]\n\n        #Get the class having maximum score, and the index of that class\n        #Get rid of num_classes softmax scores \n        #Add the class index and the class score of class having maximum score\n        max_conf, max_conf_score = torch.max(image_pred[:,5:5+ num_classes], 1)\n        max_conf = max_conf.float().unsqueeze(1)\n        max_conf_score = max_conf_score.float().unsqueeze(1)\n        seq = (image_pred[:,:5], max_conf, max_conf_score)\n        image_pred = torch.cat(seq, 1)\n\n        #Get rid of the zero entries\n        non_zero_ind =  (torch.nonzero(image_pred[:,4]))\n\n        image_pred_ = image_pred[non_zero_ind.squeeze(),:].view(-1,7)\n\n        #Get the various classes detected in the image\n        try:\n            img_classes = unique(image_pred_[:,-1])\n        except:\n            continue\n\n        #WE will do NMS classwise\n        #print(img_classes)\n        for cls in img_classes:\n            if cls != 0:\n                continue\n            #get the detections with one particular class\n            cls_mask = image_pred_*(image_pred_[:,-1] == cls).float().unsqueeze(1)\n            class_mask_ind = torch.nonzero(cls_mask[:,-2]).squeeze()\n\n            image_pred_class = image_pred_[class_mask_ind].view(-1,7)\n\n            #sort the detections such that the entry with the maximum objectness\n            #confidence is at the top\n            conf_sort_index = torch.sort(image_pred_class[:,4], descending = True )[1]\n            image_pred_class = image_pred_class[conf_sort_index]\n            idx = image_pred_class.size(0)\n\n            #if nms has to be done\n            if nms:\n                # Perform non-maximum suppression\n                max_detections = []\n                while image_pred_class.size(0):\n                    # Get detection with highest confidence and save as max detection\n                    max_detections.append(image_pred_class[0].unsqueeze(0))\n                    # Stop if we\'re at the last detection\n                    if len(image_pred_class) == 1:\n                        break\n                    # Get the IOUs for all boxes with lower confidence\n                    ious = bbox_iou(max_detections[-1], image_pred_class[1:])\n                    # Remove detections with IoU >= NMS threshold\n                    image_pred_class = image_pred_class[1:][ious < nms_conf]\n\n                image_pred_class = torch.cat(max_detections).data\n\n\n            #Concatenate the batch_id of the image to the detection\n            #this helps us identify which image does the detection correspond to \n            #We use a linear straucture to hold ALL the detections from the batch\n            #the batch_dim is flattened\n            #batch is identified by extra batch column\n\n            batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind)\n            seq = batch_ind, image_pred_class\n            if not write:\n                output = torch.cat(seq,1)\n                write = True\n            else:\n                out = torch.cat(seq,1)\n                output = torch.cat((output,out))\n            num += 1\n    \n    if not num:\n        return 0\n\n    return output\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Mar 24 00:12:16 2018\n\n@author: ayooshmac\n""""""\n\ndef predict_transform_half(prediction, inp_dim, anchors, num_classes, args):\n    batch_size = prediction.size(0)\n    stride =  inp_dim // prediction.size(2)\n\n    bbox_attrs = 5 + num_classes\n    num_anchors = len(anchors)\n    grid_size = inp_dim // stride\n\n    \n    prediction = prediction.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size)\n    prediction = prediction.transpose(1,2).contiguous()\n    prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs)\n    \n    \n    #Sigmoid the  centre_X, centre_Y. and object confidencce\n    prediction[:,:,0] = torch.sigmoid(prediction[:,:,0])\n    prediction[:,:,1] = torch.sigmoid(prediction[:,:,1])\n    prediction[:,:,4] = torch.sigmoid(prediction[:,:,4])\n\n    \n    #Add the center offsets\n    grid_len = np.arange(grid_size)\n    a,b = np.meshgrid(grid_len, grid_len)\n    \n    x_offset = torch.FloatTensor(a).view(-1,1)\n    y_offset = torch.FloatTensor(b).view(-1,1)\n    \n    if args:\n        x_offset = x_offset.to(args.device).half()\n        y_offset = y_offset.to(args.device).half()\n    else:\n        x_offset = x_offset.cuda().half()\n        y_offset = y_offset.cuda().half()\n    \n    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1,num_anchors).view(-1,2).unsqueeze(0)\n    \n    prediction[:,:,:2] += x_y_offset\n      \n    #log space transform height and the width\n    anchors = torch.HalfTensor(anchors)\n    \n    if args:\n        anchors = anchors.to(args.device)\n    else:\n        anchors = anchors.cuda()\n    \n    anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)\n    prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors\n\n    #Softmax the class scores\n    prediction[:,:,5: 5 + num_classes] = nn.Softmax(-1)(Variable(prediction[:,:, 5 : 5 + num_classes])).data\n\n    prediction[:,:,:4] *= stride\n    \n    \n    return prediction\n\n\ndef write_results_half(prediction, confidence, num_classes, nms = True, nms_conf = 0.4):\n    conf_mask = (prediction[:,:,4] > confidence).half().unsqueeze(2)\n    prediction = prediction*conf_mask\n    \n    try:\n        ind_nz = torch.nonzero(prediction[:,:,4]).transpose(0,1).contiguous()\n    except:\n        return 0\n    \n    \n    \n    box_a = prediction.new(prediction.shape)\n    box_a[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)\n    box_a[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)\n    box_a[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2) \n    box_a[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)\n    prediction[:,:,:4] = box_a[:,:,:4]\n    \n    \n    \n    batch_size = prediction.size(0)\n    \n    output = prediction.new(1, prediction.size(2) + 1)\n    write = False\n    \n    for ind in range(batch_size):\n        #select the image from the batch\n        image_pred = prediction[ind]\n\n        \n        #Get the class having maximum score, and the index of that class\n        #Get rid of num_classes softmax scores \n        #Add the class index and the class score of class having maximum score\n        max_conf, max_conf_score = torch.max(image_pred[:,5:5+ num_classes], 1)\n        max_conf = max_conf.half().unsqueeze(1)\n        max_conf_score = max_conf_score.half().unsqueeze(1)\n        seq = (image_pred[:,:5], max_conf, max_conf_score)\n        image_pred = torch.cat(seq, 1)\n        \n        \n        #Get rid of the zero entries\n        non_zero_ind =  (torch.nonzero(image_pred[:,4]))\n        try:\n            image_pred_ = image_pred[non_zero_ind.squeeze(),:]\n        except:\n            continue\n        \n        #Get the various classes detected in the image\n        img_classes = unique(image_pred_[:,-1].long()).half()\n        \n        \n        \n                \n        #WE will do NMS classwise\n        for cls in img_classes:\n            #get the detections with one particular class\n            cls_mask = image_pred_*(image_pred_[:,-1] == cls).half().unsqueeze(1)\n            class_mask_ind = torch.nonzero(cls_mask[:,-2]).squeeze()\n            \n\n            image_pred_class = image_pred_[class_mask_ind]\n\n        \n             #sort the detections such that the entry with the maximum objectness\n             #confidence is at the top\n            conf_sort_index = torch.sort(image_pred_class[:,4], descending = True )[1]\n            image_pred_class = image_pred_class[conf_sort_index]\n            idx = image_pred_class.size(0)\n            \n            #if nms has to be done\n            if nms:\n                #For each detection\n                for i in range(idx):\n                    #Get the IOUs of all boxes that come after the one we are looking at \n                    #in the loop\n                    try:\n                        ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i+1:])\n                    except ValueError:\n                        break\n        \n                    except IndexError:\n                        break\n                    \n                    #Zero out all the detections that have IoU > treshhold\n                    iou_mask = (ious < nms_conf).half().unsqueeze(1)\n                    image_pred_class[i+1:] *= iou_mask       \n                    \n                    #Remove the non-zero entries\n                    non_zero_ind = torch.nonzero(image_pred_class[:,4]).squeeze()\n                    image_pred_class = image_pred_class[non_zero_ind]\n                    \n                    \n            \n            #Concatenate the batch_id of the image to the detection\n            #this helps us identify which image does the detection correspond to \n            #We use a linear straucture to hold ALL the detections from the batch\n            #the batch_dim is flattened\n            #batch is identified by extra batch column\n            batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind)\n            seq = batch_ind, image_pred_class\n            \n            if not write:\n                output = torch.cat(seq,1)\n                write = True\n            else:\n                out = torch.cat(seq,1)\n                output = torch.cat((output,out))\n    \n    return output\n'"
detector/yolo/video_demo.py,11,"b'from __future__ import division\nimport time\nimport torch \nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nfrom util import *\nfrom darknet import Darknet\nfrom preprocess import prep_image, inp_to_image, letterbox_image\nimport pandas as pd\nimport random \nimport pickle as pkl\nimport argparse\n\n\ndef get_test_input(input_dim, CUDA):\n    img = cv2.imread(""dog-cycle-car.png"")\n    img = cv2.resize(img, (input_dim, input_dim)) \n    img_ =  img[:,:,::-1].transpose((2,0,1))\n    img_ = img_[np.newaxis,:,:,:]/255.0\n    img_ = torch.from_numpy(img_).float()\n    img_ = Variable(img_)\n    \n    if CUDA:\n        img_ = img_.cuda()\n    \n    return img_\n\ndef prep_image(img, inp_dim):\n    """"""\n    Prepare image for inputting to the neural network. \n    \n    Returns a Variable \n    """"""\n\n    orig_im = img\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n    return img_, orig_im, dim\n\ndef write(x, img):\n    c1 = tuple(x[1:3].int())\n    c2 = tuple(x[3:5].int())\n    cls = int(x[-1])\n    label = ""{0}"".format(classes[cls])\n    color = random.choice(colors)\n    cv2.rectangle(img, c1, c2,color, 1)\n    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n    cv2.rectangle(img, c1, c2,color, -1)\n    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n    return img\n\ndef arg_parse():\n    """"""\n    Parse arguements to the detect module\n    \n    """"""\n    \n    \n    parser = argparse.ArgumentParser(description=\'YOLO v3 Video Detection Module\')\n   \n    parser.add_argument(""--video"", dest = \'video\', help = \n                        ""Video to run detection upon"",\n                        default = ""video.avi"", type = str)\n    parser.add_argument(""--dataset"", dest = ""dataset"", help = ""Dataset on which the network has been trained"", default = ""pascal"")\n    parser.add_argument(""--confidence"", dest = ""confidence"", help = ""Object Confidence to filter predictions"", default = 0.5)\n    parser.add_argument(""--nms_thresh"", dest = ""nms_thresh"", help = ""NMS Threshhold"", default = 0.4)\n    parser.add_argument(""--cfg"", dest = \'cfgfile\', help = \n                        ""Config file"",\n                        default = ""cfg/yolov3-spp.cfg"", type = str)\n    parser.add_argument(""--weights"", dest = \'weightsfile\', help = \n                        ""weightsfile"",\n                        default = ""yolov3-spp.weights"", type = str)\n    parser.add_argument(""--reso"", dest = \'reso\', help = \n                        ""Input resolution of the network. Increase to increase accuracy. Decrease to increase speed"",\n                        default = ""416"", type = str)\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    args = arg_parse()\n    confidence = float(args.confidence)\n    nms_thesh = float(args.nms_thresh)\n    start = 0\n\n    CUDA = torch.cuda.is_available()\n\n    num_classes = 80\n\n    CUDA = torch.cuda.is_available()\n    \n    bbox_attrs = 5 + num_classes\n    \n    print(""Loading network....."")\n    model = Darknet(args.cfgfile)\n    model.load_weights(args.weightsfile)\n    print(""Network successfully loaded"")\n\n    model.net_info[""height""] = args.reso\n    inp_dim = int(model.net_info[""height""])\n    assert inp_dim % 32 == 0 \n    assert inp_dim > 32\n\n    if CUDA:\n        model.cuda()\n        \n    model(get_test_input(inp_dim, CUDA), CUDA)\n\n    model.eval()\n    \n    videofile = args.video\n    \n    cap = cv2.VideoCapture(videofile)\n    \n    assert cap.isOpened(), \'Cannot capture source\'\n    \n    frames = 0\n    start = time.time()    \n    while cap.isOpened():\n        \n        ret, frame = cap.read()\n        if ret:\n            \n\n            img, orig_im, dim = prep_image(frame, inp_dim)\n            \n            im_dim = torch.FloatTensor(dim).repeat(1,2)                        \n            \n            \n            if CUDA:\n                im_dim = im_dim.cuda()\n                img = img.cuda()\n            \n            with torch.no_grad():   \n                output = model(Variable(img), CUDA)\n            output = write_results(output, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n\n            if type(output) == int:\n                frames += 1\n                print(""FPS of the video is {:5.2f}"".format( frames / (time.time() - start)))\n                cv2.imshow(""frame"", orig_im)\n                key = cv2.waitKey(1)\n                if key & 0xFF == ord(\'q\'):\n                    break\n                continue\n            \n            \n\n            \n            im_dim = im_dim.repeat(output.size(0), 1)\n            scaling_factor = torch.min(inp_dim/im_dim,1)[0].view(-1,1)\n            \n            output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2\n            output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2\n            \n            output[:,1:5] /= scaling_factor\n    \n            for i in range(output.shape[0]):\n                output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])\n                output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])\n            \n            classes = load_classes(\'data/coco.names\')\n            colors = pkl.load(open(""pallete"", ""rb""))\n            \n            list(map(lambda x: write(x, orig_im), output))\n            \n            \n            cv2.imshow(""frame"", orig_im)\n            key = cv2.waitKey(1)\n            if key & 0xFF == ord(\'q\'):\n                break\n            frames += 1\n            print(""FPS of the video is {:5.2f}"".format( frames / (time.time() - start)))\n\n            \n        else:\n            break\n    \n\n    \n    \n\n'"
detector/yolo/video_demo_half.py,10,"b'from __future__ import division\nimport time\nimport torch \nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nfrom util import *\nfrom darknet import Darknet\nfrom preprocess import prep_image, inp_to_image, letterbox_image\nimport pandas as pd\nimport random \nimport pickle as pkl\nimport argparse\n\n\ndef get_test_input(input_dim, CUDA):\n    img = cv2.imread(""dog-cycle-car.png"")\n    img = cv2.resize(img, (input_dim, input_dim)) \n    img_ =  img[:,:,::-1].transpose((2,0,1))\n    img_ = img_[np.newaxis,:,:,:]/255.0\n    img_ = torch.from_numpy(img_).float()\n    img_ = Variable(img_)\n    \n    if CUDA:\n        img_ = img_.cuda()\n    \n    return img_\n\ndef prep_image(img, inp_dim):\n    """"""\n    Prepare image for inputting to the neural network. \n    \n    Returns a Variable \n    """"""\n\n    orig_im = img\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n    return img_, orig_im, dim\n\ndef write(x, img):\n    c1 = tuple(x[1:3].int())\n    c2 = tuple(x[3:5].int())\n    cls = int(x[-1])\n    label = ""{0}"".format(classes[cls])\n    color = random.choice(colors)\n    cv2.rectangle(img, c1, c2,color, 1)\n    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n    cv2.rectangle(img, c1, c2,color, -1)\n    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n    return img\n\ndef arg_parse():\n    """"""\n    Parse arguements to the detect module\n    \n    """"""\n    \n    \n    parser = argparse.ArgumentParser(description=\'YOLO v2 Video Detection Module\')\n   \n    parser.add_argument(""--video"", dest = \'video\', help = \n                        ""Video to run detection upon"",\n                        default = ""video.avi"", type = str)\n    parser.add_argument(""--dataset"", dest = ""dataset"", help = ""Dataset on which the network has been trained"", default = ""pascal"")\n    parser.add_argument(""--confidence"", dest = ""confidence"", help = ""Object Confidence to filter predictions"", default = 0.5)\n    parser.add_argument(""--nms_thresh"", dest = ""nms_thresh"", help = ""NMS Threshhold"", default = 0.4)\n    parser.add_argument(""--cfg"", dest = \'cfgfile\', help = \n                        ""Config file"",\n                        default = ""cfg/yolov3-spp.cfg"", type = str)\n    parser.add_argument(""--weights"", dest = \'weightsfile\', help = \n                        ""weightsfile"",\n                        default = ""yolov3-spp.weights"", type = str)\n    parser.add_argument(""--reso"", dest = \'reso\', help = \n                        ""Input resolution of the network. Increase to increase accuracy. Decrease to increase speed"",\n                        default = ""416"", type = str)\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    args = arg_parse()\n    confidence = float(args.confidence)\n    nms_thesh = float(args.nms_thresh)\n    start = 0\n\n    CUDA = torch.cuda.is_available()\n\n        \n\n    CUDA = torch.cuda.is_available()\n    num_classes = 80 \n    bbox_attrs = 5 + num_classes\n    \n    print(""Loading network....."")\n    model = Darknet(args.cfgfile)\n    model.load_weights(args.weightsfile)\n    print(""Network successfully loaded"")\n    \n    model.net_info[""height""] = args.reso\n    inp_dim = int(model.net_info[""height""])\n    assert inp_dim % 32 == 0 \n    assert inp_dim > 32\n\n    \n    if CUDA:\n        model.cuda().half()\n        \n    model(get_test_input(inp_dim, CUDA), CUDA)\n\n    model.eval()\n    \n    videofile = \'video.avi\'\n    \n    cap = cv2.VideoCapture(videofile)\n    \n    assert cap.isOpened(), \'Cannot capture source\'\n    \n    frames = 0\n    start = time.time()    \n    while cap.isOpened():\n        \n        ret, frame = cap.read()\n        if ret:\n            \n\n            img, orig_im, dim = prep_image(frame, inp_dim)\n            \n            im_dim = torch.FloatTensor(dim).repeat(1,2)                        \n            \n            \n            if CUDA:\n                img = img.cuda().half()\n                im_dim = im_dim.half().cuda()\n                write_results = write_results_half\n                predict_transform = predict_transform_half\n            \n            \n            output = model(Variable(img, volatile = True), CUDA)\n            output = write_results(output, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n\n           \n            if type(output) == int:\n                frames += 1\n                print(""FPS of the video is {:5.2f}"".format( frames / (time.time() - start)))\n                cv2.imshow(""frame"", orig_im)\n                key = cv2.waitKey(1)\n                if key & 0xFF == ord(\'q\'):\n                    break\n                continue\n\n        \n            im_dim = im_dim.repeat(output.size(0), 1)\n            scaling_factor = torch.min(inp_dim/im_dim,1)[0].view(-1,1)\n            \n            output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2\n            output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2\n            \n            output[:,1:5] /= scaling_factor\n    \n            for i in range(output.shape[0]):\n                output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])\n                output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])\n            \n            \n            classes = load_classes(\'data/coco.names\')\n            colors = pkl.load(open(""pallete"", ""rb""))\n            \n            list(map(lambda x: write(x, orig_im), output))\n            \n            \n            cv2.imshow(""frame"", orig_im)\n            key = cv2.waitKey(1)\n            if key & 0xFF == ord(\'q\'):\n                break\n            frames += 1\n            print(""FPS of the video is {:5.2f}"".format( frames / (time.time() - start)))\n\n            \n        else:\n            break\n    \n\n    \n    \n\n'"
alphapose/models/layers/DUC.py,1,"b""# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\nimport torch.nn as nn\n\n\nclass DUC(nn.Module):\n    '''\n    Initialize: inplanes, planes, upscale_factor\n    OUTPUT: (planes // upscale_factor^2) * ht * wd\n    '''\n\n    def __init__(self, inplanes, planes,\n                 upscale_factor=2, norm_layer=nn.BatchNorm2d):\n        super(DUC, self).__init__()\n        self.conv = nn.Conv2d(\n            inplanes, planes, kernel_size=3, padding=1, bias=False)\n        self.bn = norm_layer(planes, momentum=0.1)\n        self.relu = nn.ReLU(inplace=True)\n        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.pixel_shuffle(x)\n        return x\n"""
alphapose/models/layers/PixelUnshuffle.py,1,"b""# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\nimport torch.nn as nn\n\n\nclass PixelUnshuffle(nn.Module):\n    '''\n    Initialize: inplanes, planes, upscale_factor\n    OUTPUT: (planes // upscale_factor^2) * ht * wd\n    '''\n\n    def __init__(self, downscale_factor=2):\n        super(PixelUnshuffle, self).__init__()\n        self._r = downscale_factor\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n        out_c = c * (self._r * self._r)\n        out_h = h // self._r\n        out_w = w // self._r\n\n        x_view = x.contiguous().view(b, c, out_h, self._r, out_w, self._r)\n        x_prime = x_view.permute(0, 1, 3, 5, 2, 4).contiguous().view(b, out_c, out_h, out_w)\n\n        return x_prime\n"""
alphapose/models/layers/Resnet.py,2,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .dcn import DeformConv, ModulatedDeformConv\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError(\'BasicBlock only supports groups=1 and base_width=64\')\n        if dilation > 1:\n            raise NotImplementedError(""Dilation > 1 not supported in BasicBlock"")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1,\n                 downsample=None, norm_layer=nn.BatchNorm2d, dcn=None):\n        super(Bottleneck, self).__init__()\n        self.dcn = dcn\n        self.with_dcn = dcn is not None\n\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(planes, momentum=0.1)\n        if self.with_dcn:\n            fallback_on_stride = dcn.get(\'FALLBACK_ON_STRIDE\', False)\n            self.with_modulated_dcn = dcn.get(\'MODULATED\', False)\n        if not self.with_dcn or fallback_on_stride:\n            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                                   padding=1, bias=False)\n        else:\n            self.deformable_groups = dcn.get(\'DEFORM_GROUP\', 1)\n            if not self.with_modulated_dcn:\n                conv_op = DeformConv\n                offset_channels = 18\n            else:\n                conv_op = ModulatedDeformConv\n                offset_channels = 27\n\n            self.conv2_offset = nn.Conv2d(\n                planes,\n                self.deformable_groups * offset_channels,\n                kernel_size=3,\n                stride=stride,\n                padding=1)\n            self.conv2 = conv_op(\n                planes,\n                planes,\n                kernel_size=3,\n                stride=stride,\n                padding=1,\n                deformable_groups=self.deformable_groups,\n                bias=False)\n\n        self.bn2 = norm_layer(planes, momentum=0.1)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes * 4, momentum=0.1)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n        if not self.with_dcn:\n            out = F.relu(self.bn2(self.conv2(out)), inplace=True)\n        elif self.with_modulated_dcn:\n            offset_mask = self.conv2_offset(out)\n            offset = offset_mask[:, :18 * self.deformable_groups, :, :]\n            mask = offset_mask[:, -9 * self.deformable_groups:, :, :]\n            mask = mask.sigmoid()\n            out = F.relu(self.bn2(self.conv2(out, offset, mask)))\n        else:\n            offset = self.conv2_offset(out)\n            out = F.relu(self.bn2(self.conv2(out, offset)), inplace=True)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = F.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    """""" ResNet """"""\n\n    def __init__(self, architecture, norm_layer=nn.BatchNorm2d, dcn=None, stage_with_dcn=(False, False, False, False)):\n        super(ResNet, self).__init__()\n        self._norm_layer = norm_layer\n        assert architecture in [""resnet18"", ""resnet50"", ""resnet101"", \'resnet152\']\n        layers = {\n            \'resnet18\': [2, 2, 2, 2],\n            \'resnet34\': [3, 4, 6, 3],\n            \'resnet50\': [3, 4, 6, 3],\n            \'resnet101\': [3, 4, 23, 3],\n            \'resnet152\': [3, 8, 36, 3],\n        }\n        self.inplanes = 64\n        if architecture == ""resnet18"" or architecture == \'resnet34\':\n            self.block = BasicBlock\n        else:\n            self.block = Bottleneck\n        self.layers = layers[architecture]\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7,\n                               stride=2, padding=3, bias=False)\n        self.bn1 = norm_layer(64, eps=1e-5, momentum=0.1, affine=True)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        stage_dcn = [dcn if with_dcn else None for with_dcn in stage_with_dcn]\n\n        self.layer1 = self.make_layer(\n            self.block, 64, self.layers[0], dcn=stage_dcn[0])\n        self.layer2 = self.make_layer(\n            self.block, 128, self.layers[1], stride=2, dcn=stage_dcn[1])\n        self.layer3 = self.make_layer(\n            self.block, 256, self.layers[2], stride=2, dcn=stage_dcn[2])\n\n        self.layer4 = self.make_layer(\n            self.block, 512, self.layers[3], stride=2, dcn=stage_dcn[3])\n\n    def forward(self, x):\n        x = self.maxpool(self.relu(self.bn1(self.conv1(x))))  # 64 * h/4 * w/4\n        x = self.layer1(x)  # 256 * h/4 * w/4\n        x = self.layer2(x)  # 512 * h/8 * w/8\n        x = self.layer3(x)  # 1024 * h/16 * w/16\n        x = self.layer4(x)  # 2048 * h/32 * w/32\n        return x\n\n    def stages(self):\n        return [self.layer1, self.layer2, self.layer3, self.layer4]\n\n    def make_layer(self, block, planes, blocks, stride=1, dcn=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                self._norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample,\n                            norm_layer=self._norm_layer, dcn=dcn))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes,\n                                norm_layer=self._norm_layer, dcn=dcn))\n\n        return nn.Sequential(*layers)\n'"
alphapose/models/layers/SE_Resnet.py,2,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .dcn import DeformConv, ModulatedDeformConv\nfrom .SE_module import SELayer\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 reduction=False, norm_layer=nn.BatchNorm2d):\n        super(BasicBlock, self).__init__()\n\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n        if reduction:\n            self.se = SELayer(planes)\n        self.reduc = reduction\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.reduc:\n            out = self.se(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1,\n                 downsample=None, reduction=False,\n                 norm_layer=nn.BatchNorm2d,\n                 dcn=None):\n        super(Bottleneck, self).__init__()\n        self.dcn = dcn\n        self.with_dcn = dcn is not None\n\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(planes, momentum=0.1)\n        if self.with_dcn:\n            fallback_on_stride = dcn.get(\'FALLBACK_ON_STRIDE\', False)\n            self.with_modulated_dcn = dcn.get(\'MODULATED\', False)\n        if not self.with_dcn or fallback_on_stride:\n            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                                   padding=1, bias=False)\n        else:\n            self.deformable_groups = dcn.get(\'DEFORM_GROUP\', 1)\n            if not self.with_modulated_dcn:\n                conv_op = DeformConv\n                offset_channels = 18\n            else:\n                conv_op = ModulatedDeformConv\n                offset_channels = 27\n\n            self.conv2_offset = nn.Conv2d(\n                planes,\n                self.deformable_groups * offset_channels,\n                kernel_size=3,\n                stride=stride,\n                padding=1)\n            self.conv2 = conv_op(\n                planes,\n                planes,\n                kernel_size=3,\n                stride=stride,\n                padding=1,\n                deformable_groups=self.deformable_groups,\n                bias=False)\n\n        self.bn2 = norm_layer(planes, momentum=0.1)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes * 4, momentum=0.1)\n        if reduction:\n            self.se = SELayer(planes * 4)\n\n        self.reduc = reduction\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n        if not self.with_dcn:\n            out = F.relu(self.bn2(self.conv2(out)), inplace=True)\n        elif self.with_modulated_dcn:\n            offset_mask = self.conv2_offset(out)\n            offset = offset_mask[:, :18 * self.deformable_groups, :, :]\n            mask = offset_mask[:, -9 * self.deformable_groups:, :, :]\n            mask = mask.sigmoid()\n            out = F.relu(self.bn2(self.conv2(out, offset, mask)))\n        else:\n            offset = self.conv2_offset(out)\n            out = F.relu(self.bn2(self.conv2(out, offset)), inplace=True)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.reduc:\n            out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = F.relu(out)\n\n        return out\n\n\nclass SEResnet(nn.Module):\n    """""" SEResnet """"""\n\n    def __init__(self, architecture, norm_layer=nn.BatchNorm2d,\n                 dcn=None, stage_with_dcn=(False, False, False, False)):\n        super(SEResnet, self).__init__()\n        self._norm_layer = norm_layer\n        assert architecture in [""resnet18"", ""resnet50"", ""resnet101"", \'resnet152\']\n        layers = {\n            \'resnet18\': [2, 2, 2, 2],\n            \'resnet34\': [3, 4, 6, 3],\n            \'resnet50\': [3, 4, 6, 3],\n            \'resnet101\': [3, 4, 23, 3],\n            \'resnet152\': [3, 8, 36, 3],\n        }\n        self.inplanes = 64\n        if architecture == ""resnet18"" or architecture == \'resnet34\':\n            self.block = BasicBlock\n        else:\n            self.block = Bottleneck\n        self.layers = layers[architecture]\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7,\n                               stride=2, padding=3, bias=False)\n        self.bn1 = norm_layer(64, eps=1e-5, momentum=0.1, affine=True)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        stage_dcn = [dcn if with_dcn else None for with_dcn in stage_with_dcn]\n\n        self.layer1 = self.make_layer(self.block, 64, self.layers[0], dcn=stage_dcn[0])\n        self.layer2 = self.make_layer(\n            self.block, 128, self.layers[1], stride=2, dcn=stage_dcn[1])\n        self.layer3 = self.make_layer(\n            self.block, 256, self.layers[2], stride=2, dcn=stage_dcn[2])\n\n        self.layer4 = self.make_layer(\n            self.block, 512, self.layers[3], stride=2, dcn=stage_dcn[3])\n\n    def forward(self, x):\n        x = self.maxpool(self.relu(self.bn1(self.conv1(x))))  # 64 * h/4 * w/4\n        x = self.layer1(x)  # 256 * h/4 * w/4\n        x = self.layer2(x)  # 512 * h/8 * w/8\n        x = self.layer3(x)  # 1024 * h/16 * w/16\n        x = self.layer4(x)  # 2048 * h/32 * w/32\n        return x\n\n    def stages(self):\n        return [self.layer1, self.layer2, self.layer3, self.layer4]\n\n    def make_layer(self, block, planes, blocks, stride=1, dcn=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                self._norm_layer(planes * block.expansion, momentum=0.1),\n            )\n\n        layers = []\n        if downsample is not None:\n            layers.append(block(self.inplanes, planes,\n                                stride, downsample, reduction=True,\n                                norm_layer=self._norm_layer, dcn=dcn))\n        else:\n            layers.append(block(self.inplanes, planes, stride, downsample,\n                                norm_layer=self._norm_layer, dcn=dcn))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes,\n                                norm_layer=self._norm_layer, dcn=dcn))\n\n        return nn.Sequential(*layers)\n'"
alphapose/models/layers/SE_module.py,0,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\nfrom torch import nn\n\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=1):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n'"
alphapose/models/layers/ShuffleResnet.py,2,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .dcn import DCN\nfrom .PixelUnshuffle import PixelUnshuffle\nfrom .SE_module import SELayer\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 reduction=False, norm_layer=nn.BatchNorm2d):\n        super(BasicBlock, self).__init__()\n\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n        if reduction:\n            self.se = SELayer(planes)\n        self.reduc = reduction\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.reduc:\n            out = self.se(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1,\n                 downsample=None, reduction=False,\n                 norm_layer=nn.BatchNorm2d, dcn=None):\n        super(Bottleneck, self).__init__()\n        self.dcn = dcn\n        self.with_dcn = dcn is not None\n\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(planes, momentum=0.1)\n        if self.with_dcn:\n            fallback_on_stride = dcn.get(\'FALLBACK_ON_STRIDE\', False)\n            self.with_modulated_dcn = dcn.get(\'MODULATED\', False)\n\n        if stride > 1:\n            conv_layers = []\n            conv_layers.append(PixelUnshuffle(stride))\n            if not self.with_dcn or fallback_on_stride:\n                conv_layers.append(nn.Conv2d(planes * 4, planes, kernel_size=3, stride=1,\n                                             padding=1, bias=False))\n            else:\n                conv_layers.append(DCN(planes * 4, planes, dcn, kernel_size=3, stride=1,\n                                       padding=1, bias=False))\n            self.conv2 = nn.Sequential(*conv_layers)\n        else:\n            if not self.with_dcn or fallback_on_stride:\n                self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                                       padding=1, bias=False)\n            else:\n                self.conv2 = DCN(planes, planes, dcn, kernel_size=3, stride=stride,\n                                 padding=1, bias=False)\n\n        self.bn2 = norm_layer(planes, momentum=0.1)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes * 4, momentum=0.1)\n        if reduction:\n            self.se = SELayer(planes * 4)\n\n        self.reduc = reduction\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = F.relu(self.bn1(self.conv1(x)), inplace=True)\n        out = F.relu(self.bn2(self.conv2(out)), inplace=True)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.reduc:\n            out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = F.relu(out, inplace=True)\n\n        return out\n\n\nclass ShuffleResnet(nn.Module):\n    """""" ShuffleResnet """"""\n\n    def __init__(self, architecture, norm_layer=nn.BatchNorm2d, dcn=None, stage_with_dcn=(False, False, False, False)):\n        super(ShuffleResnet, self).__init__()\n        self._norm_layer = norm_layer\n        assert architecture in [""resnet18"", ""resnet50"", ""resnet101"", \'resnet152\']\n        layers = {\n            \'resnet18\': [2, 2, 2, 2],\n            \'resnet34\': [3, 4, 6, 3],\n            \'resnet50\': [3, 4, 6, 3],\n            \'resnet101\': [3, 4, 23, 3],\n            \'resnet152\': [3, 8, 36, 3],\n        }\n        self.inplanes = 64\n        if architecture == ""resnet18"" or architecture == \'resnet34\':\n            self.block = BasicBlock\n        else:\n            self.block = Bottleneck\n        self.layers = layers[architecture]\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7,\n                               stride=2, padding=3, bias=False)\n        self.bn1 = norm_layer(64, eps=1e-5, momentum=0.1, affine=True)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        stage_dcn = [dcn if with_dcn else None for with_dcn in stage_with_dcn]\n\n        self.layer1 = self.make_layer(\n            self.block, 64, self.layers[0], dcn=stage_dcn[0])\n        self.layer2 = self.make_layer(\n            self.block, 128, self.layers[1], stride=2, dcn=stage_dcn[1])\n        self.layer3 = self.make_layer(\n            self.block, 256, self.layers[2], stride=2, dcn=stage_dcn[2])\n\n        self.layer4 = self.make_layer(\n            self.block, 512, self.layers[3], stride=2, dcn=stage_dcn[3])\n\n    def forward(self, x):\n        x = self.maxpool(self.relu(self.bn1(self.conv1(x))))  # 64 * h/4 * w/4\n        x = self.layer1(x)  # 256 * h/4 * w/4\n        x = self.layer2(x)  # 512 * h/8 * w/8\n        x = self.layer3(x)  # 1024 * h/16 * w/16\n        x = self.layer4(x)  # 2048 * h/32 * w/32\n        return x\n\n    def stages(self):\n        return [self.layer1, self.layer2, self.layer3, self.layer4]\n\n    def make_layer(self, block, planes, blocks, stride=1, dcn=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                self._norm_layer(planes * block.expansion, momentum=0.1),\n            )\n\n        layers = []\n        if downsample is not None:\n            layers.append(block(self.inplanes, planes,\n                                stride, downsample, reduction=True,\n                                norm_layer=self._norm_layer, dcn=dcn))\n        else:\n            layers.append(block(self.inplanes, planes, stride, downsample,\n                                norm_layer=self._norm_layer, dcn=dcn))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes,\n                                norm_layer=self._norm_layer, dcn=dcn))\n\n        return nn.Sequential(*layers)\n'"
alphapose/utils/presets/__init__.py,0,"b""from .simple_transform import SimpleTransform\n\n__all__ = ['SimpleTransform']\n"""
alphapose/utils/presets/simple_transform.py,2,"b'# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\nimport platform\nimport random\n\nimport cv2\nimport numpy as np\nimport torch\n\nfrom ..bbox import (_box_to_center_scale, _center_scale_to_box,\n                    _clip_aspect_ratio)\nfrom ..transforms import (addDPG, affine_transform, flip_joints_3d,\n                          get_affine_transform, im_to_torch)\n\n# Only windows visual studio 2013 ~2017 support compile c/cuda extensions\n# If you force to compile extension on Windows and ensure appropriate visual studio\n# is intalled, you can try to use these ext_modules.\nif platform.system() != \'Windows\':\n    from ..roi_align import RoIAlign\n\n\nclass SimpleTransform(object):\n    """"""Generation of cropped input person and pose heatmaps from SimplePose.\n\n    Parameters\n    ----------\n    img: torch.Tensor\n        A tensor with shape: `(3, h, w)`.\n    label: dict\n        A dictionary with 4 keys:\n            `bbox`: [xmin, ymin, xmax, ymax]\n            `joints_3d`: numpy.ndarray with shape: (n_joints, 2),\n                    including position and visible flag\n            `width`: image width\n            `height`: image height\n    dataset:\n        The dataset to be transformed, must include `joint_pairs` property for flipping.\n    scale_factor: int\n        Scale augmentation.\n    input_size: tuple\n        Input image size, as (height, width).\n    output_size: tuple\n        Heatmap size, as (height, width).\n    rot: int\n        Ratation augmentation.\n    train: bool\n        True for training trasformation.\n    """"""\n\n    def __init__(self, dataset, scale_factor, add_dpg,\n                 input_size, output_size, rot, sigma,\n                 train, gpu_device=None):\n        self._joint_pairs = dataset.joint_pairs\n        self._scale_factor = scale_factor\n        self._rot = rot\n        self._add_dpg = add_dpg\n        self._gpu_device = gpu_device\n\n        self._input_size = input_size\n        self._heatmap_size = output_size\n\n        self._sigma = sigma\n        self._train = train\n        self._aspect_ratio = float(input_size[1]) / input_size[0]  # w / h\n        self._feat_stride = np.array(input_size) / np.array(output_size)\n\n        self.pixel_std = 1\n\n        if train:\n            self.num_joints_half_body = dataset.num_joints_half_body\n            self.prob_half_body = dataset.prob_half_body\n\n            self.upper_body_ids = dataset.upper_body_ids\n            self.lower_body_ids = dataset.lower_body_ids\n        if platform.system() != \'Windows\':\n            self.roi_align = RoIAlign(self._input_size, sample_num=-1)\n            if gpu_device is not None:\n                self.roi_align = self.roi_align.to(gpu_device)\n\n    def test_transform(self, src, bbox):\n        xmin, ymin, xmax, ymax = bbox\n        center, scale = _box_to_center_scale(\n            xmin, ymin, xmax - xmin, ymax - ymin, self._aspect_ratio)\n        scale = scale * 1.0\n\n        input_size = self._input_size\n        inp_h, inp_w = input_size\n\n        trans = get_affine_transform(center, scale, 0, [inp_w, inp_h])\n        img = cv2.warpAffine(src, trans, (int(inp_w), int(inp_h)), flags=cv2.INTER_LINEAR)\n        bbox = _center_scale_to_box(center, scale)\n\n        img = im_to_torch(img)\n        img[0].add_(-0.406)\n        img[1].add_(-0.457)\n        img[2].add_(-0.480)\n\n        return img, bbox\n\n    def align_transform(self, image, boxes):\n        """"""\n        Performs Region of Interest (RoI) Align operator described in Mask R-CNN\n\n        Arguments:\n            input (ndarray [H, W, 3]): input images\n            boxes (Tensor[K, 4]): the box coordinates in (x1, y1, x2, y2)\n                format where the regions will be taken from.\n\n        Returns:\n            cropped_img (Tensor[K, C, output_size[0], output_size[1]])\n            boxes (Tensor[K, 4]): new box coordinates\n        """"""\n        tensor_img = im_to_torch(image)\n        tensor_img[0].add_(-0.406)\n        tensor_img[1].add_(-0.457)\n        tensor_img[2].add_(-0.480)\n\n        new_boxes = _clip_aspect_ratio(boxes, self._aspect_ratio)\n        cropped_img = self.roi_align(tensor_img.unsqueeze(0).to(self._gpu_device), new_boxes.to(self._gpu_device))\n        return cropped_img, new_boxes[:, 1:]\n\n    def _target_generator(self, joints_3d, num_joints):\n        target_weight = np.ones((num_joints, 1), dtype=np.float32)\n        target_weight[:, 0] = joints_3d[:, 0, 1]\n        target = np.zeros((num_joints, self._heatmap_size[0], self._heatmap_size[1]),\n                          dtype=np.float32)\n        tmp_size = self._sigma * 3\n\n        for i in range(num_joints):\n            mu_x = int(joints_3d[i, 0, 0] / self._feat_stride[0] + 0.5)\n            mu_y = int(joints_3d[i, 1, 0] / self._feat_stride[1] + 0.5)\n            # check if any part of the gaussian is in-bounds\n            ul = [int(mu_x - tmp_size), int(mu_y - tmp_size)]\n            br = [int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)]\n            if (ul[0] >= self._heatmap_size[1] or ul[1] >= self._heatmap_size[0] or br[0] < 0 or br[1] < 0):\n                # return image as is\n                target_weight[i] = 0\n                continue\n\n            # generate gaussian\n            size = 2 * tmp_size + 1\n            x = np.arange(0, size, 1, np.float32)\n            y = x[:, np.newaxis]\n            x0 = y0 = size // 2\n            # the gaussian is not normalized, we want the center value to be equal to 1\n            g = np.exp(-((x - x0) ** 2 + (y - y0) ** 2) / (2 * (self._sigma ** 2)))\n\n            # usable gaussian range\n            g_x = max(0, -ul[0]), min(br[0], self._heatmap_size[1]) - ul[0]\n            g_y = max(0, -ul[1]), min(br[1], self._heatmap_size[0]) - ul[1]\n            # image range\n            img_x = max(0, ul[0]), min(br[0], self._heatmap_size[1])\n            img_y = max(0, ul[1]), min(br[1], self._heatmap_size[0])\n\n            v = target_weight[i]\n            if v > 0.5:\n                target[i, img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n\n        return target, np.expand_dims(target_weight, -1)\n\n    def __call__(self, src, label):\n        bbox = list(label[\'bbox\'])\n        gt_joints = label[\'joints_3d\']\n\n        imgwidth, imght = label[\'width\'], label[\'height\']\n        assert imgwidth == src.shape[1] and imght == src.shape[0]\n        self.num_joints = gt_joints.shape[0]\n\n        joints_vis = np.zeros((self.num_joints, 1), dtype=np.float32)\n        joints_vis[:, 0] = gt_joints[:, 0, 1]\n\n        input_size = self._input_size\n\n        if self._add_dpg and self._train:\n            bbox = addDPG(bbox, imgwidth, imght)\n\n        xmin, ymin, xmax, ymax = bbox\n        center, scale = _box_to_center_scale(\n            xmin, ymin, xmax - xmin, ymax - ymin, self._aspect_ratio)\n\n        # half body transform\n        if self._train and (np.sum(joints_vis[:, 0]) > self.num_joints_half_body and np.random.rand() < self.prob_half_body):\n            c_half_body, s_half_body = self.half_body_transform(\n                gt_joints[:, :, 0], joints_vis\n            )\n\n            if c_half_body is not None and s_half_body is not None:\n                center, scale = c_half_body, s_half_body\n\n        # rescale\n        if self._train:\n            sf = self._scale_factor\n            scale = scale * np.clip(np.random.randn() * sf + 1, 1 - sf, 1 + sf)\n        else:\n            scale = scale * 1.0\n\n        # rotation\n        if self._train:\n            rf = self._rot\n            r = np.clip(np.random.randn() * rf, -rf * 2, rf * 2) if random.random() <= 0.6 else 0\n        else:\n            r = 0\n\n        joints = gt_joints\n        if random.random() > 0.5 and self._train:\n            # src, fliped = random_flip_image(src, px=0.5, py=0)\n            # if fliped[0]:\n            assert src.shape[2] == 3\n            src = src[:, ::-1, :]\n\n            joints = flip_joints_3d(joints, imgwidth, self._joint_pairs)\n            center[0] = imgwidth - center[0] - 1\n\n        inp_h, inp_w = input_size\n        trans = get_affine_transform(center, scale, r, [inp_w, inp_h])\n        img = cv2.warpAffine(src, trans, (int(inp_w), int(inp_h)), flags=cv2.INTER_LINEAR)\n\n        # deal with joints visibility\n        for i in range(self.num_joints):\n            if joints[i, 0, 1] > 0.0:\n                joints[i, 0:2, 0] = affine_transform(joints[i, 0:2, 0], trans)\n\n        # generate training targets\n        target, target_weight = self._target_generator(joints, self.num_joints)\n        bbox = _center_scale_to_box(center, scale)\n\n        img = im_to_torch(img)\n        img[0].add_(-0.406)\n        img[1].add_(-0.457)\n        img[2].add_(-0.480)\n\n        return img, torch.from_numpy(target), torch.from_numpy(target_weight), torch.Tensor(bbox)\n\n    def half_body_transform(self, joints, joints_vis):\n        upper_joints = []\n        lower_joints = []\n        for joint_id in range(self.num_joints):\n            if joints_vis[joint_id][0] > 0:\n                if joint_id in self.upper_body_ids:\n                    upper_joints.append(joints[joint_id])\n                else:\n                    lower_joints.append(joints[joint_id])\n\n        if np.random.randn() < 0.5 and len(upper_joints) > 2:\n            selected_joints = upper_joints\n        else:\n            selected_joints = lower_joints \\\n                if len(lower_joints) > 2 else upper_joints\n\n        if len(selected_joints) < 2:\n            return None, None\n\n        selected_joints = np.array(selected_joints, dtype=np.float32)\n        center = selected_joints.mean(axis=0)[:2]\n\n        left_top = np.amin(selected_joints, axis=0)\n        right_bottom = np.amax(selected_joints, axis=0)\n\n        w = right_bottom[0] - left_top[0]\n        h = right_bottom[1] - left_top[1]\n\n        if w > self._aspect_ratio * h:\n            h = w * 1.0 / self._aspect_ratio\n        elif w < self._aspect_ratio * h:\n            w = h * self._aspect_ratio\n\n        scale = np.array(\n            [\n                w * 1.0 / self.pixel_std,\n                h * 1.0 / self.pixel_std\n            ],\n            dtype=np.float32\n        )\n\n        scale = scale * 1.5\n\n        return center, scale\n'"
alphapose/utils/roi_align/__init__.py,0,"b""from .roi_align import roi_align, RoIAlign\n\n__all__ = ['roi_align', 'RoIAlign']\n"""
alphapose/utils/roi_align/roi_align.py,4,"b""import torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.nn.modules.utils import _pair\n\nfrom . import roi_align_cuda\n\n\nclass RoIAlignFunction(Function):\n\n    @staticmethod\n    def forward(ctx, features, rois, out_size, spatial_scale, sample_num=0):\n        out_h, out_w = _pair(out_size)\n        assert isinstance(out_h, int) and isinstance(out_w, int)\n        ctx.spatial_scale = spatial_scale\n        ctx.sample_num = sample_num\n        ctx.save_for_backward(rois)\n        ctx.feature_size = features.size()\n\n        batch_size, num_channels, data_height, data_width = features.size()\n        num_rois = rois.size(0)\n\n        output = features.new_zeros(num_rois, num_channels, out_h, out_w)\n        if features.is_cuda:\n            roi_align_cuda.forward(features, rois, out_h, out_w, spatial_scale,\n                                   sample_num, output)\n        else:\n            raise NotImplementedError\n\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        feature_size = ctx.feature_size\n        spatial_scale = ctx.spatial_scale\n        sample_num = ctx.sample_num\n        rois = ctx.saved_tensors[0]\n        assert (feature_size is not None and grad_output.is_cuda)\n\n        batch_size, num_channels, data_height, data_width = feature_size\n        out_w = grad_output.size(3)\n        out_h = grad_output.size(2)\n\n        grad_input = grad_rois = None\n        if ctx.needs_input_grad[0]:\n            grad_input = rois.new_zeros(batch_size, num_channels, data_height,\n                                        data_width)\n            roi_align_cuda.backward(grad_output.contiguous(), rois, out_h,\n                                    out_w, spatial_scale, sample_num,\n                                    grad_input)\n\n        return grad_input, grad_rois, None, None, None\n\n\nroi_align = RoIAlignFunction.apply\n\n\nclass RoIAlign(nn.Module):\n\n    def __init__(self,\n                 out_size,\n                 spatial_scale=1,\n                 sample_num=0,\n                 use_torchvision=False):\n        super(RoIAlign, self).__init__()\n\n        self.out_size = out_size\n        self.spatial_scale = float(spatial_scale)\n        self.sample_num = int(sample_num)\n        self.use_torchvision = use_torchvision\n\n    def forward(self, features, rois):\n        if self.use_torchvision:\n            from torchvision.ops import roi_align as tv_roi_align\n            return tv_roi_align(features, rois, _pair(self.out_size),\n                                self.spatial_scale, self.sample_num)\n        else:\n            return roi_align(features, rois, self.out_size, self.spatial_scale,\n                             self.sample_num)\n\n    def __repr__(self):\n        format_str = self.__class__.__name__\n        format_str += '(out_size={}, spatial_scale={}, sample_num={}'.format(\n            self.out_size, self.spatial_scale, self.sample_num)\n        format_str += ', use_torchvision={})'.format(self.use_torchvision)\n        return format_str\n"""
detector/efficientdet/effdet/__init__.py,0,"b'from .efficientdet import EfficientDet\nfrom .bench import DetBenchEval, DetBenchTrain\nfrom .config.config import get_efficientdet_config\nfrom .helpers import load_checkpoint, load_pretrained'"
detector/efficientdet/effdet/anchors.py,11,"b'"""""" RetinaNet / EfficientDet Anchor Gen\n\nAdapted for PyTorch from Tensorflow impl at\n    https://github.com/google/automl/blob/6f6694cec1a48cdb33d5d1551a2d5db8ad227798/efficientdet/anchors.py\n\nHacked together by Ross Wightman, original copyright below\n""""""\n# Copyright 2020 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Anchor definition.\n\nThis module is borrowed from TPU RetinaNet implementation:\nhttps://github.com/tensorflow/tpu/blob/master/models/official/retinanet/anchors.py\n""""""\nimport collections\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torchvision.ops.boxes import batched_nms\n\nfrom .object_detection import argmax_matcher\nfrom .object_detection import box_list\nfrom .object_detection import faster_rcnn_box_coder\nfrom .object_detection import region_similarity_calculator\nfrom .object_detection import target_assigner\n\n# The minimum score to consider a logit for identifying detections.\nMIN_CLASS_SCORE = -5.0\n\n# The score for a dummy detection\n_DUMMY_DETECTION_SCORE = -1e5\n\n# The maximum number of (anchor,class) pairs to keep for non-max suppression.\nMAX_DETECTION_POINTS = 5000\n\n# The maximum number of detections per image.\nMAX_DETECTIONS_PER_IMAGE = 100\n\n\ndef decode_box_outputs(rel_codes, anchors, output_xyxy=False):\n    """"""Transforms relative regression coordinates to absolute positions.\n\n    Network predictions are normalized and relative to a given anchor; this\n    reverses the transformation and outputs absolute coordinates for the input image.\n\n    Args:\n        rel_codes: box regression targets.\n\n        anchors: anchors on all feature levels.\n\n    Returns:\n        outputs: bounding boxes.\n\n    """"""\n    ycenter_a = (anchors[0] + anchors[2]) / 2\n    xcenter_a = (anchors[1] + anchors[3]) / 2\n    ha = anchors[2] - anchors[0]\n    wa = anchors[3] - anchors[1]\n    ty, tx, th, tw = rel_codes\n\n    w = torch.exp(tw) * wa\n    h = torch.exp(th) * ha\n    ycenter = ty * ha + ycenter_a\n    xcenter = tx * wa + xcenter_a\n    ymin = ycenter - h / 2.\n    xmin = xcenter - w / 2.\n    ymax = ycenter + h / 2.\n    xmax = xcenter + w / 2.\n    if output_xyxy:\n        out = torch.stack([xmin, ymin, xmax, ymax], dim=1)\n    else:\n        out = torch.stack([ymin, xmin, ymax, xmax], dim=1)\n    return out\n\n\ndef _generate_anchor_configs(min_level, max_level, num_scales, aspect_ratios):\n    """"""Generates mapping from output level to a list of anchor configurations.\n\n    A configuration is a tuple of (num_anchors, scale, aspect_ratio).\n\n    Args:\n        min_level: integer number of minimum level of the output feature pyramid.\n\n        max_level: integer number of maximum level of the output feature pyramid.\n\n        num_scales: integer number representing intermediate scales added on each level.\n            For instances, num_scales=2 adds two additional anchor scales [2^0, 2^0.5] on each level.\n\n        aspect_ratios: list of tuples representing the aspect ratio anchors added on each level.\n            For instances, aspect_ratios = [(1, 1), (1.4, 0.7), (0.7, 1.4)] adds three anchors on each level.\n\n    Returns:\n        anchor_configs: a dictionary with keys as the levels of anchors and\n            values as a list of anchor configuration.\n    """"""\n    anchor_configs = {}\n    for level in range(min_level, max_level + 1):\n        anchor_configs[level] = []\n        for scale_octave in range(num_scales):\n            for aspect in aspect_ratios:\n                anchor_configs[level].append((2 ** level, scale_octave / float(num_scales), aspect))\n    return anchor_configs\n\n\ndef _generate_anchor_boxes(image_size, anchor_scale, anchor_configs):\n    """"""Generates multiscale anchor boxes.\n\n    Args:\n        image_size: integer number of input image size. The input image has the same dimension for\n            width and height. The image_size should be divided by the largest feature stride 2^max_level.\n\n        anchor_scale: float number representing the scale of size of the base\n            anchor to the feature stride 2^level.\n\n        anchor_configs: a dictionary with keys as the levels of anchors and\n            values as a list of anchor configuration.\n\n    Returns:\n        anchor_boxes: a numpy array with shape [N, 4], which stacks anchors on all feature levels.\n\n    Raises:\n        ValueError: input size must be the multiple of largest feature stride.\n    """"""\n    boxes_all = []\n    for _, configs in anchor_configs.items():\n        boxes_level = []\n        for config in configs:\n            stride, octave_scale, aspect = config\n            if image_size % stride != 0:\n                raise ValueError(""input size must be divided by the stride."")\n            base_anchor_size = anchor_scale * stride * 2 ** octave_scale\n            anchor_size_x_2 = base_anchor_size * aspect[0] / 2.0\n            anchor_size_y_2 = base_anchor_size * aspect[1] / 2.0\n\n            x = np.arange(stride / 2, image_size, stride)\n            y = np.arange(stride / 2, image_size, stride)\n            xv, yv = np.meshgrid(x, y)\n            xv = xv.reshape(-1)\n            yv = yv.reshape(-1)\n\n            boxes = np.vstack((yv - anchor_size_y_2, xv - anchor_size_x_2,\n                               yv + anchor_size_y_2, xv + anchor_size_x_2))\n            boxes = np.swapaxes(boxes, 0, 1)\n            boxes_level.append(np.expand_dims(boxes, axis=1))\n        # concat anchors on the same level to the reshape NxAx4\n        boxes_level = np.concatenate(boxes_level, axis=1)\n        boxes_all.append(boxes_level.reshape([-1, 4]))\n\n    anchor_boxes = np.vstack(boxes_all)\n    return anchor_boxes\n\n\ndef generate_detections(cls_outputs, box_outputs, anchor_boxes, indices, classes, image_scale, nms_thres=0.5, max_dets=100):\n    """"""Generates detections with RetinaNet model outputs and anchors.\n\n    Args:\n        cls_outputs: a torch tensor with shape [N, 1], which has the highest class\n            scores on all feature levels. The N is the number of selected\n            top-K total anchors on all levels.  (k being MAX_DETECTION_POINTS)\n\n        box_outputs: a torch tensor with shape [N, 4], which stacks box regression\n            outputs on all feature levels. The N is the number of selected top-k\n            total anchors on all levels. (k being MAX_DETECTION_POINTS)\n\n        anchor_boxes: a torch tensor with shape [N, 4], which stacks anchors on all\n            feature levels. The N is the number of selected top-k total anchors on all levels.\n\n        indices: a torch tensor with shape [N], which is the indices from top-k selection.\n\n        classes: a torch tensor with shape [N], which represents the class\n            prediction on all selected anchors from top-k selection.\n\n        image_scale: a float tensor representing the scale between original image\n            and input image for the detector. It is used to rescale detections for\n            evaluating with the original groundtruth annotations.\n\n    Returns:\n        detections: detection results in a tensor with shape [MAX_DETECTION_POINTS, 6],\n            each row representing [x, y, width, height, score, class]\n    """"""\n    anchor_boxes = anchor_boxes[indices, :]\n\n    # apply bounding box regression to anchors\n    boxes = decode_box_outputs(box_outputs.T.float(), anchor_boxes.T, output_xyxy=True)\n\n    scores = cls_outputs.sigmoid().squeeze(1).float()\n    human_idx = classes == 0\n    boxes = boxes[human_idx]\n    scores = scores[human_idx]\n    classes = classes[human_idx]\n    top_detection_idx = batched_nms(boxes, scores, classes, iou_threshold=nms_thres)\n\n    # keep only topk scoring predictions\n    top_detection_idx = top_detection_idx[:max_dets]\n    boxes = boxes[top_detection_idx]\n    scores = scores[top_detection_idx, None]\n    classes = classes[top_detection_idx, None]\n\n    # xyxy to xywh & rescale to original image\n    boxes[:, 2] -= boxes[:, 0]\n    boxes[:, 3] -= boxes[:, 1]\n    boxes *= image_scale\n\n    classes += 1  # back to class idx with background class = 0\n\n    # stack em and pad out to MAX_DETECTIONS_PER_IMAGE if necessary\n    detections = torch.cat([boxes, scores, classes.float()], dim=1)\n    if len(top_detection_idx) < max_dets:\n        detections = torch.cat([\n            detections,\n            torch.zeros(\n                (max_dets - len(top_detection_idx), 6), device=detections.device, dtype=detections.dtype)\n        ], dim=0)\n    return detections\n\n\nclass Anchors(nn.Module):\n    """"""RetinaNet Anchors class.""""""\n\n    def __init__(self, min_level, max_level, num_scales, aspect_ratios, anchor_scale, image_size):\n        """"""Constructs multiscale RetinaNet anchors.\n\n        Args:\n            min_level: integer number of minimum level of the output feature pyramid.\n\n            max_level: integer number of maximum level of the output feature pyramid.\n\n            num_scales: integer number representing intermediate scales added\n                on each level. For instances, num_scales=2 adds two additional\n                anchor scales [2^0, 2^0.5] on each level.\n\n            aspect_ratios: list of tuples representing the aspect ratio anchors added\n                on each level. For instances, aspect_ratios =\n                [(1, 1), (1.4, 0.7), (0.7, 1.4)] adds three anchors on each level.\n\n            anchor_scale: float number representing the scale of size of the base\n                anchor to the feature stride 2^level.\n\n            image_size: integer number of input image size. The input image has the\n                same dimension for width and height. The image_size should be divided by\n                the largest feature stride 2^max_level.\n        """"""\n        super(Anchors, self).__init__()\n        self.min_level = min_level\n        self.max_level = max_level\n        self.num_scales = num_scales\n        self.aspect_ratios = aspect_ratios\n        self.anchor_scale = anchor_scale\n        self.image_size = image_size\n        self.config = self._generate_configs()\n        self.register_buffer(\'boxes\', self._generate_boxes())\n\n    def _generate_configs(self):\n        """"""Generate configurations of anchor boxes.""""""\n        return _generate_anchor_configs(self.min_level, self.max_level, self.num_scales, self.aspect_ratios)\n\n    def _generate_boxes(self):\n        """"""Generates multiscale anchor boxes.""""""\n        boxes = _generate_anchor_boxes(self.image_size, self.anchor_scale, self.config)\n        boxes = torch.from_numpy(boxes).float()\n        return boxes\n\n    def get_anchors_per_location(self):\n        return self.num_scales * len(self.aspect_ratios)\n\n\n# FIXME PyTorch port of this class and subclasses not tested yet, needed for training\nclass AnchorLabeler(nn.Module):\n    """"""Labeler for multiscale anchor boxes.\n    """"""\n\n    def __init__(self, anchors, num_classes, match_threshold=0.5):\n        """"""Constructs anchor labeler to assign labels to anchors.\n\n        Args:\n            anchors: an instance of class Anchors.\n\n            num_classes: integer number representing number of classes in the dataset.\n\n            match_threshold: float number between 0 and 1 representing the threshold\n                to assign positive labels for anchors.\n        """"""\n        super(AnchorLabeler, self).__init__()\n        similarity_calc = region_similarity_calculator.IouSimilarity()\n        matcher = argmax_matcher.ArgMaxMatcher(\n            match_threshold,\n            unmatched_threshold=match_threshold,\n            negatives_lower_than_unmatched=True,\n            force_match_for_each_row=True)\n        box_coder = faster_rcnn_box_coder.FasterRcnnBoxCoder()\n\n        self.target_assigner = target_assigner.TargetAssigner(similarity_calc, matcher, box_coder)\n        self.anchors = anchors\n        self.match_threshold = match_threshold\n        self.num_classes = num_classes\n\n    def _unpack_labels(self, labels):\n        """"""Unpacks an array of labels into multiscales labels.""""""\n        labels_unpacked = []\n        anchors = self.anchors\n        count = 0\n        for level in range(anchors.min_level, anchors.max_level + 1):\n            feat_size = int(anchors.image_size / 2 ** level)\n            steps = feat_size ** 2 * anchors.get_anchors_per_location()\n            indices = torch.arange(count, count + steps, device=labels.device)\n            count += steps\n            labels_unpacked.append(\n                torch.index_select(labels, 0, indices).view([feat_size, feat_size, -1]))\n        return labels_unpacked\n\n    def label_anchors(self, gt_boxes, gt_labels):\n        """"""Labels anchors with ground truth inputs.\n\n        Args:\n            gt_boxes: A float tensor with shape [N, 4] representing groundtruth boxes.\n                For each row, it stores [y0, x0, y1, x1] for four corners of a box.\n\n            gt_labels: A integer tensor with shape [N, 1] representing groundtruth classes.\n\n        Returns:\n            cls_targets_dict: ordered dictionary with keys [min_level, min_level+1, ..., max_level].\n                The values are tensor with shape [height_l, width_l, num_anchors]. The height_l and width_l\n                represent the dimension of class logits at l-th level.\n\n            box_targets_dict: ordered dictionary with keys [min_level, min_level+1, ..., max_level].\n                The values are tensor with shape [height_l, width_l, num_anchors * 4]. The height_l and\n                width_l represent the dimension of bounding box regression output at l-th level.\n\n            num_positives: scalar tensor storing number of positives in an image.\n        """"""\n        gt_box_list = box_list.BoxList(gt_boxes)\n        anchor_box_list = box_list.BoxList(self.anchors.boxes)\n\n        # cls_weights, box_weights are not used\n        cls_targets, _, box_targets, _, matches = self.target_assigner.assign(anchor_box_list, gt_box_list, gt_labels)\n\n        # class labels start from 1 and the background class = -1\n        cls_targets -= 1\n        cls_targets = cls_targets.long()\n\n        # Unpack labels.\n        cls_targets_dict = self._unpack_labels(cls_targets)\n        box_targets_dict = self._unpack_labels(box_targets)\n        num_positives = (matches.match_results != -1).float().sum()\n\n        return cls_targets_dict, box_targets_dict, num_positives\n'"
detector/efficientdet/effdet/bench.py,8,"b'"""""" PyTorch EfficientDet support benches\n\nHacked together by Ross Wightman\n""""""\nimport torch\nimport torch.nn as nn\nfrom .anchors import Anchors, AnchorLabeler, generate_detections, MAX_DETECTION_POINTS\n\n\ndef _post_process(config, cls_outputs, box_outputs):\n    """"""Selects top-k predictions.\n\n    Post-proc code adapted from Tensorflow version at: https://github.com/google/automl/tree/master/efficientdet\n    and optimized for PyTorch.\n\n    Args:\n        config: a parameter dictionary that includes `min_level`, `max_level`,  `batch_size`, and `num_classes`.\n\n        cls_outputs: an OrderDict with keys representing levels and values\n            representing logits in [batch_size, height, width, num_anchors].\n\n        box_outputs: an OrderDict with keys representing levels and values\n            representing box regression targets in [batch_size, height, width, num_anchors * 4].\n    """"""\n    batch_size = cls_outputs[0].shape[0]\n    cls_outputs_all = torch.cat([\n        cls_outputs[level].permute(0, 2, 3, 1).reshape([batch_size, -1, config.num_classes])\n        for level in range(config.num_levels)], 1)\n\n    box_outputs_all = torch.cat([\n        box_outputs[level].permute(0, 2, 3, 1).reshape([batch_size, -1, 4])\n        for level in range(config.num_levels)], 1)\n\n    _, cls_topk_indices_all = torch.topk(cls_outputs_all.reshape(batch_size, -1), dim=1, k=MAX_DETECTION_POINTS)\n    indices_all = cls_topk_indices_all / config.num_classes\n    classes_all = cls_topk_indices_all % config.num_classes\n\n    box_outputs_all_after_topk = torch.gather(\n        box_outputs_all, 1, indices_all.unsqueeze(2).expand(-1, -1, 4))\n\n    cls_outputs_all_after_topk = torch.gather(\n        cls_outputs_all, 1, indices_all.unsqueeze(2).expand(-1, -1, config.num_classes))\n    cls_outputs_all_after_topk = torch.gather(\n        cls_outputs_all_after_topk, 2, classes_all.unsqueeze(2))\n\n    return cls_outputs_all_after_topk, box_outputs_all_after_topk, indices_all, classes_all\n\n\nclass DetBenchEval(nn.Module):\n    def __init__(self, model, config, nms_thres=0.5, max_dets=100):\n        super(DetBenchEval, self).__init__()\n        self.config = config\n        self.nms_thres = nms_thres\n        self.max_dets = max_dets\n        self.model = model\n        self.anchors = Anchors(\n            config.min_level, config.max_level,\n            config.num_scales, config.aspect_ratios,\n            config.anchor_scale, config.image_size)\n\n    def forward(self, x, image_scales):\n        class_out, box_out = self.model(x)\n        class_out, box_out, indices, classes = _post_process(self.config, class_out, box_out)\n\n        batch_detections = []\n        # FIXME we may be able to do this as a batch with some tensor reshaping/indexing, PR welcome\n        for i in range(x.shape[0]):\n            detections = generate_detections(\n                class_out[i], box_out[i], self.anchors.boxes, indices[i], classes[i], image_scales[i], \n                nms_thres=self.nms_thres, max_dets=self.max_dets\n                )\n            batch_detections.append(detections)\n        return torch.stack(batch_detections, dim=0)\n\n\nclass DetBenchTrain(nn.Module):\n    def __init__(self, model, config):\n        super(DetBenchTrain, self).__init__()\n        self.config = config\n        self.model = model\n        anchors = Anchors(\n            config.min_level, config.max_level,\n            config.num_scales, config.aspect_ratios,\n            config.anchor_scale, config.image_size)\n        self.anchor_labeler = AnchorLabeler(anchors, config.num_classes, match_threshold=0.5)\n        self.loss_fn = None\n\n    def forward(self, x, gt_boxes, gt_labels):\n        class_out, box_out = self.model(x)\n        loss = None\n        gcl = []\n        gbl = []\n        total_positive = 0\n        # FIXME the per-sample organization of reference code less than desirable, should change to batched\n        for i in range(x.shape[0]):\n            gt_class_out, gt_box_out, num_positive = self.anchor_labeler.label_anchors(gt_boxes[i], gt_labels[i])\n            gcl.append(gt_class_out)\n            gbl.append(gt_box_out)\n            total_positive += num_positive\n\n        # FIXME compute loss\n\n        return loss\n'"
detector/efficientdet/effdet/efficientdet.py,10,"b'"""""" PyTorch EfficientDet model\n\nBased on official Tensorflow version at: https://github.com/google/automl/tree/master/efficientdet\nPaper: https://arxiv.org/abs/1911.09070\n\nHacked together by Ross Wightman\n""""""\nimport torch\nimport torch.nn as nn\nimport logging\nfrom collections import OrderedDict\nfrom typing import List, Optional\nfrom timm import create_model\nfrom timm.models.layers import create_conv2d, drop_path, create_pool2d, Swish\nfrom .config import config\n\n\n_DEBUG = False\n\n_ACT_LAYER = Swish\n\n\nclass SequentialAppend(nn.Sequential):\n    def __init__(self, *args):\n        super(SequentialAppend, self).__init__(*args)\n\n    def forward(self, x: List[torch.Tensor]):\n        for module in self:\n            x.append(module(x))\n        return x\n\n\nclass SequentialAppendLast(nn.Sequential):\n    def __init__(self, *args):\n        super(SequentialAppendLast, self).__init__(*args)\n\n    def forward(self, x: List[torch.Tensor]):\n        for module in self:\n            x.append(module(x[-1]))\n        return x\n\n\nclass ConvBnAct2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, padding=\'\', bias=False,\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None, act_layer=_ACT_LAYER):\n        super(ConvBnAct2d, self).__init__()\n        norm_kwargs = norm_kwargs or {}\n        self.conv = create_conv2d(\n            in_channels, out_channels, kernel_size, stride=stride, dilation=dilation, padding=padding, bias=bias)\n        self.bn = None if norm_layer is None else norm_layer(out_channels, **norm_kwargs)\n        self.act = None if act_layer is None else act_layer(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.act is not None:\n            x = self.act(x)\n        return x\n\n\nclass SeparableConv2d(nn.Module):\n    """""" Separable Conv\n    """"""\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1, padding=\'\', bias=False,\n                 channel_multiplier=1.0, pw_kernel_size=1, act_layer=_ACT_LAYER,\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None):\n        super(SeparableConv2d, self).__init__()\n        norm_kwargs = norm_kwargs or {}\n\n        self.conv_dw = create_conv2d(\n            in_channels, int(in_channels * channel_multiplier), kernel_size,\n            stride=stride, dilation=dilation, padding=padding, depthwise=True)\n\n        self.conv_pw = create_conv2d(\n            int(in_channels * channel_multiplier), out_channels, pw_kernel_size, padding=padding, bias=bias)\n\n        self.bn = None if norm_layer is None else norm_layer(out_channels, **norm_kwargs)\n        self.act = None if act_layer is None else act_layer(inplace=True)\n\n    def forward(self, x):\n        x = self.conv_dw(x)\n        x = self.conv_pw(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.act is not None:\n            x = self.act(x)\n        return x\n\n\nclass ResampleFeatureMap(nn.Sequential):\n\n    def __init__(self, in_channels, out_channels, reduction_ratio=1., pad_type=\'\', pooling_type=\'max\',\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None, conv_after_downsample=False, apply_bn=False):\n        super(ResampleFeatureMap, self).__init__()\n        pooling_type = pooling_type or \'max\'\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.reduction_ratio = reduction_ratio\n        self.conv_after_downsample = conv_after_downsample\n\n        conv = None\n        if in_channels != out_channels:\n            conv = ConvBnAct2d(\n                in_channels, out_channels, kernel_size=1, padding=pad_type,\n                norm_layer=norm_layer if apply_bn else None, norm_kwargs=norm_kwargs, bias=True, act_layer=None)\n\n        if reduction_ratio > 1:\n            stride_size = int(reduction_ratio)\n            if conv is not None and not self.conv_after_downsample:\n                self.add_module(\'conv\', conv)\n            self.add_module(\n                \'downsample\',\n                create_pool2d(\n                    pooling_type, kernel_size=stride_size + 1, stride=stride_size, padding=pad_type))\n            if conv is not None and self.conv_after_downsample:\n                self.add_module(\'conv\', conv)\n        else:\n            if conv is not None:\n                self.add_module(\'conv\', conv)\n            if reduction_ratio < 1:\n                scale = int(1 // reduction_ratio)\n                self.add_module(\'upsample\', nn.UpsamplingNearest2d(scale_factor=scale))\n\n    # def forward(self, x):\n    #     #  here for debugging only\n    #     assert x.shape[1] == self.in_channels\n    #     if self.reduction_ratio > 1:\n    #         if hasattr(self, \'conv\') and not self.conv_after_downsample:\n    #             x = self.conv(x)\n    #         x = self.downsample(x)\n    #         if hasattr(self, \'conv\') and self.conv_after_downsample:\n    #             x = self.conv(x)\n    #     else:\n    #         if hasattr(self, \'conv\'):\n    #             x = self.conv(x)\n    #         if self.reduction_ratio < 1:\n    #             x = self.upsample(x)\n    #     return x\n\n\nclass FpnCombine(nn.Module):\n    def __init__(self, feature_info, fpn_config, fpn_channels, inputs_offsets, target_reduction, pad_type=\'\',\n                 pooling_type=\'max\', norm_layer=nn.BatchNorm2d, norm_kwargs=None,\n                 apply_bn_for_resampling=False, conv_after_downsample=False, weight_method=\'attn\'):\n        super(FpnCombine, self).__init__()\n        self.inputs_offsets = inputs_offsets\n        self.weight_method = weight_method\n\n        self.resample = nn.ModuleDict()\n        for idx, offset in enumerate(inputs_offsets):\n            in_channels = fpn_channels\n            if offset < len(feature_info):\n                in_channels = feature_info[offset][\'num_chs\']\n                input_reduction = feature_info[offset][\'reduction\']\n            else:\n                node_idx = offset - len(feature_info)\n                input_reduction = fpn_config.nodes[node_idx][\'reduction\']\n            reduction_ratio = target_reduction / input_reduction\n            self.resample[str(offset)] = ResampleFeatureMap(\n                in_channels, fpn_channels, reduction_ratio=reduction_ratio, pad_type=pad_type,\n                pooling_type=pooling_type, norm_layer=norm_layer, norm_kwargs=norm_kwargs,\n                apply_bn=apply_bn_for_resampling, conv_after_downsample=conv_after_downsample)\n\n        if weight_method == \'attn\' or weight_method == \'fastattn\':\n            # WSM\n            self.edge_weights = nn.Parameter(torch.ones(len(inputs_offsets)), requires_grad=True)\n        else:\n            self.edge_weights = None\n\n    def forward(self, x):\n        dtype = x[0].dtype\n        nodes = []\n        for offset in self.inputs_offsets:\n            input_node = x[offset]\n            input_node = self.resample[str(offset)](input_node)\n            nodes.append(input_node)\n\n        if self.weight_method == \'attn\':\n            normalized_weights = torch.softmax(self.edge_weights.type(dtype), dim=0)\n            x = torch.stack(nodes, dim=-1) * normalized_weights\n        elif self.weight_method == \'fastattn\':\n            edge_weights = nn.functional.relu(self.edge_weights.type(dtype))\n            weights_sum = torch.sum(edge_weights)\n            x = torch.stack(\n                [(nodes[i] * edge_weights[i]) / (weights_sum + 0.0001) for i in range(len(nodes))], dim=-1)\n        elif self.weight_method == \'sum\':\n            x = torch.stack(nodes, dim=-1)\n        else:\n            raise ValueError(\'unknown weight_method {}\'.format(self.weight_method))\n        x = torch.sum(x, dim=-1)\n        return x\n\n\nclass BiFpnLayer(nn.Module):\n    def __init__(self, feature_info, fpn_config, fpn_channels, num_levels=5, pad_type=\'\',\n                 pooling_type=\'max\', norm_layer=nn.BatchNorm2d, norm_kwargs=None, act_layer=_ACT_LAYER,\n                 apply_bn_for_resampling=False, conv_after_downsample=True, conv_bn_relu_pattern=False,\n                 separable_conv=True):\n        super(BiFpnLayer, self).__init__()\n        self.fpn_config = fpn_config\n        self.num_levels = num_levels\n        self.conv_bn_relu_pattern = False\n\n        self.feature_info = []\n        self.fnode = SequentialAppend()\n        for i, fnode_cfg in enumerate(fpn_config.nodes):\n            logging.debug(\'fnode {} : {}\'.format(i, fnode_cfg))\n            fnode_layers = OrderedDict()\n\n            # combine features\n            reduction = fnode_cfg[\'reduction\']\n            fnode_layers[\'combine\'] = FpnCombine(\n                feature_info, fpn_config, fpn_channels, fnode_cfg[\'inputs_offsets\'], target_reduction=reduction,\n                pad_type=pad_type, pooling_type=pooling_type, norm_layer=norm_layer, norm_kwargs=norm_kwargs,\n                apply_bn_for_resampling=apply_bn_for_resampling, conv_after_downsample=conv_after_downsample,\n                weight_method=fpn_config.weight_method)\n            self.feature_info.append(dict(num_chs=fpn_channels, reduction=reduction))\n\n            # after combine ops\n            after_combine = OrderedDict()\n            if not conv_bn_relu_pattern:\n                after_combine[\'act\'] = act_layer(inplace=True)\n                conv_bias = True\n                conv_act = None\n            else:\n                conv_bias = False\n                conv_act = act_layer\n            conv_kwargs = dict(\n                in_channels=fpn_channels, out_channels=fpn_channels, kernel_size=3, padding=pad_type,\n                bias=conv_bias, norm_layer=norm_layer, norm_kwargs=norm_kwargs, act_layer=conv_act)\n            after_combine[\'conv\'] = SeparableConv2d(**conv_kwargs) if separable_conv else ConvBnAct2d(**conv_kwargs)\n            fnode_layers[\'after_combine\'] = nn.Sequential(after_combine)\n\n            self.fnode.add_module(str(i), nn.Sequential(fnode_layers))\n\n        self.feature_info = self.feature_info[-num_levels::]\n\n    def forward(self, x):\n        x = self.fnode(x)\n        return x[-self.num_levels::]\n\n\ndef bifpn_sum_config(base_reduction=8):\n    """"""BiFPN config with sum.""""""\n    p = config.Config()\n    p.nodes = [\n        {\'reduction\': base_reduction << 3, \'inputs_offsets\': [3, 4]},\n        {\'reduction\': base_reduction << 2, \'inputs_offsets\': [2, 5]},\n        {\'reduction\': base_reduction << 1, \'inputs_offsets\': [1, 6]},\n        {\'reduction\': base_reduction, \'inputs_offsets\': [0, 7]},\n        {\'reduction\': base_reduction << 1, \'inputs_offsets\': [1, 7, 8]},\n        {\'reduction\': base_reduction << 2, \'inputs_offsets\': [2, 6, 9]},\n        {\'reduction\': base_reduction << 3, \'inputs_offsets\': [3, 5, 10]},\n        {\'reduction\': base_reduction << 4, \'inputs_offsets\': [4, 11]},\n    ]\n    p.weight_method = \'sum\'\n    return p\n\n\ndef bifpn_attn_config():\n    """"""BiFPN config with fast weighted sum.""""""\n    p = bifpn_sum_config()\n    p.weight_method = \'attn\'\n    return p\n\n\ndef bifpn_fa_config():\n    """"""BiFPN config with fast weighted sum.""""""\n    p = bifpn_sum_config()\n    p.weight_method = \'fastattn\'\n    return p\n\n\ndef get_fpn_config(fpn_name):\n    if not fpn_name:\n        fpn_name = \'bifpn_fa\'\n    name_to_config = {\n        \'bifpn_sum\': bifpn_sum_config(),\n        \'bifpn_attn\': bifpn_attn_config(),\n        \'bifpn_fa\': bifpn_fa_config(),\n    }\n    return name_to_config[fpn_name]\n\n\nclass BiFpn(nn.Module):\n\n    def __init__(self, config, feature_info, norm_layer=nn.BatchNorm2d, norm_kwargs=None, act_layer=_ACT_LAYER):\n        super(BiFpn, self).__init__()\n        self.config = config\n        fpn_config = config.fpn_config or get_fpn_config(config.fpn_name)\n\n        self.resample = SequentialAppendLast()\n        for level in range(config.num_levels):\n            if level < len(feature_info):\n                in_chs = feature_info[level][\'num_chs\']\n                reduction = feature_info[level][\'reduction\']\n            else:\n                # Adds a coarser level by downsampling the last feature map\n                reduction_ratio = 2\n                self.resample.add_module(str(level), ResampleFeatureMap(\n                    in_channels=in_chs,\n                    out_channels=config.fpn_channels,\n                    pad_type=config.pad_type,\n                    pooling_type=config.pooling_type,\n                    norm_layer=norm_layer,\n                    norm_kwargs=norm_kwargs,\n                    reduction_ratio=reduction_ratio,\n                    apply_bn=config.apply_bn_for_resampling,\n                    conv_after_downsample=config.conv_after_downsample,\n                ))\n                in_chs = config.fpn_channels\n                reduction = int(reduction * reduction_ratio)\n                feature_info.append(dict(num_chs=in_chs, reduction=reduction))\n\n        self.cell = nn.Sequential()\n        for rep in range(config.fpn_cell_repeats):\n            logging.debug(\'building cell {}\'.format(rep))\n            fpn_layer = BiFpnLayer(\n                feature_info=feature_info,\n                fpn_config=fpn_config,\n                fpn_channels=config.fpn_channels,\n                num_levels=config.num_levels,\n                pad_type=config.pad_type,\n                pooling_type=config.pooling_type,\n                norm_layer=norm_layer,\n                norm_kwargs=norm_kwargs,\n                act_layer=act_layer,\n                separable_conv=config.separable_conv,\n                apply_bn_for_resampling=config.apply_bn_for_resampling,\n                conv_after_downsample=config.conv_after_downsample,\n                conv_bn_relu_pattern=config.conv_bn_relu_pattern\n            )\n            self.cell.add_module(str(rep), fpn_layer)\n            feature_info = fpn_layer.feature_info\n\n        # FIXME init weights for training\n\n    def forward(self, x):\n        assert len(self.resample) == self.config.num_levels - len(x)\n        x = self.resample(x)\n        x = self.cell(x)\n        return x\n\n\nclass HeadNet(nn.Module):\n    def __init__(self, config, num_outputs, norm_layer=nn.BatchNorm2d, norm_kwargs=None,\n                 act_layer=_ACT_LAYER, predict_bias_init=None):\n        super(HeadNet, self).__init__()\n        norm_kwargs = norm_kwargs or {}\n        self.config = config\n        num_anchors = len(config.aspect_ratios) * config.num_scales\n\n        self.conv_rep = nn.ModuleList()\n        self.bn_rep = nn.ModuleList()\n        conv_kwargs = dict(\n            in_channels=config.fpn_channels, out_channels=config.fpn_channels, kernel_size=3,\n            padding=self.config.pad_type, bias=True, act_layer=None, norm_layer=None)\n        for i in range(config.box_class_repeats):\n            conv = SeparableConv2d(**conv_kwargs) if config.separable_conv else ConvBnAct2d(**conv_kwargs)\n            self.conv_rep.append(conv)\n\n            bn_levels = []\n            for _ in range(config.num_levels):\n                bn_seq = nn.Sequential()\n                bn_seq.add_module(\'bn\', norm_layer(config.fpn_channels, **norm_kwargs))\n                bn_levels.append(bn_seq)\n            self.bn_rep.append(nn.ModuleList(bn_levels))\n\n        self.act = act_layer(inplace=True)\n\n        predict_kwargs = dict(\n            in_channels=config.fpn_channels, out_channels=num_outputs * num_anchors, kernel_size=3,\n            padding=self.config.pad_type, bias=True, norm_layer=None, act_layer=None)\n        if config.separable_conv:\n            self.predict = SeparableConv2d(**predict_kwargs)\n        else:\n            self.predict = ConvBnAct2d(**predict_kwargs)\n\n        # FIXME init weights for training\n\n    def forward(self, x):\n        outputs = []\n        for level in range(self.config.num_levels):\n            x_level = x[level]\n            for i in range(self.config.box_class_repeats):\n                x_level_in = x_level\n                x_level = self.conv_rep[i](x_level)\n                x_level = self.bn_rep[i][level](x_level)\n                x_level = self.act(x_level)\n                if i > 0 and self.config.drop_path_rate:\n                    x_level = drop_path(x_level, self.config.drop_path_rate, self.is_training)\n                    x_level = x_level + x_level_in\n            outputs.append(self.predict(x_level))\n        return outputs\n\n\nclass EfficientDet(nn.Module):\n\n    def __init__(self, config, norm_kwargs=None):\n        super(EfficientDet, self).__init__()\n        norm_kwargs = norm_kwargs or dict(eps=.001)\n        self.backbone = create_model(\n            config.backbone_name, features_only=True, out_indices=(2, 3, 4))\n        feature_info = [dict(num_chs=f[\'num_chs\'], reduction=f[\'reduction\'])\n                        for i, f in enumerate(self.backbone.feature_info())]\n        self.fpn = BiFpn(config, feature_info, norm_kwargs=norm_kwargs)\n        self.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=norm_kwargs)\n        self.box_net = HeadNet(config, num_outputs=4, norm_kwargs=norm_kwargs)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.fpn(x)\n        x_class = self.class_net(x)\n        x_box = self.box_net(x)\n        return x_class, x_box\n'"
detector/efficientdet/effdet/helpers.py,3,"b'import torch\nimport os\nfrom collections import OrderedDict\ntry:\n    from torch.hub import load_state_dict_from_url\nexcept ImportError:\n    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n\n\ndef load_checkpoint(model, checkpoint_path):\n    if checkpoint_path and os.path.isfile(checkpoint_path):\n        print(""=> Loading checkpoint \'{}\'"".format(checkpoint_path))\n        checkpoint = torch.load(checkpoint_path)\n        if isinstance(checkpoint, dict) and \'state_dict\' in checkpoint:\n            new_state_dict = OrderedDict()\n            for k, v in checkpoint[\'state_dict\'].items():\n                if k.startswith(\'module\'):\n                    name = k[7:]  # remove `module.`\n                else:\n                    name = k\n                new_state_dict[name] = v\n            model.load_state_dict(new_state_dict)\n        else:\n            model.load_state_dict(checkpoint)\n        print(""=> Loaded checkpoint \'{}\'"".format(checkpoint_path))\n    else:\n        print(""=> Error: No checkpoint found at \'{}\'"".format(checkpoint_path))\n        raise FileNotFoundError()\n\n\ndef load_pretrained(model, url, filter_fn=None, strict=True):\n    if not url:\n        print(""=> Warning: Pretrained model URL is empty, using random initialization."")\n        return\n    state_dict = load_state_dict_from_url(url, progress=False, map_location=\'cpu\')\n    if filter_fn is not None:\n        state_dict = filter_fn(state_dict)\n    model.load_state_dict(state_dict, strict=strict)\n'"
detector/tracker/tracker/__init__.py,0,b''
detector/tracker/tracker/basetrack.py,0,"b'import numpy as np\nfrom collections import OrderedDict\n\n\nclass TrackState(object):\n    New = 0\n    Tracked = 1\n    Lost = 2\n    Removed = 3\n\n\nclass BaseTrack(object):\n    _count = 0\n\n    track_id = 0\n    is_activated = False\n    state = TrackState.New\n\n    history = OrderedDict()\n    features = []\n    curr_feature = None\n    score = 0\n    start_frame = 0\n    frame_id = 0\n    time_since_update = 0\n\n    # multi-camera\n    location = (np.inf, np.inf)\n\n    @property\n    def end_frame(self):\n        return self.frame_id\n\n    @staticmethod\n    def next_id():\n        BaseTrack._count += 1\n        return BaseTrack._count\n\n    def activate(self, *args):\n        raise NotImplementedError\n\n    def predict(self):\n        raise NotImplementedError\n\n    def update(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def mark_lost(self):\n        self.state = TrackState.Lost\n\n    def mark_removed(self):\n        self.state = TrackState.Removed\n\n'"
detector/tracker/tracker/matching.py,0,"b'import cv2\nimport numpy as np\nimport scipy\nfrom scipy.spatial.distance import cdist\nfrom scipy.optimize import linear_sum_assignment\n\nfrom cython_bbox import bbox_overlaps as bbox_ious\nfrom tracker.utils import kalman_filter\nimport time\n\ndef merge_matches(m1, m2, shape):\n    O,P,Q = shape\n    m1 = np.asarray(m1)\n    m2 = np.asarray(m2)\n\n    M1 = scipy.sparse.coo_matrix((np.ones(len(m1)), (m1[:, 0], m1[:, 1])), shape=(O, P))\n    M2 = scipy.sparse.coo_matrix((np.ones(len(m2)), (m2[:, 0], m2[:, 1])), shape=(P, Q))\n\n    mask = M1*M2\n    match = mask.nonzero()\n    match = list(zip(match[0], match[1]))\n    unmatched_O = tuple(set(range(O)) - set([i for i, j in match]))\n    unmatched_Q = tuple(set(range(Q)) - set([j for i, j in match]))\n\n    return match, unmatched_O, unmatched_Q\n\n\ndef _indices_to_matches(cost_matrix, indices, thresh):\n    matched_cost = cost_matrix[tuple(zip(*indices))]\n    matched_mask = (matched_cost <= thresh)\n\n    matches = indices[matched_mask]\n    unmatched_a = tuple(set(range(cost_matrix.shape[0])) - set(matches[:, 0]))\n    unmatched_b = tuple(set(range(cost_matrix.shape[1])) - set(matches[:, 1]))\n\n    return matches, unmatched_a, unmatched_b\n\n\ndef linear_assignment(cost_matrix, thresh):\n    """"""\n    Simple linear assignment\n    :type cost_matrix: np.ndarray\n    :type thresh: float\n    :return: matches, unmatched_a, unmatched_b\n    """"""\n    if cost_matrix.size == 0:\n        return np.empty((0, 2), dtype=int), tuple(range(cost_matrix.shape[0])), tuple(range(cost_matrix.shape[1]))\n\n    cost_matrix[cost_matrix > thresh] = thresh + 1e-4\n    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n    indices = np.column_stack((row_ind, col_ind))\n\n    return _indices_to_matches(cost_matrix, indices, thresh)\n\n\ndef ious(atlbrs, btlbrs):\n    """"""\n    Compute cost based on IoU\n    :type atlbrs: list[tlbr] | np.ndarray\n    :type atlbrs: list[tlbr] | np.ndarray\n\n    :rtype ious np.ndarray\n    """"""\n    ious = np.zeros((len(atlbrs), len(btlbrs)), dtype=np.float)\n    if ious.size == 0:\n        return ious\n\n    ious = bbox_ious(\n        np.ascontiguousarray(atlbrs, dtype=np.float),\n        np.ascontiguousarray(btlbrs, dtype=np.float)\n    )\n\n    return ious\n\n\ndef iou_distance(atracks, btracks):\n    """"""\n    Compute cost based on IoU\n    :type atracks: list[STrack]\n    :type btracks: list[STrack]\n\n    :rtype cost_matrix np.ndarray\n    """"""\n\n    if (len(atracks)>0 and isinstance(atracks[0], np.ndarray)) or (len(btracks) > 0 and isinstance(btracks[0], np.ndarray)):\n        atlbrs = atracks\n        btlbrs = btracks\n    else:\n        atlbrs = [track.tlbr for track in atracks]\n        btlbrs = [track.tlbr for track in btracks]\n    _ious = ious(atlbrs, btlbrs)\n    cost_matrix = 1 - _ious\n\n    return cost_matrix\n\ndef embedding_distance(tracks, detections, metric=\'cosine\'):\n    """"""\n    :param tracks: list[STrack]\n    :param detections: list[BaseTrack]\n    :param metric:\n    :return: cost_matrix np.ndarray\n    """"""\n\n    cost_matrix = np.zeros((len(tracks), len(detections)), dtype=np.float)\n    if cost_matrix.size == 0:\n        return cost_matrix\n    det_features = np.asarray([track.curr_feat for track in detections], dtype=np.float)\n    for i, track in enumerate(tracks):\n        cost_matrix[i, :] = np.maximum(0.0, cdist(track.smooth_feat.reshape(1,-1), det_features, metric))\n    return cost_matrix\n\n\ndef gate_cost_matrix(kf, cost_matrix, tracks, detections, only_position=False):\n    if cost_matrix.size == 0:\n        return cost_matrix\n    gating_dim = 2 if only_position else 4\n    gating_threshold = kalman_filter.chi2inv95[gating_dim]\n    measurements = np.asarray([det.to_xyah() for det in detections])\n    for row, track in enumerate(tracks):\n        gating_distance = kf.gating_distance(\n            track.mean, track.covariance, measurements, only_position)\n        cost_matrix[row, gating_distance > gating_threshold] = np.inf\n    return cost_matrix\n'"
detector/tracker/tracker/multitracker.py,3,"b'import numpy as np\nfrom collections import deque\nimport itertools\nimport os\nimport os.path as osp\nimport time\nimport torch\n\nfrom tracker.utils.utils import *\nfrom tracker.utils.log import logger\nfrom tracker.utils.kalman_filter import KalmanFilter\nfrom tracker.models import *\nfrom tracker.tracker import matching\nfrom tracker.tracker.basetrack import BaseTrack, TrackState\n\n\nclass STrack(BaseTrack):\n\n    def __init__(self, tlwh, score, temp_feat, buffer_size=30):\n\n        # wait activate\n        self._tlwh = np.asarray(tlwh, dtype=np.float)\n        self.kalman_filter = None\n        self.mean, self.covariance = None, None\n        self.is_activated = False\n\n        self.score = score\n        self.tracklet_len = 0\n\n        self.smooth_feat = None\n        self.update_features(temp_feat)\n        self.features = deque([], maxlen=buffer_size)\n        self.alpha = 0.9\n    \n    def update_features(self, feat):\n        self.curr_feat = feat\n        if self.smooth_feat is None:\n            self.smooth_feat = feat\n        else:\n            self.smooth_feat = self.alpha *self.smooth_feat + (1-self.alpha) * feat\n        self.features.append(feat)\n        self.smooth_feat /= np.linalg.norm(self.smooth_feat)  \n\n    def predict(self):\n        mean_state = self.mean.copy()\n        if self.state != TrackState.Tracked:\n            mean_state[7] = 0\n        self.mean, self.covariance = self.kalman_filter.predict(mean_state, self.covariance)\n\n\n    def activate(self, kalman_filter, frame_id):\n        """"""Start a new tracklet""""""\n        self.kalman_filter = kalman_filter\n        self.track_id = self.next_id()\n        self.mean, self.covariance = self.kalman_filter.initiate(self.tlwh_to_xyah(self._tlwh))\n\n        self.tracklet_len = 0\n        self.state = TrackState.Tracked\n        #self.is_activated = True\n        self.frame_id = frame_id\n        self.start_frame = frame_id\n\n    def re_activate(self, new_track, frame_id, new_id=False):\n        self.mean, self.covariance = self.kalman_filter.update(\n            self.mean, self.covariance, self.tlwh_to_xyah(new_track.tlwh)\n        )\n\n        self.update_features(new_track.curr_feat)\n        self.tracklet_len = 0\n        self.state = TrackState.Tracked\n        self.is_activated = True\n        self.frame_id = frame_id\n        if new_id:\n            self.track_id = self.next_id()\n\n    def update(self, new_track, frame_id, update_feature=True):\n        """"""\n        Update a matched track\n        :type new_track: STrack\n        :type frame_id: int\n        :type update_feature: bool\n        :return:\n        """"""\n        self.frame_id = frame_id\n        self.tracklet_len += 1\n\n        new_tlwh = new_track.tlwh\n        self.mean, self.covariance = self.kalman_filter.update(\n            self.mean, self.covariance, self.tlwh_to_xyah(new_tlwh))\n        self.state = TrackState.Tracked\n        self.is_activated = True\n\n        self.score = new_track.score\n        if update_feature:\n            self.update_features(new_track.curr_feat)\n\n    @property\n    def tlwh(self):\n        """"""Get current position in bounding box format `(top left x, top left y,\n                width, height)`.\n        """"""\n        if self.mean is None:\n            return self._tlwh.copy()\n        ret = self.mean[:4].copy()\n        ret[2] *= ret[3]\n        ret[:2] -= ret[2:] / 2\n        return ret\n\n    @property\n    def tlbr(self):\n        """"""Convert bounding box to format `(min x, min y, max x, max y)`, i.e.,\n        `(top left, bottom right)`.\n        """"""\n        ret = self.tlwh.copy()\n        ret[2:] += ret[:2]\n        return ret\n\n    @staticmethod\n    def tlwh_to_xyah(tlwh):\n        """"""Convert bounding box to format `(center x, center y, aspect ratio,\n        height)`, where the aspect ratio is `width / height`.\n        """"""\n        ret = np.asarray(tlwh).copy()\n        ret[:2] += ret[2:] / 2\n        ret[2] /= ret[3]\n        return ret\n\n    def to_xyah(self):\n        return self.tlwh_to_xyah(self.tlwh)\n\n    @staticmethod\n    def tlbr_to_tlwh(tlbr):\n        ret = np.asarray(tlbr).copy()\n        ret[2:] -= ret[:2]\n        return ret\n\n    @staticmethod\n    def tlwh_to_tlbr(tlwh):\n        ret = np.asarray(tlwh).copy()\n        ret[2:] += ret[:2]\n        return ret\n\n    def __repr__(self):\n        return \'OT_{}_({}-{})\'.format(self.track_id, self.start_frame, self.end_frame)\n\n\nclass JDETracker(object):\n    def __init__(self, args, frame_rate=30):\n        self.args = args\n        self.model = Darknet(args.cfg, args.img_size, nID=14455)\n        # load_darknet_weights(self.model, args.weights)\n        self.model.load_state_dict(torch.load(args.weights, map_location=\'cpu\')[\'model\'], strict=False)\n\n        if args:\n            if len(args.gpus) > 1:\n                self.model = torch.nn.DataParallel(self.model, device_ids=args.gpus).to(args.device)\n            else:\n                self.model.to(args.device)\n        else:\n            self.model.cuda()\n        self.model.eval()\n\n        self.tracked_stracks = []  # type: list[STrack]\n        self.lost_stracks = []  # type: list[STrack]\n        self.removed_stracks = []  # type: list[STrack]\n\n        self.frame_id = 0\n        self.det_thresh = args.conf_thres\n        self.buffer_size = int(frame_rate / 30.0 * args.track_buffer)\n        self.max_time_lost = self.buffer_size\n\n        self.kalman_filter = KalmanFilter()\n\n    def update(self, im_blob, img0):\n        self.frame_id += 1\n        activated_starcks = []\n        refind_stracks = []\n        lost_stracks = []\n        removed_stracks = []\n\n        t1 = time.time()\n        \'\'\' Step 1: Network forward, get detections & embeddings\'\'\'\n        with torch.no_grad():\n            pred = self.model(im_blob)\n        pred = pred[pred[:, :, 4] > self.args.conf_thres]\n        if len(pred) > 0:\n            dets = non_max_suppression(pred.unsqueeze(0), self.args.conf_thres, self.args.nms_thres)[0].cpu()\n            scale_coords(self.args.img_size, dets[:, :4], img0.shape).round()\n            \'\'\'Detections\'\'\'\n            detections = [STrack(STrack.tlbr_to_tlwh(tlbrs[:4]), tlbrs[4], f.numpy(), 30) for\n                          (tlbrs, f) in zip(dets[:, :5], dets[:, -self.model.emb_dim:])]\n        else:\n            detections = []\n\n        t2 = time.time()\n        # print(\'Forward: {} s\'.format(t2-t1))\n\n        \'\'\' Add newly detected tracklets to tracked_stracks\'\'\'\n        unconfirmed = []\n        tracked_stracks = []  # type: list[STrack]\n        for track in self.tracked_stracks:\n            if not track.is_activated:\n                unconfirmed.append(track)\n            else:\n                tracked_stracks.append(track)\n\n        \'\'\' Step 2: First association, with embedding\'\'\'\n        strack_pool = joint_stracks(tracked_stracks, self.lost_stracks)\n        # Predict the current location with KF\n        for strack in strack_pool:\n            strack.predict()\n\n        dists = matching.embedding_distance(strack_pool, detections)\n        dists = matching.gate_cost_matrix(self.kalman_filter, dists, strack_pool, detections)\n        matches, u_track, u_detection = matching.linear_assignment(dists, thresh=0.7)\n\n        for itracked, idet in matches:\n            track = strack_pool[itracked]\n            det = detections[idet]\n            if track.state == TrackState.Tracked:\n                track.update(detections[idet], self.frame_id)\n                activated_starcks.append(track)\n            else:\n                track.re_activate(det, self.frame_id, new_id=False)\n                refind_stracks.append(track)\n\n        \'\'\' Step 3: Second association, with IOU\'\'\'\n        detections = [detections[i] for i in u_detection]\n        r_tracked_stracks = [strack_pool[i] for i in u_track if strack_pool[i].state==TrackState.Tracked ]\n        dists = matching.iou_distance(r_tracked_stracks, detections)\n        matches, u_track, u_detection = matching.linear_assignment(dists, thresh=0.5)\n        \n        for itracked, idet in matches:\n            track = r_tracked_stracks[itracked]\n            det = detections[idet]\n            if track.state == TrackState.Tracked:\n                track.update(det, self.frame_id)\n                activated_starcks.append(track)\n            else:\n                track.re_activate(det, self.frame_id, new_id=False)\n                refind_stracks.append(track)\n\n        for it in u_track:\n            track = r_tracked_stracks[it]\n            if not track.state == TrackState.Lost:\n                track.mark_lost()\n                lost_stracks.append(track)\n\n        \'\'\'Deal with unconfirmed tracks, usually tracks with only one beginning frame\'\'\'\n        detections = [detections[i] for i in u_detection]\n        dists = matching.iou_distance(unconfirmed, detections)\n        matches, u_unconfirmed, u_detection = matching.linear_assignment(dists, thresh=0.7)\n        for itracked, idet in matches:\n            unconfirmed[itracked].update(detections[idet], self.frame_id)\n            activated_starcks.append(unconfirmed[itracked])\n        for it in u_unconfirmed:\n            track = unconfirmed[it]\n            track.mark_removed()\n            removed_stracks.append(track)\n\n        """""" Step 4: Init new stracks""""""\n        for inew in u_detection:\n            track = detections[inew]\n            if track.score < self.det_thresh:\n                continue\n            track.activate(self.kalman_filter, self.frame_id)\n            activated_starcks.append(track)\n\n        """""" Step 5: Update state""""""\n        for track in self.lost_stracks:\n            if self.frame_id - track.end_frame > self.max_time_lost:\n                track.mark_removed()\n                removed_stracks.append(track)\n        t4 = time.time()\n        # print(\'Ramained match {} s\'.format(t4-t3))\n\n        self.tracked_stracks = [t for t in self.tracked_stracks if t.state == TrackState.Tracked]\n        self.tracked_stracks = joint_stracks(self.tracked_stracks, activated_starcks)\n        self.tracked_stracks = joint_stracks(self.tracked_stracks, refind_stracks)\n        # self.lost_stracks = [t for t in self.lost_stracks if t.state == TrackState.Lost]  # type: list[STrack]\n        self.lost_stracks = sub_stracks(self.lost_stracks, self.tracked_stracks)\n        self.lost_stracks.extend(lost_stracks)\n        self.lost_stracks = sub_stracks(self.lost_stracks, self.removed_stracks)\n        self.removed_stracks.extend(removed_stracks)\n        self.tracked_stracks, self.lost_stracks = remove_duplicate_stracks(self.tracked_stracks, self.lost_stracks)\n\n        # get scores of lost tracks\n        output_stracks = [track for track in self.tracked_stracks if track.is_activated]\n\n        logger.debug(\'===========Frame {}==========\'.format(self.frame_id))\n        logger.debug(\'Activated: {}\'.format([track.track_id for track in activated_starcks]))\n        logger.debug(\'Refind: {}\'.format([track.track_id for track in refind_stracks]))\n        logger.debug(\'Lost: {}\'.format([track.track_id for track in lost_stracks]))\n        logger.debug(\'Removed: {}\'.format([track.track_id for track in removed_stracks]))\n        t5 = time.time()\n        # print(\'Final {} s\'.format(t5-t4))\n        return output_stracks\n\ndef joint_stracks(tlista, tlistb):\n    exists = {}\n    res = []\n    for t in tlista:\n        exists[t.track_id] = 1\n        res.append(t)\n    for t in tlistb:\n        tid = t.track_id\n        if not exists.get(tid, 0):\n            exists[tid] = 1\n            res.append(t)\n    return res\n\ndef sub_stracks(tlista, tlistb):\n    stracks = {}\n    for t in tlista:\n        stracks[t.track_id] = t\n    for t in tlistb:\n        tid = t.track_id\n        if stracks.get(tid, 0):\n            del stracks[tid]\n    return list(stracks.values())\n\ndef remove_duplicate_stracks(stracksa, stracksb):\n    pdist = matching.iou_distance(stracksa, stracksb)\n    pairs = np.where(pdist<0.15)\n    dupa, dupb = list(), list()\n    for p,q in zip(*pairs):\n        timep = stracksa[p].frame_id - stracksa[p].start_frame\n        timeq = stracksb[q].frame_id - stracksb[q].start_frame\n        if timep > timeq:\n            dupb.append(q)\n        else:\n            dupa.append(p)\n    resa = [t for i,t in enumerate(stracksa) if not i in dupa]\n    resb = [t for i,t in enumerate(stracksb) if not i in dupb]\n    return resa, resb\n            \n\n'"
detector/tracker/utils/__init__.py,0,b''
detector/tracker/utils/datasets.py,5,"b""import glob\nimport math\nimport os\nimport os.path as osp\nimport random\nimport time\nfrom collections import OrderedDict\n\nimport cv2\nimport numpy as np\nimport torch\n\nfrom torch.utils.data import Dataset\nfrom utils.utils import xyxy2xywh\n\nclass LoadImages:  # for inference\n    def __init__(self, path, img_size=(1088, 608)):\n        if os.path.isdir(path):\n            image_format = ['.jpg', '.jpeg', '.png', '.tif']\n            self.files = sorted(glob.glob('%s/*.*' % path))\n            self.files = list(filter(lambda x: os.path.splitext(x)[1].lower() in image_format, self.files))\n        elif os.path.isfile(path):\n            self.files = [path]\n\n        self.nF = len(self.files)  # number of image files\n        self.width = img_size[0]\n        self.height = img_size[1]\n        self.count = 0\n\n        assert self.nF > 0, 'No images found in ' + path\n\n    def __iter__(self):\n        self.count = -1\n        return self\n\n    def __next__(self):\n        self.count += 1\n        if self.count == self.nF:\n            raise StopIteration\n        img_path = self.files[self.count]\n\n        # Read image\n        img0 = cv2.imread(img_path)  # BGR\n        assert img0 is not None, 'Failed to load ' + img_path\n\n        # Padded resize\n        img, _, _, _ = letterbox(img0, height=self.height, width=self.width)\n\n        # Normalize RGB\n        img = img[:, :, ::-1].transpose(2, 0, 1)\n        img = np.ascontiguousarray(img, dtype=np.float32)\n        img /= 255.0\n\n        # cv2.imwrite(img_path + '.letterbox.jpg', 255 * img.transpose((1, 2, 0))[:, :, ::-1])  # save letterbox image\n        return img_path, img, img0\n    \n    def __getitem__(self, idx):\n        idx = idx % self.nF \n        img_path = self.files[idx]\n\n        # Read image\n        img0 = cv2.imread(img_path)  # BGR\n        assert img0 is not None, 'Failed to load ' + img_path\n\n        # Padded resize\n        img, _, _, _ = letterbox(img0, height=self.height, width=self.width)\n\n        # Normalize RGB\n        img = img[:, :, ::-1].transpose(2, 0, 1)\n        img = np.ascontiguousarray(img, dtype=np.float32)\n        img /= 255.0\n\n        return img_path, img, img0\n\n    def __len__(self):\n        return self.nF  # number of files\n\n\nclass LoadVideo:  # for inference\n    def __init__(self, path, img_size=(1088, 608)):\n        self.cap = cv2.VideoCapture(path)        \n        self.frame_rate = int(round(self.cap.get(cv2.CAP_PROP_FPS)))\n        self.vw = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        self.vh = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        self.vn = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        self.width = img_size[0]\n        self.height = img_size[1]\n        self.count = 0\n\n        self.w, self.h = self.get_size(self.vw, self.vh, self.width, self.height)\n        print('Lenth of the video: {:d} frames'.format(self.vn))\n\n    def get_size(self, vw, vh, dw, dh):\n        wa, ha = float(dw) / vw, float(dh) / vh\n        a = min(wa, ha)\n        return int(vw *a), int(vh*a)\n\n    def __iter__(self):\n        self.count = -1\n        return self\n\n    def __next__(self):\n        self.count += 1\n        if self.count == len(self):\n            raise StopIteration\n        # Read image\n        res, img0 = self.cap.read()  # BGR\n        assert img0 is not None, 'Failed to load frame {:d}'.format(self.count)\n        img0 = cv2.resize(img0, (self.w, self.h))\n\n        # Padded resize\n        img, _, _, _ = letterbox(img0, height=self.height, width=self.width)\n\n        # Normalize RGB\n        img = img[:, :, ::-1].transpose(2, 0, 1)\n        img = np.ascontiguousarray(img, dtype=np.float32)\n        img /= 255.0\n\n        # cv2.imwrite(img_path + '.letterbox.jpg', 255 * img.transpose((1, 2, 0))[:, :, ::-1])  # save letterbox image\n        return self.count, img, img0\n    \n    def __len__(self):\n        return self.vn  # number of files\n\n\nclass LoadImagesAndLabels:  # for training\n    def __init__(self, path, img_size=(1088,608),  augment=False, transforms=None):\n        with open(path, 'r') as file:\n            self.img_files = file.readlines()\n            self.img_files = [x.replace('\\n', '') for x in self.img_files]\n            self.img_files = list(filter(lambda x: len(x) > 0, self.img_files))\n\n        self.label_files = [x.replace('images', 'labels_with_ids').replace('.png', '.txt').replace('.jpg', '.txt')\n                            for x in self.img_files]\n\n        self.nF = len(self.img_files)  # number of image files\n        self.width = img_size[0]\n        self.height = img_size[1]\n        self.augment = augment\n        self.transforms = transforms\n\n\n    def __getitem__(self, files_index):\n        img_path = self.img_files[files_index]\n        label_path = self.label_files[files_index]\n        return self.get_data(img_path, label_path)\n\n    def get_data(self, img_path, label_path):\n        height = self.height\n        width = self.width\n        img = cv2.imread(img_path)  # BGR\n        if img is None:\n            raise ValueError('File corrupt {}'.format(img_path))\n        augment_hsv = True\n        if self.augment and augment_hsv:\n            # SV augmentation by 50%\n            fraction = 0.50\n            img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n            S = img_hsv[:, :, 1].astype(np.float32)\n            V = img_hsv[:, :, 2].astype(np.float32)\n\n            a = (random.random() * 2 - 1) * fraction + 1\n            S *= a\n            if a > 1:\n                np.clip(S, a_min=0, a_max=255, out=S)\n\n            a = (random.random() * 2 - 1) * fraction + 1\n            V *= a\n            if a > 1:\n                np.clip(V, a_min=0, a_max=255, out=V)\n\n            img_hsv[:, :, 1] = S.astype(np.uint8)\n            img_hsv[:, :, 2] = V.astype(np.uint8)\n            cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR, dst=img)\n\n        h, w, _ = img.shape\n        img, ratio, padw, padh = letterbox(img, height=height, width=width)\n\n        # Load labels\n        if os.path.isfile(label_path):\n            labels0 = np.loadtxt(label_path, dtype=np.float32).reshape(-1, 6)\n\n            # Normalized xywh to pixel xyxy format\n            labels = labels0.copy()\n            labels[:, 2] = ratio * w * (labels0[:, 2] - labels0[:, 4] / 2) + padw\n            labels[:, 3] = ratio * h * (labels0[:, 3] - labels0[:, 5] / 2) + padh\n            labels[:, 4] = ratio * w * (labels0[:, 2] + labels0[:, 4] / 2) + padw\n            labels[:, 5] = ratio * h * (labels0[:, 3] + labels0[:, 5] / 2) + padh\n        else:\n            labels = np.array([])\n\n        # Augment image and labels\n        if self.augment:\n            img, labels, M = random_affine(img, labels, degrees=(-5, 5), translate=(0.10, 0.10), scale=(0.50, 1.20))\n\n        plotFlag = False\n        if plotFlag:\n            import matplotlib\n            matplotlib.use('Agg')\n            import matplotlib.pyplot as plt\n            plt.figure(figsize=(50, 50)) \n            plt.imshow(img[:, :, ::-1])\n            plt.plot(labels[:, [1, 3, 3, 1, 1]].T, labels[:, [2, 2, 4, 4, 2]].T, '.-')\n            plt.axis('off')\n            plt.savefig('test.jpg')\n            time.sleep(10)\n\n        nL = len(labels)\n        if nL > 0:\n            # convert xyxy to xywh\n            labels[:, 2:6] = xyxy2xywh(labels[:, 2:6].copy()) #/ height\n            labels[:, 2] /= width\n            labels[:, 3] /= height\n            labels[:, 4] /= width\n            labels[:, 5] /= height\n        if self.augment:\n            # random left-right flip\n            lr_flip = True\n            if lr_flip & (random.random() > 0.5):\n                img = np.fliplr(img)\n                if nL > 0:\n                    labels[:, 2] = 1 - labels[:, 2]\n       \n        img = np.ascontiguousarray(img[ :, :, ::-1]) # BGR to RGB\n        if self.transforms is not None:\n            img = self.transforms(img)\n\n        return img, labels, img_path, (h, w)\n\n    def __len__(self):\n        return self.nF  # number of batches\n\n\ndef letterbox(img, height=608, width=1088, color=(127.5, 127.5, 127.5)):  # resize a rectangular image to a padded rectangular \n    shape = img.shape[:2]  # shape = [height, width]\n    ratio = min(float(height)/shape[0], float(width)/shape[1])\n    new_shape = (round(shape[1] * ratio), round(shape[0] * ratio)) # new_shape = [width, height]\n    dw = (width - new_shape[0]) / 2  # width padding\n    dh = (height - new_shape[1]) / 2  # height padding\n    top, bottom = round(dh - 0.1), round(dh + 0.1)\n    left, right = round(dw - 0.1), round(dw + 0.1)\n    img = cv2.resize(img, new_shape, interpolation=cv2.INTER_AREA)  # resized, no border\n    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # padded rectangular\n    return img, ratio, dw, dh\n\n\ndef random_affine(img, targets=None, degrees=(-10, 10), translate=(.1, .1), scale=(.9, 1.1), shear=(-2, 2),\n                  borderValue=(127.5, 127.5, 127.5)):\n    # torchvision.transforms.RandomAffine(degrees=(-10, 10), translate=(.1, .1), scale=(.9, 1.1), shear=(-10, 10))\n    # https://medium.com/uruvideo/dataset-augmentation-with-random-homographies-a8f4b44830d4\n\n    border = 0  # width of added border (optional)\n    height = img.shape[0]\n    width = img.shape[1]\n\n    # Rotation and Scale\n    R = np.eye(3)\n    a = random.random() * (degrees[1] - degrees[0]) + degrees[0]\n    # a += random.choice([-180, -90, 0, 90])  # 90deg rotations added to small rotations\n    s = random.random() * (scale[1] - scale[0]) + scale[0]\n    R[:2] = cv2.getRotationMatrix2D(angle=a, center=(img.shape[1] / 2, img.shape[0] / 2), scale=s)\n\n    # Translation\n    T = np.eye(3)\n    T[0, 2] = (random.random() * 2 - 1) * translate[0] * img.shape[0] + border  # x translation (pixels)\n    T[1, 2] = (random.random() * 2 - 1) * translate[1] * img.shape[1] + border  # y translation (pixels)\n\n    # Shear\n    S = np.eye(3)\n    S[0, 1] = math.tan((random.random() * (shear[1] - shear[0]) + shear[0]) * math.pi / 180)  # x shear (deg)\n    S[1, 0] = math.tan((random.random() * (shear[1] - shear[0]) + shear[0]) * math.pi / 180)  # y shear (deg)\n\n    M = S @ T @ R  # Combined rotation matrix. ORDER IS IMPORTANT HERE!!\n    imw = cv2.warpPerspective(img, M, dsize=(width, height), flags=cv2.INTER_LINEAR,\n                              borderValue=borderValue)  # BGR order borderValue\n\n    # Return warped points also\n    if targets is not None:\n        if len(targets) > 0:\n            n = targets.shape[0]\n            points = targets[:, 2:6].copy()\n            area0 = (points[:, 2] - points[:, 0]) * (points[:, 3] - points[:, 1])\n\n            # warp points\n            xy = np.ones((n * 4, 3))\n            xy[:, :2] = points[:, [0, 1, 2, 3, 0, 3, 2, 1]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n            xy = (xy @ M.T)[:, :2].reshape(n, 8)\n\n            # create new boxes\n            x = xy[:, [0, 2, 4, 6]]\n            y = xy[:, [1, 3, 5, 7]]\n            xy = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n\n            # apply angle-based reduction\n            radians = a * math.pi / 180\n            reduction = max(abs(math.sin(radians)), abs(math.cos(radians))) ** 0.5\n            x = (xy[:, 2] + xy[:, 0]) / 2\n            y = (xy[:, 3] + xy[:, 1]) / 2\n            w = (xy[:, 2] - xy[:, 0]) * reduction\n            h = (xy[:, 3] - xy[:, 1]) * reduction\n            xy = np.concatenate((x - w / 2, y - h / 2, x + w / 2, y + h / 2)).reshape(4, n).T\n\n            # reject warped points outside of image\n            np.clip(xy[:, 0], 0, width, out=xy[:, 0])\n            np.clip(xy[:, 2], 0, width, out=xy[:, 2])\n            np.clip(xy[:, 1], 0, height, out=xy[:, 1])\n            np.clip(xy[:, 3], 0, height, out=xy[:, 3])\n            w = xy[:, 2] - xy[:, 0]\n            h = xy[:, 3] - xy[:, 1]\n            area = w * h\n            ar = np.maximum(w / (h + 1e-16), h / (w + 1e-16))\n            i = (w > 4) & (h > 4) & (area / (area0 + 1e-16) > 0.1) & (ar < 10)\n\n            targets = targets[i]\n            targets[:, 2:6] = xy[i]\n\n        return imw, targets, M\n    else:\n        return imw\n\ndef collate_fn(batch):\n    imgs, labels, paths, sizes = zip(*batch)\n    batch_size = len(labels)\n    imgs = torch.stack(imgs, 0)\n    max_box_len = max([l.shape[0] for l in labels])\n    labels = [torch.from_numpy(l) for l in labels]\n    filled_labels = torch.zeros(batch_size, max_box_len, 6)\n    labels_len = torch.zeros(batch_size)\n\n    for i in range(batch_size):\n        isize = labels[i].shape[0]\n        if len(labels[i])>0:\n            filled_labels[i, :isize, :] = labels[i]\n        labels_len[i] = isize\n\n    return imgs, filled_labels, paths, sizes, labels_len.unsqueeze(1)\n\n\nclass JointDataset(LoadImagesAndLabels):  # for training\n    def __init__(self, root, paths, img_size=(1088,608), augment=False, transforms=None):\n        \n        dataset_names = paths.keys()\n        self.img_files = OrderedDict()\n        self.label_files = OrderedDict()\n        self.tid_num = OrderedDict()\n        self.tid_start_index = OrderedDict()\n        for ds, path in paths.items():\n            with open(path, 'r') as file:\n                self.img_files[ds] = file.readlines()\n                self.img_files[ds] = [osp.join(root, x.strip()) for x in self.img_files[ds]]\n                self.img_files[ds] = list(filter(lambda x: len(x) > 0, self.img_files[ds]))\n\n            self.label_files[ds] = [x.replace('images', 'labels_with_ids').replace('.png', '.txt').replace('.jpg', '.txt')\n                                for x in self.img_files[ds]]\n\n        for ds, label_paths in self.label_files.items():\n            max_index = -1\n            for lp in label_paths:\n                lb = np.loadtxt(lp)\n                if len(lb) < 1:\n                    continue\n                if len(lb.shape) < 2:\n                    img_max = lb[1]\n                else:\n                    img_max = np.max(lb[:,1])\n                if img_max >max_index:\n                    max_index = img_max \n            self.tid_num[ds] = max_index + 1\n        \n        last_index = 0\n        for i, (k, v) in enumerate(self.tid_num.items()):\n            self.tid_start_index[k] = last_index\n            last_index += v\n        \n        self.nID = int(last_index+1)\n        self.nds = [len(x) for x in self.img_files.values()]\n        self.cds = [sum(self.nds[:i]) for i in range(len(self.nds))]\n        self.nF = sum(self.nds)\n        self.width = img_size[0]\n        self.height = img_size[1]\n        self.augment = augment\n        self.transforms = transforms\n        \n        print('='*80)\n        print('dataset summary')\n        print(self.tid_num)\n        print('total # identities:', self.nID)\n        print('start index')\n        print(self.tid_start_index)\n        print('='*80)\n        \n\n    def __getitem__(self, files_index):\n\n        for i, c in enumerate(self.cds):\n            if files_index >= c: \n                ds = list(self.label_files.keys())[i]\n                start_index = c\n\n        img_path = self.img_files[ds][files_index - start_index]\n        label_path = self.label_files[ds][files_index - start_index]\n        \n        imgs, labels, img_path, (h, w) = self.get_data(img_path, label_path) \n        for i, _ in enumerate(labels):\n            if labels[i,1] > -1:\n                labels[i,1] += self.tid_start_index[ds]\n        \n        return imgs, labels, img_path, (h, w) \n\n\n"""
detector/tracker/utils/evaluation.py,0,"b""import os\nimport numpy as np\nimport copy\nimport motmetrics as mm\n\nfrom utils.io import read_results, unzip_objs\n\n\nclass Evaluator(object):\n\n    def __init__(self, data_root, seq_name, data_type):\n        self.data_root = data_root\n        self.seq_name = seq_name\n        self.data_type = data_type\n\n        self.load_annotations()\n        self.reset_accumulator()\n\n    def load_annotations(self):\n        assert self.data_type == 'mot'\n\n        gt_filename = os.path.join(self.data_root, self.seq_name, 'gt', 'gt.txt')\n        self.gt_frame_dict = read_results(gt_filename, self.data_type, is_gt=True)\n        self.gt_ignore_frame_dict = read_results(gt_filename, self.data_type, is_ignore=True)\n\n    def reset_accumulator(self):\n        self.acc = mm.MOTAccumulator(auto_id=True)\n\n    def eval_frame(self, frame_id, trk_tlwhs, trk_ids, rtn_events=False):\n        # results\n        trk_tlwhs = np.copy(trk_tlwhs)\n        trk_ids = np.copy(trk_ids)\n\n        # gts\n        gt_objs = self.gt_frame_dict.get(frame_id, [])\n        gt_tlwhs, gt_ids = unzip_objs(gt_objs)[:2]\n\n        # ignore boxes\n        ignore_objs = self.gt_ignore_frame_dict.get(frame_id, [])\n        ignore_tlwhs = unzip_objs(ignore_objs)[0]\n\n        # remove ignored results\n        keep = np.ones(len(trk_tlwhs), dtype=bool)\n        iou_distance = mm.distances.iou_matrix(ignore_tlwhs, trk_tlwhs, max_iou=0.5)\n        match_is, match_js = mm.lap.linear_sum_assignment(iou_distance)\n        match_is, match_js = map(lambda a: np.asarray(a, dtype=int), [match_is, match_js])\n        match_ious = iou_distance[match_is, match_js]\n\n        match_js = np.asarray(match_js, dtype=int)\n        match_js = match_js[np.logical_not(np.isnan(match_ious))]\n        keep[match_js] = False\n        trk_tlwhs = trk_tlwhs[keep]\n        trk_ids = trk_ids[keep]\n\n        # get distance matrix\n        iou_distance = mm.distances.iou_matrix(gt_tlwhs, trk_tlwhs, max_iou=0.5)\n\n        # acc\n        self.acc.update(gt_ids, trk_ids, iou_distance)\n\n        if rtn_events and iou_distance.size > 0 and hasattr(self.acc, 'last_mot_events'):\n            events = self.acc.last_mot_events  # only supported by https://github.com/longcw/py-motmetrics\n        else:\n            events = None\n        return events\n\n    def eval_file(self, filename):\n        self.reset_accumulator()\n\n        result_frame_dict = read_results(filename, self.data_type, is_gt=False)\n        frames = sorted(list(set(self.gt_frame_dict.keys()) | set(result_frame_dict.keys())))\n        for frame_id in frames:\n            trk_objs = result_frame_dict.get(frame_id, [])\n            trk_tlwhs, trk_ids = unzip_objs(trk_objs)[:2]\n            self.eval_frame(frame_id, trk_tlwhs, trk_ids, rtn_events=False)\n\n        return self.acc\n\n    @staticmethod\n    def get_summary(accs, names, metrics=('mota', 'num_switches', 'idp', 'idr', 'idf1', 'precision', 'recall')):\n        names = copy.deepcopy(names)\n        if metrics is None:\n            metrics = mm.metrics.motchallenge_metrics\n        metrics = copy.deepcopy(metrics)\n\n        mh = mm.metrics.create()\n        summary = mh.compute_many(\n            accs,\n            metrics=metrics,\n            names=names,\n            generate_overall=True\n        )\n\n        return summary\n\n    @staticmethod\n    def save_summary(summary, filename):\n        import pandas as pd\n        writer = pd.ExcelWriter(filename)\n        summary.to_excel(writer)\n        writer.save()\n"""
detector/tracker/utils/io.py,0,"b'import os\nfrom typing import Dict\nimport numpy as np\n\nfrom utils.log import logger\n\n\ndef write_results(filename, results_dict: Dict, data_type: str):\n    if not filename:\n        return\n    path = os.path.dirname(filename)\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    if data_type in (\'mot\', \'mcmot\', \'lab\'):\n        save_format = \'{frame},{id},{x1},{y1},{w},{h},1,-1,-1,-1\\n\'\n    elif data_type == \'kitti\':\n        save_format = \'{frame} {id} pedestrian -1 -1 -10 {x1} {y1} {x2} {y2} -1 -1 -1 -1000 -1000 -1000 -10 {score}\\n\'\n    else:\n        raise ValueError(data_type)\n\n    with open(filename, \'w\') as f:\n        for frame_id, frame_data in results_dict.items():\n            if data_type == \'kitti\':\n                frame_id -= 1\n            for tlwh, track_id in frame_data:\n                if track_id < 0:\n                    continue\n                x1, y1, w, h = tlwh\n                x2, y2 = x1 + w, y1 + h\n                line = save_format.format(frame=frame_id, id=track_id, x1=x1, y1=y1, x2=x2, y2=y2, w=w, h=h, score=1.0)\n                f.write(line)\n    logger.info(\'Save results to {}\'.format(filename))\n\n\ndef read_results(filename, data_type: str, is_gt=False, is_ignore=False):\n    if data_type in (\'mot\', \'lab\'):\n        read_fun = read_mot_results\n    else:\n        raise ValueError(\'Unknown data type: {}\'.format(data_type))\n\n    return read_fun(filename, is_gt, is_ignore)\n\n\n""""""\nlabels={\'ped\', ...\t\t\t% 1\n\'person_on_vhcl\', ...\t% 2\n\'car\', ...\t\t\t\t% 3\n\'bicycle\', ...\t\t\t% 4\n\'mbike\', ...\t\t\t% 5\n\'non_mot_vhcl\', ...\t\t% 6\n\'static_person\', ...\t% 7\n\'distractor\', ...\t\t% 8\n\'occluder\', ...\t\t\t% 9\n\'occluder_on_grnd\', ...\t\t%10\n\'occluder_full\', ...\t\t% 11\n\'reflection\', ...\t\t% 12\n\'crowd\' ...\t\t\t% 13\n};\n""""""\n\n\ndef read_mot_results(filename, is_gt, is_ignore):\n    valid_labels = {1}\n    ignore_labels = {2, 7, 8, 12}\n    results_dict = dict()\n    if os.path.isfile(filename):\n        with open(filename, \'r\') as f:\n            for line in f.readlines():\n                linelist = line.split(\',\')\n                if len(linelist) < 7:\n                    continue\n                fid = int(linelist[0])\n                if fid < 1:\n                    continue\n                results_dict.setdefault(fid, list())\n\n                if is_gt:\n                    if \'MOT16-\' in filename or \'MOT17-\' in filename:\n                        label = int(float(linelist[7]))\n                        mark = int(float(linelist[6]))\n                        if mark == 0 or label not in valid_labels:\n                            continue\n                    score = 1\n                elif is_ignore:\n                    if \'MOT16-\' in filename or \'MOT17-\' in filename:\n                        label = int(float(linelist[7]))\n                        vis_ratio = float(linelist[8])\n                        if label not in ignore_labels and vis_ratio >= 0:\n                            continue\n                    else:\n                        continue\n                    score = 1\n                else:\n                    score = float(linelist[6])\n\n                tlwh = tuple(map(float, linelist[2:6]))\n                target_id = int(linelist[1])\n\n                results_dict[fid].append((tlwh, target_id, score))\n\n    return results_dict\n\n\ndef unzip_objs(objs):\n    if len(objs) > 0:\n        tlwhs, ids, scores = zip(*objs)\n    else:\n        tlwhs, ids, scores = [], [], []\n    tlwhs = np.asarray(tlwhs, dtype=float).reshape(-1, 4)\n\n    return tlwhs, ids, scores'"
detector/tracker/utils/kalman_filter.py,0,"b'# vim: expandtab:ts=4:sw=4\r\nimport numpy as np\r\nimport scipy.linalg\r\n\r\n\r\n""""""\r\nTable for the 0.95 quantile of the chi-square distribution with N degrees of\r\nfreedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave\'s chi2inv\r\nfunction and used as Mahalanobis gating threshold.\r\n""""""\r\nchi2inv95 = {\r\n    1: 3.8415,\r\n    2: 5.9915,\r\n    3: 7.8147,\r\n    4: 9.4877,\r\n    5: 11.070,\r\n    6: 12.592,\r\n    7: 14.067,\r\n    8: 15.507,\r\n    9: 16.919}\r\n\r\n\r\nclass KalmanFilter(object):\r\n    """"""\r\n    A simple Kalman filter for tracking bounding boxes in image space.\r\n\r\n    The 8-dimensional state space\r\n\r\n        x, y, a, h, vx, vy, va, vh\r\n\r\n    contains the bounding box center position (x, y), aspect ratio a, height h,\r\n    and their respective velocities.\r\n\r\n    Object motion follows a constant velocity model. The bounding box location\r\n    (x, y, a, h) is taken as direct observation of the state space (linear\r\n    observation model).\r\n\r\n    """"""\r\n\r\n    def __init__(self):\r\n        ndim, dt = 4, 1.\r\n\r\n        # Create Kalman filter model matrices.\r\n        self._motion_mat = np.eye(2 * ndim, 2 * ndim)\r\n        for i in range(ndim):\r\n            self._motion_mat[i, ndim + i] = dt\r\n        self._update_mat = np.eye(ndim, 2 * ndim)\r\n\r\n        # Motion and observation uncertainty are chosen relative to the current\r\n        # state estimate. These weights control the amount of uncertainty in\r\n        # the model. This is a bit hacky.\r\n        self._std_weight_position = 1. / 20\r\n        self._std_weight_velocity = 1. / 160\r\n\r\n    def initiate(self, measurement):\r\n        """"""Create track from unassociated measurement.\r\n\r\n        Parameters\r\n        ----------\r\n        measurement : ndarray\r\n            Bounding box coordinates (x, y, a, h) with center position (x, y),\r\n            aspect ratio a, and height h.\r\n\r\n        Returns\r\n        -------\r\n        (ndarray, ndarray)\r\n            Returns the mean vector (8 dimensional) and covariance matrix (8x8\r\n            dimensional) of the new track. Unobserved velocities are initialized\r\n            to 0 mean.\r\n\r\n        """"""\r\n        mean_pos = measurement\r\n        mean_vel = np.zeros_like(mean_pos)\r\n        mean = np.r_[mean_pos, mean_vel]\r\n\r\n        std = [\r\n            2 * self._std_weight_position * measurement[3],\r\n            2 * self._std_weight_position * measurement[3],\r\n            1e-2,\r\n            2 * self._std_weight_position * measurement[3],\r\n            10 * self._std_weight_velocity * measurement[3],\r\n            10 * self._std_weight_velocity * measurement[3],\r\n            1e-5,\r\n            10 * self._std_weight_velocity * measurement[3]]\r\n        covariance = np.diag(np.square(std))\r\n        return mean, covariance\r\n\r\n    def predict(self, mean, covariance):\r\n        """"""Run Kalman filter prediction step.\r\n\r\n        Parameters\r\n        ----------\r\n        mean : ndarray\r\n            The 8 dimensional mean vector of the object state at the previous\r\n            time step.\r\n        covariance : ndarray\r\n            The 8x8 dimensional covariance matrix of the object state at the\r\n            previous time step.\r\n\r\n        Returns\r\n        -------\r\n        (ndarray, ndarray)\r\n            Returns the mean vector and covariance matrix of the predicted\r\n            state. Unobserved velocities are initialized to 0 mean.\r\n\r\n        """"""\r\n        std_pos = [\r\n            self._std_weight_position * mean[3],\r\n            self._std_weight_position * mean[3],\r\n            1e-2,\r\n            self._std_weight_position * mean[3]]\r\n        std_vel = [\r\n            self._std_weight_velocity * mean[3],\r\n            self._std_weight_velocity * mean[3],\r\n            1e-5,\r\n            self._std_weight_velocity * mean[3]]\r\n        motion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))\r\n\r\n        mean = np.dot(self._motion_mat, mean)\r\n        covariance = np.linalg.multi_dot((\r\n            self._motion_mat, covariance, self._motion_mat.T)) + motion_cov\r\n\r\n        return mean, covariance\r\n\r\n    def project(self, mean, covariance):\r\n        """"""Project state distribution to measurement space.\r\n\r\n        Parameters\r\n        ----------\r\n        mean : ndarray\r\n            The state\'s mean vector (8 dimensional array).\r\n        covariance : ndarray\r\n            The state\'s covariance matrix (8x8 dimensional).\r\n\r\n        Returns\r\n        -------\r\n        (ndarray, ndarray)\r\n            Returns the projected mean and covariance matrix of the given state\r\n            estimate.\r\n\r\n        """"""\r\n        std = [\r\n            self._std_weight_position * mean[3],\r\n            self._std_weight_position * mean[3],\r\n            1e-1,\r\n            self._std_weight_position * mean[3]]\r\n        innovation_cov = np.diag(np.square(std))\r\n\r\n        mean = np.dot(self._update_mat, mean)\r\n        covariance = np.linalg.multi_dot((\r\n            self._update_mat, covariance, self._update_mat.T))\r\n        return mean, covariance + innovation_cov\r\n\r\n    def update(self, mean, covariance, measurement):\r\n        """"""Run Kalman filter correction step.\r\n\r\n        Parameters\r\n        ----------\r\n        mean : ndarray\r\n            The predicted state\'s mean vector (8 dimensional).\r\n        covariance : ndarray\r\n            The state\'s covariance matrix (8x8 dimensional).\r\n        measurement : ndarray\r\n            The 4 dimensional measurement vector (x, y, a, h), where (x, y)\r\n            is the center position, a the aspect ratio, and h the height of the\r\n            bounding box.\r\n\r\n        Returns\r\n        -------\r\n        (ndarray, ndarray)\r\n            Returns the measurement-corrected state distribution.\r\n\r\n        """"""\r\n        projected_mean, projected_cov = self.project(mean, covariance)\r\n\r\n        chol_factor, lower = scipy.linalg.cho_factor(\r\n            projected_cov, lower=True, check_finite=False)\r\n        kalman_gain = scipy.linalg.cho_solve(\r\n            (chol_factor, lower), np.dot(covariance, self._update_mat.T).T,\r\n            check_finite=False).T\r\n        innovation = measurement - projected_mean\r\n\r\n        new_mean = mean + np.dot(innovation, kalman_gain.T)\r\n        new_covariance = covariance - np.linalg.multi_dot((\r\n            kalman_gain, projected_cov, kalman_gain.T))\r\n        return new_mean, new_covariance\r\n\r\n    def gating_distance(self, mean, covariance, measurements,\r\n                        only_position=False):\r\n        """"""Compute gating distance between state distribution and measurements.\r\n\r\n        A suitable distance threshold can be obtained from `chi2inv95`. If\r\n        `only_position` is False, the chi-square distribution has 4 degrees of\r\n        freedom, otherwise 2.\r\n\r\n        Parameters\r\n        ----------\r\n        mean : ndarray\r\n            Mean vector over the state distribution (8 dimensional).\r\n        covariance : ndarray\r\n            Covariance of the state distribution (8x8 dimensional).\r\n        measurements : ndarray\r\n            An Nx4 dimensional matrix of N measurements, each in\r\n            format (x, y, a, h) where (x, y) is the bounding box center\r\n            position, a the aspect ratio, and h the height.\r\n        only_position : Optional[bool]\r\n            If True, distance computation is done with respect to the bounding\r\n            box center position only.\r\n\r\n        Returns\r\n        -------\r\n        ndarray\r\n            Returns an array of length N, where the i-th element contains the\r\n            squared Mahalanobis distance between (mean, covariance) and\r\n            `measurements[i]`.\r\n\r\n        """"""\r\n        mean, covariance = self.project(mean, covariance)\r\n        if only_position:\r\n            mean, covariance = mean[:2], covariance[:2, :2]\r\n            measurements = measurements[:, :2]\r\n\r\n        cholesky_factor = np.linalg.cholesky(covariance)\r\n        d = measurements - mean\r\n        z = scipy.linalg.solve_triangular(\r\n            cholesky_factor, d.T, lower=True, check_finite=False,\r\n            overwrite_b=True)\r\n        squared_maha = np.sum(z * z, axis=0)\r\n        return squared_maha'"
detector/tracker/utils/log.py,0,"b""import logging\n\n\ndef get_logger(name='root'):\n    formatter = logging.Formatter(\n        # fmt='%(asctime)s [%(levelname)s]: %(filename)s(%(funcName)s:%(lineno)s) >> %(message)s')\n        fmt='%(asctime)s [%(levelname)s]: %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    logger.addHandler(handler)\n    return logger\n\n\nlogger = get_logger('root')\n"""
detector/tracker/utils/nms.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n# from ._utils import _C\nfrom utils import _C\n\nnms = _C.nms\n# nms.__doc__ = """"""\n# This function performs Non-maximum suppresion""""""\n'"
detector/tracker/utils/parse_config.py,0,"b'def parse_model_cfg(path):\n    """"""Parses the yolo-v3 layer configuration file and returns module definitions""""""\n    file = open(path, \'r\')\n    lines = file.read().split(\'\\n\')\n    lines = [x for x in lines if x and not x.startswith(\'#\')]\n    lines = [x.rstrip().lstrip() for x in lines]  # get rid of fringe whitespaces\n    module_defs = []\n    for line in lines:\n        if line.startswith(\'[\'):  # This marks the start of a new block\n            module_defs.append({})\n            module_defs[-1][\'type\'] = line[1:-1].rstrip()\n            if module_defs[-1][\'type\'] == \'convolutional\':\n                module_defs[-1][\'batch_normalize\'] = 0\n        else:\n            key, value = line.split(""="")\n            value = value.strip()\n            module_defs[-1][key.rstrip()] = value.strip()\n\n    return module_defs\n\n\ndef parse_data_cfg(path):\n    """"""Parses the data configuration file""""""\n    options = dict()\n    options[\'gpus\'] = \'0\'\n    options[\'num_workers\'] = \'10\'\n    with open(path, \'r\') as fp:\n        lines = fp.readlines()\n    for line in lines:\n        line = line.strip()\n        if line == \'\' or line.startswith(\'#\'):\n            continue\n        key, value = line.split(\'=\')\n        options[key.strip()] = value.strip()\n    return options\n'"
detector/tracker/utils/timer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport time\n\n\nclass Timer(object):\n    """"""A simple timer.""""""\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n        self.duration = 0.\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            self.duration = self.average_time\n        else:\n            self.duration = self.diff\n        return self.duration\n\n    def clear(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n        self.duration = 0.\n\n'"
detector/tracker/utils/utils.py,52,"b'import glob\nimport random\nimport time\nimport os\nimport os.path as osp\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nimport platform\nif platform.system() != \'Windows\':\n    from detector.nms import nms_wrapper\n\n# Set printoptions\ntorch.set_printoptions(linewidth=1320, precision=5, profile=\'long\')\nnp.set_printoptions(linewidth=320, formatter={\'float_kind\': \'{:11.5g}\'.format})  # format short g, %precision=5\n\ndef mkdir_if_missing(d):\n    if not osp.exists(d):\n        os.makedirs(d)\n\n\ndef float3(x):  # format floats to 3 decimals\n    return float(format(x, \'.3f\'))\n\n\ndef init_seeds(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef load_classes(path):\n    """"""\n    Loads class labels at \'path\'\n    """"""\n    fp = open(path, \'r\')\n    names = fp.read().split(\'\\n\')\n    return list(filter(None, names))  # filter removes empty strings (such as last line)\n\n\ndef model_info(model):  # Plots a line-by-line description of a PyTorch model\n    n_p = sum(x.numel() for x in model.parameters())  # number parameters\n    n_g = sum(x.numel() for x in model.parameters() if x.requires_grad)  # number gradients\n    print(\'\\n%5s %50s %9s %12s %20s %12s %12s\' % (\'layer\', \'name\', \'gradient\', \'parameters\', \'shape\', \'mu\', \'sigma\'))\n    for i, (name, p) in enumerate(model.named_parameters()):\n        name = name.replace(\'module_list.\', \'\')\n        print(\'%5g %50s %9s %12g %20s %12.3g %12.3g\' % (\n            i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std()))\n    print(\'Model Summary: %g layers, %g parameters, %g gradients\\n\' % (i + 1, n_p, n_g))\n\n\n\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):  # Plots one bounding box on image img\n    tl = line_thickness or round(0.0004 * max(img.shape[0:2])) + 1  # line thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n\n\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.03)\n    elif classname.find(\'BatchNorm2d\') != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.03)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n\n\ndef xyxy2xywh(x):\n    # Convert bounding box format from [x1, y1, x2, y2] to [x, y, w, h]\n    y = torch.zeros(x.shape) if x.dtype is torch.float32 else np.zeros(x.shape)\n    y[:, 0] = (x[:, 0] + x[:, 2]) / 2\n    y[:, 1] = (x[:, 1] + x[:, 3]) / 2\n    y[:, 2] = x[:, 2] - x[:, 0]\n    y[:, 3] = x[:, 3] - x[:, 1]\n    return y\n\n\ndef xywh2xyxy(x):\n    # Convert bounding box format from [x, y, w, h] to [x1, y1, x2, y2]\n    y = torch.zeros(x.shape) if x.dtype is torch.float32 else np.zeros(x.shape)\n    y[:, 0] = (x[:, 0] - x[:, 2] / 2)\n    y[:, 1] = (x[:, 1] - x[:, 3] / 2)\n    y[:, 2] = (x[:, 0] + x[:, 2] / 2)\n    y[:, 3] = (x[:, 1] + x[:, 3] / 2)\n    return y\n\n\ndef scale_coords(img_size, coords, img0_shape):\n    # Rescale x1, y1, x2, y2 from 416 to image size\n    coords = coords.cpu().clone()\n    gain_w = float(img_size[0]) / img0_shape[0]  # gain  = old / new\n    gain_h = float(img_size[1]) / img0_shape[1]\n    gain = min(gain_w, gain_h)\n    pad_x = (img_size[0] - img0_shape[0] * gain) / 2  # width padding\n    pad_y = (img_size[1] - img0_shape[1] * gain) / 2  # height padding\n    coords[:, [0, 2]] -= pad_x\n    coords[:, [1, 3]] -= pad_y\n    coords[:, 0:4] /= gain\n    coords[:, :4] = torch.clamp(coords[:, :4], min=0)\n    return coords\n\n\ndef ap_per_class(tp, conf, pred_cls, target_cls):\n    """""" Compute the average precision, given the recall and precision curves.\n    Method originally from https://github.com/rafaelpadilla/Object-Detection-Metrics.\n    # Arguments\n        tp:    True positives (list).\n        conf:  Objectness value from 0-1 (list).\n        pred_cls: Predicted object classes (list).\n        target_cls: True object classes (list).\n    # Returns\n        The average precision as computed in py-faster-rcnn.\n    """"""\n\n    # lists/pytorch to numpy\n    tp, conf, pred_cls, target_cls = np.array(tp), np.array(conf), np.array(pred_cls), np.array(target_cls)\n\n    # Sort by objectness\n    i = np.argsort(-conf)\n    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n\n    # Find unique classes\n    unique_classes = np.unique(np.concatenate((pred_cls, target_cls), 0))\n\n    # Create Precision-Recall curve and compute AP for each class\n    ap, p, r = [], [], []\n    for c in unique_classes:\n        i = pred_cls == c\n        n_gt = sum(target_cls == c)  # Number of ground truth objects\n        n_p = sum(i)  # Number of predicted objects\n\n        if (n_p == 0) and (n_gt == 0):\n            continue\n        elif (n_p == 0) or (n_gt == 0):\n            ap.append(0)\n            r.append(0)\n            p.append(0)\n        else:\n            # Accumulate FPs and TPs\n            fpc = np.cumsum(1 - tp[i])\n            tpc = np.cumsum(tp[i])\n\n            # Recall\n            recall_curve = tpc / (n_gt + 1e-16)\n            r.append(tpc[-1] / (n_gt + 1e-16))\n\n            # Precision\n            precision_curve = tpc / (tpc + fpc)\n            p.append(tpc[-1] / (tpc[-1] + fpc[-1]))\n\n            # AP from recall-precision curve\n            ap.append(compute_ap(recall_curve, precision_curve))\n\n    return np.array(ap), unique_classes.astype(\'int32\'), np.array(r), np.array(p)\n\n\ndef compute_ap(recall, precision):\n    """""" Compute the average precision, given the recall and precision curves.\n    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n    # Arguments\n        recall:    The recall curve (list).\n        precision: The precision curve (list).\n    # Returns\n        The average precision as computed in py-faster-rcnn.\n    """"""\n    # correct AP calculation\n    # first append sentinel values at the end\n\n    mrec = np.concatenate(([0.], recall, [1.]))\n    mpre = np.concatenate(([0.], precision, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef bbox_iou(box1, box2, x1y1x2y2=False):\n    """"""\n    Returns the IoU of two bounding boxes\n    """"""\n    N, M = len(box1), len(box2)\n    if x1y1x2y2:\n        # Get the coordinates of bounding boxes\n        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n    else:\n        # Transform from center and width to exact coordinates\n        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n\n    # get the coordinates of the intersection rectangle\n    inter_rect_x1 = torch.max(b1_x1.unsqueeze(1), b2_x1)\n    inter_rect_y1 = torch.max(b1_y1.unsqueeze(1), b2_y1)\n    inter_rect_x2 = torch.min(b1_x2.unsqueeze(1), b2_x2)\n    inter_rect_y2 = torch.min(b1_y2.unsqueeze(1), b2_y2)\n    # Intersection area\n    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1, 0) * torch.clamp(inter_rect_y2 - inter_rect_y1, 0)\n    # Union Area\n    b1_area = ((b1_x2 - b1_x1) * (b1_y2 - b1_y1))\n    b1_area = ((b1_x2 - b1_x1) * (b1_y2 - b1_y1)).view(-1,1).expand(N,M)\n    b2_area = ((b2_x2 - b2_x1) * (b2_y2 - b2_y1)).view(1,-1).expand(N,M)\n\n    return inter_area / (b1_area + b2_area - inter_area + 1e-16)\n\n\ndef build_targets_max(target, anchor_wh, nA, nC, nGh, nGw):\n    """"""\n    returns nT, nCorrect, tx, ty, tw, th, tconf, tcls\n    """"""\n    nB = len(target)  # number of images in batch\n\n    txy = torch.zeros(nB, nA, nGh, nGw, 2).cuda()  # batch size, anchors, grid size\n    twh = torch.zeros(nB, nA, nGh, nGw, 2).cuda()\n    tconf = torch.LongTensor(nB, nA, nGh, nGw).fill_(0).cuda()\n    tcls = torch.ByteTensor(nB, nA, nGh, nGw, nC).fill_(0).cuda()  # nC = number of classes\n    tid = torch.LongTensor(nB, nA, nGh, nGw, 1).fill_(-1).cuda() \n    for b in range(nB):\n        t = target[b]\n        t_id = t[:, 1].clone().long().cuda()\n        t = t[:,[0,2,3,4,5]]\n        nTb = len(t)  # number of targets\n        if nTb == 0:\n            continue\n\n        #gxy, gwh = t[:, 1:3] * nG, t[:, 3:5] * nG\n        gxy, gwh = t[: , 1:3].clone() , t[:, 3:5].clone()\n        gxy[:, 0] = gxy[:, 0] * nGw\n        gxy[:, 1] = gxy[:, 1] * nGh\n        gwh[:, 0] = gwh[:, 0] * nGw\n        gwh[:, 1] = gwh[:, 1] * nGh\n        gi = torch.clamp(gxy[:, 0], min=0, max=nGw -1).long()\n        gj = torch.clamp(gxy[:, 1], min=0, max=nGh -1).long()\n\n        # Get grid box indices and prevent overflows (i.e. 13.01 on 13 anchors)\n        #gi, gj = torch.clamp(gxy.long(), min=0, max=nG - 1).t()\n        #gi, gj = gxy.long().t()\n\n        # iou of targets-anchors (using wh only)\n        box1 = gwh\n        box2 = anchor_wh.unsqueeze(1)\n        inter_area = torch.min(box1, box2).prod(2)\n        iou = inter_area / (box1.prod(1) + box2.prod(2) - inter_area + 1e-16)\n\n        # Select best iou_pred and anchor\n        iou_best, a = iou.max(0)  # best anchor [0-2] for each target\n\n        # Select best unique target-anchor combinations\n        if nTb > 1:\n            _, iou_order = torch.sort(-iou_best)  # best to worst\n\n            # Unique anchor selection\n            u = torch.stack((gi, gj, a), 0)[:, iou_order]\n            # _, first_unique = np.unique(u, axis=1, return_index=True)  # first unique indices\n            first_unique = return_torch_unique_index(u, torch.unique(u, dim=1))  # torch alternative\n            i = iou_order[first_unique]\n            # best anchor must share significant commonality (iou) with target\n            i = i[iou_best[i] > 0.60]  # TODO: examine arbitrary threshold\n            if len(i) == 0:\n                continue\n\n            a, gj, gi, t = a[i], gj[i], gi[i], t[i]\n            t_id = t_id[i]\n            if len(t.shape) == 1:\n                t = t.view(1, 5)\n        else:\n            if iou_best < 0.60:\n                continue\n        \n        tc, gxy, gwh = t[:, 0].long(), t[:, 1:3].clone(), t[:, 3:5].clone()\n        gxy[:, 0] = gxy[:, 0] * nGw\n        gxy[:, 1] = gxy[:, 1] * nGh\n        gwh[:, 0] = gwh[:, 0] * nGw\n        gwh[:, 1] = gwh[:, 1] * nGh\n\n        # XY coordinates\n        txy[b, a, gj, gi] = gxy - gxy.floor()\n\n        # Width and height\n        twh[b, a, gj, gi] = torch.log(gwh / anchor_wh[a])  # yolo method\n        # twh[b, a, gj, gi] = torch.sqrt(gwh / anchor_wh[a]) / 2 # power method\n\n        # One-hot encoding of label\n        tcls[b, a, gj, gi, tc] = 1\n        tconf[b, a, gj, gi] = 1\n        tid[b, a, gj, gi] = t_id.unsqueeze(1)\n    tbox = torch.cat([txy, twh], -1)\n    return tconf, tbox, tid\n\n\n\ndef build_targets_thres(target, anchor_wh, nA, nC, nGh, nGw):\n    ID_THRESH = 0.5\n    FG_THRESH = 0.5\n    BG_THRESH = 0.4\n    nB = len(target)  # number of images in batch\n    assert(len(anchor_wh)==nA)\n\n    tbox = torch.zeros(nB, nA, nGh, nGw, 4).cuda()  # batch size, anchors, grid size\n    tconf = torch.LongTensor(nB, nA, nGh, nGw).fill_(0).cuda()\n    tid = torch.LongTensor(nB, nA, nGh, nGw, 1).fill_(-1).cuda() \n    for b in range(nB):\n        t = target[b]\n        t_id = t[:, 1].clone().long().cuda()\n        t = t[:,[0,2,3,4,5]]\n        nTb = len(t)  # number of targets\n        if nTb == 0:\n            continue\n\n        gxy, gwh = t[: , 1:3].clone() , t[:, 3:5].clone()\n        gxy[:, 0] = gxy[:, 0] * nGw\n        gxy[:, 1] = gxy[:, 1] * nGh\n        gwh[:, 0] = gwh[:, 0] * nGw\n        gwh[:, 1] = gwh[:, 1] * nGh\n        gxy[:, 0] = torch.clamp(gxy[:, 0], min=0, max=nGw -1)\n        gxy[:, 1] = torch.clamp(gxy[:, 1], min=0, max=nGh -1)\n\n        gt_boxes = torch.cat([gxy, gwh], dim=1)                                            # Shape Ngx4 (xc, yc, w, h)\n        \n        anchor_mesh = generate_anchor(nGh, nGw, anchor_wh)\n        anchor_list = anchor_mesh.permute(0,2,3,1).contiguous().view(-1, 4)              # Shpae (nA x nGh x nGw) x 4\n        #print(anchor_list.shape, gt_boxes.shape)\n        iou_pdist = bbox_iou(anchor_list, gt_boxes)                                      # Shape (nA x nGh x nGw) x Ng\n        iou_max, max_gt_index = torch.max(iou_pdist, dim=1)                              # Shape (nA x nGh x nGw), both\n\n        iou_map = iou_max.view(nA, nGh, nGw)       \n        gt_index_map = max_gt_index.view(nA, nGh, nGw)\n\n        #nms_map = pooling_nms(iou_map, 3)\n        \n        id_index = iou_map > ID_THRESH\n        fg_index = iou_map > FG_THRESH                                                    \n        bg_index = iou_map < BG_THRESH \n        ign_index = (iou_map < FG_THRESH) * (iou_map > BG_THRESH)\n        tconf[b][fg_index] = 1\n        tconf[b][bg_index] = 0\n        tconf[b][ign_index] = -1\n\n        gt_index = gt_index_map[fg_index]\n        gt_box_list = gt_boxes[gt_index]\n        gt_id_list = t_id[gt_index_map[id_index]]\n        #print(gt_index.shape, gt_index_map[id_index].shape, gt_boxes.shape)\n        if torch.sum(fg_index) > 0:\n            tid[b][id_index] =  gt_id_list.unsqueeze(1)\n            fg_anchor_list = anchor_list.view(nA, nGh, nGw, 4)[fg_index] \n            delta_target = encode_delta(gt_box_list, fg_anchor_list)\n            tbox[b][fg_index] = delta_target\n    return tconf, tbox, tid\n\ndef generate_anchor(nGh, nGw, anchor_wh):\n    nA = len(anchor_wh)\n    yy, xx =torch.meshgrid(torch.arange(nGh), torch.arange(nGw))\n\n    mesh = torch.stack([xx, yy], dim=0).to(anchor_wh)                                # Shape 2, nGh, nGw\n    mesh = mesh.unsqueeze(0).repeat(nA,1,1,1).float()                                # Shape nA x 2 x nGh x nGw\n    anchor_offset_mesh = anchor_wh.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, nGh,nGw) # Shape nA x 2 x nGh x nGw\n    anchor_mesh = torch.cat([mesh, anchor_offset_mesh], dim=1)                       # Shape nA x 4 x nGh x nGw\n    return anchor_mesh\n\ndef encode_delta(gt_box_list, fg_anchor_list):\n    px, py, pw, ph = fg_anchor_list[:, 0], fg_anchor_list[:,1], \\\n                     fg_anchor_list[:, 2], fg_anchor_list[:,3]\n    gx, gy, gw, gh = gt_box_list[:, 0], gt_box_list[:, 1], \\\n                     gt_box_list[:, 2], gt_box_list[:, 3]\n    dx = (gx - px) / pw\n    dy = (gy - py) / ph\n    dw = torch.log(gw/pw)\n    dh = torch.log(gh/ph)\n    return torch.stack([dx, dy, dw, dh], dim=1)\n\ndef decode_delta(delta, fg_anchor_list):\n    px, py, pw, ph = fg_anchor_list[:, 0], fg_anchor_list[:,1], \\\n                     fg_anchor_list[:, 2], fg_anchor_list[:,3]\n    dx, dy, dw, dh = delta[:, 0], delta[:, 1], delta[:, 2], delta[:, 3]\n    gx = pw * dx + px\n    gy = ph * dy + py\n    gw = pw * torch.exp(dw)\n    gh = ph * torch.exp(dh)\n    return torch.stack([gx, gy, gw, gh], dim=1)\n\ndef decode_delta_map(delta_map, anchors):\n    \'\'\'\n    :param: delta_map, shape (nB, nA, nGh, nGw, 4)\n    :param: anchors, shape (nA,4)\n    \'\'\'\n    nB, nA, nGh, nGw, _ = delta_map.shape\n    anchor_mesh = generate_anchor(nGh, nGw, anchors) \n    anchor_mesh = anchor_mesh.permute(0,2,3,1).contiguous()              # Shpae (nA x nGh x nGw) x 4\n    anchor_mesh = anchor_mesh.unsqueeze(0).repeat(nB,1,1,1,1)\n    pred_list = decode_delta(delta_map.view(-1,4), anchor_mesh.view(-1,4))\n    pred_map = pred_list.view(nB, nA, nGh, nGw, 4)\n    return pred_map\n\n\ndef pooling_nms(heatmap, kernel=1):\n    pad = (kernel -1 ) // 2\n    hmax = F.max_pool2d(heatmap, (kernel, kernel), stride=1, padding=pad)\n    keep = (hmax == heatmap).float()\n    return keep * heatmap\n\ndef soft_nms(dets, sigma=0.5, Nt=0.3, threshold=0.05, method=1):\n    keep = cpu_soft_nms(np.ascontiguousarray(dets, dtype=np.float32),\n            np.float32(sigma), np.float32(Nt),\n            np.float32(threshold),\n            np.uint8(method))\n    return keep\n\ndef non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.4, method=-1):\n    """"""\n    Removes detections with lower object confidence score than \'conf_thres\'\n    Non-Maximum Suppression to further filter detections.\n    Returns detections with shape:\n        (x1, y1, x2, y2, object_conf, class_score, class_pred)\n    """"""\n\n    output = [None for _ in range(len(prediction))]\n    for image_i, pred in enumerate(prediction):\n        # Filter out confidence scores below threshold\n        # Get score and class with highest confidence\n\n        v = pred[:, 4] > conf_thres\n        v = v.nonzero().squeeze()\n        if len(v.shape) == 0:\n            v = v.unsqueeze(0)\n\n        pred = pred[v]\n\n        # If none are remaining => process next image\n        nP = pred.shape[0]\n        if not nP:\n            continue\n        # From (center x, center y, width, height) to (x1, y1, x2, y2)\n        pred[:, :4] = xywh2xyxy(pred[:, :4])\n\n        \n        # Non-maximum suppression\n        if method == -1 and platform.system() != \'Windows\':\n            #We use faster rcnn implementation of nms (soft nms is optional)\n            nms_op = getattr(nms_wrapper, \'nms\')\n            #nms_op input:(n,(x1,y1,x2,y2,c))\n            #nms_op output: input[inds,:], inds\n            _, nms_indices = nms_op(pred[:,:5], nms_thres)\n            \n        else:\n            dets = pred[:, :5].clone().contiguous().data.cpu().numpy()\n            nms_indices = soft_nms(dets, Nt=nms_thres, method=method)\n        det_max = pred[nms_indices]        \n\n        if len(det_max) > 0:\n            # Add max detections to outputs\n            output[image_i] = det_max if output[image_i] is None else torch.cat((output[image_i], det_max))\n\n    return output\n\n\ndef return_torch_unique_index(u, uv):\n    n = uv.shape[1]  # number of columns\n    first_unique = torch.zeros(n, device=u.device).long()\n    for j in range(n):\n        first_unique[j] = (uv[:, j:j + 1] == u).all(0).nonzero()[0]\n\n    return first_unique\n\n\ndef strip_optimizer_from_checkpoint(filename=\'weights/best.pt\'):\n    # Strip optimizer from *.pt files for lighter files (reduced by 2/3 size)\n\n    a = torch.load(filename, map_location=\'cpu\')\n    a[\'optimizer\'] = []\n    torch.save(a, filename.replace(\'.pt\', \'_lite.pt\'))\n\n\ndef plot_results():\n    # Plot YOLO training results file \'results.txt\'\n    # import os; os.system(\'wget https://storage.googleapis.com/ultralytics/yolov3/results_v1.txt\')\n\n    plt.figure(figsize=(14, 7))\n    s = [\'X + Y\', \'Width + Height\', \'Confidence\', \'Classification\', \'Total Loss\', \'mAP\', \'Recall\', \'Precision\']\n    files = sorted(glob.glob(\'results*.txt\'))\n    for f in files:\n        results = np.loadtxt(f, usecols=[2, 3, 4, 5, 6, 9, 10, 11]).T  # column 11 is mAP\n        x = range(1, results.shape[1])\n        for i in range(8):\n            plt.subplot(2, 4, i + 1)\n            plt.plot(x, results[i, x], marker=\'.\', label=f)\n            plt.title(s[i])\n            if i == 0:\n                plt.legend()\n'"
detector/tracker/utils/visualization.py,0,"b""import numpy as np\nimport cv2\n\n\ndef tlwhs_to_tlbrs(tlwhs):\n    tlbrs = np.copy(tlwhs)\n    if len(tlbrs) == 0:\n        return tlbrs\n    tlbrs[:, 2] += tlwhs[:, 0]\n    tlbrs[:, 3] += tlwhs[:, 1]\n    return tlbrs\n\n\ndef get_color(idx):\n    idx = idx * 3\n    color = ((37 * idx) % 255, (17 * idx) % 255, (29 * idx) % 255)\n\n    return color\n\n\ndef resize_image(image, max_size=800):\n    if max(image.shape[:2]) > max_size:\n        scale = float(max_size) / max(image.shape[:2])\n        image = cv2.resize(image, None, fx=scale, fy=scale)\n    return image\n\n\ndef plot_tracking(image, tlwhs, obj_ids, scores=None, frame_id=0, fps=0., ids2=None):\n    im = np.ascontiguousarray(np.copy(image))\n    im_h, im_w = im.shape[:2]\n\n    top_view = np.zeros([im_w, im_w, 3], dtype=np.uint8) + 255\n\n    text_scale = max(1, image.shape[1] / 1600.)\n    text_thickness = 1 if text_scale > 1.1 else 1\n    line_thickness = max(1, int(image.shape[1] / 500.))\n\n    radius = max(5, int(im_w/140.))\n    cv2.putText(im, 'frame: %d fps: %.2f num: %d' % (frame_id, fps, len(tlwhs)),\n                (0, int(15 * text_scale)), cv2.FONT_HERSHEY_PLAIN, text_scale, (0, 0, 255), thickness=2)\n\n    for i, tlwh in enumerate(tlwhs):\n        x1, y1, w, h = tlwh\n        intbox = tuple(map(int, (x1, y1, x1 + w, y1 + h)))\n        obj_id = int(obj_ids[i])\n        id_text = '{}'.format(int(obj_id))\n        if ids2 is not None:\n            id_text = id_text + ', {}'.format(int(ids2[i]))\n        _line_thickness = 1 if obj_id <= 0 else line_thickness\n        color = get_color(abs(obj_id))\n        cv2.rectangle(im, intbox[0:2], intbox[2:4], color=color, thickness=line_thickness)\n        cv2.putText(im, id_text, (intbox[0], intbox[1] + 30), cv2.FONT_HERSHEY_PLAIN, text_scale, (0, 0, 255),\n                    thickness=text_thickness)\n    return im\n\n\ndef plot_trajectory(image, tlwhs, track_ids):\n    image = image.copy()\n    for one_tlwhs, track_id in zip(tlwhs, track_ids):\n        color = get_color(int(track_id))\n        for tlwh in one_tlwhs:\n            x1, y1, w, h = tuple(map(int, tlwh))\n            cv2.circle(image, (int(x1 + 0.5 * w), int(y1 + h)), 2, color, thickness=2)\n\n    return image\n\n\ndef plot_detections(image, tlbrs, scores=None, color=(255, 0, 0), ids=None):\n    im = np.copy(image)\n    text_scale = max(1, image.shape[1] / 800.)\n    thickness = 2 if text_scale > 1.3 else 1\n    for i, det in enumerate(tlbrs):\n        x1, y1, x2, y2 = np.asarray(det[:4], dtype=np.int)\n        if len(det) >= 7:\n            label = 'det' if det[5] > 0 else 'trk'\n            if ids is not None:\n                text = '{}# {:.2f}: {:d}'.format(label, det[6], ids[i])\n                cv2.putText(im, text, (x1, y1 + 30), cv2.FONT_HERSHEY_PLAIN, text_scale, (0, 255, 255),\n                            thickness=thickness)\n            else:\n                text = '{}# {:.2f}'.format(label, det[6])\n\n        if scores is not None:\n            text = '{:.2f}'.format(scores[i])\n            cv2.putText(im, text, (x1, y1 + 30), cv2.FONT_HERSHEY_PLAIN, text_scale, (0, 255, 255),\n                        thickness=thickness)\n\n        cv2.rectangle(im, (x1, y1), (x2, y2), color, 2)\n\n    return im\n"""
alphapose/models/layers/dcn/DCN.py,1,"b""# -----------------------------------------------------\n# Copyright (c) Shanghai Jiao Tong University. All rights reserved.\n# Written by Jiefeng Li (jeff.lee.sjtu@gmail.com)\n# -----------------------------------------------------\n\nimport torch.nn as nn\n\nfrom . import DeformConv, ModulatedDeformConv\n\n\nclass DCN(nn.Module):\n    '''\n    Initialize: inplanes, planes, upscale_factor\n    OUTPUT: (planes // upscale_factor^2) * ht * wd\n    '''\n\n    def __init__(self, inplanes, planes, dcn,\n                 kernel_size, stride=1,\n                 padding=0, bias=False):\n        super(DCN, self).__init__()\n        fallback_on_stride = dcn.get('FALLBACK_ON_STRIDE', False)\n        self.with_modulated_dcn = dcn.get('MODULATED', False)\n        if fallback_on_stride:\n            self.conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size, stride=stride,\n                                  padding=padding, bias=bias)\n        else:\n            self.deformable_groups = dcn.get('DEFORM_GROUP', 1)\n            if not self.with_modulated_dcn:\n                conv_op = DeformConv\n                offset_channels = 18\n            else:\n                conv_op = ModulatedDeformConv\n                offset_channels = 27\n\n            self.conv_offset = nn.Conv2d(\n                inplanes,\n                self.deformable_groups * offset_channels,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=padding)\n            self.conv = conv_op(\n                inplanes,\n                planes,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=padding,\n                deformable_groups=self.deformable_groups,\n                bias=bias)\n\n    def forward(self, x):\n        if self.with_modulated_dcn:\n            offset_mask = self.conv_offset(x)\n            offset = offset_mask[:, :18 * self.deformable_groups, :, :]\n            mask = offset_mask[:, -9 * self.deformable_groups:, :, :]\n            mask = mask.sigmoid()\n            out = self.conv(x, offset, mask)\n        else:\n            offset = self.conv_offset(x)\n            out = self.conv(x, offset)\n\n        return out\n"""
alphapose/models/layers/dcn/__init__.py,0,"b""from .deform_conv import (DeformConv, DeformConvPack, ModulatedDeformConv,\n                          ModulatedDeformConvPack, deform_conv,\n                          modulated_deform_conv)\nfrom .deform_pool import (DeformRoIPooling, DeformRoIPoolingPack,\n                          ModulatedDeformRoIPoolingPack, deform_roi_pooling)\nfrom .DCN import DCN\n\n__all__ = [\n    'DeformConv', 'DeformConvPack', 'ModulatedDeformConv',\n    'ModulatedDeformConvPack', 'DeformRoIPooling', 'DeformRoIPoolingPack',\n    'ModulatedDeformRoIPoolingPack', 'deform_conv', 'modulated_deform_conv',\n    'deform_roi_pooling', 'DCN'\n]\n"""
alphapose/models/layers/dcn/deform_conv.py,18,"b'import math\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.nn.modules.utils import _pair\n\nfrom . import deform_conv_cuda\n\n\nclass DeformConvFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                input,\n                offset,\n                weight,\n                stride=1,\n                padding=0,\n                dilation=1,\n                groups=1,\n                deformable_groups=1,\n                im2col_step=64):\n        if input is not None and input.dim() != 4:\n            raise ValueError(\n                ""Expected 4D tensor as input, got {}D tensor instead."".format(\n                    input.dim()))\n        ctx.stride = _pair(stride)\n        ctx.padding = _pair(padding)\n        ctx.dilation = _pair(dilation)\n        ctx.groups = groups\n        ctx.deformable_groups = deformable_groups\n        ctx.im2col_step = im2col_step\n\n        ctx.save_for_backward(input, offset, weight)\n\n        output = input.new_empty(\n            DeformConvFunction._output_size(input, weight, ctx.padding,\n                                            ctx.dilation, ctx.stride))\n\n        ctx.bufs_ = [input.new_empty(0), input.new_empty(0)]  # columns, ones\n\n        if not input.is_cuda:\n            raise NotImplementedError\n        else:\n            cur_im2col_step = min(ctx.im2col_step, input.shape[0])\n            assert (input.shape[0] %\n                    cur_im2col_step) == 0, \'im2col step must divide batchsize\'\n            deform_conv_cuda.deform_conv_forward_cuda(\n                input, weight, offset, output, ctx.bufs_[0], ctx.bufs_[1],\n                weight.size(3), weight.size(2), ctx.stride[1], ctx.stride[0],\n                ctx.padding[1], ctx.padding[0], ctx.dilation[1],\n                ctx.dilation[0], ctx.groups, ctx.deformable_groups,\n                cur_im2col_step)\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        input, offset, weight = ctx.saved_tensors\n\n        grad_input = grad_offset = grad_weight = None\n\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n        else:\n            cur_im2col_step = min(ctx.im2col_step, input.shape[0])\n            assert (input.shape[0] %\n                    cur_im2col_step) == 0, \'im2col step must divide batchsize\'\n\n            if ctx.needs_input_grad[0] or ctx.needs_input_grad[1]:\n                grad_input = torch.zeros_like(input)\n                grad_offset = torch.zeros_like(offset)\n                deform_conv_cuda.deform_conv_backward_input_cuda(\n                    input, offset, grad_output, grad_input,\n                    grad_offset, weight, ctx.bufs_[0], weight.size(3),\n                    weight.size(2), ctx.stride[1], ctx.stride[0],\n                    ctx.padding[1], ctx.padding[0], ctx.dilation[1],\n                    ctx.dilation[0], ctx.groups, ctx.deformable_groups,\n                    cur_im2col_step)\n\n            if ctx.needs_input_grad[2]:\n                grad_weight = torch.zeros_like(weight)\n                deform_conv_cuda.deform_conv_backward_parameters_cuda(\n                    input, offset, grad_output,\n                    grad_weight, ctx.bufs_[0], ctx.bufs_[1], weight.size(3),\n                    weight.size(2), ctx.stride[1], ctx.stride[0],\n                    ctx.padding[1], ctx.padding[0], ctx.dilation[1],\n                    ctx.dilation[0], ctx.groups, ctx.deformable_groups, 1,\n                    cur_im2col_step)\n\n        return (grad_input, grad_offset, grad_weight, None, None, None, None,\n                None)\n\n    @staticmethod\n    def _output_size(input, weight, padding, dilation, stride):\n        channels = weight.size(0)\n        output_size = (input.size(0), channels)\n        for d in range(input.dim() - 2):\n            in_size = input.size(d + 2)\n            pad = padding[d]\n            kernel = dilation[d] * (weight.size(d + 2) - 1) + 1\n            stride_ = stride[d]\n            output_size += ((in_size + (2 * pad) - kernel) // stride_ + 1, )\n        if not all(map(lambda s: s > 0, output_size)):\n            raise ValueError(\n                ""convolution input is too small (output would be {})"".format(\n                    \'x\'.join(map(str, output_size))))\n        return output_size\n\n\nclass ModulatedDeformConvFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                input,\n                offset,\n                mask,\n                weight,\n                bias=None,\n                stride=1,\n                padding=0,\n                dilation=1,\n                groups=1,\n                deformable_groups=1):\n        ctx.stride = stride\n        ctx.padding = padding\n        ctx.dilation = dilation\n        ctx.groups = groups\n        ctx.deformable_groups = deformable_groups\n        ctx.with_bias = bias is not None\n        if not ctx.with_bias:\n            bias = input.new_empty(1)  # fake tensor\n        if not input.is_cuda:\n            raise NotImplementedError\n        if weight.requires_grad or mask.requires_grad or offset.requires_grad \\\n                or input.requires_grad:\n            ctx.save_for_backward(input, offset, mask, weight, bias)\n        output = input.new_empty(\n            ModulatedDeformConvFunction._infer_shape(ctx, input, weight))\n        ctx._bufs = [input.new_empty(0), input.new_empty(0)]\n        deform_conv_cuda.modulated_deform_conv_cuda_forward(\n            input, weight, bias, ctx._bufs[0], offset, mask, output,\n            ctx._bufs[1], weight.shape[2], weight.shape[3], ctx.stride,\n            ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation,\n            ctx.groups, ctx.deformable_groups, ctx.with_bias)\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n        input, offset, mask, weight, bias = ctx.saved_tensors\n        grad_input = torch.zeros_like(input)\n        grad_offset = torch.zeros_like(offset)\n        grad_mask = torch.zeros_like(mask)\n        grad_weight = torch.zeros_like(weight)\n        grad_bias = torch.zeros_like(bias)\n        deform_conv_cuda.modulated_deform_conv_cuda_backward(\n            input, weight, bias, ctx._bufs[0], offset, mask, ctx._bufs[1],\n            grad_input, grad_weight, grad_bias, grad_offset, grad_mask,\n            grad_output, weight.shape[2], weight.shape[3], ctx.stride,\n            ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation,\n            ctx.groups, ctx.deformable_groups, ctx.with_bias)\n        if not ctx.with_bias:\n            grad_bias = None\n\n        return (grad_input, grad_offset, grad_mask, grad_weight, grad_bias,\n                None, None, None, None, None)\n\n    @staticmethod\n    def _infer_shape(ctx, input, weight):\n        n = input.size(0)\n        channels_out = weight.size(0)\n        height, width = input.shape[2:4]\n        kernel_h, kernel_w = weight.shape[2:4]\n        height_out = (height + 2 * ctx.padding -\n                      (ctx.dilation * (kernel_h - 1) + 1)) // ctx.stride + 1\n        width_out = (width + 2 * ctx.padding -\n                     (ctx.dilation * (kernel_w - 1) + 1)) // ctx.stride + 1\n        return n, channels_out, height_out, width_out\n\n\ndeform_conv = DeformConvFunction.apply\nmodulated_deform_conv = ModulatedDeformConvFunction.apply\n\n\nclass DeformConv(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,\n                 bias=False):\n        super(DeformConv, self).__init__()\n\n        assert not bias\n        assert in_channels % groups == 0, \\\n            \'in_channels {} cannot be divisible by groups {}\'.format(\n                in_channels, groups)\n        assert out_channels % groups == 0, \\\n            \'out_channels {} cannot be divisible by groups {}\'.format(\n                out_channels, groups)\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _pair(kernel_size)\n        self.stride = _pair(stride)\n        self.padding = _pair(padding)\n        self.dilation = _pair(dilation)\n        self.groups = groups\n        self.deformable_groups = deformable_groups\n\n        self.weight = nn.Parameter(\n            torch.Tensor(out_channels, in_channels // self.groups,\n                         *self.kernel_size))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, x, offset):\n        return deform_conv(x, offset, self.weight, self.stride, self.padding,\n                           self.dilation, self.groups, self.deformable_groups)\n\n\nclass DeformConvPack(DeformConv):\n\n    def __init__(self, *args, **kwargs):\n        super(DeformConvPack, self).__init__(*args, **kwargs)\n\n        self.conv_offset = nn.Conv2d(\n            self.in_channels,\n            self.deformable_groups * 2 * self.kernel_size[0] *\n            self.kernel_size[1],\n            kernel_size=self.kernel_size,\n            stride=_pair(self.stride),\n            padding=_pair(self.padding),\n            bias=True)\n        self.init_offset()\n\n    def init_offset(self):\n        self.conv_offset.weight.data.zero_()\n        self.conv_offset.bias.data.zero_()\n\n    def forward(self, x):\n        offset = self.conv_offset(x)\n        return deform_conv(x, offset, self.weight, self.stride, self.padding,\n                           self.dilation, self.groups, self.deformable_groups)\n\n\nclass ModulatedDeformConv(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,\n                 bias=True):\n        super(ModulatedDeformConv, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _pair(kernel_size)\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.groups = groups\n        self.deformable_groups = deformable_groups\n        self.with_bias = bias\n\n        self.weight = nn.Parameter(\n            torch.Tensor(out_channels, in_channels // groups,\n                         *self.kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter(\'bias\', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.zero_()\n\n    def forward(self, x, offset, mask):\n        return modulated_deform_conv(x, offset, mask, self.weight, self.bias,\n                                     self.stride, self.padding, self.dilation,\n                                     self.groups, self.deformable_groups)\n\n\nclass ModulatedDeformConvPack(ModulatedDeformConv):\n\n    def __init__(self, *args, **kwargs):\n        super(ModulatedDeformConvPack, self).__init__(*args, **kwargs)\n\n        self.conv_offset_mask = nn.Conv2d(\n            self.in_channels,\n            self.deformable_groups * 3 * self.kernel_size[0] *\n            self.kernel_size[1],\n            kernel_size=self.kernel_size,\n            stride=_pair(self.stride),\n            padding=_pair(self.padding),\n            bias=True)\n        self.init_offset()\n\n    def init_offset(self):\n        self.conv_offset_mask.weight.data.zero_()\n        self.conv_offset_mask.bias.data.zero_()\n\n    def forward(self, x):\n        out = self.conv_offset_mask(x)\n        o1, o2, mask = torch.chunk(out, 3, dim=1)\n        offset = torch.cat((o1, o2), dim=1)\n        mask = torch.sigmoid(mask)\n        return modulated_deform_conv(x, offset, mask, self.weight, self.bias,\n                                     self.stride, self.padding, self.dilation,\n                                     self.groups, self.deformable_groups)\n'"
alphapose/models/layers/dcn/deform_pool.py,6,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.nn.modules.utils import _pair\n\nfrom . import deform_pool_cuda\n\n\nclass DeformRoIPoolingFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                data,\n                rois,\n                offset,\n                spatial_scale,\n                out_size,\n                out_channels,\n                no_trans,\n                group_size=1,\n                part_size=None,\n                sample_per_part=4,\n                trans_std=.0):\n        # TODO: support unsquare RoIs\n        out_h, out_w = _pair(out_size)\n        assert isinstance(out_h, int) and isinstance(out_w, int)\n        assert out_h == out_w\n        out_size = out_h  # out_h and out_w must be equal\n\n        ctx.spatial_scale = spatial_scale\n        ctx.out_size = out_size\n        ctx.out_channels = out_channels\n        ctx.no_trans = no_trans\n        ctx.group_size = group_size\n        ctx.part_size = out_size if part_size is None else part_size\n        ctx.sample_per_part = sample_per_part\n        ctx.trans_std = trans_std\n\n        assert 0.0 <= ctx.trans_std <= 1.0\n        if not data.is_cuda:\n            raise NotImplementedError\n\n        n = rois.shape[0]\n        output = data.new_empty(n, out_channels, out_size, out_size)\n        output_count = data.new_empty(n, out_channels, out_size, out_size)\n        deform_pool_cuda.deform_psroi_pooling_cuda_forward(\n            data, rois, offset, output, output_count, ctx.no_trans,\n            ctx.spatial_scale, ctx.out_channels, ctx.group_size, ctx.out_size,\n            ctx.part_size, ctx.sample_per_part, ctx.trans_std)\n\n        if data.requires_grad or rois.requires_grad or offset.requires_grad:\n            ctx.save_for_backward(data, rois, offset)\n        ctx.output_count = output_count\n\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n\n        data, rois, offset = ctx.saved_tensors\n        output_count = ctx.output_count\n        grad_input = torch.zeros_like(data)\n        grad_rois = None\n        grad_offset = torch.zeros_like(offset)\n\n        deform_pool_cuda.deform_psroi_pooling_cuda_backward(\n            grad_output, data, rois, offset, output_count, grad_input,\n            grad_offset, ctx.no_trans, ctx.spatial_scale, ctx.out_channels,\n            ctx.group_size, ctx.out_size, ctx.part_size, ctx.sample_per_part,\n            ctx.trans_std)\n        return (grad_input, grad_rois, grad_offset, None, None, None, None,\n                None, None, None, None)\n\n\ndeform_roi_pooling = DeformRoIPoolingFunction.apply\n\n\nclass DeformRoIPooling(nn.Module):\n\n    def __init__(self,\n                 spatial_scale,\n                 out_size,\n                 out_channels,\n                 no_trans,\n                 group_size=1,\n                 part_size=None,\n                 sample_per_part=4,\n                 trans_std=.0):\n        super(DeformRoIPooling, self).__init__()\n        self.spatial_scale = spatial_scale\n        self.out_size = _pair(out_size)\n        self.out_channels = out_channels\n        self.no_trans = no_trans\n        self.group_size = group_size\n        self.part_size = out_size if part_size is None else part_size\n        self.sample_per_part = sample_per_part\n        self.trans_std = trans_std\n\n    def forward(self, data, rois, offset):\n        if self.no_trans:\n            offset = data.new_empty(0)\n        return deform_roi_pooling(data, rois, offset, self.spatial_scale,\n                                  self.out_size, self.out_channels,\n                                  self.no_trans, self.group_size,\n                                  self.part_size, self.sample_per_part,\n                                  self.trans_std)\n\n\nclass DeformRoIPoolingPack(DeformRoIPooling):\n\n    def __init__(self,\n                 spatial_scale,\n                 out_size,\n                 out_channels,\n                 no_trans,\n                 group_size=1,\n                 part_size=None,\n                 sample_per_part=4,\n                 trans_std=.0,\n                 num_offset_fcs=3,\n                 deform_fc_channels=1024):\n        super(DeformRoIPoolingPack,\n              self).__init__(spatial_scale, out_size, out_channels, no_trans,\n                             group_size, part_size, sample_per_part, trans_std)\n\n        self.num_offset_fcs = num_offset_fcs\n        self.deform_fc_channels = deform_fc_channels\n\n        if not no_trans:\n            seq = []\n            ic = self.out_size[0] * self.out_size[1] * self.out_channels\n            for i in range(self.num_offset_fcs):\n                if i < self.num_offset_fcs - 1:\n                    oc = self.deform_fc_channels\n                else:\n                    oc = self.out_size[0] * self.out_size[1] * 2\n                seq.append(nn.Linear(ic, oc))\n                ic = oc\n                if i < self.num_offset_fcs - 1:\n                    seq.append(nn.ReLU(inplace=True))\n            self.offset_fc = nn.Sequential(*seq)\n            self.offset_fc[-1].weight.data.zero_()\n            self.offset_fc[-1].bias.data.zero_()\n\n    def forward(self, data, rois):\n        assert data.size(1) == self.out_channels\n        if self.no_trans:\n            offset = data.new_empty(0)\n            return deform_roi_pooling(data, rois, offset, self.spatial_scale,\n                                      self.out_size, self.out_channels,\n                                      self.no_trans, self.group_size,\n                                      self.part_size, self.sample_per_part,\n                                      self.trans_std)\n        else:\n            n = rois.shape[0]\n            offset = data.new_empty(0)\n            x = deform_roi_pooling(data, rois, offset, self.spatial_scale,\n                                   self.out_size, self.out_channels, True,\n                                   self.group_size, self.part_size,\n                                   self.sample_per_part, self.trans_std)\n            offset = self.offset_fc(x.view(n, -1))\n            offset = offset.view(n, 2, self.out_size[0], self.out_size[1])\n            return deform_roi_pooling(data, rois, offset, self.spatial_scale,\n                                      self.out_size, self.out_channels,\n                                      self.no_trans, self.group_size,\n                                      self.part_size, self.sample_per_part,\n                                      self.trans_std)\n\n\nclass ModulatedDeformRoIPoolingPack(DeformRoIPooling):\n\n    def __init__(self,\n                 spatial_scale,\n                 out_size,\n                 out_channels,\n                 no_trans,\n                 group_size=1,\n                 part_size=None,\n                 sample_per_part=4,\n                 trans_std=.0,\n                 num_offset_fcs=3,\n                 num_mask_fcs=2,\n                 deform_fc_channels=1024):\n        super(ModulatedDeformRoIPoolingPack,\n              self).__init__(spatial_scale, out_size, out_channels, no_trans,\n                             group_size, part_size, sample_per_part, trans_std)\n\n        self.num_offset_fcs = num_offset_fcs\n        self.num_mask_fcs = num_mask_fcs\n        self.deform_fc_channels = deform_fc_channels\n\n        if not no_trans:\n            offset_fc_seq = []\n            ic = self.out_size[0] * self.out_size[1] * self.out_channels\n            for i in range(self.num_offset_fcs):\n                if i < self.num_offset_fcs - 1:\n                    oc = self.deform_fc_channels\n                else:\n                    oc = self.out_size[0] * self.out_size[1] * 2\n                offset_fc_seq.append(nn.Linear(ic, oc))\n                ic = oc\n                if i < self.num_offset_fcs - 1:\n                    offset_fc_seq.append(nn.ReLU(inplace=True))\n            self.offset_fc = nn.Sequential(*offset_fc_seq)\n            self.offset_fc[-1].weight.data.zero_()\n            self.offset_fc[-1].bias.data.zero_()\n\n            mask_fc_seq = []\n            ic = self.out_size[0] * self.out_size[1] * self.out_channels\n            for i in range(self.num_mask_fcs):\n                if i < self.num_mask_fcs - 1:\n                    oc = self.deform_fc_channels\n                else:\n                    oc = self.out_size[0] * self.out_size[1]\n                mask_fc_seq.append(nn.Linear(ic, oc))\n                ic = oc\n                if i < self.num_mask_fcs - 1:\n                    mask_fc_seq.append(nn.ReLU(inplace=True))\n                else:\n                    mask_fc_seq.append(nn.Sigmoid())\n            self.mask_fc = nn.Sequential(*mask_fc_seq)\n            self.mask_fc[-2].weight.data.zero_()\n            self.mask_fc[-2].bias.data.zero_()\n\n    def forward(self, data, rois):\n        assert data.size(1) == self.out_channels\n        if self.no_trans:\n            offset = data.new_empty(0)\n            return deform_roi_pooling(data, rois, offset, self.spatial_scale,\n                                      self.out_size, self.out_channels,\n                                      self.no_trans, self.group_size,\n                                      self.part_size, self.sample_per_part,\n                                      self.trans_std)\n        else:\n            n = rois.shape[0]\n            offset = data.new_empty(0)\n            x = deform_roi_pooling(data, rois, offset, self.spatial_scale,\n                                   self.out_size, self.out_channels, True,\n                                   self.group_size, self.part_size,\n                                   self.sample_per_part, self.trans_std)\n            offset = self.offset_fc(x.view(n, -1))\n            offset = offset.view(n, 2, self.out_size[0], self.out_size[1])\n            mask = self.mask_fc(x.view(n, -1))\n            mask = mask.view(n, 1, self.out_size[0], self.out_size[1])\n            return deform_roi_pooling(\n                data, rois, offset, self.spatial_scale, self.out_size,\n                self.out_channels, self.no_trans, self.group_size,\n                self.part_size, self.sample_per_part, self.trans_std) * mask\n'"
detector/efficientdet/effdet/config/config.py,0,"b'""""""EfficientDet Configurations\n\nAdapted from official impl at https://github.com/google/automl/tree/master/efficientdet\n\nTODO use a different config system, separate model from train specific hparams\n""""""\n\nimport ast\nimport copy\nimport json\nimport six\n\n\ndef eval_str_fn(val):\n    if val in {\'true\', \'false\'}:\n        return val == \'true\'\n    try:\n        return ast.literal_eval(val)\n    except ValueError:\n        return val\n\n\n# pylint: disable=protected-access\nclass Config(object):\n    """"""A config utility class.""""""\n\n    def __init__(self, config_dict=None):\n        self.update(config_dict)\n\n    def __setattr__(self, k, v):\n        self.__dict__[k] = Config(v) if isinstance(v, dict) else copy.deepcopy(v)\n\n    def __getattr__(self, k):\n        try:\n            return self.__dict__[k]\n        except KeyError:\n            raise AttributeError\n\n    def __repr__(self):\n        return repr(self.as_dict())\n\n    def __str__(self):\n        try:\n            return json.dumps(self.as_dict(), indent=4)\n        except TypeError:\n            return str(self.as_dict())\n\n    def _update(self, config_dict, allow_new_keys=True):\n        """"""Recursively update internal members.""""""\n        if not config_dict:\n            return\n\n        for k, v in six.iteritems(config_dict):\n            if k not in self.__dict__.keys():\n                if allow_new_keys:\n                    self.__setattr__(k, v)\n                else:\n                    raise KeyError(\'Key `{}` does not exist for overriding. \'.format(k))\n            else:\n                if isinstance(v, dict):\n                    self.__dict__[k]._update(v, allow_new_keys)\n                else:\n                    self.__dict__[k] = copy.deepcopy(v)\n\n    def get(self, k, default_value=None):\n        return self.__dict__.get(k, default_value)\n\n    def update(self, config_dict):\n        """"""Update members while allowing new keys.""""""\n        self._update(config_dict, allow_new_keys=True)\n\n    def override(self, config_dict_or_str):\n        """"""Update members while disallowing new keys.""""""\n        if isinstance(config_dict_or_str, str):\n            config_dict = self.parse_from_str(config_dict_or_str)\n        elif isinstance(config_dict_or_str, dict):\n            config_dict = config_dict_or_str\n        else:\n            raise ValueError(\'Unknown value type: {}\'.format(config_dict_or_str))\n\n        self._update(config_dict, allow_new_keys=False)\n\n    def parse_from_str(self, config_str):\n        """"""parse from a string in format \'x=a,y=2\' and return the dict.""""""\n        if not config_str:\n            return {}\n        config_dict = {}\n        try:\n            for kv_pair in config_str.split(\',\'):\n                if not kv_pair:  # skip empty string\n                    continue\n                k, v = kv_pair.split(\'=\')\n                config_dict[k.strip()] = eval_str_fn(v.strip())\n            return config_dict\n        except ValueError:\n            raise ValueError(\'Invalid config_str: {}\'.format(config_str))\n\n    def as_dict(self):\n        """"""Returns a dict representation.""""""\n        config_dict = {}\n        for k, v in six.iteritems(self.__dict__):\n            if isinstance(v, Config):\n                config_dict[k] = v.as_dict()\n            else:\n                config_dict[k] = copy.deepcopy(v)\n        return config_dict\n\n\ndef default_detection_configs():\n    """"""Returns a default detection configs.""""""\n    h = Config()\n\n    # model name.\n    h.name = \'tf_efficientdet_d1\'\n\n    # input preprocessing parameters\n    h.image_size = 640\n    h.input_rand_hflip = True\n    h.train_scale_min = 0.1\n    h.train_scale_max = 2.0\n    h.autoaugment_policy = None\n\n    # dataset specific parameters\n    h.num_classes = 90\n    h.skip_crowd_during_training = True\n\n    # model architecture\n    h.min_level = 3\n    h.max_level = 7\n    h.num_levels = h.max_level - h.min_level + 1\n    h.num_scales = 3\n    h.aspect_ratios = [(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)]\n    h.anchor_scale = 4.0\n    h.pad_type = \'same\'\n\n    # is batchnorm training mode\n    h.is_training_bn = True\n\n    # optimization\n    h.momentum = 0.9\n    h.learning_rate = 0.08\n    h.lr_warmup_init = 0.008\n    h.lr_warmup_epoch = 1.0\n    h.first_lr_drop_epoch = 200.0\n    h.second_lr_drop_epoch = 250.0\n    h.clip_gradients_norm = 10.0\n    h.num_epochs = 300\n\n    # classification loss\n    h.alpha = 0.25\n    h.gamma = 1.5\n\n    # localization loss\n    h.delta = 0.1\n    h.box_loss_weight = 50.0\n\n    # regularization l2 loss.\n    h.weight_decay = 4e-5\n\n    # For detection.\n    h.box_class_repeats = 3\n    h.fpn_cell_repeats = 3\n    h.fpn_channels = 88\n    h.separable_conv = True\n    h.apply_bn_for_resampling = True\n    h.conv_after_downsample = False\n    h.conv_bn_relu_pattern = False\n    h.use_native_resize_op = False\n    h.pooling_type = None\n\n    # version.\n    h.fpn_name = None\n    h.fpn_config = None\n\n    # No stochastic depth in default.\n    h.survival_prob = None  # FIXME remove\n    h.drop_path_rate = 0.\n\n    h.lr_decay_method = \'cosine\'\n    h.moving_average_decay = 0.9998\n    h.ckpt_var_scope = None\n    h.backbone_name = \'tf_efficientnet_b1\'\n    h.backbone_config = None\n\n    # RetinaNet.\n    h.resnet_depth = 50\n    return h\n\n\nefficientdet_model_param_dict = {\n    \'efficientdet_d0\':\n        dict(\n            name=\'efficientdet_d0\',\n            backbone_name=\'tf_efficientnet_b0\',\n            image_size=512,\n            fpn_channels=64,\n            fpn_cell_repeats=3,\n            box_class_repeats=3,\n        ),\n    \'efficientdet_d1\':\n        dict(\n            name=\'efficientdet_d1\',\n            backbone_name=\'tf_efficientnet_b1\',\n            image_size=640,\n            fpn_channels=88,\n            fpn_cell_repeats=4,\n            box_class_repeats=3,\n        ),\n    \'efficientdet_d2\':\n        dict(\n            name=\'efficientdet_d2\',\n            backbone_name=\'tf_efficientnet_b2\',\n            image_size=768,\n            fpn_channels=112,\n            fpn_cell_repeats=5,\n            box_class_repeats=3,\n        ),\n    \'efficientdet_d3\':\n        dict(\n            name=\'efficientdet_d3\',\n            backbone_name=\'tf_efficientnet_b3\',\n            image_size=896,\n            fpn_channels=160,\n            fpn_cell_repeats=6,\n            box_class_repeats=4,\n        ),\n    \'efficientdet_d4\':\n        dict(\n            name=\'efficientdet_d4\',\n            backbone_name=\'tf_efficientnet_b4\',\n            image_size=1024,\n            fpn_channels=224,\n            fpn_cell_repeats=7,\n            box_class_repeats=4,\n        ),\n    \'efficientdet_d5\':\n        dict(\n            name=\'efficientdet_d5\',\n            backbone_name=\'tf_efficientnet_b5\',\n            image_size=1280,\n            fpn_channels=288,\n            fpn_cell_repeats=7,\n            box_class_repeats=4,\n        ),\n    \'efficientdet_d6\':\n        dict(\n            name=\'efficientdet_d6\',\n            backbone_name=\'tf_efficientnet_b6\',\n            image_size=1280,\n            fpn_channels=384,\n            fpn_cell_repeats=8,\n            box_class_repeats=5,\n            fpn_name=\'bifpn_sum\',  # Use unweighted sum for training stability.\n        ),\n    \'efficientdet_d7\':\n        dict(\n            name=\'efficientdet_d7\',\n            backbone_name=\'tf_efficientnet_b6\',\n            image_size=1536,\n            fpn_channels=384,\n            fpn_cell_repeats=8,\n            box_class_repeats=5,\n            anchor_scale=5.0,\n            fpn_name=\'bifpn_sum\',  # Use unweighted sum for training stability.\n        ),\n}\n\n\ndef get_efficientdet_config(model_name=\'efficientdet_d1\'):\n    """"""Get the default config for EfficientDet based on model name.""""""\n    h = default_detection_configs()\n    h.override(efficientdet_model_param_dict[model_name])\n    return h\n'"
detector/efficientdet/effdet/object_detection/__init__.py,0,"b'# Copyright 2020 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# Object detection data loaders and libraries are mostly based on RetinaNet:\n# https://github.com/tensorflow/tpu/tree/master/models/official/retinanet\n'"
detector/efficientdet/effdet/object_detection/argmax_matcher.py,8,"b'# Copyright 2020 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Argmax matcher implementation.\n\nThis class takes a similarity matrix and matches columns to rows based on the\nmaximum value per column. One can specify matched_thresholds and\nto prevent columns from matching to rows (generally resulting in a negative\ntraining example) and unmatched_theshold to ignore the match (generally\nresulting in neither a positive or negative training example).\n\nThis matcher is used in Fast(er)-RCNN.\n\nNote: matchers are used in TargetAssigners. There is a create_target_assigner\nfactory function for popular implementations.\n""""""\nimport torch\nfrom torch.nn.functional import one_hot\nfrom . import matcher\n\n\nclass ArgMaxMatcher(matcher.Matcher):\n    """"""Matcher based on highest value.\n\n    This class computes matches from a similarity matrix. Each column is matched\n    to a single row.\n\n    To support object detection target assignment this class enables setting both\n    matched_threshold (upper threshold) and unmatched_threshold (lower thresholds)\n    defining three categories of similarity which define whether examples are\n    positive, negative, or ignored:\n    (1) similarity >= matched_threshold: Highest similarity. Matched/Positive!\n    (2) matched_threshold > similarity >= unmatched_threshold: Medium similarity.\n            Depending on negatives_lower_than_unmatched, this is either\n            Unmatched/Negative OR Ignore.\n    (3) unmatched_threshold > similarity: Lowest similarity. Depending on flag\n            negatives_lower_than_unmatched, either Unmatched/Negative OR Ignore.\n    For ignored matches this class sets the values in the Match object to -2.\n    """"""\n\n    def __init__(self,\n                 matched_threshold,\n                 unmatched_threshold=None,\n                 negatives_lower_than_unmatched=True,\n                 force_match_for_each_row=False):\n        """"""Construct ArgMaxMatcher.\n\n        Args:\n            matched_threshold: Threshold for positive matches. Positive if\n                sim >= matched_threshold, where sim is the maximum value of the\n                similarity matrix for a given column. Set to None for no threshold.\n            unmatched_threshold: Threshold for negative matches. Negative if\n                sim < unmatched_threshold. Defaults to matched_threshold\n                when set to None.\n            negatives_lower_than_unmatched: Boolean which defaults to True. If True\n                then negative matches are the ones below the unmatched_threshold,\n                whereas ignored matches are in between the matched and unmatched\n                threshold. If False, then negative matches are in between the matched\n                and unmatched threshold, and everything lower than unmatched is ignored.\n            force_match_for_each_row: If True, ensures that each row is matched to\n                at least one column (which is not guaranteed otherwise if the\n                matched_threshold is high). Defaults to False. See\n                argmax_matcher_test.testMatcherForceMatch() for an example.\n\n        Raises:\n            ValueError: if unmatched_threshold is set but matched_threshold is not set\n                or if unmatched_threshold > matched_threshold.\n        """"""\n        if (matched_threshold is None) and (unmatched_threshold is not None):\n            raise ValueError(\'Need to also define matched_threshold when unmatched_threshold is defined\')\n        self._matched_threshold = matched_threshold\n        if unmatched_threshold is None:\n            self._unmatched_threshold = matched_threshold\n        else:\n            if unmatched_threshold > matched_threshold:\n                raise ValueError(\'unmatched_threshold needs to be smaller or equal to matched_threshold\')\n            self._unmatched_threshold = unmatched_threshold\n        if not negatives_lower_than_unmatched:\n            if self._unmatched_threshold == self._matched_threshold:\n                raise ValueError(\'When negatives are in between matched and unmatched thresholds, these \'\n                                 \'cannot be of equal value. matched: %s, unmatched: %s\',\n                                 self._matched_threshold, self._unmatched_threshold)\n        self._force_match_for_each_row = force_match_for_each_row\n        self._negatives_lower_than_unmatched = negatives_lower_than_unmatched\n\n    def _match(self, similarity_matrix):\n        """"""Tries to match each column of the similarity matrix to a row.\n\n        Args:\n            similarity_matrix: tensor of shape [N, M] representing any similarity metric.\n\n        Returns:\n            Match object with corresponding matches for each of M columns.\n        """"""\n\n        def _match_when_rows_are_empty():\n            """"""Performs matching when the rows of similarity matrix are empty.\n\n            When the rows are empty, all detections are false positives. So we return\n            a tensor of -1\'s to indicate that the columns do not match to any rows.\n\n            Returns:\n                matches:  int32 tensor indicating the row each column matches to.\n            """"""\n            return -1 * torch.ones(similarity_matrix.shape[1], dtype=torch.long)\n\n        def _match_when_rows_are_non_empty():\n            """"""Performs matching when the rows of similarity matrix are non empty.\n\n            Returns:\n                matches:  int32 tensor indicating the row each column matches to.\n            """"""\n            # Matches for each column\n            matches = torch.argmax(similarity_matrix, 0)\n\n            # Deal with matched and unmatched threshold\n            if self._matched_threshold is not None:\n                # Get logical indices of ignored and unmatched columns as tf.int64\n                matched_vals = torch.max(similarity_matrix, 0)[0]\n                below_unmatched_threshold = self._unmatched_threshold > matched_vals\n                between_thresholds = (matched_vals >= self._unmatched_threshold) & \\\n                                     (self._matched_threshold > matched_vals)\n\n                if self._negatives_lower_than_unmatched:\n                    matches = self._set_values_using_indicator(matches, below_unmatched_threshold, -1)\n                    matches = self._set_values_using_indicator(matches, between_thresholds, -2)\n                else:\n                    matches = self._set_values_using_indicator(matches, below_unmatched_threshold, -2)\n                    matches = self._set_values_using_indicator(matches, between_thresholds, -1)\n\n            if self._force_match_for_each_row:\n                force_match_column_ids = torch.argmax(similarity_matrix, 1)\n                force_match_column_indicators = one_hot(force_match_column_ids, similarity_matrix.shape[1])\n                force_match_row_ids = torch.argmax(force_match_column_indicators, 0)\n                force_match_column_mask = torch.max(force_match_column_indicators, 0)[0].bool()\n                final_matches = torch.where(force_match_column_mask, force_match_row_ids, matches)\n                return final_matches\n            else:\n                return matches\n\n        if similarity_matrix.shape[0] == 0:\n            return _match_when_rows_are_empty()\n        else:\n            return _match_when_rows_are_non_empty()\n\n    def _set_values_using_indicator(self, x, indicator, val):\n        """"""Set the indicated fields of x to val.\n\n        Args:\n            x: tensor.\n            indicator: boolean with same shape as x.\n            val: scalar with value to set.\n\n        Returns:\n            modified tensor.\n        """"""\n        indicator = indicator.type(x.dtype)\n        return x * (1 - indicator) + val * indicator\n'"
detector/efficientdet/effdet/object_detection/box_coder.py,1,"b'# Copyright 2020 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Base box coder.\n\nBox coders convert between coordinate frames, namely image-centric\n(with (0,0) on the top left of image) and anchor-centric (with (0,0) being\ndefined by a specific anchor).\n\nUsers of a BoxCoder can call two methods:\n encode: which encodes a box with respect to a given anchor\n  (or rather, a tensor of boxes wrt a corresponding tensor of anchors) and\n decode: which inverts this encoding with a decode operation.\nIn both cases, the arguments are assumed to be in 1-1 correspondence already;\nit is not the job of a BoxCoder to perform matching.\n""""""\nfrom abc import ABCMeta\nfrom abc import abstractmethod\nfrom abc import abstractproperty\n\nimport torch\n\n# Box coder types.\nFASTER_RCNN = \'faster_rcnn\'\nKEYPOINT = \'keypoint\'\nMEAN_STDDEV = \'mean_stddev\'\nSQUARE = \'square\'\n\n\nclass BoxCoder(object):\n    """"""Abstract base class for box coder.""""""\n    __metaclass__ = ABCMeta\n\n    @abstractproperty\n    def code_size(self):\n        """"""Return the size of each code.\n\n        This number is a constant and should agree with the output of the `encode`\n        op (e.g. if rel_codes is the output of self.encode(...), then it should have\n        shape [N, code_size()]).  This abstractproperty should be overridden by\n        implementations.\n\n        Returns:\n          an integer constant\n        """"""\n        pass\n\n    def encode(self, boxes, anchors):\n        """"""Encode a box list relative to an anchor collection.\n\n        Args:\n          boxes: BoxList holding N boxes to be encoded\n          anchors: BoxList of N anchors\n\n        Returns:\n          a tensor representing N relative-encoded boxes\n        """"""\n        return self._encode(boxes, anchors)\n\n    def decode(self, rel_codes, anchors):\n        """"""Decode boxes that are encoded relative to an anchor collection.\n\n        Args:\n          rel_codes: a tensor representing N relative-encoded boxes\n          anchors: BoxList of anchors\n\n        Returns:\n          boxlist: BoxList holding N boxes encoded in the ordinary way (i.e.,\n            with corners y_min, x_min, y_max, x_max)\n        """"""\n        return self._decode(rel_codes, anchors)\n\n    @abstractmethod\n    def _encode(self, boxes, anchors):\n        """"""Method to be overridden by implementations.\n\n        Args:\n          boxes: BoxList holding N boxes to be encoded\n          anchors: BoxList of N anchors\n\n        Returns:\n          a tensor representing N relative-encoded boxes\n        """"""\n        pass\n\n    @abstractmethod\n    def _decode(self, rel_codes, anchors):\n        """"""Method to be overridden by implementations.\n\n        Args:\n          rel_codes: a tensor representing N relative-encoded boxes\n          anchors: BoxList of anchors\n\n        Returns:\n          boxlist: BoxList holding N boxes encoded in the ordinary way (i.e.,\n            with corners y_min, x_min, y_max, x_max)\n        """"""\n        pass\n\n\ndef batch_decode(encoded_boxes, box_coder, anchors):\n    """"""Decode a batch of encoded boxes.\n\n    This op takes a batch of encoded bounding boxes and transforms\n    them to a batch of bounding boxes specified by their corners in\n    the order of [y_min, x_min, y_max, x_max].\n\n    Args:\n        encoded_boxes: a float32 tensor of shape [batch_size, num_anchors,\n            code_size] representing the location of the objects.\n        box_coder: a BoxCoder object.\n        anchors: a BoxList of anchors used to encode `encoded_boxes`.\n\n    Returns:\n        decoded_boxes: a float32 tensor of shape [batch_size, num_anchors, coder_size]\n            representing the corners of the objects in the order of [y_min, x_min, y_max, x_max].\n\n    Raises:\n        ValueError: if batch sizes of the inputs are inconsistent, or if\n        the number of anchors inferred from encoded_boxes and anchors are inconsistent.\n    """"""\n    assert len(encoded_boxes.shape) == 3\n    if encoded_boxes.shape[1] != anchors.num_boxes():\n        raise ValueError(\'The number of anchors inferred from encoded_boxes\'\n                         \' and anchors are inconsistent: shape[1] of encoded_boxes\'\n                         \' %s should be equal to the number of anchors: %s.\' %\n                         (encoded_boxes.shape[1], anchors.num_boxes()))\n\n    decoded_boxes = torch.stack([\n        box_coder.decode(boxes, anchors).boxes for boxes in encoded_boxes.unbind()\n    ])\n    return decoded_boxes\n'"
detector/efficientdet/effdet/object_detection/box_list.py,2,"b'# Copyright 2020 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Bounding Box List definition.\n\nBoxList represents a list of bounding boxes as tensorflow\ntensors, where each bounding box is represented as a row of 4 numbers,\n[y_min, x_min, y_max, x_max].  It is assumed that all bounding boxes\nwithin a given list correspond to a single image.  See also\nbox_list_ops.py for common box related operations (such as area, iou, etc).\n\nOptionally, users can add additional related fields (such as weights).\nWe assume the following things to be true about fields:\n* they correspond to boxes in the box_list along the 0th dimension\n* they have inferable rank at graph construction time\n* all dimensions except for possibly the 0th can be inferred\n  (i.e., not None) at graph construction time.\n\nSome other notes:\n    * Following tensorflow conventions, we use height, width ordering,\n        and correspondingly, y,x (or ymin, xmin, ymax, xmax) ordering\n    * Tensors are always provided as (flat) [N, 4] tensors.\n""""""\n\nimport torch\n\n\nclass BoxList(object):\n    """"""Box collection.""""""\n\n    def __init__(self, boxes):\n        """"""Constructs box collection.\n\n        Args:\n            boxes: a tensor of shape [N, 4] representing box corners\n\n        Raises:\n            ValueError: if invalid dimensions for bbox data or if bbox data is not in float32 format.\n        """"""\n        if len(boxes.shape) != 2 or boxes.shape[-1] != 4:\n            raise ValueError(\'Invalid dimensions for box data.\')\n        if boxes.dtype != torch.float32:\n            raise ValueError(\'Invalid tensor type: should be tf.float32\')\n        self.data = {\'boxes\': boxes}\n\n    def num_boxes(self):\n        """"""Returns number of boxes held in collection.\n\n        Returns:\n          a tensor representing the number of boxes held in the collection.\n        """"""\n        return self.data[\'boxes\'].shape[0]\n\n    def get_all_fields(self):\n        """"""Returns all fields.""""""\n        return self.data.keys()\n\n    def get_extra_fields(self):\n        """"""Returns all non-box fields (i.e., everything not named \'boxes\').""""""\n        return [k for k in self.data.keys() if k != \'boxes\']\n\n    def add_field(self, field, field_data):\n        """"""Add field to box list.\n\n        This method can be used to add related box data such as weights/labels, etc.\n\n        Args:\n            field: a string key to access the data via `get`\n            field_data: a tensor containing the data to store in the BoxList\n        """"""\n        self.data[field] = field_data\n\n    def has_field(self, field):\n        return field in self.data\n\n    @property\n    def boxes(self):\n        """"""Convenience function for accessing box coordinates.\n\n        Returns:\n            a tensor with shape [N, 4] representing box coordinates.\n        """"""\n        return self.get_field(\'boxes\')\n\n    @boxes.setter\n    def boxes(self, boxes):\n        """"""Convenience function for setting box coordinates.\n\n        Args:\n            boxes: a tensor of shape [N, 4] representing box corners\n\n        Raises:\n            ValueError: if invalid dimensions for bbox data\n        """"""\n        if len(boxes.shape) != 2 or boxes.shape[-1] != 4:\n            raise ValueError(\'Invalid dimensions for box data.\')\n        self.data[\'boxes\'] = boxes\n\n    def get_field(self, field):\n        """"""Accesses a box collection and associated fields.\n\n        This function returns specified field with object; if no field is specified,\n        it returns the box coordinates.\n\n        Args:\n            field: this optional string parameter can be used to specify a related field to be accessed.\n\n        Returns:\n            a tensor representing the box collection or an associated field.\n\n        Raises:\n            ValueError: if invalid field\n        """"""\n        if not self.has_field(field):\n            raise ValueError(\'field \' + str(field) + \' does not exist\')\n        return self.data[field]\n\n    def set_field(self, field, value):\n        """"""Sets the value of a field.\n\n        Updates the field of a box_list with a given value.\n\n        Args:\n            field: (string) name of the field to set value.\n            value: the value to assign to the field.\n\n        Raises:\n            ValueError: if the box_list does not have specified field.\n        """"""\n        if not self.has_field(field):\n            raise ValueError(\'field %s does not exist\' % field)\n        self.data[field] = value\n\n    def get_center_coordinates_and_sizes(self):\n        """"""Computes the center coordinates, height and width of the boxes.\n\n        Returns:\n            a list of 4 1-D tensors [ycenter, xcenter, height, width].\n        """"""\n        box_corners = self.boxes\n        ymin, xmin, ymax, xmax = box_corners.T.unbind()\n        width = xmax - xmin\n        height = ymax - ymin\n        ycenter = ymin + height / 2.\n        xcenter = xmin + width / 2.\n        return [ycenter, xcenter, height, width]\n\n    def transpose_coordinates(self):\n        """"""Transpose the coordinate representation in a boxlist.\n\n        """"""\n        y_min, x_min, y_max, x_max = self.boxes.chunk(4, dim=1)\n        self.boxes = torch.cat([x_min, y_min, x_max, y_max], 1)\n\n    def as_tensor_dict(self, fields=None):\n        """"""Retrieves specified fields as a dictionary of tensors.\n\n        Args:\n            fields: (optional) list of fields to return in the dictionary.\n                If None (default), all fields are returned.\n\n        Returns:\n            tensor_dict: A dictionary of tensors specified by fields.\n\n        Raises:\n            ValueError: if specified field is not contained in boxlist.\n        """"""\n        tensor_dict = {}\n        if fields is None:\n            fields = self.get_all_fields()\n        for field in fields:\n            if not self.has_field(field):\n                raise ValueError(\'boxlist must contain all specified fields\')\n            tensor_dict[field] = self.get_field(field)\n        return tensor_dict\n\n    @property\n    def device(self):\n        return self.data[\'boxes\'].device\n'"
detector/efficientdet/effdet/object_detection/faster_rcnn_box_coder.py,6,"b'# Copyright 2020 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Faster RCNN box coder.\n\nFaster RCNN box coder follows the coding schema described below:\n  ty = (y - ya) / ha\n  tx = (x - xa) / wa\n  th = log(h / ha)\n  tw = log(w / wa)\n  where x, y, w, h denote the box\'s center coordinates, width and height\n  respectively. Similarly, xa, ya, wa, ha denote the anchor\'s center\n  coordinates, width and height. tx, ty, tw and th denote the anchor-encoded\n  center, width and height respectively.\n\n  See http://arxiv.org/abs/1506.01497 for details.\n""""""\n\nimport torch\n\nfrom . import box_coder\nfrom . import box_list\n\nEPS = 1e-8\n\n\nclass FasterRcnnBoxCoder(box_coder.BoxCoder):\n    """"""Faster RCNN box coder.""""""\n\n    def __init__(self, scale_factors=None):\n        """"""Constructor for FasterRcnnBoxCoder.\n\n        Args:\n            scale_factors: List of 4 positive scalars to scale ty, tx, th and tw.\n                If set to None, does not perform scaling. For Faster RCNN,\n                the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0].\n        """"""\n        if scale_factors:\n            assert len(scale_factors) == 4\n            for scalar in scale_factors:\n                assert scalar > 0\n        self._scale_factors = scale_factors\n\n    @property\n    def code_size(self):\n        return 4\n\n    def _encode(self, boxes, anchors):\n        """"""Encode a box collection with respect to anchor collection.\n\n        Args:\n            boxes: BoxList holding N boxes to be encoded.\n            anchors: BoxList of anchors.\n\n        Returns:\n            a tensor representing N anchor-encoded boxes of the format [ty, tx, th, tw].\n        """"""\n        # Convert anchors to the center coordinate representation.\n        ycenter_a, xcenter_a, ha, wa = anchors.get_center_coordinates_and_sizes()\n        ycenter, xcenter, h, w = boxes.get_center_coordinates_and_sizes()\n        # Avoid NaN in division and log below.\n        ha += EPS\n        wa += EPS\n        h += EPS\n        w += EPS\n\n        tx = (xcenter - xcenter_a) / wa\n        ty = (ycenter - ycenter_a) / ha\n        tw = torch.log(w / wa)\n        th = torch.log(h / ha)\n        # Scales location targets as used in paper for joint training.\n        if self._scale_factors:\n            ty *= self._scale_factors[0]\n            tx *= self._scale_factors[1]\n            th *= self._scale_factors[2]\n            tw *= self._scale_factors[3]\n        return torch.stack([ty, tx, th, tw]).T\n\n    def _decode(self, rel_codes, anchors):\n        """"""Decode relative codes to boxes.\n\n        Args:\n            rel_codes: a tensor representing N anchor-encoded boxes.\n            anchors: BoxList of anchors.\n\n        Returns:\n            boxes: BoxList holding N bounding boxes.\n        """"""\n        ycenter_a, xcenter_a, ha, wa = anchors.get_center_coordinates_and_sizes()\n\n        ty, tx, th, tw = rel_codes.T.unbind()\n        if self._scale_factors:\n            ty /= self._scale_factors[0]\n            tx /= self._scale_factors[1]\n            th /= self._scale_factors[2]\n            tw /= self._scale_factors[3]\n        w = torch.exp(tw) * wa\n        h = torch.exp(th) * ha\n        ycenter = ty * ha + ycenter_a\n        xcenter = tx * wa + xcenter_a\n        ymin = ycenter - h / 2.\n        xmin = xcenter - w / 2.\n        ymax = ycenter + h / 2.\n        xmax = xcenter + w / 2.\n        return box_list.BoxList(torch.stack([ymin, xmin, ymax, xmax]).T)\n'"
detector/efficientdet/effdet/object_detection/matcher.py,11,"b'# Copyright 2020 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Matcher interface and Match class.\n\nThis module defines the Matcher interface and the Match object. The job of the\nmatcher is to match row and column indices based on the similarity matrix and\nother optional parameters. Each column is matched to at most one row. There\nare three possibilities for the matching:\n\n1) match: A column matches a row.\n2) no_match: A column does not match any row.\n3) ignore: A column that is neither \'match\' nor no_match.\n\nThe ignore case is regularly encountered in object detection: when an anchor has\na relatively small overlap with a ground-truth box, one neither wants to\nconsider this box a positive example (match) nor a negative example (no match).\n\nThe Match class is used to store the match results and it provides simple apis\nto query the results.\n""""""\nfrom abc import ABCMeta\nfrom abc import abstractmethod\n\nimport torch\n\n\nclass Match(object):\n    """"""Class to store results from the matcher.\n\n    This class is used to store the results from the matcher. It provides\n    convenient methods to query the matching results.\n    """"""\n\n    def __init__(self, match_results):\n        """"""Constructs a Match object.\n\n        Args:\n            match_results: Integer tensor of shape [N] with (1) match_results[i]>=0,\n                meaning that column i is matched with row match_results[i].\n                (2) match_results[i]=-1, meaning that column i is not matched.\n                (3) match_results[i]=-2, meaning that column i is ignored.\n\n        Raises:\n            ValueError: if match_results does not have rank 1 or is not an integer int32 scalar tensor\n        """"""\n        if len(match_results.shape) != 1:\n            raise ValueError(\'match_results should have rank 1\')\n        if match_results.dtype not in (torch.int32, torch.int64):\n            raise ValueError(\'match_results should be an int32 or int64 scalar tensor\')\n        self._match_results = match_results\n\n    @property\n    def match_results(self):\n        """"""The accessor for match results.\n\n        Returns:\n            the tensor which encodes the match results.\n        """"""\n        return self._match_results\n\n    def matched_column_indices(self):\n        """"""Returns column indices that match to some row.\n\n        The indices returned by this op are always sorted in increasing order.\n\n        Returns:\n            column_indices: int32 tensor of shape [K] with column indices.\n        """"""\n        return self._reshape_and_cast(torch.where(self._match_results > -1))\n\n    def matched_column_indicator(self):\n        """"""Returns column indices that are matched.\n\n        Returns:\n            column_indices: int32 tensor of shape [K] with column indices.\n        """"""\n        return self._match_results >= 0\n\n    def num_matched_columns(self):\n        """"""Returns number (int32 scalar tensor) of matched columns.""""""\n        return self.matched_column_indices()\n\n    def unmatched_column_indices(self):\n        """"""Returns column indices that do not match any row.\n\n        The indices returned by this op are always sorted in increasing order.\n\n        Returns:\n          column_indices: int32 tensor of shape [K] with column indices.\n        """"""\n        return self._reshape_and_cast(torch.where(self._match_results == -1))\n\n    def unmatched_column_indicator(self):\n        """"""Returns column indices that are unmatched.\n\n        Returns:\n          column_indices: int32 tensor of shape [K] with column indices.\n        """"""\n        return self._match_results == -1\n\n    def num_unmatched_columns(self):\n        """"""Returns number (int32 scalar tensor) of unmatched columns.""""""\n        return self.unmatched_column_indices().numel()\n\n    def ignored_column_indices(self):\n        """"""Returns column indices that are ignored (neither Matched nor Unmatched).\n\n        The indices returned by this op are always sorted in increasing order.\n\n        Returns:\n          column_indices: int32 tensor of shape [K] with column indices.\n        """"""\n        return self._reshape_and_cast(torch.where(self.ignored_column_indicator()))\n\n    def ignored_column_indicator(self):\n        """"""Returns boolean column indicator where True means the column is ignored.\n\n        Returns:\n            column_indicator: boolean vector which is True for all ignored column indices.\n        """"""\n        return self._match_results == -2\n\n    def num_ignored_columns(self):\n        """"""Returns number (int32 scalar tensor) of matched columns.""""""\n        return self.ignored_column_indices().numel()\n\n    def unmatched_or_ignored_column_indices(self):\n        """"""Returns column indices that are unmatched or ignored.\n\n        The indices returned by this op are always sorted in increasing order.\n\n        Returns:\n            column_indices: int32 tensor of shape [K] with column indices.\n        """"""\n        return self._reshape_and_cast(torch.where(0 > self._match_results))\n\n    def matched_row_indices(self):\n        """"""Returns row indices that match some column.\n\n        The indices returned by this op are ordered so as to be in correspondence with the output of\n        matched_column_indicator().  For example if self.matched_column_indicator() is [0,2],\n        and self.matched_row_indices() is [7, 3], then we know that column 0 was matched to row 7 and\n        column 2 was matched to row 3.\n\n        Returns:\n            row_indices: int32 tensor of shape [K] with row indices.\n        """"""\n        return self._reshape_and_cast(torch.gather(self._match_results, 0, self.matched_column_indices()))\n\n    def _reshape_and_cast(self, t):\n        return torch.reshape(t, [-1]).long()\n\n    def gather_based_on_match(self, input_tensor, unmatched_value, ignored_value):\n        """"""Gathers elements from `input_tensor` based on match results.\n\n        For columns that are matched to a row, gathered_tensor[col] is set to input_tensor[match_results[col]].\n        For columns that are unmatched, gathered_tensor[col] is set to unmatched_value. Finally, for columns that\n        are ignored gathered_tensor[col] is set to ignored_value.\n\n        Note that the input_tensor.shape[1:] must match with unmatched_value.shape\n        and ignored_value.shape\n\n        Args:\n            input_tensor: Tensor to gather values from.\n            unmatched_value: Constant tensor value for unmatched columns.\n            ignored_value: Constant tensor value for ignored columns.\n\n        Returns:\n            gathered_tensor: A tensor containing values gathered from input_tensor.\n                The shape of the gathered tensor is [match_results.shape[0]] + input_tensor.shape[1:].\n        """"""\n        ss = torch.stack([ignored_value, unmatched_value])\n        input_tensor = torch.cat([ss, input_tensor], dim=0)\n        gather_indices = torch.clamp(self.match_results + 2, min=0)\n        gathered_tensor = torch.index_select(input_tensor, 0, gather_indices)\n        return gathered_tensor\n\n\nclass Matcher(object):\n    """"""Abstract base class for matcher.\n    """"""\n    __metaclass__ = ABCMeta\n\n    def match(self, similarity_matrix, **params):\n        """"""Computes matches among row and column indices and returns the result.\n\n        Computes matches among the row and column indices based on the similarity\n        matrix and optional arguments.\n\n        Args:\n            similarity_matrix: Float tensor of shape [N, M] with pairwise similarity\n                where higher value means more similar.\n            scope: Op scope name. Defaults to \'Match\' if None.\n            **params: Additional keyword arguments for specific implementations of\n                the Matcher.\n\n        Returns:\n            A Match object with the results of matching.\n        """"""\n        return Match(self._match(similarity_matrix, **params))\n\n    @abstractmethod\n    def _match(self, similarity_matrix, **params):\n        """"""Method to be overridden by implementations.\n\n        Args:\n            similarity_matrix: Float tensor of shape [N, M] with pairwise similarity\n                where higher value means more similar.\n            **params: Additional keyword arguments for specific implementations of the Matcher.\n\n        Returns:\n            match_results: Integer tensor of shape [M]: match_results[i]>=0 means\n                that column i is matched to row match_results[i], match_results[i]=-1\n                means that the column is not matched. match_results[i]=-2 means that\n                the column is ignored (usually this happens when there is a very weak\n                match which one neither wants as positive nor negative example).\n        """"""\n        pass\n'"
detector/efficientdet/effdet/object_detection/region_similarity_calculator.py,7,"b'# Copyright 2020 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Region Similarity Calculators for BoxLists.\n\nRegion Similarity Calculators compare a pairwise measure of similarity\nbetween the boxes in two BoxLists.\n""""""\nfrom abc import ABCMeta\nfrom abc import abstractmethod\n\nimport torch\n\n\ndef area(boxlist):\n    """"""Computes area of boxes.\n\n    Args:\n        boxlist: BoxList holding N boxes\n\n    Returns:\n        a tensor with shape [N] representing box areas.\n    """"""\n    y_min, x_min, y_max, x_max = boxlist.boxes.chunk(4, dim=1)\n    out = (y_max - y_min).squeeze(1) * (x_max - x_min).squeeze(1)\n    return out\n\n\ndef intersection(boxlist1, boxlist2):\n    """"""Compute pairwise intersection areas between boxes.\n\n    Args:\n        boxlist1: BoxList holding N boxes\n        boxlist2: BoxList holding M boxes\n\n    Returns:\n        a tensor with shape [N, M] representing pairwise intersections\n    """"""\n    y_min1, x_min1, y_max1, x_max1 = boxlist1.boxes.chunk(4, dim=1)\n    y_min2, x_min2, y_max2, x_max2 = boxlist2.boxes.chunk(4, dim=1)\n    all_pairs_min_ymax = torch.min(y_max1, y_max2.T)\n    all_pairs_max_ymin = torch.max(y_min1, y_min2.T)\n    intersect_heights = torch.clamp(all_pairs_min_ymax - all_pairs_max_ymin, min=0)\n    all_pairs_min_xmax = torch.min(x_max1, x_max2.T)\n    all_pairs_max_xmin = torch.max(x_min1, x_min2.T)\n    intersect_widths = torch.clamp(all_pairs_min_xmax - all_pairs_max_xmin, min=0)\n    return intersect_heights * intersect_widths\n\n\ndef iou(boxlist1, boxlist2):\n    """"""Computes pairwise intersection-over-union between box collections.\n\n    Args:\n        boxlist1: BoxList holding N boxes\n        boxlist2: BoxList holding M boxes\n\n    Returns:\n        a tensor with shape [N, M] representing pairwise iou scores.\n    """"""\n    intersections = intersection(boxlist1, boxlist2)\n    areas1 = area(boxlist1)\n    areas2 = area(boxlist2)\n    unions = areas1.unsqueeze(1) + areas2.unsqueeze(0) - intersections\n    return torch.where(intersections == 0.0, torch.zeros_like(intersections), intersections / unions)\n\n\nclass RegionSimilarityCalculator(object):\n    """"""Abstract base class for region similarity calculator.""""""\n    __metaclass__ = ABCMeta\n\n    def compare(self, boxlist1, boxlist2):\n        """"""Computes matrix of pairwise similarity between BoxLists.\n\n        This op (to be overridden) computes a measure of pairwise similarity between\n        the boxes in the given BoxLists. Higher values indicate more similarity.\n\n        Note that this method simply measures similarity and does not explicitly\n        perform a matching.\n\n        Args:\n            boxlist1: BoxList holding N boxes.\n            boxlist2: BoxList holding M boxes.\n\n        Returns:\n            a (float32) tensor of shape [N, M] with pairwise similarity score.\n        """"""\n        return self._compare(boxlist1, boxlist2)\n\n    @abstractmethod\n    def _compare(self, boxlist1, boxlist2):\n        pass\n\n\nclass IouSimilarity(RegionSimilarityCalculator):\n    """"""Class to compute similarity based on Intersection over Union (IOU) metric.\n\n    This class computes pairwise similarity between two BoxLists based on IOU.\n    """"""\n\n    def _compare(self, boxlist1, boxlist2):\n        """"""Compute pairwise IOU similarity between the two BoxLists.\n\n        Args:\n          boxlist1: BoxList holding N boxes.\n          boxlist2: BoxList holding M boxes.\n\n        Returns:\n          A tensor with shape [N, M] representing pairwise iou scores.\n        """"""\n        return iou(boxlist1, boxlist2)\n'"
detector/efficientdet/effdet/object_detection/target_assigner.py,10,"b'# Copyright 2020 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Base target assigner module.\n\nThe job of a TargetAssigner is, for a given set of anchors (bounding boxes) and\ngroundtruth detections (bounding boxes), to assign classification and regression\ntargets to each anchor as well as weights to each anchor (specifying, e.g.,\nwhich anchors should not contribute to training loss).\n\nIt assigns classification/regression targets by performing the following steps:\n1) Computing pairwise similarity between anchors and groundtruth boxes using a\n  provided RegionSimilarity Calculator\n2) Computing a matching based on the similarity matrix using a provided Matcher\n3) Assigning regression targets based on the matching and a provided BoxCoder\n4) Assigning classification targets based on the matching and groundtruth labels\n\nNote that TargetAssigners only operate on detections from a single\nimage at a time, so any logic for applying a TargetAssigner to multiple\nimages must be handled externally.\n""""""\nimport torch\n\nfrom . import box_list\n\nKEYPOINTS_FIELD_NAME = \'keypoints\'\n\n\nclass TargetAssigner(object):\n    """"""Target assigner to compute classification and regression targets.""""""\n\n    def __init__(self, similarity_calc, matcher, box_coder, negative_class_weight=1.0, unmatched_cls_target=None):\n        """"""Construct Object Detection Target Assigner.\n\n        Args:\n            similarity_calc: a RegionSimilarityCalculator\n\n            matcher: Matcher used to match groundtruth to anchors.\n\n            box_coder: BoxCoder used to encode matching groundtruth boxes with respect to anchors.\n\n            negative_class_weight: classification weight to be associated to negative\n                anchors (default: 1.0). The weight must be in [0., 1.].\n\n            unmatched_cls_target: a float32 tensor with shape [d_1, d_2, ..., d_k]\n                which is consistent with the classification target for each\n                anchor (and can be empty for scalar targets).  This shape must thus be\n                compatible with the groundtruth labels that are passed to the ""assign""\n                function (which have shape [num_gt_boxes, d_1, d_2, ..., d_k]).\n                If set to None, unmatched_cls_target is set to be [0] for each anchor.\n\n        Raises:\n            ValueError: if similarity_calc is not a RegionSimilarityCalculator or\n                if matcher is not a Matcher or if box_coder is not a BoxCoder\n        """"""\n        self._similarity_calc = similarity_calc\n        self._matcher = matcher\n        self._box_coder = box_coder\n        self._negative_class_weight = negative_class_weight\n        self._unmatched_cls_target = unmatched_cls_target\n\n    @property\n    def box_coder(self):\n        return self._box_coder\n\n    def assign(self, anchors, groundtruth_boxes, groundtruth_labels=None, groundtruth_weights=None, **params):\n        """"""Assign classification and regression targets to each anchor.\n\n        For a given set of anchors and groundtruth detections, match anchors\n        to groundtruth_boxes and assign classification and regression targets to\n        each anchor as well as weights based on the resulting match (specifying,\n        e.g., which anchors should not contribute to training loss).\n\n        Anchors that are not matched to anything are given a classification target\n        of self._unmatched_cls_target which can be specified via the constructor.\n\n        Args:\n            anchors: a BoxList representing N anchors\n\n            groundtruth_boxes: a BoxList representing M groundtruth boxes\n\n            groundtruth_labels:  a tensor of shape [M, d_1, ... d_k]\n                with labels for each of the ground_truth boxes. The subshape\n                [d_1, ... d_k] can be empty (corresponding to scalar inputs).  When set\n                to None, groundtruth_labels assumes a binary problem where all\n                ground_truth boxes get a positive label (of 1).\n\n            groundtruth_weights: a float tensor of shape [M] indicating the weight to\n                assign to all anchors match to a particular groundtruth box. The weights\n                must be in [0., 1.]. If None, all weights are set to 1.\n\n            **params: Additional keyword arguments for specific implementations of the Matcher.\n\n        Returns:\n            cls_targets: a float32 tensor with shape [num_anchors, d_1, d_2 ... d_k],\n                where the subshape [d_1, ..., d_k] is compatible with groundtruth_labels\n                which has shape [num_gt_boxes, d_1, d_2, ... d_k].\n\n            cls_weights: a float32 tensor with shape [num_anchors]\n\n            reg_targets: a float32 tensor with shape [num_anchors, box_code_dimension]\n\n            reg_weights: a float32 tensor with shape [num_anchors]\n\n            match: a matcher.Match object encoding the match between anchors and groundtruth boxes,\n                with rows corresponding to groundtruth boxes and columns corresponding to anchors.\n\n        Raises:\n            ValueError: if anchors or groundtruth_boxes are not of type box_list.BoxList\n        """"""\n        if not isinstance(anchors, box_list.BoxList):\n            raise ValueError(\'anchors must be an BoxList\')\n        if not isinstance(groundtruth_boxes, box_list.BoxList):\n            raise ValueError(\'groundtruth_boxes must be an BoxList\')\n\n        device = anchors.device\n\n        if groundtruth_labels is None:\n            groundtruth_labels = torch.ones(groundtruth_boxes.num_boxes(), device=device).unsqueeze(0)\n            groundtruth_labels = groundtruth_labels.unsqueeze(-1)\n\n        if groundtruth_weights is None:\n            num_gt_boxes = groundtruth_boxes.num_boxes()\n            if not num_gt_boxes:\n                num_gt_boxes = groundtruth_boxes.num_boxes()\n            groundtruth_weights = torch.ones([num_gt_boxes], device=device)\n\n        match_quality_matrix = self._similarity_calc.compare(groundtruth_boxes, anchors)\n        match = self._matcher.match(match_quality_matrix, **params)\n        reg_targets = self._create_regression_targets(anchors, groundtruth_boxes, match)\n        cls_targets = self._create_classification_targets(groundtruth_labels, match)\n        reg_weights = self._create_regression_weights(match, groundtruth_weights)\n        cls_weights = self._create_classification_weights(match, groundtruth_weights)\n\n        return cls_targets, cls_weights, reg_targets, reg_weights, match\n\n    def _create_regression_targets(self, anchors, groundtruth_boxes, match):\n        """"""Returns a regression target for each anchor.\n\n        Args:\n            anchors: a BoxList representing N anchors\n\n            groundtruth_boxes: a BoxList representing M groundtruth_boxes\n\n            match: a matcher.Match object\n\n        Returns:\n            reg_targets: a float32 tensor with shape [N, box_code_dimension]\n        """"""\n        device = anchors.device\n        zero_box = torch.zeros(4, device=device)\n        matched_gt_boxes = match.gather_based_on_match(\n            groundtruth_boxes.boxes, unmatched_value=zero_box, ignored_value=zero_box)\n        matched_gt_boxlist = box_list.BoxList(matched_gt_boxes)\n        if groundtruth_boxes.has_field(KEYPOINTS_FIELD_NAME):\n            groundtruth_keypoints = groundtruth_boxes.get_field(KEYPOINTS_FIELD_NAME)\n            zero_kp = torch.zeros(groundtruth_keypoints.shape[1:], device=device)\n            matched_keypoints = match.gather_based_on_match(\n                groundtruth_keypoints, unmatched_value=zero_kp, ignored_value=zero_kp)\n            matched_gt_boxlist.add_field(KEYPOINTS_FIELD_NAME, matched_keypoints)\n        matched_reg_targets = self._box_coder.encode(matched_gt_boxlist, anchors)\n\n        unmatched_ignored_reg_targets = self._default_regression_target(device).repeat(match.match_results.shape[0], 1)\n\n        matched_anchors_mask = match.matched_column_indicator()\n        reg_targets = torch.where(matched_anchors_mask.unsqueeze(1), matched_reg_targets, unmatched_ignored_reg_targets)\n        return reg_targets\n\n    def _default_regression_target(self, device):\n        """"""Returns the default target for anchors to regress to.\n\n        Default regression targets are set to zero (though in this implementation what\n        these targets are set to should not matter as the regression weight of any box\n        set to regress to the default target is zero).\n\n        Returns:\n            default_target: a float32 tensor with shape [1, box_code_dimension]\n        """"""\n        return torch.zeros(1, self._box_coder.code_size, device=device)\n\n    def _create_classification_targets(self, groundtruth_labels, match):\n        """"""Create classification targets for each anchor.\n\n        Assign a classification target of for each anchor to the matching\n        groundtruth label that is provided by match.  Anchors that are not matched\n        to anything are given the target self._unmatched_cls_target\n\n        Args:\n            groundtruth_labels:  a tensor of shape [num_gt_boxes, d_1, ... d_k]\n                with labels for each of the ground_truth boxes. The subshape\n                [d_1, ... d_k] can be empty (corresponding to scalar labels).\n            match: a matcher.Match object that provides a matching between anchors\n                and groundtruth boxes.\n\n        Returns:\n            a float32 tensor with shape [num_anchors, d_1, d_2 ... d_k], where the\n            subshape [d_1, ..., d_k] is compatible with groundtruth_labels which has\n            shape [num_gt_boxes, d_1, d_2, ... d_k].\n        """"""\n        if self._unmatched_cls_target is not None:\n            uct = self._unmatched_cls_target\n        else:\n            uct = torch.scalar_tensor(0, device=groundtruth_labels.device)\n        return match.gather_based_on_match(groundtruth_labels, unmatched_value=uct, ignored_value=uct)\n\n    def _create_regression_weights(self, match, groundtruth_weights):\n        """"""Set regression weight for each anchor.\n\n        Only positive anchors are set to contribute to the regression loss, so this\n        method returns a weight of 1 for every positive anchor and 0 for every\n        negative anchor.\n\n        Args:\n            match: a matcher.Match object that provides a matching between anchors and groundtruth boxes.\n            groundtruth_weights: a float tensor of shape [M] indicating the weight to\n                assign to all anchors match to a particular groundtruth box.\n\n        Returns:\n            a float32 tensor with shape [num_anchors] representing regression weights.\n        """"""\n        zs = torch.scalar_tensor(0, device=groundtruth_weights.device)\n        return match.gather_based_on_match(groundtruth_weights, ignored_value=zs, unmatched_value=zs)\n\n    def _create_classification_weights(self, match, groundtruth_weights):\n        """"""Create classification weights for each anchor.\n\n        Positive (matched) anchors are associated with a weight of\n        positive_class_weight and negative (unmatched) anchors are associated with\n        a weight of negative_class_weight. When anchors are ignored, weights are set\n        to zero. By default, both positive/negative weights are set to 1.0,\n        but they can be adjusted to handle class imbalance (which is almost always\n        the case in object detection).\n\n        Args:\n            match: a matcher.Match object that provides a matching between anchors and groundtruth boxes.\n            groundtruth_weights: a float tensor of shape [M] indicating the weight to\n                assign to all anchors match to a particular groundtruth box.\n\n        Returns:\n            a float32 tensor with shape [num_anchors] representing classification weights.\n        """"""\n        ignored = torch.scalar_tensor(0, device=groundtruth_weights.device)\n        ncw = torch.scalar_tensor(self._negative_class_weight, device=groundtruth_weights.device)\n        return match.gather_based_on_match(groundtruth_weights, ignored_value=ignored, unmatched_value=ncw)\n\n    def get_box_coder(self):\n        """"""Get BoxCoder of this TargetAssigner.\n\n        Returns:\n            BoxCoder object.\n        """"""\n        return self._box_coder\n'"
