file_path,api_count,code
images/main.py,0,"b""import cv2\nimport numpy as np\n\n\nsa = cv2.imread('./mesh1.png')\nsb = cv2.imread('./mesh2.png')\nsc = cv2.imread('./mesh3.png')\n\nda = dst_image = cv2.resize(sa, (300, 400), interpolation = cv2.INTER_CUBIC)\ndb = dst_image = cv2.resize(sb, (300, 400), interpolation = cv2.INTER_CUBIC)\ndc = dst_image = cv2.resize(sc, (300, 400), interpolation = cv2.INTER_CUBIC)\n\nd = np.zeros((400, 900, 3))\nd[:, :300, :] = da[:, :, :]\nd[:, 300:600,:] = db[:, :, :]\nd[:, 600:, :] = dc[:, :, :]\n\ncv2.imwrite('r.png', d)"""
src/Discriminator.py,5,"b""\n'''\n    file:   Discriminator.py\n\n    date:   2017_04_29\n    author: zhangxiong(1025679612@qq.com)\n'''\n\nfrom LinearModel import LinearModel\nimport config\nimport util\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom config import args\n\n'''\n    shape discriminator is used for shape discriminator\n    the inputs if N x 10\n'''\nclass ShapeDiscriminator(LinearModel):\n    def __init__(self, fc_layers, use_dropout, drop_prob, use_ac_func):\n        if fc_layers[-1] != 1:\n            msg = 'the neuron count of the last layer must be 1, but got {}'.format(fc_layers[-1])\n            sys.exit(msg)\n\n        super(ShapeDiscriminator, self).__init__(fc_layers, use_dropout, drop_prob, use_ac_func)\n    \n    def forward(self, inputs):\n        return self.fc_blocks(inputs)\n\nclass PoseDiscriminator(nn.Module):\n    def __init__(self, channels):\n        super(PoseDiscriminator, self).__init__()\n\n        if channels[-1] != 1:\n            msg = 'the neuron count of the last layer must be 1, but got {}'.format(channels[-1])\n            sys.exit(msg)\n        \n        self.conv_blocks = nn.Sequential()\n        l = len(channels)\n        for idx in range(l - 2):\n            self.conv_blocks.add_module(\n                name = 'conv_{}'.format(idx),\n                module = nn.Conv2d(in_channels = channels[idx], out_channels = channels[idx + 1], kernel_size = 1, stride = 1)\n            )\n\n        self.fc_layer = nn.ModuleList()\n        for idx in range(23):\n            self.fc_layer.append(nn.Linear(in_features = channels[l - 2], out_features = 1))\n\n    # N x 23 x 9\n    def forward(self, inputs):\n        batch_size = inputs.shape[0]\n        inputs = inputs.transpose(1, 2).unsqueeze(2) # to N x 9 x 1 x 23\n        internal_outputs = self.conv_blocks(inputs) # to N x c x 1 x 23\n        o = []\n        for idx in range(23):\n            o.append(self.fc_layer[idx](internal_outputs[:,:,0,idx]))\n        \n        return torch.cat(o, 1), internal_outputs\n\nclass FullPoseDiscriminator(LinearModel):\n    def __init__(self, fc_layers, use_dropout, drop_prob, use_ac_func):\n        if fc_layers[-1] != 1:\n            msg = 'the neuron count of the last layer must be 1, but got {}'.format(fc_layers[-1])\n            sys.exit(msg)\n\n        super(FullPoseDiscriminator, self).__init__(fc_layers, use_dropout, drop_prob, use_ac_func)\n\n    def forward(self, inputs):\n        return self.fc_blocks(inputs)\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self._read_configs()\n\n        self._create_sub_modules()\n\n    def _read_configs(self):\n        self.beta_count = args.beta_count\n        self.smpl_model = args.smpl_model\n        self.smpl_mean_theta_path = args.smpl_mean_theta_path\n        self.total_theta_count = args.total_theta_count\n        self.joint_count = args.joint_count\n        self.feature_count = args.feature_count\n\n    def _create_sub_modules(self):\n        '''\n            create theta discriminator for 23 joint\n        '''\n\n        self.pose_discriminator = PoseDiscriminator([9, 32, 32, 1])\n        \n        '''\n            create full pose discriminator for total 23 joints\n        '''\n        fc_layers = [23 * 32, 1024, 1024, 1]\n        use_dropout = [False, False, False]\n        drop_prob = [0.5, 0.5, 0.5]\n        use_ac_func = [True, True, False]\n        self.full_pose_discriminator = FullPoseDiscriminator(fc_layers, use_dropout, drop_prob, use_ac_func)\n\n        '''\n            shape discriminator for betas\n        '''\n        fc_layers = [self.beta_count, 5, 1]\n        use_dropout = [False, False]\n        drop_prob = [0.5, 0.5]\n        use_ac_func = [True, False]\n        self.shape_discriminator = ShapeDiscriminator(fc_layers, use_dropout, drop_prob, use_ac_func)\n\n        print('finished create the discriminator modules...')\n\n\n    '''\n        inputs is N x 85(3 + 72 + 10)\n    '''\n    def forward(self, thetas):\n        batch_size = thetas.shape[0]\n        cams, poses, shapes = thetas[:, :3], thetas[:, 3:75], thetas[:, 75:]\n        shape_disc_value = self.shape_discriminator(shapes)\n        rotate_matrixs = util.batch_rodrigues(poses.contiguous().view(-1, 3)).view(-1, 24, 9)[:, 1:, :]\n        pose_disc_value, pose_inter_disc_value = self.pose_discriminator(rotate_matrixs)\n        full_pose_disc_value = self.full_pose_discriminator(pose_inter_disc_value.contiguous().view(batch_size, -1))\n        return torch.cat((pose_disc_value, full_pose_disc_value, shape_disc_value), 1)\n\nif __name__ == '__main__':\n    device = torch.device('cuda')\n    net = Discriminator()\n    inputs = torch.ones((100, 85))\n    disc_value = net(inputs)\n    print(net)"""
src/HourGlass.py,2,"b""\n'''\n    file:   hourglass.py\n\n    date:   2018_05_12\n    author: zhangxiong(1025679612@qq.com)\n'''\n\nfrom __future__ import print_function\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Residual(nn.Module):\n    def __init__(self, use_bn, input_channels, out_channels, mid_channels):\n        super(Residual, self).__init__()\n        self.use_bn = use_bn\n        self.out_channels   = out_channels\n        self.input_channels = input_channels\n        self.mid_channels   = mid_channels\n\n        self.down_channel = nn.Conv2d(input_channels, self.mid_channels, kernel_size = 1)\n        self.AcFunc       = nn.ReLU()\n        if use_bn:\n            self.bn_0 = nn.BatchNorm2d(num_features = self.mid_channels)\n            self.bn_1 = nn.BatchNorm2d(num_features = self.mid_channels)\n            self.bn_2 = nn.BatchNorm2d(num_features = self.out_channels)\n\n        self.conv = nn.Conv2d(self.mid_channels, self.mid_channels, kernel_size = 3, padding = 1)\n\n        self.up_channel = nn.Conv2d(self.mid_channels, out_channels, kernel_size= 1)\n\n        if input_channels != out_channels:\n            self.trans = nn.Conv2d(input_channels, out_channels, kernel_size = 1)\n    \n    def forward(self, inputs):\n        x = self.down_channel(inputs)\n        if self.use_bn:\n            x = self.bn_0(x)\n        x = self.AcFunc(x)\n\n        x = self.conv(x)\n        if self.use_bn:\n            x = self.bn_1(x)\n        x = self.AcFunc(x)\n\n        x = self.up_channel(x)\n\n        if self.input_channels != self.out_channels:\n            x += self.trans(inputs)\n        else:\n            x += inputs\n\n        if self.use_bn:\n            x = self.bn_2(x)\n        \n        return self.AcFunc(x)\n\nclass HourGlassBlock(nn.Module):\n    def __init__(self, block_count, residual_each_block, input_channels, mid_channels, use_bn, stack_index):\n        super(HourGlassBlock, self).__init__()\n\n        self.block_count         = block_count\n        self.residual_each_block = residual_each_block\n        self.use_bn              = use_bn\n        self.stack_index         = stack_index\n        self.input_channels      = input_channels\n        self.mid_channels        = mid_channels\n\n        if self.block_count == 0: #inner block\n            self.process = nn.Sequential()\n            for _ in range(residual_each_block * 3):\n                self.process.add_module(\n                    name = 'inner_{}_{}'.format(self.stack_index, _),\n                    module = Residual(input_channels = input_channels, out_channels = input_channels, mid_channels = mid_channels, use_bn = use_bn)\n                )\n        else:\n            #down sampling\n            self.down_sampling = nn.Sequential()\n            self.down_sampling.add_module(\n                name = 'down_sample_{}_{}'.format(self.stack_index, self.block_count), \n                module = nn.MaxPool2d(kernel_size = 2, stride = 2)\n            )\n            for _ in range(residual_each_block):\n                self.down_sampling.add_module(\n                    name = 'residual_{}_{}_{}'.format(self.stack_index, self.block_count, _),\n                    module = Residual(input_channels = input_channels, out_channels = input_channels, mid_channels = mid_channels, use_bn = use_bn)\n                )\n            \n            #up sampling\n            self.up_sampling = nn.Sequential()\n            self.up_sampling.add_module(\n                name = 'up_sample_{}_{}'.format(self.stack_index, self.block_count),\n                module = nn.Upsample(scale_factor=2, mode='bilinear')\n            )\n            for _ in range(residual_each_block):\n                self.up_sampling.add_module(\n                    name   = 'residual_{}_{}_{}'.format(self.stack_index, self.block_count, _),\n                    module = Residual(input_channels = input_channels, out_channels = input_channels, mid_channels = mid_channels, use_bn = use_bn)\n                )\n\n            #sub hour glass\n            self.sub_hg = HourGlassBlock(\n                block_count         = self.block_count - 1, \n                residual_each_block = self.residual_each_block, \n                input_channels      = self.input_channels,\n                mid_channels        = self.mid_channels,\n                use_bn              = self.use_bn,\n                stack_index         = self.stack_index\n            )\n            \n            # trans\n            self.trans = nn.Sequential()\n            for _ in range(residual_each_block):\n                self.trans.add_module(\n                    name = 'trans_{}_{}_{}'.format(self.stack_index, self.block_count, _),\n                    module = Residual(input_channels = input_channels, out_channels = input_channels, mid_channels = mid_channels, use_bn = use_bn)\n            )\n\n\n    def forward(self, inputs):\n        if self.block_count == 0:\n            return self.process(inputs)\n        else:\n            down_sampled        = self.down_sampling(inputs)\n            transed             = self.trans(down_sampled)\n            sub_net_output      = self.sub_hg(down_sampled)\n            return self.up_sampling(transed + sub_net_output)\n\n'''\n    the input is a 256 x 256 x 3 image\n'''\nclass HourGlass(nn.Module):\n    def __init__(self, nStack, nBlockCount, nResidualEachBlock, nMidChannels, nChannels, nJointCount, bUseBn):\n        super(HourGlass, self).__init__()\n\n        self.nStack             = nStack\n        self.nBlockCount        = nBlockCount\n        self.nResidualEachBlock = nResidualEachBlock\n        self.nChannels          = nChannels\n        self.nMidChannels       = nMidChannels\n        self.nJointCount        = nJointCount\n        self.bUseBn             = bUseBn\n\n        self.pre_process = nn.Sequential(\n            nn.Conv2d(3, nChannels, kernel_size = 3, padding = 1),\n            Residual(use_bn = bUseBn, input_channels = nChannels, out_channels = nChannels, mid_channels = nMidChannels),\n            nn.MaxPool2d(kernel_size = 2, stride = 2), # to 128 x 128 x c\n            Residual(use_bn = bUseBn, input_channels = nChannels, out_channels = nChannels, mid_channels = nMidChannels),\n            nn.MaxPool2d(kernel_size = 2, stride = 2), # to 64 x 64 x c\n            Residual(use_bn = bUseBn, input_channels = nChannels, out_channels = nChannels, mid_channels = nMidChannels)\n        )\n\n        self.hg = nn.ModuleList()\n        for _ in range(nStack):\n            self.hg.append(\n                HourGlassBlock(\n                    block_count = nBlockCount, \n                    residual_each_block = nResidualEachBlock,\n                    input_channels = nChannels, \n                    mid_channels = nMidChannels, \n                    use_bn = bUseBn,\n                    stack_index = _\n                )\n            )\n\n        self.blocks = nn.ModuleList()\n        for _ in range(nStack - 1):\n            self.blocks.append(\n                nn.Sequential(\n                    Residual(\n                        use_bn = bUseBn, input_channels = nChannels, out_channels = nChannels, mid_channels = nMidChannels \n                    ),\n                    Residual(\n                        use_bn = bUseBn, input_channels = nChannels, out_channels = nChannels, mid_channels = nMidChannels\n                    )\n                )\n            )\n        \n        self.intermediate_supervision = nn.ModuleList()\n        for _ in range(nStack): # to 64 x 64 x joint_count\n            self.intermediate_supervision.append(\n                nn.Conv2d(nChannels, nJointCount, kernel_size = 1, stride = 1)\n            )\n\n        self.normal_feature_channel = nn.ModuleList()\n        for _ in range(nStack - 1):\n            self.normal_feature_channel.append(\n                Residual(\n                    use_bn = bUseBn, input_channels = nJointCount, out_channels = nChannels, mid_channels = nMidChannels\n                )\n            )\n\n    def forward(self, inputs):\n        o = [] #outputs include intermediate supervision result\n        x = self.pre_process(inputs)\n        for _ in range(self.nStack):\n            o1 = self.hg[_](x)\n            o2 = self.intermediate_supervision[_](o1)\n            o.append(o2.view(-1, 4096))\n            if _ == self.nStack - 1:\n                break\n            o2 = self.normal_feature_channel[_](o2)\n            o1 = self.blocks[_](o1)\n            x = o1 + o2 + x\n        return o\n\ndef _create_hourglass_net():\n    return HourGlass(\n        nStack = 2,\n        nBlockCount = 4,\n        nResidualEachBlock = 1,\n        nMidChannels = 128,\n        nChannels = 256,\n        nJointCount = 1,\n        bUseBn = True,\n    )"""
src/LinearModel.py,3,"b""\n\n'''\n    file:   LinearModel.py\n\n    date:   2018_04_29\n    author: zhangxiong(1025679612@qq.com)\n'''\n\nimport torch.nn as nn\nimport numpy as np\nimport sys\nimport torch\n\nclass LinearModel(nn.Module):\n    '''\n        input param:\n            fc_layers: a list of neuron count, such as [2133, 1024, 1024, 85]\n            use_dropout: a list of bool define use dropout or not for each layer, such as [True, True, False]\n            drop_prob: a list of float defined the drop prob, such as [0.5, 0.5, 0]\n            use_ac_func: a list of bool define use active function or not, such as [True, True, False]\n    '''\n    def __init__(self, fc_layers, use_dropout, drop_prob, use_ac_func):\n        super(LinearModel, self).__init__()\n        self.fc_layers     = fc_layers\n        self.use_dropout   = use_dropout\n        self.drop_prob     = drop_prob\n        self.use_ac_func   = use_ac_func\n        \n        if not self._check():\n            msg = 'wrong LinearModel parameters!'\n            print(msg)\n            sys.exit(msg)\n\n        self.create_layers()\n\n    def _check(self):\n        while True:\n            if not isinstance(self.fc_layers, list):\n                print('fc_layers require list, get {}'.format(type(self.fc_layers)))\n                break\n            \n            if not isinstance(self.use_dropout, list):\n                print('use_dropout require list, get {}'.format(type(self.use_dropout)))\n                break\n\n            if not isinstance(self.drop_prob, list):\n                print('drop_prob require list, get {}'.format(type(self.drop_prob)))\n                break\n\n            if not isinstance(self.use_ac_func, list):\n                print('use_ac_func require list, get {}'.format(type(self.use_ac_func)))\n                break\n            \n            l_fc_layer = len(self.fc_layers)\n            l_use_drop = len(self.use_dropout)\n            l_drop_porb = len(self.drop_prob)\n            l_use_ac_func = len(self.use_ac_func)\n\n            return l_fc_layer >= 2 and l_use_drop < l_fc_layer and l_drop_porb < l_fc_layer and l_use_ac_func < l_fc_layer and l_drop_porb == l_use_drop\n\n        return False\n\n    def create_layers(self):\n        l_fc_layer = len(self.fc_layers)\n        l_use_drop = len(self.use_dropout)\n        l_drop_porb = len(self.drop_prob)\n        l_use_ac_func = len(self.use_ac_func)\n\n        self.fc_blocks = nn.Sequential()\n        \n        for _ in range(l_fc_layer - 1):\n            self.fc_blocks.add_module(\n                name = 'regressor_fc_{}'.format(_),\n                module = nn.Linear(in_features = self.fc_layers[_], out_features = self.fc_layers[_ + 1])\n            )\n            \n            if _ < l_use_ac_func and self.use_ac_func[_]:\n                self.fc_blocks.add_module(\n                    name = 'regressor_af_{}'.format(_),\n                    module = nn.ReLU()\n                )\n            \n            if _ < l_use_drop and self.use_dropout[_]:\n                self.fc_blocks.add_module(\n                    name = 'regressor_fc_dropout_{}'.format(_),\n                    module = nn.Dropout(p = self.drop_prob[_])\n                )\n\n    def forward(self, inputs):\n        msg = 'the base class [LinearModel] is not callable!'\n        sys.exit(msg)\n\nif __name__ == '__main__':\n    fc_layers = [2133, 1024, 1024, 85]\n    iterations = 3\n    use_dropout = [True, True, False]\n    drop_prob = [0.5, 0.5, 0]\n    use_ac_func = [True, True, False]\n    device = torch.device('cuda')\n    net = LinearModel(fc_layers, use_dropout, drop_prob, use_ac_func).to(device)\n    print(net)\n    nx = np.zeros([2, 2048])\n    vx = torch.from_numpy(nx).to(device)\n"""
src/PRNetEncoder.py,3,"b""\n\n'''\n    file:   PRnetEncoder.py\n\n    date:   2018_05_22\n    author: zhangxiong(1025679612@qq.com)\n    mark:   the algorithm is cited from PRNet code\n'''\n\nfrom __future__ import print_function\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Residual(nn.Module):\n    def __init__(self, use_bn, input_channels, out_channels, mid_channels, kernel_size = 3, padding = 1, stride = 1):\n        super(Residual, self).__init__()\n        self.use_bn = use_bn\n        self.out_channels   = out_channels\n        self.input_channels = input_channels\n        self.mid_channels   = mid_channels\n\n        self.down_channel = nn.Conv2d(input_channels, self.mid_channels, kernel_size = 1)\n        self.AcFunc       = nn.ReLU()\n        if use_bn:\n            self.bn_0 = nn.BatchNorm2d(num_features = self.mid_channels)\n            self.bn_1 = nn.BatchNorm2d(num_features = self.mid_channels)\n            self.bn_2 = nn.BatchNorm2d(num_features = self.out_channels)\n\n        self.conv = nn.Conv2d(self.mid_channels, self.mid_channels, kernel_size = kernel_size, padding = padding, stride = stride)\n\n        self.up_channel = nn.Conv2d(self.mid_channels, out_channels, kernel_size= 1)\n\n        if input_channels != out_channels:\n            self.trans = nn.Conv2d(input_channels, out_channels, kernel_size = 1)\n    \n    def forward(self, inputs):\n        x = self.down_channel(inputs)\n        if self.use_bn:\n            x = self.bn_0(x)\n        x = self.AcFunc(x)\n\n        x = self.conv(x)\n        if self.use_bn:\n            x = self.bn_1(x)\n        x = self.AcFunc(x)\n\n        x = self.up_channel(x)\n\n        if self.input_channels != self.out_channels:\n            x += self.trans(inputs)\n        else:\n            x += inputs\n\n        if self.use_bn:\n            x = self.bn_2(x)\n        \n        return self.AcFunc(x)\n\nclass PRNetEncoder(nn.Module):\n    def __init__(self):\n        super(PRNetEncoder, self).__init__()\n        self.conv_blocks = nn.Sequential(\n            nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size = 3, stride = 1, padding = 1), # to 256 x 256 x 8\n            nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 3, stride = 1, padding = 1), # to 256 x 256 x 16\n            Residual(use_bn = True, input_channels = 16, out_channels = 32, mid_channels = 16, stride = 1, padding = 1), # to 256 x 256 x 32\n            nn.MaxPool2d(kernel_size = 2, stride = 2), # to 128 x 128 x 32\n            Residual(use_bn = True, input_channels = 32, out_channels = 32, mid_channels = 16, stride = 1, padding = 1), # to 128 x 128 x 32\n            Residual(use_bn = True, input_channels = 32, out_channels = 32, mid_channels = 16, stride = 1, padding = 1), # to 128 x 128 x 32\n            Residual(use_bn = True, input_channels = 32, out_channels = 64, mid_channels = 32, stride = 1, padding = 1), # to 128 x 128 x 64\n            nn.MaxPool2d(kernel_size = 2, stride = 2), # to 64 x 64 x 64\n            Residual(use_bn = True, input_channels = 64, out_channels = 64, mid_channels = 32, stride = 1, padding = 1), # to 64 x 64 x 64\n            Residual(use_bn = True, input_channels = 64, out_channels = 64, mid_channels = 32, stride = 1, padding = 1), # to 64 x 64 x 64\n            Residual(use_bn = True, input_channels = 64, out_channels = 128, mid_channels = 64, stride = 1, padding = 1), # to 64 x 64 x 128\n            nn.MaxPool2d(kernel_size = 2, stride = 2), # to 32 x 32 x 128\n            Residual(use_bn = True, input_channels = 128, out_channels = 128, mid_channels = 64, stride = 1, padding = 1), # to 32 x 32 x 128\n            Residual(use_bn = True, input_channels = 128, out_channels = 128, mid_channels = 64, stride = 1, padding = 1), # to 32 x 32 x 128\n            Residual(use_bn = True, input_channels = 128, out_channels = 256, mid_channels = 128, stride = 1, padding = 1), # to 32 x 32 x 256\n            nn.MaxPool2d(kernel_size = 2, stride = 2), # to 16 x 16 x 256\n            Residual(use_bn = True, input_channels = 256, out_channels = 256, mid_channels = 128, stride = 1, padding = 1), # to 16 x 16 x 256\n            Residual(use_bn = True, input_channels = 256, out_channels = 256, mid_channels = 128, stride = 1, padding = 1), # to 16 x 16 x 256\n            Residual(use_bn = True, input_channels = 256, out_channels = 512, mid_channels = 256, stride = 1, padding = 1), # to 16 x 16 x 512\n            nn.MaxPool2d(kernel_size = 2, stride = 2), # to 8 x 8 x 512\n            Residual(use_bn = True, input_channels = 512, out_channels = 512, mid_channels = 256, stride = 1, padding = 1), # to 8 x 8 x 512\n            nn.MaxPool2d(kernel_size = 2, stride = 2) , # to 4 x 4 x 512\n            Residual(use_bn = True, input_channels = 512, out_channels = 512, mid_channels = 256, stride = 1, padding = 1), # to 4 x 4 x 512\n            nn.MaxPool2d(kernel_size = 2, stride = 2), # to 2 x 2 x 512\n            Residual(use_bn = True, input_channels = 512, out_channels = 512, mid_channels = 256, stride = 1, padding = 1) # to 2 x 2 x 512\n        )\n    \n    def forward(self, inputs):\n        return self.conv_blocks(inputs).view(-1, 2048)\n\n\nif __name__ == '__main__':\n    net = PRNetEncoder()\n    inputs = torch.ones(size = (10, 3, 256, 256)).float()\n    r = net(inputs)\n    print(r.shape)"""
src/Resnet.py,7,"b'\n\'\'\'\n    file:   Resnet.py\n\n    date:   2018_05_02\n    author: zhangxiong(1025679612@qq.com)\n    mark:   copied from pytorch sourc code\n\'\'\'\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nfrom torch.nn.parameter import Parameter\nimport torch.optim as optim\nimport numpy as np\nimport math\nimport torchvision\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n       # self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n       # x = self.fc(x)\n\n        return x\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\ndef copy_parameter_from_resnet50(model, res50_dict):\n    cur_state_dict = model.state_dict()\n    for name, param in list(res50_dict.items())[0:None]:\n        if name not in cur_state_dict: \n            print(\'unexpected \', name, \' !\')\n            continue \n        if isinstance(param, Parameter): \n            param = param.data\n        try:\n            cur_state_dict[name].copy_(param)\n        except:\n            print(name, \' is inconsistent!\')\n            continue\n    print(\'copy state dict finished!\')\n\ndef load_Res50Model():\n    model = ResNet(Bottleneck, [3, 4, 6, 3])\n    copy_parameter_from_resnet50(model, torchvision.models.resnet50(pretrained = True).state_dict())\n    return model\n\nif __name__ == \'__main__\':\n    vx = torch.autograd.Variable(torch.from_numpy(np.array([1, 1, 1])))\n    vy = torch.autograd.Variable(torch.from_numpy(np.array([2, 2, 2])))\n    vz = torch.cat([vx, vy], 0)\n    vz[0] = 100\n    print(vz)\n    print(vx)'"
src/SMPL.py,27,"b'\n\'\'\'\n    file:   SMPL.py\n\n    date:   2018_05_03\n    author: zhangxiong(1025679612@qq.com)\n    mark:   the algorithm is cited from original SMPL\n\'\'\'\nimport torch\nfrom config import args\nimport json\nimport sys\nimport numpy as np\nfrom util import batch_global_rigid_transformation, batch_rodrigues, batch_lrotmin, reflect_pose\nimport torch.nn as nn\n\nclass SMPL(nn.Module):\n    def __init__(self, model_path, joint_type = \'cocoplus\', obj_saveable = False):\n        super(SMPL, self).__init__()\n\n        if joint_type not in [\'cocoplus\', \'lsp\']:\n            msg = \'unknow joint type: {}, it must be either ""cocoplus"" or ""lsp""\'.format(joint_type)\n            sys.exit(msg)\n\n        self.model_path = model_path\n        self.joint_type = joint_type\n        with open(model_path, \'r\') as reader:\n            model = json.load(reader)\n        \n        if obj_saveable:\n            self.faces = model[\'f\']\n        else:\n            self.faces = None\n\n        np_v_template = np.array(model[\'v_template\'], dtype = np.float)\n        self.register_buffer(\'v_template\', torch.from_numpy(np_v_template).float())\n        self.size = [np_v_template.shape[0], 3]\n\n        np_shapedirs = np.array(model[\'shapedirs\'], dtype = np.float)\n        self.num_betas = np_shapedirs.shape[-1]\n        np_shapedirs = np.reshape(np_shapedirs, [-1, self.num_betas]).T\n        self.register_buffer(\'shapedirs\', torch.from_numpy(np_shapedirs).float())\n\n        np_J_regressor = np.array(model[\'J_regressor\'], dtype = np.float)\n        self.register_buffer(\'J_regressor\', torch.from_numpy(np_J_regressor).float())\n\n        np_posedirs = np.array(model[\'posedirs\'], dtype = np.float)\n        num_pose_basis = np_posedirs.shape[-1]\n        np_posedirs = np.reshape(np_posedirs, [-1, num_pose_basis]).T\n        self.register_buffer(\'posedirs\', torch.from_numpy(np_posedirs).float())\n\n        self.parents = np.array(model[\'kintree_table\'])[0].astype(np.int32)\n\n        np_joint_regressor = np.array(model[\'cocoplus_regressor\'], dtype = np.float)\n        if joint_type == \'lsp\':\n            self.register_buffer(\'joint_regressor\', torch.from_numpy(np_joint_regressor[:, :14]).float())\n        else:\n            self.register_buffer(\'joint_regressor\', torch.from_numpy(np_joint_regressor).float())\n\n        np_weights = np.array(model[\'weights\'], dtype = np.float)\n\n        vertex_count = np_weights.shape[0] \n        vertex_component = np_weights.shape[1]\n\n        batch_size = max(args.batch_size + args.batch_3d_size, args.eval_batch_size)\n        np_weights = np.tile(np_weights, (batch_size, 1))\n        self.register_buffer(\'weight\', torch.from_numpy(np_weights).float().reshape(-1, vertex_count, vertex_component))\n\n        self.register_buffer(\'e3\', torch.eye(3).float())\n        \n        self.cur_device = None\n\n    def save_obj(self, verts, obj_mesh_name):\n        if not self.faces:\n            msg = \'obj not saveable!\'\n            sys.exit(msg)\n\n        with open(obj_mesh_name, \'w\') as fp:\n            for v in verts:\n                fp.write( \'v %f %f %f\\n\' % ( v[0], v[1], v[2]) )\n\n            for f in self.faces: # Faces are 1-based, not 0-based in obj files\n                fp.write( \'f %d %d %d\\n\' %  (f[0] + 1, f[1] + 1, f[2] + 1) )\n\n    def forward(self, beta, theta, get_skin = False):\n        if not self.cur_device:\n            device = beta.device\n            self.cur_device = torch.device(device.type, device.index)\n\n        num_batch = beta.shape[0]\n\n        v_shaped = torch.matmul(beta, self.shapedirs).view(-1, self.size[0], self.size[1]) + self.v_template\n        Jx = torch.matmul(v_shaped[:, :, 0], self.J_regressor)\n        Jy = torch.matmul(v_shaped[:, :, 1], self.J_regressor)\n        Jz = torch.matmul(v_shaped[:, :, 2], self.J_regressor)\n        J = torch.stack([Jx, Jy, Jz], dim = 2)\n\n        Rs = batch_rodrigues(theta.view(-1, 3)).view(-1, 24, 3, 3)\n        pose_feature = (Rs[:, 1:, :, :]).sub(1.0, self.e3).view(-1, 207)\n        v_posed = torch.matmul(pose_feature, self.posedirs).view(-1, self.size[0], self.size[1]) + v_shaped\n        self.J_transformed, A = batch_global_rigid_transformation(Rs, J, self.parents, rotate_base = True)\n\n        weight = self.weight[:num_batch]\n        W = weight.view(num_batch, -1, 24)\n        T = torch.matmul(W, A.view(num_batch, 24, 16)).view(num_batch, -1, 4, 4)\n        \n        v_posed_homo = torch.cat([v_posed, torch.ones(num_batch, v_posed.shape[1], 1, device = self.cur_device)], dim = 2)\n        v_homo = torch.matmul(T, torch.unsqueeze(v_posed_homo, -1))\n\n        verts = v_homo[:, :, :3, 0]\n\n        joint_x = torch.matmul(verts[:, :, 0], self.joint_regressor)\n        joint_y = torch.matmul(verts[:, :, 1], self.joint_regressor)\n        joint_z = torch.matmul(verts[:, :, 2], self.joint_regressor)\n\n        joints = torch.stack([joint_x, joint_y, joint_z], dim = 2)\n\n        if get_skin:\n            return verts, joints, Rs\n        else:\n            return joints\n\nif __name__ == \'__main__\':\n    device = torch.device(\'cuda\', 0)\n    smpl = SMPL(args.smpl_model, obj_saveable = True).to(device)\n    pose= np.array([\n            1.22162998e+00,   1.17162502e+00,   1.16706634e+00,\n            -1.20581151e-03,   8.60930011e-02,   4.45963144e-02,\n            -1.52801601e-02,  -1.16911056e-02,  -6.02894090e-03,\n            1.62427306e-01,   4.26302850e-02,  -1.55304456e-02,\n            2.58729942e-02,  -2.15941742e-01,  -6.59851432e-02,\n            7.79098943e-02,   1.96353287e-01,   6.44420758e-02,\n            -5.43042570e-02,  -3.45508829e-02,   1.13200583e-02,\n            -5.60734887e-04,   3.21716577e-01,  -2.18840033e-01,\n            -7.61821344e-02,  -3.64610642e-01,   2.97633410e-01,\n            9.65453908e-02,  -5.54007106e-03,   2.83410680e-02,\n            -9.57194716e-02,   9.02515948e-02,   3.31488043e-01,\n            -1.18847653e-01,   2.96623230e-01,  -4.76809204e-01,\n            -1.53382001e-02,   1.72342166e-01,  -1.44332021e-01,\n            -8.10869411e-02,   4.68325168e-02,   1.42248288e-01,\n            -4.60898802e-02,  -4.05981280e-02,   5.28727695e-02,\n            3.20133418e-02,  -5.23784310e-02,   2.41559884e-03,\n            -3.08033824e-01,   2.31431410e-01,   1.62540793e-01,\n            6.28208935e-01,  -1.94355965e-01,   7.23800480e-01,\n            -6.49612308e-01,  -4.07179184e-02,  -1.46422181e-02,\n            4.51475441e-01,   1.59122205e+00,   2.70355493e-01,\n            2.04248756e-01,  -6.33800551e-02,  -5.50178960e-02,\n            -1.00920045e+00,   2.39532292e-01,   3.62904727e-01,\n            -3.38783532e-01,   9.40650925e-02,  -8.44506770e-02,\n            3.55101633e-03,  -2.68924050e-02,   4.93676625e-02],dtype = np.float)\n        \n    beta = np.array([-0.25349993,  0.25009069,  0.21440795,  0.78280628,  0.08625954,\n            0.28128183,  0.06626327, -0.26495767,  0.09009246,  0.06537955 ])\n\n    vbeta = torch.tensor(np.array([beta])).float().to(device)\n    vpose = torch.tensor(np.array([pose])).float().to(device)\n\n    verts, j, r = smpl(vbeta, vpose, get_skin = True)\n\n    smpl.save_obj(verts[0].cpu().numpy(), \'./mesh.obj\')\n\n    rpose = reflect_pose(pose)\n    vpose = torch.tensor(np.array([rpose])).float().to(device)\n    \n    verts, j, r = smpl(vbeta, vpose, get_skin = True)\n    smpl.save_obj(verts[0].cpu().numpy(), \'./rmesh.obj\')\n\n'"
src/config.py,0,"b""\n'''\n    file:   config.py\n\n    date:   2018_04_29\n    author: zhangxiong(1025679612@qq.com)\n'''\n\nimport argparse\n\nparser = argparse.ArgumentParser(description = 'hmr model')\n\nparser.add_argument(\n    '--fine-tune',\n    default = True,\n    type = bool,\n    help = 'fine tune or not.'\n)\n\nparser.add_argument(\n    '--encoder-network',\n    type = str,\n    default = 'resnet50',\n    help = 'the encoder network name'\n)\n\nparser.add_argument(\n    '--smpl-mean-theta-path', \n    type = str, \n    default = 'E:/HMR/model/neutral_smpl_mean_params.h5', \n    help = 'the path for mean smpl theta value'\n)\n\nparser.add_argument(\n    '--smpl-model',\n    type = str,\n    default = 'E:/HMR/model/neutral_smpl_with_cocoplus_reg.txt',\n    help = 'smpl model path'\n)\n\nparser.add_argument(\n    '--total-theta-count', \n    type = int, \n    default = 85,\n    help = 'the count of theta param'\n)\n\nparser.add_argument(\n    '--batch-size',\n    type = int,\n    default = 8,\n    help = 'batch size'\n)\n\nparser.add_argument(\n    '--batch-3d-size',\n    type = int,\n    default = 8,\n    help = '3d data batch size'\n)\n\nparser.add_argument(\n    '--adv-batch-size',\n    type = int,\n    default = 24,\n    help = 'default adv batch size'\n)\n\nparser.add_argument(\n    '--eval-batch-size',\n    type = int,\n    default = 400,\n    help = 'default eval batch size'\n)\n\nparser.add_argument(\n    '--joint-count',\n    type = int,\n    default = 24,\n    help = 'the count of joints'\n)\n\nparser.add_argument(\n    '--beta-count',\n    type = int,\n    default = 10,\n    help = 'the count of beta'\n)\n\nparser.add_argument(\n    '--use-adv-train',\n    type = bool,\n    default = True,\n    help = 'use adv traing or not'\n)\n\nparser.add_argument(\n    '--scale-min',\n    type = float,\n    default = 1.1,\n    help = 'min scale'\n)\n\nparser.add_argument(\n    '--scale-max',\n    type = float,\n    default = 1.5,\n    help = 'max scale'\n)\n\nparser.add_argument(\n    '--num-worker',\n    type = int,\n    default = 1,\n    help = 'pytorch number worker.'\n)\n\nparser.add_argument(\n    '--iter-count',\n    type = int,\n    default = 500001,\n    help = 'iter count, eatch contains batch-size samples'\n)\n\nparser.add_argument(\n    '--e-lr',\n    type = float,\n    default = 0.00001,\n    help = 'encoder learning rate.'\n)\n\nparser.add_argument(\n    '--d-lr',\n    type = float,\n    default = 0.0001,\n    help = 'Adversarial prior learning rate.'\n)\n\nparser.add_argument(\n    '--e-wd',\n    type = float,\n    default = 0.0001,\n    help = 'encoder weight decay rate.'\n)\n\nparser.add_argument(\n    '--d-wd',\n    type = float,\n    default = 0.0001,\n    help = 'Adversarial prior weight decay'\n)\n\nparser.add_argument(\n    '--e-loss-weight', \n    type = float,\n    default = 60, \n    help = 'weight on encoder 2d kp losses.'\n)\n\nparser.add_argument(\n    '--d-loss-weight',\n    type = float,\n    default = 1,\n    help = 'weight on discriminator losses'\n)\n\n\nparser.add_argument(\n    '--d-disc-ratio',\n    type = float,\n    default = 1.0,\n    help = 'multiple weight of discriminator loss'\n)\n\nparser.add_argument(\n    '--e-3d-loss-weight',\n    type = float,\n    default = 60,\n    help = 'weight on encoder thetas losses.'\n)\n\nparser.add_argument(\n    '--e-shape-ratio',\n    type = float,\n    default = 5,\n    help = 'multiple weight of shape loss'\n)\n\nparser.add_argument(\n    '--e-3d-kp-ratio',\n    type = float,\n    default = 10.0,\n    help = 'multiple weight of 3d key point.'\n)\n\nparser.add_argument(\n    '--e-pose-ratio',\n    type = float,\n    default = 20,\n    help = 'multiple weight of pose'\n)\n\nparser.add_argument(\n    '--save-folder',\n    type = str,\n    default = 'E:/HMR/data_advanced/trained_model',\n    help = 'save model path'\n)\n\nparser.add_argument(\n    '--enable-inter-supervision',\n    type = bool,\n    default = False,\n    help = 'enable inter supervision or not.'\n)\n\ntrain_2d_set = ['coco', 'lsp', 'lsp_ext', 'ai-ch']\ntrain_3d_set = ['mpi-inf-3dhp', 'hum3.6m']\ntrain_adv_set = ['mosh']\neval_set = ['up3d']\n\nallowed_encoder_net = ['hourglass', 'resnet50', 'densenet169']\n\nencoder_feature_count = {\n    'hourglass' : 4096,\n    'resnet50' : 2048,\n    'densenet169' : 1664\n}\n\ncrop_size = {\n    'hourglass':256,\n    'resnet50':224,\n    'densenet169':224\n}\n\ndata_set_path = {\n    'coco':'E:/HMR/data/COCO/',\n    'lsp':'E:/HMR/data/lsp',\n    'lsp_ext':'E:/HMR/data/lsp_ext',\n    'ai-ch':'E:/HMR/data/ai_challenger_keypoint_train_20170902',\n    'mpi-inf-3dhp':'E:/HMR/data/mpi_inf_3dhp',\n    'hum3.6m':'E:/HMR/data/human3.6m',\n    'mosh':'E:/HMR/data/mosh_gen',\n    'up3d':'E:/HMR/data/up3d_mpii'\n}\n\npre_trained_model = {\n    'generator' : '/media/disk1/zhangxiong/HMR/hmr_resnet50/fine_tuned/3500_generator.pkl',\n    'discriminator' : '/media/disk1/zhangxiong/HMR/hmr_resnet50/fine_tuned/3500_discriminator.pkl'\n}\n\nargs = parser.parse_args()\nencoder_network = args.encoder_network\nargs.feature_count = encoder_feature_count[encoder_network]\nargs.crop_size = crop_size[encoder_network]\n"""
src/densenet.py,8,"b'\nimport re\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\nfrom collections import OrderedDict\nimport sys\n\n__all__ = [\'DenseNet\', \'densenet121\', \'densenet169\', \'densenet201\', \'densenet161\']\n\n\nmodel_urls = {\n    \'densenet121\': \'https://download.pytorch.org/models/densenet121-a639ec97.pth\',\n    \'densenet169\': \'https://download.pytorch.org/models/densenet169-b2777c0a.pth\',\n    \'densenet201\': \'https://download.pytorch.org/models/densenet201-c1103571.pth\',\n    \'densenet161\': \'https://download.pytorch.org/models/densenet161-8d451a50.pth\',\n}\n\n\ndef densenet121(pretrained=False, **kwargs):\n    r""""""Densenet-121 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16),\n                     **kwargs)\n    if pretrained:\n        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n        # They are also in the checkpoints in model_urls. This pattern is used\n        # to find such keys.\n        pattern = re.compile(\n            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n        state_dict = model_zoo.load_url(model_urls[\'densenet121\'])\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef densenet169(pretrained=False, **kwargs):\n    r""""""Densenet-169 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 32, 32),\n                     **kwargs)\n    if pretrained:\n        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n        # They are also in the checkpoints in model_urls. This pattern is used\n        # to find such keys.\n        pattern = re.compile(\n            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n        state_dict = model_zoo.load_url(model_urls[\'densenet169\'])\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef densenet201(pretrained=False, **kwargs):\n    r""""""Densenet-201 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 48, 32),\n                     **kwargs)\n    if pretrained:\n        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n        # They are also in the checkpoints in model_urls. This pattern is used\n        # to find such keys.\n        pattern = re.compile(\n            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n        state_dict = model_zoo.load_url(model_urls[\'densenet201\'])\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef densenet161(pretrained=False, **kwargs):\n    r""""""Densenet-161 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = DenseNet(num_init_features=96, growth_rate=48, block_config=(6, 12, 36, 24),\n                     **kwargs)\n    if pretrained:\n        # \'.\'s are no longer allowed in module names, but pervious _DenseLayer\n        # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n        # They are also in the checkpoints in model_urls. This pattern is used\n        # to find such keys.\n        pattern = re.compile(\n            r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n        state_dict = model_zoo.load_url(model_urls[\'densenet161\'])\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n        model.load_state_dict(state_dict)\n    return model\n\n\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module(\'norm1\', nn.BatchNorm2d(num_input_features)),\n        self.add_module(\'relu1\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv1\', nn.Conv2d(num_input_features, bn_size *\n                        growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module(\'norm2\', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module(\'relu2\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv2\', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                        kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module(\'norm\', nn.BatchNorm2d(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\'conv\', nn.Conv2d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        self.add_module(\'pool\', nn.AvgPool2d(kernel_size=2, stride=2))\n\n\nclass DenseNet(nn.Module):\n    r""""""Densenet-BC model class, based on\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n\n    Args:\n        growth_rate (int) - how many filters to add each layer (`k` in paper)\n        block_config (list of 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n          (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n    """"""\n    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000):\n\n        super(DenseNet, self).__init__()\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv0\', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n            (\'norm0\', nn.BatchNorm2d(num_init_features)),\n            (\'relu0\', nn.ReLU(inplace=True)),\n            (\'pool0\', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n        ]))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = num_features // 2\n\n        # Final batch norm\n        self.features.add_module(\'norm5\', nn.BatchNorm2d(num_features))\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        # Official init from torch repo.\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal(m.weight.data)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, inplace=True)\n        out = F.avg_pool2d(out, kernel_size=7, stride=1).view(features.size(0), -1)\n        #out = self.classifier(out)\n        return out\n\ndef load_denseNet(net_type):\n    if net_type == \'densenet121\':\n        return densenet121(pretrained=True)\n    elif net_type == \'densenet169\':\n        return densenet169(pretrained=True)\n    elif net_type == \'densenet201\':\n        return densenet201(pretrained=True)\n    elif net_type == \'densenet161\':\n        return densenet161(pretrained=True)\n    else:\n        msg = \'invalid denset net type\'\n        sys.exit(msg)\n\nif __name__ == \'__main__\':\n    net = load_denseNet(\'densenet169\')\n    print(net)'"
src/model.py,6,"b""\n'''\n    file:   model.py\n\n    date:   2018_05_03\n    author: zhangxiong(1025679612@qq.com)\n'''\n\nfrom LinearModel import LinearModel\nimport torch.nn as nn\nimport numpy as np\nimport torch\nimport util\nfrom Discriminator import ShapeDiscriminator, PoseDiscriminator, FullPoseDiscriminator\nfrom SMPL import SMPL\nfrom config import args\nimport config\nimport Resnet\nfrom HourGlass import _create_hourglass_net\nfrom densenet import load_denseNet\nimport sys\n\nclass ThetaRegressor(LinearModel):\n    def __init__(self, fc_layers, use_dropout, drop_prob, use_ac_func, iterations):\n        super(ThetaRegressor, self).__init__(fc_layers, use_dropout, drop_prob, use_ac_func)\n        self.iterations = iterations\n        batch_size = max(args.batch_size + args.batch_3d_size, args.eval_batch_size)\n        mean_theta = np.tile(util.load_mean_theta(), batch_size).reshape((batch_size, -1))\n        self.register_buffer('mean_theta', torch.from_numpy(mean_theta).float())\n    '''\n        param:\n            inputs: is the output of encoder, which has 2048 features\n        \n        return:\n            a list contains [ [theta1, theta1, ..., theta1], [theta2, theta2, ..., theta2], ... , ], shape is iterations X N X 85(or other theta count)\n    '''\n    def forward(self, inputs):\n        thetas = []\n        shape = inputs.shape\n        theta = self.mean_theta[:shape[0], :]\n        for _ in range(self.iterations):\n            total_inputs = torch.cat([inputs, theta], 1)\n            theta = theta + self.fc_blocks(total_inputs)\n            thetas.append(theta)\n        return thetas\n\nclass HMRNetBase(nn.Module):\n    def __init__(self):\n        super(HMRNetBase, self).__init__()\n        self._read_configs()\n\n        print('start creating sub modules...')\n        self._create_sub_modules()\n\n    def _read_configs(self):\n        def _check_config():\n            encoder_name = args.encoder_network\n            enable_inter_supervions = args.enable_inter_supervision\n            feature_count = args.feature_count           \n            if encoder_name == 'hourglass':\n                assert args.crop_size == 256\n            elif encoder_name == 'resnet50':\n                assert args.crop_size == 224\n                assert not enable_inter_supervions\n            elif encoder_name.startswith('densenet'):\n                assert args.crop_size == 224\n                assert not enable_inter_supervions\n            else:\n                msg = 'invalid encoder network, only {} is allowd, got {}'.format(args.allowed_encoder_net, encoder_name)\n                sys.exit(msg)\n            assert config.encoder_feature_count[encoder_name] == feature_count\n            \n        _check_config()\n        \n        self.encoder_name = args.encoder_network\n        self.beta_count = args.beta_count\n        self.smpl_model = args.smpl_model\n        self.smpl_mean_theta_path = args.smpl_mean_theta_path\n        self.total_theta_count = args.total_theta_count\n        self.joint_count = args.joint_count\n        self.feature_count = args.feature_count\n        \n    def _create_sub_modules(self):\n        '''\n            ddd smpl model, SMPL can create a mesh from beta & theta\n        '''\n        self.smpl = SMPL(self.smpl_model, obj_saveable = True)\n\n        '''\n            only resnet50 and hourglass is allowd currently, maybe other encoder will be allowd later.\n        '''\n        if self.encoder_name == 'resnet50':\n            print('creating resnet50')\n            self.encoder = Resnet.load_Res50Model()\n        elif self.encoder_name == 'hourglass':\n            print('creating hourglass')\n            self.encoder = _create_hourglass_net()\n        elif self.encoder_name.startswith('densenet'):\n            print('creating densenet')\n            self.encoder = load_denseNet(self.encoder_name)\n        else:\n            assert 0\n        '''\n            regressor can predict betas(include beta and theta which needed by SMPL) from coder extracted from encoder in a iteratirve way\n        '''\n        fc_layers = [self.feature_count + self.total_theta_count, 1024, 1024, 85]\n        use_dropout = [True, True, False]\n        drop_prob = [0.5, 0.5, 0.5]\n        use_ac_func = [True, True, False] #unactive the last layer\n        iterations = 3\n        self.regressor = ThetaRegressor(fc_layers, use_dropout, drop_prob, use_ac_func, iterations)\n        self.iterations = iterations\n\n        print('finished create the encoder modules...')\n\n    def forward(self, inputs):\n        if self.encoder_name == 'resnet50':\n            feature = self.encoder(inputs)\n            thetas = self.regressor(feature)\n            detail_info = []\n            for theta in thetas:\n                detail_info.append(self._calc_detail_info(theta))\n            return detail_info\n        elif self.encoder_name.startswith('densenet'):\n            feature = self.encoder(inputs)\n            thetas = self.regressor(feature)\n            detail_info = []\n            for theta in thetas:\n                detail_info.append(self._calc_detail_info(theta))\n            return detail_info\n        elif self.encoder_name == 'hourglass':\n            if args.enable_inter_supervision:\n                features = self.encoder(inputs)\n                detail_info = []\n                for feature in features:\n                    thetas = self.regressor(feature)\n                    detail_info.append(self._calc_detail_info(thetas[-1]))\n                return detail_info\n            else:\n                features = self.encoder(inputs)\n                thetas = self.regressor(features[-1]) #only the last block\n                detail_info = []\n                for theta in thetas:\n                    detail_info.append(self._calc_detail_info(theta))\n                return detail_info\n        else:\n            assert 0\n\n    '''\n        purpose:\n            calc verts, joint2d, joint3d, Rotation matrix\n\n        inputs:\n            theta: N X (3 + 72 + 10)\n\n        return:\n            thetas, verts, j2d, j3d, Rs\n    '''\n    \n    def _calc_detail_info(self, theta):\n        cam = theta[:, 0:3].contiguous()\n        pose = theta[:, 3:75].contiguous()\n        shape = theta[:, 75:].contiguous()\n        verts, j3d, Rs = self.smpl(beta = shape, theta = pose, get_skin = True)\n        j2d = util.batch_orth_proj(j3d, cam)\n\n        return (theta, verts, j2d, j3d, Rs)\n\nif __name__ == '__main__':\n    cam = np.array([[0.9, 0, 0]], dtype = np.float)\n    pose= np.array([[-9.44920200e+01, -4.25263865e+01, -1.30050643e+01, -2.79970490e-01,\n        3.24995661e-01,  5.03083125e-01, -6.90573755e-01, -4.12994214e-01,\n       -4.21870093e-01,  5.98717416e-01, -1.48420885e-02, -3.85911139e-02,\n        1.13642605e-01,  2.30647176e-01, -2.11843286e-01,  1.31767149e+00,\n       -6.61596447e-01,  4.02174644e-01,  3.03129424e-02,  5.91100770e-02,\n       -8.04416564e-02, -1.12944653e-01,  3.15045050e-01, -1.32838375e-01,\n       -1.33748209e-01, -4.99408923e-01,  1.40508643e-01,  6.10867911e-02,\n       -2.22951915e-02, -4.73448564e-02, -1.48489055e-01,  1.47620442e-01,\n        3.24157346e-01,  7.78414851e-04,  1.70687935e-01, -1.54716815e-01,\n        2.95053507e-01, -2.91967776e-01,  1.26000780e-01,  8.09572677e-02,\n        1.54710846e-02, -4.21941758e-01,  7.44124075e-02,  1.17146423e-01,\n        3.16305389e-01,  5.04810448e-01, -3.65526364e-01,  1.31366428e-01,\n       -2.76658949e-02, -9.17315987e-03, -1.88285742e-01,  7.86409877e-03,\n       -9.41106758e-02,  2.08424367e-01,  1.62278709e-01, -7.98170265e-01,\n       -3.97403587e-03,  1.11321421e-01,  6.07793270e-01,  1.42215980e-01,\n        4.48185010e-01, -1.38429048e-01,  3.77056061e-02,  4.48877661e-01,\n        1.31445158e-01,  5.07427503e-02, -3.80920772e-01, -2.52292254e-02,\n       -5.27745375e-02, -7.43903887e-02,  7.22498075e-02, -6.35824487e-03]])\n\n    beta = np.array([[-3.54196257,  0.90870435, -1.0978663 , -0.20436199,  0.18589762, 0.55789026, -0.18163599,  0.12002746, -0.09172286,  0.4430783 ]])\n    real_shapes = torch.from_numpy(beta).float().cuda()\n    real_poses  = torch.from_numpy(pose).float().cuda()\n\n    net = HMRNetBase().cuda()\n    nx = torch.rand(2, 3, 224, 224).float().cuda()\n"""
src/timer.py,0,"b""\n'''\n    file:   timer.py\n\n    date:   2018_05_09\n    author: zhangxiong(1025679612@qq.com)\n'''\n\nimport time\n\nclass Clock:\n    def __init__(self, start_tick = True):\n        self.pre_time = 0\n        if start_tick:\n            self.start()\n\n    def start(self):\n        self.pre_time = time.time()\n\n    def stop(self):\n        self.cur_time = time.time()\n        print('time {} elapsed!'.format(self.cur_time - self.pre_time))\n        self.pre_time = self.cur_time """
src/trainer.py,26,"b'\n\n\'\'\'\n    file:   trainer.py\n\n    date:   2018_05_07\n    author: zhangxiong(1025679612@qq.com)\n\'\'\'\n\nimport sys\nfrom model import HMRNetBase\nfrom Discriminator import Discriminator\nfrom config import args\nimport config\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\n\nfrom dataloader.AICH_dataloader import AICH_dataloader\nfrom dataloader.COCO2017_dataloader import COCO2017_dataloader\nfrom dataloader.hum36m_dataloader import hum36m_dataloader\nfrom dataloader.lsp_dataloader import LspLoader\nfrom dataloader.lsp_ext_dataloader import LspExtLoader\nfrom dataloader.mosh_dataloader import mosh_dataloader\nfrom dataloader.mpi_inf_3dhp_dataloader import mpi_inf_3dhp_dataloader\nfrom dataloader.eval_dataloader import eval_dataloader\n\nfrom util import align_by_pelvis, batch_rodrigues, copy_state_dict\nfrom timer import Clock\nimport time\nimport datetime\nfrom collections import OrderedDict\nimport os\n\nclass HMRTrainer(object):\n    def __init__(self):\n        self.pix_format = \'NCHW\'\n        self.normalize = True\n        self.flip_prob = 0.5\n        self.use_flip = False\n        self.w_smpl = torch.ones((config.args.eval_batch_size)).float().cuda()\n\n        self._build_model()\n        self._create_data_loader()\n\n    def _create_data_loader(self):\n        self.loader_2d = self._create_2d_data_loader(config.train_2d_set)\n        self.loader_mosh = self._create_adv_data_loader(config.train_adv_set)\n        self.loader_3d = self._create_3d_data_loader(config.train_3d_set)\n        \n    def _build_model(self):\n        print(\'start building modle.\')\n\n        \'\'\'\n            load pretrain model\n        \'\'\'\n        generator = HMRNetBase()\n        model_path = config.pre_trained_model[\'generator\']\n        if os.path.exists(model_path):\n            copy_state_dict(\n                generator.state_dict(), \n                torch.load(model_path),\n                prefix = \'module.\'\n            )\n        else:\n            print(\'model {} not exist!\'.format(model_path))\n\n        discriminator = Discriminator()\n        model_path = config.pre_trained_model[\'discriminator\']\n        if os.path.exists(model_path):\n            copy_state_dict(\n                discriminator.state_dict(),\n                torch.load(model_path),\n                prefix = \'module.\'\n            )\n        else:\n            print(\'model {} not exist!\'.format(model_path))\n\n        self.generator = nn.DataParallel(generator).cuda()\n        self.discriminator = nn.DataParallel(discriminator).cuda()\n        \n        self.e_opt = torch.optim.Adam(\n            self.generator.parameters(),\n            lr = args.e_lr,\n            weight_decay = args.e_wd\n        )\n    \n        self.d_opt = torch.optim.Adam(\n            self.discriminator.parameters(),\n            lr = args.d_lr,\n            weight_decay = args.d_wd\n        )\n\n        self.e_sche = torch.optim.lr_scheduler.StepLR(\n            self.e_opt,\n            step_size = 500,\n            gamma = 0.9\n        )\n\n        self.d_sche = torch.optim.lr_scheduler.StepLR(\n            self.d_opt,\n            step_size = 500,\n            gamma = 0.9\n        )\n\n        print(\'finished build model.\')\n\n    def _create_2d_data_loader(self, data_2d_set):\n        data_set = []\n        for data_set_name in data_2d_set:\n            data_set_path = config.data_set_path[data_set_name]\n            if data_set_name == \'coco\':\n                coco = COCO2017_dataloader(\n                    data_set_path = data_set_path, \n                    use_crop = True, \n                    scale_range = [1.05, 1.3], \n                    use_flip = self.use_flip, \n                    only_single_person = False, \n                    min_pts_required = 7, \n                    max_intersec_ratio = 0.5,\n                    pix_format = self.pix_format,\n                    normalize = self.normalize,\n                    flip_prob = self.flip_prob\n                )\n                data_set.append(coco)\n            elif data_set_name == \'lsp\':\n                lsp = LspLoader(\n                    data_set_path = data_set_path, \n                    use_crop = True, \n                    scale_range = [1.05, 1.3], \n                    use_flip = self.use_flip,\n                    pix_format = self.pix_format,\n                    normalize = self.normalize,\n                    flip_prob = self.flip_prob\n                )\n                data_set.append(lsp)\n            elif data_set_name == \'lsp_ext\':\n                lsp_ext = LspExtLoader(\n                    data_set_path = data_set_path, \n                    use_crop = True, \n                    scale_range = [1.1, 1.2], \n                    use_flip = self.use_flip,\n                    pix_format = self.pix_format,\n                    normalize = self.normalize,\n                    flip_prob = self.flip_prob\n                )\n                data_set.append(lsp_ext)\n            elif data_set_name == \'ai-ch\':\n                ai_ch = AICH_dataloader(\n                    data_set_path = data_set_path,\n                    use_crop = True, \n                    scale_range = [1.1, 1.2], \n                    use_flip = self.use_flip, \n                    only_single_person = False, \n                    min_pts_required = 5,\n                    max_intersec_ratio = 0.1,\n                    pix_format = self.pix_format,\n                    normalize = self.normalize,\n                    flip_prob = self.flip_prob\n                )\n                data_set.append(ai_ch)\n            else:\n                msg = \'invalid 2d dataset\'\n                sys.exit(msg)\n\n        con_2d_dataset = ConcatDataset(data_set)\n\n        return DataLoader(\n            dataset = con_2d_dataset,\n            batch_size = config.args.batch_size,\n            shuffle = True,\n            drop_last = True,\n            pin_memory = True,\n            num_workers = config.args.num_worker\n        )\n\n    def _create_3d_data_loader(self, data_3d_set):\n        data_set = []\n        for data_set_name in data_3d_set:\n            data_set_path = config.data_set_path[data_set_name]\n            if data_set_name == \'mpi-inf-3dhp\':\n                mpi_inf_3dhp = mpi_inf_3dhp_dataloader(\n                    data_set_path = data_set_path, \n                    use_crop = True, \n                    scale_range = [1.1, 1.2], \n                    use_flip = self.use_flip, \n                    min_pts_required = 5,\n                    pix_format = self.pix_format,\n                    normalize = self.normalize,\n                    flip_prob = self.flip_prob\n                )\n                data_set.append(mpi_inf_3dhp)\n            elif data_set_name == \'hum3.6m\':\n                hum36m = hum36m_dataloader(\n                    data_set_path = data_set_path, \n                    use_crop = True, \n                    scale_range = [1.1, 1.2], \n                    use_flip = self.use_flip, \n                    min_pts_required = 5,\n                    pix_format = self.pix_format,\n                    normalize = self.normalize,\n                    flip_prob = self.flip_prob\n                )\n                data_set.append(hum36m)\n            else:\n                msg = \'invalid 3d dataset\'\n                sys.exit(msg)\n\n        con_3d_dataset = ConcatDataset(data_set)\n\n        return DataLoader(\n            dataset = con_3d_dataset,\n            batch_size = config.args.batch_3d_size,\n            shuffle = True,\n            drop_last = True,\n            pin_memory = True,\n            num_workers = config.args.num_worker\n        )\n    \n    def _create_adv_data_loader(self, data_adv_set):\n        data_set = []\n        for data_set_name in data_adv_set:\n            data_set_path = config.data_set_path[data_set_name]\n            if data_set_name == \'mosh\':\n                mosh = mosh_dataloader(\n                    data_set_path = data_set_path,\n                    use_flip = self.use_flip,\n                    flip_prob = self.flip_prob\n                )\n                data_set.append(mosh)\n            else:\n                msg = \'invalid adv dataset\'\n                sys.exit(msg)\n\n        con_adv_dataset = ConcatDataset(data_set)\n        return DataLoader(\n            dataset = con_adv_dataset,\n            batch_size = config.args.adv_batch_size,\n            shuffle = True,\n            drop_last = True,\n            pin_memory = True,\n        )\n    \n    def _create_eval_data_loader(self, data_eval_set):\n        data_set = []\n        for data_set_name in data_eval_set:\n            data_set_path = config.data_set_path[data_set_name]\n            if data_set_name == \'up3d\':\n                up3d = eval_dataloader(\n                    data_set_path = data_set_path,\n                    use_flip = False,\n                    flip_prob = self.flip_prob,\n                    pix_format = self.pix_format,\n                    normalize = self.normalize\n                )\n                data_set.append(up3d)\n            else:\n                msg = \'invalid eval dataset\'\n                sys.exit(msg)\n        con_eval_dataset = ConcatDataset(data_set)\n        return DataLoader(\n            dataset = con_eval_dataset,\n            batch_size = config.args.eval_batch_size,\n            shuffle = False,\n            drop_last = False,\n            pin_memory = True,\n            num_workers = config.args.num_worker\n        )\n\n    def train(self):\n        def save_model(result):\n            exclude_key = \'module.smpl\'\n            def exclude_smpl(model_dict):\n                result = OrderedDict()\n                for (k, v) in model_dict.items():\n                    if exclude_key in k:\n                        continue\n                    result[k] = v\n                return result\n\n            parent_folder = args.save_folder\n            if not os.path.exists(parent_folder):\n                os.makedirs(parent_folder)\n\n            title = result[\'title\']\n            generator_save_path = os.path.join(parent_folder, title + \'generator.pkl\')\n            torch.save(exclude_smpl(self.generator.state_dict()), generator_save_path)\n            disc_save_path = os.path.join(parent_folder, title + \'discriminator.pkl\')\n            torch.save(exclude_smpl(self.discriminator.state_dict()), disc_save_path)\n            with open(os.path.join(parent_folder, title + \'.txt\'), \'w\') as fp:\n                fp.write(str(result))\n        \n        #pre_best_loss = None\n\n        torch.backends.cudnn.benchmark = True\n        loader_2d, loader_3d, loader_mosh = iter(self.loader_2d), iter(self.loader_3d), iter(self.loader_mosh)\n        e_opt, d_opt = self.e_opt, self.d_opt\n        \n        self.generator.train()\n        self.discriminator.train()\n\n        for iter_index in range(config.args.iter_count):\n            try:\n                data_2d = next(loader_2d)\n            except StopIteration:\n                loader_2d = iter(self.loader_2d)\n                data_2d = next(loader_2d)\n\n            try:\n                data_3d = next(loader_3d)\n            except StopIteration:\n                loader_3d = iter(self.loader_3d)\n                data_3d = next(loader_3d)\n            \n            try:\n                data_mosh = next(loader_mosh)\n            except StopIteration:\n                loader_mosh = iter(self.loader_mosh)\n                data_mosh = next(loader_mosh)\n            \n            image_from_2d, image_from_3d = data_2d[\'image\'], data_3d[\'image\']            \n            sample_2d_count, sample_3d_count, sample_mosh_count = image_from_2d.shape[0], image_from_3d.shape[0], data_mosh[\'theta\'].shape[0]\n            images = torch.cat((image_from_2d, image_from_3d), dim = 0).cuda()\n\n            generator_outputs = self.generator(images)\n\n            loss_kp_2d, loss_kp_3d, loss_shape, loss_pose, e_disc_loss, d_disc_loss, d_disc_real, d_disc_predict = self._calc_loss(generator_outputs, data_2d, data_3d, data_mosh)\n            \n            e_loss = loss_kp_2d + loss_kp_3d + loss_shape + loss_pose + e_disc_loss\n            d_loss = d_disc_loss\n\n            e_opt.zero_grad()\n            e_loss.backward()\n            e_opt.step()\n\n            d_opt.zero_grad()\n            d_loss.backward()\n            d_opt.step()\n\n            loss_kp_2d = float(loss_kp_2d)\n            loss_shape = float(loss_shape / args.e_shape_ratio)\n            loss_kp_3d = float(loss_kp_3d / args.e_3d_kp_ratio)\n            loss_pose  = float(loss_pose / args.e_pose_ratio)\n            e_disc_loss = float(e_disc_loss / args.d_disc_ratio)\n            d_disc_loss = float(d_disc_loss / args.d_disc_ratio)\n\n            d_disc_real = float(d_disc_real / args.d_disc_ratio)\n            d_disc_predict = float(d_disc_predict / args.d_disc_ratio)\n\n            e_loss = loss_kp_2d + loss_kp_3d + loss_shape + loss_pose + e_disc_loss\n            d_loss = d_disc_loss\n        \n            iter_msg = OrderedDict(\n                [\n                    (\'time\',datetime.datetime.now().strftime(\'%Y-%m-%d-%H:%M:%S\')),\n                    (\'iter\',iter_index),\n                    (\'e_loss\', e_loss),\n                    (\'2d_loss\',loss_kp_2d),\n                    (\'3d_loss\',loss_kp_3d),\n                    (\'shape_loss\',loss_shape),\n                    (\'pose_loss\', loss_pose),\n                    (\'e_disc_loss\',float(e_disc_loss)),\n                    (\'d_disc_loss\',float(d_disc_loss)),\n                    (\'d_disc_real\', float(d_disc_real)),\n                    (\'d_disc_predict\', float(d_disc_predict))\n                ]\n            )\n\n            print(iter_msg)            \n\n            if iter_index % 500 == 0:\n                iter_msg[\'title\'] = \'{}_{}_\'.format(iter_msg[\'iter\'], iter_msg[\'e_loss\'])\n                save_model(iter_msg)\n\n    def _calc_loss(self, generator_outputs, data_2d, data_3d, data_mosh):\n        def _accumulate_thetas(generator_outputs):\n            thetas = []\n            for (theta, verts, j2d, j3d, Rs) in generator_outputs:\n                thetas.append(theta)\n            return torch.cat(thetas, 0)\n\n        sample_2d_count, sample_3d_count, sample_mosh_count = data_2d[\'kp_2d\'].shape[0], data_3d[\'kp_2d\'].shape[0], data_mosh[\'theta\'].shape\n        data_3d_theta, w_3d, w_smpl = data_3d[\'theta\'].cuda(), data_3d[\'w_3d\'].float().cuda(), data_3d[\'w_smpl\'].float().cuda()\n\n        total_predict_thetas = _accumulate_thetas(generator_outputs)\n        (predict_theta, predict_verts, predict_j2d, predict_j3d, predict_Rs) = generator_outputs[-1]\n\n        real_2d, real_3d = torch.cat((data_2d[\'kp_2d\'], data_3d[\'kp_2d\']), 0).cuda(), data_3d[\'kp_3d\'].float().cuda()\n        predict_j2d, predict_j3d, predict_theta = predict_j2d, predict_j3d[sample_2d_count:, :], predict_theta[sample_2d_count:, :]\n\n        loss_kp_2d = self.batch_kp_2d_l1_loss(real_2d, predict_j2d[:,:14,:]) *  args.e_loss_weight\n        loss_kp_3d = self.batch_kp_3d_l2_loss(real_3d, predict_j3d[:,:14,:], w_3d) * args.e_3d_loss_weight * args.e_3d_kp_ratio\n        \n        real_shape, predict_shape = data_3d_theta[:, 75:], predict_theta[:, 75:]\n        loss_shape = self.batch_shape_l2_loss(real_shape, predict_shape, w_smpl) * args.e_3d_loss_weight * args.e_shape_ratio\n\n        real_pose, predict_pose = data_3d_theta[:, 3:75], predict_theta[:, 3:75]\n        loss_pose = self.batch_pose_l2_loss(real_pose.contiguous(), predict_pose.contiguous(), w_smpl) * args.e_3d_loss_weight * args.e_pose_ratio\n\n        e_disc_loss = self.batch_encoder_disc_l2_loss(self.discriminator(total_predict_thetas)) * args.d_loss_weight * args.d_disc_ratio\n        \n        mosh_real_thetas = data_mosh[\'theta\'].cuda()\n        fake_thetas = total_predict_thetas.detach()\n        fake_disc_value, real_disc_value = self.discriminator(fake_thetas), self.discriminator(mosh_real_thetas)\n        d_disc_real, d_disc_fake, d_disc_loss = self.batch_adv_disc_l2_loss(real_disc_value, fake_disc_value)\n        d_disc_real, d_disc_fake, d_disc_loss = d_disc_real  * args.d_loss_weight * args.d_disc_ratio, d_disc_fake  * args.d_loss_weight * args.d_disc_ratio, d_disc_loss * args.d_loss_weight * args.d_disc_ratio\n\n        return loss_kp_2d, loss_kp_3d, loss_shape, loss_pose, e_disc_loss, d_disc_loss, d_disc_real, d_disc_fake\n\n    """"""\n        purpose:\n            calc L1 error\n        Inputs:\n            kp_gt  : N x K x 3\n            kp_pred: N x K x 2\n    """"""\n    def batch_kp_2d_l1_loss(self, real_2d_kp, predict_2d_kp):\n        kp_gt = real_2d_kp.view(-1, 3)\n        kp_pred = predict_2d_kp.contiguous().view(-1, 2)\n        vis = kp_gt[:, 2]\n        k = torch.sum(vis) * 2.0 + 1e-8\n        dif_abs = torch.abs(kp_gt[:, :2] - kp_pred).sum(1)\n        return torch.matmul(dif_abs, vis) * 1.0 / k\n\n    \'\'\'\n        purpose:\n            calc mse * 0.5\n\n        Inputs:\n            real_3d_kp  : N x k x 3\n            fake_3d_kp  : N x k x 3\n            w_3d        : N x 1\n    \'\'\'\n    def batch_kp_3d_l2_loss(self, real_3d_kp, fake_3d_kp, w_3d):\n        shape = real_3d_kp.shape\n        k = torch.sum(w_3d) * shape[1] * 3.0 * 2.0 + 1e-8\n\n        #first align it\n        real_3d_kp, fake_3d_kp = align_by_pelvis(real_3d_kp), align_by_pelvis(fake_3d_kp)\n        kp_gt = real_3d_kp\n        kp_pred = fake_3d_kp\n        kp_dif = (kp_gt - kp_pred) ** 2\n        return torch.matmul(kp_dif.sum(1).sum(1), w_3d) * 1.0 / k\n        \n    \'\'\'\n        purpose:\n            calc mse * 0.5\n\n        Inputs:\n            real_shape  :   N x 10\n            fake_shape  :   N x 10\n            w_shape     :   N x 1\n    \'\'\'\n    def batch_shape_l2_loss(self, real_shape, fake_shape, w_shape):\n        k = torch.sum(w_shape) * 10.0 * 2.0 + 1e-8\n        shape_dif = (real_shape - fake_shape) ** 2\n        return  torch.matmul(shape_dif.sum(1), w_shape) * 1.0 / k\n\n    \'\'\'\n        Input:\n            real_pose   : N x 72\n            fake_pose   : N x 72\n    \'\'\'\n    def batch_pose_l2_loss(self, real_pose, fake_pose, w_pose):\n        k = torch.sum(w_pose) * 207.0 * 2.0 + 1e-8\n        real_rs, fake_rs = batch_rodrigues(real_pose.view(-1, 3)).view(-1, 24, 9)[:,1:,:], batch_rodrigues(fake_pose.view(-1, 3)).view(-1, 24, 9)[:,1:,:]\n        dif_rs = ((real_rs - fake_rs) ** 2).view(-1, 207)\n        return torch.matmul(dif_rs.sum(1), w_pose) * 1.0 / k\n    \'\'\'\n        Inputs:\n            disc_value: N x 25\n    \'\'\'\n    def batch_encoder_disc_l2_loss(self, disc_value):\n        k = disc_value.shape[0]\n        return torch.sum((disc_value - 1.0) ** 2) * 1.0 / k\n    \'\'\'\n        Inputs:\n            disc_value: N x 25\n    \'\'\'\n    def batch_adv_disc_l2_loss(self, real_disc_value, fake_disc_value):\n        ka = real_disc_value.shape[0]\n        kb = fake_disc_value.shape[0]\n        lb, la = torch.sum(fake_disc_value ** 2) / kb, torch.sum((real_disc_value - 1) ** 2) / ka\n        return la, lb, la + lb\n\ndef main():\n    trainer = HMRTrainer()\n    trainer.train()\n\nif __name__ == \'__main__\':\n    main()\n'"
src/util.py,20,"b'\n\'\'\'\n    file:   util.py\n\n    date:   2018_04_29\n    author: zhangxiong(1025679612@qq.com)\n\'\'\'\n\nimport h5py\nimport torch\nimport numpy as np\nfrom config import args\nimport json\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport cv2\nimport math\nfrom scipy import interpolate\n\ndef load_mean_theta():\n    mean = np.zeros(args.total_theta_count, dtype = np.float)\n\n    mean_values = h5py.File(args.smpl_mean_theta_path)\n    mean_pose = mean_values[\'pose\']\n    mean_pose[:3] = 0\n    mean_shape = mean_values[\'shape\']\n    mean_pose[0]=np.pi\n\n    #init sacle is 0.9\n    mean[0] = 0.9\n\n    mean[3:75] = mean_pose[:]\n    mean[75:] = mean_shape[:]\n\n    return mean\n\ndef batch_rodrigues(theta):\n    #theta N x 3\n    batch_size = theta.shape[0]\n    l1norm = torch.norm(theta + 1e-8, p = 2, dim = 1)\n    angle = torch.unsqueeze(l1norm, -1)\n    normalized = torch.div(theta, angle)\n    angle = angle * 0.5\n    v_cos = torch.cos(angle)\n    v_sin = torch.sin(angle)\n    quat = torch.cat([v_cos, v_sin * normalized], dim = 1)\n    \n    return quat2mat(quat)\n\ndef quat2mat(quat):\n    """"""Convert quaternion coefficients to rotation matrix.\n    Args:\n        quat: size = [B, 4] 4 <===>(w, x, y, z)\n    Returns:\n        Rotation matrix corresponding to the quaternion -- size = [B, 3, 3]\n    """"""\n    norm_quat = quat\n    norm_quat = norm_quat/norm_quat.norm(p=2, dim=1, keepdim=True)\n    w, x, y, z = norm_quat[:,0], norm_quat[:,1], norm_quat[:,2], norm_quat[:,3]\n\n    B = quat.size(0)\n\n    w2, x2, y2, z2 = w.pow(2), x.pow(2), y.pow(2), z.pow(2)\n    wx, wy, wz = w*x, w*y, w*z\n    xy, xz, yz = x*y, x*z, y*z\n\n    rotMat = torch.stack([w2 + x2 - y2 - z2, 2*xy - 2*wz, 2*wy + 2*xz,\n                          2*wz + 2*xy, w2 - x2 + y2 - z2, 2*yz - 2*wx,\n                          2*xz - 2*wy, 2*wx + 2*yz, w2 - x2 - y2 + z2], dim=1).view(B, 3, 3)\n    return rotMat\n\ndef batch_global_rigid_transformation(Rs, Js, parent, rotate_base = False):\n    N = Rs.shape[0]\n    if rotate_base:\n        np_rot_x = np.array([[1, 0, 0], [0, -1, 0], [0, 0, -1]], dtype = np.float)\n        np_rot_x = np.reshape(np.tile(np_rot_x, [N, 1]), [N, 3, 3])\n        rot_x = Variable(torch.from_numpy(np_rot_x).float()).cuda()\n        root_rotation = torch.matmul(Rs[:, 0, :, :],  rot_x)\n    else:\n        root_rotation = Rs[:, 0, :, :]\n    Js = torch.unsqueeze(Js, -1)\n\n    def make_A(R, t):\n        R_homo = F.pad(R, [0, 0, 0, 1, 0, 0])\n        t_homo = torch.cat([t, Variable(torch.ones(N, 1, 1)).cuda()], dim = 1)\n        return torch.cat([R_homo, t_homo], 2)\n    \n    A0 = make_A(root_rotation, Js[:, 0])\n    results = [A0]\n\n    for i in range(1, parent.shape[0]):\n        j_here = Js[:, i] - Js[:, parent[i]]\n        A_here = make_A(Rs[:, i], j_here)\n        res_here = torch.matmul(results[parent[i]], A_here)\n        results.append(res_here)\n\n    results = torch.stack(results, dim = 1)\n\n    new_J = results[:, :, :3, 3]\n    Js_w0 = torch.cat([Js, Variable(torch.zeros(N, 24, 1, 1)).cuda()], dim = 2)\n    init_bone = torch.matmul(results, Js_w0)\n    init_bone = F.pad(init_bone, [3, 0, 0, 0, 0, 0, 0, 0])\n    A = results - init_bone\n\n    return new_J, A\n\n\ndef batch_lrotmin(theta):\n    theta = theta[:,3:].contiguous()\n    Rs = batch_rodrigues(theta.view(-1, 3))\n    print(Rs.shape)\n    e = Variable(torch.eye(3).float())\n    Rs = Rs.sub(1.0, e)\n\n    return Rs.view(-1, 23 * 9)\n\ndef batch_orth_proj(X, camera):\n    \'\'\'\n        X is N x num_points x 3\n    \'\'\'\n    camera = camera.view(-1, 1, 3)\n    X_trans = X[:, :, :2] + camera[:, :, 1:]\n    shape = X_trans.shape\n    return (camera[:, :, 0] * X_trans.view(shape[0], -1)).view(shape)\n\ndef calc_aabb(ptSets):\n    if not ptSets or len(ptSets) == 0:\n        return False, False, False\n    \n    ptLeftTop     = np.array([ptSets[0][0], ptSets[0][1]])\n    ptRightBottom = ptLeftTop.copy()\n    for pt in ptSets:\n        ptLeftTop[0]     = min(ptLeftTop[0], pt[0])\n        ptLeftTop[1]     = min(ptLeftTop[1], pt[1])\n        ptRightBottom[0] = max(ptRightBottom[0], pt[0])\n        ptRightBottom[1] = max(ptRightBottom[1], pt[1])\n\n    return ptLeftTop, ptRightBottom, len(ptSets) >= 5\n\n\'\'\'\n    calculate a obb for a set of points\n\n    inputs: \n        ptSets: a set of points\n\n    return the center and 4 corners of a obb\n\'\'\'\ndef calc_obb(ptSets):\n    ca = np.cov(ptSets,y = None,rowvar = 0,bias = 1)\n    v, vect = np.linalg.eig(ca)\n    tvect = np.transpose(vect)\n    ar = np.dot(ptSets,np.linalg.inv(tvect))\n    mina = np.min(ar,axis=0)\n    maxa = np.max(ar,axis=0)\n    diff    = (maxa - mina)*0.5\n    center  = mina + diff\n    corners = np.array([center+[-diff[0],-diff[1]],center+[diff[0],-diff[1]],center+[diff[0],diff[1]],center+[-diff[0],diff[1]]])\n    corners = np.dot(corners, tvect)\n    return corners[0], corners[1], corners[2], corners[3]\n\ndef get_image_cut_box(leftTop, rightBottom, ExpandsRatio, Center = None):\n    try:\n        l = len(ExpandsRatio)\n    except:\n        ExpandsRatio = [ExpandsRatio, ExpandsRatio, ExpandsRatio, ExpandsRatio]\n\n    def _expand_crop_box(lt, rb, scale):\n        center = (lt + rb) / 2.0\n        xl, xr, yt, yb = lt[0] - center[0], rb[0] - center[0], lt[1] - center[1], rb[1] - center[1]\n        xl, xr, yt, yb = xl * scale[0], xr * scale[1], yt * scale[2], yb * scale[3]\n        #expand it\n        lt, rb = np.array([center[0] + xl, center[1] + yt]), np.array([center[0] + xr, center[1] + yb])\n        lb, rt = np.array([center[0] + xl, center[1] + yb]), np.array([center[0] + xr, center[1] + yt])\n        center = (lt + rb) / 2\n        return center, lt, rt, rb, lb\n\n    if Center == None:\n        Center = (leftTop + rightBottom) // 2\n\n    Center, leftTop, rightTop, rightBottom, leftBottom = _expand_crop_box(leftTop, rightBottom, ExpandsRatio)\n\n    offset = (rightBottom - leftTop) // 2\n\n    cx = offset[0]\n    cy = offset[1]\n\n    r = max(cx, cy)\n\n    cx = r\n    cy = r\n    \n    x = int(Center[0])\n    y = int(Center[1])\n\n    return [x - cx, y - cy], [x + cx, y + cy]\n\ndef shrink(leftTop, rightBottom, width, height):\n    xl = -leftTop[0]\n    xr = rightBottom[0] - width\n\n    yt = -leftTop[1]\n    yb = rightBottom[1] - height\n\n    cx = (leftTop[0] + rightBottom[0]) / 2\n    cy = (leftTop[1] + rightBottom[1]) / 2\n\n    r = (rightBottom[0] - leftTop[0]) / 2\n\n    sx = max(xl, 0) + max(xr, 0)\n    sy = max(yt, 0) + max(yb, 0)\n\n    if (xl <= 0 and xr <= 0) or (yt <= 0 and yb <=0):\n        return leftTop, rightBottom\n    elif leftTop[0] >= 0 and leftTop[1] >= 0 : # left top corner is in box\n        l = min(yb, xr)\n        r = r - l / 2\n        cx = cx - l / 2\n        cy = cy - l / 2\n    elif rightBottom[0] <= width and rightBottom[1] <= height : # right bottom corner is in box\n        l = min(yt, xl)\n        r = r - l / 2\n        cx = cx + l / 2\n        cy = cy + l / 2\n    elif leftTop[0] >= 0 and rightBottom[1] <= height : #left bottom corner is in box\n        l = min(xr, yt)\n        r = r - l  / 2\n        cx = cx - l / 2\n        cy = cy + l / 2\n    elif rightBottom[0] <= width and leftTop[1] >= 0 : #right top corner is in box\n        l = min(xl, yb)\n        r = r - l / 2\n        cx = cx + l / 2\n        cy = cy - l / 2\n    elif xl < 0 or xr < 0 or yb < 0 or yt < 0:\n        return leftTop, rightBottom\n    elif sx >= sy:\n        sx = max(xl, 0) + max(0, xr)\n        sy = max(yt, 0) + max(0, yb)\n        # cy = height / 2\n        if yt >= 0 and yb >= 0:\n            cy = height / 2\n        elif yt >= 0:\n            cy = cy + sy / 2\n        else:\n            cy = cy - sy / 2\n        r = r - sy / 2\n        \n        if xl >= sy / 2 and xr >= sy / 2:\n            pass\n        elif xl < sy / 2:\n            cx = cx - (sy / 2 - xl)\n        else:\n            cx = cx + (sy / 2 - xr)\n    elif sx < sy:\n        cx = width / 2\n        r = r - sx / 2\n        if yt >= sx / 2 and yb >= sx / 2:\n            pass\n        elif yt < sx / 2:\n            cy = cy - (sx / 2 - yt)\n        else:\n            cy = cy + (sx / 2 - yb)\n        \n\n    return [cx - r, cy - r], [cx + r, cy + r]\n\n\'\'\'\n    offset the keypoint by leftTop\n\'\'\'\ndef off_set_pts(keyPoints, leftTop):\n    result = keyPoints.copy()\n    result[:, 0] -= leftTop[0]\n    result[:, 1] -= leftTop[1]\n    return result\n\n\'\'\'\n    cut the image, by expanding a bounding box\n\'\'\'\ndef cut_image(filePath, kps, expand_ratio, leftTop, rightBottom):\n    originImage = cv2.imread(filePath)\n    height       = originImage.shape[0]\n    width        = originImage.shape[1]\n    channels     = originImage.shape[2] if len(originImage.shape) >= 3 else 1\n\n    leftTop, rightBottom = get_image_cut_box(leftTop, rightBottom, expand_ratio)\n    \n    #remove extra space.\n    #leftTop, rightBottom = shrink(leftTop, rightBottom, width, height)\n\n    lt = [int(leftTop[0]), int(leftTop[1])]\n    rb = [int(rightBottom[0]), int(rightBottom[1])]\n\n    lt[0] = max(0, lt[0])\n    lt[1] = max(0, lt[1])\n    rb[0] = min(rb[0], width)\n    rb[1] = min(rb[1], height)\n\n    leftTop      = [int(leftTop[0]), int(leftTop[1])]\n    rightBottom  = [int(rightBottom[0] + 0.5), int(rightBottom[1] + 0.5)]\n\n    dstImage = np.zeros(shape = [rightBottom[1] - leftTop[1], rightBottom[0] - leftTop[0], channels], dtype = np.uint8)\n    dstImage[:,:,:] = 0\n\n    offset = [lt[0] - leftTop[0], lt[1] - leftTop[1]]\n    size   = [rb[0] - lt[0], rb[1] - lt[1]]\n\n    dstImage[offset[1]:size[1] + offset[1], offset[0]:size[0] + offset[0], :] = originImage[lt[1]:rb[1], lt[0]:rb[0],:]\n    return dstImage, off_set_pts(kps, leftTop)\n\n\'\'\'\n    purpose:\n        reflect key point, when the image is reflect by left-right\n    \n    inputs:\n        kps:3d key point(14 x 3)\n\n    marks:\n        the key point is given by lsp order.\n\'\'\'\ndef reflect_lsp_kp(kps):\n    kp_map = [5, 4, 3, 2, 1, 0, 11, 10, 9, 8, 7, 6, 12, 13]\n    joint_ref = kps[kp_map]\n    joint_ref[:,0] = -joint_ref[:,0]\n\n    return joint_ref - np.mean(joint_ref, axis = 0)\n\n\'\'\'\n    purpose:\n        reflect poses, when the image is reflect by left-right\n    \n    inputs:\n        poses: 72 real number\n\'\'\'\ndef reflect_pose(poses):\n    swap_inds = np.array([\n            0, 1, 2, 6, 7, 8, 3, 4, 5, 9, 10, 11, 15, 16, 17, 12, 13, 14, 18,\n            19, 20, 24, 25, 26, 21, 22, 23, 27, 28, 29, 33, 34, 35, 30, 31, 32,\n            36, 37, 38, 42, 43, 44, 39, 40, 41, 45, 46, 47, 51, 52, 53, 48, 49,\n            50, 57, 58, 59, 54, 55, 56, 63, 64, 65, 60, 61, 62, 69, 70, 71, 66,\n            67, 68\n    ])\n\n    sign_flip = np.array([\n            1, -1, -1, 1, -1, -1, 1, -1, -1, 1, -1, -1, 1, -1, -1, 1, -1,\n            -1, 1, -1, -1, 1, -1, -1, 1, -1, -1, 1, -1, -1, 1, -1, -1, 1,\n            -1, -1, 1, -1, -1, 1, -1, -1, 1, -1, -1, 1, -1, -1, 1, -1, -1,\n            1, -1, -1, 1, -1, -1, 1, -1, -1, 1, -1, -1, 1, -1, -1, 1, -1,\n            -1, 1, -1, -1\n    ])\n\n    return poses[swap_inds] * sign_flip\n\n\'\'\'\n    purpose:\n        crop the image\n    inputs:\n        image_path: the \n\'\'\'\ndef crop_image(image_path, angle, lt, rb, scale, kp_2d, crop_size):\n    \'\'\'\n        given a crop box, expand it at 4 directions.(left, right, top, bottom)\n    \'\'\'\n    assert \'error algorithm exist.\' and 0\n\n    def _expand_crop_box(lt, rb, scale):\n        center = (lt + rb) / 2.0\n        xl, xr, yt, yb = lt[0] - center[0], rb[0] - center[0], lt[1] - center[1], rb[1] - center[1]\n        xl, xr, yt, yb = xl * scale[0], xr * scale[1], yt * scale[2], yb * scale[3]\n        #expand it\n        lt, rb = np.array([center[0] + xl, center[1] + yt]), np.array([center[0] + xr, center[1] + yb])\n        lb, rt = np.array([center[0] + xl, center[1] + yb]), np.array([center[0] + xr, center[1] + yt])\n        center = (lt + rb) / 2\n        return center, lt, rt, rb, lb\n    \n    \'\'\'\n        extend the box to square\n    \'\'\'\n    def _extend_box(center, lt, rt, rb, lb, crop_size):\n        lx, ly = np.linalg.norm(rt - lt), np.linalg.norm(lb - lt)\n        dx, dy = (rt - lt) / lx, (lb - lt) / ly\n        l = max(lx, ly) / 2.0\n        return center - l * dx - l * dy, center + l * dx - l *dy, center + l * dx + l * dy, center - l * dx + l * dy, dx, dy, crop_size * 1.0 / l\n\n    def _get_sample_points(lt, rt, rb, lb, crop_size):\n        vec_x = rt - lt\n        vec_y = lb - lt\n        i_x, i_y = np.meshgrid(range(crop_size), range(crop_size))\n        i_x = i_x.astype(np.float)\n        i_y = i_y.astype(np.float)\n        i_x /= float(crop_size)\n        i_y /= float(crop_size)\n        interp_points = i_x[..., np.newaxis].repeat(2, axis=2) * vec_x + i_y[..., np.newaxis].repeat(2, axis=2) * vec_y\n        interp_points += lt\n        return interp_points\n\n    def _sample_image(src_image, interp_points):\n        sample_method = \'nearest\'\n        interp_image = np.zeros((interp_points.shape[0] * interp_points.shape[1], src_image.shape[2]))\n        i_x = range(src_image.shape[1])\n        i_y = range(src_image.shape[0])\n        flatten_interp_points = interp_points.reshape([interp_points.shape[0]*interp_points.shape[1], 2])\n        for i_channel in range(src_image.shape[2]):\n            interp_image[:, i_channel] = interpolate.interpn((i_y, i_x), src_image[:, :, i_channel],\n                                                            flatten_interp_points[:, [1, 0]], method = sample_method,\n                                                            bounds_error=False, fill_value=0)\n        interp_image = interp_image.reshape((interp_points.shape[0], interp_points.shape[1], src_image.shape[2]))\n    \n        return interp_image\n    \n    def _trans_kp_2d(kps, center, dx, dy, lt, ratio):\n        kp2d_offset = kps[:, :2] - center\n        proj_x, proj_y = np.dot(kp2d_offset, dx), np.dot(kp2d_offset, dy)\n        #kp2d = (dx * proj_x + dy * proj_y + lt) * ratio\n        for idx in range(len(kps)):\n            kps[idx, :2] = (dx * proj_x[idx] + dy * proj_y[idx] + lt) * ratio\n        return kps\n\n\n    src_image = cv2.imread(image_path)\n\n    center, lt, rt, rb, lb  = _expand_crop_box(lt, rb, scale)\n\n    #calc rotated box\n    radian = angle * np.pi / 180.0\n    v_sin, v_cos = math.sin(radian), math.cos(radian)\n\n    rot_matrix = np.array(\n        [\n            [v_cos, v_sin],\n            [-v_sin, v_cos]\n        ]\n    )\n\n    n_corner = (np.dot(rot_matrix, np.array([lt - center, rt - center, rb - center, lb - center]).T).T) + center\n    n_lt, n_rt, n_rb, n_lb = n_corner[0], n_corner[1], n_corner[2], n_corner[3] \n    \n    lt, rt, rb, lb = calc_obb(np.array([lt, rt, rb, lb, n_lt, n_rt, n_rb, n_lb]))\n    lt, rt, rb, lb, dx, dy, ratio = _extend_box(center, lt, rt, rb, lb, crop_size = crop_size)\n    s_pts = _get_sample_points(lt, rt, rb, lb, crop_size)\n    dst_image = _sample_image(src_image, s_pts)\n    kp_2d = _trans_kp_2d(kp_2d, center, dx, dy, lt, ratio)\n\n    return dst_image, kp_2d\n\n\n\'\'\'\n    purpose:\n        flip a image given by src_image and the 2d keypoints\n    flip_mode: \n        0: horizontal flip\n        >0: vertical flip\n        <0: horizontal & vertical flip\n\'\'\'\ndef flip_image(src_image, kps):\n    h, w = src_image.shape[0], src_image.shape[1]\n    src_image = cv2.flip(src_image, 1)\n    if kps is not None:\n        kps[:, 0] = w - 1 - kps[:, 0]\n        kp_map = [5, 4, 3, 2, 1, 0, 11, 10, 9, 8, 7, 6, 12, 13]\n        kps[:, :] = kps[kp_map]\n\n    return src_image, kps\n\n\'\'\'\n    src_image: h x w x c\n    pts: 14 x 3\n\'\'\'\ndef draw_lsp_14kp__bone(src_image, pts):\n        bones = [\n            [0, 1, 255, 0, 0],\n            [1, 2, 255, 0, 0],\n            [2, 12, 255, 0, 0],\n            [3, 12, 0, 0, 255],\n            [3, 4, 0, 0, 255],\n            [4, 5, 0, 0, 255],\n            [12, 9, 0, 0, 255],\n            [9,10, 0, 0, 255],\n            [10,11, 0, 0, 255],\n            [12, 8, 255, 0, 0],\n            [8,7, 255, 0, 0],\n            [7,6, 255, 0, 0],\n            [12, 13, 0, 255, 0]\n        ]\n\n        for pt in pts:\n            if pt[2] > 0.2:\n                cv2.circle(src_image,(int(pt[0]), int(pt[1])),2,(0,255,255),-1)\n        \n        for line in bones:\n            pa = pts[line[0]]\n            pb = pts[line[1]]\n            xa,ya,xb,yb = int(pa[0]),int(pa[1]),int(pb[0]),int(pb[1])\n            if pa[2] > 0.2 and pb[2] > 0.2:\n                cv2.line(src_image,(xa,ya),(xb,yb),(line[2], line[3], line[4]),2)  \n\n\'\'\'\n    return whether two segment intersect\n\'\'\'\n\ndef line_intersect(sa, sb):\n    al, ar, bl, br = sa[0], sa[1], sb[0], sb[1]\n    assert al <= ar and bl <= br\n    if al >= br or bl >= ar:\n        return False\n    return True\n\n\'\'\'\n    return whether two rectangle intersect\n    ra, rb left_top point, right_bottom point\n\'\'\'\ndef rectangle_intersect(ra, rb):\n    ax = [ra[0][0], ra[1][0]]\n    ay = [ra[0][1], ra[1][1]]\n\n    bx = [rb[0][0], rb[1][0]]\n    by = [rb[0][1], rb[1][1]]\n\n    return line_intersect(ax, bx) and line_intersect(ay, by)\n\ndef get_intersected_rectangle(lt0, rb0, lt1, rb1):\n    if not rectangle_intersect([lt0, rb0], [lt1, rb1]):\n        return None, None\n\n    lt = lt0.copy()\n    rb = rb0.copy()\n\n    lt[0] = max(lt[0], lt1[0])\n    lt[1] = max(lt[1], lt1[1])\n\n    rb[0] = min(rb[0], rb1[0])\n    rb[1] = min(rb[1], rb1[1])\n    return lt, rb\n\ndef get_union_rectangle(lt0, rb0, lt1, rb1):\n    lt = lt0.copy()\n    rb = rb0.copy()\n\n    lt[0] = min(lt[0], lt1[0])\n    lt[1] = min(lt[1], lt1[1])\n\n    rb[0] = max(rb[0], rb1[0])\n    rb[1] = max(rb[1], rb1[1])\n    return lt, rb\n\ndef get_rectangle_area(lt, rb):\n    return (rb[0] - lt[0]) * (rb[1] - lt[1])\n\ndef get_rectangle_intersect_ratio(lt0, rb0, lt1, rb1):\n    (lt0, rb0), (lt1, rb1) = get_intersected_rectangle(lt0, rb0, lt1, rb1), get_union_rectangle(lt0, rb0, lt1, rb1)\n\n    if lt0 is None:\n        return 0.0\n    else:\n        return 1.0 * get_rectangle_area(lt0, rb0) / get_rectangle_area(lt1, rb1)\n\ndef convert_image_by_pixformat_normalize(src_image, pix_format, normalize):\n    if pix_format == \'NCHW\':\n        src_image = src_image.transpose((2, 0, 1))\n    \n    if normalize:\n        src_image = (src_image.astype(np.float) / 255) * 2.0 - 1.0\n    \n    return src_image\n\n\'\'\'\n    align ty pelvis\n    joints: n x 14 x 3, by lsp order\n\'\'\'\ndef align_by_pelvis(joints):\n    left_id = 3\n    right_id = 2\n    pelvis = (joints[:, left_id, :] + joints[:, right_id, :]) / 2.0\n    return joints - torch.unsqueeze(pelvis, dim=1)\n\ndef copy_state_dict(cur_state_dict, pre_state_dict, prefix = \'\'):\n    def _get_params(key):\n        key = prefix + key\n        if key in pre_state_dict:\n            return pre_state_dict[key]\n        return None\n    \n    for k in cur_state_dict.keys():\n        v = _get_params(k)\n        try:\n            if v is None:\n                print(\'parameter {} not found\'.format(k))\n                continue\n            cur_state_dict[k].copy_(v)\n        except:\n            print(\'copy param {} failed\'.format(k))\n            continue\n\nif __name__ == \'__main__\':\n    image_path = \'E:/HMR/data/COCO/images/train-valid2017/000000000009.jpg\'\n    lt = np.array([-10, -10], dtype = np.float)\n    rb = np.array([10,10], dtype = np.float)\n    print(crop_image(image_path, 45, lt, rb, [1, 1, 1, 1], None))\n'"
src/dataloader/AICH_dataloader.py,3,"b""\n'''\n    file:   AICH_dataloader.py\n\n    author: zhangxiong(1025679612@qq.com)\n    date:   2018_05_08\n    purpose:    load ai challenge dataset\n'''\n\nimport sys\nfrom torch.utils.data import Dataset, DataLoader\nimport scipy.io as scio  \nimport os \nimport glob\nimport numpy as np\nimport random\nimport cv2\nimport json\nimport torch\n\nsys.path.append('./src')\nfrom util import calc_aabb, cut_image, flip_image, draw_lsp_14kp__bone, rectangle_intersect, get_rectangle_intersect_ratio, convert_image_by_pixformat_normalize\nfrom config import args\nfrom timer import Clock\n\nclass AICH_dataloader(Dataset):\n    def __init__(self, data_set_path, use_crop, scale_range, use_flip, only_single_person, min_pts_required, max_intersec_ratio = 0.1, pix_format = 'NHWC', normalize = False, flip_prob = 0.3):\n        self.data_folder = data_set_path\n        self.use_crop = use_crop\n        self.scale_range = scale_range\n        self.use_flip = use_flip\n        self.flip_prob = flip_prob\n        self.only_single_person = only_single_person\n        self.min_pts_required = min_pts_required\n        self.max_intersec_ratio = max_intersec_ratio\n        self.img_ext = '.jpg'\n        self.pix_format = pix_format\n        self.normalize = normalize\n        self._load_data_set()\n    \n    def _load_data_set(self):\n        clk = Clock()\n\n        self.images = []\n        self.kp2ds  = []\n        self.boxs   = []\n        print('start loading AI CH keypoint data.')\n        anno_file_path = os.path.join(self.data_folder, 'keypoint_train_annotations_20170902.json')\n        with open(anno_file_path, 'r') as reader:\n            anno = json.load(reader)\n        for record in anno:\n            image_name = record['image_id'] + self.img_ext\n            image_path = os.path.join(self.data_folder, 'keypoint_train_images_20170902', image_name)\n            kp_set = record['keypoint_annotations']\n            box_set = record['human_annotations']\n            self._handle_image(image_path, kp_set, box_set)\n\n        print('finished load Ai CH keypoint data, total {} samples'.format(len(self)))\n\n        clk.stop()\n\n    def _ai_ch_to_lsp(self, pts):\n        kp_map = [8, 7, 6, 9, 10, 11, 2, 1, 0, 3, 4, 5, 13, 12]\n        pts = np.array(pts, dtype = np.float).reshape(14, 3).copy()\n        pts[:, 2] = (3.0 - pts[:, 2]) / 2.0\n        return pts[kp_map].copy()\n\n    def _handle_image(self, image_path, kp_set, box_set):\n        assert len(kp_set) == len(box_set)\n\n        if len(kp_set) > 1:\n            if self.only_single_person:\n                print('only single person supported now!')\n                return\n        for key in kp_set.keys():\n            kps = kp_set[key]\n            box = box_set[key]\n            self._handle_sample(key, image_path, kps, [ [box[0], box[1]], [box[2], box[3]] ], box_set)\n\n    def _handle_sample(self, key, image_path, pts, box, boxs):\n        def _collect_box(key, boxs):\n            r = []\n            for k, v in boxs.items():\n                if k == key:\n                    continue\n                r.append([[v[0],v[1]], [v[2],v[3]]])\n            return r\n\n        def _collide_heavily(box, boxs):\n            for it in boxs:\n                if get_rectangle_intersect_ratio(box[0], box[1], it[0], it[1]) > self.max_intersec_ratio:\n                    return True\n            return False\n        pts = self._ai_ch_to_lsp(pts)\n        valid_pt_cound = np.sum(pts[:, 2])\n        if valid_pt_cound < self.min_pts_required:\n            return\n\n        boxs = _collect_box(key, boxs)\n        if _collide_heavily(box, boxs):\n            return\n\n        self.images.append(image_path)\n        self.kp2ds.append(pts)\n        lt, rb = box[0], box[1]\n        self.boxs.append((np.array(lt), np.array(rb)))\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        image_path = self.images[index]\n        kps = self.kp2ds[index].copy()\n        box = self.boxs[index]\n\n        scale = np.random.rand(4) * (self.scale_range[1] - self.scale_range[0]) + self.scale_range[0]\n        image, kps = cut_image(image_path, kps, scale, box[0], box[1])\n        ratio = 1.0 * args.crop_size / image.shape[0]\n        kps[:, :2] *= ratio\n        dst_image = cv2.resize(image, (args.crop_size, args.crop_size), interpolation = cv2.INTER_CUBIC)\n        \n        if self.use_flip and random.random() <= self.flip_prob:\n            dst_image, kps = flip_image(dst_image, kps)\n        \n        #normalize kp to [-1, 1]\n        ratio = 1.0 / args.crop_size\n        kps[:, :2] = 2.0 * kps[:, :2] * ratio - 1.0\n\n        return {\n            'image': torch.tensor(convert_image_by_pixformat_normalize(dst_image, self.pix_format, self.normalize)).float(),\n            'kp_2d': torch.tensor(kps).float(),\n            'image_name': self.images[index],\n            'data_set':'AI Ch'\n        }\n\nif __name__ == '__main__':\n    aic = AICH_dataloader(\n        data_set_path = 'E:/HMR/data/ai_challenger_keypoint_train_20170902', \n        use_crop = True, \n        scale_range = [1.1, 1.5], \n        use_flip = True, \n        only_single_person = False, \n        min_pts_required = 5,\n        flip_prob = 1.0\n    )\n    l = len(aic)\n    for _ in range(l):\n        r = aic.__getitem__(_)\n        image = r['image'].cpu().numpy().astype(np.uint8)\n        kps = r['kp_2d'].cpu().numpy()\n        kps[:, :2] = (kps[:, :2] + 1) * args.crop_size / 2.0\n        base_name = os.path.basename(r['image_name'])\n        draw_lsp_14kp__bone(image, kps)\n        cv2.imshow(base_name, cv2.resize(image, (512, 512), interpolation = cv2.INTER_CUBIC))\n        cv2.waitKey(0)\n"""
src/dataloader/COCO2017_dataloader.py,3,"b""\n'''\n    file:   COCO2017_dataloader.py\n\n    author: zhangxiong(1025679612@qq.com)\n    date:   2018_05_09\n    purpose:  load COCO 2017 keypoint dataset\n'''\n\nimport sys\nfrom torch.utils.data import Dataset, DataLoader\nimport scipy.io as scio  \nimport os \nimport glob\nimport numpy as np\nimport random\nimport cv2\nimport json\nimport torch\n\nsys.path.append('./src')\nfrom util import calc_aabb, cut_image, flip_image, draw_lsp_14kp__bone, rectangle_intersect, get_rectangle_intersect_ratio, convert_image_by_pixformat_normalize\nfrom config import args\nfrom timer import Clock\n\nclass COCO2017_dataloader(Dataset):\n    def __init__(self, data_set_path, use_crop, scale_range, use_flip, only_single_person, min_pts_required, max_intersec_ratio = 0.1, pix_format = 'NHWC', normalize = False, flip_prob = 0.3):\n        self.data_folder = data_set_path\n        self.use_crop = use_crop\n        self.scale_range = scale_range\n        self.use_flip = use_flip\n        self.flip_prob = flip_prob\n        self.only_single_person = only_single_person\n        self.min_pts_required = min_pts_required\n        self.max_intersec_ratio = max_intersec_ratio\n        self.pix_format = pix_format\n        self.normalize = normalize\n        self._load_data_set()\n\n    def _load_data_set(self):\n        self.images = []\n        self.kp2ds  = []\n        self.boxs   = []\n        clk = Clock()\n        print('start loading coco 2017 dataset.')\n        anno_file_path = os.path.join(self.data_folder, 'annotations', 'person_keypoints_train2017.json')\n        with open(anno_file_path, 'r') as reader:\n            anno = json.load(reader)\n        \n        def _hash_image_id_(image_id_to_info, coco_images_info):\n            for image_info in coco_images_info:\n                image_id = image_info['id']\n                image_name = image_info['file_name']\n                _anno = {}\n                _anno['image_path'] = os.path.join(self.data_folder, 'images', 'train-valid2017', image_name)\n                _anno['kps'] = []\n                _anno['box'] = []\n                assert not (image_id in image_id_to_info)\n                image_id_to_info[image_id] = _anno\n                \n        images = anno['images']\n\n        image_id_to_info = {}\n        _hash_image_id_(image_id_to_info, images)\n\n\n        annos = anno['annotations']\n        for anno_info in annos:\n            self._handle_anno_info(anno_info, image_id_to_info)\n\n        for k, v in image_id_to_info.items():\n            self._handle_image_info_(v)\n\n        print('finished load coco 2017 dataset, total {} samples.'.format(len(self.images)))\n\n        clk.stop()\n        \n    def _handle_image_info_(self, image_info):\n        image_path = image_info['image_path']\n        kp_set = image_info['kps']\n        box_set = image_info['box']\n        if len(box_set) > 1:\n            if self.only_single_person:\n                return\n\n        for _ in range(len(box_set)):\n            self._handle_sample(_, kp_set, box_set, image_path)\n\n    def _handle_sample(self, key, kps, boxs, image_path):\n        def _collect_box(l, boxs):\n            r = []\n            for _ in range(len(boxs)):\n                if _ == l:\n                    continue\n                r.append(boxs[_])\n            return r\n\n        def _collide_heavily(box, boxs):\n            for it in boxs:\n                if get_rectangle_intersect_ratio(box[0], box[1], it[0], it[1]) > self.max_intersec_ratio:\n                    return True\n            return False\n\n        kp = kps[key]\n        box = boxs[key]\n\n        valid_pt_cound = np.sum(kp[:, 2])\n        if valid_pt_cound < self.min_pts_required:\n            return\n\n        r = _collect_box(key, boxs)\n        if _collide_heavily(box, r):\n            return\n        \n        self.images.append(image_path)\n        self.kp2ds.append(kp.copy())\n        self.boxs.append(box.copy())\n\n    def _handle_anno_info(self, anno_info, image_id_to_info):\n        image_id = anno_info['image_id']\n        kps = anno_info['keypoints']\n        box_info = anno_info['bbox']\n        box = [np.array([int(box_info[0]), int(box_info[1])]), np.array([int(box_info[0] + box_info[2]), int(box_info[1] + box_info[3])])]\n        assert image_id in image_id_to_info\n        _anno = image_id_to_info[image_id]\n        _anno['box'].append(box)\n        _anno['kps'].append(self._convert_to_lsp14_pts(kps))\n\n    def _convert_to_lsp14_pts(self, coco_pts):\n        kp_map = [15, 13, 11, 10, 12, 14, 9, 7, 5, 4, 6, 8, 0, 0]\n        kp_map = [16, 14, 12, 11, 13, 15, 10, 8, 6, 5, 7, 9, 0, 0]\n        kps = np.array(coco_pts, dtype = np.float).reshape(-1, 3)[kp_map].copy()\n        kps[12: ,2] = 0.0 #no neck, top head\n        kps[:, 2] /= 2.0\n        return kps\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        image_path = self.images[index]\n        kps = self.kp2ds[index].copy()\n        box = self.boxs[index]\n\n        scale = np.random.rand(4) * (self.scale_range[1] - self.scale_range[0]) + self.scale_range[0]\n        image, kps = cut_image(image_path, kps, scale, box[0], box[1])\n        ratio = 1.0 * args.crop_size / image.shape[0]\n        kps[:, :2] *= ratio\n        dst_image = cv2.resize(image, (args.crop_size, args.crop_size), interpolation = cv2.INTER_CUBIC)\n\n        if self.use_flip and random.random() <= self.flip_prob:\n            dst_image, kps = flip_image(dst_image, kps)\n        \n        #normalize kp to [-1, 1]\n        ratio = 1.0 / args.crop_size\n        kps[:, :2] = 2.0 * kps[:, :2] * ratio - 1.0\n\n        return {\n            'image': torch.tensor(convert_image_by_pixformat_normalize(dst_image, self.pix_format, self.normalize)).float(),\n            'kp_2d': torch.tensor(kps).float(),\n            'image_name': self.images[index],\n            'data_set':'COCO 2017'\n        }\n\nif __name__ == '__main__':\n    coco = COCO2017_dataloader('E:/HMR/data/COCO/', True, [1.1, 1.5], False, False, 10, 0.1)\n    l = len(coco)\n    for _ in range(l):\n        r = lsp.__getitem__(_)\n        image = r['image'].cpu().numpy().astype(np.uint8)\n        kps = r['kp_2d'].cpu().numpy()\n        base_name = os.path.basename(r['image_name'])\n        draw_lsp_14kp__bone(image, kps)\n        cv2.imshow(base_name, cv2.resize(image, (512, 512), interpolation = cv2.INTER_CUBIC))\n        cv2.waitKey(0)\n"""
src/dataloader/eval_dataloader.py,5,"b""\n'''\n    file:   eval_dataloader.py\n\n    author: zhangxiong(1025679612@qq.com)\n    date:   2018_05_20\n    purpose:  load evaluation data\n'''\nimport sys\nfrom torch.utils.data import Dataset, DataLoader\nimport scipy.io as scio  \nimport os \nimport glob\nimport numpy as np\nimport random\nimport cv2\nimport json\nimport torch\n\nsys.path.append('./src')\nfrom util import calc_aabb, cut_image, flip_image, draw_lsp_14kp__bone, rectangle_intersect, get_rectangle_intersect_ratio, convert_image_by_pixformat_normalize, reflect_pose\nfrom config import args\n# from timer import Clock\n\nclass eval_dataloader(Dataset):\n    def __init__(self, data_set_path, use_flip, pix_format = 'NHWC', normalize = False, flip_prob = 0.3):\n        self.use_flip    = use_flip\n        self.flip_prob   = flip_prob\n        self.data_folder = data_set_path\n        self.pix_format  = pix_format\n        self.normalize   = normalize\n\n        self._load_data_set()\n\n    def _load_data_set(self):\n        # clk = Clock()\n        \n        self.images = sorted(glob.glob(os.path.join(self.data_folder, 'image/*.png')))\n        self.kp2ds = []\n        self.poses = []\n        self.betas = []\n\n        for idx in range(len(self.images)):\n            image_name = os.path.basename(self.images[idx])[:5]\n            anno_path = os.path.join(self.data_folder, 'annos', image_name + '_joints.npy')\n            self.kp2ds.append(np.load(anno_path).T)\n            anno_path = os.path.join(self.data_folder, 'annos', image_name +'.json')\n            with open(anno_path, 'r') as fp:\n                annos = json.load(fp)\n                self.poses.append(np.array(annos['pose']))\n                self.betas.append(np.array(annos['betas']))\n\n        # clk.stop()\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        image_path = self.images[index]\n        kps = self.kp2ds[index].copy()\n        pose = self.poses[index].copy()\n        shape = self.betas[index].copy()\n\n        dst_image = cv2.imread(image_path)\n\n\n        if self.use_flip and random.random() <= self.flip_prob:\n            dst_image, kps = flip_image(dst_image, kps)\n            pose = reflect_poses(pose)\n    \n        #normalize kp to [-1, 1]\n        ratio = 1.0 / args.crop_size\n        kps[:, :2] = 2.0 * kps[:, :2] * ratio - 1.0\n\n        return {\n            'image': torch.tensor(convert_image_by_pixformat_normalize(dst_image, self.pix_format, self.normalize)).float(),\n            'kp_2d': torch.tensor(kps).float(),\n            'pose': torch.tensor(pose).float(),\n            'shape': torch.tensor(shape).float(),\n            'image_name': self.images[index],\n            'data_set':'up_3d_evaluation'\n        }\n\nif __name__ == '__main__':\n    evl = eval_dataloader('E:/HMR/data/up3d_mpii', True)\n    l = evl.__len__()\n    data_loader = DataLoader(evl, batch_size=10,shuffle=True)\n    for _ in range(l):\n        r = evl.__getitem__(_)\n        pass\n        \n    \n"""
src/dataloader/hum36m_dataloader.py,5,"b""\n'''\n    file:   hum36m_dataloader.py\n\n    author: zhangxiong(1025679612@qq.com)\n    date:   2018_05_09\n    purpose:  load hum3.6m data\n'''\n\nimport sys\nfrom torch.utils.data import Dataset, DataLoader\nimport os \nimport glob\nimport numpy as np\nimport random\nimport cv2\nimport json\nimport h5py\nimport torch\n\nsys.path.append('./src')\nfrom util import calc_aabb, cut_image, flip_image, draw_lsp_14kp__bone, rectangle_intersect, get_rectangle_intersect_ratio, convert_image_by_pixformat_normalize, reflect_pose, reflect_lsp_kp\nfrom config import args\nfrom timer import Clock\n\nclass hum36m_dataloader(Dataset):\n    def __init__(self, data_set_path, use_crop, scale_range, use_flip, min_pts_required, pix_format = 'NHWC', normalize = False, flip_prob = 0.3):\n        self.data_folder = data_set_path\n        self.use_crop = use_crop\n        self.scale_range = scale_range\n        self.use_flip = use_flip\n        self.flip_prob = flip_prob\n        self.min_pts_required = min_pts_required\n        self.pix_format = pix_format\n        self.normalize = normalize\n        self._load_data_set()\n\n    def _load_data_set(self):\n        \n        clk = Clock()\n\n        self.images = []\n        self.kp2ds  = []\n        self.boxs   = []\n        self.kp3ds  = []\n        self.shapes = []\n        self.poses  = []\n\n        print('start loading hum3.6m data.')\n\n        anno_file_path = os.path.join(self.data_folder, 'annot.h5')\n        with h5py.File(anno_file_path) as fp:\n            total_kp2d = np.array(fp['gt2d'])\n            total_kp3d = np.array(fp['gt3d'])\n            total_shap = np.array(fp['shape'])\n            total_pose = np.array(fp['pose'])\n            total_image_names = np.array(fp['imagename'])\n\n            assert len(total_kp2d) == len(total_kp3d) and len(total_kp2d) == len(total_image_names) and \\\n                len(total_kp2d) == len(total_shap) and len(total_kp2d) == len(total_pose)\n\n            l = len(total_kp2d)\n            def _collect_valid_pts(pts):\n                r = []\n                for pt in pts:\n                    if pt[2] != 0:\n                        r.append(pt)\n                return r\n\n            for index in range(l):\n                kp2d = total_kp2d[index].reshape((-1, 3))\n                if np.sum(kp2d[:, 2]) < self.min_pts_required:\n                    continue\n                \n                lt, rb, v = calc_aabb(_collect_valid_pts(kp2d))\n                self.kp2ds.append(np.array(kp2d.copy(), dtype = np.float))\n                self.boxs.append((lt, rb))\n                self.kp3ds.append(total_kp3d[index].copy().reshape(-1, 3))\n                self.shapes.append(total_shap[index].copy())\n                self.poses.append(total_pose[index].copy())\n                self.images.append(os.path.join(self.data_folder, 'image') + total_image_names[index].decode())\n\n        print('finished load hum3.6m data, total {} samples'.format(len(self.kp3ds)))\n        \n        clk.stop()\n\n    def __len__(self):\n        return len(self.images)\n        \n    def __getitem__(self, index):\n        image_path = self.images[index]\n        kps = self.kp2ds[index].copy()\n        box = self.boxs[index]\n        kp_3d = self.kp3ds[index].copy()\n\n        scale = np.random.rand(4) * (self.scale_range[1] - self.scale_range[0]) + self.scale_range[0]\n        image, kps = cut_image(image_path, kps, scale, box[0], box[1])\n\n        ratio = 1.0 * args.crop_size / image.shape[0]\n        kps[:, :2] *= ratio\n        dst_image = cv2.resize(image, (args.crop_size, args.crop_size), interpolation = cv2.INTER_CUBIC)\n\n        trival, shape, pose = np.zeros(3), self.shapes[index], self.poses[index]\n\n        if self.use_flip and random.random() <= self.flip_prob:\n            dst_image, kps = flip_image(dst_image, kps)\n            pose = reflect_pose(pose)\n            kp_3d = reflect_lsp_kp(kp_3d)\n\n        #normalize kp to [-1, 1]\n        ratio = 1.0 / args.crop_size\n        kps[:, :2] = 2.0 * kps[:, :2] * ratio - 1.0\n        \n        theta = np.concatenate((trival, pose, shape), axis = 0)\n\n        return {\n            'image': torch.from_numpy(convert_image_by_pixformat_normalize(dst_image, self.pix_format, self.normalize)).float(),\n            'kp_2d': torch.from_numpy(kps).float(),\n            'kp_3d': torch.from_numpy(kp_3d).float(),\n            'theta': torch.from_numpy(theta).float(),\n            'image_name': self.images[index],\n            'w_smpl':1.0,\n            'w_3d':1.0,\n            'data_set':'hum3.6m'\n        }\n\nif __name__ == '__main__':\n    h36m = hum36m_dataloader('E:/HMR/data/human3.6m', True, [1.1, 2.0], True, 5, flip_prob = 1)\n    l = len(h36m)\n    for _ in range(l):\n        r = h36m.__getitem__(_)\n        pass"""
src/dataloader/lsp_dataloader.py,3,"b""\n'''\n    file:   lsp_dataloader.py\n\n    author: zhangxiong(1025679612@qq.com)\n    date:   2018_05_07\n'''\nimport sys\nfrom torch.utils.data import Dataset, DataLoader\nimport scipy.io as scio  \nimport os \nimport glob\nimport numpy as np\nimport random\nimport cv2\nimport torch\n\nsys.path.append('./src')\n\nfrom util import calc_aabb, cut_image, flip_image, draw_lsp_14kp__bone, convert_image_by_pixformat_normalize, reflect_lsp_kp\nfrom config import args\nfrom timer import Clock\n\nclass LspLoader(Dataset):\n    def __init__(self, data_set_path, use_crop, scale_range, use_flip, pix_format = 'NHWC', normalize = False, flip_prob = 0.3):\n        '''\n            marks:\n                data_set path links to the parent folder to lsp, which contains images, joints.mat, README.txt\n            \n            inputs:\n                use_crop crop the image or not, it should be True by default\n                scale_range, contain the scale range\n                use_flip, left right flip is allowed\n        '''\n        self.use_crop    = use_crop\n        self.scale_range = scale_range\n        self.use_flip    = use_flip\n        self.flip_prob   = flip_prob\n        self.data_folder = data_set_path\n        self.pix_format  = pix_format\n        self.normalize   = normalize\n\n        self._load_data_set()\n\n    def _load_data_set(self):\n        clk = Clock()\n        print('loading LSP data.')\n        self.images = []\n        self.kp2ds  = []\n        self.boxs   = []\n\n        anno_file_path = os.path.join(self.data_folder, 'joints.mat')\n        anno = scio.loadmat(anno_file_path)\n        kp2d = anno['joints'].transpose(2, 1, 0) # N x k x 3\n        visible = np.logical_not(kp2d[:, :, 2])\n        kp2d[:, :, 2] = visible.astype(kp2d.dtype)\n        image_folder = os.path.join(self.data_folder, 'images')\n        images = sorted(glob.glob(image_folder + '/im*.jpg'))\n        for _ in range(len(images)):\n            self._handle_image(images[_], kp2d[_])\n\n        print('finished load LSP data.')\n        clk.stop()\n        \n    def _handle_image(self, image_path, kps):\n        pt_valid = []\n        for pt in kps:\n            if pt[2] == 1:\n                pt_valid.append(pt)\n        lt, rb, valid = calc_aabb(pt_valid)\n\n        if not valid:\n            return\n\n        self.kp2ds.append(kps.copy().astype(np.float))\n        self.images.append(image_path)\n        self.boxs.append((lt, rb))\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        image_path = self.images[index]\n        kps = self.kp2ds[index].copy()\n        box = self.boxs[index]\n\n        scale = np.random.rand(4) * (self.scale_range[1] - self.scale_range[0]) + self.scale_range[0]\n        image, kps = cut_image(image_path, kps, scale, box[0], box[1])\n        ratio = 1.0 * args.crop_size / image.shape[0]\n        kps[:, :2] *= ratio\n        dst_image = cv2.resize(image, (args.crop_size, args.crop_size), interpolation = cv2.INTER_CUBIC)\n\n        if self.use_flip and random.random() <= self.flip_prob:\n            dst_image, kps = flip_image(dst_image, kps)\n        \n        #normalize kp to [-1, 1]\n        ratio = 1.0 / args.crop_size\n        kps[:, :2] = 2.0 * kps[:, :2] * ratio - 1.0\n        return {\n            'image': torch.tensor(convert_image_by_pixformat_normalize(dst_image, self.pix_format, self.normalize)).float(),\n            'kp_2d': torch.tensor(kps).float(),\n            'image_name': self.images[index],\n            'data_set':'lsp'\n        }\n\nif __name__ == '__main__':\n    lsp = LspLoader(\n        data_set_path = 'E:/HMR/data/lsp', \n        use_crop = True, \n        scale_range = [1.05, 1.2], \n        use_flip = True,\n        flip_prob = 1.0\n    )\n    l = lsp.__len__()\n    data_loader = DataLoader(lsp, batch_size=10,shuffle=True)\n    for _ in range(l):\n        r = lsp.__getitem__(_)\n        image = r['image'].cpu().numpy().astype(np.uint8)\n        kps = r['kp_2d'].cpu().numpy()\n        kps[:, :2] = (kps[:, :2] + 1) * args.crop_size / 2.0\n        base_name = os.path.basename(r['image_name'])\n        draw_lsp_14kp__bone(image, kps)\n        cv2.imshow(base_name, cv2.resize(image, (512, 512), interpolation = cv2.INTER_CUBIC))\n        cv2.waitKey(0)\n    \n"""
src/dataloader/lsp_ext_dataloader.py,3,"b""\n'''\n    file:   lsp_ext_dataloader.py\n\n    author: zhangxiong(1025679612@qq.com)\n    date:   2018_05_07\n'''\nimport sys\nfrom torch.utils.data import Dataset, DataLoader\nimport scipy.io as scio  \nimport os \nimport glob\nimport numpy as np\nimport random\nimport cv2\nimport torch\n\nsys.path.append('./src')\n\nfrom util import calc_aabb, cut_image, flip_image, draw_lsp_14kp__bone, convert_image_by_pixformat_normalize\nfrom config import args\nfrom timer import Clock\n\n\nclass LspExtLoader(Dataset):\n    def __init__(self, data_set_path, use_crop, scale_range, use_flip, pix_format = 'NHWC', normalize = False, flip_prob = 0.3):\n        '''\n            marks:\n                data_set path links to the parent folder to lsp, which contains images, joints.mat, README.txt\n            \n            inputs:\n                use_crop crop the image or not, it should be True by default\n                scale_range, contain the scale range\n                use_flip, left right flip is allowed\n        '''\n        self.use_crop    = use_crop\n        self.scale_range = scale_range\n        self.use_flip    = use_flip\n        self.flip_prob   = flip_prob\n        self.data_folder = data_set_path\n        self.pix_format  = pix_format\n        self.normalize   = normalize\n\n        self._load_data_set()\n\n    def _load_data_set(self):\n        clk = Clock()\n\n        print('loading LSP ext data.')\n        self.images = []\n        self.kp2ds  = []\n        self.boxs   = []\n\n        anno_file_path = os.path.join(self.data_folder, 'joints.mat')\n        anno = scio.loadmat(anno_file_path)\n        kp2d = anno['joints'].transpose(2, 0, 1) # N x k x 3\n        image_folder = os.path.join(self.data_folder, 'images')\n        images = sorted(glob.glob(image_folder + '/im*.jpg'))\n        for _ in range(len(images)):\n            self._handle_image(images[_], kp2d[_])\n\n        print('finished load LSP ext data.')\n        clk.stop()\n        \n    def _handle_image(self, image_path, kps):\n        pt_valid = []\n        for pt in kps:\n            if pt[2] == 1:\n                pt_valid.append(pt)\n        lt, rb, valid = calc_aabb(pt_valid)\n\n        if not valid:\n            return\n\n        self.kp2ds.append(kps.copy().astype(np.float))\n        self.images.append(image_path)\n        self.boxs.append((lt, rb))\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        image_path = self.images[index]\n        kps = self.kp2ds[index].copy()\n        box = self.boxs[index]\n\n        scale = np.random.rand(4) * (self.scale_range[1] - self.scale_range[0]) + self.scale_range[0]\n        image, kps = cut_image(image_path, kps, scale, box[0], box[1])\n        ratio = 1.0 * args.crop_size / image.shape[0]\n        kps[:, :2] *= ratio\n        dst_image = cv2.resize(image, (args.crop_size, args.crop_size), interpolation = cv2.INTER_CUBIC)\n\n        if self.use_flip and random.random() <= self.flip_prob:\n            dst_image, kps = flip_image(dst_image, kps)\n            \n        #normalize kp to [-1, 1]\n        ratio = 1.0 / args.crop_size\n        kps[:, :2] = 2.0 * kps[:, :2] * ratio - 1.0\n        \n        return {\n            'image': torch.tensor(convert_image_by_pixformat_normalize(dst_image, self.pix_format, self.normalize)).float(),\n            'kp_2d': torch.tensor(kps).float(),\n            'image_name': self.images[index],\n            'data_set':'lsp_ext'\n        }\n\nif __name__ == '__main__':\n    lsp = LspExtLoader('E:/HMR/data/lsp_ext', True, [1.05, 1.5], False, flip_prob = 1.0)\n    l = lsp.__len__()\n\n    data_loader = DataLoader(lsp, batch_size=10,shuffle=True)\n    \n    for _ in range(l):\n        r = lsp.__getitem__(_)\n        image = r['image'].cpu().numpy().astype(np.uint8)\n        kps = r['kp_2d'].cpu().numpy()\n        base_name = os.path.basename(r['image_name'])\n        draw_lsp_14kp__bone(image, kps)\n        cv2.imshow(base_name, cv2.resize(image, (512, 512), interpolation = cv2.INTER_CUBIC))\n        cv2.waitKey(0)\n    \n"""
src/dataloader/mosh_dataloader.py,2,"b""\n'''\n    file:   mosh_dataloader.py\n\n    author: zhangxiong(1025679612@qq.com)\n    date:   2018_05_09\n    purpose:  load COCO 2017 keypoint dataset\n'''\n\nimport sys\nfrom torch.utils.data import Dataset, DataLoader\nimport scipy.io as scio  \nimport os \nimport glob\nimport numpy as np\nimport random\nimport cv2\nimport json\nimport h5py\nimport torch\n\nsys.path.append('./src')\nfrom util import calc_aabb, cut_image, flip_image, draw_lsp_14kp__bone, rectangle_intersect, get_rectangle_intersect_ratio, reflect_pose\nfrom config import args\nfrom timer import Clock\n\n\nclass mosh_dataloader(Dataset):\n    def __init__(self, data_set_path, use_flip = True, flip_prob = 0.3):\n        self.data_folder = data_set_path\n        self.use_flip = use_flip\n        self.flip_prob = flip_prob\n\n        self._load_data_set()\n\n    def _load_data_set(self):\n        clk = Clock()\n        print('start loading mosh data.')\n        anno_file_path = os.path.join(self.data_folder, 'mosh_annot.h5')\n        with h5py.File(anno_file_path) as fp:\n            self.shapes = np.array(fp['shape'])\n            self.poses = np.array(fp['pose'])\n        print('finished load mosh data, total {} samples'.format(len(self.poses)))\n        clk.stop()\n\n    def __len__(self):\n        return len(self.poses)\n\n    def __getitem__(self, index):\n        trival, pose, shape = np.zeros(3), self.poses[index], self.shapes[index]\n        \n        if self.use_flip and random.uniform(0, 1) <= self.flip_prob:#left-right reflect the pose\n            pose = reflect_pose(pose)\n\n        return {\n            'theta': torch.tensor(np.concatenate((trival, pose, shape), axis = 0)).float()\n        }\n\nif __name__ == '__main__':\n    print(random.rand(1))\n    mosh = mosh_dataloader('E:/HMR/data/mosh_gen')\n    l = len(mosh)\n    import time\n    for _ in range(l):\n        r = mosh.__getitem__(_)\n        print(r)"""
src/dataloader/mpi_inf_3dhp_dataloader.py,5,"b""\n'''\n    file:   mpi_inf_3dhp_dataloader.py\n\n    author: zhangxiong(1025679612@qq.com)\n    date:   2018_05_09\n    purpose:  load mpi inf 3dhp data\n'''\n\nimport sys\nfrom torch.utils.data import Dataset, DataLoader\nimport os \nimport glob\nimport numpy as np\nimport random\nimport cv2\nimport json\nimport h5py\nimport torch\n\nsys.path.append('./src')\nfrom util import calc_aabb, cut_image, flip_image, draw_lsp_14kp__bone, rectangle_intersect, get_rectangle_intersect_ratio, convert_image_by_pixformat_normalize, reflect_pose, reflect_lsp_kp\nfrom config import args\nfrom timer import Clock\n\nclass mpi_inf_3dhp_dataloader(Dataset):\n    def __init__(self, data_set_path, use_crop, scale_range, use_flip, min_pts_required, pix_format = 'NHWC', normalize = False, flip_prob = 0.3):\n        self.data_folder = data_set_path\n        self.use_crop = use_crop\n        self.scale_range = scale_range\n        self.use_flip = use_flip\n        self.flip_prob = 0.3\n        self.min_pts_required = min_pts_required\n        self.pix_format = pix_format\n        self.normalize = normalize\n        self._load_data_set()\n\n    def _load_data_set(self):\n        clk = Clock()\n\n        self.images = []\n        self.kp2ds  = []\n        self.boxs   = []\n        self.kp3ds  = []\n\n        print('start loading mpii-inf-3dhp data.')\n        anno_file_path = os.path.join(self.data_folder, 'annot.h5')\n        with h5py.File(anno_file_path) as fp:\n            total_kp2d = np.array(fp['gt2d'])\n            total_kp3d = np.array(fp['gt3d'])\n            total_image_names = np.array(fp['imagename'])\n\n            assert len(total_kp2d) == len(total_kp3d) and len(total_kp2d) == len(total_image_names)\n\n            l = len(total_kp2d)\n            def _collect_valid_pts(pts):\n                r = []\n                for pt in pts:\n                    if pt[2] != 0:\n                        r.append(pt)\n                return r\n\n            for index in range(l):\n                kp2d = total_kp2d[index].reshape((-1, 3))\n                if np.sum(kp2d[:, 2]) < self.min_pts_required:\n                    continue\n\n                lt, rb, v = calc_aabb(_collect_valid_pts(kp2d))\n                self.kp2ds.append(np.array(kp2d.copy(), dtype = np.float))\n                self.boxs.append((lt, rb))\n                self.kp3ds.append(total_kp3d[index].copy().reshape(-1, 3))\n                self.images.append(os.path.join(self.data_folder, 'image') + total_image_names[index].decode())\n                \n        print('finished load mpii-inf-3dhp data, total {} samples'.format(len(self.images)))\n        clk.stop()\n\n    def __len__(self):\n        return len(self.images)\n        \n    def __getitem__(self, index):\n        image_path = self.images[index]\n        kps = self.kp2ds[index].copy()\n        box = self.boxs[index]\n        kp_3d = self.kp3ds[index].copy()\n\n        scale = np.random.rand(4) * (self.scale_range[1] - self.scale_range[0]) + self.scale_range[0]\n        image, kps = cut_image(image_path, kps, scale, box[0], box[1])\n\n        ratio = 1.0 * args.crop_size / image.shape[0]\n        kps[:, :2] *= ratio\n        dst_image = cv2.resize(image, (args.crop_size, args.crop_size), interpolation = cv2.INTER_CUBIC)\n\n        if self.use_flip and random.random() <= self.flip_prob:\n            dst_image, kps = flip_image(dst_image, kps)\n            kp_3d = reflect_lsp_kp(kp_3d)\n\n        #normalize kp to [-1, 1]\n        ratio = 1.0 / args.crop_size\n        kps[:, :2] = 2.0 * kps[:, :2] * ratio - 1.0\n\n        return {\n            'image': torch.from_numpy(convert_image_by_pixformat_normalize(dst_image, self.pix_format, self.normalize)).float(),\n            'kp_2d': torch.from_numpy(kps).float(),\n            'kp_3d': torch.from_numpy(kp_3d).float(),\n            'theta': torch.zeros(85).float(),\n            'image_name': self.images[index],\n            'w_smpl':0.0,\n            'w_3d':1.0,\n            'data_set':'mpi inf 3dhp'\n        }\n\nif __name__ == '__main__':\n    mpi = mpi_inf_3dhp_dataloader('E:/HMR/data/mpii_inf_3dhp', True, [1.1, 2.0], False, 5)\n    l = len(mpi)\n    for _ in range(l):\n        r = mpi.__getitem__(_)\n        base_name = os.path.basename(r['image_name'])\n        draw_lsp_14kp__bone(r['image'], r['kp_2d'])\n        cv2.imshow(base_name, cv2.resize(r['image'], (512, 512), interpolation = cv2.INTER_CUBIC))\n        cv2.waitKey(0)\n    """
