file_path,api_count,code
launch.py,11,"b'# Adapted from https://github.com/pytorch/pytorch/blob/master/torch/distributed/launch.py\n\nr""""""\n`torch.distributed.launch` is a module that spawns up multiple distributed\ntraining processes on each of the training nodes.\n\nThe utility can be used for single-node distributed training, in which one or\nmore processes per node will be spawned. The utility can be used for either\nCPU training or GPU training. If the utility is used for GPU training,\neach distributed process will be operating on a single GPU. This can achieve\nwell-improved single-node training performance. It can also be used in\nmulti-node distributed training, by spawning up multiple processes on each node\nfor well-improved multi-node distributed training performance as well.\nThis will especially be benefitial for systems with multiple Infiniband\ninterfaces that have direct-GPU support, since all of them can be utilized for\naggregated communication bandwidth.\n\nIn both cases of single-node distributed training or multi-node distributed\ntraining, this utility will launch the given number of processes per node\n(``--nproc_per_node``). If used for GPU training, this number needs to be less\nor euqal to the number of GPUs on the current system (``nproc_per_node``),\nand each process will be operating on a single GPU from *GPU 0 to\nGPU (nproc_per_node - 1)*.\n\n**How to use this module:**\n\n1. Single-Node multi-process distributed training\n\n::\n\n    >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n               YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other\n               arguments of your training script)\n\n2. Multi-Node multi-process distributed training: (e.g. two nodes)\n\n\nNode 1: *(IP: 192.168.1.1, and has a free port: 1234)*\n\n::\n\n    >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n               --nnodes=2 --node_rank=0 --master_addr=""192.168.1.1""\n               --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n               and all other arguments of your training script)\n\nNode 2:\n\n::\n\n    >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n               --nnodes=2 --node_rank=1 --master_addr=""192.168.1.1""\n               --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n               and all other arguments of your training script)\n\n3. To look up what optional arguments this module offers:\n\n::\n\n    >>> python -m torch.distributed.launch --help\n\n\n**Important Notices:**\n\n1. This utilty and multi-process distributed (single-node or\nmulti-node) GPU training currently only achieves the best performance using\nthe NCCL distributed backend. Thus NCCL backend is the recommended backend to\nuse for GPU training.\n\n2. In your training program, you must parse the command-line argument:\n``--local_rank=LOCAL_PROCESS_RANK``, which will be provided by this module.\nIf your training program uses GPUs, you should ensure that your code only\nruns on the GPU device of LOCAL_PROCESS_RANK. This can be done by:\n\nParsing the local_rank argument\n\n::\n\n    >>> import argparse\n    >>> parser = argparse.ArgumentParser()\n    >>> parser.add_argument(""--local_rank"", type=int)\n    >>> args = parser.parse_args()\n\nSet your device to local rank using either\n\n::\n\n    >>> torch.cuda.set_device(arg.local_rank)  # before your code runs\n\n    or\n\n    >>> with torch.cuda.device(arg.local_rank):\n    >>>    # your code to run\n\n3. In your training program, you are supposed to call the following function\nat the beginning to start the distributed backend. You need to make sure that\nthe init_method uses ``env://``, which is the only supported ``init_method``\nby this module.\n\n::\n\n    torch.distributed.init_process_group(backend=\'YOUR BACKEND\',\n                                         init_method=\'env://\')\n\n4. In your training program, you can either use regular distributed functions\nor use :func:`torch.nn.parallel.DistributedDataParallel` module. If your\ntraining program uses GPUs for training and you would like to use\n:func:`torch.nn.parallel.DistributedDataParallel` module,\nhere is how to configure it.\n\n::\n\n    model = torch.nn.parallel.DistributedDataParallel(model,\n                                                      device_ids=[arg.local_rank],\n                                                      output_device=arg.local_rank)\n\nPlease ensure that ``device_ids`` argument is set to be the only GPU device id\nthat your code will be operating on. This is generally the local rank of the\nprocess. In other words, the ``device_ids`` needs to be ``[args.local_rank]``,\nand ``output_device`` needs to be ``args.local_rank`` in order to use this\nutility\n\n""""""\n\n\nimport subprocess\nimport os\nimport socket\nfrom argparse import ArgumentParser, REMAINDER\n\nimport torch\n\n\ndef parse_args():\n    """"""\n    Helper function parsing the command line options\n    @retval ArgumentParser\n    """"""\n    parser = ArgumentParser(description=""PyTorch distributed training launch ""\n                                        ""helper utilty that will spawn up ""\n                                        ""multiple distributed processes"")\n\n    # Optional arguments for the launch helper\n    parser.add_argument(""--nnodes"", type=int, default=1,\n                        help=""The number of nodes to use for distributed ""\n                             ""training"")\n    parser.add_argument(""--node_rank"", type=int, default=0,\n                        help=""The rank of the node for multi-node distributed ""\n                             ""training"")\n    parser.add_argument(""--nproc_per_node"", type=int, default=1,\n                        help=""The number of processes to launch on each node, ""\n                             ""for GPU training, this is recommended to be set ""\n                             ""to the number of GPUs in your system so that ""\n                             ""each process can be bound to a single GPU."")\n    parser.add_argument(""--master_addr"", default=""127.0.0.1"", type=str,\n                        help=""Master node (rank 0)\'s address, should be either ""\n                             ""the IP address or the hostname of node 0, for ""\n                             ""single node multi-proc training, the ""\n                             ""--master_addr can simply be 127.0.0.1"")\n    parser.add_argument(""--master_port"", default=29500, type=int,\n                        help=""Master node (rank 0)\'s free port that needs to ""\n                             ""be used for communciation during distributed ""\n                             ""training"")\n\n    # positional\n    parser.add_argument(""training_script"", type=str,\n                        help=""The full path to the single GPU training ""\n                             ""program/script to be launched in parallel, ""\n                             ""followed by all the arguments for the ""\n                             ""training script"")\n\n    # rest from the training program\n    parser.add_argument(\'training_script_args\', nargs=REMAINDER)\n    return parser.parse_args()\n\n\ndef main():\n\n    args = parse_args()\n\n    # world size in terms of number of processes\n    dist_world_size = args.nproc_per_node * args.nnodes\n\n    # set PyTorch distributed related environmental variables\n    current_env = os.environ.copy()\n    current_env[""MASTER_ADDR""] = args.master_addr\n    current_env[""MASTER_PORT""] = str(args.master_port)\n    current_env[""WORLD_SIZE""] = str(dist_world_size)\n\n    if args.nproc_per_node > 1:\n\n        processes = []\n\n        for local_rank in range(0, args.nproc_per_node):\n            # each process\'s rank\n            dist_rank = args.nproc_per_node * args.node_rank + local_rank\n            current_env[""RANK""] = str(dist_rank)\n\n            # spawn the processes\n            cmd = [""python"",\n                   ""-u"",\n                   args.training_script,\n                   ""--local_rank={}"".format(local_rank)] + \\\n                args.training_script_args\n\n            process = subprocess.Popen(cmd, env=current_env)\n            processes.append(process)\n\n        try:\n            for process in processes:\n                process.wait()\n        except KeyboardInterrupt:\n            for process in processes:\n                process.terminate()\n\n    elif args.nproc_per_node == 1:\n        cmd = [""python"",\n               ""-u"",\n               args.training_script,\n               ""--local_rank={}"".format(-1)] + args.training_script_args\n        subprocess.Popen(cmd, env=current_env)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
logp_gcnn_config.py,4,"b""from openchem.models.Graph2Label import Graph2Label\nfrom openchem.modules.encoders.gcn_encoder import GraphCNNEncoder\nfrom openchem.modules.mlp.openchem_mlp import OpenChemMLP\nfrom openchem.data.graph_data_layer import GraphDataset\n\nfrom openchem.utils.graph import Attribute\nfrom openchem.utils.utils import identity\n\nimport torch.nn as nn\nfrom torch.optim import RMSprop, SGD, Adam\nfrom torch.optim.lr_scheduler import ExponentialLR, StepLR\nimport torch.nn.functional as F\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nimport pandas as pd\n\nimport copy\nimport pickle\n\n\ndef get_atomic_attributes(atom):\n    attr_dict = {}\n  \n    atomic_num = atom.GetAtomicNum()\n    atomic_mapping = {5: 0, 7: 1, 6: 2, 8: 3, 9: 4, 15: 5, 16: 6, 17: 7, 35: 8,\n                      53: 9}\n    if atomic_num in atomic_mapping.keys():\n        attr_dict['atom_element'] = atomic_mapping[atomic_num]\n    else:\n        attr_dict['atom_element'] = 10\n    attr_dict['valence'] = atom.GetTotalValence()\n    attr_dict['charge'] = atom.GetFormalCharge()\n    attr_dict['hybridization'] = atom.GetHybridization().real\n    attr_dict['aromatic'] = int(atom.GetIsAromatic())\n    return attr_dict\n\n\nnode_attributes = {}\nnode_attributes['valence'] = Attribute('node', 'valence', one_hot=True, values=[1, 2, 3, 4, 5, 6])\nnode_attributes['charge'] = Attribute('node', 'charge', one_hot=True, values=[-1, 0, 1, 2, 3, 4])\nnode_attributes['hybridization'] = Attribute('node', 'hybridization',\n                                             one_hot=True, values=[0, 1, 2, 3, 4, 5, 6, 7])\nnode_attributes['aromatic'] = Attribute('node', 'aromatic', one_hot=True,\n                                        values=[0, 1])\nnode_attributes['atom_element'] = Attribute('node', 'atom_element',\n                                            one_hot=True,\n                                            values=list(range(11)))\n\ntrain_dataset = GraphDataset(get_atomic_attributes, node_attributes,\n                             './benchmark_datasets/Lipophilicity_dataset/Lipophilicity_train.csv',\n                             delimiter=',', cols_to_read=[0, 1])\ntest_dataset = GraphDataset(get_atomic_attributes, node_attributes,\n                             './benchmark_datasets/Lipophilicity_dataset/Lipophilicity_test.csv',\n                             delimiter=',', cols_to_read=[0, 1])\n\nmodel = Graph2Label\n\nmodel_params = {\n    'task': 'regression',\n    'data_layer': GraphDataset,\n    'use_clip_grad': False,\n    'batch_size': 256,\n    'num_epochs': 101,\n    'logdir': '/home/user/Work/OpenChem/logs/logp_gcnn_logs',\n    'print_every': 10,\n    'save_every': 5,\n    'train_data_layer': train_dataset,\n    'val_data_layer': test_dataset,\n    'eval_metrics': r2_score,\n    'criterion': nn.MSELoss(),\n    'optimizer': Adam,\n    'optimizer_params': {\n        'lr': 0.0005,\n    },\n    'lr_scheduler': StepLR,\n    'lr_scheduler_params': {\n        'step_size': 15,\n        'gamma': 0.8\n    },\n    'encoder': GraphCNNEncoder,\n    'encoder_params': {\n        'input_size': train_dataset.num_features,\n        'encoder_dim': 128,\n        'n_layers': 5,\n        'hidden_size': [128, 128, 128, 128, 128],\n    },\n    'mlp': OpenChemMLP,\n    'mlp_params': {\n        'input_size': 128,\n        'n_layers': 2,\n        'hidden_size': [128, 1],\n        'activation': [F.relu, identity]\n    }\n}\n\n"""
run.py,10,"b'# adapted from https://github.com/NVIDIA/OpenSeq2Seq/blob/master/run.py\n\nimport os\nimport ast\nimport copy\nimport runpy\nimport random\nimport argparse\n\nfrom six import string_types\n\nimport torch\nimport torch.distributed as dist\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel, DataParallel\n\nfrom openchem.models.openchem_model import build_training, fit, evaluate\n\nfrom openchem.data.utils import create_loader\nfrom openchem.utils.utils import get_latest_checkpoint, deco_print\nfrom openchem.utils.utils import flatten_dict, nested_update, nest_dict\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Experiment parameters\')\n    parser.add_argument(""--use_cuda"", default=torch.cuda.is_available(),\n                        help=""Whether to train on GPU"")\n    parser.add_argument(""--config_file"", required=True,\n                        help=""Path to the configuration file"")\n    parser.add_argument(""--mode"", default=\'train\',\n                        help=""Could be \\""train\\"", \\""eval\\"", \\""train_eval\\"""")\n    parser.add_argument(\'--continue_learning\', dest=\'continue_learning\',\n                        action=\'store_true\',\n                        help=""whether to continue learning"")\n    parser.add_argument(\'--dist-backend\', default=\'nccl\', type=str,\n                        help=\'distributed backend\')\n    parser.add_argument(\'--seed\', default=None, type=int,\n                        help=\'seed for initializing training. \')\n    parser.add_argument(\'--workers\', default=0, type=int, metavar=\'N\',\n                        help=\'number of data loading workers (default: 0)\')\n    parser.add_argument(\'--random_seed\', default=0, type=int, metavar=\'N\',\n                        help=\'random_seed (default: 0)\')\n    parser.add_argument(""--local_rank"", type=int)\n\n    args, unknown = parser.parse_known_args()\n\n    if args.mode not in [\'train\', \'eval\', \'train_eval\']:\n        raise ValueError(""Mode has to be one of ""\n                         ""[\'train\', \'eval\', \'train_eval\']"")\n    config_module = runpy.run_path(args.config_file)\n\n    model_config = config_module.get(\'model_params\', None)\n    random.seed(args.random_seed)\n    torch.manual_seed(args.random_seed)\n    torch.cuda.manual_seed_all(args.random_seed)\n    model_config[\'use_cuda\'] = args.use_cuda\n    if model_config is None:\n        raise ValueError(\'model_params dictionary has to be \'\n                         \'defined in the config file\')\n    model_object = config_module.get(\'model\', None)\n    if model_object is None:\n        raise ValueError(\'model class has to be defined in the config file\')\n\n        # after we read the config, trying to overwrite some of the properties\n        # with command line arguments that were passed to the script\n    parser_unk = argparse.ArgumentParser()\n    for pm, value in flatten_dict(model_config).items():\n        if type(value) == int or type(value) == float or \\\n                isinstance(value, string_types):\n            parser_unk.add_argument(\'--\' + pm, default=value, type=type(value))\n        elif type(value) == bool:\n            parser_unk.add_argument(\'--\' + pm, default=value,\n                                    type=ast.literal_eval)\n\n    config_update = parser_unk.parse_args(unknown)\n    nested_update(model_config, nest_dict(vars(config_update)))\n\n    # checking that everything is correct with log directory\n    logdir = model_config[\'logdir\']\n    ckpt_dir = logdir + \'/checkpoint/\'\n\n    if args.local_rank == 0 or args.local_rank == -1:\n        try:\n            try:\n                os.stat(logdir)\n            except:\n                os.mkdir(logdir)\n                print(\'Directory created\')\n            # check if folder checkpoint within log directory exists,\n            # create if it doesn\'t\n            try:\n                os.stat(ckpt_dir)\n            except:\n                os.mkdir(logdir + \'/checkpoint\')\n                print(""Directory created"")\n                ckpt_dir = logdir + \'/checkpoint/\'\n            if args.mode == \'train\' or args.mode == \'train_eval\':\n                if os.path.isfile(logdir):\n                    raise IOError(\n                        ""There is a file with the same name as \\""logdir\\"" ""\n                        ""parameter. You should change the log directory path ""\n                        ""or delete the file to continue."")\n\n                # check if \'logdir\' directory exists and non-empty\n                if os.path.isdir(ckpt_dir) and os.listdir(ckpt_dir) != []:\n                    if not args.continue_learning:\n                        raise IOError(\n                            ""Log directory is not empty. If you want to ""\n                            ""continue learning, you should provide ""\n                            ""\\""--continue_learning\\"" flag"")\n                    checkpoint = get_latest_checkpoint(ckpt_dir)\n                    if checkpoint is None:\n                        raise IOError(\n                            ""There is no model checkpoint in the ""\n                            ""{} directory. Can\'t load model"".format(ckpt_dir)\n                        )\n                else:\n                    if args.continue_learning:\n                        raise IOError(\n                            ""The log directory is empty or does not exist. ""\n                            ""You should probably not provide ""\n                            ""\\""--continue_learning\\"" flag?"")\n                    checkpoint = None\n            elif args.mode == \'eval\' or args.mode == \'infer\':\n                if os.path.isdir(logdir) and os.listdir(logdir) != []:\n                    checkpoint = get_latest_checkpoint(ckpt_dir)\n                    if checkpoint is None:\n                        raise IOError(\n                            ""There is no model checkpoint in the ""\n                            ""{} directory. Can\'t load model"".format(\n                                ckpt_dir\n                            )\n                        )\n                else:\n                    raise IOError(\n                        ""{} does not exist or is empty, can\'t restore"".format(\n                            ckpt_dir\n                        )\n                    )\n        except IOError:\n            raise\n    else:\n        if args.continue_learning or args.mode == \'eval\':\n            checkpoint = get_latest_checkpoint(ckpt_dir)\n        else:\n            checkpoint = None\n\n    train_config = copy.deepcopy(model_config)\n    eval_config = copy.deepcopy(model_config)\n\n    args.distributed = args.local_rank >= 0\n\n    if args.mode == \'train\' or args.mode == \'train_eval\':\n        if \'train_params\' in config_module:\n            nested_update(train_config,\n                          copy.deepcopy(config_module[\'train_params\']))\n    if args.mode == \'eval\' or args.mode == \'train_eval\' or args.mode == \'infer\':\n        if \'eval_params\' in config_module:\n            nested_update(eval_config,\n                          copy.deepcopy(config_module[\'eval_params\']))\n\n    if args.mode == \'train\' or args.mode == \'train_eval\':\n        if not args.continue_learning:\n            if args.distributed:\n                deco_print(""Starting training from scratch process "" +\n                           str(args.local_rank))\n            else:\n                deco_print(""Starting training from scratch"")\n        else:\n            deco_print(\n                ""Restored checkpoint from {}. Resuming training"".format(\n                    checkpoint),\n            )\n    elif args.mode == \'eval\' or args.mode == \'infer\':\n        deco_print(""Loading model from {}"".format(checkpoint))\n\n    if args.distributed:\n        torch.cuda.set_device(args.local_rank)\n        dist.init_process_group(backend=args.dist_backend,\n                                init_method=\'env://\')\n        print(\'Distributed process with rank \' + str(args.local_rank) +\n              \' initiated\')\n\n        args.world_size = torch.distributed.get_world_size()\n        model_config[\'world_size\'] = args.world_size\n    else:\n        model_config[\'world_size\'] = 1\n\n    cudnn.benchmark = True\n\n    if args.mode == ""train"" or args.mode == ""train_eval"":\n        train_dataset = copy.deepcopy(model_config[\'train_data_layer\'])\n        if model_config[\'task\'] == \'classification\':\n            train_dataset.target = train_dataset.target.reshape(-1)\n        if args.distributed:\n            train_sampler = DistributedSampler(train_dataset)\n        else:\n            train_sampler = None\n        train_loader = create_loader(train_dataset,\n                                     batch_size=model_config[\'batch_size\'],\n                                     shuffle=(train_sampler is None),\n                                     num_workers=args.workers,\n                                     pin_memory=True,\n                                     sampler=train_sampler)\n    else:\n        train_loader = None\n\n    if args.mode in [""eval"", ""train_eval""] and (\n            \'val_data_layer\' not in model_config.keys() or model_config[\n        \'val_data_layer\'] is None):\n        raise IOError(\n            ""When model is run in \'eval\' or \'train_eval\' modes, ""\n            ""validation data layer must be specified"")\n\n    if args.mode in [""eval"", ""train_eval""]:\n        val_dataset = copy.deepcopy(model_config[\'val_data_layer\'])\n        if model_config[\'task\'] == \'classification\':\n            val_dataset.target = val_dataset.target.reshape(-1)\n        val_loader = create_loader(val_dataset,\n                                   batch_size=model_config[\'batch_size\'],\n                                   shuffle=False,\n                                   num_workers=1,\n                                   pin_memory=True)\n    else:\n        val_loader = None\n\n    model_config[\'train_loader\'] = train_loader\n    model_config[\'val_loader\'] = val_loader\n\n    # create model\n    model = model_object(params=model_config)\n\n    model = model.cuda()\n\n    if args.distributed:\n        model = DistributedDataParallel(model, device_ids=[args.local_rank],\n                                        output_device=args.local_rank\n                                        )\n    else:\n        model = DataParallel(model)\n    if args.continue_learning or args.mode == \'eval\':\n        print(""=> loading model  pre-trained model"")\n        weights = torch.load(checkpoint)\n        model.load_state_dict(weights)\n\n    criterion, optimizer, lr_scheduler = build_training(model, model_config)\n\n    if args.mode == \'train\':\n        fit(model, lr_scheduler, train_loader, optimizer, criterion,\n            model_config, eval=False)\n    elif args.mode == \'train_eval\':\n        fit(model, lr_scheduler, train_loader, optimizer, criterion,\n            model_config, eval=True, val_loader=val_loader)\n    elif args.mode == ""eval"":\n        evaluate(model, val_loader, criterion)\n\n\nif __name__ == \'__main__\':\n    main()\n\n'"
setup.py,0,"b'""""""\nOpenChem -- Deep Learning toolkit for Cheminformatics\n""""""\nfrom setuptools import setup\nimport versioneer\n\nshort_description = __doc__.split(""\\n"")\n\ntry:\n    with open(""README.md"", ""r"") as handle:\n        long_description = handle.read()\nexcept:\n    long_description = ""\\n"".join(short_description[2:]),\n\n\nsetup(\n    # Self-descriptive entries which should always be present\n    name=\'openchem\',\n    author=\'mariewelt\',\n    author_email=\'mariewelt@gmail.com\',\n    description=short_description[0],\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    version=versioneer.get_version(),\n    cmdclass=versioneer.get_cmdclass(),\n    license=\'BSD-3-Clause\',\n\n    # Which Python importable modules should be included when your package is installed\n    packages=[\'openchem\', ""openchem.tests""],\n\n    # Optional include package data to ship with your package\n    # Comment out this line to prevent the files from being packaged with your software\n    # Extend/modify the list to include/exclude other items as need be\n    package_data={\'openchem\': [""data/*.dat""]\n                  },\n\n    # Additional entries you may want simply uncomment the lines you want and fill in the data\n    # author_email=\'me@place.org\',      # Author email\n    # url=\'http://www.my_package.com\',  # Website\n    # install_requires=[],              # Required packages, pulls from pip if needed; do not use for Conda deployment\n    # platforms=[\'Linux\',\n    #            \'Mac OS-X\',\n    #            \'Unix\',\n    #            \'Windows\'],            # Valid platforms your code works on, adjust to your flavor\n    # python_requires="">=3.5"",          # Python version restrictions\n\n    # Manual control if final package is compressible or not, set False to prevent the .egg from being made\n    # zip_safe=False,\n\n)\n'"
tox21_rnn_config.py,6,"b'from openchem.models.Smiles2Label import Smiles2Label\nfrom openchem.modules.embeddings.basic_embedding import Embedding\nfrom openchem.modules.encoders.rnn_encoder import RNNEncoder\nfrom openchem.modules.mlp.openchem_mlp import OpenChemMLP\nfrom openchem.data.smiles_data_layer import SmilesDataset\nfrom openchem.criterion.multitask_loss import MultitaskLoss\n\nimport torch\nimport torch.nn as nn\n\nimport numpy as np\n\nfrom torch.optim import RMSprop, Adam\nfrom torch.optim.lr_scheduler import ExponentialLR, StepLR\nimport torch.nn.functional as F\nfrom sklearn.metrics import roc_auc_score, mean_squared_error\n\nfrom openchem.data.utils import read_smiles_property_file\ndata = read_smiles_property_file(\'./benchmark_datasets/tox21/tox21.csv\',\n                                 cols_to_read=[13] + list(range(0,12)))\nsmiles = data[0]\nlabels = np.array(data[1:])\n\nlabels[np.where(labels==\'\')] = \'999\'\nlabels = labels.T\n\nfrom openchem.data.utils import get_tokens\ntokens, _, _ = get_tokens(smiles)\ntokens = tokens + \' \'\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(smiles, labels, test_size=0.2,\n                                                    random_state=42)\n\nfrom openchem.data.utils import save_smiles_property_file\nsave_smiles_property_file(\'./benchmark_datasets/tox21/train.smi\', X_train, y_train)\nsave_smiles_property_file(\'./benchmark_datasets/tox21/test.smi\', X_test, y_test)\n\nfrom openchem.data.smiles_data_layer import SmilesDataset\ntrain_dataset = SmilesDataset(\'./benchmark_datasets/tox21/train.smi\',\n                              delimiter=\',\', cols_to_read=list(range(13)),\n                              tokens=tokens, augment=True)\ntest_dataset = SmilesDataset(\'./benchmark_datasets/tox21/test.smi\',\n                            delimiter=\',\', cols_to_read=list(range(13)),\n                            tokens=tokens)\n\ndef multitask_auc(ground_truth, predicted):\n    from sklearn.metrics import roc_auc_score\n    import numpy as np\n    import torch\n    ground_truth = np.array(ground_truth)\n    predicted = np.array(predicted)\n    n_tasks = ground_truth.shape[1]\n    auc = []\n    for i in range(n_tasks):\n        ind = np.where(ground_truth[:, i] != 999)[0]\n        auc.append(roc_auc_score(ground_truth[ind, i], predicted[ind, i]))\n    #if torch.distributed.get_rank() == 0:\n    #    print(auc)\n    return np.mean(auc)\n\nmodel = Smiles2Label\n\nmodel_params = {\n    \'use_cuda\': True,\n    \'task\': \'multitask\',\n    \'random_seed\': 5,\n    \'use_clip_grad\': True,\n    \'max_grad_norm\': 10.0,\n    \'batch_size\': 256,\n    \'num_epochs\': 31,\n    \'logdir\': \'./logs/tox21_rnn_log\',\n    \'print_every\': 5,\n    \'save_every\': 5,\n    \'train_data_layer\': train_dataset,\n    \'val_data_layer\': test_dataset,\n    \'eval_metrics\': multitask_auc,\n    \'criterion\': MultitaskLoss(ignore_index=999, n_tasks=12).cuda(),\n    \'optimizer\': RMSprop,\n    \'optimizer_params\': {\n        \'lr\': 0.001,\n        },\n    \'lr_scheduler\': StepLR,\n    \'lr_scheduler_params\': {\n        \'step_size\': 10,\n        \'gamma\': 0.8\n    },\n    \'embedding\': Embedding,\n    \'embedding_params\': {\n        \'num_embeddings\': train_dataset.num_tokens,\n        \'embedding_dim\': 128,\n        \'padding_idx\': train_dataset.tokens.index(\' \')\n    },\n    \'encoder\': RNNEncoder,\n    \'encoder_params\': {\n        \'input_size\': 128,\n        \'layer\': ""LSTM"",\n        \'encoder_dim\': 128,\n        \'n_layers\': 4,\n        \'dropout\': 0.8,\n        \'is_bidirectional\': False\n    },\n    \'mlp\': OpenChemMLP,\n    \'mlp_params\': {\n        \'input_size\': 128,\n        \'n_layers\': 2,\n        \'hidden_size\': [128, 12],\n        \'activation\': [F.relu, torch.sigmoid],\n        \'dropout\': 0.0\n    }\n}\n'"
versioneer.py,0,"b'\n# Version: 0.18\n\n""""""The Versioneer - like a rocketeer, but for versions.\n\nThe Versioneer\n==============\n\n* like a rocketeer, but for versions!\n* https://github.com/warner/python-versioneer\n* Brian Warner\n* License: Public Domain\n* Compatible With: python2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, and pypy\n* [![Latest Version]\n(https://pypip.in/version/versioneer/badge.svg?style=flat)\n](https://pypi.python.org/pypi/versioneer/)\n* [![Build Status]\n(https://travis-ci.org/warner/python-versioneer.png?branch=master)\n](https://travis-ci.org/warner/python-versioneer)\n\nThis is a tool for managing a recorded version number in distutils-based\npython projects. The goal is to remove the tedious and error-prone ""update\nthe embedded version string"" step from your release process. Making a new\nrelease should be as easy as recording a new tag in your version-control\nsystem, and maybe making new tarballs.\n\n\n## Quick Install\n\n* `pip install versioneer` to somewhere to your $PATH\n* add a `[versioneer]` section to your setup.cfg (see below)\n* run `versioneer install` in your source tree, commit the results\n\n## Version Identifiers\n\nSource trees come from a variety of places:\n\n* a version-control system checkout (mostly used by developers)\n* a nightly tarball, produced by build automation\n* a snapshot tarball, produced by a web-based VCS browser, like github\'s\n  ""tarball from tag"" feature\n* a release tarball, produced by ""setup.py sdist"", distributed through PyPI\n\nWithin each source tree, the version identifier (either a string or a number,\nthis tool is format-agnostic) can come from a variety of places:\n\n* ask the VCS tool itself, e.g. ""git describe"" (for checkouts), which knows\n  about recent ""tags"" and an absolute revision-id\n* the name of the directory into which the tarball was unpacked\n* an expanded VCS keyword ($Id$, etc)\n* a `_version.py` created by some earlier build step\n\nFor released software, the version identifier is closely related to a VCS\ntag. Some projects use tag names that include more than just the version\nstring (e.g. ""myproject-1.2"" instead of just ""1.2""), in which case the tool\nneeds to strip the tag prefix to extract the version identifier. For\nunreleased software (between tags), the version identifier should provide\nenough information to help developers recreate the same tree, while also\ngiving them an idea of roughly how old the tree is (after version 1.2, before\nversion 1.3). Many VCS systems can report a description that captures this,\nfor example `git describe --tags --dirty --always` reports things like\n""0.7-1-g574ab98-dirty"" to indicate that the checkout is one revision past the\n0.7 tag, has a unique revision id of ""574ab98"", and is ""dirty"" (it has\nuncommitted changes.\n\nThe version identifier is used for multiple purposes:\n\n* to allow the module to self-identify its version: `myproject.__version__`\n* to choose a name and prefix for a \'setup.py sdist\' tarball\n\n## Theory of Operation\n\nVersioneer works by adding a special `_version.py` file into your source\ntree, where your `__init__.py` can import it. This `_version.py` knows how to\ndynamically ask the VCS tool for version information at import time.\n\n`_version.py` also contains `$Revision$` markers, and the installation\nprocess marks `_version.py` to have this marker rewritten with a tag name\nduring the `git archive` command. As a result, generated tarballs will\ncontain enough information to get the proper version.\n\nTo allow `setup.py` to compute a version too, a `versioneer.py` is added to\nthe top level of your source tree, next to `setup.py` and the `setup.cfg`\nthat configures it. This overrides several distutils/setuptools commands to\ncompute the version when invoked, and changes `setup.py build` and `setup.py\nsdist` to replace `_version.py` with a small static file that contains just\nthe generated version data.\n\n## Installation\n\nSee [INSTALL.md](./INSTALL.md) for detailed installation instructions.\n\n## Version-String Flavors\n\nCode which uses Versioneer can learn about its version string at runtime by\nimporting `_version` from your main `__init__.py` file and running the\n`get_versions()` function. From the ""outside"" (e.g. in `setup.py`), you can\nimport the top-level `versioneer.py` and run `get_versions()`.\n\nBoth functions return a dictionary with different flavors of version\ninformation:\n\n* `[\'version\']`: A condensed version string, rendered using the selected\n  style. This is the most commonly used value for the project\'s version\n  string. The default ""pep440"" style yields strings like `0.11`,\n  `0.11+2.g1076c97`, or `0.11+2.g1076c97.dirty`. See the ""Styles"" section\n  below for alternative styles.\n\n* `[\'full-revisionid\']`: detailed revision identifier. For Git, this is the\n  full SHA1 commit id, e.g. ""1076c978a8d3cfc70f408fe5974aa6c092c949ac"".\n\n* `[\'date\']`: Date and time of the latest `HEAD` commit. For Git, it is the\n  commit date in ISO 8601 format. This will be None if the date is not\n  available.\n\n* `[\'dirty\']`: a boolean, True if the tree has uncommitted changes. Note that\n  this is only accurate if run in a VCS checkout, otherwise it is likely to\n  be False or None\n\n* `[\'error\']`: if the version string could not be computed, this will be set\n  to a string describing the problem, otherwise it will be None. It may be\n  useful to throw an exception in setup.py if this is set, to avoid e.g.\n  creating tarballs with a version string of ""unknown"".\n\nSome variants are more useful than others. Including `full-revisionid` in a\nbug report should allow developers to reconstruct the exact code being tested\n(or indicate the presence of local changes that should be shared with the\ndevelopers). `version` is suitable for display in an ""about"" box or a CLI\n`--version` output: it can be easily compared against release notes and lists\nof bugs fixed in various releases.\n\nThe installer adds the following text to your `__init__.py` to place a basic\nversion in `YOURPROJECT.__version__`:\n\n    from ._version import get_versions\n    __version__ = get_versions()[\'version\']\n    del get_versions\n\n## Styles\n\nThe setup.cfg `style=` configuration controls how the VCS information is\nrendered into a version string.\n\nThe default style, ""pep440"", produces a PEP440-compliant string, equal to the\nun-prefixed tag name for actual releases, and containing an additional ""local\nversion"" section with more detail for in-between builds. For Git, this is\nTAG[+DISTANCE.gHEX[.dirty]] , using information from `git describe --tags\n--dirty --always`. For example ""0.11+2.g1076c97.dirty"" indicates that the\ntree is like the ""1076c97"" commit but has uncommitted changes ("".dirty""), and\nthat this commit is two revisions (""+2"") beyond the ""0.11"" tag. For released\nsoftware (exactly equal to a known tag), the identifier will only contain the\nstripped tag, e.g. ""0.11"".\n\nOther styles are available. See [details.md](details.md) in the Versioneer\nsource tree for descriptions.\n\n## Debugging\n\nVersioneer tries to avoid fatal errors: if something goes wrong, it will tend\nto return a version of ""0+unknown"". To investigate the problem, run `setup.py\nversion`, which will run the version-lookup code in a verbose mode, and will\ndisplay the full contents of `get_versions()` (including the `error` string,\nwhich may help identify what went wrong).\n\n## Known Limitations\n\nSome situations are known to cause problems for Versioneer. This details the\nmost significant ones. More can be found on Github\n[issues page](https://github.com/warner/python-versioneer/issues).\n\n### Subprojects\n\nVersioneer has limited support for source trees in which `setup.py` is not in\nthe root directory (e.g. `setup.py` and `.git/` are *not* siblings). The are\ntwo common reasons why `setup.py` might not be in the root:\n\n* Source trees which contain multiple subprojects, such as\n  [Buildbot](https://github.com/buildbot/buildbot), which contains both\n  ""master"" and ""slave"" subprojects, each with their own `setup.py`,\n  `setup.cfg`, and `tox.ini`. Projects like these produce multiple PyPI\n  distributions (and upload multiple independently-installable tarballs).\n* Source trees whose main purpose is to contain a C library, but which also\n  provide bindings to Python (and perhaps other langauges) in subdirectories.\n\nVersioneer will look for `.git` in parent directories, and most operations\nshould get the right version string. However `pip` and `setuptools` have bugs\nand implementation details which frequently cause `pip install .` from a\nsubproject directory to fail to find a correct version string (so it usually\ndefaults to `0+unknown`).\n\n`pip install --editable .` should work correctly. `setup.py install` might\nwork too.\n\nPip-8.1.1 is known to have this problem, but hopefully it will get fixed in\nsome later version.\n\n[Bug #38](https://github.com/warner/python-versioneer/issues/38) is tracking\nthis issue. The discussion in\n[PR #61](https://github.com/warner/python-versioneer/pull/61) describes the\nissue from the Versioneer side in more detail.\n[pip PR#3176](https://github.com/pypa/pip/pull/3176) and\n[pip PR#3615](https://github.com/pypa/pip/pull/3615) contain work to improve\npip to let Versioneer work correctly.\n\nVersioneer-0.16 and earlier only looked for a `.git` directory next to the\n`setup.cfg`, so subprojects were completely unsupported with those releases.\n\n### Editable installs with setuptools <= 18.5\n\n`setup.py develop` and `pip install --editable .` allow you to install a\nproject into a virtualenv once, then continue editing the source code (and\ntest) without re-installing after every change.\n\n""Entry-point scripts"" (`setup(entry_points={""console_scripts"": ..})`) are a\nconvenient way to specify executable scripts that should be installed along\nwith the python package.\n\nThese both work as expected when using modern setuptools. When using\nsetuptools-18.5 or earlier, however, certain operations will cause\n`pkg_resources.DistributionNotFound` errors when running the entrypoint\nscript, which must be resolved by re-installing the package. This happens\nwhen the install happens with one version, then the egg_info data is\nregenerated while a different version is checked out. Many setup.py commands\ncause egg_info to be rebuilt (including `sdist`, `wheel`, and installing into\na different virtualenv), so this can be surprising.\n\n[Bug #83](https://github.com/warner/python-versioneer/issues/83) describes\nthis one, but upgrading to a newer version of setuptools should probably\nresolve it.\n\n### Unicode version strings\n\nWhile Versioneer works (and is continually tested) with both Python 2 and\nPython 3, it is not entirely consistent with bytes-vs-unicode distinctions.\nNewer releases probably generate unicode version strings on py2. It\'s not\nclear that this is wrong, but it may be surprising for applications when then\nwrite these strings to a network connection or include them in bytes-oriented\nAPIs like cryptographic checksums.\n\n[Bug #71](https://github.com/warner/python-versioneer/issues/71) investigates\nthis question.\n\n\n## Updating Versioneer\n\nTo upgrade your project to a new release of Versioneer, do the following:\n\n* install the new Versioneer (`pip install -U versioneer` or equivalent)\n* edit `setup.cfg`, if necessary, to include any new configuration settings\n  indicated by the release notes. See [UPGRADING](./UPGRADING.md) for details.\n* re-run `versioneer install` in your source tree, to replace\n  `SRC/_version.py`\n* commit any changed files\n\n## Future Directions\n\nThis tool is designed to make it easily extended to other version-control\nsystems: all VCS-specific components are in separate directories like\nsrc/git/ . The top-level `versioneer.py` script is assembled from these\ncomponents by running make-versioneer.py . In the future, make-versioneer.py\nwill take a VCS name as an argument, and will construct a version of\n`versioneer.py` that is specific to the given VCS. It might also take the\nconfiguration arguments that are currently provided manually during\ninstallation by editing setup.py . Alternatively, it might go the other\ndirection and include code from all supported VCS systems, reducing the\nnumber of intermediate scripts.\n\n\n## License\n\nTo make Versioneer easier to embed, all its code is dedicated to the public\ndomain. The `_version.py` that it creates is also in the public domain.\nSpecifically, both are released under the Creative Commons ""Public Domain\nDedication"" license (CC0-1.0), as described in\nhttps://creativecommons.org/publicdomain/zero/1.0/ .\n\n""""""\n\nfrom __future__ import print_function\ntry:\n    import configparser\nexcept ImportError:\n    import ConfigParser as configparser\nimport errno\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_root():\n    """"""Get the project root directory.\n\n    We require that all commands are run from the project root, i.e. the\n    directory that contains setup.py, setup.cfg, and versioneer.py .\n    """"""\n    root = os.path.realpath(os.path.abspath(os.getcwd()))\n    setup_py = os.path.join(root, ""setup.py"")\n    versioneer_py = os.path.join(root, ""versioneer.py"")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        # allow \'python path/to/setup.py COMMAND\'\n        root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n        setup_py = os.path.join(root, ""setup.py"")\n        versioneer_py = os.path.join(root, ""versioneer.py"")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        err = (""Versioneer was unable to run the project root directory. ""\n               ""Versioneer requires setup.py to be executed from ""\n               ""its immediate directory (like \'python setup.py COMMAND\'), ""\n               ""or in a way that lets it use sys.argv[0] to find the root ""\n               ""(like \'python path/to/setup.py COMMAND\')."")\n        raise VersioneerBadRootError(err)\n    try:\n        # Certain runtime workflows (setup.py install/develop in a setuptools\n        # tree) execute all dependencies in a single python process, so\n        # ""versioneer"" may be imported multiple times, and python\'s shared\n        # module-import table will cache the first one. So we can\'t use\n        # os.path.dirname(__file__), as that will find whichever\n        # versioneer.py was first imported, even in later projects.\n        me = os.path.realpath(os.path.abspath(__file__))\n        me_dir = os.path.normcase(os.path.splitext(me)[0])\n        vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n        if me_dir != vsr_dir:\n            print(""Warning: build in %s is using versioneer.py from %s""\n                  % (os.path.dirname(me), versioneer_py))\n    except NameError:\n        pass\n    return root\n\n\ndef get_config_from_root(root):\n    """"""Read the project setup.cfg file to determine Versioneer config.""""""\n    # This might raise EnvironmentError (if setup.cfg is missing), or\n    # configparser.NoSectionError (if it lacks a [versioneer] section), or\n    # configparser.NoOptionError (if it lacks ""VCS=""). See the docstring at\n    # the top of versioneer.py for instructions on writing your setup.cfg .\n    setup_cfg = os.path.join(root, ""setup.cfg"")\n    parser = configparser.SafeConfigParser()\n    with open(setup_cfg, ""r"") as f:\n        parser.readfp(f)\n    VCS = parser.get(""versioneer"", ""VCS"")  # mandatory\n\n    def get(parser, name):\n        if parser.has_option(""versioneer"", name):\n            return parser.get(""versioneer"", name)\n        return None\n    cfg = VersioneerConfig()\n    cfg.VCS = VCS\n    cfg.style = get(parser, ""style"") or """"\n    cfg.versionfile_source = get(parser, ""versionfile_source"")\n    cfg.versionfile_build = get(parser, ""versionfile_build"")\n    cfg.tag_prefix = get(parser, ""tag_prefix"")\n    if cfg.tag_prefix in (""\'\'"", \'""""\'):\n        cfg.tag_prefix = """"\n    cfg.parentdir_prefix = get(parser, ""parentdir_prefix"")\n    cfg.verbose = get(parser, ""verbose"")\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\n# these dictionaries contain VCS-specific tools\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %s"" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %s"" % (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %s (error)"" % dispcmd)\n            print(""stdout was %s"" % stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\nLONG_VERSION_PY[\'git\'] = \'\'\'\n# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.18 (https://github.com/warner/python-versioneer)\n\n""""""Git implementation of _version.py.""""""\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    """"""Get the keywords needed to look up the version information.""""""\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = ""%(DOLLAR)sFormat:%%d%(DOLLAR)s""\n    git_full = ""%(DOLLAR)sFormat:%%H%(DOLLAR)s""\n    git_date = ""%(DOLLAR)sFormat:%%ci%(DOLLAR)s""\n    keywords = {""refnames"": git_refnames, ""full"": git_full, ""date"": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_config():\n    """"""Create, populate and return the VersioneerConfig() object.""""""\n    # these strings are filled in when \'setup.py versioneer\' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = ""git""\n    cfg.style = ""%(STYLE)s""\n    cfg.tag_prefix = ""%(TAG_PREFIX)s""\n    cfg.parentdir_prefix = ""%(PARENTDIR_PREFIX)s""\n    cfg.versionfile_source = ""%(VERSIONFILE_SOURCE)s""\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %%s"" %% dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %%s"" %% (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %%s (error)"" %% dispcmd)\n            print(""stdout was %%s"" %% stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {""version"": dirname[len(parentdir_prefix):],\n                    ""full-revisionid"": None,\n                    ""dirty"": False, ""error"": None, ""date"": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(""Tried directories %%s but none started with prefix %%s"" %%\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %%d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r\'\\d\', r)])\n        if verbose:\n            print(""discarding \'%%s\', no digits"" %% "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %%s"" %% "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(""picking %%s"" %% r)\n            return {""version"": r,\n                    ""full-revisionid"": keywords[""full""].strip(),\n                    ""dirty"": False, ""error"": None,\n                    ""date"": date}\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {""version"": ""0+unknown"",\n            ""full-revisionid"": keywords[""full""].strip(),\n            ""dirty"": False, ""error"": ""no suitable tags"", ""date"": None}\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %%s not under git control"" %% root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [""describe"", ""--tags"", ""--dirty"",\n                                          ""--always"", ""--long"",\n                                          ""--match"", ""%%s*"" %% tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\'^(.+)-(\\d+)-g([0-9a-f]+)$\', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = (""unable to parse git-describe output: \'%%s\'""\n                               %% describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%%s\' doesn\'t start with prefix \'%%s\'""\n                print(fmt %% (full_tag, tag_prefix))\n            pieces[""error""] = (""tag \'%%s\' doesn\'t start with prefix \'%%s\'""\n                               %% (full_tag, tag_prefix))\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""],\n                                    cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%%ci"", ""HEAD""],\n                       cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%%d.g%%s"" %% (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%%d.g%%s"" %% (pieces[""distance""],\n                                          pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%%d"" %% pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%%d"" %% pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%%d"" %% pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%%s"" %% pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%%d"" %% pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%%s"" %% pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%%d"" %% pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%%d"" %% pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%%d-g%%s"" %% (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%%d-g%%s"" %% (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {""version"": ""unknown"",\n                ""full-revisionid"": pieces.get(""long""),\n                ""dirty"": None,\n                ""error"": pieces[""error""],\n                ""date"": None}\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%%s\'"" %% style)\n\n    return {""version"": rendered, ""full-revisionid"": pieces[""long""],\n            ""dirty"": pieces[""dirty""], ""error"": None,\n            ""date"": pieces.get(""date"")}\n\n\ndef get_versions():\n    """"""Get version information or return default if unable to do so.""""""\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don\'t do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split(\'/\'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {""version"": ""0+unknown"", ""full-revisionid"": None,\n                ""dirty"": None,\n                ""error"": ""unable to find root of source tree"",\n                ""date"": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {""version"": ""0+unknown"", ""full-revisionid"": None,\n            ""dirty"": None,\n            ""error"": ""unable to compute version"", ""date"": None}\n\'\'\'\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r\'\\d\', r)])\n        if verbose:\n            print(""discarding \'%s\', no digits"" % "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %s"" % "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(""picking %s"" % r)\n            return {""version"": r,\n                    ""full-revisionid"": keywords[""full""].strip(),\n                    ""dirty"": False, ""error"": None,\n                    ""date"": date}\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {""version"": ""0+unknown"",\n            ""full-revisionid"": keywords[""full""].strip(),\n            ""dirty"": False, ""error"": ""no suitable tags"", ""date"": None}\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %s not under git control"" % root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [""describe"", ""--tags"", ""--dirty"",\n                                          ""--always"", ""--long"",\n                                          ""--match"", ""%s*"" % tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\'^(.+)-(\\d+)-g([0-9a-f]+)$\', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = (""unable to parse git-describe output: \'%s\'""\n                               % describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                print(fmt % (full_tag, tag_prefix))\n            pieces[""error""] = (""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                               % (full_tag, tag_prefix))\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""],\n                                    cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%ci"", ""HEAD""],\n                       cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef do_vcs_install(manifest_in, versionfile_source, ipy):\n    """"""Git-specific installation logic for Versioneer.\n\n    For Git, this means creating/changing .gitattributes to mark _version.py\n    for export-subst keyword substitution.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n    files = [manifest_in, versionfile_source]\n    if ipy:\n        files.append(ipy)\n    try:\n        me = __file__\n        if me.endswith("".pyc"") or me.endswith("".pyo""):\n            me = os.path.splitext(me)[0] + "".py""\n        versioneer_file = os.path.relpath(me)\n    except NameError:\n        versioneer_file = ""versioneer.py""\n    files.append(versioneer_file)\n    present = False\n    try:\n        f = open("".gitattributes"", ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(versionfile_source):\n                if ""export-subst"" in line.strip().split()[1:]:\n                    present = True\n        f.close()\n    except EnvironmentError:\n        pass\n    if not present:\n        f = open("".gitattributes"", ""a+"")\n        f.write(""%s export-subst\\n"" % versionfile_source)\n        f.close()\n        files.append("".gitattributes"")\n    run_command(GITS, [""add"", ""--""] + files)\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {""version"": dirname[len(parentdir_prefix):],\n                    ""full-revisionid"": None,\n                    ""dirty"": False, ""error"": None, ""date"": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(""Tried directories %s but none started with prefix %s"" %\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\nSHORT_VERSION_PY = """"""\n# This file was generated by \'versioneer.py\' (0.18) from\n# revision-control system data, or from the parent directory name of an\n# unpacked source archive. Distribution tarballs contain a pre-generated copy\n# of this file.\n\nimport json\n\nversion_json = \'\'\'\n%s\n\'\'\'  # END VERSION_JSON\n\n\ndef get_versions():\n    return json.loads(version_json)\n""""""\n\n\ndef versions_from_file(filename):\n    """"""Try to determine the version from _version.py if present.""""""\n    try:\n        with open(filename) as f:\n            contents = f.read()\n    except EnvironmentError:\n        raise NotThisMethod(""unable to read _version.py"")\n    mo = re.search(r""version_json = \'\'\'\\n(.*)\'\'\'  # END VERSION_JSON"",\n                   contents, re.M | re.S)\n    if not mo:\n        mo = re.search(r""version_json = \'\'\'\\r\\n(.*)\'\'\'  # END VERSION_JSON"",\n                       contents, re.M | re.S)\n    if not mo:\n        raise NotThisMethod(""no version_json in _version.py"")\n    return json.loads(mo.group(1))\n\n\ndef write_to_version_file(filename, versions):\n    """"""Write the given version number to the given _version.py file.""""""\n    os.unlink(filename)\n    contents = json.dumps(versions, sort_keys=True,\n                          indent=1, separators=("","", "": ""))\n    with open(filename, ""w"") as f:\n        f.write(SHORT_VERSION_PY % contents)\n\n    print(""set %s to \'%s\'"" % (filename, versions[""version""]))\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%d.g%s"" % (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%d.g%s"" % (pieces[""distance""],\n                                          pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%d"" % pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%d"" % pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%s"" % pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%s"" % pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {""version"": ""unknown"",\n                ""full-revisionid"": pieces.get(""long""),\n                ""dirty"": None,\n                ""error"": pieces[""error""],\n                ""date"": None}\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%s\'"" % style)\n\n    return {""version"": rendered, ""full-revisionid"": pieces[""long""],\n            ""dirty"": pieces[""dirty""], ""error"": None,\n            ""date"": pieces.get(""date"")}\n\n\nclass VersioneerBadRootError(Exception):\n    """"""The project root directory is unknown or missing key files.""""""\n\n\ndef get_versions(verbose=False):\n    """"""Get the project version from whatever source is available.\n\n    Returns dict with two keys: \'version\' and \'full\'.\n    """"""\n    if ""versioneer"" in sys.modules:\n        # see the discussion in cmdclass.py:get_cmdclass()\n        del sys.modules[""versioneer""]\n\n    root = get_root()\n    cfg = get_config_from_root(root)\n\n    assert cfg.VCS is not None, ""please set [versioneer]VCS= in setup.cfg""\n    handlers = HANDLERS.get(cfg.VCS)\n    assert handlers, ""unrecognized VCS \'%s\'"" % cfg.VCS\n    verbose = verbose or cfg.verbose\n    assert cfg.versionfile_source is not None, \\\n        ""please set versioneer.versionfile_source""\n    assert cfg.tag_prefix is not None, ""please set versioneer.tag_prefix""\n\n    versionfile_abs = os.path.join(root, cfg.versionfile_source)\n\n    # extract version from first of: _version.py, VCS command (e.g. \'git\n    # describe\'), parentdir. This is meant to work for developers using a\n    # source checkout, for users of a tarball created by \'setup.py sdist\',\n    # and for users of a tarball/zipball created by \'git archive\' or github\'s\n    # download-from-tag feature or the equivalent in other VCSes.\n\n    get_keywords_f = handlers.get(""get_keywords"")\n    from_keywords_f = handlers.get(""keywords"")\n    if get_keywords_f and from_keywords_f:\n        try:\n            keywords = get_keywords_f(versionfile_abs)\n            ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n            if verbose:\n                print(""got version from expanded keyword %s"" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        ver = versions_from_file(versionfile_abs)\n        if verbose:\n            print(""got version from file %s %s"" % (versionfile_abs, ver))\n        return ver\n    except NotThisMethod:\n        pass\n\n    from_vcs_f = handlers.get(""pieces_from_vcs"")\n    if from_vcs_f:\n        try:\n            pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n            ver = render(pieces, cfg.style)\n            if verbose:\n                print(""got version from VCS %s"" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        if cfg.parentdir_prefix:\n            ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n            if verbose:\n                print(""got version from parentdir %s"" % ver)\n            return ver\n    except NotThisMethod:\n        pass\n\n    if verbose:\n        print(""unable to compute version"")\n\n    return {""version"": ""0+unknown"", ""full-revisionid"": None,\n            ""dirty"": None, ""error"": ""unable to compute version"",\n            ""date"": None}\n\n\ndef get_version():\n    """"""Get the short version string for this project.""""""\n    return get_versions()[""version""]\n\n\ndef get_cmdclass():\n    """"""Get the custom setuptools/distutils subclasses used by Versioneer.""""""\n    if ""versioneer"" in sys.modules:\n        del sys.modules[""versioneer""]\n        # this fixes the ""python setup.py develop"" case (also \'install\' and\n        # \'easy_install .\'), in which subdependencies of the main project are\n        # built (using setup.py bdist_egg) in the same python process. Assume\n        # a main project A and a dependency B, which use different versions\n        # of Versioneer. A\'s setup.py imports A\'s Versioneer, leaving it in\n        # sys.modules by the time B\'s setup.py is executed, causing B to run\n        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n        # sandbox that restores sys.modules to it\'s pre-build state, so the\n        # parent is protected against the child\'s ""import versioneer"". By\n        # removing ourselves from sys.modules here, before the child build\n        # happens, we protect the child from the parent\'s versioneer too.\n        # Also see https://github.com/warner/python-versioneer/issues/52\n\n    cmds = {}\n\n    # we add ""version"" to both distutils and setuptools\n    from distutils.core import Command\n\n    class cmd_version(Command):\n        description = ""report generated version string""\n        user_options = []\n        boolean_options = []\n\n        def initialize_options(self):\n            pass\n\n        def finalize_options(self):\n            pass\n\n        def run(self):\n            vers = get_versions(verbose=True)\n            print(""Version: %s"" % vers[""version""])\n            print("" full-revisionid: %s"" % vers.get(""full-revisionid""))\n            print("" dirty: %s"" % vers.get(""dirty""))\n            print("" date: %s"" % vers.get(""date""))\n            if vers[""error""]:\n                print("" error: %s"" % vers[""error""])\n    cmds[""version""] = cmd_version\n\n    # we override ""build_py"" in both distutils and setuptools\n    #\n    # most invocation pathways end up running build_py:\n    #  distutils/build -> build_py\n    #  distutils/install -> distutils/build ->..\n    #  setuptools/bdist_wheel -> distutils/install ->..\n    #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n    #  setuptools/install -> bdist_egg ->..\n    #  setuptools/develop -> ?\n    #  pip install:\n    #   copies source tree to a tempdir before running egg_info/etc\n    #   if .git isn\'t copied too, \'git describe\' will fail\n    #   then does setup.py bdist_wheel, or sometimes setup.py install\n    #  setup.py egg_info -> ?\n\n    # we override different ""build_py"" commands for both environments\n    if ""setuptools"" in sys.modules:\n        from setuptools.command.build_py import build_py as _build_py\n    else:\n        from distutils.command.build_py import build_py as _build_py\n\n    class cmd_build_py(_build_py):\n        def run(self):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_py.run(self)\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            if cfg.versionfile_build:\n                target_versionfile = os.path.join(self.build_lib,\n                                                  cfg.versionfile_build)\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n    cmds[""build_py""] = cmd_build_py\n\n    if ""cx_Freeze"" in sys.modules:  # cx_freeze enabled?\n        from cx_Freeze.dist import build_exe as _build_exe\n        # nczeczulin reports that py2exe won\'t like the pep440-style string\n        # as FILEVERSION, but it can be used for PRODUCTVERSION, e.g.\n        # setup(console=[{\n        #   ""version"": versioneer.get_version().split(""+"", 1)[0], # FILEVERSION\n        #   ""product_version"": versioneer.get_version(),\n        #   ...\n\n        class cmd_build_exe(_build_exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _build_exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, ""w"") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(LONG %\n                            {""DOLLAR"": ""$"",\n                             ""STYLE"": cfg.style,\n                             ""TAG_PREFIX"": cfg.tag_prefix,\n                             ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                             ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                             })\n        cmds[""build_exe""] = cmd_build_exe\n        del cmds[""build_py""]\n\n    if \'py2exe\' in sys.modules:  # py2exe enabled?\n        try:\n            from py2exe.distutils_buildexe import py2exe as _py2exe  # py3\n        except ImportError:\n            from py2exe.build_exe import py2exe as _py2exe  # py2\n\n        class cmd_py2exe(_py2exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _py2exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, ""w"") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(LONG %\n                            {""DOLLAR"": ""$"",\n                             ""STYLE"": cfg.style,\n                             ""TAG_PREFIX"": cfg.tag_prefix,\n                             ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                             ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                             })\n        cmds[""py2exe""] = cmd_py2exe\n\n    # we override different ""sdist"" commands for both environments\n    if ""setuptools"" in sys.modules:\n        from setuptools.command.sdist import sdist as _sdist\n    else:\n        from distutils.command.sdist import sdist as _sdist\n\n    class cmd_sdist(_sdist):\n        def run(self):\n            versions = get_versions()\n            self._versioneer_generated_versions = versions\n            # unless we update this, the command will keep using the old\n            # version\n            self.distribution.metadata.version = versions[""version""]\n            return _sdist.run(self)\n\n        def make_release_tree(self, base_dir, files):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            _sdist.make_release_tree(self, base_dir, files)\n            # now locate _version.py in the new base_dir directory\n            # (remembering that it may be a hardlink) and replace it with an\n            # updated value\n            target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n            print(""UPDATING %s"" % target_versionfile)\n            write_to_version_file(target_versionfile,\n                                  self._versioneer_generated_versions)\n    cmds[""sdist""] = cmd_sdist\n\n    return cmds\n\n\nCONFIG_ERROR = """"""\nsetup.cfg is missing the necessary Versioneer configuration. You need\na section like:\n\n [versioneer]\n VCS = git\n style = pep440\n versionfile_source = src/myproject/_version.py\n versionfile_build = myproject/_version.py\n tag_prefix =\n parentdir_prefix = myproject-\n\nYou will also need to edit your setup.py to use the results:\n\n import versioneer\n setup(version=versioneer.get_version(),\n       cmdclass=versioneer.get_cmdclass(), ...)\n\nPlease read the docstring in ./versioneer.py for configuration instructions,\nedit setup.cfg, and re-run the installer or \'python versioneer.py setup\'.\n""""""\n\nSAMPLE_CONFIG = """"""\n# See the docstring in versioneer.py for instructions. Note that you must\n# re-run \'versioneer.py setup\' after changing this section, and commit the\n# resulting files.\n\n[versioneer]\n#VCS = git\n#style = pep440\n#versionfile_source =\n#versionfile_build =\n#tag_prefix =\n#parentdir_prefix =\n\n""""""\n\nINIT_PY_SNIPPET = """"""\nfrom ._version import get_versions\n__version__ = get_versions()[\'version\']\ndel get_versions\n""""""\n\n\ndef do_setup():\n    """"""Main VCS-independent setup function for installing Versioneer.""""""\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (EnvironmentError, configparser.NoSectionError,\n            configparser.NoOptionError) as e:\n        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):\n            print(""Adding sample versioneer config to setup.cfg"",\n                  file=sys.stderr)\n            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return 1\n\n    print("" creating %s"" % cfg.versionfile_source)\n    with open(cfg.versionfile_source, ""w"") as f:\n        LONG = LONG_VERSION_PY[cfg.VCS]\n        f.write(LONG % {""DOLLAR"": ""$"",\n                        ""STYLE"": cfg.style,\n                        ""TAG_PREFIX"": cfg.tag_prefix,\n                        ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                        ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                        })\n\n    ipy = os.path.join(os.path.dirname(cfg.versionfile_source),\n                       ""__init__.py"")\n    if os.path.exists(ipy):\n        try:\n            with open(ipy, ""r"") as f:\n                old = f.read()\n        except EnvironmentError:\n            old = """"\n        if INIT_PY_SNIPPET not in old:\n            print("" appending to %s"" % ipy)\n            with open(ipy, ""a"") as f:\n                f.write(INIT_PY_SNIPPET)\n        else:\n            print("" %s unmodified"" % ipy)\n    else:\n        print("" %s doesn\'t exist, ok"" % ipy)\n        ipy = None\n\n    # Make sure both the top-level ""versioneer.py"" and versionfile_source\n    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so\n    # they\'ll be copied into source distributions. Pip won\'t be able to\n    # install the package without this.\n    manifest_in = os.path.join(root, ""MANIFEST.in"")\n    simple_includes = set()\n    try:\n        with open(manifest_in, ""r"") as f:\n            for line in f:\n                if line.startswith(""include ""):\n                    for include in line.split()[1:]:\n                        simple_includes.add(include)\n    except EnvironmentError:\n        pass\n    # That doesn\'t cover everything MANIFEST.in can do\n    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so\n    # it might give some false negatives. Appending redundant \'include\'\n    # lines is safe, though.\n    if ""versioneer.py"" not in simple_includes:\n        print("" appending \'versioneer.py\' to MANIFEST.in"")\n        with open(manifest_in, ""a"") as f:\n            f.write(""include versioneer.py\\n"")\n    else:\n        print("" \'versioneer.py\' already in MANIFEST.in"")\n    if cfg.versionfile_source not in simple_includes:\n        print("" appending versionfile_source (\'%s\') to MANIFEST.in"" %\n              cfg.versionfile_source)\n        with open(manifest_in, ""a"") as f:\n            f.write(""include %s\\n"" % cfg.versionfile_source)\n    else:\n        print("" versionfile_source already in MANIFEST.in"")\n\n    # Make VCS-specific changes. For git, this means creating/changing\n    # .gitattributes to mark _version.py for export-subst keyword\n    # substitution.\n    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)\n    return 0\n\n\ndef scan_setup_py():\n    """"""Validate the contents of setup.py against Versioneer\'s expectations.""""""\n    found = set()\n    setters = False\n    errors = 0\n    with open(""setup.py"", ""r"") as f:\n        for line in f.readlines():\n            if ""import versioneer"" in line:\n                found.add(""import"")\n            if ""versioneer.get_cmdclass()"" in line:\n                found.add(""cmdclass"")\n            if ""versioneer.get_version()"" in line:\n                found.add(""get_version"")\n            if ""versioneer.VCS"" in line:\n                setters = True\n            if ""versioneer.versionfile_source"" in line:\n                setters = True\n    if len(found) != 3:\n        print("""")\n        print(""Your setup.py appears to be missing some important items"")\n        print(""(but I might be wrong). Please make sure it has something"")\n        print(""roughly like the following:"")\n        print("""")\n        print("" import versioneer"")\n        print("" setup( version=versioneer.get_version(),"")\n        print(""        cmdclass=versioneer.get_cmdclass(),  ...)"")\n        print("""")\n        errors += 1\n    if setters:\n        print(""You should remove lines like \'versioneer.VCS = \' and"")\n        print(""\'versioneer.versionfile_source = \' . This configuration"")\n        print(""now lives in setup.cfg, and should be removed from setup.py"")\n        print("""")\n        errors += 1\n    return errors\n\n\nif __name__ == ""__main__"":\n    cmd = sys.argv[1]\n    if cmd == ""setup"":\n        errors = do_setup()\n        errors += scan_setup_py()\n        if errors:\n            sys.exit(1)\n'"
example_configs/logp_gcnn_config.py,4,"b""from openchem.models.Graph2Label import Graph2Label\nfrom openchem.modules.encoders.gcn_encoder import GraphCNNEncoder\nfrom openchem.modules.mlp.openchem_mlp import OpenChemMLP\nfrom openchem.data.graph_data_layer import GraphDataset\n\nfrom openchem.utils.graph import Attribute\nfrom openchem.utils.utils import identity\n\nimport torch.nn as nn\nfrom torch.optim import RMSprop, SGD, Adam\nfrom torch.optim.lr_scheduler import ExponentialLR, StepLR\nimport torch.nn.functional as F\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nimport pandas as pd\n\nimport copy\nimport pickle\n\n\ndef get_atomic_attributes(atom):\n    attr_dict = {}\n  \n    atomic_num = atom.GetAtomicNum()\n    atomic_mapping = {5: 0, 7: 1, 6: 2, 8: 3, 9: 4, 15: 5, 16: 6, 17: 7, 35: 8,\n                      53: 9}\n    if atomic_num in atomic_mapping.keys():\n        attr_dict['atom_element'] = atomic_mapping[atomic_num]\n    else:\n        attr_dict['atom_element'] = 10\n    attr_dict['valence'] = atom.GetTotalValence()\n    attr_dict['charge'] = atom.GetFormalCharge()\n    attr_dict['hybridization'] = atom.GetHybridization().real\n    attr_dict['aromatic'] = int(atom.GetIsAromatic())\n    return attr_dict\n\n\nnode_attributes = {}\nnode_attributes['valence'] = Attribute('node', 'valence', one_hot=True, values=[1, 2, 3, 4, 5, 6])\nnode_attributes['charge'] = Attribute('node', 'charge', one_hot=True, values=[-1, 0, 1, 2, 3, 4])\nnode_attributes['hybridization'] = Attribute('node', 'hybridization',\n                                             one_hot=True, values=[0, 1, 2, 3, 4, 5, 6, 7])\nnode_attributes['aromatic'] = Attribute('node', 'aromatic', one_hot=True,\n                                        values=[0, 1])\nnode_attributes['atom_element'] = Attribute('node', 'atom_element',\n                                            one_hot=True,\n                                            values=list(range(11)))\n\ntrain_dataset = GraphDataset(get_atomic_attributes, node_attributes,\n                             './benchmark_datasets/Lipophilicity_dataset/Lipophilicity_train.csv',\n                             delimiter=',', cols_to_read=[0, 1])\ntest_dataset = GraphDataset(get_atomic_attributes, node_attributes,\n                             './benchmark_datasets/Lipophilicity_dataset/Lipophilicity_test.csv',\n                             delimiter=',', cols_to_read=[0, 1])\n\nmodel = Graph2Label\n\nmodel_params = {\n    'task': 'regression',\n    'data_layer': GraphDataset,\n    'use_clip_grad': False,\n    'batch_size': 256,\n    'num_epochs': 101,\n    'logdir': '/home/user/Work/OpenChem/logs/logp_gcnn_logs',\n    'print_every': 10,\n    'save_every': 5,\n    'train_data_layer': train_dataset,\n    'val_data_layer': test_dataset,\n    'eval_metrics': r2_score,\n    'criterion': nn.MSELoss(),\n    'optimizer': Adam,\n    'optimizer_params': {\n        'lr': 0.0005,\n    },\n    'lr_scheduler': StepLR,\n    'lr_scheduler_params': {\n        'step_size': 15,\n        'gamma': 0.8\n    },\n    'encoder': GraphCNNEncoder,\n    'encoder_params': {\n        'input_size': train_dataset.num_features,\n        'encoder_dim': 128,\n        'n_layers': 5,\n        'hidden_size': [128, 128, 128, 128, 128],\n    },\n    'mlp': OpenChemMLP,\n    'mlp_params': {\n        'input_size': 128,\n        'n_layers': 2,\n        'hidden_size': [128, 1],\n        'activation': [F.relu, identity]\n    }\n}\n\n"""
example_configs/tox21_rnn_config.py,6,"b'from openchem.models.Smiles2Label import Smiles2Label\nfrom openchem.modules.embeddings.basic_embedding import Embedding\nfrom openchem.modules.encoders.rnn_encoder import RNNEncoder\nfrom openchem.modules.mlp.openchem_mlp import OpenChemMLP\nfrom openchem.data.smiles_data_layer import SmilesDataset\nfrom openchem.criterion.multitask_loss import MultitaskLoss\n\nimport torch\nimport torch.nn as nn\n\nimport numpy as np\n\nfrom torch.optim import RMSprop, Adam\nfrom torch.optim.lr_scheduler import ExponentialLR, StepLR\nimport torch.nn.functional as F\nfrom sklearn.metrics import roc_auc_score, mean_squared_error\n\nfrom openchem.data.utils import read_smiles_property_file\ndata = read_smiles_property_file(\'./benchmark_datasets/tox21/tox21.csv\',\n                                 cols_to_read=[13] + list(range(0,12)))\nsmiles = data[0]\nlabels = np.array(data[1:])\n\nlabels[np.where(labels==\'\')] = \'999\'\nlabels = labels.T\n\nfrom openchem.data.utils import get_tokens\ntokens, _, _ = get_tokens(smiles)\ntokens = tokens + \' \'\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(smiles, labels, test_size=0.2,\n                                                    random_state=42)\n\nfrom openchem.data.utils import save_smiles_property_file\nsave_smiles_property_file(\'./benchmark_datasets/tox21/train.smi\', X_train, y_train)\nsave_smiles_property_file(\'./benchmark_datasets/tox21/test.smi\', X_test, y_test)\n\nfrom openchem.data.smiles_data_layer import SmilesDataset\ntrain_dataset = SmilesDataset(\'./benchmark_datasets/tox21/train.smi\',\n                              delimiter=\',\', cols_to_read=list(range(13)),\n                              tokens=tokens, augment=True)\ntest_dataset = SmilesDataset(\'./benchmark_datasets/tox21/test.smi\',\n                            delimiter=\',\', cols_to_read=list(range(13)),\n                            tokens=tokens)\n\ndef multitask_auc(ground_truth, predicted):\n    from sklearn.metrics import roc_auc_score\n    import numpy as np\n    import torch\n    ground_truth = np.array(ground_truth)\n    predicted = np.array(predicted)\n    n_tasks = ground_truth.shape[1]\n    auc = []\n    for i in range(n_tasks):\n        ind = np.where(ground_truth[:, i] != 999)[0]\n        auc.append(roc_auc_score(ground_truth[ind, i], predicted[ind, i]))\n    #if torch.distributed.get_rank() == 0:\n    #    print(auc)\n    return np.mean(auc)\n\nmodel = Smiles2Label\n\nmodel_params = {\n    \'use_cuda\': True,\n    \'task\': \'multitask\',\n    \'random_seed\': 5,\n    \'use_clip_grad\': True,\n    \'max_grad_norm\': 10.0,\n    \'batch_size\': 256,\n    \'num_epochs\': 31,\n    \'logdir\': \'/home/mpopova/Work/OpenChem/logs/rnn_log\',\n    \'print_every\': 5,\n    \'save_every\': 5,\n    \'train_data_layer\': train_dataset,\n    \'val_data_layer\': test_dataset,\n    \'eval_metrics\': multitask_auc,\n    \'criterion\': MultitaskLoss(ignore_index=999, n_tasks=12).cuda(),\n    \'optimizer\': RMSprop,\n    \'optimizer_params\': {\n        \'lr\': 0.001,\n        },\n    \'lr_scheduler\': StepLR,\n    \'lr_scheduler_params\': {\n        \'step_size\': 10,\n        \'gamma\': 0.8\n    },\n    \'embedding\': Embedding,\n    \'embedding_params\': {\n        \'num_embeddings\': train_dataset.num_tokens,\n        \'embedding_dim\': 128,\n        \'padding_idx\': train_dataset.tokens.index(\' \')\n    },\n    \'encoder\': RNNEncoder,\n    \'encoder_params\': {\n        \'input_size\': 128,\n        \'layer\': ""LSTM"",\n        \'encoder_dim\': 128,\n        \'n_layers\': 4,\n        \'dropout\': 0.8,\n        \'is_bidirectional\': False\n    },\n    \'mlp\': OpenChemMLP,\n    \'mlp_params\': {\n        \'input_size\': 128,\n        \'n_layers\': 2,\n        \'hidden_size\': [128, 12],\n        \'activation\': [F.relu, torch.sigmoid],\n        \'dropout\': 0.0\n    }\n}\n'"
devtools/scripts/create_conda_env.py,0,"b'import argparse\nimport os\nimport re\nimport glob\nimport shutil\nimport subprocess as sp\nfrom tempfile import TemporaryDirectory\nfrom contextlib import contextmanager\n# YAML imports\ntry:\n    import yaml  # PyYAML\n    loader = yaml.load\nexcept ImportError:\n    try:\n        import ruamel_yaml as yaml  # Ruamel YAML\n    except ImportError:\n        try:\n            # Load Ruamel YAML from the base conda environment\n            from importlib import util as import_util\n            CONDA_BIN = os.path.dirname(os.environ[\'CONDA_EXE\'])\n            ruamel_yaml_path = glob.glob(os.path.join(CONDA_BIN, \'..\',\n                                                      \'lib\', \'python*.*\', \'site-packages\',\n                                                      \'ruamel_yaml\', \'__init__.py\'))[0]\n            # Based on importlib example, but only needs to load_module since its the whole package, not just\n            # a module\n            spec = import_util.spec_from_file_location(\'ruamel_yaml\', ruamel_yaml_path)\n            yaml = spec.loader.load_module()\n        except (KeyError, ImportError, IndexError):\n            raise ImportError(""No YAML parser could be found in this or the conda environment. ""\n                              ""Could not find PyYAML or Ruamel YAML in the current environment, ""\n                              ""AND could not find Ruamel YAML in the base conda environment through CONDA_EXE path. "" \n                              ""Environment not created!"")\n    loader = yaml.YAML(typ=""safe"").load  # typ=""safe"" avoids odd typing on output\n\n\n@contextmanager\ndef temp_cd():\n    """"""Temporary CD Helper""""""\n    cwd = os.getcwd()\n    with TemporaryDirectory() as td:\n        try:\n            os.chdir(td)\n            yield\n        finally:\n            os.chdir(cwd)\n\n\n# Args\nparser = argparse.ArgumentParser(description=\'Creates a conda environment from file for a given Python version.\')\nparser.add_argument(\'-n\', \'--name\', type=str,\n                    help=\'The name of the created Python environment\')\nparser.add_argument(\'-p\', \'--python\', type=str,\n                    help=\'The version of the created Python environment\')\nparser.add_argument(\'conda_file\',\n                    help=\'The file for the created Python environment\')\n\nargs = parser.parse_args()\n\n# Open the base file\nwith open(args.conda_file, ""r"") as handle:\n    yaml_script = loader(handle.read())\n\npython_replacement_string = ""python {}*"".format(args.python)\n\ntry:\n    for dep_index, dep_value in enumerate(yaml_script[\'dependencies\']):\n        if re.match(\'python([ ><=*]+[0-9.*]*)?$\', dep_value):  # Match explicitly \'python\' and its formats\n            yaml_script[\'dependencies\'].pop(dep_index)\n            break  # Making the assumption there is only one Python entry, also avoids need to enumerate in reverse\nexcept (KeyError, TypeError):\n    # Case of no dependencies key, or dependencies: None\n    yaml_script[\'dependencies\'] = []\nfinally:\n    # Ensure the python version is added in. Even if the code does not need it, we assume the env does\n    yaml_script[\'dependencies\'].insert(0, python_replacement_string)\n\n# Figure out conda path\nif ""CONDA_EXE"" in os.environ:\n    conda_path = os.environ[""CONDA_EXE""]\nelse:\n    conda_path = shutil.which(""conda"")\nif conda_path is None:\n    raise RuntimeError(""Could not find a conda binary in CONDA_EXE variable or in executable search path"")\n\nprint(""CONDA ENV NAME  {}"".format(args.name))\nprint(""PYTHON VERSION  {}"".format(args.python))\nprint(""CONDA FILE NAME {}"".format(args.conda_file))\nprint(""CONDA PATH      {}"".format(conda_path))\n\n# Write to a temp directory which will always be cleaned up\nwith temp_cd():\n    temp_file_name = ""temp_script.yaml""\n    with open(temp_file_name, \'w\') as f:\n        f.write(yaml.dump(yaml_script))\n    sp.call(""{} env create -n {} -f {}"".format(conda_path, args.name, temp_file_name), shell=True)\n'"
docs/sources/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport sphinx_rtd_theme\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'../../\'))\nsys.path.insert(0, os.path.abspath(\'../../openchem\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'OpenChem\'\ncopyright = \'2018, Mariya Popova\'\nauthor = \'Mariya Popova\'\n\n# The short X.Y version\nversion = \'0.1\'\n# The full version, including alpha/beta/rc tags\nrelease = \'0.1\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n  \'sphinx.ext.autodoc\',\n  \'sphinx.ext.viewcode\',\n  \'sphinx.ext.napoleon\',\n  \'sphinx.ext.mathjax\'\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'collapse_navigation\': False,\n    \'display_version\': False,\n    \'logo_only\': True,\n}\n\nhtml_logo = \'logo.png\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'OpenChemdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'OpenChem.tex\', \'OpenChem Documentation\',\n     \'Mariya Popova\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'openchem\', \'OpenChem Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'OpenChem\', \'OpenChem Documentation\',\n     author, \'OpenChem\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n'"
openchem/criterion/multitask_loss.py,5,"b'import torch\nimport torch.nn.functional as F\nfrom torch.nn.modules.loss import _WeightedLoss #, _assert_no_grad\n\n\nclass MultitaskLoss(_WeightedLoss):\n    r""""""\n    Creates a criterion that calculated binary cross-entropy loss over\n    `n_tasks` tasks given input tensors `input` and `target`. Returns loss\n    averaged across number of samples in every task and across `n_tasks`.\n    It is useful when training a classification model with `n_tasks`\n    separate binary classes.\n\n    The loss can be described as:\n\n    ..math::\n        \\text{loss}(y, t) = -\\frac{1}{n_tasks}\\sum_{i=1}^{n_tasks}\\frac{1}{N_i}\n        \\sum_{j=1}^{N_i} \\left(t[i, j]\\log(1-y[i, j])\n        + (1-t[i, j])\\log(1-y[i, j])\\right).\n\n    Args:\n        ignore_index (int): specifies a target value that is ignored\n            and does not contribute to the gradient. For every task losses are\n            averaged only across non-ignored targets.\n        n_tasks (int): specifies number of tasks.\n\n    Shape:\n        -Input: :math: `(N, n_tasks)`. Values should be in :math:`[0, 1]` range,\n            corresponding to probability of class :math:\'1\'.\n        -Target: :math: `(N, n_tasks)`. Values should be binary: either\n            :math:`0` or :math:`1`, corresponding to class labels.\n        -Output: scalar.\n\n    """"""\n\n    def __init__(self, ignore_index, n_tasks):\n        super(MultitaskLoss, self).__init__(reduction=\'none\')\n        self.n_tasks = n_tasks\n        self.ignore_index = ignore_index\n\n    def forward(self, input, target):\n        assert target.size()[1] == self.n_tasks\n        assert input.size()[1] == self.n_tasks\n        x = torch.zeros(target.size()).cuda()\n        y = torch.ones(target.size()).cuda()\n        mask = torch.where(target == self.ignore_index, x, y)\n        loss = F.binary_cross_entropy(input, mask*target,\n                                      weight=self.weight)\n        loss = loss*mask\n        n_samples = mask.sum(dim=0)\n        return (loss.sum(dim=0)/n_samples).mean()\n'"
openchem/data/graph_data_layer.py,1,"b""# TODO: variable length batching\n\nimport numpy as np\n\n\nfrom openchem.utils.graph import Graph\n\nfrom rdkit import Chem\n\nfrom torch.utils.data import Dataset\nfrom openchem.data.utils import read_smiles_property_file, sanitize_smiles\n\n\nclass GraphDataset(Dataset):\n    def __init__(self, get_atomic_attributes, node_attributes, filename,\n                 cols_to_read, delimiter=',', get_bond_attributes=None, edge_attributes=None):\n        super(GraphDataset, self).__init__()\n        assert (get_bond_attributes is None) == (edge_attributes is None)\n        data_set = read_smiles_property_file(filename, cols_to_read,\n                                             delimiter)\n        data = data_set[0]\n        target = data_set[1:]\n        clean_smiles, clean_idx = sanitize_smiles(data)\n        target = np.array(target).T\n        max_size = 0\n        for sm in clean_smiles:\n            mol = Chem.MolFromSmiles(sm)\n            if mol.GetNumAtoms() > max_size:\n                max_size = mol.GetNumAtoms()\n        self.target = target[clean_idx, :]\n        self.graphs = []\n        self.node_feature_matrix = []\n        self.adj_matrix = []\n        for sm in clean_smiles:\n            graph = Graph(sm, max_size, get_atomic_attributes,\n                          get_bond_attributes)\n            self.node_feature_matrix.append(\n                graph.get_node_feature_matrix(node_attributes, max_size))\n            if get_bond_attributes is None:\n                self.adj_matrix.append(graph.adj_matrix)\n            else:\n                self.adj_matrix.append(\n                    graph.get_edge_attr_adj_matrix(edge_attributes, max_size)\n                )\n        self.num_features = self.node_feature_matrix[0].shape[1]\n\n    def __len__(self):\n        return len(self.target)\n\n    def __getitem__(self, index):\n        sample = {'adj_matrix': self.adj_matrix[index].astype('float32'),\n                  'node_feature_matrix':\n                      self.node_feature_matrix[index].astype('float32'),\n                  'labels': self.target[index].astype('float32')}\n        return sample\n"""
openchem/data/smiles_data_layer.py,1,"b'# TODO: packed variable length sequence\n\nimport numpy as np\n\nfrom torch.utils.data import Dataset\n\nfrom openchem.data.utils import read_smiles_property_file\nfrom openchem.data.utils import sanitize_smiles, pad_sequences, seq2tensor, canonize_smiles\nfrom openchem.data.utils import get_tokens, augment_smiles\n\n\nclass SmilesDataset(Dataset):\n    """"""\n    Creates dataset for SMILES-property data.\n    Args:\n        filename (str): string with full path to dataset file. Dataset file\n            must be csv file.\n        cols_to_read (list): list specifying columns to read from dataset file.\n            Could be of various length, `cols_to_read[0]` will be used as index\n            as index for column with SMILES, `cols_to_read[1:]` will be used as\n            indices for labels values.\n        delimiter (str): columns delimiter in `filename`. `default` is `,`.\n        tokens (list): list of unique tokens from SMILES. If not specified, will\n            be extracted from provided dataset.\n        pad (bool): argument specifying whether to pad SMILES. If `true` SMILES\n            will be padded from right and the flipped. `default` is `True`.\n        augment (bool): argument specifying whether to augment SMILES.\n\n    """"""\n    def __init__(self, filename, cols_to_read, delimiter=\',\', tokens=None,\n                 pad=True, tokenize=True, augment=False, flip=True):\n        super(SmilesDataset, self).__init__()\n        self.tokenize = tokenize\n        data = read_smiles_property_file(filename, cols_to_read, delimiter)\n        smiles = data[0]\n        clean_smiles, clean_idx = sanitize_smiles(smiles)\n        if len(data) > 1:\n            target = np.array(data[1:], dtype=\'float\')\n            target = np.array(target)\n            target = target.T\n            self.target = target[clean_idx]\n        else:\n            self.target = None\n        if augment:\n            clean_smiles, self.target = augment_smiles(clean_smiles,\n                                                       self.target)\n        if pad:\n            clean_smiles, self.length = pad_sequences(clean_smiles)\n        tokens, self.token2idx, self.num_tokens = get_tokens(clean_smiles,\n                                                                tokens)\n        if tokenize:\n            clean_smiles, self.tokens = seq2tensor(clean_smiles, tokens, flip)\n        self.data = clean_smiles\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        sample = {}\n        sample[\'tokenized_smiles\'] = self.data[index] \n        sample[\'length\'] = self.length[index]\n        if self.target is not None:\n            sample[\'labels\'] = self.target[index]\n        return sample\n'"
openchem/data/smiles_enumerator.py,0,"b'# Experimental Class for Smiles Enumeration, Iterator and SmilesIterator\n# adapted from Keras 1.2.2\nfrom rdkit import Chem\nimport numpy as np\nimport threading\n\n\nclass Iterator(object):\n    """"""Abstract base class for data iterators.\n    # Arguments\n        n: Integer, total number of samples in the dataset to loop over.\n        batch_size: Integer, size of a batch.\n        shuffle: Boolean, whether to shuffle the data between epochs.\n        seed: Random seeding for data shuffling.\n    """"""\n\n    def __init__(self, n, batch_size, shuffle, seed):\n        self.n = n\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.batch_index = 0\n        self.total_batches_seen = 0\n        self.lock = threading.Lock()\n        self.index_generator = self._flow_index(n, batch_size, shuffle, seed)\n        if n < batch_size:\n            raise ValueError(\'Input data length is shorter than batch_size\'\n                             \'Adjust batch_size\')\n\n    def reset(self):\n        self.batch_index = 0\n\n    def _flow_index(self, n, batch_size=32, shuffle=False, seed=None):\n        # Ensure self.batch_index is 0.\n        self.reset()\n        while 1:\n            if seed is not None:\n                np.random.seed(seed + self.total_batches_seen)\n            if self.batch_index == 0:\n                index_array = np.arange(n)\n                if shuffle:\n                    index_array = np.random.permutation(n)\n\n            current_index = (self.batch_index * batch_size) % n\n            if n > current_index + batch_size:\n                current_batch_size = batch_size\n                self.batch_index += 1\n            else:\n                current_batch_size = n - current_index\n                self.batch_index = 0\n            self.total_batches_seen += 1\n            yield (index_array[current_index: current_index +\n                               current_batch_size],\n                   current_index, current_batch_size)\n\n    def __iter__(self):\n        # Needed if we want to do something like:\n        # for x, y in data_gen.flow(...):\n        return self\n\n    def __next__(self, *args, **kwargs):\n        return self.next(*args, **kwargs)\n\n\nclass SmilesIterator(Iterator):\n    """"""Iterator yielding data from a SMILES array.\n    # Arguments\n        x: Numpy array of SMILES input data.\n        y: Numpy array of targets data.\n        smiles_data_generator: Instance of `SmilesEnumerator`\n            to use for random SMILES generation.\n        batch_size: Integer, size of a batch.\n        shuffle: Boolean, whether to shuffle the data between epochs.\n        seed: Random seed for data shuffling.\n        dtype: dtype to use for returned batch.\n        Set to keras.backend.floatx if using Keras\n    """"""\n\n    def __init__(self, x, y, smiles_data_generator,\n                 batch_size=32, shuffle=False, seed=None,\n                 dtype=np.float32\n                 ):\n        if y is not None and len(x) != len(y):\n            raise ValueError(\'X (images tensor) and y (labels) \'\n                             \'should have the same length. \'\n                             \'Found: X.shape = %s, y.shape = %s\' %\n                             (np.asarray(x).shape, np.asarray(y).shape))\n\n        self.x = np.asarray(x)\n\n        if y is not None:\n            self.y = np.asarray(y)\n        else:\n            self.y = None\n        self.smiles_data_generator = smiles_data_generator\n        self.dtype = dtype\n        super(SmilesIterator, self).__init__(x.shape[0], batch_size, shuffle,\n                                             seed)\n\n    def next(self):\n        """"""For python 2.x.\n        # Returns\n            The next batch.\n        """"""\n        # Keeps under lock only the mechanism which advances\n        # the indexing of each batch.\n        with self.lock:\n            index_array, current_index, current_batch_size =\\\n                next(self.index_generator)\n        # The transformation of images is not under thread lock\n        # so it can be done in parallel\n        batch_x = np.zeros(\n            tuple([current_batch_size] + [self.smiles_data_generator.pad,\n                                          self.smiles_data_generator._charlen]),\n            dtype=self.dtype)\n        for i, j in enumerate(index_array):\n            smiles = self.x[j:j + 1]\n            x = self.smiles_data_generator.transform(smiles)\n            batch_x[i] = x\n\n        if self.y is None:\n            return batch_x\n        batch_y = self.y[index_array]\n        return batch_x, batch_y\n\n\nclass SmilesEnumerator(object):\n    """"""SMILES Enumerator, vectorizer and devectorizer\n    #Arguments\n        charset: string containing the characters for the vectorization\n          can also be generated via the .fit() method\n        pad: Length of the vectorization\n        leftpad: Add spaces to the left of the SMILES\n        isomericSmiles: Generate SMILES containing information about stereogenic centers\n        enum: Enumerate the SMILES during transform\n        canonical: use canonical SMILES during transform (overrides enum)\n    """"""\n\n    def __init__(self, charset=\'@C)(=cOn1S2/H[N]\\\\\', pad=120, leftpad=True,\n                 isomericSmiles=True, enum=True,\n                 canonical=False):\n        self._charset = None\n        self.charset = charset\n        self.pad = pad\n        self.leftpad = leftpad\n        self.isomericSmiles = isomericSmiles\n        self.enumerate = enum\n        self.canonical = canonical\n\n    @property\n    def charset(self):\n        return self._charset\n\n    @charset.setter\n    def charset(self, charset):\n        self._charset = charset\n        self._charlen = len(charset)\n        self._char_to_int = dict((c, i) for i, c in enumerate(charset))\n        self._int_to_char = dict((i, c) for i, c in enumerate(charset))\n\n    def fit(self, smiles, extra_chars=[], extra_pad=5):\n        """"""Performs extraction of the charset and length of a SMILES datasets\n        and sets self.pad and self.charset\n        #Arguments\n            smiles: Numpy array or Pandas series containing smiles as strings\n            extra_chars: List of extra chars to add to the charset\n            (e.g. ""\\\\\\\\"" when ""/"" is present)\n            extra_pad: Extra padding to add before or after the\n            SMILES vectorization\n        """"""\n        charset = set("""".join(list(smiles)))\n        self.charset = """".join(charset.union(set(extra_chars)))\n        self.pad = max([len(smile) for smile in smiles]) + extra_pad\n\n    def randomize_smiles(self, smiles):\n        """"""Perform a randomization of a SMILES string\n        must be RDKit sanitizable""""""\n        m = Chem.MolFromSmiles(smiles)\n        ans = list(range(m.GetNumAtoms()))\n        np.random.shuffle(ans)\n        nm = Chem.RenumberAtoms(m, ans)\n        return Chem.MolToSmiles(nm, canonical=self.canonical,\n                                isomericSmiles=self.isomericSmiles)\n\n    def transform(self, smiles):\n        """"""Perform an enumeration (randomization) and vectorization of\n        a Numpy array of smiles strings\n        #Arguments\n            smiles: Numpy array or Pandas series containing smiles as strings\n        """"""\n        one_hot = np.zeros((smiles.shape[0], self.pad, self._charlen), dtype=np.int8)\n\n        for i, ss in enumerate(smiles):\n            if self.enumerate: ss = self.randomize_smiles(ss)\n            for j, c in enumerate(ss):\n                one_hot[i, j, self._char_to_int[c]] = 1\n        return one_hot\n\n    def reverse_transform(self, vect):\n        """""" Performs a conversion of a vectorized SMILES to a smiles strings\n        charset must be the same as used for vectorization.\n        #Arguments\n            vect: Numpy array of vectorized SMILES.\n        """"""\n        smiles = []\n        for v in vect:\n            # mask v\n            v = v[v.sum(axis=1) == 1]\n            # Find one hot encoded index with argmax, translate to char\n            # and join to string\n            smile = """".join(self._int_to_char[i] for i in v.argmax(axis=1))\n            smiles.append(smile)\n        return np.array(smiles)\n\n\nif __name__ == ""__main__"":\n    smiles = np.array([""CCC(=O)O[C@@]1(CC[NH+](C[C@H]1CC=C)C)c2ccccc2"",\n                       ""CCC[S@@](=O)c1ccc2c(c1)[nH]/c(=N/C(=O)OC)/[nH]2""] * 10\n                      )\n    # Test canonical SMILES vectorization\n    sm_en = SmilesEnumerator(canonical=True, enum=False)\n    sm_en.fit(smiles, extra_chars=[""\\\\""])\n    v = sm_en.transform(smiles)\n    transformed = sm_en.reverse_transform(v)\n    if len(set(transformed)) > 2:\n        print(""Too many different canonical SMILES generated"")\n\n    # Test enumeration\n    sm_en.canonical = False\n    sm_en.enumerate = True\n    v2 = sm_en.transform(smiles)\n    transformed = sm_en.reverse_transform(v2)\n    if len(set(transformed)) < 3: print(""Too few enumerated SMILES generated"")\n\n    # Reconstruction\n    reconstructed = sm_en.reverse_transform(v[0:5])\n    for i, smile in enumerate(reconstructed):\n        if smile != smiles[i]:\n            print(""Error in reconstruction %s %s"" % (smile, smiles[i]))\n            break\n\n    # test Pandas\n    import pandas as pd\n\n    df = pd.DataFrame(smiles)\n    v = sm_en.transform(df[0])\n    if v.shape != (20, 52, 18): print(""Possible error in pandas use"")\n\n    # BUG, when batchsize > x.shape[0], then it only returns x.shape[0]!\n    # Test batch generation\n    sm_it = SmilesIterator(smiles, np.array([1, 2] * 10), sm_en, batch_size=10,\n                           shuffle=True)\n    X, y = sm_it.next()\n    if sum(y == 1) - sum(y == 2) > 1:\n        print(""Unbalanced generation of batches"")\n    if len(X) != 10: print(""Error in batchsize generation"")\n'"
openchem/data/smiles_protein_data_layer.py,1,"b""# TODO: packed variable length sequence\n\nimport numpy as np\nimport pickle\n\nfrom torch.utils.data import Dataset\n\nfrom openchem.data.utils import read_smiles_property_file\nfrom openchem.data.utils import sanitize_smiles, pad_sequences, seq2tensor\nfrom openchem.data.utils import get_tokens\n\n\nclass SmilesProteinDataset(Dataset):\n    def __init__(self, filename, tokenized=False, cols_to_read=None,\n                 delimiter=',', mol_tokens=None, prot_tokens=None, pad=True):\n        super(SmilesProteinDataset, self).__init__()\n        if not tokenized:\n            data = read_smiles_property_file(filename, cols_to_read, delimiter)\n            smiles = data[0]\n            proteins = np.array(data[1])\n            target = np.array(data[2], dtype='float')\n            clean_smiles, clean_idx = sanitize_smiles(smiles)\n            self.target = target[clean_idx]\n            proteins = list(proteins[clean_idx])\n            if pad:\n                clean_smiles, self.mol_lengths = pad_sequences(clean_smiles)\n                proteins, self.prot_lengths = pad_sequences(proteins)\n            self.mol_tokens, self.mol_token2idx, self.mol_num_tokens = \\\n                get_tokens(clean_smiles, mol_tokens)\n            self.prot_tokens, self.prot_token2idx, self.prot_num_tokens = \\\n                get_tokens(proteins, prot_tokens)\n            clean_smiles = seq2tensor(clean_smiles, self.mol_tokens)\n            proteins = seq2tensor(proteins, self.prot_tokens)\n            self.molecules = clean_smiles\n            self.proteins = proteins\n        else:\n            f = open(filename, 'rb')\n            data = pickle.load(f)\n            self.mol_tokens = data['smiles_tokens']\n            self.prot_tokens = data['proteins_tokens']\n            self.mol_num_tokens = len(data['smiles_tokens'])\n            self.prot_num_tokens = len(data['proteins_tokens'])\n            self.molecules = data['smiles']\n            self.proteins = data['proteins']\n            self.target = data['labels']\n        assert len(self.molecules) == len(self.proteins)\n        assert len(self.molecules) == len(self.target)\n\n    def __len__(self):\n        return len(self.target)\n\n    def __getitem__(self, index):\n        sample = {'tokenized_smiles': self.molecules[index],\n                  'tokenized_protein': self.proteins[index],\n                  'labels': self.target[index],\n                  'mol_length': self.mol_lengths[index],\n                  'prot_length': self.prot_lengths[index]}\n        return sample\n\n"""
openchem/data/utils.py,1,"b'# TODO: packed variable length sequence\n\nimport time\nimport math\nimport numpy as np\nimport csv\nimport warnings\n\nfrom rdkit import Chem\n\nfrom torch.utils.data import DataLoader\nfrom openchem.data.smiles_enumerator import SmilesEnumerator\n\n\ndef cut_padding(samples, lengths, padding=\'left\'):\n    max_len = lengths.max(dim=0)[0].cpu().numpy()\n    if padding == \'right\':\n        cut_samples = samples[:, :max_len]\n    elif padding == \'left\':\n        total_len = samples.size()[1]\n        cut_samples = samples[:, total_len-max_len:]\n    else:\n        raise ValueError(\'Invalid value for padding argument. Must be right\'\n                         \'or left\')\n    return cut_samples\n\n\ndef seq2tensor(seqs, tokens, flip=True):\n    tensor = np.zeros((len(seqs), len(seqs[0])))\n    for i in range(len(seqs)):\n        for j in range(len(seqs[i])):\n            if seqs[i][j] in tokens:\n                tensor[i, j] = tokens.index(seqs[i][j])\n            else:\n                tokens = tokens + seqs[i][j]\n                tensor[i, j] = tokens.index(seqs[i][j])\n    if flip:\n        tensor = np.flip(tensor, axis=1).copy()\n    return tensor, tokens\n\n\ndef pad_sequences(seqs, max_length=None, pad_symbol=\' \'):\n    if max_length is None:\n        max_length = -1\n        for seq in seqs:\n            max_length = max(max_length, len(seq))\n    lengths = []\n    for i in range(len(seqs)):\n        cur_len = len(seqs[i])\n        lengths.append(cur_len)\n        seqs[i] = seqs[i] + pad_symbol*(max_length - cur_len)\n    return seqs, lengths\n\n\ndef create_loader(dataset, batch_size, shuffle=True, num_workers=1,\n                  pin_memory=False, sampler=None):\n    data_loader = DataLoader(dataset=dataset, batch_size=batch_size,\n                             shuffle=shuffle, num_workers=num_workers,\n                             pin_memory=pin_memory, sampler=sampler)\n    return data_loader\n\n\ndef sanitize_smiles(smiles, canonize=True):\n    """"""\n    Takes list of SMILES strings and returns list of their sanitized versions.\n    For definition of sanitized SMILES check\n    http://www.rdkit.org/docs/api/rdkit.Chem.rdmolops-module.html#SanitizeMol\n        Args:\n            smiles (list): list of SMILES strings\n            canonize (bool): parameter specifying whether to return\n            canonical SMILES or not.\n        Output:\n            new_smiles (list): list of SMILES and NaNs if SMILES string is\n            invalid or unsanitized.\n            If \'canonize = True\', return list of canonical SMILES.\n        When \'canonize = True\' the function is analogous to:\n        canonize_smiles(smiles, sanitize=True).\n    """"""\n    new_smiles = []\n    idx = []\n    for i in range(len(smiles)):\n        sm = smiles[i]\n        try:\n            if canonize:\n                new_smiles.append(\n                    Chem.MolToSmiles(Chem.MolFromSmiles(sm, sanitize=False))\n                )\n                idx.append(i)\n            else:\n                new_smiles.append(sm)\n                idx.append(i)\n        except: \n            warnings.warn(\'Unsanitized SMILES string: \' + sm)\n            new_smiles.append(\'\')\n    return new_smiles, idx\n\n\ndef canonize_smiles(smiles, sanitize=True):\n    """"""\n    Takes list of SMILES strings and returns list of their canonical SMILES.\n        Args:\n            smiles (list): list of SMILES strings\n            sanitize (bool): parameter specifying whether to sanitize\n            SMILES or not.\n            For definition of sanitized SMILES check\n            www.rdkit.org/docs/api/rdkit.Chem.rdmolops-module.html#SanitizeMol\n        Output:\n            new_smiles (list): list of canonical SMILES and NaNs\n            if SMILES string is invalid or unsanitized\n            (when \'sanitize = True\')\n        When \'sanitize = True\' the function is analogous to:\n        sanitize_smiles(smiles, canonize=True).\n    """"""\n    new_smiles = []\n    for sm in smiles:\n        try:\n            new_smiles.append(\n                Chem.MolToSmiles(Chem.MolFromSmiles(sm, sanitize=sanitize))\n            )\n        except: \n            warnings.warn(sm + \' can not be canonized: i\'\n                                \'nvalid SMILES string!\')\n            new_smiles.append(\'\')\n    return new_smiles\n\n\ndef save_smi_to_file(filename, smiles, unique=True):\n    """"""\n    Takes path to file and list of SMILES strings and writes SMILES\n    to the specified file.\n        Args:\n            filename (str): path to the file\n            smiles (list): list of SMILES strings\n            unique (bool): parameter specifying whether to write\n            only unique copies or not.\n        Output:\n            success (bool): defines whether operation\n            was successfully completed or not.\n       """"""\n    if unique:\n        smiles = list(set(smiles))\n    else:\n        smiles = list(smiles)\n    f = open(filename, \'w\')\n    for mol in smiles:\n        f.writelines([mol, \'\\n\'])\n    f.close()\n    return f.closed\n\n\ndef read_smi_file(filename, unique=True):\n    """"""\n    Reads SMILES from file. File must contain one SMILES string per line\n    with \\n token in the end of the line.\n    Args:\n        filename (str): path to the file\n        unique (bool): return only unique SMILES\n    Returns:\n        smiles (list): list of SMILES strings from specified file.\n        success (bool): defines whether operation\n        was successfully completed or not.\n    If \'unique=True\' this list contains only unique copies.\n    """"""\n    f = open(filename, \'r\')\n    molecules = []\n    for line in f:\n        molecules.append(line[:-1])\n    if unique:\n        molecules = list(set(molecules))\n    else:\n        molecules = list(molecules)\n    f.close()\n    return molecules, f.closed\n\n\ndef get_tokens(smiles, tokens=None):\n    """"""\n    Returns list of unique tokens, token-2-index dictionary and\n    number of unique tokens from the list of SMILES\n    Args:\n        smiles (list): list of SMILES strings to tokenize.\n        tokens (string): string of tokens or None.\n        If none will be extracted from dataset.\n    Returns:\n        tokens (list): list of unique tokens/SMILES alphabet.\n        token2idx (dict): dictionary mapping token to its index.\n        num_tokens (int): number of unique tokens.\n    """"""\n    if tokens is None:\n        tokens = list(set(\'\'.join(smiles)))\n        tokens = np.sort(tokens)[::-1]\n        tokens = \'\'.join(tokens)\n    token2idx = dict((token, i) for i, token in enumerate(tokens))\n    num_tokens = len(tokens)\n    return tokens, token2idx, num_tokens\n\n\ndef augment_smiles(smiles, labels, n_augment=5):\n    smiles_augmentation = SmilesEnumerator()\n    augmented_smiles = []\n    augmented_labels = []\n    for i in range(len(smiles)):\n        sm = smiles[i]\n        for _ in range(n_augment):\n            augmented_smiles.append(smiles_augmentation.randomize_smiles(sm))\n            augmented_labels.append(labels[i])\n        augmented_smiles.append(sm)\n        augmented_labels.append(labels[i])\n    return augmented_smiles, augmented_labels\n\n\ndef time_since(since):\n    s = time.time() - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return \'%dm %ds\' % (m, s)\n\n\ndef read_smiles_property_file(path, cols_to_read, delimiter=\',\',\n                              keep_header=False):\n    reader = csv.reader(open(path, \'r\'), delimiter=delimiter)\n    data_full = np.array(list(reader))\n    if keep_header:\n        start_position = 0\n    else:\n        start_position = 1\n    assert len(data_full) > start_position\n    data = [[] for _ in range(len(cols_to_read))]\n    for i in range(len(cols_to_read)):\n        col = cols_to_read[i]\n        data[i] = data_full[start_position:, col]\n\n    return data\n\n\ndef save_smiles_property_file(path, smiles, labels, delimiter=\',\'):\n    f = open(path, \'w\')\n    n_targets = labels.shape[1]\n    for i in range(len(smiles)):\n        f.writelines(smiles[i])\n        for j in range(n_targets):\n            f.writelines(delimiter + str(labels[i, j]))\n        f.writelines(\'\\n\')\n    f.close()\n'"
openchem/data/vanilla_data_layer.py,0,"b""import numpy as np\nfrom openchem.data.utils import sanitize_smiles, read_smiles_property_file\n\n\nclass VanillaDataset(object):\n    def __init__(self, filename, cols_to_read, features, delimiter=',',\n                 tokens=None):\n        super(VanillaDataset, self).__init__()\n        data = read_smiles_property_file(filename, cols_to_read, delimiter)\n        smiles = data[0]\n        target = np.array(data[1], dtype='float')\n        clean_smiles, clean_idx = sanitize_smiles(smiles)\n        target = np.array(target)\n        self.target = target[clean_idx]\n\n\n    def __len__(self):\n        return len(self.target)\n\n    def __getitem__(self, index):\n        sample = {'tokenized_smiles': self.data[index],\n                  'labels': self.target[index]}\n        return sample"""
openchem/layers/conv_bn_relu.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ConvBNReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True):\n        super(ConvBNReLU, self).__init__()\n        self.kernel_size = kernel_size\n        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride,\n                              padding, dilation, groups, bias)\n        self.bn = nn.BatchNorm1d(out_channels)\n        self.relu = F.relu\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n\n        return x\n'"
openchem/layers/gcn.py,6,"b'# modified from https://github.com/tkipf/pygcn/blob/master/pygcn/layers.py\n\n\nimport math\n\nimport torch\nimport torch.nn as nn\n\nfrom torch.nn.parameter import Parameter\n\n\nclass GraphConvolution(nn.Module):\n    """"""\n    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n    """"""\n\n    def __init__(self, in_features, out_features, bias=True):\n        super(GraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n        if bias:\n            self.bias = Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter(\'bias\', None)\n        self.reset_parameters()\n        self.bn = nn.BatchNorm1d(out_features)\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, x, adj):\n        support = torch.bmm(adj, x)\n        result = torch.mm(support.view(-1, self.in_features), self.weight)\n        output = result.view(-1, adj.data.shape[1], self.out_features)\n        if self.bias is not None:\n            output = output + self.bias\n        output = output.transpose(1, 2).contiguous()\n        output = self.bn(output)\n        output = output.transpose(1, 2)\n        return output\n\n    def __repr__(self):\n        return self.__class__.__name__ + \' (\' \\\n               + str(self.in_features) + \' -> \' \\\n               + str(self.out_features) + \')\'\n'"
openchem/layers/stack_augmentation.py,10,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass StackAugmentation(nn.Module):\n    def __init__(self, stack_width, stack_depth, in_features, use_cuda):\n        super(StackAugmentation, self).__init__()\n        self.use_cuda = use_cuda\n        self.stack_width = stack_width\n        self.stack_depth = stack_depth\n        self.in_features = in_features\n        self.stack_controls_layer = nn.Linear(in_features=in_features,\n                                              out_features=3)\n\n        self.stack_input_layer = nn.Linear(in_features=in_features,\n                                           out_features=self.stack_width)\n\n    def forward(self, input_val, prev_stack):\n        batch_size = prev_stack.size(0)\n\n        controls = self.stack_controls_layer(input_val.squeeze(0))\n        controls = F.softmax(controls, dim=1)\n        controls = controls.view(-1, 3, 1, 1)\n        stack_input = self.stack_input_layer(input_val)\n        stack_input = F.tanh(stack_input)\n        stack_input = stack_input.permute(1, 0, 2)\n        zeros_at_the_bottom = torch.zeros(batch_size, 1, self.stack_width)\n        if self.use_cuda:\n            zeros_at_the_bottom = torch.tensor(zeros_at_the_bottom.cuda(),\n                                               requires_grad=True)\n        else:\n            zeros_at_the_bottom = torch.tensor(zeros_at_the_bottom,\n                                               requires_grad=True)\n        a_push, a_pop, a_no_op = controls[:, 0], controls[:, 1], controls[:, 2]\n        stack_down = torch.cat((prev_stack[:, 1:], zeros_at_the_bottom), dim=1)\n        stack_up = torch.cat((stack_input, prev_stack[:, :-1]), dim=1)\n        new_stack = a_no_op * prev_stack + a_push * stack_up + \\\n                    a_pop * stack_down\n        return new_stack\n\n    def init_stack(self, batch_size):\n        result = torch.zeros(batch_size, self.stack_depth, self.stack_width)\n        if self.use_cuda:\n            return torch.tensor(result.cuda(), requires_grad=True)\n        else:\n            return torch.tensor(result, requires_grad=True)\n\n\n'"
openchem/models/GenerativeRNN.py,9,"b'from openchem.models.openchem_model import OpenChemModel\nfrom openchem.layers.stack_augmentation import StackAugmentation\nfrom openchem.data.utils import seq2tensor, cut_padding\n\nimport torch\nimport numpy as np\n\n\nclass GenerativeRNN(OpenChemModel):\n    def __init__(self, params):\n        super(GenerativeRNN, self).__init__(params)\n        self.has_stack = params[\'has_stack\']\n        if self.has_stack:\n            self.Stack = StackAugmentation(use_cuda=self.use_cuda,\n                                           **self.params[\'stack_params\'])\n        self.embedding = self.params[\'embedding\']\n        self.embed_params = self.params[\'embedding_params\']\n        self.Embedding = self.embedding(self.embed_params)\n        self.encoder = self.params[\'encoder\']\n        self.encoder_params = self.params[\'encoder_params\']\n        self.Encoder = self.encoder(self.encoder_params, self.use_cuda)\n        self.mlp = self.params[\'mlp\']\n        self.mlp_params = self.params[\'mlp_params\']\n        self.MLP = self.mlp(self.mlp_params)\n\n    def forward(self, inp_seq, eval=False):\n        """"""Generator forward function.""""""\n        if eval:\n            self.eval()\n        else:\n            self.train()\n        batch_size = inp_seq.size()[0]\n        seq_len = inp_seq.size()[1]\n        n_classes = self.MLP.hidden_size[-1]\n        result = torch.zeros(batch_size, seq_len, n_classes,\n                             requires_grad=True)\n        if self.use_cuda:\n            result = result.cuda()\n        hidden = self.Encoder.init_hidden(batch_size)\n        if self.has_stack:\n            stack = self.Stack.init_stack(batch_size)\n        for c in range(seq_len):\n            inp_token = self.Embedding(inp_seq[:, c].view(batch_size, -1))\n            if self.has_stack:\n                stack = self.Stack(hidden, stack)\n                stack_top = stack[:, 0, :].unsqueeze(1)\n                inp_token = torch.cat((inp_token, stack_top), dim=2)\n            output, hidden = self.Encoder(inp_token, hidden)\n            result[:, c, :] = self.MLP(output)\n\n        return result.view(-1, n_classes)\n\n    def infer(self, prime_str, n_to_generate, max_len, tokens, temperature=0.8):\n        self.eval()\n        tokens = np.array(tokens).reshape(-1)\n        prime_str = [prime_str] * n_to_generate\n        tokens = list(tokens[0])\n        num_tokens = len(tokens)\n        prime_input = seq2tensor(prime_str, tokens)\n        tokens = np.array(tokens)\n        batch_size = prime_input.shape[0]\n        seq_len = prime_input.shape[1] - 1\n        hidden = self.Encoder.init_hidden(batch_size)\n        prime_input = torch.tensor(prime_input).long()\n        if self.use_cuda:\n            prime_input = prime_input.cuda()\n        if self.has_stack:\n            stack = self.Stack.init_stack(batch_size)\n        for c in range(seq_len):\n            inp_token = self.Embedding(prime_input[:, c].view(batch_size, -1))\n            if self.has_stack:\n                stack = self.Stack(hidden, stack)\n                stack_top = stack[:, 0, :].unsqueeze(1)\n                inp_token = torch.cat((inp_token, stack_top), dim=2)\n            output, hidden = self.Encoder(inp_token, hidden)\n        inp = prime_input[:, -1]\n        predicted = [\' \'] * (batch_size * (max_len - seq_len))\n        predicted = np.reshape(predicted, (batch_size, max_len - seq_len))\n        for c in range(max_len - seq_len):\n            inp_token = self.Embedding(inp.view(batch_size, -1))\n            if self.has_stack:\n                stack = self.Stack(hidden, stack)\n                stack_top = stack[:, 0, :].unsqueeze(1)\n                inp_token = torch.cat((inp_token, stack_top), dim=2)\n            output, hidden = self.Encoder(inp_token, hidden)\n            output = self.MLP(output)\n            output_dist = output.data.view(-1).div(temperature).exp()\n            output_dist = output_dist.view(batch_size, num_tokens)\n            top_i = torch.multinomial(output_dist, 1)\n            # Add predicted character to string and use as next input\n            predicted_char = tokens[top_i]\n            predicted[:, c] = predicted_char[:, 0]\n            inp = torch.tensor(top_i)\n\n        return predicted\n\n    def cast_inputs(self, sample):\n        sample_seq = sample[\'tokenized_smiles\']\n        lengths = sample[\'length\']\n        max_len = lengths.max(dim=0)[0].cpu().numpy()\n        batch_size = len(lengths)\n        sample_seq = cut_padding(sample_seq, lengths, padding=\'right\')\n        target = sample_seq[:, 1:].contiguous().view(\n            (batch_size * (max_len - 1), 1))\n        seq = sample_seq[:, :-1]\n        seq = torch.tensor(seq, requires_grad=True).long()\n        target = torch.tensor(target).long()\n        seq = seq.cuda()\n        target = target.cuda()\n        return seq, target.squeeze(1)\n\n'"
openchem/models/Graph2Label.py,3,"b'from openchem.models.openchem_model import OpenChemModel\n\nimport torch\n\n\nclass Graph2Label(OpenChemModel):\n    r""""""\n    Creates a model that predicts one or multiple labels given object of\n    class graph as input. Consists of \'graph convolution neural network\n    encoder\'__, followed by \'graph max pooling layer\'__ and\n    multilayer perceptron.\n\n    __https://arxiv.org/abs/1609.02907\n    __https://pubs.acs.org/doi/full/10.1021/acscentsci.6b00367\n\n    Args:\n        params (dict): dictionary of parameters describing the model\n            architecture.\n\n    """"""\n    def __init__(self, params):\n        super(Graph2Label, self).__init__(params)\n        self.encoder = self.params[\'encoder\']\n        self.encoder_params = self.params[\'encoder_params\']\n        self.Encoder = self.encoder(self.encoder_params, self.use_cuda)\n        self.mlp = self.params[\'mlp\']\n        self.mlp_params = self.params[\'mlp_params\']\n        self.MLP = self.mlp(self.mlp_params)\n\n    def forward(self, inp, eval=False):\n        if eval:\n            self.eval()\n        else:\n            self.train()\n        output = self.Encoder(inp)\n        output = self.MLP(output)\n        return output\n\n    def cast_inputs(self, sample):\n        batch_adj = torch.tensor(sample[\'adj_matrix\'],\n                                 requires_grad=True).float()\n        batch_x = torch.tensor(sample[\'node_feature_matrix\'],\n                               requires_grad=True).float()\n        batch_labels = torch.tensor(sample[\'labels\'])\n        if self.task == \'classification\':\n            batch_labels = batch_labels.long()\n        else:\n            batch_labels = batch_labels.float()\n        if self.use_cuda:\n            batch_x = batch_x.cuda()\n            batch_adj = batch_adj.cuda()\n            batch_labels = batch_labels.cuda()\n        batch_inp = (batch_x, batch_adj)\n        return batch_inp, batch_labels\n'"
openchem/models/MoleculeProtein2Label.py,4,"b'from openchem.models.openchem_model import OpenChemModel\nfrom openchem.optimizer.openchem_optimizer import OpenChemOptimizer\nfrom openchem.optimizer.openchem_lr_scheduler import OpenChemLRScheduler\nfrom openchem.data.utils import cut_padding\n\n\nimport torch\n\n\nclass MoleculeProtein2Label(OpenChemModel):\n    r""""""\n    Creates a model that predicts one or multiple labels given two sequences as\n    input. Embeddings for each input are extracted separately with Embedding\n    layer, followed by encoder (could be RNN or CNN encoder) and then merged\n    together. Last layer of the model is multi-layer perceptron.\n\n    Args:\n        params (dict): dictionary describing model architecture.\n\n    """"""\n    def __init__(self, params):\n        super(MoleculeProtein2Label, self).__init__(params)\n        self.mol_embedding = self.params[\'mol_embedding\']\n        self.mol_embed_params = self.params[\'mol_embedding_params\']\n        self.prot_embedding = self.params[\'prot_embedding\']\n        self.prot_embed_params = self.params[\'prot_embedding_params\']\n        self.MolEmbedding = self.mol_embedding(self.mol_embed_params)\n        self.ProtEmbedding = self.prot_embedding(self.prot_embed_params)\n        self.mol_encoder = self.params[\'mol_encoder\']\n        self.mol_encoder_params = self.params[\'mol_encoder_params\']\n        self.prot_encoder = self.params[\'prot_encoder\']\n        self.prot_encoder_params = self.params[\'prot_encoder_params\']\n        self.MolEncoder = self.mol_encoder(self.mol_encoder_params,\n                                           self.use_cuda)\n        self.ProtEncoder = self.prot_encoder(self.prot_encoder_params,\n                                             self.use_cuda)\n        self.merge = self.params[\'merge\']\n        self.mlp = self.params[\'mlp\']\n        self.mlp_params = self.params[\'mlp_params\']\n        self.MLP = self.mlp(self.mlp_params)\n\n    def forward(self, inp, eval=False):\n        if eval:\n            self.eval()\n        else:\n            self.train()\n        mol = inp[0]\n        prot = inp[1]\n        mol_embedded = self.MolEmbedding(mol)\n        mol_output, _ = self.MolEncoder(mol_embedded)\n        prot_embedded = self.ProtEmbedding(prot)\n        prot_output, _ = self.ProtEncoder(prot_embedded)\n        if self.merge == \'mul\':\n            output = mol_output*prot_output\n        elif self.merge == \'concat\':\n            output = torch.cat((mol_output, prot_output), 1)\n        else:\n            raise ValueError(\'Invalid value for merge\')\n        output = self.MLP(output)\n        return output\n\n    def cast_inputs(self, sample):\n        batch_mols = cut_padding(sample[\'tokenized_smiles\'], sample[\'mol_length\'],\n                                  padding=\'left\')\n        batch_prots = cut_padding(sample[\'tokenized_protein\'], sample[\'prot_length\'],\n                                  padding=\'left\')\n        batch_mols = torch.tensor(batch_mols, requires_grad=True).long()\n        batch_prots = torch.tensor(batch_prots, requires_grad=True).long()\n        batch_labels = torch.tensor(sample[\'labels\'])\n        if self.task == \'classification\':\n            batch_labels = batch_labels.long()\n        elif self.task == \'regression\':\n            batch_labels = batch_labels.float()\n        if self.use_cuda:\n            batch_mols = batch_mols.cuda()\n            batch_prots = batch_prots.cuda()\n            batch_labels = batch_labels.cuda()\n        return (batch_mols, batch_prots), batch_labels\n'"
openchem/models/Smiles2Label.py,2,"b'from openchem.models.openchem_model import OpenChemModel\nfrom openchem.optimizer.openchem_optimizer import OpenChemOptimizer\nfrom openchem.optimizer.openchem_lr_scheduler import OpenChemLRScheduler\n\nimport torch\n\n\nclass Smiles2Label(OpenChemModel):\n    r""""""\n    Creates a model that predicts one or multiple labels given string of\n    characters as input. Embeddings for input sequences are extracted with\n    Embedding layer, followed by encoder (could be RNN or CNN encoder).\n    Last layer of the model is multi-layer perceptron.\n\n    Args:\n        params (dict): dictionary describing model architecture.\n\n    """"""\n    def __init__(self, params):\n        super(Smiles2Label, self).__init__(params)\n        self.embedding = self.params[\'embedding\']\n        self.embed_params = self.params[\'embedding_params\']\n        self.Embedding = self.embedding(self.embed_params)\n        self.encoder = self.params[\'encoder\']\n        self.encoder_params = self.params[\'encoder_params\']\n        self.Encoder = self.encoder(self.encoder_params, self.use_cuda)\n        self.mlp = self.params[\'mlp\']\n        self.mlp_params = self.params[\'mlp_params\']\n        self.MLP = self.mlp(self.mlp_params)\n        self.optimizer = OpenChemOptimizer([self.params[\'optimizer\'],\n                                            self.params[\'optimizer_params\']],\n                                           self.parameters())\n        self.scheduler = OpenChemLRScheduler([self.params[\'lr_scheduler\'],\n                                              self.params[\'lr_scheduler_params\']\n                                              ],\n                                             self.optimizer.optimizer)\n\n    def forward(self, inp, eval=False):\n        if eval:\n            self.eval()\n        else:\n            self.train()\n        embedded = self.Embedding(inp)\n        output, _ = self.Encoder(embedded)\n        output = self.MLP(output)\n        return output\n\n    def cast_inputs(self, sample):\n        batch_mols = torch.tensor(sample[\'tokenized_smiles\'],\n                                  requires_grad=True).long()\n        batch_labels = torch.tensor(sample[\'labels\']).float()\n        if self.task == \'classification\':\n            batch_labels = batch_labels.long()\n        if self.use_cuda:\n            batch_mols = batch_mols.cuda()\n            batch_labels = batch_labels.cuda()\n        return batch_mols, batch_labels\n'"
openchem/models/openchem_model.py,6,"b'import torch\nfrom torch import nn\nfrom torch.nn.utils import clip_grad_norm_\nimport torch.distributed as dist\n\nfrom openchem.utils.utils import check_params\n\nimport time\n\nfrom openchem.utils.logger import Logger\nfrom openchem.utils.utils import time_since, calculate_metrics\nfrom openchem.optimizer.openchem_optimizer import OpenChemOptimizer\nfrom openchem.optimizer.openchem_lr_scheduler import OpenChemLRScheduler\n\nimport numpy as np\n\n\nclass OpenChemModel(nn.Module):\n    """"""Base class for all OpenChem models. Function :func:\'forward\' and\n    :func:\'cast\' inputs must be overridden for every class, that inherits from\n    OpenChemModel.\n    """"""\n    def __init__(self, params):\n        super(OpenChemModel, self).__init__()\n        check_params(params, self.get_required_params(),\n                     self.get_optional_params())\n        self.params = params\n        self.use_cuda = self.params[\'use_cuda\']\n        self.batch_size = self.params[\'batch_size\']\n        self.eval_metrics = self.params[\'eval_metrics\']\n        self.task = self.params[\'task\']\n        self.logdir = self.params[\'logdir\']\n        self.world_size = self.params[\'world_size\']\n\n        self.num_epochs = self.params[\'num_epochs\']\n        self.use_clip_grad = self.params[\'use_clip_grad\']\n        if self.use_clip_grad:\n            self.max_grad_norm = self.params[\'max_grad_norm\']\n        else:\n            self.max_grad_norm = None\n        self.random_seed = self.params[\'random_seed\']\n        self.print_every = self.params[\'print_every\']\n        self.save_every = self.params[\'save_every\']\n\n    @staticmethod\n    def get_required_params():\n        return{\n            \'task\': str,\n            \'batch_size\': int,\n            \'num_epochs\': int,\n            \'train_data_layer\': None,\n            \'val_data_layer\': None,\n        }\n\n    @staticmethod\n    def get_optional_params():\n        return{\n            \'use_cuda\': bool,\n            \'use_clip_grad\': bool,\n            \'max_grad_norm\': float,\n            \'random_seed\': int,\n            \'print_every\': int,\n            \'save_every\': int,\n            \'lr_scheduler\': None,\n            \'lr_scheduler_params\': dict,\n            \'eval_metrics\': None,\n            \'logdir\': str\n        }\n\n    def forward(self, inp, eval=False):\n        raise NotImplementedError\n\n    def cast_inputs(self, sample):\n        raise NotImplementedError\n\n    def load_model(self, path):\n        weights = torch.load(path)\n        self.load_state_dict(weights)\n\n    def save_model(self, path):\n        torch.save(self.state_dict(), path)\n\n        \ndef build_training(model, params):\n\n    optimizer = OpenChemOptimizer([params[\'optimizer\'],\n                                   params[\'optimizer_params\']],\n                                  model.parameters())\n    lr_scheduler = OpenChemLRScheduler([params[\'lr_scheduler\'],\n                                        params[\'lr_scheduler_params\']],\n                                       optimizer.optimizer)\n    use_cuda = params[\'use_cuda\']\n    criterion = params[\'criterion\']\n    if use_cuda:\n        criterion = criterion.cuda()\n    # train_loader = params[\'train_loader\']\n    # val_loader = params[\'val_loader\']\n    return criterion, optimizer, lr_scheduler #, train_loader, val_loader\n\n\ndef train_step(model, optimizer, criterion, inp, target):\n    optimizer.zero_grad()\n    output = model.forward(inp, eval=False)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    has_module = False\n    if hasattr(model, \'module\'):\n        has_module = True\n    if has_module:\n        use_clip_grad = model.module.use_clip_grad\n        max_grad_norm = model.module.max_grad_norm\n    else:\n        use_clip_grad = model.use_clip_grad\n        max_grad_norm = model.max_grad_norm\n    if use_clip_grad:\n        clip_grad_norm_(model.parameters(), max_grad_norm)\n\n    return loss\n\n\ndef print_logs(world_size):\n    if world_size == 1:\n        return True\n    elif torch.distributed.get_rank() == 0:\n        return True\n    else:\n        return False\n\n\ndef fit(model, scheduler, train_loader, optimizer, criterion, params,\n        eval=False, val_loader=None):\n    cur_epoch = 0\n    logdir = params[\'logdir\']\n    print_every = params[\'print_every\']\n    save_every = params[\'save_every\']\n    n_epochs = params[\'num_epochs\']\n    logger = Logger(logdir + \'/tensorboard_log/\')\n    start = time.time()\n    loss_total = 0\n    n_batches = 0\n    scheduler = scheduler.scheduler\n    all_losses = []\n    val_losses = []\n    has_module = False\n    if hasattr(model, \'module\'):\n        has_module = True\n    if has_module:\n        world_size = model.module.world_size\n    else:\n        world_size = model.world_size     \n\n    for epoch in range(cur_epoch, n_epochs + cur_epoch):\n        for i_batch, sample_batched in enumerate(train_loader):\n            if has_module:\n                batch_input, batch_target = model.module.cast_inputs(sample_batched)\n            else:\n                batch_input, batch_target = model.cast_inputs(sample_batched)\n            loss = train_step(model, optimizer, criterion,\n                              batch_input, batch_target)\n            if world_size > 1:\n                reduced_loss = reduce_tensor(loss, world_size)\n            else:\n                reduced_loss = loss.clone()\n            loss_total += reduced_loss.item()\n            n_batches += 1\n        cur_loss = loss_total / n_batches\n        all_losses.append(cur_loss)\n\n        if epoch % print_every == 0:\n            if print_logs(world_size):\n                print(\'TRAINING: [Time: %s, Epoch: %d, Progress: %d%%, \'\n                      \'Loss: %.4f]\' % (time_since(start), epoch,\n                                       epoch / n_epochs * 100, cur_loss))\n            if eval:\n                assert val_loader is not None\n                val_loss, metrics = evaluate(model, val_loader, criterion)\n                val_losses.append(val_loss)\n                info = {\'Train loss\': cur_loss, \'Validation loss\': val_loss,\n                        \'Validation metrics\': metrics,\n                        \'LR\': optimizer.param_groups[0][\'lr\']}\n            else:\n                info = {\'Train loss\': cur_loss,\n                        \'LR\': optimizer.param_groups[0][\'lr\']}\n\n            if print_logs(world_size):\n                for tag, value in info.items():\n                    logger.scalar_summary(tag, value, epoch + 1)\n\n                for tag, value in model.named_parameters():\n                    tag = tag.replace(\'.\', \'/\')\n                    try:\n                        logger.histo_summary(tag, value.detach().cpu().numpy(),\n                                         epoch + 1)\n                        logger.histo_summary(tag + \'/grad\',\n                                         value.grad.detach().cpu().numpy(),\n                                         epoch + 1)\n                    except:\n                        pass\n\n        if epoch % save_every == 0 and print_logs(world_size):\n            torch.save(model.state_dict(), logdir + \'/checkpoint/epoch_\' + str(epoch))\n\n        loss_total = 0\n        n_batches = 0\n        scheduler.step()\n\n    return all_losses, val_losses\n\n\ndef evaluate(model, val_loader, criterion):\n    loss_total = 0\n    n_batches = 0\n    start = time.time()\n    prediction = []\n    ground_truth = []\n    has_module = False\n    if hasattr(model, \'module\'):\n        has_module = True\n    if has_module:\n        task = model.module.task\n        eval_metrics = model.module.eval_metrics\n        world_size = model.module.world_size\n    else:\n        task = model.task\n        eval_metrics = model.eval_metrics\n        world_size = model.world_size\n    for i_batch, sample_batched in enumerate(val_loader):\n        if has_module:\n            batch_input, batch_target = model.module.cast_inputs(sample_batched)\n        else:\n            batch_input, batch_target = model.cast_inputs(sample_batched)\n        predicted = model.forward(batch_input, eval=True)\n        prediction += list(predicted.detach().cpu().numpy())\n        ground_truth += list(batch_target.cpu().numpy())\n        loss = criterion(predicted, batch_target)\n        loss_total += loss.item()\n        n_batches += 1\n\n    cur_loss = loss_total / n_batches\n    if task == \'classification\':\n        prediction = np.argmax(prediction, axis=1)\n    metrics = calculate_metrics(prediction, ground_truth,\n                                eval_metrics)\n    if print_logs(world_size):\n        print(\'EVALUATION: [Time: %s, Loss: %.4f, Metrics: %.4f]\' %\n              (time_since(start), cur_loss, metrics))\n    return cur_loss, metrics\n\n\ndef reduce_tensor(tensor, world_size):\n    r""""""\n    Reduces input \'\'tensor\'\' across all processes in such a way that everyone\n    gets the sum of \'\'tensor\'\' from all of the processes.\n    Args:\n        tensor (Tensor): data to be reduced.\n        world_size (int): number of processes.\n    """"""\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n    rt /= world_size\n    return rt\n'"
openchem/models/vanilla_model.py,0,"b""from __future__ import print_function\nfrom __future__ import division\n\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.ensemble import RandomForestClassifier as RFC\nfrom sklearn.svm import SVC\nfrom sklearn.svm import SVR\nfrom sklearn.externals import joblib\nfrom sklearn import metrics\n\nfrom data import get_fp, get_desc, normalize_desc, cross_validation_split\n\nfrom mordred import Calculator, descriptors\n\n\nclass RandomForestQSAR(object):\n    def __init__(self, model_type='classifier', feature_type='fingerprints',\n                 n_estimators=100, n_ensemble=5):\n        super(RandomForestQSAR, self).__init__()\n        self.n_estimators = n_estimators\n        self.n_ensemble = n_ensemble\n        self.model = []\n        self.model_type = model_type\n        if self.model_type == 'classifier':\n            for i in range(n_ensemble):\n                self.model.append(RFC(n_estimators=n_estimators))\n        elif self.model_type == 'regressor':\n            for i in range(n_ensemble):\n                self.model.append(RFR(n_estimators=n_estimators))\n        else:\n            raise ValueError('invalid value for argument')\n        self.feature_type = feature_type\n        if self.feature_type == 'descriptors':\n            self.calc = Calculator(descriptors, ignore_3D=True)\n            self.desc_mean = [0]*self.n_ensemble\n\n    def load_model(self, path):\n        self.model = []\n        for i in range(self.n_ensemble):\n            m = joblib.load(path + str(i) + '.pkl')\n            self.model.append(m)\n        if self.feature_type == 'descriptors':\n            arr = np.load(path + 'desc_mean.npy', 'rb')\n            self.desc_mean = arr\n\n    def save_model(self, path):\n        assert self.n_ensemble == len(self.model)\n        for i in range(self.n_ensemble):\n            joblib.dump(self.model[i], path + str(i) + '.pkl')\n        if self.feature_type == 'descriptors':\n            np.save(path + 'desc_mean.npy', self.desc_mean)\n\n    def fit_model(self, data):\n        eval_metrics = []\n        if self.feature_type == 'fingerprints':\n            fps = get_fp(data.smiles)\n        elif self.feature_type == 'descriptors':\n            fps, _, _ = get_desc(data.smiles, self.calc)\n        if self.model_type == 'classifier':\n            cross_val_data, cross_val_labels = \\\n                cross_validation_split(fps, data.binary_labels)\n        elif self.model_type == 'regressor':\n            cross_val_data, cross_val_labels = \\\n                cross_validation_split(fps, data.property)\n        for i in range(self.n_ensemble):\n            train_sm = np.concatenate(cross_val_data[:i] + cross_val_data[(i + 1):])\n            test_sm = cross_val_data[i]\n            train_labels = np.concatenate(cross_val_labels[:i] +\n                                          cross_val_labels[(i + 1):])\n            test_labels = cross_val_labels[i]\n            if self.feature_type == 'descriptors':\n                train_sm, desc_mean = normalize_desc(train_sm)\n                self.desc_mean[i] = desc_mean\n                test_sm, _ = normalize_desc(test_sm, desc_mean)\n            self.model[i].fit(train_sm, train_labels.ravel())\n            predicted = self.model[i].predict(test_sm)\n            if self.model_type == 'classifier':\n                fpr, tpr, thresholds = metrics.roc_curve(test_labels, predicted)\n                eval_metrics.append(metrics.auc(fpr, tpr))\n                metrics_type = 'AUC'\n            elif self.model_type == 'regressor':\n                r2 = metrics.r2_score(test_labels, predicted)\n                eval_metrics.append(r2)\n                metrics_type = 'R^2 score'\n\n        return eval_metrics, metrics_type\n\n    def predict(self, smiles, average=True):\n        if self.feature_type == 'fingerprints':\n            fps = get_fp(smiles)\n            assert len(smiles) == len(fps)\n            clean_smiles = []\n            clean_fps = []\n            nan_smiles = []\n            for i in range(len(fps)):\n                if np.isnan(sum(fps[i])):\n                    nan_smiles.append(smiles[i])\n                else:\n                    clean_smiles.append(smiles[i])\n                    clean_fps.append(fps[i])\n            clean_fps = np.array(clean_fps)\n        elif self.feature_type == 'descriptors':\n            clean_fps, clean_smiles, nan_smiles = get_desc(smiles, self.calc)\n        prediction = []\n        if len(clean_fps) > 0:\n            for i in range(self.n_ensemble):\n                m = self.model[i]\n                if self.feature_type == 'descriptors':\n                    clean_fps, _ = normalize_desc(clean_fps, self.desc_mean[i])\n                prediction.append(m.predict(clean_fps))\n            prediction = np.array(prediction)\n            if average:\n                prediction = prediction.mean(axis=0)\n        assert len(clean_smiles) == len(prediction)\n\n        return clean_smiles, prediction, nan_smiles\n\n\nclass SVMQSAR(object):\n    def __init__(self, model_type='classifier', n_ensemble=5):\n        super(SVMQSAR, self).__init__()\n        self.n_ensemble = n_ensemble\n        self.model = []\n        self.model_type = model_type\n        if self.model_type == 'classifier':\n            for i in range(n_ensemble):\n                self.model.append(SVC())\n        elif self.model_type == 'regressor':\n            for i in range(n_ensemble):\n                self.model.append(SVR())\n        else:\n            raise ValueError('invalid value for argument')\n\n    def load_model(self, path):\n        self.model = []\n        for i in range(self.n_ensemble):\n            m = joblib.load(path + str(i) + '.pkl')\n            self.model.append(m)\n\n    def save_model(self, path):\n        assert self.n_ensemble == len(self.model)\n        for i in range(self.n_ensemble):\n            joblib.dump(self.model[i], path + str(i) + '.pkl')\n\n    def fit_model(self, data, cross_val_data, cross_val_labels):\n        eval_metrics = []\n        for i in range(self.n_ensemble):\n            train_sm = np.concatenate(cross_val_data[:i] +\n                                      cross_val_data[(i + 1):])\n            test_sm = cross_val_data[i]\n            train_labels = np.concatenate(cross_val_labels[:i] +\n                                          cross_val_labels[(i + 1):])\n            test_labels = cross_val_labels[i]\n            fp_train = get_fp(train_sm)\n            fp_test = get_fp(test_sm)\n            self.model[i].fit(fp_train, train_labels.ravel())\n            predicted = self.model[i].predict(fp_test)\n            if self.model_type == 'classifier':\n                fpr, tpr, thresholds = metrics.roc_curve(test_labels, predicted)\n                eval_metrics.append(metrics.auc(fpr, tpr))\n                metrics_type = 'AUC'\n            elif self.model_type == 'regressor':\n                r2 = metrics.r2_score(test_labels, predicted)\n                eval_metrics.append(r2)\n                metrics_type = 'R^2 score'\n        return eval_metrics, metrics_type\n\n    def predict(self, smiles, average=True):\n        fps = get_fp(smiles)\n        assert len(smiles) == len(fps)\n        clean_smiles = []\n        clean_fps = []\n        nan_smiles = []\n        for i in range(len(fps)):\n            if np.isnan(sum(fps[i])):\n                nan_smiles.append(smiles[i])\n            else:\n                clean_smiles.append(smiles[i])\n                clean_fps.append(fps[i])\n        clean_fps = np.array(clean_fps)\n        prediction = []\n        if len(clean_fps) > 0:\n            for m in self.model:\n                prediction.append(m.predict(clean_fps))\n            prediction = np.array(prediction)\n            if average:\n                prediction = prediction.mean(axis=0)\n        assert len(clean_smiles) == len(prediction)\n        return clean_smiles, prediction, nan_smiles\n"""
openchem/optimizer/openchem_lr_scheduler.py,0,"b'class OpenChemLRScheduler(object):\n\n    def __init__(self, params, optimizer):\n        self.params = params[1]\n        self._scheduler = params[0](optimizer, **self.params)\n\n    @property\n    def scheduler(self):\n        return self._scheduler\n\n    def step(self):\n        """"""Performs a single scheduler step.""""""\n        return self.scheduler.step()\n'"
openchem/optimizer/openchem_optimizer.py,3,"b'# Modified from\n# github.com/pytorch/fairseq/blob/master/fairseq/optim/fairseq_optimizer.py\n\n\n# Copyright (c) 2017-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the LICENSE file in\n# the root directory of this source tree. An additional grant of patent rights\n# can be found in the PATENTS file in the same directory.\n\nimport torch.optim\n\n\nclass OpenChemOptimizer(object):\n\n    def __init__(self, params, model_params):\n        self.params = params[1]\n        self._optimizer = params[0](model_params, **self.params)\n\n    @property\n    def optimizer(self):\n        if not isinstance(self._optimizer, torch.optim.Optimizer):\n            raise ValueError(\'_optimizer must be an instance of \'\n                             \'torch.optim.Optimizer\')\n        return self._optimizer\n\n    @property\n    def param_groups(self):\n        return self.optimizer.param_groups\n\n    def get_lr(self):\n        """"""Return the current learning rate.""""""\n        return self.optimizer.param_groups[0][\'lr\']\n\n    def set_lr(self, lr):\n        """"""Set the learning rate.""""""\n        for param_group in self.optimizer.param_groups:\n            param_group[\'lr\'] = lr\n\n    def state_dict(self):\n        """"""Return the optimizer\'s state dict.""""""\n        return self.optimizer.state_dict()\n\n    def load_state_dict(self, state_dict):\n        """"""Load an optimizer state dict.\n        In general we should prefer the configuration of the existing optimizer\n        instance (e.g., learning rate) over that found in the state_dict. This\n        allows us to resume training from a checkpoint using a new set of\n        optimizer args.\n        """"""\n        self.optimizer.load_state_dict(state_dict)\n\n        # override learning rate, momentum, etc. with latest values\n        for group in self.optimizer.param_groups:\n            group.update(self.params)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.""""""\n        return self.optimizer.step(closure)\n\n    def zero_grad(self):\n        """"""Clears the gradients of all optimized parameters.""""""\n        return self.optimizer.zero_grad()\n'"
openchem/tests/model_instantiation_test.py,13,"b'""""""\nUnit tests for testing if an instance of every class in openchem.models\ncan be created\n""""""\n\nimport torch\nimport pytest\nimport sys\nfrom openchem.models.openchem_model import OpenChemModel\nfrom openchem.models.Graph2Label import Graph2Label\nfrom openchem.models.Smiles2Label import Smiles2Label\nfrom openchem.models.MoleculeProtein2Label import MoleculeProtein2Label\nfrom openchem.models.GenerativeRNN import GenerativeRNN\nfrom openchem.modules.embeddings.openchem_embedding import OpenChemEmbedding\nfrom openchem.modules.encoders.rnn_encoder import RNNEncoder\nfrom openchem.modules.encoders.gcn_encoder import GraphCNNEncoder\nfrom openchem.modules.mlp.openchem_mlp import OpenChemMLP\n\n\ndef test_openchem_model_instantiation():\n    sample_config = {\n        \'task\': \'classification\',\n        \'batch_size\': 16,\n        \'num_epochs\': 1,\n        \'train_data_layer\': None,\n        \'val_data_layer\': None,\n        \'use_cuda\': True,\n        \'eval_metrics\': None,\n        \'logdir\': \'/logs\',\n        \'world_size\': 1,\n        \'use_clip_grad\': False,\n        \'random_seed\': 42,\n        \'print_every\': 10,\n        \'plot_every\': 10,\n        \'save_every\': 10\n    }\n    new_model = OpenChemModel(sample_config)\n\ndef test_graph2label_model_instantiation():\n    sample_config = {\n        \'task\': \'classification\',\n        \'batch_size\': 16,\n        \'num_epochs\': 1,\n        \'train_data_layer\': None,\n        \'val_data_layer\': None,\n        \'use_cuda\': True,\n        \'eval_metrics\': None,\n        \'logdir\': \'/logs\',\n        \'world_size\': 1,\n        \'use_clip_grad\': False,\n        \'random_seed\': 42,\n        \'print_every\': 10,\n        \'plot_every\': 10,\n        \'save_every\': 10,\n        \'optimizer\': torch.optim.Adam,\n        \'optimizer_params\': {\n            \'lr\': 0.01\n        },\n        \'lr_scheduler\': torch.optim.lr_scheduler.StepLR,\n        \'lr_scheduler_params\': {\n            \'step_size\': 10,\n            \'gamma\': 0.8,\n        },\n        \'encoder\': GraphCNNEncoder,\n        \'encoder_params\': {\n            \'input_size\': 12,\n            \'hidden_size\': [12],\n            \'encoder_dim\': 12,\n            \'n_layers\': 1,\n        },\n        \'mlp\': OpenChemMLP,\n        \'mlp_params\': {\n            \'hidden_size\': [12],\n            \'input_size\': 12,\n            \'n_layers\': 1,\n            \'activation\': torch.tanh\n        }\n    }\n    new_model = Graph2Label(sample_config)\n\n\ndef test_smiles2label_model_instantiation():\n    sample_config = {\n        \'task\': \'classification\',\n        \'batch_size\': 16,\n        \'num_epochs\': 1,\n        \'train_data_layer\': None,\n        \'val_data_layer\': None,\n        \'use_cuda\': True,\n        \'eval_metrics\': None,\n        \'logdir\': \'/logs\',\n        \'world_size\': 1,\n        \'use_clip_grad\': False,\n        \'random_seed\': 42,\n        \'print_every\': 10,\n        \'plot_every\': 10,\n        \'save_every\': 10,\n        \'optimizer\': torch.optim.Adam,\n        \'optimizer_params\': {\n            \'lr\': 0.01\n        },\n        \'lr_scheduler\': torch.optim.lr_scheduler.StepLR,\n        \'lr_scheduler_params\': {\n            \'step_size\': 10,\n            \'gamma\': 0.8,\n        },\n        \'embedding\': OpenChemEmbedding,\n        \'embedding_params\': {\n            \'num_embeddings\': 10,\n            \'embedding_dim\': 12,\n            #\'padding_idx\': None\n        },\n        \'encoder\': RNNEncoder,\n        \'encoder_params\': {\n            \'input_size\': 12,\n            \'layer\': \'LSTM\',\n            \'encoder_dim\': 12,\n            \'n_layers\': 1,\n            \'is_bidirectional\': False\n        },\n        \'mlp\': OpenChemMLP,\n        \'mlp_params\': {\n            \'hidden_size\': [12],\n            \'input_size\': 12,\n            \'n_layers\': 1,\n            \'activation\': torch.tanh\n        }\n    }\n    new_model = Smiles2Label(sample_config)\n\n\ndef test_molecule_protein2label_model_instantiation():\n    sample_config = {\n        \'task\': \'classification\',\n        \'batch_size\': 16,\n        \'num_epochs\': 1,\n        \'train_data_layer\': None,\n        \'val_data_layer\': None,\n        \'use_cuda\': True,\n        \'eval_metrics\': None,\n        \'logdir\': \'/logs\',\n        \'world_size\': 1,\n        \'use_clip_grad\': False,\n        \'random_seed\': 42,\n        \'print_every\': 10,\n        \'plot_every\': 10,\n        \'save_every\': 10,\n        \'optimizer\': torch.optim.Adam,\n        \'optimizer_params\': {\n            \'lr\': 0.01\n        },\n        \'lr_scheduler\': torch.optim.lr_scheduler.StepLR,\n        \'lr_scheduler_params\': {\n            \'step_size\': 10,\n            \'gamma\': 0.8,\n        },\n        \'mol_embedding\': OpenChemEmbedding,\n        \'mol_embedding_params\': {\n            \'num_embeddings\': 10,\n            \'embedding_dim\': 12,\n            #\'padding_idx\': None\n        },\n        \'mol_encoder\': RNNEncoder,\n        \'mol_encoder_params\': {\n            \'input_size\': 12,\n            \'layer\': \'LSTM\',\n            \'encoder_dim\': 12,\n            \'n_layers\': 1,\n            \'is_bidirectional\': False\n        },\n        \'prot_embedding\': OpenChemEmbedding,\n        \'prot_embedding_params\': {\n            \'num_embeddings\': 10,\n            \'embedding_dim\': 12,\n            # \'padding_idx\': None\n        },\n        \'prot_encoder\': RNNEncoder,\n        \'prot_encoder_params\': {\n            \'input_size\': 12,\n            \'layer\': \'LSTM\',\n            \'encoder_dim\': 12,\n            \'n_layers\': 1,\n            \'is_bidirectional\': False\n        },\n        \'merge\': \'concat\',\n        \'mlp\': OpenChemMLP,\n        \'mlp_params\': {\n            \'hidden_size\': [12],\n            \'input_size\': 12,\n            \'n_layers\': 1,\n            \'activation\': torch.tanh\n        }\n    }\n    new_model = MoleculeProtein2Label(sample_config)\n\ndef test_generative_model_instantiation():\n    sample_config = {\n        \'world_size\': 1,\n        \'use_cuda\': True,\n        \'task\': \'multitask\',\n        \'random_seed\': 5,\n        \'use_clip_grad\': False,\n        \'batch_size\': 64,\n        \'num_epochs\': 5,\n        \'logdir\': \'./logs/\',\n        \'print_every\': 1,\n        \'save_every\': 5,\n        \'train_data_layer\': None,\n        \'val_data_layer\': None,\n        \'eval_metrics\': None,\n        \'criterion\': torch.nn.CrossEntropyLoss(),\n        \'optimizer\': torch.optim.Adam,\n        \'optimizer_params\': {\n            \'lr\': 0.001,\n        },\n        \'lr_scheduler\': torch.optim.lr_scheduler.ExponentialLR,\n        \'lr_scheduler_params\': {\n            \'gamma\': 0.99\n        },\n        \'embedding\': OpenChemEmbedding,\n        \'embedding_params\': {\n            \'num_embeddings\': 10,\n            \'embedding_dim\': 10,\n            \'padding_idx\': 0\n        },\n        \'encoder\': RNNEncoder,\n        \'encoder_params\': {\n            \'input_size\': 20,\n            \'layer\': ""GRU"",\n            \'encoder_dim\': 20,\n            \'n_layers\': 1,\n            \'dropout\': 0.0,\n            \'is_bidirectional\': False\n        },\n        \'has_stack\': False,\n        \'mlp\': OpenChemMLP,\n        \'mlp_params\': {\n            \'input_size\': 20,\n            \'n_layers\': 2,\n            \'hidden_size\': [20, 20],\n            \'activation\': torch.softmax,\n            \'dropout\': 0.0\n        }\n    }\n    new_model = GenerativeRNN(sample_config)\n'"
openchem/tests/module_instantiation_test.py,1,"b'""""""\nUnit tests for testing if an instance of every class in openchem.modules\ncan be created\n""""""\nimport torch\n\nfrom openchem.modules.embeddings.basic_embedding import Embedding\nfrom openchem.modules.embeddings.openchem_embedding import OpenChemEmbedding\nfrom openchem.modules.encoders.openchem_encoder import OpenChemEncoder\nfrom openchem.modules.encoders.rnn_encoder import RNNEncoder\nfrom openchem.modules.encoders.gcn_encoder import GraphCNNEncoder\nfrom openchem.modules.encoders.edge_attention_encoder import GraphEdgeAttentionEncoder\nfrom openchem.modules.mlp.openchem_mlp import OpenChemMLP\n\ndef test_openchem_embedding_module():\n    embedding_params = {\n        \'num_embeddings\': 10,\n        \'embedding_dim\': 10,\n        \'padding_idx\': 0\n    }\n    embedding_layer = OpenChemEmbedding(embedding_params)\n\n\ndef test_basic_embedding_module():\n    embedding_params = {\n        \'num_embeddings\': 10,\n        \'embedding_dim\': 10,\n        \'padding_idx\': 0\n    }\n    embedding_layer = Embedding(embedding_params)\n\ndef test_openchem_encoder():\n    encoder_params = {\n        \'input_size\': 12,\n        \'encoder_dim\': 12\n    }\n    encoder = OpenChemEncoder(encoder_params, use_cuda=True)\n\ndef test_rnn_encoder():\n    encoder_params = {\n        \'input_size\': 12,\n        \'layer\': \'LSTM\',\n        \'encoder_dim\': 12,\n        \'n_layers\': 1,\n        \'is_bidirectional\': False\n    }\n    encoder = RNNEncoder(encoder_params, use_cuda=True)\n\ndef test_gcn_encoder():\n    encoder_params = {\n        \'input_size\': 12,\n        \'encoder_dim\': 10,\n        \'hidden_size\': [10, 10, 10],\n        \'n_layers\': 3,\n    }\n    encoder = GraphCNNEncoder(encoder_params, use_cuda=True)\n\ndef test_edge_attention_encoder():\n    encoder_params = {\n        \'input_size\': 12,\n        \'encoder_dim\': 10,\n        \'hidden_size\': [10, 10, 10],\n        \'n_layers\': 3,\n        \'edge_attr_sizes\': [10, 10, 10]\n    }\n    encoder = GraphEdgeAttentionEncoder(encoder_params, use_cuda=True)\n\ndef test_mlp():\n    mlp_params = {\n        \'input_size\': 10,\n        \'n_layers\': 3,\n        \'hidden_size\': [10, 10, 10],\n        \'activation\': torch.softmax,\n    }\n    mlp = OpenChemMLP(mlp_params)\n\n'"
openchem/utils/graph.py,0,"b'from rdkit import Chem\nimport numpy as np\n\n\nclass Attribute:\n    def __init__(self, attr_type, name, one_hot=True, values=None):\n        if attr_type not in [\'node\', \'edge\']:\n            raise ValueError(\'Invalid value for attribute type: must be ""node"" \'\n                             \'or ""edge""\')\n        self.attr_type = attr_type\n        self.name = name\n        if values is not None:\n            self.n_values = len(values)\n            self.attr_values = values\n        self.one_hot = one_hot\n        if self.one_hot:\n            self.one_hot_dict = {}\n            for i in range(self.n_values):\n                tmp = np.zeros(self.n_values)\n                tmp[i] = 1\n                self.one_hot_dict[self.attr_values[i]] = tmp\n\n\nclass Node:\n    def __init__(self, idx, rdatom, get_atom_attributes):\n        self.node_idx = idx\n        self.atom_type = rdatom.GetAtomicNum()\n        self.attributes_dict = get_atom_attributes(rdatom)\n\n\nclass Edge:\n    def __init__(self, rdbond, get_bond_attributes=None):\n        self.begin_atom_idx = rdbond.GetBeginAtomIdx()\n        self.end_atom_idx = rdbond.GetEndAtomIdx()\n        if get_bond_attributes is not None:\n            self.attributes_dict = get_bond_attributes(rdbond)\n\n\nclass Graph:\n    """"""Describes an undirected graph class""""""\n\n    def __init__(self, smiles, max_size, get_atom_attributes,\n                 get_bond_attributes=None):\n        self.smiles = smiles\n        rdmol = Chem.MolFromSmiles(smiles)\n        self.num_nodes = rdmol.GetNumAtoms()\n        self.num_edges = rdmol.GetNumBonds()\n\n        self.nodes = []\n        for k, atom in enumerate(rdmol.GetAtoms()):\n            cur_node = Node(k, atom, get_atom_attributes)\n            self.nodes.append(cur_node)\n\n        adj_matrix = np.eye(self.num_nodes)\n\n        self.edges = []\n        for _, bond in enumerate(rdmol.GetBonds()):\n            cur_edge = Edge(bond, get_bond_attributes)\n            self.edges.append(cur_edge)\n            adj_matrix[cur_edge.begin_atom_idx,\n                       cur_edge.end_atom_idx] = 1.0\n            adj_matrix[cur_edge.end_atom_idx,\n                       cur_edge.begin_atom_idx] = 1.0\n        self.adj_matrix = np.zeros((max_size, max_size))\n        self.adj_matrix[:self.num_nodes, :self.num_nodes] = adj_matrix\n        if get_bond_attributes is not None and len(self.edges) > 0:\n            tmp = self.edges[0]\n            self.n_attr = len(tmp.attributes_dict.keys())\n\n    def get_node_attr_adj_matrix(self, attr):\n        node_attr_adj_matrix = np.zeros((self.num_nodes, self.num_nodes,\n                                         attr.n_values))\n        attr_one_hot = []\n        node_idx = []\n\n        for node in self.nodes:\n            tmp = attr.one_hot_dict[node.attributes_dict[attr.name]]\n            attr_one_hot.append(tmp)\n            node_attr_adj_matrix[node.node_idx, node.node_idx] = tmp\n            node_idx.append(node.node_idx)\n\n        for edge in self.edges:\n            begin = edge.begin_atom_idx\n            end = edge.end_atom_idx\n            begin_one_hot = attr_one_hot[node_idx.index(begin)]\n            end_one_hot = attr_one_hot[node_idx.index(end)]\n            node_attr_adj_matrix[begin, end, :] = (begin_one_hot +\n                                                   end_one_hot)/2\n\n        return node_attr_adj_matrix\n\n    def get_edge_attr_adj_matrix(self, all_atr_dict, max_size):\n        fl = True\n        for edge in self.edges:\n            begin = edge.begin_atom_idx\n            end = edge.end_atom_idx\n            cur_features = []\n            for attr_name in edge.attributes_dict.keys():\n                cur_attr = all_atr_dict[attr_name]\n                if cur_attr.one_hot:\n                    cur_features += list(cur_attr.one_hot_dict[edge.\n                                         attributes_dict[cur_attr.name]])\n                else:\n                    cur_features += [edge.attributes_dict[cur_attr.name]]\n            cur_features = np.array(cur_features)\n            attr_len = len(cur_features)\n            if fl:\n                edge_attr_adj_matrix = np.zeros((max_size, max_size, attr_len))\n                fl = False\n            edge_attr_adj_matrix[begin, end, :] = cur_features\n        \n        return edge_attr_adj_matrix\n   \n    \n    def get_node_feature_matrix(self, all_atr_dict, max_size):\n        features = []\n        for node in self.nodes:\n            cur_features = []\n            for attr_name in node.attributes_dict.keys():\n                cur_attr = all_atr_dict[attr_name]\n                if cur_attr.one_hot:\n                    cur_features += list(cur_attr.one_hot_dict[node.\n                                         attributes_dict[cur_attr.name]])\n                else:\n                    cur_features += [node.attributes_dict[cur_attr.name]]\n            features.append(cur_features)\n\n        features = np.array(features)\n        padded_features = np.zeros((max_size, features.shape[1]))\n        padded_features[:features.shape[0], :features.shape[1]] = features\n        return padded_features\n'"
openchem/utils/logger.py,0,"b'# Code referenced from\n# https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\nimport tensorflow as tf\nimport numpy as np\nimport scipy.misc\n\ntry:\n    from StringIO import StringIO  # Python 2.7\nexcept ImportError:\n    from io import BytesIO  # Python 3.x\n\n\nclass Logger(object):\n\n    def __init__(self, log_dir):\n        """"""Create a summary writer logging to log_dir.""""""\n        self.writer = tf.summary.FileWriter(log_dir)\n\n    def scalar_summary(self, tag, value, step):\n        """"""Log a scalar variable.""""""\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag,\n                                                     simple_value=value)])\n        self.writer.add_summary(summary, step)\n\n    def image_summary(self, tag, images, step):\n        """"""Log a list of images.""""""\n\n        img_summaries = []\n        for i, img in enumerate(images):\n            # Write the image to a string\n            try:\n                s = StringIO()\n            except ImportError:\n                s = BytesIO()\n            scipy.misc.toimage(img).save(s, format=""png"")\n\n            # Create an Image object\n            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n                                       height=img.shape[0],\n                                       width=img.shape[1])\n            # Create a Summary value\n            img_summaries.append(tf.Summary.Value(tag=\'%s/%d\' % (tag, i),\n                                                  image=img_sum))\n\n        # Create and write Summary\n        summary = tf.Summary(value=img_summaries)\n        self.writer.add_summary(summary, step)\n\n    def histo_summary(self, tag, values, step, bins=1000):\n        """"""Log a histogram of the tensor of values.""""""\n\n        # Create a histogram using numpy\n        counts, bin_edges = np.histogram(values, bins=bins)\n\n        # Fill the fields of the histogram proto\n        hist = tf.HistogramProto()\n        hist.min = float(np.min(values))\n        hist.max = float(np.max(values))\n        hist.num = int(np.prod(values.shape))\n        hist.sum = float(np.sum(values))\n        hist.sum_squares = float(np.sum(values ** 2))\n\n        # Drop the start of the first bin\n        bin_edges = bin_edges[1:]\n\n        # Add bin edges and counts\n        for edge in bin_edges:\n            hist.bucket_limit.append(edge)\n        for c in counts:\n            hist.bucket.append(c)\n\n        # Create and write Summary\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n        self.writer.add_summary(summary, step)\n        self.writer.flush()\n'"
openchem/utils/utils.py,2,"b'import time\nimport math\nfrom six import string_types\nfrom collections import defaultdict\n\nimport torch\nimport glob\nimport os\nimport six\n\n\ndef move_to_cuda(sample):\n    # copy-pasted from\n    # https://github.com/pytorch/fairseq/blob/master/fairseq/utils.py\n    if len(sample) == 0:\n        return {}\n\n    def _move_to_cuda(maybe_tensor):\n        if torch.is_tensor(maybe_tensor):\n            return maybe_tensor.cuda()\n        elif isinstance(maybe_tensor, dict):\n            return {\n                key: _move_to_cuda(value)\n                for key, value in maybe_tensor.items()\n            }\n        elif isinstance(maybe_tensor, list):\n            return [_move_to_cuda(x) for x in maybe_tensor]\n        else:\n            return maybe_tensor\n\n    return _move_to_cuda(sample)\n\n\ndef get_latest_checkpoint(path):\n    if os.path.isdir(path) and os.listdir(path) != []:\n        list_of_files = glob.glob(path + \'*\')\n        latest_file = max(list_of_files, key=os.path.getctime)\n        return latest_file\n    else:\n        return None\n\n\ndef deco_print(line, offset=0, start=""*** "", end=\'\\n\'):\n    # copy-pasted from\n    # github.com/NVIDIA/OpenSeq2Seq/blob/master/open_seq2seq/utils/utils.py\n    if six.PY2:\n        print((start + "" "" * offset + line).encode(\'utf-8\'), end=end)\n    else:\n        print(start + "" "" * offset + line, end=end)\n\n\ndef flatten_dict(dct):\n    # copy-pasted from\n    # github.com/NVIDIA/OpenSeq2Seq/blob/master/open_seq2seq/utils/utils.py\n    flat_dict = {}\n    for key, value in dct.items():\n        if isinstance(value, int) or isinstance(value, float) or \\\n           isinstance(value, string_types) or isinstance(value, bool):\n            flat_dict.update({key: value})\n        elif isinstance(value, dict):\n            flat_dict.update(\n                {key + \'/\' + k: v for k, v in flatten_dict(dct[key]).items()})\n    return flat_dict\n\n\ndef nest_dict(flat_dict):\n    # copy-pasted from\n    # github.com/NVIDIA/OpenSeq2Seq/blob/master/open_seq2seq/utils/utils.py\n    nst_dict = {}\n    for key, value in flat_dict.items():\n        nest_keys = key.split(\'/\')\n        cur_dict = nst_dict\n        for i in range(len(nest_keys) - 1):\n            if nest_keys[i] not in cur_dict:\n                cur_dict[nest_keys[i]] = {}\n            cur_dict = cur_dict[nest_keys[i]]\n        cur_dict[nest_keys[-1]] = value\n    return nst_dict\n\n\ndef nested_update(org_dict, upd_dict):\n    # copy-pasted from\n    # github.com/NVIDIA/OpenSeq2Seq/blob/master/open_seq2seq/utils/utils.py\n    for key, value in upd_dict.items():\n        if isinstance(value, dict):\n            if key in org_dict:\n                if not isinstance(org_dict[key], dict):\n                    raise ValueError(\n                        ""Mismatch between org_dict and upd_dict ""\n                        ""at node {}"".format(key)\n                    )\n                nested_update(org_dict[key], value)\n            else:\n                org_dict[key] = value\n        else:\n            org_dict[key] = value\n\n\ndef time_since(since):\n    s = time.time() - since\n    m = math.floor(s / 60)\n    s -= m * 60\n\n    return \'%dm %ds\' % (m, s)\n\n\ndef identity(input):\n    return input\n\n\ndef calculate_metrics(predicted, ground_truth, metrics):\n    return metrics(ground_truth, predicted)\n\n\ndef check_params(config, required_dict, optional_dict):\n    if required_dict is None or optional_dict is None:\n        return\n\n    for pm, vals in required_dict.items():\n        if pm not in config:\n            raise ValueError(""{} parameter has to be specified"".format(pm))\n        else:\n            if vals == str:\n                vals = string_types\n            if vals and isinstance(vals, list) and config[pm] not in vals:\n                raise ValueError(""{} has to be one of {}"".format(pm, vals))\n            if vals and not isinstance(vals, list) and \\\n                    not isinstance(config[pm], vals):\n                raise ValueError(""{} has to be of type {}"".format(pm, vals))\n\n    for pm, vals in optional_dict.items():\n        if vals == str:\n            vals = string_types\n        if pm in config:\n            if vals and isinstance(vals, list) and config[pm] not in vals:\n                raise ValueError(""{} has to be one of {}"".format(pm, vals))\n            if vals and not isinstance(vals, list) and \\\n                    not isinstance(config[pm], vals):\n                raise ValueError(""{} has to be of type {}"".format(pm, vals))\n\n    # for pm in config:\n    #     if pm not in required_dict and pm not in optional_dict:\n    #         raise ValueError(""Unknown parameter: {}"".format(pm))\n\n\ndef cross_validation_split(data, targets, n_folds=5, split=\'random\',\n                           stratified=True, folds=None):\n    if split not in [\'random\', \'fixed\']:\n        raise ValueError(\'Invalid value for argument \\\'split\\\': \'\n                         \'must be either \\\'random\\\' or \\\'fixed\\\'.\')\n    if split == \'fixed\' and folds is None:\n        raise ValueError(\'When \\\'split\\\' is \\\'fixed\\\' \'\n                         \'argument \\\'folds\\\' must be provided.\')\n    if split == \'fixed\':\n        assert len(targets) == len(folds)\n    raise NotImplementedError\n\n\ndef make_positions(tensor, padding_idx, left_pad):\n    # Copy-pasted from\n    # https://github.com/pytorch/fairseq/blob/master/fairseq/utils.py\n    """"""Replace non-padding symbols with their position numbers.\n    Position numbers begin at padding_idx+1.\n    Padding symbols are ignored, but it is necessary to specify whether padding\n    is added on the left side (left_pad=True) or right side (left_pad=False).\n    """"""\n    max_pos = padding_idx + 1 + tensor.size(1)\n    if not hasattr(make_positions, \'range_buf\'):\n        make_positions.range_buf = tensor.new()\n    make_positions.range_buf = make_positions.range_buf.type_as(tensor)\n    if make_positions.range_buf.numel() < max_pos:\n        torch.arange(padding_idx + 1, max_pos, out=make_positions.range_buf)\n    mask = tensor.ne(padding_idx)\n    positions = make_positions.range_buf[:tensor.size(1)].expand_as(tensor)\n    if left_pad:\n        positions = positions - mask.size(1) + \\\n                    mask.long().sum(dim=1).unsqueeze(1)\n    return tensor.clone().masked_scatter_(mask, positions[mask])\n\n\n'"
openchem/modules/embeddings/basic_embedding.py,0,"b'from openchem.modules.embeddings.openchem_embedding import OpenChemEmbedding\n\n\nclass Embedding(OpenChemEmbedding):\n    def __init__(self, params):\n        super(Embedding, self).__init__(params)\n\n    def forward(self, inp):\n        embedded = self.embedding(inp)\n        return embedded\n'"
openchem/modules/embeddings/openchem_embedding.py,1,"b""import torch.nn as nn\n\nfrom openchem.utils.utils import check_params\n\n\nclass OpenChemEmbedding(nn.Module):\n    def __init__(self, params):\n        super(OpenChemEmbedding, self).__init__()\n        check_params(params, self.get_required_params(),\n                     self.get_optional_params())\n        self.params = params\n        self.num_embeddings = self.params['num_embeddings']\n        self.embedding_dim = self.params['embedding_dim']\n        if 'padding_idx' in params.keys():\n            self.padding_idx = self.params['padding_idx']\n        else:\n            self.padding_idx = None\n        self.embedding = nn.Embedding(num_embeddings=self.num_embeddings,\n                                      embedding_dim=self.embedding_dim,\n                                      padding_idx=self.padding_idx)\n\n    def forward(self, inp):\n        raise NotImplementedError\n\n    @staticmethod\n    def get_optional_params():\n        return {\n            'padding_idx': int\n        }\n\n    @staticmethod\n    def get_required_params():\n        return {\n            'num_embeddings': int,\n            'embedding_dim': int\n        }\n\n"""
openchem/modules/encoders/edge_attention_encoder.py,8,"b""import torch\nimport torch.nn as nn\n\nfrom openchem.modules.encoders.openchem_encoder import OpenChemEncoder\nfrom openchem.utils.utils import check_params\nfrom openchem.layers.gcn import GraphConvolution\n\n\nclass GraphEdgeAttentionEncoder(OpenChemEncoder):\n    def __init__(self, params, use_cuda):\n        super(GraphEdgeAttentionEncoder, self).__init__(params, use_cuda)\n        check_params(params, self.get_required_params(),\n                     self.get_optional_params())\n        self.n_layers = params['n_layers']\n        self.attr_size = params['edge_attr_sizes']\n        self.attr_dim = sum(self.attr_size)\n        self.hidden_size = params['hidden_size']\n        if 'dropout' in params.keys():\n            self.dropout = params['dropout']\n        else:\n            self.dropout = 0\n        assert len(self.hidden_size) == self.n_layers\n        self.hidden_size = [self.input_size] + self.hidden_size\n        self.graph_conv = nn.ModuleList()\n        self.dropout_layer = nn.Dropout(p=self.dropout)\n        self.dense = nn.Linear(in_features=self.hidden_size[-1],\n                               out_features=self.encoder_dim)\n        for i in range(1, self.n_layers + 1):\n            for j in range(self.attr_dim):\n                self.graph_conv.append(GraphConvolution(self.hidden_size[i-1],\n                                                        self.hidden_size[i]))\n\n    @staticmethod\n    def get_optional_params():\n        return {\n            'dropout': float\n        }\n\n    @staticmethod\n    def get_required_params():\n        return {\n            'n_layers': int,\n            'hidden_size': list,\n            'edge_attr_sizes': list,\n        }\n\n    def forward(self, inp):\n        x = inp[0]\n        edge_attr = inp[1]\n        ones = torch.ones(edge_attr.size()[0], edge_attr.size()[1], edge_attr.size()[1]).cuda()\n        zeros = torch.zeros(edge_attr.size()[0], edge_attr.size()[1], edge_attr.size()[1]).cuda()\n        adj = torch.where(edge_attr[:, :, :, 0] > 0, ones, zeros)\n        list_of_x = []\n        cur_x = x\n        for i in range(self.n_layers):\n            for j in range(self.attr_dim):\n                cur_adj = nn.functional.softmax(edge_attr[:, :, :, j], dim=2)#*adj\n                x = self.graph_conv[i*self.attr_dim + j](cur_x, cur_adj)\n                x = torch.tanh(x)\n                n = adj.size(1)\n                d = x.size()[-1]\n                adj_new = adj.unsqueeze(3)\n                adj_new = adj_new.expand(-1, n, n, d)\n                x_new = x.repeat(1, n, 1).view(-1, n, n, d)\n                res = x_new * adj_new\n                x = res.max(dim=2)[0]\n                list_of_x.append(x)\n            cur_x = torch.stack(list_of_x).sum(dim=0)\n            list_of_x = []\n        x = torch.tanh(self.dense(cur_x))\n        x = torch.tanh(x.sum(dim=1))\n        return x\n"""
openchem/modules/encoders/gcn_encoder.py,4,"b""import torch\nimport torch.nn as nn\n\nfrom openchem.modules.encoders.openchem_encoder import OpenChemEncoder\nfrom openchem.utils.utils import check_params\nfrom openchem.layers.gcn import GraphConvolution\n\n\nclass GraphCNNEncoder(OpenChemEncoder):\n    def __init__(self, params, use_cuda):\n        super(GraphCNNEncoder, self).__init__(params, use_cuda)\n        check_params(params, self.get_required_params(),\n                     self.get_optional_params())\n        self.n_layers = params['n_layers']\n        self.hidden_size = params['hidden_size']\n        if 'dropout' in params.keys():\n            self.dropout = params['dropout']\n        else:\n            self.dropout = 0\n        assert len(self.hidden_size) == self.n_layers\n        self.hidden_size = [self.input_size] + self.hidden_size\n        self.graph_convolutions = nn.ModuleList()\n        self.dropout_layer = nn.Dropout(p=self.dropout)\n        self.dense = nn.Linear(in_features=self.hidden_size[-1],\n                                      out_features=self.encoder_dim)\n        for i in range(1, self.n_layers+1):\n            self.graph_convolutions.append(GraphConvolution(self.\n                                                            hidden_size[i-1],\n                                                            self.\n                                                            hidden_size[i]))\n\n    @staticmethod\n    def get_optional_params():\n        return {\n            'dropout': float\n        }\n\n    @staticmethod\n    def get_required_params():\n        return {\n            'n_layers': int,\n            'hidden_size': list,\n        }\n\n    def forward(self, inp):\n        x = inp[0]\n        adj = inp[1]\n        for i in range(self.n_layers):\n            x = self.graph_convolutions[i](x, adj)\n            x = torch.tanh(x)\n            n = adj.size(1)\n            d = x.size()[-1]\n            adj_new = adj.unsqueeze(3)\n            adj_new = adj_new.expand(-1, n, n, d)\n            x_new = x.repeat(1, n, 1).view(-1, n, n, d)\n            res = x_new*adj_new\n            x = res.max(dim=2)[0]\n        x = torch.tanh(self.dense(x))\n        x = torch.tanh(x.sum(dim=1))\n        return x\n"""
openchem/modules/encoders/openchem_encoder.py,1,"b'\nimport torch\nfrom torch import nn\n\nfrom openchem.utils.utils import check_params\n\n\nclass OpenChemEncoder(nn.Module):\n    """"""Base class for embedding module""""""\n    def __init__(self, params, use_cuda=None):\n        super(OpenChemEncoder, self).__init__()\n        check_params(params, self.get_required_params(),\n                     self.get_required_params())\n        self.params = params\n        if use_cuda is None:\n            use_cuda = torch.cuda.is_available()\n        self.use_cuda = use_cuda\n        self.input_size = self.params[\'input_size\']\n        self.encoder_dim = self.params[\'encoder_dim\']\n\n    @staticmethod\n    def get_required_params():\n        return {\n            \'input_size\': int,\n            \'encoder_dim\': int\n        }\n\n    @staticmethod\n    def get_optional_params():\n        return {}\n\n    def forward(self, inp):\n        raise NotImplementedError\n'"
openchem/modules/encoders/rnn_encoder.py,4,"b'import torch\nfrom torch import nn\nfrom openchem.modules.encoders.openchem_encoder import OpenChemEncoder\n\nfrom openchem.utils.utils import check_params\n\n\nclass RNNEncoder(OpenChemEncoder):\n    def __init__(self, params, use_cuda):\n        super(RNNEncoder, self).__init__(params, use_cuda)\n        check_params(params, self.get_required_params(),\n                     self.get_optional_params())\n        self.layer = self.params[\'layer\']\n        layers = [\'LSTM\', \'GRU\', \'RNN\']\n        if self.layer not in [\'LSTM\', \'GRU\', \'RNN\']:\n            raise ValueError(self.layer + \' is invalid value for argument\'\n                                          \' \\\'layer\\\'. Choose one from :\'\n                             + \', \'.join(layers))\n\n        self.input_size = self.params[\'input_size\']\n        self.encoder_dim = self.params[\'encoder_dim\']\n        self.n_layers = self.params[\'n_layers\']\n        if self.n_layers > 1:\n            self.dropout = self.params[\'dropout\']\n        else:\n            UserWarning(\'dropout can be non zero only when n_layers > 1. \'\n                        \'Parameter dropout set to 0.\')\n            self.dropout = 0\n        self.bidirectional = self.params[\'is_bidirectional\']\n        if self.bidirectional:\n            self.n_directions = 2\n        else:\n            self.n_directions = 1\n        if self.layer == \'LSTM\':\n            self.rnn = nn.LSTM(self.input_size, self.encoder_dim,\n                               self.n_layers,\n                               bidirectional=self.bidirectional,\n                               dropout=self.dropout)\n        elif self.layer == \'GRU\':\n            self.rnn = nn.GRU(self.input_size, self.encoder_dim,\n                              self.n_layers,\n                              bidirectional=self.bidirectional,\n                              dropout=self.dropout)\n        else:\n            self.layer = nn.RNN(self.input_size, self.encoder_dim,\n                                self.n_layers,\n                                bidirectional=self.bidirectional,\n                                dropout=self.dropout)\n\n    @staticmethod\n    def get_required_params():\n        return {\n            \'input_size\': int,\n            \'encoder_dim\': int,\n        }\n\n    @staticmethod\n    def get_optional_params():\n        return{\n            \'layer\': str,\n            \'n_layers\': int,\n            \'dropout\': float,\n            \'is_bidirectional\': bool\n        }\n\n    def forward(self, inp, previous_hidden=None):\n        """"""\n        inp: shape batch_size, seq_len, input_size\n        previous_hidden: if given shape n_layers * num_directions,\n        batch_size, embedding_dim.\n               Initialized automatically if None\n        return: embedded\n        """"""\n        inp = inp.permute(1, 0, 2)\n        batch_size = inp.size()[1]\n        if previous_hidden is None:\n            previous_hidden = self.init_hidden(batch_size)\n            if self.layer == \'LSTM\':\n                cell = self.init_cell(batch_size)\n                previous_hidden = (previous_hidden, cell)\n        output, _ = self.rnn(inp, previous_hidden)\n        embedded = output[-1, :, :].squeeze(0)\n        return embedded, previous_hidden\n\n    def init_hidden(self, batch_size):\n        if self.use_cuda:\n            return torch.tensor(torch.zeros(self.n_layers * self.n_directions,\n                                            batch_size,\n                                            self.encoder_dim),\n                                requires_grad=True).cuda()\n        else:\n            return torch.tensor(torch.zeros(self.n_layers * self.n_directions,\n                                            batch_size,\n                                            self.encoder_dim),\n                                requires_grad=True)\n\n    def init_cell(self, batch_size):\n        if self.use_cuda:\n            return torch.tensor(torch.zeros(self.n_layers * self.n_directions,\n                                            batch_size,\n                                            self.encoder_dim),\n                                requires_grad=True).cuda()\n        else:\n            return torch.tensor(torch.zeros(self.n_layers * self.n_directions,\n                                            batch_size,\n                                            self.encoder_dim),\n                                requires_grad=True)\n\n'"
openchem/modules/mlp/openchem_mlp.py,2,"b'import torch\nimport torch.nn as nn\nimport math\nimport torch.nn.functional as F\n\nfrom openchem.utils.utils import check_params\n\n\nclass OpenChemMLP(nn.Module):\n    """"""Base class for MLP module""""""\n    def __init__(self, params):\n        super(OpenChemMLP, self).__init__()\n        check_params(params, self.get_required_params(),\n                     self.get_optional_params())\n        self.params = params\n        self.hidden_size = self.params[\'hidden_size\']\n        self.input_size = [self.params[\'input_size\']] + self.hidden_size[:-1]\n        self.n_layers = self.params[\'n_layers\']\n        self.activation = self.params[\'activation\']\n        if type(self.activation) is list:\n            assert len(self.activation) == self.n_layers\n        else:\n            self.activation = [self.activation]*self.n_layers\n        if \'dropout\' in self.params.keys():\n            self.dropout = self.params[\'dropout\']\n        else:\n            self.dropout = 0\n        self.layers = nn.ModuleList([])\n        self.bn = nn.ModuleList([])\n        self.dropouts = nn.ModuleList([])\n        for i in range(self.n_layers):\n            self.dropouts.append(nn.Dropout(self.dropout))\n            self.bn.append(nn.BatchNorm1d(self.hidden_size[i]))\n            self.layers.append(nn.Linear(in_features=self.input_size[i],\n                                      out_features=self.hidden_size[i]))\n\n    @staticmethod\n    def get_required_params():\n        return {\n            \'input_size\': int,\n            \'n_layers\': int,\n            \'hidden_size\': list,\n            \'activation\': None,\n        }\n\n    @staticmethod\n    def get_optional_params():\n        return {\'dropout\': float}\n\n    def forward(self, inp):\n        output = inp\n        for i in range(self.n_layers-1):\n            output = self.dropouts[i](output)\n            output = self.layers[i](output)\n            output = self.bn[i](output)\n            output = self.activation[i](output)\n        output = self.dropouts[-1](output)\n        output = self.layers[-1](output)\n        output = self.activation[-1](output)    \n        return output\n\n'"
