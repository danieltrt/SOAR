file_path,api_count,code
src/api/app.py,4,"b'import sys\nimport os\nimport random\n\nfrom tqdm import tqdm\n\nfrom flask import Blueprint, request, jsonify, Flask\nimport torch\nimport torch.nn.functional as F\nimport wget\n\nimport db\nimport config\nfrom ml.model import CharacterLevelCNN\nfrom ml.utils import predict_sentiment\n\napp = Flask(__name__)\napi = Blueprint(\'api\', __name__)\n\n# Load pytorch model for inference\nmodel_name = \'model_en.pth\'\nmodel_path = f\'./ml/models/{model_name}\'\nmodel = CharacterLevelCNN()\n\n\nif model_name not in os.listdir(\'./ml/models/\'):\n    print(f\'downloading the trained model {model_name}\')\n    wget.download(\n        ""https://github.com/ahmedbesbes/character-based-cnn/releases/download/model_en_tp_amazon/model_tp_amazon_1014.pth"",\n        out=model_path\n    )\nelse:\n    print(\'model already saved to api/ml/models\')\n\nif torch.cuda.is_available():\n    trained_weights = torch.load(model_path)\nelse:\n    trained_weights = torch.load(model_path, map_location=\'cpu\')\n\nmodel.load_state_dict(trained_weights)\nmodel.eval()\nprint(\'PyTorch model loaded !\')\n\n\n@api.route(\'/predict\', methods=[\'POST\'])\ndef predict_rating():\n    \'\'\'\n    Endpoint to predict the rating using the\n    review\'s text data.\n    \'\'\'\n    if request.method == \'POST\':\n        if \'review\' not in request.form:\n            return jsonify({\'error\': \'no review in body\'}), 400\n        else:\n            parameters = model.get_model_parameters()\n            review = request.form[\'review\']\n            output = predict_sentiment(model, review, **parameters)\n            return jsonify(float(output))\n\n\n@api.route(\'/review\', methods=[\'POST\'])\ndef post_review():\n    \'\'\'\n    Save review to database.\n    \'\'\'\n    if request.method == \'POST\':\n        expected_fields = [\n            \'review\',\n            \'rating\',\n            \'suggested_rating\',\n            \'sentiment_score\',\n            \'brand\',\n            \'user_agent\',\n            \'ip_address\'\n        ]\n        if any(field not in request.form for field in expected_fields):\n            return jsonify({\'error\': \'Missing field in body\'}), 400\n\n        query = db.Review.create(**request.form)\n\n        return jsonify(query.serialize())\n\n\n@api.route(\'/reviews\', methods=[\'GET\'])\ndef get_reviews():\n    \'\'\'\n    Get all reviews.\n    \'\'\'\n    if request.method == \'GET\':\n        query = db.Review.select().order_by(db.Review.created_date.desc())\n\n        return jsonify([r.serialize() for r in query])\n\n\napp.register_blueprint(api, url_prefix=\'/api\')\n\nif __name__ == \'__main__\':\n    app.run(debug=config.DEBUG, host=config.HOST)\n'"
src/api/config.py,0,"b'import os\n\nENVIRONMENT = os.environ.get(""ENVIRONMENT"", ""dev"")\nDEBUG = ENVIRONMENT == ""dev""\nHOST = \'0.0.0.0\' if ENVIRONMENT == ""prod"" else \'localhost\'\nPOSTGRES_DB = os.environ.get(""POSTGRES_DB"", ""postgres"")\nPOSTGRES_USER = os.environ.get(""POSTGRES_USER"", ""postgres"")\nPOSTGRES_PASSWORD = os.environ.get(""POSTGRES_PASSWORD"", ""password"")\nPOSTGRES_HOST = os.environ.get(""POSTGRES_HOST"", ""localhost"")\nPOSTGRES_PORT = os.environ.get(""POSTGRES_PORT"", 5432)\n'"
src/api/db.py,0,"b'import peewee as pw\nimport config\nfrom datetime import datetime\nfrom playhouse.shortcuts import model_to_dict\n\n\ndb = pw.PostgresqlDatabase(\n    config.POSTGRES_DB,\n    user=config.POSTGRES_USER, password=config.POSTGRES_PASSWORD,\n    host=config.POSTGRES_HOST, port=config.POSTGRES_PORT\n)\n\n\nclass BaseModel(pw.Model):\n    class Meta:\n        database = db\n\n\n# Table Description\nclass Review(BaseModel):\n\n    review = pw.TextField()\n    rating = pw.IntegerField()\n    suggested_rating = pw.IntegerField()\n    sentiment_score = pw.FloatField()\n    brand = pw.TextField()\n    user_agent = pw.TextField()\n    ip_address = pw.TextField()\n    created_date = pw.DateTimeField(default=datetime.now)\n\n    def serialize(self):\n        review_dict = model_to_dict(self)\n        review_dict[""created_date""] = (\n            review_dict[""created_date""].strftime(\'%Y-%m-%d %H:%M:%S\')\n        )\n\n        return review_dict\n\n\n# Connection and table creation\ndb.connect()\ndb.create_tables([Review])\n'"
src/dash/app.py,0,"b'import os\nimport requests\nimport time\nimport pandas as pd\nimport config\nfrom flask import request\nimport dash\nimport dash_core_components as dcc\nimport dash_bootstrap_components as dbc\nimport dash_html_components as html\nimport dash_table\nfrom dash.dependencies import Input, Output, State\n\nexternal_stylesheets = [\n    ""https://use.fontawesome.com/releases/v5.0.7/css/all.css"",\n    \'https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css\',\n    \'https://fonts.googleapis.com/css?family=Roboto&display=swap\'\n]\n\nexternal_script = ""https://raw.githubusercontent.com/MarwanDebbiche/post-tuto-deployment/master/src/dash/assets/gtag.js""\n\napp = dash.Dash(\n    __name__, \n    external_stylesheets=external_stylesheets,\n    meta_tags=[\n        {""name"": ""viewport"", ""content"": ""width=device-width, initial-scale=1""}\n    ],\n    suppress_callback_exceptions=True\n)\n\napp.scripts.append_script({\n    ""external_url"": external_script\n})\n\napp.title = \'Reviews AI2Prod\'\n\ncompanies = pd.read_csv(\'./csv/companies_forbes.csv\')\nrandom_reviews = pd.read_csv(\'./csv/random_reviews.csv\')\n\napp.layout = html.Div([\n    dcc.Location(id=\'url\', refresh=False),\n    html.Div(id=\'page-content\')\n])\n\nhome_layout = html.Div(\n    [\n        html.Div(\n            [\n                html.A(\n                    html.Img(\n                        id=\'company_logo\',\n                        style={\n                            \'height\': \'100px\',\n                            \'padding\': \'5px\'\n                        }\n                    ),\n                    id=""company_link"",\n                    target=""_blank""\n                )\n            ],\n            style={\n                \'height\': \'100px\',\n                \'backgroundColor\': \'white\',\n                \'borderStyle\': \'solid\',\n                \'borderRadius\': \'100px\',\n                \'borderWidth\': \'thin\'\n            }\n        ),\n\n        html.H1(\n            [\n                ""What do you think of "",\n                html.Span(\n                    id=\'company_name\'\n                ),\n                "" ?""\n            ],\n            className=""h3 mb-3 font-weight-normal"",\n            style={\n                \'marginTop\': \'5px\'\n            }\n        ),\n\n        html.Div(\n            [\n                dcc.Textarea(\n                    className=""form-control z-depth-1"",\n                    id=""review"",\n                    rows=""8"",\n                    placeholder=""Write something here...""\n                )\n            ],\n            className=""form-group shadow-textarea""\n        ),\n\n        html.H5(\n            \'Sentiment analysis \xf0\x9f\xa4\x96\'\n        ),\n\n        dbc.Progress(\n            children=html.Span(\n                id=\'proba\',\n                style={\n                    \'color\': \'black\',\n                    \'fontWeight\': \'bold\'\n                }\n            ),\n            id=""progress"",\n            striped=False,\n            animated=False,\n            style={\n                \'marginBottom\': \'10px\'\n            }\n        ),\n\n        html.H5(\n            \'Propose a rating \xf0\x9f\x98\x81\xf0\x9f\x93\xa2\'\n        ),\n\n        html.Div(\n            [\n                dcc.Slider(\n                    id=\'rating\',\n                    max=5,\n                    min=1,\n                    step=1,\n                    marks={i: f\'{i}\' for i in range(1, 6)}\n                ),\n            ],\n            style={\'marginBottom\': \'30px\'}\n        ),\n\n        html.Button(\n            [\n                html.Span(\n                    ""Submit"",\n                    style={\n                        ""marginRight"": ""10px""\n                    }\n                ),\n                html.I(\n                    className=""fa fa-paper-plane m-l-7""\n                )\n            ],\n            className=""btn btn-lg btn-primary btn-block"",\n            role=""submit"",\n            id=""submit_button"",\n            n_clicks_timestamp=0\n        ),\n        html.Button(\n            [\n                html.Span(\n                    ""Review another brand"",\n                    style={\n                        ""marginRight"": ""10px""\n                    }\n                ),\n                html.I(\n                    className=""fas fa-sync-alt""\n                )\n            ],\n            className=""btn btn-lg btn-secondary btn-block"",\n            id=\'switch_button\',\n            n_clicks_timestamp=0\n        ),\n        html.P(\n            dcc.Link(""Go to Admin \xf0\x9f\x94\x91"", id=""admin-link"", href=""/admin""),\n            className=""mt-2""\n\n        ),\n        html.P(\n            [\n                html.A(""BESBES"", href=""https://ahmedbesbes.com"", target=""_blank""),\n                "" / "",\n                html.A(""DEBBICHE"", href=""https://marwandebbiche.com"",\n                       target=""_blank""),\n                "" - 2019""\n            ],\n            className=""mt-3 mb-2 text-muted""\n        ),\n    ],\n    className=""form-review"",\n)\n\nadmin_layout = html.Div(\n    [\n        html.H1(""Admin Page \xf0\x9f\x94\x91""),\n        html.Div(id=""admin-page-content""),\n        html.P(\n            dcc.Link(""Go to Home \xf0\x9f\x8f\xa1"", href=""/""),\n            style={""marginTop"": ""20px""}\n        )\n    ]\n)\n\n\n@app.callback(\n    [\n        Output(\'company_logo\', \'src\'),\n        Output(\'company_name\', \'children\'),\n        Output(\'review\', \'value\'),\n        Output(\'company_link\', \'href\')\n    ],\n    [\n        Input(\'submit_button\', \'n_clicks_timestamp\'),\n        Input(\'switch_button\', \'n_clicks_timestamp\')\n    ],\n    [\n        State(\'review\', \'value\'),\n        State(\'progress\', \'value\'),\n        State(\'rating\', \'value\'),\n        State(\'company_name\', \'children\')\n    ]\n)\ndef change_brand(submit_click_ts, another_brand_click_ts, review_text, score, rating, brand_name):\n    if submit_click_ts > another_brand_click_ts:\n        sentiment_score = float(score) / 100\n        ip_address = request.remote_addr\n        user_agent = request.headers.get(\'User-Agent\')\n        response = requests.post(\n            f""{config.API_URL}/review"",\n            data={\n                \'review\': review_text,\n                \'rating\': rating,\n                \'suggested_rating\': min(int(sentiment_score * 5 + 1), 5),\n                \'sentiment_score\': sentiment_score,\n                \'brand\': brand_name,\n                \'user_agent\': user_agent,\n                \'ip_address\': ip_address\n            }\n        )\n\n        if response.ok:\n            print(""Review Saved"")\n        else:\n            print(""Error Saving Review"")\n\n    random_company = companies.sample(1).to_dict(orient=""records"")[0]\n\n    company_logo_url = random_company[\'company_logo\']\n    if not company_logo_url.startswith(\'http\'):\n        company_logo_url = \'https://\' + company_logo_url\n\n    company_name = random_company[\'company_name\']\n    company_website = random_company[\'company_website\']\n\n    return company_logo_url, company_name, \'\', company_website\n\n\n@app.callback(\n    [\n        Output(\'proba\', \'children\'),\n        Output(\'progress\', \'value\'),\n        Output(\'progress\', \'color\'),\n        Output(\'rating\', \'value\'),\n        Output(\'submit_button\', \'disabled\')\n    ],\n    [Input(\'review\', \'value\')]\n)\ndef update_proba(review):\n    if review is not None and review.strip() != \'\':\n        response = requests.post(\n            f""{config.API_URL}/predict"", data={\'review\': review})\n        proba = response.json()\n        proba = round(proba * 100, 2)\n        suggested_rating = min(int((proba / 100) * 5 + 1), 5)\n        text_proba = f""{proba}%""\n\n        if proba >= 67:\n            return text_proba, proba, \'success\', suggested_rating, False\n        elif 33 < proba < 67:\n            return text_proba, proba, \'warning\', suggested_rating, False\n        elif proba <= 33:\n            return text_proba, proba, \'danger\', suggested_rating, False\n    else:\n        return None, 0, None, 0, True\n\n\n# Load review table\n@app.callback(\n    Output(\'admin-page-content\', \'children\'),\n    [Input(\'url\', \'pathname\')]\n)\ndef load_review_table(pathname):\n    if pathname != ""/admin"":\n        return None\n\n    response = requests.get(f""{config.API_URL}/reviews"")\n\n    reviews = pd.DataFrame(response.json())\n\n    table = dbc.Table.from_dataframe(reviews,\n                                     striped=True,\n                                     bordered=True,\n                                     hover=True,\n                                     responsive=True,\n                                     header=[""id"", ""brand"", ""created_date"", ""review"",\n                                             ""rating"", ""suggested_rating"", ""sentiment_score""],\n                                     columns=[""id"", ""brand"", ""created_date"", ""review"",\n                                              ""rating"", ""suggested_rating"", ""sentiment_score""]\n                                     )\n\n    return table\n\n# Update page layout\n\n\n@app.callback(\n    Output(\'page-content\', \'children\'),\n    [Input(\'url\', \'pathname\')]\n)\ndef display_page(pathname):\n    if pathname == \'/\':\n        return home_layout\n    if pathname == ""/admin"":\n        return admin_layout\n    else:\n        return [\n            html.Div(\n                [\n                    html.Img(\n                        src=""./assets/404.png"",\n                        style={\n                            ""width"": ""50%""\n                        }\n                    ),\n                ],\n                className=""form-review""\n            ),\n            dcc.Link(""Go to Home"", href=""/""),\n        ]\n\n\nif __name__ == \'__main__\':\n    app.run_server(debug=config.DEBUG, host=config.HOST)\n'"
src/dash/config.py,0,"b'import os\n\nENVRIONMENT = os.environ.get(""ENVIRONMENT"", ""dev"")\nDEBUG = ENVRIONMENT == ""dev""\nHOST = \'0.0.0.0\' if ENVRIONMENT == ""prod"" else \'localhost\'\nAPI_URL = os.environ.get(""API_URL"", ""http://localhost:5000/api"")\n'"
src/training/train.py,18,"b'import os\nimport shutil\nimport json\nimport argparse\nimport time\nfrom datetime import datetime\nfrom collections import Counter\n\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\nfrom tensorboardX import SummaryWriter\n\nfrom sklearn.metrics import classification_report, f1_score\nfrom sklearn.model_selection import train_test_split\n\nfrom src.data_loader import MyDataset, load_data\nfrom src import utils\nfrom src.model import CharacterLevelCNN\nfrom src.focal_loss import FocalLoss\n\n\ndef train(model, training_generator, optimizer, criterion, epoch, writer, log_file, scheduler, class_names, args, print_every=25):\n    model.train()\n    losses = utils.AverageMeter()\n    accuracies = utils.AverageMeter()\n    num_iter_per_epoch = len(training_generator)\n\n    progress_bar = tqdm(enumerate(training_generator),\n                        total=num_iter_per_epoch)\n\n    y_true = []\n    y_pred = []\n\n    for iter, batch in progress_bar:\n        features, labels = batch\n        if torch.cuda.is_available():\n            features = features.cuda()\n            labels = labels.cuda()\n\n        optimizer.zero_grad()\n        predictions = model(features)\n\n        y_true += labels.cpu().numpy().tolist()\n        y_pred += torch.max(predictions, 1)[1].cpu().numpy().tolist()\n\n        loss = criterion(predictions, labels)\n\n        loss.backward()\n        if args.scheduler == \'clr\':\n            scheduler.step()\n\n        optimizer.step()\n        training_metrics = utils.get_evaluation(labels.cpu().numpy(),\n                                                predictions.cpu().detach().numpy(),\n                                                list_metrics=[""accuracy"", ""f1""])\n\n        losses.update(loss.data, features.size(0))\n        accuracies.update(training_metrics[""accuracy""], features.size(0))\n\n        f1 = training_metrics[\'f1\']\n\n        writer.add_scalar(\'Train/Loss\',\n                          loss.item(),\n                          epoch * num_iter_per_epoch + iter)\n\n        writer.add_scalar(\'Train/Accuracy\',\n                          training_metrics[\'accuracy\'],\n                          epoch * num_iter_per_epoch + iter)\n\n        writer.add_scalar(\'Train/f1\',\n                          f1,\n                          epoch * num_iter_per_epoch + iter)\n\n        lr = optimizer.state_dict()[""param_groups""][0][""lr""]\n\n        if (iter % print_every == 0) and (iter > 0):\n            print(""[Training - Epoch: {}], LR: {} , Iteration: {}/{} , Loss: {}, Accuracy: {}"".format(\n                epoch + 1,\n                lr,\n                iter,\n                num_iter_per_epoch,\n                losses.avg,\n                accuracies.avg\n            ))\n\n            if bool(args.log_f1):\n                intermediate_report = classification_report(\n                    y_true, y_pred, output_dict=True)\n\n                f1_by_class = \'F1 Scores by class: \'\n                for class_name in class_names:\n                    f1_by_class += f""{class_name} : {np.round(intermediate_report[class_name][\'f1-score\'], 4)} |""\n\n                print(f1_by_class)\n\n    f1_train = f1_score(y_true, y_pred, average=\'weighted\')\n\n    writer.add_scalar(\'Train/loss/epoch\', losses.avg, epoch + iter)\n    writer.add_scalar(\'Train/acc/epoch\', accuracies.avg, epoch + iter)\n    writer.add_scalar(\'Train/f1/epoch\', f1_train, epoch + iter)\n\n    report = classification_report(y_true, y_pred)\n    print(report)\n\n    with open(log_file, \'a\') as f:\n        f.write(f\'Training on Epoch {epoch} \\n\')\n        f.write(f\'Average loss: {losses.avg.item()} \\n\')\n        f.write(f\'Average accuracy: {accuracies.avg.item()} \\n\')\n        f.write(f\'F1 score: {f1_train} \\n\\n\')\n        f.write(report)\n        f.write(\'*\' * 25)\n        f.write(\'\\n\')\n\n    return losses.avg.item(), accuracies.avg.item(), f1_train\n\n\ndef evaluate(model, validation_generator, criterion, epoch, writer, log_file, print_every=25):\n    model.eval()\n    losses = utils.AverageMeter()\n    accuracies = utils.AverageMeter()\n    num_iter_per_epoch = len(validation_generator)\n\n    y_true = []\n    y_pred = []\n\n    for iter, batch in tqdm(enumerate(validation_generator), total=num_iter_per_epoch):\n        features, labels = batch\n        if torch.cuda.is_available():\n            features = features.cuda()\n            labels = labels.cuda()\n        with torch.no_grad():\n            predictions = model(features)\n        loss = criterion(predictions, labels)\n\n        y_true += labels.cpu().numpy().tolist()\n        y_pred += torch.max(predictions, 1)[1].cpu().numpy().tolist()\n\n        validation_metrics = utils.get_evaluation(labels.cpu().numpy(),\n                                                  predictions.cpu().detach().numpy(),\n                                                  list_metrics=[""accuracy"", ""f1""])\n        accuracy = validation_metrics[\'accuracy\']\n        f1 = validation_metrics[\'f1\']\n\n        losses.update(loss.data, features.size(0))\n        accuracies.update(validation_metrics[""accuracy""], features.size(0))\n\n        writer.add_scalar(\'Test/Loss\',\n                          loss.item(),\n                          epoch * num_iter_per_epoch + iter)\n\n        writer.add_scalar(\'Test/Accuracy\',\n                          accuracy,\n                          epoch * num_iter_per_epoch + iter)\n\n        writer.add_scalar(\'Test/f1\',\n                          f1,\n                          epoch * num_iter_per_epoch + iter)\n\n        if (iter % print_every == 0) and (iter > 0):\n            print(""[Validation - Epoch: {}] , Iteration: {}/{} , Loss: {}, Accuracy: {}"".format(\n                epoch + 1,\n                iter,\n                num_iter_per_epoch,\n                losses.avg,\n                accuracies.avg\n            ))\n\n    f1_test = f1_score(y_true, y_pred, average=\'weighted\')\n\n    writer.add_scalar(\'Test/loss/epoch\', losses.avg, epoch + iter)\n    writer.add_scalar(\'Test/acc/epoch\', accuracies.avg, epoch + iter)\n    writer.add_scalar(\'Test/f1/epoch\', f1_test, epoch + iter)\n\n    report = classification_report(y_true, y_pred)\n    print(report)\n\n    with open(log_file, \'a\') as f:\n        f.write(f\'Validation on Epoch {epoch} \\n\')\n        f.write(f\'Average loss: {losses.avg.item()} \\n\')\n        f.write(f\'Average accuracy: {accuracies.avg.item()} \\n\')\n        f.write(f\'F1 score {f1_test} \\n\\n\')\n        f.write(report)\n        f.write(\'=\' * 50)\n        f.write(\'\\n\')\n\n    return losses.avg.item(), accuracies.avg.item(), f1_test\n\n\ndef run(args, both_cases=False):\n\n    if args.flush_history == 1:\n        objects = os.listdir(args.log_path)\n        for f in objects:\n            if os.path.isdir(args.log_path + f):\n                shutil.rmtree(args.log_path + f)\n\n    now = datetime.now()\n    logdir = args.log_path + now.strftime(""%Y%m%d-%H%M%S"") + ""/""\n    os.makedirs(logdir)\n    log_file = logdir + \'log.txt\'\n    writer = SummaryWriter(logdir)\n\n    batch_size = args.batch_size\n\n    training_params = {""batch_size"": batch_size,\n                       ""shuffle"": True,\n                       ""num_workers"": args.workers,\n                       ""drop_last"": True}\n\n    validation_params = {""batch_size"": batch_size,\n                         ""shuffle"": False,\n                         ""num_workers"": args.workers,\n                         ""drop_last"": True}\n\n    texts, labels, number_of_classes, sample_weights = load_data(args)\n\n    class_names = sorted(list(set(labels)))\n    class_names = [str(class_name) for class_name in class_names]\n\n    train_texts, val_texts, train_labels, val_labels, train_sample_weights, _ = train_test_split(texts,\n                                                                                                 labels,\n                                                                                                 sample_weights,\n                                                                                                 test_size=args.validation_split,\n                                                                                                 random_state=42,\n                                                                                                 stratify=labels)\n    training_set = MyDataset(train_texts, train_labels, args)\n    validation_set = MyDataset(val_texts, val_labels, args)\n\n    if bool(args.use_sampler):\n        train_sample_weights = torch.from_numpy(train_sample_weights)\n        sampler = WeightedRandomSampler(train_sample_weights.type(\n            \'torch.DoubleTensor\'), len(train_sample_weights))\n        training_params[\'sampler\'] = sampler\n        training_params[\'shuffle\'] = False\n\n    training_generator = DataLoader(training_set, **training_params)\n    validation_generator = DataLoader(validation_set, **validation_params)\n\n    model = CharacterLevelCNN(args, number_of_classes)\n    if torch.cuda.is_available():\n        model.cuda()\n\n    if not bool(args.focal_loss):\n        if bool(args.class_weights):\n            class_counts = dict(Counter(train_labels))\n            m = max(class_counts.values())\n            for c in class_counts:\n                class_counts[c] = m / class_counts[c]\n            weights = []\n            for k in sorted(class_counts.keys()):\n                weights.append(class_counts[k])\n\n            weights = torch.Tensor(weights)\n            if torch.cuda.is_available():\n                weights = weights.cuda()\n                print(f\'passing weights to CrossEntropyLoss : {weights}\')\n                criterion = nn.CrossEntropyLoss(weight=weights)\n        else:\n            criterion = nn.CrossEntropyLoss()\n\n    else:\n        if args.alpha is None:\n            criterion = FocalLoss(gamma=args.gamma, alpha=None)\n        else:\n            criterion = FocalLoss(gamma=args.gamma,\n                                  alpha=[args.alpha] * number_of_classes)\n\n    if args.optimizer == \'sgd\':\n        if args.scheduler == \'clr\':\n            optimizer = torch.optim.SGD(\n                model.parameters(), lr=1, momentum=0.9, weight_decay=0.00001\n            )\n        else:\n            optimizer = torch.optim.SGD(\n                model.parameters(), lr=args.learning_rate, momentum=0.9\n            )\n    elif args.optimizer == \'adam\':\n        optimizer = torch.optim.Adam(\n            model.parameters(), lr=args.learning_rate\n        )\n\n    best_f1 = 0\n    best_epoch = 0\n\n    if args.scheduler == \'clr\':\n        stepsize = int(args.stepsize * len(training_generator))\n        clr = utils.cyclical_lr(stepsize, args.min_lr, args.max_lr)\n        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, [clr])\n    else:\n        scheduler = None\n\n    for epoch in range(args.epochs):\n        training_loss, training_accuracy, train_f1 = train(model,\n                                                           training_generator,\n                                                           optimizer,\n                                                           criterion,\n                                                           epoch,\n                                                           writer,\n                                                           log_file,\n                                                           scheduler,\n                                                           class_names,\n                                                           args,\n                                                           args.log_every)\n\n        validation_loss, validation_accuracy, validation_f1 = evaluate(model,\n                                                                       validation_generator,\n                                                                       criterion,\n                                                                       epoch,\n                                                                       writer,\n                                                                       log_file,\n                                                                       args.log_every)\n\n        print(\'[Epoch: {} / {}]\\ttrain_loss: {:.4f} \\ttrain_acc: {:.4f} \\tval_loss: {:.4f} \\tval_acc: {:.4f}\'.\n              format(epoch + 1, args.epochs, training_loss, training_accuracy, validation_loss, validation_accuracy))\n        print(""="" * 50)\n\n        # learning rate scheduling\n\n        if args.scheduler == \'step\':\n            if args.optimizer == \'sgd\' and ((epoch + 1) % 3 == 0) and epoch > 0:\n                current_lr = optimizer.state_dict()[\'param_groups\'][0][\'lr\']\n                current_lr /= 2\n                print(\'Decreasing learning rate to {0}\'.format(current_lr))\n                for param_group in optimizer.param_groups:\n                    param_group[\'lr\'] = current_lr\n\n        # model checkpoint\n\n        if validation_f1 > best_f1:\n            best_f1 = validation_f1\n            best_epoch = epoch\n            if args.checkpoint == 1:\n                torch.save(model.state_dict(), args.output + \'model_{}_epoch_{}_maxlen_{}_lr_{}_loss_{}_acc_{}_f1_{}.pth\'.format(args.model_name,\n                                                                                                                                 epoch,\n                                                                                                                                 args.max_length,\n                                                                                                                                 optimizer.state_dict()[\n                                                                                                                                     \'param_groups\'][0][\'lr\'],\n                                                                                                                                 round(\n                                                                                                                                     validation_loss, 4),\n                                                                                                                                 round(\n                                                                                                                                     validation_accuracy, 4),\n                                                                                                                                 round(\n                                                                                                                                     validation_f1, 4)\n                                                                                                                                 ))\n\n        if bool(args.early_stopping):\n            if epoch - best_epoch > args.patience > 0:\n                print(""Stop training at epoch {}. The lowest loss achieved is {} at epoch {}"".format(\n                    epoch, validation_loss, best_epoch))\n                break\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\n        \'Character Based CNN for text classification\')\n    parser.add_argument(\'--data_path\', type=str,\n                        default=\'./data/train.csv\')\n    parser.add_argument(\'--validation_split\', type=float, default=0.2)\n    parser.add_argument(\'--label_column\', type=str, default=\'Sentiment\')\n    parser.add_argument(\'--text_column\', type=str, default=\'SentimentText\')\n    parser.add_argument(\'--max_rows\', type=int, default=None)\n    parser.add_argument(\'--chunksize\', type=int, default=50000)\n    parser.add_argument(\'--encoding\', type=str, default=\'utf-8\')\n    parser.add_argument(\'--sep\', type=str, default=\',\')\n    parser.add_argument(\'--steps\', nargs=\'+\', default=[\'lower\'])\n    parser.add_argument(\'--group_labels\', type=int, default=1, choices=[0, 1])\n    parser.add_argument(\'--ignore_center\', type=int, default=1, choices=[0, 1])\n    parser.add_argument(\'--label_ignored\', type=int, default=None)\n    parser.add_argument(\'--ratio\', type=float, default=1)\n    parser.add_argument(\'--balance\', type=int, default=0, choices=[0, 1])\n    parser.add_argument(\'--use_sampler\', type=int,\n                        default=0, choices=[0, 1])\n\n    parser.add_argument(\'--alphabet\', type=str,\n                        default=""abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:\'\\""/\\\\|_@#$%^&*~`+ =<>()[]{}"")\n    parser.add_argument(\'--number_of_characters\', type=int, default=69)\n    parser.add_argument(\'--extra_characters\', type=str, default=\'\')\n    parser.add_argument(\'--max_length\', type=int, default=150)\n    parser.add_argument(\'--dropout_input\', type=float, default=0.1)\n    parser.add_argument(\'--epochs\', type=int, default=10)\n    parser.add_argument(\'--batch_size\', type=int, default=128)\n    parser.add_argument(\'--optimizer\', type=str,\n                        choices=[\'adam\', \'sgd\'], default=\'sgd\')\n    parser.add_argument(\'--learning_rate\', type=float, default=0.01)\n    parser.add_argument(\'--class_weights\', type=int,\n                        default=0, choices=[0, 1])\n    parser.add_argument(\'--focal_loss\', type=int, default=0, choices=[0, 1])\n    parser.add_argument(\'--gamma\', type=float, default=2)\n    parser.add_argument(\'--alpha\', type=float, default=None)\n\n    parser.add_argument(\'--scheduler\', type=str,\n                        default=\'step\', choices=[\'clr\', \'step\'])\n    parser.add_argument(\'--min_lr\', type=float, default=1.7e-3)\n    parser.add_argument(\'--max_lr\', type=float, default=1e-2)\n    parser.add_argument(\'--stepsize\', type=float, default=4)\n    parser.add_argument(\'--patience\', type=int, default=3)\n    parser.add_argument(\'--early_stopping\', type=int,\n                        default=0, choices=[0, 1])\n    parser.add_argument(\'--checkpoint\', type=int,\n                        choices=[0, 1], default=1)\n    parser.add_argument(\'--workers\', type=int, default=1)\n    parser.add_argument(\'--log_path\', type=str, default=\'./logs/\')\n    parser.add_argument(\'--log_every\', type=int, default=100)\n    parser.add_argument(\'--log_f1\', type=int, default=1, choices=[0, 1])\n    parser.add_argument(\'--flush_history\', type=int,\n                        default=1, choices=[0, 1])\n    parser.add_argument(\'--output\', type=str, default=\'./models/\')\n    parser.add_argument(\'--model_name\', type=str, default=\'\')\n\n    args = parser.parse_args()\n    run(args)\n'"
src/api/ml/__init__.py,0,b''
src/api/ml/model.py,2,"b'import json\nimport torch\nimport torch.nn as nn\n\n\nclass CharacterLevelCNN(nn.Module):\n    def __init__(self):\n        super(CharacterLevelCNN, self).__init__()\n\n        # model parameters\n        self.number_of_characters = 69\n        # self.extra_characters = ""\xc3\xa9\xc3\xa0\xc3\xa8\xc3\xb9\xc3\xa2\xc3\xaa\xc3\xae\xc3\xb4\xc3\xbb\xc3\xa7\xc3\xab\xc3\xaf\xc3\xbc""\n        self.extra_characters = """"\n        self.alphabet = ""abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:\'\\""/\\\\|_@#$%^&*~`+ =<>()[]{}""\n        self.max_length = 1014\n        self.number_of_classes = 3\n        self.dropout_input_p = 0\n\n        # define dropout input\n\n        self.dropout_input = nn.Dropout2d(self.dropout_input_p)\n\n        # define conv layers\n\n        self.conv1 = nn.Sequential(nn.Conv1d(self.number_of_characters + len(self.extra_characters),\n                                             256,\n                                             kernel_size=7,\n                                             padding=0),\n                                   nn.ReLU(),\n                                   nn.MaxPool1d(3)\n                                   )\n\n        self.conv2 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=7, padding=0),\n                                   nn.ReLU(),\n                                   nn.MaxPool1d(3)\n                                   )\n\n        self.conv3 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0),\n                                   nn.ReLU()\n                                   )\n\n        self.conv4 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0),\n                                   nn.ReLU()\n                                   )\n\n        self.conv5 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0),\n                                   nn.ReLU()\n                                   )\n\n        self.conv6 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0),\n                                   nn.ReLU(),\n                                   nn.MaxPool1d(3)\n                                   )\n\n        # compute the  output shape after forwarding an input to the conv layers\n\n        input_shape = (128,\n                       self.max_length,\n                       self.number_of_characters + len(self.extra_characters))\n        self.output_dimension = self._get_conv_output(input_shape)\n\n        # define linear layers\n\n        self.fc1 = nn.Sequential(\n            nn.Linear(self.output_dimension, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.5)\n        )\n\n        self.fc2 = nn.Sequential(\n            nn.Linear(1024, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.5)\n        )\n\n        self.fc3 = nn.Linear(1024, self.number_of_classes)\n\n        # initialize weights\n\n        self._create_weights()\n\n    # utility private functions\n\n    def _create_weights(self, mean=0.0, std=0.05):\n        for module in self.modules():\n            if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n                module.weight.data.normal_(mean, std)\n\n    def _get_conv_output(self, shape):\n        x = torch.rand(shape)\n        x = x.transpose(1, 2)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        x = self.conv6(x)\n        x = x.view(x.size(0), -1)\n        output_dimension = x.size(1)\n        return output_dimension\n\n    # get model params:\n\n    def get_model_parameters(self):\n        return {\n            \'alphabet\': self.alphabet,\n            \'extra_characters\': self.extra_characters,\n            \'number_of_characters\': self.number_of_characters,\n            \'max_length\': self.max_length,\n            \'num_classes\': self.number_of_classes\n        }\n\n    # forward\n\n    def forward(self, x):\n        x = self.dropout_input(x)\n        x = x.transpose(1, 2)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        x = self.conv6(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return x\n'"
src/api/ml/utils.py,3,"b'import numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\ndef predict_sentiment(model, text, alphabet, extra_characters, number_of_characters, max_length, num_classes):\n\n    text = text.lower()\n    text = text.strip()\n\n    number_of_characters = number_of_characters + len(extra_characters)\n    identity_mat = np.identity(number_of_characters)\n    vocabulary = list(alphabet) + list(extra_characters)\n    max_length = max_length\n\n    processed_output = np.array([identity_mat[vocabulary.index(i)] for i in list(\n        text[::-1]) if i in vocabulary], dtype=np.float32)\n    if len(processed_output) > max_length:\n        processed_output = processed_output[:max_length]\n    elif 0 < len(processed_output) < max_length:\n        processed_output = np.concatenate((processed_output, np.zeros(\n            (max_length - len(processed_output), number_of_characters), dtype=np.float32)))\n    elif len(processed_output) == 0:\n        processed_output = np.zeros(\n            (max_length, number_of_characters), dtype=np.float32)\n\n    processed_output = torch.tensor(processed_output)\n    processed_output = processed_output.unsqueeze(0)\n\n    prediction = model(processed_output)\n    probabilities = F.softmax(prediction, dim=1)\n    proba, index = torch.max(probabilities, dim=1)\n    proba = proba.item()\n    index = index.item()\n\n    if num_classes == 3:\n\n        if index == 0:\n            score = (0.33 - 0) * (1 - proba) + 0\n\n        elif index == 1:\n            score = (0.67 - 0.33) * proba + 0.33\n\n        elif index == 2:\n            score = (1 - 0.67) * proba + 0.67\n        \n    elif num_classes == 2:\n        score = proba\n\n    return score\n'"
src/training/src/__init__.py,0,b''
src/training/src/data_loader.py,2,"b""import json\r\nimport numpy as np\r\nfrom collections import Counter\r\n\r\nfrom torch.utils.data import Dataset\r\nimport pandas as pd\r\nfrom tqdm import tqdm\r\nfrom . import utils\r\n\r\nimport torch\r\n\r\n\r\ndef get_sample_weights(labels):\r\n    counter = Counter(labels)\r\n    counter = dict(counter)\r\n    for k in counter:\r\n        counter[k] = 1 / counter[k]\r\n    sample_weights = np.array([counter[l] for l in labels])\r\n    return sample_weights\r\n\r\n\r\ndef load_data(args):\r\n    # chunk your dataframes in small portions\r\n    chunks = pd.read_csv(args.data_path,\r\n                         usecols=[args.text_column, args.label_column],\r\n                         chunksize=args.chunksize,\r\n                         encoding=args.encoding,\r\n                         nrows=args.max_rows,\r\n                         sep=args.sep)\r\n    texts = []\r\n    labels = []\r\n    for df_chunk in tqdm(chunks):\r\n        aux_df = df_chunk.copy()\r\n        aux_df = aux_df.sample(frac=1)\r\n        aux_df = aux_df[~aux_df[args.text_column].isnull()]\r\n        aux_df = aux_df[(aux_df[args.text_column].map(len) > 1)]\r\n        aux_df['processed_text'] = (aux_df[args.text_column]\r\n                                    .map(lambda text: utils.process_text(args.steps, text)))\r\n        texts += aux_df['processed_text'].tolist()\r\n        labels += aux_df[args.label_column].tolist()\r\n\r\n    if bool(args.group_labels):\r\n\r\n        if bool(args.ignore_center):\r\n\r\n            label_ignored = args.label_ignored\r\n\r\n            clean_data = [(text, label) for (text, label) in zip(\r\n                texts, labels) if label not in [label_ignored]]\r\n\r\n            texts = [text for (text, label) in clean_data]\r\n            labels = [label for (text, label) in clean_data]\r\n\r\n            labels = list(\r\n                map(lambda l: {1: 0, 2: 0, 4: 1, 5: 1}[l], labels))\r\n\r\n        else:\r\n            labels = list(\r\n                map(lambda l: {1: 0, 2: 0, 3: 1, 4: 2, 5: 2}[l], labels))\r\n        \r\n    if bool(args.balance):\r\n\r\n        counter = Counter(labels)\r\n        keys = list(counter.keys())\r\n        values = list(counter.values())\r\n        count_minority = np.min(values)\r\n\r\n        balanced_labels = []\r\n        balanced_texts = []\r\n\r\n        for key in keys: \r\n            balanced_texts += [text for text, label in zip(texts, labels) if label == key][:int(args.ratio * count_minority)]\r\n            balanced_labels += [label for text, label in zip(texts, labels) if label == key][:int(args.ratio * count_minority)] \r\n\r\n        texts = balanced_texts\r\n        labels = balanced_labels\r\n\r\n    number_of_classes = len(set(labels))\r\n\r\n    print(\r\n        f'data loaded successfully with {len(texts)} rows and {number_of_classes} labels')\r\n    print('Distribution of the classes', Counter(labels))\r\n\r\n    sample_weights = get_sample_weights(labels)\r\n\r\n    return texts, labels, number_of_classes, sample_weights\r\n\r\n\r\nclass MyDataset(Dataset):\r\n    def __init__(self, texts, labels, args):\r\n        self.texts = texts\r\n        self.labels = labels\r\n        self.length = len(self.texts)\r\n\r\n        self.vocabulary = args.alphabet + args.extra_characters\r\n        self.number_of_characters = args.number_of_characters + \\\r\n            len(args.extra_characters)\r\n        self.max_length = args.max_length\r\n        self.preprocessing_steps = args.steps\r\n        self.identity_mat = np.identity(self.number_of_characters)\r\n\r\n    def __len__(self):\r\n        return self.length\r\n\r\n    def __getitem__(self, index):\r\n        raw_text = self.texts[index]\r\n\r\n        data = np.array([self.identity_mat[self.vocabulary.index(i)] for i in list(raw_text)[::-1] if i in self.vocabulary],\r\n                        dtype=np.float32)\r\n        if len(data) > self.max_length:\r\n            data = data[:self.max_length]\r\n        elif 0 < len(data) < self.max_length:\r\n            data = np.concatenate(\r\n                (data, np.zeros((self.max_length - len(data), self.number_of_characters), dtype=np.float32)))\r\n        elif len(data) == 0:\r\n            data = np.zeros(\r\n                (self.max_length, self.number_of_characters), dtype=np.float32)\r\n\r\n        label = self.labels[index]\r\n        data = torch.Tensor(data)\r\n\r\n        return data, label\r\n"""
src/training/src/focal_loss.py,5,"b'import torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport torch.nn as nn\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=0, alpha=None, size_average=True):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        if isinstance(alpha, (float, int)):\n            self.alpha = torch.Tensor([alpha, 1-alpha])\n        if isinstance(alpha, list):\n            self.alpha = torch.Tensor(alpha)\n        self.size_average = size_average\n\n    def forward(self, input, target):\n        if input.dim() > 2:\n            # N,C,H,W => N,C,H*W\n            input = input.view(input.size(0), input.size(1), -1)\n            input = input.transpose(1, 2)    # N,C,H*W => N,H*W,C\n            input = input.contiguous().view(-1, input.size(2))   # N,H*W,C => N*H*W,C\n        target = target.view(-1, 1)\n\n        logpt = F.log_softmax(input, dim=1)\n        logpt = logpt.gather(1, target)\n        logpt = logpt.view(-1)\n        pt = Variable(logpt.data.exp())\n\n        if self.alpha is not None:\n            if self.alpha.type() != input.data.type():\n                self.alpha = self.alpha.type_as(input.data)\n            at = self.alpha.gather(0, target.data.view(-1))\n            logpt = logpt * Variable(at)\n\n        loss = -1 * (1-pt)**self.gamma * logpt\n        if self.size_average:\n            return loss.mean()\n        else:\n            return loss.sum()\n'"
src/training/src/model.py,2,"b'import json\nimport torch\nimport torch.nn as nn\n\n\nclass CharacterLevelCNN(nn.Module):\n    def __init__(self, args, number_of_classes):\n        super(CharacterLevelCNN, self).__init__()\n\n        # define conv layers\n\n        self.dropout_input = nn.Dropout2d(args.dropout_input)\n\n        self.conv1 = nn.Sequential(nn.Conv1d(args.number_of_characters + len(args.extra_characters),\n                                             256,\n                                             kernel_size=7,\n                                             padding=0),\n                                   nn.ReLU(),\n                                   nn.MaxPool1d(3)\n                                   )\n\n        self.conv2 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=7, padding=0),\n                                   nn.ReLU(),\n                                   nn.MaxPool1d(3)\n                                   )\n\n        self.conv3 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0),\n                                   nn.ReLU()\n                                   )\n\n        self.conv4 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0),\n                                   nn.ReLU()\n                                   )\n\n        self.conv5 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0),\n                                   nn.ReLU()\n                                   )\n\n        self.conv6 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=0),\n                                   nn.ReLU(),\n                                   nn.MaxPool1d(3)\n                                   )\n\n        # compute the  output shape after forwarding an input to the conv layers\n\n        input_shape = (128,\n                       args.max_length,\n                       args.number_of_characters + len(args.extra_characters))\n        self.output_dimension = self._get_conv_output(input_shape)\n\n        # define linear layers\n\n        self.fc1 = nn.Sequential(\n            nn.Linear(self.output_dimension, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.5)\n        )\n\n        self.fc2 = nn.Sequential(\n            nn.Linear(1024, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.5)\n        )\n\n        self.fc3 = nn.Linear(1024, number_of_classes)\n\n        # initialize weights\n\n        self._create_weights()\n\n    # utility private functions\n\n    def _create_weights(self, mean=0.0, std=0.05):\n        for module in self.modules():\n            if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n                module.weight.data.normal_(mean, std)\n\n\n    def _get_conv_output(self, shape):\n        x = torch.rand(shape)\n        x = x.transpose(1, 2)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        x = self.conv6(x)\n        x = x.view(x.size(0), -1)\n        output_dimension = x.size(1)\n        return output_dimension\n\n    # forward\n\n    def forward(self, x):\n        x = self.dropout_input(x)\n        x = x.transpose(1, 2)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        x = self.conv6(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return x\n\n'"
src/training/src/utils.py,0,"b'import math\r\nimport json\r\nimport re\r\nimport numpy as np\r\nfrom sklearn import metrics\r\n\r\n# text-preprocessing\r\n\r\n\r\ndef lower(text):\r\n    return text.lower()\r\n\r\n\r\ndef remove_hashtags(text):\r\n    clean_text = re.sub(r\'#[A-Za-z0-9_]+\', """", text)\r\n    return clean_text\r\n\r\n\r\ndef remove_user_mentions(text):\r\n    clean_text = re.sub(r\'@[A-Za-z0-9_]+\', """", text)\r\n    return clean_text\r\n\r\n\r\ndef remove_urls(text):\r\n    clean_text = re.sub(r\'^https?:\\/\\/.*[\\r\\n]*\', \'\', text, flags=re.MULTILINE)\r\n    return clean_text\r\n\r\n\r\npreprocessing_setps = {\r\n    \'remove_hashtags\': remove_hashtags,\r\n    \'remove_urls\': remove_urls,\r\n    \'remove_user_mentions\': remove_user_mentions,\r\n    \'lower\': lower\r\n}\r\n\r\n\r\ndef process_text(steps, text):\r\n    if steps is not None:\r\n        for step in steps:\r\n            text = preprocessing_setps[step](text)\r\n    return text\r\n\r\n# metrics // model evaluations\r\n\r\n\r\ndef get_evaluation(y_true, y_prob, list_metrics):\r\n    y_pred = np.argmax(y_prob, -1)\r\n    output = {}\r\n    if \'accuracy\' in list_metrics:\r\n        output[\'accuracy\'] = metrics.accuracy_score(y_true, y_pred)\r\n    if \'f1\' in list_metrics:\r\n        output[\'f1\'] = metrics.f1_score(y_true, y_pred, average=\'weighted\')\r\n\r\n    return output\r\n\r\n\r\nclass AverageMeter(object):\r\n    """"""Computes and stores the average and current value""""""\r\n\r\n    def __init__(self):\r\n        self.reset()\r\n\r\n    def reset(self):\r\n        self.val = 0\r\n        self.avg = 0\r\n        self.sum = 0\r\n        self.count = 0\r\n\r\n    def update(self, val, n=1):\r\n        self.val = val\r\n        self.sum += val * n\r\n        self.count += n\r\n        self.avg = self.sum / self.count\r\n\r\n\r\ndef accuracy(output, target, topk=(1,)):\r\n    """"""Computes the precision@k for the specified values of k""""""\r\n    maxk = max(topk)\r\n    batch_size = target.size(0)\r\n\r\n    _, pred = output.topk(maxk, 1, True, True)\r\n    pred = pred.t()\r\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\r\n\r\n    res = []\r\n    for k in topk:\r\n        correct_k = correct[:k].view(-1).float().sum(0)\r\n        res.append(correct_k.mul_(100.0 / batch_size))\r\n    return res\r\n\r\n# preprocess input for prediction\r\n\r\n\r\ndef preprocess_input(args):\r\n    raw_text = args.text\r\n    steps = args.steps\r\n    for step in steps:\r\n        raw_text = preprocessing_setps[step](raw_text)\r\n\r\n    number_of_characters = args.number_of_characters + \\\r\n        len(args.extra_characters)\r\n    identity_mat = np.identity(number_of_characters)\r\n    vocabulary = list(args.alphabet) + list(args.extra_characters)\r\n    max_length = args.max_length\r\n\r\n    processed_output = np.array([identity_mat[vocabulary.index(i)] for i in list(\r\n        raw_text[::-1]) if i in vocabulary], dtype=np.float32)\r\n    if len(processed_output) > max_length:\r\n        processed_output = processed_output[:max_length]\r\n    elif 0 < len(processed_output) < max_length:\r\n        processed_output = np.concatenate((processed_output, np.zeros(\r\n            (max_length - len(processed_output), number_of_characters), dtype=np.float32)))\r\n    elif len(processed_output) == 0:\r\n        processed_output = np.zeros(\r\n            (max_length, number_of_characters), dtype=np.float32)\r\n    return processed_output\r\n\r\n\r\n# cyclic learning rate scheduling\r\n\r\ndef cyclical_lr(stepsize, min_lr=1.7e-3, max_lr=1e-2):\r\n\r\n    # Scaler: we can adapt this if we do not want the triangular CLR\r\n    def scaler(x): return 1.\r\n\r\n    # Lambda function to calculate the LR\r\n    def lr_lambda(it): return min_lr + (max_lr -\r\n                                        min_lr) * relative(it, stepsize)\r\n\r\n    # Additional function to see where on the cycle we are\r\n    def relative(it, stepsize):\r\n        cycle = math.floor(1 + it / (2 * stepsize))\r\n        x = abs(it / stepsize - 2 * cycle + 1)\r\n        return max(0, (1 - x)) * scaler(cycle)\r\n\r\n    return lr_lambda\r\n'"
src/scraping/scrapy/trustpilot/__init__.py,0,b''
src/scraping/scrapy/trustpilot/items.py,0,b'# -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# https://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\n\nclass TrustpilotItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    pass\n'
src/scraping/scrapy/trustpilot/middlewares.py,0,"b""# -*- coding: utf-8 -*-\n\n# Define here the models for your spider middleware\n#\n# See documentation in:\n# https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n\nfrom scrapy import signals\n\n\nclass TrustpilotSpiderMiddleware(object):\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the spider middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_spider_input(self, response, spider):\n        # Called for each response that goes through the spider\n        # middleware and into the spider.\n\n        # Should return None or raise an exception.\n        return None\n\n    def process_spider_output(self, response, result, spider):\n        # Called with the results returned from the Spider, after\n        # it has processed the response.\n\n        # Must return an iterable of Request, dict or Item objects.\n        for i in result:\n            yield i\n\n    def process_spider_exception(self, response, exception, spider):\n        # Called when a spider or process_spider_input() method\n        # (from other spider middleware) raises an exception.\n\n        # Should return either None or an iterable of Response, dict\n        # or Item objects.\n        pass\n\n    def process_start_requests(self, start_requests, spider):\n        # Called with the start requests of the spider, and works\n        # similarly to the process_spider_output() method, except\n        # that it doesn\xe2\x80\x99t have a response associated.\n\n        # Must return only requests (not items).\n        for r in start_requests:\n            yield r\n\n    def spider_opened(self, spider):\n        spider.logger.info('Spider opened: %s' % spider.name)\n\n\nclass TrustpilotDownloaderMiddleware(object):\n    # Not all methods need to be defined. If a method is not defined,\n    # scrapy acts as if the downloader middleware does not modify the\n    # passed objects.\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        # This method is used by Scrapy to create your spiders.\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n\n    def process_request(self, request, spider):\n        # Called for each request that goes through the downloader\n        # middleware.\n\n        # Must either:\n        # - return None: continue processing this request\n        # - or return a Response object\n        # - or return a Request object\n        # - or raise IgnoreRequest: process_exception() methods of\n        #   installed downloader middleware will be called\n        return None\n\n    def process_response(self, request, response, spider):\n        # Called with the response returned from the downloader.\n\n        # Must either;\n        # - return a Response object\n        # - return a Request object\n        # - or raise IgnoreRequest\n        return response\n\n    def process_exception(self, request, exception, spider):\n        # Called when a download handler or a process_request()\n        # (from other downloader middleware) raises an exception.\n\n        # Must either:\n        # - return None: continue processing this exception\n        # - return a Response object: stops process_exception() chain\n        # - return a Request object: stops process_exception() chain\n        pass\n\n    def spider_opened(self, spider):\n        spider.logger.info('Spider opened: %s' % spider.name)\n"""
src/scraping/scrapy/trustpilot/pipelines.py,0,"b""# -*- coding: utf-8 -*-\n\n# Define your item pipelines here\n#\n# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n\n\nclass TrustpilotPipeline(object):\n    def process_item(self, item, spider):\n        return item\n"""
src/scraping/scrapy/trustpilot/settings.py,0,"b'# -*- coding: utf-8 -*-\n\n# Scrapy settings for trustpilot project\n#\n# For simplicity, this file contains only settings considered important or\n# commonly used. You can find more settings consulting the documentation:\n#\n#     https://doc.scrapy.org/en/latest/topics/settings.html\n#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\n#     https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n\nBOT_NAME = \'trustpilot\'\n\nSPIDER_MODULES = [\'trustpilot.spiders\']\nNEWSPIDER_MODULE = \'trustpilot.spiders\'\n\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = \'trustpilot (+http://www.yourdomain.com)\'\n\n# Obey robots.txt rules\nROBOTSTXT_OBEY = False\n\n# Configure maximum concurrent requests performed by Scrapy (default: 16)\nCONCURRENT_REQUESTS = 200\n\n#Export to csv\nFEED_FORMAT = ""csv""\nFEED_URI = ""comments_trustpilot_v2.csv""\n\n\n# Configure a delay for requests for the same website (default: 0)\n# See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay\n# See also autothrottle settings and docs\n# DOWNLOAD_DELAY = 0.1\n# The download delay setting will honor only one of:\n# CONCURRENT_REQUESTS_PER_DOMAIN = 16\n# CONCURRENT_REQUESTS_PER_IP = 16\n\n# Disable cookies (enabled by default)\n# COOKIES_ENABLED = False\n\n# Disable Telnet Console (enabled by default)\n#TELNETCONSOLE_ENABLED = False\n\n# Override the default request headers:\n#DEFAULT_REQUEST_HEADERS = {\n#   \'Accept\': \'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\',\n#   \'Accept-Language\': \'en\',\n#}\n\n# Enable or disable spider middlewares\n# See https://doc.scrapy.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    \'trustpilot.middlewares.TrustpilotSpiderMiddleware\': 543,\n#}\n\n# Enable or disable downloader middlewares\n# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html\n#DOWNLOADER_MIDDLEWARES = {\n#    \'trustpilot.middlewares.TrustpilotDownloaderMiddleware\': 543,\n#}\n\n# Enable or disable extensions\n# See https://doc.scrapy.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    \'scrapy.extensions.telnet.TelnetConsole\': None,\n#}\n\n# Configure item pipelines\n# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n#ITEM_PIPELINES = {\n#    \'trustpilot.pipelines.TrustpilotPipeline\': 300,\n#}\n\n# Enable and configure the AutoThrottle extension (disabled by default)\n# See https://doc.scrapy.org/en/latest/topics/autothrottle.html\n#AUTOTHROTTLE_ENABLED = True\n# The initial download delay\n#AUTOTHROTTLE_START_DELAY = 5\n# The maximum download delay to be set in case of high latencies\n#AUTOTHROTTLE_MAX_DELAY = 60\n# The average number of requests Scrapy should be sending in parallel to\n# each remote server\n#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n# Enable showing throttling stats for every response received:\n#AUTOTHROTTLE_DEBUG = False\n\n# Enable and configure HTTP caching (disabled by default)\n# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n#HTTPCACHE_ENABLED = True\n#HTTPCACHE_EXPIRATION_SECS = 0\n#HTTPCACHE_DIR = \'httpcache\'\n#HTTPCACHE_IGNORE_HTTP_CODES = []\n#HTTPCACHE_STORAGE = \'scrapy.extensions.httpcache.FilesystemCacheStorage\'\n'"
src/scraping/scrapy/trustpilot/spiders/__init__.py,0,b'# This package will contain the spiders of your Scrapy project\n#\n# Please refer to the documentation for information on how to create and manage\n# your spiders.\n'
src/scraping/scrapy/trustpilot/spiders/scraper.py,0,"b'import re\nimport pandas as pd\nimport scrapy\n\nclass Pages(scrapy.Spider):\n    name = ""trustpilot""\n\n    company_data = pd.read_csv(\'../selenium/exports/consolidate_company_urls.csv\')\n    start_urls = company_data[\'company_url\'].unique().tolist()\n\n    def parse(self, response):\n        company_logo = response.xpath(\'//img[@class=""business-unit-profile-summary__image""]/@src\').extract_first()\n        company_website = response.xpath(""//a[@class=\'badge-card__section badge-card__section--hoverable\']/@href"").extract_first()\n        company_name = response.xpath(""//span[@class=\'multi-size-header__big\']/text()"").extract_first()\n        comments = response.xpath(""//p[@class=\'review-content__text\']"")\n        comments = [comment.xpath(\'.//text()\').extract() for comment in comments]\n        comments = [[c.strip() for c in comment_list] for comment_list in comments]\n        comments = [\' \'.join(comment_list) for comment_list in comments]\n\n        ratings = response.xpath(""//div[@class=\'star-rating star-rating--medium\']//img/@alt"").extract()\n        ratings = [int(re.match(\'\\d+\', rating).group(0)) for rating in ratings]\n\n        for comment, rating in zip(comments, ratings):\n            yield {\n                \'comment\': comment,\n                \'rating\': rating,\n                \'url_website\' : response.url,\n                \'company_name\': company_name,\n                \'company_website\': company_website,\n                \'company_logo\': company_logo\n            }\n\n        next_page = response.css(\'a[data-page-number=next-page] ::attr(href)\').extract_first()\n        if next_page is not None:\n            request = response.follow(next_page, callback=self.parse)\n            yield request'"
