file_path,api_count,code
configs/deeplabv3.py,0,"b""# work dir\nroot_workdir = 'workdir'\n\n# seed\nseed = 0\n\n# 1. logging\nlogger = dict(\n    handlers=(\n        dict(type='StreamHandler', level='INFO'),\n        # dict(type='FileHandler', level='INFO'),\n    ),\n)\n\n# 2. data\ntest_cfg = dict(\n    scales=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n    bias=[0.5, 0.25, 0.0, -0.25, -0.5, -0.75],\n    flip=True,\n)\nimg_norm_cfg = dict(mean=(123.675, 116.280, 103.530), std=(58.395, 57.120, 57.375))\nignore_label = 255\n\ndataset_type = 'VOCDataset'\ndataset_root = 'data/VOCdevkit/VOC2012'\ndata = dict(\n    train=dict(\n        dataset=dict(\n            type=dataset_type,\n            root=dataset_root,\n            imglist_name='trainaug.txt',\n        ),\n        transforms=[\n            dict(type='RandomScale', min_scale=0.5, max_scale=2.0, scale_step=0.25, mode='bilinear'),\n            dict(type='RandomCrop', height=513, width=513, image_value=img_norm_cfg['mean'], mask_value=ignore_label),\n            dict(type='HorizontalFlip', p=0.5),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ToTensor'),\n        ],\n        loader=dict(\n            type='DataLoader',\n            batch_size=16,\n            num_workers=4,\n            shuffle=True,\n            drop_last=True,\n            pin_memory=True,\n        ),\n    ),\n    val=dict(\n        dataset=dict(\n            type=dataset_type,\n            root=dataset_root,\n            imglist_name='val.txt',\n        ),\n        transforms=[\n            dict(type='PadIfNeeded', height=513, width=513, image_value=img_norm_cfg['mean'], mask_value=ignore_label),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ToTensor'),\n        ],\n        loader=dict(\n            type='DataLoader',\n            batch_size=8,\n            num_workers=4,\n            shuffle=False,\n            drop_last=False,\n            pin_memory=True,\n        ),\n    ),\n)\n\n# 3. model\nnclasses = 21\nmodel = dict(\n    # model/encoder\n    encoder=dict(\n        backbone=dict(\n            type='ResNet',\n            arch='resnet101',\n            replace_stride_with_dilation=[False, False, True],\n            multi_grid=[1, 2, 4],\n        ),\n        enhance=dict(\n            type='ASPP',\n            from_layer='c5',\n            to_layer='enhance',\n            in_channels=2048,\n            out_channels=256,\n            atrous_rates=[6, 12, 18],\n            dropout=0.1,\n        ),\n    ),\n    collect=dict(type='CollectBlock', from_layer='enhance'),\n    # model/head\n    head=dict(\n        type='Head',\n        in_channels=256,\n        inter_channels=256,\n        out_channels=nclasses,\n        num_convs=1,\n        upsample=dict(\n            type='Upsample',\n            size=(513,513),\n            mode='bilinear',\n            align_corners=True\n        ),\n    )\n)\n\n## 3.1 resume\nresume = None\n\n# 4. criterion\ncriterion = dict(type='CrossEntropyLoss', ignore_index=ignore_label)\n\n# 5. optim\noptimizer = dict(type='SGD', lr=0.007, momentum=0.9, weight_decay=0.0001)\n\n# 6. lr scheduler\nmax_epochs = 50\nlr_scheduler = dict(type='PolyLR', max_epochs=max_epochs)\n\n# 7. runner\nrunner = dict(\n    type='Runner',\n    max_epochs=max_epochs,\n    trainval_ratio=1,\n    snapshot_interval=5,\n)\n\n# 8. device\ngpu_id = '2,3'\n"""
configs/deeplabv3plus.py,0,"b""# work dir\nroot_workdir = 'workdir'\n\n# seed\nseed = 0\n\n# 1. logging\nlogger = dict(\n    handlers=(\n        dict(type='StreamHandler', level='INFO'),\n        # dict(type='FileHandler', level='INFO'),\n    ),\n)\n\n# 2. data\ntest_cfg = dict(\n    scales=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n    bias=[0.5, 0.25, 0.0, -0.25, -0.5, -0.75],\n    flip=True,\n)\nimg_norm_cfg = dict(mean=(123.675, 116.280, 103.530), std=(58.395, 57.120, 57.375))\nignore_label = 255\n\ndataset_type = 'VOCDataset'\ndataset_root = 'data/VOCdevkit/VOC2012'\ndata = dict(\n    train=dict(\n        dataset=dict(\n            type=dataset_type,\n            root=dataset_root,\n            imglist_name='trainaug.txt',\n        ),\n        transforms=[\n            dict(type='RandomScale', min_scale=0.5, max_scale=2.0, scale_step=0.25, mode='bilinear'),\n            dict(type='RandomCrop', height=513, width=513, image_value=img_norm_cfg['mean'], mask_value=ignore_label),\n            dict(type='HorizontalFlip', p=0.5),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ToTensor'),\n        ],\n        loader=dict(\n            type='DataLoader',\n            batch_size=16,\n            num_workers=4,\n            shuffle=True,\n            drop_last=True,\n            pin_memory=True,\n        ),\n    ),\n    val=dict(\n        dataset=dict(\n            type=dataset_type,\n            root=dataset_root,\n            imglist_name='val.txt',\n        ),\n        transforms=[\n            dict(type='PadIfNeeded', height=513, width=513, image_value=img_norm_cfg['mean'], mask_value=ignore_label),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ToTensor'),\n        ],\n        loader=dict(\n            type='DataLoader',\n            batch_size=8,\n            num_workers=4,\n            shuffle=False,\n            drop_last=False,\n            pin_memory=True,\n        ),\n    ),\n)\n\n# 3. model\nnclasses = 21\nmodel = dict(\n    # model/encoder\n    encoder=dict(\n        backbone=dict(\n            type='ResNet',\n            arch='resnet101',\n            replace_stride_with_dilation=[False, False, True],\n            multi_grid=[1, 2, 4],\n        ),\n        enhance=dict(\n            type='ASPP',\n            from_layer='c5',\n            to_layer='enhance',\n            in_channels=2048,\n            out_channels=256,\n            atrous_rates=[6, 12, 18],\n            dropout=0.1,\n        ),\n    ),\n    # model/decoder\n    decoder=dict(\n        type='GFPN',\n        # model/decoder/blocks\n        neck=[\n            # model/decoder/blocks/block1\n            dict(\n                type='JunctionBlock',\n                fusion_method='concat',\n                top_down=dict(\n                    from_layer='enhance',\n                    upsample=dict(\n                        type='Upsample',\n                        scale_factor=4,\n                        scale_bias=-3,\n                        mode='bilinear',\n                        align_corners=True,\n                    ),\n                ),\n                lateral=dict(\n                    from_layer='c2',\n                    type='ConvModule',\n                    in_channels=256,\n                    out_channels=48,\n                    kernel_size=1,\n                    norm_cfg=dict(type='BN'),\n                    act_cfg=dict(type='Relu', inplace=True),\n                ),\n                post=None,\n                to_layer='p5',\n            ),  # 4\n        ],\n    ),\n    # model/head\n    head=dict(\n        type='Head',\n        in_channels=304,\n        inter_channels=256,\n        out_channels=nclasses,\n        num_convs=2,\n        upsample=dict(\n            type='Upsample',\n            size=(513, 513),\n            mode='bilinear',\n            align_corners=True,\n        ),\n    )\n)\n\n## 3.1 resume\nresume = None\n\n# 4. criterion\ncriterion = dict(type='CrossEntropyLoss', ignore_index=ignore_label)\n\n# 5. optim\noptimizer = dict(type='SGD', lr=0.007, momentum=0.9, weight_decay=0.0001)\n\n# 6. lr scheduler\nmax_epochs = 50\nlr_scheduler = dict(type='PolyLR', max_epochs=max_epochs)\n\n# 7. runner\nrunner = dict(\n    type='Runner',\n    max_epochs=max_epochs,\n    trainval_ratio=1,\n    snapshot_interval=5,\n)\n\n# 8. device\ngpu_id = '0,1'\n"""
configs/fpn.py,0,"b""# work dir\nroot_workdir = 'workdir'\n\n# seed\nseed = 0\n\n# 1. logging\nlogger = dict(\n    handlers=(\n        dict(type='StreamHandler', level='INFO'),\n        # dict(type='FileHandler', level='INFO'),\n    ),\n)\n\n# 2. data\ntest_cfg = dict(\n    scales=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n    bias=[0.5, 0.25, 0.0, -0.25, -0.5, -0.75],\n    flip=True,\n)\nimg_norm_cfg = dict(mean=(123.675, 116.280, 103.530), std=(58.395, 57.120, 57.375))\nignore_label = 255\n\ndataset_type = 'VOCDataset'\ndataset_root = 'data/VOCdevkit/VOC2012'\ndata = dict(\n    train=dict(\n        dataset=dict(\n            type=dataset_type,\n            root=dataset_root,\n            imglist_name='trainaug.txt',\n        ),\n        transforms=[\n            dict(type='RandomScale', min_scale=0.5, max_scale=2.0, mode='bilinear'),\n            dict(type='RandomCrop', height=513, width=513, image_value=img_norm_cfg['mean'], mask_value=ignore_label),\n            dict(type='RandomRotate', p=0.5, degrees=10, mode='bilinear', border_mode='constant', image_value=img_norm_cfg['mean'], mask_value=ignore_label),\n            dict(type='GaussianBlur', p=0.5, ksize=7),\n            dict(type='HorizontalFlip', p=0.5),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ToTensor'),\n        ],\n        loader=dict(\n            type='DataLoader',\n            batch_size=16,\n            num_workers=4,\n            shuffle=True,\n            drop_last=True,\n        ),\n    ),\n    val=dict(\n        dataset=dict(\n            type=dataset_type,\n            root=dataset_root,\n            imglist_name='val.txt',\n        ),\n        transforms=[\n            dict(type='PadIfNeeded', height=513, width=513, image_value=img_norm_cfg['mean'], mask_value=ignore_label),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ToTensor'),\n        ],\n        loader=dict(\n            type='DataLoader',\n            batch_size=8,\n            num_workers=4,\n            shuffle=False,\n            drop_last=False,\n        ),\n    ),\n)\n\n# 3. model\nnclasses = 21\nmodel = dict(\n    # model/encoder\n    encoder=dict(\n        backbone=dict(\n            type='ResNet',\n            arch='resnet101'\n        ),\n    ),\n    # model/decoder\n    decoder=dict(\n        type='GFPN',\n        # model/decoder/blocks\n        neck=[\n            # model/decoder/blocks/block1\n            dict(\n                type='JunctionBlock',\n                top_down=None,\n                lateral=dict(\n                    from_layer='c5',\n                    type='ConvModule',\n                    in_channels=2048,\n                    out_channels=256,\n                    kernel_size=1,\n                    norm_cfg=None,\n                    act_cfg=None,\n                ),\n                post=dict(\n                    type='ConvModule',\n                    in_channels=256,\n                    out_channels=256,\n                    kernel_size=3,\n                    padding=1,\n                    norm_cfg=None,\n                    act_cfg=None,\n                ),\n                to_layer='p5',\n            ),  # 32\n            # model/decoder/blocks/block2\n            dict(\n                type='JunctionBlock',\n                fusion_method='add',\n                top_down=dict(\n                    from_layer='p5',\n                    upsample=dict(\n                        type='Upsample',\n                        scale_factor=2,\n                        scale_bias=-1,\n                        mode='bilinear',\n                        align_corners=True,\n                    ),\n                ),\n                lateral=dict(\n                    from_layer='c4',\n                    type='ConvModule',\n                    in_channels=1024,\n                    out_channels=256,\n                    kernel_size=1,\n                    norm_cfg=None,\n                    act_cfg=None\n                ),\n                post=dict(\n                    type='ConvModule',\n                    in_channels=256,\n                    out_channels=256,\n                    kernel_size=3,\n                    padding=1,\n                    norm_cfg=None,\n                    act_cfg=None,\n                ),\n                to_layer='p4',\n            ),  # 16\n            # model/decoder/blocks/block3\n            dict(\n                type='JunctionBlock',\n                fusion_method='add',\n                top_down=dict(\n                    from_layer='p4',\n                    upsample=dict(\n                        type='Upsample',\n                        scale_factor=2,\n                        scale_bias=-1,\n                        mode='bilinear',\n                        align_corners=True,\n                    ),\n                ),\n                lateral=dict(\n                    from_layer='c3',\n                    type='ConvModule',\n                    in_channels=512,\n                    out_channels=256,\n                    kernel_size=1,\n                    norm_cfg=None,\n                    act_cfg=None,\n                ),\n                post=dict(\n                    type='ConvModule',\n                    in_channels=256,\n                    out_channels=256,\n                    kernel_size=3,\n                    padding=1,\n                    norm_cfg=None,\n                    act_cfg=None\n                ),\n                to_layer='p3',\n            ),  # 8\n            # model/decoder/blocks/block2\n            dict(\n                type='JunctionBlock',\n                fusion_method='add',\n                top_down=dict(\n                    from_layer='p3',\n                    upsample=dict(\n                        type='Upsample',\n                        scale_factor=2,\n                        scale_bias=-1,\n                        mode='bilinear',\n                        align_corners=True,\n                    ),\n                ),\n                lateral=dict(\n                    from_layer='c2',\n                    type='ConvModule',\n                    in_channels=256,\n                    out_channels=256,\n                    kernel_size=1,\n                    norm_cfg=None,\n                    act_cfg=None,\n                ),\n                post=dict(\n                    type='ConvModule',\n                    in_channels=256,\n                    out_channels=256,\n                    kernel_size=3,\n                    padding=1,\n                    norm_cfg=None,\n                    act_cfg=None,\n                ),\n                to_layer='p2',\n            ),  # 4\n        ],\n        fusion=dict(\n            type='FusionBlock',\n            method='add',\n            from_layers=['p2', 'p3', 'p4', 'p5'],\n            feat_strides=[4, 8, 16, 32],\n            in_channels_list=[256, 256, 256, 256],\n            out_channels_list=[128, 128, 128, 128],\n            norm_cfg=dict(type='BN'),\n            act_cfg=dict(type='Relu', inplace=True),\n            common_stride=4,\n            upsample=dict(\n                type='Upsample',\n                scale_factor=2,\n                scale_bias=-1,\n                mode='bilinear',\n                align_corners=True,\n            ),\n        ),  # 4\n    ),\n    # model/decoer/head\n    head=dict(\n        type='Head',\n        in_channels=128,\n        inter_channels=128,\n        out_channels=nclasses,\n        num_convs=3,\n        upsample=dict(\n            type='Upsample',\n            size=(513, 513),\n            mode='bilinear',\n            align_corners=True,\n        ),\n    )\n)\n\n## 3.1 resume\nresume = None\n\n# 4. criterion\ncriterion = dict(type='CrossEntropyLoss', ignore_index=ignore_label)\n\n# 5. optim\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\n\n# 6. lr scheduler\nmax_epochs = 50\nlr_scheduler = dict(type='PolyLR', max_epochs=max_epochs)\n\n# 7. runner\nrunner = dict(\n    type='Runner',\n    max_epochs=max_epochs,\n    trainval_ratio=1,\n    snapshot_interval=5,\n)\n\n# 8. device\ngpu_id = '0,1'\n"""
configs/pspnet.py,0,"b""# work dir\nroot_workdir = 'workdir'\n\n# seed\nseed = 0\n\n# 1. logging\nlogger = dict(\n    handlers=(\n        dict(type='StreamHandler', level='INFO'),\n        # dict(type='FileHandler', level='INFO'),\n    ),\n)\n\n# 2. data\ntest_cfg = dict(\n    scales=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n    bias=[0.5, 0.25, 0.0, -0.25, -0.5, -0.75],\n    flip=True,\n)\nimg_norm_cfg = dict(mean=(123.675, 116.280, 103.530), std=(58.395, 57.120, 57.375))\nignore_label = 255\n\ndataset_type = 'VOCDataset'\ndataset_root = 'data/VOCdevkit/VOC2012'\ndata = dict(\n    train=dict(\n        dataset=dict(\n            type=dataset_type,\n            root=dataset_root,\n            imglist_name='trainaug.txt',\n        ),\n        transforms=[\n            dict(type='RandomScale', min_scale=0.5, max_scale=2.0, mode='bilinear'),\n            dict(type='RandomCrop', height=513, width=513, image_value=img_norm_cfg['mean'], mask_value=ignore_label),\n            dict(type='RandomRotate', p=0.5, degrees=10, mode='bilinear', border_mode='constant', image_value=img_norm_cfg['mean'], mask_value=ignore_label),\n            dict(type='GaussianBlur', p=0.5, ksize=7),\n            dict(type='HorizontalFlip', p=0.5),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ToTensor'),\n        ],\n        loader=dict(\n            type='DataLoader',\n            batch_size=16,\n            num_workers=4,\n            shuffle=True,\n            drop_last=True,\n        ),\n    ),\n    val=dict(\n        dataset=dict(\n            type=dataset_type,\n            root=dataset_root,\n            imglist_name='val.txt',\n        ),\n        transforms=[\n            dict(type='PadIfNeeded', height=513, width=513, image_value=img_norm_cfg['mean'], mask_value=ignore_label),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ToTensor'),\n        ],\n        loader=dict(\n            type='DataLoader',\n            batch_size=8,\n            num_workers=4,\n            shuffle=False,\n            drop_last=False,\n        ),\n    ),\n)\n\n# 3. model\nnclasses = 21\nmodel = dict(\n    # model/encoder\n    encoder=dict(\n        backbone=dict(\n            type='ResNet',\n            arch='resnet101',\n            replace_stride_with_dilation=[False, True, True],\n        ),\n        enhance=dict(\n            type='PPM',\n            from_layer='c5',\n            to_layer='enhance',\n            in_channels=2048,\n            out_channels=512,\n            bins=[1, 2, 3, 6],\n        ),\n    ),\n    collect=dict(type='CollectBlock', from_layer='enhance'),\n    # model/head\n    head=dict(\n        type='Head',\n        in_channels=4096,\n        inter_channels=512,\n        out_channels=nclasses,\n        num_convs=1,\n        dropouts=[0.1],\n        upsample=dict(\n            type='Upsample',\n            size=(513,513),\n            mode='bilinear',\n            align_corners=True\n        ),\n    )\n)\n\n## 3.1 resume\nresume = None\n\n# 4. criterion\ncriterion = dict(type='CrossEntropyLoss', ignore_index=ignore_label)\n\n# 5. optim\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\n\n# 6. lr scheduler\nmax_epochs = 50\nlr_scheduler = dict(type='PolyLR', max_epochs=max_epochs)\n\n# 7. runner\nrunner = dict(\n    type='Runner',\n    max_epochs=max_epochs,\n    trainval_ratio=1,\n    snapshot_interval=5,\n)\n\n# 8. device\ngpu_id = '0,1,2,3'\n"""
configs/unet.py,0,"b""# work dir\nroot_workdir = 'workdir'\n\n# seed\nseed = 0\n\n# 1. logging\nlogger = dict(\n    handlers=(\n        dict(type='StreamHandler', level='INFO'),\n        # dict(type='FileHandler', level='INFO'),\n    ),\n)\n\n# 2. data\ntest_cfg = dict(\n    scales=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],\n    bias=[0.5, 0.25, 0.0, -0.25, -0.5, -0.75],\n    flip=True,\n)\nimg_norm_cfg = dict(mean=(123.675, 116.280, 103.530), std=(58.395, 57.120, 57.375))\nignore_label = 255\n\ndataset_type = 'VOCDataset'\ndataset_root = 'data/VOCdevkit/VOC2012'\ndata = dict(\n    train=dict(\n        dataset=dict(\n            type=dataset_type,\n            root=dataset_root,\n            imglist_name='trainaug.txt',\n        ),\n        transforms=[\n            dict(type='RandomScale', min_scale=0.5, max_scale=2.0, mode='bilinear'),\n            dict(type='RandomCrop', height=513, width=513, image_value=img_norm_cfg['mean'], mask_value=ignore_label),\n            dict(type='RandomRotate', p=0.5, degrees=10, mode='bilinear', border_mode='constant', image_value=img_norm_cfg['mean'], mask_value=ignore_label),\n            dict(type='GaussianBlur', p=0.5, ksize=7),\n            dict(type='HorizontalFlip', p=0.5),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ToTensor'),\n        ],\n        loader=dict(\n            type='DataLoader',\n            batch_size=16,\n            num_workers=4,\n            shuffle=True,\n            drop_last=True,\n            pin_memory=True,\n        ),\n    ),\n    val=dict(\n        dataset=dict(\n            type=dataset_type,\n            root=dataset_root,\n            imglist_name='val.txt',\n        ),\n        transforms=[\n            dict(type='PadIfNeeded', height=513, width=513, image_value=img_norm_cfg['mean'], mask_value=ignore_label),\n            dict(type='Normalize', **img_norm_cfg),\n            dict(type='ToTensor'),\n        ],\n        loader=dict(\n            type='DataLoader',\n            batch_size=8,\n            num_workers=4,\n            shuffle=False,\n            drop_last=False,\n            pin_memory=True,\n        ),\n    ),\n)\n\n# 3. model\nnclasses = 21\nmodel = dict(\n    # model/encoder\n    encoder=dict(\n        backbone=dict(\n            type='ResNet',\n            arch='resnet101',\n        ),\n    ),\n    # model/decoder\n    decoder=dict(\n        type='GFPN',\n        # model/decoder/blocks\n        neck=[\n            # model/decoder/blocks/block1\n            dict(\n                type='JunctionBlock',\n                fusion_method='concat',\n                top_down=dict(\n                    from_layer='c5',\n                    upsample=dict(\n                        type='Upsample',\n                        scale_factor=2,\n                        scale_bias=-1,\n                        mode='bilinear',\n                        align_corners=True,\n                    ),\n                ),\n                lateral=dict(from_layer='c4'),\n                post=dict(\n                    type='ConvModules',\n                    in_channels=3072,  # 2048 + 1024\n                    out_channels=256,\n                    kernel_size=3,\n                    padding=1,\n                    norm_cfg=dict(type='BN'),\n                    act_cfg=dict(type='Relu', inplace=True),\n                    num_convs=2,\n                ),\n                to_layer='p4',\n            ),  # 16\n            # model/decoder/blocks/block2\n            dict(\n                type='JunctionBlock',\n                fusion_method='concat',\n                top_down=dict(\n                    from_layer='p4',\n                    upsample=dict(\n                        type='Upsample',\n                        scale_factor=2,\n                        scale_bias=-1,\n                        mode='bilinear',\n                        align_corners=True,\n                    ),\n                ),\n                lateral=dict(from_layer='c3'),\n                post=dict(\n                    type='ConvModules',\n                    in_channels=768,  # 256 + 512\n                    out_channels=128,\n                    kernel_size=3,\n                    padding=1,\n                    norm_cfg=dict(type='BN'),\n                    act_cfg=dict(type='Relu', inplace=True),\n                    num_convs=2,\n                ),\n                to_layer='p3',\n            ),  # 8\n            # model/decoder/blocks/block3\n            dict(\n                type='JunctionBlock',\n                fusion_method='concat',\n                top_down=dict(\n                    from_layer='p3',\n                    upsample=dict(\n                        type='Upsample',\n                        scale_factor=2,\n                        scale_bias=-1,\n                        mode='bilinear',\n                        align_corners=True,\n                    ),\n                ),\n                lateral=dict(from_layer='c2'),\n                post=dict(\n                    type='ConvModules',\n                    in_channels=384,  # 128 + 256\n                    out_channels=64,\n                    kernel_size=3,\n                    padding=1,\n                    norm_cfg=dict(type='BN'),\n                    act_cfg=dict(type='Relu', inplace=True),\n                    num_convs=2,\n                ),\n                to_layer='p2',\n            ),  # 4\n            # model/decoder/blocks/block4\n            dict(\n                type='JunctionBlock',\n                fusion_method='concat',\n                top_down=dict(\n                    from_layer='p2',\n                    upsample=dict(\n                        type='Upsample',\n                        scale_factor=2,\n                        scale_bias=-1,\n                        mode='bilinear',\n                        align_corners=True,\n                    ),\n                ),\n                lateral=dict(from_layer='c1'),\n                post=dict(\n                    type='ConvModules',\n                    in_channels=128,  # 64 + 64\n                    out_channels=32,\n                    kernel_size=3,\n                    padding=1,\n                    norm_cfg=dict(type='BN'),\n                    act_cfg=dict(type='Relu', inplace=True),\n                    num_convs=2,\n                ),\n                to_layer='p1',\n            ),  # 2\n            # model/decoder/blocks/block5\n            dict(\n                type='JunctionBlock',\n                top_down=dict(\n                    from_layer='p1',\n                    upsample=dict(\n                        type='Upsample',\n                        scale_factor=2,\n                        scale_bias=-1,\n                        mode='bilinear',\n                        align_corners=True,\n                    ),\n                ),\n                lateral=None,\n                post=dict(\n                    type='ConvModules',\n                    in_channels=32,\n                    out_channels=16,\n                    kernel_size=3,\n                    padding=1,\n                    norm_cfg=dict(type='BN'),\n                    act_cfg=dict(type='Relu', inplace=True),\n                    num_convs=2,\n                ),\n                to_layer='p0',\n            ),  # 1\n        ]),\n    # model/decoer/head\n    head=dict(\n        type='Head',\n        in_channels=16,\n        out_channels=nclasses,\n        num_convs=0,\n        upsample=dict(\n            type='Upsample',\n            size=(513, 513),\n            mode='bilinear',\n            align_corners=True\n        ),\n    )\n)\n\n## 3.1 resume\nresume = None\n\n# 4. criterion\ncriterion = dict(type='CrossEntropyLoss', ignore_index=ignore_label)\n\n# 5. optim\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\n\n# 6. lr scheduler\nmax_epochs = 50\nlr_scheduler = dict(type='PolyLR', max_epochs=max_epochs)\n\n# 7. runner\nrunner = dict(\n    type='Runner',\n    max_epochs=max_epochs,\n    trainval_ratio=1,\n    snapshot_interval=5,\n)\n\n# 8. device\ngpu_id = '0,1'\n"""
tools/decode.py,0,"b""# https://gist.github.com/wllhf/a4533e0adebe57e3ed06d4b50c8419ae\n# https://github.com/tensorflow/models/blob/master/research/deeplab/utils/get_dataset_colormap.py\n# https://github.com/tensorflow/models/blob/master/research/deeplab/datasets/remove_gt_colormap.py\n\nfrom PIL import Image\nimport numpy as np\nimport glob\nimport os\n\n\ndef color_map(N=256, normalized=False):\n    def bitget(byteval, idx):\n        return ((byteval & (1 << idx)) != 0)\n\n    dtype = 'float32' if normalized else 'uint8'\n    cmap = np.zeros((N, 3), dtype=dtype)\n    for i in range(N):\n        r = g = b = 0 \n        c = i \n        for j in range(8):\n            r = r | (bitget(c, 0) << 7-j)\n            g = g | (bitget(c, 1) << 7-j)\n            b = b | (bitget(c, 2) << 7-j)\n            c = c >> 3\n\n        cmap[i] = np.array([r, g, b]) \n\n    cmap = cmap/255 if normalized else cmap\n    return cmap\n\n\ndef main():\n    root = 'VOCdevkit/VOC2012'\n    src_name = 'EncodeSegmentationClass'\n    dst_name = 'DecodeSegmentationClass'\n    src_dir = '%s/%s' % (root, src_name)\n    dst_dir = '%s/%s' % (root, dst_name)\n    os.makedirs(dst_dir)\n    items = glob.glob('%s/*.png' % src_dir)\n    total = len(items)\n    for idx, item in enumerate(items):\n        print('%d/%d' % (idx, total))\n        new_item = item.replace(src_name, dst_name)\n        target = np.array(Image.open(item))[:, :, np.newaxis]\n        cmap = color_map()[:, np.newaxis, :]\n        new_im = np.dot(target == 0, cmap[0])\n        for i in range(1, cmap.shape[0]):\n            new_im += np.dot(target == i, cmap[i])\n        new_im = Image.fromarray(new_im.astype(np.uint8))\n        new_im.save(new_item)\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/encode_voc12.py,0,"b""import glob\nimport os\n\nimport numpy as np\nfrom PIL import Image\n\n\ndef main():\n    root = 'VOCdevkit/VOC2012'\n    src_name = 'SegmentationClass'\n    dst_name = 'EncodeSegmentationClassPart'\n    src_dir = '%s/%s' % (root, src_name)\n    dst_dir = '%s/%s' % (root, dst_name)\n    os.makedirs(dst_dir)\n    items = glob.glob('%s/*.png' % src_dir)\n    total = len(items)\n    for idx, item in enumerate(items):\n        print('%d/%d' % (idx, total))\n        new_item = item.replace(src_name, dst_name)\n        new_mask = np.array(Image.open(item))\n        Image.fromarray(new_mask.astype(dtype=np.uint8)).save(new_item, 'PNG')\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/encode_voc12_aug.py,0,"b""import glob\nimport os\nimport scipy.io as io\nfrom PIL import Image\nimport numpy as np\n\n\ndef main():\n    root = 'benchmark_RELEASE/dataset'\n    src_name = 'cls'\n    dst_name = 'encode_cls'\n    src_dir = '%s/%s' % (root, src_name)\n    dst_dir = '%s/%s' % (root, dst_name)\n    os.makedirs(dst_dir)\n    items = glob.glob('%s/*.mat' % src_dir)\n    total = len(items)\n    for idx, item in enumerate(items):\n        print('%d/%d' % (idx, total))\n        data = io.loadmat(item)\n        mask = data['GTcls'][0]['Segmentation'][0].astype(np.int32)\n        new_item = item.replace(src_name, dst_name).replace('.mat', '.png')\n        Image.fromarray(mask.astype(dtype=np.uint8)).save(new_item, 'PNG')\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/test.py,0,"b""import argparse\nimport sys\nimport os\nsys.path.insert(0, os.path.join(os.path.abspath(os.path.dirname(__file__)), '../../vedaseg'))\n\nfrom vedaseg.assembler import assemble\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='Train a semantic segmentatation model')\n    parser.add_argument('config', help='test config file path')\n    parser.add_argument('checkpoint', help='test checkpoint')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    cfg_fp = args.config\n    checkpoint = args.checkpoint\n\n    runner = assemble(cfg_fp, checkpoint, True)\n    runner()\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/trainval.py,0,"b""import argparse\nimport sys\nimport os\nsys.path.insert(0, os.path.join(os.path.abspath(os.path.dirname(__file__)), '../../vedaseg'))\n\nfrom vedaseg.assembler import assemble\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='Train a semantic segmentatation model')\n    parser.add_argument('config', help='train config file path')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    cfg_fp = args.config\n\n    runner = assemble(cfg_fp)\n    runner()\n\n\nif __name__ == '__main__':\n    main()\n"""
vedaseg/__init__.py,0,b''
vedaseg/assembler/__init__.py,0,b'from .assembler import assemble\n'
vedaseg/assembler/assembler.py,2,"b""import os\n\nimport torch\nfrom torch import nn\n\nfrom vedaseg import utils\nfrom vedaseg.loggers import build_logger\nfrom vedaseg.datasets import build_dataset\nfrom vedaseg.datasets.transforms.builder import build_transform\nfrom vedaseg.dataloaders import build_dataloader\nfrom vedaseg.models import build_model\nfrom vedaseg.criteria import build_criterion\nfrom vedaseg.optims import build_optim\nfrom vedaseg.lr_schedulers import build_lr_scheduler\nfrom vedaseg.utils import MetricMeter\nfrom vedaseg.runner import build_runner\n\n\ndef assemble(cfg_fp, checkpoint='', test_mode=False):\n    _, fullname = os.path.split(cfg_fp)\n    fname, ext = os.path.splitext(fullname)\n\n    cfg = utils.Config.fromfile(cfg_fp)\n\n    # set gpu environment\n    os.environ['CUDA_VISIBLE_DEVICES'] = cfg['gpu_id']\n\n    # make workdir if not exist\n    root_workdir = cfg.pop('root_workdir')\n    cfg['workdir'] = os.path.join(root_workdir, fname)\n\n    os.makedirs(cfg['workdir'], exist_ok=True)\n\n    # set seed if not None\n    seed = cfg.pop('seed')\n    if seed is not None:\n        utils.set_random_seed(seed)\n\n    # 1. logging\n    logger = build_logger(cfg['logger'], dict(workdir=cfg['workdir']))\n\n    logger.info('Assemble, Step 1, Build Dataset')\n    # 2. data\n    ## 2.1 dataset\n    train_tf = build_transform(cfg['data']['train']['transforms'])\n    train_dataset = build_dataset(cfg['data']['train']['dataset'], dict(transform=train_tf))\n\n    if cfg['data'].get('val'):\n        val_tf = build_transform(cfg['data']['val']['transforms'])\n        val_dataset = build_dataset(cfg['data']['val']['dataset'], dict(transform=val_tf))\n\n    logger.info('Assemble, Step 2, Build Dataloader')\n    # 2.2 dataloader\n    train_loader = build_dataloader(cfg['data']['train']['loader'], dict(dataset=train_dataset))\n    loader = {'train': train_loader}\n    if cfg['data'].get('val'):\n        val_loader = build_dataloader(cfg['data']['val']['loader'], dict(dataset=val_dataset))\n        loader['val'] = val_loader\n\n    logger.info('Assemble, Step 3, Build Model')\n    # 3. model\n    model = build_model(cfg['model'])\n    if torch.cuda.is_available():\n        logger.info('Using GPU {}'.format(cfg['gpu_id']))\n        gpu = True\n        if torch.cuda.device_count() > 1:\n            model = nn.DataParallel(model)\n        model.cuda()\n    else:\n        logger.info('Using CPU')\n        gpu = False\n\n    logger.info('Assemble, Step 4, Build Criterion')\n    # 4. criterion\n    criterion = build_criterion(cfg['criterion'])\n\n    logger.info('Assemble, Step 5, Build Optimizer')\n    # 5. optim\n    optim = build_optim(cfg['optimizer'], dict(params=model.parameters()))\n\n    logger.info('Assemble, Step 6, Build LR Scheduler')\n    # 6. lr scheduler\n    lr_scheduler = build_lr_scheduler(cfg['lr_scheduler'], dict(optimizer=optim, niter_per_epoch=len(train_loader)))\n\n    logger.info('Assemble, Step 7, Build Runner')\n    # 7. runner\n    runner = build_runner(\n        cfg['runner'],\n        dict(\n            loader=loader,\n            model=model,\n            criterion=criterion,\n            metric=MetricMeter(cfg['nclasses']),\n            optim=optim,\n            lr_scheduler=lr_scheduler,\n            workdir=cfg['workdir'],\n            gpu=gpu,\n            test_cfg=cfg.get('test_cfg', None),\n            test_mode=test_mode,\n        )\n    )\n\n    if test_mode:\n        cfg['resume'] = dict(checkpoint=checkpoint, resume_optimizer=False, resume_lr=False, resume_epoch=False)\n\n    if cfg['resume']:\n        runner.resume(**cfg['resume'])\n\n    return runner\n"""
vedaseg/criteria/__init__.py,0,b'from .builder import build_criterion\n'
vedaseg/criteria/builder.py,0,"b""#from .seg_wrapper import CriterionWrapper\nfrom vedaseg.utils import build_from_cfg\n\nfrom .registry import CRITERIA\n\n\ndef build_criterion(cfg):\n    #criterion = CriterionWrapper(cfg)\n    criterion = build_from_cfg(cfg, CRITERIA, src='registry')\n    return criterion\n"""
vedaseg/criteria/registry.py,1,"b""import torch.nn as nn\n\nfrom vedaseg.utils import Registry\n\nCRITERIA = Registry('criterion')\n\nBCEWithLogitsLoss = nn.BCEWithLogitsLoss\nCRITERIA.register_module(BCEWithLogitsLoss)\n\n\nCrossEntropyLoss = nn.CrossEntropyLoss\nCRITERIA.register_module(CrossEntropyLoss)\n"""
vedaseg/criteria/seg_wrapper.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\nfrom vedaseg.utils import build_from_cfg\n\nfrom .registry import CRITERIA\n\n\nclass CriterionWrapper(nn.Module):\n    """"""LossWrapper\n\n        Args:\n    """"""\n    def __init__(self, cfg):\n        super().__init__()\n        #self.criterion = build_from_cfg(cfg, nn, method=\'module\')\n        self.criterion = build_from_cfg(cfg, CRITERIA, src=\'registry\')\n\n    def forward(self, pred, target):\n        pred = F.interpolate(pred, target.shape[2:])\n        return self.criterion(pred, target)\n'"
vedaseg/dataloaders/__init__.py,0,b'from .builder import build_dataloader\n'
vedaseg/dataloaders/builder.py,1,"b""import torch.utils.data as torch_data\nfrom vedaseg.utils import build_from_cfg\n\n\ndef build_dataloader(cfg, default_args):\n    loader = build_from_cfg(cfg, torch_data, default_args, 'module')\n    return loader\n"""
vedaseg/dataloaders/steel.py,0,"b'def build_dataloader(\n        data_folder,\n        df_path,\n        phases=[\'train\', \'val\'],\n        mean=None,\n        std=None,\n        batch_sizes={\n            \'train\': 8,\n            \'val\': 4\n        },\n        num_workers=4,\n):\n    \'\'\'Returns dataloader for the model training\'\'\'\n    df = pd.read_csv(df_path)\n    # some preprocessing\n    # https://www.kaggle.com/amanooo/defect-detection-starter-u-net\n    #df[\'ImageId\'], df[\'ClassId\'] = zip(*df[\'ImageId_ClassId\'].str.split(\'_\'))\n    df[\'ImageId\'], df[\'ClassId\'] = df[\'ImageId_ClassId\'].str.slice(\n        0, -2), df[\'ImageId_ClassId\'].str.slice(-1)\n    df[\'ClassId\'] = df[\'ClassId\'].astype(int)\n    df = df.pivot(index=\'ImageId\', columns=\'ClassId\', values=\'EncodedPixels\')\n    df[\'defects\'] = df.count(axis=1)\n\n    train_df, val_df = train_test_split(df,\n                                        test_size=0.2,\n                                        stratify=df[""defects""])\n    #train_df = df\n    print(train_df.head())\n    print(val_df.head())\n    dataloaders = {}\n    for phase in phases:\n        df = train_df if phase == \'train\' else val_df\n        image_dataset = SteelDataset(df, data_folder, phase)\n        dataloader = DataLoader(\n            image_dataset,\n            batch_size=batch_sizes[phase],\n            num_workers=num_workers,\n            pin_memory=True,\n            shuffle=True,\n            #worker_init_fn=_init_fn\n        )\n        dataloaders[phase] = dataloader\n\n    return dataloaders\n'"
vedaseg/datasets/__init__.py,0,b'from .builder import build_dataset\nfrom .dummy import DummyDataset\nfrom .steel import SteelDataset\nfrom .coil import CoilDataset\nfrom .voc import VOCDataset\n'
vedaseg/datasets/base.py,1,"b'import torch\nimport numpy as np\nimport albumentations as albu\nimport albumentations.pytorch as albu_pytorch\nfrom torch.utils.data import Dataset\n\n\nclass BaseDataset(Dataset):\n    """""" BaseDataset\n    """"""\n    def __init__(self):\n        self.transform = None\n\n    def process(self, image, mask):\n        if self.transform:\n            image, mask = self.transform(image, mask)\n\n        return image, mask\n'"
vedaseg/datasets/builder.py,0,"b'from vedaseg.utils import build_from_cfg\n\nfrom .registry import DATASETS\n\n\ndef build_dataset(cfg, default_args=None):\n    dataset = build_from_cfg(cfg, DATASETS, default_args)\n    return dataset\n'"
vedaseg/datasets/coil.py,1,"b'from torch.utils.data import Dataset\nimport pandas as pd\nimport torch\nimport numpy as np\nimport os\nimport cv2\nfrom sklearn.model_selection import train_test_split\nimport logging\n\nfrom .registry import DATASETS\nfrom .base import BaseDataset\n\nlogger = logging.getLogger()\n\n\ndef read_annot(fp):\n    res = {}\n    with open(fp, \'r\') as fd:\n        for line in fd:\n            segs = line.strip().split(\' \')\n            #print(line)\n            img_name, type_, x, y ,w, h = segs\n            type_, x, y, w, h = int(type_), int(x), int(y), int(w), int(h)\n            res.setdefault(img_name, [])\n            res[img_name].append([type_, x, y, w, h])\n    res_list = []\n    for k, v in res.items():\n        res_list.append((k , v))\n    return res_list\n\n\n@DATASETS.register_module\nclass CoilDataset(BaseDataset):\n    """"""\n    """"""\n    def __init__(self, filename, data_folder, transform):\n        super().__init__()\n\n        annot_path = \'%s/%s\' % (data_folder, filename)\n        self.annots = read_annot(annot_path)\n        logger.debug(\'train_df sample is\\n %s\' % self.annots)\n        self.root = data_folder\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        img_name, bboxes = self.annots[idx]\n        img_path = os.path.join(self.root, \'images\', img_name)\n        orig_img = cv2.imread(img_path)\n        H, W, _ = orig_img.shape\n        img = np.zeros((1024, 1024, 3), dtype=np.float32)\n        img[:H, :W, :] = orig_img\n\n        mask = make_mask(bboxes, W, H)\n\n        img, mask = self.process(img, mask)\n\n        return img, mask\n\n    def __len__(self):\n        return len(self.annots)\n\n\ndef make_mask(bboxes, W, H):\n    mask = np.zeros((1, H, W), dtype=np.float32)\n    # 4:class 1\xef\xbd\x9e4 (ch:0\xef\xbd\x9e3)\n\n    for bbox in bboxes:\n        type_, x, y, w, h = bbox\n        mask[0, y : y + h, x : x + w] = 1\n    print(mask.sum())\n    return mask\n\n'"
vedaseg/datasets/dummy.py,3,"b'import torch\nimport numpy as np\nimport albumentations as albu\nimport albumentations.pytorch as albu_pytorch\nfrom torch.utils.data import Dataset\n\nfrom .registry import DATASETS\nfrom .base import BaseDataset\n\n\n@DATASETS.register_module\nclass DummyDataset(BaseDataset):\n    """""" DummyDataset\n    """"""\n    def __init__(self, total=4, transform=None):\n        super().__init__()\n\n        self.total = total\n        self.a = torch.rand(320, 320, 3).numpy()\n        self.b = torch.rand(320, 320, 7).numpy()\n        self.transform = transform\n\n    def __getitem__(self, idx):\n\n        image, mask = self.process(self.a, self.b)\n\n        return image, mask.transpose(0, 2)\n\n    def __len__(self):\n        return self.total\n'"
vedaseg/datasets/registry.py,0,"b""from vedaseg.utils import Registry\n\nDATASETS = Registry('dataset')\n"""
vedaseg/datasets/steel.py,1,"b'from torch.utils.data import Dataset\nimport pandas as pd\nimport torch\nimport numpy as np\nimport os\nimport cv2\nfrom sklearn.model_selection import train_test_split\nimport logging\n\nfrom .registry import DATASETS\nfrom .base import BaseDataset\n\nlogger = logging.getLogger()\n\n\n@DATASETS.register_module\nclass SteelDataset(BaseDataset):\n    """"""\n    """"""\n    def __init__(self, filename, data_folder, transform, phase):\n        super().__init__()\n\n        df_path = \'%s/%s\' % (data_folder, filename)\n        df = pd.read_csv(df_path)\n        df[\'ImageId\'], df[\'ClassId\'] = df[\'ImageId_ClassId\'].str.slice(\n            0, -2), df[\'ImageId_ClassId\'].str.slice(-1)\n        df[\'ClassId\'] = df[\'ClassId\'].astype(int)\n        df = df.pivot(index=\'ImageId\',\n                      columns=\'ClassId\',\n                      values=\'EncodedPixels\')\n        df[\'defects\'] = df.count(axis=1)\n\n        train_df, val_df = train_test_split(df,\n                                            test_size=0.1,\n                                            stratify=df[\'defects\'],\n                                            random_state=0)\n        logger.debug(\'train_df sample is\\n %s\' % train_df.head())\n        if phase == \'train\':\n            self.df = train_df\n        else:\n            self.df = val_df\n        self.root = data_folder\n        self.fnames = self.df.index.tolist()\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        image_id, mask = make_mask(idx, self.df)\n        image_path = os.path.join(self.root, \'train_images\', image_id)\n        image = cv2.imread(image_path)\n\n        image, mask = self.process(image, mask)\n        mask = mask.permute(2, 0, 1)  # 4x256x1600\n\n        return image, mask\n\n    def __len__(self):\n        return len(self.fnames)\n\n\ndef make_mask(row_id, df):\n    fname = df.iloc[row_id].name\n    labels = df.iloc[row_id][:4]\n    masks = np.zeros((256, 1600, 4), dtype=np.float32)\n    # 4:class 1\xef\xbd\x9e4 (ch:0\xef\xbd\x9e3)\n\n    for idx, label in enumerate(labels.values):\n        if label is not np.nan:\n            label = label.split(\' \')\n            positions = map(int, label[0::2])\n            length = map(int, label[1::2])\n            mask = np.zeros(256 * 1600, dtype=np.uint8)\n            for pos, le in zip(positions, length):\n                pos -= 1\n                mask[pos:(pos + le)] = 1\n            masks[:, :, idx] = mask.reshape(256, 1600, order=\'F\')\n    return fname, masks\n'"
vedaseg/datasets/voc.py,0,"b'import logging\nimport os\n\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom .base import BaseDataset\nfrom .registry import DATASETS\n\nlogger = logging.getLogger()\n\n\n@DATASETS.register_module\nclass VOCDataset(BaseDataset):\n    """"""\n    """"""\n    def __init__(self, imglist_name, root, transform):\n        super().__init__()\n\n        imglist_fp = \'%s/ImageSets/Segmentation/%s\' % (root, imglist_name)\n        self.imglist = self.read_imglist(imglist_fp)\n\n        logger.debug(\'Total of images is %d\' % len(self.imglist))\n        self.root = root\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        imgname = self.imglist[idx]\n        img_fp = os.path.join(self.root, \'JPEGImages\', imgname) + \'.jpg\'\n        mask_fp = os.path.join(self.root, \'EncodeSegmentationClass\', imgname) + \'.png\'\n        img = cv2.imread(img_fp).astype(np.float32)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        mask = np.array(Image.open(mask_fp), dtype=np.float32)\n        image, mask = self.process(img, mask)\n        mask = mask.long()\n\n        return image, mask\n\n    def __len__(self):\n        return len(self.imglist)\n\n    def read_imglist(self, imglist_fp):\n        ll = []\n        with open(imglist_fp, \'r\') as fd:\n            for line in fd:\n                ll.append(line.strip())\n        return ll\n\n'"
vedaseg/loggers/__init__.py,0,b'from .builder import build_logger\n'
vedaseg/loggers/builder.py,0,"b""import logging\nimport time\nimport sys\nimport os\n\n\ndef build_logger(cfg, default_args):\n    timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n    format_ = '%(asctime)s - %(levelname)s - %(message)s'\n\n    formatter = logging.Formatter(format_)\n    logger = logging.getLogger()\n    logger.setLevel(logging.DEBUG)\n\n    for handler in cfg['handlers']:\n        if handler['type'] == 'StreamHandler':\n            instance = logging.StreamHandler(sys.stdout)\n        elif handler['type'] == 'FileHandler':\n            fp = os.path.join(default_args['workdir'], '%s.log' % timestamp)\n            instance = logging.FileHandler(fp, 'w')\n        else:\n            instance = logging.StreamHandler(sys.stdout)\n\n        level = getattr(logging, handler['level'])\n\n        instance.setFormatter(formatter)\n        instance.setLevel(level)\n\n        logger.addHandler(instance)\n\n    return logger\n"""
vedaseg/lr_schedulers/__init__.py,0,b'from .builder import build_lr_scheduler\nfrom .poly_lr import PolyLR\n'
vedaseg/lr_schedulers/base.py,3,"b'from functools import wraps\nimport warnings\nimport weakref\nfrom torch.optim import Optimizer\n\n\nclass _Iter_LRScheduler(object):\n    """"""\n    """"""\n\n    _iter_based = True\n\n    def __init__(self, optimizer, niter_per_epoch, last_iter=-1):\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError(\'{} is not an Optimizer\'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n        self.niter_per_epoch = niter_per_epoch\n        if last_iter == -1:\n            for group in optimizer.param_groups:\n                group.setdefault(\'initial_lr\', group[\'lr\'])\n            last_iter = 0\n        else:\n            for i, group in enumerate(optimizer.param_groups):\n                if \'initial_lr\' not in group:\n                    raise KeyError(""param \'initial_lr\' is not specified ""\n                                   ""in param_groups[{}] when resuming an optimizer"".format(i))\n        self.base_lrs = list(map(lambda group: group[\'initial_lr\'], optimizer.param_groups))\n        self.last_epoch = int(last_iter / niter_per_epoch)\n\n        # Following https://github.com/pytorch/pytorch/issues/20124\n        # We would like to ensure that `lr_scheduler.step()` is called after\n        # `optimizer.step()`\n        def with_counter(method):\n            if getattr(method, \'_with_counter\', False):\n                # `optimizer.step()` has already been replaced, return.\n                return method\n\n            # Keep a weak reference to the optimizer instance to prevent\n            # cyclic references.\n            instance_ref = weakref.ref(method.__self__)\n            # Get the unbound method for the same purpose.\n            func = method.__func__\n            cls = instance_ref().__class__\n            del method\n\n            @wraps(func)\n            def wrapper(*args, **kwargs):\n                instance = instance_ref()\n                instance._step_count += 1\n                wrapped = func.__get__(instance, cls)\n                return wrapped(*args, **kwargs)\n\n            # Note that the returned function here is no longer a bound method,\n            # so attributes like `__func__` and `__self__` no longer exist.\n            wrapper._with_counter = True\n            return wrapper\n\n        self.optimizer.step = with_counter(self.optimizer.step)\n        self.optimizer._step_count = 0\n        self._step_count = 0\n        self.step(last_iter)\n\n    def state_dict(self):\n        """"""Returns the state of the scheduler as a :class:`dict`.\n\n        It contains an entry for every variable in self.__dict__ which\n        is not the optimizer.\n        """"""\n        return {key: value for key, value in self.__dict__.items() if key != \'optimizer\'}\n\n    def load_state_dict(self, state_dict):\n        """"""Loads the schedulers state.\n\n        Arguments:\n            state_dict (dict): scheduler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        """"""\n        self.__dict__.update(state_dict)\n\n    def get_lr(self):\n        raise NotImplementedError\n\n    def step(self, iter_=None):\n        # Raise a warning if old pattern is detected\n        # https://github.com/pytorch/pytorch/issues/20124\n        if self._step_count == 1:\n            if not hasattr(self.optimizer.step, ""_with_counter""):\n                warnings.warn(""Seems like `optimizer.step()` has been overridden after learning rate scheduler ""\n                              ""initialization. Please, make sure to call `optimizer.step()` before ""\n                              ""`lr_scheduler.step()`. See more details at ""\n                              ""https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"", UserWarning)\n\n            # Just check if there were two first lr_scheduler.step() calls before optimizer.step()\n            elif self.optimizer._step_count < 1:\n                warnings.warn(""Detected call of `lr_scheduler.step()` before `optimizer.step()`. ""\n                              ""In PyTorch 1.1.0 and later, you should call them in the opposite order: ""\n                              ""`optimizer.step()` before `lr_scheduler.step()`.  Failure to do this ""\n                              ""will result in PyTorch skipping the first value of the learning rate schedule.""\n                              ""See more details at ""\n                              ""https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"", UserWarning)\n        self._step_count += 1\n\n        if iter_ is None:\n            iter_ = self.last_iter + 1\n        self.last_iter = iter_\n        self.last_epoch = int(iter_ / self.niter_per_epoch)\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group[\'lr\'] = lr\n'"
vedaseg/lr_schedulers/builder.py,0,"b""from vedaseg.utils import build_from_cfg\n\nfrom .registry import LR_SCHEDULERS\n\n\ndef build_lr_scheduler(cfg, default_args=None):\n    scheduler = build_from_cfg(cfg, LR_SCHEDULERS, default_args, 'registry')\n    return scheduler\n"""
vedaseg/lr_schedulers/poly_lr.py,0,"b'from .base import _Iter_LRScheduler\nfrom .registry import LR_SCHEDULERS\n\n\n@LR_SCHEDULERS.register_module\nclass PolyLR(_Iter_LRScheduler):\n    """"""PolyLR\n    """"""\n    def __init__(self, optimizer, niter_per_epoch, max_epochs, power=0.9, last_iter=-1, warm_up=0):\n        self.max_iters = niter_per_epoch * max_epochs\n        self.power = power\n        self.warm_up = warm_up\n        super().__init__(optimizer, niter_per_epoch, last_iter)\n\n    def get_lr(self):\n        if self.last_iter < self.warm_up:\n            multiplier = (self.last_iter / float(self.warm_up)) ** self.power\n        else:\n            multiplier = (1 - self.last_iter / float(self.max_iters)) ** self.power\n        return [base_lr * multiplier for base_lr in self.base_lrs]\n'"
vedaseg/lr_schedulers/registry.py,1,"b""from torch.optim import lr_scheduler\n\nfrom vedaseg.utils import Registry\n\nLR_SCHEDULERS = Registry('lr_scheduler')\n\nMultiStepLR = lr_scheduler.MultiStepLR\nLR_SCHEDULERS.register_module(MultiStepLR)\n"""
vedaseg/models/__init__.py,0,b'from .registry import MODELS\nfrom .builder import build_model\n'
vedaseg/models/builder.py,1,"b""import torch.nn as nn\n\nfrom vedaseg.models.encoders import build_encoder\nfrom vedaseg.models.decoders import build_decoder\nfrom vedaseg.models.decoders import build_brick\nfrom vedaseg.models.heads import build_head\n\n\ndef build_model(cfg, default_args=None):\n    encoder = build_encoder(cfg.get('encoder'))\n\n    if cfg.get('decoder'):\n        middle = build_decoder(cfg.get('decoder'))\n        assert 'collect' not in cfg\n    else:\n        assert 'collect' in cfg\n        middle = build_brick(cfg.get('collect'))\n\n    head = build_head(cfg['head'])\n\n    model = nn.Sequential(encoder, middle, head)\n\n    return model\n"""
vedaseg/models/registry.py,0,"b""from vedaseg.utils import Registry\n\nMODELS = Registry('model')\n"""
vedaseg/models/weight_init.py,1,"b""# modify from mmcv and mmdetection\n\nimport torch.nn as nn\n\n\ndef constant_init(module, val, bias=0):\n    nn.init.constant_(module.weight, val)\n    if hasattr(module, 'bias') and module.bias is not None:\n        nn.init.constant_(module.bias, bias)\n\n\ndef xavier_init(module, gain=1, bias=0, distribution='normal'):\n    assert distribution in ['uniform', 'normal']\n    if distribution == 'uniform':\n        nn.init.xavier_uniform_(module.weight, gain=gain)\n    else:\n        nn.init.xavier_normal_(module.weight, gain=gain)\n    if hasattr(module, 'bias') and module.bias is not None:\n        nn.init.constant_(module.bias, bias)\n\n\ndef normal_init(module, mean=0, std=1, bias=0):\n    nn.init.normal_(module.weight, mean, std)\n    if hasattr(module, 'bias') and module.bias is not None:\n        nn.init.constant_(module.bias, bias)\n\n\ndef uniform_init(module, a=0, b=1, bias=0):\n    nn.init.uniform_(module.weight, a, b)\n    if hasattr(module, 'bias') and module.bias is not None:\n        nn.init.constant_(module.bias, bias)\n\n\ndef kaiming_init(module,\n                 a=0,\n                 mode='fan_out',\n                 nonlinearity='relu',\n                 bias=0,\n                 distribution='normal'):\n    assert distribution in ['uniform', 'normal']\n    if distribution == 'uniform':\n        nn.init.kaiming_uniform_(module.weight,\n                                 a=a,\n                                 mode=mode,\n                                 nonlinearity=nonlinearity)\n    else:\n        nn.init.kaiming_normal_(module.weight,\n                                a=a,\n                                mode=mode,\n                                nonlinearity=nonlinearity)\n    if hasattr(module, 'bias') and module.bias is not None:\n        nn.init.constant_(module.bias, bias)\n\n\ndef caffe2_xavier_init(module, bias=0):\n    # `XavierFill` in Caffe2 corresponds to `kaiming_uniform_` in PyTorch\n    # Acknowledgment to FAIR's internal code\n    kaiming_init(module,\n                 a=1,\n                 mode='fan_in',\n                 nonlinearity='leaky_relu',\n                 distribution='uniform')\n\n\ndef init_weights(modules):\n    for m in modules:\n        if isinstance(m, nn.Conv2d):\n            kaiming_init(m)\n        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n            constant_init(m, 1)\n"""
vedaseg/optims/__init__.py,0,b'from .builder import build_optim\n'
vedaseg/optims/builder.py,1,"b""import torch.optim as torch_optim\nfrom vedaseg.utils import build_from_cfg\n\n\ndef build_optim(cfg, default_args=None):\n    optim = build_from_cfg(cfg, torch_optim, default_args, 'module')\n    return optim\n"""
vedaseg/runner/__init__.py,0,b'from .runner import Runner\nfrom .builder import build_runner\n'
vedaseg/runner/builder.py,0,"b'from vedaseg.utils import build_from_cfg\n\nfrom .registry import RUNNERS\n\n\ndef build_runner(cfg, default_args=None):\n    runner = build_from_cfg(cfg, RUNNERS, default_args)\n    return runner\n'"
vedaseg/runner/registry.py,0,"b""from vedaseg.utils import Registry\n\nRUNNERS = Registry('runner')\n"""
vedaseg/runner/runner.py,9,"b'import torch\nimport logging\nimport os.path as osp\nimport torch.nn.functional as F\nimport numpy as np\nfrom collections.abc import Iterable\n\nfrom vedaseg.utils.checkpoint import load_checkpoint, save_checkpoint\n\nfrom .registry import RUNNERS\n\nnp.set_printoptions(precision=4)\n\nlogger = logging.getLogger()\n\n\n@RUNNERS.register_module\nclass Runner(object):\n    """""" Runner\n\n    """"""\n    def __init__(self,\n                 loader,\n                 model,\n                 criterion,\n                 metric,\n                 optim,\n                 lr_scheduler,\n                 max_epochs,\n                 workdir,\n                 start_epoch=0,\n                 trainval_ratio=1,\n                 snapshot_interval=1,\n                 gpu=True,\n                 test_cfg=None,\n                 test_mode=False):\n        self.loader = loader\n        self.model = model\n        self.criterion = criterion\n        self.metric = metric\n        self.optim = optim\n        self.lr_scheduler = lr_scheduler\n        self.start_epoch = start_epoch\n        self.max_epochs = max_epochs\n        self.workdir = workdir\n        self.trainval_ratio = trainval_ratio\n        self.snapshot_interval = snapshot_interval\n        self.gpu = gpu\n        self.test_cfg = test_cfg\n        self.test_mode = test_mode\n\n    def __call__(self):\n        if self.test_mode:\n            self.test_epoch()\n        else:\n            assert self.trainval_ratio > 0\n            for epoch in range(self.start_epoch, self.max_epochs):\n                self.train_epoch()\n                self.save_checkpoint(self.workdir)\n                if self.trainval_ratio > 0 \\\n                        and (epoch + 1) % self.trainval_ratio == 0 \\\n                        and self.loader.get(\'val\'):\n                    self.validate_epoch()\n\n    def train_epoch(self):\n        logger.info(\'Epoch %d, Start training\' % self.epoch)\n        iter_based = hasattr(self.lr_scheduler, \'_iter_based\')\n        self.metric.reset()\n        for img, label in self.loader[\'train\']:\n            self.train_batch(img, label)\n            if iter_based:\n                self.lr_scheduler.step()\n        if not iter_based:\n            self.lr_scheduler.step()\n\n    def validate_epoch(self):\n        logger.info(\'Epoch %d, Start validating\' % self.epoch)\n        self.metric.reset()\n        for img, label in self.loader[\'val\']:\n            self.validate_batch(img, label)\n\n    def test_epoch(self):\n        logger.info(\'Start testing\')\n        logger.info(\'test info: %s\' % self.test_cfg)\n        self.metric.reset()\n        for img, label in self.loader[\'val\']:\n            self.test_batch(img, label)\n\n    def train_batch(self, img, label):\n        self.model.train()\n\n        self.optim.zero_grad()\n\n        if self.gpu:\n            img = img.cuda()\n            label = label.cuda()\n        pred = self.model(img)\n        loss = self.criterion(pred, label)\n\n        loss.backward()\n        self.optim.step()\n\n        with torch.no_grad():\n            \n            \'\'\'\n            import matplotlib.pyplot as plt\n            pred = (prob[0]).permute(1, 2, 0).float().cpu().numpy()[:, :, 0]\n            im = img[0].permute(1, 2, 0).clamp(min=0, max=1).cpu().numpy()\n            label_ = label[0].permute(1, 2, 0).clamp(min=0, max=1).cpu().numpy()[:, :, 0]\n            import random\n            random_num = random.randint(0, 1000)\n            pred_name = \'output/%d_pred.jpg\' % random_num\n            plt.imsave(pred_name, pred, cmap=\'Greys\')\n            im_name = \'output/%d.jpg\' % random_num\n            plt.imsave(im_name, im, cmap=\'Greys\')\n            label_name = \'output/%d_gt.jpg\' % random_num\n            plt.imsave(label_name, label_, cmap=\'Greys\')\n            \'\'\'\n            _, pred_label = torch.max(pred, dim=1)\n            self.metric.add(pred_label.cpu().numpy(), label.cpu().numpy())\n            miou, ious = self.metric.miou()\n        if self.iter != 0 and self.iter % 10 == 0:\n            logger.info(\n                \'Train, Epoch %d, Iter %d, LR %s, Loss %.4f, mIoU %.4f, IoUs %s\' %\n                (self.epoch, self.iter, self.lr, loss.item(),\n                 miou, ious))\n\n    def validate_batch(self, img, label):\n        self.model.eval()\n        with torch.no_grad():\n            if self.gpu:\n                img = img.cuda()\n                label = label.cuda()\n\n            pred = self.model(img)\n\n            prob = pred.softmax(dim=1)\n            _, pred_label = torch.max(prob, dim=1)\n            self.metric.add(pred_label.cpu().numpy(), label.cpu().numpy())\n            miou, ious = self.metric.miou()\n            logger.info(\'Validate, mIoU %.4f, IoUs %s\' % (miou, ious))\n\n    def test_batch(self, img, label):\n        self.model.eval()\n        with torch.no_grad():\n            if self.gpu:\n                img = img.cuda()\n                label = label.cuda()\n\n            if self.test_cfg:\n                scales = self.test_cfg.get(\'scales\', [1.0])\n                flip = self.test_cfg.get(\'flip\', False)\n                biases = self.test_cfg.get(\'bias\', [0.0])\n            else:\n                scales = [1.0]\n                flip = False\n                biases = [0.0]\n\n            assert len(scales) == len(biases)\n\n            n, c, h, w = img.size()\n            probs = []\n            for scale, bias in zip(scales, biases):\n                new_h, new_w = int(h*scale + bias), int(w*scale+bias)\n                new_img = F.interpolate(img, size=(new_h, new_w), mode=\'bilinear\', align_corners=True)\n                prob = self.model(new_img).softmax(dim=1)\n                probs.append(prob)\n\n                if flip:\n                    flip_img = new_img.flip(3)\n                    flip_prob = self.model(flip_img).softmax(dim=1)\n                    prob = flip_prob.flip(3)\n                    probs.append(prob)\n            prob = torch.stack(probs, dim=0).mean(dim=0)\n\n            _, pred_label = torch.max(prob, dim=1)\n            self.metric.add(pred_label.cpu().numpy(), label.cpu().numpy())\n            miou, ious = self.metric.miou()\n            logger.info(\'Test, mIoU %.4f, IoUs %s\' % (miou, ious))\n\n    def save_checkpoint(self,\n                        out_dir,\n                        filename_tmpl=\'epoch_{}.pth\',\n                        save_optimizer=True,\n                        meta=None):\n        if self.epoch % self.snapshot_interval == 0 or self.epoch == self.max_epochs:\n            if meta is None:\n                meta = dict(epoch=self.epoch, iter=self.iter, lr=self.lr)\n            else:\n                meta.update(epoch=self.epoch, iter=self.iter, lr=self.lr)\n\n            filename = filename_tmpl.format(self.epoch)\n            filepath = osp.join(out_dir, filename)\n            optimizer = self.optim if save_optimizer else None\n            logger.info(\'Save checkpoint %s\', filename)\n            save_checkpoint(self.model,\n                            filepath,\n                            optimizer=optimizer,\n                            meta=meta)\n\n    def load_checkpoint(self, filename, map_location=\'cpu\', strict=False):\n        logger.info(\'Resume from %s\', filename)\n        return load_checkpoint(self.model, filename, map_location, strict,\n                               logger)\n\n    @property\n    def epoch(self):\n        """"""int: Current epoch.""""""\n        return self.lr_scheduler.last_epoch\n\n    @epoch.setter\n    def epoch(self, val):\n        """"""int: Current epoch.""""""\n        self.lr_scheduler.last_epoch = val\n\n    @property\n    def lr(self):\n        lr = [x[\'lr\'] for x in self.optim.param_groups]\n        return np.array(lr)\n\n    @lr.setter\n    def lr(self, val):\n        for idx, param in enumerate(self.optim.param_groups):\n            if isinstance(val, Iterable):\n                param[\'lr\'] = val[idx]\n            else:\n                param[\'lr\'] = val\n\n    @property\n    def iter(self):\n        """"""int: Current iteration.""""""\n        return self.lr_scheduler.last_iter\n\n    @iter.setter\n    def iter(self, val):\n        """"""int: Current epoch.""""""\n        self.lr_scheduler.last_iter = val\n\n    def resume(self,\n               checkpoint,\n               resume_optimizer=False,\n               resume_lr=True,\n               resume_epoch=True,\n               map_location=\'default\'):\n        if map_location == \'default\':\n            device_id = torch.cuda.current_device()\n            checkpoint = self.load_checkpoint(\n                checkpoint,\n                map_location=lambda storage, loc: storage.cuda(device_id))\n        else:\n            checkpoint = self.load_checkpoint(checkpoint, map_location=map_location)\n        if \'optimizer\' in checkpoint and resume_optimizer:\n            self.optim.load_state_dict(checkpoint[\'optimizer\'])\n        if resume_epoch:\n            self.epoch = checkpoint[\'meta\'][\'epoch\']\n            self.start_epoch = self.epoch\n            self.iter = checkpoint[\'meta\'][\'iter\']\n        if resume_lr:\n            self.lr = checkpoint[\'meta\'][\'lr\']\n'"
vedaseg/utils/__init__.py,0,"b'from .config import ConfigDict, Config\nfrom .common import build_from_cfg, get_root_logger, set_random_seed\nfrom .registry import Registry\nfrom .metrics import MetricMeter, dice_score\n'"
vedaseg/utils/checkpoint.py,7,"b'# modify from mmcv and mmdetection\n\nimport os\nimport os.path as osp\nimport pkgutil\nimport time\nimport warnings\nfrom collections import OrderedDict\nfrom importlib import import_module\n\nimport torch\nimport torchvision\nfrom terminaltables import AsciiTable\nfrom torch.utils import model_zoo\n\nfrom . import path\n\nopen_mmlab_model_urls = {\n    \'vgg16_caffe\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/vgg16_caffe-292e1171.pth\',  # noqa: E501\n    \'resnet50_caffe\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnet50_caffe-788b5fa3.pth\',  # noqa: E501\n    \'resnet101_caffe\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnet101_caffe-3ad79236.pth\',  # noqa: E501\n    \'resnext50_32x4d\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnext50-32x4d-0ab1a123.pth\',  # noqa: E501\n    \'resnext101_32x4d\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnext101_32x4d-a5af3160.pth\',  # noqa: E501\n    \'resnext101_64x4d\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnext101_64x4d-ee2c6f71.pth\',  # noqa: E501\n    \'contrib/resnet50_gn\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnet50_gn_thangvubk-ad1730dd.pth\',  # noqa: E501\n    \'detectron/resnet50_gn\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnet50_gn-9186a21c.pth\',  # noqa: E501\n    \'detectron/resnet101_gn\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnet101_gn-cac0ab98.pth\',  # noqa: E501\n    \'jhu/resnet50_gn_ws\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnet50_gn_ws-15beedd8.pth\',  # noqa: E501\n    \'jhu/resnet101_gn_ws\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnet101_gn_ws-3e3c308c.pth\',  # noqa: E501\n    \'jhu/resnext50_32x4d_gn_ws\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnext50_32x4d_gn_ws-0d87ac85.pth\',  # noqa: E501\n    \'jhu/resnext101_32x4d_gn_ws\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnext101_32x4d_gn_ws-34ac1a9e.pth\',  # noqa: E501\n    \'jhu/resnext50_32x4d_gn\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnext50_32x4d_gn-c7e8b754.pth\',  # noqa: E501\n    \'jhu/resnext101_32x4d_gn\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/resnext101_32x4d_gn-ac3bb84e.pth\',  # noqa: E501\n    \'msra/hrnetv2_w18\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/hrnetv2_w18-00eb2006.pth\',  # noqa: E501\n    \'msra/hrnetv2_w32\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/hrnetv2_w32-dc9eeb4f.pth\',  # noqa: E501\n    \'msra/hrnetv2_w40\': \'https://s3.ap-northeast-2.amazonaws.com/open-mmlab/pretrain/third_party/hrnetv2_w40-ed0b031c.pth\',  # noqa: E501\n    \'bninception_caffe\': \'https://open-mmlab.s3.ap-northeast-2.amazonaws.com/pretrain/third_party/bn_inception_caffe-ed2e8665.pth\',  # noqa: E501\n    \'kin400/i3d_r50_f32s2_k400\': \'https://open-mmlab.s3.ap-northeast-2.amazonaws.com/pretrain/third_party/i3d_r50_f32s2_k400-2c57e077.pth\',  # noqa: E501\n    \'kin400/nl3d_r50_f32s2_k400\': \'https://open-mmlab.s3.ap-northeast-2.amazonaws.com/pretrain/third_party/nl3d_r50_f32s2_k400-fa7e7caa.pth\',  # noqa: E501\n}  # yapf: disable\n\n\ndef load_state_dict(module, state_dict, strict=False, logger=None):\n    """"""Load state_dict to a module.\n\n    This method is modified from :meth:`torch.nn.Module.load_state_dict`.\n    Default value for ``strict`` is set to ``False`` and the message for\n    param mismatch will be shown even if strict is False.\n\n    Args:\n        module (Module): Module that receives the state_dict.\n        state_dict (OrderedDict): Weights.\n        strict (bool): whether to strictly enforce that the keys\n            in :attr:`state_dict` match the keys returned by this module\'s\n            :meth:`~torch.nn.Module.state_dict` function. Default: ``False``.\n        logger (:obj:`logging.Logger`, optional): Logger to log the error\n            message. If not specified, print function will be used.\n    """"""\n    unexpected_keys = []\n    shape_mismatch_pairs = []\n\n    own_state = module.state_dict()\n    for name, param in state_dict.items():\n        if name not in own_state:\n            unexpected_keys.append(name)\n            continue\n        if isinstance(param, torch.nn.Parameter):\n            # backwards compatibility for serialized parameters\n            param = param.data\n        if param.size() != own_state[name].size():\n            shape_mismatch_pairs.append(\n                [name, own_state[name].size(),\n                 param.size()])\n            continue\n        own_state[name].copy_(param)\n\n    all_missing_keys = set(own_state.keys()) - set(state_dict.keys())\n    # ignore ""num_batches_tracked"" of BN layers\n    missing_keys = [\n        key for key in all_missing_keys if \'num_batches_tracked\' not in key\n    ]\n\n    err_msg = []\n    if unexpected_keys:\n        err_msg.append(\'unexpected key in source state_dict: {}\\n\'.format(\n            \', \'.join(unexpected_keys)))\n    if missing_keys:\n        err_msg.append(\'missing keys in source state_dict: {}\\n\'.format(\n            \', \'.join(missing_keys)))\n    if shape_mismatch_pairs:\n        mismatch_info = \'these keys have mismatched shape:\\n\'\n        header = [\'key\', \'expected shape\', \'loaded shape\']\n        table_data = [header] + shape_mismatch_pairs\n        table = AsciiTable(table_data)\n        err_msg.append(mismatch_info + table.table)\n\n    if len(err_msg) > 0:\n        err_msg.insert(\n            0, \'The model and loaded state dict do not match exactly\\n\')\n        err_msg = \'\\n\'.join(err_msg)\n        if strict:\n            raise RuntimeError(err_msg)\n        elif logger is not None:\n            logger.warning(err_msg)\n        else:\n            print(err_msg)\n\n\ndef load_url_dist(url):\n    """""" In distributed setting, this function only download checkpoint at\n    local rank 0 """"""\n    checkpoint = model_zoo.load_url(url)\n    return checkpoint\n\n\ndef get_torchvision_models():\n    model_urls = dict()\n    for _, name, ispkg in pkgutil.walk_packages(torchvision.models.__path__):\n        if ispkg:\n            continue\n        _zoo = import_module(\'torchvision.models.{}\'.format(name))\n        if hasattr(_zoo, \'model_urls\'):\n            _urls = getattr(_zoo, \'model_urls\')\n            model_urls.update(_urls)\n    return model_urls\n\n\ndef load_checkpoint(model,\n                    filename,\n                    map_location=None,\n                    strict=False,\n                    logger=None):\n    """"""Load checkpoint from a file or URI.\n\n    Args:\n        model (Module): Module to load checkpoint.\n        filename (str): Either a filepath or URL or modelzoo://xxxxxxx.\n        map_location (str): Same as :func:`torch.load`.\n        strict (bool): Whether to allow different params for the model and\n            checkpoint.\n        logger (:mod:`logging.Logger` or None): The logger for error message.\n\n    Returns:\n        dict or OrderedDict: The loaded checkpoint.\n    """"""\n    # load checkpoint from modelzoo or file or url\n    if filename.startswith(\'modelzoo://\'):\n        warnings.warn(\'The URL scheme of ""modelzoo://"" is deprecated, please \'\n                      \'use ""torchvision://"" instead\')\n        model_urls = get_torchvision_models()\n        model_name = filename[11:]\n        checkpoint = load_url_dist(model_urls[model_name])\n    elif filename.startswith(\'torchvision://\'):\n        model_urls = get_torchvision_models()\n        model_name = filename[14:]\n        checkpoint = load_url_dist(model_urls[model_name])\n    elif filename.startswith(\'open-mmlab://\'):\n        model_name = filename[13:]\n        checkpoint = load_url_dist(open_mmlab_model_urls[model_name])\n    elif filename.startswith((\'http://\', \'https://\')):\n        checkpoint = load_url_dist(filename)\n    else:\n        if not osp.isfile(filename):\n            raise IOError(\'{} is not a checkpoint file\'.format(filename))\n        checkpoint = torch.load(filename, map_location=map_location)\n    # get state_dict from checkpoint\n    if isinstance(checkpoint, OrderedDict):\n        state_dict = checkpoint\n    elif isinstance(checkpoint, dict) and \'state_dict\' in checkpoint:\n        state_dict = checkpoint[\'state_dict\']\n    else:\n        raise RuntimeError(\n            \'No state_dict found in checkpoint file {}\'.format(filename))\n    # strip prefix of state_dict\n    if list(state_dict.keys())[0].startswith(\'module.\'):\n        state_dict = {k[7:]: v for k, v in checkpoint[\'state_dict\'].items()}\n    # load state_dict\n    if hasattr(model, \'module\'):\n        load_state_dict(model.module, state_dict, strict, logger)\n    else:\n        load_state_dict(model, state_dict, strict, logger)\n    return checkpoint\n\n\ndef weights_to_cpu(state_dict):\n    """"""Copy a model state_dict to cpu.\n\n    Args:\n        state_dict (OrderedDict): Model weights on GPU.\n\n    Returns:\n        OrderedDict: Model weights on GPU.\n    """"""\n    state_dict_cpu = OrderedDict()\n    for key, val in state_dict.items():\n        state_dict_cpu[key] = val.cpu()\n    return state_dict_cpu\n\n\ndef save_checkpoint(model, filename, optimizer=None, meta=None):\n    """"""Save checkpoint to file.\n\n    The checkpoint will have 3 fields: ``meta``, ``state_dict`` and\n    ``optimizer``. By default ``meta`` will contain version and time info.\n\n    Args:\n        model (Module): Module whose params are to be saved.\n        filename (str): Checkpoint filename.\n        optimizer (:obj:`Optimizer`, optional): Optimizer to be saved.\n        meta (dict, optional): Metadata to be saved in checkpoint.\n    """"""\n    if meta is None:\n        meta = {}\n    elif not isinstance(meta, dict):\n        raise TypeError(\'meta must be a dict or None, but got {}\'.format(\n            type(meta)))\n    meta.update(time=time.asctime())\n\n    path.mkdir_or_exist(osp.dirname(filename))\n    if hasattr(model, \'module\'):\n        model = model.module\n\n    checkpoint = {\n        \'meta\': meta,\n        \'state_dict\': weights_to_cpu(model.state_dict())\n    }\n    if optimizer is not None:\n        checkpoint[\'optimizer\'] = optimizer.state_dict()\n\n    torch.save(checkpoint, filename)\n'"
vedaseg/utils/common.py,2,"b'# modify from mmcv and mmdetection\n\nimport sys\nimport random\nimport torch\nimport numpy as np\nimport inspect\nimport logging\n\n\ndef build_from_cfg(cfg, parent, default_args=None, src=\'registry\'):\n    if src == \'registry\':\n        return obj_from_dict_registry(cfg, parent, default_args)\n    elif src == \'module\':\n        return obj_from_dict_module(cfg, parent, default_args)\n    else:\n        raise ValueError(\'Method %s is not supported\' % src)\n\n\ndef obj_from_dict_module(info, parent=None, default_args=None):\n    """"""Initialize an object from dict.\n    The dict must contain the key ""type"", which indicates the object type, it\n    can be either a string or type, such as ""list"" or ``list``. Remaining\n    fields are treated as the arguments for constructing the object.\n    Args:\n        info (dict): Object types and arguments.\n        parent (:class:`module`): Module which may containing expected object\n            classes.\n        default_args (dict, optional): Default arguments for initializing the\n            object.\n    Returns:\n        any type: Object built from the dict.\n    """"""\n    assert isinstance(info, dict) and \'type\' in info\n    assert isinstance(default_args, dict) or default_args is None\n    args = info.copy()\n    obj_type = args.pop(\'type\')\n    if isinstance(obj_type, str):\n        if parent is not None:\n            obj_type = getattr(parent, obj_type)\n        else:\n            obj_type = sys.modules[obj_type]\n    elif not isinstance(obj_type, type):\n        raise TypeError(\'type must be a str or valid type, but got {}\'.format(\n            type(obj_type)))\n    if default_args is not None:\n        for name, value in default_args.items():\n            args.setdefault(name, value)\n    return obj_type(**args)\n\n\ndef obj_from_dict_registry(cfg, registry, default_args=None):\n    """"""Build a module from config dict.\n    Args:\n        cfg (dict): Config dict. It should at least contain the key ""type"".\n        registry (:obj:`Registry`): The registry to search the type from.\n        default_args (dict, optional): Default initialization arguments.\n    Returns:\n        obj: The constructed object.\n    """"""\n    assert isinstance(cfg, dict) and \'type\' in cfg\n    assert isinstance(default_args, dict) or default_args is None\n    args = cfg.copy()\n    obj_type = args.pop(\'type\')\n    if isinstance(obj_type, str):\n        obj_cls = registry.get(obj_type)\n        if obj_cls is None:\n            raise KeyError(\'{} is not in the {} registry\'.format(\n                obj_type, registry.name))\n    elif inspect.isclass(obj_type):\n        obj_cls = obj_type\n    else:\n        raise TypeError(\'type must be a str or valid type, but got {}\'.format(\n            type(obj_type)))\n    if default_args is not None:\n        for name, value in default_args.items():\n            args.setdefault(name, value)\n    return obj_cls(**args)\n\n\ndef set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef get_root_logger(log_level=logging.INFO):\n    logger = logging.getLogger()\n    np.set_printoptions(precision=4)\n    if not logger.hasHandlers():\n        logging.basicConfig(format=\'%(asctime)s - %(levelname)s - %(message)s\',\n                            level=log_level)\n    return logger\n'"
vedaseg/utils/config.py,0,"b'# modify from mmcv and mmdetection\n\nimport os.path as osp\nimport sys\nfrom argparse import ArgumentParser\nfrom importlib import import_module\n\nfrom addict import Dict\n\nfrom .misc import collections_abc\nfrom .path import check_file_exist\n\n\nclass ConfigDict(Dict):\n    def __missing__(self, name):\n        raise KeyError(name)\n\n    def __getattr__(self, name):\n        try:\n            value = super(ConfigDict, self).__getattr__(name)\n        except KeyError:\n            ex = AttributeError(""\'{}\' object has no attribute \'{}\'"".format(\n                self.__class__.__name__, name))\n        except Exception as e:\n            ex = e\n        else:\n            return value\n        raise ex\n\n\ndef add_args(parser, cfg, prefix=\'\'):\n    for k, v in cfg.items():\n        if isinstance(v, str):\n            parser.add_argument(\'--\' + prefix + k)\n        elif isinstance(v, int):\n            parser.add_argument(\'--\' + prefix + k, type=int)\n        elif isinstance(v, float):\n            parser.add_argument(\'--\' + prefix + k, type=float)\n        elif isinstance(v, bool):\n            parser.add_argument(\'--\' + prefix + k, action=\'store_true\')\n        elif isinstance(v, dict):\n            add_args(parser, v, k + \'.\')\n        elif isinstance(v, collections_abc.Iterable):\n            parser.add_argument(\'--\' + prefix + k, type=type(v[0]), nargs=\'+\')\n        else:\n            print(\'connot parse key {} of type {}\'.format(prefix + k, type(v)))\n    return parser\n\n\nclass Config(object):\n    """"""A facility for config and config files.\n\n    It supports common file formats as configs: python/json/yaml. The interface\n    is the same as a dict object and also allows access config values as\n    attributes.\n\n    Example:\n        >>> cfg = Config(dict(a=1, b=dict(b1=[0, 1])))\n        >>> cfg.a\n        1\n        >>> cfg.b\n        {\'b1\': [0, 1]}\n        >>> cfg.b.b1\n        [0, 1]\n        >>> cfg = Config.fromfile(\'tests/data/config/a.py\')\n        >>> cfg.filename\n        ""/home/kchen/projects/mmcv/tests/data/config/a.py""\n        >>> cfg.item4\n        \'test\'\n        >>> cfg\n        ""Config [path: /home/kchen/projects/mmcv/tests/data/config/a.py]: ""\n        ""{\'item1\': [1, 2], \'item2\': {\'a\': 0}, \'item3\': True, \'item4\': \'test\'}""\n\n    """"""\n    @staticmethod\n    def fromfile(filename):\n        filename = osp.abspath(osp.expanduser(filename))\n        check_file_exist(filename)\n        if filename.endswith(\'.py\'):\n            module_name = osp.basename(filename)[:-3]\n            if \'.\' in module_name:\n                raise ValueError(\'Dots are not allowed in config file path.\')\n            config_dir = osp.dirname(filename)\n            sys.path.insert(0, config_dir)\n            mod = import_module(module_name)\n            sys.path.pop(0)\n            cfg_dict = {\n                name: value\n                for name, value in mod.__dict__.items()\n                if not name.startswith(\'__\')\n            }\n        #elif filename.endswith((\'.yml\', \'.yaml\', \'.json\')):\n        #    import mmcv\n        #    cfg_dict = mmcv.load(filename)\n        else:\n            raise IOError(\'Only py type are supported now!\')\n        return Config(cfg_dict, filename=filename)\n\n    @staticmethod\n    def auto_argparser(description=None):\n        """"""Generate argparser from config file automatically (experimental)\n        """"""\n        partial_parser = ArgumentParser(description=description)\n        partial_parser.add_argument(\'config\', help=\'config file path\')\n        cfg_file = partial_parser.parse_known_args()[0].config\n        cfg = Config.fromfile(cfg_file)\n        parser = ArgumentParser(description=description)\n        parser.add_argument(\'config\', help=\'config file path\')\n        add_args(parser, cfg)\n        return parser, cfg\n\n    def __init__(self, cfg_dict=None, filename=None):\n        if cfg_dict is None:\n            cfg_dict = dict()\n        elif not isinstance(cfg_dict, dict):\n            raise TypeError(\'cfg_dict must be a dict, but got {}\'.format(\n                type(cfg_dict)))\n\n        super(Config, self).__setattr__(\'_cfg_dict\', ConfigDict(cfg_dict))\n        super(Config, self).__setattr__(\'_filename\', filename)\n        if filename:\n            with open(filename, \'r\') as f:\n                super(Config, self).__setattr__(\'_text\', f.read())\n        else:\n            super(Config, self).__setattr__(\'_text\', \'\')\n\n    @property\n    def filename(self):\n        return self._filename\n\n    @property\n    def text(self):\n        return self._text\n\n    def __repr__(self):\n        return \'Config (path: {}): {}\'.format(self.filename,\n                                              self._cfg_dict.__repr__())\n\n    def __len__(self):\n        return len(self._cfg_dict)\n\n    def __getattr__(self, name):\n        return getattr(self._cfg_dict, name)\n\n    def __getitem__(self, name):\n        return self._cfg_dict.__getitem__(name)\n\n    def __setattr__(self, name, value):\n        if isinstance(value, dict):\n            value = ConfigDict(value)\n        self._cfg_dict.__setattr__(name, value)\n\n    def __setitem__(self, name, value):\n        if isinstance(value, dict):\n            value = ConfigDict(value)\n        self._cfg_dict.__setitem__(name, value)\n\n    def __iter__(self):\n        return iter(self._cfg_dict)\n'"
vedaseg/utils/metrics.py,4,"b'import numpy as np\nimport torch\n\n\ndef dice_score(pred, gt, thres_range=np.arange(0.1, 1.0, 0.1)):\n    """"""dice_score\n        \n        Args:\n            pred, n*c*h*w, torch.Tensor\n            gt, n*c*h*w, torch.Tensor\n\n        Return:\n            dice, nthres * nclasses\n    """"""\n    gt = gt.float()\n\n    dices = []\n    for thres in thres_range:\n        tpred = (pred > thres).float()\n        nu = 2 * (tpred * gt).sum(dim=[2, 3])\n        de = tpred.sum(dim=[2, 3]) + gt.sum(dim=[2, 3])\n        dice = nu / de\n        dice[torch.isnan(dice)] = 1\n        dices.append(dice.sum(0))\n    return torch.stack(dices, 0)\n\n\nclass MetricMeter(object):\n    """"""MetricMeter\n    """"""\n    def __init__(self, nclasses):\n        self.nclasses = nclasses\n        self.confusion_matrix = np.zeros((self.nclasses,)*2)\n\n    def pixel_accuracy(self):\n        acc = np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n        return acc\n\n    def pixel_accuracy_class(self):\n        Acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n        Acc = np.nanmean(Acc)\n        return Acc\n\n    def miou(self):\n        IoUs = np.diag(self.confusion_matrix) / (\n                    np.sum(self.confusion_matrix, axis=1) + np.sum(self.confusion_matrix, axis=0) -\n                    np.diag(self.confusion_matrix) + np.finfo(np.float32).eps)\n        mIoU = np.nanmean(IoUs)\n        return mIoU, IoUs\n\n    def fw_iou(self):\n        """"""fw_iou, frequency weighted iou\n        """"""\n        freq = np.sum(self.confusion_matrix, axis=1) / np.sum(self.confusion_matrix)\n        IoU = np.diag(self.confusion_matrix) / (\n                    np.sum(self.confusion_matrix, axis=1) + np.sum(self.confusion_matrix, axis=0) -\n                    np.diag(self.confusion_matrix))\n\n        fwIoU = (freq[freq > 0] * IoU[freq > 0]).sum()\n        return fwIoU\n\n    def _generate_matrix(self, pred, gt):\n        mask = (gt >= 0) & (gt < self.nclasses)\n        label = self.nclasses * gt[mask].astype(\'int\') + pred[mask]\n        count = np.bincount(label, minlength=self.nclasses**2)\n        confusion_matrix = count.reshape(self.nclasses, self.nclasses)\n        return confusion_matrix\n\n    def add(self, pred, gt):\n        assert pred.shape == gt.shape\n        self.confusion_matrix += self._generate_matrix(pred, gt)\n\n    def reset(self):\n        self.confusion_matrix = np.zeros((self.nclasses,) * 2)\n'"
vedaseg/utils/misc.py,0,"b'# modify from mmcv and mmdetection\n\nimport collections\nimport functools\nimport itertools\nimport subprocess\nfrom importlib import import_module\n\nimport six\n\n# ABCs from collections will be deprecated in python 3.8+,\n# while collections.abc is not available in python 2.7\ntry:\n    import collections.abc as collections_abc\nexcept ImportError:\n    import collections as collections_abc\n\n\ndef is_str(x):\n    """"""Whether the input is an string instance.""""""\n    return isinstance(x, six.string_types)\n\n\ndef iter_cast(inputs, dst_type, return_type=None):\n    """"""Cast elements of an iterable object into some type.\n\n    Args:\n        inputs (Iterable): The input object.\n        dst_type (type): Destination type.\n        return_type (type, optional): If specified, the output object will be\n            converted to this type, otherwise an iterator.\n\n    Returns:\n        iterator or specified type: The converted object.\n    """"""\n    if not isinstance(inputs, collections_abc.Iterable):\n        raise TypeError(\'inputs must be an iterable object\')\n    if not isinstance(dst_type, type):\n        raise TypeError(\'""dst_type"" must be a valid type\')\n\n    out_iterable = six.moves.map(dst_type, inputs)\n\n    if return_type is None:\n        return out_iterable\n    else:\n        return return_type(out_iterable)\n\n\ndef list_cast(inputs, dst_type):\n    """"""Cast elements of an iterable object into a list of some type.\n\n    A partial method of :func:`iter_cast`.\n    """"""\n    return iter_cast(inputs, dst_type, return_type=list)\n\n\ndef tuple_cast(inputs, dst_type):\n    """"""Cast elements of an iterable object into a tuple of some type.\n\n    A partial method of :func:`iter_cast`.\n    """"""\n    return iter_cast(inputs, dst_type, return_type=tuple)\n\n\ndef is_seq_of(seq, expected_type, seq_type=None):\n    """"""Check whether it is a sequence of some type.\n\n    Args:\n        seq (Sequence): The sequence to be checked.\n        expected_type (type): Expected type of sequence items.\n        seq_type (type, optional): Expected sequence type.\n\n    Returns:\n        bool: Whether the sequence is valid.\n    """"""\n    if seq_type is None:\n        exp_seq_type = collections_abc.Sequence\n    else:\n        assert isinstance(seq_type, type)\n        exp_seq_type = seq_type\n    if not isinstance(seq, exp_seq_type):\n        return False\n    for item in seq:\n        if not isinstance(item, expected_type):\n            return False\n    return True\n\n\ndef is_list_of(seq, expected_type):\n    """"""Check whether it is a list of some type.\n\n    A partial method of :func:`is_seq_of`.\n    """"""\n    return is_seq_of(seq, expected_type, seq_type=list)\n\n\ndef is_tuple_of(seq, expected_type):\n    """"""Check whether it is a tuple of some type.\n\n    A partial method of :func:`is_seq_of`.\n    """"""\n    return is_seq_of(seq, expected_type, seq_type=tuple)\n\n\ndef slice_list(in_list, lens):\n    """"""Slice a list into several sub lists by a list of given length.\n\n    Args:\n        in_list (list): The list to be sliced.\n        lens(int or list): The expected length of each out list.\n\n    Returns:\n        list: A list of sliced list.\n    """"""\n    if not isinstance(lens, list):\n        raise TypeError(\'""indices"" must be a list of integers\')\n    elif sum(lens) != len(in_list):\n        raise ValueError(\n            \'sum of lens and list length does not match: {} != {}\'.format(\n                sum(lens), len(in_list)))\n    out_list = []\n    idx = 0\n    for i in range(len(lens)):\n        out_list.append(in_list[idx:idx + lens[i]])\n        idx += lens[i]\n    return out_list\n\n\ndef concat_list(in_list):\n    """"""Concatenate a list of list into a single list.\n\n    Args:\n        in_list (list): The list of list to be merged.\n\n    Returns:\n        list: The concatenated flat list.\n    """"""\n    return list(itertools.chain(*in_list))\n\n\ndef check_prerequisites(\n        prerequisites,\n        checker,\n        msg_tmpl=\'Prerequisites ""{}"" are required in method ""{}"" but not \'\n        \'found, please install them first.\'):\n    """"""A decorator factory to check if prerequisites are satisfied.\n\n    Args:\n        prerequisites (str of list[str]): Prerequisites to be checked.\n        checker (callable): The checker method that returns True if a\n            prerequisite is meet, False otherwise.\n        msg_tmpl (str): The message template with two variables.\n\n    Returns:\n        decorator: A specific decorator.\n    """"""\n    def wrap(func):\n        @functools.wraps(func)\n        def wrapped_func(*args, **kwargs):\n            requirements = [prerequisites] if isinstance(\n                prerequisites, str) else prerequisites\n            missing = []\n            for item in requirements:\n                if not checker(item):\n                    missing.append(item)\n            if missing:\n                print(msg_tmpl.format(\', \'.join(missing), func.__name__))\n                raise RuntimeError(\'Prerequisites not meet.\')\n            else:\n                return func(*args, **kwargs)\n\n        return wrapped_func\n\n    return wrap\n\n\ndef _check_py_package(package):\n    try:\n        import_module(package)\n    except ImportError:\n        return False\n    else:\n        return True\n\n\ndef _check_executable(cmd):\n    if subprocess.call(\'which {}\'.format(cmd), shell=True) != 0:\n        return False\n    else:\n        return True\n\n\ndef requires_package(prerequisites):\n    """"""A decorator to check if some python packages are installed.\n\n    Example:\n        >>> @requires_package(\'numpy\')\n        >>> func(arg1, args):\n        >>>     return numpy.zeros(1)\n        array([0.])\n        >>> @requires_package([\'numpy\', \'non_package\'])\n        >>> func(arg1, args):\n        >>>     return numpy.zeros(1)\n        ImportError\n    """"""\n    return check_prerequisites(prerequisites, checker=_check_py_package)\n\n\ndef requires_executable(prerequisites):\n    """"""A decorator to check if some executable files are installed.\n\n    Example:\n        >>> @requires_executable(\'ffmpeg\')\n        >>> func(arg1, args):\n        >>>     print(1)\n        1\n    """"""\n    return check_prerequisites(prerequisites, checker=_check_executable)\n'"
vedaseg/utils/path.py,0,"b'# modify from mmcv and mmdetection\n\nimport os\nimport os.path as osp\nimport sys\nfrom pathlib import Path\n\nimport six\n\nfrom .misc import is_str\n\nif sys.version_info <= (3, 3):\n    FileNotFoundError = IOError\nelse:\n    FileNotFoundError = FileNotFoundError\n\n\ndef is_filepath(x):\n    if is_str(x) or isinstance(x, Path):\n        return True\n    else:\n        return False\n\n\ndef fopen(filepath, *args, **kwargs):\n    if is_str(filepath):\n        return open(filepath, *args, **kwargs)\n    elif isinstance(filepath, Path):\n        return filepath.open(*args, **kwargs)\n\n\ndef check_file_exist(filename, msg_tmpl=\'file ""{}"" does not exist\'):\n    if not osp.isfile(filename):\n        raise FileNotFoundError(msg_tmpl.format(filename))\n\n\ndef mkdir_or_exist(dir_name, mode=0o777):\n    if dir_name == \'\':\n        return\n    dir_name = osp.expanduser(dir_name)\n    if six.PY3:\n        os.makedirs(dir_name, mode=mode, exist_ok=True)\n    else:\n        if not osp.isdir(dir_name):\n            os.makedirs(dir_name, mode=mode)\n\n\ndef symlink(src, dst, overwrite=True, **kwargs):\n    if os.path.lexists(dst) and overwrite:\n        os.remove(dst)\n    os.symlink(src, dst, **kwargs)\n\n\ndef _scandir_py35(dir_path, suffix=None):\n    for entry in os.scandir(dir_path):\n        if not entry.is_file():\n            continue\n        filename = entry.name\n        if suffix is None:\n            yield filename\n        elif filename.endswith(suffix):\n            yield filename\n\n\ndef _scandir_py(dir_path, suffix=None):\n    for filename in os.listdir(dir_path):\n        if not osp.isfile(osp.join(dir_path, filename)):\n            continue\n        if suffix is None:\n            yield filename\n        elif filename.endswith(suffix):\n            yield filename\n\n\ndef scandir(dir_path, suffix=None):\n    if suffix is not None and not isinstance(suffix, (str, tuple)):\n        raise TypeError(\'""suffix"" must be a string or tuple of strings\')\n    if sys.version_info >= (3, 5):\n        return _scandir_py35(dir_path, suffix)\n    else:\n        return _scandir_py(dir_path, suffix)\n\n\ndef find_vcs_root(path, markers=(\'.git\', )):\n    """"""Finds the root directory (including itself) of specified markers.\n\n    Args:\n        path (str): Path of directory or file.\n        markers (list[str], optional): List of file or directory names.\n\n    Returns:\n        The directory contained one of the markers or None if not found.\n    """"""\n    if osp.isfile(path):\n        path = osp.dirname(path)\n\n    prev, cur = None, osp.abspath(osp.expanduser(path))\n    while cur != prev:\n        if any(osp.exists(osp.join(cur, marker)) for marker in markers):\n            return cur\n        prev, cur = cur, osp.split(cur)[0]\n    return None\n'"
vedaseg/utils/registry.py,0,"b'# modify from mmcv and mmdetection\n\nimport inspect\n\n\nclass Registry(object):\n    def __init__(self, name):\n        self._name = name\n        self._module_dict = dict()\n\n    def __repr__(self):\n        format_str = self.__class__.__name__ + \'(name={}, items={})\'.format(\n            self._name, list(self._module_dict.keys()))\n        return format_str\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def module_dict(self):\n        return self._module_dict\n\n    def get(self, key):\n        return self._module_dict.get(key, None)\n\n    def _register_module(self, module_class):\n        """"""Register a module.\n        Args:\n            module (:obj:`nn.Module`): Module to be registered.\n        """"""\n        if not inspect.isclass(module_class):\n            raise TypeError(\'module must be a class, but got {}\'.format(\n                type(module_class)))\n        module_name = module_class.__name__\n        if module_name in self._module_dict:\n            raise KeyError(\'{} is already registered in {}\'.format(\n                module_name, self.name))\n        self._module_dict[module_name] = module_class\n\n    def register_module(self, cls):\n        self._register_module(cls)\n        return cls\n'"
vedaseg/datasets/transforms/__init__.py,0,b''
vedaseg/datasets/transforms/builder.py,0,"b'from vedaseg.utils import build_from_cfg\n\nfrom .registry import TRANSFORMS\n\nfrom .transforms import Compose\n\n\ndef build_transform(cfg):\n    tfs = []\n    for icfg in cfg:\n        tf = build_from_cfg(icfg, TRANSFORMS)\n        tfs.append(tf)\n    aug = Compose(tfs)\n\n    return aug\n'"
vedaseg/datasets/transforms/registry.py,0,"b""from vedaseg.utils import Registry\n\nTRANSFORMS = Registry('transforms')\n"""
vedaseg/datasets/transforms/transforms.py,5,"b""import random\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchvision.transforms as tt\nfrom PIL import Image\n\nfrom .registry import TRANSFORMS\n\nCV2_MODE = {\n    'bilinear': cv2.INTER_LINEAR,\n    'nearest': cv2.INTER_NEAREST,\n    'cubic': cv2.INTER_CUBIC,\n    'area': cv2.INTER_AREA,\n}\n\nCV2_BORDER_MODE = {\n    'constant': cv2.BORDER_CONSTANT,\n    'reflect': cv2.BORDER_REFLECT,\n    'reflect101': cv2.BORDER_REFLECT101,\n    'replicate': cv2.BORDER_REPLICATE,\n}\n\n\nclass Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, mask):\n        for t in self.transforms:\n            image, mask = t(image, mask)\n        return image, mask\n\n\n@TRANSFORMS.register_module\nclass FactorScale:\n    def __init__(self, scale_factor=1.0, mode='bilinear'):\n        self.mode = mode\n        self.scale_factor = scale_factor\n\n    def rescale(self, image, mask):\n        h, w, c = image.shape\n\n        if self.scale_factor == 1.0:\n            return image, mask\n\n        new_h = int(h * self.scale_factor)\n        new_w = int(w * self.scale_factor)\n\n        torch_image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)\n        torch_mask = torch.from_numpy(mask).unsqueeze(0).unsqueeze(0)\n        torch_image = F.interpolate(torch_image, size=(new_h, new_w),\n                                    mode=self.mode, align_corners=True)\n        torch_mask = F.interpolate(torch_mask, size=(new_h, new_w),\n                                   mode='nearest')\n\n        new_image = torch_image.squeeze().permute(1, 2, 0).numpy()\n        new_mask = torch_mask.squeeze().numpy()\n\n        return new_image, new_mask\n\n    def __call__(self, image, mask):\n        return self.rescale(image, mask)\n\n\n@TRANSFORMS.register_module\nclass SizeScale(FactorScale):\n    def __init__(self, target_size, mode='bilinear'):\n        self.target_size = target_size\n        super().__init__(mode=mode)\n\n    def __call__(self, image, mask):\n        h, w, _ = image.shape\n        long_edge = max(h, w)\n        self.scale_factor = self.target_size / long_edge\n\n        return self.rescale(image, mask)\n\n\n@TRANSFORMS.register_module\nclass RandomScale(FactorScale):\n    def __init__(self, min_scale, max_scale, scale_step=0.0, mode='bilinear'):\n        self.min_scale = min_scale\n        self.max_scale = max_scale\n        self.scale_step = scale_step\n        super().__init__(mode=mode)\n\n    @staticmethod\n    def get_scale_factor(min_scale, max_scale, scale_step):\n        if min_scale == max_scale:\n            return min_scale\n\n        if scale_step == 0:\n            return random.uniform(min_scale, max_scale)\n\n        num_steps = int((max_scale - min_scale) / scale_step + 1)\n        scale_factors = np.linspace(min_scale, max_scale, num_steps)\n        scale_factor = np.random.choice(scale_factors).item()\n\n        return scale_factor\n\n    def __call__(self, image, mask):\n        self.scale_factor = self.get_scale_factor(self.min_scale, self.max_scale, self.scale_step)\n        return self.rescale(image, mask)\n\n\n@TRANSFORMS.register_module\nclass RandomCrop:\n    def __init__(self, height, width, image_value, mask_value):\n        self.height = height\n        self.width = width\n        self.image_value = image_value\n        self.mask_value = mask_value\n        self.channel = len(image_value)\n\n    def __call__(self, image, mask):\n        h, w, c = image.shape\n        target_height = h + max(self.height - h, 0)\n        target_width = w + max(self.width - w, 0)\n\n        image_pad_value = np.reshape(np.array(self.image_value, dtype=image.dtype), [1, 1, self.channel])\n        mask_pad_value = np.reshape(np.array(self.mask_value, dtype=mask.dtype), [1, 1])\n\n        new_image = np.tile(image_pad_value, (target_height, target_width, 1))\n        new_mask = np.tile(mask_pad_value, (target_height, target_width))\n\n        new_image[:h, :w, :] = image\n        new_mask[:h, :w] = mask\n\n        assert np.count_nonzero(mask != self.mask_value) == np.count_nonzero(new_mask != self.mask_value)\n\n        y1 = int(random.uniform(0, target_height - self.height + 1))\n        y2 = y1 + self.height\n        x1 = int(random.uniform(0, target_width - self.width + 1))\n        x2 = x1 + self.width\n\n        new_image = new_image[y1:y2, x1:x2, :]\n        new_mask = new_mask[y1:y2, x1:x2]\n\n        return new_image, new_mask\n\n\n@TRANSFORMS.register_module\nclass PadIfNeeded:\n    def __init__(self, height, width, image_value, mask_value):\n        self.height = height\n        self.width = width\n        self.image_value = image_value\n        self.mask_value = mask_value\n        self.channel = len(image_value)\n\n    def __call__(self, image, mask):\n        h, w, c = image.shape\n\n        assert h <= self.height and w <= self.width\n\n        target_height = h + max(self.height - h, 0)\n        target_width = w + max(self.width - w, 0)\n\n        image_pad_value = np.reshape(np.array(self.image_value, dtype=image.dtype), [1, 1, self.channel])\n        mask_pad_value = np.reshape(np.array(self.mask_value, dtype=mask.dtype), [1, 1])\n\n        new_image = np.tile(image_pad_value, (target_height, target_width, 1))\n        new_mask = np.tile(mask_pad_value, (target_height, target_width))\n\n        new_image[:h, :w, :] = image\n        new_mask[:h, :w] = mask\n\n        assert np.count_nonzero(mask != self.mask_value) == np.count_nonzero(new_mask != self.mask_value)\n\n        return new_image, new_mask\n\n\n@TRANSFORMS.register_module\nclass HorizontalFlip:\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, image, mask):\n        if random.random() > self.p:\n            image = cv2.flip(image, 1)\n            mask = cv2.flip(mask, 1)\n\n        return image, mask\n\n\n@TRANSFORMS.register_module\nclass RandomRotate:\n    def __init__(self, p=0.5, degrees=30, mode='bilinear', border_mode='reflect101', image_value=None, mask_value=None):\n        self.p = p\n        self.degrees = (-degrees, degrees) if isinstance(degrees, (int, float)) else degrees\n        self.mode = CV2_MODE[mode]\n        self.border_mode = CV2_BORDER_MODE[border_mode]\n        self.image_value = image_value\n        self.mask_value = mask_value\n\n    def __call__(self, image, mask):\n        if random.random() < self.p:\n            h, w, c = image.shape\n\n            angle = random.uniform(*self.degrees)\n            matrix = cv2.getRotationMatrix2D((w / 2, h / 2), angle, 1.0)\n\n            image = cv2.warpAffine(image, M=matrix, dsize=(w, h), flags=self.mode, borderMode=self.border_mode,\n                                   borderValue=self.image_value)\n            mask = cv2.warpAffine(mask, M=matrix, dsize=(w, h), flags=cv2.INTER_NEAREST, borderMode=self.border_mode,\n                                  borderValue=self.mask_value)\n\n        return image, mask\n\n\n@TRANSFORMS.register_module\nclass GaussianBlur:\n    def __init__(self, p=0.5, ksize=7):\n        self.p = p\n        self.ksize = (ksize, ksize) if isinstance(ksize, int) else ksize\n\n    def __call__(self, image, mask):\n        if random.random() < self.p:\n            image = cv2.GaussianBlur(image, ksize=self.ksize, sigmaX=0)\n\n        return image, mask\n\n\n@TRANSFORMS.register_module\nclass Normalize:\n    def __init__(self, mean=(123.675, 116.280, 103.530), std=(58.395, 57.120, 57.375)):\n        self.mean = mean\n        self.std = std\n        self.channel = len(mean)\n\n    def __call__(self, image, mask):\n        mean = np.reshape(np.array(self.mean, dtype=image.dtype), [1, 1, self.channel])\n        std = np.reshape(np.array(self.std, dtype=image.dtype), [1, 1, self.channel])\n        denominator = np.reciprocal(std, dtype=image.dtype)\n\n        new_image = (image - mean) * denominator\n        new_mask = mask\n\n        return new_image, new_mask\n\n\n@TRANSFORMS.register_module\nclass ColorJitter(tt.ColorJitter):\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n        super().__init__(brightness=brightness,\n                         contrast=contrast,\n                         saturation=saturation,\n                         hue=hue)\n\n    def __call__(self, image=None, mask=None):\n        new_image = Image.fromarray(image.astype(np.uint8))\n        new_image = super().__call__(new_image)\n        new_image = np.array(new_image).astype(np.float32)\n        return new_image, mask\n\n\n@TRANSFORMS.register_module\nclass ToTensor:\n    def __call__(self, image, mask):\n        image = torch.from_numpy(image).permute(2, 0, 1)\n        mask = torch.from_numpy(mask)\n\n        return image, mask\n"""
vedaseg/models/decoders/__init__.py,0,"b'from .builder import build_decoder\nfrom .builder import build_brick\nfrom .gfpn import GFPN\nfrom .bricks import FusionBlock, JunctionBlock\n'"
vedaseg/models/decoders/bricks.py,5,"b'import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nimport math\nimport copy\n\nfrom ..utils import build_module, build_torch_nn, ConvModules, ConvModule\nfrom .registry import BRICKS\n\n\n@BRICKS.register_module\nclass JunctionBlock(nn.Module):\n    """"""JunctionBlock\n\n    Args:\n    """"""\n    def __init__(self, top_down, lateral, post, to_layer, fusion_method=None):\n        super().__init__()\n        self.from_layer = {}\n\n        self.to_layer = to_layer\n        top_down_ = copy.copy(top_down)\n        lateral_ = copy.copy(lateral)\n        self.fusion_method = fusion_method\n\n        self.top_down_block = []\n        if top_down_:\n            self.from_layer[\'top_down\'] = top_down_.pop(\'from_layer\')\n            if \'trans\' in top_down_:\n                self.top_down_block.append(build_module(top_down_[\'trans\']))\n            self.top_down_block.append(build_module(top_down_[\'upsample\']))\n        self.top_down_block = nn.Sequential(*self.top_down_block)\n\n        if lateral_:\n            self.from_layer[\'lateral\'] = lateral_.pop(\'from_layer\')\n            if lateral_:\n                self.lateral_block = build_module(lateral_)\n            else:\n                self.lateral_block = nn.Sequential()\n        else:\n            self.lateral_block = nn.Sequential()\n\n        if post:\n            self.post_block = build_module(post)\n        else:\n            self.post_block = nn.Sequential()\n\n    def forward(self, top_down=None, lateral=None):\n\n        if top_down is not None:\n            top_down = self.top_down_block(top_down)\n        if lateral is not None:\n            lateral = self.lateral_block(lateral)\n\n        if top_down is not None:\n            if lateral is not None:\n                assert self.fusion_method in (\'concat\', \'add\')\n                if self.fusion_method == \'concat\':\n                    feat = torch.cat([top_down, lateral], 1)\n                elif self.fusion_method == \'add\':\n                    feat = top_down + lateral\n            else:\n                assert self.fusion_method is None\n                feat = top_down\n        else:\n            assert self.fusion_method is None\n            if lateral is not None:\n                feat = lateral\n            else:\n                raise ValueError(\n                    \'There is neither top down feature nor lateral feature\')\n\n        feat = self.post_block(feat)\n        return feat\n\n\n@BRICKS.register_module\nclass FusionBlock(nn.Module):\n    """"""FusionBlock\n\n        Args:\n    """"""\n    def __init__(self,\n                 method,\n                 from_layers,\n                 feat_strides,\n                 in_channels_list,\n                 out_channels_list,\n                 upsample,\n                 conv_cfg=dict(type=\'Conv\'),\n                 norm_cfg=dict(type=\'BN\'),\n                 act_cfg=dict(type=\'Relu\', inplace=True),\n                 common_stride=4,\n                 ):\n        super().__init__()\n        assert method in (\'add\', \'concat\')\n        self.method = method\n        self.from_layers = from_layers\n\n        assert len(in_channels_list) == len(out_channels_list)\n\n        self.blocks = nn.ModuleList()\n        for idx in range(len(from_layers)):\n            in_channels = in_channels_list[idx]\n            out_channels = out_channels_list[idx]\n            from_layer = from_layers[idx]\n            feat_stride = feat_strides[idx]\n            ups_num = int(max(1, math.log2(feat_stride) - math.log2(common_stride)))\n            head_ops = []\n            for idx2 in range(ups_num):\n                cur_in_channels = in_channels if idx2 == 0 else out_channels\n                conv = ConvModule(\n                    cur_in_channels,\n                    out_channels,\n                    kernel_size=3,\n                    padding=1,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                    act_cfg=act_cfg,\n                )\n                head_ops.append(conv)\n                if int(feat_stride) != int(common_stride):\n                    head_ops.append(build_module(upsample))\n            self.blocks.append(nn.Sequential(*head_ops))\n\n    def forward(self, feats):\n        outs = []\n        for idx, key in enumerate(self.from_layers):\n            block = self.blocks[idx]\n            feat = feats[key]\n            out = block(feat)\n            outs.append(out)\n        if self.method == \'add\':\n            res = torch.stack(outs, 0).sum(0)\n        else:\n            res = torch.cat(outs, 1)\n        return res\n\n\n@BRICKS.register_module\nclass CollectBlock(nn.Module):\n    """"""FusionBlock\n\n        Args:\n    """"""\n    def __init__(self, from_layer, to_layer=None):\n        super().__init__()\n        self.from_layer = from_layer\n        self.to_layer = to_layer\n\n    def forward(self, feats):\n        if self.to_layer is None:\n            return feats[self.from_layer]\n        else:\n            res[self.to_layer] = feats[self.from_layer]\n'"
vedaseg/models/decoders/builder.py,1,"b'import torch.nn as nn\n\nfrom vedaseg.utils import build_from_cfg\n\nfrom .registry import BRICKS\nfrom .registry import DECODERS\n\n\ndef build_brick(cfg, default_args=None):\n    brick = build_from_cfg(cfg, BRICKS, default_args)\n    return brick\n\n\ndef build_bricks(cfgs):\n    bricks = nn.ModuleList()\n    for brick_cfg in cfgs:\n        bricks.append(build_brick(brick_cfg))\n    return bricks\n\n\ndef build_decoder(cfg, default_args=None):\n    decoder = build_from_cfg(cfg, DECODERS, default_args)\n    return decoder\n'"
vedaseg/models/decoders/registry.py,0,"b""from vedaseg.utils import Registry\n\nBRICKS = Registry('brick')\nDECODERS = Registry('decoder')\n"""
vedaseg/models/encoders/__init__.py,0,b'from .builder import build_encoder\n'
vedaseg/models/encoders/builder.py,1,"b""import torch.nn as nn\n\nfrom vedaseg.utils import build_from_cfg\n\nfrom .backbones.registry import BACKBONES\nfrom .enhance_modules.registry import ENHANCE_MODULES\n\n\ndef build_encoder(cfg, default_args=None):\n    backbone = build_from_cfg(cfg['backbone'], BACKBONES, default_args)\n\n    enhance_cfg = cfg.get('enhance')\n    if enhance_cfg:\n        enhance_module = build_from_cfg(enhance_cfg, ENHANCE_MODULES, default_args)\n        encoder = nn.Sequential(backbone, enhance_module)\n    else:\n        encoder = backbone\n\n    return encoder\n"""
vedaseg/models/heads/__init__.py,0,b'from .builder import build_head\nfrom .head import Head\nfrom .registry import HEADS\n'
vedaseg/models/heads/builder.py,0,"b'from vedaseg.utils import build_from_cfg\n\nfrom .registry import HEADS\n\n\ndef build_head(cfg, default_args=None):\n    #import pdb\n    #pdb.set_trace()\n    head = build_from_cfg(cfg, HEADS, default_args)\n    return head\n'"
vedaseg/models/heads/head.py,2,"b'import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport logging\n\nfrom vedaseg.utils import build_from_cfg\nfrom ..utils import build_module, ConvModules\nfrom .registry import HEADS\nfrom ..weight_init import init_weights\n\nlogger = logging.getLogger()\n\n\n@HEADS.register_module\nclass Head(nn.Module):\n    """"""Head\n\n    Args:\n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 inter_channels=None,\n                 conv_cfg=dict(type=\'Conv\'),\n                 norm_cfg=dict(type=\'BN\'),\n                 act_cfg=dict(type=\'Relu\', inplace=True),\n                 num_convs=0,\n                 upsample=None,\n                 dropouts=None):\n        super().__init__()\n\n        if num_convs > 0:\n            layers = [\n                ConvModules(in_channels,\n                            inter_channels,\n                            3,\n                            padding=1,\n                            conv_cfg=conv_cfg,\n                            norm_cfg=norm_cfg,\n                            act_cfg=act_cfg,\n                            num_convs=num_convs,\n                            dropouts=dropouts),\n                nn.Conv2d(inter_channels, out_channels, 1)\n            ]\n        else:\n            layers = [nn.Conv2d(in_channels, out_channels, 1)]\n        if upsample:\n            upsample_layer = build_module(upsample)\n            layers.append(upsample_layer)\n\n        self.block = nn.Sequential(*layers)\n        logger.info(\'Head init weights\')\n        init_weights(self.modules())\n\n    def forward(self, x):\n        feat = self.block(x)\n        return feat\n'"
vedaseg/models/heads/registry.py,0,"b""from vedaseg.utils import Registry\n\nHEADS = Registry('head')\n"""
vedaseg/models/utils/__init__.py,0,"b'from .builder import build_module, build_torch_nn\nfrom .conv_module import ConvModule, ConvModules\nfrom .upsample import Upsample\n#from .registry import UTILS\n'"
vedaseg/models/utils/act.py,4,"b'# modify from mmcv and mmdetection\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parameter import Parameter\n\n\nclass TLU(nn.Module):\n    def __init__(self, num_features):\n        super(TLU, self).__init__()\n\n        self.num_features = num_features\n        self.tau = Parameter(torch.Tensor(1,num_features,1,1), requires_grad=True)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.zeros_(self.tau)\n\n    def forward(self, x):\n        return torch.max(x, self.tau)\n\n    def extra_repr(self):\n        return \'{num_features}\'.format(**self.__dict__)\n\n\nact_cfg = {\n    \'Relu\': (\'relu\', nn.ReLU),\n    \'Tlu\': (\'tlu\', TLU),\n}\n\n\ndef build_act_layer(cfg, num_features, postfix=\'\', layer_only=False):\n    """""" Build activate layer\n\n    Args:\n        cfg (dict): cfg should contain:\n            type (str): identify activate layer type.\n            layer args: args needed to instantiate a activate layer.\n            requires_grad (bool): [optional] whether stop gradient updates\n        num_features (int): number of channels from input.\n        postfix (int, str): appended into act abbreviation to\n            create named layer.\n\n    Returns:\n        name (str): abbreviation + postfix\n        layer (nn.Module): created act layer\n    """"""\n    assert isinstance(cfg, dict) and \'type\' in cfg\n    cfg_ = cfg.copy()\n\n    layer_type = cfg_.pop(\'type\')\n    if layer_type not in act_cfg:\n        raise KeyError(\'Unrecognized activate type {}\'.format(layer_type))\n    else:\n        abbr, act_layer = act_cfg[layer_type]\n        if act_layer is None:\n            raise NotImplementedError\n\n    assert isinstance(postfix, (int, str))\n    name = abbr + str(postfix)\n\n    requires_grad = cfg_.pop(\'requires_grad\', True)\n    if layer_type != \'Tlu\':\n        layer = act_layer(**cfg_)\n    else:\n        layer = act_layer(num_features, **cfg_)\n\n    for param in layer.parameters():\n        param.requires_grad = requires_grad\n\n    if layer_only:\n        return layer\n    else:\n        return name, layer\n'"
vedaseg/models/utils/builder.py,1,"b""import torch.nn as nn\nfrom vedaseg.utils import build_from_cfg\n\nfrom .registry import UTILS\n\n\ndef build_module(cfg, default_args=None):\n    util = build_from_cfg(cfg, UTILS, default_args)\n    return util\n\n\ndef build_torch_nn(cfg, default_args=None):\n    module = build_from_cfg(cfg, nn, default_args, 'module')\n    return module\n"""
vedaseg/models/utils/conv_module.py,1,"b'# modify from mmcv and mmdetection\n\nimport warnings\n\nimport torch.nn as nn\n\nfrom .norm import build_norm_layer\nfrom .act import build_act_layer\nfrom .registry import UTILS\n\nconv_cfg = {\n    \'Conv\': nn.Conv2d,\n    # TODO: octave conv\n}\n\n\ndef build_conv_layer(cfg, *args, **kwargs):\n    """""" Build convolution layer\n\n    Args:\n        cfg (None or dict): cfg should contain:\n            type (str): identify conv layer type.\n            layer args: args needed to instantiate a conv layer.\n\n    Returns:\n        layer (nn.Module): created conv layer\n    """"""\n    assert isinstance(cfg, dict) and \'type\' in cfg\n    cfg_ = cfg.copy()\n\n    layer_type = cfg_.pop(\'type\')\n    if layer_type not in conv_cfg:\n        raise KeyError(\'Unrecognized norm type {}\'.format(layer_type))\n    else:\n        conv_layer = conv_cfg[layer_type]\n\n    layer = conv_layer(*args, **kwargs, **cfg_)\n\n    return layer\n\n\n@UTILS.register_module\nclass ConvModule(nn.Module):\n    """"""A conv block that contains conv/norm/activation layers.\n\n    Args:\n        in_channels (int): Same as nn.Conv2d.\n        out_channels (int): Same as nn.Conv2d.\n        kernel_size (int or tuple[int]): Same as nn.Conv2d.\n        stride (int or tuple[int]): Same as nn.Conv2d.\n        padding (int or tuple[int]): Same as nn.Conv2d.\n        dilation (int or tuple[int]): Same as nn.Conv2d.\n        groups (int): Same as nn.Conv2d.\n        bias (bool or str): If specified as `auto`, it will be decided by the\n            norm_cfg. Bias will be set as True if norm_cfg is None, otherwise\n            False.\n        conv_cfg (dict): Config dict for convolution layer.\n        norm_cfg (dict): Config dict for normalization layer.\n        act_cfg (str or None): Config dict for activation layer.\n        order (tuple[str]): The order of conv/norm/activation layers. It is a\n            sequence of ""conv"", ""norm"" and ""act"". Examples are\n            (""conv"", ""norm"", ""act"") and (""act"", ""conv"", ""norm"").\n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 bias=\'auto\',\n                 conv_cfg=dict(type=\'Conv\'),\n                 norm_cfg=None,\n                 act_cfg=dict(type=\'Relu\', inplace=True),\n                 order=(\'conv\', \'norm\', \'act\'),\n                 dropout=None):\n        super(ConvModule, self).__init__()\n        assert isinstance(conv_cfg, dict)\n        assert norm_cfg is None or isinstance(norm_cfg, dict)\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.act_cfg = act_cfg\n        self.order = order\n        assert isinstance(self.order, tuple) and len(self.order) == 3\n        assert set(order) == set([\'conv\', \'norm\', \'act\'])\n\n        self.with_norm = norm_cfg is not None\n        self.with_act = act_cfg is not None\n        self.with_dropout = dropout is not None\n        # if the conv layer is before a norm layer, bias is unnecessary.\n        if bias == \'auto\':\n            bias = False if self.with_norm else True\n        self.with_bias = bias\n\n        if self.with_norm and self.with_bias:\n            warnings.warn(\'ConvModule has norm and bias at the same time\')\n\n        # build convolution layer\n        self.conv = build_conv_layer(conv_cfg,\n                                     in_channels,\n                                     out_channels,\n                                     kernel_size,\n                                     stride=stride,\n                                     padding=padding,\n                                     dilation=dilation,\n                                     groups=groups,\n                                     bias=bias)\n        # export the attributes of self.conv to a higher level for convenience\n        self.in_channels = self.conv.in_channels\n        self.out_channels = self.conv.out_channels\n        self.kernel_size = self.conv.kernel_size\n        self.stride = self.conv.stride\n        self.padding = self.conv.padding\n        self.dilation = self.conv.dilation\n        self.transposed = self.conv.transposed\n        self.output_padding = self.conv.output_padding\n        self.groups = self.conv.groups\n\n        # build normalization layers\n        if self.with_norm:\n            # norm layer is after conv layer\n            if order.index(\'norm\') > order.index(\'conv\'):\n                norm_channels = out_channels\n            else:\n                norm_channels = in_channels\n            self.norm_name, norm = build_norm_layer(norm_cfg, norm_channels)\n            self.add_module(self.norm_name, norm)\n\n        # build activation layer\n        if self.with_act:\n            # activate layer is after conv layer\n            if order.index(\'act\') > order.index(\'conv\'):\n                act_channels = out_channels\n            else:\n                act_channels = in_channels\n            self.act_name, act = build_act_layer(act_cfg, act_channels)\n            self.add_module(self.act_name, act)\n\n        if self.with_dropout:\n            self.dropout = nn.Dropout2d(p=dropout)\n\n    @property\n    def norm(self):\n        return getattr(self, self.norm_name)\n\n    @property\n    def activate(self):\n        return getattr(self, self.act_name)\n\n    def forward(self, x, activate=True, norm=True):\n        for layer in self.order:\n            if layer == \'conv\':\n                x = self.conv(x)\n            elif layer == \'norm\' and norm and self.with_norm:\n                x = self.norm(x)\n            elif layer == \'act\' and activate and self.with_act:\n                x = self.activate(x)\n        if self.with_dropout:\n            x = self.dropout(x)\n        return x\n\n\n@UTILS.register_module\nclass ConvModules(nn.Module):\n    """"""Head\n\n    Args:\n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 bias=\'auto\',\n                 conv_cfg=dict(type=\'Conv\'),\n                 norm_cfg=None,\n                 act_cfg=dict(type=\'Relu\', inplace=True),\n                 order=(\'conv\', \'norm\', \'act\'),\n                 dropouts=None,\n                 num_convs=1):\n        super().__init__()\n\n        if dropouts is not None:\n            assert num_convs == len(dropouts)\n            dropout = dropouts[0]\n        else:\n            dropout = None\n\n        layers = [\n            ConvModule(in_channels, out_channels, kernel_size, stride, padding,\n                       dilation, groups, bias, conv_cfg, norm_cfg, act_cfg,\n                       order, dropout),\n        ]\n        for ii in range(1, num_convs):\n            if dropouts is not None:\n                dropout = dropouts[ii]\n            else:\n                dropout = None\n            layers.append(\n                ConvModule(out_channels, out_channels, kernel_size, stride,\n                           padding, dilation, groups, bias, conv_cfg, norm_cfg, act_cfg,\n                           order, dropout))\n\n        self.block = nn.Sequential(*layers)\n\n    def forward(self, x):\n        feat = self.block(x)\n        return feat\n'"
vedaseg/models/utils/norm.py,7,"b'# modify from mmcv and mmdetection\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parameter import Parameter\n\n\nclass FRN(nn.Module):\n    def __init__(self, num_features, eps=1e-6):\n        super(FRN, self).__init__()\n\n        self.num_features = num_features\n        self.gamma = Parameter(torch.Tensor(1,num_features,1,1), requires_grad=True)\n        self.beta = Parameter(torch.Tensor(1,num_features,1,1), requires_grad=True)\n\n        self.register_buffer(\'eps\', torch.Tensor([eps]))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.ones_(self.gamma)\n        nn.init.zeros_(self.beta)\n\n    def forward(self, x):\n        nu2 = torch.mean(x.pow(2), dim=[2,3], keepdim=True)\n        x = x * torch.rsqrt(nu2 + self.eps.abs())\n        x = self.gamma * x + self.beta\n\n        return x\n\n    def extra_repr(self):\n        return \'{num_features}, eps={eps}\'.format(**self.__dict__)\n\n\nnorm_cfg = {\n    \'FRN\': (\'frn\', FRN),\n    # format: layer_type: (abbreviation, module)\n    \'BN\': (\'bn\', nn.BatchNorm2d),\n    \'SyncBN\': (\'bn\', nn.SyncBatchNorm),\n    \'GN\': (\'gn\', nn.GroupNorm),\n    # and potentially \'SN\'\n}\n\n\ndef build_norm_layer(cfg, num_features, postfix=\'\', layer_only=False):\n    """""" Build normalization layer\n\n    Args:\n        cfg (dict): cfg should contain:\n            type (str): identify norm layer type.\n            layer args: args needed to instantiate a norm layer.\n            requires_grad (bool): [optional] whether stop gradient updates\n        num_features (int): number of channels from input.\n        postfix (int, str): appended into norm abbreviation to\n            create named layer.\n\n    Returns:\n        name (str): abbreviation + postfix\n        layer (nn.Module): created norm layer\n    """"""\n    assert isinstance(cfg, dict) and \'type\' in cfg\n    cfg_ = cfg.copy()\n\n    layer_type = cfg_.pop(\'type\')\n    if layer_type not in norm_cfg:\n        raise KeyError(\'Unrecognized norm type {}\'.format(layer_type))\n    else:\n        abbr, norm_layer = norm_cfg[layer_type]\n        if norm_layer is None:\n            raise NotImplementedError\n\n    assert isinstance(postfix, (int, str))\n    name = abbr + str(postfix)\n\n    requires_grad = cfg_.pop(\'requires_grad\', True)\n    if layer_type != \'GN\':\n        layer = norm_layer(num_features, **cfg_)\n        if layer_type == \'SyncBN\':\n            layer._specify_ddp_gpu_num(1)\n    else:\n        assert \'num_groups\' in cfg_\n        layer = norm_layer(num_channels=num_features, **cfg_)\n\n    for param in layer.parameters():\n        param.requires_grad = requires_grad\n\n    if layer_only:\n        return layer\n    return name, layer\n'"
vedaseg/models/utils/registry.py,0,"b""from vedaseg.utils import Registry\n\nUTILS = Registry('utils')\n"""
vedaseg/models/utils/upsample.py,2,"b""import torch.nn as nn\nimport torch.nn.functional as F\n\n\nfrom .registry import UTILS\n\n\n@UTILS.register_module\nclass Upsample(nn.Module):\n    __constants__ = ['size', 'scale_factor', 'scale_bias', 'mode', 'align_corners', 'name']\n\n    def __init__(self, size=None, scale_factor=None, scale_bias=0, mode='nearest', align_corners=None):\n        super(Upsample, self).__init__()\n        self.size = size\n        self.scale_factor = scale_factor\n        self.scale_bias = scale_bias\n        self.mode = mode\n        self.align_corners = align_corners\n\n        assert (self.size is None) ^ (self.scale_factor is None)\n\n    def forward(self, x):\n        if self.size:\n            size = self.size\n        else:\n            n, c, h, w = x.size()\n            new_h = int(h * self.scale_factor + self.scale_bias)\n            new_w = int(w * self.scale_factor + self.scale_bias)\n\n            size = (new_h, new_w)\n\n        return F.interpolate(x, size=size, mode=self.mode, align_corners=self.align_corners)\n\n    def extra_repr(self):\n        if self.size is not None:\n            info = 'size=' + str(self.size)\n        else:\n            info = 'scale_factor=' + str(self.scale_factor)\n            info += ', scale_bias=' + str(self.scale_bias)\n        info += ', mode=' + self.mode\n        return info\n"""
vedaseg/models/decoders/gfpn/__init__.py,0,b'from .gfpn import GFPN\n'
vedaseg/models/decoders/gfpn/gfpn.py,1,"b'import torch.nn as nn\nimport logging\n\nfrom ...weight_init import init_weights\nfrom ..builder import build_brick, build_bricks\nfrom ..registry import DECODERS\n\nlogger = logging.getLogger()\n\n\n@DECODERS.register_module\nclass GFPN(nn.Module):\n    """"""GFPN\n\n    Args:\n    """"""\n    def __init__(self, neck, fusion=None):\n        super().__init__()\n        self.neck = build_bricks(neck)\n        if fusion:\n            self.fusion = build_brick(fusion)\n        else:\n            self.fusion = None\n        logger.info(\'GFPN init weights\')\n        init_weights(self.modules())\n\n    def forward(self, bottom_up):\n        x = None\n        feats = {}\n        for ii, layer in enumerate(self.neck):\n            top_down_from_layer = layer.from_layer.get(\'top_down\')\n            lateral_from_layer = layer.from_layer.get(\'lateral\')\n\n            if lateral_from_layer:\n                ll = bottom_up[lateral_from_layer]\n            else:\n                ll = None\n            if top_down_from_layer is None:\n                td = None\n            elif \'c\' in top_down_from_layer:\n                td = bottom_up[top_down_from_layer]\n            elif \'p\' in top_down_from_layer:\n                td = feats[top_down_from_layer]\n            else:\n                raise ValueError(\'Key error\')\n\n            x = layer(td, ll)\n            feats[layer.to_layer] = x\n        if self.fusion:\n            x = self.fusion(feats)\n        return x\n'"
vedaseg/models/encoders/backbones/__init__.py,0,b'from .builder import build_backbone\nfrom .resnet import ResNet\n#from .registry import ENCODERS\n'
vedaseg/models/encoders/backbones/builder.py,0,"b'from vedaseg.utils import build_from_cfg\n\nfrom .registry import BACKBONES\n\n\ndef build_backbone(cfg, default_args=None):\n    #import pdb\n    #pdb.set_trace()\n    backbone = build_from_cfg(cfg, BACKBONES, default_args)\n    return backbone\n'"
vedaseg/models/encoders/backbones/registry.py,0,"b""from vedaseg.utils import Registry\n\nBACKBONES = Registry('backbone')\n"""
vedaseg/models/encoders/backbones/resnet.py,3,"b'import torch.nn as nn\nimport logging\nfrom torchvision.models.resnet import model_urls\n\ntry:\n    from torch.hub import load_state_dict_from_url\nexcept ImportError:\n    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n\nfrom functools import partial\n\nfrom .registry import BACKBONES\nfrom ...weight_init import init_weights\nfrom ...utils.norm import build_norm_layer\nfrom ...utils.act import build_act_layer\n\nlogger = logging.getLogger()\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, norm_layer, act_layer, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1):\n        super(BasicBlock, self).__init__()\n        if groups != 1 or base_width != 64:\n            raise ValueError(\'BasicBlock only supports groups=1 and base_width=64\')\n        if dilation > 1:\n            raise NotImplementedError(""Dilation > 1 not supported in BasicBlock"")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu1 = act_layer(planes)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.relu2 = act_layer(planes)\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu1(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu2(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, norm_layer, act_layer, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1,):\n        super(Bottleneck, self).__init__()\n        width = int(planes * (base_width / 64.)) * groups\n\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.relu1 = act_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.relu2 = act_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu3 = act_layer(planes * self.expansion)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu1(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu2(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu3(out)\n\n        return out\n\n\nMODEL_CFGS = {\n    \'resnet101\': {\n        \'block\': Bottleneck,\n        \'layer\': [3, 4, 23, 3],\n        \'weights_url\': model_urls[\'resnet101\'],\n    },\n    \'resnet50\': {\n        \'block\': Bottleneck,\n        \'layer\': [3, 4, 6, 3],\n        \'weights_url\': model_urls[\'resnet50\'],\n    },\n    \'resnet18\': {\n        \'block\': BasicBlock,\n        \'layer\': [2, 2, 2, 2],\n        \'weights_url\': model_urls[\'resnet18\'],\n    }\n}\n\n\nclass ResNetCls(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None, multi_grid=None,\n                 norm_cfg=None, act_cfg=None):\n        super(ResNetCls, self).__init__()\n\n        if norm_cfg is None:\n            norm_cfg = dict(type=\'BN\')\n        self._norm_layer = partial(build_norm_layer, norm_cfg, layer_only=True)\n\n        if act_cfg is None:\n            act_cfg = dict(type=\'Relu\', inplace=True)\n        self._act_layer = partial(build_act_layer, act_cfg, layer_only=True)\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(""replace_stride_with_dilation should be None ""\n                             ""or a 3-element tuple, got {}"".format(replace_stride_with_dilation))\n\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = self._norm_layer(self.inplanes)\n        self.relu1 = self._act_layer(self.inplanes)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                       dilate=replace_stride_with_dilation[2], multi_grid=multi_grid)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False, multi_grid=None):\n        norm_layer = self._norm_layer\n        act_layer = self._act_layer\n        downsample = None\n        previous_dilation = self.dilation\n\n        if multi_grid is None:\n            multi_grid = [1 for _ in range(blocks)]\n        else:\n            assert len(multi_grid) == blocks\n\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, norm_layer, act_layer,\n                            stride, downsample, self.groups,\n                            self.base_width, previous_dilation*multi_grid[0]))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, norm_layer=norm_layer, act_layer=act_layer,\n                                groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation*multi_grid[i],))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu1(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.reshape(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\n@BACKBONES.register_module\nclass ResNet(ResNetCls):\n    """"""ResNetEncoder\n\n    Args:\n        pretrain(bool)\n    """"""\n    def __init__(self, arch, replace_stride_with_dilation=None, multi_grid=None, pretrain=True,\n                 norm_cfg=None, act_cfg=None):\n        cfg = MODEL_CFGS[arch]\n        super().__init__(\n            cfg[\'block\'],\n            cfg[\'layer\'],\n            replace_stride_with_dilation=replace_stride_with_dilation,\n            multi_grid=multi_grid,\n            norm_cfg=norm_cfg,\n            act_cfg=act_cfg)\n\n        if pretrain:\n            logger.info(\'ResNet init weights from pretreain\')\n            state_dict = load_state_dict_from_url(cfg[\'weights_url\'])\n            self.load_state_dict(state_dict, strict=False)\n        else:\n            logger.info(\'ResNet init weights\')\n            init_weights(self.modules())\n\n        del self.fc, self.avgpool\n\n    def forward(self, x):\n        feats = {}\n\n        x0 = self.conv1(x)\n        x0 = self.bn1(x0)\n        x0 = self.relu1(x0)  # 2\n        feats[\'c1\'] = x0\n\n        x1 = self.maxpool(x0)\n        x1 = self.layer1(x1)  # 4\n        feats[\'c2\'] = x1\n\n        x2 = self.layer2(x1)  # 8\n        feats[\'c3\'] = x2\n        x3 = self.layer3(x2)  # 16\n        feats[\'c4\'] = x3\n        x4 = self.layer4(x3)  # 32\n        feats[\'c5\'] = x4\n\n        #for k, v in feats.items():\n        #    print(k, v.shape)\n\n        return feats\n'"
vedaseg/models/encoders/enhance_modules/__init__.py,0,b'from .builder import build_enhance_module\nfrom .aspp import ASPP\nfrom .ppm import PPM\n#from .registry import MULTISCALES\n'
vedaseg/models/encoders/enhance_modules/aspp.py,3,"b""# modify from https://github.com/pytorch/vision/tree/master/torchvision/models/segmentation/deeplabv3.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport logging\n\nfrom .registry import ENHANCE_MODULES\nfrom ...weight_init import init_weights\nfrom ...utils.norm import build_norm_layer\nfrom ...utils.act import build_act_layer\n\nfrom functools import partial\n\nlogger = logging.getLogger()\n\n\nclass ASPPConv(nn.Sequential):\n    def __init__(self, in_channels, out_channels, dilation, norm_layer, act_layer):\n        modules = [\n            nn.Conv2d(in_channels,\n                      out_channels,\n                      3,\n                      padding=dilation,\n                      dilation=dilation,\n                      bias=False),\n            norm_layer(out_channels),\n            act_layer(out_channels)\n        ]\n        super(ASPPConv, self).__init__(*modules)\n\n\nclass ASPPPooling(nn.Sequential):\n    def __init__(self, in_channels, out_channels, norm_layer, act_layer):\n        super(ASPPPooling, self).__init__(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels), act_layer(out_channels))\n\n    def forward(self, x):\n        size = x.shape[-2:]\n        x = super(ASPPPooling, self).forward(x)\n        return F.interpolate(x, size=size, mode='bilinear', align_corners=True)\n\n\n@ENHANCE_MODULES.register_module\nclass ASPP(nn.Module):\n    def __init__(self, in_channels, out_channels, atrous_rates, from_layer,\n                 to_layer, dropout=None, norm_cfg=None, act_cfg=None):\n        super(ASPP, self).__init__()\n        self.from_layer = from_layer\n        self.to_layer = to_layer\n\n        if norm_cfg is None:\n            norm_cfg = dict(type='BN')\n        norm_layer = partial(build_norm_layer, norm_cfg, layer_only=True)\n\n        if act_cfg is None:\n            act_cfg = dict(type='Relu', inplace=True)\n        act_layer = partial(build_act_layer, act_cfg, layer_only=True)\n\n        modules = []\n        modules.append(\n            nn.Sequential(nn.Conv2d(in_channels, out_channels, 1, bias=False),\n                          norm_layer(out_channels), act_layer(out_channels)))\n\n        rate1, rate2, rate3 = tuple(atrous_rates)\n        modules.append(ASPPConv(in_channels, out_channels, rate1, norm_layer, act_layer))\n        modules.append(ASPPConv(in_channels, out_channels, rate2, norm_layer, act_layer))\n        modules.append(ASPPConv(in_channels, out_channels, rate3, norm_layer, act_layer))\n        modules.append(ASPPPooling(in_channels, out_channels, norm_layer, act_layer))\n\n        self.convs = nn.ModuleList(modules)\n\n        self.project = nn.Sequential(\n            nn.Conv2d(5 * out_channels, out_channels, 1, bias=False),\n            norm_layer(out_channels), act_layer(out_channels))\n        self.with_dropout = dropout is not None\n        if self.with_dropout:\n            self.dropout = nn.Dropout(dropout)\n\n        logger.info('ASPP init weights')\n        init_weights(self.modules())\n\n    def forward(self, feats):\n        feats_ = feats.copy()\n        x = feats_[self.from_layer]\n        res = []\n        for conv in self.convs:\n            res.append(conv(x))\n        res = torch.cat(res, dim=1)\n        res = self.project(res)\n        if self.with_dropout:\n            res = self.dropout(res)\n        feats_[self.to_layer] = res\n        return feats_\n"""
vedaseg/models/encoders/enhance_modules/builder.py,0,"b'from vedaseg.utils import build_from_cfg\n\nfrom .registry import ENHANCE_MODULES\n\n\ndef build_enhance_module(cfg, default_args=None):\n    #import pdb\n    #pdb.set_trace()\n    enhance_module = build_from_cfg(cfg, ENHANCE_MODULES, default_args)\n    return enhance_module\n'"
vedaseg/models/encoders/enhance_modules/ppm.py,3,"b""# modify from https://github.com/hszhao/semseg/blob/master/model/pspnet.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport logging\n\nfrom ...weight_init import init_weights\nfrom .registry import ENHANCE_MODULES\nfrom ...utils.norm import build_norm_layer\nfrom ...utils.act import build_act_layer\n\nlogger = logging.getLogger()\n\n\n@ENHANCE_MODULES.register_module\nclass PPM(nn.Module):\n    def __init__(self, in_channels, out_channels, bins, from_layer, to_layer, norm_cfg=None, act_cfg=None):\n        super(PPM, self).__init__()\n        self.from_layer = from_layer\n        self.to_layer = to_layer\n\n        if norm_cfg is None:\n            norm_cfg = dict(type='BN')\n\n        if act_cfg is None:\n            act_cfg = dict(type='Relu', inplace=True)\n\n        self.blocks = nn.ModuleList()\n        for bin_ in bins:\n            self.blocks.append(\n                nn.Sequential(\n                    nn.AdaptiveAvgPool2d(bin_),\n                    nn.Conv2d(in_channels, out_channels, 1, bias=False),\n                    build_norm_layer(norm_cfg, out_channels, layer_only=True),\n                    build_act_layer(act_cfg, out_channels, layer_only=True)\n                )\n            )\n        logger.info('PPM init weights')\n        init_weights(self.modules())\n\n    def forward(self, feats):\n        feats_ = feats.copy()\n        x = feats_[self.from_layer]\n        h, w = x.shape[2:]\n        out = [x]\n        for block in self.blocks:\n            feat = F.interpolate(\n                block(x),\n                (h, w),\n                mode='bilinear',\n                align_corners=True\n            )\n            out.append(feat)\n        out = torch.cat(out, 1)\n        feats_[self.to_layer] = out\n        return feats_\n"""
vedaseg/models/encoders/enhance_modules/registry.py,0,"b""from vedaseg.utils import Registry\n\nENHANCE_MODULES = Registry('enhance_module')\n"""
