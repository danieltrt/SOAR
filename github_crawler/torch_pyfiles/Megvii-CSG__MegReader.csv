file_path,api_count,code
config.py,0,"b'# DO NOT CHANGE THIS. Using libraries from community or not\ncommunity_version = True\n\ndb_path = \'/data/workspace\'  # The path for log storage.\noss_host = ""http://oss.wh-a.brainpp.cn""  # The oss base url(if used).\n# Whether to use nori(packed in vendor/nori2-1.9.18-py3-none-any.whl) for data packing.\nwill_use_nori = False\nwill_use_lmdb = True\n\n# Only needed when RedisMetaCache is applied for caching meta.\nredis_host = \'10.251.160.97\'\nredis_port = 6379\n\nsync_bn = False\n'"
eval.py,9,"b'#!python3\nimport argparse\nimport os\n\nimport torch\nimport yaml\nfrom tqdm import tqdm\n\nfrom trainer import Trainer\nimport ipdb\n# tagged yaml objects\nfrom experiment import Structure, TrainSettings, ValidationSettings, Experiment\nfrom concern.log import Logger\nfrom data.data_loader import DataLoader\nfrom data.mnist import MNistDataset\nfrom data.nori_dataset import NoriDataset\nfrom training.checkpoint import Checkpoint\nfrom training.learning_rate import (\n    ConstantLearningRate, PriorityLearningRate, FileMonitorLearningRate\n)\nfrom training.model_saver import ModelSaver\nfrom training.optimizer_scheduler import OptimizerScheduler\nfrom concern.config import Configurable, Config\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Text Recognition Training\')\n    parser.add_argument(\'exp\', type=str)\n    parser.add_argument(\'--name\', type=str)\n    parser.add_argument(\'--batch_size\', type=int,\n                        help=\'Batch size for training\')\n    parser.add_argument(\'--resume\', type=str, help=\'Resume from checkpoint\')\n    parser.add_argument(\'--epochs\', type=int, help=\'Number of training epochs\')\n    parser.add_argument(\'--start_iter\', type=int,\n                        help=\'Begin counting iterations starting from this value (should be used with resume)\')\n    parser.add_argument(\'--start_epoch\', type=int,\n                        help=\'Begin counting epoch starting from this value (should be used with resume)\')\n    parser.add_argument(\'--max_size\', type=int, help=\'max length of label\')\n    parser.add_argument(\'--data\', type=str,\n                        help=\'The name of dataloader which will be evaluated on.\')\n    parser.add_argument(\'--thresh\', type=float,\n                        help=\'The threshold to replace it in the representers\')\n    parser.add_argument(\'--box_thresh\', type=float,\n                        help=\'The threshold to replace it in the representers\')\n    parser.add_argument(\'--verbose\', action=\'store_true\',\n                        help=\'show verbose info\')\n    parser.add_argument(\'--no-verbose\', action=\'store_true\',\n                        help=\'show verbose info\')\n    parser.add_argument(\'--visualize\', action=\'store_true\',\n                        help=\'visualize maps in tensorboard\')\n    parser.add_argument(\'--eager\', \'--eager_show\', action=\'store_true\', dest=\'eager_show\',\n                        help=\'Show iamges eagerly\')\n    parser.add_argument(\'--speed\', action=\'store_true\', dest=\'test_speed\',\n                        help=\'Test speed only\')\n    parser.add_argument(\'--dest\', type=str,\n                        help=\'Specify which prediction will be used for decoding.\')\n    parser.add_argument(\'--debug\', action=\'store_true\', dest=\'debug\',\n                        help=\'Run with debug mode, which hacks dataset num_samples to toy number\')\n    parser.add_argument(\'--no-debug\', action=\'store_false\',\n                        dest=\'debug\', help=\'Run without debug mode\')\n    parser.add_argument(\'-d\', \'--distributed\', action=\'store_true\',\n                        dest=\'distributed\', help=\'Use distributed training\')\n    parser.add_argument(\'--local_rank\', dest=\'local_rank\', default=0,\n                        type=int, help=\'Use distributed training\')\n    parser.add_argument(\'-g\', \'--num_gpus\', dest=\'num_gpus\', default=4,\n                        type=int, help=\'The number of accessible gpus\')\n    parser.set_defaults(debug=False, verbose=False)\n\n    args = parser.parse_args()\n    args = vars(args)\n    args = {k: v for k, v in args.items() if v is not None}\n\n    conf = Config()\n    experiment_args = conf.compile(conf.load(args[\'exp\']))[\'Experiment\']\n    experiment_args.update(cmd=args)\n    experiment = Configurable.construct_class_from_config(experiment_args)\n\n    Eval(experiment, experiment_args, cmd=args, verbose=args[\'verbose\']).eval(args[\'visualize\'])\n\n\nclass Eval:\n    def __init__(self, experiment, args, cmd=dict(), verbose=False):\n        self.experiment = experiment\n        experiment.load(\'evaluation\', **args)\n        self.data_loaders = experiment.evaluation.data_loaders\n        self.args = cmd\n        self.logger = experiment.logger\n        model_saver = experiment.train.model_saver\n        self.structure = experiment.structure\n        self.model_path = cmd.get(\n            \'resume\', os.path.join(\n                self.logger.save_dir(model_saver.dir_path),\n                \'final\'))\n        self.verbose = verbose\n\n    def init_torch_tensor(self):\n        # Use gpu or not\n        torch.set_default_tensor_type(\'torch.FloatTensor\')\n        if torch.cuda.is_available():\n            self.device = torch.device(\'cuda\')\n        else:\n            self.device = torch.device(\'cpu\')\n\n    def init_model(self):\n        model = self.structure.builder.build(self.device)\n        return model\n\n    def resume(self, model, path):\n        if not os.path.exists(path):\n            self.logger.warning(""Checkpoint not found: "" + path)\n            return\n        self.logger.info(""Resuming from "" + path)\n        states = torch.load(\n            path, map_location=self.device)\n        model.load_state_dict(states, strict=False)\n        self.logger.info(""Resumed from "" + path)\n\n    def report_speed(self, model, batch, times=100):\n        import time\n        data = {k: v[0:1]for k, v in batch.items()}\n        cuda = torch.cuda.is_available()\n        if cuda:\n            torch.cuda.synchronize()\n        start = time.time()\n        for _ in range(times):\n            model.forward(data)\n        if cuda:\n            torch.cuda.synchronize()\n        time_cost = (time.time() - start) / times\n        self.logger.info(\'Params: %s, Inference speed: %fms, FPS: %f\' % (\n            str(sum(p.numel() for p in model.parameters() if p.requires_grad)),\n            time_cost * 1000, 1 / time_cost))\n\n    def eval(self, visualize=False):\n        self.init_torch_tensor()\n        model = self.init_model()\n        self.resume(model, self.model_path)\n        all_matircs = {}\n        model.eval()\n        vis_images = dict()\n        with torch.no_grad():\n            for name, data_loader in self.data_loaders.items():\n                raw_metrics = []\n                if self.args.get(\'data\', None) and not self.args[\'data\'] == name:\n                    continue\n                for i, batch in tqdm(enumerate(data_loader), total=len(data_loader)):\n                    if self.args[\'test_speed\']:\n                        self.report_speed(model, batch)\n                        continue\n\n                    pred = model.forward(batch, training=False)\n                    output = self.structure.representer.represent(batch, pred)\n                    raw_metric, interested = self.structure.measurer.validate_measure(batch, output)\n                    raw_metrics.append(raw_metric)\n\n                    if visualize and self.structure.visualizer:\n                        vis_image = self.structure.visualizer.visualize(batch, output, interested)\n                        self.logger.save_image_dict(vis_image)\n                        vis_images.update(vis_image)\n                metrics = self.structure.measurer.gather_measure(raw_metrics, self.logger)\n                for key, metric in metrics.items():\n                    self.logger.info(\'%s : %f (%d)\' % (key, metric.avg, metric.count))\n                    all_matircs[name + \'/\' + key] = metric\n        for key, metric in all_matircs.items():\n            self.logger.info(\'%s : %f (%d)\' % (key, metric.avg, metric.count))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
experiment.py,0,"b""from concern.config import Configurable, State\nfrom concern.log import Logger\nfrom structure.builder import Builder\nfrom structure.representers import *\nfrom structure.measurers import *\nfrom structure.visualizer import TrivalVisualizer\nfrom structure.visualizers import *\nfrom data.data_loader import *\nfrom data import *\nfrom training.model_saver import ModelSaver\nfrom training.checkpoint import Checkpoint\nfrom training.optimizer_scheduler import OptimizerScheduler\n\n\nclass Structure(Configurable):\n    builder = State()\n    representer = State()\n    measurer = State()\n    visualizer = State()\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n    @property\n    def model_name(self):\n        return self.builder.model_name\n\n\nclass TrainSettings(Configurable):\n    data_loader = State()\n    model_saver = State()\n    checkpoint = State()\n    scheduler = State()\n    epochs = State(default=10)\n\n    def __init__(self, **kwargs):\n        kwargs['cmd'].update(is_train=True)\n        self.load_all(**kwargs)\n        if 'epochs' in kwargs['cmd']:\n            self.epochs = kwargs['cmd']['epochs']\n\n\nclass ValidationSettings(Configurable):\n    data_loaders = State()\n    visualize = State()\n    interval = State(default=100)\n    exempt = State(default=-1)\n\n    def __init__(self, **kwargs):\n        kwargs['cmd'].update(is_train=False)\n        self.load_all(**kwargs)\n\n        cmd = kwargs['cmd']\n        self.visualize = cmd['visualize']\n\n\nclass EvaluationSettings(Configurable):\n    data_loaders = State()\n    visualize = State(default=True)\n    resume = State()\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n\nclass EvaluationSettings2(Configurable):\n    structure = State()\n    data_loaders = State()\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n\nclass ShowSettings(Configurable):\n    data_loader = State()\n    representer = State()\n    visualizer = State()\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n\nclass Experiment(Configurable):\n    structure = State(autoload=False)\n    train = State()\n    validation = State(autoload=False)\n    evaluation = State(autoload=False)\n    logger = State(autoload=True)\n\n    def __init__(self, cmd={}, **kwargs):\n        self.load('structure', cmd=cmd, **kwargs)\n\n        if 'name' not in cmd:\n            cmd['name'] = self.structure.model_name\n\n        self.load_all(cmd=cmd, **kwargs)\n        self.distributed = cmd.get('distributed', False)\n        self.local_rank = cmd.get('local_rank', 0)\n\n        if cmd.get('validate', False):\n            self.load('validation', cmd=cmd, **kwargs)\n        else:\n            self.validation = None\n"""
train.py,3,"b""#!python3\nimport argparse\nimport time\n\nimport torch\nimport yaml\n\nfrom trainer import Trainer\n\n# tagged yaml objects\nfrom experiment import Structure, TrainSettings, ValidationSettings, Experiment\nfrom concern.log import Logger\nfrom data.data_loader import DataLoader\nfrom data.mnist import MNistDataset\nfrom data.nori_dataset import NoriDataset\nfrom training.checkpoint import Checkpoint\nfrom training.model_saver import ModelSaver\nfrom training.optimizer_scheduler import OptimizerScheduler\n\nfrom concern.config import Configurable, Config\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Text Recognition Training')\n    parser.add_argument('exp', type=str)\n    parser.add_argument('--name', type=str)\n    parser.add_argument('--batch_size', type=int, help='Batch size for training')\n    parser.add_argument('--resume', type=str, help='Resume from checkpoint')\n    parser.add_argument('--epochs', type=int, help='Number of training epochs')\n    parser.add_argument('--num_workers', type=int, help='Number of dataloader workers')\n    parser.add_argument('--start_iter', type=int, help='Begin counting iterations starting from this value (should be used with resume)')\n    parser.add_argument('--start_epoch', type=int, help='Begin counting epoch starting from this value (should be used with resume)')\n    parser.add_argument('--max_size', type=int, help='max length of label')\n    parser.add_argument('--lr', type=float, help='initial learning rate')\n    parser.add_argument('--optimizer', type=str, help='The optimizer want to use')\n    parser.add_argument('--thresh', type=float, help='The threshold to replace it in the representers')\n    parser.add_argument('--verbose', action='store_true', help='show verbose info')\n    parser.add_argument('--visualize', action='store_true', help='visualize maps in tensorboard')\n    parser.add_argument('--force_reload', action='store_true', dest='force_reload', help='Force reload data meta')\n    parser.add_argument('--no-force_reload', action='store_false', dest='force_reload', help='Force reload data meta')\n    parser.add_argument('--validate', action='store_true', dest='validate', help='Validate during training')\n    parser.add_argument('--no-validate', action='store_false', dest='validate', help='Validate during training')\n    parser.add_argument('--print-config-only', action='store_true', help='print config without actual training')\n    parser.add_argument('--debug', action='store_true', dest='debug', help='Run with debug mode, which hacks dataset num_samples to toy number')\n    parser.add_argument('--no-debug', action='store_false', dest='debug', help='Run without debug mode')\n    parser.add_argument('--benchmark', action='store_true', dest='benchmark', help='Open cudnn benchmark mode')\n    parser.add_argument('--no-benchmark', action='store_false', dest='benchmark', help='Turn cudnn benchmark mode off')\n    parser.add_argument('-d', '--distributed', action='store_true', dest='distributed', help='Use distributed training')\n    parser.add_argument('--local_rank', dest='local_rank', default=0, type=int, help='Use distributed training')\n    parser.add_argument('-g', '--num_gpus', dest='num_gpus', default=4, type=int, help='The number of accessible gpus')\n    parser.set_defaults(debug=False)\n    parser.set_defaults(benchmark=True)\n\n    args = parser.parse_args()\n    args = vars(args)\n    args = {k: v for k, v in args.items() if v is not None}\n\n    if args['distributed']:\n        torch.cuda.set_device(args['local_rank'])\n        torch.distributed.init_process_group(backend='nccl', init_method='env://')\n\n    conf = Config()\n    experiment_args = conf.compile(conf.load(args['exp']))['Experiment']\n    experiment_args.update(cmd=args)\n    experiment = Configurable.construct_class_from_config(experiment_args)\n\n    if not args['print_config_only']:\n        torch.backends.cudnn.benchmark = args['benchmark']\n        trainer = Trainer(experiment)\n        trainer.train()\n\nif __name__ == '__main__':\n    main()\n\n# flake8: noqa\n"""
trainer.py,7,"b'import os\n\nimport torch\nimport torch.distributed as dist\nfrom tqdm import tqdm\n\nfrom experiment import Experiment\nfrom data.data_loader import DistributedSampler\nfrom concern.distributed import reduce, gather\n\n\nclass Trainer:\n    def __init__(self, experiment: Experiment):\n        self.init_torch_tensor()\n\n        self.experiment = experiment\n        self.structure = experiment.structure\n        self.logger = experiment.logger\n        self.model_saver = experiment.train.model_saver\n        self.is_main = self.experiment.local_rank == 0\n\n        # FIXME: Hack the save model path into logger path\n        self.model_saver.dir_path = self.logger.save_dir(\n            self.model_saver.dir_path)\n        self.current_lr = 0\n\n        self.total = 0\n\n    def init_torch_tensor(self):\n        # Use gpu or not\n        torch.set_default_tensor_type(\'torch.FloatTensor\')\n\n        if torch.cuda.is_available():\n            self.device = torch.device(\'cuda\')\n        else:\n            self.device = torch.device(\'cpu\')\n\n    def init_model(self):\n        model = self.structure.builder.build(\n            self.device, self.experiment.distributed, self.experiment.local_rank)\n        return model\n\n    def update_learning_rate(self, optimizer, epoch, step):\n        lr = self.experiment.train.scheduler.learning_rate.get_learning_rate(\n            epoch, step)\n\n        for group in optimizer.param_groups:\n            group[\'lr\'] = lr\n        self.current_lr = lr\n\n    def train(self):\n        self.logger.report_time(\'Start\')\n        self.logger.args(self.experiment)\n        model = self.init_model()\n\n        train_data_loader = self.experiment.train.data_loader\n        if self.experiment.validation:\n            validation_loaders = self.experiment.validation.data_loaders\n\n        self.steps = 0\n        if self.experiment.train.checkpoint:\n            self.experiment.train.checkpoint.restore_model(\n                model, self.device, self.logger)\n            epoch, iter_delta = self.experiment.train.checkpoint.restore_counter()\n            self.steps = epoch * self.total + iter_delta\n\n        # Init start epoch and iter\n        optimizer = self.experiment.train.scheduler.create_optimizer(\n            model.parameters())\n\n        self.logger.report_time(\'Init\')\n\n        model.train()\n\n        while True:\n            self.logger.info(\'Training epoch \' + str(epoch))\n            self.logger.epoch(epoch)\n            self.total = len(train_data_loader)\n\n            for batch in train_data_loader:\n                self.update_learning_rate(optimizer, epoch, self.steps)\n\n                self.logger.report_time(""Data loading"")\n\n                if self.experiment.validation and\\\n                        self.steps % self.experiment.validation.interval == 0 and\\\n                        self.steps > self.experiment.validation.exempt:\n                    self.validate(validation_loaders, model, epoch, self.steps)\n                self.logger.report_time(\'Validating \')\n                if self.logger.verbose:\n                    torch.cuda.synchronize()\n\n                self.train_step(model, optimizer, batch,\n                                epoch=epoch, step=self.steps)\n                if self.logger.verbose:\n                    torch.cuda.synchronize()\n                self.logger.report_time(\'Forwarding \')\n\n                self.model_saver.maybe_save_model(\n                    model, epoch, self.steps, self.logger)\n\n                self.steps += 1\n                self.logger.report_eta(self.steps, self.total, epoch)\n                del batch\n            epoch += 1\n            if epoch > self.experiment.train.epochs:\n                self.model_saver.save_checkpoint(model, \'final\')\n                if self.experiment.validation:\n                    self.validate(validation_loaders, model, epoch, self.steps)\n                self.logger.info(\'Training done\')\n                break\n            iter_delta = 0\n\n    def train_step(self, model, optimizer, batch, epoch, step, **kwards):\n        optimizer.zero_grad()\n\n        results = model.forward(batch, training=True)\n\n        if len(results) == 2:\n            l, pred = results\n            metrics = {}\n        elif len(results) == 3:\n            l, pred, metrics = results\n        else:\n            l = results\n\n        loss = l.mean()\n\n        loss.backward()\n        optimizer.step()\n\n        if step % self.experiment.logger.log_interval == 0:\n            loss_for_log = reduce(loss)\n            if self.is_main:\n                self.logger.info(\'step: %6d, epoch: %3d, loss: %.6f, lr: %f\' % (\n                    step, epoch, loss_for_log.mean().item(), self.current_lr))\n                self.logger.add_scalar(\'loss\', loss, step)\n                self.logger.add_scalar(\'learning_rate\', self.current_lr, step)\n                for name, metric in metrics.items():\n                    self.logger.add_scalar(name, metric.mean(), step)\n                    self.logger.info(\'%s: %6f\' % (name, metric.mean()))\n\n                self.logger.report_time(\'Logging\')\n\n    def validate(self, validation_loaders, model, epoch, step):\n        all_matircs = {}\n        model.eval()\n        for name, loader in validation_loaders.items():\n            if self.experiment.validation.visualize:\n                metrics, vis_images = self.validate_step(\n                    loader, model, True)\n                self.logger.images(\n                    os.path.join(\'vis\', name), vis_images, step)\n            else:\n                metrics, vis_images = self.validate_step(loader, model, False)\n\n            for _key, metric in metrics.items():\n                key = name + \'/\' + _key\n                if key in all_matircs:\n                    all_matircs[key].update(metric.val, metric.count)\n                else:\n                    all_matircs[key] = metric\n\n        for key, metric in all_matircs.items():\n            self.logger.info(\'%s : %f (%d)\' % (key, metric.avg, metric.count))\n        self.logger.metrics(epoch, self.steps, all_matircs)\n        model.train()\n        return all_matircs\n\n    def validate_step(self, data_loader, model, visualize=False):\n        raw_metrics = []\n        vis_images = dict()\n        for _, batch in tqdm(enumerate(data_loader), total=len(data_loader)):\n            pred = model.forward(batch, training=False)\n\n            # The post-processing and evaluation are performed in the rank-0 process.\n            if dist.is_available() and dist.is_initialized():\n                pred = gather(pred)\n                batch = gather(batch)\n            if not self.is_main:\n                continue\n\n            output = self.structure.representer.represent(batch, pred)\n            raw_metric, interested = self.structure.measurer.validate_measure(\n                batch, output)\n            raw_metrics.append(raw_metric)\n\n            if visualize and self.structure.visualizer:\n                vis_image = self.structure.visualizer.visualize(\n                    batch, output, interested)\n                vis_images.update(vis_image)\n        metrics = self.structure.measurer.gather_measure(\n            raw_metrics, self.logger)\n        return metrics, vis_images\n\n    def to_np(self, x):\n        return x.cpu().data.numpy()\n'"
backbones/__init__.py,0,"b'from .resnet_fpn import Resnet18FPN, Resnet34FPN, Resnet50FPN, Resnet101FPN, Resnet152FPN\nfrom .resnet import resnet18, resnet34, resnet50, resnet101, deformable_resnet50\nfrom .crnn import crnn_backbone\nfrom .resnet_ppm import resnet50dilated_ppm\n'"
backbones/base.py,1,"b'import torch\nimport torch.nn as nn\n\n\ndef conv3x3(in_planes, out_planes, stride=1, has_bias=False):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=has_bias)\n\n\ndef conv3x3_bn_relu(in_planes, out_planes, stride=1):\n    return nn.Sequential(\n        conv3x3(in_planes, out_planes, stride),\n        nn.BatchNorm2d(out_planes),\n        nn.ReLU(inplace=True),\n    )\n\n\n\n'"
backbones/crnn.py,1,"b""import torch.nn as nn\n\n\nclass CRNN(nn.Module):\n\n    def __init__(self, imgH, nc, nclass, nh):\n        super(CRNN, self).__init__()\n        assert imgH % 16 == 0, 'imgH has to be a multiple of 16'\n\n        self.kernels = [3, 3, 3, 3, 3, 3, 2]\n        self.paddings = [1, 1, 1, 1, 1, 1, 0]\n        self.strides = [1, 1, 1, 1, 1, 1, 1]\n        self.channels = [64, 128, 256, 256, 512, 512, 512, nc]\n\n        conv0 = nn.Sequential(\n            self._make_layer(0),\n            nn.MaxPool2d((2, 2))\n        )\n        conv1 = nn.Sequential(\n            self._make_layer(1),\n            nn.MaxPool2d((2, 2))\n        )\n        conv2 = self._make_layer(2, True)\n        conv3 = nn.Sequential(\n            self._make_layer(3),\n            nn.MaxPool2d((2, 2), (2, 1), (0, 1))\n        )\n        conv4 = self._make_layer(4, True)\n        conv5 = nn.Sequential(\n            self._make_layer(5),\n            nn.MaxPool2d((2, 2), (2, 1), (0, 1))\n        )\n        conv6 = self._make_layer(6, True)\n\n        self.cnn = nn.Sequential(\n            conv0,\n            conv1,\n            conv2,\n            conv3,\n            conv4,\n            conv5,\n            conv6\n        )\n\n\n    def _make_layer(self, i, batch_normalization=False):\n        in_channel = self.channels[i - 1]\n        out_channel = self.channels[i]\n        layer = list()\n        layer.append(nn.Conv2d(in_channel, out_channel, self.kernels[i], self.strides[i], self.paddings[i]))\n        if batch_normalization:\n            layer.append(nn.BatchNorm2d(out_channel))\n        else:\n            layer.append(nn.ReLU())\n        return nn.Sequential(*layer)\n\n    def forward(self, input):\n        # conv features\n        return self.cnn(input)\n\n\ndef crnn_backbone(imgH=32, nc=3, nclass=37, nh=256):\n    return CRNN(imgH, nc, nclass, nh)\n"""
backbones/feature_pyramid.py,1,"b'import torch.nn as nn\n\n\nclass FeaturePyramid(nn.Module):\n    def __init__(self, bottom_up, top_down):\n        nn.Module.__init__(self)\n\n        self.bottom_up = bottom_up\n        self.top_down = top_down\n\n    def forward(self, feature):\n        pyramid_features = self.bottom_up(feature)\n        feature = self.top_down(pyramid_features[::-1])\n        return feature\n'"
backbones/fpn_top_down.py,2,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FPNTopDown(nn.Module):\n    def __init__(self, pyramid_channels, feature_channel):\n        nn.Module.__init__(self)\n\n        self.reduction_layers = nn.ModuleList()\n        for pyramid_channel in pyramid_channels:\n            reduction_layer = nn.Conv2d(pyramid_channel, feature_channel, kernel_size=1, stride=1, padding=0, bias=False)\n            self.reduction_layers.append(reduction_layer)\n\n        self.merge_layer = nn.Conv2d(feature_channel, feature_channel, kernel_size=3, stride=1, padding=1, bias=False)\n\n    def upsample_add(self, x, y):\n        _, _, H, W = y.size()\n        return F.interpolate(x, size=(H, W), mode='bilinear') + y\n\n    def forward(self, pyramid_features):\n        feature = None\n        for pyramid_feature, reduction_layer in zip(pyramid_features, self.reduction_layers):\n            pyramid_feature = reduction_layer(pyramid_feature)\n            if feature is None:\n                feature = pyramid_feature\n            else:\n                feature = self.upsample_add(feature, pyramid_feature)\n        feature = self.merge_layer(feature)\n        return feature\n"""
backbones/ppm.py,2,"b""import torch\nimport torch.nn as nn\nfrom .base import conv3x3_bn_relu, conv3x3\n\n\nclass PPMDeepsup(nn.Module):\n    def __init__(self, inner_channels=256, fc_dim=2048,\n                 pool_scales=(1, 2, 3, 6)):\n        super(PPMDeepsup, self).__init__()\n\n        self.ppm = []\n        for scale in pool_scales:\n            self.ppm.append(nn.Sequential(\n                nn.AdaptiveAvgPool2d(scale),\n                nn.Conv2d(fc_dim, 512, kernel_size=1, bias=False),\n                nn.BatchNorm2d(512),\n                nn.ReLU(inplace=True)\n            ))\n        self.ppm = nn.ModuleList(self.ppm)\n        self.cbr_deepsup = conv3x3_bn_relu(fc_dim // 2, fc_dim // 4, 1)\n\n        self.conv_last = nn.Sequential(\n            nn.Conv2d(fc_dim+len(pool_scales)*512, 512,\n                      kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(0.1),\n            nn.Conv2d(512, inner_channels, kernel_size=1)\n        )\n\n    def forward(self, conv_out, segSize=None):\n        conv5 = conv_out[-1]\n\n        input_size = conv5.size()\n        ppm_out = [conv5]\n        for pool_scale in self.ppm:\n            ppm_out.append(nn.functional.interpolate(\n                pool_scale(conv5),\n                (input_size[2], input_size[3]),\n                mode='bilinear', align_corners=False))\n        ppm_out = torch.cat(ppm_out, 1)\n\n        x = self.conv_last(ppm_out)\n        return x\n\n"""
backbones/resnet.py,2,"b'import torch.nn as nn\r\nimport math\r\nimport torch.utils.model_zoo as model_zoo\r\nimport apex\r\nimport config\r\n\r\nfrom concern.distributed import is_distributed\r\n\r\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\r\n           \'resnet152\']\r\n\r\n\r\nmodel_urls = {\r\n    \'resnet18\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet18-imagenet.pth\',\r\n    \'resnet50\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet50-imagenet.pth\',\r\n    \'resnet101\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet101-imagenet.pth\'\r\n}\r\n\r\n\r\ndef constant_init(module, constant, bias=0):\r\n    nn.init.constant_(module.weight, constant)\r\n    if hasattr(module, \'bias\'):\r\n        nn.init.constant_(module.bias, bias)\r\n\r\n\r\ndef bn(*args, **kwargs):\r\n    if config.sync_bn:\r\n        return apex.parallel.SyncBatchNorm(*args, **kwargs)\r\n    else:\r\n        return nn.BatchNorm2d(*args, **kwargs)\r\n\r\n\r\ndef conv3x3(in_planes, out_planes, stride=1):\r\n    """"""3x3 convolution with padding""""""\r\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\r\n                     padding=1, bias=False)\r\n\r\n\r\nclass BasicBlock(nn.Module):\r\n    expansion = 1\r\n\r\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dcn=None):\r\n        super(BasicBlock, self).__init__()\r\n        self.with_dcn = dcn is not None\r\n        self.conv1 = conv3x3(inplanes, planes, stride)\r\n        self.bn1 = bn(planes)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.with_modulated_dcn = False\r\n        if self.with_dcn:\r\n            fallback_on_stride = dcn.get(\'fallback_on_stride\', False)\r\n            self.with_modulated_dcn = dcn.get(\'modulated\', False)\r\n\r\n        if not self.with_dcn or fallback_on_stride:\r\n            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\r\n                                   padding=1, bias=False)\r\n        else:\r\n            deformable_groups = dcn.get(\'deformable_groups\', 1)\r\n            if not self.with_modulated_dcn:\r\n                from assets.ops.dcn import DeformConv\r\n                conv_op = DeformConv\r\n                offset_channels = 18\r\n            else:\r\n                from assets.ops.dcn import ModulatedDeformConv\r\n                conv_op = ModulatedDeformConv\r\n                offset_channels = 27\r\n            self.conv2_offset = nn.Conv2d(\r\n                planes,\r\n                deformable_groups * offset_channels,\r\n                kernel_size=3,\r\n                padding=1)\r\n            self.conv2 = conv_op(\r\n                planes,\r\n                planes,\r\n                kernel_size=3,\r\n                padding=1,\r\n                deformable_groups=deformable_groups,\r\n                bias=False)\r\n        self.bn2 = bn(planes)\r\n        self.downsample = downsample\r\n        self.stride = stride\r\n\r\n    def forward(self, x):\r\n        residual = x\r\n\r\n        out = self.conv1(x)\r\n        out = self.bn1(out)\r\n        out = self.relu(out)\r\n\r\n        if not self.with_dcn:\r\n            out = self.conv2(out)\r\n        elif self.with_modulated_dcn:\r\n            offset_mask = self.conv2_offset(out)\r\n            offset = offset_mask[:, :18, :, :]\r\n            mask = offset_mask[:, -9:, :, :].sigmoid()\r\n            out = self.conv2(out, offset, mask)\r\n        else:\r\n            offset = self.conv2_offset(out)\r\n            out = self.conv2(out, offset)\r\n        out = self.bn2(out)\r\n\r\n        if self.downsample is not None:\r\n            residual = self.downsample(x)\r\n\r\n        out += residual\r\n        out = self.relu(out)\r\n\r\n        return out\r\n\r\n\r\nclass Bottleneck(nn.Module):\r\n    expansion = 4\r\n\r\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dcn=None):\r\n        super(Bottleneck, self).__init__()\r\n        self.with_dcn = dcn is not None\r\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\r\n        self.bn1 = bn(planes)\r\n        fallback_on_stride = False\r\n        self.with_modulated_dcn = False\r\n        if self.with_dcn:\r\n            fallback_on_stride = dcn.get(\'fallback_on_stride\', False)\r\n            self.with_modulated_dcn = dcn.get(\'modulated\', False)\r\n        if not self.with_dcn or fallback_on_stride:\r\n            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\r\n                                   stride=stride, padding=1, bias=False)\r\n        else:\r\n            deformable_groups = dcn.get(\'deformable_groups\', 1)\r\n            if not self.with_modulated_dcn:\r\n                from assets.ops.dcn import DeformConv\r\n                conv_op = DeformConv\r\n                offset_channels = 18\r\n            else:\r\n                from assets.ops.dcn import ModulatedDeformConv\r\n                conv_op = ModulatedDeformConv\r\n                offset_channels = 27\r\n            self.conv2_offset = nn.Conv2d(\r\n                planes, deformable_groups * offset_channels,\r\n                kernel_size=3,\r\n                padding=1)\r\n            self.conv2 = conv_op(\r\n                planes, planes, kernel_size=3, padding=1, stride=stride,\r\n                deformable_groups=deformable_groups, bias=False)\r\n        self.bn2 = bn(planes)\r\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\r\n        self.bn3 = bn(planes * 4)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.downsample = downsample\r\n        self.stride = stride\r\n        self.dcn = dcn\r\n        self.with_dcn = dcn is not None\r\n\r\n    def forward(self, x):\r\n        residual = x\r\n\r\n        out = self.conv1(x)\r\n        out = self.bn1(out)\r\n        out = self.relu(out)\r\n\r\n        if not self.with_dcn:\r\n            out = self.conv2(out)\r\n        elif self.with_modulated_dcn:\r\n            offset_mask = self.conv2_offset(out)\r\n            offset = offset_mask[:, :18, :, :]\r\n            mask = offset_mask[:, -9:, :, :].sigmoid()\r\n            out = self.conv2(out, offset, mask)\r\n        else:\r\n            offset = self.conv2_offset(out)\r\n            out = self.conv2(out, offset)\r\n        out = self.bn2(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv3(out)\r\n        out = self.bn3(out)\r\n\r\n        if self.downsample is not None:\r\n            residual = self.downsample(x)\r\n\r\n        out += residual\r\n        out = self.relu(out)\r\n\r\n        return out\r\n\r\n\r\nclass ResNet(nn.Module):\r\n    def __init__(self, block, layers, num_classes=1000,\r\n                 dcn=None, stage_with_dcn=(False, False, False, False),\r\n                 dilations=[1, 1, 1, 1]):\r\n        self.dcn = dcn\r\n        self.stage_with_dcn = stage_with_dcn\r\n        self.inplanes = 128\r\n        super(ResNet, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\r\n                               bias=False)\r\n        self.bn1 = bn(64)\r\n        self.relu1 = nn.ReLU(inplace=True)\r\n        self.conv2 = conv3x3(64, 64)\r\n        self.bn2 = bn(64)\r\n        self.relu2 = nn.ReLU(inplace=True)\r\n        self.conv3 = conv3x3(64, 128)\r\n        self.bn3 = bn(128)\r\n        self.relu3 = nn.ReLU(inplace=True)\r\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n        self.layer1 = self._make_layer(block, 64, layers[0], dilation=dilations[0])\r\n        self.layer2 = self._make_layer(\r\n            block, 128, layers[1], stride=2, dcn=dcn, dilation=dilations[1])\r\n        self.layer3 = self._make_layer(\r\n            block, 256, layers[2], stride=2, dcn=dcn, dilation=dilations[2])\r\n        self.layer4 = self._make_layer(\r\n            block, 512, layers[3], stride=2, dcn=dcn, dilation=dilations[3])\r\n        self.avgpool = nn.AvgPool2d(7, stride=1)\r\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\r\n\r\n        self.smooth = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=1)\r\n\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n                m.weight.data.normal_(0, math.sqrt(2. / n))\r\n            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.SyncBatchNorm):\r\n                m.weight.data.fill_(1)\r\n                m.bias.data.zero_()\r\n        if self.dcn is not None:\r\n            for m in self.modules():\r\n                if isinstance(m, Bottleneck) or isinstance(m, BasicBlock):\r\n                    if hasattr(m, \'conv2_offset\'):\r\n                        constant_init(m.conv2_offset, 0)\r\n\r\n    def _make_layer(self, block, planes, blocks, stride=1, dcn=None, dilation=1):\r\n        downsample = None\r\n        if stride != 1 or self.inplanes != planes * block.expansion:\r\n            downsample = nn.Sequential(\r\n                nn.Conv2d(self.inplanes, planes * block.expansion,\r\n                          kernel_size=1, stride=stride, bias=False, dilation=dilation),\r\n                bn(planes * block.expansion),\r\n            )\r\n\r\n        layers = []\r\n        layers.append(block(self.inplanes, planes, stride, downsample, dcn=dcn))\r\n        self.inplanes = planes * block.expansion\r\n        for _ in range(1, blocks):\r\n            layers.append(block(self.inplanes, planes, dcn=dcn))\r\n\r\n        return nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        x = self.relu1(self.bn1(self.conv1(x)))\r\n        x = self.relu2(self.bn2(self.conv2(x)))\r\n        x = self.relu3(self.bn3(self.conv3(x)))\r\n        x = self.maxpool(x)\r\n\r\n        x2 = self.layer1(x)\r\n        x3 = self.layer2(x2)\r\n        x4 = self.layer3(x3)\r\n        x5 = self.layer4(x4)\r\n\r\n        return x2, x3, x4, x5\r\n\r\n\r\ndef resnet18(pretrained=True, **kwargs):\r\n    """"""Constructs a ResNet-18 model.\r\n    Args:\r\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n    """"""\r\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\r\n    if pretrained:\r\n        model.load_state_dict(model_zoo.load_url(\r\n            model_urls[\'resnet18\']), strict=False)\r\n    return model\r\n\r\n\r\ndef resnet34(pretrained=True, **kwargs):\r\n    """"""Constructs a ResNet-34 model.\r\n    Args:\r\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n    """"""\r\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\r\n    if pretrained:\r\n        model.load_state_dict(model_zoo.load_url(\r\n            model_urls[\'resnet34\']), strict=False)\r\n    return model\r\n\r\n\r\ndef resnet50(pretrained=True, **kwargs):\r\n    """"""Constructs a ResNet-50 model.\r\n    Args:\r\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n    """"""\r\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\r\n    if pretrained:\r\n        model.load_state_dict(model_zoo.load_url(\r\n            model_urls[\'resnet50\']), strict=False)\r\n    return model\r\n\r\n\r\ndef deformable_resnet50(pretrained=True, **kwargs):\r\n    """"""Constructs a ResNet-50 model with deformable conv.\r\n    Args:\r\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n    """"""\r\n    model = ResNet(Bottleneck, [3, 4, 6, 3],\r\n                   dcn=dict(modulated=True,\r\n                            deformable_groups=1,\r\n                            fallback_on_stride=False),\r\n                   stage_with_dcn=[False, True, True, True],\r\n                   **kwargs)\r\n    if pretrained:\r\n        model.load_state_dict(model_zoo.load_url(\r\n            model_urls[\'resnet50\']), strict=False)\r\n    return model\r\n\r\n\r\ndef resnet101(pretrained=True, **kwargs):\r\n    """"""Constructs a ResNet-101 model.\r\n    Args:\r\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n    """"""\r\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\r\n    if pretrained:\r\n        model.load_state_dict(model_zoo.load_url(\r\n            model_urls[\'resnet101\']), strict=False)\r\n    return model\r\n\r\n\r\ndef resnet152(pretrained=True, **kwargs):\r\n    """"""Constructs a ResNet-152 model.\r\n    Args:\r\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\r\n    """"""\r\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\r\n    if pretrained:\r\n        model.load_state_dict(model_zoo.load_url(\r\n            model_urls[\'resnet152\']), strict=False)\r\n    return model\r\n'"
backbones/resnet_dilated.py,1,"b""import torch\nimport torch.nn as nn\n\n\nclass ResnetDilated(nn.Module):\n    def __init__(self, orig_resnet, dilate_scale=8):\n        super(ResnetDilated, self).__init__()\n        from functools import partial\n\n        if dilate_scale == 8:\n            orig_resnet.layer3.apply(\n                partial(self._nostride_dilate, dilate=2))\n            orig_resnet.layer4.apply(\n                partial(self._nostride_dilate, dilate=4))\n        elif dilate_scale == 16:\n            orig_resnet.layer4.apply(\n                partial(self._nostride_dilate, dilate=2))\n\n        # take pretrained resnet, except AvgPool and FC\n        self.conv1 = orig_resnet.conv1\n        self.bn1 = orig_resnet.bn1\n        self.relu1 = orig_resnet.relu1\n        self.conv2 = orig_resnet.conv2\n        self.bn2 = orig_resnet.bn2\n        self.relu2 = orig_resnet.relu2\n        self.conv3 = orig_resnet.conv3\n        self.bn3 = orig_resnet.bn3\n        self.relu3 = orig_resnet.relu3\n        self.maxpool = orig_resnet.maxpool\n        self.layer1 = orig_resnet.layer1\n        self.layer2 = orig_resnet.layer2\n        self.layer3 = orig_resnet.layer3\n        self.layer4 = orig_resnet.layer4\n\n    def _nostride_dilate(self, m, dilate):\n        classname = m.__class__.__name__\n        if classname.find('Conv') != -1:\n            # the convolution with stride\n            if m.stride == (2, 2):\n                m.stride = (1, 1)\n                if m.kernel_size == (3, 3):\n                    m.dilation = (dilate//2, dilate//2)\n                    m.padding = (dilate//2, dilate//2)\n            # other convoluions\n            else:\n                if m.kernel_size == (3, 3):\n                    m.dilation = (dilate, dilate)\n                    m.padding = (dilate, dilate)\n\n    def forward(self, x, return_feature_maps=True):\n        conv_out = []\n\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        conv_out.append(x)\n        x = self.layer2(x)\n        conv_out.append(x)\n        x = self.layer3(x)\n        conv_out.append(x)\n        x = self.layer4(x)\n        conv_out.append(x)\n\n        if return_feature_maps:\n            return conv_out\n        return x\n"""
backbones/resnet_fpn.py,0,"b'from .resnet import resnet18, resnet34, resnet50, resnet101, resnet152\nfrom .fpn_top_down import FPNTopDown\nfrom .feature_pyramid import FeaturePyramid\n\n\ndef Resnet18FPN(resnet_pretrained=True):\n    bottom_up = resnet18(pretrained=resnet_pretrained)\n    top_down = FPNTopDown([512, 256, 128, 64], 256)\n    feature_pyramid = FeaturePyramid(bottom_up, top_down)\n    return feature_pyramid\n\n\ndef Resnet34FPN(resnet_pretrained=True):\n    bottom_up = resnet50(pretrained=resnet_pretrained)\n    top_down = FPNTopDown([2048, 1024, 512, 256], 256)\n    feature_pyramid = FeaturePyramid(bottom_up, top_down)\n    return feature_pyramid\n\n\ndef Resnet50FPN(resnet_pretrained=True):\n    bottom_up = resnet50(pretrained=resnet_pretrained)\n    top_down = FPNTopDown([2048, 1024, 512, 256], 256)\n    feature_pyramid = FeaturePyramid(bottom_up, top_down)\n    return feature_pyramid\n\n\ndef Resnet101FPN(resnet_pretrained=True):\n    bottom_up = resnet101(pretrained=resnet_pretrained)\n    top_down = FPNTopDown([2048, 1024, 512, 256], 256)\n    feature_pyramid = FeaturePyramid(bottom_up, top_down)\n    return feature_pyramid\n\n\ndef Resnet152FPN(resnet_pretrained=True):\n    bottom_up = resnet152(pretrained=resnet_pretrained)\n    top_down = FPNTopDown([2048, 1024, 512, 256], 256)\n    feature_pyramid = FeaturePyramid(bottom_up, top_down)\n    return feature_pyramid\n'"
backbones/resnet_ppm.py,1,"b'import torch\nimport torch.nn as nn\n\nfrom .resnet import resnet18, resnet34, resnet50, resnet101, resnet152\nfrom .resnet_dilated import ResnetDilated\nfrom .ppm import PPMDeepsup\n\n\ndef resnet50dilated_ppm(resnet_pretrained=False, **kwargs):\n    resnet = resnet50(pretrained=resnet_pretrained)\n    resnet_dilated = ResnetDilated(resnet, dilate_scale=8)\n    ppm = PPMDeepsup(**kwargs)\n    return nn.Sequential(resnet_dilated, ppm)\n'"
backbones/upsample_head.py,1,"b'import torch.nn as nn\n\n\ndef SimpleUpsampleHead(feature_channel, layer_channels):\n    modules = []\n    modules.append(nn.Conv2d(feature_channel, layer_channels[0], kernel_size=3, stride=1, padding=1, bias=False))\n    for layer_index in range(len(layer_channels) - 1):\n        modules.extend([\n            nn.BatchNorm2d(layer_channels[layer_index]),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(layer_channels[layer_index], layer_channels[layer_index + 1], kernel_size=2, stride=2, padding=0, bias=False),\n        ])\n    return nn.Sequential(*modules)\n'"
concern/__init__.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# File              : __init__.py\n# Author            : Zhaoyi Wan <wanzhaoyi@megvii.com>\n# Date              : 21.11.2018\n# Last Modified Date: 08.01.2019\n# Last Modified By  : Zhaoyi Wan <wanzhaoyi@megvii.com>\n\nfrom .log import Logger\nfrom .average_meter import AverageMeter\nfrom .visualizer import Visualize\nfrom .box2seg import resize_with_coordinates, box2seg\nfrom .convert import convert\n'"
concern/average_meter.py,0,"b'class AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        return self\n'"
concern/box2seg.py,0,"b""import cv2\nimport numpy as np\nfrom scipy import interpolate\n\ndef intersection(x, p1, p2):\n    x1, y1 = p1\n    x2, y2 = p2\n    if x2 == x1:\n        return 0\n    k = (x - x1) / (x2 - x1)\n    return k * (y2 - y1) + y1\n\n\ndef midpoint(p1, p2, typed=float):\n    return [typed((p1[0] + p2[0]) / 2), typed((p1[1] + p2[1]) / 2)]\n\n\ndef resize_with_coordinates(image, width, height, coordinates):\n    original_height, original_width = image.shape[:2]\n    resized_image = cv2.resize(image, (width, height))\n    if coordinates is not None:\n        assert coordinates.ndim == 2\n        assert coordinates.shape[-1] == 2\n\n        rate_x = width / original_width\n        rate_y = height / original_height\n\n        coordinates = coordinates * (rate_x, rate_y)\n    return resized_image, coordinates\n\n\ndef box2seg(image, boxes, label):\n    height, width = image.shape[:2]\n    mask = np.zeros((height, width), dtype=np.float32)\n    seg = np.zeros((height, width), dtype=np.float32)\n    points = []\n    for box_index in range(boxes.shape[0]):\n        box = boxes[box_index, :, :] # 4x2\n        left_top = box[0]\n        right_top = box[1]\n        right_bottom = box[2]\n        left_bottom = box[3]\n\n        left = [(left_top[0] + left_bottom[0]) / 2, (left_top[1] + left_bottom[1]) / 2]\n        right = [(right_top[0] + right_bottom[0]) / 2, (right_top[1] + right_bottom[1]) / 2]\n\n        center = midpoint(left, right)\n        points.append(midpoint(left, center))\n        points.append(midpoint(right, center))\n\n        poly = np.array([midpoint(left_top, center),\n            midpoint(right_top, center),\n            midpoint(right_bottom, center),\n            midpoint(left_bottom, center)\n            ])\n        seg = cv2.fillPoly(seg, [poly.reshape(4, 1, 2).astype(np.int32)], int(label[box_index]))\n\n    left_y = intersection(0, points[0], points[1])\n    right_y = intersection(width, points[-1], points[-2])\n    points.insert(0, [0, left_y])\n    points.append([width, right_y])\n    points = np.array(points)\n\n    f = interpolate.interp1d(points[:, 0], points[:, 1], fill_value='extrapolate')\n    xnew = np.arange(0, width, 1)\n    ynew = f(xnew).clip(0, height-1)\n    for x in range(width - 1):\n        mask[int(ynew[x]), x] = 1\n    return ynew.reshape(1, -1).round(), seg\n"""
concern/charset_tool.py,0,"b'\n""""""\xe5\x88\xa4\xe6\x96\xad\xe4\xb8\x80\xe4\xb8\xaaunicode\xe6\x98\xaf\xe5\x90\xa6\xe6\x98\xaf\xe6\xb1\x89\xe5\xad\x97""""""\n\n\ndef is_chinese(uchar):\n    if uchar >= u\'\\u4e00\' and uchar <= u\'\\u9fa5\':\n        return True\n    else:\n        return False\n\n\n""""""\xe5\x88\xa4\xe6\x96\xad\xe4\xb8\x80\xe4\xb8\xaaunicode\xe6\x98\xaf\xe5\x90\xa6\xe6\x98\xaf\xe6\x95\xb0\xe5\xad\x97""""""\n\n\ndef is_number(uchar):\n    if uchar >= u\'\\u0030\' and uchar <= u\'\\u0039\':\n        return True\n    else:\n        return False\n\n\n""""""\xe5\x88\xa4\xe6\x96\xad\xe4\xb8\x80\xe4\xb8\xaaunicode\xe6\x98\xaf\xe5\x90\xa6\xe6\x98\xaf\xe8\x8b\xb1\xe6\x96\x87\xe5\xad\x97\xe6\xaf\x8d""""""\n\n\ndef is_alphabet(uchar):\n    if (uchar >= u\'\\u0041\' and uchar <= u\'\\u005a\') or (uchar >= u\'\\u0061\' and uchar <= u\'\\u007a\'):\n        return True\n    else:\n        return False\n\n\n""""""\xe5\x88\xa4\xe6\x96\xad\xe6\x98\xaf\xe5\x90\xa6\xe6\x98\xaf\xef\xbc\x88\xe6\xb1\x89\xe5\xad\x97\xef\xbc\x8c\xe6\x95\xb0\xe5\xad\x97\xe5\x92\x8c\xe8\x8b\xb1\xe6\x96\x87\xe5\xad\x97\xe7\xac\xa6\xe4\xb9\x8b\xe5\xa4\x96\xe7\x9a\x84\xef\xbc\x89\xe5\x85\xb6\xe4\xbb\x96\xe5\xad\x97\xe7\xac\xa6""""""\n\n\ndef is_other(uchar):\n    if not (is_chinese(uchar) or is_number(uchar) or is_alphabet(uchar)):\n        return True\n    else:\n        return False\n\n\n""""""\xe5\x8d\x8a\xe8\xa7\x92\xe8\xbd\xac\xe5\x85\xa8\xe8\xa7\x92""""""\n\n\ndef B2Q(uchar):\n    inside_code = ord(uchar)\n    if inside_code < 0x0020 or inside_code > 0x7e:  # \xe4\xb8\x8d\xe6\x98\xaf\xe5\x8d\x8a\xe8\xa7\x92\xe5\xad\x97\xe7\xac\xa6\xe5\xb0\xb1\xe8\xbf\x94\xe5\x9b\x9e\xe5\x8e\x9f\xe6\x9d\xa5\xe7\x9a\x84\xe5\xad\x97\xe7\xac\xa6\n        return uchar\n    if inside_code == 0x0020:  # \xe9\x99\xa4\xe4\xba\x86\xe7\xa9\xba\xe6\xa0\xbc\xe5\x85\xb6\xe4\xbb\x96\xe7\x9a\x84\xe5\x85\xa8\xe8\xa7\x92\xe5\x8d\x8a\xe8\xa7\x92\xe7\x9a\x84\xe5\x85\xac\xe5\xbc\x8f\xe4\xb8\xba:\xe5\x8d\x8a\xe8\xa7\x92=\xe5\x85\xa8\xe8\xa7\x92-0xfee0\n        inside_code = 0x3000\n    else:\n        inside_code += 0xfee0\n    return chr(inside_code)\n\n\n""""""\xe5\x85\xa8\xe8\xa7\x92\xe8\xbd\xac\xe5\x8d\x8a\xe8\xa7\x92""""""\n\n\ndef Q2B(uchar):\n    inside_code = ord(uchar)\n    if inside_code == 0x3000:\n        inside_code = 0x0020\n    else:\n        inside_code -= 0xfee0\n    if inside_code < 0x0020 or inside_code > 0x7e:  # \xe8\xbd\xac\xe5\xae\x8c\xe4\xb9\x8b\xe5\x90\x8e\xe4\xb8\x8d\xe6\x98\xaf\xe5\x8d\x8a\xe8\xa7\x92\xe5\xad\x97\xe7\xac\xa6\xe8\xbf\x94\xe5\x9b\x9e\xe5\x8e\x9f\xe6\x9d\xa5\xe7\x9a\x84\xe5\xad\x97\xe7\xac\xa6\n        return uchar\n    return chr(inside_code)\n\n\n""""""\xe6\x8a\x8a\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe5\x85\xa8\xe8\xa7\x92\xe8\xbd\xac\xe5\x8d\x8a\xe8\xa7\x92""""""\n\n\ndef stringQ2B(ustring):\n    return """".join([Q2B(uchar) for uchar in ustring])\n\n\n""""""\xe5\xb0\x86UTF-8\xe7\xbc\x96\xe7\xa0\x81\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbaUnicode\xe7\xbc\x96\xe7\xa0\x81""""""\n\n\ndef convert_toUnicode(string):\n    # if not isinstance(string, unicode):\n    ustring = string.decode(\'UTF-8\')\n'"
concern/charsets.py,0,"b'import config\nfrom sortedcontainers import SortedSet\nimport string\nimport numpy as np\n\nfrom .config import Configurable, State\n\n\nclass Charset(Configurable):\n    corups = State(default=string.hexdigits)\n    blank = State(default=0)\n    unknown = State(default=1)\n    blank_char = State(\'\\t\')\n    unknown_char = State(\'\\n\')\n    case_sensitive = State(default=False)\n\n    def __init__(self, corups=None, cmd={}, **kwargs):\n        self.load_all(**kwargs)\n\n        self._corpus = SortedSet(self._filter_corpus(corups))\n        if self.blank_char in self._corpus:\n            self._corpus.remove(self.blank_char)\n        if self.unknown_char in self._corpus:\n            self._corpus.remove(self.unknown_char)\n        self._charset = list(self._corpus)\n        self._charset.insert(self.blank, self.blank_char)\n        self._charset.insert(self.unknown, self.unknown_char)\n        self._charset_lut = {char: index\n                             for index, char in enumerate(self._charset)}\n\n    def _filter_corpus(self, corups):\n        return corups\n\n    def __getitem__(self, index):\n        return self._charset[index]\n\n    def index(self, x):\n        target = x\n        if not self.case_sensitive:\n            target = target.upper()\n        return self._charset_lut.get(target, self.unknown)\n\n    def is_empty(self, index):\n        return index == self.blank or index == self.unknown\n\n    def is_empty_char(self, x):\n        return x == self.blank_char or x == self.unknown_char\n\n    def __len__(self):\n        return len(self._charset)\n\n    def string_to_label(self, string_input, max_size=32):\n        length = max(max_size, len(string_input))\n        target = np.zeros((length, ), dtype=np.int32)\n        for index, c in enumerate(string_input):\n            value = self.index(c)\n            target[index] = value\n        return target\n\n    def label_to_string(self, label):\n        ingnore = [self.unknown, self.blank]\n        return """".join([self._charset[i] for i in label if i not in ingnore])\n\n\nclass ChineseCharset(Charset):\n\n    def __init__(self, cmd={}, **kwargs):\n        with open(\'./assets/chinese_charset.dic\') as reader:\n            corups = reader.read().strip()\n        super().__init__(corups, cmd, **kwargs)\n\n    def _filter_corpus(self, iterable):\n        corups = []\n        for char in iterable:\n            if not self.case_sensitive:\n                char = char.upper()\n            corups.append(char)\n        return corups\n\n\nclass ChineseOnlyCharset(ChineseCharset):\n    def __init__(self, cmd={}, **kwargs):\n        self.upper_range = (ord(\'A\'), ord(\'Z\'))\n        self.lower_range = (ord(\'a\'), ord(\'z\'))\n        super().__init__(cmd, **kwargs)\n\n    def _filter_corpus(self, iterable):\n        corups = []\n        for char in iterable:\n            if not self.is_english(char):\n                corups.append(char)\n        return corups\n\n    def is_english(self, char):\n        def between(x, lower, higher):\n            return lower <= x <= higher\n        return between(ord(char), *self.lower_range) or\\\n            between(ord(char), *self.upper_range)\n\n\nclass EnglishCharset(Charset):\n    def __init__(self, cmd={}, **kwargs):\n        corups = string.digits + string.ascii_uppercase\n        super().__init__(corups, cmd, **kwargs)\n\n\nclass EnglishPrintableCharset(Charset):\n    def __init__(self, cmd={}, **kwargs):\n        corups = string.digits + string.ascii_letters + string.punctuation\n        super().__init__(corups, cmd, **kwargs)\n\n\nDefaultCharset = EnglishCharset\n'"
concern/config.py,0,"b""import importlib\nfrom collections import OrderedDict\n\nimport anyconfig\nimport munch\n\n\nclass Config(object):\n    def __init__(self):\n        pass\n\n    def load(self, conf):\n        conf = anyconfig.load(conf)\n        return munch.munchify(conf)\n\n    def compile(self, conf, return_packages=False):\n        packages = conf.get('package', [])\n        defines = {}\n\n        for path in conf.get('import', []):\n            parent_conf = self.load(path)\n            parent_packages, parent_defines = self.compile(\n                parent_conf, return_packages=True)\n            packages.extend(parent_packages)\n            defines.update(parent_defines)\n\n        modules = []\n        for package in packages:\n            module = importlib.import_module(package)\n            modules.append(module)\n\n        if isinstance(conf['define'], dict):\n            conf['define'] = [conf['define']]\n\n        for define in conf['define']:\n            name = define.copy().pop('name')\n\n            if not isinstance(name, str):\n                raise RuntimeError('name must be str')\n\n            defines[name] = self.compile_conf(define, defines, modules)\n\n        if return_packages:\n            return packages, defines\n        else:\n            return defines\n\n    def compile_conf(self, conf, defines, modules):\n        if isinstance(conf, (int, float)):\n            return conf\n        elif isinstance(conf, str):\n            if conf.startswith('^'):\n                return defines[conf[1:]]\n            if conf.startswith('$'):\n                return {'class': self.find_class_in_modules(conf[1:], modules)}\n            return conf\n        elif isinstance(conf, dict):\n            if 'class' in conf:\n                conf['class'] = self.find_class_in_modules(\n                    conf['class'], modules)\n            if 'base' in conf:\n                base = conf.copy().pop('base')\n\n                if not isinstance(base, str):\n                    raise RuntimeError('base must be str')\n\n                conf = {\n                    **defines[base],\n                    **conf,\n                }\n            return {key: self.compile_conf(value, defines, modules) for key, value in conf.items()}\n        elif isinstance(conf, (list, tuple)):\n            return [self.compile_conf(value, defines, modules) for value in conf]\n        else:\n            return conf\n\n    def find_class_in_modules(self, cls, modules):\n        if not isinstance(cls, str):\n            raise RuntimeError('class name must be str')\n\n        if cls.find('.') != -1:\n            package, cls = cls.rsplit('.', 1)\n            module = importlib.import_module(package)\n            if hasattr(module, cls):\n                return module.__name__ + '.' + cls\n\n        for module in modules:\n            if hasattr(module, cls):\n                return module.__name__ + '.' + cls\n        raise RuntimeError('class not found ' + cls)\n\n\nclass State:\n    def __init__(self, cmd_key=None, autoload=True, default=None):\n        self.autoload = autoload\n        self.default = default\n        self.cmd_key = cmd_key\n\n\nclass StateMeta(type):\n    def __new__(mcs, name, bases, attrs):\n        current_states = []\n        for key, value in attrs.items():\n            if isinstance(value, State):\n                current_states.append((key, value))\n\n        current_states.sort(key=lambda x: x[0])\n        attrs['states'] = OrderedDict(current_states)\n        new_class = super(StateMeta, mcs).__new__(mcs, name, bases, attrs)\n\n        # Walk through the MRO\n        states = OrderedDict()\n        for base in reversed(new_class.__mro__):\n            if hasattr(base, 'states'):\n                states.update(base.states)\n        new_class.states = states\n\n        for key, value in states.items():\n            setattr(new_class, key, value.default)\n\n        return new_class\n\n\nclass Configurable(metaclass=StateMeta):\n    def __init__(self, *args, cmd={}, **kwargs):\n        self.load_all(cmd=cmd, **kwargs)\n\n    @staticmethod\n    def construct_class_from_config(args):\n        cls = Configurable.extract_class_from_args(args)\n        return cls(**args)\n\n    @staticmethod\n    def extract_class_from_args(args):\n        cls = args.copy().pop('class')\n        package, cls = cls.rsplit('.', 1)\n        module = importlib.import_module(package)\n        cls = getattr(module, cls)\n        return cls\n\n    def load_all(self, cmd={}, **kwargs):\n        for name, state in self.states.items():\n            if state.cmd_key is not None and name in cmd:\n                setattr(self, name, cmd[name])\n            elif state.autoload:\n                self.load(name, cmd=cmd, **kwargs)\n\n    def load(self, state_name, **kwargs):\n        # FIXME: kwargs should be filtered\n        # Args passed from command line\n        cmd = kwargs.pop('cmd', dict())\n        if state_name in kwargs:\n            setattr(self, state_name, self.create_member_from_config(\n                (kwargs[state_name], cmd)))\n        else:\n            setattr(self, state_name, self.states[state_name].default)\n\n    def create_member_from_config(self, conf):\n        args, cmd = conf\n        if args is None or isinstance(args, (int, float, str)):\n            return args\n        elif isinstance(args, (list, tuple)):\n            return [self.create_member_from_config((subargs, cmd)) for subargs in args]\n        elif isinstance(args, dict):\n            if 'class' in args:\n                cls = self.extract_class_from_args(args)\n                return cls(**args, cmd=cmd)\n            return {key: self.create_member_from_config((subargs, cmd)) for key, subargs in args.items()}\n        else:\n            return args\n\n    def dump(self):\n        state = {}\n        state['class'] = self.__class__.__module__ + \\\n            '.' + self.__class__.__name__\n        for name, value in self.states.items():\n            obj = getattr(self, name)\n            state[name] = self.dump_obj(obj)\n        return state\n\n    def dump_obj(self, obj):\n        if obj is None:\n            return None\n        elif hasattr(obj, 'dump'):\n            return obj.dump()\n        elif isinstance(obj, (int, float, str)):\n            return obj\n        elif isinstance(obj, (list, tuple)):\n            return [self.dump_obj(value) for value in obj]\n        elif isinstance(obj, dict):\n            return {key: self.dump_obj(value) for key, value in obj.items()}\n        else:\n            return str(obj)\n"""
concern/convert.py,0,"b""from PIL import Image\nimport cv2\nimport base64\nimport io\nimport numpy as np\n\n\ndef convert(data):\n    if isinstance(data, dict):\n        ndata = {}\n        for key, value in data.items():\n            nkey = key.decode()\n            if nkey == 'img':\n                img = Image.open(io.BytesIO(value))\n                img = img.convert('RGB')\n                img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n                nvalue = img\n            else:\n                nvalue = convert(value)\n            ndata[nkey] = nvalue\n        return ndata\n    elif isinstance(data, list):\n        return [convert(item) for item in data]\n    elif isinstance(data, bytes):\n        return data.decode()\n    else:\n        return data\n\n\ndef to_np(x):\n    return x.cpu().data.numpy()\n"""
concern/cv.py,0,"b""import cv2\nimport numpy as np\n\n\ndef min_area_rect(poly):\n    poly = cv2.minAreaRect(np.array(poly, 'float32'))\n    if poly[2] < -45:\n        poly = (poly[0], poly[1], poly[2] + 180)\n    else:\n        poly = (poly[0], poly[1][::-1], poly[2] + 90)\n    poly = cv2.boxPoints(poly)\n    return poly\n"""
concern/distributed.py,14,"b'import sys\nimport functools\nimport torch\nimport torch.distributed as dist\nfrom concern.average_meter import AverageMeter\nfrom collections import defaultdict\nimport pickle\n\n\ndef do(function_name, value):\n    if world_size() < 2:\n        return value\n\n    function = getattr(sys.modules[__name__], function_name + \'_single\')\n    if isinstance(value, (list, tuple)):\n        return do_list(function, value)\n    elif isinstance(value, dict):\n        return do_dict(function, value)\n    return function(value)\n\n\nreduce = functools.partial(do, \'reduce\')\n#gather = functools.partial(do, \'gather\')\n\n\ndef get_average_meter(function, item):\n    meter_avg = torch.tensor(item.avg).reshape(1,).cuda()\n    meter_count = torch.tensor(item.count).reshape(1,).cuda()\n    meter_count = function(meter_count)\n    meter_avg = function(meter_avg)\n    metrics = AverageMeter()\n    if is_main():\n        for i in range(meter_avg.shape[0]):\n            metrics.update(meter_avg[i].double().item(),\n                           meter_count[i].int().item())\n    return metrics\n\n\ndef do_list(function, value):\n    new_list = []\n    for item in value:\n        if isinstance(item, list):\n            item = do_list(function, item)\n        elif isinstance(item, dict):\n            item = do_dict(function, item)\n        elif isinstance(item, torch.Tensor):\n            item = item.cuda()\n            item = function(item)\n        elif isinstance(item, AverageMeter):\n            item = get_average_meter(item)\n        new_list.append(item)\n    return new_list\n\n\ndef do_dict(function, value):\n    new_dict = dict()\n    for key, item in value.items():\n        if isinstance(item, list):\n            print(\'key %s len %d value %s\' % (key, len(item), item))\n            item = do_list(function, item)\n            print(\'key %s value finished %s\' % (key, item))\n        elif isinstance(item, dict):\n            item = do_dict(function, item)\n        elif isinstance(item, torch.Tensor):\n            item = item.cuda()\n            item = function(item)\n        elif isinstance(item, AverageMeter):\n            item = get_average_meter(function, item)\n        new_dict[key] = item\n    return new_dict\n\n\ndef gather_single(value):\n    gathered = [torch.zeros_like(value) for _ in range(dist.get_world_size())]\n    dist.all_gather(gathered, value)\n    return torch.cat(gathered, dim=0)\n\n\ndef reduce_single(value):\n    dist.reduce(value, dst=0, op=dist.ReduceOp.SUM)\n    return value / world_size()\n\n\ndef is_main():\n    if not dist.is_available():\n        return True\n    if not dist.is_initialized():\n        return True\n    return dist.get_rank() == 0\n\n\ndef is_distributed():\n    if not dist.is_available():\n        return False\n    return dist.is_initialized()\n\n\ndef world_size():\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef merge_dict_list(dict_list):\n    """"""\n    merge dict_list into one dict\n    Args:\n        dict_list: list of dict\n    Returns:\n        merged_dict:\n    """"""\n    assert isinstance(dict_list, list), \'dict_list must be list\'\n    assert len(dict_list) > 0, \'dict_list must have at least one element\'\n    merged_dict = defaultdict(list)\n    for dic in dict_list:\n        for key, value in dic.items():\n            merged_dict[key].append(value)\n    return merged_dict\n\n\ndef all_gather(data):\n    """"""\n    Run all_gather on arbitrary picklable data (not necessarily tensors)\n    Args:\n        data: any picklable object\n    Returns:\n        list[data]: list of data gathered from each rank\n    """"""\n    world_size = world_size()\n    if world_size == 1:\n        return [data]\n\n    # serialized to a Tensor\n    buffer = pickle.dumps(data)\n    storage = torch.ByteStorage.from_buffer(buffer)\n    tensor = torch.ByteTensor(storage).to(""cuda"")\n\n    # obtain Tensor size of each rank\n    local_size = torch.LongTensor([tensor.numel()]).to(""cuda"")\n    size_list = [torch.LongTensor([0]).to(""cuda"") for _ in range(world_size)]\n    dist.all_gather(size_list, local_size)\n    size_list = [int(size.item()) for size in size_list]\n    max_size = max(size_list)\n\n    # receiving Tensor from all ranks\n    # we pad the tensor because torch all_gather does not support\n    # gathering tensors of different shapes\n    tensor_list = []\n    for _ in size_list:\n        tensor_list.append(torch.ByteTensor(size=(max_size,)).to(""cuda""))\n    if local_size != max_size:\n        padding = torch.ByteTensor(size=(max_size - local_size,)).to(""cuda"")\n        tensor = torch.cat((tensor, padding), dim=0)\n    dist.all_gather(tensor_list, tensor)\n\n    data_list = []\n    for size, tensor in zip(size_list, tensor_list):\n        buffer = tensor.cpu().numpy().tobytes()[:size]\n        data_list.append(pickle.loads(buffer))\n\n    return data_list\n\n\ndef gather(data):\n    dict_list = all_gather(data)\n    return merge_dict_list(dict_list)\n'"
concern/log.py,0,"b'import os\nimport logging\nimport functools\nimport time\nfrom datetime import datetime\n\nfrom tensorboardX import SummaryWriter\nimport yaml\nimport cv2\nimport numpy as np\n\nfrom concern.config import Configurable, State\nimport config\n\n\nclass Log:\n\n    @classmethod\n    def instance(cls, ins):\n        cls.__instance = ins\n\n    @classmethod\n    def __getattr__(cls, name):\n        if cls.__instance is not None:\n            return cls.__instance.__getattribute__(name)\n        else:\n            return logging.info\n\n\nclass Logger(Configurable):\n    SUMMARY_DIR_NAME = \'summaries\'\n    VISUALIZE_NAME = \'visualize\'\n    LOG_FILE_NAME = \'output.log\'\n    ARGS_FILE_NAME = \'args.log\'\n    METRICS_FILE_NAME = \'metrics.log\'\n\n    database_dir = State(default=config.db_path)\n    log_dir = State(default=\'workspace\')\n    verbose = State(default=False)\n    level = State(default=\'info\')\n    log_interval = State(default=100)\n    local_rank = State(default=0, cmd_key=\'local_rank\')\n\n    __instance = None\n\n    def __new__(cls, *args, **kwargs):\n        if cls.__instance is None:\n            instance = super(Logger, cls).__new__(cls)\n            cls.__instance = instance\n        return cls.__instance\n\n    def __init__(self, cmd={}, **kwargs):\n        self.load_all(cmd=cmd, **kwargs)\n\n        self._make_storage()\n\n        self.name = cmd[\'name\']\n        self.log_dir = os.path.join(self.log_dir, self.name)\n        self.verbose = cmd[\'verbose\']\n        if cmd[\'debug\']:\n            self.log_interval = 1\n        if self.verbose:\n            print(\'Initializing log dir for\', self.log_dir)\n\n        while not os.path.exists(self.log_dir):\n            if self.local_rank == 0:\n                os.makedirs(self.log_dir)\n            else:\n                time.sleep(1)\n\n        self.message_logger = self._init_message_logger()\n\n        summary_path = os.path.join(self.log_dir, self.SUMMARY_DIR_NAME)\n        if self.local_rank == 0:\n            self.tf_board_logger = SummaryWriter(summary_path)\n        else:\n            self.tf_board_logger = None\n\n        self.metrics_writer = open(os.path.join(\n            self.log_dir, self.METRICS_FILE_NAME), \'at\')\n\n        self.timestamp = time.time()\n        self.logged = -1\n        self.speed = None\n        self.eta_time = None\n\n    def _make_storage(self):\n        application = os.path.basename(os.getcwd())\n        storage_dir = os.path.join(\n            self.database_dir, self.log_dir, application)\n        while not os.path.exists(storage_dir):\n            if self.local_rank == 0:\n                os.makedirs(storage_dir)\n            else:\n                time.sleep(1)\n\n        while not os.path.exists(self.log_dir):\n            if self.local_rank == 0:\n                os.symlink(storage_dir, self.log_dir)\n            else:\n                time.sleep(1)\n\n    def save_dir(self, dir_name):\n        return os.path.join(self.log_dir, dir_name)\n\n    def _init_message_logger(self):\n        message_logger = logging.getLogger(\'messages\')\n        message_logger.setLevel(\n            logging.DEBUG if self.verbose else logging.INFO)\n\n        formatter = logging.Formatter(\n            \'[%(levelname)s{}] [%(asctime)s] %(message)s\'.format(self.local_rank))\n        std_handler = logging.StreamHandler()\n        std_handler.setLevel(message_logger.level)\n        std_handler.setFormatter(formatter)\n\n        file_handler = logging.FileHandler(\n            os.path.join(self.log_dir, self.LOG_FILE_NAME))\n        file_handler.setLevel(message_logger.level)\n        file_handler.setFormatter(formatter)\n\n        message_logger.addHandler(std_handler)\n        message_logger.addHandler(file_handler)\n        return message_logger\n\n    def report_time(self, name: str):\n        if self.verbose:\n            self.info(name + "" time :"" + str(time.time() - self.timestamp))\n            self.timestamp = time.time()\n\n    def report_eta(self, steps, total, epoch):\n        if self.local_rank > 0:\n            return\n\n        self.logged = self.logged % total + 1\n        steps = steps % total\n        if self.eta_time is None:\n            self.eta_time = time.time()\n            speed = -1\n        else:\n            eta_time = time.time()\n            speed = eta_time - self.eta_time\n            if self.speed is not None:\n                speed = ((self.logged - 1) * self.speed + speed) / self.logged\n            self.speed = speed\n            self.eta_time = eta_time\n\n        seconds = (total - steps) * speed\n        hours = seconds // 3600\n        minutes = (seconds - (hours * 3600)) // 60\n        seconds = seconds % 60\n\n        print(\'%d/%d batches processed in epoch %d, ETA: %2d:%2d:%2d\' %\n              (steps, total, epoch,\n               hours, minutes, seconds), end=\'\\r\')\n\n    def args(self, parameters=None):\n        if parameters is None:\n            with open(os.path.join(self.log_dir, self.ARGS_FILE_NAME), \'rt\') as reader:\n                return yaml.load(reader.read())\n        with open(os.path.join(self.log_dir, self.ARGS_FILE_NAME), \'wt\') as writer:\n            yaml.dump(parameters.dump(), writer)\n\n    def metrics(self, epoch, steps, metrics_dict):\n        results = {}\n        if not self.local_rank == 0:\n            return\n        for name, a in metrics_dict.items():\n            results[name] = {\'count\': a.count, \'value\': float(a.avg)}\n            self.add_scalar(\'metrics/\' + name, a.avg, steps)\n        result_dict = {\n            str(datetime.now()): {\n                \'epoch\': epoch,\n                \'steps\': steps,\n                **results\n            }\n        }\n        string_result = yaml.dump(result_dict)\n        self.info(string_result)\n        self.metrics_writer.write(string_result)\n        self.metrics_writer.flush()\n\n    def named_number(self, name, num=None, default=0):\n        if num is None:\n            return int(self.has_signal(name)) or default\n        else:\n            with open(os.path.join(self.log_dir, name), \'w\') as writer:\n                writer.write(str(num))\n            return num\n\n    epoch = functools.partialmethod(named_number, \'epoch\')\n    iter = functools.partialmethod(named_number, \'iter\')\n\n    def message(self, level, content):\n        self.message_logger.__getattribute__(level)(content)\n\n    def images(self, prefix, image_dict, step):\n        for name, image in image_dict.items():\n            self.add_image(prefix + \'/\' + name, image, step, dataformats=\'HWC\')\n\n    def merge_save_images(self, name, images):\n        for i, image in enumerate(images):\n            if i == 0:\n                result = image\n            else:\n                result = np.concatenate([result, image], 0)\n        cv2.imwrite(os.path.join(self.vis_dir(), name+\'.jpg\'), result)\n\n    def vis_dir(self):\n        vis_dir = os.path.join(self.log_dir, self.VISUALIZE_NAME)\n        if not os.path.exists(vis_dir):\n            os.mkdir(vis_dir)\n        return vis_dir\n\n    def save_image_dict(self, images, max_size=1024):\n        for file_name, image in images.items():\n            height, width = image.shape[:2]\n            if height > width:\n                actual_height = min(height, max_size)\n                actual_width = int(round(actual_height * width / height))\n            else:\n                actual_width = min(width, max_size)\n                actual_height = int(round(actual_width * height / width))\n                image = cv2.resize(image, (actual_width, actual_height))\n            cv2.imwrite(os.path.join(self.vis_dir(), file_name+\'.jpg\'), image)\n\n    def holder(self, *args, **kwargs):\n        pass\n\n    def __getattr__(self, name):\n        message_levels = set([\'debug\', \'info\', \'warning\', \'error\', \'critical\'])\n        if name == \'__setstate__\':\n            raise AttributeError(\'haha\')\n        if name in message_levels:\n            if self.local_rank > 0:\n                return self.holder\n            return functools.partial(self.message, name)\n        elif hasattr(self.__dict__.get(\'tf_board_logger\'), name):\n            if self.local_rank > 0:\n                return self.holder\n            return self.tf_board_logger.__getattribute__(name)\n        else:\n            super()\n'"
concern/nori_reader.py,0,b'import config\n\n\ndef NoriReader(paths=[]):\n    import nori2 as nori\n    from nori2.multi import MultiSourceReader\n    if config.community_version:\n        return MultiSourceReader(paths)\n    else:\n        return nori.Fetcher(paths)\n'
concern/redis_meta.py,0,"b""import os\nimport bisect\n\nimport redis\n\n\nclass RedisMeta:\n    def __init__(self, connection, prefix, key=None):\n        self.connection = connection\n        self._key = key\n        self.prefix = prefix\n\n    def __getitem__(self, key):\n        if self.key() is None:\n            if self.connection.exists(self.key(key)):\n                return type(self)(self.connection, prefix=self.prefix, key=key)\n            raise KeyError\n        return self.connection.lindex(self.key(), key).decode()\n\n    def __contains__(self, key):\n        if self.key() is None:\n            assert isinstance(key, str)\n            return self.connection.exists(self.key(key))\n        else:\n            assert isinstance(key, int)\n            return key < len(self) and key > -len(self) + 1\n\n    def __iter__(self):\n        if self.key() is None:\n            iter(self.keys)\n        else:\n            for i in range(len(self)):\n                yield self.__getitem__(i)\n\n    def __add__(self, another):\n        return ConcateMetaRedisMeta(self, another)\n\n    def get(self, key, default=None):\n        assert self.key() is None\n\n    def key(self, key=None):\n        if key is None:\n            key = self._key\n        if key is None:\n            return None\n        return os.path.join(self.prefix, key)\n\n    def keys(self):\n        assert self.key() is None\n        keys = self.connection.smembers(self.key('__keys__'))\n        return {key.decode() for key in keys}\n\n    def items(self):\n        assert self.key() is None\n        tuples = []\n        for key in self.keys():\n            tuples.append((key, self.__getitem__(key)))\n        return tuples\n\n\n    def __len__(self):\n        if self.key() is None:\n            return len(self.keys())\n        return self.connection.llen(self.key())\n\n\nclass ConcateMetaRedisMeta:\n    def __init__(self, *meta_list):\n        milestones = []\n        start = 0\n        for meta in meta_list:\n            assert meta.key() is not None\n            start += len(meta)\n            milestones.append(start)\n\n        self.milestones = milestones\n        self.num_samples = start\n        self.meta_list = list(meta_list)\n\n    def __getitem__(self, index):\n        meta_index = bisect.bisect(self.milestones, index)\n        if meta_index == 0:\n            real_index = index\n        else:\n            real_index = index - self.milestones[meta_index - 1]\n        return self.meta_list[meta_index][real_index]\n\n    def __len__(self):\n        return self.num_samples\n\n    def __add__(self, another):\n        self.num_samples += len(another)\n        self.milestones.append(self.num_samples)\n        self.meta_list.append(another)\n        return self\n"""
concern/signal_monitor.py,0,"b'import os\n\n\nclass SignalMonitor(object):\n    def __init__(self, file_path):\n        self.file_path = file_path\n\n    def get_signal(self):\n        if self.file_path is None:\n            return None\n        if os.path.exists(self.file_path):\n            with open(self.file_path) as f:\n                data = self.file.read()\n                os.remove(f)\n                return data\n        else:\n            return None\n'"
concern/tensorboard.py,0,b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# File              : concern/tensorboard.py\n# Author            : Zhaoyi Wan <wanzhaoyi@megvii.com>\n# Date              : 03.01.2019\n# Last Modified Date: 03.01.2019\n# Last Modified By  : Zhaoyi Wan <wanzhaoyi@megvii.com>\n'
concern/test_log.py,1,"b""#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# File              : test_log.py\n# Author            : Zhaoyi Wan <wanzhaoyi@megvii.com>\n# Date              : 06.01.2019\n# Last Modified Date: 06.01.2019\n# Last Modified By  : Zhaoyi Wan <wanzhaoyi@megvii.com>\n\nimport argparse\nimport os\nimport torch\nimport torch.utils.data as data\n\nfrom models import Builder\nfrom concern import Log, AverageMeter\nfrom data import NoriDataset\nfrom tqdm import tqdm\nimport time\nimport config\nimport glob\n\nparser = argparse.ArgumentParser(description='Text Recognition Training')\nparser.add_argument('--name', default='', type=str)\nparser.add_argument('--batch_size', default=256, type=int, help='Batch size for training')\nparser.add_argument('--resume', default='', type=str, help='Resume from checkpoint')\nparser.add_argument('--num_workers', default=8, type=int, help='Number of workers used in dataloading')\nparser.add_argument('--iterations', default=1000000, type=int, help='Number of training iterations')\nparser.add_argument('--start_iter', default=0, type=int, help='Begin counting iterations starting from this value (should be used with resume)')\nparser.add_argument('--start_epoch', default=0, type=int, help='Begin counting epoch starting from this value (should be used with resume)')\nparser.add_argument('--use_isilon', default=True, type=bool, help='Use isilon to save logs and checkpoints')\nparser.add_argument('--num_gpus', default=4, type=int, help='number of gpus to use')\nparser.add_argument('--backbone', default='resnet50_fpn', type=str, help='The backbone want to use')\nparser.add_argument('--decoder', default='1d_ctc', type=str, help='The ctc formulation want to use')\nparser.add_argument('--lr', default=1e-3, type=float, help='initial learning rate')\nparser.add_argument('--aug', default=True, type=bool, help='add data augmentation')\nparser.add_argument('--verbose', default=True, type=bool, help='show verbose info')\nparser.add_argument('--validate', action='store_true', dest='validate', help='Validate during training')\nparser.add_argument('--no-validate', action='store_false', dest='validate', help='Validate during training')\nparser.set_defaults(validate=True)\nparser.add_argument('--nori', default='/unsullied/sharefs/_csg_algorithm/OCR/zhangjian02/data/ocr-data/synth-data/SynthText/croped_sorted.nori', type=str, help='nori_file_path')\nparser.add_argument('--test_nori', default='/unsullied/sharefs/wanzhaoyi/data/text-recognition-test/*.nori', type=str, help='nori_file_path for validation')\n\nargs = parser.parse_args()\nname = args.name\nif name == '':\n    name = args.backbone + '-' + args.decoder\nlogger = Log('test_logger', args.use_isilon, args.verbose)\nlogger.args(args)\n\n\nepoch = 0\nfor e in range(3):\n    for i in range(28385):\n        loss = i * 1e-7\n        logger.add_scalar('loss', loss, i + epoch * 28385);\n\n        if i % 100 == 0:\n                logger.info('iter: %6d, epoch: %3d, loss: %.6f, lr: %f' % (i, epoch, loss, 1e-4))\n\n    epoch += 1\n\n"""
concern/textsnake.py,0,"b'import cv2\nimport numpy as np\n\n\ndef fill_hole(input_mask):\n    height, width = input_mask.shape\n    canvas = np.zeros((height + 2, width + 2), np.uint8)\n    canvas[1:height + 1, 1:width + 1] = input_mask.copy()\n\n    mask = np.zeros((height + 4, width + 4), np.uint8)\n\n    cv2.floodFill(canvas, mask, (0, 0), 1)\n    canvas = canvas[1:height + 1, 1:width + 1].astype(np.bool)\n\n    return ~canvas | input_mask.astype(np.uint8)\n\n\ndef regularize_sin_cos(sin, cos):\n    # regularization\n    scale = np.sqrt(1.0 / (sin ** 2 + cos ** 2))\n    return sin * scale, cos * scale\n\n\ndef norm2(x, axis=None):\n    return np.sqrt(np.sum(x ** 2, axis=axis))\n\n\ndef cos(p1, p2):\n    return (p1 * p2).sum() / (norm2(p1) * norm2(p2))\n\n\ndef vector_sin(v):\n    assert len(v) == 2\n    return v[1] / np.sqrt(v[0] ** 2 + v[1] ** 2)\n\n\ndef vector_cos(v):\n    assert len(v) == 2\n    return v[0] / np.sqrt(v[0] ** 2 + v[1] ** 2)\n\n\ndef find_bottom(pts):\n    pts = np.array(pts)\n    if len(pts) > 4:\n        e = np.concatenate([pts, pts[:3]])\n        candidate = []\n        for i in range(1, len(pts) + 1):\n            v_prev = e[i] - e[i - 1]\n            v_next = e[i + 2] - e[i + 1]\n            if cos(v_prev, v_next) < -0.7:\n                candidate.append((i % len(pts), (i + 1) % len(pts), cos(v_prev, v_next)))\n        candidate.sort(key=lambda x: x[2])\n\n        if len(candidate) < 2 or candidate[0][0] == candidate[1][1] or candidate[0][1] == candidate[1][0]:\n            # if candidate number < 2, or two bottom are joined, select 2 farthest edge\n            mid_list = []\n            for i in range(len(pts)):\n                mid_point = (e[i] + e[(i + 1) % len(pts)]) / 2\n                mid_list.append((i, (i + 1) % len(pts), mid_point))\n\n            dist_list = []\n            for i in range(len(pts)):\n                for j in range(len(pts)):\n                    s1, e1, mid1 = mid_list[i]\n                    s2, e2, mid2 = mid_list[j]\n                    dist = norm2(mid1 - mid2)\n                    dist_list.append((s1, e1, s2, e2, dist))\n            bottom_idx = np.argsort([dist for s1, e1, s2, e2, dist in dist_list])[-2:]\n            bottoms = [dist_list[bottom_idx[0]][:2], dist_list[bottom_idx[1]][:2]]\n        else:\n            bottoms = [candidate[0][:2], candidate[1][:2]]\n        if bottoms[0][0] == bottoms[1][1] or bottoms[1][0] == bottoms[0][1]:\n            print(1)\n\n    else:\n        # try:\n        d1 = norm2(pts[1] - pts[0]) + norm2(pts[2] - pts[3])\n        d2 = norm2(pts[2] - pts[1]) + norm2(pts[0] - pts[3])\n        bottoms = [(0, 1), (2, 3)] if d1 < d2 else [(1, 2), (3, 0)]\n        # except:\n        #     print(pts)\n    assert len(bottoms) == 2, \'fewer than 2 bottoms\'\n    return bottoms\n\n\ndef split_long_edges(points, bottoms):\n    """"""\n    Find two long edge sequence of and polygon\n    """"""\n    b1_start, b1_end = bottoms[0]\n    b2_start, b2_end = bottoms[1]\n    n_pts = len(points)\n\n    i = b1_end + 1\n    long_edge_1 = []\n    while i % n_pts != b2_end:\n        long_edge_1.append((i - 1, i))\n        i = (i + 1) % n_pts\n\n    i = b2_end + 1\n    long_edge_2 = []\n    while i % n_pts != b1_end:\n        long_edge_2.append((i - 1, i))\n        i = (i + 1) % n_pts\n    return long_edge_1, long_edge_2\n\n\ndef find_long_edges(points, bottoms):\n    b1_start, b1_end = bottoms[0]\n    b2_start, b2_end = bottoms[1]\n    n_pts = len(points)\n    i = b1_end\n    long_edge_1 = []\n\n    while i != b2_start:\n        start = i\n        end = (i + 1) % n_pts\n        long_edge_1.append((start, end))\n        i = end\n\n    i = b2_end % n_pts\n    long_edge_2 = []\n    while i != b1_start:\n        start = i\n        end = (i + 1) % n_pts\n        long_edge_2.append((start, end))\n        i = end\n    return long_edge_1, long_edge_2\n\n\ndef split_edge_seqence(points, long_edge, n_parts):\n    points = np.array(points)\n\n    edge_length = [norm2(points[e1] - points[e2]) for e1, e2 in long_edge]\n    point_cumsum = np.cumsum([0] + edge_length)\n    total_length = point_cumsum[-1]\n    length_per_part = total_length / n_parts\n\n    # first point\n    cur_node = 0\n    splited_result = []\n\n    for i in range(1, n_parts):\n        cur_end = i * length_per_part\n\n        try:\n            while cur_end > point_cumsum[cur_node + 1]:\n                cur_node += 1\n        except IndexError:\n            print(points)\n\n        e1, e2 = long_edge[cur_node]\n        e1, e2 = points[e1], points[e2]\n\n        end_shift = cur_end - point_cumsum[cur_node]\n        ratio = end_shift / edge_length[cur_node]\n        new_point = e1 + ratio * (e2 - e1)\n        splited_result.append(new_point)\n\n    # add first and last point\n    p_first = points[long_edge[0][0]]\n    p_last = points[long_edge[-1][1]]\n    splited_result = [p_first] + splited_result + [p_last]\n    return np.stack(splited_result)\n'"
concern/visualizer.py,3,"b""import torch\nimport numpy as np\nimport cv2\nimport config\n\n\nclass Visualize:\n    @classmethod\n    def visualize(cls, x):\n        dimension = len(x.shape)\n        if dimension == 2:\n            pass\n        elif dimension == 3:\n            pass\n\n    @classmethod\n    def to_np(cls, x):\n        return x.cpu().data.numpy()\n\n    @classmethod\n    def visualize_weights(cls, tensor, format='HW', normalize=True):\n        if isinstance(tensor, torch.Tensor):\n            x = cls.to_np(tensor.permute(format.index('H'), format.index('W')))\n        else:\n            x = tensor.transpose(format.index('H'), format.index('W'))\n        if normalize:\n            x = (x - x.min()) / (x.max() - x.min())\n        return cv2.applyColorMap((x * 255).astype(np.uint8), cv2.COLORMAP_JET)\n\n    @classmethod\n    def visualize_points(cls, image, tensor, radius=5, normalized=True):\n        if isinstance(tensor, torch.Tensor):\n            points = cls.to_np(tensor)\n        else:\n            points = tensor\n        if normalized:\n            points = points * image.shape[:2][::-1]\n        for i in range(points.shape[0]):\n            color = np.random.randint(\n                0, 255, (3, ), dtype=np.uint8).astype(np.float)\n            image = cv2.circle(image,\n                               tuple(points[i].astype(np.int32).tolist()),\n                               radius, color, thickness=radius//2)\n        return image\n\n    @classmethod\n    def visualize_heatmap(cls, tensor, format='CHW'):\n        if isinstance(tensor, torch.Tensor):\n            x = cls.to_np(tensor.permute(format.index('H'),\n                                         format.index('W'), format.index('C')))\n        else:\n            x = tensor.transpose(\n                format.index('H'), format.index('W'), format.index('C'))\n        canvas = np.zeros((x.shape[0], x.shape[1], 3), dtype=np.float)\n\n        for c in range(0, x.shape[-1]):\n            color = np.random.randint(\n                0, 255, (3, ), dtype=np.uint8).astype(np.float)\n            canvas += np.tile(x[:, :, c], (3, 1, 1)\n                              ).swapaxes(0, 2).swapaxes(1, 0) * color\n\n        canvas = canvas.astype(np.uint8)\n        return canvas\n\n    @classmethod\n    def visualize_classes(cls, x):\n        canvas = np.zeros((x.shape[0], x.shape[1], 3), dtype=np.uint8)\n        for c in range(int(x.max())):\n            color = np.random.randint(\n                0, 255, (3, ), dtype=np.uint8).astype(np.float)\n            canvas[np.where(x == c)] = color\n        return canvas\n\n    @classmethod\n    def visualize_grid(cls, x, y, stride=16, color=(0, 0, 255), canvas=None):\n        h, w = x.shape\n        if canvas is None:\n            canvas = np.zeros((h, w, 3), dtype=np.uint8)\n        # canvas = np.concatenate([canvas, canvas], axis=1)\n        i, j = 0, 0\n        while i < w:\n            j = 0\n            while j < h:\n                canvas = cv2.circle(canvas, (int(x[i, j] * w + 0.5), int(y[i, j] * h + 0.5)), radius=max(stride//4, 1), color=color, thickness=stride//8)\n                j += stride\n            i += stride\n        return canvas\n\n    @classmethod\n    def visualize_rect(cls, canvas, _rect, color=(0, 0, 255)):\n        rect = (_rect + 0.5).astype(np.int32)\n        return cv2.rectangle(canvas, (rect[0], rect[1]), (rect[2], rect[3]), color)\n"""
data/__init__.py,0,b'from .file_dataset import FileDataset\nfrom .mingle_dataset import MingleDataset\nfrom .lmdb_dataset import LMDBDataset'
data/augmenter.py,0,"b""import imgaug\nimport imgaug.augmenters as iaa\n\nfrom concern.config import Configurable, State\n\n\nclass AugmenterBuilder(object):\n    def __init__(self):\n        pass\n\n    def build(self, args, root=True):\n        if args is None:\n            return None\n        elif isinstance(args, (int, float, str)):\n            return args\n        elif isinstance(args, list):\n            if root:\n                sequence = [self.build(value, root=False) for value in args]\n                return iaa.Sequential(sequence)\n            else:\n                return getattr(iaa, args[0])(\n                    *[self.to_tuple_if_list(a) for a in args[1:]])\n        elif isinstance(args, dict):\n            if 'cls' in args:\n                cls = getattr(iaa, args['cls'])\n                return cls(**{k: self.to_tuple_if_list(v) for k, v in args.items() if not k == 'cls'})\n            else:\n                return {key: self.build(value, root=False) for key, value in args.items()}\n        else:\n            raise RuntimeError('unknown augmenter arg: ' + str(args))\n\n    def to_tuple_if_list(self, obj):\n        if isinstance(obj, list):\n            return tuple(obj)\n        return obj\n"""
data/crop_file_dataset.py,2,"b""import torch\nimport torch.utils.data as data\nimport cv2\nimport numpy as np\nimport glob\nimport os\nfrom concern.config import Configurable, State\nfrom concern.cv import min_area_rect\n\nimport config\nfrom concern.charsets import DefaultCharset\nfrom data.processes.resize_image import ResizeImage\n\n\nclass CropFileDataset(data.Dataset, Configurable):\n    file_pattern = State()\n    charset = State(default=DefaultCharset())\n    max_size = State(default=32)\n    image_size = State(default=[64, 512])\n    mode = State(default='resize')\n\n    RGB_MEAN = np.array([122.67891434, 116.66876762, 104.00698793])\n\n    def __init__(self, file_pattern=None, cmd={}, **kwargs):\n        self.load_all(**kwargs)\n        self.file_pattern = file_pattern or self.file_pattern\n        self.resize = ResizeImage(self.image_size, self.mode)\n        self.prepare()\n\n    def prepare(self):\n        self.file_paths = glob.glob(self.file_pattern)\n        assert len(self.file_paths) > 0\n\n        self.gt = []\n        for file_name in self.file_paths:\n            base_name = os.path.basename(file_name)\n            names = base_name.split('_')\n            gt_with_suffix = '_'.join(names[1:])\n            assert gt_with_suffix.endswith('.jpg')\n            gt = gt_with_suffix[:gt_with_suffix.rindex('.jpg')]\n            self.gt.append(gt)\n        self.num_samples = len(self.file_paths)\n        return self\n\n    def is_vertival(self, height, width):\n        return height > width * 1.5\n\n    def ensure_horizontal(self, image):\n        if self.is_vertival(*image.shape[:2]):\n            image = np.flip(np.swapaxes(image, 0, 1), 0)\n        return image\n\n    def __getitem__(self, index, retry=0):\n        if index >= self.num_samples:\n            index = index % self.num_samples\n        file_path = self.file_paths[index]\n        gt = self.gt[index]\n\n        image = cv2.imread(file_path, cv2.IMREAD_COLOR).astype('float32')\n        image = self.ensure_horizontal(image)\n        image = self.resize(image)\n\n        image -= self.RGB_MEAN\n        image /= 255.\n        length = np.array(min(len(gt), self.max_size), dtype=np.int32)\n\n        image = torch.from_numpy(image).permute(2, 0, 1).float()\n        label = self.gt_to_label(gt, image)\n        return image, label, length\n\n    def gt_to_label(self, gt, image=None):\n        return self.charset.string_to_label(gt)[:config.max_size]\n\n    def __len__(self):\n        return self.num_samples\n\n    @classmethod\n    def restore(cls, data):\n        data = data.permute(1, 2, 0).to('cpu').data.numpy()\n        data = data * 255.\n        data += cls.RGB_MEAN\n        return data.astype(np.uint8)\n\n\nclass ImageCropper(Configurable):\n    charset = State(default=DefaultCharset())\n    max_size = State(default=32)\n    image_size = State(default=[64, 512])\n    mode = State(default='resize')\n\n    RGB_MEAN = np.array([122.67891434, 116.66876762, 104.00698793])\n\n    def __init__(self, cmd={}, **kwargs):\n        self.load_all(**kwargs)\n        self.resize = ResizeImage(self.image_size, self.mode)\n\n    def is_vertival(self, height, width):\n        return height > width * 1.5\n\n    def ensure_horizontal(self, image):\n        if self.is_vertival(*image.shape[:2]):\n            image = np.flip(np.swapaxes(image, 0, 1), 0)\n        return image\n\n    def crop(self, image, poly):\n        box = min_area_rect(poly)\n\n        w = np.linalg.norm(box[1] - box[0])\n        h = np.linalg.norm(box[2] - box[1])\n\n        src = box.astype('float32')\n        dst = np.array([(0, 0), (w, 0), (w, h), (0, h)], 'float32')\n        mat = cv2.getPerspectiveTransform(src, dst)\n\n        image = cv2.warpPerspective(image, mat, (w, h))\n        image = image.astype('float32')\n\n        image = self.ensure_horizontal(image)\n        image = self.resize(image)\n\n        image -= self.RGB_MEAN\n        image /= 255.\n\n        return image\n"""
data/data_loader.py,21,"b'import math\nimport bisect\n\nimport imgaug\nimport numpy as np\n\nimport torch\nimport torch.distributed as dist\nfrom torch.utils.data import Sampler, ConcatDataset, BatchSampler\n\nfrom concern.config import Configurable, State\n\n\ndef default_worker_init_fn(worker_id):\n    np.random.seed(worker_id)\n    imgaug.seed(worker_id)\n\n\nclass DataLoader(Configurable, torch.utils.data.DataLoader):\n    dataset = State()\n    batch_size = State(default=256)\n    num_workers = State(default=2)\n    is_train = State(default=True)\n    collect_fn = State(default=None)\n    drop_last = State(default=True)\n    shuffle = State()\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n        if self.collect_fn is None:\n            self.collect_fn = torch.utils.data.dataloader.default_collate\n        cmd = kwargs.get(\'cmd\', {})\n        self.is_train = cmd[\'is_train\']\n        if \'batch_size\' in cmd:\n            self.batch_size = cmd[\'batch_size\']\n        if self.shuffle is None:\n            self.shuffle = self.is_train\n        self.num_workers = cmd.get(\'num_workers\', self.num_workers)\n\n        if cmd.get(\'distributed\'):\n            sampler = torch.utils.data.distributed.DistributedSampler(self.dataset)\n            batch_sampler = BatchSampler(\n                sampler, self.batch_size//dist.get_world_size(), False)\n            torch.utils.data.DataLoader.__init__(\n                self, self.dataset, batch_sampler=batch_sampler,\n                num_workers=self.num_workers, pin_memory=False,\n                collate_fn=self.collect_fn,\n                worker_init_fn=default_worker_init_fn)\n        else:\n            torch.utils.data.DataLoader.__init__(\n                self, self.dataset,\n                batch_size=self.batch_size, num_workers=self.num_workers,\n                drop_last=self.drop_last, shuffle=self.shuffle,\n                pin_memory=True, collate_fn=self.collect_fn,\n                worker_init_fn=default_worker_init_fn)\n        self.collect_fn = str(self.collect_fn)\n\n\nclass SuccessiveRandomSampler(Sampler):\n    \'\'\'Random Sampler that yields sorted data in successive ranges.\n    Args:\n        dataset: Dataset used for sampling.\n    \'\'\'\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.epoch = 0\n\n    def __iter__(self):\n        if self.shuffle:\n            # deterministically shuffle based on epoch\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset)).tolist()\n        else:\n            indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        indices += indices[: (self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset: offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n\n\nclass DistributedSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\n                    ""Requires distributed package to be available"")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\n                    ""Requires distributed package to be available"")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n\n        self.num_samples = int(\n            math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        if self.shuffle:\n            # deterministically shuffle based on epoch\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset)).tolist()\n        else:\n            indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        indices += indices[: (self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset: offset + self.num_samples]\n        assert len(indices) == self.num_samples\n        self.dataset.select(indices)\n\n        return iter(torch.arange(self.num_samples).tolist())\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n\n\nclass InfiniteOrderedSampler(Sampler):\n    def __init__(self, data_source, limit_size):\n        self.data_source = data_source\n        self.limit_size = limit_size\n\n    def __iter__(self):\n        n = len(self.data_source)\n\n        def wrapper():\n            cnt = 0\n            while cnt < self.limit_size:\n                if cnt % n == 0:\n                    idx = torch.randperm(n).tolist()\n                yield idx[cnt % n]\n                cnt += 1\n        return wrapper()\n\n    def __len__(self):\n        return self.limit_size\n\n\nclass InfiniteDataLoader(Configurable, torch.utils.data.DataLoader):\n    dataset = State()\n    batch_size = State(default=256)\n    num_workers = State(default=10)\n    limit_size = State(default=2 ** 31)\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n        cmd = kwargs[\'cmd\']\n        if \'batch_size\' in cmd:\n            self.batch_size = cmd[\'batch_size\']\n\n        sampler = InfiniteOrderedSampler(self.dataset, self.limit_size)\n\n        torch.utils.data.DataLoader.__init__(\n            self, self.dataset,\n            batch_size=self.batch_size, num_workers=self.num_workers,\n            sampler=sampler, worker_init_fn=default_worker_init_fn,\n        )\n\n\nclass RandomSampleSampler(Sampler):\n    def __init__(self, data_source, weights=None, size=2 ** 31):\n        self.data_source = data_source\n        if weights is None:\n            self.probabilities = np.full(len(data_source), 1 / len(data_source))\n        else:\n            self.probabilities = np.array(weights) / np.sum(weights)\n        self.cum_prob = np.cumsum(self.probabilities)\n        self.size = size\n\n    def __iter__(self):\n        def wrapper():\n            for i in range(self.size):\n                yield bisect.bisect(self.cum_prob, torch.rand(1)[0], hi=len(self.data_source) - 1)\n        return wrapper()\n\n    def __len__(self):\n        return self.size\n\n\nclass RandomSampleDataLoader(Configurable, torch.utils.data.DataLoader):\n    datasets = State()\n    weights = State()\n    batch_size = State(default=256)\n    num_workers = State(default=10)\n    size = State(default=2 ** 31)\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n        cmd = kwargs[\'cmd\']\n        if \'batch_size\' in cmd:\n            self.batch_size = cmd[\'batch_size\']\n\n        probs = []\n        for dataset, weight in zip(self.datasets, self.weights):\n            probs.append(np.full(len(dataset), weight / len(dataset)))\n\n        dataset = ConcatDataset(self.datasets)\n        probs = np.concatenate(probs)\n        assert(len(dataset) == len(probs))\n\n        sampler = RandomSampleSampler(dataset, probs, self.size)\n\n        torch.utils.data.DataLoader.__init__(\n            self, dataset,\n            batch_size=self.batch_size, num_workers=self.num_workers,\n            sampler=sampler, worker_init_fn=default_worker_init_fn,\n        )\n'"
data/dataset.py,1,"b""import glob\nfrom torch.utils.data import Dataset as TorchDataset\n\nfrom concern.config import Configurable, State\n\n\nclass Dataset(TorchDataset, Configurable):\n    r'''Dataset reading data.\n    Args:\n        meta_loader: Instance of Metaloader, which is defined in data/meta_loaders/meta_loader.py,\n            for decoding annotation from packed files.\n        unpack: Callable instance which unpack data from packed bytes such as pickle or msgpack.\n            If not provided, the `default_unpack` will be invoked.\n        Processes: A series of Callable object, which accept as parameter and return the data dict,\n            typically inherrited the `DataProcess`(data/processes/data_process.py) class.\n    '''\n    meta_loader = State()\n    unpack = State(default=None)\n    split = State(default=1)\n\n    processes = State(default=[])\n\n    def prepare_meta_single(self, path_name):\n        return self.meta_loader.load_meta(path_name)\n\n    def list_or_pattern(self, path):\n        if isinstance(path, str):\n            if '*' in path:\n                return glob.glob(path)\n            else:\n                return [path]\n        else:\n            return path\n\n    def __getitem__(self, index, retry=0):\n        if index >= self.num_samples:\n            index = index % self.num_samples\n\n        data_id = self.data_ids[index]\n        meta = dict()\n\n        for key in self.meta:\n            meta[key] = self.meta[key][index]\n\n        try:\n            data = self.unpack(data_id, meta)\n            if self.processes is not None:\n                for data_process in self.processes:\n                    data = data_process(data)\n        except Exception as e:\n            if self.debug or retry > 10:\n                raise e\n            return self.__getitem__(index + 1, retry=retry+1)\n        return data\n\n    def truncate(self, rank, total):\n        clip_size = self.num_samples // total\n        start = rank * clip_size\n        if rank == total - 1:\n            ending = self.num_samples\n        else:\n            ending = start + clip_size\n        new_data_ids = self.data_ids[start:ending]\n        del self.data_ids\n        self.data_ids = new_data_ids\n\n        new_meta = dict()\n        for key in self.meta.keys():\n            new_meta[key] = self.meta[key][start:ending]\n        for key in new_meta.keys():\n            del self.meta[key]\n        self.meta = new_meta\n        self.num_samples = len(self.data_ids)\n        self.truncated = True\n\n    def select(self, indices):\n        if self.truncated:\n            return\n        new_data_ids = [self.data_ids[i] for i in indices]\n        del self.data_ids\n        self.data_ids = new_data_ids\n\n        new_meta = {\n            key: [self.meta[key][i] for i in indices] for key in self.meta.keys()\n        }\n        del self.meta\n        self.meta = new_meta\n        # self.num_samples = len(self.data_ids)\n        self.truncated = True\n\n    def __len__(self):\n        if self.debug:\n            return 512000\n        return self.num_samples // self.split\n"""
data/east.py,0,"b""import pickle\n\nimport cv2\nimport numpy as np\nfrom shapely.geometry import Polygon\n\nfrom concern.config import Configurable, State\n\n\nclass MakeEASTData(Configurable):\n    shrink = State(default=0.5)\n    background_weight = State(default=3.0)\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n    def find_polygon_radius(self, poly):\n        poly = Polygon(poly)\n        low = 0\n        high = 65536\n        while high - low > 0.1:\n            mid = (high + low) / 2\n            area = poly.buffer(-mid).area\n            if area > 0.1:\n                low = mid\n            else:\n                high = mid\n        return high\n\n    def __call__(self, data, *args, **kwargs):\n        image, label, meta = data\n        lines = label['polys']\n        image_id = meta['data_id']\n\n        h, w = image.shape[:2]\n\n        heatmap = np.zeros((h, w), np.float32)\n        heatmap_weight = np.zeros((h, w), np.float32)\n        densebox = np.zeros((8, h, w), np.float32)\n        densebox_weight = np.zeros((h, w), np.float32)\n        train_mask = np.ones((h, w), np.float32)\n\n        densebox_anchor = np.indices((h, w))[::-1].astype(np.float32)\n\n        for line in lines:\n            poly = line['points']\n\n            assert(len(poly) == 4)\n            quad = poly\n\n            radius = self.find_polygon_radius(poly)\n            shrinked_poly = list(Polygon(poly).buffer(-radius * self.shrink).exterior.coords)[:-1]\n            shrinked_poly_points = np.array([shrinked_poly], np.int32)\n\n            cv2.fillConvexPoly(heatmap, shrinked_poly_points, 1.0)\n            cv2.fillConvexPoly(densebox_weight, shrinked_poly_points, 1.0)\n\n            for i in range(0, 4):\n                for j in range(0, 2):\n                    cv2.fillConvexPoly(densebox[i * 2 + j], shrinked_poly_points, float(quad[i][j]))\n\n            if line['ignore']:\n                cv2.fillConvexPoly(train_mask, np.array([poly], np.int32), 0.0)\n\n        heatmap_neg = np.logical_and(heatmap == 0, train_mask)\n        heatmap_pos = np.logical_and(heatmap > 0, train_mask)\n\n        heatmap_weight[heatmap_neg] = self.background_weight\n        heatmap_weight[heatmap_pos] = train_mask.sum() / max(heatmap_pos.sum(), train_mask.sum() * 0.05)\n\n        densebox_weight = densebox_weight * train_mask\n\n        densebox = densebox - np.tile(densebox_anchor, (4, 1, 1))\n\n        # to pytorch channel sequence\n        image = image.transpose(2, 0, 1)\n\n        label = {\n            'heatmap': heatmap[np.newaxis],\n            'heatmap_weight': heatmap_weight[np.newaxis],\n            'densebox': densebox,\n            'densebox_weight': densebox_weight[np.newaxis],\n        }\n        meta = {\n            'image_id': image_id,\n            'lines': lines,\n        }\n        return image, label, pickle.dumps(meta)\n"""
data/file_dataset.py,0,"b""import functools\n\nimport cv2\nfrom concern.config import Configurable, State\nfrom concern.log import Log\nfrom .dataset import Dataset\nfrom concern.distributed import is_main\n\n\nclass FileDataset(Dataset, Configurable):\n    r'''Dataset reading from files.\n    Args:\n        file_paths: Pattern or list, required, the file_paths containing data and annotations.\n    '''\n    file_paths = State()\n\n    def __init__(self, path=None, file_paths=None, cmd={}, **kwargs):\n        self.load_all(**kwargs)\n\n        if file_paths is None:\n            file_paths = path\n        self.file_paths = self.list_or_pattern(file_paths) or self.file_paths\n\n        self.debug = cmd.get('debug', False)\n        self.prepare()\n\n    def prepare(self):\n        self.meta = self.prepare_meta(self.file_paths)\n\n        if self.unpack is None:\n            self.unpack = self.default_unpack\n\n        self.data_ids = self.meta.get('data_ids', self.meta.get('data_id', []))\n        if self.debug:\n            self.data_ids = self.data_ids[:32]\n        self.num_samples = len(self.data_ids)\n        if is_main():\n            print(self.num_samples, 'images found')\n        return self\n\n    def prepare_meta(self, path_or_list):\n        def add(a_dict: dict, another_dict: dict):\n            for key, value in another_dict.items():\n                if key in a_dict:\n                    a_dict[key] = a_dict[key] + value\n                else:\n                    a_dict[key] = value\n            return a_dict\n\n        if isinstance(path_or_list, list):\n            return functools.reduce(add, [self.prepare_meta(path) for path in path_or_list])\n\n        return self.prepare_meta_single(path_or_list)\n\n    def default_unpack(self, data_id, meta):\n        image = cv2.imread(data_id, cv2.IMREAD_COLOR).astype('float32')\n        meta['image'] = image\n        return meta\n"""
data/list_dataset.py,1,"b""import io\nimport os\n\nfrom torch.utils.data import Dataset as TorchDataset\n\nimport cv2\nimport numpy as np\nfrom matplotlib.pyplot import imread\n\nfrom concern.config import Configurable, State\n\n\nclass ListDataset(TorchDataset, Configurable):\n    list_file = State()\n    processes = State()\n    image_path = State()\n    gt_path = State()\n\n    def __init__(self,\n                 debug=False,\n                 list_file=None, image_path=None, gt_path=None,\n                 **kwargs):\n        self.load_all(**kwargs)\n\n        self.image_path = image_path or self.image_path\n        self.gt_path = gt_path or self.gt_path\n        self.list_file = list_file or self.list_file\n        self.gt_paths, self.image_paths =\\\n            self.load_meta()  # FIXME: this should be removed\n\n    def load_meta(self):\n        base_path = os.path.dirname(self.list_file)\n        gt_base_path = os.path.join(base_path, 'gts')\n        image_base_path = self.image_path\n        gt_paths = []\n        image_paths = []\n        with open(self.list_file, 'rt') as list_reader:\n            for _line in list_reader.readlines():\n                line = _line.strip()\n                gt_paths.append(os.path.join(gt_base_path, line + '.txt'))\n                image_paths.append(os.path.join(image_base_path, line))\n        print(len(gt_paths), 'images found')\n        self.loaded = True\n        return gt_paths, image_paths\n\n    def __getitem__(self, index):\n        if not self.loaded:\n            self.load()\n        gt_path = self.gt_paths[index]\n        image_path = self.image_paths[index]\n        data = (gt_path, image_path)\n        for process in self.processes:\n            data = process(data)\n        return data\n\n    def __len__(self):\n        return len(self.gt_paths)\n\n\nclass UnpackTxtData(Configurable):\n    def __init__(self, **kwargs):\n        pass\n\n    def __call__(self, data):\n        gt_path, image_path = data\n        image_name = os.path.basename(image_path)\n\n        lines = []\n        with open(gt_path) as reader:\n            for line in reader.readlines():\n                line = line.replace('\\ufeff', '').strip()\n                if '\\xef\\xbb\\xbf' in line:\n                    import pdb; pdb.set_trace()\n                line_list = line.split(',')\n                assert len(line_list) == 9\n                points = np.array([float(scalar)\n                                   for scalar in line_list[:-1]]).reshape(-1, 2)\n                lines.append(dict(\n                    poly=points.tolist(),\n                    text=line_list[-1],\n                    filename=image_name))\n\n        return dict(\n            img=imread(image_path, mode='RGB'),\n            lines=lines,\n            data_id=image_name,\n            filename=image_name)\n"""
data/lmdb_dataset.py,0,"b""import functools\nimport cv2\nimport numpy as np\nimport pickle\nimport lmdb\nimport os\nfrom .dataset import Dataset\nfrom concern.config import Configurable, State\nfrom concern.distributed import is_main\n\n\nclass LMDBDataset(Dataset, Configurable):\n    r'''Dataset reading from lmdb.\n    Args:\n        lmdb_paths: Pattern or list, required, the path of `data.mdb`,\n            e.g. `the/path/`, `['the/path/a/', 'the/path/b/']`\n    '''\n    lmdb_paths = State()\n\n    def __init__(self, lmdb_paths=None, cmd={}, **kwargs):\n        self.load_all(**kwargs)\n        self.lmdb_paths = self.list_or_pattern(lmdb_paths) or self.lmdb_paths\n        self.debug = cmd.get('debug', False)\n        self.envs = []\n        self.txns = {}\n        self.prepare()\n        self.truncated = False\n        self.data = None\n\n    def __del__(self):\n        for env in self.envs:\n            env.close()\n\n    def prepare_meta(self, path_or_list):\n        def add(a_dict: dict, another_dict: dict):\n            new_dict = dict()\n            for key, value in another_dict.items():\n                if key in a_dict:\n                    new_dict[key] = a_dict[key] + value\n                else:\n                    new_dict[key] = value\n            return new_dict\n\n        if isinstance(path_or_list, list):\n            return functools.reduce(add, [self.prepare_meta(path) for path in path_or_list])\n\n        assert type(path_or_list) == str, path_or_list\n\n        return self.prepare_meta_single(path_or_list)\n\n    def prepare_meta_single(self, path_name):\n        return self.meta_loader.load_meta(path_name)\n\n    def prepare(self):\n        self.meta = self.prepare_meta(self.lmdb_paths)\n        if self.unpack is None:\n            self.unpack = self.default_unpack\n            # prepare lmdb environments\n            for path in self.lmdb_paths:\n                path = os.path.join(path, '')\n                env = lmdb.open(path, max_dbs=1, lock=False)\n                db_image = env.open_db('image'.encode())\n                self.envs.append(env)\n                self.txns[path] = env.begin(db=db_image)\n            # The fetcher is supposed to be initialized in the\n            # sub-processes, or it will cause CRC Error.\n            self.fetcher = None\n\n        self.data_ids = self.meta.get('data_ids', self.meta.get('data_id', []))\n        if self.debug:\n            self.data_ids = self.data_ids[:32]\n        self.num_samples = len(self.data_ids)\n\n        if is_main():\n            print(self.num_samples, 'images found')\n        return self\n\n    def search_image(self, data_id, path):\n        maybe_image = self.txns[path].get(data_id)\n        assert maybe_image is not None, 'image %s not found at %s' % (\n            data_id, path)\n        return maybe_image\n\n    def default_unpack(self, data_id, meta):\n        data = self.search_image(data_id, meta['db_path'])\n        image = np.fromstring(data, dtype=np.uint8)\n        image = cv2.imdecode(image, cv2.IMREAD_COLOR).astype('float32')\n        meta['image'] = image\n        return meta\n"""
data/local_csv.py,1,"b""import os\n\nfrom torch.utils.data import Dataset as TorchDataset\n\nimport cv2\nimport numpy as np\n\nfrom concern.config import Configurable, State\n\n\nclass LocalCSVDataset(TorchDataset, Configurable):\n    csv_path = State()\n    processes = State()\n    debug = State(default=False)\n\n    def __init__(self, cmd={}, **kwargs):\n        self.load_all(cmd=cmd, **kwargs)\n\n        self.load_meta()\n        self.debug = cmd.get('debug', False)\n\n    def load_meta(self):\n        self.meta = []\n        for textline in open(self.csv_path):\n            tokens = textline.strip().split('\\t')\n            filename = tokens[0]\n            filepath = os.path.join(os.path.dirname(self.csv_path), filename)\n            lines_coords = tokens[1::2]\n            lines_text = tokens[2::2]\n            lines = []\n            for coords, text in zip(lines_coords, lines_text):\n                poly = np.array(list(map(int, coords[1:-1].split(',')))).reshape(4, 2).tolist()\n                lines.append({\n                    'poly': poly,\n                    'text': text,\n                })\n            self.meta.append({\n                'img': cv2.imread(filepath, cv2.IMREAD_COLOR),\n                'lines': lines,\n                'filename': filename,\n                'data_id': filename,\n            })\n\n        print(len(self.meta), 'images found')\n\n    def __getitem__(self, index):\n        data = self.meta[index]\n        for process in self.processes:\n            data = process(data)\n        return data\n\n    def __len__(self):\n        return len(self.meta)\n"""
data/meta.py,0,b''
data/meta_loader.py,0,b'from data.meta_loaders import *\n'
data/mingle_dataset.py,1,"b""import bisect\n\nimport torch.utils.data as data\n\nfrom concern.config import Configurable, State\n\n\nclass MingleDataset(data.Dataset, Configurable):\n    datasets = State(default=[])\n\n    def __init__(self, cmd={}, **kwargs):\n        self.load_all(cmd=cmd, **kwargs)\n\n        ratios = []\n        sizes = []\n        indices = []\n        self.data_sources = []\n        for i in range(len(self.datasets)):\n            dataset_dict = self.datasets[i]\n            ratio = dataset_dict['ratio']\n            size = len(dataset_dict['dataset'])\n            if size == 0:\n                continue\n            indices.append(i)\n            ratios.append(ratio)\n            sizes.append(size)\n            self.data_sources.append(dataset_dict['dataset'])\n\n        ratio_sum = sum(ratios)\n        ratios = [r / ratio_sum for r in ratios]\n        total = sum(sizes)\n        for index, ratio in enumerate(ratios):\n            quota = ratio * total\n            if sizes[index] < quota:\n                total = int(sizes[index] / ratio + 0.5)\n\n        milestones = []\n        for ratio in ratios[:-1]:\n            milestones.append(int(ratio * total + 0.5))\n        self.milestones = milestones\n        self.total = total\n        print('total', self.total)\n\n    def __len__(self):\n        return self.total\n\n    def __getitem__(self, index):\n        dataset_index = bisect.bisect(self.milestones, index)\n        dataset = self.data_sources[dataset_index]\n        if dataset_index == 0:\n            real_index = index\n        else:\n            real_index = index - self.milestones[dataset_index - 1]\n        return dataset.__getitem__(real_index)\n"""
data/mnist.py,0,"b""import torchvision\n\nfrom concern.config import Configurable, State\n\n\nclass MNistDataset(Configurable, torchvision.datasets.MNIST):\n    root = State()\n    is_train = State(autoload=False)\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n        cmd = kwargs['cmd']\n        self.is_train = cmd['is_train']\n\n        transform = torchvision.transforms.Compose([\n            torchvision.transforms.Resize((64, 64)),\n            torchvision.transforms.Grayscale(num_output_channels=3),\n            torchvision.transforms.ToTensor(),\n        ])\n        torchvision.datasets.MNIST.__init__(\n            self, self.root,\n            train=self.is_train, download=True, transform=transform\n        )\n"""
data/nori_dataset.py,0,"b""import functools\nimport cv2\nimport numpy as np\n\nfrom .dataset import Dataset\nfrom concern.config import Configurable, State\nfrom concern.nori_reader import NoriReader\nfrom concern.distributed import is_main\n\n\nclass NoriDataset(Dataset, Configurable):\n    r'''Dataset reading from nori.\n    Args:\n        nori_paths: Pattern or list, required, the noris containing data,\n            e.g. `the/path/*.nori`, `['a.nori', 'b.nori']`\n    '''\n    nori_paths = State()\n\n    def __init__(self, nori_paths=None, cmd={}, **kwargs):\n        self.load_all(**kwargs)\n\n        self.nori_paths = self.list_or_pattern(nori_paths) or self.nori_paths\n        self.debug = cmd.get('debug', False)\n\n        self.prepare()\n        self.truncated = False\n        self.data = None\n\n    def prepare_meta(self, path_or_list):\n        def add(a_dict: dict, another_dict: dict):\n            new_dict = dict()\n            for key, value in another_dict.items():\n                if key in a_dict:\n                    new_dict[key] = a_dict[key] + value\n                else:\n                    new_dict[key] = value\n            return new_dict\n\n        if isinstance(path_or_list, list):\n            return functools.reduce(add, [self.prepare_meta(path) for path in path_or_list])\n\n        assert type(path_or_list) == str, path_or_list\n        assert path_or_list.endswith('.nori') or path_or_list.endswith('.nori/')\n        return self.prepare_meta_single(path_or_list)\n\n    def prepare_meta_single(self, path_name):\n        return self.meta_loader.load_meta(path_name)\n\n    def prepare(self):\n        self.meta = self.prepare_meta(self.nori_paths)\n        if self.unpack is None:\n            self.unpack = self.default_unpack\n\n            # The fetcher is supposed to be initialized in the\n            # sub-processes, or it will cause CRC Error.\n            self.fetcher = None\n\n        self.data_ids = self.meta.get('data_ids', self.meta.get('data_id', []))\n        if self.debug:\n            self.data_ids = self.data_ids[:32]\n        self.num_samples = len(self.data_ids)\n        if is_main():\n            print(self.num_samples, 'images found')\n        return self\n\n    def default_unpack(self, data_id, meta):\n        if self.fetcher is None:\n            self.fetcher = NoriReader(self.nori_paths)\n        data = self.fetcher.get(data_id)\n        image = np.fromstring(data, dtype=np.uint8)\n        image = cv2.imdecode(image, cv2.IMREAD_COLOR).astype('float32')\n        meta['image'] = image\n        return meta\n"""
data/quad.py,3,"b""import torch\nimport numpy as np\n\n\nclass Quad:\n    def __init__(self, points, format='NP2'):\n        self._rect = None\n        self.tensorized = False\n        self._points = None\n        self.set_points(points, format)\n\n    @property\n    def points(self):\n        return self._points\n\n    def set_points(self, new_points, format='NP2'):\n        order = (format.index('N'), format.index('P'), format.index('2'))\n\n        if isinstance(new_points, torch.Tensor):\n            self._points = new_points.permute(*order)\n            self.tensorized = True\n        else:\n            points = np.array(new_points, dtype=np.float32)\n            self._points = points.transpose(*order)\n\n            if self.tensorized:\n                self.tensorized = False\n                self.tensor\n\n    @points.setter\n    def points(self, new_points):\n        self.set_points(new_points)\n\n    @property\n    def tensor(self):\n        if not self.tensorized:\n            self._points = torch.from_numpy(self._points)\n        return self._points\n\n    def to(self, device):\n        self._points.to(device)\n        return self._points\n\n    def __iter__(self):\n        for i in range(self._points.shape[0]):\n            if self.tensorized:\n                yield self.tensor[i]\n            else:\n                yield self.points[i]\n\n\n    def rect(self):\n        if self._rect is None:\n            self._rect = self.rectify()\n        return self._rect\n\n    def __getitem__(self, *args, **kwargs):\n        return self._points.__getitem__(*args, **kwargs)\n\n    def numpy(self):\n        if not self.tensorized:\n            return self._points\n        return self._points.cpu().data.numpy()\n\n    def rectify(self):\n        if self.tensorized:\n            return self.rectify_tensor()\n\n        xmin = self._points[:, :, 0].min(axis=1)\n        ymin = self._points[:, :, 1].min(axis=1)\n        xmax = self._points[:, :, 0].max(axis=1)\n        ymax = self._points[:, :, 1].max(axis=1)\n        return np.stack([xmin, ymin, xmax, ymax], axis=1)\n\n    def rectify_tensor(self):\n        xmin, _ = self.tensor[:, :, 0].min(dim=1, keepdim=True)\n        ymin, _ = self.tensor[:, :, 1].min(dim=1, keepdim=True)\n        xmax, _ = self.tensor[:, :, 0].max(dim=1, keepdim=True)\n        ymax, _ = self.tensor[:, :, 1].max(dim=1, keepdim=True)\n        return torch.cat([xmin, ymin, xmax, ymax], dim=1)\n\n    def __getattribute__(self, name):\n        try:\n            return super().__getattribute__(name)\n        except AttributeError:\n            return self._points.__getattribute__(name)\n"""
data/simple_detection.py,0,"b""import pickle\n\nimport cv2\nimport skimage\nimport numpy as np\nfrom shapely.geometry import Polygon\n\nfrom concern.config import Configurable, State\n\n\ndef binary_search_smallest_width(poly):\n    if len(poly) < 3:\n        return 0\n    poly = Polygon(poly)\n    low = 0\n    high = 65536\n    while high - low > 0.1:\n        mid = (high + low) / 2\n        mid_poly = poly.buffer(-mid)\n        if mid_poly.geom_type == 'Polygon' and mid_poly.area > 0.1:\n            low = mid\n        else:\n            high = mid\n    height = (low + high) / 2\n    if height < 0.1:\n        return 0\n    else:\n        return height\n\n\ndef project_point_to_line(x, u, v, axis=0):\n    n = v - u\n    n = n / (np.linalg.norm(n, axis=axis, keepdims=True) + np.finfo(np.float32).eps)\n    p = u + n * np.sum((x - u) * n, axis=axis, keepdims=True)\n    return p\n\n\ndef project_point_to_segment(x, u, v, axis=0):\n    p = project_point_to_line(x, u, v, axis=axis)\n    outer = np.greater_equal(np.sum((u - p) * (v - p), axis=axis, keepdims=True), 0)\n    near_u = np.less_equal(\n        np.linalg.norm(u - p, axis=axis, keepdims=True),\n        np.linalg.norm(v - p, axis=axis, keepdims=True)\n    )\n    o = np.where(outer, np.where(near_u, u, v), p)\n    return o\n\n\nclass MakeSimpleDetectionData(Configurable):\n    center_shrink = State(default=0.5)\n    background_weight = State(default=3.0)\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n    def get_mask(self, w, h, polys, ignores):\n        mask = np.ones((h, w), np.float32)\n\n        for poly, ignore in zip(polys, ignores):\n            if ignore:\n                cv2.fillPoly(mask, np.array([poly], np.int32), 0.0)\n\n        return mask\n\n    def get_line_height(self, poly):\n        return binary_search_smallest_width(poly)\n\n    def get_regions_coords(self, w, h, polys, heights, shrink):\n        label_map = np.zeros((h, w), np.int32)\n        for line_id, (poly, height) in enumerate(zip(polys, heights)):\n            if height > 0:\n                shrinked_poly = Polygon(poly).buffer(-height * shrink)\n                if shrinked_poly.geom_type == 'Polygon' and not shrinked_poly.is_empty:\n                    shrinked_poly = np.array(list(shrinked_poly.exterior.coords), np.int32)\n                    cv2.fillPoly(label_map, shrinked_poly[np.newaxis], line_id + 1)\n\n        regions = skimage.measure.regionprops(label_map)\n        regions_coords = [\n            region.coords[:, ::-1] for region in regions\n        ] + [\n            np.zeros((0, 2), 'int32')\n        ] * (len(polys) - len(regions))\n\n        return regions_coords\n\n    def get_coords_poly_projection(self, coords, poly):\n        start_points = np.array(poly)\n        end_points = np.concatenate([poly[1:], poly[:1]], axis=0)\n        region_points = coords\n\n        projected_points = project_point_to_segment(\n            region_points[:, np.newaxis],\n            start_points[np.newaxis],\n            end_points[np.newaxis],\n            axis=2,\n        )\n        projection_distances = np.linalg.norm(region_points[:, np.newaxis] - projected_points, axis=2)\n        best_projected_points = projected_points[np.arange(len(region_points)), np.argmin(projection_distances, axis=1)]\n        return best_projected_points\n\n    def get_coords_poly_distance(self, coords, poly):\n        projection = self.get_coords_poly_projection(coords, poly)\n        return np.linalg.norm(projection - coords, axis=1)\n\n    def get_normalized_weight(self, heatmap, mask):\n        pos = np.greater_equal(heatmap, 0.5)\n        neg = 1 - pos\n        pos = np.logical_and(pos, mask)\n        neg = np.logical_and(neg, mask)\n        npos = pos.sum()\n        nneg = neg.sum()\n        smooth = (npos + nneg + 1) * 0.05\n        wpos = (nneg + smooth) / (npos + smooth)\n        weight = np.zeros_like(heatmap)\n        weight[neg] = self.background_weight\n        weight[pos] = wpos\n        return weight\n\n    def draw_maps(self, w, h, polys, ignores):\n        raise NotImplementedError()\n\n    def __call__(self, data, *args, **kwargs):\n        image, label, meta = data\n        lines = label['polys']\n\n        h, w = image.shape[:2]\n\n        polys = []\n        ignores = []\n        for line in lines:\n            if len(line['points']) >= 4:\n                polys.append(line['points'])\n                ignores.append(line['ignore'])\n\n        maps = self.draw_maps(w, h, polys, ignores)\n\n        # to pytorch channel sequence\n        image = image.transpose(2, 0, 1)\n\n        label = maps\n        return image, label, pickle.dumps(meta)\n\n\nclass MakeSimpleSegData(MakeSimpleDetectionData):\n    def draw_maps(self, w, h, polys, ignores):\n        heatmap = np.zeros((h, w), np.float32)\n\n        heights = [self.get_line_height(poly) for poly in polys]\n        regions_center_coords = self.get_regions_coords(w, h, polys, heights, self.center_shrink)\n        train_mask = self.get_mask(w, h, polys, ignores)\n        for region_center_coords in regions_center_coords:\n            x, y = region_center_coords[:, 0], region_center_coords[:, 1]\n            heatmap[y, x] = 1.0\n        heatmap_weight = self.get_normalized_weight(heatmap, train_mask)\n\n        return {\n            'heatmap': heatmap[np.newaxis],\n            'heatmap_weight': heatmap_weight[np.newaxis],\n        }\n\n\nclass MakeSimpleEASTData(MakeSimpleDetectionData):\n    def draw_maps(self, w, h, polys, ignores):\n        heatmap = np.zeros((h, w), np.float32)\n        densebox = np.zeros((8, h, w), np.float32)\n        densebox_weight = np.zeros((h, w), np.float32)\n\n        heights = [self.get_line_height(poly) for poly in polys]\n        regions_center_coords = self.get_regions_coords(w, h, polys, heights, self.center_shrink)\n        train_mask = self.get_mask(w, h, polys, ignores)\n        for poly, region_center_coords in zip(polys, regions_center_coords):\n            x, y = region_center_coords[:, 0], region_center_coords[:, 1]\n            heatmap[y, x] = 1.0\n            densebox_weight[y, x] = 1.0\n\n            for i in range(0, 4):\n                densebox[i * 2, y, x] = float(poly[i][0]) - x\n                densebox[i * 2 + 1, y, x] = float(poly[i][1]) - y\n\n        heatmap_weight = self.get_normalized_weight(heatmap, train_mask)\n        densebox_weight = densebox_weight * train_mask\n\n        return {\n            'heatmap': heatmap[np.newaxis],\n            'heatmap_weight': heatmap_weight[np.newaxis],\n            'densebox': densebox,\n            'densebox_weight': densebox_weight[np.newaxis],\n        }\n\n\nclass MakeSimpleTextsnakeData(MakeSimpleDetectionData):\n    def draw_maps(self, w, h, polys, ignores):\n        heatmap = np.zeros((h, w), np.float32)\n        radius = np.zeros((h, w), np.float32)\n        radius_weight = np.zeros((h, w), np.float32)\n\n        heights = [self.get_line_height(poly) for poly in polys]\n        regions_center_coords = self.get_regions_coords(w, h, polys, heights, self.center_shrink)\n        train_mask = self.get_mask(w, h, polys, ignores)\n        for poly, region_center_coords in zip(polys, regions_center_coords):\n            x, y = region_center_coords[:, 0], region_center_coords[:, 1]\n            heatmap[y, x] = 1.0\n\n            distance = self.get_coords_poly_distance(region_center_coords, poly)\n            radius[y, x] = distance\n            radius_weight[y, x] = 1.0\n\n        heatmap_weight = self.get_normalized_weight(heatmap, train_mask)\n        radius_weight = radius_weight * train_mask\n\n        return {\n            'heatmap': heatmap[np.newaxis],\n            'heatmap_weight': heatmap_weight[np.newaxis],\n            'radius': radius[np.newaxis],\n            'radius_weight': radius_weight[np.newaxis],\n        }\n\n\nclass MakeSimpleMSRData(MakeSimpleDetectionData):\n    def draw_maps(self, w, h, polys, ignores):\n        heatmap = np.zeros((h, w), np.float32)\n        offset = np.zeros((2, h, w), np.float32)\n        offset_weight = np.zeros((h, w), np.float32)\n\n        heights = [self.get_line_height(poly) for poly in polys]\n        regions_center_coords = self.get_regions_coords(w, h, polys, heights, self.center_shrink)\n        train_mask = self.get_mask(w, h, polys, ignores)\n        for poly, region_center_coords in zip(polys, regions_center_coords):\n            x, y = region_center_coords[:, 0], region_center_coords[:, 1]\n            heatmap[y, x] = 1.0\n\n            projection_points = self.get_coords_poly_projection(region_center_coords, poly)\n            offset[0, y, x] = projection_points[:, 0] - x\n            offset[1, y, x] = projection_points[:, 1] - y\n            offset_weight[y, x] = 1.0\n\n        heatmap_weight = self.get_normalized_weight(heatmap, train_mask)\n        offset_weight = offset_weight * train_mask\n\n        return {\n            'heatmap': heatmap[np.newaxis],\n            'heatmap_weight': heatmap_weight[np.newaxis],\n            'offset': offset,\n            'offset_weight': offset_weight[np.newaxis],\n        }\n\n\nclass SimpleDetectionCropper(Configurable):\n    patch_cropper = State()\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n    def crop(self, batch, output):\n        img, label, meta = batch\n\n        images_polys = []\n        images_patches = []\n        for polys, image_meta in zip(output['polygons_pred'], meta):\n            image_meta = pickle.loads(image_meta)\n\n            images_polys.append(polys)\n            images_patches.append([self.patch_cropper.crop(image_meta['image'], p) for p in polys])\n\n        return images_polys, images_patches\n"""
data/text_lines.py,0,"b""from .quad import Quad\n\n\nclass TextLines:\n    '''\n    The abstrct class of text lines in an input image for scene text detection and recognition.\n    Input:\n        lines:\n            - text: the text content of a text instance.\n              poly: the quadrangle-box of the text instance.\n              charboxes: the quadrangle-box of the characters inside the corresponding text instance.\n    '''\n    def __init__(self, lines, with_charboxes=True):\n        self.texts = []\n        quads = []\n        self.charboxes = []\n        for line in lines:\n            self.texts.append(line['text'])\n            quads.append(line['poly'])\n            if with_charboxes and 'charboxes' in line:\n                self.charboxes.append(Quad(line['charboxes']))\n        self.with_charboxes = len(self.charboxes) > 0\n        self.quads = Quad(quads)\n        self._rects = None\n\n    def __iter__(self):\n        for text, quad in zip(self.texts, self.quads):\n            yield(text, quad)\n\n    @property\n    def rects(self):\n        if self._rects is None:\n            self._rects = self.quads.rectify()\n        return self._rects\n\n    def __len__(self):\n        return len(self.texts)\n\n    def char_count(self):\n        return sum([len(t) for t in self.texts])\n"""
data/textsnake.py,0,"b'import pickle\n\nimport cv2\nimport numpy as np\nfrom skimage.draw import polygon as drawpoly\n\nfrom concern.textsnake import find_bottom, find_long_edges, split_edge_seqence, \\\n    norm2, vector_cos, vector_sin\nfrom concern.config import Configurable, State\n\n\nclass MakeTextsnakeData(Configurable):\n    n_disk = State(default=15)\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n    def make_text_region(self, image, polygons):\n\n        tr_mask = np.zeros(image.shape[:2], np.uint8)\n        train_mask = np.ones(image.shape[:2], np.uint8)\n\n        for polygon in polygons:\n            cv2.fillPoly(tr_mask, [np.array(polygon[\'points\'], np.int32)], color=(1,))\n            # we will  ignore the text that can not be identified, so we need this train mask.\n            if polygon[\'ignore\']:\n                cv2.fillPoly(train_mask, [np.array(polygon[\'points\'], np.int32)], color=(0,))\n        return tr_mask, train_mask\n\n    def fill_polygon(self, mask, polygon, value):\n        """"""\n        fill polygon in the mask with value\n        :param mask: input mask\n        :param polygon: polygon to draw\n        :param value: fill value\n        """"""\n        rr, cc = drawpoly(polygon[:, 1], polygon[:, 0], shape=(mask.shape[0], mask.shape[1]))\n        mask[rr, cc] = value\n\n    def make_text_center_line(self, sideline1, sideline2, center_line, radius,\n                              tcl_mask, radius_map, sin_map, cos_map, expand=0.2):\n\n        count = 0\n        begin = 0\n        end = 1\n        while count < radius[0] / 2 and begin < 5:\n            count += norm2(center_line[begin + 1] - center_line[begin])\n            begin += 1\n\n        count = 0\n        while count < radius[-1] / 2 and end < 6:\n            count += norm2(center_line[-end] - center_line[-end - 1])\n            end += 1\n\n        for i in range(begin, len(center_line) - 1 - end):\n            c1 = center_line[i]\n            c2 = center_line[i + 1]\n            top1 = sideline1[i]\n            top2 = sideline1[i + 1]\n            bottom1 = sideline2[i]\n            bottom2 = sideline2[i + 1]\n\n            sin_theta = vector_sin(c2 - c1)\n            cos_theta = vector_cos(c2 - c1)\n\n            p1 = c1 + (top1 - c1) * expand\n            p2 = c1 + (bottom1 - c1) * expand\n            p3 = c2 + (bottom2 - c2) * expand\n            p4 = c2 + (top2 - c2) * expand\n            polygon = np.stack([p1, p2, p3, p4])\n\n            self.fill_polygon(tcl_mask, polygon, value=1)\n            self.fill_polygon(radius_map, polygon, value=radius[i])\n            self.fill_polygon(sin_map, polygon, value=sin_theta)\n            self.fill_polygon(cos_map, polygon, value=cos_theta)\n\n    def disk_cover(self, points, n_disk=15):\n        """"""\n        cover text region with several disks\n        :param n_disk: number of disks\n        :return:\n        """"""\n        bottoms = find_bottom(points)  # find two bottoms of this Text\n        e1, e2 = find_long_edges(points, bottoms)  # find two long edge sequence\n\n        inner_points1 = split_edge_seqence(points, e1, n_disk)\n        inner_points2 = split_edge_seqence(points, e2, n_disk)\n        # inverse one of long edge because original long edge is inversed\n        inner_points2 = inner_points2[::-1]\n\n        center_points = (inner_points1 + inner_points2) / 2  # disk center\n        radius = norm2(inner_points1 - center_points, axis=1)  # disk radius\n\n        return inner_points1, inner_points2, center_points, radius\n\n    def __call__(self, data, *args, **kwargs):\n        image, label, meta = data\n        polygons = label[\'polys\']\n        image_id = meta[\'data_id\']\n\n        height, weight, _ = image.shape\n\n        tcl_mask = np.zeros(image.shape[:2], np.uint8)\n        radius_map = np.zeros(image.shape[:2], np.float32)\n        sin_map = np.zeros(image.shape[:2], np.float32)\n        cos_map = np.ones(image.shape[:2], np.float32)\n\n        Cnts = []\n        cares = []\n        for i, polygon in enumerate(polygons):\n            Cnts.append(polygon[\'points\'])\n            if not polygon[\'ignore\']:\n                sideline1, sideline2, center_points, radius = self.disk_cover(polygon[\'points\'], n_disk=self.n_disk)\n                self.make_text_center_line(sideline1, sideline2, center_points, radius, tcl_mask, radius_map, sin_map,\n                                           cos_map)\n                cares.append(1)\n            else:\n                cares.append(0)\n        tr_mask, train_mask = self.make_text_region(image, polygons)\n        # if tr_mask.sum() < 9:\n        #     tr_mask[0:3, 0:3] = 1\n\n        # to pytorch channel sequence\n        image = image.transpose(2, 0, 1)\n\n        label = {\n            \'train_mask\': train_mask,\n            \'tr_mask\': tr_mask,\n            \'tcl_mask\': tcl_mask,\n            \'radius_map\': radius_map,\n            \'sin_map\': sin_map,\n            \'cos_map\': cos_map,\n        }\n        meta = {\n            \'image_id\': image_id,\n            \'Height\': height,\n            \'Width\': weight,\n            \'Cnts\': Cnts,\n            \'cares\': cares\n        }\n        return image, label, pickle.dumps(meta)\n'"
data/unpack_msgpack_data.py,0,"b""import io\n\nimport cv2\nimport numpy as np\nimport msgpack\nimport config\nimport os\nimport lmdb\nfrom PIL import Image\n\nfrom concern.config import Configurable, State\n\n\nclass UnpackMsgpackData(Configurable):\n    mode = State(default='BGR')\n\n    def __init__(self, cmd={}, **kwargs):\n        self.load_all(**kwargs)\n        if config.will_use_nori:\n            self.fetcher = nori.Fetcher()\n        elif config.will_use_lmdb:\n            self.envs = []\n            self.txns = {}\n        if 'mode' in cmd:\n            self.mode = cmd['mode']\n\n    def convert_obj(self, obj):\n        if isinstance(obj, dict):\n            ndata = {}\n            for key, value in obj.items():\n                nkey = key.decode()\n                if nkey == 'img':\n                    img = Image.open(io.BytesIO(value))\n                    img = np.array(img.convert('RGB'))\n                    if self.mode == 'BGR':\n                        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n                    nvalue = img\n                else:\n                    nvalue = self.convert_obj(value)\n                ndata[nkey] = nvalue\n            return ndata\n        elif isinstance(obj, list):\n            return [self.convert_obj(item) for item in obj]\n        elif isinstance(obj, bytes):\n            return obj.decode()\n        else:\n            return obj\n\n    def convert(self, data):\n        obj = msgpack.loads(data, max_str_len=2 ** 31)\n        return self.convert_obj(obj)\n\n    def __call__(self, data_id, meta=None):\n        if meta is None:\n            meta = {}\n        item = self.convert(self.fetcher.get(data_id))\n        item['data_id'] = data_id\n        meta.update(item)\n        return meta\n\n\nclass TransformMsgpackData(UnpackMsgpackData):\n    meta_loader = State(default=None)\n\n    def __init__(self, meta_loader=None, cmd={}, **kwargs):\n        super().__init__(cmd=cmd, meta_loader=meta_loader, **kwargs)\n        print('transform')\n        self.meta_loader = cmd.get('meta_loader', self.meta_loader)\n\n    def get_item(self, data_id, meta):\n        if config.will_use_nori:\n            item = self.fetcher.get(data_id)\n        elif config.will_use_lmdb:\n            db_path = meta['db_path']\n            if db_path not in self.envs:\n                path = os.path.join(db_path, '')\n                env = lmdb.open(db_path, max_dbs=1, lock=False)\n                db_image = env.open_db('image'.encode())\n                self.envs.append(env)\n                self.txns[db_path] = env.begin(db=db_image)\n            item = self.txns[db_path].get(data_id)\n        else:\n            raise NotImplementedError\n        return item\n\n    def __call__(self, data_id, meta):\n        item = self.get_item(data_id, meta)\n        item = self.convert(item)\n        image = item.pop('img').astype(np.float32)\n        if self.meta_loader is not None:\n            meta['extra'] = item\n            data = self.meta_loader.parse_meta(data_id, meta)\n        else:\n            data = meta\n            data.update(**item)\n        data.update(image=image, data_id=data_id)\n        return data\n"""
decoders/__init__.py,0,"b'from .classification import ClassificationDecoder\nfrom .attention_decoder import AttentionDecoder\nfrom .textsnake import TextsnakeDecoder\nfrom .east import EASTDecoder\nfrom .dice_loss import DiceLoss\nfrom .pss_loss import PSS_Loss\nfrom .ctc_decoder2d import CTCDecoder2D\nfrom .simple_detection import SimpleSegDecoder, SimpleEASTDecoder, SimpleTextsnakeDecoder, SimpleMSRDecoder\nfrom .ctc_decoder import CTCDecoder\nfrom .l1_loss import MaskL1Loss\nfrom .balance_cross_entropy_loss import BalanceCrossEntropyLoss\nfrom .crnn import CRNNDecoder\nfrom .seg_recognizer import SegRecognizer\nfrom .seg_detector import SegDetector'"
decoders/attention_decoder.py,23,"b""import torch\nimport torch.nn as nn\nimport config\nimport numpy as np\nimport math\n\nfrom concern.charsets import DefaultCharset\n\n\nclass AttentionDecoder(nn.Module):\n    def __init__(self, in_channels, charset=DefaultCharset(),\n                 inner_channels=512, max_size=32, height=1,\n                 gt_as_output=None, step_dropout=0, **kwargs):\n        super(AttentionDecoder, self).__init__()\n\n        self.inner_channels = inner_channels\n        self.encode = self._init_encoder(in_channels)\n\n        self.max_size = max_size\n        self.charset = charset\n        self.height = height\n        self.decoder = AttentionRNNCell(\n            inner_channels, max_size + height, len(charset))\n        self.step_dropout = step_dropout\n\n        self.onehot_embedding_x = nn.Embedding(max_size, max_size)\n        self.onehot_embedding_x.weight.data = torch.eye(max_size)\n        self.onehot_embedding_y = nn.Embedding(height, height)\n        self.onehot_embedding_y.weight.data = torch.eye(height)\n\n        self.gt_as_output = gt_as_output\n        self.loss_function = nn.NLLLoss(reduction='none')\n\n    def _init_encoder(self, in_channels, stride=(2, 1), padding=(0, 1)):\n        encode = nn.Sequential(\n            self.conv_bn_relu(in_channels, self.inner_channels),\n            self.conv_bn_relu(self.inner_channels, self.inner_channels),\n            nn.MaxPool2d((2, 2), (2, 2), (0, 0)),\n            self.conv_bn_relu(self.inner_channels, self.inner_channels),\n            self.conv_bn_relu(self.inner_channels, self.inner_channels),\n            nn.MaxPool2d(stride, stride, (0, 0)),\n            self.conv_bn_relu(self.inner_channels, self.inner_channels),\n            self.conv_bn_relu(self.inner_channels, self.inner_channels),\n            nn.MaxPool2d(stride, stride, (0, 0)),\n            self.conv_bn_relu(self.inner_channels, self.inner_channels,\n                              kernel_size=(2, 3),\n                              stride=stride, padding=padding),\n        )\n        return encode\n\n    def _get_gt_as_output(self):\n        if self.gt_as_output is not None:\n            return self.gt_as_output\n        return np.random.rand() < 0.5\n\n    def conv_bn_relu(self, input_channels, output_channels,\n                     kernel_size=3, stride=1, padding=1):\n        return nn.Sequential(nn.Conv2d(input_channels, output_channels,\n                                       kernel_size=kernel_size,\n                                       stride=stride, padding=padding),\n                             nn.BatchNorm2d(output_channels),\n                             nn.ReLU(inplace=True), )\n\n    def forward(self, feature,\n                targets=None,\n                lengths=None,\n                train=False):\n        decoder_input_sequence = self.encode(feature)\n        index_y, index_x = torch.meshgrid(torch.linspace(0, self.height - 1, self.height),\n                                          torch.linspace(0, self.max_size - 1, self.max_size))\n        index_y = index_y.to(feature.device).type(torch.long)\n        index_x = index_x.to(feature.device).type(torch.long)\n\n        batch_size = feature.shape[0]\n        onehot_embedded_x = self.onehot_embedding_x(index_x).permute(2, 0, 1)\n        onehot_embedded_x = onehot_embedded_x.repeat(batch_size, 1, 1, 1)\n        onehot_embedded_y = self.onehot_embedding_y(index_y).permute(2, 0, 1)\n        onehot_embedded_y = onehot_embedded_y.repeat(batch_size, 1, 1, 1)\n        decoder_input = torch.cat(\n            [decoder_input_sequence, onehot_embedded_y, onehot_embedded_x],\n            dim=1)\n        decoder_input = decoder_input.view(\n            batch_size, decoder_input.shape[1], -1).permute(2, 0, 1)\n\n        onehot_bos = torch.zeros(batch_size, ) + self.charset.blank\n        decoder_hidden = torch.zeros(batch_size, self.inner_channels).to(feature.device)\n\n        if self.training:\n            max_steps = lengths.max()\n            timestep_input = onehot_bos.to(feature.device)\n            targets = targets.type(torch.long)\n            for timestep in range(self.max_size):\n                decoder_output, decoder_hidden, decoder_attention = self.decoder(\n                    timestep_input, decoder_hidden, decoder_input, train=True)\n                mask = (timestep <= lengths).type(torch.float)\n                if timestep == 0:\n                    loss = self.loss_function(\n                        decoder_output, targets[:, timestep]) * mask\n                    attention_pred = decoder_attention.unsqueeze(1)\n                else:\n                    attention_pred = torch.cat(\n                        [attention_pred, decoder_attention.unsqueeze(1)], dim=1)\n                    loss += self.loss_function(decoder_output,\n                                               targets[:, timestep]) * mask\n                i = decoder_output.argmax(dim=1)\n\n                if self._get_gt_as_output():\n                    timestep_input = targets[:, timestep]\n                else:\n                    timestep_input = i.detach()  # will never require gradients\n\n                factor = (torch.rand(*timestep_input.shape) < self.step_dropout).long().to(feature.device)\n                timestep_input = timestep_input.to(feature.device) * \\\n                                 (1 - factor) + torch.randint(high=len(self.charset),\n                                                              size=timestep_input.shape).to(feature.device) * factor\n\n                # if timestep > max_steps: break\n            return loss, attention_pred.view(batch_size, -1, self.height, self.max_size).to(feature.device)\n        else:\n            timestep_input = onehot_bos\n            pred = torch.zeros(batch_size, self.max_size,\n                               dtype=torch.int) + self.charset.blank\n            for timestep in range(self.max_size):\n                decoder_output, decoder_hidden, decoder_attention = self.decoder(\n                    timestep_input, decoder_hidden, decoder_input, train=False)\n                i = decoder_output.argmax(dim=1)\n                timestep_input = i.detach()\n                pred[:, timestep] = i\n                if (i == self.charset.blank).all():\n                    break\n            return pred.to(feature.device)\n\n\nclass Attn(nn.Module):\n    def __init__(self, method, hidden_dims, embed_size):\n        super(Attn, self).__init__()\n        self.method = method\n        self.hidden_dims = hidden_dims\n        self.embed_size = embed_size\n        self.attn = nn.Linear(2 * self.hidden_dims + embed_size, hidden_dims)\n        # self.attn = nn.Linear(hidden_dims, hidden_dims)\n        self.v = nn.Parameter(torch.rand(hidden_dims))\n        stdv = 1. / math.sqrt(self.v.size(0))\n        self.v.data.normal_(mean=0, std=stdv)\n\n    def forward(self, hidden, encoder_outputs):\n        '''\n        :param hidden: \n            previous hidden state of the decoder, in shape (layers*directions,B,H)\n        :param encoder_outputs:\n            encoder outputs from Encoder, in shape (T,B,H)\n        :return\n            attention energies in shape (B,T)\n        '''\n        max_len = encoder_outputs.size(0)\n        H = hidden.repeat(max_len, 1, 1).transpose(0, 1)\n        encoder_outputs = encoder_outputs.transpose(0, 1)  # [B*T*H]\n        # compute attention score\n        attn_energies = self.score(H, encoder_outputs)\n        # normalize with softmax\n        return nn.functional.softmax(attn_energies, dim=1).unsqueeze(1)\n\n    def score(self, hidden, encoder_outputs):\n        # print('hidden: ', hidden.size())\n        # print('encoder_outputs: ', encoder_outputs.size())\n        # [B*T*2H]->[B*T*H]\n        energy = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2)))\n        # energy = F.tanh(self.attn(hidden + encoder_outputs)) # [B*T*2H]->[B*T*H]\n        energy = energy.transpose(2, 1)  # [B*H*T]\n        v = self.v.repeat(\n            encoder_outputs.data.shape[0], 1).unsqueeze(1)  # [B*1*H]\n        energy = torch.bmm(v, energy)  # [B*1*T]\n        return energy.squeeze(1)  # [B*T]\n\n\nclass AttentionRNNCell(nn.Module):  # Bahdanau attention based\n    def __init__(self, hidden_dims, embedded_dims, nr_classes,\n                 n_layers=1, dropout_p=0, bidirectional=False):\n        super(AttentionRNNCell, self).__init__()\n\n        self.hidden_dims = hidden_dims\n        self.embedded_dims = embedded_dims\n        self.nr_classes = nr_classes\n        self.n_layers = n_layers\n        self.dropout_p = dropout_p\n\n        self.embedding = nn.Embedding(nr_classes, nr_classes)\n        self.embedding.weight.data = torch.eye(nr_classes)\n        self.dropout = nn.Dropout(dropout_p)\n        self.word_linear = nn.Linear(nr_classes, hidden_dims)\n        self.attn = Attn('concat', hidden_dims, embedded_dims)\n        self.rnn = nn.GRUCell(2 * hidden_dims + embedded_dims, hidden_dims)\n        self.out = nn.Linear(hidden_dims, nr_classes)\n\n    def forward(self, word_input, last_hidden, encoder_outputs, train=True):\n        '''\n        :param word_input:\n            word input for current time step, in shape (B)\n        :param last_hidden:\n            last hidden stat of the decoder, in shape (layers*direction*B*H)\n        :param encoder_outputs:\n            encoder outputs in shape (T*B*H)\n        :return\n            decoder output\n        Note: we run this one step at a time i.e. you should use a outer loop \n            to process the whole sequence\n        '''\n\n        # Get the embedding of the current input word (last output word)\n        batch_size = word_input.size(0)\n        word_embedded_onehot = self.embedding(word_input.to(last_hidden.device).type(\n            torch.long)).view(1, batch_size, -1).to(last_hidden.device)  # (1,N,V)\n        word_embedded = self.word_linear(word_embedded_onehot)  # (1, N, H)\n\n        # Calculate attention weights and apply to encoder outputs\n        # print('pre encoder_outputs: ', encoder_outputs.size())\n        attn_weights = self.attn(last_hidden, encoder_outputs)\n        context = attn_weights.bmm(\n            encoder_outputs.transpose(0, 1))  # (N, 1, V)\n        context = context.transpose(0, 1)  # (1, N, V)\n\n        rnn_input = torch.cat([word_embedded, context], 2)\n        last_hidden = last_hidden.view(batch_size, -1)\n        rnn_input = rnn_input.view(batch_size, -1)\n        hidden = self.rnn(rnn_input, last_hidden)\n\n        if train:\n            output = nn.functional.log_softmax(self.out(hidden), dim=1)\n        else:\n            output = nn.functional.softmax(self.out(hidden), dim=1)\n        return output, hidden, attn_weights\n"""
decoders/balance_cross_entropy_loss.py,7,"b""import torch\nimport torch.nn as nn\n\n\nclass BalanceCrossEntropyLoss(nn.Module):\n    '''\n    Balanced cross entropy loss.\n    Shape:\n        - Input: :math:`(N, 1, H, W)`\n        - GT: :math:`(N, 1, H, W)`, same shape as the input\n        - Mask: :math:`(N, H, W)`, same spatial shape as the input\n        - Output: scalar.\n\n    Examples::\n\n        >>> m = nn.Sigmoid()\n        >>> loss = nn.BCELoss()\n        >>> input = torch.randn(3, requires_grad=True)\n        >>> target = torch.empty(3).random_(2)\n        >>> output = loss(m(input), target)\n        >>> output.backward()\n    '''\n\n    def __init__(self, negative_ratio=3.0, eps=1e-6):\n        super(BalanceCrossEntropyLoss, self).__init__()\n        self.negative_ratio = negative_ratio\n        self.eps = eps\n\n    def forward(self,\n                pred: torch.Tensor,\n                gt: torch.Tensor,\n                mask: torch.Tensor,\n                return_origin=False):\n        '''\n        Args:\n            pred: shape :math:`(N, 1, H, W)`, the prediction of network\n            gt: shape :math:`(N, 1, H, W)`, the target\n            mask: shape :math:`(N, H, W)`, the mask indicates positive regions\n        '''\n        positive = (gt * mask).byte()\n        negative = ((1 - gt) * mask).byte()\n        positive_count = int(positive.float().sum())\n        negative_count = min(int(negative.float().sum()),\n                            int(positive_count * self.negative_ratio))\n        loss = nn.functional.binary_cross_entropy(\n            pred, gt, reduction='none')[:, 0, :, :]\n        positive_loss = loss * positive.float()\n        negative_loss = loss * negative.float()\n        negative_loss, _ = torch.topk(negative_loss.view(-1), negative_count)\n\n        balance_loss = (positive_loss.sum() + negative_loss.sum()) /\\\n            (positive_count + negative_count + self.eps)\n\n        if return_origin:\n            return balance_loss, loss\n        return balance_loss\n"""
decoders/classification.py,4,"b'import torch\nimport torch.nn as nn\n\n\nclass ClassificationDecoder(nn.Module):\n\n    def __init__(self):\n        super(ClassificationDecoder, self).__init__()\n\n        self.fc = torch.nn.Linear(256, 10)\n        self.criterion = torch.nn.CrossEntropyLoss()\n\n    def forward(self, feature_map, targets=None, train=False):\n        x = torch.max(torch.max(feature_map, dim=3)[0], dim=2)[0]\n        x = self.fc(x)\n        pred = x\n        if train:\n            loss = self.criterion(pred, targets)\n            return loss, pred\n        else:\n            return pred\n'"
decoders/crnn.py,3,"b'import torch\nimport torch.nn as nn\nimport config\nfrom .ctc_loss import CTCLoss\nfrom concern.charsets import DefaultCharset\n\n\nclass BidirectionalLSTM(nn.Module):\n\n    def __init__(self, nIn, nHidden, nOut):\n        super(BidirectionalLSTM, self).__init__()\n\n        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n        self.embedding = nn.Linear(nHidden * 2, nOut)\n\n    def forward(self, input):\n        recurrent, _ = self.rnn(input)\n        T, b, h = recurrent.size()\n        t_rec = recurrent.view(T * b, h)\n\n        output = self.embedding(t_rec)  # [T * b, nOut]\n        output = output.view(T, b, -1)\n\n        return output\n\n\nclass CRNNDecoder(nn.Module):\n\n    def __init__(self, charset=DefaultCharset(),\n                 inner_channels=256, in_channels=256, need_reduce=False, reduce_func=None, loss_func=\'pytorch\'):\n\n        super().__init__()\n        rnn_input = in_channels\n        if need_reduce:\n            rnn_input = inner_channels\n        self.rnn = nn.Sequential(\n            BidirectionalLSTM(rnn_input, inner_channels, inner_channels),\n            BidirectionalLSTM(inner_channels, inner_channels, len(charset))\n        )\n        self.inner_channels = inner_channels\n        if need_reduce:\n            if reduce_func == \'conv\':\n                self.fpn2rnn = self._init_conv(in_channels)\n            elif need_reduce and reduce_func == \'pooling\':\n                self.fpn2rnn = self._init_pooling()\n        self.softmax = nn.Softmax()\n        if loss_func == \'pytorch\':\n            self.ctc_loss = nn.CTCLoss(zero_infinity=True)\n        else:\n            self.ctc_loss = CTCLoss()\n\n    def _init_conv(self, in_channels, stride=(2, 1), padding=(0, 1)):\n        encode = nn.Sequential(\n            self.conv_bn_relu(in_channels, self.inner_channels),\n            nn.MaxPool2d((2, 2), (2, 2), (0, 0)),\n            self.conv_bn_relu(self.inner_channels, self.inner_channels),\n            nn.MaxPool2d(stride, stride, (0, 0)),\n            self.conv_bn_relu(self.inner_channels, self.inner_channels),\n            nn.MaxPool2d(stride, stride, (0, 0)),\n\n            # nn.AdaptiveMaxPool2d((1, None))\n            # self.conv_bn_relu(self.inner_channels, self.inner_channels,\n            #                    kernel_size=(2, 3),\n            #                    stride=stride, padding=padding),\n        )\n        return encode\n\n    def _init_pooling(self):\n        return nn.AdaptiveMaxPool2d((1, None))\n\n    def conv_bn_relu(self, input_channels, output_channels,\n                     kernel_size=3, stride=1, padding=1):\n        return nn.Sequential(nn.Conv2d(\n            input_channels, output_channels,\n            kernel_size=kernel_size, stride=stride, padding=padding),\n            nn.BatchNorm2d(output_channels),\n            nn.ReLU(inplace=True),)\n\n\n    def forward(self, feature, targets=None, lengths=None, train=False):\n        b, c, h, w = feature.size()\n        # print(feature)\n        # print(feature.size())\n        if h > 1:\n            feature = self.fpn2rnn(feature)\n            b, c, h, w = feature.size()\n        assert h == 1, ""the height of conv must be 1""\n\n        feature = feature.squeeze(2)  # N, C, W\n        feature = feature.permute(2, 0, 1)  # W, N, C\n        for r in self.rnn:\n            r.rnn.flatten_parameters()\n        pred = self.rnn(feature)\n\n        if train:\n            pred = nn.functional.log_softmax(pred, dim=2).to(torch.float64)\n            pred_size = torch.Tensor([pred.size(0)] * b).int()\n            loss = self.ctc_loss(pred, targets, pred_size, lengths)\n            return loss, pred\n        else:\n            pred = pred.permute(1, 2, 0)\n            pred = pred.unsqueeze(2)\n            pred = nn.functional.softmax(pred, dim=1)\n            return pred\n'"
decoders/ctc_decoder.py,2,"b""#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# File              : ctc_decoder.py\n# Author            : Zhaoyi Wan <wanzhaoyi@megvii.com>\n# Date              : 18.12.2018\n# Last Modified Date: 20.01.2019\n# Last Modified By  : Zhaoyi Wan <wanzhaoyi@megvii.com>\n\nimport torch\nimport torch.nn as nn\nfrom concern.charsets import DefaultCharset\n\n\nclass CTCDecoder(nn.Module):\n    def __init__(self, in_channels, charset=DefaultCharset(), inner_channels=256, **kwargs):\n        super(CTCDecoder, self).__init__()\n        self.ctc_loss = nn.CTCLoss(reduction='mean')\n        self.inner_channels = inner_channels\n        self.encode = self._init_encoder(in_channels)\n\n        self.pred_conv = nn.Conv2d(\n            inner_channels, len(charset), kernel_size=1, bias=True, padding=0)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n        self.blank = 0\n        if 'blank' in kwargs:\n            self.blank = kwargs['blank']\n\n    def _init_encoder(self, in_channels, stride=(2, 1), padding=(0, 1)):\n        encode = nn.Sequential(\n            self.conv_bn_relu(in_channels, self.inner_channels),\n            self.conv_bn_relu(self.inner_channels, self.inner_channels),\n            nn.MaxPool2d((2, 2), (2, 2), (0, 0)),\n            self.conv_bn_relu(self.inner_channels, self.inner_channels),\n            self.conv_bn_relu(self.inner_channels, self.inner_channels),\n            nn.MaxPool2d(stride, stride, (0, 0)),\n            self.conv_bn_relu(self.inner_channels, self.inner_channels),\n            self.conv_bn_relu(self.inner_channels, self.inner_channels),\n            nn.MaxPool2d(stride, stride, (0, 0)),\n            self.conv_bn_relu(self.inner_channels, self.inner_channels,\n                              kernel_size=(2, 3),\n                              stride=stride, padding=padding),\n        )\n        return encode\n\n    def conv_bn_relu(self, input_channels, output_channels,\n                     kernel_size=3, stride=1, padding=1):\n        return nn.Sequential(nn.Conv2d(\n            input_channels, output_channels,\n            kernel_size=kernel_size, stride=stride, padding=padding),\n            nn.BatchNorm2d(output_channels),\n            nn.ReLU(inplace=True),)\n\n    def forward(self, feature, targets=None, lengths=None, train=False):\n        pred = self.encode(feature)\n        pred = self.pred_conv(pred)\n        if train:\n            pred = self.softmax(pred)\n            pred = pred.select(2, 0)  # N, C, W\n            pred = pred.permute(2, 0, 1)  # W, N, C\n            input_lengths = torch.zeros((feature.size()[0], ), dtype=torch.int) + 32\n            loss = self.ctc_loss(pred, targets, input_lengths, lengths)\n            return loss, pred.permute(1, 2, 0)\n        else:\n            pred = nn.functional.softmax(pred, dim=1)\n            return pred\n"""
decoders/ctc_decoder2d.py,5,"b""import torch\nimport torch.nn as nn\n\nfrom concern.charsets import DefaultCharset\n\n\nclass CTCDecoder2D(nn.Module):\n    def __init__(self, in_channels, charset=DefaultCharset(),\n                 inner_channels=256, stride=1, blank=0, **kwargs):\n        super(CTCDecoder2D, self).__init__()\n        self.charset = charset\n        from ops import ctc_loss_2d\n        self.ctc_loss = ctc_loss_2d\n\n        self.inner_channels = inner_channels\n        self.pred_mask = nn.Sequential(\n            nn.AvgPool2d(kernel_size=(stride, stride),\n                         stride=(stride, stride)),\n            nn.Conv2d(in_channels, inner_channels, kernel_size=3, padding=1),\n            nn.Conv2d(inner_channels, 1, kernel_size=1),\n            nn.Softmax(dim=2))\n\n        self.pred_classify = nn.Sequential(\n            nn.AvgPool2d(kernel_size=(stride, stride),\n                         stride=(stride, stride)),\n            nn.Conv2d(in_channels, inner_channels, kernel_size=3, padding=1),\n            nn.Conv2d(inner_channels, len(charset), kernel_size=1))\n        self.blank = blank\n        self.tiny = torch.tensor(torch.finfo().tiny, requires_grad=False)\n        self.register_buffer('saved_tiny', self.tiny)\n\n    def forward(self, feature, targets=None, lengths=None, train=False,\n                masks=None, segs=None):\n        tiny = self.saved_tiny\n        if isinstance(feature, tuple):\n            feature = feature[-1]\n        masking = self.pred_mask(feature)\n        # mask = masking / torch.max(masking.sum(dim=2, keepdim=True), tiny)\n        mask = masking\n        classify = self.pred_classify(feature)\n        classify = nn.functional.softmax(classify, dim=1)\n        if self.training:\n            pred = mask * classify  # N, C, H ,W\n            pred = torch.log(torch.max(pred, tiny))\n            pred = pred.permute(3, 2, 0, 1).contiguous()  # W, H, N, C\n            input_lengths = torch.zeros(\n                (feature.size()[0], )) + pred.shape[0]\n            loss = self.ctc_loss(pred, targets.long(), input_lengths.long().to(\n                pred.device), lengths.long()) / lengths.float()\n            # return loss, pred.permute(2, 3, 1, 0)\n            return loss, pred\n        else:\n            return classify, mask\n\n    def mask_loss(self, mask, weight, gt):\n        batch_size, _, height, _ = mask.shape\n        loss = nn.functional.nll_loss(\n            (mask.permute(0, 1, 3, 2).reshape(-1, height) + self.saved_tiny).log(),\n            gt.reshape(-1), reduction='none').view(batch_size, -1).mean(1)\n        return loss * weight\n\n    def classify_loss(self, classify, weight, gt):\n        batch_size, classes = classify.shape[:2]\n        loss = nn.functional.nll_loss(\n            (classify.permute(0, 2, 3, 1).reshape(-1, classes) + self.saved_tiny).log(),\n            gt.reshape(-1), reduction='none').view(batch_size, -1)\n        position_weights = (gt.view(batch_size, -1) == self.blank).float()\n        loss = loss * position_weights\n\n        return loss.mean(1) * weight\n"""
decoders/ctc_loss.py,25,"b'import torch\nimport torch.nn as nn\nimport numpy as np\nimport time\n\nclass CTCLoss(nn.Module):\n\n    def __init__(self, blank=0, reduction=\'mean\'):\n        r""""""The Connectionist Temporal Classification loss.\n\n\tArgs:\n\t    blank (int, optional): blank label. Default :math:`0`.\n\t    reduction (string, optional): Specifies the reduction to apply to the output:\n\t\t\'none\' | \'mean\' | \'sum\'. \'none\': no reduction will be applied,\n\t\t\'mean\': the output losses will be divided by the target lengths and\n\t\tthen the mean over the batch is taken. Default: \'mean\'\n\n\tInputs:\n\t    log_probs: Tensor of size :math:`(T, N, C)` where `C = number of characters in alphabet including blank`,\n\t\t`T = input length`, and `N = batch size`.\n\t\tThe logarithmized probabilities of the outputs\n\t\t(e.g. obtained with :func:`torch.nn.functional.log_softmax`).\n\t    targets: Tensor of size :math:`(N, S)` or `(sum(target_lengths))`.\n\t\tTargets (cannot be blank). In the second form, the targets are assumed to be concatenated.\n\t    input_lengths: Tuple or tensor of size :math:`(N)`.\n\t\tLengths of the inputs (must each be :math:`\\leq T`)\n\t    target_lengths: Tuple or tensor of size  :math:`(N)`.\n\t\tLengths of the targets\n\n\n\tExample::\n\n\t    >>> ctc_loss = CTCLoss()\n\t    >>> log_probs = torch.randn(12, 16, 20).log_softmax(2).detach().requires_grad_()\n\t    >>> targets = torch.randint(1, 20, (16, 30), dtype=torch.long)\n\t    >>> input_lengths = torch.full((16,), 12, dtype=torch.long)\n\t    >>> target_lengths = torch.randint(10,30,(16,), dtype=torch.long)\n\t    >>> loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n\t    >>> loss.backward()\n\n\tReference:\n\t    A. Graves et al.: Connectionist Temporal Classification:\n\t    Labelling Unsegmented Sequence Data with Recurrent Neural Networks:\n\t    https://www.cs.toronto.edu/~graves/icml_2006.pdf\n\t""""""\n        super(CTCLoss, self).__init__()\n        self.blank = blank\n        self.reduction = reduction\n\n\n    def expand_with_blank(self, targets):\n        N, S = targets.shape\n        blank = torch.tensor([self.blank], dtype=torch.long).repeat(targets.shape)\n        expanded_targets = torch.cat([blank.unsqueeze(-1), targets.unsqueeze(-1)], -1)\n        expanded_targets = expanded_targets.view(N, -1)\n        expanded_targets = torch.cat([expanded_targets, blank[:, 0:1]], dim=-1)\n        return expanded_targets\n\n\n    def log_add(self, a, b):\n        x, y = torch.max(a, b), torch.min(a, b)\n        return x + torch.log1p(torch.exp(y - x))\n\n\n    def forward(self, log_probs, targets, input_lengths, target_lengths):\n        """"""\n        Args:\n        log_probs: :math:`(T, N, C)` where `C = number of characters in alphabet including blank`,\n            `T = input length`, and `N = batch size`.\n            The logarithmized probabilities of the outputs\n            (e.g. obtained with :func:`torch.nn.functional.log_softmax`).\n        targets: :math:`(N, S)` or `(sum(target_lengths))`[NOT IMPLEMENTED YET].\n            Targets (cannot be blank). In the second form, the targets are assumed to be concatenated.\n        input_lengths: :math:`(N)`.\n            Lengths of the inputs (must each be :math:`\\leq T`)\n        target_lengths: :math:`(N)`.\n            Lengths of the targets\n\t""""""\n        targets = targets.type(torch.long)\n        expanded_targets = self.expand_with_blank(targets)\n        N, S = expanded_targets.shape\n        T = log_probs.shape[0]\n\n        tiny = torch.finfo().tiny\n        # probability of current time step\n        #probability = torch.zeros(S, N) + torch.log(torch.tensor(tiny))\n        probability = torch.log(torch.zeros(S, N) + tiny)\n        probability[0] = log_probs[0, :, self.blank]\n        batch_indices = torch.linspace(0, N-1, N).type(torch.long) * log_probs.shape[-1]\n        indices = batch_indices + expanded_targets[:, 1]\n        probability[1] = log_probs[0].take(indices)\n\n        # (S - 2, N)\n        # The previous token can NOT be ignored when the second previous token is identical with\n        # specific token.\n        mask_skipping = torch.ne(expanded_targets[:, 2:], expanded_targets[:, :-2]).transpose(0, 1)\n        mask_skipping = mask_skipping.type(torch.float)\n        mask_not_skipping = 1 - mask_skipping\n        \n        for timestep in range(1, T):\n            new_probability1 = self.log_add(probability[1:], probability[:-1])\n            new_probability2 = self.log_add(new_probability1[1:], probability[:-2]) * mask_skipping +\\\n                    new_probability1[1:] * mask_not_skipping\n            new_probability = torch.cat([probability[:1],\n                new_probability1[:1],\n                new_probability2], dim=0)\n            probability = new_probability + log_probs[timestep].gather(1, expanded_targets).transpose(0, 1)\n\n            \'\'\'\n            probability[2:] = torch.log(torch.exp(probability[2:]) +\\\n                    torch.exp(probability[1:-1]) +\\\n                    torch.exp(probability[:-2]) * mask_skipping)\n            probability[1] = torch.log(torch.exp(probability[0]) + torch.exp(probability[1]) + tiny)\n            probability = probability + log_probs[timestep].gather(1, expanded_targets).transpose(0, 1)\n            \'\'\'\n        lengths = (target_lengths * 2 + 1).unsqueeze(0)\n        loss = self.log_add(probability.gather(0, lengths - 1), probability.gather(0, lengths - 2))\n        #import pdb; pdb.set_trace()\n        loss = loss.squeeze(0)\n        if self.reduction == \'mean\':\n            return -(loss / target_lengths.type(torch.float))\n        return -loss\n'"
decoders/ctc_loss2d.py,29,"b'import torch\nimport torch.nn as nn\nimport numpy as np\nimport time\nimport warnings\n\n\nclass CTCLoss2D(nn.Module):\n    def __init__(self, blank=0, reduction=\'mean\'):\n        r""""""The python-implementation of 2D-CTC loss.\n        NOTICE: This class is only for the useage of understanding the principle of 2D-CTC.\n            Please use `ops.ctc_loss_2d` for practice.\n        Args:\n            blank (int, optional): blank label. Default :math:`0`.\n            reduction (string, optional): Specifies the reduction to apply to the output:\n                \'none\' | \'mean\' | \'sum\'. \'none\': no reduction will be applied,\n                \'mean\': the output losses will be divided by the target lengths and\n                then the mean over the batch is taken. Default: \'mean\'\n\n        Inputs:\n            mask: Tensor of size :math:`(T, H, N)` where `H = height`,\n                `T = input length`, and `N = batch size`.\n                The logarithmized path transition probabilities.\n                (e.g. obtained with :func:`torch.nn.functional.log_softmax`).\n            classify: Tensor of size :math:`(T, H, N, C)` where `C = number of classes`, `H = height`, `T = input length`, and `N = batch size`.\n                The logarithmized character classification probabilities at all possible path pixels.\n                (e.g. obtained with :func:`torch.nn.functional.log_softmax`).\n            targets: Tensor of size :math:`(N, S)` or `(sum(target_lengths))`.\n                Targets (cannot be blank). In the second form, the targets are assumed to be concatenated.\n            input_lengths: Tuple or tensor of size :math:`(N)`.\n                Lengths of the inputs (must each be :math:`\\leq T`)\n            target_lengths: Tuple or tensor of size  :math:`(N)`.\n                Lengths of the targets\n\n        Example::\n\n            >>> ctc_loss = CTCLoss2D()\n            >>> N, H, T, C = 16, 8, 32, 20\n            >>> mask = torch.randn(T, H, N).log_softmax(1).detach().requires_grad_()\n            >>> classify = torch.randn(T, H, N, C).log_softmax(3).detach().requires_grad_()\n            >>> targets = torch.randint(1, C, (N, C), dtype=torch.long)\n            >>> input_lengths = torch.full((N,), T, dtype=torch.long)\n            >>> target_lengths = torch.randint(10, 31, (N,), dtype=torch.long)\n            >>> loss = ctc_loss(mask, classify, targets, input_lengths, target_lengths)\n            >>> loss.backward()\n\n        Reference:\n            2D-CTC for Scene Text Recognition, https://arxiv.org/abs/1907.09705.\n        """"""\n        super(CTCLoss2D, self).__init__()\n        warnings.warn(\n            ""NOTICE: This class is only for the useage of understanding the principle of 2D-CTC.""\n            ""Please use `ops.ctc_loss_2d` for practice."")\n        self.blank = blank\n        self.reduction = reduction\n        self.register_buffer(\'tiny\', torch.tensor(torch.finfo().tiny, requires_grad=False))\n        self.register_buffer(\'blank_buffer\', torch.tensor([self.blank], dtype=torch.long))\n        self.register_buffer(\'zeros\',  torch.log(self.tiny))\n        self.registered = False\n\n    def expand_with_blank(self, targets):\n        N, S = targets.shape\n        blank = self.blank_buffer.repeat(targets.shape)\n        expanded_targets = torch.cat([blank.unsqueeze(-1), targets.unsqueeze(-1)], -1)\n        expanded_targets = expanded_targets.view(N, -1)\n        expanded_targets = torch.cat([expanded_targets, blank[:, 0:1]], dim=-1)\n        return expanded_targets\n\n    def log_add(self, a, b):\n        x, y = torch.max(a, b), torch.min(a, b)\n        return x + torch.log1p(torch.exp(y - x))\n\n    def log_sum(self, x, dim, keepdim=False):\n        tiny = self.tiny\n        return torch.log(torch.max(\n            torch.sum(torch.exp(x), dim=dim, keepdim=keepdim), tiny))\n\n    def safe_log_sum(self, x, keepdim=False):\n        result = x[:, 0]\n        for i in range(1, x.size(1)):\n            result = self.log_add(result, x[:, i])\n        if keepdim:\n            result = result.unsqueeze(1)\n        return result\n\n    def forward(self, mask, classify, targets, input_lengths, target_lengths):\n        r""""""\n        mask: Tensor of size :math:`(T, H, N)` where `H = height`,\n            `T = input length`, and `N = batch size`.\n            The logarithmized path transition probabilities.\n            (e.g. obtained with :func:`torch.nn.functional.log_softmax`).\n        classify: Tensor of size :math:`(T, H, N, C)` where `C = number of classes`, `H = height`, `T = input length`, and `N = batch size`.\n            The logarithmized character classification probabilities at all possible path pixels.\n            (e.g. obtained with :func:`torch.nn.functional.log_softmax`).\n        targets: :math:`(N, S)` or `(sum(target_lengths))`[NOT IMPLEMENTED YET].\n            Targets (cannot be blank). In the second form, the targets are assumed to be concatenated.\n        input_lengths: :math:`(N)`.\n            Lengths of the inputs (must each be :math:`\\leq T`)\n        target_lengths: :math:`(N)`.\n            Lengths of the targets\n        """"""\n        device = classify.device\n        targets = targets.type(torch.long)\n        expanded_targets = self.expand_with_blank(targets)\n        N, S = expanded_targets.shape\n        T, H = classify.shape[:2]\n        targets_indices = expanded_targets.repeat(H, 1, 1)\n\n        tiny = self.tiny\n        # probability of current time step\n        probability = torch.log((torch.zeros(S, H, N) + tiny) / H).to(device)\n        probability[0] = classify[0, :, :, self.blank]\n        probability[1] = classify[0].gather(\n            2, targets_indices[:, :, 1:2]).permute(2, 0, 1)\n\n        # (S - 2, N)\n        # The previous token can NOT be ignored when the second previous token is identical with\n        # specific token.\n        mask_skipping = torch.ne(expanded_targets[:, 2:], expanded_targets[:, :-2]).transpose(0, 1)\n        mask_skipping = mask_skipping.unsqueeze(1).type(torch.float).to(device)\n        mask_not_skipping = 1 - mask_skipping\n        length_indices = torch.linspace(0, S - 1, S).repeat(N, 1, 1).transpose(0, 2).to(device)\n        zeros = self.zeros.repeat(S, 1, N).view(S, 1, N)\n\n        count_computable = torch.cat([mask_skipping[0:1] + 1,\n                                      mask_skipping[0:1] + 1,\n                                      mask_skipping + 1], dim=0)  # (S, N)\n        count_computable = torch.cumsum(count_computable, dim=0)\n\n        for timestep in range(1, T):\n            mask_uncomputed = (length_indices > count_computable[timestep]).type(torch.float)\n            height_summed = self.log_sum(\n                probability + mask[timestep - 1].unsqueeze(0), dim=1, keepdim=True)\n\n            height_summed = height_summed * (1 - mask_uncomputed) + zeros * mask_uncomputed\n\n            new_probability1 = self.log_add(height_summed[1:], height_summed[:-1])  # (S-1, H, N)\n            new_probability2 = self.log_add(new_probability1[1:], height_summed[:-2]) * mask_skipping\\\n                + new_probability1[1:] * mask_not_skipping\n\n            new_probability = torch.cat([height_summed[:1],\n                                         new_probability1[:1],\n                                         new_probability2], dim=0)\n            probability = new_probability + \\\n                classify[timestep].gather(2, targets_indices).permute(2, 0, 1)\n        probability = self.safe_log_sum(probability + mask[T-1].unsqueeze(0))  # (S, N)\n        lengths = (target_lengths * 2 + 1).unsqueeze(0)\n        loss = self.log_add(probability.gather(0, lengths - 1), probability.gather(0, lengths - 2))\n        loss = loss.squeeze(0)\n        if self.reduction == \'mean\':\n            return -(loss / target_lengths.type(torch.float))\n        elif self.reduction == \'sum\':\n            return -loss.sum()\n        return -loss\n'"
decoders/dice_loss.py,6,"b""import torch\nimport torch.nn as nn\nimport numpy as np\nimport cv2\nfrom scipy import ndimage\n\n\nclass DiceLoss(nn.Module):\n    '''\n    Loss function from https://arxiv.org/abs/1707.03237,\n    where iou computation is introduced heatmap manner to measure the\n    diversity bwtween tow heatmaps.\n    '''\n    def __init__(self, eps=1e-6):\n        super(DiceLoss, self).__init__()\n        self.eps = eps\n\n    def forward(self, pred: torch.Tensor, gt, mask, weights=None):\n        '''\n        pred: one or two heatmaps of shape (N, 1, H, W),\n            the losses of tow heatmaps are added together.\n        gt: (N, 1, H, W)\n        mask: (N, H, W)\n        '''\n        assert pred.dim() == 4, pred.dim()\n        return self._compute(pred, gt, mask, weights)\n\n    def _compute(self, pred, gt, mask, weights):\n        if pred.dim() == 4:\n            pred = pred[:, 0, :, :]\n            gt = gt[:, 0, :, :]\n        assert pred.shape == gt.shape\n        assert pred.shape == mask.shape\n        if weights is not None:\n            assert weights.shape == mask.shape\n            mask = weights * mask\n\n        intersection = (pred * gt * mask).sum()\n        union = (pred * mask).sum() + (gt * mask).sum() + self.eps\n        loss = 1 - 2.0 * intersection / union\n        assert loss <= 1\n        return loss\n\n\nclass LeakyDiceLoss(nn.Module):\n    '''\n    Variation from DiceLoss.\n    The coverage and union are computed separately.\n    '''\n    def __init__(self, eps=1e-6, coverage_scale=5.0):\n        super(LeakyDiceLoss, self).__init__()\n        self.eps = eps\n        self.coverage_scale = coverage_scale\n\n    def forward(self, pred, gt, mask):\n        if pred.dim() == 4:\n            pred = pred[:, 0, :, :]\n            gt = gt[:, 0, :, :]\n        assert pred.shape == gt.shape\n        assert pred.shape == mask.shape\n\n        coverage = (pred * mask * gt).sum() / ((gt * mask).sum() + self.eps)\n        assert coverage <= 1\n        coverage = 1 - coverage\n        excede = (pred * mask * gt).sum() / ((pred * mask).sum() + self.eps)\n        assert excede <= 1\n        excede = 1 - excede\n        loss = coverage * self.coverage_scale + excede\n        return loss, dict(coverage=coverage, excede=excede)\n\n\nclass InstanceDiceLoss(DiceLoss):\n    '''\n    DiceLoss normalized on each instance.\n    Input:\n        pred: (N, 1, H, W)\n        gt: (N, 1, H, W)\n        mask: (N, H, W)\n    Note: This class assume that input tensors are on gpu,\n        while cput computation is required to find union areas.\n    '''\n    REDUCTION = ['mean', 'sum', 'none']\n\n    def __init__(self, threshold=0.3, iou_thresh=0.2, reduction=None,\n                 max_regions=100, eps=1e-6):\n        nn.Module.__init__(self)\n        self.threshold = threshold\n        self.iou_thresh = iou_thresh\n        self.reduction = reduction\n        if self.reduction is None:\n            self.reduction = 'mean'\n        assert self.reduction in self.REDUCTION\n        self.max_regions = max_regions\n        self.eps = eps\n\n    def label(self, tensor_on_gpu, blur=None):\n        '''\n        Args:\n            tensor_on_gpu: (N, 1, H, W)\n            blur: Lambda. If exists, each instance will be blured using `blur`.\n        '''\n        tensor = tensor_on_gpu.cpu().detach().numpy()\n\n        instance_maps = []\n        instance_counts = []\n        for batch_index in range(tensor_on_gpu.shape[0]):\n            instance = tensor[batch_index]\n            if blur is not None:\n                instance = blur(instance)\n            lable_map, instance_count = ndimage.label(instance[0])\n            instance_count = min(self.max_regions, instance_count)\n            instance_map = []\n            for index in range(1, instance_count):\n                instance = torch.from_numpy(\n                        lable_map == index).to(tensor_on_gpu.device).type(torch.float32)\n                instance_map.append(instance)\n            instance_maps.append(instance_map)\n        return instance_maps, instance_counts\n\n    def iou(self, pred, gt):\n        overlap = (pred * gt).sum()\n        return max(overlap / pred.sum(), overlap / gt.sum())\n\n    def replace_or_add(self, dest, value):\n        if dest is None:\n            return value\n        if value is None:\n            return dest\n        return dest + value\n\n    def forward(self, pred, gt, mask):\n        # pred_label_maps: N, P, H, W, where P is the number of regions.\n        torch.cuda.synchronize()\n        pred_label_maps, _ = self.label(pred > self.threshold)\n        gt_label_maps, _ = self.label(gt)\n\n        losses = []\n        for batch_index, gt_instance_maps in enumerate(gt_label_maps):\n            pred_instance_maps = pred_label_maps[batch_index]\n            if gt_instance_maps is None or pred_instance_maps is None:\n                continue\n\n            single_loss = None  # loss on a single image in a batch\n            mask_not_matched = set(range(len(pred_instance_maps)))\n            for gt_instance_map in gt_instance_maps:\n                instance_loss = None  # loss on a specific gt region\n                for instance_index, pred_instance_map in enumerate(pred_instance_maps):\n                    if self.iou(pred_instance_map, gt_instance_map) > self.iou_thresh:\n                        match_loss = self._compute(\n                                pred[batch_index][0], gt[batch_index][0],\n                                mask[batch_index] * (pred_instance_map + gt_instance_map > 0).type(torch.float32))\n                        instance_loss = self.replace_or_add(instance_loss, match_loss)\n                        if instance_index in mask_not_matched:\n                            mask_not_matched.remove(instance_index)\n                if instance_loss is None:\n                    instance_loss = self._compute(\n                            pred[batch_index][0], gt[batch_index][0],\n                            mask[batch_index] * gt_instance_map)\n                single_loss = self.replace_or_add(single_loss, instance_loss)\n\n            '''Whether to compute single loss on instances which contrain no positive sample.\n            if single_loss is None:\n                single_loss = self._compute(\n                        pred[batch_index][0], gt[batch_index][0],\n                        mask[batch_index])\n            '''\n\n            for instance_index in mask_not_matched:\n                single_loss = self.replace_or_add(\n                        single_loss,\n                        self._compute(\n                            pred[batch_index][0], gt[batch_index][0],\n                            mask[batch_index] * pred_instance_maps[instance_index]))\n\n            if single_loss is not None:\n                losses.append(single_loss)\n\n        if self.reduction == 'none':\n            loss = losses\n        else:\n            assert self.reduction in ['sum', 'mean']\n            count = len(losses)\n            loss = sum(losses)\n            if self.reduction == 'mean':\n                loss = loss / count\n        return loss\n"""
decoders/east.py,2,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass EASTDecoder(nn.Module):\n    def __init__(self, channels=256, heatmap_ratio=1.0, densebox_ratio=0.01, densebox_rescale_factor=512):\n        nn.Module.__init__(self)\n\n        self.heatmap_ratio = heatmap_ratio\n        self.densebox_ratio = densebox_ratio\n        self.densebox_rescale_factor = densebox_rescale_factor\n\n        self.head_layer = nn.Sequential(\n            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(channels),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(channels, channels // 2, kernel_size=2, stride=2, padding=0),\n            nn.BatchNorm2d(channels // 2),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(channels // 2, channels // 4, kernel_size=2, stride=2, padding=0),\n        )\n\n        self.heatmap_pred_layer = nn.Sequential(\n            nn.Conv2d(channels // 4, 1, kernel_size=1, stride=1, padding=0),\n        )\n\n        self.densebox_pred_layer = nn.Sequential(\n            nn.Conv2d(channels // 4, 8, kernel_size=1, stride=1, padding=0),\n        )\n\n    def forward(self, input, label, meta, train):\n        heatmap = label['heatmap']\n        heatmap_weight = label['heatmap_weight']\n        densebox = label['densebox']\n        densebox_weight = label['densebox_weight']\n\n        feature = self.head_layer(input)\n        heatmap_pred = self.heatmap_pred_layer(feature)\n        densebox_pred = self.densebox_pred_layer(feature) * self.densebox_rescale_factor\n\n        heatmap_loss = F.binary_cross_entropy_with_logits(heatmap_pred, heatmap, reduction='none')\n        heatmap_loss = (heatmap_loss * heatmap_weight).mean(dim=(1, 2, 3))\n        densebox_loss = F.mse_loss(densebox_pred, densebox, reduction='none')\n        densebox_loss = (densebox_loss * densebox_weight).mean(dim=(1, 2, 3))\n\n        loss = heatmap_loss * self.heatmap_ratio + densebox_loss * self.densebox_ratio\n\n        pred = {\n            'heatmap': F.sigmoid(heatmap_pred),\n            'densebox': densebox_pred,\n        }\n        metrics = {\n            'heatmap_loss': heatmap_loss,\n            'densebox_loss': densebox_loss,\n        }\n        if train:\n            return loss, pred, metrics\n        else:\n            return pred\n"""
decoders/l1_loss.py,6,"b""import torch\nimport torch.nn as nn\n\n\nclass MaskL1Loss(nn.Module):\n    def __init__(self):\n        super(MaskL1Loss, self).__init__()\n\n    def forward(self, pred: torch.Tensor, gt, mask):\n        loss = (torch.abs(pred[:, 0] - gt) * mask).sum() / mask.sum()\n        return loss, dict(l1_loss=loss)\n\n\nclass BalanceL1Loss(nn.Module):\n    def __init__(self, negative_ratio=3.):\n        super(BalanceL1Loss, self).__init__()\n        self.negative_ratio = negative_ratio\n\n    def forward(self, pred: torch.Tensor, gt, mask):\n        '''\n        Args:\n            pred: (N, 1, H, W).\n            gt: (N, H, W).\n            mask: (N, H, W).\n        '''\n        loss = torch.abs(pred[:, 0] - gt)\n        positive = loss * mask\n        negative = loss * (1 - mask)\n        positive_count = int(mask.sum())\n        negative_count = min(\n                int((1 - mask).sum()),\n                int(positive_count * self.negative_ratio))\n        negative_loss, _ = torch.topk(negative.view(-1), negative_count)\n        negative_loss = negative_loss.sum() / negative_count\n        positive_loss = positive.sum() / positive_count\n        return positive_loss + negative_loss,\\\n            dict(l1_loss=positive_loss, nge_l1_loss=negative_loss)\n"""
decoders/pss_loss.py,18,"b""import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport ipdb\r\n\r\n\r\nclass PSS_Loss(nn.Module):\r\n\r\n    def __init__(self, cls_loss):\r\n        super(PSS_Loss, self).__init__()\r\n        self.eps = 1e-6\r\n        self.criterion = eval('self.' + cls_loss + '_loss')\r\n\r\n    def dice_loss(self, pred, gt, m):\r\n        intersection = torch.sum(pred*gt*m)\r\n        union = torch.sum(pred*m) + torch.sum(gt*m) + self.eps\r\n        loss = 1 - 2.0*intersection/union\r\n        if loss > 1:\r\n            print(intersection, union)\r\n            ipdb.set_trace()\r\n        # ipdb.set_trace()\r\n        return loss\r\n\r\n    def dice_ohnm_loss(self, pred, gt, m):\r\n        pos_index = (gt == 1) * (m == 1)\r\n        neg_index = (gt == 0) * (m == 1)\r\n        pos_num = pos_index.float().sum().item()\r\n        neg_num = neg_index.float().sum().item()\r\n        if pos_num == 0 or neg_num < pos_num*3.0:\r\n            return self.dice_loss(pred, gt, m)\r\n        else:\r\n            neg_num = int(pos_num*3)\r\n            pos_pred = pred[pos_index]\r\n            neg_pred = pred[neg_index]\r\n            neg_sort, _ = torch.sort(neg_pred, descending=True)\r\n            sampled_neg_pred = neg_sort[:neg_num]\r\n            pos_gt = pos_pred.clone()\r\n            pos_gt.data.fill_(1.0)\r\n            pos_gt = pos_gt.detach()\r\n            neg_gt = sampled_neg_pred.clone()\r\n            neg_gt.data.fill_(0)\r\n            neg_gt = neg_gt.detach()\r\n            tpred = torch.cat((pos_pred, sampled_neg_pred))\r\n            tgt = torch.cat((pos_gt, neg_gt))\r\n            # ipdb.set_trace()\r\n            intersection = torch.sum(tpred * tgt)\r\n            union = torch.sum(tpred) + torch.sum(gt) + self.eps\r\n            loss = 1 - 2.0 * intersection / union\r\n        return loss\r\n\r\n    def focal_loss(self, pred, gt, m, alpha=0.25, gamma=0.6):\r\n        pos_mask = (gt == 1).float()\r\n        neg_mask = (gt == 0).float()\r\n        mask = alpha*pos_mask * \\\r\n            torch.pow(1-pred.data, gamma)+(1-alpha) * \\\r\n            neg_mask*torch.pow(pred.data, gamma)\r\n        l = F.binary_cross_entropy(pred, gt, weight=mask, reduction='none')\r\n        loss = torch.sum(l*m)/(self.eps+m.sum())\r\n        loss *= 10\r\n        return loss\r\n\r\n    def wbce_orig_loss(self, pred, gt, m):\r\n        n, h, w = pred.size()\r\n        assert (torch.max(gt) == 1)\r\n        pos_neg_p = pred[m.byte()]\r\n        pos_neg_t = gt[m.byte()]\r\n        pos_mask = (pos_neg_t == 1).squeeze()\r\n        w = pos_mask.float() * (1 - pos_mask).sum().item() + \\\r\n            (1 - pos_mask).float() * pos_mask.sum().item()\r\n        w = w / (pos_mask.size(0))\r\n        loss = F.binary_cross_entropy(pos_neg_p, pos_neg_t, w, reduction='sum')\r\n        return loss\r\n\r\n    def wbce_loss(self, pred, gt, m):\r\n        pos_mask = (gt == 1).float()*m\r\n        neg_mask = (gt == 0).float()*m\r\n        # mask=(pos_mask*neg_mask.sum()+neg_mask*pos_mask.sum())/m.sum()\r\n        # loss=torch.sum(l)\r\n        mask = pos_mask * neg_mask.sum() / pos_mask.sum() + neg_mask\r\n        l = F.binary_cross_entropy(pred, gt, weight=mask, reduction='none')\r\n        loss = torch.sum(l)/(m.sum()+self.eps)\r\n        return loss\r\n\r\n    def bce_loss(self, pred, gt, m):\r\n        # ipdb.set_trace()\r\n        l = F.binary_cross_entropy(pred, gt, weight=m, reduction='sum')\r\n        loss = l/(m.sum()+self.eps)\r\n        return loss\r\n\r\n    def dice_bce_loss(self, pred, gt, m):\r\n        return (self.dice_loss(pred, gt, m) + self.bce_loss(pred, gt, m)) / 2.0\r\n\r\n    def dice_ohnm_bce_loss(self, pred, gt, m):\r\n        return (self.dice_ohnm_loss(pred, gt, m) + self.bce_loss(pred, gt, m)) / 2.0\r\n\r\n    def forward(self, pred, gt, mask, gt_type='shrink'):\r\n        if gt_type == 'shrink':\r\n            loss = self.get_loss(pred, gt, mask)\r\n            return loss\r\n        elif gt_type == 'pss':\r\n            loss = self.get_loss(pred, gt[:, :4, :, :], mask)\r\n            g_g = gt[:, 4, :, :]\r\n            g_p, _ = torch.max(pred, 1)\r\n            loss += self.criterion(g_p, g_g, mask)\r\n            return loss\r\n        elif gt_type == 'both':\r\n            pss_loss = self.get_loss(pred[:, :4, :, :], gt[:, :4, :, :], mask)\r\n            g_g = gt[:, 4, :, :]\r\n            g_p, _ = torch.max(pred, 1)\r\n            pss_loss += self.criterion(g_p, g_g, mask)\r\n            shrink_loss = self.criterion(\r\n                pred[:, 4, :, :], gt[:, 5, :, :], mask)\r\n            return pss_loss, shrink_loss\r\n        else:\r\n            return NotImplementedError('gt_type [%s] is not implemented', gt_type)\r\n\r\n    def get_loss(self, pred, gt, mask):\r\n        loss = torch.tensor(0.)\r\n        for ind in range(pred.size(1)):\r\n            loss += self.criterion(pred[:, ind, :, :], gt[:, ind, :, :], mask)\r\n        return loss\r\n"""
decoders/seg_detector.py,4,"b""from collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\n\n\nclass SegDetector(nn.Module):\n    def __init__(self,\n                 in_channels=[64, 128, 256, 512],\n                 inner_channels=256, k=10,\n                 bias=False, adaptive=False, smooth=False, serial=False,\n                 *args, **kwargs):\n        '''\n        bias: Whether conv layers have bias or not.\n        adaptive: Whether to use adaptive threshold training or not.\n        smooth: If true, use bilinear instead of deconv.\n        serial: If true, thresh prediction will combine segmentation result as input.\n        '''\n        super(SegDetector, self).__init__()\n        self.k = k\n        self.serial = serial\n        self.up5 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.up4 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.up3 = nn.Upsample(scale_factor=2, mode='nearest')\n\n        self.in5 = nn.Conv2d(in_channels[-1], inner_channels, 1, bias=bias)\n        self.in4 = nn.Conv2d(in_channels[-2], inner_channels, 1, bias=bias)\n        self.in3 = nn.Conv2d(in_channels[-3], inner_channels, 1, bias=bias)\n        self.in2 = nn.Conv2d(in_channels[-4], inner_channels, 1, bias=bias)\n\n        self.out5 = nn.Sequential(\n            nn.Conv2d(inner_channels, inner_channels //\n                      4, 3, padding=1, bias=bias),\n            nn.Upsample(scale_factor=8, mode='nearest'))\n        self.out4 = nn.Sequential(\n            nn.Conv2d(inner_channels, inner_channels //\n                      4, 3, padding=1, bias=bias),\n            nn.Upsample(scale_factor=4, mode='nearest'))\n        self.out3 = nn.Sequential(\n            nn.Conv2d(inner_channels, inner_channels //\n                      4, 3, padding=1, bias=bias),\n            nn.Upsample(scale_factor=2, mode='nearest'))\n        self.out2 = nn.Conv2d(\n            inner_channels, inner_channels//4, 3, padding=1, bias=bias)\n\n        self.binarize = nn.Sequential(\n            nn.Conv2d(inner_channels, inner_channels //\n                      4, 3, padding=1, bias=bias),\n            nn.BatchNorm2d(inner_channels//4),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(inner_channels//4, inner_channels//4, 2, 2),\n            nn.BatchNorm2d(inner_channels//4),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(inner_channels//4, 1, 2, 2),\n            nn.Sigmoid())\n        self.binarize.apply(self.weights_init)\n\n        self.adaptive = adaptive\n        if adaptive:\n            self.thresh = self._init_thresh(\n                    inner_channels, serial=serial, smooth=smooth, bias=bias)\n            self.thresh.apply(self.weights_init)\n\n        self.in5.apply(self.weights_init)\n        self.in4.apply(self.weights_init)\n        self.in3.apply(self.weights_init)\n        self.in2.apply(self.weights_init)\n        self.out5.apply(self.weights_init)\n        self.out4.apply(self.weights_init)\n        self.out3.apply(self.weights_init)\n        self.out2.apply(self.weights_init)\n\n    def weights_init(self, m):\n        classname = m.__class__.__name__\n        if classname.find('Conv') != -1:\n            nn.init.kaiming_normal_(m.weight.data)\n        elif classname.find('BatchNorm') != -1:\n            m.weight.data.fill_(1.)\n            m.bias.data.fill_(1e-4)\n\n    def _init_thresh(self, inner_channels,\n                     serial=False, smooth=False, bias=False):\n        in_channels = inner_channels\n        if serial:\n            in_channels += 1\n        self.thresh = nn.Sequential(\n            nn.Conv2d(in_channels, inner_channels //\n                      4, 3, padding=1, bias=bias),\n            nn.BatchNorm2d(inner_channels//4),\n            nn.ReLU(inplace=True),\n            self._init_upsample(inner_channels // 4, inner_channels//4, smooth=smooth, bias=bias),\n            nn.BatchNorm2d(inner_channels//4),\n            nn.ReLU(inplace=True),\n            self._init_upsample(inner_channels // 4, 1, smooth=smooth, bias=bias),\n            nn.Sigmoid())\n        return self.thresh\n\n    def _init_upsample(self,\n                       in_channels, out_channels,\n                       smooth=False, bias=False):\n        if smooth:\n            inter_out_channels = out_channels\n            if out_channels == 1:\n                inter_out_channels = in_channels\n            module_list = [\n                    nn.Upsample(scale_factor=2, mode='nearest'),\n                    nn.Conv2d(in_channels, inter_out_channels, 3, 1, 1, bias=bias)]\n            if out_channels == 1:\n                module_list.append(\n                    nn.Conv2d(in_channels, out_channels,\n                              kernel_size=1, stride=1, padding=1, bias=True))\n\n            return nn.Sequential(module_list)\n        else:\n            return nn.ConvTranspose2d(in_channels, out_channels, 2, 2)\n\n    def forward(self, features, gt=None, masks=None, training=False):\n        c2, c3, c4, c5 = features\n        in5 = self.in5(c5)\n        in4 = self.in4(c4)\n        in3 = self.in3(c3)\n        in2 = self.in2(c2)\n\n        out4 = self.up5(in5) + in4  # 1/16\n        out3 = self.up4(out4) + in3  # 1/8\n        out2 = self.up3(out3) + in2  # 1/4\n\n        p5 = self.out5(in5)\n        p4 = self.out4(out4)\n        p3 = self.out3(out3)\n        p2 = self.out2(out2)\n\n        fuse = torch.cat((p5, p4, p3, p2), 1)\n        binary = self.binarize(fuse)\n        result = OrderedDict(binary=binary)\n        if self.adaptive:\n            if self.serial:\n                fuse = torch.cat(\n                        (fuse, nn.functional.interpolate(\n                            binary, fuse.shape[2:])), 1)\n            thresh = self.thresh(fuse)\n            thresh_binary = self.step_function(binary, thresh)\n            result.update(thresh=thresh, thresh_binary=thresh_binary)\n        return result\n\n    def step_function(self, x, y):\n        return torch.reciprocal(1 + torch.exp(-self.k * (x - y)))\n"""
decoders/seg_detector_loss.py,6,"b""import sys\n\nimport torch\nimport torch.nn as nn\n\n\nclass SegDetectorLossBuilder():\n    '''\n    Build loss functions for SegDetector.\n    Details about the built functions:\n        Input:\n            pred: A dict which contains predictions.\n                thresh: The threshold prediction\n                binary: The text segmentation prediction.\n                thresh_binary: Value produced by `step_function(binary - thresh)`.\n            batch:\n                gt: Text regions bitmap gt.\n                mask: Ignore mask,\n                    pexels where value is 1 indicates no contribution to loss.\n                thresh_mask: Mask indicates regions cared by thresh supervision.\n                thresh_map: Threshold gt.\n        Return:\n            (loss, metrics).\n            loss: A scalar loss value.\n            metrics: A dict contraining partial loss values.\n    '''\n\n    def __init__(self, loss_class, *args, **kwargs):\n        self.loss_class = loss_class\n        self.loss_args = args\n        self.loss_kwargs = kwargs\n\n    def build(self):\n        return getattr(sys.modules[__name__], self.loss_class)(*self.loss_args, **self.loss_kwargs)\n\n\nclass DiceLoss(nn.Module):\n    '''\n    DiceLoss on binary.\n    For SegDetector without adaptive module.\n    '''\n\n    def __init__(self, eps=1e-6):\n        super(DiceLoss, self).__init__()\n        from .dice_loss import DiceLoss as Loss\n        self.loss = Loss(eps)\n\n    def forward(self, pred, batch):\n        loss = self.loss(pred['binary'], batch['gt'], batch['mask'])\n        return loss, dict(dice_loss=loss)\n\n\nclass AdaptiveDiceLoss(nn.Module):\n    '''\n    Integration of DiceLoss on both binary\n        prediction and thresh prediction.\n    '''\n\n    def __init__(self, eps=1e-6):\n        super(AdaptiveDiceLoss, self).__init__()\n        from .dice_loss import DiceLoss\n        self.main_loss = DiceLoss(eps)\n        self.thresh_loss = DiceLoss(eps)\n\n    def forward(self, pred, batch):\n        assert isinstance(pred, dict)\n        assert 'binary' in pred\n        assert 'thresh_binary' in pred\n\n        binary = pred['binary']\n        thresh_binary = pred['thresh_binary']\n        gt = batch['gt']\n        mask = batch['mask']\n        main_loss = self.main_loss(binary, gt, mask)\n        thresh_loss = self.thresh_loss(thresh_binary, gt, mask)\n        loss = main_loss + thresh_loss\n        return loss, dict(main_loss=main_loss, thresh_loss=thresh_loss)\n\n\nclass AdaptiveInstanceDiceLoss(nn.Module):\n    '''\n    InstanceDiceLoss on both binary and thresh_bianry.\n    '''\n\n    def __init__(self, iou_thresh=0.2, thresh=0.3):\n        super(AdaptiveInstanceDiceLoss, self).__init__()\n        from .dice_loss import InstanceDiceLoss, DiceLoss\n        self.main_loss = DiceLoss()\n        self.main_instance_loss = InstanceDiceLoss()\n        self.thresh_loss = DiceLoss()\n        self.thresh_instance_loss = InstanceDiceLoss()\n        self.weights = nn.ParameterDict(dict(\n            main=nn.Parameter(torch.ones(1)),\n            thresh=nn.Parameter(torch.ones(1)),\n            main_instance=nn.Parameter(torch.ones(1)),\n            thresh_instance=nn.Parameter(torch.ones(1))))\n\n    def partial_loss(self, weight, loss):\n        return loss / weight + torch.log(torch.sqrt(weight))\n\n    def forward(self, pred, batch):\n        main_loss = self.main_loss(pred['binary'], batch['gt'], batch['mask'])\n        thresh_loss = self.thresh_loss(pred['thresh_binary'], batch['gt'], batch['mask'])\n        main_instance_loss = self.main_instance_loss(\n            pred['binary'], batch['gt'], batch['mask'])\n        thresh_instance_loss = self.thresh_instance_loss(\n            pred['thresh_binary'], batch['gt'], batch['mask'])\n        loss = self.partial_loss(self.weights['main'], main_loss) \\\n               + self.partial_loss(self.weights['thresh'], thresh_loss) \\\n               + self.partial_loss(self.weights['main_instance'], main_instance_loss) \\\n               + self.partial_loss(self.weights['thresh_instance'], thresh_instance_loss)\n        metrics = dict(\n            main_loss=main_loss,\n            thresh_loss=thresh_loss,\n            main_instance_loss=main_instance_loss,\n            thresh_instance_loss=thresh_instance_loss)\n        metrics.update(self.weights)\n        return loss, metrics\n\n\nclass L1DiceLoss(nn.Module):\n    '''\n    L1Loss on thresh, DiceLoss on thresh_binary and binary.\n    '''\n\n    def __init__(self, eps=1e-6, l1_scale=10):\n        super(L1DiceLoss, self).__init__()\n        self.dice_loss = AdaptiveDiceLoss(eps=eps)\n        from .l1_loss import MaskL1Loss\n        self.l1_loss = MaskL1Loss()\n        self.l1_scale = l1_scale\n\n    def forward(self, pred, batch):\n        dice_loss, metrics = self.dice_loss(pred, batch)\n        l1_loss, l1_metric = self.l1_loss(\n            pred['thresh'], batch['thresh_map'], batch['thresh_mask'])\n\n        loss = dice_loss + self.l1_scale * l1_loss\n        metrics.update(**l1_metric)\n        return loss, metrics\n\n\nclass FullL1DiceLoss(L1DiceLoss):\n    '''\n    L1loss on thresh, pixels with topk losses in non-text regions are also counted.\n    DiceLoss on thresh_binary and binary.\n    '''\n\n    def __init__(self, eps=1e-6, l1_scale=10):\n        nn.Module.__init__(self)\n        self.dice_loss = AdaptiveDiceLoss(eps=eps)\n        from .l1_loss import BalanceL1Loss\n        self.l1_loss = BalanceL1Loss()\n        self.l1_scale = l1_scale\n\n\nclass L1BalanceCELoss(nn.Module):\n    '''\n    Balanced CrossEntropy Loss on `binary`,\n    MaskL1Loss on `thresh`,\n    DiceLoss on `thresh_binary`.\n    Note: The meaning of inputs can be figured out in `SegDetectorLossBuilder`.\n    '''\n\n    def __init__(self, eps=1e-6, l1_scale=10, bce_scale=5):\n        super(L1BalanceCELoss, self).__init__()\n        from .dice_loss import DiceLoss\n        from .l1_loss import MaskL1Loss\n        from .balance_cross_entropy_loss import BalanceCrossEntropyLoss\n        self.dice_loss = DiceLoss(eps=eps)\n        self.l1_loss = MaskL1Loss()\n        self.bce_loss = BalanceCrossEntropyLoss()\n\n        self.l1_scale = l1_scale\n        self.bce_scale = bce_scale\n\n    def forward(self, pred, batch):\n        bce_loss = self.bce_loss(pred['binary'], batch['gt'], batch['mask'])\n        metrics = dict(bce_loss=bce_loss)\n        l1_loss, l1_metric = self.l1_loss(pred['thresh'], batch['thresh_map'], batch['thresh_mask'])\n        dice_loss = self.dice_loss(pred['thresh_binary'], batch['gt'], batch['mask'])\n        metrics['thresh_loss'] = dice_loss\n        loss = dice_loss + self.l1_scale * l1_loss + bce_loss * self.bce_scale\n        metrics.update(**l1_metric)\n        return loss, metrics\n\n\nclass L1BCEMiningLoss(nn.Module):\n    '''\n    Basicly the same with L1BalanceCELoss, where the bce loss map is used as\n        attention weigts for DiceLoss\n    '''\n\n    def __init__(self, eps=1e-6, l1_scale=10, bce_scale=5):\n        super(L1BCEMiningLoss, self).__init__()\n        from .dice_loss import DiceLoss\n        from .l1_loss import MaskL1Loss\n        from .balance_cross_entropy_loss import BalanceCrossEntropyLoss\n        self.dice_loss = DiceLoss(eps=eps)\n        self.l1_loss = MaskL1Loss()\n        self.bce_loss = BalanceCrossEntropyLoss()\n\n        self.l1_scale = l1_scale\n        self.bce_scale = bce_scale\n\n    def forward(self, pred, batch):\n        bce_loss, bce_map = self.bce_loss(pred['binary'], batch['gt'], batch['mask'],\n                                          return_origin=True)\n        l1_loss, l1_metric = self.l1_loss(pred['thresh'], batch['thresh_map'], batch['thresh_mask'])\n        bce_map = (bce_map - bce_map.min()) / (bce_map.max() - bce_map.min())\n        dice_loss = self.dice_loss(\n            pred['thresh_binary'], batch['gt'],\n            batch['mask'], weights=bce_map + 1)\n        metrics = dict(bce_loss=bce_loss)\n        metrics['thresh_loss'] = dice_loss\n        loss = dice_loss + self.l1_scale * l1_loss + bce_loss * self.bce_scale\n        metrics.update(**l1_metric)\n        return loss, metrics\n\n\nclass L1LeakyDiceLoss(nn.Module):\n    '''\n    LeakyDiceLoss on binary,\n    MaskL1Loss on thresh,\n    DiceLoss on thresh_binary.\n    '''\n\n    def __init__(self, eps=1e-6, coverage_scale=5, l1_scale=10):\n        super(L1LeakyDiceLoss, self).__init__()\n        from .dice_loss import DiceLoss, LeakyDiceLoss\n        from .l1_loss import MaskL1Loss\n        self.main_loss = LeakyDiceLoss(coverage_scale=coverage_scale)\n        self.l1_loss = MaskL1Loss()\n        self.thresh_loss = DiceLoss(eps=eps)\n\n        self.l1_scale = l1_scale\n\n    def forward(self, pred, batch):\n        main_loss, metrics = self.main_loss(pred['binary'], batch['gt'], batch['mask'])\n        thresh_loss = self.thresh_loss(pred['thresh_binary'], batch['gt'], batch['mask'])\n        l1_loss, l1_metric = self.l1_loss(\n            pred['thresh'], batch['thresh_map'], batch['thresh_mask'])\n        metrics.update(**l1_metric, thresh_loss=thresh_loss)\n        loss = main_loss + thresh_loss + l1_loss * self.l1_scale\n        return loss, metrics\n"""
decoders/seg_recognizer.py,1,"b""import torch\nimport torch.nn as nn\nfrom concern.charsets import DefaultCharset\n\n\nclass SegRecognizer(nn.Module):\n    def __init__(self, in_channels, charset=DefaultCharset(), inner_channels=256, use_resnet=False, bias=False):\n        super(SegRecognizer, self).__init__()\n        self.use_resnet = use_resnet\n        self.mask = nn.Sequential(\n                nn.Conv2d(in_channels, inner_channels, kernel_size=3, padding=1),\n                nn.Conv2d(inner_channels, 1, kernel_size=1, padding=0),\n                nn.Sigmoid())\n        self.classify = nn.Sequential(\n                nn.Conv2d(in_channels, inner_channels, kernel_size=3, padding=1),\n                nn.Conv2d(inner_channels, len(charset), kernel_size=1, padding=0))\n    \n        #for fpn\n        self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0) #to reduce channels\n        #upsample \n        self.up5 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.up4 = nn.Upsample(scale_factor=2, mode='nearset')\n        self.up3 = nn.Upsample(scale_factor=2, mode='nearset')\n\n        self.smooth1 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n        self.smooth2 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n        self.smooth3 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n\n        self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n        self.latlayer2 = nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)\n        self.latlayer3 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n    \n    def _upsample_add(self, x, y):\n        '''Upsample and add two feature maps.\n\n        Args:\n          x: (Variable) top feature map to be upsampled.\n          y: (Variable) lateral feature map.\n\n        Returns:\n          (Variable) added feature map.\n\n        Note in PyTorch, when input size is odd, the upsampled feature map\n        with `F.upsample(..., scale_factor=2, mode='nearest')`\n        maybe not equal to the lateral feature map size.\n\n        e.g.\n        original input size: [N,_,15,15] ->\n        conv2d feature map size: [N,_,8,8] ->\n        upsampled feature map size: [N,_,16,16]\n\n        So we choose bilinear upsample which supports arbitrary output sizes.\n        '''\n        _,_,H,W = y.size()\n        return nn.functional.interpolate(x, size=(H,W), mode='bilinear') + y\n\n\n\n    def forward(self, feature, mask=None, classify=None, train=False):\n        '''\n        Args:\n            feature: Features extracted from backbone with shape N, C, H, W.\n            mask: N, H, W. Float tensor indicating the text regions.\n            classify: N, H, W. Integer tensor indicating the text classes.\n        '''\n        cur_feature = feature\n\n        if self.use_resnet:\n            x2, x3, x4, x5 = feature\n            p5 = self.toplayer(x5)\n            p4 = self._upsample_add(p5, self.latlayer1(x4)) #self.up5(p5) + self.latlayer1(x4)\n            p3 = self._upsample_add(p4, self.latlayer2(x3)) #self.up4(p4) + self.latlayer2(x3)\n            p2 = self._upsample_add(p3, self.latlayer3(x2)) #self.up3(p3) + self.latlayer3(x2)\n            \n            #Smooth\n            p4 = self.smooth1(p4)\n            p3 = self.smooth2(p3)\n            p2 = self.smooth3(p2)\n\n            cur_feature = p2\n\n        mask_pred = self.mask(cur_feature)\n        mask_pred = mask_pred.squeeze(1)  # N, H, W\n        classify_pred = self.classify(cur_feature)\n        pred = dict(mask=mask_pred, classify=classify_pred)\n        if self.training:\n            mask_loss = nn.functional.binary_cross_entropy(mask_pred, mask)\n            classify_loss = nn.functional.cross_entropy(classify_pred, classify)\n            loss = mask_loss + classify_loss\n            metrics = dict(mask_loss=mask_loss, classify_loss=classify_loss)\n            return loss, pred, metrics\n        pred['classify'] = nn.functional.softmax(classify_pred, dim=1)\n        return pred\n"""
decoders/simple_detection.py,4,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nfrom backbones.upsample_head import SimpleUpsampleHead\n\n\nclass SimpleDetectionDecoder(nn.Module):\n    def __init__(self, feature_channel=256):\n        nn.Module.__init__(self)\n\n        self.feature_channel = feature_channel\n        self.head_layer = self.create_head_layer()\n\n        self.pred_layers = nn.ModuleDict(self.create_pred_layers())\n\n    def create_head_layer(self):\n        return SimpleUpsampleHead(\n            self.feature_channel,\n            [self.feature_channel, self.feature_channel // 2, self.feature_channel // 4]\n        )\n\n    def create_pred_layer(self, channels):\n        return nn.Sequential(\n            nn.Conv2d(self.feature_channel // 4, channels, kernel_size=1, stride=1, padding=0, bias=False),\n        )\n\n    def create_pred_layers(self):\n        return {}\n\n    def postprocess_pred(self, pred):\n        return pred\n\n    def calculate_losses(self, preds, label):\n        raise NotImplementedError()\n\n    def forward(self, input, label, meta, train):\n        feature = self.head_layer(input)\n\n        pred = {}\n        for name, pred_layer in self.pred_layers.items():\n            pred[name] = pred_layer(feature)\n\n        if train:\n            losses = self.calculate_losses(pred, label)\n            pred = self.postprocess_pred(pred)\n            loss = sum(losses.values())\n            return loss, pred, losses\n        else:\n            pred = self.postprocess_pred(pred)\n            return pred\n\n\nclass SimpleSegDecoder(SimpleDetectionDecoder):\n    def create_pred_layers(self):\n        return {\n            'heatmap': self.create_pred_layer(1)\n        }\n\n    def postprocess_pred(self, pred):\n        pred['heatmap'] = F.sigmoid(pred['heatmap'])\n        return pred\n\n    def calculate_losses(self, pred, label):\n        heatmap = label['heatmap']\n        heatmap_weight = label['heatmap_weight']\n\n        heatmap_pred = pred['heatmap']\n\n        heatmap_loss = F.binary_cross_entropy_with_logits(heatmap_pred, heatmap, reduction='none')\n        heatmap_loss = (heatmap_loss * heatmap_weight).mean(dim=(1, 2, 3))\n\n        return {\n            'heatmap_loss': heatmap_loss,\n        }\n\n\nclass SimpleEASTDecoder(SimpleDetectionDecoder):\n    def __init__(self, feature_channels=256, densebox_ratio=1000.0, densebox_rescale_factor=512):\n        SimpleDetectionDecoder.__init__(self, feature_channels)\n\n        self.densebox_ratio = densebox_ratio\n        self.densebox_rescale_factor = densebox_rescale_factor\n\n    def create_pred_layers(self):\n        return {\n            'heatmap': self.create_pred_layer(1),\n            'densebox': self.create_pred_layer(8),\n        }\n\n    def postprocess_pred(self, pred):\n        pred['heatmap'] = F.sigmoid(pred['heatmap'])\n        pred['densebox'] = pred['densebox'] * self.densebox_rescale_factor\n        return pred\n\n    def calculate_losses(self, pred, label):\n        heatmap = label['heatmap']\n        heatmap_weight = label['heatmap_weight']\n        densebox = label['densebox'] / self.densebox_rescale_factor\n        densebox_weight = label['densebox_weight']\n\n        heatmap_pred = pred['heatmap']\n        densebox_pred = pred['densebox']\n\n        heatmap_loss = F.binary_cross_entropy_with_logits(heatmap_pred, heatmap, reduction='none')\n        heatmap_loss = (heatmap_loss * heatmap_weight).mean(dim=(1, 2, 3))\n\n        densebox_loss = F.mse_loss(densebox_pred, densebox, reduction='none')\n        densebox_loss = (densebox_loss * densebox_weight).mean(dim=(1, 2, 3)) * self.densebox_ratio\n\n        return {\n            'heatmap_loss': heatmap_loss,\n            'densebox_loss': densebox_loss,\n        }\n\n\nclass SimpleTextsnakeDecoder(SimpleDetectionDecoder):\n    def __init__(self, feature_channels=256, radius_ratio=10.0):\n        SimpleDetectionDecoder.__init__(self, feature_channels)\n\n        self.radius_ratio = radius_ratio\n\n    def create_pred_layers(self):\n        return {\n            'heatmap': self.create_pred_layer(1),\n            'radius': self.create_pred_layer(1),\n        }\n\n    def postprocess_pred(self, pred):\n        pred['heatmap'] = F.sigmoid(pred['heatmap'])\n        pred['radius'] = torch.exp(pred['radius'])\n        return pred\n\n    def calculate_losses(self, pred, label):\n        heatmap = label['heatmap']\n        heatmap_weight = label['heatmap_weight']\n        radius = torch.log(label['radius'] + 1)\n        radius_weight = label['radius_weight']\n\n        heatmap_pred = pred['heatmap']\n        radius_pred = pred['radius']\n\n        heatmap_loss = F.binary_cross_entropy_with_logits(heatmap_pred, heatmap, reduction='none')\n        heatmap_loss = (heatmap_loss * heatmap_weight).mean(dim=(1, 2, 3))\n\n        radius_loss = F.smooth_l1_loss(radius_pred, radius, reduction='none')\n        radius_loss = (radius_loss * radius_weight).mean(dim=(1, 2, 3)) * self.radius_ratio\n\n        return {\n            'heatmap_loss': heatmap_loss,\n            'radius_loss': radius_loss,\n        }\n\n\nclass SimpleMSRDecoder(SimpleDetectionDecoder):\n    def __init__(self, feature_channels=256, offset_ratio=1000.0, offset_rescale_factor=512):\n        SimpleDetectionDecoder.__init__(self, feature_channels)\n\n        self.offset_ratio = offset_ratio\n        self.offset_rescale_factor = offset_rescale_factor\n\n    def create_pred_layers(self):\n        return {\n            'heatmap': self.create_pred_layer(1),\n            'offset': self.create_pred_layer(2),\n        }\n\n    def postprocess_pred(self, pred):\n        pred['heatmap'] = F.sigmoid(pred['heatmap'])\n        pred['offset'] = pred['offset'] * self.offset_rescale_factor\n        return pred\n\n    def calculate_losses(self, pred, label):\n        heatmap = label['heatmap']\n        heatmap_weight = label['heatmap_weight']\n        offset = label['offset'] / self.offset_rescale_factor\n        offset_weight = label['offset_weight']\n\n        heatmap_pred = pred['heatmap']\n        offset_pred = pred['offset']\n\n        heatmap_loss = F.binary_cross_entropy_with_logits(heatmap_pred, heatmap, reduction='none')\n        heatmap_loss = (heatmap_loss * heatmap_weight).mean(dim=(1, 2, 3))\n        offset_loss = F.mse_loss(offset_pred, offset, reduction='none')\n        offset_loss = (offset_loss * offset_weight).mean(dim=(1, 2, 3)) * self.offset_ratio\n\n        return {\n            'heatmap_loss': heatmap_loss,\n            'offset_loss': offset_loss,\n        }\n"""
decoders/textsnake.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass TextsnakeDecoder(nn.Module):\n\n    def __init__(self, channels=256):\n        nn.Module.__init__(self)\n\n        self.head_layer = nn.Sequential(\n            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(channels),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(channels, channels // 2, kernel_size=2, stride=2, padding=0),\n            nn.BatchNorm2d(channels // 2),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(channels // 2, channels // 4, kernel_size=2, stride=2, padding=0),\n        )\n\n        self.pred_layer = nn.Sequential(\n            nn.Conv2d(channels // 4, 7, kernel_size=1, stride=1, padding=0),\n        )\n\n    @staticmethod\n    def ohem(predict, target, train_mask, negative_ratio=3.):\n        pos = (target * train_mask).byte()\n        neg = ((1 - target) * train_mask).byte()\n\n        n_pos = pos.float().sum()\n        n_neg = min(int(neg.float().sum().item()), int(negative_ratio * n_pos.float()))\n\n        loss_pos = F.cross_entropy(predict, target, reduction=\'none\')[pos].sum()\n\n        loss_neg = F.cross_entropy(predict, target, reduction=\'none\')[neg]\n        loss_neg, _ = torch.topk(loss_neg, n_neg)\n\n        return (loss_pos + loss_neg.sum()) / (n_pos + n_neg).float()\n\n    def forward(self, input, label, meta, train):\n        """"""\n        calculate textsnake loss\n        :param input: (Variable), network predict, (BS, 7, H, W)\n        :param label: (dict)\n            :param tr_mask: (Variable), TR target, (BS, H, W)\n            :param tcl_mask: (Variable), TCL target, (BS, H, W)\n            :param sin_map: (Variable), sin target, (BS, H, W)\n            :param cos_map: (Variable), cos target, (BS, H, W)\n            :param radius_map: (Variable), radius target, (BS, H, W)\n            :param train_mask: (Variable), training mask, (BS, H, W)\n            :return: loss_tr, loss_tcl, loss_radius, loss_sin, loss_cos\n        """"""\n        tr_mask = label[\'tr_mask\']\n        tcl_mask = label[\'tcl_mask\']\n        sin_map = label[\'sin_map\']\n        cos_map = label[\'cos_map\']\n        radius_map = label[\'radius_map\']\n        train_mask = label[\'train_mask\']\n\n        feature = self.head_layer(input)\n        pred = self.pred_layer(feature)\n\n        tr_out = pred[:, :2]\n        tcl_out = pred[:, 2:4]\n        sin_out = pred[:, 4]\n        cos_out = pred[:, 5]\n        radius_out = pred[:, 6]\n\n        tr_pred = tr_out.permute(0, 2, 3, 1).reshape(-1, 2)  # (BSxHxW, 2)\n        tcl_pred = tcl_out.permute(0, 2, 3, 1).reshape(-1, 2)  # (BSxHxW, 2)\n        sin_pred = sin_out.reshape(-1)  # (BSxHxW,)\n        cos_pred = cos_out.reshape(-1)  # (BSxHxW,)\n        radius_pred = radius_out.reshape(-1)  # (BSxHxW,)\n\n        # regularize sin and cos: sum to 1\n        scale = torch.sqrt(1.0 / (sin_pred ** 2 + cos_pred ** 2))\n        sin_pred = sin_pred * scale\n        cos_pred = cos_pred * scale\n\n        train_mask = train_mask.view(-1)  # (BSxHxW,)\n\n        tr_mask = tr_mask.reshape(-1)\n        tcl_mask = tcl_mask.reshape(-1)\n        radius_map = radius_map.reshape(-1)\n        sin_map = sin_map.reshape(-1)\n        cos_map = cos_map.reshape(-1)\n\n        loss_tr = self.ohem(tr_pred, tr_mask.long(), train_mask.long())\n        loss_tcl = F.cross_entropy(tcl_pred, tcl_mask.long(), reduction=\'none\')[train_mask * tr_mask].mean()\n\n        # geometry losses\n        ones = radius_map.new(radius_pred[tcl_mask].size()).fill_(1.).float()\n        loss_radius = F.smooth_l1_loss(radius_pred[tcl_mask] / radius_map[tcl_mask], ones)\n        loss_sin = F.smooth_l1_loss(sin_pred[tcl_mask], sin_map[tcl_mask])\n        loss_cos = F.smooth_l1_loss(cos_pred[tcl_mask], cos_map[tcl_mask])\n\n        loss = loss_tr + loss_tcl + loss_radius + loss_sin + loss_cos\n        pred = {\n            \'tr_pred\': F.softmax(tr_out, dim=1)[:, 1],\n            \'tcl_pred\': F.softmax(tcl_out, dim=1)[:, 1],\n            \'sin_pred\': sin_out,\n            \'cos_pred\': cos_out,\n            \'radius_pred\': radius_out,\n        }\n        metrics = {\n            \'loss_tr\': loss_tr,\n            \'loss_tcl\': loss_tcl,\n            \'loss_radius\': loss_radius,\n            \'loss_sin\': loss_sin,\n            \'loss_cos\': loss_cos,\n        }\n        if train:\n            return loss, pred, metrics\n        else:\n            return pred\n'"
ops/__init__.py,0,"b'from .ctc_2d.ctc_loss_2d import CTCLoss2DFunction, ctc_loss_2d\n'"
scripts/json_to_lmdb.py,0,"b'import lmdb\nimport json\nfrom fire import Fire\nfrom collections import defaultdict\nimport os\nimport pickle\nfrom tqdm import tqdm\n\n\ndef main(json_path=None, lmdb_path=None):\n    assert json_path is not None, \'json_path is needed\'\n    if lmdb_path is None:\n        lmdb_path = json_path\n\n    meta = os.path.join(json_path, \'meta.json\')\n    data_ids = []\n    value = {}\n    env = lmdb.Environment(lmdb_path, subdir=True,\n                           map_size=int(1e9), max_dbs=2, lock=False)\n    db_extra = env.open_db(\'extra\'.encode(), create=True)\n    db_image = env.open_db(\'image\'.encode(), create=True)\n    with open(meta, \'r\') as meta_reader:\n        for line in tqdm(meta_reader):\n            single_meta = json.loads(line)\n            data_id = os.path.join(json_path, single_meta[\'filename\'])\n            data_id = str(data_id.encode(\'utf-8\').decode(\'utf-8\'))\n            with open(data_id.encode(), \'rb\') as file_reader:\n                image = file_reader.read()\n            value[\'extra\'] = {}\n            for key in single_meta[\'extra\']:\n                value[\'extra\'][key] = single_meta[\'extra\'][key]\n            with env.begin(write=True) as lmdb_writer:\n                lmdb_writer.put(data_id.encode(),\n                                pickle.dumps(value), db=db_extra)\n            with env.begin(write=True) as image_writer:\n                image_writer.put(data_id.encode(), image, db=db_image)\n    env.close()\n\n\nif __name__ == ""__main__"":\n    Fire(main)\n'"
scripts/nori_to_lmdb.py,0,"b""import pickle\nimport lmdb\nimport nori2 as nori\nfrom tqdm import tqdm\nfrom fire import Fire\n\n\ndef main(nori_path, lmdb_path=None):\n    if lmdb_path is None:\n        lmdb_path = nori_path\n    env = lmdb.Environment(lmdb_path, map_size=int(\n        5e10), writemap=True, max_dbs=2, lock=False)\n    fetcher = nori.Fetcher(nori_path)\n    db_extra = env.open_db('extra'.encode(), create=True)\n    db_image = env.open_db('image'.encode(), create=True)\n    with nori.open(nori_path, 'r') as nr:\n        with env.begin(write=True) as writer:\n            for data_id, data, meta in tqdm(nr.scan()):\n                value = {}\n                image = fetcher.get(data_id)\n                value['extra'] = {}\n                for key in meta['extra']:\n                    value['extra'][key] = meta['extra'][key]\n                writer.put(data_id.encode(), pickle.dumps(value), db=db_extra)\n                writer.put(data_id.encode(), image, db=db_image)\n    env.close()\n    print('Finished')\n\n\nif __name__ == '__main__':\n    Fire(main)\n"""
structure/__init__.py,0,b''
structure/builder.py,1,"b""from collections import OrderedDict\n\nimport torch\n\nimport structure.model\nfrom structure.ensemble_model import EnsembleModel\nfrom concern.config import Configurable, State\n\n\nclass Builder(Configurable):\n    model = State()\n    model_args = State()\n\n    def __init__(self, cmd={}, **kwargs):\n        self.load_all(**kwargs)\n        if 'backbone' in cmd:\n            self.model_args['backbone'] = cmd['backbone']\n\n    @property\n    def model_name(self):\n        return self.model + '-' + getattr(structure.model, self.model).model_name(self.model_args)\n\n    def build(self, device, distributed=False, local_rank: int = 0):\n        Model = getattr(structure.model,self.model)\n        model = Model(self.model_args, device,\n                      distributed=distributed, local_rank=local_rank)\n        return model\n\n\nclass EnsembleBuilder(Configurable):\n    '''Ensemble multiple models into one model\n    Input:\n        builders: A dict which consists of several builders.\n    Example:\n        >>> builder:\n                class: EnsembleBuilder\n                builders:\n                    ctc:\n                        model: CTCModel\n                    atten:\n                        model: AttentionDecoderModel\n    '''\n    builders = State(default={})\n\n    def __init__(self, cmd={}, **kwargs):\n        resume_paths = dict()\n        for key, value_dict in kwargs['builders'].items():\n            resume_paths[key] = value_dict.pop('resume')\n        self.resume_paths = resume_paths\n        self.load_all(**kwargs)\n\n    @property\n    def model_name(self):\n        return 'ensembled-model'\n\n    def build(self, device, *args, **kwargs):\n        models = OrderedDict()\n        for key, builder in self.builders.items():\n            models[key] = builder.build(device=device, *args, **kwargs)\n            models[key].load_state_dict(torch.load(\n                self.resume_paths[key], map_location=device),\n                strict=True)\n        return EnsembleModel(models)\n"""
structure/ensemble_model.py,1,"b'import glob\nimport os\nimport pickle\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\n\nfrom backbones import *\nfrom decoders import *\n\n\nclass EnsembleModel(nn.Module):\n    def __init__(self, models, *args, **kwargs):\n        super(EnsembleModel, self).__init__()\n        self.models = nn.ModuleDict(models)\n\n    def forward(self, batch, select_key=None, training=False):\n        pred = dict()\n\n        for key, module in self.models.items():\n            if select_key is not None and key != select_key:\n                continue\n            pred[key] = module(batch, training)\n        return pred\n'"
structure/model.py,4,"b""import glob\nimport os\nimport pickle\n\nimport numpy as np\nimport apex\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport backbones\nimport decoders\n\n\nclass BasicModel(nn.Module):\n    def __init__(self, args):\n        nn.Module.__init__(self)\n\n        self.backbone = getattr(backbones, args['backbone'])(**args.get('backbone_args', {}))\n        self.decoder = getattr(decoders, args['decoder'])(**args.get('decoder_args', {}))\n\n    def forward(self, data, *args, **kwargs):\n        return self.decoder(self.backbone(data), *args, **kwargs)\n\n\ndef parallelize(model, distributed, local_rank):\n    if distributed:\n        # return nn.parallel.DistributedDataParallel(\n        #     model.cuda(),\n        #     device_ids=[local_rank],\n        #     output_device=[local_rank],\n        #     find_unused_parameters=True)\n        return apex.parallel.DistributedDataParallel(model.cuda())\n    else:\n        return nn.DataParallel(model)\n\n\nclass ClassificationModel(nn.Module):\n    def __init__(self, args, device, distributed: bool = False, local_rank: int = 0):\n        nn.Module.__init__(self)\n\n        self.model = parallelize(BasicModel(args), distributed, local_rank)\n        self.device = device\n        self.to(self.device)\n\n    @staticmethod\n    def model_name(args):\n        return args['backbone'] + '-' + args['decoder']\n\n    def forward(self, batch, training=True):\n        data, label = batch\n        data = data.to(self.device)\n        label = label.to(self.device)\n\n        if training:\n            loss, pred = self.model(data, targets=label, train=True)\n            return loss, pred\n        else:\n            return self.model(data, train=False)\n\n\nclass DetectionModel(nn.Module):\n    def __init__(self, args, device, distributed: bool = False, local_rank: int = 0):\n        nn.Module.__init__(self)\n\n        self.model = parallelize(BasicModel(args), distributed, local_rank)\n        self.device = device\n        self.to(self.device)\n\n    @staticmethod\n    def model_name(args):\n        return args['backbone'] + '-' + args['decoder']\n\n    def forward(self, batch, training=True):\n        data, label, meta = batch\n        data = data.to(self.device)\n        data = data.float() / 255.0\n        for key, value in label.items():\n            label[key] = value.to(self.device)\n\n        if training:\n            loss, pred, metrics = self.model(data, label, meta, train=True)\n            loss = loss.mean()\n            return loss, pred, metrics\n        else:\n            return self.model(data, label, meta, train=False)\n\n\nclass DetectionEnsembleModel(nn.Module):\n    def __init__(self, args, device, distributed: bool = False, local_rank: int = 0):\n        nn.Module.__init__(self)\n\n        self.sizes = args['sizes']\n        self.model = parallelize(BasicModel(args), distributed, local_rank)\n        self.device = device\n        self.to(self.device)\n\n    @staticmethod\n    def model_name(args):\n        return args['backbone'] + '-' + args['decoder']\n\n    def forward(self, batch, training=True):\n        assert (not training)\n        data, label, meta = batch\n        data = data.to(self.device)\n        data = data.float() / 255.0\n        for key, value in label.items():\n            label[key] = value.to(self.device)\n\n        size = (data.shape[2], data.shape[3])\n\n        heatmaps = []\n        for size0 in self.sizes:\n            data0 = F.interpolate(data, size0, mode='bilinear')\n            pred0 = self.model(data0, label, meta, train=False)\n            heatmap0 = F.interpolate(pred0['heatmap'], size, mode='bilinear')\n            heatmaps.append(heatmap0)\n        heatmap = sum(heatmaps) / len(heatmaps)\n\n        return {\n            'heatmap': heatmap,\n        }\n\n\nclass SegDetectorModel(nn.Module):\n    def __init__(self, args, device, distributed: bool = False, local_rank: int = 0):\n        super(SegDetectorModel, self).__init__()\n        from decoders.seg_detector_loss import SegDetectorLossBuilder\n\n        self.model = BasicModel(args)\n        # for loading models\n        self.model = parallelize(self.model, distributed, local_rank)\n        self.criterion = SegDetectorLossBuilder(\n            args['loss_class'], *args.get('loss_args', []), **args.get('loss_kwargs', {})).build()\n        self.criterion = parallelize(self.criterion, distributed, local_rank)\n        self.device = device\n        self.to(self.device)\n\n    @staticmethod\n    def model_name(args):\n        return os.path.join('seg_detector', args['backbone'], args['loss_class'])\n\n    def forward(self, batch, training=True):\n        data = batch['image'].to(self.device)\n        for key, value in batch.items():\n            if value is not None:\n                if hasattr(value, 'to'):\n                    batch[key] = value.to(self.device)\n        data = data.float()\n        pred = self.model(data, training=training)\n\n        if self.training:\n            loss_with_metrics = self.criterion(pred, batch)\n            loss, metrics = loss_with_metrics\n            return loss, pred, metrics\n        return pred\n\n\nclass SequenceRecognitionModel(nn.Module):\n    def __init__(self, args, device, distributed: bool = False, local_rank: int = 0):\n        nn.Module.__init__(self)\n\n        self.model = parallelize(BasicModel(args), distributed, local_rank)\n        self.device = device\n        self.to(self.device)\n\n    @staticmethod\n    def model_name(args):\n        return '/' + args['backbone'] + '/' + args['decoder']\n\n    def forward(self, batch, training=True):\n        images = batch['image'].to(self.device)\n        if self.training:\n            labels = batch['label'].to(self.device)\n            lengths = batch['length'].to(self.device).type(torch.long)\n            loss, pred = self.model(\n                images, targets=labels, lengths=lengths, train=True)\n            return loss, pred\n        else:\n            return self.model(images, train=False)\n\n\nclass SegRecognitionModel(nn.Module):\n    def __init__(self, args, device, distributed: bool = False, local_rank: int = 0):\n        nn.Module.__init__(self)\n\n        self.model = parallelize(BasicModel(args), distributed, local_rank)\n        self.device = device\n        self.to(self.device)\n\n    @staticmethod\n    def model_name(args):\n        return '/' + args['backbone'] + '/' + args['decoder']\n\n    def forward(self, batch, training=True):\n        images = batch['image'].to(self.device)\n        if self.training:\n            mask = batch['mask'].to(self.device)\n            classify = batch['classify'].to(self.device).type(torch.long)\n            return self.model(\n                images, mask=mask, classify=classify, train=True)\n        else:\n            return self.model(images, train=False)\n\n\nclass IntegralRegressionRecognitionModel(nn.Module):\n    def __init__(self, args, device, distributed: bool = False, local_rank: int = 0):\n        nn.Module.__init__(self)\n\n        self.model = parallelize(BasicModel(args), distributed, local_rank)\n        self.device = device\n        self.to(self.device)\n\n    @staticmethod\n    def model_name(args):\n        return '/' + args['backbone'] + '/' + args['decoder']\n\n    def forward(self, batch, training=True):\n        args = dict()\n        for key in batch.keys():\n            args[key] = batch[key].to(self.device)\n        images = args.pop('image')\n        return self.model(images, **args)\n\n\nclass GridSamplingModel(nn.Module):\n    def __init__(self, args, device, distributed: bool = False, local_rank: int = 0):\n        nn.Module.__init__(self)\n\n        self.model = parallelize(BasicModel(args), distributed, local_rank)\n        self.device = device\n        self.to(self.device)\n\n    @staticmethod\n    def model_name(args):\n        return '/' + args['backbone'] + '/' + args['decoder']\n\n    def forward(self, batch, training=True):\n        args = dict()\n        for key in batch.keys():\n            args[key] = batch[key].to(self.device)\n        images = args.pop('image')\n        return self.model(images, **args)\n\n\nclass MaskRCNNTestModel(object):\n    def __init__(self, args, device, distributed: bool = False, local_rank: int = 0):\n        nn.Module.__init__(self)\n\n        res = {}\n        for respath in glob.glob(args['output-dir'] + '/res_*.txt'):\n            name = os.path.basename(respath)[4:-4]\n            polys = []\n            with open(respath) as f:\n                for line in f:\n                    poly = np.array(list(map(int, line.strip().split(',')))).reshape(4, 2).tolist()\n                    polys.append(poly)\n            res[name] = polys\n        self.res = res\n\n    def eval(self):\n        pass\n\n    def forward(self, batch, training=False):\n        assert (not training)\n\n        image, label, meta = batch\n\n        meta = [pickle.loads(value) for value in meta]\n\n        results = []\n        for image_meta in meta:\n            name = image_meta['filename'].split('.')[0]\n            pred_polys = self.res[name]\n            results.append(pred_polys)\n\n        return results\n"""
structure/visualizer.py,0,"b'import torch\nimport cv2\nimport numpy as np\n\nfrom concern.config import Configurable, State\n\n\nclass TrivalVisualizer(Configurable):\n    def __init__(self, **kwargs):\n        pass\n\n    def visualize(self, batch, output):\n        return {}\n'"
training/checkpoint.py,1,"b'from concern.config import Configurable, State\nimport os\nimport torch\n\n\nclass Checkpoint(Configurable):\n    start_epoch = State(default=0)\n    start_iter = State(default=0)\n    resume = State()\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n        cmd = kwargs[\'cmd\']\n        if \'start_epoch\' in cmd:\n            self.start_epoch = cmd[\'start_epoch\']\n        if \'start_iter\' in cmd:\n            self.start_iter = cmd[\'start_iter\']\n        if \'resume\' in cmd:\n            self.resume = cmd[\'resume\']\n\n    def restore_model(self, model, device, logger):\n        if self.resume is None:\n            return\n\n        if not os.path.exists(self.resume):\n            self.logger.warning(""Checkpoint not found: "" +\n                                self.resume)\n            return\n\n        logger.info(""Resuming from "" + self.resume)\n        state_dict = torch.load(self.resume, map_location=device)\n        model.load_state_dict(state_dict, strict=False)\n        logger.info(""Resumed from "" + self.resume)\n\n    def restore_counter(self):\n        return self.start_epoch, self.start_iter\n'"
training/learning_rate.py,1,"b""from bisect import bisect_right\nimport numpy as np\nimport torch.optim.lr_scheduler as lr_scheduler\n\nfrom concern.config import Configurable, State\nfrom concern.signal_monitor import SignalMonitor\n\n\nclass ConstantLearningRate(Configurable):\n    lr = State(default=0.0001)\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n    def get_learning_rate(self, epoch, step):\n        return self.lr\n\n\nclass FileMonitorLearningRate(Configurable):\n    file_path = State()\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n        self.monitor = SignalMonitor(self.file_path)\n\n    def get_learning_rate(self, epoch, step):\n        signal = self.monitor.get_signal()\n        if signal is not None:\n            return float(signal)\n        return None\n\n\nclass PriorityLearningRate(Configurable):\n    learning_rates = State()\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n    def get_learning_rate(self, epoch, step):\n        for learning_rate in self.learning_rates:\n            lr = learning_rate.get_learning_rate(epoch, step)\n            if lr is not None:\n                return lr\n        return None\n\n\nclass MultiStepLR(Configurable):\n    lr = State()\n    milestones = State(default=[])  # milestones must be sorted\n    gamma = State(default=0.1)\n\n    def __init__(self, cmd={}, **kwargs):\n        self.load_all(**kwargs)\n        self.lr = cmd.get('lr', self.lr)\n\n    def get_learning_rate(self, epoch, step):\n        return self.lr * self.gamma ** bisect_right(self.milestones, epoch)\n\n\nclass WarmupLR(Configurable):\n    steps = State(default=4000)\n    warmup_lr = State(default=1e-5)\n    origin_lr = State()\n\n    def __init__(self, cmd={}, **kwargs):\n        self.load_all(**kwargs)\n\n    def get_learning_rate(self, epoch, step):\n        if epoch == 0 and step < self.steps:\n            return self.warmup_lr\n        return self.origin_lr.get_learning_rate(epoch, step)\n\n\nclass PiecewiseConstantLearningRate(Configurable):\n    boundaries = State(default=[10000, 20000])\n    values = State(default=[0.001, 0.0001, 0.00001])\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n    def get_learning_rate(self, epoch, step):\n        for boundary, value in zip(self.boundaries, self.values[:-1]):\n            if step < boundary:\n                return value\n        return self.values[-1]\n\n\nclass DecayLearningRate(Configurable):\n    lr = State(default=0.007)\n    epochs = State(default=1200)\n    factor = State(default=0.9)\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n    def get_learning_rate(self, epoch, step=None):\n        rate = np.power(1.0 - epoch / float(self.epochs + 1), self.factor)\n        return rate * self.lr\n\n\nclass BuitlinLearningRate(Configurable):\n    lr = State(default=0.001)\n    klass = State(default='StepLR')\n    args = State(default=[])\n    kwargs = State(default={})\n\n    def __init__(self, cmd={}, **kwargs):\n        self.load_all(**kwargs)\n        self.lr = cmd.get('lr', None) or self.lr\n        self.scheduler = None\n\n    def prepare(self, optimizer):\n        self.scheduler = getattr(lr_scheduler, self.klass)(\n            optimizer, *self.args, **self.kwargs)\n\n    def get_learning_rate(self, epoch, step=None):\n        if self.scheduler is None:\n            raise 'learning rate not ready(prepared with optimizer) '\n        self.scheduler.last_epoch = epoch\n        # return value of gt_lr is a list,\n        # where each element is the corresponding learning rate for a\n        # paramater group.\n        return self.scheduler.get_lr()[0]\n"""
training/model_saver.py,2,"b""import os\n\nimport torch\nimport torch.distributed as dist\n\nfrom concern.config import Configurable, State\nfrom concern.signal_monitor import SignalMonitor\n\n\nclass ModelSaver(Configurable):\n    dir_path = State()\n    save_interval = State(default=1000)\n    signal_path = State()\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n        # BUG: signal path should not be global\n        self.monitor = SignalMonitor(self.signal_path)\n\n    def maybe_save_model(self, model, epoch, step, logger):\n        if step % self.save_interval == 0 or self.monitor.get_signal() is not None:\n            self.save_model(model, epoch, step)\n            logger.report_time('Saving ')\n            logger.iter(step)\n\n    def save_model(self, model, epoch=None, step=None):\n        if isinstance(model, dict):\n            for name, net in model.items():\n                checkpoint_name = self.make_checkpoint_name(name, epoch, step)\n                self.save_checkpoint(net, checkpoint_name)\n        else:\n            checkpoint_name = self.make_checkpoint_name('model', epoch, step)\n            self.save_checkpoint(model, checkpoint_name)\n\n    def save_checkpoint(self, net, name):\n        if dist.is_available() and dist.is_initialized() and not dist.get_rank() == 0:\n            return\n        os.makedirs(self.dir_path, exist_ok=True)\n        torch.save(net.state_dict(), os.path.join(self.dir_path, name))\n\n    def make_checkpoint_name(self, name, epoch=None, step=None):\n        if epoch is None or step is None:\n            c_name = name + '_latest'\n        else:\n            c_name = '{}_epoch_{}_minibatch_{}'.format(name, epoch, step)\n        return c_name\n"""
training/optimizer_scheduler.py,1,"b""import torch\n\nfrom concern.config import Configurable, State\n\n\nclass OptimizerScheduler(Configurable):\n    optimizer = State()\n    optimizer_args = State(default={})\n    learning_rate = State(autoload=False)\n\n    def __init__(self, cmd={}, **kwargs):\n        self.load_all(**kwargs)\n        self.load('learning_rate', cmd=cmd, **kwargs)\n        if 'lr' in cmd:\n            self.optimizer_args['lr'] = cmd['lr']\n\n    def create_optimizer(self, parameters):\n        optimizer = getattr(torch.optim, self.optimizer)(\n                parameters, **self.optimizer_args)\n        if hasattr(self.learning_rate, 'prepare'):\n            self.learning_rate.prepare(optimizer)\n        return optimizer\n"""
assets/ic15_eval/rrc_evaluation_funcs.py,0,"b'#!/usr/bin/env python2\n#encoding: UTF-8\nimport json\nimport sys;sys.path.append(\'./\')\nimport zipfile\nimport re\nimport sys\nimport os\nimport codecs\nimport importlib\nfrom StringIO import StringIO\n\ndef print_help():\n    sys.stdout.write(\'Usage: python %s.py -g=<gtFile> -s=<submFile> [-o=<outputFolder> -p=<jsonParams>]\' %sys.argv[0])\n    sys.exit(2)\n    \n\ndef load_zip_file_keys(file,fileNameRegExp=\'\'):\n    """"""\n    Returns an array with the entries of the ZIP file that match with the regular expression.\n    The key\'s are the names or the file or the capturing group definied in the fileNameRegExp\n    """"""\n    try:\n        archive=zipfile.ZipFile(file, mode=\'r\', allowZip64=True)\n    except :\n        raise Exception(\'Error loading the ZIP archive.\')\n\n    pairs = []\n    \n    for name in archive.namelist():\n        addFile = True\n        keyName = name\n        if fileNameRegExp!="""":\n            m = re.match(fileNameRegExp,name)\n            if m == None:\n                addFile = False\n            else:\n                if len(m.groups())>0:\n                    keyName = m.group(1)\n                    \n        if addFile:\n            pairs.append( keyName )\n                \n    return pairs\n    \n\ndef load_zip_file(file,fileNameRegExp=\'\',allEntries=False):\n    """"""\n    Returns an array with the contents (filtered by fileNameRegExp) of a ZIP file.\n    The key\'s are the names or the file or the capturing group definied in the fileNameRegExp\n    allEntries validates that all entries in the ZIP file pass the fileNameRegExp\n    """"""\n    try:\n        archive=zipfile.ZipFile(file, mode=\'r\', allowZip64=True)\n    except :\n        raise Exception(\'Error loading the ZIP archive\')    \n\n    pairs = []\n    for name in archive.namelist():\n        addFile = True\n        keyName = name\n        if fileNameRegExp!="""":\n            m = re.match(fileNameRegExp,name)\n            if m == None:\n                addFile = False\n            else:\n                if len(m.groups())>0:\n                    keyName = m.group(1)\n        \n        if addFile:\n            pairs.append( [ keyName , archive.read(name)] )\n        else:\n            if allEntries:\n                raise Exception(\'ZIP entry not valid: %s\' %name)             \n\n    return dict(pairs)\n\t\ndef decode_utf8(raw):\n    """"""\n    Returns a Unicode object on success, or None on failure\n    """"""\n    try:\n        raw = codecs.decode(raw,\'utf-8\', \'replace\')\n        #extracts BOM if exists\n        raw = raw.encode(\'utf8\')\n        if raw.startswith(codecs.BOM_UTF8):\n            raw = raw.replace(codecs.BOM_UTF8, \'\', 1)\n        return raw.decode(\'utf-8\')\n    except:\n       return None\n   \ndef validate_lines_in_file(fileName,file_contents,CRLF=True,LTRB=True,withTranscription=False,withConfidence=False,imWidth=0,imHeight=0):\n    """"""\n    This function validates that all lines of the file calling the Line validation function for each line\n    """"""\n    utf8File = decode_utf8(file_contents)\n    if (utf8File is None) :\n        raise Exception(""The file %s is not UTF-8"" %fileName)\n\n    lines = utf8File.split( ""\\r\\n"" if CRLF else ""\\n"" )\n    for line in lines:\n        line = line.replace(""\\r"","""").replace(""\\n"","""")\n        if(line != """"):\n            try:\n                validate_tl_line(line,LTRB,withTranscription,withConfidence,imWidth,imHeight)\n            except Exception as e:\n                raise Exception((""Line in sample not valid. Sample: %s Line: %s Error: %s"" %(fileName,line,str(e))).encode(\'utf-8\', \'replace\'))\n    \n   \n   \ndef validate_tl_line(line,LTRB=True,withTranscription=True,withConfidence=True,imWidth=0,imHeight=0):\n    """"""\n    Validate the format of the line. If the line is not valid an exception will be raised.\n    If maxWidth and maxHeight are specified, all points must be inside the imgage bounds.\n    Posible values are:\n    LTRB=True: xmin,ymin,xmax,ymax[,confidence][,transcription] \n    LTRB=False: x1,y1,x2,y2,x3,y3,x4,y4[,confidence][,transcription] \n    """"""\n    get_tl_line_values(line,LTRB,withTranscription,withConfidence,imWidth,imHeight)\n    \n   \ndef get_tl_line_values(line,LTRB=True,withTranscription=False,withConfidence=False,imWidth=0,imHeight=0):\n    """"""\n    Validate the format of the line. If the line is not valid an exception will be raised.\n    If maxWidth and maxHeight are specified, all points must be inside the imgage bounds.\n    Posible values are:\n    LTRB=True: xmin,ymin,xmax,ymax[,confidence][,transcription] \n    LTRB=False: x1,y1,x2,y2,x3,y3,x4,y4[,confidence][,transcription] \n    Returns values from a textline. Points , [Confidences], [Transcriptions]\n    """"""\n    confidence = 0.0\n    transcription = """";\n    points = []\n    \n    numPoints = 4;\n    \n    if LTRB:\n    \n        numPoints = 4;\n        \n        if withTranscription and withConfidence:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*,(.*)$\',line)\n            if m == None :\n                m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*,(.*)$\',line)\n                raise Exception(""Format incorrect. Should be: xmin,ymin,xmax,ymax,confidence,transcription"")\n        elif withConfidence:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: xmin,ymin,xmax,ymax,confidence"")\n        elif withTranscription:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,(.*)$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: xmin,ymin,xmax,ymax,transcription"")\n        else:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,?\\s*$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: xmin,ymin,xmax,ymax"")\n            \n        xmin = int(m.group(1))\n        ymin = int(m.group(2))\n        xmax = int(m.group(3))\n        ymax = int(m.group(4))\n        if(xmax<xmin):\n                raise Exception(""Xmax value (%s) not valid (Xmax < Xmin)."" %(xmax))\n        if(ymax<ymin):\n                raise Exception(""Ymax value (%s)  not valid (Ymax < Ymin)."" %(ymax))  \n\n        points = [ float(m.group(i)) for i in range(1, (numPoints+1) ) ]\n        \n        if (imWidth>0 and imHeight>0):\n            validate_point_inside_bounds(xmin,ymin,imWidth,imHeight);\n            validate_point_inside_bounds(xmax,ymax,imWidth,imHeight);\n\n    else:\n        \n        numPoints = 8;\n        \n        if withTranscription and withConfidence:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*,(.*)$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4,confidence,transcription"")\n        elif withConfidence:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4,confidence"")\n        elif withTranscription:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,(.*)$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4,transcription"")\n        else:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*$\',line)\n            if m == None :\n                raise Exception(""Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4"")\n            \n        points = [ float(m.group(i)) for i in range(1, (numPoints+1) ) ]\n        \n        validate_clockwise_points(points)\n        \n        if (imWidth>0 and imHeight>0):\n            validate_point_inside_bounds(points[0],points[1],imWidth,imHeight);\n            validate_point_inside_bounds(points[2],points[3],imWidth,imHeight);\n            validate_point_inside_bounds(points[4],points[5],imWidth,imHeight);\n            validate_point_inside_bounds(points[6],points[7],imWidth,imHeight);\n            \n    \n    if withConfidence:\n        try:\n            confidence = float(m.group(numPoints+1))\n        except ValueError:\n            raise Exception(""Confidence value must be a float"")       \n            \n    if withTranscription:\n        posTranscription = numPoints + (2 if withConfidence else 1)\n        transcription = m.group(posTranscription)\n        m2 = re.match(r\'^\\s*\\""(.*)\\""\\s*$\',transcription)\n        if m2 != None : #Transcription with double quotes, we extract the value and replace escaped characters\n            transcription = m2.group(1).replace(""\\\\\\\\"", ""\\\\"").replace(""\\\\\\"""", ""\\"""")\n    \n    return points,confidence,transcription\n    \n            \ndef validate_point_inside_bounds(x,y,imWidth,imHeight):\n    if(x<0 or x>imWidth):\n            raise Exception(""X value (%s) not valid. Image dimensions: (%s,%s)"" %(xmin,imWidth,imHeight))\n    if(y<0 or y>imHeight):\n            raise Exception(""Y value (%s)  not valid. Image dimensions: (%s,%s) Sample: %s Line:%s"" %(ymin,imWidth,imHeight))\n\ndef validate_clockwise_points(points):\n    """"""\n    Validates that the points that the 4 points that dlimite a polygon are in clockwise order.\n    """"""\n    \n    if len(points) != 8:\n        raise Exception(""Points list not valid."" + str(len(points)))\n    \n    point = [\n                [int(points[0]) , int(points[1])],\n                [int(points[2]) , int(points[3])],\n                [int(points[4]) , int(points[5])],\n                [int(points[6]) , int(points[7])]\n            ]\n    edge = [\n                ( point[1][0] - point[0][0])*( point[1][1] + point[0][1]),\n                ( point[2][0] - point[1][0])*( point[2][1] + point[1][1]),\n                ( point[3][0] - point[2][0])*( point[3][1] + point[2][1]),\n                ( point[0][0] - point[3][0])*( point[0][1] + point[3][1])\n    ]\n    \n    summatory = edge[0] + edge[1] + edge[2] + edge[3];\n    if summatory>0:\n        raise Exception(""Points are not clockwise. The coordinates of bounding quadrilaterals have to be given in clockwise order. Regarding the correct interpretation of \'clockwise\' remember that the image coordinate system used is the standard one, with the image origin at the upper left, the X axis extending to the right and Y axis extending downwards."")\n\ndef get_tl_line_values_from_file_contents(content,CRLF=True,LTRB=True,withTranscription=False,withConfidence=False,imWidth=0,imHeight=0,sort_by_confidences=True):\n    """"""\n    Returns all points, confindences and transcriptions of a file in lists. Valid line formats:\n    xmin,ymin,xmax,ymax,[confidence],[transcription]\n    x1,y1,x2,y2,x3,y3,x4,y4,[confidence],[transcription]\n    """"""\n    pointsList = []\n    transcriptionsList = []\n    confidencesList = []\n    \n    lines = content.split( ""\\r\\n"" if CRLF else ""\\n"" )\n    for line in lines:\n        line = line.replace(""\\r"","""").replace(""\\n"","""")\n        if(line != """") :\n            points, confidence, transcription = get_tl_line_values(line,LTRB,withTranscription,withConfidence,imWidth,imHeight);\n            pointsList.append(points)\n            transcriptionsList.append(transcription)\n            confidencesList.append(confidence)\n\n    if withConfidence and len(confidencesList)>0 and sort_by_confidences:\n        import numpy as np\n        sorted_ind = np.argsort(-np.array(confidencesList))\n        confidencesList = [confidencesList[i] for i in sorted_ind]\n        pointsList = [pointsList[i] for i in sorted_ind]\n        transcriptionsList = [transcriptionsList[i] for i in sorted_ind]        \n        \n    return pointsList,confidencesList,transcriptionsList\n\ndef main_evaluation(p,default_evaluation_params_fn,validate_data_fn,evaluate_method_fn,show_result=True,per_sample=True):\n    """"""\n    This process validates a method, evaluates it and if it succed generates a ZIP file with a JSON entry for each sample.\n    Params:\n    p: Dictionary of parmeters with the GT/submission locations. If None is passed, the parameters send by the system are used.\n    default_evaluation_params_fn: points to a function that returns a dictionary with the default parameters used for the evaluation\n    validate_data_fn: points to a method that validates the corrct format of the submission\n    evaluate_method_fn: points to a function that evaluated the submission and return a Dictionary with the results\n    """"""\n    \n    if (p == None):\n        p = dict([s[1:].split(\'=\') for s in sys.argv[2:]])\n        model_name=sys.argv[1].split(\'=\')[-1]\n        if(len(sys.argv)<4):\n            print_help()\n\n    evalParams = default_evaluation_params_fn()\n    if \'p\' in p.keys():\n        evalParams.update( p[\'p\'] if isinstance(p[\'p\'], dict) else json.loads(p[\'p\'][1:-1]) )\n\n    resDict={\'calculated\':True,\'Message\':\'\',\'method\':\'{}\',\'per_sample\':\'{}\'}    \n    try:\n        validate_data_fn(p[\'g\'], p[\'s\'], evalParams)  \n        evalData = evaluate_method_fn(p[\'g\'], p[\'s\'], evalParams)\n        resDict.update(evalData)\n        \n    except Exception, e:\n        resDict[\'Message\']= str(e)\n        resDict[\'calculated\']=False\n\n    if \'o\' in p:\n        if not os.path.exists(p[\'o\']):\n            os.makedirs(p[\'o\'])\n\n        resultsOutputname = p[\'o\'] + \'/results.zip\'\n        outZip = zipfile.ZipFile(resultsOutputname, mode=\'w\', allowZip64=True)\n\n        del resDict[\'per_sample\']\n        if \'output_items\' in resDict.keys():\n            del resDict[\'output_items\']\n\n        outZip.writestr(\'method.json\',json.dumps(resDict))\n        \n    if not resDict[\'calculated\']:\n        if show_result:\n            sys.stderr.write(\'Error!\\n\'+ resDict[\'Message\']+\'\\n\\n\')\n        if \'o\' in p:\n            outZip.close()\n        return resDict\n    \n    if \'o\' in p:\n        if per_sample == True:\n            for k,v in evalData[\'per_sample\'].iteritems():\n                outZip.writestr( k + \'.json\',json.dumps(v)) \n\n            if \'output_items\' in evalData.keys():\n                for k, v in evalData[\'output_items\'].iteritems():\n                    outZip.writestr( k,v) \n\n        outZip.close()\n\n    if show_result:\n        sys.stdout.write(""Calculated!"")\n        sys.stdout.write(json.dumps(resDict[\'method\']))\n    \n    return resDict,model_name\n\n\ndef main_validation(default_evaluation_params_fn,validate_data_fn):\n    """"""\n    This process validates a method\n    Params:\n    default_evaluation_params_fn: points to a function that returns a dictionary with the default parameters used for the evaluation\n    validate_data_fn: points to a method that validates the corrct format of the submission\n    """"""    \n    try:\n        p = dict([s[1:].split(\'=\') for s in sys.argv[1:]])\n        evalParams = default_evaluation_params_fn()\n        if \'p\' in p.keys():\n            evalParams.update( p[\'p\'] if isinstance(p[\'p\'], dict) else json.loads(p[\'p\'][1:-1]) )\n\n        validate_data_fn(p[\'g\'], p[\'s\'], evalParams)              \n        print \'SUCCESS\'\n        sys.exit(0)\n    except Exception as e:\n        print str(e)\n        sys.exit(101)\n'"
assets/ic15_eval/script.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom collections import namedtuple\nimport rrc_evaluation_funcs\nimport importlib\nimport json\nimport ipdb\ndef evaluation_imports():\n    """"""\n    evaluation_imports: Dictionary ( key = module name , value = alias  )  with python modules used in the evaluation. \n    """"""    \n    return {\n            \'Polygon\':\'plg\',\n            \'numpy\':\'np\'\n            }\n\ndef default_evaluation_params():\n    """"""\n    default_evaluation_params: Default parameters to use for the validation and evaluation.\n    """"""\n    return {\n                \'IOU_CONSTRAINT\' :0.5,\n                \'AREA_PRECISION_CONSTRAINT\' :0.5,\n                \'GT_SAMPLE_NAME_2_ID\':\'gt_img_([0-9]+).txt\',\n                \'DET_SAMPLE_NAME_2_ID\':\'res_img_([0-9]+).txt\',\n                \'LTRB\':False, #LTRB:2points(left,top,right,bottom) or 4 points(x1,y1,x2,y2,x3,y3,x4,y4)\n                \'CRLF\':False, # Lines are delimited by Windows CRLF format\n                \'CONFIDENCES\':False, #Detections must include confidence value. AP will be calculated\n                \'PER_SAMPLE_RESULTS\':True #Generate per sample results and produce data for visualization\n            }\n\ndef validate_data(gtFilePath, submFilePath,evaluationParams):\n    """"""\n    Method validate_data: validates that all files in the results folder are correct (have the correct name contents).\n                            Validates also that there are no missing files in the folder.\n                            If some error detected, the method raises the error\n    """"""\n    gt = rrc_evaluation_funcs.load_zip_file(gtFilePath,evaluationParams[\'GT_SAMPLE_NAME_2_ID\'])\n\n    subm = rrc_evaluation_funcs.load_zip_file(submFilePath,evaluationParams[\'DET_SAMPLE_NAME_2_ID\'],True)\n    \n    #Validate format of GroundTruth\n    for k in gt:\n        rrc_evaluation_funcs.validate_lines_in_file(k,gt[k],evaluationParams[\'CRLF\'],evaluationParams[\'LTRB\'],True)\n\n    #Validate format of results\n    for k in subm:\n        if (k in gt) == False :\n            raise Exception(""The sample %s not present in GT"" %k)\n        \n        rrc_evaluation_funcs.validate_lines_in_file(k,subm[k],evaluationParams[\'CRLF\'],evaluationParams[\'LTRB\'],False,evaluationParams[\'CONFIDENCES\'])\n\n    \ndef evaluate_method(gtFilePath, submFilePath, evaluationParams):\n    """"""\n    Method evaluate_method: evaluate method and returns the results\n        Results. Dictionary with the following values:\n        - method (required)  Global method metrics. Ex: { \'Precision\':0.8,\'Recall\':0.9 }\n        - samples (optional) Per sample metrics. Ex: {\'sample1\' : { \'Precision\':0.8,\'Recall\':0.9 } , \'sample2\' : { \'Precision\':0.8,\'Recall\':0.9 }\n    """"""    \n    \n    for module,alias in evaluation_imports().iteritems():\n        globals()[alias] = importlib.import_module(module)    \n    \n    def polygon_from_points(points):\n        """"""\n        Returns a Polygon object to use with the Polygon2 class from a list of 8 points: x1,y1,x2,y2,x3,y3,x4,y4\n        """"""        \n        resBoxes=np.empty([1,8],dtype=\'int32\')\n        resBoxes[0,0]=int(points[0])\n        resBoxes[0,4]=int(points[1])\n        resBoxes[0,1]=int(points[2])\n        resBoxes[0,5]=int(points[3])\n        resBoxes[0,2]=int(points[4])\n        resBoxes[0,6]=int(points[5])\n        resBoxes[0,3]=int(points[6])\n        resBoxes[0,7]=int(points[7])\n        pointMat = resBoxes[0].reshape([2,4]).T\n        return plg.Polygon( pointMat)    \n    \n    def rectangle_to_polygon(rect):\n        resBoxes=np.empty([1,8],dtype=\'int32\')\n        resBoxes[0,0]=int(rect.xmin)\n        resBoxes[0,4]=int(rect.ymax)\n        resBoxes[0,1]=int(rect.xmin)\n        resBoxes[0,5]=int(rect.ymin)\n        resBoxes[0,2]=int(rect.xmax)\n        resBoxes[0,6]=int(rect.ymin)\n        resBoxes[0,3]=int(rect.xmax)\n        resBoxes[0,7]=int(rect.ymax)\n\n        pointMat = resBoxes[0].reshape([2,4]).T\n        \n        return plg.Polygon( pointMat)\n    \n    def rectangle_to_points(rect):\n        points = [int(rect.xmin), int(rect.ymax), int(rect.xmax), int(rect.ymax), int(rect.xmax), int(rect.ymin), int(rect.xmin), int(rect.ymin)]\n        return points\n        \n    def get_union(pD,pG):\n        areaA = pD.area();\n        areaB = pG.area();\n        return areaA + areaB - get_intersection(pD, pG);\n        \n    def get_intersection_over_union(pD,pG):\n        try:\n            return get_intersection(pD, pG) / get_union(pD, pG);\n        except:\n            return 0\n        \n    def get_intersection(pD,pG):\n        pInt = pD & pG\n        if len(pInt) == 0:\n            return 0\n        return pInt.area()\n    \n    def compute_ap(confList, matchList,numGtCare):\n        correct = 0\n        AP = 0\n        if len(confList)>0:\n            confList = np.array(confList)\n            matchList = np.array(matchList)\n            sorted_ind = np.argsort(-confList)\n            confList = confList[sorted_ind]\n            matchList = matchList[sorted_ind]\n            for n in range(len(confList)):\n                match = matchList[n]\n                if match:\n                    correct += 1\n                    AP += float(correct)/(n + 1)\n\n            if numGtCare>0:\n                AP /= numGtCare\n            \n        return AP\n    \n    perSampleMetrics = {}\n    \n    matchedSum = 0\n    \n    Rectangle = namedtuple(\'Rectangle\', \'xmin ymin xmax ymax\')\n    \n    gt = rrc_evaluation_funcs.load_zip_file(gtFilePath,evaluationParams[\'GT_SAMPLE_NAME_2_ID\'])\n    subm = rrc_evaluation_funcs.load_zip_file(submFilePath,evaluationParams[\'DET_SAMPLE_NAME_2_ID\'],True)\n   \n    numGlobalCareGt = 0;\n    numGlobalCareDet = 0;\n    \n    arrGlobalConfidences = [];\n    arrGlobalMatches = [];\n\n    for resFile in gt:\n        \n        gtFile = rrc_evaluation_funcs.decode_utf8(gt[resFile])\n        recall = 0\n        precision = 0\n        hmean = 0    \n        \n        detMatched = 0\n        \n        iouMat = np.empty([1,1])\n        \n        gtPols = []\n        detPols = []\n        \n        gtPolPoints = []\n        detPolPoints = []  \n        \n        #Array of Ground Truth Polygons\' keys marked as don\'t Care\n        gtDontCarePolsNum = []\n        #Array of Detected Polygons\' matched with a don\'t Care GT\n        detDontCarePolsNum = []   \n        \n        pairs = [] \n        detMatchedNums = []\n        \n        arrSampleConfidences = [];\n        arrSampleMatch = [];\n        sampleAP = 0;\n\n        evaluationLog = """"\n        \n        pointsList,_,transcriptionsList = rrc_evaluation_funcs.get_tl_line_values_from_file_contents(gtFile,evaluationParams[\'CRLF\'],evaluationParams[\'LTRB\'],True,False)\n        for n in range(len(pointsList)):\n            points = pointsList[n]\n            transcription = transcriptionsList[n]\n            dontCare = transcription == ""###""\n            if evaluationParams[\'LTRB\']:\n                gtRect = Rectangle(*points)\n                gtPol = rectangle_to_polygon(gtRect)\n            else:\n                gtPol = polygon_from_points(points)\n            gtPols.append(gtPol)\n            gtPolPoints.append(points)\n            if dontCare:\n                gtDontCarePolsNum.append( len(gtPols)-1 )\n                \n        evaluationLog += ""GT polygons: "" + str(len(gtPols)) + ("" ("" + str(len(gtDontCarePolsNum)) + "" don\'t care)\\n"" if len(gtDontCarePolsNum)>0 else ""\\n"")\n        \n        if resFile in subm:\n            \n            detFile = rrc_evaluation_funcs.decode_utf8(subm[resFile]) \n            \n            pointsList,confidencesList,_ = rrc_evaluation_funcs.get_tl_line_values_from_file_contents(detFile,evaluationParams[\'CRLF\'],evaluationParams[\'LTRB\'],False,evaluationParams[\'CONFIDENCES\'])\n            for n in range(len(pointsList)):\n                points = pointsList[n]\n                \n                if evaluationParams[\'LTRB\']:\n                    detRect = Rectangle(*points)\n                    detPol = rectangle_to_polygon(detRect)\n                else:\n                    detPol = polygon_from_points(points)                    \n                detPols.append(detPol)\n                detPolPoints.append(points)\n                if len(gtDontCarePolsNum)>0 :\n                    for dontCarePol in gtDontCarePolsNum:\n                        dontCarePol = gtPols[dontCarePol]\n                        intersected_area = get_intersection(dontCarePol,detPol)\n                        pdDimensions = detPol.area()\n                        precision = 0 if pdDimensions == 0 else intersected_area / pdDimensions\n                        if (precision > evaluationParams[\'AREA_PRECISION_CONSTRAINT\'] ):\n                            detDontCarePolsNum.append( len(detPols)-1 )\n                            break\n                                \n            evaluationLog += ""DET polygons: "" + str(len(detPols)) + ("" ("" + str(len(detDontCarePolsNum)) + "" don\'t care)\\n"" if len(detDontCarePolsNum)>0 else ""\\n"")\n            \n            if len(gtPols)>0 and len(detPols)>0:\n                #Calculate IoU and precision matrixs\n                outputShape=[len(gtPols),len(detPols)]\n                iouMat = np.empty(outputShape)\n                gtRectMat = np.zeros(len(gtPols),np.int8)\n                detRectMat = np.zeros(len(detPols),np.int8)\n                for gtNum in range(len(gtPols)):\n                    for detNum in range(len(detPols)):\n                        pG = gtPols[gtNum]\n                        pD = detPols[detNum]\n                        iouMat[gtNum,detNum] = get_intersection_over_union(pD,pG)\n\n                for gtNum in range(len(gtPols)):\n                    for detNum in range(len(detPols)):\n                        if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and gtNum not in gtDontCarePolsNum and detNum not in detDontCarePolsNum :\n                            if iouMat[gtNum,detNum]>evaluationParams[\'IOU_CONSTRAINT\']:\n                                gtRectMat[gtNum] = 1\n                                detRectMat[detNum] = 1\n                                detMatched += 1\n                                pairs.append({\'gt\':gtNum,\'det\':detNum})\n                                detMatchedNums.append(detNum)\n                                evaluationLog += ""Match GT #"" + str(gtNum) + "" with Det #"" + str(detNum) + ""\\n""\n\n            if evaluationParams[\'CONFIDENCES\']:\n                for detNum in range(len(detPols)):\n                    if detNum not in detDontCarePolsNum :\n                        #we exclude the don\'t care detections\n                        match = detNum in detMatchedNums\n\n                        arrSampleConfidences.append(confidencesList[detNum])\n                        arrSampleMatch.append(match)\n\n                        arrGlobalConfidences.append(confidencesList[detNum]);\n                        arrGlobalMatches.append(match);\n                            \n        numGtCare = (len(gtPols) - len(gtDontCarePolsNum))\n        numDetCare = (len(detPols) - len(detDontCarePolsNum))\n        if numGtCare == 0:\n            recall = float(1)\n            precision = float(0) if numDetCare >0 else float(1)\n            sampleAP = precision\n        else:\n            recall = float(detMatched) / numGtCare\n            precision = 0 if numDetCare==0 else float(detMatched) / numDetCare\n            if evaluationParams[\'CONFIDENCES\'] and evaluationParams[\'PER_SAMPLE_RESULTS\']:\n                sampleAP = compute_ap(arrSampleConfidences, arrSampleMatch, numGtCare )                    \n\n        hmean = 0 if (precision + recall)==0 else 2.0 * precision * recall / (precision + recall)                \n\n        matchedSum += detMatched\n        numGlobalCareGt += numGtCare\n        numGlobalCareDet += numDetCare\n        \n        if evaluationParams[\'PER_SAMPLE_RESULTS\']:\n            perSampleMetrics[resFile] = {\n                                            \'precision\':precision,\n                                            \'recall\':recall,\n                                            \'hmean\':hmean,\n                                            \'pairs\':pairs,\n                                            \'AP\':sampleAP,\n                                            \'iouMat\':[] if len(detPols)>100 else iouMat.tolist(),\n                                            \'gtPolPoints\':gtPolPoints,\n                                            \'detPolPoints\':detPolPoints,\n                                            \'gtDontCare\':gtDontCarePolsNum,\n                                            \'detDontCare\':detDontCarePolsNum,\n                                            \'evaluationParams\': evaluationParams,\n                                            \'evaluationLog\': evaluationLog                                        \n                                        }\n                                    \n    # Compute MAP and MAR\n    AP = 0\n    if evaluationParams[\'CONFIDENCES\']:\n        AP = compute_ap(arrGlobalConfidences, arrGlobalMatches, numGlobalCareGt)\n\n    methodRecall = 0 if numGlobalCareGt == 0 else float(matchedSum)/numGlobalCareGt\n    methodPrecision = 0 if numGlobalCareDet == 0 else float(matchedSum)/numGlobalCareDet\n    methodHmean = 0 if methodRecall + methodPrecision==0 else 2* methodRecall * methodPrecision / (methodRecall + methodPrecision)\n    \n    methodMetrics = {\'precision\':methodPrecision, \'recall\':methodRecall,\'hmean\': methodHmean, \'AP\': AP  }\n\n    resDict = {\'calculated\':True,\'Message\':\'\',\'method\': methodMetrics,\'per_sample\': perSampleMetrics}\n    \n    \n    return resDict;\n\n\n\nif __name__==\'__main__\':\n    res_dict,model_name=rrc_evaluation_funcs.main_evaluation(None,default_evaluation_params,validate_data,evaluate_method)\n    with open(\'./log.txt\',\'a\') as f:\n        f.write(model_name+\':\'+json.dumps(res_dict[\'method\'])+\'\\n\')\n'"
concern/icdar2015_eval/__init__.py,0,b''
concern/webcv2/__init__.py,0,"b'#!/usr/bin/env mdl\nclass WebCV2:\n    def __init__(self):\n        import cv2\n        self._cv2 = cv2\n        from .manager import global_manager as gm\n        self._gm = gm\n\n    def __getattr__(self, name):\n        if hasattr(self._gm, name):\n            return getattr(self._gm, name)\n        elif hasattr(self._cv2, name):\n            return getattr(self._cv2, name)\n        else:\n            raise AttributeError\n\nimport sys\nsys.modules[__name__] = WebCV2()\n\n'"
concern/webcv2/manager.py,0,"b'#!/usr/bin/env mdl\nimport socket\nimport base64\nimport cv2\nimport numpy as np\nfrom collections import OrderedDict\n\nfrom .server import get_server\n\n\ndef jpeg_encode(img):\n    return cv2.imencode(\'.png\', img)[1]\n\n\ndef get_free_port(rng, low=2000, high=10000):\n    in_use = True\n    while in_use:\n        port = rng.randint(high - low) + low\n        in_use = False\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        try:\n            s.bind((""0.0.0.0"", port))\n        except socket.error as e:\n            if e.errno == 98:  # port already in use\n                in_use = True\n        s.close()\n    return port\n\n\nclass Manager:\n    def __init__(self, img_encode_method=jpeg_encode, rng=None):\n        self._queue = OrderedDict()\n        self._server = None\n        self.img_encode_method = img_encode_method\n        if rng is None:\n            rng = np.random.RandomState(self.get_default_seed())\n        self.rng = rng\n\n    def get_default_seed(self):\n        return 0\n\n    def imshow(self, title, img):\n        data = self.img_encode_method(img)\n        data = base64.b64encode(data)\n        data = data.decode(\'utf8\')\n        self._queue[title] = data\n\n    def waitKey(self, delay=0):\n        if self._server is None:\n            self.port = get_free_port(self.rng)\n            self._server, self._conn = get_server(port=self.port)\n        self._conn.send([delay, list(self._queue.items())])\n        # self._queue = OrderedDict()\n        return self._conn.recv()\n\nglobal_manager = Manager()\n\n'"
concern/webcv2/server.py,0,"b'#!/usr/bin/env mdl\nimport os\nBASE_DIR = os.path.dirname(os.path.realpath(__file__))\nimport time\nimport json\nimport select\nimport traceback\nimport socket\nfrom multiprocessing import Process, Pipe\n\nimport gevent\nfrom gevent.pywsgi import WSGIServer\nfrom geventwebsocket.handler import WebSocketHandler\nfrom flask import Flask, request, render_template, abort\n\n\ndef log_important_msg(msg, *, padding=3):\n    msg_len = len(msg)\n    width = msg_len + padding * 2 + 2\n    print(\'#\' * width)\n    print(\'#\' + \' \' * (width - 2) + \'#\')\n    print(\'#\' + \' \' * padding + msg + \' \' * padding + \'#\')\n    print(\'#\' + \' \' * (width - 2) + \'#\')\n    print(\'#\' * width)\n\n\ndef hint_url(url, port):\n    log_important_msg(\n        \'The server is running at: {}\'.format(url))\n\n\ndef _set_server(conn, name=\'webcv2\', port=7788):\n    package = None\n    package_alive = False\n\n    app = Flask(name)\n    app.root_path = BASE_DIR\n\n    @app.route(\'/\')\n    def index():\n        return render_template(\'index.html\', title=name)\n\n    @app.route(\'/stream\')\n    def stream():\n        def poll_ws(ws, delay):\n            return len(select.select([ws.stream.handler.rfile], [], [], delay / 1000.)[0]) > 0\n\n        if request.environ.get(\'wsgi.websocket\'):\n            ws = request.environ[\'wsgi.websocket\']\n            if ws is None:\n                abort(404)\n            else:\n                should_send = True\n                while not ws.closed:\n                    global package\n                    global package_alive\n                    if conn.poll():\n                        package = conn.recv()\n                        package_alive = True\n                        should_send = True\n                    if not should_send:\n                        continue\n                    should_send = False\n                    if package is None:\n                        ws.send(None)\n                    else:\n                        delay, info_lst = package\n                        ws.send(json.dumps((time.time(), package_alive, delay, info_lst)))\n                        if package_alive:\n                            if delay <= 0 or poll_ws(ws, delay):\n                                message = ws.receive()\n                                if ws.closed or message is None:\n                                    break\n                                try:\n                                    if isinstance(message, bytes):\n                                        message = message.decode(\'utf8\')\n                                    message = int(message)\n                                except:\n                                    traceback.print_exc()\n                                    message = -1\n                            else:\n                                message = -1\n                            conn.send(message)\n                            package_alive = False\n        return """"\n\n    http_server = WSGIServer((\'\', port), app, handler_class=WebSocketHandler)\n    hint_url(\'http://{}:{}\'.format(socket.getfqdn(), port), port)\n    http_server.serve_forever()\n\n\ndef get_server(name=\'webcv2\', port=7788):\n    conn_server, conn_factory = Pipe()\n    p_server = Process(\n        target=_set_server,\n        args=(conn_server,),\n        kwargs=dict(\n            name=name, port=port,\n        ),\n    )\n    p_server.daemon = True\n    p_server.start()\n    return p_server, conn_factory\n\n'"
data/meta_loaders/__init__.py,0,"b'from .data_id_meta_loader import DataIdMetaLoader\nfrom .recognition_meta_loader import RecognitionMetaLoader\nfrom .text_lines_meta_loader import TextLinesMetaLoader\nfrom .charbox_meta_loader import CharboxMetaLoader\nfrom .meta_cache import OSSMetaCache, FileMetaCache, RedisMetaCache\nfrom .lmdb_meta_loader import LMDBMetaLoader\nfrom .detection_meta_loader import DetectionMetaLoader'"
data/meta_loaders/charbox_meta_loader.py,0,"b""import numpy as np\n\nfrom concern.config import State\nfrom .recognition_meta_loader import RecognitionMetaLoader\n\n\nclass CharboxMetaLoader(RecognitionMetaLoader):\n    charbox_key = State(default='charboxes')\n    transpose = State(default=False)\n\n    def __init__(self, charbox_key=None, transpose=None, cmd={}, **kwargs):\n        super().__init__(cmd=cmd, **kwargs)\n        print('load CharBox')\n        if charbox_key is not None:\n            self.charbox_key = charbox_key\n        if transpose is not None:\n            self.transpose = transpose\n\n    def parse_meta(self, data_id, meta):\n        parsed = super().parse_meta(data_id, meta)\n        if parsed is None:\n            return\n\n        charbox = np.array(self.get_annotation(meta)[self.charbox_key])\n        if self.transpose:\n            charbox = charbox.transpose(2, 1, 0)\n        parsed['charboxes'] = charbox\n        return parsed\n"""
data/meta_loaders/data_id_meta_loader.py,0,"b""from concern.config import State\nfrom .meta_loader import MetaLoader\n\n\nclass DataIdMetaLoader(MetaLoader):\n    return_dict = State(default=False)\n    scan_meta = False\n\n    def __init__(self, return_dict=None, cmd={}, **kwargs):\n        super().__init__(cmd=cmd, **kwargs)\n        if return_dict is not None:\n            self.return_dict = return_dict\n\n    def parse_meta(self, data_id):\n        return dict(data_id=data_id)\n\n    def post_prosess(self, meta):\n        if self.return_dict:\n            return meta\n        return meta['data_id']\n"""
data/meta_loaders/detection_meta_loader.py,0,"b""from concern.config import State\nfrom .meta_loader import MetaLoader\n\n\nclass DetectionMetaLoader(MetaLoader):\n    key = State(default='gt')\n    scan_meta = True\n\n    def __init__(self, key=None, cmd={}, **kwargs):\n        super().__init__(cmd=cmd, **kwargs)\n        if key is not None:\n            self.key = key\n\n    def parse_meta(self, data_id, meta):\n        return dict(data_ids=data_id, gt=self.get_annotation(meta)[self.key])\n"""
data/meta_loaders/json_meta_loader.py,0,"b'import os\nimport json\nfrom concern.config import Configurable, State\n\n\nclass JsonMetaLoader(Configurable):\n    cache = State()\n    force_reload = State(default=False)\n    image_folder = State(default=\'images\')\n    json_file = State(default=\'meta.json\')\n\n    scan_meta = True\n    scan_data = False\n    post_prosess = None\n\n    def __init__(self, force_reload=None, cmd={}, **kwargs):\n        self.load_all(cmd=cmd, **kwargs)\n        self.force_reload = cmd.get(\'force_reload\', self.force_reload)\n        if force_reload is not None:\n            self.force_reload = force_reload\n\n    def load_meta(self, json_path):\n        if not self.force_reload and self.cache is not None:\n            meta = self.cache.read(json_path)\n            if meta is not None:\n                return meta\n\n        meta_info = dict()\n        valid_count = 0\n        with open(os.path.join(json_path, self.json_file)) as reader:\n            for line in reader.readlines():\n                line = line.strip()\n                single_meta = json.loads(line)\n                data_id = os.path.join(json_path, single_meta[\'filename\'])\n                args_dict = dict(data_id=data_id)\n\n                if self.scan_data:\n                    with open(self.same_dir_with(json_path), self.image_folder) as reader:\n                        data = reader.read()\n                    args_dict.update(data=data)\n                elif self.scan_meta:\n                    args_dict.update(meta=single_meta)\n\n                meta_instance = self.parse_meta(**args_dict)\n                if meta_instance is None:\n                    continue\n\n                valid_count += 1\n                if valid_count % 100000 == 0:\n                    print(""%d instances processd"" % valid_count)\n\n                for key in meta_instance:\n                    the_list = meta_info.get(key, [])\n                    the_list.append(meta_instance[key])\n                    meta_info[key] = the_list\n\n        print(valid_count, \'instances found\')\n        if self.post_prosess is not None:\n            meta_info = self.post_prosess(meta_info)\n\n        if self.cache is not None:\n            self.cache.save(json_path, meta_info)\n\n        return meta_info\n\n    def parse_meta(self, data_id, meta):\n        raise NotImplementedError\n\n    def get_annotation(self, meta):\n        return meta[\'extra\']\n\n    def same_dir_with(self, full_path, dest):\n        return os.path.join(os.path.dirname(full_path), dest)\n'"
data/meta_loaders/lmdb_meta_loader.py,0,"b'import lmdb\nimport pickle\nfrom collections import defaultdict\nfrom concern.config import Configurable, State\nimport time\nimport os\n\n\nclass LMDBMetaLoader(Configurable):\n    cache = State()\n    force_reload = State(default=False)\n    post_prosess = None\n\n    def __init__(self, force_reload=None, cmd={}, **kwargs):\n        self.load_all(cmd=cmd, **kwargs)\n        self.force_reload = cmd.get(\'force_reload\', self.force_reload)\n\n        # lmdb environments\n        self.envs = {}\n        self.txns = {}\n        if force_reload is not None:\n            self.force_reload = force_reload\n\n    def __del__(self):\n        for path in self.envs:\n            self.envs[path].close()\n\n    def load_meta(self, lmdb_path):\n        lmdb_path = os.path.join(lmdb_path, \'\')\n        if not self.force_reload and self.cache is not None:\n            meta = self.cache.read(lmdb_path)\n            if meta is not None:\n                return meta\n        meta_info = defaultdict(list)\n        valid_count = 0\n        if lmdb_path not in self.envs:\n            env = lmdb.open(lmdb_path, max_dbs=1, lock=False)\n            self.envs[lmdb_path] = env\n            db_extra = env.open_db(\'extra\'.encode())\n            self.txns[lmdb_path] = env.begin(db=db_extra)\n\n        txn = self.txns[lmdb_path]\n        cursor = txn.cursor()\n        for data_id, value in cursor:\n            args_tuple = (data_id, )\n            if self.scan_meta:\n                args_tuple = tuple((*args_tuple, pickle.loads(value)))\n            meta_instance = self.parse_meta(*args_tuple)\n            if meta_instance is None:\n                continue\n            meta_instance[\'db_path\'] = lmdb_path\n            valid_count += 1\n            if valid_count % 100000 == 0:\n                print(""%d instances processd"" % valid_count)\n            for key in meta_instance:\n                meta_info[key].append(meta_instance[key])\n\n        print(valid_count, \'instances found\')\n        if self.post_prosess is not None:\n            meta_info = self.post_prosess(meta_info)\n\n        if self.cache is not None:\n            self.cache.save(lmdb_path, meta_info)\n\n        return meta_info\n\n    def parse_meta(self, data_id, meta):\n        raise NotImplementedError\n\n    def get_annotation(self, meta):\n        return meta[\'extra\']\n'"
data/meta_loaders/meta_cache.py,0,"b""import pickle\nimport hashlib\nimport os\nimport io\nimport time\nimport urllib.parse as urlparse\nimport warnings\nimport boto3\nfrom botocore.exceptions import ClientError\n\nfrom concern.config import Configurable, State\nfrom concern.distributed import is_main\nfrom concern.redis_meta import RedisMeta\nimport config\nimport redis\n\n\nclass MetaCache(Configurable):\n    META_FILE = 'meta_cache.pickle'\n    client = State(default='all')\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n    def cache(self, nori_path, meta=None):\n        if meta is None:\n            return self.read(nori_path)\n        else:\n            return self.save(nori_path, meta)\n\n    def read(self, nori_path):\n        raise NotImplementedError\n\n    def save(self, nori_path, meta):\n        raise NotImplementedError\n\n\nclass FileMetaCache(MetaCache):\n    storage_dir = State(default=os.path.join(config.db_path, 'meta_cache'))\n    inplace = State(default=not config.will_use_nori)\n\n    def __init__(self, storage_dir=None, cmd={}, **kwargs):\n        super(FileMetaCache, self).__init__(cmd=cmd, **kwargs)\n\n        self.debug = cmd.get('debug', False)\n\n    def ensure_dir(self):\n        if not os.path.exists(self.storage_dir):\n            os.makedirs(self.storage_dir)\n\n    def storate_path(self, nori_path):\n        if self.inplace:\n            if config.will_use_lmdb:\n                return os.path.join(\n                    nori_path,\n                    'meta_cache-%s.pickle' % self.client)\n            else:\n                return os.path.join(\n                    os.path.dirname(nori_path),\n                    'meta_cache-%s.pickle' % self.client)\n        return os.path.join(self.storage_dir, self.hash(nori_path) + '.pickle')\n\n    def hash(self, nori_path: str):\n        return hashlib.md5(nori_path.encode('utf-8')).hexdigest() + '-' + self.client\n\n    def read(self, nori_path):\n        file_path = self.storate_path(nori_path)\n        if not os.path.exists(file_path):\n            warnings.warn(\n                'Meta cache not found: ' + file_path)\n            warnings.warn('Now trying to read meta from scratch')\n            return None\n        with open(file_path, 'rb') as reader:\n            try:\n                return pickle.load(reader)\n            except EOFError as e:  # recover from broken file\n                if self.debug:\n                    raise e\n                return None\n\n    def save(self, nori_path, meta):\n        self.ensure_dir()\n\n        with open(self.storate_path(nori_path), 'wb') as writer:\n            pickle.dump(meta, writer)\n        return True\n\n\nclass OSSMetaCache(MetaCache):\n    HOST = State(default=config.oss_host)\n\n    def __init__(self, **kwargs):\n        super(OSSMetaCache, self).__init__(**kwargs)\n\n    def oss_client(self):\n        return boto3.client('s3', endpoint_url=self.HOST)\n\n    def parse(self, nori_path):\n        parse = urlparse.urlparse(nori_path)\n        assert parse.scheme == 's3'\n\n        bucket = parse.netloc\n        path = os.path.join(parse.path, self.client, self.META_FILE)\n        if path.startswith('/'):\n            path = path[1:]\n\n        return bucket, path\n\n    def read(self, nori_path):\n        bucket, path = self.parse(nori_path)\n        try:\n            resp = self.oss_client().get_object(Bucket=bucket, Key=path)\n            data = resp['Body'].read()\n        except ClientError as e:\n            warnings.warn(\n                'Get error when loading meta cache from oss:' + str(e))\n            warnings.warn(\n                'Nori path:' + str(nori_path))\n            warnings.warn('Now trying to read meta from scratch')\n            return None\n        return pickle.loads(data)\n\n    def save(self, nori_path, meta):\n        data = io.BytesIO(pickle.dumps(meta))\n        bucket, path = self.parse(nori_path)\n        try:\n            resp = self.oss_client().upload_fileobj(data, bucket, path)\n            if resp is None:\n                return True\n        except ClientError as e:\n            warnings.warn('Get error when loading meta from oss:' + e.message)\n            return False\n        return True\n\n\nclass RedisMetaCache(MetaCache):\n    host = State(default=config.redis_host)\n    port = State(default=config.redis_port)\n\n    client = State(default='all')\n\n    def __init__(self, **kwargs):\n        super(RedisMetaCache, self).__init__(**kwargs)\n        self.connection = redis.Redis(host=self.host, port=self.port)\n\n    def hash(self, nori_path):\n        return os.path.join(nori_path, self.client)\n\n    def read(self, nori_path):\n        path = self.hash(nori_path)\n        while not self.connection.exists(os.path.join(path, '__keys__')):\n            warnings.warn(\n                'Cache not found in redis:' + str(path))\n            if is_main():\n                warnings.warn('Now may try to load and save meta from file')\n                return None\n            else:\n                time.sleep(2)\n        return RedisMeta(self.connection, path)\n\n    def save(self, nori_path, meta):\n        if not is_main():\n            return False\n\n        path = self.hash(nori_path)\n        for key, value_list in meta.items():\n            pipe = self.connection.pipeline()\n            pipe.sadd(os.path.join(path, '__keys__'), key)\n            for value in value_list:\n                pipe.lpush(os.path.join(path, key), value)\n            pipe.execute()\n        return True\n"""
data/meta_loaders/meta_loader.py,0,"b""import config\n\nassert not (config.will_use_nori and config.will_use_lmdb), 'only one metaloader can be used'\nif config.will_use_nori:\n    from .nori_meta_loader import NoriMetaLoader\n    MetaLoader = NoriMetaLoader\nelif config.will_use_lmdb:\n    from .lmdb_meta_loader import LMDBMetaLoader\n    MetaLoader = LMDBMetaLoader\nelse:\n    from .json_meta_loader import JsonMetaLoader\n    MetaLoader = JsonMetaLoader\n"""
data/meta_loaders/nori_meta_loader.py,0,"b'from concern.config import Configurable, State\nimport nori2 as nori\n\n\nclass NoriMetaLoader(Configurable):\n    cache = State()\n    force_reload = State(default=False)\n\n    scan_meta = True\n    scan_data = False\n    post_prosess = None\n\n    def __init__(self, force_reload=None, cmd={}, **kwargs):\n        self.load_all(cmd=cmd, **kwargs)\n        self.force_reload = cmd.get(\'force_reload\', self.force_reload)\n        if force_reload is not None:\n            self.force_reload = force_reload\n\n    def load_meta(self, nori_path):\n        if not self.force_reload and self.cache is not None:\n            meta = self.cache.read(nori_path)\n            if meta is not None:\n                return meta\n\n        meta_info = dict()\n        valid_count = 0\n        with nori.open(nori_path) as reader:\n            for data_id, data, meta in reader.scan(\n                    scan_data=self.scan_data, scan_meta=self.scan_meta):\n                args_tuple = (data_id, )\n                if self.scan_data:\n                    args_tuple = tuple((*args_tuple, data))\n                if self.scan_meta:\n                    args_tuple = tuple((*args_tuple, meta))\n                meta_instance = self.parse_meta(*args_tuple)\n                if meta_instance is None:\n                    continue\n                valid_count += 1\n                if valid_count % 100000 == 0:\n                    print(""%d instances processd"" % valid_count)\n                for key in meta_instance:\n                    the_list = meta_info.get(key, [])\n                    the_list.append(meta_instance[key])\n                    meta_info[key] = the_list\n\n        print(valid_count, \'instances found\')\n        if self.post_prosess is not None:\n            meta_info = self.post_prosess(meta_info)\n\n        if self.cache is not None:\n            self.cache.save(nori_path, meta_info)\n\n        return meta_info\n\n    def parse_meta(self, data_id, meta):\n        raise NotImplementedError\n\n    def get_annotation(self, meta):\n        return meta[\'extra\']\n'"
data/meta_loaders/recognition_meta_loader.py,0,"b""from hanziconv import HanziConv\n\nfrom concern.charset_tool import stringQ2B\nfrom concern.config import State\nfrom .meta_loader import MetaLoader\n\n\nclass RecognitionMetaLoader(MetaLoader):\n    skip_vertical = State(default=False)\n    case_sensitive = State(default=False)\n    simplify = State(default=False)\n    key = State(default='words')\n    scan_meta = True\n\n    def __init__(self, key=None, cmd={}, **kwargs):\n        super().__init__(cmd=cmd, **kwargs)\n        if key is not None:\n            self.key = key\n\n    def may_simplify(self, words):\n        garbled = stringQ2B(words)\n        if self.simplify:\n            return HanziConv.toSimplified(garbled)\n        return garbled\n\n    def parse_meta(self, data_id, meta):\n        word = self.may_simplify(self.get_annotation(meta)[self.key])\n        vertical = self.get_annotation(meta).get('vertical', False)\n        if self.skip_vertical and vertical:\n            return None\n\n        if word == '###':\n            return None\n        return dict(data_ids=data_id, gt=word)\n"""
data/meta_loaders/redis_meta.py,0,"b'import redis\n\n\nclass RedisMeta:\n    def __init__(self, socket_path):\n        redis.StrictRedis()\n'"
data/meta_loaders/text_lines_meta_loader.py,0,"b""from data.text_lines import TextLines\nfrom .meta_loader import MetaLoader\n\n\nclass TextLinesMetaLoader(MetaLoader):\n    def parse_meta(self, data_id, meta):\n        return dict(\n                data_id=data_id,\n                lines=TextLines(self.get_annotation(meta)['lines']))\n"""
data/processes/__init__.py,0,"b'from .normalize_image import NormalizeImage\nfrom .make_recognition_label import MakeRecognitionLabel\nfrom .make_keypoint_map import MakeKeyPointMap\nfrom .make_center_points import MakeCenterPoints\nfrom .resize_image import ResizeImage, ResizeData\nfrom .make_seg_recognition_label import MakeSegRecognitionLabel\nfrom .filter_keys import FilterKeys\nfrom .make_center_map import MakeCenterMap\nfrom .augment_data import AugmentData, AugmentDetectionData\nfrom .random_crop_data import RandomCropData\nfrom .make_icdar_data import MakeICDARData, ICDARCollectFN\nfrom .make_seg_detection_data import MakeSegDetectionData\nfrom .make_border_map import MakeBorderMap\nfrom .extract_detetion_data import ExtractDetectionData\nfrom .make_decouple_map import MakeDecoupleMap\n'"
data/processes/augment_data.py,0,"b""import imgaug\nimport numpy as np\n\nfrom concern.config import State\nfrom .data_process import DataProcess\nfrom data.augmenter import AugmenterBuilder\n\n\nclass AugmentData(DataProcess):\n    augmenter_args = State(autoload=False)\n\n    def __init__(self, **kwargs):\n        self.augmenter_args = kwargs.get('augmenter_args')\n        self.augmenter = AugmenterBuilder().build(self.augmenter_args)\n\n    def may_augment_annotation(self, aug, data):\n        pass\n\n    def process(self, data):\n        image = data['image']\n        aug = None\n        shape = image.shape\n\n        if self.augmenter:\n            aug = self.augmenter.to_deterministic()\n            data['image'] = aug.augment_image(image)\n            self.may_augment_annotation(aug, data, shape)\n\n        filename = data.get('filename', data.get('data_id', ''))\n        data.update(filename=filename, shape=shape[:2])\n        return data\n\n\nclass AugmentDetectionData(AugmentData):\n    def may_augment_annotation(self, aug, data, shape):\n        if aug is None:\n            return data\n\n        line_polys = []\n        for line in data['lines']:\n            line_polys.append({\n                'points': self.may_augment_poly(aug, shape, line['poly']),\n                'ignore': line['text'] == '###',\n                'text': line['text'],\n            })\n        data['polys'] = line_polys\n        return data\n\n    def may_augment_poly(self, aug, img_shape, poly):\n        keypoints = [imgaug.Keypoint(p[0], p[1]) for p in poly]\n        keypoints = aug.augment_keypoints(\n            [imgaug.KeypointsOnImage(keypoints, shape=img_shape)])[0].keypoints\n        poly = [(p.x, p.y) for p in keypoints]\n        return poly\n\n\nclass AugmentTextLine(AugmentData):\n    def may_augment_annotation(self, aug, data, shape):\n        if aug is None:\n            return data\n\n        lines = data['lines']\n        points_shape = lines.quads.points.shape\n        lines.quads.points = self.may_augment_poly(\n            aug, shape, lines.quads.points.reshape(-1, 2)).reshape(*points_shape)\n        for quads in lines.charboxes:\n            points_shape = quads.points.shape\n            quads.points = self.may_augment_poly(\n                aug, shape, quads.points.reshape(-1, 2)).reshape(*points_shape)\n        return data\n\n    def may_augment_poly(self, aug, img_shape, poly):\n        keypoints = [imgaug.Keypoint(p[0], p[1]) for p in poly]\n        keypoints = aug.augment_keypoints(\n            [imgaug.KeypointsOnImage(keypoints, shape=img_shape)])[0].keypoints\n        poly = [(p.x, p.y) for p in keypoints]\n        return np.array(poly, dtype=np.float32)\n"""
data/processes/charboxes_from_textlines.py,0,"b""import numpy as np\n\nfrom .data_process import DataProcess\n\n\nclass CharboxesFromTextlines(DataProcess):\n    def process(self, data):\n        text_lines = data['lines']\n\n        charboxes = None\n        shape = None\n        for boxes in text_lines.charboxes:\n            shape = boxes.shape[1:]\n            if charboxes is None:\n                charboxes = boxes\n            else:\n                charboxes = np.concatenate([charboxes, boxes], axis=0)\n        charboxes = np.concatenate(charboxes, axis=0)\n        if shape is not None:\n            charboxes = charboxes.reshape(-1, *shape)\n        data['charboxes'] = charboxes\n\n        return data\n"""
data/processes/data_process.py,0,"b""from concern.config import Configurable\n\n\nclass DataProcess(Configurable):\n    r'''Processes of data dict.\n    '''\n\n    def __call__(self, data):\n        return self.process(data)\n\n    def process(self, data):\n        raise NotImplementedError\n\n    def render_constant(self, canvas, xmin, xmax, ymin, ymax, value=1, shrink=0):\n        def shrink_rect(xmin, xmax, ratio):\n            center = (xmin + xmax) / 2\n            width = center - xmin\n            return int(center - width * ratio + 0.5), int(center + width * ratio + 0.5)\n\n        if shrink > 0:\n            xmin, xmax = shrink_rect(xmin, xmax, shrink)\n            ymin, ymax = shrink_rect(ymin, ymax, shrink)\n\n        canvas[int(ymin+0.5):int(ymax+0.5)+1, int(xmin+0.5):int(xmax+0.5)+1] = value\n        return canvas\n"""
data/processes/extract_detetion_data.py,0,"b""import imgaug\nfrom concern.config import Configurable, State\nfrom data.augmenter import AugmenterBuilder\n\n\nclass ExtractDetectionData(Configurable):\n    augmenter_args = State(autoload=False)\n    augmenter_class = State(default=None)\n\n    def __init__(self, augmenter_class=None, **kwargs):\n        self.augmenter_args = kwargs.get('augmenter_args')\n        self.augmenter_class = augmenter_class or self.augmenter_class\n        if self.augmenter_class is not None:\n            self.augmenter = eval(self.augmenter_class)().augmenter\n        else:\n            self.augmenter = AugmenterBuilder().build(self.augmenter_args)\n\n    def may_augment_poly(self, aug, img_shape, poly):\n        if aug is None:\n            return poly\n        keypoints = [imgaug.Keypoint(p[0], p[1]) for p in poly]\n        keypoints = aug.augment_keypoints(\n            [imgaug.KeypointsOnImage(keypoints, shape=img_shape)])[0].keypoints\n        poly = [(p.x, p.y) for p in keypoints]\n        return poly\n\n    def __call__(self, data):\n        img = data['img']\n        aug = None\n        shape = img.shape\n\n        if self.augmenter:\n            aug = self.augmenter.to_deterministic()\n            img = aug.augment_image(data['img'])\n\n        line_polys = []\n        for line in data['lines']:\n            line_polys.append({\n                'points': self.may_augment_poly(aug, shape, line['poly']),\n                'ignore': line['text'] == '###',\n                'text': line['text'],\n            })\n        filename = data.get('filename', data.get('data_id', ''))\n        label = {\n            'polys': line_polys\n        }\n        meta = {\n            'data_id': data['data_id'],\n            'filename': filename,\n            'shape': shape[:2]\n        }\n        return img, label, meta\n"""
data/processes/filter_keys.py,0,"b""from concern.config import State\n\nfrom .data_process import DataProcess\n\n\nclass FilterKeys(DataProcess):\n    required = State(default=[])\n    superfluous = State(default=[])\n\n    def __init__(self, **kwargs):\n        super().__init__(self, **kwargs)\n\n        self.required_keys = set(self.required)\n        self.superfluous_keys = set(self.superfluous)\n        if len(self.required_keys) > 0 and len(self.superfluous_keys) > 0:\n            raise ValueError(\n                'required_keys and superfluous_keys can not be specified at the same time.')\n\n    def process(self, data):\n        for key in self.required:\n            assert key in data, '%s is required in data' % key\n\n        superfluous = self.superfluous_keys\n        if len(superfluous) == 0:\n            for key in data.keys():\n                if key not in self.required_keys:\n                    superfluous.add(key)\n\n        for key in superfluous:\n            del data[key]\n        return data\n"""
data/processes/make_border_map.py,0,"b'import warnings\nimport numpy as np\nimport cv2\nfrom shapely.geometry import Polygon\nimport pyclipper\n\nfrom concern.config import State\nfrom .data_process import DataProcess\n\n\nclass MakeBorderMap(DataProcess):\n    r\'\'\'\n    Making the border map from detection data with ICDAR format.\n    Typically following the process of class `MakeICDARData`.\n    \'\'\'\n    shrink_ratio = State(default=0.4)\n    thresh_min = State(default=0.3)\n    thresh_max = State(default=0.7)\n\n    def __init__(self, cmd={}, *args, **kwargs):\n        self.load_all(cmd=cmd, **kwargs)\n        warnings.simplefilter(""ignore"")\n\n    def process(self, data, *args, **kwargs):\n        r\'\'\'\n        required keys:\n            image, polygons, ignore_tags\n        adding keys:\n            thresh_map, thresh_mask\n        \'\'\'\n        image = data[\'image\']\n        polygons = data[\'polygons\']\n        ignore_tags = data[\'ignore_tags\']\n\n        canvas = np.zeros(image.shape[:2], dtype=np.float32)\n        mask = np.zeros(image.shape[:2], dtype=np.float32)\n\n        for i in range(polygons.shape[0]):\n            if ignore_tags[i]:\n                continue\n            self.draw_border_map(polygons[i], canvas, mask=mask)\n        canvas = canvas * (self.thresh_max - self.thresh_min) + self.thresh_min\n        data[\'thresh_map\'] = canvas\n        data[\'thresh_mask\'] = mask\n        return data\n\n    def draw_border_map(self, polygon, canvas, mask):\n        polygon = np.array(polygon)\n        assert polygon.ndim == 2\n        assert polygon.shape[1] == 2\n\n        polygon_shape = Polygon(polygon)\n        distance = polygon_shape.area * \\\n            (1 - np.power(self.shrink_ratio, 2)) / polygon_shape.length\n        subject = [tuple(l) for l in polygon]\n        padding = pyclipper.PyclipperOffset()\n        padding.AddPath(subject, pyclipper.JT_ROUND,\n                        pyclipper.ET_CLOSEDPOLYGON)\n        padded_polygon = np.array(padding.Execute(distance)[0])\n        cv2.fillPoly(mask, [padded_polygon.astype(np.int32)], 1.0)\n\n        xmin = padded_polygon[:, 0].min()\n        xmax = padded_polygon[:, 0].max()\n        ymin = padded_polygon[:, 1].min()\n        ymax = padded_polygon[:, 1].max()\n        width = xmax - xmin + 1\n        height = ymax - ymin + 1\n\n        polygon[:, 0] = polygon[:, 0] - xmin\n        polygon[:, 1] = polygon[:, 1] - ymin\n\n        xs = np.broadcast_to(\n            np.linspace(0, width - 1, num=width).reshape(1, width), (height, width))\n        ys = np.broadcast_to(\n            np.linspace(0, height - 1, num=height).reshape(height, 1), (height, width))\n\n        distance_map = np.zeros(\n            (polygon.shape[0], height, width), dtype=np.float32)\n        for i in range(polygon.shape[0]):\n            j = (i + 1) % polygon.shape[0]\n            absolute_distance = self.distance(xs, ys, polygon[i], polygon[j])\n            distance_map[i] = np.clip(absolute_distance / distance, 0, 1)\n        distance_map = distance_map.min(axis=0)\n\n        xmin_valid = min(max(0, xmin), canvas.shape[1] - 1)\n        xmax_valid = min(max(0, xmax), canvas.shape[1] - 1)\n        ymin_valid = min(max(0, ymin), canvas.shape[0] - 1)\n        ymax_valid = min(max(0, ymax), canvas.shape[0] - 1)\n        canvas[ymin_valid:ymax_valid + 1, xmin_valid:xmax_valid + 1] = np.fmax(\n            1 - distance_map[\n                ymin_valid-ymin:ymax_valid-ymax+height,\n                xmin_valid-xmin:xmax_valid-xmax+width],\n            canvas[ymin_valid:ymax_valid + 1, xmin_valid:xmax_valid + 1])\n\n    def distance(self, xs, ys, point_1, point_2):\n        \'\'\'\n        compute the distance from point to a line\n        ys: coordinates in the first axis\n        xs: coordinates in the second axis\n        point_1, point_2: (x, y), the end of the line\n        \'\'\'\n        height, width = xs.shape[:2]\n        square_distance_1 = np.square(\n            xs - point_1[0]) + np.square(ys - point_1[1])\n        square_distance_2 = np.square(\n            xs - point_2[0]) + np.square(ys - point_2[1])\n        square_distance = np.square(\n            point_1[0] - point_2[0]) + np.square(point_1[1] - point_2[1])\n\n        cosin = (square_distance - square_distance_1 - square_distance_2) / \\\n            (2 * np.sqrt(square_distance_1 * square_distance_2))\n        square_sin = 1 - np.square(cosin)\n        square_sin = np.nan_to_num(square_sin)\n        result = np.sqrt(square_distance_1 * square_distance_2 *\n                         square_sin / square_distance)\n\n        result[cosin < 0] = np.sqrt(np.fmin(\n            square_distance_1, square_distance_2))[cosin < 0]\n        # self.extend_line(point_1, point_2, result)\n        return result\n\n    def extend_line(self, point_1, point_2, result):\n        ex_point_1 = (int(round(point_1[0] + (point_1[0] - point_2[0]) * (1 + self.shrink_ratio))),\n                      int(round(point_1[1] + (point_1[1] - point_2[1]) * (1 + self.shrink_ratio))))\n        cv2.line(result, tuple(ex_point_1), tuple(point_1),\n                 4096.0, 1, lineType=cv2.LINE_AA, shift=0)\n        ex_point_2 = (int(round(point_2[0] + (point_2[0] - point_1[0]) * (1 + self.shrink_ratio))),\n                      int(round(point_2[1] + (point_2[1] - point_1[1]) * (1 + self.shrink_ratio))))\n        cv2.line(result, tuple(ex_point_2), tuple(point_2),\n                 4096.0, 1, lineType=cv2.LINE_AA, shift=0)\n        return ex_point_1, ex_point_2\n\n'"
data/processes/make_center_distance_map.py,0,"b'import warnings\nimport numpy as np\nimport cv2\nfrom shapely.geometry import Polygon\nimport pyclipper\n\nfrom concern.config import State\nfrom .data_process import DataProcess\n\n\nclass MakeCenterDistanceMap(DataProcess):\n    r\'\'\'\n    Making the border map from detection data with ICDAR format.\n    Typically following the process of class `MakeICDARData`.\n    \'\'\'\n    expansion_ratio = State(default=0.1)\n\n    def __init__(self, cmd={}, *args, **kwargs):\n        self.load_all(cmd=cmd, **kwargs)\n        warnings.simplefilter(""ignore"")\n\n    def process(self, data, *args, **kwargs):\n        r\'\'\'\n        required keys:\n            image.\n            lines: Instace of `TextLines`, which is defined in data/text_lines.py\n        adding keys:\n            distance_map\n        \'\'\'\n        image = data[\'image\']\n        lines = data[\'lines\']\n\n        h, w = image.shape[:2]\n        canvas = np.zeros(image.shape[:2], dtype=np.float32)\n        mask = np.zeros(image.shape[:2], dtype=np.float32)\n        for _, quad in lines:\n            padded = self.expand_quad(quad)\n            center_x = padded[:, 0].mean()\n            center_y = padded[:, 1].mean()\n            index_x, index_y = np.meshgrid(np.arange(w), np.arange(h))\n            self.render_distance_map(canvas, center_x, center_y, index_x, index_y)\n            self.render_constant(mask, quad, 1)\n\n        canvas = canvas * (self.thresh_max - self.thresh_min) + self.thresh_min\n        data[\'thresh_map\'] = canvas\n        return data\n\n    def expand_quad(self, polygon):\n        polygon = np.array(polygon)\n        assert polygon.ndim == 2\n        assert polygon.shape[1] == 2\n\n        polygon_shape = Polygon(polygon)\n        distance = polygon_shape.area * \\\n            (1 - np.power(self.expansion_ratio, 2)) / polygon_shape.length\n        subject = [tuple(l) for l in polygon]\n        padding = pyclipper.PyclipperOffset()\n        padding.AddPath(subject, pyclipper.JT_ROUND,\n                        pyclipper.ET_CLOSEDPOLYGON)\n        padded_polygon = np.array(padding.Execute(distance)[0])\n        return padded_polygon\n        cv2.fillPoly(mask, [padded_polygon.astype(np.int32)], 1.0)\n\n\n\n\n    def distance(self, xs, ys, point):\n        \'\'\'\n        compute the distance from point to a line\n        ys: coordinates in the first axis\n        xs: coordinates in the second axis\n        point_1, point_2: (x, y), the end of the line\n        \'\'\'\n        height, width = xs.shape[:2]\n        square_distance_1 = np.square(\n            xs - point_1[0]) + np.square(ys - point_1[1])\n        square_distance_2 = np.square(\n            xs - point_2[0]) + np.square(ys - point_2[1])\n        square_distance = np.square(\n            point_1[0] - point_2[0]) + np.square(point_1[1] - point_2[1])\n\n        cosin = (square_distance - square_distance_1 - square_distance_2) / \\\n            (2 * np.sqrt(square_distance_1 * square_distance_2))\n        square_sin = 1 - np.square(cosin)\n        square_sin = np.nan_to_num(square_sin)\n        result = np.sqrt(square_distance_1 * square_distance_2 *\n                         square_sin / square_distance)\n\n        result[cosin < 0] = np.sqrt(np.fmin(\n            square_distance_1, square_distance_2))[cosin < 0]\n        # self.extend_line(point_1, point_2, result)\n        return result\n\n    def extend_line(self, point_1, point_2, result):\n        ex_point_1 = (int(round(point_1[0] + (point_1[0] - point_2[0]) * (1 + self.shrink_ratio))),\n                      int(round(point_1[1] + (point_1[1] - point_2[1]) * (1 + self.shrink_ratio))))\n        cv2.line(result, tuple(ex_point_1), tuple(point_1),\n                 4096.0, 1, lineType=cv2.LINE_AA, shift=0)\n        ex_point_2 = (int(round(point_2[0] + (point_2[0] - point_1[0]) * (1 + self.shrink_ratio))),\n                      int(round(point_2[1] + (point_2[1] - point_1[1]) * (1 + self.shrink_ratio))))\n        cv2.line(result, tuple(ex_point_2), tuple(point_2),\n                 4096.0, 1, lineType=cv2.LINE_AA, shift=0)\n        return ex_point_1, ex_point_2\n\n\n'"
data/processes/make_center_map.py,0,"b""import numpy as np\nimport scipy.ndimage.filters as fi\n\nfrom concern.config import State\n\nfrom .data_process import DataProcess\n\n\nclass MakeCenterMap(DataProcess):\n    max_size = State(default=32)\n    shape = State(default=(64, 256))\n    sigma_ratio = State(default=16)\n    function_name = State(default='sample_gaussian')\n    points_key = 'points'\n    correlation = 0  # The formulation of guassian is simplified when correlation is 0\n\n    def process(self, data):\n        assert self.points_key in data, '%s in data is required' % self.points_key\n        points = data['points'] * self.shape[::-1]  # N, 2\n        assert points.shape[0] >= self.max_size\n        func = getattr(self, self.function_name)\n        data['charmaps'] = func(points, *self.shape)\n        return data\n\n    def gaussian(self, points, height, width):\n        index_x, index_y = np.meshgrid(np.linspace(0, width, width),\n                                       np.linspace(0, height, height))\n        index_x = np.repeat(index_x[np.newaxis], points.shape[0], axis=0)\n        index_y = np.repeat(index_y[np.newaxis], points.shape[0], axis=0)\n        mu_x = points[:, 0][:, np.newaxis, np.newaxis]\n        mu_y = points[:, 1][:, np.newaxis, np.newaxis]\n        mask_is_zero = ((mu_x == 0) + (mu_y == 0)) == 0\n        result = np.reciprocal(2 * np.pi * width / self.sigma_ratio * height / self.sigma_ratio)\\\n            * np.exp(- 0.5 * (np.square((index_x - mu_x) / width * self.sigma_ratio) +\n                              np.square((index_y - mu_y) / height * self.sigma_ratio)))\n\n        result = result / \\\n            np.maximum(result.max(axis=1, keepdims=True).max(\n                axis=2, keepdims=True), np.finfo(np.float32).eps)\n        result = result * mask_is_zero\n        return result.astype(np.float32)\n\n    def sample_gaussian(self, points, height, width):\n        points = (points + 0.5).astype(np.int32)\n        canvas = np.zeros((self.max_size, height, width), dtype=np.float32)\n        for index in range(canvas.shape[0]):\n            point = points[index]\n            canvas[index, point[1], point[0]] = 1.\n            if point.sum() > 0:\n                fi.gaussian_filter(canvas[index], (height // self.sigma_ratio,\n                                                   width // self.sigma_ratio),\n                                   output=canvas[index], mode='mirror')\n                canvas[index] = canvas[index] / canvas[index].max()\n                x_range = min(point[0], width - point[0])\n                canvas[index, :, :point[0] - x_range] = 0\n                canvas[index, :, point[0] + x_range:] = 0\n                y_range = min(point[1], width - point[1])\n                canvas[index, :point[1] - y_range, :] = 0\n                canvas[index, point[1] + y_range:, :] = 0\n        return canvas\n"""
data/processes/make_center_points.py,0,"b""import numpy as np\n\nfrom concern.config import State\nfrom .data_process import DataProcess\n\n\nclass MakeCenterPoints(DataProcess):\n    box_key = State(default='charboxes')\n    size = State(default=32)\n\n    def process(self, data):\n        shape = data['image'].shape[:2]\n        points = np.zeros((self.size, 2), dtype=np.float32)\n        boxes = np.array(data[self.box_key])[:self.size]\n\n        size = boxes.shape[0]\n        points[:size] = boxes.mean(axis=1)\n        data['points'] = (points / shape[::-1]).astype(np.float32)\n        return data\n"""
data/processes/make_decouple_map.py,0,"b""import numpy as np\nimport scipy.ndimage.filters as fi\n\nfrom concern.config import State\n\nfrom .data_process import DataProcess\n\n\nclass MakeDecoupleMap(DataProcess):\n    max_size = State(default=32)\n    shape = State(default=(64, 256))\n    sigma = State(default=2)\n    summation = State(default=False)\n    box_key = State(default='charboxes')\n    function = State(default='gaussian')\n\n    def process(self, data):\n        assert self.box_key in data, '%s in data is required' % self.box_key\n        shape = data['image'].shape[:2]\n        boxes = np.array(data[self.box_key])\n\n        ratio_x = shape[1] / self.shape[1]\n        boxes[:, :, 0] = (boxes[:, :, 0] / ratio_x).clip(0, self.shape[1])\n        ratio_y = shape[0] / self.shape[0]\n        boxes[:, :, 1] = (boxes[:, :, 1] / ratio_y).clip(0, self.shape[0])\n        boxes = (boxes + .5).astype(np.int32)\n        xmins = boxes[:, :, 0].min(axis=1)\n        xmaxs = np.maximum(boxes[:, :, 0].max(axis=1), xmins + 1)\n\n        ymins = boxes[:, :, 1].min(axis=1)\n        ymaxs = np.maximum(boxes[:, :, 1].max(axis=1), ymins + 1)\n\n        if self.summation:\n            canvas = np.zeros(self.shape, dtype=np.int32)\n        else:\n            canvas = np.zeros((self.max_size, *self.shape), dtype=np.float32)\n\n        mask = np.zeros(self.shape, dtype=np.float32)\n        orders = self.orders(data)\n        for i in range(xmins.shape[0]):\n            function = getattr(self, 'render_' + self.function)\n            order = min(orders[i], self.max_size)\n            if self.summation:\n                function(canvas, xmins[i], xmaxs[i], ymins[i], ymaxs[i],\n                         value=order+1, shrink=0.6)\n            else:\n                function(canvas[order], xmins[i], xmaxs[i], ymins[i], ymaxs[i])\n            self.render_gaussian(mask, xmins[i], xmaxs[i], ymins[i], ymaxs[i])\n\n        data['ordermaps'] = canvas\n        data['mask'] = mask\n        return data\n\n    def orders(self, data):\n        orders = []\n        if 'lines' in data:\n            for text in data['lines'].texts:\n                orders += list(range(len(text)))\n        else:\n            orders = list(range(data[self.box_key].shape[0]))\n        return orders\n\n    def render_gaussian_thresh(self, canvas, xmin, xmax, ymin, ymax,\n                               value=1, thresh=0.2, shrink=None):\n        out = np.zeros_like(canvas).astype(np.float32)\n        out[(ymax+ymin+1)//2, (xmax+xmin+1)//2] = 1.\n        h, w = canvas.shape[:2]\n        out = fi.gaussian_filter(out, (self.sigma, self.sigma),\n                           output=out, mode='mirror')\n        out = out / out.max()\n        canvas[out > thresh] = value\n\n    def render_gaussian(self, canvas, xmin, xmax, ymin, ymax):\n        out = np.zeros_like(canvas)\n        out[(ymax+ymin+1)//2, (xmax+xmin+1)//2] = 1.\n        h, w = canvas.shape[:2]\n        fi.gaussian_filter(out, (self.sigma, self.sigma),\n                           output=out, mode='mirror')\n        out = out / out.max()\n        canvas[out > canvas] = out[out > canvas]\n\n    def render_gaussian_fast(self, canvas, xmin, xmax, ymin, ymax):\n        out = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.float32)\n        out[(ymax-ymin+1)//2, (xmax-xmin+1)//2] = 1.\n        h, w = canvas.shape[:2]\n        fi.gaussian_filter(out, (self.sigma, self.sigma),\n                           output=out, mode='mirror')\n        out = out / out.max()\n        canvas[ymin:ymax+1, xmin:xmax+1] = np.maximum(out, canvas[ymin:ymax+1, xmin:xmax+1])\n"""
data/processes/make_density_map.py,0,"b""import numpy as np\nimport cv2\nfrom shapely.geometry import box\nfrom scipy.optimize import broyden1 as solver\n\nfrom concern.config import State\nfrom .data_process import DataProcess\nimport concern.webcv2 as webcv2\nfrom concern.visualizer import Visualize\n\n\nclass MakeDensityMap(DataProcess):\n    '''\n    Make density map from TextLines.\n    '''\n    size = State(default=None)\n\n    def process(self, data):\n        text_lines = data['lines']\n        size = self.size\n        if size is None:\n            size = data['image'].shape[:2]\n\n        density_map = np.zeros((*size, 2), dtype=np.float32)\n        density_map[:, :, 0] = 1 / size[0]\n        density_map[:, :, 1] = 1 / size[1]\n\n        for text, rect in zip(text_lines.texts, text_lines.rects):\n            '''\n            rect: [xmin, ymin, xmax, ymax] * n\n            '''\n            cv2.rectangle(data['image'], (int(rect[0]), int(rect[1])),\n                          (int(rect[2]), int(rect[3])), (0, 0, 255))\n            self.render_constant(density_map[:, :, 0], *rect.tolist(),\n                                 len(text) / (rect[2] - rect[0]))\n            self.render_constant(density_map[:, :, 1], *rect.tolist(),\n                                 1 / (rect[3] - rect[1]))\n        data['density_map'] = density_map\n        return data\n\n    def render_constant(self, canvas, xmin, ymin, xmax, ymax, value=1):\n        canvas[int(ymin+0.5):int(ymax+0.5)+1, int(xmin+0.5):int(xmax+0.5)+1] += value\n        return canvas\n\n\nclass OptimizeArrangement(DataProcess):\n\n    def process(self, data):\n        text_lines = data['lines']\n\n        rects = text_lines.rects\n        centers = np.stack(((rects[:, 0] + rects[:, 2]) / 2,\n                           (rects[:, 1] + rects[:, 3]) / 2), axis=1)  # Nx2\n        min_height = 1\n        ratios = []\n        for rect in rects:\n            width = (rect[2] - rect[0])\n            height = (rect[3] - rect[1])\n            ratio = width / height\n            ratios.append(ratio)\n            if ratio < 1:\n                min_height = 1 / ratio\n\n        ratios = np.array(ratios)\n        heights = np.empty([rects.shape[0]])\n        heights.fill(min_height)\n        widths = ratios * heights\n        shapes = np.stack((widths, heights), axis=1) / 2\n        min_rects = np.zeros_like(rects)\n        min_rects[:, :2] = centers - shapes\n        min_rects[:, 2:] = centers + shapes\n\n        for rect in rects:\n            data['image'] = Visualize.visualize_rect(data['image'], rect, color=(0, 255, 0))\n\n        for rect in min_rects:\n            data['image'] = Visualize.visualize_rect(data['image'], rect)\n\n\n        best_ratio = 100\n        area = data['image'].shape[0] * data['image'].shape[1]\n\n        def F(r):\n            new_rects = self.expand(min_rects, r)\n            return self.loss(new_rects, area)\n\n\n        best_ratio = solver(F, best_ratio, f_tol=1e-2, maxiter=1000)\n        print(best_ratio)\n        best_rects = self.expand(min_rects, best_ratio)\n        for rect in best_rects:\n            data['image'] = Visualize.visualize_rect(data['image'], rect, color=(255, 0, 0))\n\n        data['min_rects'] = min_rects\n        data['rects'] = rects\n        return data\n\n    def iou(self, rect1, rect2):\n        rect1 = box(*rect1)\n        rect2 = box(*rect2)\n\n        intersection = rect1.intersection(rect2).area\n        union = rect1.union(rect2).area\n\n        return intersection / union\n\n    def union_area(self, rects):\n        union_rect = None\n        for rect in rects:\n            if union_rect is None:\n                union_rect = box(*rect)\n            else:\n                union_rect = union_rect.union(box(*rect))\n        if union_rect is None:\n            return 0\n        return union_rect.area\n\n    def loss(self, rects, area):\n        iou = 0\n        for i in range(len(rects)):\n            for j in range(len(rects)):\n                if i == j:\n                    continue\n                iou += self.iou(rects[i], rects[j])\n\n        return iou * 10 + 1 - self.union_area(rects) / area\n\n    def expand(self, rects, ratio):\n        widths = (rects[:, 2] - rects[:, 0]) * ratio\n        heights = (rects[:, 3] - rects[:, 1]) * ratio\n        centers = np.stack(((rects[:, 0] + rects[:, 2]) / 2,\n                           (rects[:, 1] + rects[:, 3]) / 2), axis=1)  # Nx2\n        shapes = np.stack((widths, heights), axis=1) / 2\n        new_rects = np.zeros_like(rects)\n        new_rects[:, :2] = centers - shapes\n        new_rects[:, 2:] = centers + shapes\n        return new_rects\n"""
data/processes/make_icdar_data.py,2,"b""from collections import OrderedDict\n\nimport cv2\nimport numpy as np\nimport torch\n\nfrom concern.config import Configurable, State\nfrom .data_process import DataProcess\n\n\nclass MakeICDARData(DataProcess):\n    shrink_ratio = State(default=0.4)\n\n    def __init__(self, debug=False, cmd={}, **kwargs):\n        self.load_all(**kwargs)\n\n        self.debug = debug\n        if 'debug' in cmd:\n            self.debug = cmd['debug']\n\n    def process(self, data):\n        polygons = []\n        ignore_tags = []\n        annotations = data['polys']\n        for annotation in annotations:\n            polygons.append(annotation['points'])\n            ignore_tags.append(annotation['ignore'])\n        ignore_tags = np.array(ignore_tags, dtype=np.uint8)\n        polygons = np.array(polygons)\n        filename = data.get('filename', data['data_id'])\n        if self.debug:\n            self.draw_polygons(data['image'], polygons, ignore_tags)\n        shape = np.array(data['shape'])\n        return OrderedDict(image=data['image'],\n                           polygons=polygons,\n                           ignore_tags=ignore_tags,\n                           shape=shape,\n                           filename=filename)\n\n    def draw_polygons(self, image, polygons, ignore_tags):\n        import cv2\n        for i in range(polygons.shape[0]):\n            polygon = polygons[i].reshape(4, 2).astype(np.int32)\n            ignore = ignore_tags[i]\n            if ignore:\n                color = (255, 0, 0)  # depict ignorable polygons in blue\n            else:\n                color = (0, 0, 255)  # depict polygons in red\n\n            cv2.polylines(image, [polygon], True, color, 1)\n\n    polylines = staticmethod(draw_polygons)\n\n\nclass ICDARCollectFN(Configurable):\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def __call__(self, batch):\n        data_dict = OrderedDict()\n        for sample in batch:\n            for k, v in sample.items():\n                if k not in data_dict:\n                    data_dict[k] = []\n                if isinstance(v, np.ndarray):\n                    v = torch.from_numpy(v)\n                data_dict[k].append(v)\n        data_dict['image'] = torch.stack(data_dict['image'], 0)\n        return data_dict\n"""
data/processes/make_keypoint_map.py,0,"b""import numpy as np\nimport cv2\n\nfrom concern.config import State\n\nfrom .data_process import DataProcess\n\n\nclass MakeKeyPointMap(DataProcess):\n    max_size = State(default=32)\n    box_key = State(default='charboxes')\n    shape = State(default=[16, 64])\n\n    def process(self, data):\n        assert self.box_key in data, '%s in data is required' % self.box_key\n\n        ori_h, ori_w = data['image'].shape[:2]\n        charboxes = data[self.box_key]\n        height, width = self.shape\n        ratio_h, ratio_w = float(height) / ori_h, float(width) / ori_w\n        boxes = np.zeros_like(charboxes)\n        boxes[..., 0] = (charboxes[..., 0] * ratio_w + 0.5).astype(np.int32)\n        boxes[..., 1] = (charboxes[..., 1] * ratio_h + 0.5).astype(np.int32)\n        charmaps = self.gen_keypoint_map((boxes).astype(np.float32), self.shape[0], self.shape[1])\n        data['charmaps'] = charmaps\n        return data\n\n    def get_gaussian(self, h, w, m):\n        xs, ys = np.meshgrid(np.arange(w), np.arange(h))\n        g_map = np.exp(-m * (np.power((xs * 2 / w - 1), 2) + np.power((ys * 2 / h - 1), 2)))\n        return g_map\n\n    def gen_keypoint_map(self, boxes, h, w):\n        maps = np.zeros((self.max_size, h, w), dtype=np.float32)\n        for ind, box in enumerate(boxes):\n            _, _, box_w, box_h = cv2.boundingRect(box)\n            src = np.array([[0, box_h], [box_w, box_h], [box_w, 0], [0, 0]]).astype(np.float32)\n            g = self.get_gaussian(box_h, box_w, m=2)\n            M = cv2.getPerspectiveTransform(src, box.astype(np.float32))\n            maps[ind] = cv2.warpPerspective(g, M, (w, h))\n        return maps\n"""
data/processes/make_recognition_label.py,0,"b""import numpy as np\n\nfrom concern.config import State\nfrom concern.charsets import DefaultCharset\n\nfrom .data_process import DataProcess\n\n\nclass MakeRecognitionLabel(DataProcess):\n    charset = State(default=DefaultCharset())\n    max_size = State(default=32)\n\n    def process(self, data):\n        assert 'gt' in data, '`gt` in data is required by this process'\n        gt = data['gt']\n        label = self.gt_to_label(gt)\n        data['label'] = label\n        if label.sum() == 0:\n            raise 'Empty Label'  # FIXME: package into a class.\n\n        length = len(gt)\n        if self.max_size is not None:\n            length = min(length, self.max_size)\n        length = np.array(length, dtype=np.int32)\n        data['length'] = length\n        return data\n\n    def gt_to_label(self, gt, image=None):\n        if self.max_size is not None:\n            return self.charset.string_to_label(gt)[:self.max_size]\n        else:\n            return self.charset.string_to_label(gt)\n"""
data/processes/make_seg_detection_data.py,0,"b""import numpy as np\nimport cv2\nfrom shapely.geometry import Polygon\nimport pyclipper\n\nfrom concern.config import State\nfrom .data_process import DataProcess\n\n\nclass MakeSegDetectionData(DataProcess):\n    r'''\n    Making binary mask from detection data with ICDAR format.\n    Typically following the process of class `MakeICDARData`.\n    '''\n    min_text_size = State(default=8)\n    shrink_ratio = State(default=0.4)\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n    def process(self, data):\n        '''\n        requied keys:\n            image, polygons, ignore_tags, filename\n        adding keys:\n            mask\n        '''\n        image = data['image']\n        polygons = data['polygons']\n        ignore_tags = data['ignore_tags']\n        image = data['image']\n        filename = data['filename']\n\n        h, w = image.shape[:2]\n        polygons, ignore_tags = self.validate_polygons(\n            polygons, ignore_tags, h, w)\n        gt = np.zeros((1, h, w), dtype=np.float32)\n        mask = np.ones((h, w), dtype=np.float32)\n        for i in range(polygons.shape[0]):\n            polygon = polygons[i]\n            height = min(np.linalg.norm(polygon[0] - polygon[3]),\n                         np.linalg.norm(polygon[1] - polygon[2]))\n            width = min(np.linalg.norm(polygon[0] - polygon[1]),\n                        np.linalg.norm(polygon[2] - polygon[3]))\n            if ignore_tags[i] or min(height, width) < self.min_text_size:\n                cv2.fillPoly(mask, polygon.astype(\n                    np.int32)[np.newaxis, :, :], 0)\n                ignore_tags[i] = True\n            else:\n                polygon_shape = Polygon(polygon)\n                distance = polygon_shape.area * \\\n                    (1 - np.power(self.shrink_ratio, 2)) / polygon_shape.length\n                subject = [tuple(l) for l in polygons[i]]\n                padding = pyclipper.PyclipperOffset()\n                padding.AddPath(subject, pyclipper.JT_ROUND,\n                                pyclipper.ET_CLOSEDPOLYGON)\n                shrinked = padding.Execute(-distance)\n                if shrinked == []:\n                    cv2.fillPoly(mask, polygon.astype(\n                        np.int32)[np.newaxis, :, :], 0)\n                    ignore_tags[i] = True\n                    continue\n                shrinked = np.array(shrinked[0]).reshape(-1, 2)\n                cv2.fillPoly(gt[0], [shrinked.astype(np.int32)], 1)\n\n        if filename is None:\n            filename = ''\n        data.update(image=image,\n                    polygons=polygons,\n                    gt=gt, mask=mask, filename=filename)\n        return data\n\n    def validate_polygons(self, polygons, ignore_tags, h, w):\n        '''\n        polygons (numpy.array, required): of shape (num_instances, num_points, 2)\n        '''\n        if polygons.shape[0] == 0:\n            return polygons, ignore_tags\n        assert polygons.shape[0] == len(ignore_tags)\n\n        polygons[:, :, 0] = np.clip(polygons[:, :, 0], 0, w - 1)\n        polygons[:, :, 1] = np.clip(polygons[:, :, 1], 0, h - 1)\n\n        for i in range(polygons.shape[0]):\n            area = self.polygon_area(polygons[i])\n            if abs(area) < 1:\n                ignore_tags[i] = True\n            if area > 0:\n                polygons[i] = polygons[i][(0, 3, 2, 1), :]\n        return polygons, ignore_tags\n\n    def polygon_area(self, polygon):\n        edge = [\n            (polygon[1][0] - polygon[0][0]) * (polygon[1][1] + polygon[0][1]),\n            (polygon[2][0] - polygon[1][0]) * (polygon[2][1] + polygon[1][1]),\n            (polygon[3][0] - polygon[2][0]) * (polygon[3][1] + polygon[2][1]),\n            (polygon[0][0] - polygon[3][0]) * (polygon[0][1] + polygon[3][1])\n        ]\n        return np.sum(edge) / 2.\n\n"""
data/processes/make_seg_recognition_label.py,1,"b""import numpy as np\nimport cv2\nfrom shapely.geometry import Polygon\nimport pyclipper\nimport torch\nimport torch.nn.functional as F\nfrom concern.config import State\nfrom concern.charsets import DefaultCharset\nimport ipdb\nfrom .data_process import DataProcess\n\n\nclass MakeSegRecognitionLabel(DataProcess):\n    charset = State(default=DefaultCharset())\n    shrink = State(default=True)\n    shrink_ratio = State(default=0.25)\n    exempt_chars = State(default=list(''))\n    shape = State(default=(16, 64))\n\n    max_size = 256\n\n    def process(self, data: dict):\n        assert 'charboxes' in data, 'charboxes in data is required'\n        ori_height, ori_width = data['image'].shape[:2]\n        charboxes = data['charboxes']\n        gt = data['gt']\n        height, width = self.shape\n        ratio_h, ratio_w = float(height) / ori_height, float(width) / ori_width\n        assert len(charboxes) == len(gt)\n        mask = np.zeros((height, width), dtype=np.float32)\n        classify = np.zeros((height, width), dtype=np.int32)\n        order_map = np.zeros((height, width), dtype=np.int32)\n        shrink_ratio = self.shrink_ratio\n        boxes = np.zeros_like(charboxes)\n        boxes[..., 0] = (charboxes[..., 0] * ratio_w + 0.5).astype(np.int32)\n        boxes[..., 1] = (charboxes[..., 1] * ratio_h + 0.5).astype(np.int32)\n        for box_index, box in enumerate(boxes):\n            class_code = self.charset.index(gt[box_index])\n            if self.shrink:\n                if self.charset.is_empty(class_code) or gt[box_index] in self.exempt_chars:\n                    shrink_ratio = 0\n                try:\n                    rect = self.poly_to_rect(box, shrink_ratio)\n                except AssertionError:\n                    # invalid poly\n                    continue\n            else:\n                rect = box\n            if rect is not None:\n                self.render_rect(mask, rect)\n                self.render_rect(classify, rect, class_code)\n                self.render_rect(order_map, rect, box_index + 1)\n        data['mask'] = mask\n        data['classify'] = classify\n        if classify.sum() == 0:\n            raise 'gt is empty!'\n        data['ordermaps'] = order_map\n        return data\n\n    def poly_to_rect(self, poly, shrink_ratio=None):\n        if shrink_ratio is None:\n            shrink_ratio = self.shrink_ratio\n        polygon_shape = Polygon(poly)\n        distance = polygon_shape.area * \\\n                   (1 - np.power(shrink_ratio, 2)) / polygon_shape.length\n        subject = [tuple(l) for l in poly]\n        padding = pyclipper.PyclipperOffset()\n        padding.AddPath(subject, pyclipper.JT_ROUND,\n                        pyclipper.ET_CLOSEDPOLYGON)\n        shrinked = padding.Execute(-distance)\n        if shrinked == []:\n            return None\n        return shrinked\n\n    def render_rect(self, canvas, poly, value=1):\n        poly = np.array(poly, dtype=np.int32).reshape(-1, 2)\n        return cv2.fillPoly(canvas, [poly], value)\n        '''\n        xmin, xmax = poly[:, 0].min(), poly[:, 0].max()\n        ymin, ymax = poly[:, 1].min(), poly[:, 1].max()\n        canvas[ymin:ymax+1, xmin:xmax+1] = value\n        return canvas\n        '''\n"""
data/processes/normalize_image.py,1,"b""import numpy as np\nimport torch\n\nfrom .data_process import DataProcess\n\n\nclass NormalizeImage(DataProcess):\n    RGB_MEAN = np.array([122.67891434, 116.66876762, 104.00698793])\n\n    def process(self, data):\n        assert 'image' in data, '`image` in data is required by this process'\n        image = data['image']\n        image -= self.RGB_MEAN\n        image /= 255.\n        image = torch.from_numpy(image).permute(2, 0, 1).float()\n        data['image'] = image\n        return data\n\n    @classmethod\n    def restore(self, image):\n        image = image.permute(1, 2, 0).to('cpu').numpy()\n        image = image * 255.\n        image += self.RGB_MEAN\n        image = image.astype(np.uint8)\n        return image\n"""
data/processes/random_crop_data.py,0,"b""import numpy as np\nimport cv2\n\nfrom .data_process import DataProcess\nfrom concern.config import Configurable, State\n\n\n# random crop algorithm similar to https://github.com/argman/EAST\nclass RandomCropData(DataProcess):\n    size = State(default=(512, 512))\n    max_tries = State(default=50)\n    min_crop_side_ratio = State(default=0.1)\n    require_original_image = State(default=False)\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n    def process(self, data):\n        img = data['image']\n        ori_img = img\n        ori_lines = data['polys']\n\n        all_care_polys = [line['points']\n                          for line in data['polys'] if not line['ignore']]\n        crop_x, crop_y, crop_w, crop_h = self.crop_area(img, all_care_polys, self.size[0], self.size[1])\n        scale_w = self.size[0] / crop_w\n        scale_h = self.size[1] / crop_h\n        scale = min(scale_w, scale_h)\n        h = int(crop_h * scale)\n        w = int(crop_w * scale)\n        padimg = np.zeros(\n            (self.size[1], self.size[0], img.shape[2]), img.dtype)\n        padimg[:h, :w] = cv2.resize(\n            img[crop_y:crop_y + crop_h, crop_x:crop_x + crop_w], (w, h))\n        img = padimg\n\n        lines = []\n        for line in data['polys']:\n            poly = ((np.array(line['points']) -\n                     (crop_x, crop_y)) * scale).tolist()\n            if not self.is_poly_outside_rect(poly, 0, 0, w, h):\n                lines.append({**line, 'points': poly})\n        data['polys'] = lines\n\n        if self.require_original_image:\n            data['image'] = ori_img\n        else:\n            data['image'] = img\n        data['lines'] = ori_lines\n        data['scale_w'] = scale\n        data['scale_h'] = scale\n\n        return data\n\n    def is_poly_in_rect(self, poly, x, y, w, h):\n        poly = np.array(poly)\n        if poly[:, 0].min() < x or poly[:, 0].max() > x + w:\n            return False\n        if poly[:, 1].min() < y or poly[:, 1].max() > y + h:\n            return False\n        return True\n\n    def is_poly_outside_rect(self, poly, x, y, w, h):\n        poly = np.array(poly)\n        if poly[:, 0].max() < x or poly[:, 0].min() > x + w:\n            return True\n        if poly[:, 1].max() < y or poly[:, 1].min() > y + h:\n            return True\n        return False\n\n    def crop_area(self, img, polys, width=None, height=None):\n        h, w, _ = img.shape\n        h_array = np.zeros(h, dtype=np.int32)\n        w_array = np.zeros(w, dtype=np.int32)\n        for points in polys:\n            points = np.round(points, decimals=0).astype(np.int32)\n            minx = np.min(points[:, 0])\n            maxx = np.max(points[:, 0])\n            w_array[minx:maxx] = 1\n            miny = np.min(points[:, 1])\n            maxy = np.max(points[:, 1])\n            h_array[miny:maxy] = 1\n        # ensure the cropped area not across a text\n        h_axis = np.where(h_array == 0)[0]\n        w_axis = np.where(w_array == 0)[0]\n\n        if len(h_axis) == 0 or len(w_axis) == 0:\n            return 0, 0, w, h\n\n        for i in range(self.max_tries):\n            xx = np.random.choice(w_axis, size=2)\n            xmin = np.min(xx)\n            if width is not None:\n                xmax = xmin + width\n            else:\n                xmax = np.max(xx)\n\n            xmin = np.clip(xmin, 0, w - 1)\n            xmax = np.clip(xmax, 0, w - 1)\n            yy = np.random.choice(h_axis, size=2)\n            ymin = np.min(yy)\n            if height is not None:\n                ymax = ymin + width\n            else:\n                ymax = np.max(yy)\n\n            ymin = np.clip(ymin, 0, h - 1)\n            ymax = np.clip(ymax, 0, h - 1)\n\n            if xmax - xmin < self.min_crop_side_ratio * w or ymax - ymin < self.min_crop_side_ratio * h:\n                # area too small\n                continue\n\n            num_poly_in_rect = 0\n            for poly in polys:\n                if not self.is_poly_outside_rect(poly, xmin, ymin, xmax - xmin, ymax - ymin):\n                    num_poly_in_rect += 1\n                    break\n\n            if num_poly_in_rect > 0:\n                return xmin, ymin, xmax - xmin, ymax - ymin\n        if height is not None:\n            return self.crop_area(img, polys)\n        return 0, 0, w, h\n"""
data/processes/resize_image.py,0,"b'import cv2\nimport numpy as np\n\nfrom concern.config import Configurable, State\nimport concern.webcv2 as webcv2\nfrom .data_process import DataProcess\n\n\nclass _ResizeImage:\n    \'\'\'\n    Resize images.\n    Inputs:\n        image_size: two-tuple-like object (height, width).\n        mode: the mode used to resize image. Valid options:\n            ""keep_size"": keep the original size of image.\n            ""resize"":    arbitrarily resize the image to image_size.\n            ""keep_ratio"": resize to dest height\n                while keepping the height/width ratio of the input.\n            ""pad"": pad the image to image_size after applying\n                ""keep_ratio"" resize.\n    \'\'\'\n    MODES = [\'resize\', \'keep_size\', \'keep_ratio\', \'pad\']\n\n    def __init__(self, image_size, mode):\n        self.image_size = image_size\n        assert mode in self.MODES\n        self.mode = mode\n\n    def resize_or_pad(self, image):\n        if self.mode == \'keep_size\':\n            return image\n        if self.mode == \'pad\':\n            return self.pad_iamge(image)\n\n        assert self.mode in [\'resize\', \'keep_ratio\']\n        height, width = self.get_image_size(image)\n        image = cv2.resize(image, (width, height))\n        return image\n\n    def get_image_size(self, image):\n        height, width = self.image_size\n        if self.mode == \'keep_ratio\':\n            width = max(width, int(\n                height / image.shape[0] * image.shape[1] / 32 + 0.5) * 32)\n        if self.mode == \'pad\':\n            width = min(width,\n                        max(int(height / image.shape[0] * image.shape[1] / 32 + 0.5) * 32, 32))\n        return height, width\n\n    def pad_iamge(self, image):\n        canvas = np.zeros((*self.image_size, 3), np.float32)\n        height, width = self.get_image_size(image)\n        image = cv2.resize(image, (width, height))\n        canvas[:, :width, :] = image\n        return canvas\n\n\nclass ResizeImage(_ResizeImage, DataProcess):\n    mode = State(default=\'keep_ratio\')\n    image_size = State(default=[1152, 2048])  # height, width\n    key = State(default=\'image\')\n\n    def __init__(self, cmd={}, mode=None, **kwargs):\n        self.load_all(**kwargs)\n        if mode is not None:\n            self.mode = mode\n        if \'resize_mode\' in cmd:\n            self.mode = cmd[\'resize_mode\']\n        assert self.mode in self.MODES\n\n    def process(self, data):\n        data[self.key] = self.resize_or_pad(data[self.key])\n        return data\n\n\nclass ResizeData(_ResizeImage, DataProcess):\n    key = State(default=\'image\')\n    box_key = State(default=\'polygons\')\n    image_size = State(default=[64, 256])  # height, width\n\n    def __init__(self, cmd={}, mode=None, key=None, box_key=None, **kwargs):\n        self.load_all(**kwargs)\n        if mode is not None:\n            self.mode = mode\n        if key is not None:\n            self.key = key\n        if box_key is not None:\n            self.box_key = box_key\n        if \'resize_mode\' in cmd:\n            self.mode = cmd[\'resize_mode\']\n        assert self.mode in self.MODES\n\n    def process(self, data):\n        height, width = data[\'image\'].shape[:2]\n        new_height, new_width = self.get_image_size(data[\'image\'])\n        data[self.key] = self.resize_or_pad(data[self.key])\n\n        charboxes = data[self.box_key]\n        data[self.box_key] = charboxes.copy()\n        data[self.box_key][:, :, 0] = data[self.box_key][:, :, 0] * \\\n            new_width / width\n        data[self.box_key][:, :, 1] = data[self.box_key][:, :, 1] * \\\n            new_height / height\n        return data\n'"
data/processes/serialize_box.py,0,"b""import numpy as np\n\nfrom data.quad import Quad\nfrom concern.config import State\nfrom .data_process import DataProcess\n\n\nclass SerializeBox(DataProcess):\n    box_key = State(default='charboxes')\n    format = State(default='NP2')\n\n    def process(self, data):\n        data[self.box_key] = data['lines'].quads\n        return data\n\n\nclass UnifyRect(SerializeBox):\n    max_size = State(default=64)\n\n    def process(self, data):\n        h, w = data['image'].shape[:2]\n        boxes = np.zeros((self.max_size, 4), dtype=np.float32)\n        mask_has_box = np.zeros(self.max_size, dtype=np.float32)\n        data = super().process(data)\n        quad = data[self.box_key]\n        assert quad.shape[0] <= self.max_size\n        boxes[:quad.shape[0]] = quad.rectify() / np.array([w, h, w, h]).reshape(1, 4)\n        mask_has_box[:quad.shape[0]] = 1.\n        data['boxes'] = boxes\n        data['mask_has_box'] = mask_has_box\n        return data\n"""
ops/ctc_2d/ctc_loss_2d.py,3,"b'import torch\nfrom torch.autograd import Function\n\nfrom . import ctc_2d_csrc\n\n\nclass CTCLoss2DFunction(Function):\n\n    @staticmethod\n    def forward(ctx, log_probs, targets, input_lengths, target_lengths, blank=0):\n        ctx.blank = blank\n        if not log_probs.is_cuda:\n            raise NotImplementedError\n\n        neg_log_likelihood, log_alpha = ctc_2d_csrc.ctc2d_forward(\n            log_probs, targets, input_lengths, target_lengths, blank, torch.finfo().tiny)\n\n        if log_probs.requires_grad:\n            ctx.save_for_backward(log_probs, targets, input_lengths,\n                                  target_lengths, neg_log_likelihood, log_alpha)\n        return neg_log_likelihood\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        log_probs, targets, input_lengths, target_lengths, neg_log_likelihood, log_alpha = ctx.saved_tensors\n\n        grad_log_probs = torch.ones(2, 3)\n        if ctx.needs_input_grad[0]:\n            grad_log_probs = ctc_2d_csrc.ctc2d_backward(\n                grad_output, log_probs, targets, input_lengths, target_lengths,\n                neg_log_likelihood, log_alpha,\n                ctx.blank\n            )\n        return grad_log_probs, None, None, None, None\n\n\nctc_loss_2d = CTCLoss2DFunction.apply\n'"
ops/ctc_2d/setup.py,5,"b'#!/usr/bin/env python\n\nimport glob\nimport os\n\nimport torch\nfrom setuptools import find_packages\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import CUDA_HOME\nfrom torch.utils.cpp_extension import CppExtension\nfrom torch.utils.cpp_extension import CUDAExtension\n\nrequirements = [""torch"", ""torchvision""]\n\n\nop_name = \'ctc_2d\'\ndef get_extensions():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    ext_modules = []\n    extensions_dir = os.path.join(this_dir, ""csrc"")\n\n    main_file = glob.glob(os.path.join(extensions_dir, ""*.cpp""))\n    source_cpu = glob.glob(os.path.join(extensions_dir, ""cpu"", ""*.cpp""))\n    source_cuda = glob.glob(os.path.join(extensions_dir, ""cuda"", ""*.cu""))\n\n    sources = main_file + source_cpu\n    extension = CppExtension\n\n    extra_compile_args = {""cxx"": []}\n    define_macros = []\n\n    if (torch.cuda.is_available() and CUDA_HOME is not None) or os.getenv(""FORCE_CUDA"", ""0"") == ""1"":\n        extension = CUDAExtension\n        sources += source_cuda\n        define_macros += [(""WITH_CUDA"", None)]\n        extra_compile_args[""nvcc""] = [\n            ""-DCUDA_HAS_FP16=1"",\n            ""-D__CUDA_NO_HALF_OPERATORS__"",\n            ""-D__CUDA_NO_HALF_CONVERSIONS__"",\n            ""-D__CUDA_NO_HALF2_OPERATORS__"",\n        ]\n\n    sources = [os.path.join(extensions_dir, s) for s in sources]\n    include_dirs = [extensions_dir]\n    ext_modules.append(\n        extension(\n            op_name + ""_csrc"",\n            sources,\n            include_dirs=include_dirs,\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args,\n        )\n    )\n    return ext_modules\n\n\nsetup(\n    name=op_name,\n    version=""0.0"",\n    packages=find_packages(),\n    ext_modules=get_extensions(),\n    cmdclass={""build_ext"": torch.utils.cpp_extension.BuildExtension},\n)\n'"
structure/measurers/__init__.py,0,b'from .textsnake import TextsnakeMeasurer\nfrom .classification_measurer import ClassificationMeasurer\nfrom .sequence_recognition_measurer import SequenceRecognitionMeasurer\nfrom .icdar_detection_measurer import ICDARDetectionMeasurer\nfrom .quad_measurer import QuadMeasurer\nfrom .grid_sampling_measurer import GridSamplingMeasurer\n'
structure/measurers/classification_measurer.py,1,"b""import torch\n\nfrom concern import AverageMeter\nfrom concern.config import Configurable\n\n\nclass ClassificationMeasurer(Configurable):\n    def __init__(self, **kwargs):\n        pass\n\n    def measure(self, batch, output):\n        correct = torch.eq(output.cpu(), batch[1]).numpy()\n        return correct\n\n    def validate_measure(self, batch, output):\n        return self.measure(batch, output), [0]\n\n    def gather_measure(self, raw_metrics, logger=None):\n        accuracy_meter = AverageMeter()\n        for raw_metric in raw_metrics:\n            total = raw_metric.shape[0]\n            accuracy = raw_metric.sum() / total\n            accuracy_meter.update(accuracy, total)\n\n        return {\n            'accuracy': accuracy_meter,\n        }\n"""
structure/measurers/grid_sampling_measurer.py,0,"b""import torch\n\nfrom concern import AverageMeter\nfrom concern.config import Configurable\n\n\nclass GridSamplingMeasurer(Configurable):\n    def __init__(self, **kwargs):\n        pass\n\n    def measure(self, batch, output):\n        return 0\n\n    def validate_measure(self, batch, output):\n        return 1, [0]\n\n    def gather_measure(self, raw_metrics, logger=None):\n        return {\n            'accuracy': 0,\n        }\n"""
structure/measurers/icdar_detection_measurer.py,0,"b""import os\nimport subprocess\nimport shutil\n\nimport numpy as np\nimport json\n\nfrom concern import Logger, AverageMeter\nfrom concern.config import Configurable\n\n\nclass ICDARDetectionMeasurer(Configurable):\n    def __init__(self, **kwargs):\n        self.visualized = False\n\n    def measure(self, batch, output):\n        pairs = []\n        for i in range(len(batch[-1])):\n            pairs.append((batch[-1][i], output[i][0]))\n        return pairs\n\n    def validate_measure(self, batch, output):\n        return self.measure(batch, output), [int(self.visualized)]\n\n    def evaluate_measure(self, batch, output):\n        return self.measure(batch, output), np.linspace(0, batch[0].shape[0]).tolist()\n\n    def gather_measure(self, name, raw_metrics, logger: Logger):\n        save_dir = os.path.join(logger.log_dir, name)\n        shutil.rmtree(save_dir, ignore_errors=True)\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        log_file_path = os.path.join(save_dir, name + '.log')\n        count = 0\n        for batch_pairs in raw_metrics:\n            for _filename, boxes in batch_pairs:\n                boxes = np.array(boxes).reshape(-1, 8).astype(np.int32)\n                filename = 'res_' + _filename.replace('.jpg', '.txt')\n                with open(os.path.join(save_dir, filename), 'wt') as f:\n                    if len(boxes) == 0:\n                        f.write('')\n                    for box in boxes:\n                        f.write(','.join(map(str, box)) + '\\n')\n                count += 1\n\n        self.packing(save_dir)\n        try:\n            raw_out = subprocess.check_output(['python assets/ic15_eval/script.py -m=' + name\n                                               + ' -g=assets/ic15_eval/gt.zip -s=' +\n                                               os.path.join(save_dir, 'submit.zip') +\n                                               '|tee -a ' + log_file_path],\n                                              timeout=30, shell=True)\n        except subprocess.TimeoutExpired:\n            return {}\n        raw_out = raw_out.decode().replace('Calculated!', '')\n        dict_out = json.loads(raw_out)\n        return {k: AverageMeter().update(v, n=count) for k, v in dict_out.items()}\n\n    def packing(self, save_dir):\n        pack_name = 'submit.zip'\n        os.system(\n            'zip -r -j -q ' +\n            os.path.join(save_dir, pack_name) + ' ' + save_dir + '/*.txt')\n"""
structure/measurers/quad_measurer.py,0,"b""import numpy as np\n\nfrom concern import Logger, AverageMeter\nfrom concern.config import Configurable\nfrom concern.icdar2015_eval.detection.iou import DetectionIoUEvaluator\n\n\nclass QuadMeasurer(Configurable):\n    def __init__(self, **kwargs):\n        self.evaluator = DetectionIoUEvaluator()\n\n    def measure(self, batch, output):\n        '''\n        batch: (image, polygons, ignore_tags\n        batch: a dict produced by dataloaders.\n            image: tensor of shape (N, C, H, W).\n            polygons: tensor of shape (N, K, 4, 2), the polygons of objective regions.\n            ignore_tags: tensor of shape (N, K), indicates whether a region is ignorable or not.\n            shape: the original shape of images.\n            filename: the original filenames of images.\n        output: (polygons, ...)\n        '''\n        results = []\n        gt_polyons_batch = batch['polygons']\n        ignore_tags_batch = batch['ignore_tags']\n        pred_polygons_batch = np.array(output[0])\n        for polygons, pred_polygons, ignore_tags in\\\n                zip(gt_polyons_batch, pred_polygons_batch, ignore_tags_batch):\n            gt = [dict(points=polygons[i], ignore=ignore_tags[i])\n                  for i in range(len(polygons))]\n            pred = [dict(points=pred_polygons[i])\n                    for i in range(len(pred_polygons))]\n            results.append(self.evaluator.evaluate_image(gt, pred))\n        return results\n\n    def validate_measure(self, batch, output):\n        return self.measure(batch, output), [0]\n\n    def evaluate_measure(self, batch, output):\n        return self.measure(batch, output),\\\n            np.linspace(0, batch['image'].shape[0]).tolist()\n\n    def gather_measure(self, raw_metrics, logger: Logger):\n        raw_metrics = [image_metrics\n                       for batch_metrics in raw_metrics\n                       for image_metrics in batch_metrics]\n\n        result = self.evaluator.combine_results(raw_metrics)\n\n        precision = AverageMeter()\n        recall = AverageMeter()\n        fmeasure = AverageMeter()\n\n        precision.update(result['precision'], n=len(raw_metrics))\n        recall.update(result['recall'], n=len(raw_metrics))\n        fmeasure_score = 2 * precision.val * recall.val /\\\n            (precision.val + recall.val + 1e-8)\n        fmeasure.update(fmeasure_score)\n\n        return {\n            'precision': precision,\n            'recall': recall,\n            'fmeasure': fmeasure\n        }\n"""
structure/measurers/sequence_recognition_measurer.py,0,"b""import numpy as np\n\nimport config\nfrom concern import AverageMeter, Logger\nfrom concern.config import Configurable, State\nimport concern.webcv2 as webcv2\nimport editdistance as ed\nimport warnings\n\n\nclass SequenceRecognitionMeasurer(Configurable):\n    nori_lexicon_path = State(default=None)\n\n    def __init__(self, *args, **kwargs):\n        self.load_all(*args, **kwargs)\n        if self.nori_lexicon_path:\n            with open(self.nori_lexicon_path) as f:\n                self.nori_lexicon = set(f.read().split())\n        else:\n            self.nori_lexicon = None\n\n    def validate_measure(self, batch, output):\n        interested = []\n        return self.measure(batch, output), interested\n\n    evaluate_measure = validate_measure\n\n    def measure(self, batch, output):\n        edit_distance = self.edit_distance(batch, output)\n        accuracy = self.accuracy(batch, output)\n        if self.nori_lexicon:\n            in_lexicon = self.in_lexicon(batch, output)\n            return dict(accuracy=accuracy, edit_distance=edit_distance, in_lexicon=in_lexicon)\n        else:\n            return dict(accuracy=accuracy, edit_distance=edit_distance)\n\n    def gather_measure(self, raw_metrics, logger: Logger = None):\n        if not self.nori_lexicon:\n            edit_distance = self.gather_edit_distance([(m['edit_distance']) for m in raw_metrics])\n            accuracy = self.gather_accuracy([(m['accuracy']) for m in raw_metrics])\n            return dict(accuracy=accuracy, edit_distance=edit_distance)\n        else:\n            total_edit_distance, in_lexicon_edit_distance, out_lexicon_edit_distance = \\\n                self.gather_edit_distance([(m['edit_distance'], m['in_lexicon']) for m in raw_metrics])\n            total_accuracy, in_lexicon_accuracy, out_lexicon_accuracy = \\\n                self.gather_edit_distance([(m['accuracy'], m['in_lexicon']) for m in raw_metrics])\n            return dict(total_edit_distance=total_edit_distance, in_lexicon_edit_distance=in_lexicon_edit_distance,\n                        out_lexicon_edit_distance=out_lexicon_edit_distance, total_accuracy=total_accuracy,\n                        in_lexicon_accuracy=in_lexicon_accuracy, out_lexicon_accuracy=out_lexicon_accuracy)\n\n    def gather_accuracy(self, raw_metrics):\n        accuracy_meter = AverageMeter()\n        for raw_metric in raw_metrics:\n            total = len(raw_metric)\n            accuracy = np.array(raw_metric).sum() / total\n            accuracy_meter.update(accuracy, total)\n        return accuracy_meter\n\n    def in_lexicon(self, batch, output):\n        result = []\n        for i in range(len(output)):\n            label_string = output[i]['label_string'].upper()\n            result.append(label_string in self.nori_lexicon)\n        return result\n\n    def accuracy(self, batch, output):\n        eq = []\n        for i in range(len(output)):\n            label_string = output[i]['label_string'].upper()\n            pred_string = output[i]['pred_string'].upper()\n            eq.append(label_string == pred_string)\n        return eq\n\n    def gather_edit_distance(self, raw_metrics_ed, logger: Logger = None):\n        meter = AverageMeter()\n        if not self.nori_lexicon:\n            for raw_metric in raw_metrics_ed:\n                total = len(raw_metric)\n                edit_distance = np.array(raw_metric).sum() / total\n                meter.update(edit_distance, total)\n            return meter\n        else:\n            in_lexicon_meter = AverageMeter()\n            out_lexicon_meter = AverageMeter()\n            for raw_metric, in_lexicon in raw_metrics_ed:\n                raw_metric = np.array(raw_metric)\n                in_lexicon = np.array(in_lexicon)\n\n                total = len(raw_metric)\n                edit_distance = raw_metric.sum() / total\n                meter.update(edit_distance, total)\n\n                all_in_lexicon = raw_metric[in_lexicon == True]\n                in_lexicon_edit_distance = all_in_lexicon.sum() / max(len(all_in_lexicon), 1)\n                in_lexicon_meter.update(in_lexicon_edit_distance, len(all_in_lexicon))\n\n                all_out_lexicon = raw_metric[in_lexicon == False]\n                out_lexicon_edit_distance = all_out_lexicon.sum() / max(len(all_out_lexicon), 1)\n                out_lexicon_meter.update(out_lexicon_edit_distance, len(all_out_lexicon))\n            return meter, in_lexicon_meter, out_lexicon_meter\n\n    def edit_distance(self, batch, output):\n        ed_vec = []\n        for i in range(len(output)):\n            label_string = output[i]['label_string'].upper()\n            pred_string = output[i]['pred_string'].upper()\n            length = len(label_string)\n            if length == 0:\n                ed_vec.append(0.0)\n            else:\n                ed_vec.append(float(1 - min(length, ed.eval(label_string, pred_string)) * 1.0 / length))\n        return ed_vec\n"""
structure/measurers/simple_detection.py,0,"b""import pickle\n\nimport editdistance\nimport numpy as np\n\nfrom concern import AverageMeter\nfrom concern.config import Configurable\nfrom concern.icdar2015_eval.detection.iou import DetectionIoUEvaluator\n\n\nclass SimpleDetectionMeasurer(Configurable):\n    def __init__(self, **kwargs):\n        self.evaluator = DetectionIoUEvaluator()\n\n    def validate_measure(self, batch, output):\n        return self.measure(batch, output), [0]\n\n    def evaluate_measure(self, batch, output):\n        return self.measure(batch, output), np.linspace(0, batch[0].shape[0]).tolist()\n\n    def measure(self, batch, output):\n        batch_meta = output['meta']\n        batch_pred_polys = output['polygons_pred']\n\n        results = []\n        for pred_polys, meta in zip(batch_pred_polys, batch_meta):\n            gt = meta['lines']\n            pred = [{'points': points} for points in pred_polys]\n            result = self.evaluator.evaluate_image(gt, pred)\n            results.append(result)\n\n        return results\n\n    def gather_measure(self, raw_metrics, logger=None):\n        raw_metrics = [image_metrics for batch_metrics in raw_metrics for image_metrics in batch_metrics]\n\n        result = self.evaluator.combine_results(raw_metrics)\n\n        precision = AverageMeter()\n        recall = AverageMeter()\n        hmean = AverageMeter()\n\n        precision.update(result['precision'], n=len(raw_metrics))\n        recall.update(result['recall'], n=len(raw_metrics))\n        hmean.update(2 * result['precision'] * result['recall'] / (result['precision'] + result['recall']), n=len(raw_metrics))\n\n        return {\n            'precision': precision,\n            'recall': recall,\n            'hmean': hmean,\n        }\n\n\nclass SimpleDetectionE2EMeasurer(Configurable):\n    def __init__(self, **kwargs):\n        self.evaluator = DetectionIoUEvaluator()\n\n    def validate_measure(self, batch, output):\n        return self.measure(batch, output), [0]\n\n    def evaluate_measure(self, batch, output):\n        return self.measure(batch, output), np.linspace(0, batch[0].shape[0]).tolist()\n\n    def measure(self, batch, output):\n        images_meta = batch[2]\n\n        results = []\n        for pred, meta in zip(output, images_meta):\n            gt = pickle.loads(meta)['lines']\n            pred = [{'points': line['poly'], **line} for line in pred['lines']]\n            result = self.evaluator.evaluate_image(gt, pred)\n            gt_to_pred = {pair['gt']: pair['det'] for pair in result['pairs']}\n            distances = []\n            for gt_id in range(len(gt)):\n                if not gt[gt_id]['ignore']:\n                    gt_text = gt[gt_id]['text']\n                    if gt_id in gt_to_pred:\n                        pred_text = pred[gt_to_pred[gt_id]]['text']\n                    else:\n                        pred_text = ''\n                    distance = editdistance.eval(gt_text, pred_text)\n                    distances.append(distance / len(gt_text))\n            result['edit_distance'] = distances\n            results.append(result)\n\n        return results\n\n    def gather_measure(self, raw_metrics, logger=None):\n        raw_metrics = [image_metrics for batch_metrics in raw_metrics for image_metrics in batch_metrics]\n\n        result = self.evaluator.combine_results(raw_metrics)\n        precision = result['precision']\n        recall = result['recall']\n        hmean = 2 * precision * recall / (precision + recall)\n        edit_distance = [dis for m in raw_metrics for dis in m['edit_distance']]\n        edit_distance = sum(edit_distance) / len(edit_distance)\n\n        mprecision = AverageMeter()\n        mrecall = AverageMeter()\n        mhmean = AverageMeter()\n        medit_distance = AverageMeter()\n\n        mprecision.update(precision, n=len(raw_metrics))\n        mrecall.update(recall, n=len(raw_metrics))\n        mhmean.update(hmean, n=len(raw_metrics))\n        medit_distance.update(edit_distance, n=len(raw_metrics))\n\n        return {\n            'precision': mprecision,\n            'recall': mrecall,\n            'hmean': mhmean,\n            'edit_distance': medit_distance,\n        }\n"""
structure/measurers/textsnake.py,0,"b""import numpy as np\n\nfrom concern import AverageMeter\nfrom concern.config import Configurable\nfrom concern.icdar2015_eval.detection.iou import DetectionIoUEvaluator\n\n\nclass TextsnakeMeasurer(Configurable):\n    def __init__(self, **kwargs):\n        self.evaluator = DetectionIoUEvaluator()\n\n    def validate_measure(self, batch, output):\n        return self.measure(batch, output), [0]\n\n    def evaluate_measure(self, batch, output):\n        return self.measure(batch, output), np.linspace(0, batch[0].shape[0]).tolist()\n\n    def measure(self, batch, output):\n        batch_meta = output['meta']\n        batch_gt_polys = output['polygons_gt']\n        batch_pred_polys = output['contours_pred']\n\n        results = []\n        for gt_polys, pred_polys, meta in zip(batch_gt_polys, batch_pred_polys, batch_meta):\n            gt = [{'points': points, 'ignore': not cares} for points, cares in zip(gt_polys, meta['cares'])]\n            pred = [{'points': points} for points in pred_polys]\n            result = self.evaluator.evaluate_image(gt, pred)\n            results.append(result)\n\n        return results\n\n    def gather_measure(self, raw_metrics, logger=None):\n        raw_metrics = [image_metrics for batch_metrics in raw_metrics for image_metrics in batch_metrics]\n\n        result = self.evaluator.combine_results(raw_metrics)\n\n        precision = AverageMeter()\n        recall = AverageMeter()\n\n        precision.update(result['precision'], n=len(raw_metrics))\n        recall.update(result['recall'], n=len(raw_metrics))\n\n        return {\n            'precision': precision,\n            'recall': recall,\n        }\n"""
structure/representers/__init__.py,0,b'from .textsnake import TextsnakeRepresenter\nfrom .sequence_recognition_representer import SequenceRecognitionRepresenter\nfrom .classification_representer import ClassificationRepresenter\nfrom .ctc_representer import CTCRepresenter\nfrom .ctc_representer2d import CTCRepresenter2D\nfrom .seg_recognition_representer import SegRecognitionRepresenter\nfrom .integral_regression_representer import IntegralRegressionRepresenter\nfrom .seg_detector_representer import SegDetectorRepresenter'
structure/representers/classification_representer.py,1,"b'import torch\nfrom concern.config import Configurable\n\n\nclass ClassificationRepresenter(Configurable):\n    def __init__(self, **kwargs):\n        pass\n\n    def represent(self, batch, pred):\n        return torch.argmax(pred, dim=1)\n'"
structure/representers/ctc_representer.py,3,"b""import torch\n\nimport config\nfrom .sequence_recognition_representer import SequenceRecognitionRepresenter\nfrom concern.config import State\n\n\nclass CTCRepresenter(SequenceRecognitionRepresenter):\n    def represent(self, batch, pred):\n        '''\n        decode ctc using greedy search\n        pred: (N, C, W)\n        return:\n            output: {\n                'label_string': string of gt label,\n                'pred_string': string of prediction\n            }\n        '''\n        labels = batch['label']\n        pred = torch.argmax(pred, dim=1)\n        pred = pred.select(1, 0)  # N, W\n        output = torch.zeros(\n            pred.shape[0], pred.shape[-1], dtype=torch.int) + self.charset.blank\n        for i in range(pred.shape[0]):\n            valid = 0\n            previous = self.charset.blank\n            for j in range(pred.shape[1]):\n                c = pred[i][j]\n                if c == previous or c == self.charset.unknown:\n                    continue\n                if not c == self.charset.blank:\n                    output[i][valid] = c\n                    valid += 1\n                previous = c\n\n        result = []\n        for i in range(labels.shape[0]):\n            label_str = self.label_to_string(labels[i])\n            pred_str = self.label_to_string(output[i])\n            result.append({\n                'label_string': label_str,\n                'pred_string': pred_str\n            })\n\n        return result\n"""
structure/representers/ctc_representer2d.py,2,"b""import torch\n\nfrom concern.config import State\nfrom .sequence_recognition_representer import SequenceRecognitionRepresenter\n\n\nclass CTCRepresenter2D(SequenceRecognitionRepresenter):\n    max_size = State(default=32)\n\n    def represent(self, batch, pred):\n        '''\n        This class is supposed to be used with\n        the measurer `SequenceRecognitionMeasurer`\n        '''\n        classify, mask = pred\n        labels = batch['label']\n\n        '''\n        classify: (N, C, H, W)\n        mask: (N, 1, H, W)\n        return:\n            output: {\n                'label_string': string of gt label,\n                'pred_string': string of prediction\n            }\n        '''\n        heatmap = classify * mask\n        classify = classify.to('cpu')\n        mask = mask.to('cpu')\n        paths = heatmap.max(1, keepdim=True)[0].argmax(\n            2, keepdim=True)  # (N, 1, 1, W)\n        C = classify.size(1)\n        paths = paths.repeat(1, C, 1, 1)  # (N, C, 1, W)\n        selected_probabilities = heatmap.gather(2, paths)  # (N, C, W)\n        pred = selected_probabilities.argmax(1).squeeze(1)  # (N, W)\n        output = torch.zeros(\n            pred.shape[0], pred.shape[-1], dtype=torch.int) + self.charset.blank\n        pred = pred.to('cpu')\n        output = output.to('cpu')\n\n        for i in range(pred.shape[0]):\n            valid = 0\n            previous = self.charset.blank\n            for j in range(pred.shape[1]):\n                c = pred[i][j]\n                if c == previous or c == self.charset.unknown:\n                    continue\n                if not c == self.charset.blank:\n                    output[i][valid] = c\n                    valid += 1\n                previous = c\n\n        result = []\n        for i in range(labels.shape[0]):\n            result.append({\n                'label_string': self.label_to_string(labels[i]),\n                'pred_string': self.label_to_string(output[i]),\n                'mask': mask[i][0],\n                'classify': classify[i]\n            })\n\n        return result\n"""
structure/representers/east.py,0,"b""import pickle\n\nimport cv2\nimport numpy as np\n\nfrom concern.convert import to_np\nfrom concern.config import Configurable, State\n\n\nclass EASTRepresenter(Configurable):\n    heatmap_thr = State(default=0.5)\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n    def get_polygons(self, heatmask, densebox):\n        _, contours, _ = cv2.findContours(heatmask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n\n        polygons = []\n        for contour in contours:\n            points = []\n            for x, y in contour[:, 0]:\n                quad = densebox[:, y, x].reshape(4, 2) + (x, y)\n                points.extend(quad)\n            quad = cv2.boxPoints(cv2.minAreaRect(np.array(points, np.float32)))\n            polygons.append(quad)\n\n        return polygons\n\n    def represent_batch(self, batch):\n        image, label, meta = batch\n        batch_size = image.shape[0]\n\n        output = {\n            'image': to_np(image),\n            'heatmap': to_np(label['heatmap'][:, 0]),\n            'heatmap_weight': to_np(label['heatmap_weight']),\n            'densebox': to_np(label['densebox']),\n            'densebox_weight': to_np(label['densebox_weight']),\n            'meta': [pickle.loads(value) for value in meta],\n        }\n        output['heatmask'] = (output['heatmap'] > self.heatmap_thr).astype('uint8')\n\n        output['polygons'] = [self.get_polygons(\n            output['heatmask'][i],\n            output['densebox'][i],\n        ) for i in range(batch_size)]\n\n        return output\n\n    def represent(self, batch, pred):\n        image, label, meta = batch\n        batch_size = image.shape[0]\n\n        output = self.represent_batch(batch)\n        output = {\n            **output,\n            'heatmap_pred': to_np(pred['heatmap'][:, 0]),\n            'densebox_pred': to_np(pred['densebox']),\n        }\n        output['heatmask_pred'] = (output['heatmap_pred'] > self.heatmap_thr).astype('uint8')\n        output['polygons_pred'] = [self.get_polygons(\n            output['heatmask_pred'][i],\n            output['densebox_pred'][i],\n        ) for i in range(batch_size)]\n\n        return output\n"""
structure/representers/ensemble_ctc_representer.py,2,"b""from collections import OrderedDict\n\nimport torch\n\nimport config\nfrom concern.config import State\nfrom .ctc_representer import CTCRepresenter\nfrom concern.charsets import Charset\nimport concern.webcv2 as webcv2\nfrom data.nori_dataset import NoriDataset\n\n\nclass EnsembleCTCRepresenter(CTCRepresenter):\n    '''decode multiple ensembled ctc models.\n    '''\n    charsets = State(default={})\n    offset = State(default=0.5)\n\n    def get_charset(self, key):\n        return self.charsets.get(key, self.charset)\n\n    def one_hot_to_chars(self, score: torch.Tensor, charset: Charset):\n        '''\n        Args:\n            score: (C, 1, W)\n            charset: The corresponding charset.\n        Return:\n            chars: the chars with maximum scores.\n            scores: corresponding scores.\n        '''\n        pred = torch.argmax(score, dim=0)\n        pred = pred[0]\n        chars = []\n        scores = []\n\n        for w in range(pred.shape[0]):\n            chars.append(charset[pred[w]])\n            scores.append(score[pred[w], 0, w])\n        return chars, scores\n\n    def represent(self, batch, preds: dict, max_size=config.max_size):\n        _, labels, _ = batch\n        batch_size = labels.shape[0]\n\n        output = []\n        string_scores = []\n        for batch_index in range(batch_size):\n            pred_sequences = OrderedDict()\n            pred_scores = OrderedDict()\n            for key, pred in preds.items():\n                chars, scores = self.one_hot_to_chars(\n                    pred[batch_index], self.get_charset(key))\n                pred_sequences[key] = chars\n                pred_scores[key] = scores\n\n            result_string, pred_score = self.merge_encode(pred_sequences, pred_scores)\n            output.append(result_string)\n            string_scores.append(pred_score)\n\n        result = []\n        for i in range(labels.shape[0]):\n            result.append({\n                'label_string': self.label_to_string(labels[i]),\n                'pred_string': ''.join(output[i]),\n                'score': string_scores[i],\n            })\n\n        return result\n\n    def merge_encode(self, sequences, scores):\n        result = []\n        previous = self.charset.blank_char\n        main_sequence = sequences['main']\n\n        score_sum = 0\n        for index, _char in enumerate(main_sequence):\n            char, score = self.choose_char(_char,\n                                    [s[index] for s in sequences.values()],\n                                    [s[index] for s in scores.values()])\n\n            if char == previous or self.charset.is_empty_char(char):\n                previous = char\n                continue\n            else:\n                previous = char\n                result.append(previous)\n                score_sum += score\n        if score_sum > 0:\n            score_sum /= len(result)\n        return result, score_sum\n\n    def choose_char(self, char, substitudes, scores):\n        # if not self.charset.is_empty_char(char):\n        #     return char\n\n        max_score = -1\n        index = None\n        for i, (char, score) in enumerate(zip(substitudes, scores)):\n            if self.charset.is_empty_char(char):\n                score -= self.offset\n            if score > max_score:\n                max_score = score\n                index = i\n        return substitudes[index], max_score\n"""
structure/representers/integral_regression_representer.py,4,"b""import numpy as np\nimport torch\nfrom concern.config import State\nfrom .sequence_recognition_representer import SequenceRecognitionRepresenter\n\n\nclass IntegralRegressionRepresenter(SequenceRecognitionRepresenter):\n    represent_type = State(default='max')\n    order_th = State(default=0.1)\n    loc_th = State(default=0.1)\n    score_th = State(default=0.3)\n    vis = State(default=True)\n\n    def represent(self, batch, pred):\n        labels = batch['label']\n        global_map = (pred['global_map'] > self.loc_th).float() * pred['global_map']\n        heatmap = (pred['ordermaps'][:, 1:] > self.order_th).float() * global_map\n        # heatmap = pred['heatmap']*(pred['heatmap']>0.2).float()\n        N, C, H, W = heatmap.shape\n        score = heatmap / torch.max(heatmap.sum(dim=2, keepdim=True).sum(dim=3, keepdim=True), torch.tensor(1e-8))\n        if self.represent_type == 'integral':\n            index_y, index_x = torch.meshgrid(torch.linspace(0, H - 1, H),\n                                              torch.linspace(0, W - 1, W))\n            index_x = index_x.repeat(N, C, 1, 1)  # N, H, W\n            index_y = index_y.repeat(N, C, 1, 1)  # N, H, W\n            max_x = (score * index_x).sum(2).sum(2).data.cpu().numpy()  # N, C\n            max_x = (max_x + 0.5).astype(np.int32)\n            max_y = (score * index_y).sum(2).sum(2).data.cpu().numpy()  # N, C\n            max_y = (max_y + 0.5).astype(np.int32)\n            result = self.gen_result(pred['classify'], labels, max_x, max_y, heatmap)\n        elif self.represent_type == 'max':\n            max_x = heatmap.max(dim=2)[0].argmax(dim=2).data.cpu().numpy()  # N, C\n            max_y = heatmap.max(dim=3)[0].argmax(dim=2).data.cpu().numpy()  # N, C\n            result = self.gen_result(pred['classify'], labels, max_x, max_y, heatmap)\n        elif self.represent_type == 'weighted_max':\n            # heatmap = (heatmap > self.thresh).float() * heatmap\n            result = self.gen_result_weighted(pred['classify'], heatmap, labels)\n        else:\n            raise NotImplementedError\n        if 'global_map' in pred.keys():\n            for ind, res in enumerate(result):\n                res.update(dict(global_map=pred['global_map'].data.cpu().numpy()))\n        if 'ordermaps' in pred.keys():\n            for ind, res in enumerate(result):\n                res.update(dict(ordermaps=pred['ordermaps'].data.cpu().numpy()))\n        if self.vis:\n            pass\n\n        return result\n\n    def keep_class_max(self, maps):\n        val, inds = maps.max(1)\n        val_map = val.unsqueeze(1).expand_as(maps)\n        out = torch.where(maps == val_map, maps, torch.tensor(0.))\n        return out\n\n    def gen_result(self, pred_classify, labels, max_x, max_y, heatmap):\n        classify = pred_classify.argmax(dim=1).data.cpu().numpy()  # N, H, W\n        result = []\n        for batch_index in range(classify.shape[0]):\n            label_string = self.label_to_string(labels[batch_index])\n            pred_string = self.collect_chars(\n                pred_classify[batch_index], max_x[batch_index], max_y[batch_index], heatmap[batch_index])\n            result.append(dict(label_string=label_string, pred_string=pred_string,\n                               heatmap=heatmap, classify=pred_classify, x=max_x[batch_index], y=max_y[batch_index]))\n        return result\n\n    def gen_result_weighted(self, classify, heatmap, labels):\n        result = []\n        for batch_index in range(classify.shape[0]):\n            label_string = self.label_to_string(labels[batch_index])\n            p_label = []\n            for ind, h_map in enumerate(heatmap[batch_index]):\n                c_map = classify[batch_index] * h_map\n                c_score = c_map.sum(1).sum(1)\n                if c_score.max() > self.score_th:\n                    c_char = c_score.argmax().item()\n                    p_label.append(c_char)\n                else:\n                    break\n            pred_string = self.label_to_string(p_label)\n            result.append(dict(label_string=label_string, pred_string=pred_string,\n                               heatmap=heatmap, classify=classify))\n        return result\n\n    def collect_chars(self, classify, x, y, heatmap):\n        result = []\n        for char_index in range(x.shape[0]):\n            if heatmap[char_index].max() < self.score_th:\n                klass = 0\n            else:\n                klass = classify[:, y[char_index]-1:y[char_index]+1, x[char_index]-1:x[char_index]+1].mean(1).mean(1).argmax().item()\n            result.append(klass)\n        return self.label_to_string(result)\n"""
structure/representers/mask_rcnn.py,0,"b""import pickle\n\nfrom concern.config import Configurable\n\n\nclass MaskRCNNRepresenter(Configurable):\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n    def represent(self, batch, pred):\n        image, label, meta = batch\n\n        output = {\n            'meta': [pickle.loads(value) for value in meta],\n            'polygons_pred': pred,\n        }\n        return output\n"""
structure/representers/seg_detector_representer.py,0,"b""import cv2\nimport numpy as np\nfrom shapely.geometry import Polygon\nimport pyclipper\n\nfrom concern.config import Configurable, State\nimport concern.webcv2 as webcv2\n\n\nclass SegDetectorRepresenter(Configurable):\n    thresh = State(default=0.3)\n    box_thresh = State(default=0.7)\n    max_candidates = State(default=100)\n    resize = State(default=False)\n\n    dest = State(default='binary')\n\n    def __init__(self, cmd={}, **kwargs):\n        self.load_all(**kwargs)\n\n        self.min_size = 3\n        self.scale_ratio = 0.4\n        if 'debug' in cmd:\n            self.debug = cmd['debug']\n        if 'thresh' in cmd:\n            self.thresh = cmd['thresh']\n        if 'box_thresh' in cmd:\n            self.box_thresh = cmd['box_thresh']\n        if 'dest' in cmd:\n            self.dest = cmd['dest']\n\n    def represent(self, batch, _pred):\n        '''\n        batch: (image, polygons, ignore_tags\n        batch: a dict produced by dataloaders.\n            image: tensor of shape (N, C, H, W).\n            polygons: tensor of shape (N, K, 4, 2), the polygons of objective regions.\n            ignore_tags: tensor of shape (N, K), indicates whether a region is ignorable or not.\n            shape: the original shape of images.\n            filename: the original filenames of images.\n        pred:\n            binary: text region segmentation map, with shape (N, 1, H, W)\n            thresh: [if exists] thresh hold prediction with shape (N, 1, H, W)\n            thresh_binary: [if exists] binarized with threshhold, (N, 1, H, W)\n        '''\n        images = batch['image']\n        pred = _pred[self.dest]\n        segmentation = self.binarize(pred)\n        boxes_batch = []\n        preds = []\n        for batch_index in range(images.size(0)):\n            height, width = batch['shape'][batch_index]\n            boxes, single_pred = self.boxes_from_bitmap(\n                _pred['binary'][batch_index],\n                segmentation[batch_index], width, height)\n            boxes_batch.append(boxes)\n            preds.append(single_pred.reshape(1, *single_pred.shape))\n        return boxes_batch, _pred\n\n    def binarize(self, pred):\n        return pred > self.thresh\n\n    def boxes_from_bitmap(self, pred, _bitmap, dest_width, dest_height):\n        '''\n        _bitmap: single map with shape (1, H, W),\n            whose values are binarized as {0, 1}\n        '''\n        assert _bitmap.size(0) == 1\n        bitmap = _bitmap.data.cpu().numpy()[0]  # The first channel\n        pred = pred.cpu().detach().numpy()[0]\n        height, width = bitmap.shape\n        boxes = []\n        _, contours, _ = cv2.findContours(\n            (bitmap*255).astype(np.uint8),\n            cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n\n        if self.debug:\n            bitmap = cv2.cvtColor(pred * 255, cv2.COLOR_GRAY2BGR)\n\n        for contour in contours[:self.max_candidates]:\n            points, sside = self.get_mini_boxes(contour)\n            if sside < self.min_size:\n                continue\n            points = np.array(points)\n            score = self.box_score_fast(pred, points.reshape(-1, 2))\n\n            if self.debug:\n                points = points.astype(np.int32)\n                bitmap = cv2.polylines(\n                        bitmap, [points.reshape(-1, 2)], True, (255, 0, 0), 3)\n                bitmap = cv2.putText(\n                        bitmap, str(round(score, 3)),\n                        (points[:, 0].min(), points[:, 1].min()),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0))\n            if self.box_thresh > score:\n                continue\n            box = self.unclip(points).reshape(-1, 1, 2)\n            box, sside = self.get_mini_boxes(box)\n            if sside < self.min_size + 2:\n                continue\n            box = np.array(box)\n\n            if not self.resize:\n                dest_width = width\n                dest_height = height\n\n            box[:, 0] = np.clip(\n                np.round(box[:, 0] / width * dest_width), 0, dest_width)\n            box[:, 1] = np.clip(\n                np.round(box[:, 1] / height * dest_height), 0, dest_height)\n            boxes.append(box.tolist())\n\n        if self.debug:\n            webcv2.imshow('mask', bitmap)\n        return boxes, bitmap\n\n    def unclip(self, box):\n        poly = Polygon(box)\n        distance = poly.area * 1.5 / poly.length\n        offset = pyclipper.PyclipperOffset()\n        offset.AddPath(box, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n        expanded = np.array(offset.Execute(distance))\n        return expanded\n\n    def get_mini_boxes(self, contour):\n        bounding_box = cv2.minAreaRect(contour)\n        points = sorted(list(cv2.boxPoints(bounding_box)), key=lambda x: x[0])\n\n        index_1, index_2, index_3, index_4 = 0, 1, 2, 3\n        if points[1][1] > points[0][1]:\n            index_1 = 0\n            index_4 = 1\n        else:\n            index_1 = 1\n            index_4 = 0\n        if points[3][1] > points[2][1]:\n            index_2 = 2\n            index_3 = 3\n        else:\n            index_2 = 3\n            index_3 = 2\n\n        box = [points[index_1], points[index_2],\n               points[index_3], points[index_4]]\n        return box, min(bounding_box[1])\n\n    def box_score(self, bitmap, box):\n        '''\n        naive version of box score computation,\n        only for helping principle understand.\n        '''\n        mask = np.zeros_like(bitmap, dtype=np.uint8)\n        cv2.fillPoly(mask, box.reshape(1, 4, 2).astype(np.int32), 1)\n        return cv2.mean(bitmap, mask)[0]\n\n    def box_score_fast(self, bitmap, _box):\n        h, w = bitmap.shape[:2]\n        box = _box.copy()\n        xmin = np.clip(np.floor(box[:, 0].min()).astype(np.int), 0, w - 1)\n        xmax = np.clip(np.ceil(box[:, 0].max()).astype(np.int), 0, w - 1)\n        ymin = np.clip(np.floor(box[:, 1].min()).astype(np.int), 0, h - 1)\n        ymax = np.clip(np.ceil(box[:, 1].max()).astype(np.int), 0, h - 1)\n\n        mask = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.uint8)\n        box[:, 0] = box[:, 0] - xmin\n        box[:, 1] = box[:, 1] - ymin\n        cv2.fillPoly(mask, box.reshape(1, -1, 2).astype(np.int32), 1)\n        return cv2.mean(bitmap[ymin:ymax+1, xmin:xmax+1], mask)[0]\n"""
structure/representers/seg_recognition_representer.py,0,"b""from concern.config import State\nfrom .sequence_recognition_representer import SequenceRecognitionRepresenter\nimport config\n\nimport cv2\nimport numpy as np\n\n\nclass SegRecognitionRepresenter(SequenceRecognitionRepresenter):\n    max_candidates = State(default=64)\n    min_size = State(default=2)\n    thresh = State(default=0.3)\n    box_thresh = State(default=0.7)\n\n    def represent(self, batch, pred):\n        labels = batch['label']\n        mask = pred['mask']\n        classify = pred['classify']\n        result = []\n\n        for batch_index in range(mask.shape[0]):\n            label_string = self.label_to_string(labels[batch_index])\n            result_dict = self.result_from_heatmap(mask[batch_index], classify[batch_index])\n            result_dict.update(label_string=label_string)\n            result.append(result_dict)\n        return result\n\n    def result_from_heatmap(self, bitmap, heatmap):\n        bitmap = (bitmap > self.thresh).data.cpu().numpy()\n        score_map = heatmap.data.cpu().detach().numpy()\n        result = []\n        _, contours, _ = cv2.findContours(\n            (bitmap*255).astype(np.uint8),\n            cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n\n        boxes = []\n        for contour in contours[:self.max_candidates]:\n            points, sside = self.get_boxes(contour)\n\n            #if sside < self.min_size:\n            #    print('by min side')\n            #    continue\n            boxes.append(np.array(points).reshape(-1, 2))\n\n        for points in sorted(boxes, key=lambda x: x[0][0]):\n            score, char_index = self.box_score_fast(bitmap, points.reshape(-1, 2), score_map)\n            #if self.box_thresh > score:\n            #    continue\n            result.append(char_index)\n\n        pred_string = self.charset.label_to_string(result)\n        return dict(mask=bitmap, classify=score_map, pred_string=pred_string)\n\n    def get_boxes(self, contour):\n        bounding_box = cv2.minAreaRect(contour)\n        points = sorted(list(cv2.boxPoints(bounding_box)), key=lambda x: x[0])\n\n        index_1, index_2, index_3, index_4 = 0, 1, 2, 3\n        if points[1][1] > points[0][1]:\n            index_1 = 0\n            index_4 = 1\n        else:\n            index_1 = 1\n            index_4 = 0\n        if points[3][1] > points[2][1]:\n            index_2 = 2\n            index_3 = 3\n        else:\n            index_2 = 3\n            index_3 = 2\n\n        box = [points[index_1], points[index_2],\n               points[index_3], points[index_4]]\n        return box, min(bounding_box[1])\n\n    def box_score_fast(self, bitmap, _box, score_map):\n        h, w = bitmap.shape[:2]\n        box = _box.copy()\n        xmin = np.clip(np.floor(box[:, 0].min()).astype(np.int), 0, w - 1)\n        xmax = np.clip(np.ceil(box[:, 0].max()).astype(np.int), 0, w - 1)\n        ymin = np.clip(np.floor(box[:, 1].min()).astype(np.int), 0, h - 1)\n        ymax = np.clip(np.ceil(box[:, 1].max()).astype(np.int), 0, h - 1)\n\n        mask = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.uint8)\n        box[:, 0] = box[:, 0] - xmin\n        box[:, 1] = box[:, 1] - ymin\n        cv2.fillPoly(mask, box.reshape(1, -1, 2).astype(np.int32), 1)\n        score = cv2.mean(bitmap[ymin:ymax+1, xmin:xmax+1], mask)[0]\n        char_index = int(score_map[1:, ymin:ymax+1, xmin:xmax+1].mean(axis=1).mean(axis=1).argmax()) + 1\n        return score, char_index\n"""
structure/representers/sequence_recognition_representer.py,4,"b""import torch\n\nimport config\nfrom concern.config import Configurable, State\nfrom concern.charsets import DefaultCharset\n\nimport cv2\nimport numpy as np\nimport concern.webcv2 as webcv\n\n\nclass SequenceRecognitionRepresenter(Configurable):\n    charset = State(default=DefaultCharset())\n\n    def __init__(self, cmd={}, **kwargs):\n        self.load_all(**kwargs)\n\n    def label_to_string(self, label):\n        return self.charset.label_to_string(label)\n\n    def represent(self, batch, pred):\n        images, labels = batch['image'], batch['label']\n        mask = torch.ones(pred.shape[0], dtype=torch.int).to(pred.device)\n\n        for i in range(pred.shape[1]):\n            mask = (\n                1 - (pred[:, i] == self.charset.blank).type(torch.int)) * mask\n            pred[:, i] = pred[:, i] * mask + self.charset.blank * (1 - mask)\n\n        output = []\n        for i in range(labels.shape[0]):\n            label_str = self.label_to_string(labels[i])\n            pred_str = self.label_to_string(pred[i])\n            if False and label_str != pred_str:\n                print('label: %s , pred: %s' % (label_str, pred_str))\n                img = (np.clip(images[i].cpu().data.numpy().transpose(\n                    1, 2, 0) + 0.5, 0, 1) * 255).astype('uint8')\n                webcv.imshow('\xe3\x80\x90 pred: <%s> , label: <%s> \xe3\x80\x91' % (\n                    pred_str, label_str), np.array(img, dtype=np.uint8))\n                if webcv.waitKey() == ord('q'):\n                    continue\n            output.append({\n                'label_string': label_str,\n                'pred_string': pred_str\n            })\n\n        return output\n\n\nclass SequenceRecognitionEvaluationRepresenter(Configurable):\n    charset = State(default=DefaultCharset())\n\n    def __init__(self, cmd={}, **kwargs):\n        self.load_all(**kwargs)\n\n    def label_to_string(self, label):\n        return self.charset.label_to_string(label)\n\n    def represent(self, batch, pred):\n        images, labels, lengths = batch\n        mask = torch.ones(pred.shape[0], dtype=torch.int)\n\n        for i in range(pred.shape[1]):\n            mask = (\n                1 - (pred[:, i] == self.charset.blank).type(torch.int)) * mask\n            pred[:, i] = pred[:, i] * mask + self.charset.blank * (1 - mask)\n\n        output = []\n        for i in range(images.shape[0]):\n            pred_str = self.label_to_string(pred[i])\n            output.append(pred_str)\n        return output\n"""
structure/representers/simple_detection.py,0,"b""import pickle\n\nimport cv2\nimport numpy as np\nimport skimage\nfrom shapely.geometry import Polygon, LineString\n\nfrom concern.convert import to_np\nfrom concern.config import Configurable, State\nfrom data.simple_detection import binary_search_smallest_width\n\n\nclass SimpleDetectionRepresenter(Configurable):\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n    def represent_batch(self, batch):\n        image, label, meta = batch\n\n        output = {\n            'image': to_np(image),\n            **{name: to_np(m) for name, m in label.items()},\n            'meta': [pickle.loads(value) for value in meta],\n        }\n        self.postprocess(output)\n\n        unrecovered_polygons = []\n        polygons = []\n        for idx in range(image.shape[0]):\n            single_output = self.get_single_output(output, idx)\n            p = self.get_polygons(single_output)\n            unrecovered_polygons.append(p)\n            polygons.append(self.recover_polygons(single_output, p))\n        output['unrecovered_polygons'] = unrecovered_polygons\n        output['polygons'] = polygons\n\n        return output\n\n    def get_single_output(self, output, idx):\n        return {name: value[idx] for name, value in output.items()}\n\n    def threshold_heatmap(self, heatmap, thr):\n        return (heatmap > thr).astype('uint8')\n\n    def postprocess(self, output):\n        raise NotImplementedError()\n\n    def get_polygons(self, output):\n        raise NotImplementedError()\n\n    def recover_polygons(self, output, polygons):\n        scale_h = output['meta']['scale_h']\n        scale_w = output['meta']['scale_w']\n\n        recovered_polygons = []\n        for polygon in polygons:\n            recovered_polygons.append([(x / scale_w, y / scale_h) for x, y in polygon])\n        return recovered_polygons\n\n    def represent(self, batch, pred):\n        image, label, meta = batch\n\n        batch_output = self.represent_batch(batch)\n        pred_output = {name: to_np(m) for name, m in pred.items()}\n        self.postprocess(pred_output)\n\n        unrecovered_polygons = []\n        polygons = []\n        for idx in range(image.shape[0]):\n            batch_single_output = self.get_single_output(batch_output, idx)\n            pred_single_output = self.get_single_output(pred_output, idx)\n            p = self.get_polygons(pred_single_output)\n            unrecovered_polygons.append(p)\n            polygons.append(self.recover_polygons(batch_single_output, p))\n        pred_output['unrecovered_polygons'] = unrecovered_polygons\n        pred_output['polygons'] = polygons\n\n        output = {\n            **batch_output,\n            **{name + '_pred': m for name, m in pred_output.items()},\n        }\n        return output\n\n\nclass SimpleSegRepresenter(SimpleDetectionRepresenter):\n    heatmap_thr = State(default=0.5)\n    min_average_score = State(default=0.8)\n    min_area = State(default=200)\n    max_poly_points = State(default=32)\n    expand = State(default=2.0)\n    max_polys = State(default=128)\n\n    def postprocess(self, output):\n        output['heatmask'] = self.threshold_heatmap(output['heatmap'], self.heatmap_thr)\n\n    def get_polygons(self, output):\n        heatmask = output['heatmask'][0]\n        heatmap = output['heatmap'][0]\n\n        _, contours, _ = cv2.findContours(heatmask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n\n        polygons = []\n        for contour in contours[:self.max_polys]:\n            poly = contour[:, 0]\n            stride = len(poly) // self.max_poly_points + 1\n            poly = poly[::stride]\n            if len(poly) >= 3:\n                height = binary_search_smallest_width(poly)\n                expanded_poly = Polygon(poly).buffer(height * (self.expand - 1))\n                if expanded_poly.geom_type == 'Polygon' and not expanded_poly.is_empty and expanded_poly.area > self.min_area:\n                    poly_r, poly_c = skimage.draw.polygon(poly[:, 1], poly[:, 0])\n                    if heatmap[poly_r, poly_c].mean() > self.min_average_score:\n                        expanded_poly = list(expanded_poly.exterior.coords)[:-1]\n                        polygons.append(expanded_poly)\n\n        return polygons\n\n\nclass SimpleEASTRepresenter(SimpleDetectionRepresenter):\n    heatmap_thr = State(default=0.5)\n\n    def postprocess(self, output):\n        output['heatmask'] = self.threshold_heatmap(output['heatmap'], self.heatmap_thr)\n\n    def get_polygons(self, output):\n        heatmask = output['heatmask'][0]\n        densebox = output['densebox']\n\n        _, contours, _ = cv2.findContours(heatmask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n\n        polygons = []\n        for contour in contours:\n            points = []\n            for x, y in contour[:, 0]:\n                quad = densebox[:, y, x].reshape(4, 2) + (x, y)\n                points.extend(quad)\n            quad = cv2.boxPoints(cv2.minAreaRect(np.array(points, np.float32)))\n            polygons.append(quad)\n\n        return polygons\n\n\nclass SimpleTextsnakeRepresenter(SimpleDetectionRepresenter):\n    heatmap_thr = State(default=0.5)\n    min_area = State(default=200)\n\n    def postprocess(self, output):\n        output['heatmask'] = self.threshold_heatmap(output['heatmap'], self.heatmap_thr)\n\n    def get_polygons(self, output):\n        heatmask = output['heatmask'][0]\n        radius = output['radius']\n\n        _, contours, _ = cv2.findContours(heatmask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n\n        polygons = []\n        for contour in contours:\n            contour = contour[:, 0]\n            mask = np.zeros(heatmask.shape[:2], dtype=np.uint8)\n            for x, y in contour:\n                r = radius[0, y, x]\n                if r > 1:\n                    cv2.circle(mask, (int(x), int(y)), int(r), 1, -1)\n\n            _, conts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n            if len(conts) == 1:\n                poly = conts[0][:, 0]\n                if Polygon(poly).geom_type == 'Polygon' and Polygon(poly).area > self.min_area:\n                    polygons.append(poly)\n\n        return polygons\n\n\nclass SimpleMSRRepresenter(SimpleDetectionRepresenter):\n    heatmap_thr = State(default=0.5)\n    min_area = State(default=200)\n    max_poly_points = State(default=32)\n\n    def postprocess(self, output):\n        output['heatmask'] = self.threshold_heatmap(output['heatmap'], self.heatmap_thr)\n\n    def get_polygons(self, output):\n        heatmask = output['heatmask'][0]\n        offset = output['offset']\n        _, contours, _ = cv2.findContours(heatmask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n\n        polygons = []\n        for contour in contours:\n            contour = contour[:, 0]\n            poly = []\n            stride = len(contour) // self.max_poly_points + 1\n            for x, y in contour[::stride]:\n                point = offset[:, y, x] + (x, y)\n                poly.append(point)\n            if len(poly) >= 3:\n                if Polygon(poly).area > self.min_area:\n                    polygons.append(poly)\n\n        return polygons\n"""
structure/representers/textsnake.py,0,"b'import pickle\n\nimport cv2\nimport numpy as np\n\nfrom concern.convert import to_np\nfrom concern.textsnake import fill_hole, regularize_sin_cos, norm2, vector_sin, vector_cos\nfrom concern.config import Configurable, State\n\n\nclass TextsnakeResultBuilder(Configurable):\n    tr_thresh = State(default=0.6)\n    tcl_thresh = State(default=0.4)\n\n    @staticmethod\n    def find_innerpoint(cont):\n        """"""\n        generate an inner point of input polygon using mean of x coordinate by:\n        1. calculate mean of x coordinate(xmean)\n        2. calculate maximum and minimum of y coordinate(ymax, ymin)\n        2. iterate for each y in range (ymin, ymax), find first segment in the polygon\n        3. calculate means of segment\n        :param cont: input polygon\n        :return:\n        """"""\n\n        xmean = cont[:, 0, 0].mean()\n        ymin, ymax = cont[:, 0, 1].min(), cont[:, 0, 1].max()\n        found = False\n        found_y = []\n        for i in np.arange(ymin - 1, ymax + 1, 0.5):\n            # if in_poly > 0, (xmean, i) is in `cont`\n            in_poly = cv2.pointPolygonTest(cont, (xmean, i), False)\n            if in_poly > 0:\n                found = True\n                found_y.append(i)\n            # first segment found\n            if in_poly < 0 and found:\n                break\n\n        if found_y:\n            return xmean, np.array(found_y).mean()\n\n        # if cannot find using above method, try each point\'s neighbor\n        else:\n            for p in range(len(cont)):\n                point = cont[p, 0]\n                for i in range(-1, 2, 1):\n                    for j in range(-1, 2, 1):\n                        test_pt = point + [i, j]\n                        if cv2.pointPolygonTest(cont, (test_pt[0], test_pt[1]), False) > 0:\n                            return test_pt\n\n    def in_contour(self, cont, point):\n        x, y = point\n        return cv2.pointPolygonTest(cont, (x, y), False) >= 0\n\n    def centerlize(self, x, y, height, width, tangent_cos, tangent_sin, tcl_contour, stride=1):\n        """"""\n        centralizing (x, y) using tangent line and normal line.\n        :return:\n        """"""\n\n        # calculate normal sin and cos\n        normal_cos = -tangent_sin\n        normal_sin = tangent_cos\n\n        # find upward\n        _x, _y = x, y\n        while self.in_contour(tcl_contour, (_x, _y)):\n            _x = _x + normal_cos * stride\n            _y = _y + normal_sin * stride\n            if int(_x) >= width or int(_x) < 0 or int(_y) >= height or int(_y) < 0:\n                break\n        end1 = np.array([_x, _y])\n\n        # find downward\n        _x, _y = x, y\n        while self.in_contour(tcl_contour, (_x, _y)):\n            _x = _x - normal_cos * stride\n            _y = _y - normal_sin * stride\n            if int(_x) >= width or int(_x) < 0 or int(_y) >= height or int(_y) < 0:\n                break\n        end2 = np.array([_x, _y])\n\n        # centralizing\n        center = (end1 + end2) / 2\n        return center\n\n    def mask_to_tcl(self, pred_sin, pred_cos, pred_radius, tcl_contour, init_xy, direct=1):\n        """"""\n        Iteratively find center line in tcl mask using initial point (x, y)\n        :param pred_sin: predict sin map\n        :param pred_cos: predict cos map\n        :param tcl_mask: predict tcl mask\n        :param init_xy: initial (x, y)\n        :param direct: direction [-1|1]\n        :return:\n        """"""\n\n        height, width = pred_sin.shape\n        x_init, y_init = init_xy\n\n        sin = pred_sin[int(y_init), int(x_init)]\n        cos = pred_cos[int(y_init), int(x_init)]\n        radius = pred_radius[int(y_init), int(x_init)]\n\n        x_shift, y_shift = self.centerlize(x_init, y_init, height, width, cos, sin, tcl_contour)\n        result = []\n\n        attempt = 0\n        while self.in_contour(tcl_contour, (x_shift, y_shift)):\n\n            if attempt < 100:\n                attempt += 1\n            else:\n                break\n\n            x, y = x_shift, y_shift\n\n            sin = pred_sin[int(y), int(x)]\n            cos = pred_cos[int(y), int(x)]\n\n            x_c, y_c = self.centerlize(x, y, height, width, cos, sin, tcl_contour)\n            result.append(np.array([x_c, y_c, radius]))\n\n            sin_c = pred_sin[int(y_c), int(x_c)]\n            cos_c = pred_cos[int(y_c), int(x_c)]\n            radius = pred_radius[int(y_c), int(x_c)]\n\n            # shift stride\n            for shrink in [1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32]:\n                # stride = +/- 0.5 * [sin|cos](theta), if new point is outside, shrink it until shrink < 0.1, hit ends\n                t = shrink * radius\n                x_shift_pos = x_c + cos_c * t * direct  # positive direction\n                y_shift_pos = y_c + sin_c * t * direct  # positive direction\n                x_shift_neg = x_c - cos_c * t * direct  # negative direction\n                y_shift_neg = y_c - sin_c * t * direct  # negative direction\n\n                # if first point, select positive direction shift\n                if len(result) == 1:\n                    x_shift, y_shift = x_shift_pos, y_shift_pos\n                else:\n                    # else select point further with second last point\n                    dist_pos = norm2(result[-2][:2] - (x_shift_pos, y_shift_pos))\n                    dist_neg = norm2(result[-2][:2] - (x_shift_neg, y_shift_neg))\n                    if dist_pos > dist_neg:\n                        x_shift, y_shift = x_shift_pos, y_shift_pos\n                    else:\n                        x_shift, y_shift = x_shift_neg, y_shift_neg\n                # if out of bounds, skip\n                if int(x_shift) >= width or int(x_shift) < 0 or int(y_shift) >= height or int(y_shift) < 0:\n                    continue\n                # found an inside point\n                if self.in_contour(tcl_contour, (x_shift, y_shift)):\n                    break\n            # if out of bounds, break\n            if int(x_shift) >= width or int(x_shift) < 0 or int(y_shift) >= height or int(y_shift) < 0:\n                break\n        return result\n\n    def build_tcl(self, tcl_pred, sin_pred, cos_pred, radius_pred):\n        """"""\n        Find TCL\'s center points and radius of each point\n        :param tcl_pred: output tcl mask, (512, 512)\n        :param sin_pred: output sin map, (512, 512)\n        :param cos_pred: output cos map, (512, 512)\n        :param radius_pred: output radius map, (512, 512)\n        :return: (list), tcl array: (n, 3), 3 denotes (x, y, radius)\n        """"""\n        all_tcls = []\n\n        # find disjoint regions\n        mask = fill_hole(tcl_pred)\n        _, conts, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        conts = [cont for cont in conts if cv2.contourArea(cont) > 20]\n\n        for index, cont in enumerate(conts):\n\n            # find an inner point of polygon\n            init = self.find_innerpoint(cont)\n\n            if init is None:\n                continue\n\n            x_init, y_init = init\n\n            # find left tcl\n            tcl_left = self.mask_to_tcl(sin_pred, cos_pred, radius_pred, cont, (x_init, y_init), direct=1)\n            tcl_left = np.array(tcl_left)\n            # find right tcl\n            tcl_right = self.mask_to_tcl(sin_pred, cos_pred, radius_pred, cont, (x_init, y_init), direct=-1)\n            tcl_right = np.array(tcl_right)\n            # concat\n            tcl = np.concatenate([tcl_left[::-1][:-1], tcl_right])\n            if len(tcl) > 1:\n                all_tcls.append(tcl)\n\n        return all_tcls\n\n    def detect(self, tr_pred, tcl_pred, sin_pred, cos_pred, radius_pred):\n        tr_pred_mask = tr_pred > self.tr_thresh\n        tcl_pred_mask = tcl_pred > self.tcl_thresh\n\n        tcl = tcl_pred_mask * tr_pred_mask\n\n        sin_pred, cos_pred = regularize_sin_cos(sin_pred, cos_pred)\n\n        detect_result = self.build_tcl(tcl, sin_pred, cos_pred, radius_pred)\n\n        return detect_result\n\n    def result2polygon(self, mask_shape, result):\n        """""" convert geometric info(center_x, center_y, radius) into contours\n        :param image: image\n        :param result: (list), each with (n, 3), 3 denotes (x, y, radius)\n        :return: (np.ndarray list), polygon format contours\n        """"""\n        all_conts = []\n        for disk in result:\n            mask = np.zeros(mask_shape, dtype=np.uint8)\n            for x, y, r in disk:\n                if r > 1:\n                    cv2.circle(mask, (int(x), int(y)), int(r), 1, -1)\n\n            _, conts, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n            if len(conts) > 1:\n                conts.sort(key=lambda x: cv2.contourArea(x), reverse=True)\n            elif not conts:\n                continue\n            if cv2.contourArea(conts[0]) < 20:\n                continue\n            all_conts.append(conts[0][:, 0, :])\n        return all_conts\n\n    def remove_fp_1(self, tcl_pred_mask, detect_tcl, contours):\n        result = []\n        for contour, disk in zip(contours, detect_tcl):\n            mask = np.zeros(tcl_pred_mask.shape, dtype=np.uint8)\n            cv2.fillPoly(mask, [contour], 1)\n            tcl_sum = (mask * tcl_pred_mask).sum()\n            area = 0\n            for i in range(len(disk) - 1):\n                area += (disk[i][2] + disk[i + 1][2]) / 2 * norm2(\n                    disk[i][:2] - disk[i + 1][:2]) * 0.2\n            if tcl_sum > area:\n                result.append(contour)\n        return result\n\n    def remove_fp_2(self, tr_pred_mask, contours):\n        result = []\n        for contour in contours:\n            mask = np.zeros(tr_pred_mask.shape, dtype=np.uint8)\n            cv2.fillPoly(mask, [contour], 1)\n            if (tr_pred_mask * mask).sum() * 2 > mask.sum():\n                result.append(contour)\n        return result\n\n    def stride(self, detect_tcl, contours, detected_conts, left=True, step=0.5):\n        if left:\n            last_point, before_point = detect_tcl[:2]\n        else:\n            before_point, last_point = detect_tcl[-2:]\n        radius = last_point[2]\n        cos = vector_cos(last_point[:2] - before_point[:2])\n        sin = vector_sin(last_point[:2] - before_point[:2])\n        new_point = last_point[:2] + radius * step * np.array((cos, sin))\n        for index, contour in enumerate(contours):\n            if index not in detected_conts and (self.in_contour(contour, new_point)):\n                return new_point, index\n        return None, None\n\n    def remove_fp_stride(self, mask_shape, detect_tcl, contours):\n        detected = set()\n        result = []\n        for index, tcl in enumerate(detect_tcl):\n            if index in detected:\n                continue\n            detected.add(index)\n            _, left_index = self.stride(tcl, contours, detected, step=0)\n            _, right_index = self.stride(tcl, contours, detected, False, step=0)\n            if left_index:\n                detected.add(left_index)\n                tcl = np.concatenate((tcl, detect_tcl[left_index]))\n            if right_index:\n                detected.add(right_index)\n                tcl = np.concatenate((tcl, detect_tcl[right_index]))\n            result.append(tcl)\n        return self.result2polygon(mask_shape, result)\n\n    def get_polygons(self, tr_pred, tcl_pred, sin_pred, cos_pred, radius_pred):\n        tr_pred_mask = tr_pred > self.tr_thresh\n        result = self.detect(tr_pred, tcl_pred, sin_pred, cos_pred, radius_pred)\n        polygons = self.result2polygon(sin_pred.shape, result)\n        polygons = self.remove_fp_stride(sin_pred.shape, result, polygons)\n        polygons = self.remove_fp_2(tr_pred_mask, polygons)\n        return polygons\n\n\nclass TextsnakeRepresenter(Configurable):\n    result_builder = TextsnakeResultBuilder()\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n    def represent_batch(self, batch):\n        image, label, meta = batch\n        batch_size = image.shape[0]\n\n        output = {\n            \'image\': to_np(image).transpose(0, 2, 3, 1),\n            **{key: to_np(value) for key, value in label.items()},\n            \'meta\': [pickle.loads(value) for value in meta],\n        }\n\n        output[\'contours_gt\'] = [self.result_builder.get_polygons(\n            output[\'tr_mask\'][i],\n            output[\'tcl_mask\'][i],\n            output[\'sin_map\'][i],\n            output[\'cos_map\'][i],\n            output[\'radius_map\'][i],\n        ) for i in range(batch_size)]\n\n        output[\'polygons_gt\'] = [output[\'meta\'][i][\'Cnts\'] for i in range(batch_size)]\n\n        return output\n\n    def represent(self, batch, pred):\n        image, label, meta = batch\n        batch_size = image.shape[0]\n\n        output = self.represent_batch(batch)\n        output.update({key: to_np(value) for key, value in pred.items()})\n        output[\'contours_pred\'] = [self.result_builder.get_polygons(\n            output[\'tr_pred\'][i],\n            output[\'tcl_pred\'][i],\n            output[\'sin_pred\'][i],\n            output[\'cos_pred\'][i],\n            output[\'radius_pred\'][i],\n        ) for i in range(batch_size)]\n\n        return output\n'"
structure/visualizers/__init__.py,0,b'from .textsnake import TextsnakeVisualizer\nfrom .seg_recognition_visualizer import SegRecognitionVisualizer\nfrom .ctc_visualizer2d import CTCVisualizer2D\nfrom .seg_detector_visualizer import SegDetectorVisualizer\n# from .sequence_recognition_visualizer import SequenceRecognitionVisualizer\n'
structure/visualizers/ctc_visualizer2d.py,0,"b'import cv2\nimport numpy as np\n\nfrom concern.config import Configurable, State\nfrom concern.visualizer import Visualize\nfrom data.processes import NormalizeImage\nimport concern.webcv2 as webcv2\n\n\nclass CTCVisualizer2D(Configurable):\n    eager_show = State(default=False, cmd_key=\'eager_show\')\n\n    def visualize(self, batch, output, interested):\n        return self.visualize_batch(batch, output)\n\n    def visualize_batch(self, batch, output):\n        visualization = dict()\n        for index, output_dict in enumerate(output):\n            image = batch[\'image\'][index]\n            image = NormalizeImage.restore(image)\n\n            mask = output_dict[\'mask\']\n            mask = cv2.resize(Visualize.visualize_weights(mask), image.shape[:2][::-1])\n\n            classify = output_dict[\'classify\']\n            classify = cv2.resize(Visualize.visualize_heatmap(classify, format=\'CHW\'), image.shape[:2][::-1])\n\n            canvas = np.concatenate([image, mask, classify], axis=0)\n            key = ""\xe3\x80\x90%s-%s\xe3\x80\x91"" % (output_dict[\'label_string\'], output_dict[\'pred_string\'])\n            vis_dict = {\n                key: canvas\n            }\n\n            if self.eager_show:\n                for k, v in vis_dict.items():\n                    # if output_dict[\'label_string\'] != output_dict[\'pred_string\']:\n                    webcv2.imshow(k, v)\n            visualization.update(mask=mask, classify=classify, image=image)\n        if self.eager_show:\n            webcv2.waitKey()\n        return visualization\n'"
structure/visualizers/east.py,0,"b""import cv2\nimport numpy as np\n\nfrom concern.config import Configurable, State\n\n\nclass EASTVisualizer(Configurable):\n    vis_num = State(default=4)\n\n    def __init__(self, **kwargs):\n        pass\n\n    def visualize_detection(self, image, heatmap, heatmask, densebox, polygons):\n        image_show = image.transpose(1, 2, 0).copy()\n\n        h, w = image_show.shape[:2]\n        densebox_image = np.zeros((h, w, 3), 'uint8')\n        densebox_anchor = np.indices((h, w))[::-1]\n\n        colors = [(64, 64, 255), (64, 255, 255), (64, 255, 64), (255, 255, 64)]\n        for i in range(0, 4):\n            points = densebox[i * 2: i * 2 + 2] + densebox_anchor\n            points = points[np.tile(heatmask[np.newaxis], (2, 1, 1)) > 0]\n            points = points.reshape(2, -1).astype('int32')\n            mask = np.logical_and.reduce([points[0] >= 0, points[0] < w, points[1] >= 0, points[1] < h])\n            densebox_image[points[1, mask], points[0, mask]] = colors[i]\n\n        image_show = cv2.polylines(image_show, np.array(polygons, 'int32'), True, (0, 0, 255), 1)\n\n        result_image = np.concatenate([\n            image_show,\n            cv2.cvtColor((heatmap * 255).astype('uint8'), cv2.COLOR_GRAY2BGR),\n            cv2.cvtColor((heatmask * 255).astype('uint8'), cv2.COLOR_GRAY2BGR),\n            densebox_image,\n        ], axis=1)\n\n        return result_image\n\n    def visualize_weight(self, idx, output):\n        heatmap_weight = output['heatmap_weight'][idx]\n        densebox_weight = output['densebox_weight'][idx]\n\n        result_image = np.concatenate([\n            cv2.cvtColor(((heatmap_weight / heatmap_weight.max())[0] * 255).astype('uint8'), cv2.COLOR_GRAY2BGR),\n            cv2.cvtColor((densebox_weight[0] * 255).astype('uint8'), cv2.COLOR_GRAY2BGR),\n        ], axis=1)\n\n        return result_image\n\n    def visualize_pred_detection(self, idx, output):\n        image = output['image'][idx]\n        heatmap = output['heatmap_pred'][idx]\n        heatmask = output['heatmask_pred'][idx]\n        densebox = output['densebox_pred'][idx]\n        polygons = output['polygons_pred'][idx]\n\n        return self.visualize_detection(image, heatmap, heatmask, densebox, polygons)\n\n    def visualize_gt_detection(self, idx, output):\n        image = output['image'][idx]\n        heatmap = output['heatmap'][idx]\n        heatmask = output['heatmask'][idx]\n        densebox = output['densebox'][idx]\n        polygons = output['polygons'][idx]\n\n        return self.visualize_detection(image, heatmap, heatmask, densebox, polygons)\n\n    def get_image(self, idx, output):\n        return np.concatenate([\n            self.visualize_gt_detection(idx, output),\n            self.visualize_weight(idx, output),\n            self.visualize_pred_detection(idx, output),\n        ], axis=1)\n\n    def gt_get_image(self, idx, output):\n        return np.concatenate([\n            self.visualize_gt_detection(idx, output),\n            self.visualize_weight(idx, output),\n        ], axis=1)\n\n    def visualize(self, batch, output, interested):\n        images = {}\n        for i in range(min(self.vis_num, len(output['image']))):\n            show = self.get_image(i, output)\n            images['image_%d' % i] = show.astype(np.uint8)\n        return images\n\n    def visualize_batch(self, batch, output):\n        images = {}\n        for i in range(len(output['image'])):\n            show = self.gt_get_image(i, output)\n            images['image_%d' % i] = show.astype(np.uint8)\n        return images\n"""
structure/visualizers/seg_detector_visualizer.py,2,"b""import cv2\nimport concern.webcv2 as webcv2\nimport numpy as np\nimport torch\n\nfrom concern.config import Configurable, State\nfrom data.processes.make_icdar_data import MakeICDARData\n\n\nclass SegDetectorVisualizer(Configurable):\n    vis_num = State(default=4)\n    eager_show = State(default=False)\n\n    def __init__(self, **kwargs):\n        cmd = kwargs['cmd']\n        if 'eager_show' in cmd:\n            self.eager_show = cmd['eager_show']\n\n    def visualize(self, batch, output_pair, interested):\n        output, pred = output_pair\n        result_dict = {}\n        for i in range(batch['image'].size(0)):\n            result_dict.update(\n                self.single_visualize(batch, i, output[i], pred))\n        if self.eager_show:\n            webcv2.waitKey()\n            return {}\n        return result_dict\n\n    def _visualize_heatmap(self, heatmap, canvas=None):\n        if isinstance(heatmap, torch.Tensor):\n            heatmap = heatmap.detach().cpu().numpy()\n        heatmap = (heatmap[0] * 255).astype(np.uint8)\n        if canvas is None:\n            pred_image = heatmap\n        else:\n            pred_image = (heatmap.reshape(\n                *heatmap.shape[:2], 1).astype(np.float32) / 255 + 1) / 2 * canvas\n            pred_image = pred_image.astype(np.uint8)\n        return pred_image\n\n    def single_visualize(self, batch, index, output, pred):\n        image = batch['image'][index]\n        polygons = batch['polygons'][index]\n        if isinstance(polygons, torch.Tensor):\n            polygons = polygons.cpu().data.numpy()\n        ignore_tags = batch['ignore_tags'][index]\n        original_shape = batch['shape'][index]\n        filename = batch['filename'][index]\n        std = np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n        mean = np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n        image = (image.cpu().numpy() * std + mean).transpose(1, 2, 0) * 255\n        pred_canvas = image.copy().astype(np.uint8)\n        original_shape = tuple(original_shape.tolist())\n        pred_canvas = self._visualize_heatmap(pred['binary'][index], pred_canvas)\n\n        if 'thresh' in pred:\n            thresh = self._visualize_heatmap(pred['thresh'][index])\n\n        if 'thresh_binary' in pred:\n            thresh_binary = self._visualize_heatmap(pred['thresh_binary'][index])\n            MakeICDARData.polylines(self, thresh_binary, polygons, ignore_tags)\n        MakeICDARData.polylines(self, pred_canvas, polygons, ignore_tags)\n\n        for box in output:\n            box = np.array(box).astype(np.int32).reshape(-1, 2)\n            cv2.polylines(pred_canvas, [box], True, (0, 255, 0), 1)\n            if 'thresh_binary' in pred:\n                cv2.polylines(thresh_binary, [box], True, (0, 255, 0), 1)\n\n        if self.eager_show:\n            webcv2.imshow(filename + ' output', cv2.resize(pred_canvas, (1024, 1024)))\n            if 'thresh' in pred:\n                webcv2.imshow(filename + ' thresh', cv2.resize(thresh, (1024, 1024)))\n                webcv2.imshow(filename + ' pred', cv2.resize(pred_canvas, (1024, 1024)))\n            if 'thresh_binary' in pred:\n                webcv2.imshow(filename + ' thresh_binary', cv2.resize(thresh_binary, (1024, 1024)))\n            return {}\n        else:\n            return {\n                filename + '_output': pred_canvas,\n                filename + '_pred': np.expand_dims(thresh_binary, 2) if thresh_binary is not None else None\n            }\n"""
structure/visualizers/seg_recognition_visualizer.py,0,"b'import cv2\nimport numpy as np\n\nimport concern.webcv2 as webcv2\nfrom concern.config import Configurable, State\nfrom concern.visualizer import Visualize\nfrom data.processes import NormalizeImage\n\n\nclass SegRecognitionVisualizer(Configurable):\n    eager_show = State(default=False)\n\n    def __init__(self, cmd, **kwargs):\n        self.load_all(cmd=cmd, **kwargs)\n        self.eager_show = cmd.get(\'eager_show\', self.eager_show)\n\n    def visualize(self, batch, output, interested):\n        return self.visualize_batch(batch, output)\n\n    def visualize_batch(self, batch, output):\n        visualization = dict()\n        for index, output_dict in enumerate(output):\n            image = batch[\'image\'][index]\n            image = NormalizeImage.restore(image)\n\n            mask = output_dict[\'mask\']\n            mask = cv2.resize(Visualize.visualize_weights(mask), image.shape[:2][::-1])\n\n            classify = output_dict[\'classify\']\n            classify = cv2.resize(Visualize.visualize_heatmap(classify, format=\'CHW\'), image.shape[:2][::-1])\n\n            canvas = np.concatenate([image, mask, classify], axis=0)\n            key = ""\xe3\x80\x90%s-%s\xe3\x80\x91"" % (output_dict[\'label_string\'], output_dict[\'pred_string\'])\n            vis_dict = {\n                key: canvas\n            }\n\n            if self.eager_show:\n                for k, v in vis_dict.items():\n                    # if output_dict[\'label_string\'] != output_dict[\'pred_string\']:\n                    webcv2.imshow(k, v)\n            visualization.update(vis_dict)\n        if self.eager_show:\n            webcv2.waitKey()\n        return visualization\n'"
structure/visualizers/sequence_recognition_visualizer.py,0,"b""import cv2\nimport numpy as np\n\nfrom concern.config import Configurable, State\nimport concern.webcv2 as webcv2\nfrom concern.charsets import DefaultCharset\nfrom data.processes.normalize_image import NormalizeImage\n\n\nclass SequenceRecognitionVisualizer(Configurable):\n    charset = State(default=DefaultCharset())\n\n    def __init__(self, cmd={}, **kwargs):\n        self.eager = cmd.get('eager_show', False)\n        self.load_all(**kwargs)\n\n    def visualize(self, batch, output, interested):\n        return self.visualize_batch(batch, output)\n\n    def visualize_batch(self, batch, output):\n        images, labels, lengths = batch['image'], batch['label'], batch['length']\n        for i in range(images.shape[0]):\n            image = NormalizeImage.restore(images[i])\n            gt = self.charset.label_to_string(labels[i])\n            webcv2.imshow(output[i]['pred_string'] + '_' + str(i) + '_' + gt, image)\n            # folder = 'images/dropout/lexicon/'\n            # np.save(folder + output[i]['pred_string'] + '_' + gt + '_' + batch['data_ids'][i], image)\n        webcv2.waitKey()\n        return {\n            'image': (np.clip(batch['image'][0].cpu().data.numpy().transpose(1, 2, 0) + 0.5, 0, 1) * 255).astype(\n                'uint8')\n        }\n"""
structure/visualizers/simple_detection.py,0,"b""import cv2\nimport numpy as np\n\nfrom concern.config import Configurable, State\n\n\nclass SimpleDetectionVisualizer(Configurable):\n    vis_num = State(default=4)\n\n    def __init__(self, **kwargs):\n        self.load_all(**kwargs)\n\n    def get_image_item(self):\n        return 'image'\n\n    def get_polygon_items(self):\n        return ['unrecovered_polygons', 'unrecovered_polygons_pred']\n\n    def get_offset_items(self):\n        return []\n\n    def get_heatmap_items(self):\n        return []\n\n    def get_weight_items(self):\n        return []\n\n    def draw_polygon_image(self, image, polygons):\n        image = image.copy()\n        for polygon in polygons:\n            cv2.polylines(image, np.array([polygon], 'int32'), True, (0, 0, 255), 1)\n        return image\n\n    def draw_offset_image(self, image, offset, mask):\n        image = image.copy()\n\n        h, w = image.shape[:2]\n        anchor = np.indices((h, w))[::-1]\n        num_points = offset.shape[0] // 2\n\n        color_table = np.zeros((num_points, 3), 'uint8')\n        color_table[:, 0] = np.linspace(0, 255, num_points, dtype='uint8')\n        color_table[:, 1] = 255\n        color_table[:, 2] = 255\n        color_table = cv2.cvtColor(color_table[np.newaxis], cv2.COLOR_HSV2BGR)[0]\n\n        for i in range(0, num_points):\n            points = offset[i * 2: i * 2 + 2] + anchor\n            points = points[np.tile(mask, (2, 1, 1)) > 0]\n            points = points.reshape(2, -1).astype('int32')\n            inside_mask = np.logical_and.reduce([points[0] >= 0, points[0] < w, points[1] >= 0, points[1] < h])\n            image[points[1, inside_mask], points[0, inside_mask]] = color_table[i]\n\n        return image\n\n    def draw_heatmap_image(self, heatmap):\n        heatmap = heatmap[0]\n        return cv2.cvtColor((heatmap * 255).astype('uint8'), cv2.COLOR_GRAY2BGR)\n\n    def draw_weight_image(self, weight):\n        weight = weight[0]\n        return cv2.cvtColor(((weight / (weight.max() + 1e-8)) * 255).astype('uint8'), cv2.COLOR_GRAY2BGR)\n\n    def draw_image(self, idx, output):\n        image = output[self.get_image_item()][idx].transpose(1, 2, 0)\n        images = []\n\n        for item in self.get_polygon_items():\n            if item in output:\n                images.append(self.draw_polygon_image(image, output[item][idx]))\n\n        for item, mask_item in self.get_offset_items():\n            if item in output:\n                images.append(self.draw_offset_image(image, output[item][idx], output[mask_item][idx]))\n\n        for item in self.get_heatmap_items():\n            if item in output:\n                images.append(self.draw_heatmap_image(output[item][idx]))\n\n        for item in self.get_weight_items():\n            if item in output:\n                images.append(self.draw_weight_image(output[item][idx]))\n\n        return np.concatenate(images, axis=1)\n\n    def visualize(self, batch, output, interested):\n        images = {}\n        for i in range(min(self.vis_num, len(output[self.get_image_item()]))):\n            show = self.draw_image(i, output)\n            images['image_%d' % i] = show.astype(np.uint8)\n        return images\n\n    def visualize_batch(self, batch, output):\n        return self.visualize(batch, output, None)\n\n\nclass SimpleSegVisualizer(SimpleDetectionVisualizer):\n    def get_heatmap_items(self):\n        return ['heatmap', 'heatmap_pred']\n\n    def get_weight_items(self):\n        return ['heatmap_weight']\n\n\nclass SimpleEASTVisualizer(SimpleDetectionVisualizer):\n    def get_offset_items(self):\n        return [\n            ('densebox', 'heatmask'),\n            ('densebox_pred', 'heatmask_pred'),\n        ]\n\n    def get_heatmap_items(self):\n        return ['heatmap', 'heatmap_pred']\n\n    def get_weight_items(self):\n        return ['heatmap_weight', 'densebox_weight']\n\n\nclass SimpleTextsnakeVisualizer(SimpleDetectionVisualizer):\n    def get_heatmap_items(self):\n        return ['heatmap', 'heatmap_pred']\n\n    def get_weight_items(self):\n        return ['heatmap_weight', 'radius_weight', 'radius', 'radius_pred']\n\n\nclass SimpleMSRVisualizer(SimpleDetectionVisualizer):\n    def get_offset_items(self):\n        return [\n            ('offset', 'heatmask'),\n            ('offset_pred', 'heatmask_pred'),\n        ]\n\n    def get_heatmap_items(self):\n        return ['heatmap', 'heatmap_pred']\n\n    def get_weight_items(self):\n        return ['heatmap_weight', 'offset_weight']\n"""
structure/visualizers/textsnake.py,0,"b""import torch\nimport cv2\nimport numpy as np\n\nfrom concern.config import Configurable, State\n\n\nclass TextsnakeVisualizer(Configurable):\n    vis_num = State(default=4)\n\n    def __init__(self, **kwargs):\n        pass\n\n    def gt_get_image(self, idx, output):\n        img = output['image'][idx]\n        img_show = img.copy()\n\n        tr_mask = output['tr_mask'][idx]\n        tcl_mask = output['tcl_mask'][idx]\n\n        polygons_gt = output['polygons_gt'][idx]\n        contours_gt = output['contours_gt'][idx]\n\n        contours_gt_im = np.zeros_like(img_show)\n        for contour in contours_gt:\n            cv2.fillPoly(\n                contours_gt_im, np.array([contour], 'int32'),\n                (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\n            )\n\n        gt_vis = self.visualize_detection(img_show, tr_mask, tcl_mask, polygons_gt)\n        gt_vis = np.concatenate([gt_vis, contours_gt_im], axis=1)\n\n        return gt_vis\n\n    def pred_get_image(self, idx, output):\n        img = output['image'][idx]\n\n        tr_pred = output['tr_pred'][idx]\n        tcl_pred = output['tcl_pred'][idx]\n\n        # visualization\n        img_show = img.copy()\n        contours = output['contours_pred'][idx]\n        contours_im = np.zeros_like(img_show)\n        for contour in contours:\n            cv2.fillPoly(\n                contours_im, np.array([contour], 'int32'),\n                (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\n            )\n\n        predict_vis = self.visualize_detection(img_show, tr_pred, tcl_pred, contours)\n        predict_vis = np.concatenate([predict_vis, contours_im], axis=1)\n\n        return predict_vis\n\n    def get_image(self, idx, output):\n        gt_vis = self.gt_get_image(idx, output)\n        predict_vis = self.pred_get_image(idx, output)\n        return np.concatenate([predict_vis, gt_vis], axis=0)\n\n    def visualize_detection(self, image, tr, tcl, contours):\n        image_show = image.copy()\n        for contour in contours:\n            image_show = cv2.polylines(image_show, np.array([contour], 'int32'), True, (0, 0, 255), 2)\n        tr = cv2.cvtColor((tr * 255).astype('uint8'), cv2.COLOR_GRAY2BGR)\n        tcl = cv2.cvtColor((tcl * 255).astype('uint8'), cv2.COLOR_GRAY2BGR)\n        image_show = np.concatenate([image_show, tr, tcl], axis=1)\n        return image_show\n\n    def visualize(self, batch, output, interested):\n        images = {}\n        for i in range(min(self.vis_num, len(output['image']))):\n            show = self.get_image(i, output)\n            images['image_%d' % i] = show.astype(np.uint8)\n        return images\n\n    def visualize_batch(self, batch, output):\n        images = {}\n        for i in range(len(output['image'])):\n            show = self.gt_get_image(i, output)\n            train_mask = cv2.cvtColor((output['train_mask'][i] * 255).astype('uint8'), cv2.COLOR_GRAY2BGR)\n            show = np.concatenate([show, train_mask], axis=1)\n            images['image_%d' % i] = show.astype(np.uint8)\n        return images\n"""
assets/ops/dcn/__init__.py,0,"b""from .functions.deform_conv import deform_conv, modulated_deform_conv\nfrom .functions.deform_pool import deform_roi_pooling\nfrom .modules.deform_conv import (DeformConv, ModulatedDeformConv,\n                                  DeformConvPack, ModulatedDeformConvPack)\nfrom .modules.deform_pool import (DeformRoIPooling, DeformRoIPoolingPack,\n                                  ModulatedDeformRoIPoolingPack)\n\n__all__ = [\n    'DeformConv', 'DeformConvPack', 'ModulatedDeformConv',\n    'ModulatedDeformConvPack', 'DeformRoIPooling', 'DeformRoIPoolingPack',\n    'ModulatedDeformRoIPoolingPack', 'deform_conv', 'modulated_deform_conv',\n    'deform_roi_pooling'\n]\n"""
assets/ops/dcn/setup.py,1,"b""from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name='deform_conv',\n    ext_modules=[\n        CUDAExtension('deform_conv_cuda', [\n            'src/deform_conv_cuda.cpp',\n            'src/deform_conv_cuda_kernel.cu',\n        ]),\n        CUDAExtension('deform_pool_cuda', [\n            'src/deform_pool_cuda.cpp', 'src/deform_pool_cuda_kernel.cu'\n        ]),\n    ],\n    cmdclass={'build_ext': BuildExtension})\n"""
concern/icdar2015_eval/detection/__init__.py,0,b''
concern/icdar2015_eval/detection/deteval.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport math\nfrom collections import namedtuple\nimport numpy as np\nfrom shapely.geometry import Polygon\n\n\nclass DetectionDetEvalEvaluator(object):\n    def __init__(\n        self,\n        area_recall_constraint=0.8, area_precision_constraint=0.4,\n        ev_param_ind_center_diff_thr=1,\n        mtype_oo_o=1.0, mtype_om_o=0.8, mtype_om_m=1.0\n    ):\n\n\n        self.area_recall_constraint = area_recall_constraint\n        self.area_precision_constraint = area_precision_constraint\n        self.ev_param_ind_center_diff_thr = ev_param_ind_center_diff_thr\n        self.mtype_oo_o = mtype_oo_o\n        self.mtype_om_o = mtype_om_o\n        self.mtype_om_m = mtype_om_m\n\n    def evaluate_image(self, gt, pred):\n\n        def get_union(pD,pG):\n            return Polygon(pD).union(Polygon(pG)).area\n\n        def get_intersection_over_union(pD,pG):\n            return get_intersection(pD, pG) / get_union(pD, pG)\n\n        def get_intersection(pD,pG):\n            return Polygon(pD).intersection(Polygon(pG)).area\n\n        def one_to_one_match(row, col):\n            cont = 0\n            for j in range(len(recallMat[0])):    \n                if recallMat[row,j] >= self.area_recall_constraint and precisionMat[row,j] >= self.area_precision_constraint:\n                    cont = cont +1\n            if (cont != 1):\n                return False\n            cont = 0\n            for i in range(len(recallMat)):    \n                if recallMat[i,col] >= self.area_recall_constraint and precisionMat[i,col] >= self.area_precision_constraint:\n                    cont = cont +1\n            if (cont != 1):\n                return False\n            \n            if recallMat[row,col] >= self.area_recall_constraint and precisionMat[row,col] >= self.area_precision_constraint:\n                return True\n            return False\n        \n        def num_overlaps_gt(gtNum):\n            cont = 0\n            for detNum in range(len(detRects)):\n                if detNum not in detDontCareRectsNum:\n                    if recallMat[gtNum,detNum] > 0 :\n                        cont = cont +1\n            return cont\n\n        def num_overlaps_det(detNum):\n            cont = 0\n            for gtNum in range(len(recallMat)):    \n                if gtNum not in gtDontCareRectsNum:\n                    if recallMat[gtNum,detNum] > 0 :\n                        cont = cont +1\n            return cont\n        \n        def is_single_overlap(row, col):\n            if num_overlaps_gt(row)==1 and num_overlaps_det(col)==1:\n                return True\n            else:\n                return False\n        \n        def one_to_many_match(gtNum):\n            many_sum = 0\n            detRects = []\n            for detNum in range(len(recallMat[0])):    \n                if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and detNum not in detDontCareRectsNum:\n                    if precisionMat[gtNum,detNum] >= self.area_precision_constraint:\n                        many_sum += recallMat[gtNum,detNum]\n                        detRects.append(detNum)\n            if round(many_sum,4) >= self.area_recall_constraint:\n                return True,detRects\n            else:\n                return False,[]         \n        \n        def many_to_one_match(detNum):\n            many_sum = 0\n            gtRects = []\n            for gtNum in range(len(recallMat)):    \n                if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and gtNum not in gtDontCareRectsNum:\n                    if recallMat[gtNum,detNum] >= self.area_recall_constraint:\n                        many_sum += precisionMat[gtNum,detNum]\n                        gtRects.append(gtNum)\n            if round(many_sum,4) >= self.area_precision_constraint:\n                return True,gtRects\n            else:\n                return False,[]\n        \n        def center_distance(r1, r2):\n            return ((np.mean(r1, axis=0) - np.mean(r2, axis=0)) ** 2).sum() ** 0.5\n        \n        def diag(r):\n            r = np.array(r)\n            return ((r[:, 0].max() - r[:, 0].min()) ** 2 + (r[:, 1].max() - r[:, 1].min()) ** 2) ** 0.5\n        \n        perSampleMetrics = {}\n        \n        recall = 0\n        precision = 0\n        hmean = 0        \n        recallAccum = 0.\n        precisionAccum = 0.\n        gtRects = []\n        detRects = []\n        gtPolPoints = []\n        detPolPoints = []\n        gtDontCareRectsNum = []#Array of Ground Truth Rectangles\' keys marked as don\'t Care\n        detDontCareRectsNum = []#Array of Detected Rectangles\' matched with a don\'t Care GT\n        pairs = []\n        evaluationLog = """"\n        \n        recallMat = np.empty([1,1])\n        precisionMat = np.empty([1,1])              \n        \n        for n in range(len(gt)):\n            points = gt[n][\'points\']\n            # transcription = gt[n][\'text\']\n            dontCare = gt[n][\'ignore\']\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            gtRects.append(points)\n            gtPolPoints.append(points)\n            if dontCare:\n                gtDontCareRectsNum.append( len(gtRects)-1 )                 \n        \n        evaluationLog += ""GT rectangles: "" + str(len(gtRects)) + ("" ("" + str(len(gtDontCareRectsNum)) + "" don\'t care)\\n"" if len(gtDontCareRectsNum)>0 else ""\\n"")\n        \n        for n in range(len(pred)):\n            points = pred[n][\'points\']\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            detRect = points\n            detRects.append(detRect)\n            detPolPoints.append(points)\n            if len(gtDontCareRectsNum)>0 :\n                for dontCareRectNum in gtDontCareRectsNum:\n                    dontCareRect = gtRects[dontCareRectNum]\n                    intersected_area = get_intersection(dontCareRect,detRect)\n                    rdDimensions = Polygon(detRect).area\n                    if (rdDimensions==0) :\n                        precision = 0\n                    else:\n                        precision= intersected_area / rdDimensions\n                    if (precision > self.area_precision_constraint):\n                        detDontCareRectsNum.append( len(detRects)-1 )\n                        break\n\n        evaluationLog += ""DET rectangles: "" + str(len(detRects)) + ("" ("" + str(len(detDontCareRectsNum)) + "" don\'t care)\\n"" if len(detDontCareRectsNum)>0 else ""\\n"")\n\n        if len(gtRects)==0:\n            recall = 1\n            precision = 0 if len(detRects)>0 else 1\n\n        if len(detRects)>0:\n            #Calculate recall and precision matrixs\n            outputShape=[len(gtRects),len(detRects)]\n            recallMat = np.empty(outputShape)\n            precisionMat = np.empty(outputShape)\n            gtRectMat = np.zeros(len(gtRects),np.int8)\n            detRectMat = np.zeros(len(detRects),np.int8)\n            for gtNum in range(len(gtRects)):\n                for detNum in range(len(detRects)):\n                    rG = gtRects[gtNum]\n                    rD = detRects[detNum]\n                    intersected_area = get_intersection(rG,rD)\n                    rgDimensions = Polygon(rG).area\n                    rdDimensions = Polygon(rD).area\n                    recallMat[gtNum,detNum] = 0 if rgDimensions==0 else  intersected_area / rgDimensions\n                    precisionMat[gtNum,detNum] = 0 if rdDimensions==0 else intersected_area / rdDimensions\n\n            # Find one-to-one matches\n            evaluationLog += ""Find one-to-one matches\\n""\n            for gtNum in range(len(gtRects)):\n                for detNum in range(len(detRects)):\n                    if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and gtNum not in gtDontCareRectsNum and detNum not in detDontCareRectsNum :\n                        match = one_to_one_match(gtNum, detNum)\n                        if match is True :\n                            #in deteval we have to make other validation before mark as one-to-one\n                            if is_single_overlap(gtNum, detNum) is True :\n                                rG = gtRects[gtNum]\n                                rD = detRects[detNum]\n                                normDist = center_distance(rG, rD);\n                                normDist /= diag(rG) + diag(rD);\n                                normDist *= 2.0;\n                                if normDist < self.ev_param_ind_center_diff_thr:\n                                    gtRectMat[gtNum] = 1\n                                    detRectMat[detNum] = 1\n                                    recallAccum += self.mtype_oo_o\n                                    precisionAccum += self.mtype_oo_o\n                                    pairs.append({\'gt\':gtNum,\'det\':detNum,\'type\':\'OO\'})\n                                    evaluationLog += ""Match GT #"" + str(gtNum) + "" with Det #"" + str(detNum) + ""\\n""\n                                else:\n                                    evaluationLog += ""Match Discarded GT #"" + str(gtNum) + "" with Det #"" + str(detNum) + "" normDist: "" + str(normDist) + "" \\n""\n                            else:\n                                evaluationLog += ""Match Discarded GT #"" + str(gtNum) + "" with Det #"" + str(detNum) + "" not single overlap\\n""\n            # Find one-to-many matches\n            evaluationLog += ""Find one-to-many matches\\n""\n            for gtNum in range(len(gtRects)):\n                if gtNum not in gtDontCareRectsNum:\n                    match,matchesDet = one_to_many_match(gtNum)\n                    if match is True :\n                        evaluationLog += ""num_overlaps_gt="" + str(num_overlaps_gt(gtNum))\n                        #in deteval we have to make other validation before mark as one-to-one\n                        if num_overlaps_gt(gtNum)>=2 :\n                            gtRectMat[gtNum] = 1\n                            recallAccum += (self.mtype_oo_o if len(matchesDet)==1 else self.mtype_om_o)\n                            precisionAccum += (self.mtype_oo_o if len(matchesDet)==1 else self.mtype_om_o*len(matchesDet))\n                            pairs.append({\'gt\':gtNum,\'det\':matchesDet,\'type\': \'OO\' if len(matchesDet)==1 else \'OM\'})\n                            for detNum in matchesDet :\n                                detRectMat[detNum] = 1\n                            evaluationLog += ""Match GT #"" + str(gtNum) + "" with Det #"" + str(matchesDet) + ""\\n""\n                        else:\n                            evaluationLog += ""Match Discarded GT #"" + str(gtNum) + "" with Det #"" + str(matchesDet) + "" not single overlap\\n""    \n\n            # Find many-to-one matches\n            evaluationLog += ""Find many-to-one matches\\n""\n            for detNum in range(len(detRects)):\n                if detNum not in detDontCareRectsNum:\n                    match,matchesGt = many_to_one_match(detNum)\n                    if match is True :\n                        #in deteval we have to make other validation before mark as one-to-one\n                        if num_overlaps_det(detNum)>=2 :                          \n                            detRectMat[detNum] = 1\n                            recallAccum += (self.mtype_oo_o if len(matchesGt)==1 else self.mtype_om_m*len(matchesGt))\n                            precisionAccum += (self.mtype_oo_o if len(matchesGt)==1 else self.mtype_om_m)\n                            pairs.append({\'gt\':matchesGt,\'det\':detNum,\'type\': \'OO\' if len(matchesGt)==1 else \'MO\'})\n                            for gtNum in matchesGt :\n                                gtRectMat[gtNum] = 1\n                            evaluationLog += ""Match GT #"" + str(matchesGt) + "" with Det #"" + str(detNum) + ""\\n""\n                        else:\n                            evaluationLog += ""Match Discarded GT #"" + str(matchesGt) + "" with Det #"" + str(detNum) + "" not single overlap\\n""                                    \n\n            numGtCare = (len(gtRects) - len(gtDontCareRectsNum))\n            if numGtCare == 0:\n                recall = float(1)\n                precision = float(0) if len(detRects)>0 else float(1)\n            else:\n                recall = float(recallAccum) / numGtCare\n                precision =  float(0) if (len(detRects) - len(detDontCareRectsNum))==0 else float(precisionAccum) / (len(detRects) - len(detDontCareRectsNum))\n            hmean = 0 if (precision + recall)==0 else 2.0 * precision * recall / (precision + recall)  \n\n        numGtCare = len(gtRects) - len(gtDontCareRectsNum)\n        numDetCare = len(detRects) - len(detDontCareRectsNum)\n\n        perSampleMetrics = {\n            \'precision\':precision,\n            \'recall\':recall,\n            \'hmean\':hmean,\n            \'pairs\':pairs,\n            \'recallMat\':[] if len(detRects)>100 else recallMat.tolist(),\n            \'precisionMat\':[] if len(detRects)>100 else precisionMat.tolist(),\n            \'gtPolPoints\':gtPolPoints,\n            \'detPolPoints\':detPolPoints,\n            \'gtCare\': numGtCare,\n            \'detCare\': numDetCare,\n            \'gtDontCare\':gtDontCareRectsNum,\n            \'detDontCare\':detDontCareRectsNum,\n            \'recallAccum\':recallAccum,\n            \'precisionAccum\':precisionAccum,\n            \'evaluationLog\': evaluationLog\n        }\n\n        return perSampleMetrics\n\n    def combine_results(self, results):\n        numGt = 0\n        numDet = 0\n        methodRecallSum = 0\n        methodPrecisionSum = 0\n\n        for result in results:\n            numGt += result[\'gtCare\']\n            numDet += result[\'detCare\']\n            methodRecallSum += result[\'recallAccum\']\n            methodPrecisionSum += result[\'precisionAccum\']\n\n        methodRecall = 0 if numGt==0 else methodRecallSum/numGt\n        methodPrecision = 0 if numDet==0 else methodPrecisionSum/numDet\n        methodHmean = 0 if methodRecall + methodPrecision==0 else 2* methodRecall * methodPrecision / (methodRecall + methodPrecision)\n        \n        methodMetrics = {\'precision\':methodPrecision, \'recall\':methodRecall,\'hmean\': methodHmean  }\n\n        return methodMetrics\n\n\nif __name__==\'__main__\':\n    evaluator = DetectionDetEvalEvaluator()\n    gts = [[{\n        \'points\': [(0, 0), (1, 0), (1, 1), (0, 1)],\n        \'text\': 1234,\n        \'ignore\': False,\n    }, {\n        \'points\': [(2, 2), (3, 2), (3, 3), (2, 3)],\n        \'text\': 5678,\n        \'ignore\': True,\n    }]]\n    preds = [[{\n        \'points\': [(0.1, 0.1), (1, 0), (1, 1), (0, 1)],\n        \'text\': 123,\n        \'ignore\': False,\n    }]]\n    results = []\n    for gt, pred in zip(gts, preds):\n        results.append(evaluator.evaluate_image(gt, pred))\n    metrics = evaluator.combine_results(results)\n    print(metrics)\n'"
concern/icdar2015_eval/detection/icdar2013.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport math\nfrom collections import namedtuple\nimport numpy as np\nfrom shapely.geometry import Polygon\n\n\nclass DetectionICDAR2013Evaluator(object):\n    def __init__(\n        self,\n        area_recall_constraint=0.8, area_precision_constraint=0.4,\n        ev_param_ind_center_diff_thr=1,\n        mtype_oo_o=1.0, mtype_om_o=0.8, mtype_om_m=1.0\n    ):\n\n\n        self.area_recall_constraint = area_recall_constraint\n        self.area_precision_constraint = area_precision_constraint\n        self.ev_param_ind_center_diff_thr = ev_param_ind_center_diff_thr\n        self.mtype_oo_o = mtype_oo_o\n        self.mtype_om_o = mtype_om_o\n        self.mtype_om_m = mtype_om_m\n\n    def evaluate_image(self, gt, pred):\n\n        def get_union(pD,pG):\n            return Polygon(pD).union(Polygon(pG)).area\n\n        def get_intersection_over_union(pD,pG):\n            return get_intersection(pD, pG) / get_union(pD, pG)\n\n        def get_intersection(pD,pG):\n            return Polygon(pD).intersection(Polygon(pG)).area\n\n        def one_to_one_match(row, col):\n            cont = 0\n            for j in range(len(recallMat[0])):    \n                if recallMat[row,j] >= self.area_recall_constraint and precisionMat[row,j] >= self.area_precision_constraint:\n                    cont = cont +1\n            if (cont != 1):\n                return False\n            cont = 0\n            for i in range(len(recallMat)):    \n                if recallMat[i,col] >= self.area_recall_constraint and precisionMat[i,col] >= self.area_precision_constraint:\n                    cont = cont +1\n            if (cont != 1):\n                return False\n            \n            if recallMat[row,col] >= self.area_recall_constraint and precisionMat[row,col] >= self.area_precision_constraint:\n                return True\n            return False\n        \n        def one_to_many_match(gtNum):\n            many_sum = 0\n            detRects = []\n            for detNum in range(len(recallMat[0])):    \n                if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and detNum not in detDontCareRectsNum:\n                    if precisionMat[gtNum,detNum] >= self.area_precision_constraint:\n                        many_sum += recallMat[gtNum,detNum]\n                        detRects.append(detNum)\n            if round(many_sum,4) >= self.area_recall_constraint:\n                return True,detRects\n            else:\n                return False,[]         \n        \n        def many_to_one_match(detNum):\n            many_sum = 0\n            gtRects = []\n            for gtNum in range(len(recallMat)):    \n                if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and gtNum not in gtDontCareRectsNum:\n                    if recallMat[gtNum,detNum] >= self.area_recall_constraint:\n                        many_sum += precisionMat[gtNum,detNum]\n                        gtRects.append(gtNum)\n            if round(many_sum,4) >= self.area_precision_constraint:\n                return True,gtRects\n            else:\n                return False,[]\n        \n        def center_distance(r1, r2):\n            return ((np.mean(r1, axis=0) - np.mean(r2, axis=0)) ** 2).sum() ** 0.5\n        \n        def diag(r):\n            r = np.array(r)\n            return ((r[:, 0].max() - r[:, 0].min()) ** 2 + (r[:, 1].max() - r[:, 1].min()) ** 2) ** 0.5\n        \n        perSampleMetrics = {}\n        \n        recall = 0\n        precision = 0\n        hmean = 0        \n        recallAccum = 0.\n        precisionAccum = 0.\n        gtRects = []\n        detRects = []\n        gtPolPoints = []\n        detPolPoints = []\n        gtDontCareRectsNum = []#Array of Ground Truth Rectangles\' keys marked as don\'t Care\n        detDontCareRectsNum = []#Array of Detected Rectangles\' matched with a don\'t Care GT\n        pairs = []\n        evaluationLog = """"\n        \n        recallMat = np.empty([1,1])\n        precisionMat = np.empty([1,1])              \n        \n        for n in range(len(gt)):\n            points = gt[n][\'points\']\n            # transcription = gt[n][\'text\']\n            dontCare = gt[n][\'ignore\']\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            gtRects.append(points)\n            gtPolPoints.append(points)\n            if dontCare:\n                gtDontCareRectsNum.append( len(gtRects)-1 )                 \n        \n        evaluationLog += ""GT rectangles: "" + str(len(gtRects)) + ("" ("" + str(len(gtDontCareRectsNum)) + "" don\'t care)\\n"" if len(gtDontCareRectsNum)>0 else ""\\n"")\n        \n        for n in range(len(pred)):\n            points = pred[n][\'points\']\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            detRect = points\n            detRects.append(detRect)\n            detPolPoints.append(points)\n            if len(gtDontCareRectsNum)>0 :\n                for dontCareRectNum in gtDontCareRectsNum:\n                    dontCareRect = gtRects[dontCareRectNum]\n                    intersected_area = get_intersection(dontCareRect,detRect)\n                    rdDimensions = Polygon(detRect).area\n                    if (rdDimensions==0) :\n                        precision = 0\n                    else:\n                        precision= intersected_area / rdDimensions\n                    if (precision > self.area_precision_constraint):\n                        detDontCareRectsNum.append( len(detRects)-1 )\n                        break\n\n        evaluationLog += ""DET rectangles: "" + str(len(detRects)) + ("" ("" + str(len(detDontCareRectsNum)) + "" don\'t care)\\n"" if len(detDontCareRectsNum)>0 else ""\\n"")\n\n        if len(gtRects)==0:\n            recall = 1\n            precision = 0 if len(detRects)>0 else 1\n\n        if len(detRects)>0:\n            #Calculate recall and precision matrixs\n            outputShape=[len(gtRects),len(detRects)]\n            recallMat = np.empty(outputShape)\n            precisionMat = np.empty(outputShape)\n            gtRectMat = np.zeros(len(gtRects),np.int8)\n            detRectMat = np.zeros(len(detRects),np.int8)\n            for gtNum in range(len(gtRects)):\n                for detNum in range(len(detRects)):\n                    rG = gtRects[gtNum]\n                    rD = detRects[detNum]\n                    intersected_area = get_intersection(rG,rD)\n                    rgDimensions = Polygon(rG).area\n                    rdDimensions = Polygon(rD).area\n                    recallMat[gtNum,detNum] = 0 if rgDimensions==0 else  intersected_area / rgDimensions\n                    precisionMat[gtNum,detNum] = 0 if rdDimensions==0 else intersected_area / rdDimensions\n\n            # Find one-to-one matches\n            evaluationLog += ""Find one-to-one matches\\n""\n            for gtNum in range(len(gtRects)):\n                for detNum in range(len(detRects)):\n                    if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and gtNum not in gtDontCareRectsNum and detNum not in detDontCareRectsNum :\n                        match = one_to_one_match(gtNum, detNum)\n                        if match is True :\n                            #in deteval we have to make other validation before mark as one-to-one\n                            rG = gtRects[gtNum]\n                            rD = detRects[detNum]\n                            normDist = center_distance(rG, rD);\n                            normDist /= diag(rG) + diag(rD);\n                            normDist *= 2.0;\n                            if normDist < self.ev_param_ind_center_diff_thr:\n                                gtRectMat[gtNum] = 1\n                                detRectMat[detNum] = 1\n                                recallAccum += self.mtype_oo_o\n                                precisionAccum += self.mtype_oo_o\n                                pairs.append({\'gt\':gtNum,\'det\':detNum,\'type\':\'OO\'})\n                                evaluationLog += ""Match GT #"" + str(gtNum) + "" with Det #"" + str(detNum) + ""\\n""\n                            else:\n                                evaluationLog += ""Match Discarded GT #"" + str(gtNum) + "" with Det #"" + str(detNum) + "" normDist: "" + str(normDist) + "" \\n""\n            # Find one-to-many matches\n            evaluationLog += ""Find one-to-many matches\\n""\n            for gtNum in range(len(gtRects)):\n                if gtNum not in gtDontCareRectsNum:\n                    match,matchesDet = one_to_many_match(gtNum)\n                    if match is True :\n                        evaluationLog += ""num_overlaps_gt="" + str(num_overlaps_gt(gtNum))\n                        gtRectMat[gtNum] = 1\n                        recallAccum += (self.mtype_oo_o if len(matchesDet)==1 else self.mtype_om_o)\n                        precisionAccum += (self.mtype_oo_o if len(matchesDet)==1 else self.mtype_om_o*len(matchesDet))\n                        pairs.append({\'gt\':gtNum,\'det\':matchesDet,\'type\': \'OO\' if len(matchesDet)==1 else \'OM\'})\n                        for detNum in matchesDet :\n                            detRectMat[detNum] = 1\n                        evaluationLog += ""Match GT #"" + str(gtNum) + "" with Det #"" + str(matchesDet) + ""\\n""\n\n            # Find many-to-one matches\n            evaluationLog += ""Find many-to-one matches\\n""\n            for detNum in range(len(detRects)):\n                if detNum not in detDontCareRectsNum:\n                    match,matchesGt = many_to_one_match(detNum)\n                    if match is True :\n                        detRectMat[detNum] = 1\n                        recallAccum += (self.mtype_oo_o if len(matchesGt)==1 else self.mtype_om_m*len(matchesGt))\n                        precisionAccum += (self.mtype_oo_o if len(matchesGt)==1 else self.mtype_om_m)\n                        pairs.append({\'gt\':matchesGt,\'det\':detNum,\'type\': \'OO\' if len(matchesGt)==1 else \'MO\'})\n                        for gtNum in matchesGt :\n                            gtRectMat[gtNum] = 1\n                        evaluationLog += ""Match GT #"" + str(matchesGt) + "" with Det #"" + str(detNum) + ""\\n""\n\n            numGtCare = (len(gtRects) - len(gtDontCareRectsNum))\n            if numGtCare == 0:\n                recall = float(1)\n                precision = float(0) if len(detRects)>0 else float(1)\n            else:\n                recall = float(recallAccum) / numGtCare\n                precision =  float(0) if (len(detRects) - len(detDontCareRectsNum))==0 else float(precisionAccum) / (len(detRects) - len(detDontCareRectsNum))\n            hmean = 0 if (precision + recall)==0 else 2.0 * precision * recall / (precision + recall)  \n\n        numGtCare = len(gtRects) - len(gtDontCareRectsNum)\n        numDetCare = len(detRects) - len(detDontCareRectsNum)\n\n        perSampleMetrics = {\n            \'precision\':precision,\n            \'recall\':recall,\n            \'hmean\':hmean,\n            \'pairs\':pairs,\n            \'recallMat\':[] if len(detRects)>100 else recallMat.tolist(),\n            \'precisionMat\':[] if len(detRects)>100 else precisionMat.tolist(),\n            \'gtPolPoints\':gtPolPoints,\n            \'detPolPoints\':detPolPoints,\n            \'gtCare\': numGtCare,\n            \'detCare\': numDetCare,\n            \'gtDontCare\':gtDontCareRectsNum,\n            \'detDontCare\':detDontCareRectsNum,\n            \'recallAccum\':recallAccum,\n            \'precisionAccum\':precisionAccum,\n            \'evaluationLog\': evaluationLog\n        }\n\n        return perSampleMetrics\n\n    def combine_results(self, results):\n        numGt = 0\n        numDet = 0\n        methodRecallSum = 0\n        methodPrecisionSum = 0\n\n        for result in results:\n            numGt += result[\'gtCare\']\n            numDet += result[\'detCare\']\n            methodRecallSum += result[\'recallAccum\']\n            methodPrecisionSum += result[\'precisionAccum\']\n\n        methodRecall = 0 if numGt==0 else methodRecallSum/numGt\n        methodPrecision = 0 if numDet==0 else methodPrecisionSum/numDet\n        methodHmean = 0 if methodRecall + methodPrecision==0 else 2* methodRecall * methodPrecision / (methodRecall + methodPrecision)\n        \n        methodMetrics = {\'precision\':methodPrecision, \'recall\':methodRecall,\'hmean\': methodHmean  }\n\n        return methodMetrics\n\n\nif __name__==\'__main__\':\n    evaluator = DetectionICDAR2013Evaluator()\n    gts = [[{\n        \'points\': [(0, 0), (1, 0), (1, 1), (0, 1)],\n        \'text\': 1234,\n        \'ignore\': False,\n    }, {\n        \'points\': [(2, 2), (3, 2), (3, 3), (2, 3)],\n        \'text\': 5678,\n        \'ignore\': True,\n    }]]\n    preds = [[{\n        \'points\': [(0.1, 0.1), (1, 0), (1, 1), (0, 1)],\n        \'text\': 123,\n        \'ignore\': False,\n    }]]\n    results = []\n    for gt, pred in zip(gts, preds):\n        results.append(evaluator.evaluate_image(gt, pred))\n    metrics = evaluator.combine_results(results)\n    print(metrics)\n'"
concern/icdar2015_eval/detection/iou.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom collections import namedtuple\nimport numpy as np\nfrom shapely.geometry import Polygon\n\n\nclass DetectionIoUEvaluator(object):\n    def __init__(self, iou_constraint=0.5, area_precision_constraint=0.5):\n        self.iou_constraint = iou_constraint\n        self.area_precision_constraint = area_precision_constraint\n\n    def evaluate_image(self, gt, pred):\n\n        def get_union(pD, pG):\n            return Polygon(pD).union(Polygon(pG)).area\n\n        def get_intersection_over_union(pD, pG):\n            return get_intersection(pD, pG) / get_union(pD, pG)\n\n        def get_intersection(pD, pG):\n            return Polygon(pD).intersection(Polygon(pG)).area\n\n        def compute_ap(confList, matchList, numGtCare):\n            correct = 0\n            AP = 0\n            if len(confList) > 0:\n                confList = np.array(confList)\n                matchList = np.array(matchList)\n                sorted_ind = np.argsort(-confList)\n                confList = confList[sorted_ind]\n                matchList = matchList[sorted_ind]\n                for n in range(len(confList)):\n                    match = matchList[n]\n                    if match:\n                        correct += 1\n                        AP += float(correct)/(n + 1)\n\n                if numGtCare > 0:\n                    AP /= numGtCare\n\n            return AP\n\n        perSampleMetrics = {}\n\n        matchedSum = 0\n\n        Rectangle = namedtuple(\'Rectangle\', \'xmin ymin xmax ymax\')\n\n        numGlobalCareGt = 0\n        numGlobalCareDet = 0\n\n        arrGlobalConfidences = []\n        arrGlobalMatches = []\n\n        recall = 0\n        precision = 0\n        hmean = 0\n\n        detMatched = 0\n\n        iouMat = np.empty([1, 1])\n\n        gtPols = []\n        detPols = []\n\n        gtPolPoints = []\n        detPolPoints = []\n\n        # Array of Ground Truth Polygons\' keys marked as don\'t Care\n        gtDontCarePolsNum = []\n        # Array of Detected Polygons\' matched with a don\'t Care GT\n        detDontCarePolsNum = []\n\n        pairs = []\n        detMatchedNums = []\n\n        arrSampleConfidences = []\n        arrSampleMatch = []\n\n        evaluationLog = """"\n\n        for n in range(len(gt)):\n            points = gt[n][\'points\']\n            # transcription = gt[n][\'text\']\n            dontCare = gt[n][\'ignore\']\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            gtPol = points\n            gtPols.append(gtPol)\n            gtPolPoints.append(points)\n            if dontCare:\n                gtDontCarePolsNum.append(len(gtPols)-1)\n\n        evaluationLog += ""GT polygons: "" + str(len(gtPols)) + ("" ("" + str(len(\n            gtDontCarePolsNum)) + "" don\'t care)\\n"" if len(gtDontCarePolsNum) > 0 else ""\\n"")\n\n        for n in range(len(pred)):\n            points = pred[n][\'points\']\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            detPol = points\n            detPols.append(detPol)\n            detPolPoints.append(points)\n            if len(gtDontCarePolsNum) > 0:\n                for dontCarePol in gtDontCarePolsNum:\n                    dontCarePol = gtPols[dontCarePol]\n                    intersected_area = get_intersection(dontCarePol, detPol)\n                    pdDimensions = Polygon(detPol).area\n                    precision = 0 if pdDimensions == 0 else intersected_area / pdDimensions\n                    if (precision > self.area_precision_constraint):\n                        detDontCarePolsNum.append(len(detPols)-1)\n                        break\n\n        evaluationLog += ""DET polygons: "" + str(len(detPols)) + ("" ("" + str(len(\n            detDontCarePolsNum)) + "" don\'t care)\\n"" if len(detDontCarePolsNum) > 0 else ""\\n"")\n\n        if len(gtPols) > 0 and len(detPols) > 0:\n            # Calculate IoU and precision matrixs\n            outputShape = [len(gtPols), len(detPols)]\n            iouMat = np.empty(outputShape)\n            gtRectMat = np.zeros(len(gtPols), np.int8)\n            detRectMat = np.zeros(len(detPols), np.int8)\n            for gtNum in range(len(gtPols)):\n                for detNum in range(len(detPols)):\n                    pG = gtPols[gtNum]\n                    pD = detPols[detNum]\n                    iouMat[gtNum, detNum] = get_intersection_over_union(pD, pG)\n\n            for gtNum in range(len(gtPols)):\n                for detNum in range(len(detPols)):\n                    if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and gtNum not in gtDontCarePolsNum and detNum not in detDontCarePolsNum:\n                        if iouMat[gtNum, detNum] > self.iou_constraint:\n                            gtRectMat[gtNum] = 1\n                            detRectMat[detNum] = 1\n                            detMatched += 1\n                            pairs.append({\'gt\': gtNum, \'det\': detNum})\n                            detMatchedNums.append(detNum)\n                            evaluationLog += ""Match GT #"" + \\\n                                str(gtNum) + "" with Det #"" + str(detNum) + ""\\n""\n\n        numGtCare = (len(gtPols) - len(gtDontCarePolsNum))\n        numDetCare = (len(detPols) - len(detDontCarePolsNum))\n        if numGtCare == 0:\n            recall = float(1)\n            precision = float(0) if numDetCare > 0 else float(1)\n        else:\n            recall = float(detMatched) / numGtCare\n            precision = 0 if numDetCare == 0 else float(\n                detMatched) / numDetCare\n\n        hmean = 0 if (precision + recall) == 0 else 2.0 * \\\n            precision * recall / (precision + recall)\n\n        matchedSum += detMatched\n        numGlobalCareGt += numGtCare\n        numGlobalCareDet += numDetCare\n\n        perSampleMetrics = {\n            \'precision\': precision,\n            \'recall\': recall,\n            \'hmean\': hmean,\n            \'pairs\': pairs,\n            \'iouMat\': [] if len(detPols) > 100 else iouMat.tolist(),\n            \'gtPolPoints\': gtPolPoints,\n            \'detPolPoints\': detPolPoints,\n            \'gtCare\': numGtCare,\n            \'detCare\': numDetCare,\n            \'gtDontCare\': gtDontCarePolsNum,\n            \'detDontCare\': detDontCarePolsNum,\n            \'detMatched\': detMatched,\n            \'evaluationLog\': evaluationLog\n        }\n\n        return perSampleMetrics\n\n    def combine_results(self, results):\n        numGlobalCareGt = 0\n        numGlobalCareDet = 0\n        matchedSum = 0\n        for result in results:\n            numGlobalCareGt += result[\'gtCare\']\n            numGlobalCareDet += result[\'detCare\']\n            matchedSum += result[\'detMatched\']\n\n        methodRecall = 0 if numGlobalCareGt == 0 else float(\n            matchedSum)/numGlobalCareGt\n        methodPrecision = 0 if numGlobalCareDet == 0 else float(\n            matchedSum)/numGlobalCareDet\n        methodHmean = 0 if methodRecall + methodPrecision == 0 else 2 * \\\n            methodRecall * methodPrecision / (methodRecall + methodPrecision)\n\n        methodMetrics = {\'precision\': methodPrecision,\n                         \'recall\': methodRecall, \'hmean\': methodHmean}\n\n        return methodMetrics\n\n\nif __name__ == \'__main__\':\n    evaluator = DetectionIoUEvaluator()\n    gts = [[{\n        \'points\': [(0, 0), (1, 0), (1, 1), (0, 1)],\n        \'text\': 1234,\n        \'ignore\': False,\n    }, {\n        \'points\': [(2, 2), (3, 2), (3, 3), (2, 3)],\n        \'text\': 5678,\n        \'ignore\': False,\n    }]]\n    preds = [[{\n        \'points\': [(0.1, 0.1), (1, 0), (1, 1), (0, 1)],\n        \'text\': 123,\n        \'ignore\': False,\n    }]]\n    results = []\n    for gt, pred in zip(gts, preds):\n        results.append(evaluator.evaluate_image(gt, pred))\n    metrics = evaluator.combine_results(results)\n    print(metrics)\n'"
concern/icdar2015_eval/detection/mtwi2018.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport math\nfrom collections import namedtuple\nimport numpy as np\nfrom shapely.geometry import Polygon\n\n\nclass DetectionMTWI2018Evaluator(object):\n    def __init__(\n        self,\n        area_recall_constraint=0.7, area_precision_constraint=0.7,\n        ev_param_ind_center_diff_thr=1,\n    ):\n\n\n        self.area_recall_constraint = area_recall_constraint\n        self.area_precision_constraint = area_precision_constraint\n        self.ev_param_ind_center_diff_thr = ev_param_ind_center_diff_thr\n\n    def evaluate_image(self, gt, pred):\n\n        def get_union(pD,pG):\n            return Polygon(pD).union(Polygon(pG)).area\n\n        def get_intersection_over_union(pD,pG):\n            return get_intersection(pD, pG) / get_union(pD, pG)\n\n        def get_intersection(pD,pG):\n            return Polygon(pD).intersection(Polygon(pG)).area\n\n        def one_to_one_match(row, col):\n            cont = 0\n            for j in range(len(recallMat[0])):    \n                if recallMat[row,j] >= self.area_recall_constraint and precisionMat[row,j] >= self.area_precision_constraint:\n                    cont = cont +1\n            if (cont != 1):\n                return False\n            cont = 0\n            for i in range(len(recallMat)):    \n                if recallMat[i,col] >= self.area_recall_constraint and precisionMat[i,col] >= self.area_precision_constraint:\n                    cont = cont +1\n            if (cont != 1):\n                return False\n            \n            if recallMat[row,col] >= self.area_recall_constraint and precisionMat[row,col] >= self.area_precision_constraint:\n                return True\n            return False\n        \n        def one_to_many_match(gtNum):\n            many_sum = 0\n            detRects = []\n            for detNum in range(len(recallMat[0])):    \n                if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and detNum not in detDontCareRectsNum:\n                    if precisionMat[gtNum,detNum] >= self.area_precision_constraint:\n                        many_sum += recallMat[gtNum,detNum]\n                        detRects.append(detNum)\n            if round(many_sum,4) >= self.area_recall_constraint:\n                return True,detRects\n            else:\n                return False,[]         \n        \n        def many_to_one_match(detNum):\n            many_sum = 0\n            gtRects = []\n            for gtNum in range(len(recallMat)):    \n                if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and gtNum not in gtDontCareRectsNum:\n                    if recallMat[gtNum,detNum] >= self.area_recall_constraint:\n                        many_sum += precisionMat[gtNum,detNum]\n                        gtRects.append(gtNum)\n            if round(many_sum,4) >= self.area_precision_constraint:\n                return True,gtRects\n            else:\n                return False,[]\n        \n        def center_distance(r1, r2):\n            return ((np.mean(r1, axis=0) - np.mean(r2, axis=0)) ** 2).sum() ** 0.5\n        \n        def diag(r):\n            r = np.array(r)\n            return ((r[:, 0].max() - r[:, 0].min()) ** 2 + (r[:, 1].max() - r[:, 1].min()) ** 2) ** 0.5\n        \n        perSampleMetrics = {}\n        \n        recall = 0\n        precision = 0\n        hmean = 0        \n        recallAccum = 0.\n        precisionAccum = 0.\n        gtRects = []\n        detRects = []\n        gtPolPoints = []\n        detPolPoints = []\n        gtDontCareRectsNum = []#Array of Ground Truth Rectangles\' keys marked as don\'t Care\n        detDontCareRectsNum = []#Array of Detected Rectangles\' matched with a don\'t Care GT\n        pairs = []\n        evaluationLog = """"\n        \n        recallMat = np.empty([1,1])\n        precisionMat = np.empty([1,1])              \n        \n        for n in range(len(gt)):\n            points = gt[n][\'points\']\n            # transcription = gt[n][\'text\']\n            dontCare = gt[n][\'ignore\']\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            gtRects.append(points)\n            gtPolPoints.append(points)\n            if dontCare:\n                gtDontCareRectsNum.append( len(gtRects)-1 )                 \n        \n        evaluationLog += ""GT rectangles: "" + str(len(gtRects)) + ("" ("" + str(len(gtDontCareRectsNum)) + "" don\'t care)\\n"" if len(gtDontCareRectsNum)>0 else ""\\n"")\n        \n        for n in range(len(pred)):\n            points = pred[n][\'points\']\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            detRect = points\n            detRects.append(detRect)\n            detPolPoints.append(points)\n            if len(gtDontCareRectsNum)>0 :\n                for dontCareRectNum in gtDontCareRectsNum:\n                    dontCareRect = gtRects[dontCareRectNum]\n                    intersected_area = get_intersection(dontCareRect,detRect)\n                    rdDimensions = Polygon(detRect).area\n                    if (rdDimensions==0) :\n                        precision = 0\n                    else:\n                        precision= intersected_area / rdDimensions\n                    if (precision > 0.5):\n                        detDontCareRectsNum.append( len(detRects)-1 )\n                        break\n\n        evaluationLog += ""DET rectangles: "" + str(len(detRects)) + ("" ("" + str(len(detDontCareRectsNum)) + "" don\'t care)\\n"" if len(detDontCareRectsNum)>0 else ""\\n"")\n\n        if len(gtRects)==0:\n            recall = 1\n            precision = 0 if len(detRects)>0 else 1\n\n        if len(detRects)>0:\n            #Calculate recall and precision matrixs\n            outputShape=[len(gtRects),len(detRects)]\n            recallMat = np.empty(outputShape)\n            precisionMat = np.empty(outputShape)\n            gtRectMat = np.zeros(len(gtRects),np.int8)\n            detRectMat = np.zeros(len(detRects),np.int8)\n            for gtNum in range(len(gtRects)):\n                for detNum in range(len(detRects)):\n                    rG = gtRects[gtNum]\n                    rD = detRects[detNum]\n                    intersected_area = get_intersection(rG,rD)\n                    rgDimensions = Polygon(rG).area\n                    rdDimensions = Polygon(rD).area\n                    recallMat[gtNum,detNum] = 0 if rgDimensions==0 else  intersected_area / rgDimensions\n                    precisionMat[gtNum,detNum] = 0 if rdDimensions==0 else intersected_area / rdDimensions\n\n            # Find one-to-one matches\n            evaluationLog += ""Find one-to-one matches\\n""\n            for gtNum in range(len(gtRects)):\n                for detNum in range(len(detRects)):\n                    if gtRectMat[gtNum] == 0 and detRectMat[detNum] == 0 and gtNum not in gtDontCareRectsNum and detNum not in detDontCareRectsNum :\n                        match = one_to_one_match(gtNum, detNum)\n                        if match is True :\n                            #in deteval we have to make other validation before mark as one-to-one\n                            rG = gtRects[gtNum]\n                            rD = detRects[detNum]\n                            normDist = center_distance(rG, rD);\n                            normDist /= diag(rG) + diag(rD);\n                            normDist *= 2.0;\n                            if normDist < self.ev_param_ind_center_diff_thr:\n                                gtRectMat[gtNum] = 1\n                                detRectMat[detNum] = 1\n                                recallAccum += 1.0\n                                precisionAccum += 1.0\n                                pairs.append({\'gt\':gtNum,\'det\':detNum,\'type\':\'OO\'})\n                                evaluationLog += ""Match GT #"" + str(gtNum) + "" with Det #"" + str(detNum) + ""\\n""\n                            else:\n                                evaluationLog += ""Match Discarded GT #"" + str(gtNum) + "" with Det #"" + str(detNum) + "" normDist: "" + str(normDist) + "" \\n""\n            # Find one-to-many matches\n            evaluationLog += ""Find one-to-many matches\\n""\n            for gtNum in range(len(gtRects)):\n                if gtNum not in gtDontCareRectsNum:\n                    match,matchesDet = one_to_many_match(gtNum)\n                    if match is True :\n                        gtRectMat[gtNum] = 1\n                        recallAccum += 1.0\n                        precisionAccum += len(matchesDet) / (1 + math.log(len(matchesDet)))\n                        pairs.append({\'gt\':gtNum,\'det\':matchesDet,\'type\': \'OO\' if len(matchesDet)==1 else \'OM\'})\n                        for detNum in matchesDet :\n                            detRectMat[detNum] = 1\n                        evaluationLog += ""Match GT #"" + str(gtNum) + "" with Det #"" + str(matchesDet) + ""\\n""\n\n            # Find many-to-one matches\n            evaluationLog += ""Find many-to-one matches\\n""\n            for detNum in range(len(detRects)):\n                if detNum not in detDontCareRectsNum:\n                    match,matchesGt = many_to_one_match(detNum)\n                    if match is True :\n                        detRectMat[detNum] = 1\n                        recallAccum += len(matchesGt) / (1 + math.log(len(matchesGt)))\n                        precisionAccum += 1.0\n                        pairs.append({\'gt\':matchesGt,\'det\':detNum,\'type\': \'OO\' if len(matchesGt)==1 else \'MO\'})\n                        for gtNum in matchesGt :\n                            gtRectMat[gtNum] = 1\n                        evaluationLog += ""Match GT #"" + str(matchesGt) + "" with Det #"" + str(detNum) + ""\\n""\n\n            numGtCare = (len(gtRects) - len(gtDontCareRectsNum))\n            if numGtCare == 0:\n                recall = float(1)\n                precision = float(0) if len(detRects)>0 else float(1)\n            else:\n                recall = float(recallAccum) / numGtCare\n                precision =  float(0) if (len(detRects) - len(detDontCareRectsNum))==0 else float(precisionAccum) / (len(detRects) - len(detDontCareRectsNum))\n            hmean = 0 if (precision + recall)==0 else 2.0 * precision * recall / (precision + recall)  \n\n        numGtCare = len(gtRects) - len(gtDontCareRectsNum)\n        numDetCare = len(detRects) - len(detDontCareRectsNum)\n\n        perSampleMetrics = {\n            \'precision\':precision,\n            \'recall\':recall,\n            \'hmean\':hmean,\n            \'pairs\':pairs,\n            \'recallMat\':[] if len(detRects)>100 else recallMat.tolist(),\n            \'precisionMat\':[] if len(detRects)>100 else precisionMat.tolist(),\n            \'gtPolPoints\':gtPolPoints,\n            \'detPolPoints\':detPolPoints,\n            \'gtCare\': numGtCare,\n            \'detCare\': numDetCare,\n            \'gtDontCare\':gtDontCareRectsNum,\n            \'detDontCare\':detDontCareRectsNum,\n            \'recallAccum\':recallAccum,\n            \'precisionAccum\':precisionAccum,\n            \'evaluationLog\': evaluationLog\n        }\n\n        return perSampleMetrics\n\n    def combine_results(self, results):\n        numGt = 0\n        numDet = 0\n        methodRecallSum = 0\n        methodPrecisionSum = 0\n\n        for result in results:\n            numGt += result[\'gtCare\']\n            numDet += result[\'detCare\']\n            methodRecallSum += result[\'recallAccum\']\n            methodPrecisionSum += result[\'precisionAccum\']\n\n        methodRecall = 0 if numGt==0 else methodRecallSum/numGt\n        methodPrecision = 0 if numDet==0 else methodPrecisionSum/numDet\n        methodHmean = 0 if methodRecall + methodPrecision==0 else 2* methodRecall * methodPrecision / (methodRecall + methodPrecision)\n        \n        methodMetrics = {\'precision\':methodPrecision, \'recall\':methodRecall,\'hmean\': methodHmean  }\n\n        return methodMetrics\n\n\nif __name__==\'__main__\':\n    evaluator = DetectionICDAR2013Evaluator()\n    gts = [[{\n        \'points\': [(0, 0), (1, 0), (1, 1), (0, 1)],\n        \'text\': 1234,\n        \'ignore\': False,\n    }, {\n        \'points\': [(2, 2), (3, 2), (3, 3), (2, 3)],\n        \'text\': 5678,\n        \'ignore\': True,\n    }]]\n    preds = [[{\n        \'points\': [(0.1, 0.1), (1, 0), (1, 1), (0, 1)],\n        \'text\': 123,\n        \'ignore\': False,\n    }]]\n    results = []\n    for gt, pred in zip(gts, preds):\n        results.append(evaluator.evaluate_image(gt, pred))\n    metrics = evaluator.combine_results(results)\n    print(metrics)\n'"
structure/models/maskrcnn_benchmark/__init__.py,0,b''
assets/ops/dcn/functions/__init__.py,0,b''
assets/ops/dcn/functions/deform_conv.py,10,"b'import torch\nfrom torch.autograd import Function\nfrom torch.nn.modules.utils import _pair\n\nfrom .. import deform_conv_cuda\n\n\nclass DeformConvFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                input,\n                offset,\n                weight,\n                stride=1,\n                padding=0,\n                dilation=1,\n                groups=1,\n                deformable_groups=1,\n                im2col_step=64):\n        if input is not None and input.dim() != 4:\n            raise ValueError(\n                ""Expected 4D tensor as input, got {}D tensor instead."".format(\n                    input.dim()))\n        ctx.stride = _pair(stride)\n        ctx.padding = _pair(padding)\n        ctx.dilation = _pair(dilation)\n        ctx.groups = groups\n        ctx.deformable_groups = deformable_groups\n        ctx.im2col_step = im2col_step\n\n        ctx.save_for_backward(input, offset, weight)\n\n        output = input.new_empty(\n            DeformConvFunction._output_size(input, weight, ctx.padding,\n                                            ctx.dilation, ctx.stride))\n\n        ctx.bufs_ = [input.new_empty(0), input.new_empty(0)]  # columns, ones\n\n        if not input.is_cuda:\n            raise NotImplementedError\n        else:\n            cur_im2col_step = min(ctx.im2col_step, input.shape[0])\n            assert (input.shape[0] %\n                    cur_im2col_step) == 0, \'im2col step must divide batchsize\'\n            deform_conv_cuda.deform_conv_forward_cuda(\n                input, weight, offset, output, ctx.bufs_[0], ctx.bufs_[1],\n                weight.size(3), weight.size(2), ctx.stride[1], ctx.stride[0],\n                ctx.padding[1], ctx.padding[0], ctx.dilation[1],\n                ctx.dilation[0], ctx.groups, ctx.deformable_groups,\n                cur_im2col_step)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, offset, weight = ctx.saved_tensors\n\n        grad_input = grad_offset = grad_weight = None\n\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n        else:\n            cur_im2col_step = min(ctx.im2col_step, input.shape[0])\n            assert (input.shape[0] %\n                    cur_im2col_step) == 0, \'im2col step must divide batchsize\'\n\n            if ctx.needs_input_grad[0] or ctx.needs_input_grad[1]:\n                grad_input = torch.zeros_like(input)\n                grad_offset = torch.zeros_like(offset)\n                deform_conv_cuda.deform_conv_backward_input_cuda(\n                    input, offset, grad_output, grad_input,\n                    grad_offset, weight, ctx.bufs_[0], weight.size(3),\n                    weight.size(2), ctx.stride[1], ctx.stride[0],\n                    ctx.padding[1], ctx.padding[0], ctx.dilation[1],\n                    ctx.dilation[0], ctx.groups, ctx.deformable_groups,\n                    cur_im2col_step)\n\n            if ctx.needs_input_grad[2]:\n                grad_weight = torch.zeros_like(weight)\n                deform_conv_cuda.deform_conv_backward_parameters_cuda(\n                    input, offset, grad_output,\n                    grad_weight, ctx.bufs_[0], ctx.bufs_[1], weight.size(3),\n                    weight.size(2), ctx.stride[1], ctx.stride[0],\n                    ctx.padding[1], ctx.padding[0], ctx.dilation[1],\n                    ctx.dilation[0], ctx.groups, ctx.deformable_groups, 1,\n                    cur_im2col_step)\n\n        return (grad_input, grad_offset, grad_weight, None, None, None, None,\n                None)\n\n    @staticmethod\n    def _output_size(input, weight, padding, dilation, stride):\n        channels = weight.size(0)\n        output_size = (input.size(0), channels)\n        for d in range(input.dim() - 2):\n            in_size = input.size(d + 2)\n            pad = padding[d]\n            kernel = dilation[d] * (weight.size(d + 2) - 1) + 1\n            stride_ = stride[d]\n            output_size += ((in_size + (2 * pad) - kernel) // stride_ + 1, )\n        if not all(map(lambda s: s > 0, output_size)):\n            raise ValueError(\n                ""convolution input is too small (output would be {})"".format(\n                    \'x\'.join(map(str, output_size))))\n        return output_size\n\n\nclass ModulatedDeformConvFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                input,\n                offset,\n                mask,\n                weight,\n                bias=None,\n                stride=1,\n                padding=0,\n                dilation=1,\n                groups=1,\n                deformable_groups=1):\n        ctx.stride = stride\n        ctx.padding = padding\n        ctx.dilation = dilation\n        ctx.groups = groups\n        ctx.deformable_groups = deformable_groups\n        ctx.with_bias = bias is not None\n        if not ctx.with_bias:\n            bias = input.new_empty(1)  # fake tensor\n        if not input.is_cuda:\n            raise NotImplementedError\n        if weight.requires_grad or mask.requires_grad or offset.requires_grad \\\n                or input.requires_grad:\n            ctx.save_for_backward(input, offset, mask, weight, bias)\n        output = input.new_empty(\n            ModulatedDeformConvFunction._infer_shape(ctx, input, weight))\n        ctx._bufs = [input.new_empty(0), input.new_empty(0)]\n        deform_conv_cuda.modulated_deform_conv_cuda_forward(\n            input, weight, bias, ctx._bufs[0], offset, mask, output,\n            ctx._bufs[1], weight.shape[2], weight.shape[3], ctx.stride,\n            ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation,\n            ctx.groups, ctx.deformable_groups, ctx.with_bias)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n        input, offset, mask, weight, bias = ctx.saved_tensors\n        grad_input = torch.zeros_like(input)\n        grad_offset = torch.zeros_like(offset)\n        grad_mask = torch.zeros_like(mask)\n        grad_weight = torch.zeros_like(weight)\n        grad_bias = torch.zeros_like(bias)\n        deform_conv_cuda.modulated_deform_conv_cuda_backward(\n            input, weight, bias, ctx._bufs[0], offset, mask, ctx._bufs[1],\n            grad_input, grad_weight, grad_bias, grad_offset, grad_mask,\n            grad_output, weight.shape[2], weight.shape[3], ctx.stride,\n            ctx.stride, ctx.padding, ctx.padding, ctx.dilation, ctx.dilation,\n            ctx.groups, ctx.deformable_groups, ctx.with_bias)\n        if not ctx.with_bias:\n            grad_bias = None\n\n        return (grad_input, grad_offset, grad_mask, grad_weight, grad_bias,\n                None, None, None, None, None)\n\n    @staticmethod\n    def _infer_shape(ctx, input, weight):\n        n = input.size(0)\n        channels_out = weight.size(0)\n        height, width = input.shape[2:4]\n        kernel_h, kernel_w = weight.shape[2:4]\n        height_out = (height + 2 * ctx.padding -\n                      (ctx.dilation * (kernel_h - 1) + 1)) // ctx.stride + 1\n        width_out = (width + 2 * ctx.padding -\n                     (ctx.dilation * (kernel_w - 1) + 1)) // ctx.stride + 1\n        return n, channels_out, height_out, width_out\n\n\ndeform_conv = DeformConvFunction.apply\nmodulated_deform_conv = ModulatedDeformConvFunction.apply\n'"
assets/ops/dcn/functions/deform_pool.py,3,"b'import torch\nfrom torch.autograd import Function\n\nfrom .. import deform_pool_cuda\n\n\nclass DeformRoIPoolingFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                data,\n                rois,\n                offset,\n                spatial_scale,\n                out_size,\n                out_channels,\n                no_trans,\n                group_size=1,\n                part_size=None,\n                sample_per_part=4,\n                trans_std=.0):\n        ctx.spatial_scale = spatial_scale\n        ctx.out_size = out_size\n        ctx.out_channels = out_channels\n        ctx.no_trans = no_trans\n        ctx.group_size = group_size\n        ctx.part_size = out_size if part_size is None else part_size\n        ctx.sample_per_part = sample_per_part\n        ctx.trans_std = trans_std\n\n        assert 0.0 <= ctx.trans_std <= 1.0\n        if not data.is_cuda:\n            raise NotImplementedError\n\n        n = rois.shape[0]\n        output = data.new_empty(n, out_channels, out_size, out_size)\n        output_count = data.new_empty(n, out_channels, out_size, out_size)\n        deform_pool_cuda.deform_psroi_pooling_cuda_forward(\n            data, rois, offset, output, output_count, ctx.no_trans,\n            ctx.spatial_scale, ctx.out_channels, ctx.group_size, ctx.out_size,\n            ctx.part_size, ctx.sample_per_part, ctx.trans_std)\n\n        if data.requires_grad or rois.requires_grad or offset.requires_grad:\n            ctx.save_for_backward(data, rois, offset)\n        ctx.output_count = output_count\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n\n        data, rois, offset = ctx.saved_tensors\n        output_count = ctx.output_count\n        grad_input = torch.zeros_like(data)\n        grad_rois = None\n        grad_offset = torch.zeros_like(offset)\n\n        deform_pool_cuda.deform_psroi_pooling_cuda_backward(\n            grad_output, data, rois, offset, output_count, grad_input,\n            grad_offset, ctx.no_trans, ctx.spatial_scale, ctx.out_channels,\n            ctx.group_size, ctx.out_size, ctx.part_size, ctx.sample_per_part,\n            ctx.trans_std)\n        return (grad_input, grad_rois, grad_offset, None, None, None, None,\n                None, None, None, None)\n\n\ndeform_roi_pooling = DeformRoIPoolingFunction.apply\n'"
assets/ops/dcn/modules/__init__.py,0,b''
assets/ops/dcn/modules/deform_conv.py,8,"b""import math\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.modules.utils import _pair\n\nfrom ..functions.deform_conv import deform_conv, modulated_deform_conv\n\n\nclass DeformConv(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,\n                 bias=False):\n        super(DeformConv, self).__init__()\n\n        assert not bias\n        assert in_channels % groups == 0, \\\n            'in_channels {} cannot be divisible by groups {}'.format(\n                in_channels, groups)\n        assert out_channels % groups == 0, \\\n            'out_channels {} cannot be divisible by groups {}'.format(\n                out_channels, groups)\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _pair(kernel_size)\n        self.stride = _pair(stride)\n        self.padding = _pair(padding)\n        self.dilation = _pair(dilation)\n        self.groups = groups\n        self.deformable_groups = deformable_groups\n\n        self.weight = nn.Parameter(\n            torch.Tensor(out_channels, in_channels // self.groups,\n                         *self.kernel_size))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, x, offset):\n        return deform_conv(x, offset, self.weight, self.stride, self.padding,\n                           self.dilation, self.groups, self.deformable_groups)\n\n\nclass DeformConvPack(DeformConv):\n\n    def __init__(self, *args, **kwargs):\n        super(DeformConvPack, self).__init__(*args, **kwargs)\n\n        self.conv_offset = nn.Conv2d(\n            self.in_channels,\n            self.deformable_groups * 2 * self.kernel_size[0] *\n            self.kernel_size[1],\n            kernel_size=self.kernel_size,\n            stride=_pair(self.stride),\n            padding=_pair(self.padding),\n            bias=True)\n        self.init_offset()\n\n    def init_offset(self):\n        self.conv_offset.weight.data.zero_()\n        self.conv_offset.bias.data.zero_()\n\n    def forward(self, x):\n        offset = self.conv_offset(x)\n        return deform_conv(x, offset, self.weight, self.stride, self.padding,\n                           self.dilation, self.groups, self.deformable_groups)\n\n\nclass ModulatedDeformConv(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,\n                 bias=True):\n        super(ModulatedDeformConv, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _pair(kernel_size)\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.groups = groups\n        self.deformable_groups = deformable_groups\n        self.with_bias = bias\n\n        self.weight = nn.Parameter(\n            torch.Tensor(out_channels, in_channels // groups,\n                         *self.kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.zero_()\n\n    def forward(self, x, offset, mask):\n        return modulated_deform_conv(x, offset, mask, self.weight, self.bias,\n                                     self.stride, self.padding, self.dilation,\n                                     self.groups, self.deformable_groups)\n\n\nclass ModulatedDeformConvPack(ModulatedDeformConv):\n\n    def __init__(self, *args, **kwargs):\n        super(ModulatedDeformConvPack, self).__init__(*args, **kwargs)\n\n        self.conv_offset_mask = nn.Conv2d(\n            self.in_channels,\n            self.deformable_groups * 3 * self.kernel_size[0] *\n            self.kernel_size[1],\n            kernel_size=self.kernel_size,\n            stride=_pair(self.stride),\n            padding=_pair(self.padding),\n            bias=True)\n        self.init_offset()\n\n    def init_offset(self):\n        self.conv_offset_mask.weight.data.zero_()\n        self.conv_offset_mask.bias.data.zero_()\n\n    def forward(self, x):\n        out = self.conv_offset_mask(x)\n        o1, o2, mask = torch.chunk(out, 3, dim=1)\n        offset = torch.cat((o1, o2), dim=1)\n        mask = torch.sigmoid(mask)\n        return modulated_deform_conv(x, offset, mask, self.weight, self.bias,\n                                     self.stride, self.padding, self.dilation,\n                                     self.groups, self.deformable_groups)\n"""
assets/ops/dcn/modules/deform_pool.py,0,"b'from torch import nn\n\nfrom ..functions.deform_pool import deform_roi_pooling\n\n\nclass DeformRoIPooling(nn.Module):\n\n    def __init__(self,\n                 spatial_scale,\n                 out_size,\n                 out_channels,\n                 no_trans,\n                 group_size=1,\n                 part_size=None,\n                 sample_per_part=4,\n                 trans_std=.0):\n        super(DeformRoIPooling, self).__init__()\n        self.spatial_scale = spatial_scale\n        self.out_size = out_size\n        self.out_channels = out_channels\n        self.no_trans = no_trans\n        self.group_size = group_size\n        self.part_size = out_size if part_size is None else part_size\n        self.sample_per_part = sample_per_part\n        self.trans_std = trans_std\n\n    def forward(self, data, rois, offset):\n        if self.no_trans:\n            offset = data.new_empty(0)\n        return deform_roi_pooling(\n            data, rois, offset, self.spatial_scale, self.out_size,\n            self.out_channels, self.no_trans, self.group_size, self.part_size,\n            self.sample_per_part, self.trans_std)\n\n\nclass DeformRoIPoolingPack(DeformRoIPooling):\n\n    def __init__(self,\n                 spatial_scale,\n                 out_size,\n                 out_channels,\n                 no_trans,\n                 group_size=1,\n                 part_size=None,\n                 sample_per_part=4,\n                 trans_std=.0,\n                 num_offset_fcs=3,\n                 deform_fc_channels=1024):\n        super(DeformRoIPoolingPack,\n              self).__init__(spatial_scale, out_size, out_channels, no_trans,\n                             group_size, part_size, sample_per_part, trans_std)\n\n        self.num_offset_fcs = num_offset_fcs\n        self.deform_fc_channels = deform_fc_channels\n\n        if not no_trans:\n            seq = []\n            ic = self.out_size * self.out_size * self.out_channels\n            for i in range(self.num_offset_fcs):\n                if i < self.num_offset_fcs - 1:\n                    oc = self.deform_fc_channels\n                else:\n                    oc = self.out_size * self.out_size * 2\n                seq.append(nn.Linear(ic, oc))\n                ic = oc\n                if i < self.num_offset_fcs - 1:\n                    seq.append(nn.ReLU(inplace=True))\n            self.offset_fc = nn.Sequential(*seq)\n            self.offset_fc[-1].weight.data.zero_()\n            self.offset_fc[-1].bias.data.zero_()\n\n    def forward(self, data, rois):\n        assert data.size(1) == self.out_channels\n        if self.no_trans:\n            offset = data.new_empty(0)\n            return deform_roi_pooling(\n                data, rois, offset, self.spatial_scale, self.out_size,\n                self.out_channels, self.no_trans, self.group_size,\n                self.part_size, self.sample_per_part, self.trans_std)\n        else:\n            n = rois.shape[0]\n            offset = data.new_empty(0)\n            x = deform_roi_pooling(data, rois, offset, self.spatial_scale,\n                                   self.out_size, self.out_channels, True,\n                                   self.group_size, self.part_size,\n                                   self.sample_per_part, self.trans_std)\n            offset = self.offset_fc(x.view(n, -1))\n            offset = offset.view(n, 2, self.out_size, self.out_size)\n            return deform_roi_pooling(\n                data, rois, offset, self.spatial_scale, self.out_size,\n                self.out_channels, self.no_trans, self.group_size,\n                self.part_size, self.sample_per_part, self.trans_std)\n\n\nclass ModulatedDeformRoIPoolingPack(DeformRoIPooling):\n\n    def __init__(self,\n                 spatial_scale,\n                 out_size,\n                 out_channels,\n                 no_trans,\n                 group_size=1,\n                 part_size=None,\n                 sample_per_part=4,\n                 trans_std=.0,\n                 num_offset_fcs=3,\n                 num_mask_fcs=2,\n                 deform_fc_channels=1024):\n        super(ModulatedDeformRoIPoolingPack, self).__init__(\n            spatial_scale, out_size, out_channels, no_trans, group_size,\n            part_size, sample_per_part, trans_std)\n\n        self.num_offset_fcs = num_offset_fcs\n        self.num_mask_fcs = num_mask_fcs\n        self.deform_fc_channels = deform_fc_channels\n\n        if not no_trans:\n            offset_fc_seq = []\n            ic = self.out_size * self.out_size * self.out_channels\n            for i in range(self.num_offset_fcs):\n                if i < self.num_offset_fcs - 1:\n                    oc = self.deform_fc_channels\n                else:\n                    oc = self.out_size * self.out_size * 2\n                offset_fc_seq.append(nn.Linear(ic, oc))\n                ic = oc\n                if i < self.num_offset_fcs - 1:\n                    offset_fc_seq.append(nn.ReLU(inplace=True))\n            self.offset_fc = nn.Sequential(*offset_fc_seq)\n            self.offset_fc[-1].weight.data.zero_()\n            self.offset_fc[-1].bias.data.zero_()\n\n            mask_fc_seq = []\n            ic = self.out_size * self.out_size * self.out_channels\n            for i in range(self.num_mask_fcs):\n                if i < self.num_mask_fcs - 1:\n                    oc = self.deform_fc_channels\n                else:\n                    oc = self.out_size * self.out_size\n                mask_fc_seq.append(nn.Linear(ic, oc))\n                ic = oc\n                if i < self.num_mask_fcs - 1:\n                    mask_fc_seq.append(nn.ReLU(inplace=True))\n                else:\n                    mask_fc_seq.append(nn.Sigmoid())\n            self.mask_fc = nn.Sequential(*mask_fc_seq)\n            self.mask_fc[-2].weight.data.zero_()\n            self.mask_fc[-2].bias.data.zero_()\n\n    def forward(self, data, rois):\n        assert data.size(1) == self.out_channels\n        if self.no_trans:\n            offset = data.new_empty(0)\n            return deform_roi_pooling(\n                data, rois, offset, self.spatial_scale, self.out_size,\n                self.out_channels, self.no_trans, self.group_size,\n                self.part_size, self.sample_per_part, self.trans_std)\n        else:\n            n = rois.shape[0]\n            offset = data.new_empty(0)\n            x = deform_roi_pooling(data, rois, offset, self.spatial_scale,\n                                   self.out_size, self.out_channels, True,\n                                   self.group_size, self.part_size,\n                                   self.sample_per_part, self.trans_std)\n            offset = self.offset_fc(x.view(n, -1))\n            offset = offset.view(n, 2, self.out_size, self.out_size)\n            mask = self.mask_fc(x.view(n, -1))\n            mask = mask.view(n, 1, self.out_size, self.out_size)\n            return deform_roi_pooling(\n                data, rois, offset, self.spatial_scale, self.out_size,\n                self.out_channels, self.no_trans, self.group_size,\n                self.part_size, self.sample_per_part, self.trans_std) * mask\n'"
