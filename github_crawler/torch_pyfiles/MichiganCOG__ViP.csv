file_path,api_count,code
__init__.py,0,b'from . import datasets\nfrom . import models\n'
checkpoint.py,2,"b'import os\nimport torch\n\ndef save_checkpoint(epoch, step, model, optimizer, save_path):\n    """"""\n    Save checkpoint pickle file with model weights and other experimental settings \n    Args:\n        epoch     (Int):    Current epoch when model is being saved\n        step      (Int):    Mini-batch iteration count when model is being saved\n        model     (Object): Current copy of model\n        optimizer (Object): Optimizer object\n        save_path (String): Full directory path to results folder\n\n    Return:\n        None\n    """"""\n\n    state = {   \'epoch\':epoch,\n                \'step\': step,\n                \'state_dict\': model.state_dict(),\n                \'optimizer\' : optimizer.state_dict(),\n             }\n\n    torch.save(state, save_path)\n \ndef load_checkpoint(name, key_name=\'state_dict\'):\n    """"""\n    Load checkpoint pickle file and return selected element from pickle file\n    Args:\n        name     (String): Full path, including pickle file name, to load \n        key_name (String): Key name to return from saved pickle file \n\n    Return:\n        Selected element from loaded checkpoint pickle file\n    """"""\n    checkpoint = torch.load(name)\n    return checkpoint[key_name]\n'"
eval.py,9,"b'import os\nimport sys\nimport datetime\nimport yaml\nimport torch\n\nimport numpy                    as np\nimport torch.nn                 as nn\nimport torch.optim              as optim\nimport torch.utils.data         as Data\n\nfrom tensorboardX                       import SummaryWriter\n\nfrom parse_args                         import Parse\nfrom models.models_import               import create_model_object\nfrom datasets                           import data_loader \nfrom metrics                            import Metrics\nfrom checkpoint                         import load_checkpoint\n\ndef eval(**args):\n    """"""\n    Evaluate selected model \n    Args:\n        seed       (Int):        Integer indicating set seed for random state\n        save_dir   (String):     Top level directory to generate results folder\n        model      (String):     Name of selected model \n        dataset    (String):     Name of selected dataset  \n        exp        (String):     Name of experiment \n        load_type  (String):     Keyword indicator to evaluate the testing or validation set\n        pretrained (Int/String): Int/String indicating loading of random, pretrained or saved weights\n        \n    Return:\n        None\n    """"""\n\n    print(""\\n############################################################################\\n"")\n    print(""Experimental Setup: "", args)\n    print(""\\n############################################################################\\n"")\n\n    d          = datetime.datetime.today()\n    date       = d.strftime(\'%Y%m%d-%H%M%S\')\n    result_dir = os.path.join(args[\'save_dir\'], args[\'model\'], \'_\'.join((args[\'dataset\'],args[\'exp\'],date)))\n    log_dir    = os.path.join(result_dir, \'logs\')\n    save_dir   = os.path.join(result_dir, \'checkpoints\')\n\n    if not args[\'debug\']:\n        os.makedirs(result_dir, exist_ok=True)\n        os.makedirs(log_dir,    exist_ok=True) \n        os.makedirs(save_dir,   exist_ok=True) \n\n        # Save copy of config file\n        with open(os.path.join(result_dir, \'config.yaml\'),\'w\') as outfile:\n            yaml.dump(args, outfile, default_flow_style=False)\n\n        # Tensorboard Element\n        writer = SummaryWriter(log_dir)\n\n    # Check if GPU is available (CUDA)\n    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n    # Load Network\n    model = create_model_object(**args).to(device)\n\n    # Load Data\n    loader = data_loader(**args, model_obj=model)\n\n    if args[\'load_type\'] == \'train_val\':\n        eval_loader = loader[\'valid\']\n\n    elif args[\'load_type\'] == \'train\':\n        eval_loader = loader[\'train\']\n\n    elif args[\'load_type\'] == \'test\':\n        eval_loader  = loader[\'test\'] \n\n    else:\n        sys.exit(\'load_type must be valid or test for eval, exiting\')\n\n    # END IF\n\n    if isinstance(args[\'pretrained\'], str):\n        ckpt = load_checkpoint(args[\'pretrained\'])\n        model.load_state_dict(ckpt)\n\n    # Training Setup\n    params     = [p for p in model.parameters() if p.requires_grad]\n\n    acc_metric = Metrics(**args, result_dir=result_dir, ndata=len(eval_loader.dataset))\n    acc = 0.0\n\n    # Setup Model To Evaluate \n    model.eval()\n\n    with torch.no_grad():\n        for step, data in enumerate(eval_loader):\n            x_input     = data[\'data\']\n            annotations = data[\'annots\']\n\n            if isinstance(x_input, torch.Tensor):\n                outputs = model(x_input.to(device))\n            else:\n                for i, item in enumerate(x_input):\n                    if isinstance(item, torch.Tensor):\n                        x_input[i] = item.to(device)\n                outputs = model(*x_input)\n\n            # END IF\n\n\n            acc = acc_metric.get_accuracy(outputs, annotations)\n\n            if step % 100 == 0:\n                print(\'Step: {}/{} | {} acc: {:.4f}\'.format(step, len(eval_loader), args[\'load_type\'], acc))\n\n    print(\'Accuracy of the network on the {} set: {:.3f} %\\n\'.format(args[\'load_type\'], 100.*acc))\n\n    if not args[\'debug\']:\n        writer.add_scalar(args[\'dataset\']+\'/\'+args[\'model\']+\'/\'+args[\'load_type\']+\'_accuracy\', 100.*acc)\n        # Close Tensorboard Element\n        writer.close()\n\nif __name__ == \'__main__\':\n\n    parse = Parse()\n    args = parse.get_args()\n\n    # For reproducibility\n    torch.backends.cudnn.deterministic = True\n    torch.manual_seed(args[\'seed\'])\n    np.random.seed(args[\'seed\'])\n\n    eval(**args)\n'"
losses.py,8,"b'import numpy as np\nfrom scipy import ndimage\nimport os\nimport cv2\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Losses(object):\n    def __init__(self, *args, **kwargs): #loss_type, size_average=None, reduce=None, reduction=\'mean\', *args, **kwargs):\n        """"""\n        Class used to initialize and handle all available loss types in ViP\n\n        Args: \n            loss_type (String): String indicating which custom loss function is to be loaded.\n\n        Return:\n            Loss object \n        """"""\n\n        self.loss_type   = kwargs[\'loss_type\']\n        self.loss_object = None\n        \n        if self.loss_type == \'MSE\':\n            self.loss_object = MSE(*args, **kwargs)\n\n        elif self.loss_type == \'M_XENTROPY\':\n            self.loss_object = M_XENTROPY(*args, **kwargs)\n\n        elif self.loss_type == \'YC2BB_Attention_Loss\':\n            self.loss_object = YC2BB_Attention_Loss(*args, **kwargs)\n\n        else:\n            print(\'Invalid loss type selected. Quitting!\')\n            exit(1)\n\n    def loss(self, predictions, data, **kwargs):\n        """"""\n        Function that calculates loss from selected loss type\n\n        Args:\n            predictions (Tensor, shape [N,*]): Tensor output by the network\n            target      (Tensor, shape [N,*]): Target tensor used with predictions to compute the loss\n\n        Returns:\n            Calculated loss value\n        """""" \n        return self.loss_object.loss(predictions, data, **kwargs)\n\nclass MSE():\n    def __init__(self, *args, **kwargs):\n        """"""\n        Mean squared error (squared L2 norm) between predictions and target\n\n        Args:\n            reduction (String): \'none\', \'mean\', \'sum\' (see PyTorch Docs). Default: \'mean\'\n            device    (String): \'cpu\' or \'cuda\'\n\n        Returns:\n            None \n        """"""\n\n        reduction = \'mean\' if \'reduction\' not in kwargs else kwargs[\'reduction\']\n        self.device = kwargs[\'device\']\n\n        self.mse_loss = torch.nn.MSELoss(reduction=reduction)\n\n    def loss(self, predictions, data):\n        """"""\n        Args:\n            predictions  (Tensor, shape [N,*]): Output by the network\n            data         (dictionary)\n                - labels (Tensor, shape [N,*]):  Targets from ground truth data\n\n        Returns:\n            Return mean squared error loss\n        """"""\n\n        targets = data[\'labels\'].to(self.device)\n\n        return self.mse_loss(predictions, targets)\n\nclass M_XENTROPY(object):\n    def __init__(self, *args, **kwargs):\n        """"""\n        Cross-entropy Loss with a distribution of values, not just 1-hot vectors \n\n        Args:\n            dim (integer): Dimension to reduce \n\n        Returns:\n            None \n        """"""\n        self.logsoftmax = nn.LogSoftmax(dim=1)\n\n    def loss(self, predictions, data):\n        """"""\n        Args:\n            predictions  (Tensor, shape [N,*]): Output by the network\n            data         (dictionary)\n                - labels (Tensor, shape [N,*]):  Targets from ground truth data\n                \n        Return:\n            Cross-entropy loss  \n        """"""\n\n        targets = data[\'labels\']\n        one_hot = np.zeros((targets.shape[0], predictions.shape[1]))\n        one_hot[np.arange(targets.shape[0]), targets.cpu().numpy().astype(\'int32\')[:, -1]] = 1\n        one_hot = torch.Tensor(one_hot).cuda()\n\n        return torch.mean(torch.sum(-one_hot * self.logsoftmax(predictions), dim=1))\n\n#Code source: https://github.com/MichiganCOG/Video-Grounding-from-Text/blob/master/train.py\nclass YC2BB_Attention_Loss(object):\n    def __init__(self, *args, **kwargs):\n       """"""\n       Frame-wise attention loss used in Weakly-Supervised Object Video Grounding... \n       https://arxiv.org/pdf/1805.02834.pdf\n       \n       Weakly-supervised, no groundtruth labels are used.\n       """"""\n\n       self.loss_weighting = kwargs[\'has_loss_weighting\']\n       self.obj_interact   = kwargs[\'obj_interact\']\n       self.ranking_margin = kwargs[\'ranking_margin\']\n       self.loss_factor    = kwargs[\'loss_factor\']\n\n    def loss(self, predictions, data):\n        """"""\n        Args:\n            predictions (List): \n                - output (Tensor, shape [2*T, 2]): Positive and negative attention weights for each sample\n                - loss_weigh (Tensor, shape [2*T, 1]): Loss weighting applied to each sampled frame\n            data        (None) \n\n            T: number of sampled frames from video (default: 5)\n        Return:\n            Frame-wise weighting loss \n        """"""\n        output, loss_weigh = predictions\n\n        if self.loss_weighting or self.obj_interact: \n            rank_batch = F.margin_ranking_loss(output[:,0:1], output[:,1:2], \n                torch.ones(output.size()).type(output.data.type()), margin=self.ranking_margin, reduction=\'none\')\n            if self.loss_weighting and self.obj_interact:\n                loss_weigh = (output[:, 0:1]+loss_weigh)/2. # avg\n            elif self.loss_weighting:\n                loss_weigh = output[:,0:1]\n            else:\n                loss_weigh = loss_weigh.unsqueeze(1)\n            # ranking loss\n            cls_loss = self.loss_factor*(rank_batch*loss_weigh).mean()+ \\\n                        (1-self.loss_factor)*-torch.log(2*loss_weigh).mean()\n        else:\n            # ranking loss\n            cls_loss = F.margin_ranking_loss(output[:,0:1], output[:,1:2],\n                torch.Tensor([[1],[1]]).type(output.data.type()), margin=self.ranking_margin)\n\n\n        return cls_loss\n\n'"
metrics.py,44,"b'import os\nimport json \nimport numpy as np\n\nimport torch\n\nclass Metrics(object):\n    def __init__(self, *args, **kwargs):\n        """"""\n        Compute accuracy metrics from this Metrics class\n        Args:\n            acc_metric (String): String used to indicate selected accuracy metric \n    \n        Return:\n            None\n        """"""\n        self.metric_type = kwargs[\'acc_metric\'] \n\n        if self.metric_type == \'Accuracy\':\n            self.metric_object = Accuracy(*args, **kwargs) \n        elif self.metric_type == \'AveragePrecision\':\n            self.metric_object = AveragePrecision(*args, **kwargs)\n        elif self.metric_type == \'mAP\':\n            self.metric_object = MAP(*args, **kwargs)\n        elif self.metric_type == \'SSD_AP\':\n            self.metric_object = SSD_AP(*args, **kwargs)\n        elif self.metric_type == \'Box_Accuracy\':\n            self.metric_object = Box_Accuracy(*args, **kwargs)\n        else:\n            self.metric_type = None\n\n    def get_accuracy(self, predictions, targets, **kwargs):\n        """"""\n        Return accuracy from selected metric type\n\n        Args:\n            predictions: model predictions \n            targets: ground truth or targets \n        """"""\n\n        if self.metric_type == None:\n            return -1\n\n        else:\n            return self.metric_object.get_accuracy(predictions, targets, **kwargs)\n\nclass Accuracy(object):\n    """"""\n    Standard accuracy computation. # of correct cases/# of total cases\n\n    """"""\n    def __init__(self, *args, **kwargs):\n        self.correct = 0.\n        self.total   = 0. \n\n    def get_accuracy(self, predictions, data):\n        """"""\n        Args:\n            predictions (Tensor, shape [N,*])\n            data        (dictionary):\n                - labels (Tensor, shape [N,*]) \n\n        Return:\n            Accuracy # of correct case/ # of total cases\n        """"""\n        targets = data[\'labels\']\n        assert (predictions.shape[0] == targets.shape[0])\n\n        targets     = targets.detach().cpu().numpy()\n        predictions = predictions.detach().cpu().numpy()\n\n        if len(targets.shape) == 2 and len(predictions.shape) == 2:\n            self.correct += np.sum(np.argmax(predictions,1) == targets[:, -1])\n            self.total   += predictions.shape[0]\n\n        else: \n            self.correct += np.sum(np.argmax(predictions,1) == targets[:, -1])\n            self.total   += predictions.shape[0]\n\n        # END IF\n\n        return self.correct/self.total\n\nclass IOU():\n    """"""\n    Intersection-over-union between one prediction bounding box \n    and plausible ground truth bounding boxes\n\n    """"""\n    def __init__(self, *args, **kwargs):\n        pass        \n\n    def intersect(self, box_p, box_t):\n        """"""\n        Intersection area between predicted bounding box and \n        all ground truth bounding boxes\n\n        Args:\n            box_p (Tensor, shape [4]): prediction bounding box, coordinate format [x1, y1, x2, y2]\n            box_t (Tensor, shape [N,4]): target bounding boxes\n\n        Return:\n            intersect area (Tensor, shape [N]): intersect_area for all target bounding boxes\n        """"""\n        x_left = torch.max(box_p[0], box_t[:,0])\n        y_top = torch.max(box_p[1], box_t[:,1])\n        x_right = torch.min(box_p[2], box_t[:,2])\n        y_bottom = torch.min(box_p[3], box_t[:,3])\n\n        width = torch.clamp(x_right - x_left, min=0)\n        height = torch.clamp(y_bottom - y_top, min=0)\n\n        intersect_area = width * height\n\n        return intersect_area\n\n    def iou(self, box_p, box_t):\n        """"""\n        Performs intersection-over-union \n\n        Args:\n            box_p (Tensor, shape [4]): prediction bounding box, coordinate format [x1, y1, x2, y2]\n            box_t (Tensor, shape [N,4]): target bounding boxes\n\n        Return:\n            overlap (Tensor, shape [1]): max overlap\n            ind     (Tensor, shape [1]): index of bounding box with largest overlap\n        """"""\n        \n        intersect_area = self.intersect(box_p, box_t)\n\n        box_p_area = (box_p[2] - box_p[0]) * (box_p[3] - box_p[1])\n        box_t_area = (box_t[:,2] - box_t[:,0]) * (box_t[:,3] - box_t[:,1])\n        union = box_p_area + box_t_area - intersect_area \n        overlap = torch.max(intersect_area/union)\n        ind     = torch.argmax(intersect_area/union)\n\n        assert overlap >= 0.0\n        assert overlap <= 1.0\n        \n        return overlap, ind\n\n    def get_accuracy(self, prediction, targets):\n        """"""\n        Args:\n            prediction (Tensor, shape [4]): prediction bounding box, coordinate format [x1, y1, x2, y2]\n            targets    (Tensor, shape [N,4]): target bounding boxes\n\n        Return:\n            iou (Tensor, shape[1]): Highest iou amongst target bounding boxes\n            ind (Tensor, shape[1]): Index of target bounding box with highest score\n        """"""\n\n        iou_score, ind = self.iou(prediction, targets)\n        return iou_score, ind\n\nclass AveragePrecision():\n\n    """"""\n    Average Precision is computed per class and then averaged across all classes\n    """"""\n\n    def __init__(self, threshold=0.5, num_points=101, *args, **kwargs):\n        """"""\n        Compute Average Precision (AP)\n        Args:\n            threshold  (float): iou threshold \n            num_points (int): number of points to average for the interpolated AP calculation\n\n        Return:\n            None \n        """"""\n\n        self.threshold = threshold \n        self.num_points = num_points\n        self.IOU = IOU(average=False)\n\n        self.result_dir = kwargs[\'result_dir\']\n        final_shape = kwargs[\'final_shape\']\n        #assuming model predictions are normalized between 0-1\n        self.scale = torch.Tensor([1, final_shape[0], final_shape[1], final_shape[0], final_shape[1]]) #[1, height, width, height, width]\n\n        self.ndata = kwargs[\'ndata\']\n        self.count = 0\n\n    def update_threshold(self, threshold):\n        self.threshold = threshold\n\n    def compute_class_ap(self, tp, fp, npos):\n        """"""\n        Args:\n            tp   (Tensor, shape [N*D]): cumulative sum of true positive detections \n            fp   (Tensor, shape [N*D]): cumulative sum of false positive detections \n            npos (Tensor, int): actual positives (from ground truth)\n\n        Return:\n            ap (Tensor, float): average precision calculation\n        """"""\n        \n        #Values for precision-recall curve\n        rc = tp/npos\n        pr = tp / torch.clamp(tp + fp, min=torch.finfo(torch.float).eps)\n        rc_values = torch.linspace(0,1,self.num_points) #sampled recall points for n-point precision-recall curve\n\n        #The interpotaled P-R curve will take on the max precision value to the right at each recall\n        ap = 0.\n        for t in rc_values:\n            if torch.sum(rc >= t) == 0:\n                p = 0\n            else:\n                p = torch.max(pr[rc >= t])\n            ap = ap + p/self.num_points\n\n        return ap\n    \n    def get_AP(self, predictions, targets):\n        """"""\n        Args:\n            predictions (Tensor, shape [N,C,D,5]): prediction bounding boxes, coordinate format [confidence, x1, y1, x2, y2]\n            targets     (Tensor, shape [N,C,D_,4]): ground truth bounding boxes \n            C:  num of classes + 1 (0th class is background class, not included in calculation)\n            D:  predicted detections\n            D_: ground truth detections \n\n        Return:\n            avg_ap (Tensor, float): mean ap across all classes \n        """"""\n\n        N,C,D,_ = predictions.shape\n        _,_,D_,_ = targets.shape \n        ap = []\n        \n        mask_g = torch.zeros(N,C,D_)\n        for c in range(1,C): #skip background class (c=0)\n\n            #Sort predictions in descending order, by confidence value\n            pred = predictions[:,c].contiguous().view(N*D,-1)\n            idx  = pred[:,0].argsort(descending=True)\n            pred = pred[idx]\n\n            img_labels = torch.arange(0,N).unsqueeze(1).repeat(1,D).view(N*D)\n            img_labels = img_labels[idx]\n\n            tp   = []\n            fp   = []\n            mask = torch.zeros(N,D_,dtype=torch.uint8)\n            class_targets = targets[:,c]\n\n            for i in range(class_targets.shape[0]):\n                for j in range(class_targets.shape[1]):\n                    if not torch.equal(class_targets[i,j], torch.Tensor([-1,-1,-1,-1])):\n                        mask[i,j] = 1\n\n            npos = torch.sum(mask)\n\n            for n, p in zip(img_labels, pred[:,1:]): #get iou for all detections\n                trgts = targets[n,c]\n\n                gt_mask = mask[n]\n                exists = torch.sum(gt_mask) > 0 #gt exists on this image\n                if not torch.equal(p, torch.Tensor([0,0,0,0])):\n                    if exists:\n                        score, ind = self.IOU.get_accuracy(p,trgts[gt_mask])\n                    else:\n                        score = 0.0\n\n                    if score > self.threshold:\n                        if mask_g[n,c,ind] == 1: #duplicate detection (false positive)\n                            tp.append(0.)\n                            fp.append(1.)\n                        else: #true positive\n                            tp.append(1.)\n                            fp.append(0.)\n                            mask_g[n,c,ind] = 1\n                    else: #below threshold (false positive)\n                        tp.append(0.)\n                        fp.append(1.)\n                else:\n                    break\n\n            tp = torch.cumsum(torch.Tensor(tp), dim=0)\n            fp = torch.cumsum(torch.Tensor(fp), dim=0)\n            ap.append(self.compute_class_ap(tp, fp, npos)) #add class Average Precision\n            \n        #Average across all classes\n        avg_ap = torch.mean(torch.Tensor(ap))\n        return avg_ap\n\n    def get_accuracy(self, detections, data):\n        """"""\n        Args:\n            detections (Tensor, shape [N,C,D,5]): predicted detections, each item [confidence, x1, y1, x2, y2]\n            data:      (dictionary)\n                - labels      (Tensor, shape [N,T,D_,5]):, each item [x1, y1, x2, y3, class] \n\n        Return:\n           Computes Average Precision  \n        """"""\n\n        gt     = data[\'labels\'].squeeze(1)\n\n        detections = detections.data\n        N,C,D,_    = detections.shape\n        _,D_,_     = gt.shape \n\n        if self.count == 0:\n            self.predictions = -1*torch.ones(self.ndata,C,D,5)\n            self._targets    = -1*torch.ones(self.ndata,D_,5)\n\n        self.predictions[self.count:self.count+N] = detections * self.scale\n        self._targets[self.count:self.count+N]    = gt \n\n        self.count += N\n\n        #Only compute Average Precision after accumulating all predictions\n        if self.count < self.ndata:\n            return -1\n\n        self.targets = -1*torch.ones(self.ndata,C,D_,4)\n        for n, trgt in enumerate(self._targets):\n            for d_ in range(D_):\n                c = trgt[d_,-1].long() + 1 #c=0 is now the background class\n\n                if c != 0:\n                    self.targets[n,c,d_] = trgt[d_,:4]\n\n        return self.get_AP(self.predictions, self.targets) \n\nclass MAP():\n\n    def __init__(self, threshold=torch.linspace(0.5,0.95,10), num_points=101, *args, **kwargs):\n        """"""\n        (COCO) Mean average precision\n\n        Args:\n            threshold  (Tensor, shape[10]): Calculate AP at each of these threshold values\n            num_points (float): number of points to average for the interpolated AP calculation\n        """"""\n\n        self.threshold = threshold\n        self.IOU = IOU(average=False)\n        self.AP = AveragePrecision(num_points=num_points, *args, **kwargs)\n\n        self.result_dir = kwargs[\'result_dir\']\n        final_shape = kwargs[\'final_shape\']\n        #assuming model predictions are normalized between 0-1\n        self.scale = torch.Tensor([1, final_shape[0], final_shape[1], final_shape[0], final_shape[1]]) #[1, height, width, height, width]\n\n        self.ndata = kwargs[\'ndata\']\n        self.count = 0\n\n    def get_mAP(self, predictions, targets):\n        """"""\n        Args:\n            predictions (Tensor, shape [N,C,D,5]): prediction bounding boxes, coordinate format [confidence, x1, y1, x2, y2]\n            targets (Tensor, shape [N,C,D_,4]): ground truth bounding boxes\n            C:  num of classes + 1 (0th class is background class, not included in calculation)\n            D:  predicted detections\n            D_: ground truth detections \n\n        Return:\n            Returns mAP score \n        """"""\n\n        AP_scores = torch.zeros(self.threshold.shape)\n\n        for n,t in enumerate(self.threshold):\n            self.AP.update_threshold(t)\n            AP_scores[n] = self.AP.get_AP(predictions, targets)\n\n        return torch.mean(AP_scores)\n\n    def get_accuracy(self, detections, data):\n        """"""\n        Args:\n            detections (Tensor, shape [N,C,D,5]): predicted detections, each item [confidence, x1, y1, x2, y2]\n            data:      (dictionary)\n                - labels      (Tensor, shape [N,T,D_,5]):, each item [x1, y1, x2, y3, class] \n\n        Return:\n            Returns mAP score \n        """"""\n        gt     = data[\'labels\'].squeeze(1)\n\n        detections = detections.data\n        N,C,D,_    = detections.shape\n        _,D_,_     = gt.shape \n\n        if self.count == 0:\n            self.predictions = -1*torch.ones(self.ndata,C,D,5)\n            self._targets    = -1*torch.ones(self.ndata,D_,5)\n\n        self.predictions[self.count:self.count+N] = detections * self.scale \n        self._targets[self.count:self.count+N]    = gt \n\n        self.count += N\n\n        #Only compute Mean Average Precision after accumulating all predictions\n        if self.count < self.ndata:\n            return -1\n\n        self.targets = -1*torch.ones(self.ndata,C,D_,4)\n        for n, trgt in enumerate(self._targets):\n            for d_ in range(D_):\n                c = trgt[d_,-1].long() + 1 #c=0 is now the background class\n\n                if c != 0:\n                    self.targets[n,c,d_] = trgt[d_,:4]\n\n        return self.get_mAP(self.predictions, self.targets)\n\nclass AverageRecall():\n    #TODO: Incomplete\n    def __init__(self, threshold=0.5, det=None, *args, **kwargs):\n        """"""\n        Compute Average Recall (AR)\n\n        Args:\n            threshold: (float)\n            det: max number of detections per image (optional)\n        """"""\n        \n        self.threshold = threshold\n        self.det = det\n        self.IOU = IOU()\n\n    def get_recall(self, predictions, targets, targets_mask):\n        """"""\n        Args:\n            predictions: shape [N,C,4], coordinate format [x1, y1, x2, y2]\n            targets: shape [N,C,4]\n            targets_mask: binary mask, shape [N,C]\n        """"""\n        iou_values = self.IOU.get_accuracy(predictions, targets) #[N,C] \n\n        TP = torch.sum((iou_values * targets_mask) >= self.threshold).float()\n        FN = torch.sum((iou_values * targets_mask) < self.threshold).float()\n\n        if self.det:\n            return TP/self.det\n        else:\n            return TP/(TP+FN)\n    \n    def get_accuracy(self, predictions, targets):\n\n        if len(targets.shape) > 2:\n            n,c,_ = targets.shape \n            targets_mask = torch.ones((n,c))\n        else: #Input shape of [N,4] is also acceptable\n            n,_ = targets.shape \n            targets_mask = torch.ones(n)\n\n        return self.get_recall(predictions, targets, targets_mask)\n\nclass SSD_AP(AveragePrecision):\n    """"""\n    Compute Average Precision from the output of the SSD model\n    Accumulates all predictions before computing AP\n    """"""\n    \n    def __init__(self, threshold=0.5, num_points=11, *args, **kwargs):\n        """"""\n        Compute Average Precision (AP)\n        Args:\n            threshold    (float): iou threshold \n            num_points   (int): number of points to average for the interpolated AP calculation\n            final_shape  (list) : [height, width] of input given to CNN\n            result_dir   (String): save detections to this location\n            ndata        (int): total number of datapoints in dataset \n\n        Return:\n            None \n        """"""\n        super(SSD_AP, self).__init__(threshold=threshold, num_points=num_points, *args, **kwargs)\n\n    def get_accuracy(self, detections, data):\n        """"""\n        Args:\n            detections (Tensor, shape [N,C,D,5]): predicted detections, each item [confidence, x1, y1, x2, y2]\n            data:      (dictionary)\n                - labels      (Tensor, shape [N,T,D_,5]):, each item [x1, y1, x2, y3, class] \n                - diff_labels (Tensor, shape [N,T,D_]):, difficult labels, each item (True or False)\n\n        Return:\n           Average Precision for SSD model  \n        """"""\n\n        gt     = data[\'labels\'].squeeze(1)\n        diff   = data[\'diff_labels\'].squeeze(1)\n\n        detections = detections.data\n        N,C,D,_    = detections.shape\n        _,D_,_     = gt.shape \n\n        if self.count == 0:\n            self.predictions = -1*torch.ones(self.ndata,C,D,5)\n            self._targets    = -1*torch.ones(self.ndata,D_,5)\n            self._diff       = torch.zeros(self.ndata,D_, dtype=torch.long)\n\n        self.predictions[self.count:self.count+N] = detections * self.scale\n        self._targets[self.count:self.count+N]    = gt \n        self._diff[self.count:self.count+N]       = diff\n\n        self.count += N\n\n        #Only compute Average Precision after accumulating all predictions\n        if self.count < self.ndata:\n            return -1\n\n        self.targets = -1*torch.ones(self.ndata,C,D_,4)\n        for n, trgt in enumerate(self._targets):\n            for d_ in range(D_):\n                c = trgt[d_,-1].long() + 1 #c=0 is now the background class\n                c = c * (1-self._diff[n,d_]) #skip difficult labels during calculation\n\n                if c != 0:\n                    self.targets[n,c,d_] = trgt[d_,:4]\n\n        return self.get_AP(self.predictions, self.targets) \n\nclass Box_Accuracy():\n    """"""\n    Box accuracy computation for YC2-BB model.\n    Adapted from: https://github.com/MichiganCOG/Video-Grounding-from-Text/blob/master/tools/test_util.py \n\n    Args:\n        accu_thres: (float)  iou threshold\n        fps:        (int)    frames per second video annotations were sampled at\n        load_type:  (String) data split, only validation has publicly available annotations\n        ndata       (int):   total number of datapoints in dataset \n\n    """"""\n    def __init__(self, *args, **kwargs):\n        from collections import defaultdict\n\n        self.result_dir = os.path.join(kwargs[\'result_dir\'], \'submission_yc2_bb.json\')\n        self.thresh     = kwargs[\'accu_thresh\']\n        self.fps        = kwargs[\'fps\']\n        self.debug      = kwargs[\'debug\']\n        self.test_mode  = 1 if kwargs[\'load_type\'] == \'test\' else 0\n        self.IOU        = IOU()\n        self.ba_score   = defaultdict(list) #box accuracy metric\n\n        if self.test_mode:\n            print(\'*\'*62)\n            print(\'* [WARNING] Eval unavailable for the test set! *\\\n                 \\n* Results will be saved to: \'+self.result_dir+\' *\\\n                 \\n* Please submit your results to the eval server!  *\')\n            print(\'*\'*62)\n\n        self.ndata = kwargs[\'ndata\']\n        self.count = 0\n        \n        self.json_data = {}\n        self.database  = {}\n\n    def get_accuracy(self, predictions, data):\n        """"""\n        Args:\n            predictions: (Tensor, shape [N,W,T,D]), attention weight output from model\n            data:      (dictionary)\n                - rpn_original      (Tensor, shape [N,T,D,4]) \n                - box               (Tensor, shape [N,O,T,5]), [cls_label, ytl, xtl, ybr, xbr] (note order in coordinates is different) \n                - box_label         (Tensor, shape [N,W]) \n                - vis_name          (List, shape [N]), unique segment identifier  \n                - class_labels_dict (dict, length 67) class index to class label mapping \n\n            T: number of frames\n            D: dimension of features\n            O: number of objects to ground \n            W: unique word in segment (from YC2BB class dictionary)\n        Return:\n           Box accuracy score  \n        """"""\n        attn_weights = predictions\n\n        N = attn_weights.shape[0] \n        self.count += N\n\n        rpn_batch         = data[\'rpn_original\']\n        box_batch         = data[\'box\']\n        obj_batch         = data[\'box_label\']\n        box_label_batch   = obj_batch \n        vis_name          = data[\'vis_name\']\n        class_labels_dict = data[\'class_labels_dict\']\n\n        # fps is the frame rate of the attention map\n        # both rpn_batch and box_batch have fps=1\n        _, T_rp, num_proposals, _ = rpn_batch.size()\n        _, O, T_gt, _ = box_batch.size()\n        T_attn = attn_weights.size(2)\n\n        assert(T_rp == T_gt) # both sampled at 1fps\n        #print(\'# of frames in gt: {}, # of frames in resampled attn. map: {}\'.format(T_gt, np.rint(T_attn/self.fps)))\n\n        hits, misses = [0 for o in range(O)], [0 for o in range(O)]\n\n        results = []\n        pos_counter = 0 \n        neg_counter = 0 \n        segment_dict = {} #segment dictionary - to output results to JSON file\n        all_objects = []\n\n        for o in range(O):\n            object_dict = {}\n            if box_label_batch[0, o] not in obj_batch[0, :]: \n                print(\'object {} is not grounded!\'.format(box_label_batch[0, o]))\n                continue # don\'t compute score if the object is not grounded\n            obj_ind_in_attn = (obj_batch[0, :] == box_label_batch[0, o]).nonzero().squeeze()\n            if obj_ind_in_attn.numel() > 1:\n                obj_ind_in_attn = obj_ind_in_attn[0]\n            else:\n                obj_ind_in_attn = obj_ind_in_attn.item()\n\n            new_attn_weights = attn_weights[0, obj_ind_in_attn]\n            _, max_attn_ind = torch.max(new_attn_weights, dim=1)\n\n            # uncomment this for the random baseline\n            # max_attn_ind = torch.floor(torch.rand(T_attn)*num_proposals).long()\n            label = class_labels_dict[box_label_batch[0,o].item()]\n            object_dict = {\'label\':label}\n        \n            boxes = []\n            for t in range(T_gt):\n                if box_batch[0,o,t,0] == -1: # object is outside/non-exist/occlusion\n                    boxes.append({\'xtl\':-1, \'ytl\':-1, \'xbr\':-1, \'ybr\':-1, \'outside\':1, \'occluded\':1}) #object is either occluded or outside of frame \n                    neg_counter += 1\n                    continue\n                pos_counter += 1\n                box_ind = max_attn_ind[int(min(np.rint(t*self.fps), T_attn-1))]\n                box_coord = rpn_batch[0, t, box_ind, :].view(4) # x_tl, y_tl, x_br, y_br\n                gt_box = box_batch[0,o,t][torch.Tensor([2,1,4,3]).type(box_batch.type()).long()].view(1,4) # inverse x and y\n\n                if self.IOU.get_accuracy(box_coord, gt_box.float())[0].item() > self.thresh:\n                    hits[o] += 1\n                else:\n                    misses[o] += 1\n\n                xtl = box_coord[0].item()\n                ytl = box_coord[1].item()\n                xbr = box_coord[2].item()\n                ybr = box_coord[3].item()\n                boxes.append({\'xtl\':xtl, \'ytl\':ytl, \'xbr\':xbr, \'ybr\':ybr, \'outside\':0, \'occluded\':0}) \n\n            object_dict[\'boxes\'] = boxes\n            all_objects.append(object_dict)\n\n            results.append((box_label_batch[0, o].item(), hits[o], misses[o]))\n\n        segment_dict[\'objects\'] = all_objects\n        #print(\'percentage of frames with box: {}\'.format(pos_counter/(pos_counter+neg_counter)))\n        \n        for (i,h,m) in results:\n            self.ba_score[i].append((h,m))\n\n        #Annotations for the testing split are not publicly available\n        if self.test_mode: \n            split, rec, video_name, segment = vis_name[0].split(\'_-_\')\n\n            if video_name not in self.database:\n                self.database[video_name] = {}\n                self.database[video_name][\'recipe_type\'] = rec\n            if \'segments\' not in self.database[video_name]:\n                self.database[video_name][\'segments\'] = {}\n\n            self.database[video_name][\'segments\'][int(segment)] = segment_dict \n\n            #Predictions will be saved to JSON file (if not in debug mode)\n            if self.count >= self.ndata and not self.debug:\n                self.json_data[\'database\'] = self.database\n\n                with open(self.result_dir, \'w\') as f:\n                    json.dump(self.json_data, f)\n\n                print(\'Saved submission file to: {}\'.format(self.result_dir))\n\n            return -1\n\n        ba_final = []\n        for k, r in self.ba_score.items():\n            cur_hit = 0 \n            cur_miss = 0 \n            for v in r:\n                cur_hit += v[0]\n                cur_miss += v[1]\n\n            if cur_hit+cur_miss != 0:\n                #print(\'BA for {}(...): {:.4f}\'.format(k, cur_hit/(cur_hit+cur_miss)))\n                ba_final.append(cur_hit/(cur_hit+cur_miss))\n\n        return np.mean(ba_final)\n'"
parse_args.py,0,"b'import argparse\nimport yaml\n\nclass Parse():\n\n    def __init__(self):\n        """"""\n        Override config args with command-line args\n        """"""\n        \n        parser = argparse.ArgumentParser()\n\n        parser.add_argument(\'--cfg_file\', type=str, default=\'config_default_example.yaml\', help=\'Configuration file with experiment parameters\')\n\n        #Command-line arguments will override any config file arguments\n        parser.add_argument(\'--rerun\',             type=int, help=\'Number of trials to repeat an experiment\')\n        parser.add_argument(\'--dataset\',           type=str, help=\'Name of dataset\')\n        parser.add_argument(\'--batch_size\',        type=int, help=\'Numbers of videos in a mini-batch\')\n        parser.add_argument(\'--pseudo_batch_loop\', type=int, help=\'Number of loops for mini-batch\')\n        parser.add_argument(\'--num_workers\',       type=int, help=\'Number of subprocesses for dataloading\')\n        parser.add_argument(\'--load_type\',         type=str, help=\'Environment selection, to include only training/training and validation/testing dataset (train, train_val, test)\')\n        parser.add_argument(\'--model\',             type=str, help=\'Name of model to be loaded\')\n        parser.add_argument(\'--labels\',            type=int, help=\'Number of total classes in the dataset\')\n\n        parser.add_argument(\'--loss_type\',    type=str,   help=\'Loss function\')\n        parser.add_argument(\'--acc_metric\',   type=str,   help=\'Accuracy metric\')\n        parser.add_argument(\'--opt\',          type=str,   help=\'Name of optimizer\')\n        parser.add_argument(\'--lr\',           type=float, help=\'Learning rate\')\n        parser.add_argument(\'--momentum\',     type=float, help=\'Momentum value in optimizer\')\n        parser.add_argument(\'--weight_decay\', type=float, help=\'Weight decay\')\n        parser.add_argument(\'--milestones\',   type=int,   nargs=\'+\',  help=\'Epoch values to change learning rate\')\n        parser.add_argument(\'--gamma\',        type=float, help=\'Multiplier with which to change learning rate\')\n        parser.add_argument(\'--epoch\',        type=int,   help=\'Total number of epochs\')\n\n        parser.add_argument(\'--json_path\',    type=str, help=\'Path to train and test json files\')\n        parser.add_argument(\'--save_dir\',     type=str, help=\'Path to results directory\')\n        parser.add_argument(\'--exp\',          type=str, help=\'Experiment name\')\n        parser.add_argument(\'--preprocess\',   type=str, help=\'Name of the preprocessing method to load\')\n        parser.add_argument(\'--pretrained\',   type=str, help=\'Load pretrained network or continue training (0 to randomly init weights, 1 to load default weights, str(path.pkl) to load checkpoint weights\')\n        parser.add_argument(\'--subtract_mean\',type=str, help=\'Subtract mean (R,G,B) from all frames during preprocessing\')\n        parser.add_argument(\'--resize_shape\', type=int, nargs=2,  help=\'(Height, Width) to resize original data\')\n        parser.add_argument(\'--final_shape\',  type=int, nargs=2,  help=\'(Height, Width) of input to be given to CNN\')\n        parser.add_argument(\'--clip_length\',  type=int, help=\'Number of frames within a clip\')\n        parser.add_argument(\'--clip_offset\',  type=int, help=\'Frame offset between beginning of video and clip (1st clip only)\')\n        parser.add_argument(\'--random_offset\',type=int, help=\'Randomly select clip_length number of frames from the video\')\n        parser.add_argument(\'--clip_stride\',  type=int, help=\'Frame offset between successive frames\')\n        parser.add_argument(\'--crop_shape\',   type=int, nargs=2,  help=\'(Height, Width) of frame\') \n        parser.add_argument(\'--crop_type\',    type=str, help=\'Type of cropping operation (Random, Center and None)\')\n        parser.add_argument(\'--num_clips\',    type=int, help=\'Number clips to be generated from a video (<0: uniform sampling, 0: Divide entire video into clips, >0: Defines number of clips)\')\n        parser.add_argument(\'--scale\',        type=float, nargs=2, help=\'[min scale, max scale] amounts to randomly scale videos for augmentation purposes. scale >1 zooms in and scale <1 zooms out.  \')\n\n\n        parser.add_argument(\'--debug\',   type=int, help=\'Run an experiment but do not save any data or create any folders\')\n        parser.add_argument(\'--seed\',    type=int, help=\'Seed for reproducibility\')\n        parser.add_argument(\'--resume\',  type=int, help=\'Flag to resume training or switch to alternate objective after loading\')\n\n        # Default dict, anything not present is required to exist as an argument or in yaml file\n        self.defaults = dict(\n            rerun            = 5,\n            batch_size       = 1,\n            pseudo_batch_loop= 1,\n            num_workers      = 1,\n            acc_metric       = None,\n            opt              = \'sgd\',\n            lr               = 0.001,\n            momentum         = 0.9,\n            weight_decay     = 0.0005,\n            milestones       = [5],\n            gamma            = 0.1,\n            epoch            = 10,\n            save_dir         = \'./results\',\n            exp              = \'exp\',\n            preprocess       = \'default\',\n            pretrained       = 0,\n            subtract_mean    = \'\',\n            clip_offset      = 0,\n            random_offset    = 0,\n            clip_stride      = 0,\n            crop_type        = None,\n            num_clips        = 1,\n            debug            = 0,\n            seed             = 0,\n            scale            = [1,1],\n            resume           = 0)                       \n\n\n\n\n        #Dictionary of the command-line arguments passed\n        self.cmd_args = vars(parser.parse_args()) \n\n        config_file = self.cmd_args[\'cfg_file\']\n        with open(config_file, \'r\') as f:\n            self.cfg_args = yaml.safe_load(f) #config file arguments\n\n    def get_args(self):\n        yaml_keys = self.cfg_args.keys() \n\n        # If pretrained is the string 0 or 1, set it to int, otherwise leave the path as a string\n        if \'pretrained\' in yaml_keys:\n            v = self.cfg_args[\'pretrained\']\n            if v==\'0\' or v==\'1\':\n                self.cfg_args[\'pretrained\'] = int(v)\n\n\n        for (k,v) in self.cmd_args.items():\n            if (k == \'pretrained\'):\n                if v==\'0\' or v==\'1\':\n                    v = int(v)\n\n            if v is not None:\n                self.cfg_args[k] = v\n            else:\n                if k not in yaml_keys:\n                    self.cfg_args[k] = self.defaults[k]\n\n\n        # Force clip_stride to be >= 1 when extracting clips from a video\n        # This represents the # of frames between successive clips \n        if self.cfg_args[\'clip_stride\'] < 1:\n            self.cfg_args[\'clip_stride\'] = 1\n\n\n\n        return self.cfg_args\n'"
train.py,11,"b'import os\nimport sys \nimport datetime\nimport yaml \nimport torch\n\nimport numpy             as np\nimport torch.nn          as nn\nimport torch.optim       as optim\n\nfrom torch.optim.lr_scheduler           import MultiStepLR\nfrom tensorboardX                       import SummaryWriter\n\nfrom parse_args                         import Parse\nfrom models.models_import               import create_model_object\nfrom datasets.loading_function          import data_loader \nfrom losses                             import Losses\nfrom metrics                            import Metrics\nfrom checkpoint                         import save_checkpoint, load_checkpoint\n\ndef train(**args):\n    """"""\n    Evaluate selected model \n    Args:\n        rerun        (Int):        Integer indicating number of repetitions for the select experiment \n        seed         (Int):        Integer indicating set seed for random state\n        save_dir     (String):     Top level directory to generate results folder\n        model        (String):     Name of selected model \n        dataset      (String):     Name of selected dataset  \n        exp          (String):     Name of experiment \n        debug        (Int):        Debug state to avoid saving variables \n        load_type    (String):     Keyword indicator to evaluate the testing or validation set\n        pretrained   (Int/String): Int/String indicating loading of random, pretrained or saved weights\n        opt          (String):     Int/String indicating loading of random, pretrained or saved weights\n        lr           (Float):      Learning rate \n        momentum     (Float):      Momentum in optimizer \n        weight_decay (Float):      Weight_decay value \n        final_shape  ([Int, Int]): Shape of data when passed into network\n        \n    Return:\n        None\n    """"""\n\n    print(""\\n############################################################################\\n"")\n    print(""Experimental Setup: "", args)\n    print(""\\n############################################################################\\n"")\n\n    for total_iteration in range(args[\'rerun\']):\n\n        # Generate Results Directory\n        d          = datetime.datetime.today()\n        date       = d.strftime(\'%Y%m%d-%H%M%S\')\n        result_dir = os.path.join(args[\'save_dir\'], args[\'model\'], \'_\'.join((args[\'dataset\'],args[\'exp\'],date)))\n        log_dir    = os.path.join(result_dir,       \'logs\')\n        save_dir   = os.path.join(result_dir,       \'checkpoints\')\n\n        if not args[\'debug\']:\n            os.makedirs(result_dir, exist_ok=True)\n            os.makedirs(log_dir,    exist_ok=True) \n            os.makedirs(save_dir,   exist_ok=True) \n\n            # Save copy of config file\n            with open(os.path.join(result_dir, \'config.yaml\'),\'w\') as outfile:\n                yaml.dump(args, outfile, default_flow_style=False)\n\n\n            # Tensorboard Element\n            writer = SummaryWriter(log_dir)\n\n        # Check if GPU is available (CUDA)\n        device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n    \n        # Load Network\n        model = create_model_object(**args).to(device)\n\n        # Load Data\n        loader = data_loader(model_obj=model, **args)\n\n        if args[\'load_type\'] == \'train\':\n            train_loader = loader[\'train\']\n            valid_loader = loader[\'train\'] # Run accuracy on train data if only `train` selected\n\n        elif args[\'load_type\'] == \'train_val\':\n            train_loader = loader[\'train\']\n            valid_loader = loader[\'valid\'] \n\n        else:\n            sys.exit(\'Invalid environment selection for training, exiting\')\n\n        # END IF\n    \n        # Training Setup\n        params     = [p for p in model.parameters() if p.requires_grad]\n\n        if args[\'opt\'] == \'sgd\':\n            optimizer  = optim.SGD(params, lr=args[\'lr\'], momentum=args[\'momentum\'], weight_decay=args[\'weight_decay\'])\n\n        elif args[\'opt\'] == \'adam\':\n            optimizer  = optim.Adam(params, lr=args[\'lr\'], weight_decay=args[\'weight_decay\'])\n        \n        else:\n            sys.exit(\'Unsupported optimizer selected. Exiting\')\n\n        # END IF\n\n        scheduler  = MultiStepLR(optimizer, milestones=args[\'milestones\'], gamma=args[\'gamma\'])\n\n        if isinstance(args[\'pretrained\'], str):\n            ckpt        = load_checkpoint(args[\'pretrained\'])\n            model.load_state_dict(ckpt)\n\n            if args[\'resume\']:\n                start_epoch = load_checkpoint(args[\'pretrained\'], key_name=\'epoch\') + 1\n\n                optimizer.load_state_dict(load_checkpoint(args[\'pretrained\'], key_name=\'optimizer\'))\n                scheduler.step(epoch=start_epoch)\n\n            else:\n                start_epoch = 0\n\n            # END IF \n\n        else:\n            start_epoch = 0\n\n        # END IF\n            \n        model_loss   = Losses(device=device, **args)\n        best_val_acc = 0.0\n\n    ############################################################################################################################################################################\n\n        # Start: Training Loop\n        for epoch in range(start_epoch, args[\'epoch\']):\n            running_loss = 0.0\n            print(\'Epoch: \', epoch)\n\n            # Setup Model To Train \n            model.train()\n\n            # Start: Epoch\n            for step, data in enumerate(train_loader):\n                if step% args[\'pseudo_batch_loop\'] == 0:\n                    loss = 0.0\n                    running_batch = 0\n                    optimizer.zero_grad()\n\n                # END IF\n\n                x_input       = data[\'data\'] \n                annotations   = data[\'annots\']\n\n                if isinstance(x_input, torch.Tensor):\n                    mini_batch_size = x_input.shape[0]\n                    outputs = model(x_input.to(device))\n\n                    assert args[\'final_shape\']==list(x_input.size()[-2:]), ""Input to model does not match final_shape argument""\n                else: #Model takes several inputs in forward function \n                    mini_batch_size = x_input[0].shape[0] #Assuming the first element contains the true data input \n                    for i, item in enumerate(x_input):\n                        if isinstance(item, torch.Tensor):\n                            x_input[i] = item.to(device)\n                    outputs = model(*x_input)\n\n                loss    = model_loss.loss(outputs, annotations)\n                loss    = loss * mini_batch_size \n                loss.backward()\n\n                running_loss  += loss.item()\n                running_batch += mini_batch_size\n\n                if np.isnan(running_loss):\n                    import pdb; pdb.set_trace()\n\n                # END IF\n\n                if not args[\'debug\']:\n                    # Add Learning Rate Element\n                    for param_group in optimizer.param_groups:\n                        writer.add_scalar(args[\'dataset\']+\'/\'+args[\'model\']+\'/learning_rate\', param_group[\'lr\'], epoch*len(train_loader) + step)\n\n                    # END FOR\n                \n                    # Add Loss Element\n                    writer.add_scalar(args[\'dataset\']+\'/\'+args[\'model\']+\'/minibatch_loss\', loss.item()/mini_batch_size, epoch*len(train_loader) + step)\n\n                # END IF\n\n                if ((epoch*len(train_loader) + step+1) % 100 == 0):\n                    print(\'Epoch: {}/{}, step: {}/{} | train loss: {:.4f}\'.format(epoch, args[\'epoch\'], step+1, len(train_loader), running_loss/float(step+1)/mini_batch_size))\n\n                # END IF\n\n                if (epoch * len(train_loader) + (step+1)) % args[\'pseudo_batch_loop\'] == 0 and step > 0:\n                    # Apply large mini-batch normalization\n                    for param in model.parameters():\n                        if param.requires_grad:\n                            param.grad *= 1./float(running_batch)\n\n                    # END FOR\n                    \n                    # Apply gradient clipping\n                    if (""grad_max_norm"" in args) and float(args[\'grad_max_norm\'] > 0):\n                        nn.utils.clip_grad_norm_(model.parameters(),float(args[\'grad_max_norm\']))\n\n                    optimizer.step()\n                    running_batch = 0\n\n                # END IF\n    \n\n            # END FOR: Epoch\n            \n            scheduler.step(epoch=epoch)\n            print(\'Schedulers lr: %f\', scheduler.get_lr()[0])\n\n            if not args[\'debug\']:\n                # Save Current Model\n                save_path = os.path.join(save_dir, args[\'dataset\']+\'_epoch\'+str(epoch)+\'.pkl\')\n                save_checkpoint(epoch, step, model, optimizer, save_path)\n   \n            # END IF: Debug\n\n            ## START FOR: Validation Accuracy\n            running_acc = []\n            running_acc = valid(valid_loader, running_acc, model, device)\n\n            if not args[\'debug\']:\n                writer.add_scalar(args[\'dataset\']+\'/\'+args[\'model\']+\'/validation_accuracy\', 100.*running_acc[-1], epoch*len(train_loader) + step)\n\n            print(\'Accuracy of the network on the validation set: %f %%\\n\' % (100.*running_acc[-1]))\n\n            # Save Best Validation Accuracy Model Separately\n            if best_val_acc < running_acc[-1]:\n                best_val_acc = running_acc[-1]\n\n                if not args[\'debug\']:\n                    # Save Current Model\n                    save_path = os.path.join(save_dir, args[\'dataset\']+\'_best_model.pkl\')\n                    save_checkpoint(epoch, step, model, optimizer, save_path)\n\n                # END IF\n\n            # END IF\n\n        # END FOR: Training Loop\n\n    ############################################################################################################################################################################\n\n        if not args[\'debug\']:\n            # Close Tensorboard Element\n            writer.close()\n\ndef valid(valid_loader, running_acc, model, device):\n    acc_metric = Metrics(**args)\n    model.eval()\n\n    with torch.no_grad():\n        for step, data in enumerate(valid_loader):\n            x_input     = data[\'data\']\n            annotations = data[\'annots\'] \n\n            if isinstance(x_input, torch.Tensor):\n                outputs = model(x_input.to(device))\n            else:\n                for i, item in enumerate(x_input):\n                    if isinstance(item, torch.Tensor):\n                        x_input[i] = item.to(device)\n                outputs = model(*x_input)\n        \n            running_acc.append(acc_metric.get_accuracy(outputs, annotations))\n\n            if step % 100 == 0:\n                print(\'Step: {}/{} | validation acc: {:.4f}\'.format(step, len(valid_loader), running_acc[-1]))\n    \n        # END FOR: Validation Accuracy\n\n    return running_acc\n\n\nif __name__ == ""__main__"":\n\n    parse = Parse()\n    args = parse.get_args()\n\n    # For reproducibility\n    torch.backends.cudnn.deterministic = True\n    torch.manual_seed(args[\'seed\'])\n\n    if not args[\'resume\']:\n        np.random.seed(args[\'seed\'])\n\n    train(**args)\n'"
datasets/DHF1K.py,3,"b""import torch\ntry:\n    from .abstract_datasets import DetectionDataset \nexcept:\n    from abstract_datasets import DetectionDataset \nimport cv2\nimport os\nimport numpy as np\nimport json\ntry:\n    import datasets.preprocessing_transforms as pt\nexcept:\n    import preprocessing_transforms as pt\n\nclass DHF1K(DetectionDataset):\n    def __init__(self, *args, **kwargs):\n        super(DHF1K, self).__init__(*args, **kwargs)\n\n        # Get model object in case preprocessing other than default is used\n        self.model_object   = kwargs['model_obj']\n        self.load_type = kwargs['load_type']\n        \n        print(self.load_type)\n        if self.load_type=='train':\n            self.transforms = kwargs['model_obj'].train_transforms\n        \n        else:\n            self.transforms = kwargs['model_obj'].test_transforms\n    \n\n\n    \n    def __getitem__(self, idx):\n        vid_info = self.samples[idx]\n\n        \n        base_path = vid_info['base_path']\n        vid_size  = vid_info['frame_size']\n\n        input_data = []\n        map_data = []\n        bin_data = []\n\n        for frame_ind in range(len(vid_info['frames'])):\n            frame      = vid_info['frames'][frame_ind]\n            frame_path = frame['img_path']\n            map_path   = frame['map_path']\n            bin_path   = frame['bin_path']\n            \n            # Load frame, convert to RGB from BGR and normalize from 0 to 1\n            input_data.append(cv2.imread(os.path.join(base_path, frame_path))[...,::-1]/255.)\n            \n            # Load frame, Normalize from 0 to 1\n            # All frame channels have repeated values\n            map_data.append(cv2.imread(map_path)/255.)\n            bin_data.append(cv2.imread(bin_path)/255.)\n\n\n\n        vid_data = self.transforms(input_data)\n\n        # Annotations must be resized in the loss/metric\n        map_data = torch.Tensor(map_data)\n        bin_data = torch.Tensor(bin_data)\n\n        # Permute the PIL dimensions (Frame, Height, Width, Chan) to pytorch (Chan, frame, height, width) \n        vid_data = vid_data.permute(3, 0, 1, 2)\n        map_data = map_data.permute(3, 0, 1, 2)\n        bin_data = bin_data.permute(3, 0, 1, 2)\n        # All channels are repeated so remove the unnecessary channels\n        map_data = map_data[0].unsqueeze(0)\n        bin_data = bin_data[0].unsqueeze(0)\n\n\n        ret_dict         = dict() \n        ret_dict['data'] = vid_data \n\n        annot_dict                = dict()\n        annot_dict['map']         = map_data\n        annot_dict['bin']         = bin_data\n        annot_dict['input_shape'] = vid_data.size()\n        annot_dict['name']        = base_path\n        ret_dict['annots']        = annot_dict\n\n        return ret_dict\n\n\nif __name__=='__main__':\n\n    class tts():\n        def __call__(self, x):\n            return pt.ToTensorClip()(x)\n    class debug_model():\n        def __init__(self):\n            self.train_transforms = tts()\n\n\n    json_path = '/path/to/DHF1K' #### Change this when testing ####\n\n\n    dataset = DHF1K(model_obj=debug_model(), json_path=json_path, load_type='train', clip_length=16, clip_offset=0, clip_stride=1, num_clips=0, random_offset=0, resize_shape=0, crop_shape=0, crop_type='Center', final_shape=0, batch_size=1)\n    train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=1, shuffle=False)\n\n\n    import matplotlib.pyplot as plt\n    for x in enumerate(train_loader):\n        dat = x[1]['data'][0,:,0].permute(1,2,0).numpy()\n        bin = x[1]['annots']['bin'][0,:,0].permute(1,2,0).numpy().repeat(3,axis=2)\n        map = x[1]['annots']['map'][0,:,0].permute(1,2,0).numpy().repeat(3, axis=2)\n        img = np.concatenate([dat,bin,map], axis=0)\n        plt.imshow(img)\n        plt.show()\n        import pdb; pdb.set_trace()\n"""
datasets/HMDB51.py,1,"b'import torch\nfrom .abstract_datasets import RecognitionDataset \nfrom PIL import Image\nimport cv2\nimport os\nimport numpy as np\nfrom torchvision import transforms\n\nclass HMDB51(RecognitionDataset):\n    def __init__(self, *args, **kwargs):\n        """"""\n        Initialize HMDB51 class  \n        Args:\n            load_type    (String): Select training or testing set \n            resize_shape (Int):    [Int, Int] Array indicating desired height and width to resize input\n            crop_shape   (Int):    [Int, Int] Array indicating desired height and width to crop input\n            final_shape  (Int):    [Int, Int] Array indicating desired height and width of input to deep network\n            preprocess   (String): Keyword to select different preprocessing types            \n\n        Return:\n            None\n        """"""\n        super(HMDB51, self).__init__(*args, **kwargs)\n\n        self.load_type    = kwargs[\'load_type\']\n        self.resize_shape = kwargs[\'resize_shape\']\n        self.crop_shape   = kwargs[\'crop_shape\']\n        self.final_shape  = kwargs[\'final_shape\']\n        self.preprocess   = kwargs[\'preprocess\']\n        \n        if self.load_type==\'train\':\n            self.transforms = kwargs[\'model_obj\'].train_transforms\n\n        else:\n            self.transforms = kwargs[\'model_obj\'].test_transforms\n\n\n    def __getitem__(self, idx):\n        vid_info  = self.samples[idx]\n        base_path = vid_info[\'base_path\']\n\n        input_data = []\n        vid_length = len(vid_info[\'frames\'])\n        vid_data   = np.zeros((vid_length, self.final_shape[0], self.final_shape[1], 3))-1\n        labels     = np.zeros((vid_length))-1\n        input_data = []\n    \n        for frame_ind in range(len(vid_info[\'frames\'])):\n            frame_path   = os.path.join(base_path, vid_info[\'frames\'][frame_ind][\'img_path\'])\n\n            for frame_labels in vid_info[\'frames\'][frame_ind][\'actions\']:\n                labels[frame_ind] = frame_labels[\'action_class\']\n\n            # Load frame image data and preprocess image accordingly\n            input_data.append(cv2.imread(frame_path)[...,::-1]/1.)\n\n\n        # Preprocess data\n        vid_data   = self.transforms(input_data)\n        labels     = torch.from_numpy(labels).float()\n\n        # Permute the PIL dimensions (Frame, Height, Width, Chan) to pytorch (Chan, frame, height, width) \n        vid_data = vid_data.permute(3, 0, 1, 2)\n\n        ret_dict           = dict() \n        ret_dict[\'data\']   = vid_data \n\n        annot_dict           = dict()\n        annot_dict[\'labels\'] = labels\n\n        ret_dict[\'annots\']   = annot_dict\n\n        return ret_dict\n\n\n#dataset = HMDB51(json_path=\'/z/dat/HMDB51\', dataset_type=\'train\', clip_length=100, num_clips=0)\n#dat = dataset.__getitem__(0)\n#import pdb; pdb.set_trace()\n'"
datasets/ImageNetVID.py,3,"b""import torch\nimport torchvision\nfrom .abstract_datasets import DetectionDataset \nimport cv2\nimport os\nimport numpy as np\nimport json\nimport datasets.preprocessing_transforms as pt\n\nclass ImageNetVID(DetectionDataset):\n    def __init__(self, *args, **kwargs):\n        super(ImageNetVID, self).__init__(*args, **kwargs)\n\n        # Get model object in case preprocessing other than default is used\n        self.model_object   = kwargs['model_obj']\n        self.load_type = kwargs['load_type']\n        self.json_path = kwargs['json_path']\n        lab_file = open(os.path.join(self.json_path, 'labels_number_keys.json'), 'r')\n        self.labels_dict = json.load(lab_file)\n        lab_file.close()\n        self.label_values = list(self.labels_dict.values())\n        self.label_values.sort()\n\n\n        # Maximum number of annotated object present in a single frame in entire dataset\n        # Dictates the return size of annotations in __getitem__\n        self.max_objects = 38 \n        \n        #self.transforms = self.model_object.get_transforms()\n        \n        if self.load_type=='train':\n            self.transforms = kwargs['model_obj'].train_transforms\n        \n        else:\n            self.transforms = kwargs['model_obj'].test_transforms\n    \n    \n    def __getitem__(self, idx):\n        vid_info = self.samples[idx]\n        \n        base_path = vid_info['base_path']\n        vid_size  = vid_info['frame_size']\n\n        input_data = []\n\n        vid_length = len(vid_info['frames'])\n        vid_data   = np.zeros((vid_length, self.final_shape[0], self.final_shape[1], 3))-1\n        bbox_data  = np.zeros((vid_length, self.max_objects, 4))-1\n        labels     = np.zeros((vid_length, self.max_objects))-1\n        occlusions = np.zeros((vid_length, self.max_objects))-1\n\n\n\n\n        for frame_ind in range(len(vid_info['frames'])):\n            frame      = vid_info['frames'][frame_ind]\n            frame_path = frame['img_path']\n            \n            # Extract bbox and label data from video info\n            for obj in frame['objs']:\n                trackid   = obj['trackid']\n                label     = obj['c']\n                occlusion = obj['occ']\n                obj_bbox  = obj['bbox'] # [xmin, ymin, xmax, ymax]\n\n                label_name = self.labels_dict[label]\n                label      = self.label_values.index(label_name)\n\n\n                bbox_data[frame_ind, trackid, :] = obj_bbox\n                labels[frame_ind, trackid]       = label \n                occlusions[frame_ind, trackid]   = occlusion\n\n            # Load frame, convert to RGB from BGR and normalize from 0 to 1\n            input_data.append(cv2.imread(os.path.join(base_path, frame_path))[...,::-1]/255.)\n\n        vid_data, bbox_data = self.transforms(input_data, bbox_data)\n\n        bbox_data = bbox_data.type(torch.LongTensor)\n        xmin_data  = bbox_data[:,:,0]\n        ymin_data  = bbox_data[:,:,1]\n        xmax_data  = bbox_data[:,:,2]\n        ymax_data  = bbox_data[:,:,3]\n        labels     = torch.from_numpy(labels)\n        occlusions = torch.from_numpy(occlusions)\n\n        # Permute the PIL dimensions (Frame, Height, Width, Chan) to pytorch (Chan, frame, height, width) \n        vid_data = vid_data.permute(3, 0, 1, 2)\n\n        ret_dict = dict() \n        ret_dict['data']       = vid_data \n        annot_dict = dict()\n        annot_dict['xmin']        = xmin_data\n        annot_dict['ymin']        = ymin_data\n        annot_dict['xmax']        = xmax_data\n        annot_dict['ymax']        = ymax_data\n        annot_dict['bbox_data']   = bbox_data\n        annot_dict['labels']      = labels\n        annot_dict['occlusions']  = occlusions\n        annot_dict['input_shape'] = vid_data.size() \n        ret_dict['annots']     = annot_dict\n\n        return ret_dict\n"""
datasets/KTH.py,1,"b'import torch\nfrom .abstract_datasets import RecognitionDataset \nfrom PIL import Image\nimport cv2\nimport os\nimport numpy as np\nfrom torchvision import transforms\n\nclass KTH(RecognitionDataset):\n    def __init__(self, *args, **kwargs):\n        """"""\n        Initialize KTH class  \n        Args:\n            load_type    (String): Select training or testing set \n            resize_shape (Int):    [Int, Int] Array indicating desired height and width to resize input\n            crop_shape   (Int):    [Int, Int] Array indicating desired height and width to crop input\n            final_shape  (Int):    [Int, Int] Array indicating desired height and width of input to deep network\n            preprocess   (String): Keyword to select different preprocessing types            \n\n        Return:\n            None\n        """"""\n        super(KTH, self).__init__(*args, **kwargs)\n\n        self.load_type    = kwargs[\'load_type\']\n        self.resize_shape = kwargs[\'resize_shape\']\n        self.crop_shape   = kwargs[\'crop_shape\']\n        self.final_shape  = kwargs[\'final_shape\']\n        self.preprocess   = kwargs[\'preprocess\']\n        \n        if self.load_type==\'train\':\n            self.transforms = kwargs[\'model_obj\'].train_transforms\n\n        else:\n            self.transforms = kwargs[\'model_obj\'].test_transforms\n\n\n    def __getitem__(self, idx):\n        vid_info  = self.samples[idx]\n        base_path = vid_info[\'base_path\']\n\n        input_data = []\n\n        vid_length = len(vid_info[\'frames\'])\n        vid_data   = np.zeros((vid_length, self.final_shape[0], self.final_shape[1], 3))-1\n        labels     = np.zeros((vid_length))-1\n        input_data = []\n    \n        for frame_ind in range(len(vid_info[\'frames\'])):\n            frame_path   = os.path.join(base_path, vid_info[\'frames\'][frame_ind][\'img_path\'])\n\n            for frame_labels in vid_info[\'frames\'][frame_ind][\'actions\']:\n                labels[frame_ind] = frame_labels[\'action_class\']\n\n            # Load frame image data and preprocess image accordingly\n            input_data.append(cv2.imread(frame_path)[...,::-1]/1.)\n\n\n        # Preprocess data\n        vid_data   = self.transforms(input_data)\n        labels     = torch.from_numpy(labels).float()\n\n        # Permute the PIL dimensions (Frame, Height, Width, Chan) to pytorch (Chan, frame, height, width) \n        vid_data = vid_data.permute(3, 0, 1, 2)\n\n        ret_dict           = dict() \n        ret_dict[\'data\']   = vid_data \n\n        annot_dict           = dict()\n        annot_dict[\'labels\'] = labels\n\n        ret_dict[\'annots\']   = annot_dict\n\n        return ret_dict\n\n\n#dataset = HMDB51(json_path=\'/z/dat/HMDB51\', dataset_type=\'train\', clip_length=100, num_clips=0)\n#dat = dataset.__getitem__(0)\n#import pdb; pdb.set_trace()\n'"
datasets/MSCOCO.py,3,"b""import torch\nfrom .abstract_datasets import DetectionDataset \nimport cv2\nimport os\nimport numpy as np\nimport datasets.preprocessing_transforms as pt\n\nclass MSCOCO(DetectionDataset):\n    def __init__(self, *args, **kwargs):\n        super(MSCOCO, self).__init__(*args, **kwargs)\n\n        self.load_type = kwargs['load_type']\n\n        # Some category labels are missing so the labels in the dataset must be indexed into the array below for the network to output consecutive labels\n        self.category_remap = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]\n\n\n        # Maximum number of annotated object present in a single frame in entire dataset\n        # Dictates the return size of annotations in __getitem__\n        self.max_objects = 93\n\n\n        if self.load_type=='train':\n            self.transforms = kwargs['model_obj'].train_transforms \n\n        else:\n            self.transforms = kwargs['model_obj'].test_transforms\n\n\n    def __getitem__(self, idx):\n        vid_info = self.samples[idx]\n        \n        base_path = vid_info['base_path']\n        vid_size  = vid_info['frame_size']\n\n        input_data = []\n        vid_length = len(vid_info['frames'])\n        vid_data   = np.zeros((vid_length, self.final_shape[0], self.final_shape[1], 3))-1\n        bbox_data  = np.zeros((vid_length, self.max_objects, 4))-1\n        labels     = np.zeros((vid_length, self.max_objects))-1\n        iscrowds   = np.zeros((vid_length, self.max_objects))-1\n\n\n\n\n        for frame_ind in range(len(vid_info['frames'])):\n            frame      = vid_info['frames'][frame_ind]\n            frame_path = frame['img_path']\n            \n            # Extract bbox and label data from video info\n            for obj in frame['objs']:\n                trackid   = obj['trackid']\n                label     = obj['c']\n                iscrowd   = obj['iscrowd']\n                obj_bbox  = obj['bbox'] # [xmin, ymin, xmax, ymax]\n                \n                # Category remap\n                label      = self.category_remap.index(label)\n\n\n                bbox_data[frame_ind, trackid, :] = obj_bbox\n                labels[frame_ind, trackid]       = label \n                iscrowds[frame_ind, trackid]     = iscrowd\n\n\n            input_data.append(cv2.imread(os.path.join(base_path, frame_path))[...,::-1])\n\n        vid_data, bbox_data = self.transforms(input_data, bbox_data)\n\n        bbox_data = bbox_data.type(torch.LongTensor)\n        xmin_data  = bbox_data[:,:,0]\n        ymin_data  = bbox_data[:,:,1]\n        xmax_data  = bbox_data[:,:,2]\n        ymax_data  = bbox_data[:,:,3]\n        labels     = torch.from_numpy(labels)\n        iscrowds   = torch.from_numpy(iscrowds)\n\n        # Permute the PIL dimensions (Frame, Height, Width, Chan) to pytorch (Chan, frame, height, width) \n        vid_data = vid_data.permute(3, 0, 1, 2)\n\n        ret_dict   = dict() \n        annot_dict = dict()\n        annot_dict['xmin']       = xmin_data\n        annot_dict['ymin']       = ymin_data\n        annot_dict['xmax']       = xmax_data\n        annot_dict['ymax']       = ymax_data\n        annot_dict['bbox_data']  = bbox_data\n        annot_dict['labels']     = labels\n        annot_dict['iscrowds']   = iscrowds\n        ret_dict['annots']       = annot_dict\n        ret_dict['data']         = vid_data \n\n        return ret_dict\n"""
datasets/Manual_Hands.py,4,"b'import torch\nimport torchvision\nfrom .abstract_datasets import DetectionDataset \nimport cv2\nimport os\nimport numpy as np\nimport json\n\nclass Manual_Hands(DetectionDataset):\n    """"""\n    Manually-annotated keypoints on hands for pose estimation.\n    Includes images from The MPII Human Pose and New Zealand Sign Language (NZSL) datasets\n\n    Source: https://arxiv.org/1704.07809\n    """"""\n    def __init__(self, *args, **kwargs):\n        super(Manual_Hands, self).__init__(*args, **kwargs)\n\n        self.load_type = kwargs[\'load_type\']\n        self.json_path = kwargs[\'json_path\']\n\n        # Maximum number of annotated object present in a single frame in entire dataset\n        # Dictates the return size of annotations in __getitem__\n        self.max_objects = 1\n        self.sigma       = 3.0\n        self.stride      = 8 #effective stride of the entire network\n\n        if self.load_type==\'train\':\n            self.transforms = kwargs[\'model_obj\'].train_transforms\n\n        else:\n            self.transforms = kwargs[\'model_obj\'].test_transforms\n\n    #Adapted from: https://github.com/namedBen/Convolutional-Pose-Machines-Pytorch\n    def gaussian_kernel(self, size_w, size_h, center_x, center_y, sigma):\n        #Outputs a gaussian heat map on defined point\n        gridy, gridx = torch.meshgrid(torch.arange(0,size_h,dtype=torch.float), torch.arange(0,size_w,dtype=torch.float))\n        D2 = (gridx - center_x)**2 + (gridy - center_y)**2\n\n        return torch.exp(-0.5 * D2 / sigma**2)\n\n    def __getitem__(self, idx):\n        vid_info = self.samples[idx]\n        \n        base_path = vid_info[\'base_path\']\n        vid_size  = vid_info[\'frame_size\']\n\n        input_data    = []\n\n        vid_length = len(vid_info[\'frames\'])\n        vid_data      = np.zeros((vid_length, self.final_shape[0], self.final_shape[1], 3))-1\n        bbox_data     = np.zeros((vid_length, self.max_objects, 4))-1\n        hand_pts_data = np.zeros((vid_length, self.max_objects, 21, 3))-1\n        labels        = np.zeros((vid_length, self.max_objects))-1\n        occlusions    = np.zeros((vid_length, self.max_objects, 22), dtype=np.int32)-1 #21 keypoints + background = 22 points\n\n        for frame_ind in range(len(vid_info[\'frames\'])):\n            frame          = vid_info[\'frames\'][frame_ind]\n            width, height  = vid_info[\'frame_size\']\n            frame_path     = frame[\'img_path\']\n            \n            # Extract bbox and label data from video info\n            for obj in frame[\'objs\']:\n                #trackid   = obj[\'trackid\'] #Let\'s ignore trackid for now, only one annotation per image\n                trackid   = 0\n                label     = 1 if obj[\'c\'] == \'left\' else 0 #1: left hand, 0: right hand\n                occluded  = obj[\'occ\']\n                obj_bbox  = obj[\'bbox\'] # [xmin, ymin, xmax, ymax]\n                body_pts  = obj[\'body_pts\'] #16 points (x,y,valid)\n                hand_pts  = obj[\'hand_pts\'] #21 points (x,y,valid)\n                head_box  = obj[\'head_box\']\n                head_size = obj[\'head_size\'] #max dim of tightest box around head\n                hand_ctr  = obj[\'hand_ctr\']\n                mpii      = obj[\'mpii\']\n\n                #During training square patch is 2.2*B where B is max(obj_bbox)\n                if self.load_type == \'train\':\n                    B = max(obj_bbox[2]-obj_bbox[0], obj_bbox[3]-obj_bbox[1])\n                else: #During testing B is 0.7*head_size\n                    B = 0.7*head_size\n\n                hand_size = 2.2*B\n                xtl       = np.clip(int(hand_ctr[0]-hand_size/2), 0, width)\n                ytl       = np.clip(int(hand_ctr[1]-hand_size/2), 0, height)\n                xbr       = np.clip(int(hand_ctr[0]+hand_size/2), 0, width)\n                ybr       = np.clip(int(hand_ctr[1]+hand_size/2), 0, height)\n\n                hand_crop = [xtl, ytl, xbr, ybr]\n                bbox_data[frame_ind, trackid, :]     = obj_bbox\n                labels[frame_ind, trackid]           = label \n                hand_pts_data[frame_ind, trackid, :] = hand_pts\n                occlusions[frame_ind, trackid]       = occluded + [0] #Add element for background\n\n            # Load frame, convert to RGB from BGR and normalize from 0 to 1\n            input_data.append(cv2.imread(os.path.join(base_path, frame_path))[...,::-1])\n\n        #Crop hand and resize, perform same transforms to ground truth keypoints\n        vid_data, hand_pts_coords = self.transforms(input_data, hand_pts_data[:,:,:,:2], hand_crop, labels)\n\n        h_width  = int(self.final_shape[1]/self.stride)\n        h_height = int(self.final_shape[0]/self.stride)\n        heatmaps = torch.zeros((22, h_width, h_height), dtype=torch.float) #heatmaps for 21 keypoints + background\n        for i,pts in enumerate(hand_pts_coords[0][0]):\n            x = pts[0] / self.stride\n            y = pts[1] / self.stride \n            heatmaps[i,:,:] = self.gaussian_kernel(h_width, h_height, x, y, self.sigma)\n\n        heatmaps[-1,:,:] = 1 - torch.max(heatmaps[:-1,:,:], dim=0)[0] #Last layer is background\n\n        vid_data = vid_data/255\n        # Permute the PIL dimensions (Frame, Height, Width, Chan) to pytorch (Chan, frame, height, width) \n        vid_data = vid_data.permute(3, 0, 1, 2)\n        vid_data = vid_data.squeeze(1) #Remove frame dimension, b/c this is an image dataset\n\n        ret_dict = dict() \n        ret_dict[\'data\']       = vid_data \n        annot_dict = dict()\n        annot_dict[\'head_size\']    = head_size\n        annot_dict[\'hand_pts\']    = hand_pts_coords \n        annot_dict[\'heatmaps\']    = heatmaps\n        annot_dict[\'labels\']      = labels\n        annot_dict[\'occ\']         = occlusions \n        annot_dict[\'frame_path\']  = frame_path \n        annot_dict[\'frame_size\']  = vid_size #width, height\n        ret_dict[\'annots\']     = annot_dict\n\n        return ret_dict\n'"
datasets/UCF101.py,1,"b'import torch\nfrom .abstract_datasets import RecognitionDataset \nfrom PIL import Image\nimport cv2\nimport os\nimport numpy as np\nimport datasets.preprocessing_transforms as pt\nfrom torchvision import transforms\n\nclass UCF101(RecognitionDataset):\n    def __init__(self, *args, **kwargs):\n        """"""\n        Initialize UCF101 class  \n        Args:\n            load_type    (String): Select training or testing set \n            resize_shape (Int):    [Int, Int] Array indicating desired height and width to resize input\n            crop_shape   (Int):    [Int, Int] Array indicating desired height and width to crop input\n            final_shape  (Int):    [Int, Int] Array indicating desired height and width of input to deep network\n            preprocess   (String): Keyword to select different preprocessing types            \n\n        Return:\n            None\n        """"""\n        super(UCF101, self).__init__(*args, **kwargs)\n\n        self.load_type    = kwargs[\'load_type\']\n        self.resize_shape = kwargs[\'resize_shape\']\n        self.crop_shape   = kwargs[\'crop_shape\']\n        self.final_shape  = kwargs[\'final_shape\']\n        self.preprocess   = kwargs[\'preprocess\']\n        \n        if self.load_type==\'train\':\n            self.transforms = kwargs[\'model_obj\'].train_transforms \n\n        else:\n            self.transforms = kwargs[\'model_obj\'].test_transforms\n\n\n    def __getitem__(self, idx):\n        vid_info  = self.samples[idx]\n        base_path = vid_info[\'base_path\']\n\n        input_data = []\n        vid_length = len(vid_info[\'frames\'])\n        vid_data   = np.zeros((vid_length, self.final_shape[0], self.final_shape[1], 3))-1\n        labels     = np.zeros((vid_length))-1\n        input_data = []\n    \n        for frame_ind in range(len(vid_info[\'frames\'])):\n            frame_path   = os.path.join(base_path, vid_info[\'frames\'][frame_ind][\'img_path\'])\n\n            for frame_labels in vid_info[\'frames\'][frame_ind][\'actions\']:\n                labels[frame_ind] = frame_labels[\'action_class\']\n            try:\n                # Load frame image data and preprocess image accordingly\n                input_data.append(cv2.imread(frame_path)[...,::-1]/1.)\n\n            except:\n                print(frame_path)\n\n\n        # Preprocess data\n        vid_data   = self.transforms(input_data)\n        labels     = torch.from_numpy(labels).float()\n\n        # Permute the PIL dimensions (Frame, Height, Width, Chan) to pytorch (Chan, frame, height, width) \n        vid_data = vid_data.permute(3, 0, 1, 2)\n\n        ret_dict           = dict() \n        ret_dict[\'data\']   = vid_data \n\n        annot_dict           = dict()\n        annot_dict[\'labels\'] = labels\n\n        ret_dict[\'annots\']   = annot_dict\n\n        return ret_dict\n\n\n#dataset = HMDB51(json_path=\'/z/dat/HMDB51\', dataset_type=\'train\', clip_length=100, num_clips=0)\n#dat = dataset.__getitem__(0)\n#import pdb; pdb.set_trace()\n'"
datasets/VOC2007.py,6,"b""import torch\nfrom .abstract_datasets import DetectionDataset \nimport cv2\nimport os\nimport numpy as np\n\nclass VOC2007(DetectionDataset):\n    def __init__(self, *args, **kwargs):\n        super(VOC2007, self).__init__(*args, **kwargs)\n\n        self.load_type = kwargs['load_type']\n\n        # Maximum number of annotated object present in a single frame in entire dataset\n        # Dictates the return size of annotations in __getitem__\n        self.max_objects = 50 #TODO: Verify real value\n\n        #Map class name to a class id\n        self.class_to_id = {'aeroplane':0,\n                            'bicycle':1,\n                            'bird':2,\n                            'boat':3,\n                            'bottle':4,\n                            'bus':5,\n                            'car':6,\n                            'cat':7,\n                            'chair':8,\n                            'cow':9,\n                            'diningtable':10,\n                            'dog':11,\n                            'horse':12,\n                            'motorbike':13,\n                            'person':14,\n                            'pottedplant':15,\n                            'sheep':16,\n                            'sofa':17,\n                            'train':18,\n                            'tvmonitor':19\n                            }\n        #TODO: maybe add a reverse mapping\n\n        if self.load_type=='train':\n            self.transforms = kwargs['model_obj'].train_transforms\n\n        else:\n            self.transforms = kwargs['model_obj'].test_transforms\n\n\n    def __getitem__(self, idx):\n        vid_info = self.samples[idx]\n        \n        base_path = vid_info['base_path']\n        vid_size  = vid_info['frame_size']\n\n        input_data  = []\n        vid_length = len(vid_info['frames'])\n        vid_data    = np.zeros((vid_length, self.final_shape[0], self.final_shape[1], 3))-1\n        bbox_data   = np.zeros((vid_length, self.max_objects, 4))-1\n        labels      = np.zeros((vid_length, self.max_objects))-1\n        diff_labels = np.zeros((vid_length, self.max_objects)) #difficult object labels\n\n        for frame_ind in range(len(vid_info['frames'])):\n            frame      = vid_info['frames'][frame_ind]\n            frame_path = frame['img_path']\n            \n            # Extract bbox and label data from video info\n            for obj in frame['objs']:\n                trackid   = obj['trackid']\n                label     = self.class_to_id[obj['c']]\n                obj_bbox  = obj['bbox'] # [xmin, ymin, xmax, ymax]\n                difficult = obj['difficult']\n                \n                bbox_data[frame_ind, trackid, :] = obj_bbox\n                labels[frame_ind, trackid]       = label \n                diff_labels[frame_ind, trackid]  = difficult \n\n            #Read each image and change from BGR to RGB\n            input_data.append(cv2.imread(os.path.join(base_path, frame_path), cv2.IMREAD_COLOR)[:,:,(2,1,0)])\n\n        vid_data, bbox_data = self.transforms(input_data, bbox_data) #preprocess frames\n\n        bbox_data   = bbox_data.type(torch.LongTensor)\n        xmin_data   = bbox_data[:,:,0]\n        ymin_data   = bbox_data[:,:,1]\n        xmax_data   = bbox_data[:,:,2]\n        ymax_data   = bbox_data[:,:,3]\n        labels      = torch.from_numpy(labels).type(torch.LongTensor)\n        diff_labels = torch.from_numpy(diff_labels).type(torch.LongTensor)\n\n        # Permute the vid dimensions (Frame, Height, Width, Chan) to PyTorch (Chan, Frame, Height, Width) \n        vid_data = vid_data.permute(3, 0, 1, 2)\n\n        ret_dict = dict() \n        ret_dict['data']       = vid_data \n\n        annot_dict = dict()\n        annot_dict['height']     = torch.Tensor([vid_size[0]])\n        annot_dict['width']      = torch.Tensor([vid_size[1]])\n        annot_dict['xmin']       = xmin_data\n        annot_dict['ymin']       = ymin_data\n        annot_dict['xmax']       = xmax_data\n        annot_dict['ymax']       = ymax_data\n        annot_dict['bbox_data']  = bbox_data\n        annot_dict['labels']     = torch.cat((bbox_data, labels.unsqueeze(2)),2) #[xmin,ymin,xmax,ymax,class_id]\n        annot_dict['diff_labels'] = diff_labels \n        ret_dict['annots'] = annot_dict\n\n        return ret_dict\n"""
datasets/YC2BB.py,13,"b""#Adapted from: https://github.com/MichiganCOG/Video-Grounding-from-Text\nimport torch\nfrom .abstract_datasets import DetectionDataset \nfrom PIL import Image\nimport cv2\nimport os\nimport csv\nimport numpy as np\n\nimport torchtext\n\nclass YC2BB(DetectionDataset):\n    '''\n    YouCook2-Bounding Boxes dataset. Introduced in weakly-supervised video object grounding task\n    Paper: https://arxiv.org/pdf/1805.02834.pdf\n\n    training: no bounding box annotations, only sentence describing sentence\n    validation: bounding box annotations and grounded words available\n    testing: bounding box annotations not publicly available, only grounded words\n    '''\n    def __init__(self, *args, **kwargs):\n        super(YC2BB, self).__init__(*args, **kwargs)\n        \n        #Define the following configuration parameters in your config_*.yaml file\n        #Or as a system arg\n        class_file           = kwargs['yc2bb_class_file']\n        num_proposals        = kwargs['yc2bb_num_proposals']\n        rpn_proposal_root    = kwargs['yc2bb_rpn_proposal_root']\n        roi_pooled_feat_root = kwargs['yc2bb_roi_pooled_feat_root']\n        self.num_frm         = kwargs['yc2bb_num_frm']\n\n        self.load_type = kwargs['load_type']\n\n        self.max_objects = 20 \n        self.num_class   = kwargs['labels']\n        self.class_dict  = _get_class_labels(class_file)\n\n        sentences_proc, segments_tuple = _get_segments_and_sentences(self.samples, self.load_type)\n\n        assert(len(sentences_proc) == len(segments_tuple))\n\n        #YC2 split names, slightly different\n        split_to_split = {'train':'training','val':'validation','test':'testing'}\n        self.yc2_split = split_to_split[self.load_type]\n\n\t# read rpn object proposals\n        self.rpn_dict = {}\n        self.rpn_chunk = []\n\n        total_num_proposals = 100 # always load all the proposals we have\n        rpn_lst_file = os.path.join(rpn_proposal_root, self.yc2_split+'-box-'+str(total_num_proposals)+'.txt')\n        rpn_chunk_file = os.path.join(rpn_proposal_root, self.yc2_split+'-box-'+str(total_num_proposals)+'.pth')\n        key_counter = len(self.rpn_dict)\n        with open(rpn_lst_file) as f:\n            rpn_lst = f.readline().split(',')\n            self.rpn_dict.update({r.strip():(i+key_counter) for i,r in enumerate(rpn_lst)})\n\n        self.rpn_chunk.append(torch.load(rpn_chunk_file))\n\n        self.rpn_chunk = torch.cat(self.rpn_chunk).cpu()\n        assert(self.rpn_chunk.size(0) == len(self.rpn_dict))\n        assert(self.rpn_chunk.size(2) == 4)\n\n        self.num_proposals = num_proposals\n        self.roi_pooled_feat_root = roi_pooled_feat_root\n\n        #Extract all dictionary words from each input sentence\n        #Only for the training set b/c it's un-annotated\n        self.sample_obj_labels = []\n        idx_to_remove = []\n        if self.load_type == 'train':\n            total_seg = len(self.samples)\n            for idx, sample in enumerate(self.samples):\n                sentence = sample['frames'][0]['sentence'].split(' ')\n                obj_label = []\n                inc_flag = 0\n                for w in sentence:\n                    if self.class_dict.get(w,-1) >= 0:\n                        obj_label.append(self.class_dict[w]) \n                        inc_flag = 1\n\n                if inc_flag:\n                    self.sample_obj_labels.append(obj_label)\n                else:\n                    idx_to_remove.append(idx)\n\n            #Remove segments without object from dictionay\n            self.samples[:] = [s for idx,s in enumerate(self.samples) if idx not in idx_to_remove]\n\n            assert(len(self.samples) == len(self.sample_obj_labels))\n\n            print('{}/{} valid segments in {} split'.format(len(self.samples), total_seg, self.load_type))\n\n        '''\n        if self.load_type=='train':\n            self.transforms = kwargs['model_obj'].train_transforms\n        else:\n            self.transforms = kwargs['model_obj'].test_transforms\n        '''\n\n    #Reverse-mapping between class index to canonical label name\n    def _get_class_labels_reverse(self):\n        return {v:k for k,v in self.class_dict.items()}\n    \n    #For the training set, extract positive and negative samples\n    def sample_rpn_regions(self, x_rpn, idx):\n        # randomly sample 5 frames from 5 uniform intervals\n        T = x_rpn.size(1)\n        itv = T*1./self.num_frm\n        ind = [min(T-1, int((i+np.random.rand())*itv)) for i in range(self.num_frm)]\n        x_rpn = x_rpn[:, ind, :]\n\n        obj_label = self.sample_obj_labels[idx]\n\n        #Generate example\n        obj_tensor = torch.tensor(obj_label, dtype=torch.long)\n        obj_tensor = torch.cat((obj_tensor, torch.LongTensor(self.max_objects - len(obj_label)).fill_(self.num_class))) #padding\n        sample     = [x_rpn, obj_tensor]\n\n        return sample \n\n    def __getitem__(self, idx):\n        vid_info = self.samples[idx]\n        \n        base_path       = vid_info['base_path']\n        width, height   = vid_info['frame_size']\n        num_frames_1fps = len(vid_info['frames'])\n        rec             = base_path.split('/')[-3]\n        vid             = base_path.split('/')[-2]\n        seg             = base_path.split('/')[-1]\n\n        bbox_data   = np.zeros((self.max_objects, num_frames_1fps, 5))-1 #[cls_label, xmin, ymin, xmax ymax]\n        labels      = np.zeros(self.max_objects)-1\n\n        for frame_ind in range(num_frames_1fps):\n            frame      = vid_info['frames'][frame_ind]\n            #frame_path = frame['img_path']\n            num_objs    = len(frame['objs'])\n            obj_label   = np.zeros((num_objs))-1 #List all unique class ids in entire segment\n            \n            # Extract bbox and label data from video info\n            for obj_ind, obj in enumerate(frame['objs']):\n                label   = self.class_dict[obj['c']]\n                trackid = obj['trackid']\n\n                if self.load_type == 'test' or self.load_type == 'train': #Annotations for test set not publicly available, train not annotated\n                    bbox_data[trackid, frame_ind] = [label, -1, -1, -1, -1] \n                else:\n                    if obj['occ'] or obj['outside']:\n                        bbox_data[trackid, frame_ind] = [-1, -1, -1, -1, -1] \n                    else:   \n                        obj_bbox  = obj['bbox'] # [xmin, ymin, xmax, ymax]\n\n                        #re-order to [ymin, xmin, ymax, xmax], rpn proposals are this way I believe\n                        new_order = [1,0,3,2]\n                        obj_bbox  = [obj_bbox[i] for i in new_order]\n                        bbox_data[trackid, frame_ind, :] = [label] + obj_bbox\n\n                obj_label[obj_ind] = label\n                labels[trackid]    = label \n\n        #Only keep annotations for valid objects\n        bbox_data = bbox_data[:num_objs, :]\n        labels    = labels[:num_objs]\n\n        obj_label = torch.from_numpy(obj_label).long()\n        num_frames = num_frames_1fps * 25 #video sampled at 25 fps\n        \n        '''\n\tif self.vis_output:\n            image_path = os.path.join(self.image_root, split, rec, vid, seg)\n            img_notrans = []\n            for i in range(num_frames):\n                img_notrans.append(self.spatial_transform_notrans(self.loader(os.path.join(image_path, '{:04d}.jpg'.format(i+1)))))\n            img_notrans = torch.stack(img_notrans, dim=1) # 3, T, H, W\n        else:\n            # no need to load raw images\n            img_notrans = torch.zeros(3, num_frames, 1, 1) # dummy\n        '''\t\n\n\t# rpn object propoals\n        rpn = []\n        x_rpn = []\n        frm=1\n\n        feat_name = vid+'_'+seg+'.pth'\n        img_name = vid+'_'+seg+'_'+str(frm).zfill(4)+'.jpg'\n        x_rpn = torch.load(os.path.join(self.roi_pooled_feat_root, self.yc2_split, feat_name))\n        while self.rpn_dict.get(img_name, -1) > -1:\n            ind = self.rpn_dict[img_name]\n            rpn.append(self.rpn_chunk[ind])\n            frm+=1\n            img_name = vid+'_'+seg+'_'+str(frm).zfill(4)+'.jpg'\n\n        rpn = torch.stack(rpn) # number of frames x number of proposals per frame x 4\n        rpn = rpn[:, :self.num_proposals, :]\n\n        x_rpn = x_rpn.permute(2,0,1).contiguous() # encoding size x number of frames x number of proposals\n        x_rpn = x_rpn[:, :, :self.num_proposals]\n\n        rpn_original = rpn-1 # convert to 1-indexed\n\n        # normalize coordidates to 0-1\n        # coordinates are 1-indexed:  (x_tl, y_tl, x_br, y_br)\n        rpn[:, :, 0] = (rpn[:, :, 0]-0.5)/width\n        rpn[:, :, 2] = (rpn[:, :, 2]-0.5)/width\n        rpn[:, :, 1] = (rpn[:, :, 1]-0.5)/height\n        rpn[:, :, 3] = (rpn[:, :, 3]-0.5)/height\n\n        assert(torch.max(rpn) <= 1)\n\n        vis_name = '_-_'.join((self.yc2_split, rec, vid, seg))\n\n        ret_dict = dict()\n        annot_dict = dict()\n\n        if self.load_type == 'train': #Training input data is generated differently\n            #Generate postive example\n            pos_sample = self.sample_rpn_regions(x_rpn, idx)\n\n            #Sample negative index \n            total_s = len(self.samples)\n            neg_index = np.random.randint(total_s)\n            #Shouldn't include any overlapping object in description\n            while len(set(obj_label).intersection(set(self.sample_obj_labels[neg_index]))) != 0:\n                neg_index = np.random.randint(total_s)\n\n            vid_info = self.samples[neg_index]\n            \n            base_path       = vid_info['base_path']\n            width, height   = vid_info['frame_size']\n            num_frames_1fps = len(vid_info['frames'])\n            rec             = base_path.split('/')[-3]\n            vid             = base_path.split('/')[-2]\n            seg             = base_path.split('/')[-1]\n\n            # rpn object propoals\n            rpn = []\n            x_rpn = []\n            frm=1\n\n            feat_name = vid+'_'+seg+'.pth'\n            img_name = vid+'_'+seg+'_'+str(frm).zfill(4)+'.jpg'\n            x_rpn = torch.load(os.path.join(self.roi_pooled_feat_root, self.yc2_split, feat_name))\n            while self.rpn_dict.get(img_name, -1) > -1:\n                ind = self.rpn_dict[img_name]\n                rpn.append(self.rpn_chunk[ind])\n                frm+=1\n                img_name = vid+'_'+seg+'_'+str(frm).zfill(4)+'.jpg'\n\n            rpn = torch.stack(rpn) # number of frames x number of proposals per frame x 4\n            rpn = rpn[:, :self.num_proposals, :]\n\n            x_rpn = x_rpn.permute(2,0,1).contiguous() # encoding size x number of frames x number of proposals\n            x_rpn = x_rpn[:, :, :self.num_proposals]\n\n            #Generate negative example\n            neg_sample = self.sample_rpn_regions(x_rpn, neg_index)\n\n            output = [torch.stack(i) for i in zip(pos_sample, neg_sample)]\n            output.append(self.load_type)\n            ret_dict['data'] = output \n\n        else: #Validation or Testing set\n            ret_dict['data']     = [x_rpn, obj_label, self.load_type] \n\n            annot_dict['box']               = bbox_data \n            annot_dict['box_label']         = obj_label \n            annot_dict['rpn']               = rpn\n            annot_dict['rpn_original']      = rpn_original \n            annot_dict['vis_name']          = vis_name\n            annot_dict['class_labels_dict'] = self._get_class_labels_reverse()\n\n        ret_dict['annots']         = annot_dict\n\n        return ret_dict\n\ndef _get_segments_and_sentences(data, split):\n    # build vocab and tokenized sentences\n    text_proc = torchtext.data.Field(sequential=True, tokenize='spacy',\n                                lower=True, batch_first=True)\n    split_sentences = []\n    split_segments = []\n\n    for dat in data:\n        rec    = dat['base_path'].split('/')[-3]\n        vid    = dat['base_path'].split('/')[-2]\n        seg    = dat['base_path'].split('/')[-1]\n        frame = dat['frames'][0]\n        segment_labels = []\n        if 'sentence' in frame: # for now, training json file only contains full sentence\n            segment_labels = frame['sentence']\n        else:\n            for obj in frame['objs']:\n                segment_labels.append(obj['c'])\n        split_sentences.append(segment_labels)\n        split_segments.append((split, rec, vid, str(seg).zfill(2))) #tuple of id (split, vid, seg)\n\n    sentences_proc = list(map(text_proc.preprocess, split_sentences)) # build vocab on train and val\n\n    print('{} sentences in {} split'.format(len(sentences_proc), split))\n\n    return sentences_proc, split_segments\n\ndef _get_class_labels(class_file):\n    class_dict = {} # both singular form & plural form are associated with the same label\n    with open(class_file) as f:\n        cls = csv.reader(f, delimiter=',')\n        for i, row in enumerate(cls):\n            for r in range(1, len(row)):\n                if row[r]:\n                    class_dict[row[r]] = int(row[0])\n\n    return class_dict\n\n"""
datasets/__init__.py,0,b'from .loading_function import data_loader\n'
datasets/abstract_datasets.py,1,"b'from abc import ABCMeta\nfrom torch.utils.data import Dataset\nimport json\nimport numpy as np\nimport os\nfrom PIL import Image\n\nclass VideoDataset(Dataset):\n    __metaclass__ = ABCMeta\n    def __init__(self, *args, **kwargs):\n        """"""\n        Args: \n            json_path:     Path to the directory containing the dataset\'s JSON file (not including the file itself)\n            load_type:     String indicating whether to load training or validation data (\'train\' or \'val\') \n            clip_length:   Number of frames in each clip that will be input into the network\n            clip_offset:   Number of frames from beginning of video to start extracting clips \n            clip_stride:   The temporal stride between clips\n            num_clips:     Number of clips to extract from each video (-1 uses the entire video)\n            resize_shape:  The shape [h, w] of each frame after resizing\n            crop_shape:    The shape [h, w] of each frame after cropping\n            crop_type:     The method used to crop (either random or center)\n            final_shape:   Final shape [h, w] of each frame after all preprocessing, this is input to network\n            random_offset: Randomly select a clip_length sized clip from a video\n        """"""\n\n        # JSON loading arguments\n        self.json_path      = kwargs[\'json_path\']\n        self.load_type      = kwargs[\'load_type\']\n        \n        # Clips processing arguments\n        self.clip_length    = kwargs[\'clip_length\']\n        self.clip_offset    = kwargs[\'clip_offset\']\n        self.clip_stride    = kwargs[\'clip_stride\']\n        self.num_clips      = kwargs[\'num_clips\']\n        self.random_offset  = kwargs[\'random_offset\']\n\n        # Frame-wise processing arguments\n        self.resize_shape   = kwargs[\'resize_shape\']\n        self.crop_shape     = kwargs[\'crop_shape\'] \n        self.crop_type      = kwargs[\'crop_type\'] \n        self.final_shape    = kwargs[\'final_shape\']\n\n        #Experiment arguments\n        self.batch_size     = kwargs[\'batch_size\']\n\n        # Creates the self.samples list which will be indexed by each __getitem__ call\n        self._getClips()\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        raise NotImplementedError(""Dataset must contain __getitem__ method which loads videos from memory."")\n\n    def _getClips(self):\n        """"""\n        Loads the JSON file associated with the videos in a datasets and processes each of them into clips\n        """"""\n        raise NotImplementedError(""Dataset must contain getClips method which loads and processes the dataset\'s JSON file."") \n\n    def _extractClips(self, video):\n        """"""\n        Processes a single video into uniform sized clips that will be loaded by __getitem__\n        Args:\n            video:       List containing a dictionary of annontations per frame\n\n        Additional Parameters:\n            self.clip_length: Number of frames extracted from each clip\n            self.num_clips:   Number of clips to extract from each video (-1 uses the entire video, 0 paritions the entire video in clip_length clips)\n            self.clip_offset: Number of frames from beginning of video to start extracting clips \n            self.clip_stride: Number of frames between clips when extracting them from videos \n            self.random_offset: Randomly select a clip_length sized clip from a video\n        """"""\n        if self.clip_offset > 0:\n            if len(video)-self.clip_offset >= self.clip_length:\n                video = video[self.clip_offset:]\n\n        if self.num_clips < 0:\n            if len(video) >= self.clip_length:\n                # Uniformly sample one clip from the video\n                final_video = [video[_idx] for _idx in np.linspace(0, len(video)-1, self.clip_length, dtype=\'int32\')]\n                final_video = [final_video]\n\n            else:\n                # Loop if insufficient elements\n                indices = np.ceil(self.clip_length/float(len(video))) # Number of times to repeat the video to exceed one clip_length\n                indices = indices.astype(\'int32\')\n                indices = np.tile(np.arange(0, len(video), 1, dtype=\'int32\'), indices) # Repeat the video indices until it exceeds a clip_length\n                indices = indices[np.linspace(0, len(indices)-1, self.clip_length, dtype=\'int32\')] # Uniformly sample clip_length frames from the looped video\n\n                final_video = [video[_idx] for _idx in indices]\n                final_video = [final_video]\n\n\n            # END IF\n\n        elif self.num_clips == 0:\n            # Divide entire video into the max number of clip_length segments\n            if len(video) >= self.clip_length:\n                indices     = np.arange(start=0, stop=len(video)-self.clip_length+1, step=self.clip_stride)\n                final_video = []\n\n                for _idx in indices:\n                    if _idx + self.clip_length <= len(video):\n                        final_video.append([video[true_idx] for true_idx in range(_idx, _idx+self.clip_length)])\n\n                # END FOR\n\n            else:\n                # Loop if insufficient elements\n                indices = np.ceil(self.clip_length/float(len(video)))\n                indices = indices.astype(\'int32\')\n                indices = np.tile(np.arange(0, len(video), 1, dtype=\'int32\'), indices)\n                indices = indices[:self.clip_length]\n\n                final_video = [video[_idx] for _idx in indices]\n                final_video = [final_video]\n\n            # END IF                               \n    \n        else:\n            # num_clips > 0, select exactly num_clips from a video\n\n            if self.clip_length == -1:\n                # This is a special case where we will return the entire video\n\n                # Batch size must equal one or dataloader items may have varying lengths \n                # and can\'t be stacked i.e. throws an error\n                assert(self.batch_size == 1) \n                return [video]\n\n\n            required_length = (self.num_clips-1)*(self.clip_stride)+self.clip_length\n\n\n            if self.random_offset:\n                if len(video) >= required_length:\n                    vid_start = np.random.choice(np.arange(len(video) - required_length + 1), 1)\n                    video = video[int(vid_start):]\n\n            if len(video) >= required_length:\n                # Get indices of sequential clips overlapped by a clip_stride number of frames\n                indices = np.arange(0, len(video), self.clip_stride)\n\n                # Select only the first num clips\n                indices = indices.astype(\'int32\')[:self.num_clips]\n\n                video = np.array(video)\n                final_video = [video[np.arange(_idx, _idx+self.clip_length).astype(\'int32\')].tolist() for _idx in indices]\n\n            else:\n                # If the video is too small to get num_clips given the clip_length and clip_stride, loop it until you can\n                indices = np.ceil(required_length /float(len(video)))\n                indices = indices.astype(\'int32\')\n                indices = np.tile(np.arange(0, len(video), 1, dtype=\'int32\'), indices)\n\n                # Starting index of each clip\n                clip_starts = np.arange(0, len(indices), self.clip_stride).astype(\'int32\')[:self.num_clips]\n\n                video = np.array(video)\n                final_video = [video[indices[_idx:_idx+self.clip_length]].tolist() for _idx in clip_starts]\n            \n            # END IF\n\n        # END IF\n\n        return final_video\n\n\n        \n\n\nclass RecognitionDataset(VideoDataset):\n    __metaclass__ = ABCMeta\n    def __init__(self, *args, **kwargs):\n        super(RecognitionDataset, self).__init__(*args, **kwargs)\n        self.load_type = kwargs[\'load_type\']\n\n    def _getClips(self, *args, **kwargs):\n        """"""\n        Required format for all recognition dataset JSON files:\n        \n        List(Vidnumber: Dict{\n                   List(Frames: Dict{\n                                     Frame Size,\n                                     Frame Path,\n                                     List(Actions: Dict{\n                                                        Track ID\n                                                        Action Class\n                                     }) End Object List in Frame\n        \n                   }) End Frame List in Video\n        \n                   Str(Base Vid Path)\n        \n             }) End Video List in Dataset\n        \n        Eg: action_label = dataset[vid_index][\'frames\'][frame_index][\'actions\'][action_index][\'action_class\']\n        """"""\n\n        self.samples   = []\n \n        if self.load_type == \'train\':\n            full_json_path = os.path.join(self.json_path, \'train.json\')\n\n        elif self.load_type == \'val\':\n            full_json_path = os.path.join(self.json_path, \'val.json\') \n\n            #If val.json doesn\'t exist, it will default to test.json\n            if not os.path.exists(full_json_path):\n                full_json_path = os.path.join(self.json_path, \'test.json\')\n\n        else:\n            full_json_path = os.path.join(self.json_path, \'test.json\')\n\n        # END IF \n\n        json_file = open(full_json_path,\'r\')\n        json_data = json.load(json_file) \n        json_file.close()\n\n        # Load the information for each video and process it into clips\n        for video_info in json_data:\n            clips = self._extractClips(video_info[\'frames\'])\n\n            # Each clip is a list of dictionaries per frame containing information\n            # Example info: object bbox annotations, object classes, frame img path\n            for clip in clips:    \n                self.samples.append(dict(frames=clip, base_path=video_info[\'base_path\']))\n\n\n\nclass DetectionDataset(VideoDataset):\n    __metaclass__ = ABCMeta\n    def __init__(self, *args, **kwargs):\n        super(DetectionDataset, self).__init__(*args, **kwargs)\n        self.load_type = kwargs[\'load_type\']\n\n    def _getClips(self):\n        """"""\n        Required format for all detection datset JSON files:\n        Json (List of dicts) List where each element contains a dict with annotations for a video:\n            Dict{\n            \'frame_size\' (int,int): Width, Height for all frames in video\n            \'base_path\' (str): The path to the folder containing frame images for the video\n            \'frame\' (List of dicts): A list with annotation dicts per frame\n                Dict{\n                \'img_path\' (Str): File name of the image corresponding to the frame annotations\n                \'objs\' (List of dicts): A list of dicts containing annotations for each object in the frame  \n                    Dict{\n                    \'trackid\' (Int): Id of the current object\n                    \'c\' (Str or Int): Value indicating the class of the current object \n                    \'bbox\' (int, int, int, int): Bbox coordinates of the current object in the current frame (xmin, ymin, xmax, ymax)\n                    (Optional) \'iscrowd\' (int): Boolean indicating if the object represents a crowed (Used in MSCOCO dataset)\n                    (Optional) \'occ\' (int): Boolean indicating if the object is occluded in the current frame (Used in ImageNetVID dataset)\n                    }\n                }\n            }\n            \n        \n        Ex: coordinates = dataset[vid_index][\'frames\'][frame_index][\'objs\'][obj_index][\'bbox\']\n        """"""\n\n        # Load all video paths into the samples array to be loaded by __getitem__ \n\n        self.samples   = []\n        \n        if self.load_type == \'train\':\n            full_json_path = os.path.join(self.json_path, \'train.json\')\n\n        elif self.load_type == \'val\':\n            full_json_path = os.path.join(self.json_path, \'val.json\') \n\n            #If val.json doesn\'t exist, it will default to test.json\n            if not os.path.exists(full_json_path):\n                full_json_path = os.path.join(self.json_path, \'test.json\')\n\n        else:\n            full_json_path = os.path.join(self.json_path, \'test.json\')\n\n        json_file = open(full_json_path,\'r\')\n        json_data = json.load(json_file) \n        json_file.close()\n\n        # Load the information for each video and process it into clips\n        for video_info in json_data:\n            clips = self._extractClips(video_info[\'frames\'])\n\n            # Each clip is a list of dictionaries per frame containing information\n            # Example info: object bbox annotations, object classes, frame img path\n            for clip in clips:    \n                self.samples.append(dict(frames=clip, base_path=video_info[\'base_path\'], frame_size=video_info[\'frame_size\']))\n\n\n'"
datasets/loading_function.py,4,"b'import torch\nimport importlib\nimport sys\nimport glob\n\ndef create_dataset_object(**kwargs):\n    """"""\n    Use dataset_name to find a matching dataset class\n\n    Args:\n        kwargs: arguments specifying dataset and dataset parameters\n\n    Returns:\n        dataset: initialized dataset object \n    """"""\n\n    dataset_name = kwargs[\'dataset\']\n\n    dataset_files = glob.glob(\'datasets/*.py\')\n    ignore_files = [\'__init__.py\', \'loading_function.py\', \'abstract_datasets.py\', \'preprocessing_transforms.py\']\n\n    for df in dataset_files:\n        if df in ignore_files:\n            continue\n\n        module_name = df[:-3].replace(\'/\',\'.\')\n        module = importlib.import_module(module_name)\n        module_lower = list(map(lambda module_x: module_x.lower(), dir(module)))\n\n        if dataset_name.lower() in module_lower:\n            dataset_index = module_lower.index(dataset_name.lower())\n            dataset = getattr(module, dir(module)[dataset_index])(**kwargs)\n\n            return dataset \n\n    sys.exit(\'Dataset not found. Ensure dataset is in datasets/, with a matching class name\')\n\n\ndef data_loader(**kwargs):\n    """"""\n    Args:\n        dataset:    The name of the dataset to be loaded\n        batch_size: The number of clips to load in each batch\n        train_type: (test, train, or train_val) indicating whether to load clips to only train or train and validate or to test\n    """"""\n\n    load_type = kwargs[\'load_type\']\n    if load_type == \'train_val\':\n        kwargs[\'load_type\'] = \'train\'\n        train_data = create_dataset_object(**kwargs) \n        kwargs[\'load_type\'] = \'val\' \n        val_data   = create_dataset_object(**kwargs) \n        kwargs[\'load_type\'] = load_type \n\n        trainloader = torch.utils.data.DataLoader(dataset=train_data, batch_size=kwargs[\'batch_size\'], shuffle=True,  num_workers=kwargs[\'num_workers\'])\n        valloader   = torch.utils.data.DataLoader(dataset=val_data,   batch_size=kwargs[\'batch_size\'], shuffle=False, num_workers=kwargs[\'num_workers\'])\n        ret_dict    = dict(train=trainloader, valid=valloader)\n\n    elif load_type == \'train\':\n        data = create_dataset_object(**kwargs)\n\n        loader = torch.utils.data.DataLoader(dataset=data, batch_size=kwargs[\'batch_size\'], shuffle=True, num_workers=kwargs[\'num_workers\'])\n        ret_dict = dict(train=loader)\n\n    else:\n        data = create_dataset_object(**kwargs)\n\n        loader = torch.utils.data.DataLoader(dataset=data, batch_size=kwargs[\'batch_size\'], shuffle=False, num_workers=kwargs[\'num_workers\'])\n        ret_dict = dict(test=loader)\n\n\n    # END IF\n\n    return ret_dict\n\n'"
datasets/preprocessing_transforms.py,16,"b'""""""\nFunctions used to process and augment video data prior to passing into a model to train. \nAdditionally also processing all bounding boxes in a video according to the transformations performed on the video.\n\nUsage:\n    In a custom dataset class:\n    from preprocessing_transforms import *\n\nclip: Input to __call__ of each transform is a list of PIL images\n\nAll functions have an example in the TestPreproc class at the bottom of this file\n""""""\n\nimport torch\nimport torchvision\nfrom torchvision.transforms import functional as F\nfrom PIL import Image\nfrom PIL import ImageChops\nimport cv2\nfrom scipy import ndimage\nimport numpy as np\nfrom abc import ABCMeta\nfrom math import floor, ceil\n\nclass PreprocTransform(object):\n    """"""\n    Abstract class for preprocessing transforms that contains methods to convert clips to PIL images.\n    """"""\n    __metaclass__ = ABCMeta\n    def __init__(self, **kwargs):\n        pass\n\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def _to_pil(self, clip):\n        # Must be of type uint8 if images have multiple channels, int16, int32, or float32 if there is only one channel\n        if isinstance(clip[0], np.ndarray):\n            if \'float\' in str(clip[0].dtype):\n                clip = np.array(clip).astype(\'float32\')\n            if \'int64\' == str(clip[0].dtype):\n                clip = np.array(clip).astype(\'int32\')\n            if clip[0].ndim == 3:\n                clip = np.array(clip).astype(\'uint8\')\n\n        output=[]\n        for frame in clip:\n            output.append(F.to_pil_image(frame))\n        \n        return output\n\n\n    def _to_numpy(self, clip):\n        output = []\n        if isinstance(clip[0], torch.Tensor):\n            if isinstance(clip, torch.Tensor):\n                output = clip.numpy()\n            else:\n                for frame in clip:\n                    f_shape = frame.shape\n                    # Convert from torch\'s C, H, W to numpy H, W, C\n                    frame = frame.numpy().reshape(f_shape[1], f_shape[2], f_shape[0])\n\n                    output.append(frame)\n            \n\n        elif isinstance(clip[0], Image.Image):\n            for frame in clip:\n                output.append(np.array(frame))\n\n        else:\n            output = clip \n\n        output = np.array(output)\n\n        #if output.max() > 1.0:\n        #    output = output/255.\n\n        return output\n\n\n    def _to_tensor(self, clip):\n        """"""\n        torchvision converts PIL images and numpy arrays that are uint8 0 to 255 to float 0 to 1\n        Converts numpy arrays that are float to float tensor\n        """"""\n            \n        if isinstance(clip[0], torch.Tensor):\n            return clip\n\n        output = []\n        for frame in clip:\n            output.append(F.to_tensor(frame))\n        \n        return output\n\n\n\n\n\nclass ResizeClip(PreprocTransform):\n    def __init__(self, *args, **kwargs):\n        super(ResizeClip, self).__init__(*args, **kwargs)\n        self.size_h, self.size_w = kwargs[\'resize_shape\']\n        \n    def resize_bbox(self, xmin, ymin, xmax, ymax, img_shape, resize_shape):\n        # Resize a bounding box within a frame relative to the amount that the frame was resized\n    \n        img_h = img_shape[0]\n        img_w = img_shape[1]\n    \n        res_h = resize_shape[0]\n        res_w = resize_shape[1]\n    \n        frac_h = res_h/float(img_h)\n        frac_w = res_w/float(img_w)\n    \n        xmin_new = int(xmin * frac_w)\n        xmax_new = int(xmax * frac_w)\n    \n        ymin_new = int(ymin * frac_h)\n        ymax_new = int(ymax * frac_h)\n    \n        return xmin_new, ymin_new, xmax_new, ymax_new \n\n    def resize_pt_coords(self, x, y, img_shape, resize_shape):\n        # Get relative position for point coords within a frame, after it\'s resized\n\n        img_h = img_shape[0]\n        img_w = img_shape[1]\n    \n        res_h = resize_shape[0]\n        res_w = resize_shape[1]\n    \n        frac_h = res_h/float(img_h)\n        frac_w = res_w/float(img_w)\n\n        x_new = (x * frac_w).astype(int)\n        y_new = (y * frac_h).astype(int)\n\n        return x_new, y_new\n\n    def __call__(self, clip, bbox=[]):\n\n        clip = self._to_numpy(clip)\n        out_clip = []\n        out_bbox = []\n        for frame_ind in range(len(clip)):\n            frame = clip[frame_ind]\n\n            proc_frame = cv2.resize(frame, (self.size_w, self.size_h))\n            out_clip.append(proc_frame)\n            if bbox!=[]:\n                temp_bbox = np.zeros(bbox[frame_ind].shape)-1 \n                for class_ind, box in enumerate(bbox[frame_ind]):\n                    if np.array_equal(box,-1*np.ones(box.shape)): #only annotated objects\n                        continue\n\n                    if box.shape[-1] == 2: #Operate on point coordinates\n                        proc_bbox = np.stack(self.resize_pt_coords(box[:,0], box[:,1], frame.shape, (self.size_h, self.size_w)),1)\n                    else: #Operate on bounding box\n                        xmin, ymin, xmax, ymax = box\n                        proc_bbox = self.resize_bbox(xmin, ymin, xmax, ymax, frame.shape, (self.size_h, self.size_w))\n\n                    temp_bbox[class_ind,:] = proc_bbox\n                out_bbox.append(temp_bbox)\n\n        out_clip = np.array(out_clip)\n\n        assert(out_clip.shape[1:3] == (self.size_h, self.size_w)), \'Proc frame: {} Crop h,w: {},{}\'.format(out_clip.shape,self.size_h,self.size_w)\n\n        if bbox!=[]:\n            return out_clip, np.array(out_bbox)\n        else:\n            return out_clip\n\n\nclass CropClip(PreprocTransform):\n    def __init__(self, xmin=None, xmax=None, ymin=None, ymax=None, *args, **kwargs):\n        super(CropClip, self).__init__(*args, **kwargs)\n        self.crop_xmin = xmin\n        self.crop_xmax = xmax\n        self.crop_ymin = ymin\n        self.crop_ymax = ymax\n\n        self.crop_h, self.crop_w = kwargs[\'crop_shape\']\n\n\n    def _update_bbox(self, xmin, xmax, ymin, ymax, update_crop_shape=False):\n        \'\'\'\n            Args:\n                xmin (Float, shape []):\n                xmax (Float, shape []):\n                ymin (Float, shape []):\n                ymax (Float, shape []):\n                update_crop_shape (Boolean): Update expected crop shape along with bbox update call \n        \'\'\'\n        self.crop_xmin = xmin\n        self.crop_xmax = xmax\n        self.crop_ymin = ymin\n        self.crop_ymax = ymax\n\n        if update_crop_shape:\n            self.crop_h = ymax - ymin\n            self.crop_w = xmax - xmin \n\n    def crop_bbox(self, xmin, ymin, xmax, ymax, crop_xmin, crop_ymin, crop_xmax, crop_ymax):\n        if (xmin >= crop_xmax) or (xmax <= crop_xmin) or (ymin >= crop_ymax) or (ymax <= crop_ymin):\n            return -1, -1, -1, -1\n    \n        if ymax > crop_ymax:\n            ymax_new = crop_ymax\n        else:\n            ymax_new = ymax\n    \n        if xmax > crop_xmax:\n            xmax_new = crop_xmax\n        else:\n            xmax_new = xmax\n        \n        if ymin < crop_ymin:\n            ymin_new = crop_ymin\n        else:\n            ymin_new = ymin \n    \n        if xmin < crop_xmin:\n            xmin_new = crop_xmin\n        else:\n            xmin_new = xmin \n    \n        return xmin_new-crop_xmin, ymin_new-crop_ymin, xmax_new-crop_xmin, ymax_new-crop_ymin\n\n    def crop_coords(self, x, y, crop_xmin, crop_ymin, crop_xmax, crop_ymax):\n        if np.any(x >= crop_xmax) or np.any(x <= crop_xmin) or np.any(y >= crop_ymax) or np.any(y <= crop_ymin):\n            return -1*np.ones(x.shape), -1*np.ones(y.shape)\n\n        x_new = np.clip(x, crop_xmin, crop_xmax)\n        y_new = np.clip(y, crop_ymin, crop_ymax)\n\n        return x_new-crop_xmin, y_new-crop_ymin \n  \n    def __call__(self, clip, bbox=[]):\n        out_clip = []\n        out_bbox = []\n\n        for frame_ind in range(len(clip)):\n            frame = clip[frame_ind]\n            proc_frame = np.array(frame[self.crop_ymin:self.crop_ymax, self.crop_xmin:self.crop_xmax]) \n            out_clip.append(proc_frame)\n\n            assert(proc_frame.shape[:2] == (self.crop_h, self.crop_w)), \'Frame shape: {}, Proc frame: {} Crop h,w: {},{}\'.format(frame.shape, proc_frame.shape,self.crop_h,self.crop_w)\n\n            if bbox!=[]:\n                temp_bbox = np.zeros(bbox[frame_ind].shape)-1 \n                for class_ind, box in enumerate(bbox[frame_ind]):\n                    if np.array_equal(box,-1*np.ones(box.shape)): #only annotated objects\n                        continue\n\n                    if box.shape[-1] == 2: #Operate on point coordinates\n                        proc_bbox = np.stack(self.crop_coords(box[:,0], box[:,1], self.crop_xmin, self.crop_ymin, self.crop_xmax, self.crop_ymax), 1)\n                    else: #Operate on bounding box\n                        xmin, ymin, xmax, ymax = box\n                        proc_bbox = self.crop_bbox(xmin, ymin, xmax, ymax, self.crop_xmin, self.crop_ymin, self.crop_xmax, self.crop_ymax)\n                    temp_bbox[class_ind,:] = proc_bbox\n                out_bbox.append(temp_bbox)\n\n        if bbox!=[]:\n            return np.array(out_clip), np.array(out_bbox)\n        else:\n            return np.array(out_clip)\n\n\nclass RandomCropClip(PreprocTransform):\n    def __init__(self, *args, **kwargs):\n        super(RandomCropClip, self).__init__(*args, **kwargs)\n        self.crop_h, self.crop_w = kwargs[\'crop_shape\']\n\n        self.crop_transform = CropClip(0, 0, self.crop_w, self.crop_h, **kwargs)\n\n        self.xmin = None\n        self.xmax = None\n        self.ymin = None\n        self.ymax = None\n\n\n    def _update_random_sample(self, frame_h, frame_w):\n        if frame_w == self.crop_w:\n            self.xmin = 0\n        else:\n            self.xmin = np.random.randint(0, frame_w-self.crop_w)   \n\n        self.xmax = self.xmin + self.crop_w\n\n        if frame_h == self.crop_h:\n            self.ymin = 0\n        else:\n            self.ymin = np.random.randint(0, frame_h-self.crop_h)\n        \n        self.ymax = self.ymin + self.crop_h\n\n    def get_random_sample(self):\n        return self.xmin, self.xmax, self.ymin, self.ymax\n        \n    def __call__(self, clip, bbox=[]):\n        frame_shape = clip[0].shape\n        self._update_random_sample(frame_shape[0], frame_shape[1])\n        self.crop_transform._update_bbox(self.xmin, self.xmax, self.ymin, self.ymax) \n        proc_clip = self.crop_transform(clip, bbox)\n        if isinstance(proc_clip, tuple):\n            assert(proc_clip[0].shape[1:3] == (self.crop_h, self.crop_w)), \'Proc frame: {} Crop h,w: {},{}\'.format(proc_clip[0].shape,self.crop_h,self.crop_w)\n        else:\n            assert(proc_clip.shape[1:3] == (self.crop_h, self.crop_w)), \'Proc frame: {} Crop h,w: {},{}\'.format(proc_clip.shape,self.crop_h,self.crop_w)\n        return proc_clip \n\nclass CenterCropClip(PreprocTransform):\n    def __init__(self, *args, **kwargs):\n        super(CenterCropClip, self).__init__(*args, **kwargs)\n        self.crop_h, self.crop_w = kwargs[\'crop_shape\']\n\n        self.crop_transform = CropClip(0, 0, self.crop_w, self.crop_h, **kwargs)\n\n    def _calculate_center(self, frame_h, frame_w):\n        xmin = int(frame_w/2 - self.crop_w/2)\n        xmax = int(frame_w/2 + self.crop_w/2)\n        ymin = int(frame_h/2 - self.crop_h/2)\n        ymax = int(frame_h/2 + self.crop_h/2)\n        return xmin, xmax, ymin, ymax\n        \n    def __call__(self, clip, bbox=[]):\n        frame_shape = clip[0].shape\n        xmin, xmax, ymin, ymax = self._calculate_center(frame_shape[0], frame_shape[1])\n        self.crop_transform._update_bbox(xmin, xmax, ymin, ymax) \n        proc_clip =  self.crop_transform(clip, bbox)\n        if isinstance(proc_clip, tuple):\n            assert(proc_clip[0].shape[1:3] == (self.crop_h, self.crop_w)), \'Proc frame: {} Crop h,w: {},{}\'.format(proc_clip[0].shape,self.crop_h,self.crop_w)\n        else:\n            assert(proc_clip.shape[1:3] == (self.crop_h, self.crop_w)), \'Proc frame: {} Crop h,w: {},{}\'.format(proc_clip.shape,self.crop_h,self.crop_w)\n        return proc_clip\n\n\nclass RandomFlipClip(PreprocTransform):\n    """"""   \n    Specify a flip direction:\n    Horizontal, left right flip = \'h\' (Default)\n    Vertical, top bottom flip = \'v\'\n    """"""\n    def __init__(self, direction=\'h\', p=0.5, *args, **kwargs):\n        super(RandomFlipClip, self).__init__(*args, **kwargs)\n        self.direction = direction\n        self.p = p\n\n    def _update_p(self, p):\n        self.p = p\n            \n    def _random_flip(self):\n        flip_prob = np.random.random()\n        if flip_prob >= self.p:\n            return 0\n        else:\n            return 1\n\n    def _h_flip(self, bbox, frame_size):\n        width = frame_size[1]\n        bbox_shape = bbox.shape\n        output_bbox = np.zeros(bbox_shape)-1\n        for bbox_ind, box in enumerate(bbox):\n            if np.array_equal(box,-1*np.ones(box.shape)): #only annotated objects\n                continue\n\n            if box.shape[-1] == 2: #Operate on point coordinates\n                x = box[:,0]\n                x_new = width - x\n\n                output_bbox[bbox_ind] = np.stack((x_new,box[:,1]),1)\n            else: #Operate on bounding box\n                xmin, ymin, xmax, ymax = box\n                xmax_new = width - xmin\n                xmin_new = width - xmax\n                output_bbox[bbox_ind] = xmin_new, ymin, xmax_new, ymax\n        return output_bbox\n\n    def _v_flip(self, bbox, frame_size):\n        height = frame_size[0]\n        bbox_shape = bbox.shape\n        output_bbox = np.zeros(bbox_shape)-1\n        for bbox_ind, box in enumerate(bbox):\n            if np.array_equal(box,-1*np.ones(box.shape)): #only annotated objects\n                continue\n\n            if box.shape[-1] == 2: #Operate on point coordinates\n                y = box[:,1]\n                y_new = height - y\n\n                output_bbox[bbox_ind] = np.stack((box[:,0],y_new),1)\n            else: #Operate on bounding box\n                xmin, ymin, xmax, ymax = box\n                ymax_new = height - ymin\n                ymin_new = height - ymax\n                output_bbox[bbox_ind] = xmin, ymin_new, xmax, ymax_new\n        return output_bbox\n\n        bbox_shape = bbox.shape\n        output_bbox = np.zeros(bbox_shape)-1\n        for bbox_ind in range(bbox_shape[0]):\n            xmin, ymin, xmax, ymax = bbox[bbox_ind] \n            height = frame_size[0]\n            ymax_new = height - ymin \n            ymin_new = height - ymax\n            output_bbox[bbox_ind] = xmin, ymin_new, xmax, ymax_new\n        return output_bbox \n\n\n    def _flip_data(self, clip, bbox=[]):\n        output_bbox = []\n        \n        if self.direction == \'h\':\n            output_clip = [cv2.flip(np.array(frame), 1) for frame in clip]\n            \n            if bbox!=[]:\n                output_bbox = [self._h_flip(frame, output_clip[0].shape) for frame in bbox] \n\n        elif self.direction == \'v\':\n            output_clip = [cv2.flip(np.array(frame), 0) for frame in clip]\n\n            if bbox!=[]:\n                output_bbox = [self._v_flip(frame, output_clip[0].shape) for frame in bbox]\n\n        return output_clip, output_bbox \n        \n\n    def __call__(self, clip, bbox=[]):\n        input_shape = np.array(clip).shape\n        flip = self._random_flip()\n        out_clip = clip\n        out_bbox = bbox\n        if flip:\n            out_clip, out_bbox = self._flip_data(clip, bbox)\n\n        out_clip = np.array(out_clip)\n        assert(input_shape == out_clip.shape), ""Input shape {}, output shape {}"".format(input_shape, out_clip.shape)\n\n        if bbox!=[]:\n            return out_clip, out_bbox\n        else:\n            return out_clip\n\nclass ToTensorClip(PreprocTransform):\n    """"""\n    Convert a list of PIL images or numpy arrays to a 5 dimensional pytorch tensor [batch, frame, channel, height, width]\n    """"""\n    def __init__(self, *args, **kwargs):\n        super(ToTensorClip, self).__init__(*args, **kwargs)\n\n        self.transform = torchvision.transforms.ToTensor()\n\n    def __call__(self, clip, bbox=[]):\n        \n        if isinstance(clip[0], Image.Image):\n            # a little round-about but it maintains consistency\n            temp_clip = []\n            for c in clip:\n                temp_clip.append(np.array(c))\n            clip = temp_clip \n\n        output_clip = torch.from_numpy(np.array(clip)).float() #Numpy array to Tensor\n\n        if bbox!=[]:\n            bbox = torch.from_numpy(np.array(bbox))\n            return output_clip, bbox\n        else:\n            return output_clip\n        \n\nclass RandomRotateClip(PreprocTransform):\n    """"""\n    Randomly rotate a clip from a fixed set of angles.\n    The rotation is counterclockwise\n    """"""\n    def __init__(self,  angles=[0,90,180,270], *args, **kwargs):\n        super(RandomRotateClip, self).__init__(*args, **kwargs)\n        self.angles = angles\n\n    ######\n    # Code from: https://stackoverflow.com/questions/20924085/python-conversion-between-coordinates\n    def _cart2pol(self, point):\n        x,y = point\n        rho = np.sqrt(x**2 + y**2)\n        phi = np.arctan2(y, x)\n        return(rho, phi)\n    \n    def _pol2cart(self, point):\n        rho, phi = point\n        x = rho * np.cos(phi)\n        y = rho * np.sin(phi)\n        return(x, y)\n    #####\n\n    def _update_angles(self, angles):\n        self.angles=angles\n\n\n    def _rotate_bbox(self, bboxes, frame_shape, angle):\n        angle = np.deg2rad(angle)\n        bboxes_shape = bboxes.shape\n        output_bboxes = np.zeros(bboxes_shape)-1\n        frame_h, frame_w = frame_shape[0], frame_shape[1] \n        half_h = frame_h/2. \n        half_w = frame_w/2. \n\n        for bbox_ind in range(bboxes_shape[0]):\n            xmin, ymin, xmax, ymax = bboxes[bbox_ind]\n            tl = (xmin-half_w, ymax-half_h)\n            tr = (xmax-half_w, ymax-half_h)\n            bl = (xmin-half_w, ymin-half_h)\n            br = (xmax-half_w, ymin-half_h)\n\n            tl = self._cart2pol(tl) \n            tr = self._cart2pol(tr)    \n            bl = self._cart2pol(bl)\n            br = self._cart2pol(br)\n\n            tl = (tl[0], tl[1] - angle)\n            tr = (tr[0], tr[1] - angle)\n            bl = (bl[0], bl[1] - angle)\n            br = (br[0], br[1] - angle)\n\n            tl = self._pol2cart(tl) \n            tr = self._pol2cart(tr)    \n            bl = self._pol2cart(bl)\n            br = self._pol2cart(br)\n\n            tl = (tl[0]+half_w, tl[1]+half_h)\n            tr = (tr[0]+half_w, tr[1]+half_h)\n            bl = (bl[0]+half_w, bl[1]+half_h)\n            br = (br[0]+half_w, br[1]+half_h)\n\n            xmin_new = int(np.clip(min(floor(tl[0]), floor(tr[0]), floor(bl[0]), floor(br[0])), 0, frame_w-1))\n            xmax_new = int(np.clip(max(ceil(tl[0]), ceil(tr[0]), ceil(bl[0]), ceil(br[0])), 0, frame_w-1))\n            ymin_new = int(np.clip(min(floor(tl[1]), floor(tr[1]), floor(bl[1]), floor(br[1])), 0, frame_h-1))\n            ymax_new = int(np.clip(max(ceil(tl[1]), ceil(tr[1]), ceil(bl[1]), ceil(br[1])), 0, frame_h-1))\n\n            output_bboxes[bbox_ind] = [xmin_new, ymin_new, xmax_new, ymax_new]\n\n        return output_bboxes\n\n\n    def _rotate_coords(self, bboxes, frame_shape, angle):\n        angle = np.deg2rad(angle)\n        bboxes_shape = bboxes.shape\n        output_bboxes = np.zeros(bboxes_shape)-1\n        frame_h, frame_w = frame_shape[0], frame_shape[1] \n        half_h = frame_h/2. \n        half_w = frame_w/2. \n\n        for bbox_ind in range(bboxes_shape[0]):\n            x, y = bboxes[bbox_ind].transpose()\n\n            pts  = (x-half_w, y-half_h)\n\n            pts = self._cart2pol(pts)\n\n            pts = (pts[0], pts[1]-angle)\n\n            pts = self._pol2cart(pts)\n\n            pts  = (pts[0]+half_w, pts[1]+half_h)\n\n            output_bboxes[bbox_ind,:,0] = (np.clip(pts[0], 0, frame_w-1))\n            output_bboxes[bbox_ind,:,1] = (np.clip(pts[1], 0, frame_h-1))\n\n        return output_bboxes\n\n    def __call__(self, clip, bbox=[]):\n        angle = np.random.choice(self.angles)\n        output_clip = []\n        clip = self._to_numpy(clip)\n        for frame in clip:\n            output_clip.append(ndimage.rotate(frame, angle, reshape=False))\n\n        if bbox!=[]:\n            bbox = np.array(bbox)\n            output_bboxes = np.zeros(bbox.shape)-1\n            for bbox_ind in range(bbox.shape[0]):\n                if bbox.shape[-1] == 2:\n                    output_bboxes[bbox_ind] = self._rotate_coords(bbox[bbox_ind], clip[0].shape, angle)\n                else:\n                    output_bboxes[bbox_ind] = self._rotate_bbox(bbox[bbox_ind], clip[0].shape, angle)\n\n            return output_clip, output_bboxes \n\n        return output_clip\n\nclass RandomTranslateClip(PreprocTransform):\n    """"""\n    Random horizontal and/or vertical shift on frames in a clip. All frames receive same shifting \n    Shift will be bounded by object bounding box (if given). Meaning, object will always be in view\n    Input numpy array must be of type np.uint8\n\n    Args:\n        - translate (Tuple)\n            - max_x (float): maximum absolute fraction for horizontal shift \n            - max_y (float): maximum absolute fraction for vertical shift \n    """"""\n    def __init__(self, translate, **kwargs):\n        super(RandomTranslateClip, self).__init__(**kwargs)\n\n        self.max_x, self.max_y = translate\n\n        assert(self.max_x >= 0.0 and self.max_y >= 0.0)\n        assert(self.max_x < 1.0  and self.max_y < 1.0) #Cannot shift past image bounds\n\n    def _shift_frame(self, bbox, frame, tx, ty):\n        M       = np.array([[1, 0, tx],[0, 1, ty]], dtype=np.float) # 2 x 3 transformation matrix\n        out_frame = cv2.warpAffine(frame, M, (frame.shape[1], frame.shape[0]))\n\n        if bbox is not None:\n            bbox_h = np.reshape(bbox, (-1,2)) #x-y coords\n            bbox_h = np.concatenate((bbox_h, np.ones((bbox_h.shape[0],1))), axis=1).transpose() #homography coords\n\n            out_box = M @ bbox_h\n\n            if bbox.shape[-1] == 2: #Operate on point coordinates\n                out_box = np.reshape(out_box.transpose(), (bbox.shape[0], bbox.shape[1],2))\n            else: #Operate on bounding box\n                out_box = np.reshape(out_box.transpose(), (-1,4))\n\n            return out_frame, out_box \n        else:\n            return out_frame \n\n    def __call__(self, clip, bbox=[]):\n        out_clip = []\n        clip = self._to_numpy(clip)\n\n        frac_x = np.random.rand()*(2*self.max_x)-self.max_x \n        frac_y = np.random.rand()*(2*self.max_y)-self.max_y  \n\n        if bbox != []:\n            out_bbox = []\n            \n            for frame, box in zip(clip,bbox):\n                img_h, img_w = frame.shape[:2] \n                tx = int(img_w * frac_x)\n                ty = int(img_h * frac_y) \n\n                #Bound translation amount so all objects remain in scene\n                if box.shape[-1] == 2: #Operate on point coordinates\n                    mask = box[:,:,0] != -1\n                    tx = np.clip(tx, np.max(-1*box[mask,0]), np.min(img_w-box[mask,0]))\n                    ty = np.clip(ty, np.max(-1*box[mask,1]), np.min(img_h-box[mask,1]))\n                    out_frame, out_box = self._shift_frame(box, frame, tx, ty)\n                    out_box[~mask] = -1*np.ones(2)\n\n                else: #Operate on bounding box \n                    #bbox is bounding box object\n                    mask = box[:,0] != -1\n                    tx = np.clip(tx, np.max(-1*box[mask,0]), np.min(img_w-box[mask,2]))\n                    ty = np.clip(ty, np.max(-1*box[mask,1]), np.min(img_h-box[mask,3]))\n                    out_frame, out_box = self._shift_frame(box, frame, tx, ty)\n                    out_box[~mask] = -1*np.ones(4)\n\n                out_clip.append(out_frame)\n                out_bbox.append(out_box)\n\n            return out_clip, out_bbox \n        else:\n            for frame in clip:\n                img_h, img_w = frame.shape[:2] \n                tx = int(img_w * frac_x)\n                ty = int(img_h * frac_y) \n\n                out_clip.append(self._shift_frame(None, frame, tx, ty))\n\n            return out_clip \n\nclass RandomZoomClip(PreprocTransform):\n    """"""\n    Random zoom on all frames in a clip. All frames receive same scaling\n    Scale will be bounded by object bounding box (if given). Meaning, object will always be in view\n    If zooming out, the borders will be filled with black.\n\n    >1: Zoom in\n    <1: Zoom out\n    =1: Same size\n\n    Args:\n        - scale (Tuple)\n            - min_scale (float): minimum scaling on frame \n            - max_scale (float): maximum scaling on frame  \n    """"""\n    def __init__(self, scale, **kwargs):\n        super(RandomZoomClip, self).__init__(**kwargs)\n\n        self.min_scale, self.max_scale = scale\n\n        assert(self.min_scale > 0 and self.min_scale <= self.max_scale)\n\n    def _scale_frame(self, bbox, frame, sc):\n        M = cv2.getRotationMatrix2D((frame.shape[1]/2, frame.shape[0]/2), 0, sc) # 2 x 3 rotation matrix\n        out_frame = cv2.warpAffine(frame, M, (frame.shape[1], frame.shape[0]))\n\n        if bbox is not None:\n            bbox_h = np.reshape(bbox, (-1,2)) #x-y coords\n            bbox_h = np.concatenate((bbox_h, np.ones((bbox_h.shape[0],1))), axis=1).transpose() #homography coords\n\n            out_box = M @ bbox_h\n\n            if bbox.shape[-1] == 2: #Operate on point coordinates\n                out_box = np.reshape(out_box.transpose(), (bbox.shape[0], bbox.shape[1],2))\n            else: #Operate on bounding box\n                out_box = np.reshape(out_box.transpose(), (-1,4))\n\n            return out_frame, out_box \n        else:\n            return out_frame \n\n    def __call__(self, clip, bbox=[]):\n        out_clip = []\n        clip = self._to_numpy(clip)\n\n        sc = np.random.uniform(self.min_scale, self.max_scale) \n\n        if bbox != []:\n            out_bbox = []\n            \n            for frame, box in zip(clip,bbox):\n                img_h, img_w = frame.shape[:2]\n                cx, cy = (img_w/2, img_h/2)\n\n                #Bound scaling so all objects remain in scene\n                if box.shape[-1] == 2: #Operate on point coordinates\n                    mask = box[:,:,0] != -1\n\n                    max_x = min(img_w, np.max(cx + sc * (box[mask,0] - cx)))\n                    min_x = max(0, np.min(cx + sc * (box[mask,0] - cx)))\n                    sx = (max_x - cx) / np.max(box[mask,0] - cx)\n                    if min_x == 0:\n                        sx = min(sx, (min_x - cx) / np.min(box[mask,0] - cx))\n\n                    max_y = min(img_h, np.max(cy + sc * (box[mask,1] - cy)))\n                    min_y = max(0, np.min(cy + sc * (box[mask,1] - cy)))\n                    sy = (max_y - cy) / np.max(box[mask,1] - cy)\n                    if min_y == 0:\n                        sy = min(sy, (min_y - cy) / np.min(box[mask,1] - cy))\n            \n                    sc = min(sx, sy)\n                    out_frame, out_box = self._scale_frame(box, frame, sc)\n                    out_box[~mask] = -1*np.ones(2)\n\n                else: #Operate on bounding box \n                    mask = box[:,0] != -1\n\n                    max_x = min(img_w, np.max(cx + sc * (box[mask,2] - cx)))\n                    min_x = max(0, np.min(cx + sc * (box[mask,0] - cx)))\n                    sx = (max_x - cx) / np.max(box[mask,2] - cx)\n                    if min_x == 0:\n                        sx = min(sx, (min_x - cx) / np.min(box[mask,0] - cx))\n\n                    max_y = min(img_h, np.max(cy + sc * (box[mask,3] - cy)))\n                    min_y = max(0, np.min(cy + sc * (box[mask,1] - cy)))\n                    sy = (max_y - cy) / np.max(box[mask,3] - cy)\n                    if min_y == 0:\n                        sy = min(sy, (min_y - cy) / np.min(box[mask,1] - cy))\n            \n                    sc = min(sx, sy)\n                    out_frame, out_box = self._scale_frame(box, frame, sc)\n                    out_box[~mask] = -1*np.ones(4)\n\n                out_clip.append(out_frame)\n                out_bbox.append(out_box)\n\n            return out_clip, out_bbox \n        else:\n            for frame in clip:\n                img_h, img_w = frame.shape[:2]\n                sx = int(img_w * sc)\n                sy = int(img_h * sc) \n\n                out_clip.append(self._scale_frame(None, frame, sc))\n            return out_clip \n\nclass SubtractMeanClip(PreprocTransform):\n    def __init__(self, **kwargs):\n        super(SubtractMeanClip, self).__init__(**kwargs)\n#        self.clip_mean = torch.tensor(kwargs[\'clip_mean\']).float()\n        self.clip_mean = kwargs[\'clip_mean\']\n#        self.clip_mean      = []\n#\n#        for frame in self.clip_mean_args:\n#            self.clip_mean.append(Image.fromarray(frame))\n\n        \n    def __call__(self, clip, bbox=[]):\n        #clip = clip-self.clip_mean\n        for clip_ind in range(len(clip)):\n            clip[clip_ind] = clip[clip_ind] - self.clip_mean[clip_ind]\n\n        \n        if bbox!=[]:\n            return clip, bbox\n\n        else:\n            return clip\n\nclass SubtractRGBMean(PreprocTransform):\n    def __init__(self, **kwargs):\n        super(SubtractRGBMean, self).__init__(**kwargs)\n        self.rgb_mean = kwargs[\'subtract_mean\']\n    \n    def __call__(self, clip, bbox=[]):\n\n        clip = self._to_numpy(clip)\n        out_clip = []\n        out_bbox = []\n\n        for frame_ind in range(len(clip)):\n            frame = clip[frame_ind]\n\n            proc_frame = frame - self.rgb_mean\n            out_clip.append(proc_frame)\n\n        if bbox != []:\n            return out_clip, bbox\n        else:\n            return out_clip\n\nclass ApplyToPIL(PreprocTransform):\n    """"""\n    Apply standard pytorch transforms that require PIL images as input to their __call__ function, for example Resize\n\n    NOTE: The __call__ function of this class converts the clip to a list of PIL images in the form of integers from 0-255. If the clips are floats (for example afer mean subtraction), then only call this transform before the float transform\n\n    Bounding box coordinates are not guaranteed to be transformed properly!\n\n    https://pytorch.org/docs/stable/_modules/torchvision/transforms/transforms.html\n    """"""\n    def __init__(self, **kwargs):\n        """"""\n        class_kwargs is a dictionary that contains the keyword arguments to be passed to the chosen pytorch transform class\n        """"""\n        super(ApplyToPIL, self).__init__( **kwargs)\n        self.kwargs = kwargs\n        self.class_kwargs = kwargs[\'class_kwargs\']\n        self.transform = kwargs[\'transform\'](**self.class_kwargs)\n\n    def __call__(self, clip, bbox=[]):\n        input_pil = True\n        output_clip = []\n\n        if not isinstance(clip[0], Image.Image):\n            clip = self._to_pil(clip)\n            clip = [frame.convert(\'RGB\') for frame in clip]\n            input_pil = False\n\n        if input_pil:\n            for frame in clip:\n                transformed_frame = self.transform(frame)\n                if isinstance(transformed_frame, tuple) or isinstance(transformed_frame, list):\n                    for tf in transformed_frame:\n                        output_clip.append(tf)\n                else:\n                    output_clip.append(self.transform(frame)) #Apply transform and convert back to Numpy\n\n        else:\n            for frame in clip:\n                transformed_frame = self.transform(frame)\n                if isinstance(transformed_frame, tuple) or isinstance(transformed_frame, list):\n                    for tf in transformed_frame:\n                        output_clip.append(np.array(tf))\n                else:\n                    output_clip.append(np.array(self.transform(frame))) #Apply transform and convert back to Numpy\n\n        if bbox!=[]:\n            return output_clip, bbox\n\n        else:\n            return output_clip\n\n\nclass ApplyToTensor(PreprocTransform):\n    """"""\n    Apply standard pytorch transforms that require pytorch Tensors as input to their __call__ function, for example Normalize \n\n    NOTE: The __call__ function of this class converts the clip to a pytorch float tensor. If other transforms require PIL inputs, call them prior tho this one\n    Bounding box coordinates are not guaranteed to be transformed properly!\n\n    https://pytorch.org/docs/stable/_modules/torchvision/transforms/transforms.html\n    """"""\n    def __init__(self, **kwargs):\n        super(ApplyToTensor, self).__init__(**kwargs)\n        self.kwargs = kwargs\n        self.class_kwargs = kwargs[\'class_kwargs\']\n        self.transform = kwargs[\'transform\'](**self.class_kwargs)\n\n    def __call__(self, clip, bbox=[]):\n        if not isinstance(clip, torch.Tensor):\n            clip = self._to_tensor(clip)\n\n        output_clip = []\n        for frame in clip:\n            output_clip.append(self.transform(frame))\n\n        output_clip = torch.stack(output_clip)\n\n        if bbox!=[]:\n            return output_clip, bbox\n\n        else:\n            return output_clip\n\nclass ApplyOpenCV(PreprocTransform):\n    """"""\n    Apply opencv transforms that require numpy arrays as input to their __call__ function, for example Rotate \n\n    NOTE: The __call__ function of this class converts the clip to a Numpy array. If other transforms require PIL inputs, call them prior tho this one\n\n    Bounding box coordinates are not guaranteed to be transformed properly!\n    """"""\n    def __init__(self, **kwargs):\n        super(ApplyOpenCV, self).__init__(**kwargs)\n        self.kwargs = kwargs\n        self.class_kwargs = kwargs[\'class_kwargs\']\n        self.transform = kwargs[\'transform\']\n\n    def __call__(self, clip, bbox=[]):\n        if not isinstance(clip, torch.Tensor):\n            clip = self._to_numpy(clip)\n\n        output_clip = []\n        for frame in clip:\n            output_clip.append(self.transform(frame, **self.class_kwargs))\n\n\n        if bbox!=[]:\n            return output_clip, bbox\n\n        else:\n            return output_clip\n\n\n\n\nclass TestPreproc(object):\n    def __init__(self):\n        self.resize = ResizeClip(resize_shape = [2,2])\n        self.crop = CropClip(0,0,0,0, crop_shape=[2,2])\n        self.rand_crop = RandomCropClip(crop_shape=[2,2])\n        self.cent_crop = CenterCropClip(crop_shape=[2,2])\n        self.rand_flip_h = RandomFlipClip(direction=\'h\', p=1.0)\n        self.rand_flip_v = RandomFlipClip(direction=\'v\', p=1.0)\n        self.rand_rot = RandomRotateClip(angles=[90])\n        self.rand_trans = RandomTranslateClip(translate=(0.5,0.5))\n        self.rand_zoom  = RandomZoomClip(scale=(1.25,1.25)) \n        self.sub_mean = SubtractMeanClip(clip_mean=np.zeros(1))\n        self.applypil = ApplyToPIL(transform=torchvision.transforms.ColorJitter, class_kwargs=dict(brightness=1))\n        self.applypil2 = ApplyToPIL(transform=torchvision.transforms.FiveCrop, class_kwargs=dict(size=(64,64)))\n        self.applytensor = ApplyToTensor(transform=torchvision.transforms.Normalize, class_kwargs=dict(mean=torch.tensor([0.,0.,0.]), std=torch.tensor([1.,1.,1.])))\n        self.applycv = ApplyOpenCV(transform=cv2.threshold, class_kwargs=dict(thresh=100, maxval=100, type=cv2.THRESH_TRUNC))\n        self.preproc = PreprocTransform()\n\n    def resize_test(self):\n        inp = np.array([[[.1,.2,.3,.4],[.1,.2,.3,.4],[.1,.2,.3,.4]]]).astype(float)\n        inp2 = np.array([[[.1,.1,.1,.1],[.2,.2,.2,.2],[.3,.3,.3,.3]]]).astype(float)\n        expected_out = np.array([[[.15,.35],[.15,.35]]]).astype(float)\n        expected_out2 = np.array([[[.125,.125],[.275,.275]]]).astype(float)\n        out = self.resize(inp)\n        out2 = self.resize(inp2)\n        assert (False not in np.isclose(out,expected_out)) and (False not in np.isclose(out2,expected_out2))\n        \n        bbox = np.array([[[0,0,3,3]]]).astype(float)\n        _, bbox_out = self.resize(inp, bbox)\n        exp_bbox = np.array([[[0,0,1,2]]])\n        assert (False not in np.isclose(bbox_out, exp_bbox))\n\n        coord_pts = np.array([[[[1,1], [7,5], [9,6]]]]).astype(float)\n        _, bbox_out = self.resize(inp, coord_pts)\n        exp_bbox = np.array([[[[0., 0.],\n                               [3., 3.],\n                               [4., 4.]]]])\n        assert (False not in np.isclose(bbox_out, exp_bbox))\n\n\n    def crop_test(self):\n        inp = np.array([[[.1,.2,.3],[.4,.5,.6],[.7,.8,.9]]]).astype(float)\n        self.crop._update_bbox(1, 3, 1, 3)\n\n        exp_out = np.array([[[.5,.6],[.8,.9]]]).astype(float)\n        out = self.crop(inp)\n        assert (False not in np.isclose(out,exp_out))\n\n    def cent_crop_test(self):\n        inp = np.array([[[.1,.2,.3,.4],[.1,.2,.3,.4],[.1,.2,.3,.4],[.1,.2,.3,.4]]]).astype(float)\n        exp_out = np.array([[[.2,.3],[.2,.3]]]).astype(float)\n        out = self.cent_crop(inp)\n        assert (False not in np.isclose(out, exp_out))\n\n    def rand_crop_test(self):\n        inp = np.array([[[.1,.2,.3,.4],[.3,.4,.5,.6],[.2,.3,.4,.5],[.1,.2,.3,.4]]]).astype(float)\n        out = self.rand_crop(inp)\n        coords = self.rand_crop.get_random_sample()\n        exp_out = np.array(inp[:, coords[2]:coords[3], coords[0]:coords[1]]).astype(float)\n        assert (False not in np.isclose(out, exp_out))\n\n    def rand_flip_test(self):\n        inp = np.array([[[.1,.2,.3],[.4,.5,.6],[.7,.8,.9]]]).astype(float)\n        exp_outh = np.array([[[.3,.2,.1],[.6,.5,.4],[.9,.8,.7]]]).astype(float)\n        exp_outv = np.array([[[.7,.8,.9],[.4,.5,.6],[.1,.2,.3]]]).astype(float)\n        outh = self.rand_flip_h(inp)\n        outv = self.rand_flip_v(inp)       \n        assert (False not in np.isclose(outh,exp_outh)) and (False not in np.isclose(outv,exp_outv))\n\n\n        inp2 = np.arange(36).reshape(6,6)\n        bbox = np.array([[[0,0,2,2]]]).astype(float)\n        exp_bboxh = np.array([[[4,0,6,2]]]).astype(float)\n        exp_bboxv = np.array([[[0,4,2,6]]]).astype(float)\n        _, bboxh = self.rand_flip_h([inp2], bbox)\n        _, bboxv = self.rand_flip_v([inp2], bbox)       \n         \n        assert (False not in np.isclose(bboxh, exp_bboxh)) and (False not in np.isclose(bboxv, exp_bboxv))\n\n    def rand_flip_vis(self):\n        import matplotlib.pyplot as plt\n        x = np.arange(112*112).reshape(112,112)\n        x[:, 10] = 10000\n        x[:, 50] = 5000\n        x[10, :] = 5000\n        x[50, :] = 10000\n\n        plt.subplot(1,3,1); plt.imshow(x); plt.title(\'Original image\')\n        h = self.rand_flip_h([x])\n        plt.subplot(1,3,2); plt.imshow(h[0]); plt.title(\'Flip Horizontal\')\n        v = self.rand_flip_v([x])\n        plt.subplot(1,3,3); plt.imshow(v[0]); plt.title(\'Flip Vertical\')\n        \n        plt.show()\n\n    def rand_rot_test(self):\n        inp = np.array([[[.1,.2,.3],[.4,.5,.6],[.7,.8,.9]]]).astype(float)\n        exp_out = np.array([[[.3,.6,.9],[.2,.5,.8],[.1,.4,.7]]]).astype(float)\n\n        out = self.rand_rot(inp)\n\n\n        self.rand_rot._update_angles([45])\n        inp2 = np.arange(6*6).reshape(6,6)\n        bbox = [[2,2,4,4]]\n        exp_bbox = [1,1,5,5]\n        out_bbox = self.rand_rot([inp2], np.array([bbox]))[1][0].tolist()\n        assert (False not in np.isclose(out, exp_out)) and (False not in np.isclose(exp_bbox, out_bbox))\n\n\n    def rand_trans_test(self):\n        x = np.arange(112*112).reshape(112,112).astype(np.uint8)\n        out = self.rand_trans([x])\n        out2 = self.rand_trans([x], bbox=[np.array([[32,32,96,96]])])\n\n        assert (out2[1][0].min() >= 0) and (out[0].shape==(112,112)) and (out2[0][0].shape==(112,112))\n\n    def rand_rot_vis(self):\n        import matplotlib.pyplot as plt\n        import matplotlib.patches as patches \n        angle = 45\n        self.rand_rot._update_angles([angle])\n        x = np.arange(112*112).reshape(112,112)\n\n        bbox = [30,40,50,100]\n        pts = np.array([[30,40],[30,80]])\n        fig = plt.figure()\n        ax1 = fig.add_subplot(121)\n        x[bbox[1]:bbox[3], bbox[0]] = 0\n        x[bbox[1]:bbox[3], bbox[2]-1] = 0\n        x[bbox[1], bbox[0]:bbox[2]] = 0\n        x[bbox[3]-1, bbox[0]:bbox[2]] = 0\n        \n        ax1.imshow(x); ax1.set_title(\'Original image\')\n        rect = patches.Rectangle((bbox[0],bbox[1]), bbox[2]-bbox[0],\\\n                                  bbox[3]-bbox[1], linewidth=1, edgecolor=\'k\', facecolor=\'none\')\n        #ax1.add_patch(rect)\n        ax1.scatter(pts[:,0], pts[:,1], c=\'r\')\n\n        out2 = self.rand_rot([x], np.array([[bbox]]))\n        x_rot = out2[0][0]\n        bbox_rot = out2[1][0,0]\n\n        out2 = self.rand_rot([x], np.array([[pts]]))\n        pts_rot  = out2[1][0,0]\n\n        ax2 = fig.add_subplot(122)\n        rect = patches.Rectangle((bbox_rot[0],bbox_rot[1]), bbox_rot[2]-bbox_rot[0],\\\n                                  bbox_rot[3]-bbox_rot[1], linewidth=1, edgecolor=\'k\', facecolor=\'none\')\n        ax2.add_patch(rect)\n        ax2.imshow(x_rot); ax2.set_title(\'Rotation: {} degress\'.format(angle))\n        ax2.scatter(pts_rot[:,0],pts_rot[:,1], c=\'r\')\n        plt.show()\n\n    def rand_zoom_test(self):\n        inp = np.array([[[.1,.2,.3],[.4,.5,.6],[.7,.8,.9]]]).astype(float)\n        exp_out = np.array([[0.225   , 0.303125, 0.384375],\n                            [0.459375, 0.5375  , 0.61875 ],\n                            [0.703125, 0.78125 , 0.8625  ]]).astype(float)\n        out = self.rand_zoom(inp)\n\n        inp2 = np.arange(6*6, dtype=np.uint8).reshape(6,6)\n        bbox = [[2,2,4,4]]\n        exp_bbox = [1.75,1.75,4.25,4.25]\n        _,out_bbox = self.rand_zoom([inp2], np.array([bbox]))\n\n        assert (False not in np.isclose(out, exp_out)) and (False not in np.isclose(exp_bbox, out_bbox))\n\n    def rand_zoom_vis(self):\n        import matplotlib.pyplot as plt\n        import matplotlib.patches as patches \n        x = np.arange(112*112, dtype=np.uint8).reshape(112,112)\n\n        bbox = [30,40,50,100]\n        pts = np.array([[30,40],[30,80]])\n        fig = plt.figure()\n        ax1 = fig.add_subplot(121)\n\n        x[bbox[1]:bbox[3], bbox[0]] = 0\n        x[bbox[1]:bbox[3], bbox[2]-1] = 0\n        x[bbox[1], bbox[0]:bbox[2]] = 0\n        x[bbox[3]-1, bbox[0]:bbox[2]] = 0\n        ax1.imshow(x); ax1.set_title(\'Original image\')\n        ax1.scatter(pts[:,0], pts[:,1], c=\'r\')\n\n        out = self.rand_zoom([x], np.array([[pts]]))\n        pts_zoom = out[1][0][0]\n\n        out = self.rand_zoom([x], np.array([[bbox]]))\n        x_zoom = out[0][0]\n        bbox_zoom = out[1][0][0]\n\n        ax2 = fig.add_subplot(122)\n        rect = patches.Rectangle((bbox_zoom[0],bbox_zoom[1]), bbox_zoom[2]-bbox_zoom[0],\\\n                                  bbox_zoom[3]-bbox_zoom[1], linewidth=1, edgecolor=\'k\', facecolor=\'none\')\n        ax2.add_patch(rect)\n        ax2.imshow(x_zoom); ax2.set_title(\'Zoomed image\')\n        ax2.scatter(pts_zoom[:,0],pts_zoom[:,1], c=\'r\')\n        \n        plt.show()\n\n    def applypil_test(self):\n        inp = np.arange(112*112).reshape(112,112)\n        np_inp = [inp, inp]\n        inp = self.applypil._to_pil([inp, inp])\n        inp = [inp[0].convert(\'RGB\'), inp[1].convert(\'RGB\')]\n        out = self.applypil(inp)\n        out2 = self.applypil2(out)\n        out3 = self.applypil(np_inp)\n        assert (len(out2)==2*5) and (out2[0].size==(64,64)) and (isinstance(out2[0], Image.Image)) and (isinstance(out3[0], np.ndarray))\n\n    def applytensor_test(self):\n        inp = np.arange(112*112*3).reshape(3,112,112).astype(\'float32\')\n        inp = torch.from_numpy(inp)\n        out = self.applytensor([inp, inp])\n        assert False not in np.array(inp==out)\n\n\n    def applycv_test(self):\n        inp = np.arange(112*112).reshape(112,112).astype(\'float32\')\n        out = self.applycv([inp])\n        assert (out[0][1].min()==0.0) and (out[0][1].max()==100.0) \n\n\n    def to_numpy_test(self):\n        inp_torch = [torch.zeros((3,112,112))]\n        inp_pil = [Image.fromarray(np.zeros((112,112,3)).astype(\'uint8\'))]\n        out_torch = self.preproc._to_numpy(inp_torch)\n        out_pil = self.preproc._to_numpy(inp_pil)\n\n        assert (False not in np.array(out_pil==out_torch))\n\n\n    def to_tensor_test(self):\n        inp_np_f = np.zeros((1,112,112,3)).astype(\'float\')+201\n        inp_np = np.zeros((1,112,112,3)).astype(\'uint8\')+1\n        inp_pil = [Image.fromarray(inp_np[0], mode=\'RGB\')]\n        out_np_f = self.preproc._to_tensor(inp_np_f)\n        out_np = self.preproc._to_tensor(inp_np)\n        out_pil = self.preproc._to_tensor(inp_pil)\n        assert (False not in np.array(out_np[0]==out_pil[0])) and isinstance(out_np_f[0], torch.DoubleTensor)\n\n\n    def to_pil_test(self):\n        inp_np = [np.zeros((112,112,3)).astype(\'int32\')]\n        inp_torch = [torch.zeros((3,112,112))]\n        out_np = self.preproc._to_pil(inp_np)\n        out_torch = self.preproc._to_pil(inp_torch)\n\n        assert (False not in np.array(np.array(out_np[0])==np.array(out_torch[0])))\n\n\n    def run_tests(self):\n        self.resize_test()\n        self.crop_test()\n        self.cent_crop_test()\n        self.rand_crop_test()\n        self.rand_flip_test()\n        self.rand_rot_test()\n        self.rand_trans_test()\n        self.rand_zoom_test()\n        self.applypil_test()\n        self.applytensor_test()\n        self.applycv_test()\n        self.to_tensor_test()\n        self.to_pil_test()\n        self.to_numpy_test()\n        print(""Tests passed"")\n\n        self.rand_flip_vis()\n        self.rand_rot_vis()\n        self.rand_zoom_vis()\n        \nif __name__==\'__main__\':\n    test = TestPreproc()\n    test.run_tests()\n\n\n'"
models/__init__.py,0,b'from .models_import import create_model_object\n'
models/models_import.py,0,"b'import importlib\nimport sys\nimport glob\n\ndef create_model_object(*args, **kwargs):\n    """"""\n    Use model_name to find a matching model class\n\n    Args:\n        kwargs: arguments specifying model and model parameters\n\n    Returns:\n        model: initialized model object \n    """"""\n    model_name = kwargs[\'model\']\n\n    model_files = glob.glob(\'models/*/*.py\')\n    ignore_files = [\'__init__.py\', \'models_import.py\']\n\n    for mf in model_files:\n        if mf in ignore_files:\n            continue\n\n        module_name = mf[:-3].replace(\'/\',\'.\')\n        module = importlib.import_module(module_name)\n        module_lower = list(map(lambda module_x: module_x.lower(), dir(module)))\n\n        if model_name.lower() in module_lower:\n            model_index = module_lower.index(model_name.lower())\n            model = getattr(module, dir(module)[model_index])(**kwargs)\n\n            return model\n\n    sys.exit(\'Model not found. Ensure model is in models/, with a matching class name\')\n'"
datasets/scripts/gen_json_DHF1K.py,0,"b""import os\nimport cv2\nimport json\n\n\ndef get_split(base_vid_path):\n    vids = os.listdir(base_vid_path)\n    vids = [int(vid) for vid in vids]\n    vids.sort()\n\n    # Out of the 1000 videos, the first 600 are annotated for training, 601-700 annotated for val, 701-1000 not annotated must be sent in to test\n    train_cutoff = 600\n    val_cutoff = 700\n    train_vids = vids[:vids.index(600)+1] \n    val_vids = vids[vids.index(600)+1:vids.index(700)+1] \n    test_vids = vids[vids.index(700)+1:]\n    \n    train_vids = [str(vid).zfill(3) for vid in train_vids]\n    test_vids  = [str(vid).zfill(3) for vid in test_vids]\n    val_vids   = [str(vid).zfill(3) for vid in val_vids]\n    annot_train_vids = [vid.zfill(4) for vid in train_vids]\n    annot_val_vids = [vid.zfill(4) for vid in val_vids]\n    return train_vids, test_vids, val_vids, annot_train_vids, annot_val_vids\n\n\ndef save_json(load_type):\n    base_vid_path = '/path/to/DHF1K/video_png'\n    base_annot_path = '/path/to/DHF1K/annotation'\n    output_path = '/any/path/'\n   \n    train_vids, test_vids, val_vids, annot_train, annot_val = get_split(base_vid_path)\n    \n    if load_type == 'train':\n        tv_vids = train_vids\n        tv_ann = annot_train\n    elif load_type == 'val':\n        tv_vids = val_vids\n        tv_ann = annot_val\n\n    else:\n        tv_vids = test_vids\n        tv_ann = []\n\n    json_dat = [] \n    for vid in sorted(tv_vids):\n        vid_dict = {}\n        frames = []\n        frame_size = []\n        for img in sorted(os.listdir(os.path.join(base_vid_path, vid))):\n            if frame_size == []:\n                frame_shape = cv2.imread(os.path.join(base_vid_path, vid, img)).shape\n                frame_size = [frame_shape[1], frame_shape[0]] # Width, Height\n            frame_dict = {}\n            frame_dict['img_path'] = img\n            if load_type != 'test':\n                frame_dict['map_path'] = os.path.join(base_annot_path, tv_ann[tv_vids.index(vid)], 'maps', img)\n                frame_dict['bin_path'] = os.path.join(base_annot_path, tv_ann[tv_vids.index(vid)], 'fixation', img)\n            else:\n                frame_dict['map_path'] = '' \n                frame_dict['bin_path'] = ''\n\n            frames.append(frame_dict)\n        vid_dict['base_path'] = os.path.join(base_vid_path, vid)\n        vid_dict['frames'] = frames\n        vid_dict['frame_size'] = frame_size\n        json_dat.append(vid_dict)\n\n    writef = open(os.path.join(output_path,load_type+'.json'), 'w')\n    json.dump(json_dat, writef)\n    writef.close()\n\nsave_json('train')\nsave_json('val')\nsave_json('test')\n"""
datasets/scripts/gen_json_HMDB51.py,0,"b""# Generating JSON file compatible with ViP for HMDB51 dataset\n\nimport os\nimport sys\nimport json\n\n\nsplits      = ['train', 'test']\nsource_root = '$DATA_DIRECTORY/HMDB51'\ntarget_root = '$JSON_DIRECTORY'\n\nlabel_dict = {'brush_hair': 0,      'cartwheel': 1,\n              'catch': 2,           'chew': 3,\n              'clap': 4,            'climb_stairs': 5,\n              'climb': 6,           'dive': 7,\n              'draw_sword': 8,      'dribble': 9,\n              'drink': 10,          'eat': 11,\n              'fall_floor': 12,     'fencing': 13,\n              'flic_flac': 14,      'golf': 15,\n              'handstand': 16,      'hit': 17,\n              'hug': 18,            'jump': 19,\n              'kick_ball': 20,      'kick': 21,\n              'kiss': 22,           'laugh': 23,\n              'pick': 24,           'pour': 25,\n              'pullup': 26,         'punch': 27,\n              'push': 28,           'pushup': 29,\n              'ride_bike': 30,      'ride_horse': 31,\n              'run': 32,            'shake_hands': 33,\n              'shoot_ball': 34,     'shoot_bow': 35,\n              'shoot_gun': 36,      'sit': 37, \n              'situp': 38,          'smile': 39, \n              'smoke': 40,          'somersault': 41, \n              'stand': 42,          'swing_baseball': 43, \n              'sword_exercise': 44, 'sword': 45, \n              'talk': 46,           'throw': 47, \n              'turn': 48,           'walk': 49, \n              'wave': 50} \n\nfor split in splits:\n    base_path = os.path.join(source_root, split+'images')\n    actions   = os.listdir(base_path) \n\n    dest_path = os.path.join(target_root, split+'.json')\n    dest_data = [] \n    samples   = []\n    count     = 0\n\n    for action in actions:\n        for video in os.listdir(os.path.join(base_path, action)):\n            if not '.DS' in video:\n                video_images = sorted(os.listdir(os.path.join(base_path, action, video.replace('.avi',''))))\n                samples      = [os.path.join(base_path, action, video.replace('.avi',''), video_image) for video_image in video_images]\n                    \n                dest_data_lvl1           = {}\n                dest_data_lvl1['frames'] = [] \n    \n                for frame in samples:\n                    dest_data_lvl1['frames'].append({'img_path': os.path.split(frame)[1],\n                                                     'actions':[{'action_class': label_dict[action]}] })\n    \n                # END FOR\n               \n                dest_data_lvl1['base_path'] = os.path.split(frame)[0]\n                dest_data.append(dest_data_lvl1)     \n    \n            # END IF\n    \n        # END FOR\n    \n    # END FOR\n    \n    with open(dest_path, 'w') as outfile:\n        json.dump(dest_data, outfile, indent=4)\n"""
datasets/scripts/gen_json_imagenetvid.py,0,"b'from os.path import join, isdir\nfrom os import listdir\nimport os\nimport cv2\nimport argparse\nimport json\nimport glob\nimport xml.etree.ElementTree as ET\n\n\n# Download the dataset from here: http://bvisionweb1.cs.unc.edu/ilsvrc2015/download-videos-3j16.php\n\nlabel_mappings = {""n02374451"": ""horse"", ""n02691156"": ""airplane"", ""n02062744"": ""whale"", ""n01503061"": ""bird"", ""n03790512"": ""motorcycle "", ""n02402425"": ""cattle"", ""n02342885"": ""hamster"", ""n04530566"": ""watercraft "", ""n02958343"": ""car"", ""n02510455"": ""giant panda"", ""n02129165"": ""lion"", ""n02503517"": ""elephant"", ""n02129604"": ""tiger"", ""n02419796"": ""antelope"", ""n02391049"": ""zebra"", ""n02131653"": ""bear "", ""n01674464"": ""lizard"", ""n04468005"": ""train"", ""n02509815"": ""red panda"", ""n02834778"": ""bicycle"", ""n02484322"": ""monkey"", ""n01726692"": ""snake"", ""n02084071"": ""dog"", ""n02324045"": ""rabbit"", ""n02924116"": ""bus"", ""n02118333"": ""fox"", ""n02355227"": ""squirrel"", ""n01662784"": ""turtle"", ""n02121808"": ""domestic cat"", ""n02411705"": ""sheep""}\n\ndef gen_label_keys():\n    json.dump(label_mappings, open(\'labels_number_keys.json\', \'w\'))\n\ndef gen_json(load_type):\n    VID_base_path = \'/path/to/datasets/ILSVRC2015/\' ###### REPLACE with the path to ImageNetVID\n    ann_base_path = join(VID_base_path, \'Annotations/VID\', load_type)\n    img_base_path = join(VID_base_path, \'Data/VID\', load_type)\n    \n    vid = []\n    count = 0\n    if load_type == \'train\':\n        vid_list = listdir(ann_base_path)\n    else:\n        vid_list = [\'\'] \n    \n    for curr_path in vid_list:\n        if curr_path != \'\':\n            curr_ann_path = join(ann_base_path, curr_path)\n            curr_img_path = join(img_base_path, curr_path)\n    \n        else:\n            curr_ann_path = ann_base_path\n            curr_img_path = img_base_path\n    \n        videos = sorted(listdir(curr_img_path))\n        for vi, video in enumerate(videos):\n            v = dict()\n            v[\'base_path\'] = join(curr_img_path, video)\n            v[\'frames\'] = []\n            video_base_path = join(curr_ann_path, video)\n            if os.path.exists(video_base_path):\n                xmls = sorted(glob.glob(join(video_base_path, \'*.xml\')))\n                for xml in xmls:\n                    f = dict()\n                    xmltree = ET.parse(xml)\n                    size = xmltree.findall(\'size\')[0]\n                    frame_sz = [int(it.text) for it in size]\n                    objects = xmltree.findall(\'object\')\n                    objs = []\n                    for object_iter in objects:\n                        trackid = int(object_iter.find(\'trackid\').text)\n                        name = (object_iter.find(\'name\')).text\n                        bndbox = object_iter.find(\'bndbox\')\n                        occluded = int(object_iter.find(\'occluded\').text)\n                        o = dict()\n                        o[\'c\'] = name\n                        o[\'bbox\'] = [int(bndbox.find(\'xmin\').text), int(bndbox.find(\'ymin\').text),\n                                     int(bndbox.find(\'xmax\').text), int(bndbox.find(\'ymax\').text)]\n                        o[\'trackid\'] = trackid\n                        o[\'occ\'] = occluded\n                        objs.append(o)\n                    f[\'img_path\'] = xml.split(\'/\')[-1].replace(\'xml\', \'JPEG\')\n                    f[\'objs\'] = objs\n                    v[\'frames\'].append(f)\n                    v[\'frame_size\'] = frame_sz\n            else:\n                for img in sorted(listdir(join(curr_img_path, video))):\n                    f = dict()\n                    f[\'img_path\'] = img \n                    f[\'objs\'] = []\n                    v[\'frames\'].append(f)\n                size = cv2.imread(join(curr_img_path, video, img)).shape\n                v[\'frame_size\'] = size[:2][::-1] \n\n            vid.append(v)\n            count += 1\n    \n    print(\'save json (raw vid info), please wait 1 min~\')\n    json.dump(vid, open(\'ilsvrc_\'+load_type+\'.json\', \'w\'), indent=2)\n    print(\'done!\')\n\ngen_json(\'test\')\ngen_json(\'val\')\ngen_json(\'train\')\ngen_label_keys()\n\n'"
datasets/scripts/gen_json_mscoco.py,0,"b""import json\nimport os\n\n\nyear = '2014'\n\ndef save_json(load_type):\n    \n    # Define path to mscoco images data\n    base_img_path = '/path/to/mscoco/images/'       ###### REPLACE with path to dataset\n    base_annot_path = '/path/to/mscoco/annotations/'###### REPLACE with path to dataset\n\n    save_location = '/path/to/save/location' ######### REPLACE with save path\n\n    with open(os.path.join(base_annot_path,'instances_'+load_type+year+'.json'),'r') as f:\n        x = json.load(f)\n    \n    imgids = [[idx['id'], idx['file_name'], idx['width'], idx['height']] for idx in x['images']]\n    \n    dd = {}\n    for idx in imgids:\n        frame_dict = dict(objs=[], img_path=idx[1]) \n        dd[idx[0]] = dict(frames=[frame_dict], base_path=os.path.join(base_img_path,load_type+year), frame_size=[idx[2],idx[3]])\n    \n    \n    print('finished imgids')\n    \n    count = 0\n    for annot in x['annotations']:\n        image_id = annot['image_id']\n        trackid = len(dd[image_id]['frames'][0]['objs'])  \n        cat = annot['category_id']\n        bbox = annot['bbox'] # [x,y,width,height]\n        bbox = [bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]] # [xmin, ymin, xmax, ymax]\n        iscrowd=annot['iscrowd']\n        obj_info = dict(trackid=trackid, c=cat, iscrowd=iscrowd, bbox=bbox)\n        dd[image_id]['frames'][0]['objs'].append(obj_info)\n        count+=1\n        if count%1000==0:\n            print(count)\n    \n    with open(os.path.join(save_location,load_type+'.json'), 'w') as f:\n        json.dump(list(dd.values()), f)\n                \n \nsave_json('train')\nsave_json('val')\n"""
datasets/scripts/gen_json_voc2007.py,0,"b""#Compile all VOC XML annotation files into a single JSON file (one for each data split)\n#Existing folder structure will remain the same\n#Coordinates will be zero-indexed. Originally top-left pixel is (1,1)\n\nimport os\nfrom glob import glob \nimport xml.etree.ElementTree as ET\nimport json\n\nsource_root = '$DATA_DIRECTORY/VOC2007' #replace this value\ntarget_root = '$JSON_DIRECTORY'         #replace this value\n\n\nsplits = ['train', 'val', 'test']\n\nfor split in splits:\n    split_file = os.path.join(source_root,'ImageSets/Main/'+split+'.txt')\n    target_file = os.path.join(target_root,split+'.json')\n\n    print('Compiling annotations from: {}'.format(split_file))\n\n    #Get all image names from split\n    with open(split_file, 'r') as f:\n        image_names = f.read().splitlines()\n\n    ann_files = [] #xml annotation files\n    for img in image_names:\n        ann_files.append(os.path.join(source_root,'Annotations',img+'.xml'))\n\n    json_ann = []\n\n    #Parse through XML files and add to JSON dictionary\n    base_path = os.path.join(source_root, 'JPEGImages')\n    for f in ann_files:\n        frames = []\n\n        #An extra loop would be here if this was a video dataset\n        frame = {}\n        root = ET.parse(f).getroot()\n\n        width = int(root.find('size/width').text)\n        height = int(root.find('size/height').text)\n        img_path = root.find('filename').text\n\n        objects = root.findall('object')\n        occluded = 0 #No occluded annotations on VOC_2007\n\n        track_id = 0\n\n        objs = []\n        for obj in objects:\n            class_name = obj.find('name').text  \n\n            #zero-index coordinates\n            xmin = int(obj.find('bndbox/xmin').text)-1\n            ymin = int(obj.find('bndbox/ymin').text)-1\n            xmax = int(obj.find('bndbox/xmax').text)-1\n            ymax = int(obj.find('bndbox/ymax').text)-1\n\n            truncated = int(obj.find('truncated').text)\n            difficult = int(obj.find('difficult').text)\n\n            objs.append({'trackid':track_id, 'c':class_name, 'occ':occluded, 'truncated':truncated, 'difficult':difficult, 'bbox':[xmin, ymin, xmax, ymax]})\n            track_id += 1\n\n        frame['objs'] = objs \n        frame['img_path'] =  img_path\n\n        frames.append(frame)\n        json_ann.append({'frames':frames, 'base_path':base_path, 'frame_size':[width, height]})\n\n    #Write out to JSON file\n    with open(target_file, 'w') as f:\n        json.dump(json_ann, f)\n\n"""
datasets/scripts/gen_json_yc2bb.py,0,"b""#Convert YC2-BB JSON annotation files to ViP JSON format\n\nimport os \nimport json\n\nsource_root = '$ANNOTATIONS_ROOT/annotations' #replace this value\ntarget_root = '$JSON_TARGET_ROOT' #replace this value\n#Link to videos sampled at 1 fps\nframe_root  = '$SAMPLED_FRAMES_ROOT' #replace this value\nfiles = ['yc2_training_vid.json', 'yc2_bb_val_annotations.json', 'yc2_bb_public_test_annotations.json']\n\nsplits    = ['train', 'val', 'test']\nann_files = [os.path.join(source_root, f) for f in files]\n\nfor split, ann_file in zip(splits, ann_files):\n\n\n    #YC2 split names, slightly different\n    split_to_split = {'train':'training','val':'validation','test':'testing'}\n    split_name = split_to_split[split]\n    \n    with open(ann_file) as f:\n        ann_json_data = json.load(f)\n\n    yc2_json_data = ann_json_data['database']\n    json_data = []\n\n    for vid_name in yc2_json_data.keys():\n        frm_height  = yc2_json_data[vid_name]['rheight']\n        frm_width   = yc2_json_data[vid_name]['rwidth']\n        recipe_type = yc2_json_data[vid_name]['recipe_type'] \n        yc2_segments = yc2_json_data[vid_name]['segments']\n        \n        #Loop through segments, YC2 breaks down all each video into segment clips\n        for seg,item in sorted(yc2_segments.items()):\n            base_path   = os.path.join(frame_root, split_name, recipe_type, vid_name, str(seg).zfill(2))\n            frames = []\n            if 'objects' in item: #validation or testing file\n                num_objs   = len(item['objects'])\n                num_frames = len(item['objects'][0]['boxes'])\n\n                #Loop through frames\n                for f in range(num_frames):\n                    frame = {}\n                    objs = []\n\n                    #Loop through objects\n                    for track_id in range(num_objs):\n                        obj = item['objects'][track_id]\n\n                        cls_name = obj['label']\n                        box    = obj['boxes'][f]\n                        \n                        if len(box) == 0: #No annotations\n                            objs.append({'trackid':track_id, 'c':cls_name})\n                            continue \n\n                        xmin = box['xtl']\n                        ymin = box['ytl']\n                        xmax = box['xbr']\n                        ymax = box['ybr']\n\n                        outside  = box['outside'] #outside or inside of frame\n                        occluded = box['occluded'] \n\n                        objs.append({'trackid':track_id, 'c':cls_name, 'occ':occluded, 'outside':outside, 'bbox':[xmin, ymin, xmax, ymax]})\n\n                    frame['img_path'] = os.path.join(base_path, str(seg).zfill(2), str(f).zfill(2)+'.jpg') \n                    frame['objs']     = objs \n                    frame['seg']      = seg\n                    frames.append(frame) \n            else: #training annotation file\n                frame = {}\n                objs = []\n\n                frame['sentence'] = yc2_segments[seg]['sentence'] \n                frame['objs']     = objs \n                frame['seg']      = seg\n                frames.append(frame) \n\n            json_data.append({'frames':frames, 'base_path':base_path, 'frame_size':[frm_width, frm_height], 'recipe_type':recipe_type})\n\n    target_file = os.path.join(target_root, split+'.json')\n    print('Writing out to: {}'.format(target_file))\n    with open(target_file, 'w') as f:\n        json.dump(json_data, f)\n        \n"""
datasets/templates/dataset_template.py,2,"b""import torch\nimport torchvision\n\nfrom .abstract_datasets import DetectionDataset \n#OR\nfrom .abstract_datasets import RecognitionDataset \n\nimport cv2\nimport os\nimport numpy as np\nimport datasets.preprocessing_transforms as pt\n\n\nclass CustomDataset(DetectionDataset):\n    # OR\nclass CustomDataset(RecognitionDataset):\n    def __init__(self, *args, **kwargs):\n        super(CustomDataset, self).__init__(*args, **kwargs)\n\n        # Get model object in case preprocessing other than default is used\n        self.model_object   = kwargs['model_object']\n        self.load_type = kwargs['load_type']\n\n        if self.load_type=='train':\n            self.transforms = kwargs['model_obj'].train_transforms\n\n        else:\n            self.transforms = kwargs['model_obj'].test_transforms\n\n    def __getitem__(self, idx):\n        vid_info = self.samples[idx]\n        \n        base_path = vid_info['base_path']\n        vid_size  = vid_info['frame_size']\n\n        input_data = []\n        vid_data   = np.zeros((self.clip_length, self.final_shape[0], self.final_shape[1], 3))-1\n        labels     = np.zeros((self.clip_length, self.max_objects))-1\n\n        # For Detection Datasets\n        bbox_data  = np.zeros((self.clip_length, self.max_objects, 4))-1\n\n\n\n\n        for frame_ind in range(len(vid_info['frames'])):\n            frame      = vid_info['frames'][frame_ind]\n            frame_path = frame['img_path']\n            \n            # Extract bbox and label data from video info\n            for obj in frame['objs']:\n                trackid   = obj['trackid']\n                label     = obj['label']\n                obj_bbox  = obj['bbox'] # [xmin, ymin, xmax, ymax]\n\n                bbox_data[frame_ind, trackid, :] = obj_bbox\n                labels[frame_ind, trackid]       = label \n            \n            # Read framewise image data from storage\n            input_data.append(cv2.imread(os.path.join(base_path, frame_path))[...,::-1]/255.)\n\n        # Apply preprocessing transformations\n        vid_data, bbox_data = self.transforms(input_data, bbox_data)\n\n        bbox_data = bbox_data.type(torch.LongTensor)\n        xmin_data  = bbox_data[:,:,0]\n        ymin_data  = bbox_data[:,:,1]\n        xmax_data  = bbox_data[:,:,2]\n        ymax_data  = bbox_data[:,:,3]\n        labels     = torch.from_numpy(labels)\n\n        # Permute the PIL dimensions (Frame, Height, Width, Chan) to pytorch (Chan, frame, height, width) \n        vid_data = vid_data.permute(3, 0, 1, 2)\n\n        ret_dict = dict() \n        ret_dict['data']       = vid_data \n        annot_dict = dict()\n        annot_dict['xmin']        = xmin_data\n        annot_dict['ymin']        = ymin_data\n        annot_dict['xmax']        = xmax_data\n        annot_dict['ymax']        = ymax_data\n        annot_dict['bbox_data']   = bbox_data\n        annot_dict['labels']      = labels\n        annot_dict['input_shape'] = vid_data.size() \n        ret_dict['annots']     = annot_dict\n\n        return ret_dict\n"""
models/c3d/c3d.py,5,"b'import torch\nimport numpy                             as np\nimport torch.nn                          as nn\nimport torch.nn.functional               as F\nimport datasets.preprocessing_transforms as pt\n\nclass C3D(nn.Module):\n    """"""\n    The C3D network.\n    """"""\n\n    def __init__(self, **kwargs):\n        """"""\n        Initialize C3D model  \n        Args:\n            labels     (Int):    Total number of classes in the dataset\n            pretrained (Int/String): Initialize with random (0) or pretrained (1) weights \n\n        Return:\n            None\n        """"""\n        super(C3D, self).__init__()\n\n        self.train_transforms = PreprocessTrainC3D(**kwargs)\n        self.test_transforms  = PreprocessEvalC3D(**kwargs)\n\n        self.conv1 = nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n\n        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n\n        self.conv3a = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.conv3b = nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n\n        self.conv4a = nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.conv4b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.pool4 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n\n        self.conv5a = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.conv5b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.pool5 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1))\n\n        self.fc6 = nn.Linear(8192, 4096)\n        self.fc7 = nn.Linear(4096, 4096)\n        self.fc8 = nn.Linear(4096, kwargs[\'labels\'])\n\n        self.dropout = nn.Dropout(p=0.5)\n\n        self.relu = nn.ReLU()\n\n        self.__init_weight()\n\n        if isinstance(kwargs[\'pretrained\'], int) and kwargs[\'pretrained\']:\n            self.__load_pretrained_weights()\n\n    def forward(self, x, labels=False):\n        x = self.relu(self.conv1(x))\n        x = self.pool1(x)\n\n        x = self.relu(self.conv2(x))\n        x = self.pool2(x)\n\n        x = self.relu(self.conv3a(x))\n        x = self.relu(self.conv3b(x))\n        x = self.pool3(x)\n\n        x = self.relu(self.conv4a(x))\n        x = self.relu(self.conv4b(x))\n        x = self.pool4(x)\n\n        x = self.relu(self.conv5a(x))\n        x = self.relu(self.conv5b(x))\n        x = self.pool5(x)\n\n        x = x.view(-1, 8192)\n\n        x = self.relu(self.fc6(x))\n\n        x = self.dropout(x)\n        x = self.relu(self.fc7(x))\n        x = self.dropout(x)\n\n\n        logits = self.fc8(x)\n\n        if labels:\n            logits = F.softmax(logits, dim=1)\n\n        return logits\n\n    def __load_pretrained_weights(self):\n        """"""Initialiaze network.""""""\n        corresp_name = {\n                        # Conv1\n                        ""features.0.weight"": ""conv1.weight"",\n                        ""features.0.bias"": ""conv1.bias"",\n                        # Conv2\n                        ""features.3.weight"": ""conv2.weight"",\n                        ""features.3.bias"": ""conv2.bias"",\n                        # Conv3a\n                        ""features.6.weight"": ""conv3a.weight"",\n                        ""features.6.bias"": ""conv3a.bias"",\n                        # Conv3b\n                        ""features.8.weight"": ""conv3b.weight"",\n                        ""features.8.bias"": ""conv3b.bias"",\n                        # Conv4a\n                        ""features.11.weight"": ""conv4a.weight"",\n                        ""features.11.bias"": ""conv4a.bias"",\n                        # Conv4b\n                        ""features.13.weight"": ""conv4b.weight"",\n                        ""features.13.bias"": ""conv4b.bias"",\n                        # Conv5a\n                        ""features.16.weight"": ""conv5a.weight"",\n                        ""features.16.bias"": ""conv5a.bias"",\n                         # Conv5b\n                        ""features.18.weight"": ""conv5b.weight"",\n                        ""features.18.bias"": ""conv5b.bias"",\n                        # fc6\n                        ""classifier.0.weight"": ""fc6.weight"",\n                        ""classifier.0.bias"": ""fc6.bias"",\n                        # fc7\n                        ""classifier.3.weight"": ""fc7.weight"",\n                        ""classifier.3.bias"": ""fc7.bias"",\n                        }\n\n        p_dict = torch.load(\'weights/c3d-pretrained.pth\')\n        s_dict = self.state_dict()\n        for name in p_dict:\n            if name not in corresp_name:\n                continue\n            s_dict[corresp_name[name]] = p_dict[name]\n        self.load_state_dict(s_dict)\n\n    def __init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm3d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\nclass PreprocessTrainC3D(object):\n    """"""\n    Container for all transforms used to preprocess clips for training in this dataset.\n    """"""\n    def __init__(self, **kwargs):\n        """"""\n        Initialize preprocessing class for training set\n        Args:\n            preprocess (String): Keyword to select different preprocessing types            \n            crop_type  (String): Select random or central crop \n\n        Return:\n            None\n        """"""\n\n        self.transforms  = []\n        self.transforms1 = []\n        self.preprocess  = kwargs[\'preprocess\']\n        crop_type        = kwargs[\'crop_type\']\n\n        self.clip_mean  = np.load(\'weights/sport1m_train16_128_mean.npy\')[0]\n        self.clip_mean  = np.transpose(self.clip_mean, (1,2,3,0))\n\n        self.transforms.append(pt.ResizeClip(**kwargs))\n        self.transforms.append(pt.SubtractMeanClip(clip_mean=self.clip_mean, **kwargs))\n        \n        if crop_type == \'Random\':\n            self.transforms.append(pt.RandomCropClip(**kwargs))\n\n        else:\n            self.transforms.append(pt.CenterCropClip(**kwargs))\n\n        self.transforms.append(pt.RandomFlipClip(direction=\'h\', p=0.5, **kwargs))\n        self.transforms.append(pt.ToTensorClip(**kwargs))\n\n    def __call__(self, input_data):\n        for transform in self.transforms:\n            input_data = transform(input_data)\n\n        return input_data\n\n\nclass PreprocessEvalC3D(object):\n    """"""\n    Container for all transforms used to preprocess clips for training in this dataset.\n    """"""\n    def __init__(self, **kwargs):\n        """"""\n        Initialize preprocessing class for training set\n        Args:\n            preprocess (String): Keyword to select different preprocessing types            \n            crop_type  (String): Select random or central crop \n\n        Return:\n            None\n        """"""\n\n        self.transforms = []\n        self.clip_mean  = np.load(\'weights/sport1m_train16_128_mean.npy\')[0]\n        self.clip_mean  = np.transpose(self.clip_mean, (1,2,3,0))\n\n        self.transforms.append(pt.ResizeClip(**kwargs))\n        self.transforms.append(pt.SubtractMeanClip(clip_mean=self.clip_mean, **kwargs))\n        self.transforms.append(pt.CenterCropClip(**kwargs))\n        self.transforms.append(pt.ToTensorClip(**kwargs))\n\n\n    def __call__(self, input_data):\n        for transform in self.transforms:\n            input_data = transform(input_data)\n\n        return input_data\n\ndef get_1x_lr_params(model):\n    """"""\n    This generator returns all the parameters for conv and two fc layers of the net.\n    """"""\n    b = [model.conv1, model.conv2, model.conv3a, model.conv3b, model.conv4a, model.conv4b,\n         model.conv5a, model.conv5b, model.fc6, model.fc7]\n    for i in range(len(b)):\n        for k in b[i].parameters():\n            if k.requires_grad:\n                yield k\n\ndef get_10x_lr_params(model):\n    """"""\n    This generator returns all the parameters for the last fc layer of the net.\n    """"""\n    b = [model.fc8]\n    for j in range(len(b)):\n        for k in b[j].parameters():\n            if k.requires_grad:\n                yield k\n\nif __name__ == ""__main__"":\n    inputs = torch.rand(1, 3, 16, 112, 112)\n    net = C3D(labels=101, pretrained=True)\n\n    outputs = net.forward(inputs)\n    print(outputs.size())\n'"
models/dvsa/dvsa.py,16,"b'#Code heavily adapted from: https://github.com/MichiganCOG/Video-Grounding-from-Text/blob/master/model/dvsa.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport numpy as np\nfrom functools import partial\nimport os\n\nfrom models.dvsa.dvsa_utils.transformer import Transformer\n\nclass DVSA(nn.Module):\n    """"""\n    Deep Visual-Semantic Alignments (DVSA). \n    Implementation used as baseline in Weakly-Supervised Video Object Grounding...\n    Source: https://arxiv.org/pdf/1805.02834.pdf\n    \n    Original paper: Deep visual-semantic alignments for generating image descriptions\n    https://cs.stanford.edu/people/karpathy/cvpr2015.pdf\n    """"""\n    def __init__(self, **kwargs):\n        super().__init__()\n        num_class          = kwargs[\'labels\']\n        input_size         = kwargs[\'input_size\']\n        enc_size           = kwargs[\'enc_size\']\n        dropout            = kwargs[\'dropout\']\n        hidden_size        = kwargs[\'hidden_size\']\n        n_layers           = kwargs[\'n_layers\']\n        n_heads            = kwargs[\'n_heads\']\n        attn_drop          = kwargs[\'attn_drop\']\n        num_frm            = kwargs[\'yc2bb_num_frm\']\n        has_loss_weighting = kwargs[\'has_loss_weighting\']\n        \n        # encode the region feature\n        self.feat_enc = nn.Sequential(\n            nn.Linear(input_size, enc_size),\n            nn.Dropout(p=dropout),\n            nn.ReLU()\n        )\n\n        self.sigmoid = nn.Sigmoid()\n\n        # lookup table for object label embedding\n        self.obj_emb = nn.Embedding(num_class+1, enc_size) # +1 for the dummy paddings\n        self.num_class = num_class\n\n        self.obj_interact = Transformer(enc_size, 0, 0,\n            d_hidden=hidden_size,\n            n_layers=n_layers,\n            n_heads=n_heads,\n            drop_ratio=attn_drop)\n\n        self.obj_interact_fc = nn.Sequential(\n            nn.Linear(enc_size*2, int(enc_size/2)),\n            nn.ReLU(),\n            nn.Linear(int(enc_size/2), 5), # object interaction guidance (always 5 snippets)\n            nn.Sigmoid()\n        )\n\n        self.num_frm = num_frm \n        self.has_loss_weighting = has_loss_weighting\n\n        if isinstance(kwargs[\'pretrained\'], int) and kwargs[\'pretrained\']:\n            self._load_pretrained_weights()\n\n    def forward(self, x_o, obj, load_type):\n        is_evaluate = 1 if load_type[0] == \'test\' or load_type[0] == \'val\' else 0\n        if is_evaluate:\n            return self.output_attn(x_o, obj)\n\n        #only a single batch expected\n        x_o = x_o[0]  \n        obj = obj[0]\n\n        x_o = self.feat_enc(x_o.permute(0,2,3,1).contiguous()).permute(0,3,1,2).contiguous()\n\n        x_o = torch.stack([x_o[0], x_o[1], x_o[0]])\n        obj = torch.stack([obj[0], obj[0], obj[1]])\n\n        N, C_out, T, num_proposals = x_o.size()\n        assert(N == 3) # two pos samples and one neg sample\n\n        # attention\n        O = obj.size(1)\n        attn_key = self.obj_emb(obj)\n\n        num_pos_obj = torch.sum(obj[0]<self.num_class).long().item()\n        num_neg_obj = torch.sum(obj[2]<self.num_class).long().item()\n        # object interaction guidance\n        attn_key_frm_feat = attn_key[0:1, :num_pos_obj] # cat visual feature\n        obj_attn_emb,_ = self.obj_interact(attn_key_frm_feat)\n        obj_attn_emb = obj_attn_emb[:, :num_pos_obj, :]\n        obj_attn_emb = torch.cat((obj_attn_emb, attn_key[0:1, :num_pos_obj], ), dim=2)\n        obj_attn_emb = self.obj_interact_fc(obj_attn_emb) # N, O, 5\n        \n        itv = math.ceil(T/5)\n        tmp = [] # expand obj_attn_emb to N, O, T\n        for i in range(5):\n            l = min(itv*(i+1), T)-itv*i\n            if l>0:\n                tmp.append(obj_attn_emb[:, :, i:(i+1)].expand(1, num_pos_obj, l))\n        obj_attn_emb = torch.cat(tmp, 2).squeeze(0)\n        assert(obj_attn_emb.size(1) == self.num_frm)\n\n        loss_weigh = torch.mean(obj_attn_emb, dim=0)\n        loss_weigh = torch.cat((loss_weigh, loss_weigh)).unsqueeze(1)\n\n        if self.has_loss_weighting:\n            # dot-product attention\n            x_o = x_o.view(N, 1, C_out, T, num_proposals)\n            attn_weights = self.sigmoid((x_o*attn_key.view(N, O, C_out, 1, 1)).sum(2)/math.sqrt(C_out))\n\n            pos_weights = attn_weights[0, :num_pos_obj, :, :]\n            neg1_weights = attn_weights[1, :num_pos_obj, :, :]\n            neg2_weights = attn_weights[2, :num_neg_obj, :, :]\n\n            return torch.cat((torch.stack((torch.mean(torch.max(pos_weights, dim=2)[0], dim=0), torch.mean(torch.max(neg1_weights, dim=2)[0], dim=0)), dim=1),\n                torch.stack((torch.mean(torch.max(pos_weights, dim=2)[0], dim=0), torch.mean(torch.max(neg2_weights, dim=2)[0], dim=0)), dim=1))), loss_weigh\n        else:\n            # dot-product attention\n            x_o = x_o.view(N, 1, C_out, T*num_proposals)\n            attn_weights = self.sigmoid((x_o*attn_key.view(N, O, C_out, 1)).sum(2)/math.sqrt(C_out))\n\n            pos_weights = attn_weights[0, :num_pos_obj, :]\n            neg1_weights = attn_weights[1, :num_pos_obj, :]\n            neg2_weights = attn_weights[2, :num_neg_obj, :]\n\n            return torch.stack((torch.stack((torch.mean(torch.max(pos_weights, dim=1)[0]), torch.mean(torch.max(neg1_weights, dim=1)[0]))),\n                torch.stack((torch.mean(torch.max(pos_weights, dim=1)[0]), torch.mean(torch.max(neg2_weights, dim=1)[0]))))), loss_weigh\n\n    def output_attn(self, x_o, obj):\n        x_o = self.feat_enc(x_o.permute(0,2,3,1).contiguous()).permute(0,3,1,2).contiguous()\n\n        N, C_out, T, num_proposals = x_o.size()\n        assert(N == 1)\n\n        # attention\n        O = obj.size(1)\n        attn_key = self.obj_emb(obj)\n\n        # dot-product attention\n        x_o = x_o.view(N, 1, C_out, T*num_proposals)\n        attn_weights = self.sigmoid((x_o*attn_key.view(N, O, C_out, 1)).sum(2)/math.sqrt(C_out))\n        # attn_weights = self.sigmoid((x_e*attn_key.view(N, O, C_out, 1).expand(N, O, C_out, T*num_proposals)).sum(2)) # N, O, T, H*W\n\n        # additive attention\n        # x_e = x_o.view(N, 1, C_out, T, H*W).contiguous().expand(N, O, C_out, T, H*W)\n        # attn_e = attn_key.view(N, O, C_out, 1, 1).expand(N, O, C_out, T, H*W)\n        # attn_weights = self.attn_mlp(torch.cat((x_e, attn_e), dim=2).permute(0,1,3,4,2).contiguous()).squeeze(4) # N, O, T, H*W\n\n        return attn_weights.view(N, O, T, num_proposals)\n\n    def _load_pretrained_weights(self):\n        state_dict = torch.load(\'weights/yc2bb_full-model.pth\', map_location=lambda storage, location: storage)\n\n        self.load_state_dict(state_dict)\n'"
models/i3d/i3d.py,7,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport datasets.preprocessing_transforms as pt \n\nimport numpy as np\n\nimport os\nimport sys\nfrom collections import OrderedDict\n\n\n""""""\nCode from the implementation of i3d by AJ Piergiovanni: https://github.com/piergiaj/pytorch-i3d\n""""""\n\nclass MaxPool3dSamePadding(nn.MaxPool3d):\n    \n    def compute_pad(self, dim, s):\n        if s % self.stride[dim] == 0:\n            return max(self.kernel_size[dim] - self.stride[dim], 0)\n        else:\n            return max(self.kernel_size[dim] - (s % self.stride[dim]), 0)\n\n    def forward(self, x):\n        # compute \'same\' padding\n        (batch, channel, t, h, w) = x.size()\n        #print t,h,w\n        out_t = np.ceil(float(t) / float(self.stride[0]))\n        out_h = np.ceil(float(h) / float(self.stride[1]))\n        out_w = np.ceil(float(w) / float(self.stride[2]))\n        #print out_t, out_h, out_w\n        pad_t = self.compute_pad(0, t)\n        pad_h = self.compute_pad(1, h)\n        pad_w = self.compute_pad(2, w)\n        #print pad_t, pad_h, pad_w\n\n        pad_t_f = pad_t // 2\n        pad_t_b = pad_t - pad_t_f\n        pad_h_f = pad_h // 2\n        pad_h_b = pad_h - pad_h_f\n        pad_w_f = pad_w // 2\n        pad_w_b = pad_w - pad_w_f\n\n        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n        x = F.pad(x, pad)\n        return super(MaxPool3dSamePadding, self).forward(x)\n    \n\nclass Unit3D(nn.Module):\n\n    def __init__(self, in_channels,\n                 output_channels,\n                 kernel_shape=(1, 1, 1),\n                 stride=(1, 1, 1),\n                 padding=0,\n                 activation_fn=F.relu,\n                 use_batch_norm=True,\n                 use_bias=False,\n                 name=\'unit_3d\',\n                 dilation=1):\n        \n        """"""Initializes Unit3D module.""""""\n        super(Unit3D, self).__init__()\n        \n        self._output_channels = output_channels\n        self._kernel_shape = kernel_shape\n        self._stride = stride\n        self._use_batch_norm = use_batch_norm\n        self._activation_fn = activation_fn\n        self._use_bias = use_bias\n        self.name = name\n        self.padding = padding\n        \n        self.conv3d = nn.Conv3d(in_channels=in_channels,\n                                out_channels=self._output_channels,\n                                kernel_size=self._kernel_shape,\n                                stride=self._stride,\n                                padding=0, # we always want padding to be 0 here. We will dynamically pad based on input size in forward function\n                                bias=self._use_bias,\n                                dilation=dilation)\n        \n        if self._use_batch_norm:\n            self.bn = nn.BatchNorm3d(self._output_channels, eps=0.001, momentum=0.01)\n\n    def compute_pad(self, dim, s):\n        if s % self._stride[dim] == 0:\n            return max(self._kernel_shape[dim] - self._stride[dim], 0)\n        else:\n            return max(self._kernel_shape[dim] - (s % self._stride[dim]), 0)\n\n            \n    def forward(self, x):\n        # compute \'same\' padding\n        (batch, channel, t, h, w) = x.size()\n        #print t,h,w\n        out_t = np.ceil(float(t) / float(self._stride[0]))\n        out_h = np.ceil(float(h) / float(self._stride[1]))\n        out_w = np.ceil(float(w) / float(self._stride[2]))\n        #print out_t, out_h, out_w\n        pad_t = self.compute_pad(0, t)\n        pad_h = self.compute_pad(1, h)\n        pad_w = self.compute_pad(2, w)\n        #print pad_t, pad_h, pad_w\n\n        pad_t_f = pad_t // 2\n        pad_t_b = pad_t - pad_t_f\n        pad_h_f = pad_h // 2\n        pad_h_b = pad_h - pad_h_f\n        pad_w_f = pad_w // 2\n        pad_w_b = pad_w - pad_w_f\n\n        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n        x = F.pad(x, pad)\n\n        x = self.conv3d(x)\n        if self._use_batch_norm:\n            x = self.bn(x)\n        if self._activation_fn is not None:\n            x = self._activation_fn(x)\n        return x\n\n\n\nclass InceptionModule(nn.Module):\n    def __init__(self, in_channels, out_channels, name):\n        super(InceptionModule, self).__init__()\n\n        self.b0 = Unit3D(in_channels=in_channels, output_channels=out_channels[0], kernel_shape=[1, 1, 1], padding=0,\n                         name=name+\'/Branch_0/Conv3d_0a_1x1\')\n        self.b1a = Unit3D(in_channels=in_channels, output_channels=out_channels[1], kernel_shape=[1, 1, 1], padding=0,\n                          name=name+\'/Branch_1/Conv3d_0a_1x1\')\n        self.b1b = Unit3D(in_channels=out_channels[1], output_channels=out_channels[2], kernel_shape=[3, 3, 3],\n                          name=name+\'/Branch_1/Conv3d_0b_3x3\')\n        self.b2a = Unit3D(in_channels=in_channels, output_channels=out_channels[3], kernel_shape=[1, 1, 1], padding=0,\n                          name=name+\'/Branch_2/Conv3d_0a_1x1\')\n        self.b2b = Unit3D(in_channels=out_channels[3], output_channels=out_channels[4], kernel_shape=[3, 3, 3],\n                          name=name+\'/Branch_2/Conv3d_0b_3x3\')\n        self.b3a = MaxPool3dSamePadding(kernel_size=[3, 3, 3],\n                                stride=(1, 1, 1), padding=0)\n        self.b3b = Unit3D(in_channels=in_channels, output_channels=out_channels[5], kernel_shape=[1, 1, 1], padding=0,\n                          name=name+\'/Branch_3/Conv3d_0b_1x1\')\n        self.name = name\n\n    def forward(self, x):    \n        b0 = self.b0(x)\n        b1 = self.b1b(self.b1a(x))\n        b2 = self.b2b(self.b2a(x))\n        b3 = self.b3b(self.b3a(x))\n        return torch.cat([b0,b1,b2,b3], dim=1)\n\n\nclass I3D(nn.Module):\n    """"""Inception-v1 I3D architecture.\n    The model is introduced in:\n        Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\n        Joao Carreira, Andrew Zisserman\n        https://arxiv.org/pdf/1705.07750v1.pdf.\n    See also the Inception architecture, introduced in:\n        Going deeper with convolutions\n        Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n        Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n        http://arxiv.org/pdf/1409.4842v1.pdf.\n    """"""\n\n    # Endpoints of the model in order. During construction, all the endpoints up\n    # to a designated `final_endpoint` are returned in a dictionary as the\n    # second return value.\n    VALID_ENDPOINTS = (\n        \'Conv3d_1a_7x7\',\n        \'MaxPool3d_2a_3x3\',\n        \'Conv3d_2b_1x1\',\n        \'Conv3d_2c_3x3\',\n        \'MaxPool3d_3a_3x3\',\n        \'Mixed_3b\',\n        \'Mixed_3c\',\n        \'MaxPool3d_4a_3x3\',\n        \'Mixed_4b\',\n        \'Mixed_4c\',\n        \'Mixed_4d\',\n        \'Mixed_4e\',\n        \'Mixed_4f\',\n        \'MaxPool3d_5a_2x2\',\n        \'Mixed_5b\',\n        \'Mixed_5c\',\n        \'Logits\',\n        \'Predictions\',\n    )\n\n    def __init__(self, spatial_squeeze=True,\n                 final_endpoint=\'Logits\', name=\'inception_i3d\', in_channels=3, dropout_keep_prob=0.5, **kwargs):\n        """"""Initializes I3D model instance.\n        Args:\n          num_classes: The number of outputs in the logit layer (default 400, which\n              matches the Kinetics dataset).\n          spatial_squeeze: Whether to squeeze the spatial dimensions for the logits\n              before returning (default True).\n          final_endpoint: The model contains many possible endpoints.\n              `final_endpoint` specifies the last endpoint for the model to be built\n              up to. In addition to the output at `final_endpoint`, all the outputs\n              at endpoints up to `final_endpoint` will also be returned, in a\n              dictionary. `final_endpoint` must be one of\n              InceptionI3d.VALID_ENDPOINTS (default \'Logits\').\n          name: A string (optional). The name of this module.\n        Raises:\n          ValueError: if `final_endpoint` is not recognized.\n        """"""\n\n        if final_endpoint not in self.VALID_ENDPOINTS:\n            raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n        super(I3D, self).__init__()\n        self._num_classes = kwargs[\'labels\'] \n        self._spatial_squeeze = spatial_squeeze\n        self._final_endpoint = final_endpoint\n        self.logits = None\n\n        self.train_transforms = PreprocessTrain(**kwargs)\n        self.test_transforms  = PreprocessEval(**kwargs)\n\n\n        if self._final_endpoint not in self.VALID_ENDPOINTS:\n            raise ValueError(\'Unknown final endpoint %s\' % self._final_endpoint)\n\n        self.end_points = {}\n        end_point = \'Conv3d_1a_7x7\'\n        self.end_points[end_point] = Unit3D(in_channels=in_channels, output_channels=64, kernel_shape=[7, 7, 7],\n                                            stride=(2, 2, 2), padding=(3,3,3),  name=name+end_point)\n        if self._final_endpoint == end_point: return\n        \n        end_point = \'MaxPool3d_2a_3x3\'\n        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),\n                                                             padding=0)\n        if self._final_endpoint == end_point: return\n        \n        end_point = \'Conv3d_2b_1x1\'\n        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=64, kernel_shape=[1, 1, 1], padding=0,\n                                       name=name+end_point)\n        if self._final_endpoint == end_point: return\n        \n        end_point = \'Conv3d_2c_3x3\'\n        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=192, kernel_shape=[3, 3, 3], padding=1,\n                                       name=name+end_point)\n        if self._final_endpoint == end_point: return\n\n        end_point = \'MaxPool3d_3a_3x3\'\n        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),\n                                                             padding=0)\n        if self._final_endpoint == end_point: return\n        \n        end_point = \'Mixed_3b\'\n        self.end_points[end_point] = InceptionModule(192, [64,96,128,16,32,32], name+end_point)\n        if self._final_endpoint == end_point: return\n\n        end_point = \'Mixed_3c\'\n        self.end_points[end_point] = InceptionModule(256, [128,128,192,32,96,64], name+end_point)\n        if self._final_endpoint == end_point: return\n\n        end_point = \'MaxPool3d_4a_3x3\'\n        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(2, 2, 2),\n                                                             padding=0)\n        if self._final_endpoint == end_point: return\n\n        end_point = \'Mixed_4b\'\n        self.end_points[end_point] = InceptionModule(128+192+96+64, [192,96,208,16,48,64], name+end_point)\n        if self._final_endpoint == end_point: return\n\n        end_point = \'Mixed_4c\'\n        self.end_points[end_point] = InceptionModule(192+208+48+64, [160,112,224,24,64,64], name+end_point)\n        if self._final_endpoint == end_point: return\n\n        end_point = \'Mixed_4d\'\n        self.end_points[end_point] = InceptionModule(160+224+64+64, [128,128,256,24,64,64], name+end_point)\n        if self._final_endpoint == end_point: return\n\n        end_point = \'Mixed_4e\'\n        self.end_points[end_point] = InceptionModule(128+256+64+64, [112,144,288,32,64,64], name+end_point)\n        if self._final_endpoint == end_point: return\n\n        end_point = \'Mixed_4f\'\n        self.end_points[end_point] = InceptionModule(112+288+64+64, [256,160,320,32,128,128], name+end_point)\n        if self._final_endpoint == end_point: return\n\n        end_point = \'MaxPool3d_5a_2x2\'\n        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[2, 2, 2], stride=(2, 2, 2),\n                                                             padding=0)\n        if self._final_endpoint == end_point: return\n\n        end_point = \'Mixed_5b\'\n        self.end_points[end_point] = InceptionModule(256+320+128+128, [256,160,320,32,128,128], name+end_point)\n        if self._final_endpoint == end_point: return\n\n        end_point = \'Mixed_5c\'\n        self.end_points[end_point] = InceptionModule(256+320+128+128, [384,192,384,48,128,128], name+end_point)\n        if self._final_endpoint == end_point: return\n\n        end_point = \'Logits\'\n        self.avg_pool = nn.AvgPool3d(kernel_size=[2, 7, 7],\n                                     stride=(1, 1, 1))\n        self.dropout = nn.Dropout(dropout_keep_prob)\n        self.logits = Unit3D(in_channels=384+384+128+128, output_channels=self._num_classes,\n                             kernel_shape=[1, 1, 1],\n                             padding=0,\n                             activation_fn=None,\n                             use_batch_norm=False,\n                             use_bias=True,\n                             name=\'logits\')\n\n        \n\n\n\n        self.build()\n\n        if \'pretrained\' in kwargs.keys() and kwargs[\'pretrained\']:\n            if \'i3d_pretrained\' in kwargs.keys():\n                self._load_checkpoint(kwargs[\'i3d_pretrained\'])\n            else:\n                self._load_pretrained_weights() \n\n    def _load_pretrained_weights(self):\n        p_dict = torch.load(\'weights/i3d_rgb_imagenet.pt\')\n        s_dict = self.state_dict()\n        for name in p_dict:\n            if name in s_dict.keys():\n                if p_dict[name].shape == s_dict[name].shape:\n                    s_dict[name] = p_dict[name]\n\n        self.load_state_dict(s_dict)\n\n    def _load_checkpoint(self, saved_weights):\n        p_dict = torch.load(saved_weights)[\'state_dict\']\n        s_dict = self.state_dict()\n        for name in p_dict:\n            if name in s_dict.keys():\n                if p_dict[name].shape == s_dict[name].shape:\n                    s_dict[name] = p_dict[name]\n\n        self.load_state_dict(s_dict)\n\n\n\n    def replace_logits(self, num_classes):\n        self._num_classes = num_classes\n        self.logits = Unit3D(in_channels=384+384+128+128, output_channels=self._num_classes,\n                             kernel_shape=[1, 1, 1],\n                             padding=0,\n                             activation_fn=None,\n                             use_batch_norm=False,\n                             use_bias=True,\n                             name=\'logits\')\n        \n    \n    def build(self):\n        for k in self.end_points.keys():\n            self.add_module(k, self.end_points[k])\n        \n    def forward(self, x):\n        for end_point in self.VALID_ENDPOINTS:\n            if end_point in self.end_points:\n                x = self._modules[end_point](x) # use _modules to work with dataparallel\n\n        x = self.logits(self.dropout(self.avg_pool(x)))\n\n        if self._spatial_squeeze:\n            logits = x.squeeze(3).squeeze(3)\n        # logits is batch X classes X time, which is what we want to work with\n\n        logits = torch.mean(logits, dim=2)\n        return logits\n        \n\n    def extract_features(self, x):\n        for end_point in self.VALID_ENDPOINTS:\n            if end_point in self.end_points:\n                x = self._modules[end_point](x)\n        return self.avg_pool(x)\n\nclass PreprocessTrain(object):\n    """"""\n    Container for all transforms used to preprocess clips for training in this dataset.\n    """"""\n    def __init__(self, **kwargs):\n        """"""\n        Initialize preprocessing class for training set\n        Args:\n            preprocess (String): Keyword to select different preprocessing types            \n            crop_type  (String): Select random or central crop \n\n        Return:\n            None\n        """"""\n\n        self.transforms  = []\n        self.transforms1 = []\n        self.preprocess  = kwargs[\'preprocess\']\n        crop_type        = kwargs[\'crop_type\']\n\n\n        self.transforms.append(pt.ResizeClip(**kwargs))\n        \n        if crop_type == \'Random\':\n            self.transforms.append(pt.RandomCropClip(**kwargs))\n\n        else:\n            self.transforms.append(pt.CenterCropClip(**kwargs))\n\n        self.transforms.append(pt.SubtractRGBMean(**kwargs))\n        self.transforms.append(pt.RandomFlipClip(direction=\'h\', p=0.5, **kwargs))\n        self.transforms.append(pt.ToTensorClip(**kwargs))\n\n    def __call__(self, input_data):\n        for transform in self.transforms:\n            input_data = transform(input_data)\n\n        return input_data\n\n\nclass PreprocessEval(object):\n    """"""\n    Container for all transforms used to preprocess clips for training in this dataset.\n    """"""\n    def __init__(self, **kwargs):\n        """"""\n        Initialize preprocessing class for training set\n        Args:\n            preprocess (String): Keyword to select different preprocessing types            \n            crop_type  (String): Select random or central crop \n\n        Return:\n            None\n        """"""\n\n        self.transforms = []\n\n        self.transforms.append(pt.ResizeClip(**kwargs))\n        self.transforms.append(pt.CenterCropClip(**kwargs))\n        self.transforms.append(pt.SubtractRGBMean(**kwargs))\n        self.transforms.append(pt.ToTensorClip(**kwargs))\n\n\n    def __call__(self, input_data):\n        for transform in self.transforms:\n            input_data = transform(input_data)\n\n        return input_data\n'"
models/ssd/ssd.py,7,"b'#Original source: https://github.com/amdegroot/ssd.pytorch\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom models.ssd.ssd_utils import *\nimport os\nimport datasets.preprocessing_transforms as pt\n\n__all__ = [\n        \'SSD\'\n]\n\nclass SSD(nn.Module):\n    """"""Single Shot Multibox Architecture\n    The network is composed of a base VGG network followed by the\n    added multibox conv layers.  Each multibox layer branches into\n        1) conv2d for class conf scores\n        2) conv2d for localization predictions\n        3) associated priorbox layer to produce default bounding\n           boxes specific to the layer\'s feature map size.\n    See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n\n    Args:\n        load_type: (string) Can be ""test"" or ""train""\n        resize_shape: input image size\n        base: VGG16 layers for input, size of either 300 or 500\n        extras: extra layers that feed to multibox loc and conf layers\n        head: ""multibox head"" consists of loc and conf conv layers\n    """"""\n\n    def __init__(self, **kwargs):\n        super(SSD, self).__init__()\n\n        self.train_transforms = PreprocessTrainSSD(**kwargs)\n        self.test_transforms  = PreprocessEvalSSD(**kwargs)\n\n        self.load_type = kwargs[\'load_type\']\n        self.num_classes = kwargs[\'labels\']\n        #self.cfg = (coco, voc)[num_classes == 21]\n        self.cfg = {\'num_classes\': 21, \'lr_steps\': (80000, 100000, 120000), \'max_iter\': 120000, \'feature_maps\': [38, 19, 10, 5, 3, 1], \'min_dim\': 300, \'steps\': [8, 16, 32, 64, 100, 300], \'min_sizes\': [30, 60, 111, 162, 213, 264], \'max_sizes\': [60, 111, 162, 213, 264, 315], \'aspect_ratios\': [[2], [2, 3], [2, 3], [2, 3], [2], [2]], \'variance\': [0.1, 0.2], \'clip\': True, \'name\': \'VOC\'}\n\n        self.priorbox = PriorBox(self.cfg)\n        with torch.no_grad():\n            self.priors = self.priorbox.forward()\n        self.size = kwargs[\'resize_shape\'][0]\n\n        base = {\n            \'300\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'C\', 512, 512, 512, \'M\',\n                    512, 512, 512],\n            \'512\': [],\n        }\n        extras = {\n            \'300\': [256, \'S\', 512, 128, \'S\', 256, 128, 256, 128, 256],\n            \'512\': [],\n        }\n        mbox = {\n            \'300\': [4, 6, 6, 6, 4, 4],  # number of boxes per feature map location\n            \'512\': [],\n        }\n        base, extras, head = multibox(vgg(base[str(self.size)], 3),\n                                         add_extras(extras[str(self.size)], 1024),\n                                         mbox[str(self.size)], self.num_classes)\n        # SSD network\n        self.vgg = nn.ModuleList(base)\n        # Layer learns to scale the l2 normalized features from conv4_3\n        self.L2Norm = L2Norm(512, 20)\n        self.extras = nn.ModuleList(extras)\n\n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n\n        if self.load_type == \'test\':\n            self.softmax = nn.Softmax(dim=-1)\n            self.detect = Detect(self.num_classes, 0, 200, 0.01, 0.45)\n\n    def forward(self, x):\n        """"""Applies network layers and ops on input image(s) x.\n\n        Args:\n            x: input image or batch of images. Shape: [batch,3,300,300].\n\n        Return:\n            Depending on load_type:\n            test:\n                Variable(tensor) of output class label predictions,\n                confidence score, and corresponding location predictions for\n                each object detected. Shape: [batch,topk,7]\n\n            train:\n                list of concat outputs from:\n                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n                    3: priorbox layers, Shape: [2,num_priors*4]\n        """"""\n        if len(x.shape) > 4: #dataset outputs shape [batch,3,T,300,300]\n            x.squeeze_(2)\n\n        assert(len(x.shape) == 4)\n\n        sources = list()\n        loc = list()\n        conf = list()\n\n        # apply vgg up to conv4_3 relu\n        for k in range(23):\n            x = self.vgg[k](x)\n\n        s = self.L2Norm(x)\n        sources.append(s)\n\n        # apply vgg up to fc7\n        for k in range(23, len(self.vgg)):\n            x = self.vgg[k](x)\n        sources.append(x)\n\n        # apply extra layers and cache source layer outputs\n        for k, v in enumerate(self.extras):\n            x = F.relu(v(x), inplace=True)\n            if k % 2 == 1:\n                sources.append(x)\n\n        # apply multibox head to source layers\n        for (x, l, c) in zip(sources, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n        if self.load_type == ""test"":\n            output = self.detect(\n                loc.view(loc.size(0), -1, 4),                   # loc preds\n                self.softmax(conf.view(conf.size(0), -1,\n                             self.num_classes)),                # conf preds\n                self.priors.to(x.device)                  # default boxes\n            )\n        else:\n            output = (\n                loc.view(loc.size(0), -1, 4),\n                conf.view(conf.size(0), -1, self.num_classes),\n                self.priors\n            )\n        return output\n\n    def load_weights(self, base_file):\n        other, ext = os.path.splitext(base_file)\n        if ext == \'.pkl\' or \'.pth\':\n            print(\'Loading weights into state dict...\')\n            self.load_state_dict(torch.load(base_file,\n                                 map_location=lambda storage, loc: storage))\n            print(\'Finished!\')\n        else:\n            print(\'Sorry only .pth and .pkl files supported.\')\n\n\n# This function is derived from torchvision VGG make_layers()\n# https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\ndef vgg(cfg, i, batch_norm=False):\n    layers = []\n    in_channels = i\n    for v in cfg:\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        elif v == \'C\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n    layers += [pool5, conv6,\n               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n    return layers\n\n\ndef add_extras(cfg, i, batch_norm=False):\n    # Extra layers added to VGG for feature scaling\n    layers = []\n    in_channels = i\n    flag = False\n    for k, v in enumerate(cfg):\n        if in_channels != \'S\':\n            if v == \'S\':\n                layers += [nn.Conv2d(in_channels, cfg[k + 1],\n                           kernel_size=(1, 3)[flag], stride=2, padding=1)]\n            else:\n                layers += [nn.Conv2d(in_channels, v, kernel_size=(1, 3)[flag])]\n            flag = not flag\n        in_channels = v\n    return layers\n\n\ndef multibox(vgg, extra_layers, cfg, num_classes):\n    loc_layers = []\n    conf_layers = []\n    vgg_source = [21, -2]\n    for k, v in enumerate(vgg_source):\n        loc_layers += [nn.Conv2d(vgg[v].out_channels,\n                                 cfg[k] * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(vgg[v].out_channels,\n                        cfg[k] * num_classes, kernel_size=3, padding=1)]\n    for k, v in enumerate(extra_layers[1::2], 2):\n        loc_layers += [nn.Conv2d(v.out_channels, cfg[k]\n                                 * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(v.out_channels, cfg[k]\n                                  * num_classes, kernel_size=3, padding=1)]\n    return vgg, extra_layers, (loc_layers, conf_layers)\n\nclass PreprocessTrainSSD(object):\n    """"""\n    Container for all transforms used to preprocess clips for training in this dataset.\n    """"""\n    def __init__(self, **kwargs):\n        crop_shape = kwargs[\'crop_shape\']\n        crop_type = kwargs[\'crop_type\']\n        resize_shape = kwargs[\'resize_shape\']\n        self.transforms = []\n\n        if crop_type == \'Random\':\n            self.transforms.append(pt.RandomCropClip(**kwargs))\n        elif crop_type == \'Center\':\n            self.transforms.append(pt.CenterCropClip(**kwargs))\n\n        self.transforms.append(pt.ResizeClip(**kwargs))\n        self.transforms.append(pt.SubtractRGBMean(**kwargs))\n        self.transforms.append(pt.ToTensorClip())\n\n    def __call__(self, input_data, bbox_data=[]):\n        """"""\n        Preprocess the clip and the bbox data accordingly\n        Args:\n            input_data: List of PIL images containing clip frames \n            bbox_data:  Numpy array containing bbox coordinates per object per frame \n\n        Return:\n            input_data: Pytorch tensor containing the processed clip data \n            bbox_data:  Numpy tensor containing the augmented bbox coordinates\n        """"""\n        if bbox_data == []:\n            for transform in self.transforms:\n                input_data = transform(input_data)\n\n            return input_data\n        else:\n            for transform in self.transforms:\n                input_data, bbox_data = transform(input_data, bbox_data)\n\n            return input_data, bbox_data\n\nclass PreprocessEvalSSD(object):\n    """"""\n    Container for all transforms used to preprocess clips for evaluation in this dataset.\n    """"""\n    def __init__(self, **kwargs):\n        crop_shape = kwargs[\'crop_shape\']\n        crop_type = kwargs[\'crop_type\']\n        resize_shape = kwargs[\'resize_shape\']\n        self.transforms = []\n\n        if crop_type == \'Random\':\n            self.transforms.append(pt.RandomCropClip(**kwargs))\n        elif crop_type == \'Center\':\n            self.transforms.append(pt.CenterCropClip(**kwargs))\n\n        self.transforms.append(pt.ResizeClip(**kwargs))\n        self.transforms.append(pt.SubtractRGBMean(**kwargs))\n        self.transforms.append(pt.ToTensorClip())\n\n\n    def __call__(self, input_data, bbox_data=[]):\n        """"""\n        Preprocess the clip and the bbox data accordingly\n        Args:\n            input_data: List of PIL images containing clip frames \n            bbox_data:  Numpy array containing bbox coordinates per object per frame \n\n        Return:\n            input_data: Pytorch tensor containing the processed clip data \n            bbox_data:  Numpy tensor containing the augmented bbox coordinates\n        """"""\n        if bbox_data == []:\n            for transform in self.transforms:\n                input_data = transform(input_data)\n\n            return input_data\n        else:\n            for transform in self.transforms:\n                input_data, bbox_data = transform(input_data, bbox_data)\n\n            return input_data, bbox_data\n\n'"
models/dvsa/dvsa_utils/transformer.py,10,"b'# Originally from https://github.com/salesforce/densecap\n""""""\n Copyright (c) 2018, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n""""""\n# Last modified by Luowei Zhou on 07/01/2018\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\n\nimport random\nimport string\nimport sys\nimport math\nimport uuid\nimport numpy as np\n\nINF = 1e10\n\ndef positional_encodings_like(x, t=None):\n    if t is None:\n        positions = torch.arange(0, x.size(1))\n        if x.is_cuda:\n           positions = positions.cuda(x.get_device())\n    else:\n        positions = t\n    encodings = x.new(*x.size()[1:]).fill_(0)\n    if x.is_cuda:\n        encodings = encodings.cuda(x.get_device())\n\n\n    for channel in range(x.size(-1)):\n        if channel % 2 == 0:\n            encodings[:, channel] = torch.sin(\n                positions.float() / 10000 ** (channel / x.size(2)))\n        else:\n            encodings[:, channel] = torch.cos(\n                positions.float() / 10000 ** ((channel - 1) / x.size(2)))\n    return Variable(encodings)\n\nclass Linear(nn.Linear):\n\n    def forward(self, x):\n        size = x.size()\n        return super().forward(\n            x.contiguous().view(-1, size[-1])).view(*size[:-1], -1)\n\n# F.softmax has strange default behavior, normalizing over dim 0 for 3D inputs\n# deprecated since PyTorch 0.3\n# def softmax(x):\n#     if x.dim() == 3:\n#         return F.softmax(x.transpose(0, 2)).transpose(0, 2)\n#     return F.softmax(x)\n\n# torch.matmul can\'t do (4, 3, 2) @ (4, 2) -> (4, 3)\ndef matmul(x, y):\n    if x.dim() == y.dim():\n        return x @ y\n    if x.dim() == y.dim() - 1:\n        return (x.unsqueeze(-2) @ y).squeeze(-2)\n    return (x @ y.unsqueeze(-2)).squeeze(-2)\n\nclass LayerNorm(nn.Module):\n\n    def __init__(self, d_model, eps=1e-6):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(d_model))\n        self.beta = nn.Parameter(torch.zeros(d_model))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\nclass ResidualBlock(nn.Module):\n\n    def __init__(self, layer, d_model, drop_ratio):\n        super().__init__()\n        self.layer = layer\n        self.dropout = nn.Dropout(drop_ratio)\n        self.layernorm = LayerNorm(d_model)\n\n    def forward(self, *x):\n        return self.layernorm(x[0] + self.dropout(self.layer(*x)))\n\nclass Attention(nn.Module):\n\n    def __init__(self, d_key, drop_ratio, causal):\n        super().__init__()\n        self.scale = math.sqrt(d_key)\n        self.dropout = nn.Dropout(drop_ratio)\n        self.causal = causal\n\n    def forward(self, query, key, value):\n        dot_products = matmul(query, key.transpose(1, 2))\n        if query.dim() == 3 and (self is None or self.causal):\n            tri = torch.ones(key.size(1), key.size(1)).triu(1) * INF\n            if key.is_cuda:\n                tri = tri.cuda(key.get_device())\n            dot_products.data.sub_(tri.unsqueeze(0))\n        return matmul(self.dropout(F.softmax(dot_products / self.scale, dim=2)), value)\n\nclass MultiHead(nn.Module):\n\n    def __init__(self, d_key, d_value, n_heads, drop_ratio, causal=False):\n        super().__init__()\n        self.attention = Attention(d_key, drop_ratio, causal=causal)\n        self.wq = Linear(d_key, d_key, bias=False)\n        self.wk = Linear(d_key, d_key, bias=False)\n        self.wv = Linear(d_value, d_value, bias=False)\n        self.wo = Linear(d_value, d_key, bias=False)\n        self.n_heads = n_heads\n\n    def forward(self, query, key, value):\n        query, key, value = self.wq(query), self.wk(key), self.wv(value)\n        query, key, value = (\n            x.chunk(self.n_heads, -1) for x in (query, key, value))\n        return self.wo(torch.cat([self.attention(q, k, v)\n                          for q, k, v in zip(query, key, value)], -1))\n\nclass FeedForward(nn.Module):\n\n    def __init__(self, d_model, d_hidden):\n        super().__init__()\n        self.linear1 = Linear(d_model, d_hidden)\n        self.linear2 = Linear(d_hidden, d_model)\n\n    def forward(self, x):\n        return self.linear2(F.relu(self.linear1(x)))\n\nclass EncoderLayer(nn.Module):\n\n    def __init__(self, d_model, d_hidden, n_heads, drop_ratio):\n        super().__init__()\n        self.selfattn = ResidualBlock(\n            MultiHead(d_model, d_model, n_heads, drop_ratio),\n            d_model, drop_ratio)\n        self.feedforward = ResidualBlock(FeedForward(d_model, d_hidden),\n                                         d_model, drop_ratio)\n\n    def forward(self, x):\n        return self.feedforward(self.selfattn(x, x, x))\n\nclass Encoder(nn.Module):\n\n    def __init__(self, d_model, d_hidden, n_vocab, n_layers, n_heads,\n                 drop_ratio):\n        super().__init__()\n        # self.linear = nn.Linear(d_model*2, d_model)\n        self.layers = nn.ModuleList(\n            [EncoderLayer(d_model, d_hidden, n_heads, drop_ratio)\n             for i in range(n_layers)])\n        self.dropout = nn.Dropout(drop_ratio)\n\n    def forward(self, x, mask=None):\n        # x = self.linear(x)\n        x = x+positional_encodings_like(x)\n        x = self.dropout(x)\n        if mask is not None:\n            x = x*mask\n        encoding = []\n        for layer in self.layers:\n            x = layer(x)\n            if mask is not None:\n                x = x*mask\n            encoding.append(x)\n        return encoding\n\nclass Transformer(nn.Module):\n\n    def __init__(self, d_model, n_vocab_src, vocab_trg, d_hidden=2048,\n                 n_layers=6, n_heads=8, drop_ratio=0.1):\n        super().__init__()\n        self.encoder = Encoder(d_model, d_hidden, n_vocab_src, n_layers,\n                               n_heads, drop_ratio)\n\n    def denum(self, data):\n        return \' \'.join(self.decoder.vocab.itos[i] for i in data).replace(\n            \' <eos>\', \'#\').replace(\' <pad>\', \'\')\n\n    def forward(self, x):\n        encoding = self.encoder(x)\n\n        return encoding[-1], encoding\n\n'"
models/ssd/ssd_utils/__init__.py,0,b'from .functions import *\nfrom .modules import *\n'
models/ssd/ssd_utils/box_utils.py,23,"b'# -*- coding: utf-8 -*-\nimport torch\n\n\ndef point_form(boxes):\n    """""" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n    representation for comparison to point form ground truth data.\n    Args:\n        boxes: (tensor) center-size default boxes from priorbox layers.\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n\n\ndef center_size(boxes):\n    """""" Convert prior_boxes to (cx, cy, w, h)\n    representation for comparison to center-size form ground truth data.\n    Args:\n        boxes: (tensor) point_form boxes\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat((boxes[:, 2:] + boxes[:, :2])/2,  # cx, cy\n                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h\n\n\ndef intersect(box_a, box_b):\n    """""" We resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].\n    """"""\n    A = box_a.size(0)\n    B = box_b.size(0)\n    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef jaccard(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\ndef match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n    """"""Match each prior box with the ground truth box of the highest jaccard\n    overlap, encode the bounding boxes, then return the matched indices\n    corresponding to both confidence and location preds.\n    Args:\n        threshold: (float) The overlap threshold used when mathing boxes.\n        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n        variances: (tensor) Variances corresponding to each prior coord,\n            Shape: [num_priors, 4].\n        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n        idx: (int) current batch index\n    Return:\n        The matched indices corresponding to 1)location and 2)confidence preds.\n    """"""\n    # jaccard index\n    overlaps = jaccard(\n        truths,\n        point_form(priors)\n    )\n    # (Bipartite Matching)\n    # [1,num_objects] best prior for each ground truth\n    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n    # [1,num_priors] best ground truth for each prior\n    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n    best_truth_idx.squeeze_(0)\n    best_truth_overlap.squeeze_(0)\n    best_prior_idx.squeeze_(1)\n    best_prior_overlap.squeeze_(1)\n    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior\n    # TODO refactor: index  best_prior_idx with long tensor\n    # ensure every gt matches with its prior of max overlap\n    for j in range(best_prior_idx.size(0)):\n        best_truth_idx[best_prior_idx[j]] = j\n    matches = truths[best_truth_idx]          # Shape: [num_priors,4]\n    conf = labels[best_truth_idx] + 1         # Shape: [num_priors]\n    conf[best_truth_overlap < threshold] = 0  # label as background\n    loc = encode(matches, priors, variances)\n    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n    conf_t[idx] = conf  # [num_priors] top class label for each prior\n\n\ndef encode(matched, priors, variances):\n    """"""Encode the variances from the priorbox layers into the ground truth boxes\n    we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 4].\n        priors: (tensor) Prior boxes in center-offset form\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        encoded boxes (tensor), Shape: [num_priors, 4]\n    """"""\n\n    # dist b/t match center and prior\'s center\n    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n    # encode variance\n    g_cxcy /= (variances[0] * priors[:, 2:])\n    # match wh / prior wh\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n    g_wh = torch.log(g_wh) / variances[1]\n    # return target for smooth_l1_loss\n    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n\n\n# Adapted from https://github.com/Hakuyume/chainer-ssd\ndef decode(loc, priors, variances):\n    """"""Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded bounding box predictions\n    """"""\n\n    boxes = torch.cat((\n        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n    return boxes\n\n\ndef log_sum_exp(x):\n    """"""Utility function for computing log_sum_exp while determining\n    This will be used to determine unaveraged confidence loss across\n    all examples in a batch.\n    Args:\n        x (Variable(tensor)): conf_preds from conf layers\n    """"""\n    x_max = x.data.max()\n    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max\n\n\n# Original author: Francisco Massa:\n# https://github.com/fmassa/object-detection.torch\n# Ported to PyTorch by Max deGroot (02/01/2017)\ndef nms(boxes, scores, overlap=0.5, top_k=200):\n    """"""Apply non-maximum suppression at test time to avoid detecting too many\n    overlapping bounding boxes for a given object.\n    Args:\n        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n        scores: (tensor) The class predscores for the img, Shape:[num_priors].\n        overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n        top_k: (int) The Maximum number of box preds to consider.\n    Return:\n        The indices of the kept boxes with respect to num_priors.\n    """"""\n\n    keep = scores.new(scores.size(0)).zero_().long()\n    if boxes.numel() == 0:\n        return keep\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    area = torch.mul(x2 - x1, y2 - y1)\n    v, idx = scores.sort(0)  # sort in ascending order\n    # I = I[v >= 0.01]\n    idx = idx[-top_k:]  # indices of the top-k largest vals\n    xx1 = boxes.new()\n    yy1 = boxes.new()\n    xx2 = boxes.new()\n    yy2 = boxes.new()\n    w = boxes.new()\n    h = boxes.new()\n\n    # keep = torch.Tensor()\n    count = 0\n    while idx.numel() > 0:\n        i = idx[-1]  # index of current largest val\n        # keep.append(i)\n        keep[count] = i\n        count += 1\n        if idx.size(0) == 1:\n            break\n        idx = idx[:-1]  # remove kept element from view\n        # load bboxes of next highest vals\n        torch.index_select(x1, 0, idx, out=xx1)\n        torch.index_select(y1, 0, idx, out=yy1)\n        torch.index_select(x2, 0, idx, out=xx2)\n        torch.index_select(y2, 0, idx, out=yy2)\n        # store element-wise max with next highest score\n        xx1 = torch.clamp(xx1, min=x1[i])\n        yy1 = torch.clamp(yy1, min=y1[i])\n        xx2 = torch.clamp(xx2, max=x2[i])\n        yy2 = torch.clamp(yy2, max=y2[i])\n        w.resize_as_(xx2)\n        h.resize_as_(yy2)\n        w = xx2 - xx1\n        h = yy2 - yy1\n        # check sizes of xx1 and xx2.. after each iteration\n        w = torch.clamp(w, min=0.0)\n        h = torch.clamp(h, min=0.0)\n        inter = w*h\n        # IoU = i / (area(a) + area(b) - i)\n        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n        union = (rem_areas - inter) + area[i]\n        IoU = inter/union  # store result in iou\n        # keep only elements with an IoU <= overlap\n        idx = idx[IoU.le(overlap)]\n    return keep, count\n'"
models/ssd/ssd_utils/functions/__init__.py,0,"b""from .detection import Detect\nfrom .prior_box import PriorBox\n\n\n__all__ = ['Detect', 'PriorBox']\n"""
models/ssd/ssd_utils/functions/detection.py,3,"b'import torch\nfrom torch.autograd import Function\nfrom ..box_utils import decode, nms\n#from data import voc as cfg\n\n\nclass Detect(Function):\n    """"""At test time, Detect is the final layer of SSD.  Decode location preds,\n    apply non-maximum suppression to location predictions based on conf\n    scores and threshold to a top_k number of output predictions for both\n    confidence score and locations.\n    """"""\n    def __init__(self, num_classes, bkg_label, top_k, conf_thresh, nms_thresh):\n        self.num_classes = num_classes\n        self.background_label = bkg_label\n        self.top_k = top_k\n        # Parameters used in nms.\n        self.nms_thresh = nms_thresh\n        if nms_thresh <= 0:\n            raise ValueError(\'nms_threshold must be non negative.\')\n        self.conf_thresh = conf_thresh\n        #self.variance = cfg[\'variance\']\n        self.variance = [0.1, 0.2]\n\n    def forward(self, loc_data, conf_data, prior_data):\n        """"""\n        Args:\n            loc_data: (tensor) Loc preds from loc layers\n                Shape: [batch,num_priors*4]\n            conf_data: (tensor) Shape: Conf preds from conf layers\n                Shape: [batch*num_priors,num_classes]\n            prior_data: (tensor) Prior boxes and variances from priorbox layers\n                Shape: [1,num_priors,4]\n        """"""\n        num = loc_data.size(0)  # batch size\n        num_priors = prior_data.size(0)\n        output = torch.zeros(num, self.num_classes, self.top_k, 5)\n        conf_preds = conf_data.view(num, num_priors,\n                                    self.num_classes).transpose(2, 1)\n\n        # Decode predictions into bboxes.\n        for i in range(num):\n            decoded_boxes = decode(loc_data[i], prior_data, self.variance)\n            # For each class, perform nms\n            conf_scores = conf_preds[i].clone()\n\n            for cl in range(1, self.num_classes):\n                c_mask = conf_scores[cl].gt(self.conf_thresh)\n                scores = conf_scores[cl][c_mask]\n                if scores.size(0) == 0:\n                    continue\n                l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)\n                boxes = decoded_boxes[l_mask].view(-1, 4)\n                # idx of highest scoring and non-overlapping boxes per class\n                ids, count = nms(boxes, scores, self.nms_thresh, self.top_k)\n                output[i, cl, :count] = \\\n                    torch.cat((scores[ids[:count]].unsqueeze(1),\n                               boxes[ids[:count]]), 1)\n        flt = output.contiguous().view(num, -1, 5)\n        _, idx = flt[:, :, 0].sort(1, descending=True)\n        _, rank = idx.sort(1)\n        flt[(rank < self.top_k).unsqueeze(-1).expand_as(flt)].fill_(0)\n        return output\n'"
models/ssd/ssd_utils/functions/prior_box.py,1,"b'from __future__ import division\nfrom math import sqrt as sqrt\nfrom itertools import product as product\nimport torch\n\n\nclass PriorBox(object):\n    """"""Compute priorbox coordinates in center-offset form for each source\n    feature map.\n    """"""\n    def __init__(self, cfg):\n        super(PriorBox, self).__init__()\n        self.image_size = cfg[\'min_dim\']\n        # number of priors for feature map location (either 4 or 6)\n        self.num_priors = len(cfg[\'aspect_ratios\'])\n        self.variance = cfg[\'variance\'] or [0.1]\n        self.feature_maps = cfg[\'feature_maps\']\n        self.min_sizes = cfg[\'min_sizes\']\n        self.max_sizes = cfg[\'max_sizes\']\n        self.steps = cfg[\'steps\']\n        self.aspect_ratios = cfg[\'aspect_ratios\']\n        self.clip = cfg[\'clip\']\n        self.version = cfg[\'name\']\n        for v in self.variance:\n            if v <= 0:\n                raise ValueError(\'Variances must be greater than 0\')\n\n    def forward(self):\n        mean = []\n        for k, f in enumerate(self.feature_maps):\n            for i, j in product(range(f), repeat=2):\n                f_k = self.image_size / self.steps[k]\n                # unit center x,y\n                cx = (j + 0.5) / f_k\n                cy = (i + 0.5) / f_k\n\n                # aspect_ratio: 1\n                # rel size: min_size\n                s_k = self.min_sizes[k]/self.image_size\n                mean += [cx, cy, s_k, s_k]\n\n                # aspect_ratio: 1\n                # rel size: sqrt(s_k * s_(k+1))\n                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n                mean += [cx, cy, s_k_prime, s_k_prime]\n\n                # rest of aspect ratios\n                for ar in self.aspect_ratios[k]:\n                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n        # back to torch land\n        output = torch.Tensor(mean).view(-1, 4)\n        if self.clip:\n            output.clamp_(max=1, min=0)\n        return output\n'"
models/ssd/ssd_utils/modules/__init__.py,0,"b""from .l2norm import L2Norm\nfrom .multibox_loss import MultiBoxLoss\n\n__all__ = ['L2Norm', 'MultiBoxLoss']\n"""
models/ssd/ssd_utils/modules/l2norm.py,6,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.autograd import Variable\nimport torch.nn.init as init\n\nclass L2Norm(nn.Module):\n    def __init__(self,n_channels, scale):\n        super(L2Norm,self).__init__()\n        self.n_channels = n_channels\n        self.gamma = scale or None\n        self.eps = 1e-10\n        self.weight = nn.Parameter(torch.Tensor(self.n_channels))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        init.constant_(self.weight,self.gamma)\n\n    def forward(self, x):\n        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()+self.eps\n        #x /= norm\n        x = torch.div(x,norm)\n        out = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x\n        return out\n'"
models/ssd/ssd_utils/modules/multibox_loss.py,9,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n#from data import coco as cfg\nfrom ..box_utils import match, log_sum_exp\n\n\nclass MultiBoxLoss(nn.Module):\n    """"""SSD Weighted Loss Function\n    Compute Targets:\n        1) Produce Confidence Target Indices by matching  ground truth boxes\n           with (default) \'priorboxes\' that have jaccard index > threshold parameter\n           (default threshold: 0.5).\n        2) Produce localization target by \'encoding\' variance into offsets of ground\n           truth boxes and their matched  \'priorboxes\'.\n        3) Hard negative mining to filter the excessive number of negative examples\n           that comes with using a large number of default bounding boxes.\n           (default negative:positive ratio 3:1)\n    Objective Loss:\n        L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n        weighted by \xce\xb1 which is set to 1 by cross val.\n        Args:\n            c: class confidences,\n            l: predicted boxes,\n            g: ground truth boxes\n            N: number of matched default boxes\n        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n    """"""\n\n    def __init__(self, num_classes, overlap_thresh, prior_for_matching,\n                 bkg_label, neg_mining, neg_pos, neg_overlap, encode_target,\n                 use_gpu=True):\n        super(MultiBoxLoss, self).__init__()\n        self.use_gpu = use_gpu\n        self.num_classes = num_classes\n        self.threshold = overlap_thresh\n        self.background_label = bkg_label\n        self.encode_target = encode_target\n        self.use_prior_for_matching = prior_for_matching\n        self.do_neg_mining = neg_mining\n        self.negpos_ratio = neg_pos\n        self.neg_overlap = neg_overlap\n#        self.variance = cfg[\'variance\']\n        self.variance = [0.1, 0.2]\n\n    def forward(self, predictions, targets):\n        """"""Multibox Loss\n        Args:\n            predictions (tuple): A tuple containing loc preds, conf preds,\n            and prior boxes from SSD net.\n                conf shape: torch.size(batch_size,num_priors,num_classes)\n                loc shape: torch.size(batch_size,num_priors,4)\n                priors shape: torch.size(num_priors,4)\n\n            targets (tensor): Ground truth boxes and labels for a batch,\n                shape: [batch_size,num_objs,5] (last idx is the label).\n        """"""\n        loc_data, conf_data, priors = predictions\n        num = loc_data.size(0)\n        priors = priors[:loc_data.size(1), :]\n        num_priors = (priors.size(0))\n        num_classes = self.num_classes\n\n        # match priors (default boxes) and ground truth boxes\n        loc_t = torch.Tensor(num, num_priors, 4)\n        conf_t = torch.LongTensor(num, num_priors)\n        for idx in range(num):\n            truths = targets[idx][:, :-1].data\n            labels = targets[idx][:, -1].data\n            defaults = priors.data\n            match(self.threshold, truths, defaults, self.variance, labels,\n                  loc_t, conf_t, idx)\n        if self.use_gpu:\n            loc_t = loc_t.cuda()\n            conf_t = conf_t.cuda()\n        # wrap targets\n        loc_t = Variable(loc_t, requires_grad=False)\n        conf_t = Variable(conf_t, requires_grad=False)\n\n        pos = conf_t > 0\n        num_pos = pos.sum(dim=1, keepdim=True)\n\n        # Localization Loss (Smooth L1)\n        # Shape: [batch,num_priors,4]\n        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n        loc_p = loc_data[pos_idx].view(-1, 4)\n        loc_t = loc_t[pos_idx].view(-1, 4)\n        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)\n\n        # Compute max conf across batch for hard negative mining\n        batch_conf = conf_data.view(-1, self.num_classes)\n        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n\n        # Hard Negative Mining\n        loss_c[pos] = 0  # filter out pos boxes for now\n        loss_c = loss_c.view(num, -1)\n        _, loss_idx = loss_c.sort(1, descending=True)\n        _, idx_rank = loss_idx.sort(1)\n        num_pos = pos.long().sum(1, keepdim=True)\n        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n        neg = idx_rank < num_neg.expand_as(idx_rank)\n\n        # Confidence Loss Including Positive and Negative Examples\n        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n        targets_weighted = conf_t[(pos+neg).gt(0)]\n        loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False)\n\n        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n\n        N = num_pos.data.sum()\n        loss_l /= N\n        loss_c /= N\n        return loss_l, loss_c\n'"
