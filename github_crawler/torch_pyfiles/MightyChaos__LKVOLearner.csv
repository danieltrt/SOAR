file_path,api_count,code
src/BilinearSampling.py,4,"b""import torch\nfrom torch import FloatTensor\nfrom torch.autograd import Variable\nfrom timeit import default_timer as timer\nfrom torch.nn.functional import grid_sample\nfrom torch.nn import ReplicationPad2d\n\n\ndef grid_bilinear_sampling(A, x, y):\n    batch_size, k, h, w = A.size()\n    x_norm = x/((w-1)/2) - 1\n    y_norm = y/((h-1)/2) - 1\n    grid = torch.cat((x_norm.view(batch_size, h, w, 1), y_norm.view(batch_size, h, w, 1)), 3)\n    Q = grid_sample(A, grid, mode='bilinear')\n    in_view_mask = Variable(((x_norm.data > -1+2/w) & (x_norm.data < 1-2/w) & (y_norm.data > -1+2/h) & (y_norm.data < 1-2/h)).type_as(A.data))\n    # in_view_mask = Variable(((x.data > 1) & (x.data < w-2) & (y.data > 1) & (y.data < h-2)).type_as(A.data))\n    # in_view_mask = Variable(((x.data > -1+3/w) & (x.data < 1-3/w) & (y.data > -1+3/h) & (y.data < 1-3/h)).type_as(A.data))\n    return Q.view(batch_size, k, h*w), in_view_mask\n"""
src/DirectVOLayer.py,43,"b'# class DirectVO():\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch import optim\nfrom torch.nn import AvgPool2d\nfrom ImagePyramid import ImagePyramidLayer\nfrom BilinearSampling import grid_bilinear_sampling\nfrom MatInverse import inv\nimport os\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport scipy.io as sio\nfrom timeit import default_timer as timer\n\nIMG_CHAN = 3\n\n# helper functions\n\ndef meshgrid(x, y):\n    imW = x.size(0)\n    imH = y.size(0)\n    X = x.unsqueeze(0).repeat(imH, 1)\n    Y = y.unsqueeze(1).repeat(1, imW)\n    return X, Y\n\ndef inv_rigid_transformation(rot_mat_batch, trans_batch):\n    inv_rot_mat_batch = rot_mat_batch.transpose(1,2)\n    inv_trans_batch = -inv_rot_mat_batch.bmm(trans_batch.unsqueeze(-1)).squeeze(-1)\n    return inv_rot_mat_batch, inv_trans_batch\n\nclass LaplacianLayer(nn.Module):\n    def __init__(self):\n        super(LaplacianLayer, self).__init__()\n        w_nom = torch.FloatTensor([[0, -1, 0], [-1, 4, -1], [0, -1, 0]]).view(1,1,3,3)\n        w_den = torch.FloatTensor([[0, 1, 0], [1, 4, 1], [0, 1, 0]]).view(1,1,3,3)\n        self.register_buffer(\'w_nom\', w_nom)\n        self.register_buffer(\'w_den\', w_den)\n\n    def forward(self, input, do_normalize=True):\n        assert(input.dim() == 2 or input.dim()==3 or input.dim()==4)\n        input_size = input.size()\n        if input.dim()==4:\n            x = input.view(input_size[0]*input_size[1], 1,\n                            input_size[2], input_size[3])\n        elif input.dim()==3:\n            x = input.unsqueeze(1)\n        else:\n            x = input.unsqueeze(0).unsqueeze(0)\n        x_nom = torch.nn.functional.conv2d(input=x,\n                        weight=Variable(self.w_nom),\n                        stride=1,\n                        padding=0)\n        if do_normalize:\n            x_den = torch.nn.functional.conv2d(input=x,\n                        weight=Variable(self.w_den),\n                        stride=1,\n                        padding=0)\n            # x_den = x.std() + 1e-5\n            x = (x_nom.abs()/x_den)\n        else:\n            x = x_nom.abs()\n        if input.dim() == 4:\n            return x.view(input_size[0], input_size[1], input_size[2]-2, input_size[3]-2)\n        elif input.dim() == 3:\n            return x.squeeze(1)\n        elif input.dim() == 2:\n            return x.squeeze(0).squeeze(0)\n\n\n\nclass GradientLayer(nn.Module):\n    def __init__(self):\n        super(GradientLayer, self).__init__()\n        wx = torch.FloatTensor([-.5, 0, .5]).view(1, 1, 1, 3)\n        wy = torch.FloatTensor([[-.5], [0], [.5]]).view(1, 1, 3, 1)\n        self.register_buffer(\'wx\', wx)\n        self.register_buffer(\'wy\', wy)\n        self.padx_func = torch.nn.ReplicationPad2d((1,1,0,0))\n        self.pady_func = torch.nn.ReplicationPad2d((0,0,1,1))\n\n    def forward(self, img):\n        img_ = img.unsqueeze(1)\n        img_padx = self.padx_func(img_)\n        img_dx = torch.nn.functional.conv2d(input=img_padx,\n                        weight=Variable(self.wx),\n                        stride=1,\n                        padding=0).squeeze(1)\n\n        img_pady = self.pady_func(img_)\n        img_dy = torch.nn.functional.conv2d(input=img_pady,\n                        weight=Variable(self.wy),\n                        stride=1,\n                        padding=0).squeeze(1)\n\n        return img_dx, img_dy\n\n\nclass Twist2Mat(nn.Module):\n    def __init__(self):\n        super(Twist2Mat, self).__init__()\n        self.register_buffer(\'o\', torch.zeros(1,1))\n        self.register_buffer(\'E\', torch.eye(3))\n\n    def cprodmat_batch(self, a_batch):\n        batch_size, _ = a_batch.size()\n        o = Variable(self.o).expand(batch_size, 1)\n        a0 = a_batch[:, 0:1]\n        a1 = a_batch[:, 1:2]\n        a2 = a_batch[:, 2:3]\n        return torch.cat((o, -a2, a1, a2, o, -a0, -a1, a0, o), 1).view(batch_size, 3, 3)\n\n    def forward(self, twist_batch):\n        batch_size, _ = twist_batch.size()\n        rot_angle = twist_batch.norm(p=2, dim=1).view(batch_size, 1).clamp(min=1e-5)\n        rot_axis = twist_batch / rot_angle.expand(batch_size, 3)\n        A = self.cprodmat_batch(rot_axis)\n        return Variable(self.E).view(1, 3, 3).expand(batch_size, 3, 3)\\\n            + A*rot_angle.sin().view(batch_size, 1, 1).expand(batch_size, 3, 3)\\\n            + A.bmm(A)*((1-rot_angle.cos()).view(batch_size, 1, 1).expand(batch_size, 3, 3))\n\ndef compute_img_stats(img):\n    # img_pad = torch.nn.ReplicationPad2d(1)(img)\n    img_pad = img\n    mu = AvgPool2d(kernel_size=3, stride=1, padding=0)(img_pad)\n    sigma = AvgPool2d(kernel_size=3, stride=1, padding=0)(img_pad**2) - mu**2\n    return mu, sigma\n\ndef compute_img_stats_pyramid(frames_pyramid):\n    mu_pyramid = []\n    sigma_pyramid = []\n    for layer_idx in range(len(frames_pyramid)):\n        mu, sigma = compute_img_stats(frames_pyramid[layer_idx])\n        mu_pyramid.append(mu)\n        sigma_pyramid.append(sigma)\n    return mu_pyramid, sigma_pyramid\n\n\ndef compute_SSIM(img0, mu0, sigma0, img1, mu1, sigma1):\n    # img0_img1_pad = torch.nn.ReplicationPad2d(1)(img0 * img1)\n    img0_img1_pad = img0*img1\n    sigma01 = AvgPool2d(kernel_size=3, stride=1, padding=0)(img0_img1_pad) - mu0*mu1\n    # C1 = .01 ** 2\n    # C2 = .03 ** 2\n    C1 = .001\n    C2 = .009\n\n    ssim_n = (2*mu0*mu1 + C1) * (2*sigma01 + C2)\n    ssim_d = (mu0**2 + mu1**2 + C1) * (sigma0 + sigma1 + C2)\n    ssim = ssim_n / ssim_d\n    return ((1-ssim)*.5).clamp(0, 1)\n\n\ndef compute_photometric_cost(img_diff, mask):\n    # cost = ((img0.expand_as(img1) - img1) * mask).abs().sum()\n    k = img_diff.size(1)\n    batch_size = img_diff.size(0)\n    mask_ = (mask.view(batch_size, 1, -1) * (1/127.5)).expand_as(img_diff)\n    # mask_ = (mask.view(batch_size, 1, -1)*(1/127.5)).repeat(1, k, 1)\n    cost = img_diff.abs() * mask_\n    return cost\n\ndef compute_photometric_cost_norm(img_diff, mask):\n    # cost = ((img0.expand_as(img1) - img1) * mask).abs().sum()\n    # k = img_diff.size(1)\n    # batch_size = img_diff.size(0)\n    # mask_ = mask.view(batch_size, 1, -1).expand_as(img_diff)\n\n    cost = img_diff.abs().sum(1) * mask\n    # cost = img_diff.abs() * mask_\n    num_in_view = mask.sum(1)\n    # cost_norm = cost.contiguous().sum() / (num_in_view+1e-10)\n    cost_norm = cost.sum(1) / (num_in_view+1e-10)\n    # print(mask.size())\n    return cost_norm * (1 / 127.5), (num_in_view / mask.size(1)).min()\n\n\ndef gradient(input, do_normalize=False):\n    if input.dim() == 2:\n        D_ry = input[1:, :]\n        D_ly = input[:-1, :]\n        D_rx = input[:, 1:]\n        D_lx = input[:, :-1]\n    elif input.dim() == 3:\n        D_ry = input[:, 1:, :]\n        D_ly = input[:, :-1, :]\n        D_rx = input[:, :, 1:]\n        D_lx = input[:, :, :-1]\n    elif input.dim() == 4:\n        D_ry = input[:, :, 1:, :]\n        D_ly = input[:, :, :-1, :]\n        D_rx = input[:, :, :, 1:]\n        D_lx = input[:, :, :, :-1]\n    # if input.dim() == 2:\n    #     D_dy = input[1:, :] - input[:-1, :]\n    #     D_dx = input[:, 1:] - input[:, :-1]\n    # elif input.dim() == 3:\n    #     D_dy = input[:, 1:, :] - input[:, :-1, :]\n    #     D_dx = input[:, :, 1:] - input[:, :, :-1]\n    # elif input.dim() == 4:\n    #     D_dy = input[:, :, 1:, :] - input[:, :, :-1, :]\n    #     D_dx = input[:, :, :, 1:] - input[:, :, :, :-1]\n    Dx = D_rx - D_lx\n    Dy = D_ry - D_ly\n    if do_normalize:\n        Dx = Dx / (D_rx + D_lx)\n        Dy = Dy / (D_ry + D_ly)\n    return Dx, Dy\n\n\n\n\nclass DirectVO(nn.Module):\n\n    def __init__(self, imH=128, imW=416, pyramid_layer_num=5, max_itr_num=20):\n        super(DirectVO, self).__init__()\n        self.max_itr_num = max_itr_num\n        self.imH = imH\n        self.imW = imW\n        self.pyramid_layer_num = pyramid_layer_num\n\n        self.twist2mat_batch_func = Twist2Mat()\n        self.img_gradient_func = GradientLayer()\n        self.pyramid_func = ImagePyramidLayer(chan=3,\n                                pyramid_layer_num=self.pyramid_layer_num)\n        self.laplacian_func = LaplacianLayer()\n\n        x_pyramid, y_pyramid = self.pyramid_func.get_coords(self.imH, self.imW)\n\n\n        # self.x_pyramid = nn.ParameterList(\n        #             [nn.Parameter(torch.from_numpy(x), False) for x in x_pyramid])\n        # self.y_pyramid = nn.ParameterList(\n        #             [nn.Parameter(torch.from_numpy(y), False) for y in y_pyramid])\n\n        for i in range(self.pyramid_layer_num):\n            self.register_buffer(\'x_\'+str(i), torch.from_numpy(x_pyramid[i]).float())\n            self.register_buffer(\'y_\'+str(i), torch.from_numpy(y_pyramid[i]).float())\n        self.register_buffer(\'o\', torch.zeros(1,1))\n        self.register_buffer(\'E\', torch.eye(3))\n\n\n    def setCamera(self, cx, cy, fx, fy):\n        self.camparams = dict(fx=fx, fy=fy, cx=cx, cy=cy)\n\n\n    def init(self, ref_frame_pyramid, inv_depth_pyramid):\n        # ref_frame 3 * H * W\n        assert(self.pyramid_layer_num == len(inv_depth_pyramid))\n        self.inv_depth_pyramid = inv_depth_pyramid\n        # self.ref_frame_pyramid = self.pyramid_func(ref_frame)\n        self.ref_frame_pyramid = ref_frame_pyramid\n\n        for i in range(self.pyramid_layer_num):\n            assert(self.ref_frame_pyramid[i].size(-1) == inv_depth_pyramid[i].size(-1))\n            assert (self.ref_frame_pyramid[i].size(-2) == inv_depth_pyramid[i].size(-2))\n        self.init_lk_terms()\n\n\n\n    def init_lk_terms(self):\n        # self.inv_depth_pyramid, self.x_pyramid, self.y_pyramid = buildImagePyramid(inv_depth, self.pyramid_layer_num)\n        self.xy_pyramid = []\n        self.ref_imgrad_x_pyramid = []\n        self.ref_imgrad_y_pyramid = []\n        self.invH_pyramid = []\n        self.dIdp_pyramid = []\n        self.invH_dIdp_pyramid = []\n\n        for i in range(self.pyramid_layer_num):\n            _, h, w = self.ref_frame_pyramid[i].size()\n\n            x = (Variable(getattr(self, \'x_\'+str(i))) - self.camparams[\'cx\']) / self.camparams[\'fx\']\n            y = (Variable(getattr(self, \'y_\'+str(i))) - self.camparams[\'cy\']) / self.camparams[\'fy\']\n\n            X, Y = meshgrid(x, y)\n            xy = torch.cat((X.view(1, X.numel()),\n                            Y.view(1, Y.numel())), 0)\n            self.xy_pyramid.append(xy)\n\n            # compute image gradient\n            imgrad_x, imgrad_y = self.img_gradient_func(self.ref_frame_pyramid[i])\n\n            self.ref_imgrad_x_pyramid.append(imgrad_x*(self.camparams[\'fx\']/2**i))\n            self.ref_imgrad_y_pyramid.append(imgrad_y*(self.camparams[\'fy\']/2**i))\n\n            # precompute terms for LK regress\n            dIdp = self.compute_dIdp(self.ref_imgrad_x_pyramid[i],\n                                     self.ref_imgrad_y_pyramid[i],\n                                     self.inv_depth_pyramid[i],\n                                     self.xy_pyramid[i])\n            self.dIdp_pyramid.append(dIdp)\n            invH = inv(dIdp.t().mm(dIdp))\n            self.invH_pyramid.append(invH)\n            # self.invH_dIdp_pyramid.append(self.invH_pyramid[-1].mm(dIdp.t()))\n            self.invH_dIdp_pyramid.append(invH.mm(dIdp.t()))\n\n    def init_xy_pyramid(self, ref_frames_pyramid):\n        # self.inv_depth_pyramid, self.x_pyramid, self.y_pyramid = buildImagePyramid(inv_depth, self.pyramid_layer_num)\n        self.xy_pyramid = []\n        self.ref_imgrad_x_pyramid = []\n        self.ref_imgrad_y_pyramid = []\n\n        for i in range(self.pyramid_layer_num):\n            _, h, w = ref_frames_pyramid[i].size()\n\n            x = (Variable(getattr(self, \'x_\'+str(i))) - self.camparams[\'cx\']) / self.camparams[\'fx\']\n            y = (Variable(getattr(self, \'y_\'+str(i))) - self.camparams[\'cy\']) / self.camparams[\'fy\']\n\n            X, Y = meshgrid(x, y)\n            xy = torch.cat((X.view(1, X.numel()),\n                            Y.view(1, Y.numel())), 0)\n            self.xy_pyramid.append(xy)\n\n\n\n    def compute_dIdp(self, imgrad_x, imgrad_y, inv_depth, xy):\n        k, h, w = imgrad_x.size()\n        _, pt_num = xy.size()\n        assert(h*w == pt_num)\n        feat_dim = pt_num*k\n        x = xy[0, :].view(pt_num, 1)\n        y = xy[1, :].view(pt_num, 1)\n        xty = x * y\n        O = Variable(self.o).expand(pt_num, 1)\n        inv_depth_ = inv_depth.view(pt_num, 1)\n        dxdp = torch.cat((-xty, 1 + x ** 2, -y, inv_depth_, O, -inv_depth_.mul(x)), 1)\n        dydp = torch.cat((-1 - y ** 2, xty, x, O, inv_depth_, -inv_depth_.mul(y)), 1)\n\n        imgrad_x_ = imgrad_x.view(feat_dim, 1).expand(feat_dim, 6)\n        imgrad_y_ = imgrad_y.view(feat_dim, 1).expand(feat_dim, 6)\n\n        dIdp = imgrad_x_.mul(dxdp.repeat(k, 1)) + \\\n            imgrad_y_.mul(dydp.repeat(k, 1))\n        return dIdp\n\n\n\n    def LKregress(self, invH_dIdp, mask, It):\n        batch_size, pt_num = mask.size()\n        _, k, _ = It.size()\n        feat_dim = k*pt_num\n        invH_dIdp_ = invH_dIdp.view(1, 6, feat_dim).expand(batch_size, 6, feat_dim)\n        mask_ = mask.view(batch_size, 1, pt_num).expand(batch_size, k, pt_num)\n        # huber_weights = ((255*.2) / (It.abs()+1e-5)).clamp(max=1)\n        # huber_weights = Variable(huber_weights.data)\n        # dp = invH_dIdp_.bmm((mask_* huber_weights * It).view(batch_size, feat_dim, 1))\n        dp = invH_dIdp_.bmm((mask_ * It).view(batch_size, feat_dim, 1))\n        return dp.view(batch_size, 6)\n\n    def warp_batch(self, img_batch, level_idx, R_batch, t_batch):\n        return self.warp_batch_func(img_batch, self.inv_depth_pyramid[level_idx], level_idx, R_batch, t_batch)\n\n\n    def warp_batch_func(self, img_batch, inv_depth, level_idx, R_batch, t_batch):\n        batch_size, k, h, w = img_batch.size()\n        xy = self.xy_pyramid[level_idx]\n        _, N = xy.size()\n        # xyz = R_batch.bmm(torch.cat((xy.view(1, 2, N).expand(batch_size, 2, N), Variable(self.load_to_device(torch.ones(batch_size, 1, N)))), 1)) \\\n        #     + t_batch.view(batch_size, 3, 1).expand(batch_size, 3, N) * inv_depth.view(1, 1, N).expand(batch_size, 3, N)\n        xyz = R_batch[:, :, 0:2].bmm(xy.view(1, 2, N).expand(batch_size, 2, N))\\\n            + R_batch[:, :, 2:3].expand(batch_size, 3, N)\\\n            + t_batch.view(batch_size, 3, 1).expand(batch_size, 3, N) * inv_depth.view(-1, 1, N).expand(batch_size, 3, N)\n        z = xyz[:, 2:3, :].clamp(min=1e-10)\n        xy_warp = xyz[:, 0:2, :] / z.expand(batch_size, 2, N)\n        # u_warp = ((xy_warp[:, 0, :]*self.camparams[\'fx\'] + self.camparams[\'cx\'])/2**level_idx - .5).view(batch_size, N)\n        # v_warp = ((xy_warp[:, 1, :]*self.camparams[\'fy\'] + self.camparams[\'cy\'])/2**level_idx - .5).view(batch_size, N)\n        # print(self.x_pyramid[level_idx][0])\n        u_warp = ((xy_warp[:, 0, :] * self.camparams[\'fx\'] + self.camparams[\'cx\']) - getattr(self, \'x_\'+str(level_idx))[0]).view(\n            batch_size, N) / 2 ** level_idx\n        v_warp = ((xy_warp[:, 1, :] * self.camparams[\'fy\'] + self.camparams[\'cy\']) - getattr(self, \'y_\'+str(level_idx))[0]).view(\n            batch_size, N) / 2 ** level_idx\n\n        Q, in_view_mask =  grid_bilinear_sampling(img_batch, u_warp, v_warp)\n        return Q, in_view_mask * (z.view_as(in_view_mask)>1e-10).float()\n\n\n    def compute_phtometric_loss(self, ref_frames_pyramid, src_frames_pyramid, ref_inv_depth_pyramid, src_inv_depth_pyramid,\n                                rot_mat_batch, trans_batch,\n                                use_ssim=True, levels=None,\n                                ref_expl_mask_pyramid=None,\n                                src_expl_mask_pyramid=None):\n        bundle_size = rot_mat_batch.size(0)+1\n        inv_rot_mat_batch, inv_trans_batch = inv_rigid_transformation(rot_mat_batch, trans_batch)\n        src_pyramid = []\n        ref_pyramid = []\n        depth_pyramid = []\n        if levels is None:\n            levels = range(self.pyramid_layer_num)\n\n        use_expl_mask = not (ref_expl_mask_pyramid is None \\\n                            or src_expl_mask_pyramid is None)\n        if use_expl_mask:\n            expl_mask_pyramid = []\n            for level_idx in levels:\n                ref_mask = ref_expl_mask_pyramid[level_idx].unsqueeze(0).repeat(bundle_size-1,1,1)\n                src_mask = src_expl_mask_pyramid[level_idx]\n                expl_mask_pyramid.append(torch.cat(\n                    (ref_mask, src_mask), 0))\n\n\n        # for level_idx in range(len(ref_frames_pyramid)):\n        for level_idx in levels:\n        # for level_idx in range(3):\n            ref_frame = ref_frames_pyramid[level_idx].unsqueeze(0).repeat(bundle_size-1, 1, 1, 1)\n            src_frame = src_frames_pyramid[level_idx]\n            ref_depth = ref_inv_depth_pyramid[level_idx].unsqueeze(0).repeat(bundle_size-1, 1, 1)\n            src_depth = src_inv_depth_pyramid[level_idx]\n            # print(src_depth.size())\n            ref_pyramid.append(torch.cat((ref_frame,\n                                    src_frame), 0)/127.5)\n            src_pyramid.append(torch.cat((src_frame,\n                                    ref_frame), 0)/127.5)\n            depth_pyramid.append(torch.cat((ref_depth,\n                                    src_depth), 0))\n\n\n        rot_mat = torch.cat((rot_mat_batch,\n                            inv_rot_mat_batch) ,0)\n        trans = torch.cat((trans_batch,\n                           inv_trans_batch), 0)\n\n        loss = 0\n\n        frames_warp_pyramid = []\n        ref_frame_warp_pyramid = []\n\n        # for level_idx in range(len(ref_pyramid)):\n        # for level_idx in range(3):\n        for level_idx in levels:\n            # print(depth_pyramid[level_idx].size())\n            _, h, w = depth_pyramid[level_idx].size()\n            warp_img, in_view_mask = self.warp_batch_func(\n                    src_pyramid[level_idx],\n                    depth_pyramid[level_idx],\n                    level_idx, rot_mat, trans)\n            warp_img = warp_img.view((bundle_size-1)*2, IMG_CHAN, h, w)\n            if use_expl_mask:\n                mask = in_view_mask.view(-1,h,w)*expl_mask_pyramid[level_idx]\n            else:\n                mask = in_view_mask\n            mask_expand = mask.view((bundle_size-1)*2, 1, h, w).expand((bundle_size-1)*2, IMG_CHAN, h, w)\n            rgb_loss = ((ref_pyramid[level_idx] - warp_img).abs()*mask_expand).mean()\n            if use_ssim and level_idx<1:\n                # print(""compute ssim loss------"")\n                warp_mu, warp_sigma = compute_img_stats(warp_img)\n                ref_mu, ref_sigma = compute_img_stats(ref_pyramid[level_idx])\n                ssim = compute_SSIM(ref_pyramid[level_idx],\n                                ref_mu,\n                                ref_sigma,\n                                warp_img,\n                                warp_mu,\n                                warp_sigma)\n                ssim_loss = (ssim*mask_expand[:,:,1:-1,1:-1]).mean()\n                loss += .85*ssim_loss+.15*rgb_loss\n            else:\n                loss += rgb_loss\n\n        #     frames_warp_pyramid.append(warp_img*127.5)\n        #     ref_frame_warp_pyramid.append(ref_pyramid[level_idx]*127.5)\n        #\n        # return loss, frames_warp_pyramid, ref_frame_warp_pyramid\n        return loss\n\n    def compute_smoothness_cost(self, inv_depth):\n        x = self.laplacian_func(inv_depth)\n        return x.mean()\n\n    def compute_image_aware_laplacian_smoothness_cost(self, depth, img):\n        img_lap = self.laplacian_func(img/255, do_normalize=False)\n        depth_lap = self.laplacian_func(depth, do_normalize=False)\n        x = (-img_lap.mean(1)).exp()*(depth_lap)\n        return x.mean()\n\n    def compute_image_aware_2nd_smoothness_cost(self, depth, img):\n        img_lap = self.laplacian_func(img/255, do_normalize=False)\n        depth_grad_x, depth_grad_y = gradient(depth, do_normalize=False)\n        depth_grad_x2, depth_grad_xy = gradient(depth_grad_x, do_normalize=False)\n        depth_grad_yx, depth_grad_y2 = gradient(depth_grad_y, do_normalize=False)\n        return depth_grad_x2.abs().mean() \\\n            + depth_grad_xy.abs().mean() + depth_grad_yx.abs().mean() + depth_grad_y2.abs().mean()\n\n\n    def compute_image_aware_1st_smoothness_cost(self, depth, img):\n        depth_grad_x, depth_grad_y = gradient(depth, do_normalize=False)\n        img_grad_x, img_grad_y = gradient(img/255, do_normalize=False)\n        if img.dim() == 3:\n            weight_x = torch.exp(-img_grad_x.abs().mean(0))\n            weight_y = torch.exp(-img_grad_y.abs().mean(0))\n            cost = ((depth_grad_x.abs() * weight_x)[:-1, :] + (depth_grad_y.abs() * weight_y)[:, :-1]).mean()\n        else:\n            weight_x = torch.exp(-img_grad_x.abs().mean(1))\n            weight_y = torch.exp(-img_grad_y.abs().mean(1))\n            cost = ((depth_grad_x.abs() * weight_x)[:, :-1, :] + (depth_grad_y.abs() * weight_y)[:, :, :-1]).mean()\n        return cost\n\n\n\n\n    def multi_scale_smoothness_cost(self, inv_depth_pyramid, levels = None):\n        cost = 0\n        if levels is None:\n            levels = range(self.pyramid_layer_num)\n\n        # for level_idx in range(2, self.pyramid_layer_num):\n        for level_idx in levels:\n        # for level_idx in range(3,4):\n            inv_depth = inv_depth_pyramid[level_idx]\n            if inv_depth.dim() == 4:\n                inv_depth = inv_depth.squeeze(1)\n            # cost_this_level = compute_img_aware_smoothness_cost(inv_depth, self.ref_frame_pyramid[level_idx]/255)/(2**level_idx)\n            cost += self.compute_smoothness_cost(inv_depth)/(2**level_idx)\n        return cost\n\n    def multi_scale_image_aware_smoothness_cost(self, inv_depth_pyramid, img_pyramid, levels=None, type=\'lap\'):\n        # for level_idx in range(self.pyramid_layer_num):\n        cost = 0\n        if levels is None:\n            levels = range(self.pyramid_layer_num)\n        for level_idx in levels:\n            # print(level_idx)\n            inv_depth = inv_depth_pyramid[level_idx]\n            if inv_depth.dim() == 4:\n                inv_depth = inv_depth.squeeze(1)\n            # cost += compute_img_aware_smoothness_cost(inv_depth, img_pyramid[level_idx])/(2**level_idx)\n            if type == \'lap\':\n                c = self.compute_image_aware_laplacian_smoothness_cost(inv_depth, img_pyramid[level_idx])\n            elif type == \'1st\':\n                c = self.compute_image_aware_1st_smoothness_cost(inv_depth, img_pyramid[level_idx])\n            elif type == \'2nd\':\n                c = self.compute_image_aware_2nd_smoothness_cost(inv_depth, img_pyramid[level_idx])\n            else:\n                print(""%s not implemented!"" %(type))\n            cost += (c / (2**level_idx))\n\n        return cost\n\n\n    def update(self, frames_pyramid, max_itr_num=10):\n        frame_num, k, h, w = frames_pyramid[0].size()\n        trans_batch = Variable(self.o).expand(frame_num, 3).contiguous()\n        trans_batch_prev = Variable(self.o).expand(frame_num, 3).contiguous()\n\n        rot_mat_batch = Variable(self.E).unsqueeze(0).expand(frame_num, 3, 3).contiguous()\n        rot_mat_batch_prev = Variable(self.E).unsqueeze(0).expand(frame_num, 3, 3).contiguous()\n\n        pixel_warp = []\n        in_view_mask = []\n\n        cur_time = timer()\n\n        for level_idx in range(self.pyramid_layer_num-1, -1, -1):\n\n            max_photometric_cost = self.o.squeeze().expand(frame_num)+10000\n            # print(level_idx)\n            for i in range(max_itr_num):\n                # print(i)\n                # cur_time = timer()\n                pixel_warp, in_view_mask = self.warp_batch(frames_pyramid[level_idx], level_idx, rot_mat_batch, trans_batch)\n                # t_warp += timer()-cur_time\n\n                temporal_grad = pixel_warp - self.ref_frame_pyramid[level_idx].view(3, -1).unsqueeze(0).expand_as(pixel_warp)\n\n                photometric_cost, min_perc_in_view = compute_photometric_cost_norm(temporal_grad.data, in_view_mask.data)\n                # print(photometric_cost)\n                # print(max_photometric_cost)\n                # print(min_perc_in_view)\n                # print((photometric_cost < max_photometric_cost).max())\n                if min_perc_in_view < .5:\n                    break\n\n                if (photometric_cost < max_photometric_cost).max()>0:\n                    # print(photometric_cost)\n                    trans_batch_prev = trans_batch\n\n                    rot_mat_batch_prev = rot_mat_batch\n\n                    dp = self.LKregress(invH_dIdp=self.invH_dIdp_pyramid[level_idx],\n                                        mask=in_view_mask,\n                                        It=temporal_grad)\n\n                    d_rot_mat_batch = self.twist2mat_batch_func(-dp[:, 0:3])\n                    trans_batch_new = d_rot_mat_batch.bmm(trans_batch.view(frame_num, 3, 1)).view(frame_num, 3) - dp[:, 3:6]\n                    rot_mat_batch_new = d_rot_mat_batch.bmm(rot_mat_batch)\n\n                    trans_list = []\n                    rot_list = []\n                    for k in range(frame_num):\n                        if photometric_cost[k] < max_photometric_cost[k]:\n                            rot_list.append(rot_mat_batch_new[k:k+1, :, :])\n                            trans_list.append(trans_batch_new[k:k+1, :])\n                            max_photometric_cost[k] = photometric_cost[k]\n                        else:\n                            rot_list.append(rot_mat_batch[k:k+1, :, :])\n                            trans_list.append(trans_batch[k:k+1, :])\n                    rot_mat_batch = torch.cat(rot_list, 0)\n                    trans_batch = torch.cat(trans_list, 0)\n                    # if photometric_cost[k] < max_photometric_cost[k]:\n                    #     trans_batch = d_rot_mat_batch.bmm(trans_batch.view(frame_num, 3, 1)).view(frame_num, 3) - dp[:, 3:6]\n                    #     rot_mat_batch = d_rot_mat_batch.bmm(rot_mat_batch)\n                else:\n                    break\n            rot_mat_batch = rot_mat_batch_prev\n            trans_batch = trans_batch_prev\n\n        return rot_mat_batch, trans_batch, frames_pyramid\n\n    def update_with_init_pose(self, frames_pyramid, rot_mat_batch, trans_batch, max_itr_num=10):\n        frame_num, k, h, w = frames_pyramid[0].size()\n        trans_batch_prev = trans_batch\n        rot_mat_batch_prev = rot_mat_batch\n\n        pixel_warp = []\n        in_view_mask = []\n\n        cur_time = timer()\n\n        for level_idx in range(len(frames_pyramid)-1, -1, -1):\n\n            max_photometric_cost = self.o.squeeze().expand(frame_num)+10000\n            # print(level_idx)\n            for i in range(max_itr_num):\n                # print(i)\n                # cur_time = timer()\n                pixel_warp, in_view_mask = self.warp_batch(frames_pyramid[level_idx], level_idx, rot_mat_batch, trans_batch)\n                # t_warp += timer()-cur_time\n\n                temporal_grad = pixel_warp - self.ref_frame_pyramid[level_idx].view(3, -1).unsqueeze(0).expand_as(pixel_warp)\n\n                photometric_cost, min_perc_in_view = compute_photometric_cost_norm(temporal_grad.data, in_view_mask.data)\n\n                if min_perc_in_view < .5:\n                    break\n\n                if (photometric_cost < max_photometric_cost).max()>0:\n\n                    trans_batch_prev = trans_batch\n                    rot_mat_batch_prev = rot_mat_batch\n\n                    dp = self.LKregress(invH_dIdp=self.invH_dIdp_pyramid[level_idx],\n                                        mask=in_view_mask,\n                                        It=temporal_grad)\n\n                    # d_rot_mat_batch = self.twist2mat_batch_func(-dp[:, 0:3])\n                    d_rot_mat_batch = self.twist2mat_batch_func(dp[:, 0:3]).transpose(1,2)\n                    trans_batch_new = d_rot_mat_batch.bmm(trans_batch.view(frame_num, 3, 1)).view(frame_num, 3) - dp[:, 3:6]\n                    rot_mat_batch_new = d_rot_mat_batch.bmm(rot_mat_batch)\n\n                    trans_list = []\n                    rot_list = []\n                    # print(photometric_cost)\n                    for k in range(frame_num):\n                        if photometric_cost[k] < max_photometric_cost[k]:\n                            rot_list.append(rot_mat_batch_new[k:k+1, :, :])\n                            trans_list.append(trans_batch_new[k:k+1, :])\n                            max_photometric_cost[k] = photometric_cost[k]\n                        else:\n                            rot_list.append(rot_mat_batch[k:k+1, :, :])\n                            trans_list.append(trans_batch[k:k+1, :])\n                    rot_mat_batch = torch.cat(rot_list, 0)\n                    trans_batch = torch.cat(trans_list, 0)\n                else:\n                    break\n            rot_mat_batch = rot_mat_batch_prev\n            trans_batch = trans_batch_prev\n\n        return rot_mat_batch, trans_batch\n\n\n    def forward(self, ref_frame_pyramid, src_frame_pyramid, ref_inv_depth_pyramid, max_itr_num=10):\n            self.init(ref_frame_pyramid=ref_frame_pyramid, inv_depth_pyramid=ref_inv_depth_pyramid)\n            rot_mat_batch, trans_batch, src_frames_pyramid = self.update(src_frame_pyramid, max_itr_num=max_itr_num)\n            return rot_mat_batch, trans_batch\n'"
src/ImagePyramid.py,14,"b'# from torchvision.transforms import Scale # it appears torchvision has some bugs, work on that latter\nimport torch.nn as nn\nfrom torch.nn import AvgPool2d\nfrom torch.nn.functional import conv2d\nfrom torch.autograd import Variable\nfrom timeit import default_timer as timer\nimport torch\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport scipy.io as sio\nimport os\n\n\n\nclass ImageSmoothLayer(nn.Module):\n    def __init__(self, chan):\n        super(ImageSmoothLayer, self).__init__()\n        F = torch.FloatTensor([[0.0751,   0.1238,    0.0751],\n                              [0.1238,   0.2042,    0.1238],\n                              [0.0751,   0.1238,    0.0751]]).view(1, 1, 3, 3)\n        self.register_buffer(\'smooth_kernel\', F)\n        if chan>1:\n            f = F\n            F = torch.zeros(chan, chan, 3, 3)\n            for i in range(chan):\n                F[i, i, :, :] = f\n        self.register_buffer(\'smooth_kernel_K\', F)\n        self.reflection_pad_func = torch.nn.ReflectionPad2d(1)\n\n    def forward(self, input):\n        output_dim = input.dim()\n        output_size = input.size()\n        if output_dim==2:\n            F = self.smooth_kernel\n            input = input.unsqueeze(0).unsqueeze(0)\n        elif output_dim==3:\n            F = self.smooth_kernel\n            input = input.unsqueeze(1)\n        else:\n            F = self.smooth_kernel_K\n\n        x = self.reflection_pad_func(input)\n\n        x = conv2d(input=x,\n                    weight=Variable(F),\n                    stride=1,\n                    padding=0)\n\n        if output_dim==2:\n            x =  x.squeeze(0).squeeze(0)\n        elif output_dim==3:\n            x =  x.squeeze(1)\n\n        return x\n\n\nclass ImagePyramidLayer(nn.Module):\n    def __init__(self, chan, pyramid_layer_num):\n        super(ImagePyramidLayer, self).__init__()\n        self.pyramid_layer_num = pyramid_layer_num\n        F = torch.FloatTensor([[0.0751,   0.1238,    0.0751],\n                              [0.1238,   0.2042,    0.1238],\n                              [0.0751,   0.1238,    0.0751]]).view(1, 1, 3, 3)\n        self.register_buffer(\'smooth_kernel\', F)\n        if chan>1:\n            f = F\n            F = torch.zeros(chan, chan, 3, 3)\n            for i in range(chan):\n                F[i, i, :, :] = f\n        self.register_buffer(\'smooth_kernel_K\', F)\n        self.avg_pool_func = torch.nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n        self.reflection_pad_func = torch.nn.ReflectionPad2d(1)\n\n\n    def downsample(self, input):\n        output_dim = input.dim()\n        output_size = input.size()\n        if output_dim==2:\n            F = self.smooth_kernel\n            input = input.unsqueeze(0).unsqueeze(0)\n        elif output_dim==3:\n            F = self.smooth_kernel\n            input = input.unsqueeze(1)\n        else:\n            F = self.smooth_kernel_K\n\n        x = self.reflection_pad_func(input)\n\n        x = conv2d(input=x,\n                    weight=Variable(F),\n                    stride=1,\n                    padding=0)\n        # remove here if not work out\n        padding = [0, int(np.mod(input.size(-1), 2)), 0, int(np.mod(input.size(-2), 2))]\n        x = torch.nn.ReplicationPad2d(padding)(x)\n        # -----\n        x = self.avg_pool_func(x)\n\n        if output_dim==2:\n            x =  x.squeeze(0).squeeze(0)\n        elif output_dim==3:\n            x =  x.squeeze(1)\n\n        return x\n\n\n    def forward(self, input, do_detach=True):\n        pyramid = [input]\n        for i in range(self.pyramid_layer_num-1):\n            img_d = self.downsample(pyramid[i])\n            if isinstance(img_d, Variable) and do_detach:\n                img_d = img_d.detach()\n            pyramid.append(img_d)\n            assert(np.ceil(pyramid[i].size(-1)/2) == img_d.size(-1))\n        return pyramid\n\n\n    def get_coords(self, imH, imW):\n        x_pyramid = [np.arange(imW)+.5]\n        y_pyramid = [np.arange(imH)+.5]\n        for i in range(self.pyramid_layer_num-1):\n            offset = 2**i\n            stride = 2**(i+1)\n            x_pyramid.append(np.arange(offset, offset + stride*np.ceil(x_pyramid[i].shape[0]/2), stride))\n            y_pyramid.append(np.arange(offset, offset + stride*np.ceil(y_pyramid[i].shape[0]/2), stride))\n\n        return x_pyramid, y_pyramid\n\n\n\nif __name__ == ""__main__"":\n    imH = 128\n    imW = 416\n    img = Variable(torch.randn(3, 128, 416))\n    n = ImagePyramidLayer(3, 7)\n    x_pyramid, y_pyramid = n.get_coords(imH, imW)\n    t = timer()\n    # pyramid, x_pyramid, y_pyramid = buildImagePyramid(Variable(torch.rand(1, 3, 256, 320)), 6)\n    pyramid = n.forward(img)\n    print(timer()-t)\n\n    for i in range(n.pyramid_layer_num):\n        print(x_pyramid[i].shape[0])\n        print(y_pyramid[i].shape[0])\n        print(pyramid[i].size())\n    # print(pyramid)\n'"
src/KITTIdataset.py,1,"b'from torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport scipy.io as sio\nfrom PIL import Image\nimport os\n\n\nclass KITTIdataset(Dataset):\n    """"""KITTIdataset""""""\n    def __init__(self, list_file=\'train.txt\', data_root_path=\'/home/chaoyang/LKVOLearner/data_kitti\', img_size=[128, 416], bundle_size=3):\n        self.data_root_path = data_root_path\n        self.img_size = img_size\n        self.bundle_size = bundle_size\n        self.frame_pathes = []\n        list_file = os.path.join(data_root_path, list_file)\n        with open(list_file) as file:\n            for line in file:\n                frame_path = line.strip()\n                seq_path, frame_name = frame_path.split("" "")\n                # print(seq_path)\n                if seq_path in [\'2011_09_26_drive_0119_sync_02\', \'2011_09_28_drive_0225_sync_02\',\n                                \'2011_09_29_drive_0108_sync_02\', \'2011_09_30_drive_0072_sync_02\',\n                                \'2011_10_03_drive_0058_sync_02\', \'2011_09_29_drive_0108_sync_03\']:\n                    print(seq_path)\n                    continue\n                frame_path = os.path.join(seq_path, frame_name)\n                self.frame_pathes.append(frame_path)\n                # print(frame_path)\n        # self.frame_pathes = self.frame_pathes[0:40000:800]\n\n    def __len__(self):\n        return len(self.frame_pathes)\n\n    def __getitem__(self, item):\n        # read camera intrinsics\n        cam_file = os.path.join(self.data_root_path, self.frame_pathes[item]+\'_cam.txt\')\n        with open(cam_file) as file:\n            cam_intrinsics = [float(x) for x in next(file).split(\',\')]\n        # camparams = dict(fx=cam_intrinsics[0], cx=cam_intrinsics[2],\n        #             fy=cam_intrinsics[4], cy=cam_intrinsics[5])\n        camparams = np.asarray(cam_intrinsics)\n\n        # read image bundle\n        img_file = os.path.join(self.data_root_path, self.frame_pathes[item]+\'.jpg\')\n        frames_cat = np.array(Image.open(img_file))\n        # slice the image into #bundle_size number of images\n        frame_list = []\n        for i in range(self.bundle_size):\n            frame_list.append(frames_cat[:,i*self.img_size[1]:(i+1)*self.img_size[1],:])\n        frames = np.asarray(frame_list).astype(float).transpose(0, 3, 1, 2)\n\n        return frames, camparams\n\nif __name__ == ""__main__"":\n    dataset = KITTIdataset()\n    dataset.__getitem__(0)\n    for i, data in enumerate(dataset):\n        print(data[1])\n'"
src/LKVOLearner.py,9,"b'from DirectVOLayer import DirectVO\nfrom networks import VggDepthEstimator\nfrom ImagePyramid import ImagePyramidLayer\nimport torch.nn as nn\nimport torch\nimport numpy as np\nfrom torch.autograd import Variable\n\nfrom timeit import default_timer as timer\n\nclass FlipLR(nn.Module):\n    def __init__(self, imW, dim_w):\n        super(FlipLR, self).__init__()\n        inv_indices = torch.arange(imW-1, -1, -1).long()\n        self.register_buffer(\'inv_indices\', inv_indices)\n        self.dim_w = dim_w\n\n\n    def forward(self, input):\n        return input.index_select(self.dim_w, Variable(self.inv_indices))\n\n\n\nclass LKVOLearner(nn.Module):\n    def __init__(self, img_size=[128, 416], ref_frame_idx=1, lambda_S=.5, use_ssim=True, smooth_term = \'lap\', gpu_ids=[0]):\n        super(LKVOLearner, self).__init__()\n        self.lkvo = nn.DataParallel(LKVOKernel(img_size, smooth_term = smooth_term), device_ids=gpu_ids)\n        self.ref_frame_idx = ref_frame_idx\n        self.lambda_S = lambda_S\n        self.use_ssim = use_ssim\n\n    def forward(self, frames, camparams, max_lk_iter_num=10):\n        cost, photometric_cost, smoothness_cost, ref_frame, ref_inv_depth \\\n            = self.lkvo.forward(frames, camparams, self.ref_frame_idx, self.lambda_S, max_lk_iter_num=max_lk_iter_num, use_ssim=self.use_ssim)\n        return cost.mean(), photometric_cost.mean(), smoothness_cost.mean(), ref_frame, ref_inv_depth\n\n    def save_model(self, file_path):\n        torch.save(self.cpu().lkvo.module.depth_net.state_dict(),\n            file_path)\n        self.cuda()\n\n    def load_model(self, file_path):\n        self.lkvo.module.depth_net.load_state_dict(torch.load(file_path))\n\n    def init_weights(self):\n        self.lkvo.module.depth_net.init_weights()\n\n    def get_parameters(self):\n        return self.lkvo.module.depth_net.parameters()\n\n\n\nclass LKVOKernel(nn.Module):\n    """"""\n     only support single training isinstance\n    """"""\n    def __init__(self, img_size=[128, 416], smooth_term = \'lap\'):\n        super(LKVOKernel, self).__init__()\n        self.img_size = img_size\n        self.fliplr_func = FlipLR(imW=img_size[1], dim_w=3)\n        self.vo = DirectVO(imH=img_size[0], imW=img_size[1], pyramid_layer_num=5)\n        self.depth_net = VggDepthEstimator(img_size)\n        self.pyramid_func = ImagePyramidLayer(chan=1, pyramid_layer_num=5)\n        self.smooth_term = smooth_term\n\n\n    def forward(self, frames, camparams, ref_frame_idx, lambda_S=.5, do_data_augment=True, use_ssim=True, max_lk_iter_num=10):\n        assert(frames.size(0) == 1 and frames.dim() == 5)\n        frames = frames.squeeze(0)\n        camparams = camparams.squeeze(0).data\n\n\n        if do_data_augment:\n            if np.random.rand()>.5:\n                # print(""fliplr"")\n                frames = self.fliplr_func(frames)\n                camparams[2] = self.img_size[1] - camparams[2]\n                # camparams[5] = self.img_size[0] - camparams[5]\n\n        bundle_size = frames.size(0)\n        src_frame_idx = tuple(range(0,ref_frame_idx)) + tuple(range(ref_frame_idx+1,bundle_size))\n        # ref_frame = frames[ref_frame_idx, :, :, :]\n        # src_frames = frames[src_frame_idx, :, :, :]\n        frames_pyramid = self.vo.pyramid_func(frames)\n        ref_frame_pyramid = [frame[ref_frame_idx, :, :, :] for frame in frames_pyramid]\n        src_frames_pyramid = [frame[src_frame_idx, :, :, :] for frame in frames_pyramid]\n\n\n        self.vo.setCamera(fx=camparams[0], cx=camparams[2],\n                            fy=camparams[4], cy=camparams[5])\n\n        inv_depth_pyramid = self.depth_net.forward((frames-127)/127)\n        inv_depth_mean_ten = inv_depth_pyramid[0].mean()*0.1\n        #\n        # inv_depth0_pyramid = self.pyramid_func(inv_depth_pyramid[0], do_detach=False)\n        # ref_inv_depth_pyramid = [depth[ref_frame_idx, :, :] for depth in inv_depth_pyramid]\n        # ref_inv_depth0_pyramid = [depth[ref_frame_idx, :, :] for depth in inv_depth0_pyramid]\n        # src_inv_depth_pyramid = [depth[src_frame_idx, :, :] for depth in inv_depth_pyramid]\n        # src_inv_depth0_pyramid = [depth[src_frame_idx, :, :] for depth in inv_depth0_pyramid]\n\n        inv_depth_norm_pyramid = [depth/inv_depth_mean_ten for depth in inv_depth_pyramid]\n        inv_depth0_pyramid = self.pyramid_func(inv_depth_norm_pyramid[0], do_detach=False)\n        ref_inv_depth_pyramid = [depth[ref_frame_idx, :, :] for depth in inv_depth_norm_pyramid]\n        ref_inv_depth0_pyramid = [depth[ref_frame_idx, :, :] for depth in inv_depth0_pyramid]\n        src_inv_depth_pyramid = [depth[src_frame_idx, :, :] for depth in inv_depth_norm_pyramid]\n        src_inv_depth0_pyramid = [depth[src_frame_idx, :, :] for depth in inv_depth0_pyramid]\n\n        rot_mat_batch, trans_batch = \\\n            self.vo.forward(ref_frame_pyramid, src_frames_pyramid, ref_inv_depth0_pyramid, max_itr_num=max_lk_iter_num)\n        #\n        # smoothness_cost = self.vo.multi_scale_smoothness_cost(inv_depth_pyramid)\n        # smoothness_cost += self.vo.multi_scale_smoothness_cost(inv_depth0_pyramid)\n\n        # smoothness_cost = self.vo.multi_scale_smoothness_cost(inv_depth_pyramid, levels=range(1,5))\n        # smoothness_cost = self.vo.multi_scale_smoothness_cost(inv_depth0_pyramid, levels=range(1,5))\n        photometric_cost = self.vo.compute_phtometric_loss(self.vo.ref_frame_pyramid, src_frames_pyramid, ref_inv_depth_pyramid, src_inv_depth_pyramid, rot_mat_batch, trans_batch, levels=[0,1,2,3], use_ssim=use_ssim)\n        smoothness_cost = self.vo.multi_scale_image_aware_smoothness_cost(inv_depth0_pyramid, frames_pyramid, levels=[2,3], type=self.smooth_term) \\\n                            + self.vo.multi_scale_image_aware_smoothness_cost(inv_depth_norm_pyramid, frames_pyramid, levels=[2,3], type=self.smooth_term)\n\n        # photometric_cost0, reproj_cost0, _, _ = self.vo.compute_phtometric_loss(self.vo.ref_frame_pyramid, src_frames_pyramid, ref_inv_depth0_pyramid, src_inv_depth0_pyramid, rot_mat_batch, trans_batch)\n\n\n        # cost = photometric_cost + photometric_cost0 + reproj_cost + reproj_cost0 + lambda_S*smoothness_cost\n        cost = photometric_cost + lambda_S*smoothness_cost\n        return cost, photometric_cost, smoothness_cost, self.vo.ref_frame_pyramid[0], ref_inv_depth0_pyramid[0]*inv_depth_mean_ten\n\n\nif __name__  == ""__main__"":\n    from KITTIdataset import KITTIdataset\n    from torch.utils.data import DataLoader\n    from torch import optim\n    from torch.autograd import Variable\n\n    dataset = KITTIdataset()\n    dataloader = DataLoader(dataset, batch_size=3,\n                            shuffle=True, num_workers=4, pin_memory=True)\n    lkvolearner = LKVOLearner(gpu_ids = [0, 1, 2])\n    def weights_init(m):\n        classname = m.__class__.__name__\n        if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.ConvTranspose2d):\n            # m.weight.data.normal_(0.0, 0.02)\n            m.bias.data = torch.zeros(m.bias.data.size())\n\n    lkvolearner.apply(weights_init)\n    lkvolearner.cuda()\n\n    optimizer = optim.Adam(lkvolearner.parameters(), lr=.0001)\n    for ii, data in enumerate(dataloader):\n        t = timer()\n        optimizer.zero_grad()\n        frames = Variable(data[0].float().cuda())\n        # print(data[1])\n        camparams = Variable(data[1])\n        a = lkvolearner.forward(frames, camparams)\n        print(timer()-t)\n        # print(a)\n'"
src/LKVOLearnerFinetune.py,10,"b'from DirectVOLayer import DirectVO\nfrom networks import VggDepthEstimator, PoseNet\nfrom ImagePyramid import ImagePyramidLayer\nimport torch.nn as nn\nimport torch\nimport numpy as np\nfrom torch.autograd import Variable\nfrom timeit import default_timer as timer\n\nclass FlipLR(nn.Module):\n    def __init__(self, imW, dim_w):\n        super(FlipLR, self).__init__()\n        inv_indices = torch.arange(imW-1, -1, -1).long()\n        self.register_buffer(\'inv_indices\', inv_indices)\n        self.dim_w = dim_w\n\n\n    def forward(self, input):\n        return input.index_select(self.dim_w, Variable(self.inv_indices))\n\n\n\nclass LKVOLearner(nn.Module):\n    def __init__(self, img_size=[128, 416], ref_frame_idx=1, lambda_S=.5, use_ssim=True, smooth_term = \'lap\', gpu_ids=[0]):\n        super(LKVOLearner, self).__init__()\n        self.lkvo = nn.DataParallel(LKVOKernel(img_size, smooth_term = smooth_term), device_ids=gpu_ids)\n        self.ref_frame_idx = ref_frame_idx\n        self.lambda_S = lambda_S\n        self.use_ssim = use_ssim\n\n    def forward(self, frames, camparams, max_lk_iter_num=10, lk_level=1):\n        cost, photometric_cost, smoothness_cost, ref_frame, ref_inv_depth \\\n            = self.lkvo.forward(frames, camparams, self.ref_frame_idx, self.lambda_S, max_lk_iter_num=max_lk_iter_num, use_ssim=self.use_ssim, lk_level=lk_level)\n        return cost.mean(), photometric_cost.mean(), smoothness_cost.mean(), ref_frame, ref_inv_depth\n\n    def save_model(self, file_path):\n        torch.save(self.cpu().lkvo.module.depth_net.state_dict(),\n            file_path)\n        self.cuda()\n\n    def load_model(self, depth_net_file_path, pose_net_file_path):\n        self.lkvo.module.depth_net.load_state_dict(torch.load(depth_net_file_path))\n        self.lkvo.module.pose_net.load_state_dict(torch.load(pose_net_file_path))\n\n    def init_weights(self):\n        self.lkvo.module.depth_net.init_weights()\n\n    def get_parameters(self):\n        return self.lkvo.module.depth_net.parameters()\n\n\n\nclass LKVOKernel(nn.Module):\n    """"""\n     only support single training isinstance\n    """"""\n    def __init__(self, img_size=[128, 416], smooth_term = \'lap\'):\n        super(LKVOKernel, self).__init__()\n        self.img_size = img_size\n        self.fliplr_func = FlipLR(imW=img_size[1], dim_w=3)\n        self.vo = DirectVO(imH=img_size[0], imW=img_size[1], pyramid_layer_num=4)\n        self.pose_net = PoseNet(3)\n        self.depth_net = VggDepthEstimator(img_size)\n        self.pyramid_func = ImagePyramidLayer(chan=1, pyramid_layer_num=4)\n        self.smooth_term = smooth_term\n\n\n    def forward(self, frames, camparams, ref_frame_idx, lambda_S=.5, do_data_augment=True, use_ssim=True, max_lk_iter_num=10, lk_level=1):\n        assert(frames.size(0) == 1 and frames.dim() == 5)\n        frames = frames.squeeze(0)\n        camparams = camparams.squeeze(0).data\n\n\n        if do_data_augment:\n            if np.random.rand()>.5:\n                # print(""fliplr"")\n                frames = self.fliplr_func(frames)\n                camparams[2] = self.img_size[1] - camparams[2]\n                # camparams[5] = self.img_size[0] - camparams[5]\n\n        bundle_size = frames.size(0)\n        src_frame_idx = tuple(range(0,ref_frame_idx)) + tuple(range(ref_frame_idx+1,bundle_size))\n        # ref_frame = frames[ref_frame_idx, :, :, :]\n        # src_frames = frames[src_frame_idx, :, :, :]\n        frames_pyramid = self.vo.pyramid_func(frames)\n        ref_frame_pyramid = [frame[ref_frame_idx, :, :, :] for frame in frames_pyramid]\n        src_frames_pyramid = [frame[src_frame_idx, :, :, :] for frame in frames_pyramid]\n\n\n        self.vo.setCamera(fx=camparams[0], cx=camparams[2],\n                            fy=camparams[4], cy=camparams[5])\n\n        inv_depth_pyramid = self.depth_net.forward((frames-127)/127)\n        inv_depth_mean_ten = inv_depth_pyramid[0].mean()*0.1\n\n        inv_depth_norm_pyramid = [depth/inv_depth_mean_ten for depth in inv_depth_pyramid]\n        inv_depth0_pyramid = self.pyramid_func(inv_depth_norm_pyramid[0], do_detach=False)\n        ref_inv_depth_pyramid = [depth[ref_frame_idx, :, :] for depth in inv_depth_norm_pyramid]\n        ref_inv_depth0_pyramid = [depth[ref_frame_idx, :, :] for depth in inv_depth0_pyramid]\n        src_inv_depth_pyramid = [depth[src_frame_idx, :, :] for depth in inv_depth_norm_pyramid]\n        src_inv_depth0_pyramid = [depth[src_frame_idx, :, :] for depth in inv_depth0_pyramid]\n\n        self.vo.init(ref_frame_pyramid=ref_frame_pyramid, inv_depth_pyramid=ref_inv_depth0_pyramid)\n        # init_pose with pose CNN\n        p = self.pose_net.forward((frames.view(1, -1, frames.size(2), frames.size(3))-127) / 127)\n        rot_mat_batch = self.vo.twist2mat_batch_func(p[0,:,0:3]).contiguous()\n        trans_batch = p[0,:,3:6].contiguous()#*inv_depth_mean_ten\n        # fine tune pose with direct VO\n        rot_mat_batch, trans_batch = self.vo.update_with_init_pose(src_frames_pyramid[0:lk_level], max_itr_num=max_lk_iter_num, rot_mat_batch=rot_mat_batch, trans_batch=trans_batch)\n        # rot_mat_batch, trans_batch = \\\n        #     self.vo.forward(ref_frame_pyramid, src_frames_pyramid, ref_inv_depth0_pyramid, max_itr_num=max_lk_iter_num)\n\n        photometric_cost = self.vo.compute_phtometric_loss(self.vo.ref_frame_pyramid, src_frames_pyramid, ref_inv_depth_pyramid, src_inv_depth_pyramid, rot_mat_batch, trans_batch, levels=[0,1,2,3], use_ssim=use_ssim)\n        smoothness_cost = self.vo.multi_scale_image_aware_smoothness_cost(inv_depth0_pyramid, frames_pyramid, levels=[2,3], type=self.smooth_term) \\\n                            + self.vo.multi_scale_image_aware_smoothness_cost(inv_depth_norm_pyramid, frames_pyramid, levels=[2,3], type=self.smooth_term)\n\n        cost = photometric_cost + lambda_S*smoothness_cost\n        return cost, photometric_cost, smoothness_cost, self.vo.ref_frame_pyramid[0], ref_inv_depth0_pyramid[0]*inv_depth_mean_ten\n\n\nif __name__  == ""__main__"":\n    from KITTIdataset import KITTIdataset\n    from torch.utils.data import DataLoader\n    from torch import optim\n    from torch.autograd import Variable\n\n    dataset = KITTIdataset()\n    dataloader = DataLoader(dataset, batch_size=3,\n                            shuffle=True, num_workers=2, pin_memory=True)\n    lkvolearner = LKVOLearner(gpu_ids = [0])\n    def weights_init(m):\n        classname = m.__class__.__name__\n        if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.ConvTranspose2d):\n            # m.weight.data.normal_(0.0, 0.02)\n            m.bias.data = torch.zeros(m.bias.data.size())\n\n    lkvolearner.apply(weights_init)\n    lkvolearner.cuda()\n\n    optimizer = optim.Adam(lkvolearner.parameters(), lr=.0001)\n    for ii, data in enumerate(dataloader):\n        t = timer()\n        optimizer.zero_grad()\n        frames = Variable(data[0].float().cuda())\n        # print(data[1])\n        camparams = Variable(data[1])\n        a = lkvolearner.forward(frames, camparams)\n        print(timer()-t)\n        # print(a)\n'"
src/MatInverse.py,6,"b'import torch\nfrom torch import FloatTensor\nfrom torch.autograd import Variable\nfrom timeit import default_timer as timer\nfrom torch.autograd import gradcheck\n\nclass Inverse(torch.autograd.Function):\n\n    def forward(self, input):\n        h, w = input.size()\n        assert(h == w)\n        H = input.inverse()\n        self.save_for_backward(H)\n        return H\n\n    def backward(self, grad_output):\n        # print(grad_output.is_contiguous())\n        H, = self.saved_tensors\n        h, w = H.size()\n        assert(h == w)\n        Hl = H.t().repeat(1, h).view(h*h, h, 1)\n        # print(Hl.view(batch_size, h, h, h, 1))\n        Hr = H.repeat(h, 1).view(h*h, 1, h)\n        # print(Hr.view(batch_size, h, h, 1, h))\n\n        r = Hl.bmm(Hr).view(h, h, h, h) * \\\n            grad_output.contiguous().view(1, 1, h, h).expand(h, h, h, h)\n        # print(r.size())\n        return -r.sum(-1).sum(-1)\n        # print(r)\n\ndef inv(input):\n    return Inverse()(input)\n\nclass InverseBatch(torch.autograd.Function):\n\n    def forward(self, input):\n        batch_size, h, w = input.size()\n        assert(h == w)\n        H = torch.Tensor(batch_size, h, h).type_as(input)\n        for i in range(0, batch_size):\n            H[i, :, :] = input[i, :, :].inverse()\n        self.save_for_backward(H)\n        return H\n\n    def backward(self, grad_output):\n        # print(grad_output.is_contiguous())\n        H, = self.saved_tensors\n        [batch_size, h, w] = H.size()\n        assert(h == w)\n        Hl = H.transpose(1,2).repeat(1, 1, h).view(batch_size*h*h, h, 1)\n        # print(Hl.view(batch_size, h, h, h, 1))\n        Hr = H.repeat(1, h, 1).view(batch_size*h*h, 1, h)\n        # print(Hr.view(batch_size, h, h, 1, h))\n\n        r = Hl.bmm(Hr).view(batch_size, h, h, h, h) * \\\n            grad_output.contiguous().view(batch_size, 1, 1, h, h).expand(batch_size, h, h, h, h)\n        # print(r.size())\n        return -r.sum(-1).sum(-1)\n        # print(r)\n\ndef inv_batch(input):\n    return InverseBatch()(input)\n\n\nif __name__ == ""__main__"":\n\n    W = torch.rand(2,2)\n    s = timer()\n    invH = inv(Variable(W, requires_grad=True))\n    print(timer() - s)\n    c = invH.mean()\n    print(c)\n    c.backward()\n    print(timer()-s)\n    test = gradcheck(inv, (Variable(W, requires_grad=True),), eps=1e-5, atol=1e-4)\n    print(test)\n'"
src/SfMLearner.py,7,"b'from DirectVOLayer import DirectVO\nfrom networks import VggDepthEstimator, PoseNet, PoseExpNet\nfrom ImagePyramid import ImagePyramidLayer\nimport torch.nn as nn\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\n\nimport itertools\n\nfrom timeit import default_timer as timer\n\nclass FlipLR(nn.Module):\n    def __init__(self, imW, dim_w):\n        super(FlipLR, self).__init__()\n        inv_indices = torch.arange(imW-1, -1, -1).long()\n        self.register_buffer(\'inv_indices\', inv_indices)\n        self.dim_w = dim_w\n\n\n    def forward(self, input):\n        return input.index_select(self.dim_w, Variable(self.inv_indices))\n\n\n\nclass SfMLearner(nn.Module):\n    def __init__(self, img_size=[128, 416], ref_frame_idx=1,\n        lambda_S=.5, lambda_E=0.01, use_ssim=True, smooth_term = \'lap\',\n        use_expl_mask=False, gpu_ids=[0]):\n        super(SfMLearner, self).__init__()\n        self.sfmkernel = nn.DataParallel(SfMKernel(img_size, smooth_term = smooth_term, use_expl_mask=use_expl_mask),\n                            device_ids=gpu_ids)\n        self.ref_frame_idx = ref_frame_idx\n        self.lambda_S = lambda_S\n        self.lambda_E = lambda_E\n        self.use_ssim = use_ssim\n        self.use_expl_mask = use_expl_mask\n\n    def forward(self, frames, camparams, max_lk_iter_num=10):\n        cost, photometric_cost, smoothness_cost, ref_frame, ref_inv_depth, ref_expl_mask \\\n            = self.sfmkernel.forward(frames, camparams, self.ref_frame_idx,\n            self.lambda_S, self.lambda_E, use_ssim=self.use_ssim)\n        return cost.mean(), photometric_cost.mean(), smoothness_cost.mean(), ref_frame, ref_inv_depth, ref_expl_mask\n\n    def save_model(self, file_path):\n        torch.save(self.cpu().sfmkernel.module.depth_net.state_dict(),\n            file_path+\'_depth_net.pth\')\n        torch.save(self.sfmkernel.module.pose_net.state_dict(),\n            file_path+\'_pose_net.pth\')\n        self.cuda()\n\n    def load_model(self, file_path):\n        self.sfmkernel.module.depth_net.load_state_dict(torch.load(file_path+\'_depth_net.pth\'))\n        self.sfmkernel.module.pose_net.load_state_dict(torch.load(file_path+\'_pose_net.pth\'))\n\n    def init_weights(self):\n        self.sfmkernel.module.depth_net.init_weights()\n\n    def get_parameters(self):\n        return itertools.chain(self.sfmkernel.module.depth_net.parameters(),\n                    self.sfmkernel.module.pose_net.parameters())\n\n\n\nclass SfMKernel(nn.Module):\n    """"""\n     only support single training isinstance\n    """"""\n    def __init__(self, img_size=[128, 416], smooth_term = \'lap\', use_expl_mask=False):\n        super(SfMKernel, self).__init__()\n        self.img_size = img_size\n        self.fliplr_func = FlipLR(imW=img_size[1], dim_w=3)\n        self.vo = DirectVO(imH=img_size[0], imW=img_size[1], pyramid_layer_num=4)\n        self.depth_net = VggDepthEstimator(img_size)\n        if use_expl_mask:\n            self.pose_net = PoseExpNet(3)\n        else:\n            self.pose_net = PoseNet(3)\n        self.pyramid_func = ImagePyramidLayer(chan=1, pyramid_layer_num=4)\n        self.smooth_term = smooth_term\n        self.use_expl_mask = use_expl_mask\n\n\n    def forward(self, frames, camparams, ref_frame_idx, lambda_S=.5, lambda_E=.01, do_data_augment=True, use_ssim=True):\n        assert(frames.size(0) == 1 and frames.dim() == 5)\n        frames = frames.squeeze(0)\n        camparams = camparams.squeeze(0).data\n\n\n        if do_data_augment:\n            if np.random.rand()>.5:\n                frames = self.fliplr_func(frames)\n                camparams[2] = self.img_size[1] - camparams[2]\n\n        bundle_size = frames.size(0)\n        src_frame_idx = tuple(range(0,ref_frame_idx)) + tuple(range(ref_frame_idx+1,bundle_size))\n        frames_pyramid = self.vo.pyramid_func(frames)\n        ref_frame_pyramid = [frame[ref_frame_idx, :, :, :] for frame in frames_pyramid]\n        src_frames_pyramid = [frame[src_frame_idx, :, :, :] for frame in frames_pyramid]\n\n\n        self.vo.setCamera(fx=camparams[0], cx=camparams[2],\n                            fy=camparams[4], cy=camparams[5])\n        self.vo.init_xy_pyramid(ref_frame_pyramid)\n        if self.use_expl_mask:\n            p, expl_mask_pyramid = self.pose_net.forward((frames.view(1, -1, frames.size(2), frames.size(3))-127) / 127)\n            expl_mask_reg_cost = 0\n            for mask in expl_mask_pyramid:\n                expl_mask_reg_cost += mask.mean()\n            ref_expl_mask_pyramid = [mask.squeeze(0)[ref_frame_idx, ...] for mask in expl_mask_pyramid]\n            src_expl_mask_pyramid = [mask.squeeze(0)[src_frame_idx, ...] for mask in expl_mask_pyramid]\n            expl_mask = ref_expl_mask_pyramid[0]\n\n        else:\n            p = self.pose_net.forward((frames.view(1, -1, frames.size(2), frames.size(3))-127) / 127)\n            ref_expl_mask_pyramid = None\n            src_expl_mask_pyramid = None\n            expl_mask_reg_cost = 0\n            expl_mask = None\n\n        rot_mat_batch = self.vo.twist2mat_batch_func(p[0,:,0:3])\n        trans_batch = p[0,:,3:6]\n\n        inv_depth_pyramid = self.depth_net.forward((frames-127)/127)\n        inv_depth_mean_ten = inv_depth_pyramid[0].mean()*0.1 #uncommment this to use normalization\n\n        # normalize\n        #trans_batch = trans_batch*inv_depth_mean_ten\n        inv_depth_norm_pyramid = [depth/inv_depth_mean_ten for depth in inv_depth_pyramid]\n\n        ref_inv_depth_pyramid = [depth[ref_frame_idx, :, :] for depth in inv_depth_norm_pyramid]\n        src_inv_depth_pyramid = [depth[src_frame_idx, :, :] for depth in inv_depth_norm_pyramid]\n\n        photometric_cost = self.vo.compute_phtometric_loss(\n                                                ref_frame_pyramid,\n                                                src_frames_pyramid,\n                                                ref_inv_depth_pyramid,\n                                                src_inv_depth_pyramid,\n                                                rot_mat_batch, trans_batch,\n                                                levels=[0,1,2,3], use_ssim=use_ssim,\n                                                ref_expl_mask_pyramid=ref_expl_mask_pyramid,\n                                                src_expl_mask_pyramid=src_expl_mask_pyramid)\n        # compute smoothness smoothness loss\n        # instead of directly compute the loss on the finest level, it\'s evaluated on the downsamples.\n        inv_depth0_pyramid = self.pyramid_func(inv_depth_norm_pyramid[0], do_detach=False)\n        smoothness_cost = self.vo.multi_scale_image_aware_smoothness_cost(inv_depth0_pyramid, frames_pyramid, levels=[2,3], type=self.smooth_term) \\\n                            + self.vo.multi_scale_image_aware_smoothness_cost(inv_depth_norm_pyramid, frames_pyramid, levels=[2,3], type=self.smooth_term)\n\n        cost = photometric_cost + lambda_S*smoothness_cost - lambda_E*expl_mask_reg_cost\n        \n        return cost, photometric_cost, smoothness_cost, ref_frame_pyramid[0], ref_inv_depth_pyramid[0]*inv_depth_mean_ten, expl_mask\n'"
src/networks.py,12,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport functools\nfrom torch.autograd import Variable\nimport numpy as np\n\n\nDISP_SCALING = 10\nMIN_DISP = 0.01\n\nclass ConvBlock(nn.Module):\n    def __init__(self, input_nc, output_nc, kernel_size):\n        super(ConvBlock, self).__init__()\n        p = int(np.floor((kernel_size - 1) / 2))\n        self.activation_fn = nn.ELU()\n        self.conv1 = Conv(input_nc, output_nc, kernel_size, 1, p, self.activation_fn)\n        # self.conv2 = Conv(output_nc, output_nc, kernel_size, 2, p)\n        self.conv2 = Conv(output_nc, output_nc, kernel_size, 1, p, None)\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.conv2(x)\n        padding = [0, int(np.mod(input.size(-1), 2)), 0, int(np.mod(input.size(-2), 2))]\n        x_pad = torch.nn.ReplicationPad2d(padding)(x)\n        return torch.nn.AvgPool2d(kernel_size=2, stride=2, padding=0)(self.activation_fn(x_pad))\n\n\nclass UpConv(nn.Module):\n    def __init__(self, input_nc, output_nc, kernel_size):\n        super(UpConv, self).__init__()\n        self.deconv = nn.ConvTranspose2d(in_channels=input_nc,\n                                                out_channels=output_nc,\n                                                 kernel_size=2,\n                                                 bias=True,\n                                                 stride=2,\n                                                 padding=0)\n        self.activation_fn = nn.ELU()\n    def forward(self, input):\n        return self.activation_fn(self.deconv(input))\n\nclass Conv(nn.Module):\n    def __init__(self, input_nc, output_nc, kernel_size, stride, padding, activation_func=nn.ELU()):\n        super(Conv, self).__init__()\n        self.conv = nn.Conv2d(in_channels=input_nc,\n                              out_channels=output_nc,\n                              kernel_size=kernel_size,\n                              stride=stride,\n                              padding=0,\n                              bias=True)\n        self.activation_fn = activation_func\n        self.pad_fn = nn.ReplicationPad2d(padding)\n\n    def forward(self, input):\n        if self.activation_fn == None:\n            return self.conv(self.pad_fn(input))\n        else:\n            return self.activation_fn(self.conv(self.pad_fn(input)))\n\n\nclass VggDepthEstimator(nn.Module):\n    def __init__(self, input_size=None):\n        super(VggDepthEstimator, self).__init__()\n        self.conv_layers = nn.ModuleList([ConvBlock(3, 32, 7)])\n        self.conv_layers.append(ConvBlock(32, 64, 5))\n        self.conv_layers.append(ConvBlock(64, 128, 3))\n\n        self.conv_layers.append(ConvBlock(128, 256, 3))\n\n        self.conv_layers.append(ConvBlock(256, 512, 3))\n\n\n        self.conv_layers.append(ConvBlock(512, 512, 3))\n\n\n        self.conv_layers.append(ConvBlock(512, 512, 3))\n\n\n        # print(conv_feat_sizes)\n\n        self.upconv_layers = nn.ModuleList([UpConv(512, 512, 3)])\n        self.iconv_layers = nn.ModuleList([Conv(512*2, 512, 3, 1, 1)])\n\n        self.upconv_layers.append(UpConv(512, 512, 3))\n        self.iconv_layers.append(Conv(512*2, 512, 3, 1, 1))\n        self.invdepth_layers = nn.ModuleList([Conv(512, 1, 3, 1, 1, nn.Sigmoid())])\n\n        self.upconv_layers.append(UpConv(512, 256, 3))\n        self.iconv_layers.append(Conv(256*2, 256, 3, 1, 1))\n        self.invdepth_layers.append(Conv(256, 1, 3, 1, 1, nn.Sigmoid()))\n\n        self.upconv_layers.append(UpConv(256, 128, 3))\n        self.iconv_layers.append(Conv(128*2, 128, 3, 1, 1))\n        self.invdepth_layers.append(Conv(128, 1, 3, 1, 1, nn.Sigmoid()))\n\n        self.upconv_layers.append(UpConv(128, 64, 3))\n        self.iconv_layers.append(Conv(64*2+1, 64, 3, 1, 1))\n        self.invdepth_layers.append(Conv(64, 1, 3, 1, 1, nn.Sigmoid()))\n\n        self.upconv_layers.append(UpConv(64, 32, 3))\n        self.iconv_layers.append(Conv(32*2+1, 32, 3, 1, 1))\n        self.invdepth_layers.append(Conv(32, 1, 3, 1, 1, nn.Sigmoid()))\n\n        self.upconv_layers.append(UpConv(32, 16, 3))\n        self.iconv_layers.append(Conv(16+1, 16, 3, 1, 1))\n        self.invdepth_layers.append(Conv(16, 1, 3, 1, 1, nn.Sigmoid()))\n        # self.invdepth_layers.append(InvDepth(16))\n\n    def init_weights(self):\n        def weights_init(m):\n            classname = m.__class__.__name__\n            if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.ConvTranspose2d):\n                # m.weight.data.normal_(0.0, 0.02)\n                m.bias.data = torch.zeros(m.bias.data.size())\n\n        self.apply(weights_init)\n\n    def forward(self, input):\n        conv_feat = self.conv_layers[0].forward(input)\n        self.conv_feats = [conv_feat]\n        for i in range(1, len(self.conv_layers)):\n            conv_feat = self.conv_layers[i].forward(self.conv_feats[i-1])\n            self.conv_feats.append(conv_feat)\n\n        upconv_feats = []\n        invdepth_pyramid = []\n        for i in range(0, len(self.upconv_layers)):\n            if i==0:\n                x = self.upconv_layers[i].forward(self.conv_feats[-1])\n            else:\n                x = self.upconv_layers[i].forward(upconv_feats[i-1])\n            if i<len(self.upconv_layers)-1:\n                if x.size(-1) != self.conv_feats[-2-i].size(-1):\n                    x = x[:, :, :, :-1]\n                if x.size(-2) != self.conv_feats[-2-i].size(-2):\n                    x = x[:, :, :-1, :]\n\n            if i==(len(self.upconv_layers)-1):\n                x = torch.cat((x, nn.Upsample(scale_factor=2, mode=\'bilinear\')(invdepth_pyramid[-1])), 1)\n            elif i > 3:\n                x = torch.cat((x, self.conv_feats[-(2+i)], nn.Upsample(scale_factor=2, mode=\'bilinear\')(invdepth_pyramid[-1])), 1)\n            else:\n                x = torch.cat((x, self.conv_feats[-(2+i)]), 1)\n            upconv_feats.append(self.iconv_layers[i].forward(x))\n            if i>0:\n                # invdepth_pyramid.append(self.invdepth_layers[i-1].forward(upconv_feats[-1])*DISP_SCALING+MIN_DISP)\n                invdepth_pyramid.append(self.invdepth_layers[i-1].forward(upconv_feats[-1]))\n                # invdepth_pyramid.append(self.invdepth_layers[i-1].forward(upconv_feats[-1]))\n        invdepth_pyramid = invdepth_pyramid[-1::-1]\n        invdepth_pyramid = invdepth_pyramid[0:5]\n        # conv_feats_output = conv_feats_output[0:5]\n        for i in range(len(invdepth_pyramid)):\n            invdepth_pyramid[i] = invdepth_pyramid[i].squeeze(1)*DISP_SCALING+MIN_DISP\n        return invdepth_pyramid\n        # return invdepth_pyramid, invdepth0_pyramid, normalize_convfeat_pyramid(conv_feats_output)\n\nclass PoseNet(nn.Module):\n    def __init__(self, bundle_size):\n        super(PoseNet, self).__init__()\n        self.bundle_size = bundle_size\n\n        model = [nn.Conv2d(bundle_size*3, 16, kernel_size=7, stride=2, padding=3, bias=True),\n                 nn.ReLU(True),\n                 nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2, bias=True),\n                 nn.ReLU(True),\n                 nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=True),\n                 nn.ReLU(True),\n                 nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=True),\n                 nn.ReLU(True),\n                 nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=True),\n                 nn.ReLU(True),\n                 nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, bias=True),\n                 nn.ReLU(True),\n                 nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, bias=True),\n                 nn.ReLU(True),\n                 nn.Conv2d(256, 6*(bundle_size-1), kernel_size=3, stride=2, padding=1, bias=True)\n                 ]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        assert(self.bundle_size*3 == input.size(1))\n        p = self.model.forward(input)\n        p = p.view(input.size(0), 6*(self.bundle_size-1), -1).mean(2)\n        return p.view(input.size(0), self.bundle_size-1, 6) * 0.01\n\n\nclass PoseExpNet(nn.Module):\n    def __init__(self, bundle_size):\n        super(PoseExpNet, self).__init__()\n        self.bundle_size = bundle_size\n        self.convlyr1 = nn.Sequential(*[nn.Conv2d(bundle_size*3, 16, kernel_size=7, stride=2, padding=3, bias=True),\n                 nn.ReLU(True)])\n        self.convlyr2 = nn.Sequential(*[nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2, bias=True),\n                 nn.ReLU(True)])\n        self.convlyr3 = nn.Sequential(*[nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=True),\n                 nn.ReLU(True)])\n        self.convlyr4 = nn.Sequential(*[nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=True),\n                 nn.ReLU(True)])\n        self.convlyr5 = nn.Sequential(*[nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=True),\n                 nn.ReLU(True)])\n\n        self.poselyr = nn.Sequential(*[nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, bias=True),\n                        nn.ReLU(True),\n                        nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1, bias=True),\n                        nn.ReLU(True),\n                        nn.Conv2d(256, 6*(bundle_size-1), kernel_size=3, stride=2, padding=1, bias=True)])\n\n        self.uplyr5 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=256,\n                                          out_channels=256,\n                                          kernel_size=2,\n                                          bias=True,\n                                          stride=2,\n                                          padding=0),\n                       nn.ReLU(True)])\n        self.uplyr4 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=256,\n                                          out_channels=128,\n                                          kernel_size=2,\n                                          bias=True,\n                                          stride=2,\n                                          padding=0),\n                       nn.ReLU(True)])\n        self.uplyr3 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=128,\n                                          out_channels=64,\n                                          kernel_size=2,\n                                          bias=True,\n                                          stride=2,\n                                          padding=0),\n                                    nn.ReLU(True)])\n        self.uplyr2 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=64,\n                                          out_channels=32,\n                                          kernel_size=2,\n                                          bias=True,\n                                          stride=2,\n                                          padding=0),\n                                          nn.ReLU(True)])\n        self.uplyr1 = nn.Sequential(*[nn.ConvTranspose2d(in_channels=32,\n                                          out_channels=16,\n                                          kernel_size=2,\n                                          bias=True,\n                                          stride=2,\n                                          padding=0),\n                                          nn.ReLU(True)])\n        self.explyr4 = nn.Sequential(*[nn.Conv2d(128, bundle_size, kernel_size=3,\n                                        stride=1, padding=1, bias=True),\n                       nn.Sigmoid()])\n        self.explyr3 = nn.Sequential(*[nn.Conv2d(64, bundle_size, kernel_size=3,\n                                        stride=1, padding=1, bias=True),\n                       nn.Sigmoid()])\n        self.explyr2 = nn.Sequential(*[nn.Conv2d(32, bundle_size, kernel_size=3,\n                                        stride=1, padding=1, bias=True),\n                       nn.Sigmoid()])\n        self.explyr1 = nn.Sequential(*[nn.Conv2d(16, bundle_size, kernel_size=3,\n                                        stride=1, padding=1, bias=True),\n                       nn.Sigmoid()])\n\n    def forward(self, input):\n        conv1 = self.convlyr1(input)\n        conv2 = self.convlyr2(conv1)\n        conv3 = self.convlyr3(conv2)\n        conv4 = self.convlyr4(conv3)\n        conv5 = self.convlyr5(conv4)\n\n        # output pose\n        p = self.poselyr.forward(conv5)\n        p = p.view(input.size(0), 6*(self.bundle_size-1), -1).mean(2)\n        # multiply predicted pose with a small constant\n        p = p.view(input.size(0), self.bundle_size-1, 6) * 0.01\n        # predict multi-scale explainable mask\n        upcnv5 = self.uplyr5(conv5)\n        upcnv4 = self.uplyr4(upcnv5)\n        upcnv3 = self.uplyr3(upcnv4)\n        upcnv2 = self.uplyr2(upcnv3)\n        upcnv1 = self.uplyr1(upcnv2)\n\n        mask4 = self.explyr4(upcnv4)\n        mask3 = self.explyr3(upcnv3)\n        mask2 = self.explyr2(upcnv2)\n        mask1 = self.explyr1(upcnv1)\n\n        return p, [mask1, mask2, mask3, mask4]\n\n\n\nif __name__ == ""__main__"":\n    model = PoseExpNet(3).cuda()\n    x = Variable(torch.randn(1,9,128,416).cuda())\n    p, masks = model.forward(x)\n    for i in range(4):\n        print(masks[i].size())\n\n\n    dnet = VggDepthEstimator([128,416]).cuda()\n    I = Variable(torch.randn(1,3,128,416).cuda())\n    invdepth_pyramid = dnet.forward(I)\n    for i in range(len(invdepth_pyramid)):\n        print(invdepth_pyramid[i].size())\n'"
src/testKITTI.py,4,"b'from networks import VggDepthEstimator\nfrom LKVOLearner import FlipLR\nimport torch\nfrom torch.autograd import Variable\nimport os\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport argparse\n\n""""""\nCUDA_VISIBLE_DEVICES=1 nice -10 python3 testKITTI.py --dataset_root /newfoundland/chaoyang/kitti --ckpt_file /home/chaoyang/LKVOLearner/checkpoints/checkpoints_19_416_scratch/9_model.pth --test_file_list\n""""""\nparser = argparse.ArgumentParser()\n\nparser.add_argument(""--dataset_root"", type=str, default=""/newfoundland/chaoyang/kitti"", help=""dataset root path"")\nparser.add_argument(""--test_file_list"", type=str, default=""/newfoundland/chaoyang/SfMLearner/data/kitti/test_files_eigen.txt"", help=""test file list"")\nparser.add_argument(""--ckpt_file"", type=str, default=None, help=""checkpoint file"")\nparser.add_argument(""--output_path"", type=str, default=""pred_depths"", help=""output path"")\nparser.add_argument(""--use_pp"", default=False, action=""store_true"", help=\'use post processing\')\n\nFLAGS = parser.parse_args()\n\n# dataset_root = ""/newfoundland/chaoyang/kitti""\n# model_path = ""/home/chaoyang/LKVOLearner/checkpoints_new/12_model.pth""\n# test_file_list = ""/newfoundland/chaoyang/SfMLearner/data/kitti/test_files_eigen.txt""\ndataset_root = FLAGS.dataset_root\nmodel_path = FLAGS.ckpt_file\ntest_file_list = FLAGS.test_file_list\noutput_path = FLAGS.output_path\n\nimg_size = [128, 416]\nvgg_depth_net = VggDepthEstimator(img_size)\nvgg_depth_net.load_state_dict(torch.load(model_path))\nvgg_depth_net.cuda()\n\nfliplr = FlipLR(imW=img_size[1], dim_w=2).cuda()\n\ndef read_text_lines(file_path):\n    f = open(file_path, \'r\')\n    lines = f.readlines()\n    f.close()\n    lines = [l.rstrip() for l in lines]\n    return lines\n\ntest_files = read_text_lines(test_file_list)\npred_depths = []\ni = 0\nfor filename in test_files:\n    print(i)\n    filename = filename.split()[0]\n    im_path = os.path.join(dataset_root, filename)\n    img_pil = Image.open(im_path).resize((img_size[1], img_size[0]), Image.ANTIALIAS)\n    # img_pil.save(\'kitti_test_images/%04d.png\'%(i))\n    img = np.array(img_pil)\n    # print(img.shape)\n    img = img.transpose(2, 0, 1)\n    # print(img.shape)\n    print(filename)\n    img_var = Variable(torch.from_numpy(img).float().cuda(), volatile=True)\n\n\n    if FLAGS.use_pp:\n        # flip image\n        img_vars = (torch.cat((fliplr(img_var).unsqueeze(0), img_var.unsqueeze(0)), 0)-127)/127\n        pred_depth_pyramid = vgg_depth_net.forward(img_vars)\n        depth = pred_depth_pyramid[0]\n        print(depth.size())\n        depth_mean = (fliplr(depth[0:1, :, :]) + depth[1:2, :, :])*.5\n        pred_depths.append(depth_mean.data.cpu().squeeze().numpy())\n        # compute mean\n    else:\n        pred_depth_pyramid = vgg_depth_net.forward((img_var.unsqueeze(0)-127)/127)\n        pred_depths.append(pred_depth_pyramid[0].data.cpu().squeeze().numpy())\n    i = i+1\n    # if i==3:\n    #     break\npred_depths = np.asarray(pred_depths)\nprint(pred_depths.shape)\nnp.save(output_path, 1/pred_depths)\nimport scipy.io as sio\nsio.savemat(output_path, {\'D\': pred_depths})\n    # print(pred_depth_pyramid[0].size())\n    # plt.imshow(pred_depth_pyramid[0].data.cpu().squeeze().numpy())\n    # plt.show()\n'"
src/train_main_ddvo.py,2,"b'import torch\nfrom torch import optim\nimport matplotlib.pyplot as plt\nimport torchvision\nimport numpy as np\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport scipy.io as sio\nimport os\n\nfrom LKVOLearner import LKVOLearner\nfrom KITTIdataset import KITTIdataset\n\nfrom collections import OrderedDict\nfrom options.train_options import TrainOptions\nfrom util.visualizer import Visualizer\n\nfrom timeit import default_timer as timer\n# data_root_path = \'/newfoundland/chaoyang/SfMLearner/data_kitti\'\n\nopt = TrainOptions().parse()\nimg_size = [opt.imH, opt.imW]\n\nvisualizer = Visualizer(opt)\n\ndataset = KITTIdataset(data_root_path=opt.dataroot, img_size=img_size, bundle_size=3)\ndataloader = DataLoader(dataset, batch_size=opt.batchSize,\n                        shuffle=True, num_workers=opt.nThreads, pin_memory=True)\n\ngpu_ids = list(range(opt.batchSize))\n\n\nlkvolearner = LKVOLearner(img_size=img_size, ref_frame_idx=1, lambda_S=opt.lambda_S, gpu_ids = gpu_ids, smooth_term = opt.smooth_term, use_ssim=opt.use_ssim)\nlkvolearner.init_weights()\n\n\nif opt.which_epoch >= 0:\n    print(""load pretrained model"")\n    lkvolearner.load_model(os.path.join(opt.checkpoints_dir, \'%s_model.pth\' % (opt.which_epoch)))\n\nlkvolearner.cuda()\n\nref_frame_idx = 1\n\n\ndef vis_depthmap(input):\n    x = (input-input.min()) * (255/(input.max()-input.min()+.00001))\n    return x.unsqueeze(2).repeat(1, 1, 3)\n\n\n\n\n\noptimizer = optim.Adam(lkvolearner.get_parameters(), lr=.0001)\n\nstep_num = 0\n\n\n\nfor epoch in range(max(0, opt.which_epoch), opt.epoch_num+1):\n    t = timer()\n    for ii, data in enumerate(dataloader):\n        optimizer.zero_grad()\n        frames = Variable(data[0].float().cuda())\n        camparams = Variable(data[1])\n        cost, photometric_cost, smoothness_cost, frames, inv_depths = \\\n            lkvolearner.forward(frames, camparams, max_lk_iter_num=opt.max_lk_iter_num)\n        # print(frames.size())\n        # print(inv_depths.size())\n        cost_ = cost.data.cpu()\n        inv_depths_mean = inv_depths.mean().data.cpu().numpy()\n        # if np.isnan(cost_.numpy()) or np.isinf(cost_.numpy()) or inv_depths_mean<1 or inv_depths_mean>7:\n        #     # lkvolearner.save_model(os.path.join(opt.checkpoints_dir, \'%s_model.pth\' % (epoch)))\n        #     print(""detect nan or inf-----!!!!! %f"" %(inv_depths_mean))\n        #     continue\n\n        # print(cost)\n        # print(inv_depth_pyramid)\n        cost.backward()\n        optimizer.step()\n\n        step_num+=1\n\n        if np.mod(step_num, opt.print_freq)==0:\n            elapsed_time = timer()-t\n            print(\'%s: %s / %s, ... elapsed time: %f (s)\' % (epoch, step_num, int(len(dataset)/opt.batchSize), elapsed_time))\n            print(inv_depths_mean)\n            t = timer()\n            visualizer.plot_current_errors(step_num, 1, opt,\n                        OrderedDict([(\'photometric_cost\', photometric_cost.data.cpu()[0]),\n                         (\'smoothness_cost\', smoothness_cost.data.cpu()[0]),\n                         (\'cost\', cost.data.cpu()[0])]))\n\n        if np.mod(step_num, opt.display_freq)==0:\n            # frame_vis = frames.data[:,1,:,:,:].permute(0,2,3,1).contiguous().view(-1,opt.imW, 3).cpu().numpy().astype(np.uint8)\n            # depth_vis = vis_depthmap(inv_depths.data[:,1,:,:].contiguous().view(-1,opt.imW).cpu()).numpy().astype(np.uint8)\n            frame_vis = frames.data.permute(1,2,0).contiguous().cpu().numpy().astype(np.uint8)\n            depth_vis = vis_depthmap(inv_depths.data.cpu()).numpy().astype(np.uint8)\n            visualizer.display_current_results(\n                            OrderedDict([(\'%s frame\' % (opt.name), frame_vis),\n                                    (\'%s inv_depth\' % (opt.name), depth_vis)]),\n                                    epoch)\n            sio.savemat(os.path.join(opt.checkpoints_dir, \'depth_%s.mat\' % (step_num)),\n                {\'D\': inv_depths.data.cpu().numpy(),\n                 \'I\': frame_vis})\n\n        if np.mod(step_num, opt.save_latest_freq)==0:\n            print(""cache model...."")\n            lkvolearner.save_model(os.path.join(opt.checkpoints_dir, \'%s_model.pth\' % (epoch)))\n            lkvolearner.cuda()\n            print(\'..... saved\')\n'"
src/train_main_finetune.py,2,"b'import torch\nfrom torch import optim\nimport matplotlib.pyplot as plt\nimport torchvision\nimport numpy as np\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport scipy.io as sio\nimport os\n\nfrom LKVOLearnerFinetune import LKVOLearner\nfrom KITTIdataset import KITTIdataset\n\nfrom collections import OrderedDict\nfrom options.train_options import TrainOptions\nfrom util.visualizer import Visualizer\n\nfrom timeit import default_timer as timer\n# data_root_path = \'/newfoundland/chaoyang/SfMLearner/data_kitti\'\n\nopt = TrainOptions().parse()\nimg_size = [opt.imH, opt.imW]\n\nvisualizer = Visualizer(opt)\n\ndataset = KITTIdataset(data_root_path=opt.dataroot, img_size=img_size, bundle_size=3)\ndataloader = DataLoader(dataset, batch_size=opt.batchSize,\n                        shuffle=True, num_workers=opt.nThreads, pin_memory=True)\n\ngpu_ids = list(range(opt.batchSize))\n\n\nlkvolearner = LKVOLearner(img_size=img_size, ref_frame_idx=1, lambda_S=opt.lambda_S, gpu_ids = gpu_ids,\n                smooth_term = opt.smooth_term, use_ssim=opt.use_ssim)\nlkvolearner.init_weights()\n\n\nif opt.which_epoch >= 0:\n    print(""load last checkpoint"")\n    lkvolearner.load_model(os.path.join(opt.checkpoints_dir, \'%s_model.pth\' % (opt.which_epoch)),\n                        os.path.join(opt.checkpoints_dir, \'pose_net.pth\'))\nelse:\n    print(""load pretrained models"")\n    lkvolearner.load_model(os.path.join(opt.checkpoints_dir, \'depth_net.pth\'),\n                        os.path.join(opt.checkpoints_dir, \'pose_net.pth\'))\n\nlkvolearner.cuda()\n\nref_frame_idx = 1\n\n\ndef vis_depthmap(input):\n    x = (input-input.min()) * (255/(input.max()-input.min()+.00001))\n    return x.unsqueeze(2).repeat(1, 1, 3)\n\n\n\n\n\noptimizer = optim.Adam(lkvolearner.get_parameters(), lr=.0001)\n\nstep_num = 0\n\n\n\nfor epoch in range(max(0, opt.which_epoch), opt.epoch_num+1):\n    t = timer()\n    for ii, data in enumerate(dataloader):\n        optimizer.zero_grad()\n        frames = Variable(data[0].float().cuda())\n        camparams = Variable(data[1])\n        cost, photometric_cost, smoothness_cost, frames, inv_depths = \\\n            lkvolearner.forward(frames, camparams, max_lk_iter_num=opt.max_lk_iter_num, lk_level=opt.lk_level)\n        # print(frames.size())\n        # print(inv_depths.size())\n        cost_ = cost.data.cpu()\n        inv_depths_mean = inv_depths.mean().data.cpu().numpy()\n        # if np.isnan(cost_.numpy()) or np.isinf(cost_.numpy()) or inv_depths_mean<1 or inv_depths_mean>7:\n        #     # lkvolearner.save_model(os.path.join(opt.checkpoints_dir, \'%s_model.pth\' % (epoch)))\n        #     print(""detect nan or inf-----!!!!! %f"" %(inv_depths_mean))\n        #     continue\n\n        # print(cost)\n        # print(inv_depth_pyramid)\n        cost.backward()\n        optimizer.step()\n\n        step_num+=1\n\n        if np.mod(step_num, opt.print_freq)==0:\n            elapsed_time = timer()-t\n            print(\'%s: %s / %s, ... elapsed time: %f (s)\' % (epoch, step_num, int(len(dataset)/opt.batchSize), elapsed_time))\n            print(inv_depths_mean)\n            t = timer()\n            visualizer.plot_current_errors(step_num, 1, opt,\n                        OrderedDict([(\'photometric_cost\', photometric_cost.data.cpu()[0]),\n                         (\'smoothness_cost\', smoothness_cost.data.cpu()[0]),\n                         (\'cost\', cost.data.cpu()[0])]))\n\n        if np.mod(step_num, opt.display_freq)==0:\n            # frame_vis = frames.data[:,1,:,:,:].permute(0,2,3,1).contiguous().view(-1,opt.imW, 3).cpu().numpy().astype(np.uint8)\n            # depth_vis = vis_depthmap(inv_depths.data[:,1,:,:].contiguous().view(-1,opt.imW).cpu()).numpy().astype(np.uint8)\n            frame_vis = frames.data.permute(1,2,0).contiguous().cpu().numpy().astype(np.uint8)\n            depth_vis = vis_depthmap(inv_depths.data.cpu()).numpy().astype(np.uint8)\n            visualizer.display_current_results(\n                            OrderedDict([(\'%s frame\' % (opt.name), frame_vis),\n                                    (\'%s inv_depth\' % (opt.name), depth_vis)]),\n                                    epoch)\n            sio.savemat(os.path.join(opt.checkpoints_dir, \'depth_%s.mat\' % (step_num)),\n                {\'D\': inv_depths.data.cpu().numpy(),\n                 \'I\': frame_vis})\n\n        if np.mod(step_num, opt.save_latest_freq)==0:\n            print(""cache model...."")\n            lkvolearner.save_model(os.path.join(opt.checkpoints_dir, \'%s_model.pth\' % (epoch)))\n            lkvolearner.cuda()\n            print(\'..... saved\')\n'"
src/train_main_posenet.py,2,"b'import torch\nfrom torch import optim\nimport matplotlib.pyplot as plt\nimport torchvision\nimport numpy as np\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport scipy.io as sio\nimport os\n\nfrom SfMLearner import SfMLearner\nfrom KITTIdataset import KITTIdataset\n# from KCSdataset import KCSdataset\n\nfrom collections import OrderedDict\nfrom options.train_options import TrainOptions\nfrom util.visualizer import Visualizer\n\nfrom timeit import default_timer as timer\n# data_root_path = \'/newfoundland/chaoyang/SfMLearner/data_kitti\'\n\nopt = TrainOptions().parse()\nimg_size = [opt.imH, opt.imW]\n\nvisualizer = Visualizer(opt)\n\ndataset = KITTIdataset(data_root_path=opt.dataroot, img_size=img_size, bundle_size=3)\n# dataset = KCSdataset(img_size=img_size, bundle_size=3)\ndataloader = DataLoader(dataset, batch_size=opt.batchSize,\n                        shuffle=True, num_workers=opt.nThreads, pin_memory=True)\n\ngpu_ids = list(range(opt.batchSize))\n\n\nsfmlearner = SfMLearner(img_size=img_size, ref_frame_idx=1, lambda_S=opt.lambda_S, gpu_ids = gpu_ids, smooth_term = opt.smooth_term, use_ssim=opt.use_ssim)\nsfmlearner.init_weights()\n\n\nif opt.which_epoch >= 0:\n    print(""load pretrained model"")\n    sfmlearner.load_model(os.path.join(opt.checkpoints_dir, \'%s\' % (opt.which_epoch)))\n\nsfmlearner.cuda()\n\nref_frame_idx = 1\n\n\ndef vis_depthmap(input):\n    x = (input-input.min()) * (255/(input.max()-input.min()+.00001))\n    return x.unsqueeze(2).repeat(1, 1, 3)\n\n\noptimizer = optim.Adam(sfmlearner.get_parameters(), lr=.0001)\n\nstep_num = 0\n\n\n\nfor epoch in range(max(0, opt.which_epoch), opt.epoch_num+1):\n    t = timer()\n    for ii, data in enumerate(dataloader):\n        optimizer.zero_grad()\n        frames = Variable(data[0].float().cuda())\n        camparams = Variable(data[1])\n        cost, photometric_cost, smoothness_cost, frames, inv_depths, _ = \\\n            sfmlearner.forward(frames, camparams)\n        # print(frames.size())\n        # print(inv_depths.size())\n        cost_ = cost.data.cpu()\n        inv_depths_mean = inv_depths.mean().data.cpu().numpy()\n        # if np.isnan(cost_.numpy()) or np.isinf(cost_.numpy()) or inv_depths_mean<1 or inv_depths_mean>7:\n        #     # lkvolearner.save_model(os.path.join(opt.checkpoints_dir, \'%s_model.pth\' % (epoch)))\n        #     print(""detect nan or inf-----!!!!! %f"" %(inv_depths_mean))\n        #     continue\n\n        # print(cost)\n        # print(inv_depth_pyramid)\n        cost.backward()\n        optimizer.step()\n\n        step_num+=1\n\n        if np.mod(step_num, opt.print_freq)==0:\n            elapsed_time = timer()-t\n            print(\'%s: %s / %s, ... elapsed time: %f (s)\' % (epoch, step_num, int(len(dataset)/opt.batchSize), elapsed_time))\n            print(inv_depths_mean)\n            t = timer()\n            visualizer.plot_current_errors(step_num, 1, opt,\n                        OrderedDict([(\'photometric_cost\', photometric_cost.data.cpu()[0]),\n                         (\'smoothness_cost\', smoothness_cost.data.cpu()[0]),\n                         (\'cost\', cost.data.cpu()[0])]))\n\n        if np.mod(step_num, opt.display_freq)==0:\n            # frame_vis = frames.data[:,1,:,:,:].permute(0,2,3,1).contiguous().view(-1,opt.imW, 3).cpu().numpy().astype(np.uint8)\n            # depth_vis = vis_depthmap(inv_depths.data[:,1,:,:].contiguous().view(-1,opt.imW).cpu()).numpy().astype(np.uint8)\n            frame_vis = frames.data.permute(1,2,0).contiguous().cpu().numpy().astype(np.uint8)\n            depth_vis = vis_depthmap(inv_depths.data.cpu()).numpy().astype(np.uint8)\n            visualizer.display_current_results(\n                            OrderedDict([(\'%s frame\' % (opt.name), frame_vis),\n                                    (\'%s inv_depth\' % (opt.name), depth_vis)]),\n                                    epoch)\n            sio.savemat(os.path.join(opt.checkpoints_dir, \'depth_%s.mat\' % (step_num)),\n                {\'D\': inv_depths.data.cpu().numpy(),\n                 \'I\': frame_vis})\n\n        if np.mod(step_num, opt.save_latest_freq)==0:\n            print(""cache model...."")\n            sfmlearner.save_model(os.path.join(opt.checkpoints_dir, \'%s\' % (epoch)))\n            sfmlearner.cuda()\n            print(\'..... saved\')\n'"
src/options/__init__.py,0,b''
src/options/base_options.py,1,"b""import argparse\nimport os\nfrom util import util\nimport torch\n\nclass BaseOptions():\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n        self.initialized = False\n\n    def initialize(self):\n        self.parser.add_argument('--dataroot', required=True, help='path to images (should have subfolders trainA, trainB, valA, valB, etc)')\n        self.parser.add_argument('--batchSize', type=int, default=1, help='input batch size')\n        self.parser.add_argument('--imH', type=int, default=128, help='imH')\n        self.parser.add_argument('--imW', type=int, default=416, help='imW')\n        self.parser.add_argument('--max_lk_iter_num', type=int, default=10, help='maximum iteration for LK update')\n\n        self.parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n        self.parser.add_argument('--name', type=str, default='experiment_name', help='name of the experiment. It decides where to store samples and models')\n\n        self.parser.add_argument('--nThreads', default=2, type=int, help='# threads for loading data')\n        self.parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n\n        self.parser.add_argument('--display_winsize', type=int, default=256,  help='display window size')\n        self.parser.add_argument('--display_id', type=int, default=1, help='window id of the web display')\n        self.parser.add_argument('--display_port', type=int, default=8097, help='visdom port of the web display')\n        self.parser.add_argument('--display_single_pane_ncols', type=int, default=0, help='if positive, display all images in a single visdom web panel with certain number of images per row.')\n\n\n\n    def parse(self):\n        if not self.initialized:\n            self.initialize()\n        self.opt = self.parser.parse_args()\n        self.opt.isTrain = self.isTrain   # train or test\n\n        str_ids = self.opt.gpu_ids.split(',')\n        self.opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                self.opt.gpu_ids.append(id)\n\n        # set gpu ids\n        if len(self.opt.gpu_ids) > 0:\n            torch.cuda.set_device(self.opt.gpu_ids[0])\n\n        args = vars(self.opt)\n\n        print('------------ Options -------------')\n        for k, v in sorted(args.items()):\n            print('%s: %s' % (str(k), str(v)))\n        print('-------------- End ----------------')\n\n        # save to the disk\n        expr_dir = os.path.join(self.opt.checkpoints_dir, self.opt.name)\n        util.mkdirs(expr_dir)\n        file_name = os.path.join(expr_dir, 'opt.txt')\n        with open(file_name, 'wt') as opt_file:\n            opt_file.write('------------ Options -------------\\n')\n            for k, v in sorted(args.items()):\n                opt_file.write('%s: %s\\n' % (str(k), str(v)))\n            opt_file.write('-------------- End ----------------\\n')\n        return self.opt\n"""
src/options/test_options.py,0,"b'from .base_options import BaseOptions\n\n\nclass TestOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        self.parser.add_argument(\'--ntest\', type=int, default=float(""inf""), help=\'# of test examples.\')\n        self.parser.add_argument(\'--results_dir\', type=str, default=\'./results/\', help=\'saves results here.\')\n        self.parser.add_argument(\'--aspect_ratio\', type=float, default=1.0, help=\'aspect ratio of result images\')\n        self.parser.add_argument(\'--phase\', type=str, default=\'test\', help=\'train, val, test, etc\')\n        self.parser.add_argument(\'--which_epoch\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n        self.parser.add_argument(\'--how_many\', type=int, default=50, help=\'how many test images to run\')\n        self.isTrain = False\n\n        self.parser.add_argument(\'--no_lsgan\', action=\'store_true\', help=\'do *not* use least square GAN, if false, use vanilla GAN\')\n'"
src/options/train_options.py,0,"b""from .base_options import BaseOptions\n\n\nclass TrainOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        self.parser.add_argument('--lk_level', type=int, default=1, help='number of image levels for direct vo')\n        self.parser.add_argument('--use_ssim', default=False, action='store_true', help='use ssim loss')\n        self.parser.add_argument('--smooth_term', type=str, default='lap', help='smoothness term type, choose between lap, 1st, 2nd')\n        self.parser.add_argument('--lambda_S', type=float, default=.01, help='smoothness cost weight')\n        self.parser.add_argument('--lambda_E', type=float, default=.01, help='explainable mask regulariation cost weight')\n        self.parser.add_argument('--epoch_num', type=int, default=20, help='number of epochs for training')\n        self.parser.add_argument('--display_freq', type=int, default=100, help='frequency of showing training results on screen')\n        self.parser.add_argument('--print_freq', type=int, default=10, help='frequency of showing training results on console')\n        self.parser.add_argument('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')\n        self.parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n        self.parser.add_argument('--which_epoch', type=int, default=-1, help='which epoch to load? set to epoch number, set -1 to train from scratch')\n        self.parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n        self.parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero')\n        self.parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n        self.parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n        self.parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n        self.isTrain = True\n"""
src/util/__init__.py,0,b''
src/util/get_data.py,0,"b'from __future__ import print_function\nimport os\nimport tarfile\nimport requests\nfrom warnings import warn\nfrom zipfile import ZipFile\nfrom bs4 import BeautifulSoup\nfrom os.path import abspath, isdir, join, basename\n\n\nclass GetData(object):\n    """"""\n\n    Download CycleGAN or Pix2Pix Data.\n\n    Args:\n        technique : str\n            One of: \'cyclegan\' or \'pix2pix\'.\n        verbose : bool\n            If True, print additional information.\n\n    Examples:\n        >>> from util.get_data import GetData\n        >>> gd = GetData(technique=\'cyclegan\')\n        >>> new_data_path = gd.get(save_path=\'./datasets\')  # options will be displayed.\n\n    """"""\n\n    def __init__(self, technique=\'cyclegan\', verbose=True):\n        url_dict = {\n            \'pix2pix\': \'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets\',\n            \'cyclegan\': \'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets\'\n        }\n        self.url = url_dict.get(technique.lower())\n        self._verbose = verbose\n\n    def _print(self, text):\n        if self._verbose:\n            print(text)\n\n    @staticmethod\n    def _get_options(r):\n        soup = BeautifulSoup(r.text, \'lxml\')\n        options = [h.text for h in soup.find_all(\'a\', href=True)\n                   if h.text.endswith((\'.zip\', \'tar.gz\'))]\n        return options\n\n    def _present_options(self):\n        r = requests.get(self.url)\n        options = self._get_options(r)\n        print(\'Options:\\n\')\n        for i, o in enumerate(options):\n            print(""{0}: {1}"".format(i, o))\n        choice = input(""\\nPlease enter the number of the ""\n                       ""dataset above you wish to download:"")\n        return options[int(choice)]\n\n    def _download_data(self, dataset_url, save_path):\n        if not isdir(save_path):\n            os.makedirs(save_path)\n\n        base = basename(dataset_url)\n        temp_save_path = join(save_path, base)\n\n        with open(temp_save_path, ""wb"") as f:\n            r = requests.get(dataset_url)\n            f.write(r.content)\n\n        if base.endswith(\'.tar.gz\'):\n            obj = tarfile.open(temp_save_path)\n        elif base.endswith(\'.zip\'):\n            obj = ZipFile(temp_save_path, \'r\')\n        else:\n            raise ValueError(""Unknown File Type: {0}."".format(base))\n\n        self._print(""Unpacking Data..."")\n        obj.extractall(save_path)\n        obj.close()\n        os.remove(temp_save_path)\n\n    def get(self, save_path, dataset=None):\n        """"""\n\n        Download a dataset.\n\n        Args:\n            save_path : str\n                A directory to save the data to.\n            dataset : str, optional\n                A specific dataset to download.\n                Note: this must include the file extension.\n                If None, options will be presented for you\n                to choose from.\n\n        Returns:\n            save_path_full : str\n                The absolute path to the downloaded data.\n\n        """"""\n        if dataset is None:\n            selected_dataset = self._present_options()\n        else:\n            selected_dataset = dataset\n\n        save_path_full = join(save_path, selected_dataset.split(\'.\')[0])\n\n        if isdir(save_path_full):\n            warn(""\\n\'{0}\' already exists. Voiding Download."".format(\n                save_path_full))\n        else:\n            self._print(\'Downloading Data...\')\n            url = ""{0}/{1}"".format(self.url, selected_dataset)\n            self._download_data(url, save_path=save_path)\n\n        return abspath(save_path_full)\n'"
src/util/html.py,0,"b'import dominate\nfrom dominate.tags import *\nimport os\n\n\nclass HTML:\n    def __init__(self, web_dir, title, reflesh=0):\n        self.title = title\n        self.web_dir = web_dir\n        self.img_dir = os.path.join(self.web_dir, \'images\')\n        if not os.path.exists(self.web_dir):\n            os.makedirs(self.web_dir)\n        if not os.path.exists(self.img_dir):\n            os.makedirs(self.img_dir)\n        # print(self.img_dir)\n\n        self.doc = dominate.document(title=title)\n        if reflesh > 0:\n            with self.doc.head:\n                meta(http_equiv=""reflesh"", content=str(reflesh))\n\n    def get_image_dir(self):\n        return self.img_dir\n\n    def add_header(self, str):\n        with self.doc:\n            h3(str)\n\n    def add_table(self, border=1):\n        self.t = table(border=border, style=""table-layout: fixed;"")\n        self.doc.add(self.t)\n\n    def add_images(self, ims, txts, links, width=400):\n        self.add_table()\n        with self.t:\n            with tr():\n                for im, txt, link in zip(ims, txts, links):\n                    with td(style=""word-wrap: break-word;"", halign=""center"", valign=""top""):\n                        with p():\n                            with a(href=os.path.join(\'images\', link)):\n                                img(style=""width:%dpx"" % width, src=os.path.join(\'images\', im))\n                            br()\n                            p(txt)\n\n    def save(self):\n        html_file = \'%s/index.html\' % self.web_dir\n        f = open(html_file, \'wt\')\n        f.write(self.doc.render())\n        f.close()\n\n\nif __name__ == \'__main__\':\n    html = HTML(\'web/\', \'test_html\')\n    html.add_header(\'hello world\')\n\n    ims = []\n    txts = []\n    links = []\n    for n in range(4):\n        ims.append(\'image_%d.png\' % n)\n        txts.append(\'text_%d\' % n)\n        links.append(\'image_%d.png\' % n)\n    html.add_images(ims, txts, links)\n    html.save()\n'"
src/util/image_pool.py,3,"b'import random\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nclass ImagePool():\n    def __init__(self, pool_size):\n        self.pool_size = pool_size\n        if self.pool_size > 0:\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        if self.pool_size == 0:\n            return images\n        return_images = []\n        for image in images.data:\n            image = torch.unsqueeze(image, 0)\n            if self.num_imgs < self.pool_size:\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:\n                    random_id = random.randint(0, self.pool_size-1)\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:\n                    return_images.append(image)\n        return_images = Variable(torch.cat(return_images, 0))\n        return return_images\n'"
src/util/png.py,0,"b'import struct\nimport zlib\n\ndef encode(buf, width, height):\n  """""" buf: must be bytes or a bytearray in py3, a regular string in py2. formatted RGBRGB... """"""\n  assert (width * height * 3 == len(buf))\n  bpp = 3\n\n  def raw_data():\n    # reverse the vertical line order and add null bytes at the start\n    row_bytes = width * bpp\n    for row_start in range((height - 1) * width * bpp, -1, -row_bytes):\n      yield b\'\\x00\'\n      yield buf[row_start:row_start + row_bytes]\n\n  def chunk(tag, data):\n    return [\n        struct.pack(""!I"", len(data)),\n        tag,\n        data,\n        struct.pack(""!I"", 0xFFFFFFFF & zlib.crc32(data, zlib.crc32(tag)))\n      ]\n\n  SIGNATURE = b\'\\x89PNG\\r\\n\\x1a\\n\'\n  COLOR_TYPE_RGB = 2\n  COLOR_TYPE_RGBA = 6\n  bit_depth = 8\n  return b\'\'.join(\n      [ SIGNATURE ] +\n      chunk(b\'IHDR\', struct.pack(""!2I5B"", width, height, bit_depth, COLOR_TYPE_RGB, 0, 0, 0)) +\n      chunk(b\'IDAT\', zlib.compress(b\'\'.join(raw_data()), 9)) +\n      chunk(b\'IEND\', b\'\')\n    )\n'"
src/util/util.py,1,"b'from __future__ import print_function\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport inspect, re\nimport numpy as np\nimport os\nimport collections\n\n# Converts a Tensor into a Numpy array\n# |imtype|: the desired type of the converted numpy array\ndef tensor2im(image_tensor, imtype=np.uint8):\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n    return image_numpy.astype(imtype)\n\n\ndef diagnose_network(net, name=\'network\'):\n    mean = 0.0\n    count = 0\n    for param in net.parameters():\n        if param.grad is not None:\n            mean += torch.mean(torch.abs(param.grad.data))\n            count += 1\n    if count > 0:\n        mean = mean / count\n    print(name)\n    print(mean)\n\n\ndef save_image(image_numpy, image_path):\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path)\n\ndef info(object, spacing=10, collapse=1):\n    """"""Print methods and doc strings.\n    Takes module, class, list, dictionary, or string.""""""\n    methodList = [e for e in dir(object) if isinstance(getattr(object, e), collections.Callable)]\n    processFunc = collapse and (lambda s: "" "".join(s.split())) or (lambda s: s)\n    print( ""\\n"".join([""%s %s"" %\n                     (method.ljust(spacing),\n                      processFunc(str(getattr(object, method).__doc__)))\n                     for method in methodList]) )\n\ndef varname(p):\n    for line in inspect.getframeinfo(inspect.currentframe().f_back)[3]:\n        m = re.search(r\'\\bvarname\\s*\\(\\s*([A-Za-z_][A-Za-z0-9_]*)\\s*\\)\', line)\n        if m:\n            return m.group(1)\n\ndef print_numpy(x, val=True, shp=False):\n    x = x.astype(np.float64)\n    if shp:\n        print(\'shape,\', x.shape)\n    if val:\n        x = x.flatten()\n        print(\'mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f\' % (\n            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n'"
src/util/visualizer.py,0,"b'import numpy as np\nimport os\nimport ntpath\nimport time\nfrom . import util\nfrom . import html\n\nclass Visualizer():\n    def __init__(self, opt):\n        # self.opt = opt\n        self.display_id = opt.display_id\n        self.use_html = opt.isTrain and not opt.no_html\n        self.win_size = opt.display_winsize\n        self.name = opt.name\n        if self.display_id > 0:\n            import visdom\n            self.vis = visdom.Visdom(port = opt.display_port)\n            self.display_single_pane_ncols = opt.display_single_pane_ncols\n\n        if self.use_html:\n            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, \'web\')\n            self.img_dir = os.path.join(self.web_dir, \'images\')\n            print(\'create web directory %s...\' % self.web_dir)\n            util.mkdirs([self.web_dir, self.img_dir])\n        self.log_name = os.path.join(opt.checkpoints_dir, opt.name, \'loss_log.txt\')\n        with open(self.log_name, ""a"") as log_file:\n            now = time.strftime(""%c"")\n            log_file.write(\'================ Training Loss (%s) ================\\n\' % now)\n\n    # |visuals|: dictionary of images to display or save\n    def display_current_results(self, visuals, epoch):\n        if self.display_id > 0: # show images in the browser\n            if self.display_single_pane_ncols > 0:\n                h, w = next(iter(visuals.values())).shape[:2]\n                table_css = """"""<style>\n    table {border-collapse: separate; border-spacing:4px; white-space:nowrap; text-align:center}\n    table td {width: %dpx; height: %dpx; padding: 4px; outline: 4px solid black}\n</style>"""""" % (w, h)\n                ncols = self.display_single_pane_ncols\n                title = self.name\n                label_html = \'\'\n                label_html_row = \'\'\n                nrows = int(np.ceil(len(visuals.items()) / ncols))\n                images = []\n                idx = 0\n                for label, image_numpy in visuals.items():\n                    label_html_row += \'<td>%s</td>\' % label\n                    images.append(image_numpy.transpose([2, 0, 1]))\n                    idx += 1\n                    if idx % ncols == 0:\n                        label_html += \'<tr>%s</tr>\' % label_html_row\n                        label_html_row = \'\'\n                white_image = np.ones_like(image_numpy.transpose([2, 0, 1]))*255\n                while idx % ncols != 0:\n                    images.append(white_image)\n                    label_html_row += \'<td></td>\'\n                    idx += 1\n                if label_html_row != \'\':\n                    label_html += \'<tr>%s</tr>\' % label_html_row\n                # pane col = image row\n                self.vis.images(images, nrow=ncols, win=self.display_id + 1,\n                                padding=2, opts=dict(title=title + \' images\'))\n                label_html = \'<table>%s</table>\' % label_html\n                self.vis.text(table_css + label_html, win = self.display_id + 2,\n                              opts=dict(title=title + \' labels\'))\n            else:\n                idx = 1\n                for label, image_numpy in visuals.items():\n                    #image_numpy = np.flipud(image_numpy)\n                    self.vis.image(image_numpy.transpose([2,0,1]), opts=dict(title=label),\n                                       win=self.display_id + idx)\n                    idx += 1\n\n        if self.use_html: # save images to a html file\n            for label, image_numpy in visuals.items():\n                img_path = os.path.join(self.img_dir, \'epoch%.3d_%s.png\' % (epoch, label))\n                util.save_image(image_numpy, img_path)\n            # update website\n            webpage = html.HTML(self.web_dir, \'Experiment name = %s\' % self.name, reflesh=1)\n            for n in range(epoch, 0, -1):\n                webpage.add_header(\'epoch [%d]\' % n)\n                ims = []\n                txts = []\n                links = []\n\n                for label, image_numpy in visuals.items():\n                    img_path = \'epoch%.3d_%s.png\' % (n, label)\n                    ims.append(img_path)\n                    txts.append(label)\n                    links.append(img_path)\n                webpage.add_images(ims, txts, links, width=self.win_size)\n            webpage.save()\n\n    # errors: dictionary of error labels and values\n    def plot_current_errors(self, epoch, counter_ratio, opt, errors):\n        if not hasattr(self, \'plot_data\'):\n            self.plot_data = {\'X\':[],\'Y\':[], \'legend\':list(errors.keys())}\n        self.plot_data[\'X\'].append(epoch + counter_ratio)\n        self.plot_data[\'Y\'].append([errors[k] for k in self.plot_data[\'legend\']])\n        self.vis.line(\n            X=np.stack([np.array(self.plot_data[\'X\'])]*len(self.plot_data[\'legend\']),1),\n            Y=np.array(self.plot_data[\'Y\']),\n            opts={\n                \'title\': self.name + \' loss over time\',\n                \'legend\': self.plot_data[\'legend\'],\n                \'xlabel\': \'epoch\',\n                \'ylabel\': \'loss\'},\n            win=self.display_id)\n\n    # errors: same format as |errors| of plotCurrentErrors\n    def print_current_errors(self, epoch, i, errors, t):\n        message = \'(epoch: %d, iters: %d, time: %.3f) \' % (epoch, i, t)\n        for k, v in errors.items():\n            message += \'%s: %.3f \' % (k, v)\n\n        print(message)\n        with open(self.log_name, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n\n    # save image to the disk\n    def save_images(self, webpage, visuals, image_path):\n        image_dir = webpage.get_image_dir()\n        short_path = ntpath.basename(image_path[0])\n        name = os.path.splitext(short_path)[0]\n\n        webpage.add_header(name)\n        ims = []\n        txts = []\n        links = []\n\n        for label, image_numpy in visuals.items():\n            image_name = \'%s_%s.png\' % (name, label)\n            save_path = os.path.join(image_dir, image_name)\n            util.save_image(image_numpy, save_path)\n\n            ims.append(image_name)\n            txts.append(label)\n            links.append(image_name)\n        webpage.add_images(ims, txts, links, width=self.win_size)\n'"
