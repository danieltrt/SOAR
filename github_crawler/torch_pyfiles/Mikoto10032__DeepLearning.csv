file_path,api_count,code
books/PRML/PRML-master-Python/setup.py,0,"b'from setuptools import setup, find_packages\n\n\nsetup(\n    name=""prml"",\n    version=""0.0.1"",\n    description=""Collection of PRML algorithms"",\n    author=""ctgk"",\n    python_requires="">=3.6"",\n    install_requires=[""numpy"", ""scipy""],\n    packages=find_packages(exclude=[""test"", ""test.*""]),\n    test_suite=""test""\n)\n'"
Projects/lihang-code/code/第3章 k近邻法(KNearestNeighbors)/KDT.py,0,"b'import numpy as np\nfrom math import sqrt\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\niris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf[\'label\'] = iris.target\ndf.columns = [\'sepal length\', \'sepal width\', \'petal length\', \'petal width\', \'label\']\n\ndata = np.array(df.iloc[:100, [0, 1, -1]])\ntrain, test = train_test_split(data, test_size=0.1)\nx0 = np.array([x0 for i, x0 in enumerate(train) if train[i][-1] == 0])\nx1 = np.array([x1 for i, x1 in enumerate(train) if train[i][-1] == 1])\n\n\ndef show_train():\n    plt.scatter(x0[:, 0], x0[:, 1], c=\'pink\', label=\'[0]\')\n    plt.scatter(x1[:, 0], x1[:, 1], c=\'orange\', label=\'[1]\')\n    plt.xlabel(\'sepal length\')\n    plt.ylabel(\'sepal width\')\n\n\nclass Node:\n    def __init__(self, data, depth=0, lchild=None, rchild=None):\n        self.data = data\n        self.depth = depth\n        self.lchild = lchild\n        self.rchild = rchild\n\n\nclass KdTree:\n    def __init__(self):\n        self.KdTree = None\n        self.n = 0\n        self.nearest = None\n\n    def create(self, dataSet, depth=0):\n        if len(dataSet) > 0:\n            m, n = np.shape(dataSet)\n            self.n = n - 1\n            axis = depth % self.n\n            mid = int(m / 2)\n            dataSetcopy = sorted(dataSet, key=lambda x: x[axis])\n            node = Node(dataSetcopy[mid], depth)\n            if depth == 0:\n                self.KdTree = node\n            node.lchild = self.create(dataSetcopy[:mid], depth+1)\n            node.rchild = self.create(dataSetcopy[mid+1:], depth+1)\n            return node\n        return None\n\n    def preOrder(self, node):\n        if node is not None:\n            print(node.depth, node.data)\n            self.preOrder(node.lchild)\n            self.preOrder(node.rchild)\n\n    def search(self, x, count=1):\n        nearest = []\n        for i in range(count):\n            nearest.append([-1, None])\n        self.nearest = np.array(nearest)\n\n        def recurve(node):\n            if node is not None:\n                axis = node.depth % self.n\n                daxis = x[axis] - node.data[axis]\n                if daxis < 0:\n                    recurve(node.lchild)\n                else:\n                    recurve(node.rchild)\n\n                dist = sqrt(sum((p1 - p2) ** 2 for p1, p2 in zip(x, node.data)))\n                for i, d in enumerate(self.nearest):\n                    if d[0] < 0 or dist < d[0]:\n                        self.nearest = np.insert(self.nearest, i, [dist, node], axis=0)\n                        self.nearest = self.nearest[:-1]\n                        break\n\n                n = list(self.nearest[:, 0]).count(-1)\n                if self.nearest[-n-1, 0] > abs(daxis):\n                    if daxis < 0:\n                        recurve(node.rchild)\n                    else:\n                        recurve(node.lchild)\n\n        recurve(self.KdTree)\n\n        knn = self.nearest[:, 1]\n        belong = []\n        for i in knn:\n            belong.append(i.data[-1])\n        b = max(set(belong), key=belong.count)\n\n        return self.nearest, b\n\n\nkdt = KdTree()\nkdt.create(train)\nkdt.preOrder(kdt.KdTree)\n\nscore = 0\nfor x in test:\n    input(\'press Enter to show next:\')\n    show_train()\n    plt.scatter(x[0], x[1], c=\'red\', marker=\'x\')  # \xe6\xb5\x8b\xe8\xaf\x95\xe7\x82\xb9\n    near, belong = kdt.search(x[:-1], 5)  # \xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xb4\xe8\xbf\x91\xe7\x82\xb9\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\n    if belong == x[-1]:\n        score += 1\n    print(""test:"")\n    print(x, ""predict:"", belong)\n    print(""nearest:"")\n    for n in near:\n        print(n[1].data, ""dist:"", n[0])\n        plt.scatter(n[1].data[0], n[1].data[1], c=\'green\', marker=\'+\')  # k\xe4\xb8\xaa\xe6\x9c\x80\xe8\xbf\x91\xe9\x82\xbb\xe7\x82\xb9\n    plt.legend()\n    plt.show()\n\nscore /= len(test)\nprint(""score:"", score)\n'"
Projects/lihang-code/code/第6章 逻辑斯谛回归(LogisticRegression)/最大熵模型 IIS.py,0,"b'import math\nfrom copy import deepcopy\n\n\nclass MaxEntropy:\n    def __init__(self, EPS=0.005):\n        self._samples = []\n        self._Y = set()  # \xe6\xa0\x87\xe7\xad\xbe\xe9\x9b\x86\xe5\x90\x88\xef\xbc\x8c\xe7\x9b\xb8\xe5\xbd\x93\xe5\x8e\xbb\xe5\x8e\xbb\xe9\x87\x8d\xe5\x90\x8e\xe7\x9a\x84y\n        self._numXY = {}  # key\xe4\xb8\xba(x,y)\xef\xbc\x8cvalue\xe4\xb8\xba\xe5\x87\xba\xe7\x8e\xb0\xe6\xac\xa1\xe6\x95\xb0\n        self._N = 0  # \xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\n        self._Ep_ = []   # \xe6\xa0\xb7\xe6\x9c\xac\xe5\x88\x86\xe5\xb8\x83\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe6\x9c\x9f\xe6\x9c\x9b\xe5\x80\xbc\n        self._xyID = {}   # key\xe8\xae\xb0\xe5\xbd\x95(x,y),value\xe8\xae\xb0\xe5\xbd\x95id\xe5\x8f\xb7\n        self._n = 0  # \xe7\x89\xb9\xe5\xbe\x81\xe9\x94\xae\xe5\x80\xbc(x,y)\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\n        self._C = 0   # \xe6\x9c\x80\xe5\xa4\xa7\xe7\x89\xb9\xe5\xbe\x81\xe6\x95\xb0\n        self._IDxy = {}    # key\xe4\xb8\xba(x,y)\xef\xbc\x8cvalue\xe4\xb8\xba\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84id\xe5\x8f\xb7\n        self._w = []\n        self._EPS = EPS   # \xe6\x94\xb6\xe6\x95\x9b\xe6\x9d\xa1\xe4\xbb\xb6\n        self._lastw = []    # \xe4\xb8\x8a\xe4\xb8\x80\xe6\xac\xa1w\xe5\x8f\x82\xe6\x95\xb0\xe5\x80\xbc\n\n    def loadData(self, dataset):\n        self._samples = deepcopy(dataset)\n        for items in self._samples:\n                y = items[0]\n                X = items[1:]\n                self._Y.add(y)  # \xe9\x9b\x86\xe5\x90\x88\xe4\xb8\xady\xe8\x8b\xa5\xe5\xb7\xb2\xe5\xad\x98\xe5\x9c\xa8\xe5\x88\x99\xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8\xe5\xbf\xbd\xe7\x95\xa5\n                for x in X:\n                    if (x, y) in self._numXY:\n                        self._numXY[(x, y)] += 1\n                    else:\n                        self._numXY[(x, y)] = 1\n\n        self._N = len(self._samples)\n        self._n = len(self._numXY)\n        self._C = max([len(sample)-1 for sample in self._samples])\n        self._w = [0]*self._n\n        self._lastw = self._w[:]\n\n        self._Ep_ = [0] * self._n\n        for i, xy in enumerate(self._numXY):   # \xe8\xae\xa1\xe7\xae\x97\xe7\x89\xb9\xe5\xbe\x81\xe5\x87\xbd\xe6\x95\xb0fi\xe5\x85\xb3\xe4\xba\x8e\xe7\xbb\x8f\xe9\xaa\x8c\xe5\x88\x86\xe5\xb8\x83\xe7\x9a\x84\xe6\x9c\x9f\xe6\x9c\x9b\n            self._Ep_[i] = self._numXY[xy]/self._N\n            self._xyID[xy] = i\n            self._IDxy[i] = xy\n\n    def _Zx(self, X):    # \xe8\xae\xa1\xe7\xae\x97\xe6\xaf\x8f\xe4\xb8\xaaZ(x)\xe5\x80\xbc\n        zx = 0\n        for y in self._Y:\n            ss = 0\n            for x in X:\n                if (x, y) in self._numXY:\n                    ss += self._w[self._xyID[(x, y)]]\n            zx += math.exp(ss)\n        return zx\n\n    def _model_pyx(self, y, X):   # \xe8\xae\xa1\xe7\xae\x97\xe6\xaf\x8f\xe4\xb8\xaaP(y|x)\n        zx = self._Zx(X)\n        ss = 0\n        for x in X:\n            if (x, y) in self._numXY:\n                ss += self._w[self._xyID[(x, y)]]\n        pyx = math.exp(ss)/zx\n        return pyx\n\n    def _model_ep(self, index):   # \xe8\xae\xa1\xe7\xae\x97\xe7\x89\xb9\xe5\xbe\x81\xe5\x87\xbd\xe6\x95\xb0fi\xe5\x85\xb3\xe4\xba\x8e\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe6\x9c\x9f\xe6\x9c\x9b\n        x, y = self._IDxy[index]\n        ep = 0\n        for sample in self._samples:\n            if x not in sample:\n                continue\n            pyx = self._model_pyx(y, sample)\n            ep += pyx/self._N\n        return ep\n\n    def _convergence(self):  # \xe5\x88\xa4\xe6\x96\xad\xe6\x98\xaf\xe5\x90\xa6\xe5\x85\xa8\xe9\x83\xa8\xe6\x94\xb6\xe6\x95\x9b\n        for last, now in zip(self._lastw, self._w):\n            if abs(last - now) >= self._EPS:\n                return False\n        return True\n\n    def predict(self, X):   # \xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa6\x82\xe7\x8e\x87\n        Z = self._Zx(X)\n        result = {}\n        for y in self._Y:\n            ss = 0\n            for x in X:\n                if (x, y) in self._numXY:\n                    ss += self._w[self._xyID[(x, y)]]\n            pyx = math.exp(ss)/Z\n            result[y] = pyx\n        return result\n\n    def train(self, maxiter=1000):   # \xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\n        for loop in range(maxiter):  # \xe6\x9c\x80\xe5\xa4\xa7\xe8\xae\xad\xe7\xbb\x83\xe6\xac\xa1\xe6\x95\xb0\n            print(""iter:%d"" % loop)\n            self._lastw = self._w[:]\n            for i in range(self._n):\n                ep = self._model_ep(i)    # \xe8\xae\xa1\xe7\xae\x97\xe7\xac\xaci\xe4\xb8\xaa\xe7\x89\xb9\xe5\xbe\x81\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x9c\x9f\xe6\x9c\x9b\n                self._w[i] += math.log(self._Ep_[i]/ep)/self._C   # \xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\n            print(""w:"", self._w)\n            if self._convergence():  # \xe5\x88\xa4\xe6\x96\xad\xe6\x98\xaf\xe5\x90\xa6\xe6\x94\xb6\xe6\x95\x9b\n                break\n\n\ndataset = [[\'no\', \'sunny\', \'hot\', \'high\', \'FALSE\'],\n           [\'no\', \'sunny\', \'hot\', \'high\', \'TRUE\'],\n           [\'yes\', \'overcast\', \'hot\', \'high\', \'FALSE\'],\n           [\'yes\', \'rainy\', \'mild\', \'high\', \'FALSE\'],\n           [\'yes\', \'rainy\', \'cool\', \'normal\', \'FALSE\'],\n           [\'no\', \'rainy\', \'cool\', \'normal\', \'TRUE\'],\n           [\'yes\', \'overcast\', \'cool\', \'normal\', \'TRUE\'],\n           [\'no\', \'sunny\', \'mild\', \'high\', \'FALSE\'],\n           [\'yes\', \'sunny\', \'cool\', \'normal\', \'FALSE\'],\n           [\'yes\', \'rainy\', \'mild\', \'normal\', \'FALSE\'],\n           [\'yes\', \'sunny\', \'mild\', \'normal\', \'TRUE\'],\n           [\'yes\', \'overcast\', \'mild\', \'high\', \'TRUE\'],\n           [\'yes\', \'overcast\', \'hot\', \'normal\', \'FALSE\'],\n           [\'no\', \'rainy\', \'mild\', \'high\', \'TRUE\']]\n\nmaxent = MaxEntropy()\nx = [\'overcast\', \'mild\', \'high\', \'FALSE\']\nmaxent.loadData(dataset)\nmaxent.train()\nprint(\'predict:\', maxent.predict(x))\n'"
books/PRML/PRML-master-Python/prml/__init__.py,0,"b'from prml import (\n    bayesnet,\n    clustering,\n    dimreduction,\n    kernel,\n    linear,\n    markov,\n    nn,\n    rv,\n    sampling\n)\n\n\n__all__ = [\n    ""bayesnet"",\n    ""clustering"",\n    ""dimreduction"",\n    ""kernel"",\n    ""linear"",\n    ""markov"",\n    ""nn"",\n    ""rv"",\n    ""sampling""\n]\n'"
books/PRML/PRML-master-Python/test/__init__.py,0,b''
books/李航-统计学习/machine_learning_algorithm-master/AdaBoost/AdaBoost.py,0,"b'""""""\n    @ jetou\n    @ AdaBoost algorithm\n    @ date 2017 11 19\n\n""""""\nfrom weaker_classifier import *\nimport math\n\nclass adaBoost:\n    def __init__(self, feature, label, Epsilon = 0):\n        self.feature = np.array(feature)\n        self.label   = np.array(label)\n        self.Epsilon = Epsilon\n        self.N       = self.feature.shape[0]\n        self.error   = 1\n        self.count_error = 1\n\n        self.alpha = []\n        self.classifier = []\n        self.W = [1.0 / self.N for i in range(self.N)]\n\n\n    def sign(self, value):\n        if value > 0:\n            value = 1\n        elif value < 0:\n            value = -1\n        else:\n            value = 0\n\n        return value\n\n    def update_W_(self):\n        update_W = []\n        z = 0\n        for i in range(self.N):\n            pe = np.array([self.feature[i]])\n            z += self.W[i] * math.exp(-1 * self.alpha[-1] * self.label[i] * self.classifier[-1].prediction(pe))\n\n        for i in range(self.N):\n            kk = np.array([self.feature[i]])\n            w = self.W[i] * math.exp(-1 * self.alpha[-1] * self.label[i] * self.classifier[-1].prediction(kk)) / z\n            update_W.append(w)\n        self.W = update_W\n\n    def __alpha__(self):\n       self.alpha.append(math.log((1-self.error)/self.error)/2)\n\n\n    def prediction(self, label):\n        finaly_prediction = []\n        classifier_offset = len(self.classifier)\n\n        for i in range(len(label)):\n            result = 0\n            for j in range(classifier_offset):\n                pe = np.array([label[i]])\n                result += self.alpha[j] * self.classifier[j].prediction(pe)\n            finaly_prediction.append(self.sign(result))\n\n        return finaly_prediction\n\n    def complute_error(self):\n        # compute error\n        result = self.prediction(self.feature)\n        count_error = 0\n        for i in range(self.N):\n            if result[i] * self.label[i] < 0:\n                count_error+=1\n        self.count_error = count_error / (self.N * 1.0)  #compute error%\n\n\n\n    def train(self):\n        while(self.count_error > self.Epsilon):\n            classifier = weake_classifier(self.feature, self.label, self.W)\n            self.classifier.append(classifier)\n            classifier.train()\n            self.error, self.W, dem = classifier.get_information()\n            self.__alpha__()\n            self.update_W_()\n            self.complute_error()\n\n\n'"
books/李航-统计学习/machine_learning_algorithm-master/AdaBoost/AdaBoost_test.py,0,"b'from AdaBoost import *\n\n\nfeature = np.array([\n    [0],\n    [1],\n    [2],\n    [3],\n    [4],\n    [5],\n    [6],\n    [7],\n    [8],\n    [9],\n])\n\nlabel = np.array([\n    [1],\n    [1],\n    [1],\n    [-1],\n    [-1],\n    [-1],\n    [1],\n    [1],\n    [1],\n    [-1],\n])\n\ntest = np.array([\n    [2],\n    [2],\n    [6],\n    [4],\n])\n\na = adaBoost(feature, label)\na.train()\nprint a.prediction(test)'"
books/李航-统计学习/machine_learning_algorithm-master/AdaBoost/weaker_classifier.py,0,"b'""""""\n    @ jetou\n    @ weaker_classifier algorithm\n    @ date 2017 11 19\n\n""""""\nimport numpy as np\nclass weake_classifier:\n    def __init__(self, feature, label, W = None):\n        self.feature = np.array(feature)\n        self.label   = np.array(label)\n\n        self.setlable = np.unique(label)\n        self.feature_dem = self.feature.shape[1]\n        self.N = self.feature.shape[0]\n\n        if W != None:\n            self.W = np.array(W)\n        else:\n            self.W = [1.0 / self.N for i in range(self.N)]\n\n\n    def prediction(self, feature):\n        test_feature = np.array(feature)\n        output = np.ones((test_feature.shape[0],1))\n        output[test_feature[:, self.demention] * self.finaly_label < self.threshold * self.finaly_label] = -1\n\n\n        return output\n\n    def __str__(self):\n        string  = ""opt_threshold:"" + str(self.threshold)    + ""\\n""\n        string += ""opt_demention:"" + str(self.demention)    + ""\\n""\n        string += ""opt_errorRate:"" + str(self.error)        + ""\\n""\n        string += ""opt_label    :"" + str(self.finaly_label) + ""\\n""\n        string += ""weights      :"" + str(self.W)            + ""\\n""\n\n        return string\n\n    def best_along_dem(self, demention, label):\n        feature_max = np.max(self.feature)\n        feature_min = np.min(self.feature)\n        step = (feature_max - feature_min) / (self.N * 1.0)\n        min_error = self.N * 1.0\n\n        for i in np.arange(feature_min, feature_max, step):\n            output = np.ones((self.N, 1))\n            output[self.feature[:, demention] * label < i * label] = -1\n\n            errorRate = 0.0\n            for j in range(self.N):\n                if output[j] * self.label[j] < 0:\n                    errorRate += self.W[j]\n\n            if errorRate < min_error:\n                min_error = errorRate\n                threshold = i\n\n        return  threshold, min_error\n\n    def train(self):\n        self.error = self.N * 1.0\n        for demention in range(self.feature_dem):\n            for label in self.label:\n                threshold, err = self.best_along_dem(demention, label)\n                if self.error > err:\n                    self.error = err\n                    self.finaly_label = label\n                    self.threshold = threshold\n                    self.demention = demention\n\n    def get_information(self):\n        return self.error, self.W, self.demention\n'"
books/李航-统计学习/machine_learning_algorithm-master/AdaBoost/weaker_test.py,0,"b'from weaker_classifier import *\n\nfeature = np.array([\n    [0],\n    [1],\n    [2],\n    [3],\n    [4],\n    [5],\n    [6],\n    [7],\n    [8],\n    [9],\n])\n\nlabel = np.array([\n    [1],\n    [1],\n    [1],\n    [-1],\n    [-1],\n    [-1],\n    [1],\n    [1],\n    [1],\n    [-1],\n])\n\ntest = np.array([\n    [2],\n])\n\nd=np.array([\n    [0.007143],\n    [0.07143],\n    [0.07143],\n    [0.07143],\n    [0.07143],\n    [0.07143],\n    [0.16667],\n    [0.16667],\n    [0.16667],\n    [0.07143],\n])\npp = []\na = weake_classifier(feature, label)\nb = weake_classifier(feature, label,d)\na.train()\nprint a.__str__()\nprint a.prediction(test)\nb.train()\nprint b.__str__()\n'"
books/李航-统计学习/machine_learning_algorithm-master/EM/Gmm.py,0,"b'# -*- coding: utf-8 -*-\n\n""""""\n    @ jetou\n    @ Gaussian misture model\n    @ date 2017 11 27\n\n""""""\n# Reference http://blog.csdn.net/jinping_shi/article/details/59613054\n\n\nimport numpy as np\nimport math\nimport copy\n\nclass EmGMM:\n    def __init__(self, sigma, k, N, MU, epsilon):\n        """"""\n        k is the number of Gaussian distribution\n        N is the number of feature\n        sigma is variance\n        """"""\n        self.k = k\n        self.N = N\n        self.epsilon = epsilon\n        self.sigma = sigma\n        self.MU = np.matrix(MU)\n        self.alpha = [0.5, 0.5]\n\n    def init_data(self):\n        self.X = np.matrix(np.zeros((self.N, 2)))\n        self.Mu = np.random.random(self.k)\n        self.Expectations = np.zeros((self.N, self.k))\n        for i in xrange(self.N):\n            if np.random.random(1) > 0.5:\n                self.X[i,:] = np.random.multivariate_normal(self.MU.tolist()[0], self.sigma, 1)\n            else:\n                self.X[i,:] = np.random.multivariate_normal(self.MU.tolist()[1], self.sigma, 1)\n\n    def e_step(self):\n        for i in range(self.N):\n            Denom = 0\n            Numer = [0.0] * self.k\n            for j in range (self.k):\n                Numer[j] = self.alpha[j] * math.exp(-(self.X[i,:] - self.MU[j,:]) * self.sigma.I * np.transpose(self.X[i,:] - self.MU[j,:])) \\\n                           / np.sqrt(np.linalg.det(self.sigma))\n                Denom += Numer[j]\n            for j in range(0, self.k):\n                self.Expectations[i, j] = Numer[j] / Denom\n\n    def m_step(self):\n        for j in xrange(0, self.k):\n            Numer = 0\n            Denom = 0\n            sabi = 0\n            for i in xrange(0, self.N):\n                Numer += self.Expectations[i, j] * self.X[i, :]\n                Denom += self.Expectations[i, j]\n            self.MU[j, :] = Numer / Denom\n            self.alpha[j] = Denom / self.N\n            for i in xrange(0, self.N):\n                sabi += self.Expectations[i, j] * np.square((self.X[i, :] - self.MU[j, :]))\n            self.sigma[j, :]=  sabi / Denom\n\n    def train(self, inter=1000):\n        self.init_data()\n        for i in range(inter):\n            error = 0\n            err_alpha = 0\n            err_sigma = 0\n            old_mu = copy.deepcopy(self.MU)\n            old_alpha = copy.deepcopy(self.alpha)\n            old_sigma = copy.deepcopy(self.sigma)\n            self.e_step()\n            self.m_step()\n            print ""The number of iterations"", i\n            print ""Location parameters: mu"", self.MU\n            print ""variance: sigma"", self.sigma\n            print ""Selected probability: alpha"", self.alpha\n            for j in range(self.k):\n                error += (abs(old_mu[j, 0] - self.MU[j, 0]) + abs(old_mu[j, 1] - self.MU[j, 1]))\n                err_sigma += (abs(old_sigma[j, 0] - self.sigma[j, 0]) + abs(old_sigma[j, 1] - self.sigma[j, 1]))\n                err_alpha += abs(old_alpha[j] - self.alpha[j])\n            if (error <= self.epsilon) and (err_sigma <= self.epsilon) and (err_alpha <= self.epsilon):\n                print error, err_alpha, err_sigma\n                break\n\n'"
books/李航-统计学习/machine_learning_algorithm-master/EM/gmm_test.py,0,"b'from Gmm import *\n\nsigma = np.matrix([[30, 0], [0, 30]])\nMU = [[40, 20], [5, 35]]\na = EmGMM(sigma, 2, 1000, MU, 0.001)\n\na.train()'"
books/李航-统计学习/machine_learning_algorithm-master/decision_tree/cart_Classification_tree.py,0,"b'""""""\n    @ jetou\n    @ cart decision_tree\n    @ date 2017 10 29\n\n""""""\nimport numpy as np\n\nclass tree:\n    def __init__(self, feature, label):\n        self.feature = feature\n        self.label = label\n\n    def Gini(self, dataset):\n\n        data_set = set(dataset)\n        sum = 0.0\n        data_length = dataset.shape[0] * 1.0\n        for i in data_set:\n            sum += (np.count_nonzero(dataset == i) / data_length) ** 2\n\n        return 1.0 - sum\n\n    def cmpgini(self, feature, label):\n        label = np.array(label).flatten(1)\n        unique = set(feature)\n        min = 1\n        count_feature = np.shape(feature)[0]\n        for i in unique:\n            c1 = np.count_nonzero(feature == i) * self.Gini(label[feature == i]) / count_feature\n            c2 = np.count_nonzero(feature != i) * self.Gini(label[feature != i]) / count_feature\n            if min > (c1 + c2):\n                min = c1 + c2\n                result = i\n        return result, min\n\n    def maketree(self, feature, label):\n        label = label.flatten(1)\n        train_feature = feature.transpose()\n\n        min = 1.0\n        opt_feature = 0\n        opt_feature_val = 0\n\n        if np.unique(label).size == 1:\n            return label[0]\n\n        for i in range(len(train_feature)):\n            result, p = self.cmpgini(train_feature[i], label)\n            if p < min:\n                min = p\n                opt_feature = i\n                opt_feature_val = result\n\n        if min == 1.0:\n            return label\n\n        left = []\n        right = []\n\n        left = self.maketree(train_feature.transpose()[train_feature.transpose()[:, opt_feature] != opt_feature_val],\n                        label[train_feature.transpose()[:, opt_feature] != opt_feature_val])\n        #\n        right = self.maketree(train_feature.transpose()[train_feature.transpose()[:, opt_feature] == opt_feature_val],\n                         label[train_feature.transpose()[:, opt_feature] == opt_feature_val])\n\n\n        return [(opt_feature, opt_feature_val), left, right]\n\n    def train(self):\n        self.train_result = self.maketree(self.feature, self.label)\n\n    def prediction(self, Mat):\n        Mat = np.array(Mat).transpose()\n        result = np.zeros((Mat.shape[0], 1))\n        for i in range(Mat.shape[0]):\n            tree = self.train_result\n            while self.isLeaf(tree) == False:\n                feature, val = tree[0]\n                if Mat[i][feature] == val:\n\n                    tree = self.getRight(tree)\n                else:\n                    tree = self.getLeft(tree)\n\n            result[i] = tree\n\n        return result\n\n    def isLeaf(self, tree):\n        if isinstance(tree, list):\n            return False\n        else:\n            return True\n\n    def getLeft(self, tree):\n        assert isinstance(tree, list)\n        return tree[1]\n\n    def getRight(self, tree):\n        assert isinstance(tree, list)\n        return tree[2]\n\n\n\n\n'"
books/李航-统计学习/machine_learning_algorithm-master/decision_tree/test.py,0,"b""import numpy\nfrom cart_Classification_tree import *\ntrain_feature = np.array([\n['teenager',        'no',   'no',   0.0],\n['teenager',        'no',   'no',   1.0],\n['teenager',        'yes',  'no',   1.0],\n['teenager',        'yes',  'yes',  0.0],\n['teenager',        'no',   'no',   0.0],\n['senior citizen',  'no',   'no',   0.0],\n['senior citizen',  'no',   'no',   1.0],\n['senior citizen',  'yes',  'yes',  1.0],\n['senior citizen',  'no',   'yes',  2.0],\n['senior citizen',  'no',   'yes',  2.0],\n['old pepple',      'no',   'yes',  2.0],\n['old pepple',      'no',   'yes',  1.0],\n['old pepple',      'yes',  'no',   1.0],\n['old pepple',      'yes',  'no',   2.0],\n['old pepple',      'no',   'no',   0.0],\n])\n\nTag = np.array([\n[-1],\n[-1],\n[+1],\n[+1],\n[-1],\n[-1],\n[-1],\n[+1],\n[+1],\n[+1],\n[+1],\n[+1],\n[+1],\n[+1],\n[-1],\n]).transpose()\n\na = tree(train_feature, Tag)\na.train()\ns = a.prediction(np.array([\n    ['teenager',        'no',   'no',   0],\n    ['old pepple',      'yes',  'no',   2],\n    ]).transpose())\nprint s"""
books/李航-统计学习/machine_learning_algorithm-master/k_nearest_neighbor/knn_kdtree.py,0,"b'import numpy as np\n\n\nimport numpy as np\n\nclass Node:\n    def __init__(self, data, lchild=None, rchild=None):\n        self.data = data\n        self.lchild = lchild\n        self.rchild = rchild\n\n    def sort(self, dataSet, axis):\n        sortDataSet = dataSet[:]\n        m, n = np.shape(sortDataSet)\n        for i in range(m):\n            for j in range(0, m - i - 1):\n                if (sortDataSet[j][axis] > sortDataSet[j + 1][axis]):\n                    temp = sortDataSet[j]\n                    sortDataSet[j] = sortDataSet[j + 1]\n                    sortDataSet[j + 1] = temp\n        return sortDataSet\n\n    def create(self, dataSet, depth):\n        if len(dataSet) > 0:\n            m, n = np.shape(dataSet)\n            midIndex = len(dataSet)/2\n            axis = depth % n\n            sortedDataSet = self.sort(dataSet, axis)\n            node = Node(sortedDataSet[midIndex])\n\n            leftDataSet = sortedDataSet[: midIndex]\n            rightDataSet = sortedDataSet[midIndex + 1:]\n            node.lchild = self.create(leftDataSet, depth + 1)\n            node.rchild = self.create(rightDataSet, depth + 1)\n            return node\n        else:\n            return None\n\n    def preOrder(self, node):\n        if node != None:\n            self.preOrder(node.lchild)\n            self.preOrder(node.rchild)\n\n    def search(self, tree, x):\n        self.nearestPoint = None\n        self.nearestValue = 0\n\n        def travel(node, depth=0):\n            if node != None:\n                n = len(x)\n                axis = depth % n\n                if x[axis] < node.data[axis]:\n                    travel(node.lchild, depth + 1)\n                else:\n                    travel(node.rchild, depth + 1)\n\n\n                distNodeAndX = self.dist(x, node.data)\n                if (self.nearestPoint == None):\n                    self.nearestPoint = node.data\n                    self.nearestValue = distNodeAndX\n                elif (self.nearestValue > distNodeAndX):\n                    self.nearestPoint = node.data\n                    self.nearestValue = distNodeAndX\n\n                if (abs(x[axis] - node.data[axis]) <= self.nearestValue):\n                    if x[axis] < node.data[axis]:\n                        travel(node.rchild, depth + 1)\n                    else:\n                        travel(node.lchild, depth + 1)\n\n        travel(tree)\n        return self.nearestPoint\n\n    def dist(self, x1, x2):\n        return ((np.array(x1) - np.array(x2)) ** 2).sum() ** 0.5\n'"
books/李航-统计学习/machine_learning_algorithm-master/k_nearest_neighbor/test.py,0,"b'import numpy as np\nfrom knn_kdtree import *\n\ndataSet = [[2, 3],\n           [5, 4],\n           [9, 6],\n           [4, 7],\n           [8, 1],\n           [7, 2]]\nx = [3, 4.5]\nkdtree = Node(dataSet)\ntree = kdtree.create(dataSet,0)\nkdtree.preOrder(tree)\nprint kdtree.search(tree, x)'"
books/李航-统计学习/machine_learning_algorithm-master/logistic_regression/logistic_regression.py,0,"b'""""""\n    @ jetou\n    @ logistic_regression binary\n    @ date 2017 11 03\n\n""""""\n\nimport numpy as np\nimport random\nimport math\n\nclass LogisticRegression:\n    def __init__(self, feature, label, step=0.4, max_iteration=5000):\n        self.feature = np.array(feature).transpose()\n        self.label = np.array(label).transpose()\n        self.learning_step = step\n        self.max_iteration = max_iteration\n\n    def compute(self, x):\n        wx = sum([self.w[j] * x[j] for j in xrange(self.w.shape[0])])\n        exp_wx = math.exp(wx)\n\n        p1 = exp_wx / (1 + exp_wx)\n        p0 = 1 / (1 + exp_wx)\n\n        if p1 > p0:\n            return 1\n        else:\n            return 0\n\n    def train(self):\n        self.w = np.zeros((self.feature.shape[0],1))\n\n        correct = 0\n        time = 0\n\n        while time < self.max_iteration:\n            index = random.randint(0, self.label.shape[1] - 1)\n            x = list(self.feature[:,index])\n            x.append(1.0)\n            y = self.label[:,index]\n\n            if y == self.compute(x):\n                correct += 1\n                if correct > self.max_iteration:\n                    break\n                continue\n\n            time+=1\n            correct = 0\n\n            wx = sum([self.w[j] * x[j] for j in xrange(self.w.shape[0])])\n            exp_wx = math.exp(wx)\n\n            for i in xrange(self.w.shape[0]):\n                self.w[i] -= self.learning_step * (-y * x[i] + float(x[i] * exp_wx) / float(1 + exp_wx))\n\n    def prediction(self,features):\n        labels = []\n\n        for feature in features:\n            x = list(feature)\n            x.append(1)\n            labels.append(self.compute(x))\n\n        return labels\n\n\n\n'"
books/李航-统计学习/machine_learning_algorithm-master/logistic_regression/test.py,0,"b'import numpy as np\nfrom logistic_regression import *\n\n\n\nfeature = np.array([\n    [3,3],\n    [4,3],\n    [1,1],\n])\n\nlabel = np.array([\n    [1],\n    [1],\n    [0],\n])\n\ntest_point = np.array([\n    [+5,+5],\n    [-1,-1],\n])\n\na = LogisticRegression(feature, label)\na.train()\nf = a.prediction(test_point)\nprint  f'"
books/李航-统计学习/machine_learning_algorithm-master/naive_bayes/naive_bayes.py,0,"b'""""""\n    @ jetou\n    @ cart decision_tree\n    @ date 2017 10 31\n\n""""""\n\nimport numpy as np\n\nclass naive_bayes:\n    def __init__(self, feature, label):\n        self.feature = feature.transpose()\n        self.label = label.transpose().flatten(1)\n        self.positive = np.count_nonzero(self.label == 1) * 1.0\n        self.negative = np.count_nonzero(self.label == -1) * 1.0\n\n    def train(self):\n        positive_dict = {}\n        negative_dict = {}\n        for i in self.feature:\n            unqiue = set(i)\n            for j in unqiue:\n                positive_dict[j] = np.count_nonzero(self.label[i==j]==1) / self.positive\n                negative_dict[j] = np.count_nonzero(self.label[i==j]==-1) / self.negative\n\n        return positive_dict, negative_dict\n\n    def prediction(self,  pre_feature):\n\n        positive_chance = self.positive / self.label.shape[0]\n        negative_chance = self.negative / self.label.shape[0]\n        positive_dict, negative_dict = self.train()\n        for i in pre_feature:\n            i = str(i)\n            positive_chance *= positive_dict[i]\n            negative_chance *= negative_dict[i]\n\n        if positive_chance > negative_chance:\n            return 1\n        else:\n            return -1\n\n'"
books/李航-统计学习/machine_learning_algorithm-master/naive_bayes/test.py,0,"b""import numpy as np\nfrom naive_bayes import *\n\nfeature = np.array([\n    [1, 'S'],\n    [1, 'M'],\n    [1, 'M'],\n    [1, 'S'],\n    [1, 'S'],\n    [2, 'S'],\n    [2, 'M'],\n    [2, 'M'],\n    [2, 'L'],\n    [2, 'L'],\n    [3, 'L'],\n    [3, 'M'],\n    [3, 'M'],\n    [3, 'L'],\n    [3, 'L'],\n])\n\nlabel = np.array([\n    [-1],\n    [-1],\n    [1],\n    [1],\n    [-1],\n    [-1],\n    [-1],\n    [1],\n    [1],\n    [1],\n    [1],\n    [1],\n    [1],\n    [1],\n    [-1],\n])\n\na = naive_bayes(feature, label)\nprint a.prediction([2,'S'])\n\n"""
books/李航-统计学习/machine_learning_algorithm-master/perceptron/perceptron.py,0,"b'""""""\n    @ jetou\n    @ perceptron Duality\n    @ date 2017 11 01\n\n""""""\nimport numpy as np\n\nclass perceptron:\n    def __init__(self, feature, label, gama=1):\n        self.feature = feature.transpose()\n        self.label = label.transpose()\n        self.__feature = feature\n\n        self.row = self.feature.shape[0]\n        self.col = self.feature.shape[1]\n\n        self.alpha = [0] * self.col\n        self.b = 0\n        self.gama = gama\n\n    def gram(self): # compute gram matrix\n        self.gram_mat = np.empty(shape=(self.col, self.col))\n        for i in range(self.col):\n            for j in range(self.col):\n                self.gram_mat[i][j] = self.inner(self.__feature[i], self.__feature[j])\n\n    def inner(self, a, b):\n        result = a[0] * b[0] + a[1] * b[1]\n        return result\n\n    def misinterpreted(self, yi, index):\n        pa1 = self.alpha * self.label\n        pa2 = self.gram_mat[:,index]\n        num = yi * (np.dot(pa1, pa2) + self.b)\n        return num\n\n    def fit(self, alpha, b):\n        label = self.label.flatten(1)\n        for i in range(self.col):\n            while self.misinterpreted(label[i],i) <= 0:\n                self.alpha[i] += self.gama\n                self.b += self.gama * label[i]\n        if alpha!=self.alpha or b != self.b: #To prevent the front of a bit not fit\n            self.fit(self.alpha, self.b)\n        return self.alpha\n\n    def train(self):\n        self.gram()\n        self.w = sum((self.fit(1,2)*self.feature*self.label).transpose()) #self.fit(x,y) Any two numbers\n\n    def dot_prediction(self, A, B):\n        assert len(A) == len(B)\n        summer = 0\n        for i in range(len(A)):\n            summer += A[i] * B[i]\n\n        return summer\n\n    def prediction(self, feature):\n        Mat = np.array(feature).transpose()\n        col = Mat.shape[1]\n\n        output = []\n        for i in range(col):\n            if self.dot_prediction(self.w, Mat[:,i]) + self.b > 0:\n                output.append(+1)\n            else:\n                output.append(-1)\n\n        return output\n\n    def get_wandb(self): # plot w b\n        return self.w, self.b\n\n\n\n\n\n'"
books/李航-统计学习/machine_learning_algorithm-master/perceptron/test.py,0,"b""import numpy as np\nfrom perceptron import *\nimport matplotlib.pyplot as plt\n\nfeature = np.array([\n    [3,3],\n    [4,3],\n    [1,1],\n])\n\nlabel = np.array([\n    [1],\n    [1],\n    [-1],\n])\n\na = perceptron(feature, label)\na.train()\n\ntest_point = np.array([\n    [+5,+5],\n    [-1,-1]])\nprint a.prediction(test_point)\n\nw,b = a.get_wandb()\n\n\n#########plot##############################\ndef plot_function(samples, labels, w, b):\n    index = 0\n    for i in samples:\n        if labels[index] == 1:\n            s = 'x'\n        else:\n            s = 'o'\n        plt.scatter(i[0], i[1], marker=s)\n        index += 1\n    xData = np.linspace(0, 5, 100)\n    yData = (-b - w[0] * xData)/pow(w[1],2)\n    plt.plot(xData, yData, color='r', label='sample data')\n\n    plt.show()\n\nplot_function(feature, label, w, b)\nplot_function(test_point, a.prediction(test_point), w, b)"""
books/李航-统计学习/machine_learning_algorithm-master/support_vector_machine/dual_algorithm_test.py,0,"b'import numpy as np\nfrom support_vector_machine import *\n\n\n\nx = np.array([\n    [3,3],\n    [4,3],\n    [1,1],\n    [7,7],\n    [10,10],\n])\n\ny = np.array([\n    [1],\n    [1],\n    [-1],\n    [1],\n    [1],\n])\n\n\ntest = np.array([\n    [1,2],\n    [7,6],\n    [-1,3],\n])\n\na = supprot_vector_machine(x, y)\na.fit()\nprint a.prediction(x)\nprint a.prediction(test)'"
books/李航-统计学习/machine_learning_algorithm-master/support_vector_machine/support_vector_machine.py,0,"b'""""""\n    @ jetou\n    @ support_vector_machine dual algorithm\n    @ date 2017 11 12\n\n""""""\n\n\nimport numpy as np\nfrom sympy import *\n\n\nclass supprot_vector_machine:\n    def __init__(self, feature, label):\n        self.feature = np.array(feature)\n        self.label =  np.array(label)\n        self.N = len(self.feature)\n        self.n = -1\n        self.alpha = symbols([""alpha"" + str(i) for i in range(1, self.N + 1)])\n        self.solution = []\n\n    #train\n    def transvection(self, a,b): # compute inner product\n        result = 0 # conpute inner product function most used gram marix\n        offset = len(a)\n        for i in range(offset):\n            result += a[i] * b[i]\n        return result\n\n    def eval_function(self): # compute Master equation\n        c = 0\n        s = 0 #Temporary variable\n        offset = self.N\n        for i in range(offset):\n            s += self.alpha[i]\n            for j in range(offset):\n                c += self.alpha[i] * self.alpha[j] * self.label[i][0] * self.label[j][0] * self.transvection(self.feature[i], self.feature[j])\n        self.equation = c/2 - s\n\n    def replace_x(self): # add the constraints use alpha * y  and subject to [alpha1 + alpha2 + ... alphaN] = 0\n        self.n+=1\n        alpha_ = self.alpha[:]\n        for i in range(len(alpha_)):\n            alpha_[i] = alpha_[i] * self.label[i][0]\n        equation = Eq(sum(alpha_),0)\n        self.solve_equation =  solve([equation], self.alpha[0])\n        self.equation = self.equation.replace(self.solve_equation.keys()[0],\n                                              self.solve_equation.get(self.solve_equation.keys()[0]))\n\n    def derivative(self): # conpute derivative\n        self.surplus_alpha = self.alpha[:]\n        self.equation_list = []\n        self.surplus_alpha.remove(self.solve_equation.keys()[0])\n        for i in self.surplus_alpha:\n            self.equation_list.append(diff(self.equation, i))\n        self.solution = solve(self.equation_list)\n\n    def boundary(self): # If the constraint conditions are violated\n        min = 10000\n        self.surplus_alpha.reverse()\n\n        s = solve(self.replace_model(self.equation_list[:]))\n\n        finaly = []\n        for i in range(len(s)):\n            m = self.equation\n            for j in range(len(s)):\n                if j!=i:\n                    m = m.replace(symbols(""alpha""+str(j+2)), 0)\n            finaly.append(m)\n\n        for i in range(len(s)):\n            K = Eq(symbols(""pp""), finaly[i])\n            sad = solve([K, Eq(symbols(""alpha"" + str(i+2)), s.get(symbols(""alpha"" + str(i+2))))])[0].get(symbols(""pp""))\n            if min > sad:\n                result = {}\n                min = sad\n                result[symbols(""alpha"" + str(i+2))] = s.get(symbols(""alpha"" + str(i+2)))\n\n        for i in range(2, len(s)+2):\n            if symbols(""alpha""+str(i)) not in result.keys():\n                result[symbols(""alpha""+str(i))] = 0\n\n        self.solution = result\n\n    # auxiliary function\n    def replace_model(self, lists):\n        bound = lists\n        k=len(bound)\n        for i in range(k):\n            for j in range(k):\n                if j!=i:\n                    bound[i] = bound[i].replace(symbols(""alpha""+str(j+2)), 0)\n        return bound\n\n    def get_origin(self, source):\n        value_list = [Eq(i, source.get(i)) for i in source.keys()]\n        value_list.append(Eq(self.solve_equation.keys()[0], self.solve_equation.get(self.solve_equation.keys()[0])))\n        return solve(value_list)\n\n    #enter\n    def fit(self):\n        self.eval_function()\n        self.replace_x()\n        self.derivative()\n        for i in self.solution:\n            if (self.solution.get(i)) < 0:\n                self.boundary()\n                break\n        if self.solution == []:\n            self.boundary()\n        self.solution = self.get_origin(self.solution)\n\n        final_alpha = [self.solution.get(self.alpha[i]) for i in range(len(self.solution))]\n        final_alpha = [[i] for i in final_alpha]\n\n\n        #self.w\n        self.w = sum(final_alpha *self.feature*self.label)\n\n        #self.b\n        for i in range(len(final_alpha)): #The presence of subscript j makes alpha_j>0\n            if final_alpha[i] != 0:\n                alpha_j = i\n                break\n        kk = []\n        for i in range(self.N):\n            kk.append(self.transvection(self.feature[i], self.feature[alpha_j]))\n\n        self.b = [final_alpha[i] * self.label[i] * kk[i] for i in range(self.N)]\n        self.b = self.label[alpha_j] - sum(self.b)\n\n    #prediction\n    def prediction(self, feature):\n        Mat = np.array(feature).transpose()\n        col = Mat.shape[1]\n\n        output = []\n        for i in range(col):\n            if sum(Mat[:,i] * self.w) + self.b > 0:\n                output.append(+1)\n            else:\n                output.append(-1)\n\n        return output\n\n\n\n\n\n\n\n\n\n\n\n'"
books/PRML/PRML-master-Python/prml/bayesnet/__init__.py,0,"b'from prml.bayesnet.discrete import discrete, DiscreteVariable\n\n\n__all__ = [\n    ""DiscreteVariable"",\n    ""discrete""\n]\n'"
books/PRML/PRML-master-Python/prml/bayesnet/discrete.py,0,"b'import numpy as np\nfrom prml.bayesnet.probability_function import ProbabilityFunction\nfrom prml.bayesnet.random_variable import RandomVariable\n\n\nclass DiscreteVariable(RandomVariable):\n    """"""\n    Discrete random variable\n    """"""\n\n    def __init__(self, n_class:int):\n        """"""\n        intialize a discrete random variable\n\n        parameters\n        ----------\n        n_class : int\n            number of classes\n\n        Attributes\n        ----------\n        parent : DiscreteProbability, optional\n            parent node this variable came out from\n        message_from : dict\n            dictionary of message from neighbor node and itself\n        child : list of DiscreteProbability\n            probability function this variable is conditioning\n        proba : np.ndarray\n            current estimate\n        """"""\n        self.n_class = n_class\n        self.parent = []\n        self.message_from = {self: np.ones(n_class)}\n        self.child = []\n        self.is_observed = False\n\n    def __repr__(self):\n        string = f""DiscreteVariable(""\n        if self.is_observed:\n            string += f""observed={self.proba})""\n        else:\n            string += f""proba={self.proba})""\n        return string\n\n    def add_parent(self, parent):\n        self.parent.append(parent)\n\n    def add_child(self, child):\n        self.child.append(child)\n        self.message_from[child] = np.ones(self.n_class)\n\n    @property\n    def proba(self):\n        return self.posterior\n\n    def receive_message(self, message, giver, proprange):\n        self.message_from[giver] = message\n        self.summarize_message()\n        self.send_message(proprange, exclude=giver)\n\n    def summarize_message(self):\n        if self.is_observed:\n            self.prior = self.message_from[self]\n            self.likelihood = self.prior\n            self.posterior = self.prior\n            return\n\n        self.prior = np.ones(self.n_class)\n        for func in self.parent:\n            self.prior *= self.message_from[func]\n        self.prior /= np.sum(self.prior, keepdims=True)\n\n        self.likelihood = np.copy(self.message_from[self])\n        for func in self.child:\n            self.likelihood *= self.message_from[func]\n\n        self.posterior = self.prior * self.likelihood\n        self.posterior /= self.posterior.sum()\n\n    def send_message(self, proprange=-1, exclude=None):\n        for func in self.parent:\n            if func is not exclude:\n                func.receive_message(self.likelihood, self, proprange)\n        for func in self.child:\n            if func is not exclude:\n                func.receive_message(self.prior, self, proprange)\n\n    def observe(self, data:int, proprange=-1):\n        """"""\n        set observed data of this variable\n\n        Parameters\n        ----------\n        data : int\n            observed data of this variable\n            This must be smaller than n_class and must be non-negative\n        propagate : int, optional\n            Range to propagate the observation effect to the other random variable using belief propagation alg.\n            If proprange=1, the effect only propagate to the neighboring random variables.\n            Default is -1, which is infinite range.\n        """"""\n        assert(0 <= data < self.n_class)\n        self.is_observed = True\n        self.receive_message(np.eye(self.n_class)[data], self, proprange=proprange)\n\n\nclass DiscreteProbability(ProbabilityFunction):\n    """"""\n    Discrete probability function\n    """"""\n\n    def __init__(self, table, *condition, out=None, name=None):\n        """"""\n        initialize discrete probability function\n\n        Parameters\n        ----------\n        table : (K, ...) np.ndarray or array-like\n            probability table\n            If a discrete variable A is conditioned with B and C,\n            table[a,b,c] give probability of A=a when B=b and C=c.\n            Thus, the sum along the first axis should equal to 1.\n            If a table is 1 dimensional, the variable is not conditioned.\n        condition : tuple of DiscreteVariable, optional\n            parent node, discrete variable this function is conidtioned by\n            len(condition) should equal to (table.ndim - 1)\n            (Default is (), which means no condition)\n        out : DiscreteVariable or list of DiscreteVariable, optional\n            output of this discrete probability function\n            Default is None which construct a new output instance\n        name : str\n            name of this discrete probability function\n        """"""\n        self.table = np.asarray(table)\n        self.condition = condition\n        if condition:\n            for var in condition:\n                var.add_child(self)\n        self.message_from = {var: var.prior for var in condition}\n\n        if out is None:\n            self.out = [DiscreteVariable(len(table))]\n        elif isinstance(out, DiscreteVariable):\n            self.out = [out]\n        else:\n            self.out = out\n\n        for i, random_variable in enumerate(self.out):\n            random_variable.add_parent(self)\n            self.message_from[random_variable] = np.ones(np.size(self.table, i))\n\n        for random_variable in self.out:\n            self.send_message_to(random_variable, proprange=0)\n\n        self.name = name\n\n    def __repr__(self):\n        if self.name is not None:\n            return self.name\n        else:\n            return super().__repr__()\n\n    def receive_message(self, message, giver, proprange):\n        self.message_from[giver] = message\n        if proprange:\n            self.send_message(proprange, exclude=giver)\n\n    @staticmethod\n    def expand_dims(x, ndim, axis):\n        shape = [-1 if i == axis else 1 for i in range(ndim)]\n        return x.reshape(*shape)\n\n    def compute_message_to(self, destination):\n        proba = np.copy(self.table)\n        for i, random_variable in enumerate(self.out):\n            if random_variable is destination:\n                index = i\n                continue\n            message = self.message_from[random_variable]\n            proba *= self.expand_dims(message, proba.ndim, i)\n        for i, random_variable in enumerate(self.condition, len(self.out)):\n            if random_variable is destination:\n                index = i\n                continue\n            message = self.message_from[random_variable]\n            proba *= self.expand_dims(message, proba.ndim, i)\n        axis = list(range(proba.ndim))\n        axis.remove(index)\n        message = np.sum(proba, axis=tuple(axis))\n        message /= np.sum(message, keepdims=True)\n        return message\n\n    def send_message_to(self, destination, proprange=-1):\n        message = self.compute_message_to(destination)\n        destination.receive_message(message, self, proprange)\n\n    def send_message(self, proprange, exclude=None):\n        proprange = proprange - 1\n\n        for random_variable in self.out:\n            if random_variable is not exclude:\n                self.send_message_to(random_variable, proprange)\n\n        if proprange == 0: return\n\n        for random_variable in self.condition:\n            if random_variable is not exclude:\n                self.send_message_to(random_variable, proprange - 1)\n\n\ndef discrete(table, *condition, out=None, name=None):\n    """"""\n    discrete probability function\n\n    Parameters\n    ----------\n    table : (K, ...) np.ndarray or array-like\n        probability table\n        If a discrete variable A is conditioned with B and C,\n        table[a,b,c] give probability of A=a when B=b and C=c.\n        Thus, the sum along the first axis should equal to 1.\n        If a table is 1 dimensional, the variable is not conditioned.\n    condition : tuple of DiscreteVariable, optional\n        parent node, discrete variable this function is conidtioned by\n        len(condition) should equal to (table.ndim - 1)\n        (Default is (), which means no condition)\n    out : DiscreteVariable, optional\n        output of this discrete probability function\n        Default is None which construct a new output instance\n    name : str\n        name of the discrete probability function\n\n    Returns\n    -------\n    DiscreteVariable\n        output discrete random variable of discrete probability function\n    """"""\n    function = DiscreteProbability(table, *condition, out=out, name=name)\n    if len(function.out) == 1:\n        return function.out[0]\n    else:\n        return function.out\n'"
books/PRML/PRML-master-Python/prml/bayesnet/probability_function.py,0,b'class ProbabilityFunction(object):\n    pass\n'
books/PRML/PRML-master-Python/prml/bayesnet/random_variable.py,0,"b'class RandomVariable(object):\n    """"""\n    Base class for random variable\n    """"""\n'"
books/PRML/PRML-master-Python/prml/clustering/__init__.py,0,"b'from .k_means import KMeans\n\n\n__all__ = [\n    ""KMeans""\n]\n'"
books/PRML/PRML-master-Python/prml/clustering/k_means.py,0,"b'import numpy as np\nfrom scipy.spatial.distance import cdist\n\n\nclass KMeans(object):\n\n    def __init__(self, n_clusters):\n        self.n_clusters = n_clusters\n\n    def fit(self, X, iter_max=100):\n        """"""\n        perform k-means algorithm\n\n        Parameters\n        ----------\n        X : (sample_size, n_features) ndarray\n            input data\n        iter_max : int\n            maximum number of iterations\n\n        Returns\n        -------\n        centers : (n_clusters, n_features) ndarray\n            center of each cluster\n        """"""\n        I = np.eye(self.n_clusters)\n        centers = X[np.random.choice(len(X), self.n_clusters, replace=False)]\n        for _ in range(iter_max):\n            prev_centers = np.copy(centers)\n            D = cdist(X, centers)\n            cluster_index = np.argmin(D, axis=1)\n            cluster_index = I[cluster_index]\n            centers = np.sum(X[:, None, :] * cluster_index[:, :, None], axis=0) / np.sum(cluster_index, axis=0)[:, None]\n            if np.allclose(prev_centers, centers):\n                break\n        self.centers = centers\n\n    def predict(self, X):\n        """"""\n        calculate closest cluster center index\n\n        Parameters\n        ----------\n        X : (sample_size, n_features) ndarray\n            input data\n\n        Returns\n        -------\n        index : (sample_size,) ndarray\n            indicates which cluster they belong\n        """"""\n        D = cdist(X, self.centers)\n        return np.argmin(D, axis=1)\n'"
books/PRML/PRML-master-Python/prml/dimreduction/__init__.py,0,"b'from prml.dimreduction.autoencoder import Autoencoder\nfrom prml.dimreduction.bayesian_pca import BayesianPCA\nfrom prml.dimreduction.pca import PCA\n\n\n__all__ = [\n    ""Autoencoder"",\n    ""BayesianPCA"",\n    ""PCA"",\n]\n'"
books/PRML/PRML-master-Python/prml/dimreduction/autoencoder.py,0,"b'import numpy as np\nfrom prml import nn\n\n\nclass Autoencoder(nn.Network):\n\n    def __init__(self, *args):\n        self.n_unit = len(args)\n        super().__init__()\n        for i in range(self.n_unit - 1):\n            self.parameter[f""w_encode{i}""] = nn.Parameter(np.random.randn(args[i], args[i + 1]))\n            self.parameter[f""b_encode{i}""] = nn.Parameter(np.zeros(args[i + 1]))\n            self.parameter[f""w_decode{i}""] = nn.Parameter(np.random.randn(args[i + 1], args[i]))\n            self.parameter[f""b_decode{i}""] = nn.Parameter(np.zeros(args[i]))\n\n    def transform(self, x):\n        h = x\n        for i in range(self.n_unit - 1):\n            h = nn.tanh(h @ self.parameter[f""w_encode{i}""] + self.parameter[f""b_encode{i}""])\n        return h.value\n\n    def forward(self, x):\n        h = x\n        for i in range(self.n_unit - 1):\n            h = nn.tanh(h @ self.parameter[f""w_encode{i}""] + self.parameter[f""b_encode{i}""])\n        for i in range(self.n_unit - 2, 0, -1):\n            h = nn.tanh(h @ self.parameter[f""w_decode{i}""] + self.parameter[f""b_decode{i}""])\n        x_ = h @ self.parameter[""w_decode0""] + self.parameter[""b_decode0""]\n        self.px = nn.random.Gaussian(x_, 1., data=x)\n\n    def fit(self, x, n_iter=100, learning_rate=1e-3):\n        optimizer = nn.optimizer.Adam(self.parameter, learning_rate)\n        for _ in range(n_iter):\n            self.clear()\n            self.forward(x)\n            log_likelihood = self.log_pdf()\n            log_likelihood.backward()\n            optimizer.update()\n'"
books/PRML/PRML-master-Python/prml/dimreduction/bayesian_pca.py,0,"b'import numpy as np\nfrom prml.dimreduction.pca import PCA\n\n\nclass BayesianPCA(PCA):\n\n    def fit(self, X, iter_max=100, initial=""random""):\n        """"""\n        empirical bayes estimation of pca parameters\n\n        Parameters\n        ----------\n        X : (sample_size, n_features) ndarray\n            input data\n        iter_max : int\n            maximum number of em steps\n\n        Returns\n        -------\n        mean : (n_features,) ndarray\n            sample mean fo the input data\n        W : (n_features, n_components) ndarray\n            projection matrix\n        var : float\n            variance of observation noise\n        """"""\n        initial_list = [""random"", ""eigen""]\n        self.mean = np.mean(X, axis=0)\n        self.I = np.eye(self.n_components)\n        if initial not in initial_list:\n            print(""availabel initializations are {}"".format(initial_list))\n        if initial == ""random"":\n            self.W = np.eye(np.size(X, 1), self.n_components)\n            self.var = 1.\n        elif initial == ""eigen"":\n            self.eigen(X)\n        self.alpha = len(self.mean) / np.sum(self.W ** 2, axis=0).clip(min=1e-10)\n        for i in range(iter_max):\n            W = np.copy(self.W)\n            stats = self._expectation(X - self.mean)\n            self._maximization(X - self.mean, *stats)\n            self.alpha = len(self.mean) / np.sum(self.W ** 2, axis=0).clip(min=1e-10)\n            if np.allclose(W, self.W):\n                break\n        self.n_iter = i + 1\n\n    def _maximization(self, X, Ez, Ezz):\n        self.W = X.T @ Ez @ np.linalg.inv(np.sum(Ezz, axis=0) + self.var * np.diag(self.alpha))\n        self.var = np.mean(\n            np.mean(X ** 2, axis=-1)\n            - 2 * np.mean(Ez @ self.W.T * X, axis=-1)\n            + np.trace((Ezz @ self.W.T @ self.W).T) / len(self.mean))\n\n    def maximize(self, D, Ez, Ezz):\n        self.W = D.T.dot(Ez).dot(np.linalg.inv(np.sum(Ezz, axis=0) + self.var * np.diag(self.alpha)))\n        self.var = np.mean(\n            np.mean(D ** 2, axis=-1)\n            - 2 * np.mean(Ez.dot(self.W.T) * D, axis=-1)\n            + np.trace(Ezz.dot(self.W.T).dot(self.W).T) / self.ndim)\n'"
books/PRML/PRML-master-Python/prml/dimreduction/pca.py,0,"b'import numpy as np\n\n\nclass PCA(object):\n\n    def __init__(self, n_components):\n        """"""\n        construct principal component analysis\n\n        Parameters\n        ----------\n        n_components : int\n            number of components\n        """"""\n        assert isinstance(n_components, int)\n        self.n_components = n_components\n\n    def fit(self, X, method=""eigen"", iter_max=100):\n        """"""\n        maximum likelihood estimate of pca parameters\n        x ~ \\int_z N(x|Wz+mu,sigma^2)N(z|0,I)dz\n\n        Parameters\n        ----------\n        X : (sample_size, n_features) ndarray\n            input data\n        method : str\n            method to estimate the parameters\n            [""eigen"", ""em""]\n        iter_max : int\n            maximum number of iterations for em algorithm\n\n        Attributes\n        ----------\n        mean : (n_features,) ndarray\n            sample mean of the data\n        W : (n_features, n_components) ndarray\n            projection matrix\n        var : float\n            variance of observation noise\n        C : (n_features, n_features) ndarray\n            variance of the marginal dist N(x|mean,C)\n        Cinv : (n_features, n_features) ndarray\n            precision of the marginal dist N(x|mean, C)\n        """"""\n        method_list = [""eigen"", ""em""]\n        if method not in method_list:\n            print(""availabel methods are {}"".format(method_list))\n        self.mean = np.mean(X, axis=0)\n        getattr(self, method)(X - self.mean, iter_max)\n\n    def eigen(self, X, *arg):\n        sample_size, n_features = X.shape\n        if sample_size >= n_features:\n            cov = np.cov(X, rowvar=False)\n            values, vectors = np.linalg.eigh(cov)\n            index = n_features - self.n_components\n        else:\n            cov = np.cov(X)\n            values, vectors = np.linalg.eigh(cov)\n            vectors = (X.T @ vectors) / np.sqrt(sample_size * values)\n            index = sample_size - self.n_components\n        self.I = np.eye(self.n_components)\n        if index == 0:\n            self.var = 0\n        else:\n            self.var = np.mean(values[:index])\n\n        self.W = vectors[:, index:].dot(np.sqrt(np.diag(values[index:]) - self.var * self.I))\n        self.__M = self.W.T @ self.W + self.var * self.I\n        self.C = self.W @ self.W.T + self.var * np.eye(n_features)\n        if index == 0:\n            self.Cinv = np.linalg.inv(self.C)\n        else:\n            self.Cinv = np.eye(n_features) / np.sqrt(self.var) - self.W @ np.linalg.inv(self.__M) @ self.W.T / self.var\n\n    def em(self, X, iter_max):\n        self.I = np.eye(self.n_components)\n        self.W = np.eye(np.size(X, 1), self.n_components)\n        self.var = 1.\n        for i in range(iter_max):\n            W = np.copy(self.W)\n            stats = self._expectation(X)\n            self._maximization(X, *stats)\n            if np.allclose(W, self.W):\n                break\n        self.C = self.W @ self.W.T + self.var * np.eye(np.size(X, 1))\n        self.Cinv = np.linalg.inv(self.C)\n\n    def _expectation(self, X):\n        self.__M = self.W.T @ self.W + self.var * self.I\n        Minv = np.linalg.inv(self.__M)\n        Ez = X @ self.W @ Minv\n        Ezz = self.var * Minv + Ez[:, :, None] * Ez[:, None, :]\n        return Ez, Ezz\n\n    def _maximization(self, X, Ez, Ezz):\n        self.W = X.T @ Ez @ np.linalg.inv(np.sum(Ezz, axis=0))\n        self.var = np.mean(\n            np.mean(X ** 2, axis=1)\n            - 2 * np.mean(Ez @ self.W.T * X, axis=1)\n            + np.trace((Ezz @ self.W.T @ self.W).T) / np.size(X, 1))\n\n    def transform(self, X):\n        """"""\n        project input data into latent space\n        p(Z|X) = N(Z|(X-mu)WMinv, sigma^-2M)\n\n        Parameters\n        ----------\n        X : (sample_size, n_features) ndarray\n            input data\n\n        Returns\n        -------\n        Z : (sample_size, n_components) ndarray\n            projected input data\n        """"""\n        return np.linalg.solve(self.__M, ((X - self.mean) @ self.W).T).T\n\n    def fit_transform(self, X, method=""eigen""):\n        """"""\n        perform pca and whiten the input data\n\n        Parameters\n        ----------\n        X : (sample_size, n_features) ndarray\n            input data\n\n        Returns\n        -------\n        Z : (sample_size, n_components) ndarray\n            projected input data\n        """"""\n        self.fit(X, method)\n        return self.transform(X)\n\n    def proba(self, X):\n        """"""\n        the marginal distribution of the observed variable\n\n        Parameters\n        ----------\n        X : (sample_size, n_features) ndarray\n            input data\n\n        Returns\n        -------\n        p : (sample_size,) ndarray\n            value of the marginal distribution\n        """"""\n        d = X - self.mean\n        return (\n            np.exp(-0.5 * np.sum(d @ self.Cinv * d, axis=-1))\n            / np.sqrt(np.linalg.det(self.C))\n            / np.power(2 * np.pi, 0.5 * np.size(X, 1)))\n'"
books/PRML/PRML-master-Python/prml/kernel/__init__.py,0,"b'from prml.kernel.polynomial import PolynomialKernel\nfrom prml.kernel.rbf import RBF\n\nfrom prml.kernel.gaussian_process_classifier import GaussianProcessClassifier\nfrom prml.kernel.gaussian_process_regressor import GaussianProcessRegressor\nfrom prml.kernel.relevance_vector_classifier import RelevanceVectorClassifier\nfrom prml.kernel.relevance_vector_regressor import RelevanceVectorRegressor\nfrom prml.kernel.support_vector_classifier import SupportVectorClassifier\n\n\n__all__ = [\n    ""PolynomialKernel"",\n    ""RBF"",\n    ""GaussianProcessClassifier"",\n    ""GaussianProcessRegressor"",\n    ""RelevanceVectorClassifier"",\n    ""RelevanceVectorRegressor"",\n    ""SupportVectorClassifier""\n]\n'"
books/PRML/PRML-master-Python/prml/kernel/gaussian_process_classifier.py,0,"b'import numpy as np\n\n\nclass GaussianProcessClassifier(object):\n\n    def __init__(self, kernel, noise_level=1e-4):\n        """"""\n        construct gaussian process classifier\n\n        Parameters\n        ----------\n        kernel\n            kernel function to be used to compute Gram matrix\n        noise_level : float\n            parameter to ensure the matrix to be positive\n        """"""\n        self.kernel = kernel\n        self.noise_level = noise_level\n\n    def _sigmoid(self, a):\n        return np.tanh(a * 0.5) * 0.5 + 0.5\n\n    def fit(self, X, t):\n        if X.ndim == 1:\n            X = X[:, None]\n        self.X = X\n        self.t = t\n        Gram = self.kernel(X, X)\n        self.covariance = Gram + np.eye(len(Gram)) * self.noise_level\n        self.precision = np.linalg.inv(self.covariance)\n\n    def predict(self, X):\n        if X.ndim == 1:\n            X = X[:, None]\n        K = self.kernel(X, self.X)\n        a_mean = K @ self.precision @ self.t\n        return self._sigmoid(a_mean)\n'"
books/PRML/PRML-master-Python/prml/kernel/gaussian_process_regressor.py,0,"b'import numpy as np\n\n\nclass GaussianProcessRegressor(object):\n\n    def __init__(self, kernel, beta=1.):\n        """"""\n        construct gaussian process regressor\n\n        Parameters\n        ----------\n        kernel\n            kernel function\n        beta : float\n            precision parameter of observation noise\n        """"""\n        self.kernel = kernel\n        self.beta = beta\n\n    def fit(self, X, t, iter_max=0, learning_rate=0.1):\n        """"""\n        maximum likelihood estimation of parameters in kernel function\n\n        Parameters\n        ----------\n        X : ndarray (sample_size, n_features)\n            input\n        t : ndarray (sample_size,)\n            corresponding target\n        iter_max : int\n            maximum number of iterations updating hyperparameters\n        learning_rate : float\n            updation coefficient\n\n        Attributes\n        ----------\n        covariance : ndarray (sample_size, sample_size)\n            variance covariance matrix of gaussian process\n        precision : ndarray (sample_size, sample_size)\n            precision matrix of gaussian process\n\n        Returns\n        -------\n        log_likelihood_list : list\n            list of log likelihood value at each iteration\n        """"""\n        if X.ndim == 1:\n            X = X[:, None]\n        log_likelihood_list = [-np.Inf]\n        self.X = X\n        self.t = t\n        I = np.eye(len(X))\n        Gram = self.kernel(X, X)\n        self.covariance = Gram + I / self.beta\n        self.precision = np.linalg.inv(self.covariance)\n        for i in range(iter_max):\n            gradients = self.kernel.derivatives(X, X)\n            updates = np.array(\n                [-np.trace(self.precision.dot(grad)) + t.dot(self.precision.dot(grad).dot(self.precision).dot(t)) for grad in gradients])\n            for j in range(iter_max):\n                self.kernel.update_parameters(learning_rate * updates)\n                Gram = self.kernel(X, X)\n                self.covariance = Gram + I / self.beta\n                self.precision = np.linalg.inv(self.covariance)\n                log_like = self.log_likelihood()\n                if log_like > log_likelihood_list[-1]:\n                    log_likelihood_list.append(log_like)\n                    break\n                else:\n                    self.kernel.update_parameters(-learning_rate * updates)\n                    learning_rate *= 0.9\n        log_likelihood_list.pop(0)\n        return log_likelihood_list\n\n    def log_likelihood(self):\n        return -0.5 * (\n            np.linalg.slogdet(self.covariance)[1]\n            + self.t @ self.precision @ self.t\n            + len(self.t) * np.log(2 * np.pi))\n\n    def predict(self, X, with_error=False):\n        """"""\n        mean of the gaussian process\n\n        Parameters\n        ----------\n        X : ndarray (sample_size, n_features)\n            input\n\n        Returns\n        -------\n        mean : ndarray (sample_size,)\n            predictions of corresponding inputs\n        """"""\n        if X.ndim == 1:\n            X = X[:, None]\n        K = self.kernel(X, self.X)\n        mean = K @ self.precision @ self.t\n        if with_error:\n            var = (\n                self.kernel(X, X, False)\n                + 1 / self.beta\n                - np.sum(K @ self.precision * K, axis=1))\n            return mean.ravel(), np.sqrt(var.ravel())\n        return mean\n'"
books/PRML/PRML-master-Python/prml/kernel/kernel.py,0,"b'import numpy as np\n\n\nclass Kernel(object):\n    """"""\n    Base class for kernel function\n    """"""\n\n    def _pairwise(self, x, y):\n        """"""\n        all pairs of x and y\n\n        Parameters\n        ----------\n        x : (sample_size, n_features)\n            input\n        y : (sample_size, n_features)\n            another input\n\n        Returns\n        -------\n        output : tuple\n            two array with shape (sample_size, sample_size, n_features)\n        """"""\n        return (\n            np.tile(x, (len(y), 1, 1)).transpose(1, 0, 2),\n            np.tile(y, (len(x), 1, 1))\n        )'"
books/PRML/PRML-master-Python/prml/kernel/polynomial.py,0,"b'import numpy as np\nfrom prml.kernel.kernel import Kernel\n\n\nclass PolynomialKernel(Kernel):\n    """"""\n    Polynomial kernel\n    k(x,y) = (x @ y + c)^M\n    """"""\n\n    def __init__(self, degree=2, const=0.):\n        """"""\n        construct Polynomial kernel\n\n        Parameters\n        ----------\n        const : float\n            a constant to be added\n        degree : int\n            degree of polynomial order\n        """"""\n        self.const = const\n        self.degree = degree\n\n    def __call__(self, x, y, pairwise=True):\n        """"""\n        calculate pairwise polynomial kernel\n\n        Parameters\n        ----------\n        x : (..., ndim) ndarray\n            input\n        y : (..., ndim) ndarray\n            another input with the same shape\n\n        Returns\n        -------\n        output : ndarray\n            polynomial kernel\n        """"""\n        if pairwise:\n            x, y = self._pairwise(x, y)\n        return (np.sum(x * y, axis=-1) + self.const) ** self.degree\n'"
books/PRML/PRML-master-Python/prml/kernel/rbf.py,0,"b'import numpy as np\nfrom prml.kernel.kernel import Kernel\n\n\nclass RBF(Kernel):\n\n    def __init__(self, params):\n        """"""\n        construct Radial basis kernel function\n\n        Parameters\n        ----------\n        params : (ndim + 1,) ndarray\n            parameters of radial basis function\n\n        Attributes\n        ----------\n        ndim : int\n            dimension of expected input data\n        """"""\n        assert params.ndim == 1\n        self.params = params\n        self.ndim = len(params) - 1\n\n    def __call__(self, x, y, pairwise=True):\n        """"""\n        calculate radial basis function\n        k(x, y) = c0 * exp(-0.5 * c1 * (x1 - y1) ** 2 ...)\n\n        Parameters\n        ----------\n        x : ndarray [..., ndim]\n            input of this kernel function\n        y : ndarray [..., ndim]\n            another input\n\n        Returns\n        -------\n        output : ndarray\n            output of this radial basis function\n        """"""\n        assert x.shape[-1] == self.ndim\n        assert y.shape[-1] == self.ndim\n        if pairwise:\n            x, y = self._pairwise(x, y)\n        d = self.params[1:] * (x - y) ** 2\n        return self.params[0] * np.exp(-0.5 * np.sum(d, axis=-1))\n\n    def derivatives(self, x, y, pairwise=True):\n        if pairwise:\n            x, y = self._pairwise(x, y)\n        d = self.params[1:] * (x - y) ** 2\n        delta = np.exp(-0.5 * np.sum(d, axis=-1))\n        deltas = -0.5 * (x - y) ** 2 * (delta * self.params[0])[:, :, None]\n        return np.concatenate((np.expand_dims(delta, 0), deltas.T))\n\n    def update_parameters(self, updates):\n        self.params += updates\n'"
books/PRML/PRML-master-Python/prml/kernel/relevance_vector_classifier.py,0,"b'import numpy as np\n\n\nclass RelevanceVectorClassifier(object):\n\n    def __init__(self, kernel, alpha=1.):\n        """"""\n        construct relevance vector classifier\n\n        Parameters\n        ----------\n        kernel : Kernel\n            kernel function to compute components of feature vectors\n        alpha : float\n            initial precision of prior weight distribution\n        """"""\n        self.kernel = kernel\n        self.alpha = alpha\n\n    def _sigmoid(self, a):\n        return np.tanh(a * 0.5) * 0.5 + 0.5\n\n    def _map_estimate(self, X, t, w, n_iter=10):\n        for _ in range(n_iter):\n            y = self._sigmoid(X @ w)\n            g = X.T @ (y - t) + self.alpha * w\n            H = (X.T * y * (1 - y)) @ X + np.diag(self.alpha)\n            w -= np.linalg.solve(H, g)\n        return w, np.linalg.inv(H)\n\n    def fit(self, X, t, iter_max=100):\n        """"""\n        maximize evidence with respect ot hyperparameter\n\n        Parameters\n        ----------\n        X : (sample_size, n_features) ndarray\n            input\n        t : (sample_size,) ndarray\n            corresponding target\n        iter_max : int\n            maximum number of iterations\n\n        Attributes\n        ----------\n        X : (N, n_features) ndarray\n            relevance vector\n        t : (N,) ndarray\n            corresponding target\n        alpha : (N,) ndarray\n            hyperparameter for each weight or training sample\n        cov : (N, N) ndarray\n            covariance matrix of weight\n        mean : (N,) ndarray\n            mean of each weight\n        """"""\n        if X.ndim == 1:\n            X = X[:, None]\n        assert X.ndim == 2\n        assert t.ndim == 1\n        Phi = self.kernel(X, X)\n        N = len(t)\n        self.alpha = np.zeros(N) + self.alpha\n        mean = np.zeros(N)\n        for _ in range(iter_max):\n            param = np.copy(self.alpha)\n            mean, cov = self._map_estimate(Phi, t, mean, 10)\n            gamma = 1 - self.alpha * np.diag(cov)\n            self.alpha = gamma / np.square(mean)\n            np.clip(self.alpha, 0, 1e10, out=self.alpha)\n            if np.allclose(param, self.alpha):\n                break\n        mask = self.alpha < 1e8\n        self.X = X[mask]\n        self.t = t[mask]\n        self.alpha = self.alpha[mask]\n        Phi = self.kernel(self.X, self.X)\n        mean = mean[mask]\n        self.mean, self.covariance = self._map_estimate(Phi, self.t, mean, 100)\n\n    def predict(self, X):\n        """"""\n        predict class label\n\n        Parameters\n        ----------\n        X : (sample_size, n_features)\n            input\n\n        Returns\n        -------\n        label : (sample_size,) ndarray\n            predicted label\n        """"""\n        if X.ndim == 1:\n            X = X[:, None]\n        assert X.ndim == 2\n        phi = self.kernel(X, self.X)\n        label = (phi @ self.mean > 0).astype(np.int)\n        return label\n\n    def predict_proba(self, X):\n        """"""\n        probability of input belonging class one\n\n        Parameters\n        ----------\n        X : (sample_size, n_features) ndarray\n            input\n\n        Returns\n        -------\n        proba : (sample_size,) ndarray\n            probability of predictive distribution p(C1|x)\n        """"""\n        if X.ndim == 1:\n            X = X[:, None]\n        assert X.ndim == 2\n        phi = self.kernel(X, self.X)\n        mu_a = phi @ self.mean\n        var_a = np.sum(phi @ self.covariance * phi, axis=1)\n        return self._sigmoid(mu_a / np.sqrt(1 + np.pi * var_a / 8))\n'"
books/PRML/PRML-master-Python/prml/kernel/relevance_vector_regressor.py,0,"b'import numpy as np\n\n\nclass RelevanceVectorRegressor(object):\n\n    def __init__(self, kernel, alpha=1., beta=1.):\n        """"""\n        construct relevance vector regressor\n\n        Parameters\n        ----------\n        kernel : Kernel\n            kernel function to compute components of feature vectors\n        alpha : float\n            initial precision of prior weight distribution\n        beta : float\n            precision of observation\n        """"""\n        self.kernel = kernel\n        self.alpha = alpha\n        self.beta = beta\n\n    def fit(self, X, t, iter_max=1000):\n        """"""\n        maximize evidence with respect to hyperparameter\n\n        Parameters\n        ----------\n        X : (sample_size, n_features) ndarray\n            input\n        t : (sample_size,) ndarray\n            corresponding target\n        iter_max : int\n            maximum number of iterations\n\n        Attributes\n        -------\n        X : (N, n_features) ndarray\n            relevance vector\n        t : (N,) ndarray\n            corresponding target\n        alpha : (N,) ndarray\n            hyperparameter for each weight or training sample\n        cov : (N, N) ndarray\n            covariance matrix of weight\n        mean : (N,) ndarray\n            mean of each weight\n        """"""\n        if X.ndim == 1:\n            X = X[:, None]\n        assert X.ndim == 2\n        assert t.ndim == 1\n        N = len(t)\n        Phi = self.kernel(X, X)\n        self.alpha = np.zeros(N) + self.alpha\n        for _ in range(iter_max):\n            params = np.hstack([self.alpha, self.beta])\n            precision = np.diag(self.alpha) + self.beta * Phi.T @ Phi\n            covariance = np.linalg.inv(precision)\n            mean = self.beta * covariance @ Phi.T @ t\n            gamma = 1 - self.alpha * np.diag(covariance)\n            self.alpha = gamma / np.square(mean)\n            np.clip(self.alpha, 0, 1e10, out=self.alpha)\n            self.beta = (N - np.sum(gamma)) / np.sum((t - Phi.dot(mean)) ** 2)\n            if np.allclose(params, np.hstack([self.alpha, self.beta])):\n                break\n        mask = self.alpha < 1e9\n        self.X = X[mask]\n        self.t = t[mask]\n        self.alpha = self.alpha[mask]\n        Phi = self.kernel(self.X, self.X)\n        precision = np.diag(self.alpha) + self.beta * Phi.T @ Phi\n        self.covariance = np.linalg.inv(precision)\n        self.mean = self.beta * self.covariance @ Phi.T @ self.t\n\n    def predict(self, X, with_error=True):\n        """"""\n        predict output with this model\n\n        Parameters\n        ----------\n        X : (sample_size, n_features)\n            input\n        with_error : bool\n            if True, predict with standard deviation of the outputs\n\n        Returns\n        -------\n        mean : (sample_size,) ndarray\n            mean of predictive distribution\n        std : (sample_size,) ndarray\n            standard deviation of predictive distribution\n        """"""\n        if X.ndim == 1:\n            X = X[:, None]\n        assert X.ndim == 2\n        phi = self.kernel(X, self.X)\n        mean = phi @ self.mean\n        if with_error:\n            var = 1 / self.beta + np.sum(phi @ self.covariance * phi, axis=1)\n            return mean, np.sqrt(var)\n        return mean\n'"
books/PRML/PRML-master-Python/prml/kernel/support_vector_classifier.py,0,"b'import numpy as np\n\n\nclass SupportVectorClassifier(object):\n\n    def __init__(self, kernel, C=np.Inf):\n        """"""\n        construct support vector classifier\n\n        Parameters\n        ----------\n        kernel : Kernel\n            kernel function to compute inner products\n        C : float\n            penalty of misclassification\n        """"""\n        self.kernel = kernel\n        self.C = C\n\n    def fit(self, X:np.ndarray, t:np.ndarray, tol:float=1e-8):\n        """"""\n        estimate support vectors and their parameters\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            training independent variable\n        t : (N,) np.ndarray\n            training dependent variable\n            binary -1 or 1\n        tol : float, optional\n            numerical tolerance (the default is 1e-8)\n        """"""\n\n        N = len(t)\n        coef = np.zeros(N)\n        grad = np.ones(N)\n        Gram = self.kernel(X, X)\n\n        while True:\n            tg = t * grad\n            mask_up = (t == 1) & (coef < self.C - tol)\n            mask_up |= (t == -1) & (coef > tol)\n            mask_down = (t == -1) & (coef < self.C - tol)\n            mask_down |= (t == 1) & (coef > tol)\n            i = np.where(mask_up)[0][np.argmax(tg[mask_up])]\n            j = np.where(mask_down)[0][np.argmin(tg[mask_down])]\n            if tg[i] < tg[j] + tol:\n                self.b = 0.5 * (tg[i] + tg[j])\n                break\n            else:\n                A = self.C - coef[i] if t[i] == 1 else coef[i]\n                B = coef[j] if t[j] == 1 else self.C - coef[j]\n                direction = (tg[i] - tg[j]) / (Gram[i, i] - 2 * Gram[i, j] + Gram[j, j])\n                direction = min(A, B, direction)\n                coef[i] += direction * t[i]\n                coef[j] -= direction * t[j]\n                grad -= direction * t * (Gram[i] - Gram[j])\n        support_mask = coef > tol\n        self.a = coef[support_mask]\n        self.X = X[support_mask]\n        self.t = t[support_mask]\n\n    def lagrangian_function(self):\n        return (\n            np.sum(self.a)\n            - self.a\n            @ (self.t * self.t[:, None] * self.kernel(self.X, self.X))\n            @ self.a)\n\n    def predict(self, x):\n        """"""\n        predict labels of the input\n\n        Parameters\n        ----------\n        x : (sample_size, n_features) ndarray\n            input\n\n        Returns\n        -------\n        label : (sample_size,) ndarray\n            predicted labels\n        """"""\n        y = self.distance(x)\n        label = np.sign(y)\n        return label\n\n    def distance(self, x):\n        """"""\n        calculate distance from the decision boundary\n\n        Parameters\n        ----------\n        x : (sample_size, n_features) ndarray\n            input\n\n        Returns\n        -------\n        distance : (sample_size,) ndarray\n            distance from the boundary\n        """"""\n        distance = np.sum(\n            self.a * self.t\n            * self.kernel(x, self.X),\n            axis=-1) + self.b\n        return distance\n'"
books/PRML/PRML-master-Python/prml/linear/__init__.py,0,"b'from prml.linear.bayesian_logistic_regression import BayesianLogisticRegression\nfrom prml.linear.bayesian_regression import BayesianRegression\nfrom prml.linear.emprical_bayes_regression import EmpiricalBayesRegression\nfrom prml.linear.least_squares_classifier import LeastSquaresClassifier\nfrom prml.linear.linear_regression import LinearRegression\nfrom prml.linear.fishers_linear_discriminant import FishersLinearDiscriminant\nfrom prml.linear.logistic_regression import LogisticRegression\nfrom prml.linear.perceptron import Perceptron\nfrom prml.linear.ridge_regression import RidgeRegression\nfrom prml.linear.softmax_regression import SoftmaxRegression\nfrom prml.linear.variational_linear_regression import VariationalLinearRegression\nfrom prml.linear.variational_logistic_regression import VariationalLogisticRegression\n\n\n__all__ = [\n    ""BayesianLogisticRegression"",\n    ""BayesianRegression"",\n    ""EmpiricalBayesRegression"",\n    ""LeastSquaresClassifier"",\n    ""LinearRegression"",\n    ""FishersLinearDiscriminant"",\n    ""LogisticRegression"",\n    ""Perceptron"",\n    ""RidgeRegression"",\n    ""SoftmaxRegression"",\n    ""VariationalLinearRegression"",\n    ""VariationalLogisticRegression""\n]\n'"
books/PRML/PRML-master-Python/prml/linear/bayesian_logistic_regression.py,0,"b'import numpy as np\nfrom prml.linear.logistic_regression import LogisticRegression\n\n\nclass BayesianLogisticRegression(LogisticRegression):\n    """"""\n    Logistic regression model\n\n    w ~ Gaussian(0, alpha^(-1)I)\n    y = sigmoid(X @ w)\n    t ~ Bernoulli(t|y)\n    """"""\n\n    def __init__(self, alpha:float=1.):\n        self.alpha = alpha\n\n    def fit(self, X:np.ndarray, t:np.ndarray, max_iter:int=100):\n        """"""\n        bayesian estimation of logistic regression model\n        using Laplace approximation\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            training data independent variable\n        t : (N,) np.ndarray\n            training data dependent variable\n            binary 0 or 1\n        max_iter : int, optional\n            maximum number of paramter update iteration (the default is 100)\n        """"""\n        w = np.zeros(np.size(X, 1))\n        eye = np.eye(np.size(X, 1))\n        self.w_mean = np.copy(w)\n        self.w_precision = self.alpha * eye\n        for _ in range(max_iter):\n            w_prev = np.copy(w)\n            y = self._sigmoid(X @ w)\n            grad = X.T @ (y - t) + self.w_precision @ (w - self.w_mean)\n            hessian = (X.T * y * (1 - y)) @ X + self.w_precision\n            try:\n                w -= np.linalg.solve(hessian, grad)\n            except np.linalg.LinAlgError:\n                break\n            if np.allclose(w, w_prev):\n                break\n        self.w_mean = w\n        self.w_precision = hessian\n\n    def proba(self, X:np.ndarray):\n        """"""\n        compute probability of input belonging class 1\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            training data independent variable\n\n        Returns\n        -------\n        (N,) np.ndarray\n            probability of positive\n        """"""\n        mu_a = X @ self.w_mean\n        var_a = np.sum(np.linalg.solve(self.w_precision, X.T).T * X, axis=1)\n        return self._sigmoid(mu_a / np.sqrt(1 + np.pi * var_a / 8))\n'"
books/PRML/PRML-master-Python/prml/linear/bayesian_regression.py,0,"b'import numpy as np\nfrom prml.linear.regression import Regression\n\n\nclass BayesianRegression(Regression):\n    """"""\n    Bayesian regression model\n\n    w ~ N(w|0, alpha^(-1)I)\n    y = X @ w\n    t ~ N(t|X @ w, beta^(-1))\n    """"""\n\n    def __init__(self, alpha:float=1., beta:float=1.):\n        self.alpha = alpha\n        self.beta = beta\n        self.w_mean = None\n        self.w_precision = None\n\n    def _is_prior_defined(self) -> bool:\n        return self.w_mean is not None and self.w_precision is not None\n\n    def _get_prior(self, ndim:int) -> tuple:\n        if self._is_prior_defined():\n            return self.w_mean, self.w_precision\n        else:\n            return np.zeros(ndim), self.alpha * np.eye(ndim)\n\n    def fit(self, X:np.ndarray, t:np.ndarray):\n        """"""\n        bayesian update of parameters given training dataset\n\n        Parameters\n        ----------\n        X : (N, n_features) np.ndarray\n            training data independent variable\n        t : (N,) np.ndarray\n            training data dependent variable\n        """"""\n\n        mean_prev, precision_prev = self._get_prior(np.size(X, 1))\n\n        w_precision = precision_prev + self.beta * X.T @ X\n        w_mean = np.linalg.solve(\n            w_precision,\n            precision_prev @ mean_prev + self.beta * X.T @ t\n        )\n        self.w_mean = w_mean\n        self.w_precision = w_precision\n        self.w_cov = np.linalg.inv(self.w_precision)\n\n    def predict(self, X:np.ndarray, return_std:bool=False, sample_size:int=None):\n        """"""\n        return mean (and standard deviation) of predictive distribution\n\n        Parameters\n        ----------\n        X : (N, n_features) np.ndarray\n            independent variable\n        return_std : bool, optional\n            flag to return standard deviation (the default is False)\n        sample_size : int, optional\n            number of samples to draw from the predictive distribution\n            (the default is None, no sampling from the distribution)\n\n        Returns\n        -------\n        y : (N,) np.ndarray\n            mean of the predictive distribution\n        y_std : (N,) np.ndarray\n            standard deviation of the predictive distribution\n        y_sample : (N, sample_size) np.ndarray\n            samples from the predictive distribution\n        """"""\n\n        if sample_size is not None:\n            w_sample = np.random.multivariate_normal(\n                self.w_mean, self.w_cov, size=sample_size\n            )\n            y_sample = X @ w_sample.T\n            return y_sample\n        y = X @ self.w_mean\n        if return_std:\n            y_var = 1 / self.beta + np.sum(X @ self.w_cov * X, axis=1)\n            y_std = np.sqrt(y_var)\n            return y, y_std\n        return y\n'"
books/PRML/PRML-master-Python/prml/linear/classifier.py,0,"b'class Classifier(object):\n    """"""\n    Base class for classifiers\n    """"""\n    pass\n'"
books/PRML/PRML-master-Python/prml/linear/emprical_bayes_regression.py,0,"b'import numpy as np\nfrom prml.linear.bayesian_regression import BayesianRegression\n\n\nclass EmpiricalBayesRegression(BayesianRegression):\n    """"""\n    Empirical Bayes Regression model\n    a.k.a.\n    type 2 maximum likelihood,\n    generalized maximum likelihood,\n    evidence approximation\n\n    w ~ N(w|0, alpha^(-1)I)\n    y = X @ w\n    t ~ N(t|X @ w, beta^(-1))\n    evidence function p(t|X,alpha,beta) = S p(t|w;X,beta)p(w|0;alpha) dw\n    """"""\n\n    def __init__(self, alpha:float=1., beta:float=1.):\n        super().__init__(alpha, beta)\n\n    def fit(self, X:np.ndarray, t:np.ndarray, max_iter:int=100):\n        """"""\n        maximization of evidence function with respect to\n        the hyperparameters alpha and beta given training dataset\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            training independent variable\n        t : (N,) np.ndarray\n            training dependent variable\n        max_iter : int\n            maximum number of iteration\n        """"""\n        M = X.T @ X\n        eigenvalues = np.linalg.eigvalsh(M)\n        eye = np.eye(np.size(X, 1))\n        N = len(t)\n        for _ in range(max_iter):\n            params = [self.alpha, self.beta]\n\n            w_precision = self.alpha * eye + self.beta * X.T @ X\n            w_mean = self.beta * np.linalg.solve(w_precision, X.T @ t)\n\n            gamma = np.sum(eigenvalues / (self.alpha + eigenvalues))\n            self.alpha = float(gamma / np.sum(w_mean ** 2).clip(min=1e-10))\n            self.beta = float(\n                (N - gamma) / np.sum(np.square(t - X @ w_mean))\n            )\n            if np.allclose(params, [self.alpha, self.beta]):\n                break\n        self.w_mean = w_mean\n        self.w_precision = w_precision\n        self.w_cov = np.linalg.inv(w_precision)\n\n    def _log_prior(self, w):\n        return -0.5 * self.alpha * np.sum(w ** 2)\n\n    def _log_likelihood(self, X, t, w):\n        return -0.5 * self.beta * np.square(t - X @ w).sum()\n\n    def _log_posterior(self, X, t, w):\n        return self._log_likelihood(X, t, w) + self._log_prior(w)\n\n    def log_evidence(self, X:np.ndarray, t:np.ndarray):\n        """"""\n        logarithm or the evidence function\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            indenpendent variable\n        t : (N,) np.ndarray\n            dependent variable\n        Returns\n        -------\n        float\n            log evidence\n        """"""\n        N = len(t)\n        D = np.size(X, 1)\n        return 0.5 * (\n            D * np.log(self.alpha) + N * np.log(self.beta)\n            - np.linalg.slogdet(self.w_precision)[1] - D * np.log(2 * np.pi)\n        ) + self._log_posterior(X, t, self.w_mean)\n'"
books/PRML/PRML-master-Python/prml/linear/fishers_linear_discriminant.py,0,"b'import numpy as np\nfrom prml.linear.classifier import Classifier\nfrom prml.rv.gaussian import Gaussian\n\n\nclass FishersLinearDiscriminant(Classifier):\n    """"""\n    Fisher\'s Linear discriminant model\n    """"""\n\n    def __init__(self, w:np.ndarray=None, threshold:float=None):\n        self.w = w\n        self.threshold = threshold\n\n    def fit(self, X:np.ndarray, t:np.ndarray):\n        """"""\n        estimate parameter given training dataset\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            training dataset independent variable\n        t : (N,) np.ndarray\n            training dataset dependent variable\n            binary 0 or 1\n        """"""\n        X0 = X[t == 0]\n        X1 = X[t == 1]\n        m0 = np.mean(X0, axis=0)\n        m1 = np.mean(X1, axis=0)\n        cov_inclass = np.cov(X0, rowvar=False) + np.cov(X1, rowvar=False)\n        self.w = np.linalg.solve(cov_inclass, m1 - m0)\n        self.w /= np.linalg.norm(self.w).clip(min=1e-10)\n\n        g0 = Gaussian()\n        g0.fit((X0 @ self.w))\n        g1 = Gaussian()\n        g1.fit((X1 @ self.w))\n        root = np.roots([\n            g1.var - g0.var,\n            2 * (g0.var * g1.mu - g1.var * g0.mu),\n            g1.var * g0.mu ** 2 - g0.var * g1.mu ** 2\n            - g1.var * g0.var * np.log(g1.var / g0.var)\n        ])\n        if g0.mu < root[0] < g1.mu or g1.mu < root[0] < g0.mu:\n            self.threshold = root[0]\n        else:\n            self.threshold = root[1]\n\n    def transform(self, X:np.ndarray):\n        """"""\n        project data\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            independent variable\n\n        Returns\n        -------\n        y : (N,) np.ndarray\n            projected data\n        """"""\n        return X @ self.w\n\n    def classify(self, X:np.ndarray):\n        """"""\n        classify input data\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            independent variable to be classified\n\n        Returns\n        -------\n        (N,) np.ndarray\n            binary class for each input\n        """"""\n        return (X @ self.w > self.threshold).astype(np.int)\n'"
books/PRML/PRML-master-Python/prml/linear/least_squares_classifier.py,0,"b'import numpy as np\nfrom prml.linear.classifier import Classifier\nfrom prml.preprocess.label_transformer import LabelTransformer\n\n\nclass LeastSquaresClassifier(Classifier):\n    """"""\n    Least squares classifier model\n\n    X : (N, D)\n    W : (D, K)\n    y = argmax_k X @ W\n    """"""\n\n    def __init__(self, W:np.ndarray=None):\n        self.W = W\n\n    def fit(self, X:np.ndarray, t:np.ndarray):\n        """"""\n        least squares fitting for classification\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            training independent variable\n        t : (N,) or (N, K) np.ndarray\n            training dependent variable\n            in class index (N,) or one-of-k coding (N,K)\n        """"""\n        if t.ndim == 1:\n            t = LabelTransformer().encode(t)\n        self.W = np.linalg.pinv(X) @ t\n\n    def classify(self, X:np.ndarray):\n        """"""\n        classify input data\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            independent variable to be classified\n\n        Returns\n        -------\n        (N,) np.ndarray\n            class index for each input\n        """"""\n        return np.argmax(X @ self.W, axis=-1)\n'"
books/PRML/PRML-master-Python/prml/linear/linear_regression.py,0,"b'import numpy as np\nfrom prml.linear.regression import Regression\n\n\nclass LinearRegression(Regression):\n    """"""\n    Linear regression model\n    y = X @ w\n    t ~ N(t|X @ w, var)\n    """"""\n\n    def fit(self, X:np.ndarray, t:np.ndarray):\n        """"""\n        perform least squares fitting\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            training independent variable\n        t : (N,) np.ndarray\n            training dependent variable\n        """"""\n        self.w = np.linalg.pinv(X) @ t\n        self.var = np.mean(np.square(X @ self.w - t))\n\n    def predict(self, X:np.ndarray, return_std:bool=False):\n        """"""\n        make prediction given input\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            samples to predict their output\n        return_std : bool, optional\n            returns standard deviation of each predition if True\n\n        Returns\n        -------\n        y : (N,) np.ndarray\n            prediction of each sample\n        y_std : (N,) np.ndarray\n            standard deviation of each predition\n        """"""\n        y = X @ self.w\n        if return_std:\n            y_std = np.sqrt(self.var) + np.zeros_like(y)\n            return y, y_std\n        return y\n'"
books/PRML/PRML-master-Python/prml/linear/logistic_regression.py,0,"b'import numpy as np\nfrom prml.linear.classifier import Classifier\n\n\nclass LogisticRegression(Classifier):\n    """"""\n    Logistic regression model\n\n    y = sigmoid(X @ w)\n    t ~ Bernoulli(t|y)\n    """"""\n\n    @staticmethod\n    def _sigmoid(a):\n        return np.tanh(a * 0.5) * 0.5 + 0.5\n\n    def fit(self, X:np.ndarray, t:np.ndarray, max_iter:int=100):\n        """"""\n        maximum likelihood estimation of logistic regression model\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            training data independent variable\n        t : (N,) np.ndarray\n            training data dependent variable\n            binary 0 or 1\n        max_iter : int, optional\n            maximum number of paramter update iteration (the default is 100)\n        """"""\n        w = np.zeros(np.size(X, 1))\n        for _ in range(max_iter):\n            w_prev = np.copy(w)\n            y = self._sigmoid(X @ w)\n            grad = X.T @ (y - t)\n            hessian = (X.T * y * (1 - y)) @ X\n            try:\n                w -= np.linalg.solve(hessian, grad)\n            except np.linalg.LinAlgError:\n                break\n            if np.allclose(w, w_prev):\n                break\n        self.w = w\n\n    def proba(self, X:np.ndarray):\n        """"""\n        compute probability of input belonging class 1\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            training data independent variable\n\n        Returns\n        -------\n        (N,) np.ndarray\n            probability of positive\n        """"""\n        return self._sigmoid(X @ self.w)\n\n    def classify(self, X:np.ndarray, threshold:float=0.5):\n        """"""\n        classify input data\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            independent variable to be classified\n        threshold : float, optional\n            threshold of binary classification (default is 0.5)\n\n        Returns\n        -------\n        (N,) np.ndarray\n            binary class for each input\n        """"""\n        return (self.proba(X) > threshold).astype(np.int)\n'"
books/PRML/PRML-master-Python/prml/linear/perceptron.py,0,"b'import numpy as np\nfrom prml.linear.classifier import Classifier\n\n\nclass Perceptron(Classifier):\n    """"""\n    Perceptron model\n    """"""\n\n    def fit(self, X, t, max_epoch=100):\n        """"""\n        fit perceptron model on given input pair\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            training independent variable\n        t : (N,)\n            training dependent variable\n            binary -1 or 1\n        max_epoch : int, optional\n            maximum number of epoch (the default is 100)\n        """"""\n        self.w = np.zeros(np.size(X, 1))\n        for _ in range(max_epoch):\n            N = len(t)\n            index = np.random.permutation(N)\n            X = X[index]\n            t = t[index]\n            for x, label in zip(X, t):\n                self.w += x * label\n                if (X @ self.w * t > 0).all():\n                    break\n            else:\n                continue\n            break\n\n    def classify(self, X):\n        """"""\n        classify input data\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            independent variable to be classified\n\n        Returns\n        -------\n        (N,) np.ndarray\n            binary class (-1 or 1) for each input\n        """"""\n        return np.sign(X @ self.w).astype(np.int)\n'"
books/PRML/PRML-master-Python/prml/linear/regression.py,0,"b'class Regression(object):\n    """"""\n    Base class for regressors\n    """"""\n    pass\n'"
books/PRML/PRML-master-Python/prml/linear/ridge_regression.py,0,"b'import numpy as np\nfrom prml.linear.regression import Regression\n\n\nclass RidgeRegression(Regression):\n    """"""\n    Ridge regression model\n\n    w* = argmin |t - X @ w| + alpha * |w|_2^2\n    """"""\n\n    def __init__(self, alpha:float=1.):\n        self.alpha = alpha\n\n    def fit(self, X:np.ndarray, t:np.ndarray):\n        """"""\n        maximum a posteriori estimation of parameter\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            training data independent variable\n        t : (N,) np.ndarray\n            training data dependent variable\n        """"""\n\n        eye = np.eye(np.size(X, 1))\n        self.w = np.linalg.solve(self.alpha * eye + X.T @ X, X.T @ t)\n\n    def predict(self, X:np.ndarray):\n        """"""\n        make prediction given input\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            samples to predict their output\n\n        Returns\n        -------\n        (N,) np.ndarray\n            prediction of each input\n        """"""\n        return X @ self.w\n'"
books/PRML/PRML-master-Python/prml/linear/softmax_regression.py,0,"b'import numpy as np\nfrom prml.linear.classifier import Classifier\nfrom prml.preprocess.label_transformer import LabelTransformer\n\n\nclass SoftmaxRegression(Classifier):\n    """"""\n    Softmax regression model\n    aka\n    multinomial logistic regression,\n    multiclass logistic regression,\n    maximum entropy classifier.\n\n    y = softmax(X @ W)\n    t ~ Categorical(t|y)\n    """"""\n\n    @staticmethod\n    def _softmax(a):\n        a_max = np.max(a, axis=-1, keepdims=True)\n        exp_a = np.exp(a - a_max)\n        return exp_a / np.sum(exp_a, axis=-1, keepdims=True)\n\n    def fit(self, X:np.ndarray, t:np.ndarray, max_iter:int=100, learning_rate:float=0.1):\n        """"""\n        maximum likelihood estimation of the parameter\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            training independent variable\n        t : (N,) or (N, K) np.ndarray\n            training dependent variable\n            in class index or one-of-k encoding\n        max_iter : int, optional\n            maximum number of iteration (the default is 100)\n        learning_rate : float, optional\n            learning rate of gradient descent (the default is 0.1)\n        """"""\n        if t.ndim == 1:\n            t = LabelTransformer().encode(t)\n        self.n_classes = np.size(t, 1)\n        W = np.zeros((np.size(X, 1), self.n_classes))\n        for _ in range(max_iter):\n            W_prev = np.copy(W)\n            y = self._softmax(X @ W)\n            grad = X.T @ (y - t)\n            W -= learning_rate * grad\n            if np.allclose(W, W_prev):\n                break\n        self.W = W\n\n    def proba(self, X:np.ndarray):\n        """"""\n        compute probability of input belonging each class\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            independent variable\n\n        Returns\n        -------\n        (N, K) np.ndarray\n            probability of each class\n        """"""\n        return self._softmax(X @ self.W)\n\n    def classify(self, X:np.ndarray):\n        """"""\n        classify input data\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            independent variable to be classified\n\n        Returns\n        -------\n        (N,) np.ndarray\n            class index for each input\n        """"""\n        return np.argmax(self.proba(X), axis=-1)\n'"
books/PRML/PRML-master-Python/prml/linear/variational_linear_regression.py,0,"b'import numpy as np\nfrom prml.linear.regression import Regression\n\n\nclass VariationalLinearRegression(Regression):\n    """"""\n    variational bayesian estimation of linear regression model\n    p(w,alpha|X,t)\n    ~ q(w)q(alpha)\n    = N(w|w_mean, w_var)Gamma(alpha|a,b)\n\n    Attributes\n    ----------\n    a : float\n        a parameter of variational posterior gamma distribution\n    b : float\n        another parameter of variational posterior gamma distribution\n    w_mean : (n_features,) ndarray\n        mean of variational posterior gaussian distribution\n    w_var : (n_features, n_feautures) ndarray\n        variance of variational posterior gaussian distribution\n    n_iter : int\n        number of iterations performed\n    """"""\n\n    def __init__(self, beta:float=1., a0:float=1., b0:float=1.):\n        """"""\n        construct variational linear regressor\n        Parameters\n        ----------\n        beta : float\n            precision of observation noise\n        a0 : float\n            a parameter of prior gamma distribution\n            Gamma(alpha|a0,b0)\n        b0 : float\n            another parameter of prior gamma distribution\n            Gamma(alpha|a0,b0)\n        """"""\n        self.beta = beta\n        self.a0 = a0\n        self.b0 = b0\n\n    def fit(self, X:np.ndarray, t:np.ndarray, iter_max:int=100):\n        """"""\n        variational bayesian estimation of parameter\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            training independent variable\n        t : (N,) np.ndarray\n            training dependent variable\n        iter_max : int, optional\n            maximum number of iteration (the default is 100)\n        """"""\n        D = np.size(X, 1)\n        self.a = self.a0 + 0.5 * D\n        self.b = self.b0\n        I = np.eye(D)\n        for _ in range(iter_max):\n            param = self.b\n            self.w_var = np.linalg.inv(self.a * I / self.b + self.beta * X.T @ X)\n            self.w_mean = self.beta * self.w_var @ X.T @ t\n            self.b = self.b0 + 0.5 * (np.sum(self.w_mean ** 2) + np.trace(self.w_var))\n            if np.allclose(self.b, param):\n                break\n\n    def predict(self, X:np.ndarray, return_std:bool=False):\n        """"""\n        make prediction of input\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            independent variable\n        return_std : bool, optional\n            return standard deviation of predictive distribution if True\n            (the default is False)\n\n        Returns\n        -------\n        y : (N,) np.ndarray\n            mean of predictive distribution\n        y_std : (N,) np.ndarray\n            standard deviation of predictive distribution\n        """"""\n        y = X @ self.w_mean\n        if return_std:\n            y_var = 1 / self.beta + np.sum(X @ self.w_var * X, axis=1)\n            y_std = np.sqrt(y_var)\n            return y, y_std\n        return y\n'"
books/PRML/PRML-master-Python/prml/linear/variational_logistic_regression.py,0,"b'import numpy as np\nfrom prml.linear.logistic_regression import LogisticRegression\n\n\nclass VariationalLogisticRegression(LogisticRegression):\n\n    def __init__(self, alpha:float=None, a0:float=1., b0:float=1.):\n        """"""\n        construct variational logistic regressor\n\n        Parameters\n        ----------\n        alpha : float\n            precision parameter of the prior\n            if None, this is also the subject to estimate\n        a0 : float\n            a parameter of hyper prior Gamma dist.\n            Gamma(alpha|a0,b0)\n            if alpha is not None, this argument will be ignored\n        b0 : float\n            another parameter of hyper prior Gamma dist.\n            Gamma(alpha|a0,b0)\n            if alpha is not None, this argument will be ignored\n        """"""\n        if alpha is not None:\n            self.__alpha = alpha\n        else:\n            self.a0 = a0\n            self.b0 = b0\n\n    def fit(self, X:np.ndarray, t:np.ndarray, iter_max:int=1000):\n        """"""\n        variational bayesian estimation of the parameter\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            training independent variable\n        t : (N,) np.ndarray\n            training dependent variable\n        iter_max : int, optional\n            maximum number of iteration (the default is 1000)\n        """"""\n        N, D = X.shape\n        if hasattr(self, ""a0""):\n            self.a = self.a0 + 0.5 * D\n        xi = np.random.uniform(-1, 1, size=N)\n        I = np.eye(D)\n        param = np.copy(xi)\n        for _ in range(iter_max):\n            lambda_ = np.tanh(xi) * 0.25 / xi\n            self.w_var = np.linalg.inv(I / self.alpha + 2 * (lambda_ * X.T) @ X)\n            self.w_mean = self.w_var @ np.sum(X.T * (t - 0.5), axis=1)\n            xi = np.sqrt(np.sum(X @ (self.w_var + self.w_mean * self.w_mean[:, None]) * X, axis=-1))\n            if np.allclose(xi, param):\n                break\n            else:\n                param = np.copy(xi)\n\n    @property\n    def alpha(self):\n        if hasattr(self, ""__alpha""):\n            return self.__alpha\n        else:\n            try:\n                self.b = self.b0 + 0.5 * (np.sum(self.w_mean ** 2) + np.trace(self.w_var))\n            except AttributeError:\n                self.b = self.b0\n            return self.a / self.b\n\n    def proba(self, X:np.ndarray):\n        """"""\n        compute probability of input belonging class 1\n\n        Parameters\n        ----------\n        X : (N, D) np.ndarray\n            training data independent variable\n\n        Returns\n        -------\n        (N,) np.ndarray\n            probability of positive\n        """"""\n        mu_a = X @ self.w_mean\n        var_a = np.sum(X @ self.w_var * X, axis=1)\n        y = self._sigmoid(mu_a / np.sqrt(1 + np.pi * var_a / 8))\n        return y\n'"
books/PRML/PRML-master-Python/prml/markov/__init__.py,0,"b'from .categorical_hmm import CategoricalHMM\nfrom .gaussian_hmm import GaussianHMM\nfrom prml.markov.kalman import Kalman, kalman_filter, kalman_smoother\nfrom .particle import Particle\n\n\n__all__ = [\n    ""GaussianHMM"",\n    ""CategoricalHMM"",\n    ""Kalman"",\n    ""kalman_filter"",\n    ""kalman_smoother"",\n    ""Particle""\n]\n'"
books/PRML/PRML-master-Python/prml/markov/categorical_hmm.py,0,"b'import numpy as np\nfrom .hmm import HiddenMarkovModel\n\n\nclass CategoricalHMM(HiddenMarkovModel):\n    """"""\n    Hidden Markov Model with categorical emission model\n    """"""\n\n    def __init__(self, initial_proba, transition_proba, means):\n        """"""\n        construct hidden markov model with categorical emission model\n\n        Parameters\n        ----------\n        initial_proba : (n_hidden,) np.ndarray\n            probability of initial latent state\n        transition_proba : (n_hidden, n_hidden) np.ndarray\n            transition probability matrix\n            (i, j) component denotes the transition probability from i-th to j-th hidden state\n        means : (n_hidden, ndim) np.ndarray\n            mean parameters of categorical distribution\n\n        Returns\n        -------\n        ndim : int\n            number of observation categories\n        n_hidden : int\n            number of hidden states\n        """"""\n        assert initial_proba.size == transition_proba.shape[0] == transition_proba.shape[1] == means.shape[0]\n        assert np.allclose(means.sum(axis=1), 1)\n        super().__init__(initial_proba, transition_proba)\n        self.ndim = means.shape[1]\n        self.means = means\n\n    def draw(self, n=100):\n        """"""\n        draw random sequence from this model\n\n        Parameters\n        ----------\n        n : int\n            length of the random sequence\n\n        Returns\n        -------\n        seq : (n,) np.ndarray\n            generated random sequence\n        """"""\n        hidden_state = np.random.choice(self.n_hidden, p=self.initial_proba)\n        seq = []\n        while len(seq) < n:\n            seq.append(np.random.choice(self.ndim, p=self.means[hidden_state]))\n            hidden_state = np.random.choice(self.n_hidden, p=self.transition_proba[hidden_state])\n        return np.asarray(seq)\n\n    def likelihood(self, X):\n        return self.means[X]\n\n    def maximize(self, seq, p_hidden, p_transition):\n        self.initial_proba = p_hidden[0] / np.sum(p_hidden[0])\n        self.transition_proba = np.sum(p_transition, axis=0) / np.sum(p_transition, axis=(0, 2))\n        x = p_hidden[:, None, :] * (np.eye(self.ndim)[seq])[:, :, None]\n        self.means = np.sum(x, axis=0) / np.sum(p_hidden, axis=0)\n'"
books/PRML/PRML-master-Python/prml/markov/gaussian_hmm.py,0,"b'import numpy as np\nfrom prml.rv import MultivariateGaussian\nfrom .hmm import HiddenMarkovModel\n\n\nclass GaussianHMM(HiddenMarkovModel):\n    """"""\n    Hidden Markov Model with Gaussian emission model\n    """"""\n\n    def __init__(self, initial_proba, transition_proba, means, covs):\n        """"""\n        construct hidden markov model with Gaussian emission model\n\n        Parameters\n        ----------\n        initial_proba : (n_hidden,) np.ndarray or None\n            probability of initial states\n        transition_proba : (n_hidden, n_hidden) np.ndarray or None\n            transition probability matrix\n            (i, j) component denotes the transition probability from i-th to j-th hidden state\n        means : (n_hidden, ndim) np.ndarray\n            mean of each gaussian component\n        covs : (n_hidden, ndim, ndim) np.ndarray\n            covariance matrix of each gaussian component\n\n        Attributes\n        ----------\n        ndim : int\n            dimensionality of observation space\n        n_hidden : int\n            number of hidden states\n        """"""\n        assert initial_proba.size == transition_proba.shape[0] == transition_proba.shape[1] == means.shape[0] == covs.shape[0]\n        assert means.shape[1] == covs.shape[1] == covs.shape[2]\n        super().__init__(initial_proba, transition_proba)\n        self.ndim = means.shape[1]\n        self.means = means\n        self.covs = covs\n        self.precisions = np.linalg.inv(self.covs)\n        self.gaussians = [MultivariateGaussian(m, cov) for m, cov in zip(means, covs)]\n\n    def draw(self, n=100):\n        """"""\n        draw random sequence from this model\n\n        Parameters\n        ----------\n        n : int\n            length of the random sequence\n\n        Returns\n        -------\n        seq : (n, ndim) np.ndarray\n            generated random sequence\n        """"""\n        hidden_state = np.random.choice(self.n_hidden, p=self.initial_proba)\n        seq = []\n        while len(seq) < n:\n            seq.extend(self.gaussians[hidden_state].draw())\n            hidden_state = np.random.choice(self.n_hidden, p=self.transition_proba[hidden_state])\n        return np.asarray(seq)\n\n    def likelihood(self, X):\n        diff = X[:, None, :] - self.means\n        exponents = np.sum(\n            np.einsum(\'nki,kij->nkj\', diff, self.precisions) * diff, axis=-1)\n        return np.exp(-0.5 * exponents) / np.sqrt(np.linalg.det(self.covs) * (2 * np.pi) ** self.ndim)\n\n    def maximize(self, seq, p_hidden, p_transition):\n        self.initial_proba = p_hidden[0] / np.sum(p_hidden[0])\n        self.transition_proba = np.sum(p_transition, axis=0) / np.sum(p_transition, axis=(0, 2))\n        Nk = np.sum(p_hidden, axis=0)\n        self.means = (seq.T @ p_hidden / Nk).T\n        diffs = seq[:, None, :] - self.means\n        self.covs = np.einsum(\'nki,nkj->kij\', diffs, diffs * p_hidden[:, :, None]) / Nk[:, None, None]\n'"
books/PRML/PRML-master-Python/prml/markov/hmm.py,0,"b'import numpy as np\n\n\nclass HiddenMarkovModel(object):\n    """"""\n    Base class of Hidden Markov models\n    """"""\n\n    def __init__(self, initial_proba, transition_proba):\n        """"""\n        construct hidden markov model\n\n        Parameters\n        ----------\n        initial_proba : (n_hidden,) np.ndarray\n            initial probability of each hidden state\n        transition_proba : (n_hidden, n_hidden) np.ndarray\n            transition probability matrix\n            (i, j) component denotes the transition probability from i-th to j-th hidden state\n\n        Attribute\n        ---------\n        n_hidden : int\n            number of hidden state\n        """"""\n        self.n_hidden = initial_proba.size\n        self.initial_proba = initial_proba\n        self.transition_proba = transition_proba\n\n    def fit(self, seq, iter_max=100):\n        """"""\n        perform EM algorithm to estimate parameter of emission model and hidden variables\n\n        Parameters\n        ----------\n        seq : (N, ndim) np.ndarray\n            observed sequence\n        iter_max : int\n            maximum number of EM steps\n\n        Returns\n        -------\n        posterior : (N, n_hidden) np.ndarray\n            posterior distribution of each latent variable\n        """"""\n        params = np.hstack(\n            (self.initial_proba.ravel(), self.transition_proba.ravel()))\n        for i in range(iter_max):\n            p_hidden, p_transition = self.expect(seq)\n            self.maximize(seq, p_hidden, p_transition)\n            params_new = np.hstack(\n                (self.initial_proba.ravel(), self.transition_proba.ravel()))\n            if np.allclose(params, params_new):\n                break\n            else:\n                params = params_new\n        return self.forward_backward(seq)\n\n    def expect(self, seq):\n        """"""\n        estimate posterior distributions of hidden states and\n        transition probability between adjacent latent variables\n\n        Parameters\n        ----------\n        seq : (N, ndim) np.ndarray\n            observed sequence\n\n        Returns\n        -------\n        p_hidden : (N, n_hidden) np.ndarray\n            posterior distribution of each hidden variable\n        p_transition : (N - 1, n_hidden, n_hidden) np.ndarray\n            posterior transition probability between adjacent latent variables\n        """"""\n        likelihood = self.likelihood(seq)\n\n        f = self.initial_proba * likelihood[0]\n        constant = [f.sum()]\n        forward = [f / f.sum()]\n        for like in likelihood[1:]:\n            f = forward[-1] @ self.transition_proba * like\n            constant.append(f.sum())\n            forward.append(f / f.sum())\n        forward = np.asarray(forward)\n        constant = np.asarray(constant)\n\n        backward = [np.ones(self.n_hidden)]\n        for like, c in zip(likelihood[-1:0:-1], constant[-1:0:-1]):\n            backward.insert(0, self.transition_proba @ (like * backward[0]) / c)\n        backward = np.asarray(backward)\n\n        p_hidden = forward * backward\n        p_transition = self.transition_proba * likelihood[1:, None, :] * backward[1:, None, :] * forward[:-1, :, None]\n        return p_hidden, p_transition\n\n    def forward_backward(self, seq):\n        """"""\n        estimate posterior distributions of hidden states\n\n        Parameters\n        ----------\n        seq : (N, ndim) np.ndarray\n            observed sequence\n\n        Returns\n        -------\n        posterior : (N, n_hidden) np.ndarray\n            posterior distribution of hidden states\n        """"""\n        likelihood = self.likelihood(seq)\n\n        f = self.initial_proba * likelihood[0]\n        constant = [f.sum()]\n        forward = [f / f.sum()]\n        for like in likelihood[1:]:\n            f = forward[-1] @ self.transition_proba * like\n            constant.append(f.sum())\n            forward.append(f / f.sum())\n\n        backward = [np.ones(self.n_hidden)]\n        for like, c in zip(likelihood[-1:0:-1], constant[-1:0:-1]):\n            backward.insert(0, self.transition_proba @ (like * backward[0]) / c)\n\n        forward = np.asarray(forward)\n        backward = np.asarray(backward)\n        posterior = forward * backward\n        return posterior\n\n    def filtering(self, seq):\n        """"""\n        bayesian filtering\n\n        Parameters\n        ----------\n        seq : (N, ndim) np.ndarray\n            observed sequence\n\n        Returns\n        -------\n        posterior : (N, n_hidden) np.ndarray\n            posterior distributions of each latent variables\n        """"""\n        likelihood = self.likelihood(seq)\n        p = self.initial_proba * likelihood[0]\n        posterior = [p / np.sum(p)]\n        for like in likelihood[1:]:\n            p = posterior[-1] @ self.transition_proba * like\n            posterior.append(p / np.sum(p))\n        posterior = np.asarray(posterior)\n        return posterior\n\n    def viterbi(self, seq):\n        """"""\n        viterbi algorithm (a.k.a. max-sum algorithm)\n\n        Parameters\n        ----------\n        seq : (N, ndim) np.ndarray\n            observed sequence\n\n        Returns\n        -------\n        seq_hid : (N,) np.ndarray\n            the most probable sequence of hidden variables\n        """"""\n        nll = -np.log(self.likelihood(seq))\n        cost_total = nll[0]\n        from_list = []\n        for i in range(1, len(seq)):\n            cost_temp = cost_total[:, None] - np.log(self.transition_proba) + nll[i]\n            cost_total = np.min(cost_temp, axis=0)\n            index = np.argmin(cost_temp, axis=0)\n            from_list.append(index)\n        seq_hid = [np.argmin(cost_total)]\n        for source in from_list[::-1]:\n            seq_hid.insert(0, source[seq_hid[0]])\n        return seq_hid\n'"
books/PRML/PRML-master-Python/prml/markov/kalman.py,0,"b'import numpy as np\nfrom prml.rv.multivariate_gaussian import MultivariateGaussian as Gaussian\nfrom prml.markov.state_space_model import StateSpaceModel\n\n\nclass Kalman(StateSpaceModel):\n    """"""\n    A class to perform kalman filtering or smoothing\n    z : internal state\n    x : observation\n\n    z_1 ~ N(z_1|mu_0, P_0)\\n\n    z_n ~ N(z_n|A z_n-1, P)\\n\n    x_n ~ N(x_n|C z_n, S)\n\n    Parameters\n    ----------\n    system : (Dz, Dz) np.ndarray\n        system matrix aka transition matrix (A)\n    cov_system : (Dz, Dz) np.ndarray\n        covariance matrix of process noise\n    measure : (Dx, Dz) np.ndarray\n        measurement matrix aka observation matrix (C)\n    cov_measure : (Dx, Dx) np.ndarray\n        covariance matrix of measurement noise\n    mu0 : (Dz,) np.ndarray\n        mean parameter of initial hidden variable\n    P0 : (Dz, Dz) np.ndarray\n        covariance parameter of initial hidden variable\n\n    Attributes\n    ----------\n    Dz : int\n        dimensionality of hidden variable\n    Dx : int\n        dimensionality of observed variable\n    """"""\n\n\n    def __init__(self, system, cov_system, measure, cov_measure, mu0, P0):\n        """"""\n        construct Kalman model\n\n        z_1 ~ N(z_1|mu_0, P_0)\\n\n        z_n ~ N(z_n|A z_n-1, P)\\n\n        x_n ~ N(x_n|C z_n, S)\n\n        Parameters\n        ----------\n        system : (Dz, Dz) np.ndarray\n            system matrix aka transition matrix (A)\n        cov_system : (Dz, Dz) np.ndarray\n            covariance matrix of process noise\n        measure : (Dx, Dz) np.ndarray\n            measurement matrix aka observation matrix (C)\n        cov_measure : (Dx, Dx) np.ndarray\n            covariance matrix of measurement noise\n        mu0 : (Dz,) np.ndarray\n            mean parameter of initial hidden variable\n        P0 : (Dz, Dz) np.ndarray\n            covariance parameter of initial hidden variable\n\n        Attributes\n        ----------\n        hidden_mean : list of (Dz,) np.ndarray\n            list of mean of hidden state starting from the given hidden state\n        hidden_cov : list of (Dz, Dz) np.ndarray\n            list of covariance of hidden state starting from the given hidden state\n        """"""\n        self.system = system\n        self.cov_system = cov_system\n        self.measure = measure\n        self.cov_measure = cov_measure\n\n        self.hidden_mean = [mu0]\n        self.hidden_cov = [P0]\n        self.hidden_cov_predicted = [None]\n\n        self.smoothed_until = -1\n        self.smoothing_gain = [None]\n\n    def predict(self):\n        """"""\n        predict hidden state at current step given estimate at previous step\n\n        Returns\n        -------\n        tuple ((Dz,) np.ndarray, (Dz, Dz) np.ndarray)\n            tuple of mean and covariance of the estimate at current step\n        """"""\n        mu_prev, cov_prev = self.hidden_mean[-1], self.hidden_cov[-1]\n        mu = self.system @ mu_prev\n        cov = self.system @ cov_prev @ self.system.T + self.cov_system\n        self.hidden_mean.append(mu)\n        self.hidden_cov.append(cov)\n        self.hidden_cov_predicted.append(np.copy(cov))\n        return mu, cov\n\n    def filter(self, observed):\n        """"""\n        bayesian update of current estimate given current observation\n\n        Parameters\n        ----------\n        observed : (Dx,) np.ndarray\n            current observation\n\n        Returns\n        -------\n        tuple ((Dz,) np.ndarray, (Dz, Dz) np.ndarray)\n            tuple of mean and covariance of the updated estimate\n        """"""\n        mu, cov = self.hidden_mean[-1], self.hidden_cov[-1]\n        innovation = observed - self.measure @ mu\n        cov_innovation = self.cov_measure + self.measure @ cov @ self.measure.T\n        kalman_gain = np.linalg.solve(cov_innovation, self.measure @ cov).T\n        mu += kalman_gain @ innovation\n        cov -= kalman_gain @ self.measure @ cov\n        return mu, cov\n\n    def filtering(self, observed_sequence):\n        """"""\n        perform kalman filtering given observed sequence\n\n        Parameters\n        ----------\n        observed_sequence : (T, Dx) np.ndarray\n            sequence of observations\n\n        Returns\n        -------\n        tuple ((T, Dz) np.ndarray, (T, Dz, Dz) np.ndarray)\n            seuquence of mean and covariance of hidden variable at each time step\n        """"""\n        for obs in observed_sequence:\n            self.predict()\n            self.filter(obs)\n        mean_sequence = np.asarray(self.hidden_mean[1:])\n        cov_sequence = np.asarray(self.hidden_cov[1:])\n        return mean_sequence, cov_sequence\n\n    def smooth(self):\n        """"""\n        bayesian update of current estimate with future observations\n        """"""\n        mean_smoothed_next = self.hidden_mean[self.smoothed_until]\n        cov_smoothed_next = self.hidden_cov[self.smoothed_until]\n        cov_pred_next = self.hidden_cov_predicted[self.smoothed_until]\n\n        self.smoothed_until -= 1\n        mean = self.hidden_mean[self.smoothed_until]\n        cov = self.hidden_cov[self.smoothed_until]\n        gain = np.linalg.solve(cov_pred_next, self.system @ cov).T\n        mean += gain @ (mean_smoothed_next - self.system @ mean)\n        cov += gain @ (cov_smoothed_next - cov_pred_next) @ gain.T\n        self.smoothing_gain.insert(0, gain)\n\n    def smoothing(self, observed_sequence:np.ndarray=None):\n        """"""\n        perform Kalman smoothing (given observed sequence)\n\n        Parameters\n        ----------\n        observed_sequence : (T, Dx) np.ndarray, optional\n            sequence of observation\n            run Kalman filter if given (the default is None)\n\n        Returns\n        -------\n        tuple ((T, Dz) np.ndarray, (T, Dz, Dz) np.ndarray)\n            sequence of mean and covariance of hidden variable at each time step\n        """"""\n        if observed_sequence is not None:\n            self.filtering(observed_sequence)\n        while self.smoothed_until != -len(self.hidden_mean):\n            self.smooth()\n        mean_sequence = np.asarray(self.hidden_mean[1:])\n        cov_sequence = np.asarray(self.hidden_cov[1:])\n        return mean_sequence, cov_sequence\n\n    def update_parameter(self, observation_sequence):\n        """"""\n        maximization step of EM algorithm\n        """"""\n        mu0 = self.hidden_mean[1]\n        P0 = self.hidden_cov[1]\n\n        Ezn = np.asarray(self.hidden_mean)\n        Eznzn = np.asarray(self.hidden_cov) + Ezn[..., None] * Ezn[:, None, :]\n        Eznzn_1 = np.einsum(""nij,nkj->nik"", self.hidden_cov[2:], self.smoothing_gain[1:-1]) + Ezn[2:, :, None] * Ezn[1:-1, None, :]\n        self.system = np.linalg.solve(np.sum(Eznzn[2:], axis=0), np.sum(Eznzn_1, axis=0).T).T\n        self.cov_system = np.mean(\n            Eznzn[2:]\n            - np.einsum(""ij,nkj->nik"", self.system, Eznzn_1)\n            - np.einsum(""nij,kj->nik"", Eznzn_1, self.system)\n            + np.einsum(""ij,njk,lk->nil"", self.system, Eznzn[1:-1], self.system),\n            axis=0\n        )\n        self.measure = np.linalg.solve(\n            np.sum(Eznzn[1:], axis=0),\n            np.sum(np.einsum(""ni,nj->nij"", Ezn[1:], observation_sequence), axis=0)\n        ).T\n        self.cov_measure = np.mean(\n            np.einsum(""ni,nj->nij"", observation_sequence, observation_sequence)\n            - np.einsum(""ij,nj,nk->nik"", self.measure, Ezn[1:], observation_sequence)\n            - np.einsum(""ni,nj,kj->nik"", observation_sequence, Ezn[1:], self.measure)\n            + np.einsum(""ij,njk,lk->nil"", self.measure, Eznzn[1:], self.measure),\n            axis=0\n        )\n        return self.system, self.cov_system, self.measure, self.cov_measure, mu0, P0\n\n    def fit(self, sequence, max_iter=10):\n        for _ in range(max_iter):\n            kalman_smoother(self, sequence)\n            param = self.update_parameter(sequence)\n            self.__init__(*param)\n        return kalman_smoother(self, sequence)\n\n\ndef kalman_filter(kalman:Kalman, observed_sequence:np.ndarray)->tuple:\n    """"""\n    perform kalman filtering given Kalman model and observed sequence\n\n    Parameters\n    ----------\n    kalman : Kalman\n        Kalman model\n    observed_sequence : (T, Dx) np.ndarray\n        sequence of observations\n\n    Returns\n    -------\n    tuple ((T, Dz) np.ndarray, (T, Dz, Dz) np.ndarray)\n        seuquence of mean and covariance of hidden variable at each time step\n    """"""\n    for obs in observed_sequence:\n        kalman.predict()\n        kalman.filter(obs)\n    mean_sequence = np.asarray(kalman.hidden_mean[1:])\n    cov_sequence = np.asarray(kalman.hidden_cov[1:])\n    return mean_sequence, cov_sequence\n\n\ndef kalman_smoother(kalman:Kalman, observed_sequence:np.ndarray=None):\n    """"""\n    perform Kalman smoothing given Kalman model (and observed sequence)\n\n    Parameters\n    ----------\n    kalman : Kalman\n        Kalman model\n    observed_sequence : (T, Dx) np.ndarray, optional\n        sequence of observation\n        run Kalman filter if given (the default is None)\n\n    Returns\n    -------\n    tuple ((T, Dz) np.ndarray, (T, Dz, Dz) np.ndarray)\n        seuqnce of mean and covariance of hidden variable at each time step\n    """"""\n\n    if observed_sequence is not None:\n        kalman_filter(kalman, observed_sequence)\n    while kalman.smoothed_until != -len(kalman.hidden_mean):\n        kalman.smooth()\n    mean_sequence = np.asarray(kalman.hidden_mean[1:])\n    cov_sequence = np.asarray(kalman.hidden_cov[1:])\n    return mean_sequence, cov_sequence\n'"
books/PRML/PRML-master-Python/prml/markov/particle.py,0,"b'import numpy as np\nfrom scipy.misc import logsumexp\nfrom scipy.spatial.distance import cdist\nfrom .state_space_model import StateSpaceModel\n\n\nclass Particle(StateSpaceModel):\n    """"""\n    A class to perform particle filtering, smoothing\n\n    z_1 ~ p(z_1)\\n\n    z_n ~ p(z_n|z_n-1)\\n\n    x_n ~ p(x_n|z_n)\n\n    Parameters\n    ----------\n    init_particle : (n_particle, ndim_hidden)\n        initial hidden state\n    sampler : callable (particles)\n        function to sample particles at current step given previous state\n    nll : callable (observation, particles)\n        function to compute negative log likelihood for each particle\n\n    Attribute\n    ---------\n    hidden_state : list of (n_paticle, ndim_hidden) np.ndarray\n        list of particles\n    """"""\n\n    def __init__(self, init_particle, system, cov_system, nll, pdf=None):\n        """"""\n        construct state space model to perform particle filtering or smoothing\n\n        Parameters\n        ----------\n        init_particle : (n_particle, ndim_hidden) np.ndarray\n            initial hidden state\n        system : (ndim_hidden, ndim_hidden) np.ndarray\n            system matrix aka transition matrix\n        cov_system : (ndim_hidden, ndim_hidden) np.ndarray\n            covariance matrix of process noise\n        nll : callable (observation, particles)\n            function to compute negative log likelihood for each particle\n\n        Attribute\n        ---------\n        particle : list of (n_paticle, ndim_hidden) np.ndarray\n            list of particles at each step\n        weight : list of (n_particle,) np.ndarray\n            list of importance of each particle at each step\n        n_particle : int\n            number of particles at each step\n        """"""\n        self.particle = [init_particle]\n        self.n_particle, self.ndim_hidden = init_particle.shape\n        self.weight = [np.ones(self.n_particle) / self.n_particle]\n        self.system = system\n        self.cov_system = cov_system\n        self.nll = nll\n        self.smoothed_until = -1\n\n    def resample(self):\n        index = np.random.choice(self.n_particle, self.n_particle, p=self.weight[-1])\n        return self.particle[-1][index]\n\n    def predict(self):\n        predicted = self.resample() @ self.system.T\n        predicted += np.random.multivariate_normal(np.zeros(self.ndim_hidden), self.cov_system, self.n_particle)\n        self.particle.append(predicted)\n        self.weight.append(np.ones(self.n_particle) / self.n_particle)\n        return predicted, self.weight[-1]\n\n    def weigh(self, observed):\n        logit = -self.nll(observed, self.particle[-1])\n        logit -= logsumexp(logit)\n        self.weight[-1] = np.exp(logit)\n\n    def filter(self, observed):\n        self.weigh(observed)\n        return self.particle[-1], self.weight[-1]\n\n    def filtering(self, observed_sequence):\n        mean = []\n        cov = []\n        for obs in observed_sequence:\n            self.predict()\n            p, w = self.filter(obs)\n            mean.append(np.average(p, axis=0, weights=w))\n            cov.append(np.cov(p, rowvar=False, aweights=w))\n        return np.asarray(mean), np.asarray(cov)\n\n    def transition_probability(self, particle, particle_prev):\n        dist = cdist(\n            particle,\n            particle_prev @ self.system.T,\n            ""mahalanobis"",\n            VI=np.linalg.inv(self.cov_system))\n        matrix = np.exp(-0.5 * np.square(dist))\n        matrix /= np.sum(matrix, axis=1, keepdims=True)\n        matrix[np.isnan(matrix)] = 1 / self.n_particle\n        return matrix\n\n    def smooth(self):\n        particle_next = self.particle[self.smoothed_until]\n        weight_next = self.weight[self.smoothed_until]\n\n        self.smoothed_until -= 1\n        particle = self.particle[self.smoothed_until]\n        weight = self.weight[self.smoothed_until]\n        matrix = self.transition_probability(particle_next, particle).T\n        weight *= matrix @ weight_next / (weight @ matrix)\n        weight /= np.sum(weight, keepdims=True)\n\n    def smoothing(self, observed_sequence:np.ndarray=None):\n        if observed_sequence is not None:\n            self.filtering(observed_sequence)\n        while self.smoothed_until != -len(self.particle):\n            self.smooth()\n        mean = []\n        cov = []\n        for p, w in zip(self.particle, self.weight):\n            mean.append(np.average(p, axis=0, weights=w))\n            cov.append(np.cov(p, rowvar=False, aweights=w))\n        return np.asarray(mean), np.asarray(cov)\n'"
books/PRML/PRML-master-Python/prml/markov/state_space_model.py,0,"b'class StateSpaceModel(object):\n    """"""\n    Base class for state-space models\n    """"""\n    pass\n'"
books/PRML/PRML-master-Python/prml/nn/__init__.py,0,"b'from prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.parameter import Parameter\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.array.flatten import flatten\nfrom prml.nn.array.reshape import reshape\nfrom prml.nn.array.split import split\nfrom prml.nn.array.transpose import transpose\nfrom prml.nn import linalg\nfrom prml.nn.image.convolve2d import convolve2d\nfrom prml.nn.image.max_pooling2d import max_pooling2d\nfrom prml.nn.math.abs import abs\nfrom prml.nn.math.exp import exp\nfrom prml.nn.math.gamma import gamma\nfrom prml.nn.math.log import log\nfrom prml.nn.math.mean import mean\nfrom prml.nn.math.power import power\nfrom prml.nn.math.product import prod\nfrom prml.nn.math.sqrt import sqrt\nfrom prml.nn.math.square import square\nfrom prml.nn.math.sum import sum\nfrom prml.nn.nonlinear.relu import relu\nfrom prml.nn.nonlinear.sigmoid import sigmoid\nfrom prml.nn.nonlinear.softmax import softmax\nfrom prml.nn.nonlinear.softplus import softplus\nfrom prml.nn.nonlinear.tanh import tanh\nfrom prml.nn import optimizer\nfrom prml.nn import random\nfrom prml.nn.network import Network\n\n\n__all__ = [\n    ""optimizer"",\n    ""Network""\n]\n'"
books/PRML/PRML-master-Python/prml/nn/function.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\n\n\nclass Function(object):\n    """"""\n    Base class for differentiable functions\n    """"""\n\n    def _convert2tensor(self, x):\n        if isinstance(x, (int, float, np.number, np.ndarray)):\n            x = Constant(x)\n        elif not isinstance(x, Tensor):\n            raise TypeError(\n                ""Unsupported class for input: {}"".format(type(x))\n            )\n        return x\n\n    def _equal_ndim(self, x, ndim):\n        if x.ndim != ndim:\n            raise ValueError(\n                ""dimensionality of the input must be {}, not {}""\n                .format(ndim, x.ndim)\n            )\n\n    def _atleast_ndim(self, x, ndim):\n        if x.ndim < ndim:\n            raise ValueError(\n                ""dimensionality of the input must be""\n                "" larger or equal to {}, not {}""\n                .format(ndim, x.ndim)\n            )\n'"
books/PRML/PRML-master-Python/prml/nn/network.py,0,"b'from prml.nn.random.random import RandomVariable\nfrom prml.nn.tensor.parameter import Parameter\n\n\nclass Network(object):\n    """"""\n    a base class for network building\n\n    Parameters\n    ----------\n    kwargs : tensor_like\n        parameters to be optimized\n\n    Attributes\n    ----------\n    parameter : dict\n        dictionary of parameters to be optimized\n    random_variable : dict\n        dictionary of random varibles\n    """"""\n\n    def __init__(self, **kwargs):\n        self.random_variable = {}\n        self.parameter = {}\n        for key, value in kwargs.items():\n            if isinstance(value, Parameter):\n                self.parameter[key] = value\n            else:\n                try:\n                    value = Parameter(value)\n                except TypeError:\n                    raise TypeError(f""invalid type argument: {type(value)}"")\n                self.parameter[key] = value\n            object.__setattr__(self, key, value)\n\n    def __setattr__(self, key, value):\n        if isinstance(value, RandomVariable):\n            self.random_variable[key] = value\n        object.__setattr__(self, key, value)\n\n    def clear(self):\n        """"""\n        clear gradient and constructed bayesian network\n        """"""\n        for p in self.parameter.values():\n            p.cleargrad()\n        self.random_variable = {}\n\n    def log_pdf(self, coef=1.):\n        """"""\n        compute logarithm of probabilty density function\n        Parameters\n        ----------\n        coef : float\n            coefficient to balance likelihood and prior\n            assuming mini-batch size / whole data size for mini-batch training\n        Returns\n        -------\n        logp : tensor_like\n            logarithm of probability density function\n        """"""\n        logp = 0\n        for rv in self.random_variable.values():\n            if rv.observed:\n                logp += rv.log_pdf().sum()\n            else:\n                logp += coef * rv.log_pdf().sum()\n        return logp\n\n    def elbo(self, coef=1.):\n        """"""\n        compute evidence lower bound of this model\n        ln p(output) >= elbo\n        Parameters\n        ----------\n        coef : float\n            coefficient to balance likelihood and prior\n            assuming mini-batch size / whole data size for mini-batch training\n        Returns\n        -------\n        evidence : tensor_like\n            evidence lower bound\n        """"""\n        evidence = 0\n        for rv in self.random_variable.values():\n            if rv.observed:\n                evidence += rv.log_pdf().sum()\n            else:\n                evidence += -coef * rv.KLqp().sum()\n        return evidence\n'"
books/PRML/PRML-master-Python/prml/preprocess/__init__.py,0,"b'from prml.preprocess.gaussian import GaussianFeature\nfrom prml.preprocess.label_transformer import LabelTransformer\nfrom prml.preprocess.polynomial import PolynomialFeature\nfrom prml.preprocess.sigmoidal import SigmoidalFeature\n\n\n__all__ = [\n    ""GaussianFeature"",\n    ""LabelTransformer"",\n    ""PolynomialFeature"",\n    ""SigmoidalFeature""\n]\n'"
books/PRML/PRML-master-Python/prml/preprocess/gaussian.py,0,"b'import numpy as np\n\n\nclass GaussianFeature(object):\n    """"""\n    Gaussian feature\n\n    gaussian function = exp(-0.5 * (x - m) / v)\n    """"""\n\n    def __init__(self, mean, var):\n        """"""\n        construct gaussian features\n\n        Parameters\n        ----------\n        mean : (n_features, ndim) or (n_features,) ndarray\n            places to locate gaussian function at\n        var : float\n            variance of the gaussian function\n        """"""\n        if mean.ndim == 1:\n            mean = mean[:, None]\n        else:\n            assert mean.ndim == 2\n        assert isinstance(var, float) or isinstance(var, int)\n        self.mean = mean\n        self.var = var\n\n    def _gauss(self, x, mean):\n        return np.exp(-0.5 * np.sum(np.square(x - mean), axis=-1) / self.var)\n\n    def transform(self, x):\n        """"""\n        transform input array with gaussian features\n\n        Parameters\n        ----------\n        x : (sample_size, ndim) or (sample_size,)\n            input array\n\n        Returns\n        -------\n        output : (sample_size, n_features)\n            gaussian features\n        """"""\n        if x.ndim == 1:\n            x = x[:, None]\n        else:\n            assert x.ndim == 2\n        assert np.size(x, 1) == np.size(self.mean, 1)\n        basis = [np.ones(len(x))]\n        for m in self.mean:\n            basis.append(self._gauss(x, m))\n        return np.asarray(basis).transpose()\n'"
books/PRML/PRML-master-Python/prml/preprocess/label_transformer.py,0,"b'import numpy as np\n\n\nclass LabelTransformer(object):\n    """"""\n    Label encoder decoder\n\n    Attributes\n    ----------\n    n_classes : int\n        number of classes, K\n    """"""\n\n    def __init__(self, n_classes:int=None):\n        self.n_classes = n_classes\n\n    @property\n    def n_classes(self):\n        return self.__n_classes\n\n    @n_classes.setter\n    def n_classes(self, K):\n        self.__n_classes = K\n        self.__encoder = None if K is None else np.eye(K)\n\n    @property\n    def encoder(self):\n        return self.__encoder\n\n    def encode(self, class_indices:np.ndarray):\n        """"""\n        encode class index into one-of-k code\n\n        Parameters\n        ----------\n        class_indices : (N,) np.ndarray\n            non-negative class index\n            elements must be integer in [0, n_classes)\n\n        Returns\n        -------\n        (N, K) np.ndarray\n            one-of-k encoding of input\n        """"""\n        if self.n_classes is None:\n            self.n_classes = np.max(class_indices) + 1\n\n        return self.encoder[class_indices]\n\n    def decode(self, onehot:np.ndarray):\n        """"""\n        decode one-of-k code into class index\n\n        Parameters\n        ----------\n        onehot : (N, K) np.ndarray\n            one-of-k code\n\n        Returns\n        -------\n        (N,) np.ndarray\n            class index\n        """"""\n\n        return np.argmax(onehot, axis=1)\n'"
books/PRML/PRML-master-Python/prml/preprocess/polynomial.py,0,"b'import itertools\nimport functools\nimport numpy as np\n\n\nclass PolynomialFeature(object):\n    """"""\n    polynomial features\n\n    transforms input array with polynomial features\n\n    Example\n    =======\n    x =\n    [[a, b],\n    [c, d]]\n\n    y = PolynomialFeatures(degree=2).transform(x)\n    y =\n    [[1, a, b, a^2, a * b, b^2],\n    [1, c, d, c^2, c * d, d^2]]\n    """"""\n\n    def __init__(self, degree=2):\n        """"""\n        construct polynomial features\n\n        Parameters\n        ----------\n        degree : int\n            degree of polynomial\n        """"""\n        assert isinstance(degree, int)\n        self.degree = degree\n\n    def transform(self, x):\n        """"""\n        transforms input array with polynomial features\n\n        Parameters\n        ----------\n        x : (sample_size, n) ndarray\n            input array\n\n        Returns\n        -------\n        output : (sample_size, 1 + nC1 + ... + nCd) ndarray\n            polynomial features\n        """"""\n        if x.ndim == 1:\n            x = x[:, None]\n        x_t = x.transpose()\n        features = [np.ones(len(x))]\n        for degree in range(1, self.degree + 1):\n            for items in itertools.combinations_with_replacement(x_t, degree):\n                features.append(functools.reduce(lambda x, y: x * y, items))\n        return np.asarray(features).transpose()\n'"
books/PRML/PRML-master-Python/prml/preprocess/sigmoidal.py,0,"b'import numpy as np\n\n\nclass SigmoidalFeature(object):\n    """"""\n    Sigmoidal features\n\n    1 / (1 + exp((m - x) @ c)\n    """"""\n\n    def __init__(self, mean, coef=1):\n        """"""\n        construct sigmoidal features\n\n        Parameters\n        ----------\n        mean : (n_features, ndim) or (n_features,) ndarray\n            center of sigmoid function\n        coef : (ndim,) ndarray or int or float\n            coefficient to be multplied with the distance\n        """"""\n        if mean.ndim == 1:\n            mean = mean[:, None]\n        else:\n            assert mean.ndim == 2\n        if isinstance(coef, int) or isinstance(coef, float):\n            if np.size(mean, 1) == 1:\n                coef = np.array([coef])\n            else:\n                raise ValueError(""mismatch of dimension"")\n        else:\n            assert coef.ndim == 1\n            assert np.size(mean, 1) == len(coef)\n        self.mean = mean\n        self.coef = coef\n\n    def _sigmoid(self, x, mean):\n        return np.tanh((x - mean) @ self.coef * 0.5) * 0.5 + 0.5\n\n    def transform(self, x):\n        """"""\n        transform input array with sigmoidal features\n\n        Parameters\n        ----------\n        x : (sample_size, ndim) or (sample_size,) ndarray\n            input array\n\n        Returns\n        -------\n        output : (sample_size, n_features) ndarray\n            sigmoidal features\n        """"""\n        if x.ndim == 1:\n            x = x[:, None]\n        else:\n            assert x.ndim == 2\n        assert np.size(x, 1) == np.size(self.mean, 1)\n        basis = [np.ones(len(x))]\n        for m in self.mean:\n            basis.append(self._sigmoid(x, m))\n        return np.asarray(basis).transpose()\n'"
books/PRML/PRML-master-Python/prml/rv/__init__.py,0,"b'from prml.rv.bernoulli import Bernoulli\nfrom prml.rv.bernoulli_mixture import BernoulliMixture\nfrom prml.rv.beta import Beta\nfrom prml.rv.categorical import Categorical\nfrom prml.rv.dirichlet import Dirichlet\nfrom prml.rv.gamma import Gamma\nfrom prml.rv.gaussian import Gaussian\nfrom prml.rv.multivariate_gaussian import MultivariateGaussian\nfrom prml.rv.multivariate_gaussian_mixture import MultivariateGaussianMixture\nfrom prml.rv.students_t import StudentsT\nfrom prml.rv.uniform import Uniform\nfrom prml.rv.variational_gaussian_mixture import VariationalGaussianMixture\n\n\n__all__ = [\n    ""Bernoulli"",\n    ""BernoulliMixture"",\n    ""Beta"",\n    ""Categorical"",\n    ""Dirichlet"",\n    ""Gamma"",\n    ""Gaussian"",\n    ""MultivariateGaussian"",\n    ""MultivariateGaussianMixture"",\n    ""StudentsT"",\n    ""Uniform"",\n    ""VariationalGaussianMixture""\n]\n'"
books/PRML/PRML-master-Python/prml/rv/bernoulli.py,0,"b'import numpy as np\nfrom prml.rv.rv import RandomVariable\nfrom prml.rv.beta import Beta\n\n\nclass Bernoulli(RandomVariable):\n    """"""\n    Bernoulli distribution\n    p(x|mu) = mu^x (1 - mu)^(1 - x)\n    """"""\n\n    def __init__(self, mu=None):\n        """"""\n        construct Bernoulli distribution\n\n        Parameters\n        ----------\n        mu : np.ndarray or Beta\n            probability of value 1 for each element\n        """"""\n        super().__init__()\n        self.mu = mu\n\n    @property\n    def mu(self):\n        return self.parameter[""mu""]\n\n    @mu.setter\n    def mu(self, mu):\n        if isinstance(mu, (int, float, np.number)):\n            if mu > 1 or mu < 0:\n                raise ValueError(f""mu must be in [0, 1], not {mu}"")\n            self.parameter[""mu""] = np.asarray(mu)\n        elif isinstance(mu, np.ndarray):\n            if (mu > 1).any() or (mu < 0).any():\n                raise ValueError(""mu must be in [0, 1]"")\n            self.parameter[""mu""] = mu\n        elif isinstance(mu, Beta):\n            self.parameter[""mu""] = mu\n        else:\n            if mu is not None:\n                raise TypeError(f""{type(mu)} is not supported for mu"")\n            self.parameter[""mu""] = None\n\n    @property\n    def ndim(self):\n        if hasattr(self.mu, ""ndim""):\n            return self.mu.ndim\n        else:\n            return None\n\n    @property\n    def size(self):\n        if hasattr(self.mu, ""size""):\n            return self.mu.size\n        else:\n            return None\n\n    @property\n    def shape(self):\n        if hasattr(self.mu, ""shape""):\n            return self.mu.shape\n        else:\n            return None\n\n    def _fit(self, X):\n        if isinstance(self.mu, Beta):\n            self._bayes(X)\n        elif isinstance(self.mu, RandomVariable):\n            raise NotImplementedError\n        else:\n            self._ml(X)\n\n    def _ml(self, X):\n        n_zeros = np.count_nonzero((X == 0).astype(np.int))\n        n_ones = np.count_nonzero((X == 1).astype(np.int))\n        assert X.size == n_zeros + n_ones, (\n            ""{X.size} is not equal to {n_zeros} plus {n_ones}""\n        )\n        self.mu = np.mean(X, axis=0)\n\n    def _map(self, X):\n        assert isinstance(self.mu, Beta)\n        assert X.shape[1:] == self.mu.shape\n        n_ones = (X == 1).sum(axis=0)\n        n_zeros = (X == 0).sum(axis=0)\n        assert X.size == n_zeros.sum() + n_ones.sum(), (\n            f""{X.size} is not equal to {n_zeros} plus {n_ones}""\n        )\n        n_ones = n_ones + self.mu.n_ones\n        n_zeros = n_zeros + self.mu.n_zeros\n        self.prob = (n_ones - 1) / (n_ones + n_zeros - 2)\n\n    def _bayes(self, X):\n        assert isinstance(self.mu, Beta)\n        assert X.shape[1:] == self.mu.shape\n        n_ones = (X == 1).sum(axis=0)\n        n_zeros = (X == 0).sum(axis=0)\n        assert X.size == n_zeros.sum() + n_ones.sum(), (\n            ""input X must only has 0 or 1""\n        )\n        self.mu.n_zeros += n_zeros\n        self.mu.n_ones += n_ones\n\n    def _pdf(self, X):\n        assert isinstance(mu, np.ndarray)\n        return np.prod(\n            self.mu ** X * (1 - self.mu) ** (1 - X)\n        )\n\n    def _draw(self, sample_size=1):\n        if isinstance(self.mu, np.ndarray):\n            return (\n                self.mu > np.random.uniform(size=(sample_size,) + self.shape)\n            ).astype(np.int)\n        elif isinstance(self.mu, Beta):\n            return (\n                self.mu.n_ones / (self.mu.n_ones + self.mu.n_zeros)\n                > np.random.uniform(size=(sample_size,) + self.shape)\n            ).astype(np.int)\n        elif isinstance(self.mu, RandomVariable):\n            return (\n                self.mu.draw(sample_size)\n                > np.random.uniform(size=(sample_size,) + self.shape)\n            )\n'"
books/PRML/PRML-master-Python/prml/rv/bernoulli_mixture.py,0,"b'import numpy as np\nfrom scipy.misc import logsumexp\nfrom prml.rv.rv import RandomVariable\n\n\nclass BernoulliMixture(RandomVariable):\n    """"""\n    p(x|pi,mu)\n    = sum_k pi_k mu_k^x (1 - mu_k)^(1 - x)\n    """"""\n\n    def __init__(self, n_components=3, mu=None, coef=None):\n        """"""\n        construct mixture of Bernoulli\n\n        Parameters\n        ----------\n        n_components : int\n            number of bernoulli component\n        mu : (n_components, ndim) np.ndarray\n            probability of value 1 for each component\n        coef : (n_components,) np.ndarray\n            mixing coefficients\n        """"""\n        super().__init__()\n        assert isinstance(n_components, int)\n        self.n_components = n_components\n        self.mu = mu\n        self.coef = coef\n\n    @property\n    def mu(self):\n        return self.parameter[""mu""]\n\n    @mu.setter\n    def mu(self, mu):\n        if isinstance(mu, np.ndarray):\n            assert mu.ndim == 2\n            assert np.size(mu, 0) == self.n_components\n            assert (mu >= 0.).all() and (mu <= 1.).all()\n            self.ndim = np.size(mu, 1)\n            self.parameter[""mu""] = mu\n        else:\n            assert mu is None\n            self.parameter[""mu""] = None\n\n    @property\n    def coef(self):\n        return self.parameter[""coef""]\n\n    @coef.setter\n    def coef(self, coef):\n        if isinstance(coef, np.ndarray):\n            assert coef.ndim == 1\n            assert np.allclose(coef.sum(), 1)\n            self.parameter[""coef""] = coef\n        else:\n            assert coef is None\n            self.parameter[""coef""] = np.ones(self.n_components) / self.n_components\n\n    def _log_bernoulli(self, X):\n        np.clip(self.mu, 1e-10, 1 - 1e-10, out=self.mu)\n        return (\n            X[:, None, :] * np.log(self.mu)\n            + (1 - X[:, None, :]) * np.log(1 - self.mu)\n        ).sum(axis=-1)\n\n    def _fit(self, X):\n        self.mu = np.random.uniform(0.25, 0.75, size=(self.n_components, np.size(X, 1)))\n        params = np.hstack((self.mu.ravel(), self.coef.ravel()))\n        while True:\n            resp = self._expectation(X)\n            self._maximization(X, resp)\n            new_params = np.hstack((self.mu.ravel(), self.coef.ravel()))\n            if np.allclose(params, new_params):\n                break\n            else:\n                params = new_params\n\n    def _expectation(self, X):\n        log_resps = np.log(self.coef) + self._log_bernoulli(X)\n        log_resps -= logsumexp(log_resps, axis=-1)[:, None]\n        resps = np.exp(log_resps)\n        return resps\n\n    def _maximization(self, X, resp):\n        Nk = np.sum(resp, axis=0)\n        self.coef = Nk / len(X)\n        self.mu = (X.T @ resp / Nk).T\n\n    def classify(self, X):\n        """"""\n        classify input\n        max_z p(z|x, theta)\n\n        Parameters\n        ----------\n        X : (sample_size, ndim) ndarray\n            input\n\n        Returns\n        -------\n        output : (sample_size,) ndarray\n            corresponding cluster index\n        """"""\n        return np.argmax(self.classify_proba(X), axis=1)\n\n    def classfiy_proba(self, X):\n        """"""\n        posterior probability of cluster\n        p(z|x,theta)\n\n        Parameters\n        ----------\n        X : (sample_size, ndim) ndarray\n            input\n\n        Returns\n        -------\n        output : (sample_size, n_components) ndarray\n            posterior probability of cluster\n        """"""\n        return self._expectation(X)\n'"
books/PRML/PRML-master-Python/prml/rv/beta.py,0,"b'import numpy as np\nfrom scipy.special import gamma\nfrom prml.rv.rv import RandomVariable\n\n\nnp.seterr(all=""ignore"")\n\n\nclass Beta(RandomVariable):\n    """"""\n    Beta distribution\n    p(mu|n_ones, n_zeros)\n    = gamma(n_ones + n_zeros)\n      * mu^(n_ones - 1) * (1 - mu)^(n_zeros - 1)\n      / gamma(n_ones) / gamma(n_zeros)\n    """"""\n\n    def __init__(self, n_zeros, n_ones):\n        """"""\n        construct beta distribution\n\n        Parameters\n        ----------\n        n_zeros : int, float, or np.ndarray\n            pseudo count of zeros\n        n_ones : int, float, or np.ndarray\n            pseudo count of ones\n        """"""\n        super().__init__()\n        if not isinstance(n_ones, (int, float, np.number, np.ndarray)):\n            raise ValueError(\n                ""{} is not supported for n_ones""\n                .format(type(n_ones))\n            )\n        if not isinstance(n_zeros, (int, float, np.number, np.ndarray)):\n            raise ValueError(\n                ""{} is not supported for n_zeros""\n                .format(type(n_zeros))\n            )\n        n_ones = np.asarray(n_ones)\n        n_zeros = np.asarray(n_zeros)\n        if n_ones.shape != n_zeros.shape:\n            raise ValueError(\n                ""the sizes of the arrays don\'t match: {}, {}""\n                .format(n_ones.shape, n_zeros.shape)\n            )\n        self.n_ones = n_ones\n        self.n_zeros = n_zeros\n\n    @property\n    def ndim(self):\n        return self.n_ones.ndim\n\n    @property\n    def size(self):\n        return self.n_ones.size\n\n    @property\n    def shape(self):\n        return self.n_ones.shape\n\n    def _pdf(self, mu):\n        return (\n            gamma(self.n_ones + self.n_zeros)\n            * np.power(mu, self.n_ones - 1)\n            * np.power(1 - mu, self.n_zeros - 1)\n            / gamma(self.n_ones)\n            / gamma(self.n_zeros)\n        )\n\n    def _draw(self, sample_size=1):\n        return np.random.beta(\n            self.n_ones, self.n_zeros, size=(sample_size,) + self.shape\n        )\n'"
books/PRML/PRML-master-Python/prml/rv/categorical.py,0,"b'import numpy as np\nfrom prml.rv.rv import RandomVariable\nfrom prml.rv.dirichlet import Dirichlet\n\n\nclass Categorical(RandomVariable):\n    """"""\n    Categorical distribution\n    p(x|mu) = prod_k mu_k^x_k\n    """"""\n\n    def __init__(self, mu=None):\n        """"""\n        construct categorical distribution\n\n        Parameters\n        ----------\n        mu : (n_classes,) np.ndarray or Dirichlet\n            probability of each class\n        """"""\n        super().__init__()\n        self.mu = mu\n\n    @property\n    def mu(self):\n        return self.parameter[""mu""]\n\n    @mu.setter\n    def mu(self, mu):\n        if isinstance(mu, np.ndarray):\n            if mu.ndim != 1:\n                raise ValueError(""dimensionality of mu must be 1"")\n            if (mu < 0).any():\n                raise ValueError(""mu must be non-negative"")\n            if not np.allclose(mu.sum(), 1):\n                raise ValueError(""sum of mu must be 1"")\n            self.n_classes = mu.size\n            self.parameter[""mu""] = mu\n        elif isinstance(mu, Dirichlet):\n            self.n_classes = mu.size\n            self.parameter[""mu""] = mu\n        else:\n            if mu is not None:\n                raise TypeError(f""{type(mu)} is not supported for mu"")\n            self.parameter[""mu""] = None\n\n    @property\n    def ndim(self):\n        if hasattr(self.mu, ""ndim""):\n            return self.mu.ndim\n        else:\n            return None\n\n    @property\n    def size(self):\n        if hasattr(self.mu, ""size""):\n            return self.mu.size\n        else:\n            return None\n\n    @property\n    def shape(self):\n        if hasattr(self.mu, ""shape""):\n            return self.mu.shape\n        else:\n            return None\n\n    def _check_input(self, X):\n        assert X.ndim == 2\n        assert (X >= 0).all()\n        assert (X.sum(axis=-1) == 1).all()\n\n    def _fit(self, X):\n        if isinstance(self.mu, Dirichlet):\n            self._bayes(X)\n        elif isinstance(self.mu, RandomVariable):\n            raise NotImplementedError\n        else:\n            self._ml(X)\n\n    def _ml(self, X):\n        self._check_input(X)\n        self.mu = np.mean(X, axis=0)\n\n    def _map(self, X):\n        self._check_input(X)\n        assert isinstance(self.mu, Dirichlet)\n        alpha = self.mu.alpha + X.sum(axis=0)\n        self.mu = (alpha - 1) / (alpha - 1).sum()\n\n    def _bayes(self, X):\n        self._check_input(X)\n        assert isinstance(self.mu, Dirichlet)\n        self.mu.alpha += X.sum(axis=0)\n\n    def _pdf(self, X):\n        self._check_input(X)\n        assert isinstance(self.mu, np.ndarray)\n        return np.prod(self.mu ** X, axis=-1)\n\n    def _draw(self, sample_size=1):\n        assert isinstance(self.mu, np.ndarray)\n        return np.eye(self.n_classes)[\n            np.random.choice(self.n_classes, sample_size, p=self.mu)\n        ]\n'"
books/PRML/PRML-master-Python/prml/rv/dirichlet.py,0,"b'import numpy as np\nfrom scipy.special import gamma\nfrom prml.rv.rv import RandomVariable\n\n\nclass Dirichlet(RandomVariable):\n    """"""\n    Dirichlet distribution\n    p(mu|alpha)\n    = gamma(sum(alpha))\n      * prod_k mu_k ^ (alpha_k - 1)\n      / gamma(alpha_1) / ... / gamma(alpha_K)\n    """"""\n\n    def __init__(self, alpha):\n        """"""\n        construct dirichlet distribution\n\n        Parameters\n        ----------\n        alpha : (size,) np.ndarray\n            pseudo count of each outcome, aka concentration parameter\n        """"""\n        super().__init__()\n        self.alpha = alpha\n\n    @property\n    def alpha(self):\n        return self.parameter[""alpha""]\n\n    @alpha.setter\n    def alpha(self, alpha):\n        assert isinstance(alpha, np.ndarray)\n        assert alpha.ndim == 1\n        assert (alpha >= 0).all()\n        self.parameter[""alpha""] = alpha\n\n    @property\n    def ndim(self):\n        return self.alpha.ndim\n\n    @property\n    def size(self):\n        return self.alpha.size\n\n    @property\n    def shape(self):\n        return self.alpha.shape\n\n    def _pdf(self, mu):\n        return (\n            gamma(self.alpha.sum())\n            * np.prod(mu ** (self.alpha - 1), axis=-1)\n            / np.prod(gamma(self.alpha), axis=-1)\n        )\n\n    def _draw(self, sample_size=1):\n        return np.random.dirichlet(self.alpha, sample_size)\n'"
books/PRML/PRML-master-Python/prml/rv/gamma.py,0,"b'import numpy as np\nfrom scipy.special import gamma\nfrom prml.rv.rv import RandomVariable\n\n\nnp.seterr(all=""ignore"")\n\n\nclass Gamma(RandomVariable):\n    """"""\n    Gamma distribution\n    p(x|a, b)\n    = b^a x^(a-1) exp(-bx) / gamma(a)\n    """"""\n\n    def __init__(self, a, b):\n        """"""\n        construct Gamma distribution\n\n        Parameters\n        ----------\n        a : int, float, or np.ndarray\n            shape parameter\n        b : int, float, or np.ndarray\n            rate parameter\n        """"""\n        super().__init__()\n        a = np.asarray(a)\n        b = np.asarray(b)\n        assert a.shape == b.shape\n        self.a = a\n        self.b = b\n\n    @property\n    def a(self):\n        return self.parameter[""a""]\n\n    @a.setter\n    def a(self, a):\n        if isinstance(a, (int, float, np.number)):\n            if a <= 0:\n                raise ValueError(""a must be positive"")\n            self.parameter[""a""] = np.asarray(a)\n        elif isinstance(a, np.ndarray):\n            if (a <= 0).any():\n                raise ValueError(""a must be positive"")\n            self.parameter[""a""] = a\n        else:\n            if a is not None:\n                raise TypeError(f""{type(a)} is not supported for a"")\n            self.parameter[""a""] = None\n\n    @property\n    def b(self):\n        return self.parameter[""b""]\n\n    @b.setter\n    def b(self, b):\n        if isinstance(b, (int, float, np.number)):\n            if b <= 0:\n                raise ValueError(""b must be positive"")\n            self.parameter[""b""] = np.asarray(b)\n        elif isinstance(b, np.ndarray):\n            if (b <= 0).any():\n                raise ValueError(""b must be positive"")\n            self.parameter[""b""] = b\n        else:\n            if b is not None:\n                raise TypeError(f""{type(b)} is not supported for b"")\n            self.parameter[""b""] = None\n\n    @property\n    def ndim(self):\n        return self.a.ndim\n\n    @property\n    def shape(self):\n        return self.a.shape\n\n    @property\n    def size(self):\n        return self.a.size\n\n    def _pdf(self, X):\n        return (\n            self.b ** self.a\n            * X ** (self.a - 1)\n            * np.exp(-self.b * X)\n            / gamma(self.a))\n\n    def _draw(self, sample_size=1):\n        return np.random.gamma(\n            shape=self.a,\n            scale=1 / self.b,\n            size=(sample_size,) + self.shape\n        )\n'"
books/PRML/PRML-master-Python/prml/rv/gaussian.py,0,"b'import numpy as np\nfrom prml.rv.rv import RandomVariable\nfrom prml.rv.gamma import Gamma\n\n\nclass Gaussian(RandomVariable):\n    """"""\n    The Gaussian distribution\n    p(x|mu, var)\n    = exp{-0.5 * (x - mu)^2 / var} / sqrt(2pi * var)\n    """"""\n\n    def __init__(self, mu=None, var=None, tau=None):\n        super().__init__()\n        self.mu = mu\n        if var is not None:\n            self.var = var\n        elif tau is not None:\n            self.tau = tau\n        else:\n            self.var = None\n            self.tau = None\n\n    @property\n    def mu(self):\n        return self.parameter[""mu""]\n\n    @mu.setter\n    def mu(self, mu):\n        if isinstance(mu, (int, float, np.number)):\n            self.parameter[""mu""] = np.array(mu)\n        elif isinstance(mu, np.ndarray):\n            self.parameter[""mu""] = mu\n        elif isinstance(mu, Gaussian):\n            self.parameter[""mu""] = mu\n        else:\n            if mu is not None:\n                raise TypeError(f""{type(mu)} is not supported for mu"")\n            self.parameter[""mu""] = None\n\n    @property\n    def var(self):\n        return self.parameter[""var""]\n\n    @var.setter\n    def var(self, var):\n        if isinstance(var, (int, float, np.number)):\n            assert var > 0\n            var = np.array(var)\n            assert var.shape == self.shape\n            self.parameter[""var""] = var\n            self.parameter[""tau""] = 1 / var\n        elif isinstance(var, np.ndarray):\n            assert (var > 0).all()\n            assert var.shape == self.shape\n            self.parameter[""var""] = var\n            self.parameter[""tau""] = 1 / var\n        else:\n            assert var is None\n            self.parameter[""var""] = None\n            self.parameter[""tau""] = None\n\n    @property\n    def tau(self):\n        return self.parameter[""tau""]\n\n    @tau.setter\n    def tau(self, tau):\n        if isinstance(tau, (int, float, np.number)):\n            assert tau > 0\n            tau = np.array(tau)\n            assert tau.shape == self.shape\n            self.parameter[""tau""] = tau\n            self.parameter[""var""] = 1 / tau\n        elif isinstance(tau, np.ndarray):\n            assert (tau > 0).all()\n            assert tau.shape == self.shape\n            self.parameter[""tau""] = tau\n            self.parameter[""var""] = 1 / tau\n        elif isinstance(tau, Gamma):\n            assert tau.shape == self.shape\n            self.parameter[""tau""] = tau\n            self.parameter[""var""] = None\n        else:\n            assert tau is None\n            self.parameter[""tau""] = None\n            self.parameter[""var""] = None\n\n    @property\n    def ndim(self):\n        if hasattr(self.mu, ""ndim""):\n            return self.mu.ndim\n        else:\n            return None\n\n    @property\n    def size(self):\n        if hasattr(self.mu, ""size""):\n            return self.mu.size\n        else:\n            return None\n\n    @property\n    def shape(self):\n        if hasattr(self.mu, ""shape""):\n            return self.mu.shape\n        else:\n            return None\n\n    def _fit(self, X):\n        mu_is_gaussian = isinstance(self.mu, Gaussian)\n        tau_is_gamma = isinstance(self.tau, Gamma)\n        if mu_is_gaussian and tau_is_gamma:\n            raise NotImplementedError\n        elif mu_is_gaussian:\n            self._bayes_mu(X)\n        elif tau_is_gamma:\n            self._bayes_tau(X)\n        else:\n            self._ml(X)\n\n    def _ml(self, X):\n        self.mu = np.mean(X, axis=0)\n        self.var = np.var(X, axis=0)\n\n    def _map(self, X):\n        assert isinstance(self.mu, Gaussian)\n        assert isinstance(self.var, np.ndarray)\n        N = len(X)\n        mu = np.mean(X, 0)\n        self.mu = (\n            (self.tau * self.mu.mu + N * self.mu.tau * mu)\n            / (N * self.mu.tau + self.tau)\n        )\n\n    def _bayes_mu(self, X):\n        N = len(X)\n        mu = np.mean(X, 0)\n        tau = self.mu.tau + N * self.tau\n        self.mu = Gaussian(\n            mu=(self.mu.mu * self.mu.tau + N * mu * self.tau) / tau,\n            tau=tau\n        )\n\n    def _bayes_tau(self, X):\n        N = len(X)\n        var = np.var(X, axis=0)\n        a = self.tau.a + 0.5 * N\n        b = self.tau.b + 0.5 * N * var\n        self.tau = Gamma(a, b)\n\n    def _bayes(self, X):\n        N = len(X)\n        mu_is_gaussian = isinstance(self.mu, Gaussian)\n        tau_is_gamma = isinstance(self.tau, Gamma)\n        if mu_is_gaussian and not tau_is_gamma:\n            mu = np.mean(X, 0)\n            tau = self.mu.tau + N * self.tau\n            self.mu = Gaussian(\n                mu=(self.mu.mu * self.mu.tau + N * mu * self.tau) / tau,\n                tau=tau\n            )\n        elif not mu_is_gaussian and tau_is_gamma:\n            var = np.var(X, axis=0)\n            a = self.tau.a + 0.5 * N\n            b = self.tau.b + 0.5 * N * var\n            self.tau = Gamma(a, b)\n        elif mu_is_gaussian and tau_is_gamma:\n            raise NotImplementedError\n        else:\n            raise NotImplementedError\n\n    def _pdf(self, X):\n        d = X - self.mu\n        return (\n            np.exp(-0.5 * self.tau * d ** 2) / np.sqrt(2 * np.pi * self.var)\n        )\n\n    def _draw(self, sample_size=1):\n        return np.random.normal(\n            loc=self.mu,\n            scale=np.sqrt(self.var),\n            size=(sample_size,) + self.shape\n        )\n'"
books/PRML/PRML-master-Python/prml/rv/multivariate_gaussian.py,0,"b'import numpy as np\nfrom prml.rv.rv import RandomVariable\n\n\nclass MultivariateGaussian(RandomVariable):\n    """"""\n    The multivariate Gaussian distribution\n    p(x|mu, cov)\n    = exp{-0.5 * (x - mu)^T @ cov^-1 @ (x - mu)}\n      / (2pi)^(D/2) / |cov|^0.5\n    """"""\n\n    def __init__(self, mu=None, cov=None, tau=None):\n        super().__init__()\n        self.mu = mu\n        if cov is not None:\n            self.cov = cov\n        elif tau is not None:\n            self.tau = tau\n        else:\n            self.cov = None\n            self.tau = None\n\n    @property\n    def mu(self):\n        return self.parameter[""mu""]\n\n    @mu.setter\n    def mu(self, mu):\n        if isinstance(mu, np.ndarray):\n            assert mu.ndim == 1\n            self.parameter[""mu""] = mu\n        else:\n            assert mu is None\n            self.parameter[""mu""] = None\n\n    @property\n    def cov(self):\n        return self.parameter[""cov""]\n\n    @cov.setter\n    def cov(self, cov):\n        if isinstance(cov, np.ndarray):\n            assert cov.ndim == 2\n            self.tau_ = np.linalg.inv(cov)\n            self.parameter[""cov""] = cov\n        else:\n            assert cov is None\n            self.tau_ = None\n            self.parameter[""cov""] = None\n\n    @property\n    def tau(self):\n        return self.tau_\n\n    @tau.setter\n    def tau(self, tau):\n        if isinstance(tau, np.ndarray):\n            assert tau.ndim == 2\n            self.parameter[""cov""] = np.linalg.inv(tau)\n            self.tau_ = tau\n        else:\n            assert tau is None\n            self.tau_ = None\n            self.parameter[""cov""] = None\n\n    @property\n    def ndim(self):\n        if hasattr(self.mu, ""ndim""):\n            return self.mu.ndim\n        else:\n            return None\n\n    @property\n    def size(self):\n        if hasattr(self.mu, ""size""):\n            return self.mu.size\n        else:\n            return None\n\n    @property\n    def shape(self):\n        if hasattr(self.mu, ""shape""):\n            return self.mu.shape\n        else:\n            return None\n\n    def _fit(self, X):\n        self.mu = np.mean(X, axis=0)\n        self.cov = np.atleast_2d(np.cov(X.T, bias=True))\n\n    def _pdf(self, X):\n        d = X - self.mu\n        return (\n            np.exp(-0.5 * np.sum(d @ self.tau * d, axis=-1))\n            * np.sqrt(np.linalg.det(self.tau))\n            / np.power(2 * np.pi, 0.5 * self.size))\n\n    def _draw(self, sample_size=1):\n        return np.random.multivariate_normal(self.mu, self.cov, sample_size)\n'"
books/PRML/PRML-master-Python/prml/rv/multivariate_gaussian_mixture.py,0,"b'import numpy as np\nfrom prml.clustering import KMeans\nfrom prml.rv.rv import RandomVariable\n\n\nclass MultivariateGaussianMixture(RandomVariable):\n    """"""\n    p(x|mu, L, pi(coef))\n    = sum_k pi_k N(x|mu_k, L_k^-1)\n    """"""\n\n    def __init__(self,\n                 n_components,\n                 mu=None,\n                 cov=None,\n                 tau=None,\n                 coef=None):\n        """"""\n        construct mixture of Gaussians\n\n        Parameters\n        ----------\n        n_components : int\n            number of gaussian component\n        mu : (n_components, ndim) np.ndarray\n            mean parameter of each gaussian component\n        cov : (n_components, ndim, ndim) np.ndarray\n            variance parameter of each gaussian component\n        tau : (n_components, ndim, ndim) np.ndarray\n            precision parameter of each gaussian component\n        coef : (n_components,) np.ndarray\n            mixing coefficients\n        """"""\n        super().__init__()\n        assert isinstance(n_components, int)\n        self.n_components = n_components\n        self.mu = mu\n        if cov is not None and tau is not None:\n            raise ValueError(""Cannot assign both cov and tau at a time"")\n        elif cov is not None:\n            self.cov = cov\n        elif tau is not None:\n            self.tau = tau\n        else:\n            self.cov = None\n            self.tau = None\n        self.coef = coef\n\n    @property\n    def mu(self):\n        return self.parameter[""mu""]\n\n    @mu.setter\n    def mu(self, mu):\n        if isinstance(mu, np.ndarray):\n            assert mu.ndim == 2\n            assert np.size(mu, 0) == self.n_components\n            self.ndim = np.size(mu, 1)\n            self.parameter[""mu""] = mu\n        elif mu is None:\n            self.parameter[""mu""] = None\n        else:\n            raise TypeError(""mu must be either np.ndarray or None"")\n\n    @property\n    def cov(self):\n        return self.parameter[""cov""]\n\n    @cov.setter\n    def cov(self, cov):\n        if isinstance(cov, np.ndarray):\n            assert cov.shape == (self.n_components, self.ndim, self.ndim)\n            self._tau = np.linalg.inv(cov)\n            self.parameter[""cov""] = cov\n        elif cov is None:\n            self.parameter[""cov""] = None\n            self._tau = None\n        else:\n            raise TypeError(""cov must be either np.ndarray or None"")\n\n    @property\n    def tau(self):\n        return self._tau\n\n    @tau.setter\n    def tau(self, tau):\n        if isinstance(tau, np.ndarray):\n            assert tau.shape == (self.n_components, self.ndim, self.ndim)\n            self.parameter[""cov""] = np.linalg.inv(tau)\n            self._tau = tau\n        elif tau is None:\n            self.parameter[""cov""] = None\n            self._tau = None\n        else:\n            raise TypeError(""tau must be either np.ndarray or None"")\n\n    @property\n    def coef(self):\n        return self.parameter[""coef""]\n\n    @coef.setter\n    def coef(self, coef):\n        if isinstance(coef, np.ndarray):\n            assert coef.ndim == 1\n            if np.isnan(coef).any():\n                self.parameter[""coef""] = np.ones(self.n_components) / self.n_components\n            elif not np.allclose(coef.sum(), 1):\n                raise ValueError(f""sum of coef must be equal to 1 {coef}"")\n            self.parameter[""coef""] = coef\n        elif coef is None:\n            self.parameter[""coef""] = None\n        else:\n            raise TypeError(""coef must be either np.ndarray or None"")\n\n    @property\n    def shape(self):\n        if hasattr(self.mu, ""shape""):\n            return self.mu.shape[1:]\n        else:\n            return None\n\n    def _gauss(self, X):\n        d = X[:, None, :] - self.mu\n        D_sq = np.sum(np.einsum(\'nki,kij->nkj\', d, self.cov) * d, -1)\n        return (\n            np.exp(-0.5 * D_sq)\n            / np.sqrt(\n                np.linalg.det(self.cov) * (2 * np.pi) ** self.ndim\n            )\n        )\n\n    def _fit(self, X):\n        cov = np.cov(X.T)\n        kmeans = KMeans(self.n_components)\n        kmeans.fit(X)\n        self.mu = kmeans.centers\n        self.cov = np.array([cov for _ in range(self.n_components)])\n        self.coef = np.ones(self.n_components) / self.n_components\n        params = np.hstack(\n            (self.mu.ravel(),\n             self.cov.ravel(),\n             self.coef.ravel())\n        )\n        while True:\n            stats = self._expectation(X)\n            self._maximization(X, stats)\n            new_params = np.hstack(\n                (self.mu.ravel(),\n                 self.cov.ravel(),\n                 self.coef.ravel())\n            )\n            if np.allclose(params, new_params):\n                break\n            else:\n                params = new_params\n\n    def _expectation(self, X):\n        resps = self.coef * self._gauss(X)\n        resps /= resps.sum(axis=-1, keepdims=True)\n        return resps\n\n    def _maximization(self, X, resps):\n        Nk = np.sum(resps, axis=0)\n        self.coef = Nk / len(X)\n        self.mu = (X.T @ resps / Nk).T\n        d = X[:, None, :] - self.mu\n        self.cov = np.einsum(\n            \'nki,nkj->kij\', d, d * resps[:, :, None]) / Nk[:, None, None]\n\n    def joint_proba(self, X):\n        """"""\n        calculate joint probability p(X, Z)\n\n        Parameters\n        ----------\n        X : (sample_size, n_features) ndarray\n            input data\n\n        Returns\n        -------\n        joint_prob : (sample_size, n_components) ndarray\n            joint probability of input and component\n        """"""\n        return self.coef * self._gauss(X)\n\n    def _pdf(self, X):\n        joint_prob = self.coef * self._gauss(X)\n        return np.sum(joint_prob, axis=-1)\n\n    def classify(self, X):\n        """"""\n        classify input\n        max_z p(z|x, theta)\n\n        Parameters\n        ----------\n        X : (sample_size, ndim) ndarray\n            input\n\n        Returns\n        -------\n        output : (sample_size,) ndarray\n            corresponding cluster index\n        """"""\n        return np.argmax(self.classify_proba(X), axis=1)\n\n    def classify_proba(self, X):\n        """"""\n        posterior probability of cluster\n        p(z|x,theta)\n\n        Parameters\n        ----------\n        X : (sample_size, ndim) ndarray\n            input\n\n        Returns\n        -------\n        output : (sample_size, n_components) ndarray\n            posterior probability of cluster\n        """"""\n        return self._expectation(X)\n'"
books/PRML/PRML-master-Python/prml/rv/rv.py,0,"b'import numpy as np\n\n\nclass RandomVariable(object):\n    """"""\n    base class for random variables\n    """"""\n\n    def __init__(self):\n        self.parameter = {}\n\n    def __repr__(self):\n        string = f""{self.__class__.__name__}(\\n""\n        for key, value in self.parameter.items():\n            string += ("" "" * 4)\n            if isinstance(value, RandomVariable):\n                string += f""{key}={value:8}""\n            else:\n                string += f""{key}={value}""\n            string += ""\\n""\n        string += "")""\n        return string\n\n    def __format__(self, indent=""4""):\n        indent = int(indent)\n        string = f""{self.__class__.__name__}(\\n""\n        for key, value in self.parameter.items():\n            string += ("" "" * indent)\n            if isinstance(value, RandomVariable):\n                string += f""{key}="" + value.__format__(str(indent + 4))\n            else:\n                string += f""{key}={value}""\n            string += ""\\n""\n        string += ("" "" * (indent - 4)) + "")""\n        return string\n\n    def fit(self, X, **kwargs):\n        """"""\n        estimate parameter(s) of the distribution\n\n        Parameters\n        ----------\n        X : np.ndarray\n            observed data\n        """"""\n        self._check_input(X)\n        if hasattr(self, ""_fit""):\n            self._fit(X, **kwargs)\n        else:\n            raise NotImplementedError\n\n    # def ml(self, X, **kwargs):\n    #     """"""\n    #     maximum likelihood estimation of the parameter(s)\n    #     of the distribution given data\n\n    #     Parameters\n    #     ----------\n    #     X : (sample_size, ndim) np.ndarray\n    #         observed data\n    #     """"""\n    #     self._check_input(X)\n    #     if hasattr(self, ""_ml""):\n    #         self._ml(X, **kwargs)\n    #     else:\n    #         raise NotImplementedError\n\n    # def map(self, X, **kwargs):\n    #     """"""\n    #     maximum a posteriori estimation of the parameter(s)\n    #     of the distribution given data\n\n    #     Parameters\n    #     ----------\n    #     X : (sample_size, ndim) np.ndarray\n    #         observed data\n    #     """"""\n    #     self._check_input(X)\n    #     if hasattr(self, ""_map""):\n    #         self._map(X, **kwargs)\n    #     else:\n    #         raise NotImplementedError\n\n    # def bayes(self, X, **kwargs):\n    #     """"""\n    #     bayesian estimation of the parameter(s)\n    #     of the distribution given data\n\n    #     Parameters\n    #     ----------\n    #     X : (sample_size, ndim) np.ndarray\n    #         observed data\n    #     """"""\n    #     self._check_input(X)\n    #     if hasattr(self, ""_bayes""):\n    #         self._bayes(X, **kwargs)\n    #     else:\n    #         raise NotImplementedError\n\n    def pdf(self, X):\n        """"""\n        compute probability density function\n        p(X|parameter)\n\n        Parameters\n        ----------\n        X : (sample_size, ndim) np.ndarray\n            input of the function\n\n        Returns\n        -------\n        p : (sample_size,) np.ndarray\n            value of probability density function for each input\n        """"""\n        self._check_input(X)\n        if hasattr(self, ""_pdf""):\n            return self._pdf(X)\n        else:\n            raise NotImplementedError\n\n    def draw(self, sample_size=1):\n        """"""\n        draw samples from the distribution\n\n        Parameters\n        ----------\n        sample_size : int\n            sample size\n\n        Returns\n        -------\n        sample : (sample_size, ndim) np.ndarray\n            generated samples from the distribution\n        """"""\n        assert isinstance(sample_size, int)\n        if hasattr(self, ""_draw""):\n            return self._draw(sample_size)\n        else:\n            raise NotImplementedError\n\n    def _check_input(self, X):\n        assert isinstance(X, np.ndarray)\n'"
books/PRML/PRML-master-Python/prml/rv/students_t.py,0,"b'import numpy as np\nfrom scipy.special import gamma, digamma\nfrom prml.rv.rv import RandomVariable\n\n\nclass StudentsT(RandomVariable):\n    """"""\n    Student\'s t-distribution\n    p(x|mu, tau, dof)\n    = (1 + tau * (x - mu)^2 / dof)^-(D + dof)/2 / const.\n    """"""\n\n    def __init__(self, mu=None, tau=None, dof=None):\n        super().__init__()\n        self.mu = mu\n        self.tau = tau\n        self.dof = dof\n\n    @property\n    def mu(self):\n        return self.parameter[""mu""]\n\n    @mu.setter\n    def mu(self, mu):\n        if isinstance(mu, (int, float, np.number)):\n            self.parameter[""mu""] = np.array(mu)\n        elif isinstance(mu, np.ndarray):\n            self.parameter[""mu""] = mu\n        else:\n            assert mu is None\n            self.parameter[""mu""] = None\n\n    @property\n    def tau(self):\n        return self.parameter[""tau""]\n\n    @tau.setter\n    def tau(self, tau):\n        if isinstance(tau, (int, float, np.number)):\n            tau = np.array(tau)\n            assert tau.shape == self.shape\n            self.parameter[""tau""] = tau\n        elif isinstance(tau, np.ndarray):\n            assert tau.shape == self.shape\n            self.parameter[""tau""] = tau\n        else:\n            assert tau is None\n            self.parameter[""tau""] = None\n\n    @property\n    def dof(self):\n        return self.parameter[""dof""]\n\n    @dof.setter\n    def dof(self, dof):\n        if isinstance(dof, (int, float, np.number)):\n            self.parameter[""dof""] = dof\n        else:\n            assert dof is None\n            self.parameter[""dof""] = None\n\n    @property\n    def ndim(self):\n        if hasattr(self.mu, ""ndim""):\n            return self.mu.ndim\n        else:\n            return None\n\n    @property\n    def size(self):\n        if hasattr(self.mu, ""size""):\n            return self.mu.size\n        else:\n            return None\n\n    @property\n    def shape(self):\n        if hasattr(self.mu, ""shape""):\n            return self.mu.shape\n        else:\n            return None\n\n    def _fit(self, X, learning_rate=0.01):\n        self.mu = np.mean(X, axis=0)\n        self.tau = 1 / np.var(X, axis=0)\n        self.dof = 1\n        params = np.hstack(\n            (self.mu.ravel(),\n             self.tau.ravel(),\n             self.dof)\n        )\n        while True:\n            E_eta, E_lneta = self._expectation(X)\n            self._maximization(X, E_eta, E_lneta, learning_rate)\n            new_params = np.hstack(\n                (self.mu.ravel(),\n                 self.tau.ravel(),\n                 self.dof)\n            )\n            if np.allclose(params, new_params):\n                break\n            else:\n                params = new_params\n\n    def _expectation(self, X):\n        d = X - self.mu\n        a = 0.5 * (self.dof + 1)\n        b = 0.5 * (self.dof + self.tau * d ** 2)\n        E_eta = a / b\n        E_lneta = digamma(a) - np.log(b)\n        return E_eta, E_lneta\n\n    def _maximization(self, X, E_eta, E_lneta, learning_rate):\n        self.mu = np.sum(E_eta * X, axis=0) / np.sum(E_eta, axis=0)\n        d = X - self.mu\n        self.tau = 1 / np.mean(E_eta * d ** 2, axis=0)\n        N = len(X)\n        self.dof += learning_rate * 0.5 * (\n            N * np.log(0.5 * self.dof) + N\n            - N * digamma(0.5 * self.dof)\n            + np.sum(E_lneta - E_eta, axis=0)\n        )\n\n    def _pdf(self, X):\n        d = X - self.mu\n        D_sq = self.tau * d ** 2\n        return (\n            gamma(0.5 * (self.dof + 1))\n            * self.tau ** 0.5\n            * (1 + D_sq / self.dof) ** (-0.5 * (1 + self.dof))\n            / gamma(self.dof * 0.5)\n            / (np.pi * self.dof) ** 0.5\n        )\n'"
books/PRML/PRML-master-Python/prml/rv/uniform.py,0,"b'import numpy as np\nfrom prml.rv.rv import RandomVariable\n\n\nclass Uniform(RandomVariable):\n    """"""\n    Uniform distribution\n    p(x|a, b)\n    = 1 / ((b_0 - a_0) * (b_1 - a_1)) if a <= x <= b else 0\n    """"""\n\n    def __init__(self, low, high):\n        """"""\n        construct uniform distribution\n\n        Parameters\n        ----------\n        low : int, float, or np.ndarray\n            lower boundary\n        high : int, float, or np.ndarray\n            higher boundary\n        """"""\n        super().__init__()\n        low = np.asarray(low)\n        high = np.asarray(high)\n        assert low.shape == high.shape\n        assert (low <= high).all()\n        self.low = low\n        self.high = high\n        self.value = 1 / np.prod(high - low)\n\n    @property\n    def low(self):\n        return self.parameter[""low""]\n\n    @low.setter\n    def low(self, low):\n        self.parameter[""low""] = low\n\n    @property\n    def high(self):\n        return self.parameter[""high""]\n\n    @high.setter\n    def high(self, high):\n        self.parameter[""high""] = high\n\n    @property\n    def ndim(self):\n        return self.low.ndim\n\n    @property\n    def size(self):\n        return self.low.size\n\n    @property\n    def shape(self):\n        return self.low.shape\n\n    @property\n    def mean(self):\n        return 0.5 * (self.low + self.high)\n\n    def _pdf(self, X):\n        higher = np.logical_and.reduce(X >= self.low, 1)\n        lower = np.logical_and.reduce(X <= self.high, 1)\n        return self.value * np.logical_and(higher, lower)\n\n    def _draw(self, sample_size=1):\n        u01 = np.random.uniform(size=(sample_size,) + self.shape)\n        return u01 * (self.high - self.low) + self.low\n'"
books/PRML/PRML-master-Python/prml/rv/variational_gaussian_mixture.py,0,"b'import numpy as np\nfrom scipy.misc import logsumexp\nfrom scipy.special import digamma, gamma\nfrom prml.rv.rv import RandomVariable\n\n\nclass VariationalGaussianMixture(RandomVariable):\n\n    def __init__(self, n_components=1, alpha0=None, m0=None, W0=1., dof0=None, beta0=1.):\n        """"""\n        construct variational gaussian mixture model\n        Parameters\n        ----------\n        n_components : int\n            maximum numnber of gaussian components\n        alpha0 : float\n            parameter of prior dirichlet distribution\n        m0 : float\n            mean parameter of prior gaussian distribution\n        W0 : float\n            mean of the prior Wishart distribution\n        dof0 : float\n            number of degrees of freedom of the prior Wishart distribution\n        beta0 : float\n            prior on the precision distribution\n        """"""\n        super().__init__()\n        self.n_components = n_components\n        if alpha0 is None:\n            self.alpha0 = 1 / n_components\n        else:\n            self.alpha0 = alpha0\n        self.m0 = m0\n        self.W0 = W0\n        self.dof0 = dof0\n        self.beta0 = beta0\n\n    def _init_params(self, X):\n        sample_size, self.ndim = X.shape\n        self.alpha0 = np.ones(self.n_components) * self.alpha0\n        if self.m0 is None:\n            self.m0 = np.mean(X, axis=0)\n        else:\n            self.m0 = np.zeros(self.ndim) + self.m0\n        self.W0 = np.eye(self.ndim) * self.W0\n        if self.dof0 is None:\n            self.dof0 = self.ndim\n\n        self.component_size = sample_size / self.n_components + np.zeros(self.n_components)\n        self.alpha = self.alpha0 + self.component_size\n        self.beta = self.beta0 + self.component_size\n        indices = np.random.choice(sample_size, self.n_components, replace=False)\n        self.mu = X[indices]\n        self.W = np.tile(self.W0, (self.n_components, 1, 1))\n        self.dof = self.dof0 + self.component_size\n\n    @property\n    def alpha(self):\n        return self.parameter[""alpha""]\n\n    @alpha.setter\n    def alpha(self, alpha):\n        self.parameter[""alpha""] = alpha\n\n    @property\n    def beta(self):\n        return self.parameter[""beta""]\n\n    @beta.setter\n    def beta(self, beta):\n        self.parameter[""beta""] = beta\n\n    @property\n    def mu(self):\n        return self.parameter[""mu""]\n\n    @mu.setter\n    def mu(self, mu):\n        self.parameter[""mu""] = mu\n\n    @property\n    def W(self):\n        return self.parameter[""W""]\n\n    @W.setter\n    def W(self, W):\n        self.parameter[""W""] = W\n\n    @property\n    def dof(self):\n        return self.parameter[""dof""]\n\n    @dof.setter\n    def dof(self, dof):\n        self.parameter[""dof""] = dof\n\n    def get_params(self):\n        return self.alpha, self.beta, self.mu, self.W, self.dof\n\n    def _fit(self, X, iter_max=100):\n        self._init_params(X)\n        for _ in range(iter_max):\n            params = np.hstack([p.flatten() for p in self.get_params()])\n            r = self._variational_expectation(X)\n            self._variational_maximization(X, r)\n            if np.allclose(params, np.hstack([p.flatten() for p in self.get_params()])):\n                break\n\n    def _variational_expectation(self, X):\n        d = X[:, None, :] - self.mu\n        maha_sq = -0.5 * (\n            self.ndim / self.beta\n            + self.dof * np.sum(\n                np.einsum(""kij,nkj->nki"", self.W, d) * d, axis=-1))\n        ln_pi = digamma(self.alpha) - digamma(self.alpha.sum())\n        ln_Lambda = digamma(0.5 * (self.dof - np.arange(self.ndim)[:, None])).sum(axis=0) + self.ndim * np.log(2) + np.linalg.slogdet(self.W)[1]\n        ln_r = ln_pi + 0.5 * ln_Lambda + maha_sq\n        ln_r -= logsumexp(ln_r, axis=-1)[:, None]\n        r = np.exp(ln_r)\n        return r\n\n    def _variational_maximization(self, X, r):\n        self.component_size = r.sum(axis=0)\n        Xm = (X.T.dot(r) / self.component_size).T\n        d = X[:, None, :] - Xm\n        S = np.einsum(\'nki,nkj->kij\', d, r[:, :, None] * d) / self.component_size[:, None, None]\n        self.alpha = self.alpha0 + self.component_size\n        self.beta = self.beta0 + self.component_size\n        self.mu = (self.beta0 * self.m0 + self.component_size[:, None] * Xm) / self.beta[:, None]\n        d = Xm - self.m0\n        self.W = np.linalg.inv(\n            np.linalg.inv(self.W0)\n            + (self.component_size * S.T).T\n            + (self.beta0 * self.component_size * np.einsum(\'ki,kj->kij\', d, d).T / (self.beta0 + self.component_size)).T)\n        self.dof = self.dof0 + self.component_size\n\n    def classify(self, X):\n        """"""\n        index of highest posterior of the latent variable\n        Parameters\n        ----------\n        X : (sample_size, ndim) ndarray\n            input\n        Returns\n        -------\n        output : (sample_size, n_components) ndarray\n            index of maximum posterior of the latent variable\n        """"""\n        return np.argmax(self._variational_expectation(X), 1)\n\n    def classify_proba(self, X):\n        """"""\n        compute posterior of the latent variable\n        Parameters\n        ----------\n        X : (sample_size, ndim) ndarray\n            input\n        Returns\n        -------\n        output : (sample_size, n_components) ndarray\n            posterior of the latent variable\n        """"""\n        return self._variational_expectation(X)\n\n    def student_t(self, X):\n        nu = self.dof + 1 - self.ndim\n        L = (nu * self.beta * self.W.T / (1 + self.beta)).T\n        d = X[:, None, :] - self.mu\n        maha_sq = np.sum(np.einsum(\'nki,kij->nkj\', d, L) * d, axis=-1)\n        return (\n            gamma(0.5 * (nu + self.ndim))\n            * np.sqrt(np.linalg.det(L))\n            * (1 + maha_sq / nu) ** (-0.5 * (nu + self.ndim))\n            / (gamma(0.5 * nu) * (nu * np.pi) ** (0.5 * self.ndim)))\n\n    def _pdf(self, X):\n        return (self.alpha * self.student_t(X)).sum(axis=-1) / self.alpha.sum()\n'"
books/PRML/PRML-master-Python/prml/sampling/__init__.py,0,"b'from prml.sampling.metropolis import metropolis\nfrom prml.sampling.metropolis_hastings import metropolis_hastings\nfrom prml.sampling.rejection_sampling import rejection_sampling\nfrom prml.sampling.sir import sir\n\n\n__all__ = [\n    ""metropolis"",\n    ""metropolis_hastings"",\n    ""rejection_sampling"",\n    ""sir""\n]\n'"
books/PRML/PRML-master-Python/prml/sampling/metropolis.py,0,"b'import random\nimport numpy as np\n\n\ndef metropolis(func, rv, n, downsample=1):\n    """"""\n    Metropolis algorithm\n\n    Parameters\n    ----------\n    func : callable\n        (un)normalized distribution to be sampled from\n    rv : RandomVariable\n        proposal distribution which is symmetric at the origin\n    n : int\n        number of samples to draw\n    downsample : int\n        downsampling factor\n\n    Returns\n    -------\n    sample : (n, ndim) ndarray\n        generated sample\n    """"""\n    x = np.zeros((1, rv.ndim))\n    sample = []\n    for i in range(n * downsample):\n        x_new = x + rv.draw()\n        accept_proba = func(x_new) / func(x)\n        if random.random() < accept_proba:\n            x = x_new\n        if i % downsample == 0:\n            sample.append(x[0])\n    sample = np.asarray(sample)\n    assert sample.shape == (n, rv.ndim), sample.shape\n    return sample\n'"
books/PRML/PRML-master-Python/prml/sampling/metropolis_hastings.py,0,"b'import random\nimport numpy as np\n\n\ndef metropolis_hastings(func, rv, n, downsample=1):\n    """"""\n    Metropolis Hastings algorith\n\n    Parameters\n    ----------\n    func : callable\n        (un)normalized distribution to be sampled from\n    rv : RandomVariable\n        proposal distribution\n    n : int\n        number of samples to draw\n    downsample : int\n        downsampling factor\n\n    Returns\n    -------\n    sample : (n, ndim) ndarray\n        generated sample\n    """"""\n    x = np.zeros((1, rv.ndim))\n    sample = []\n    for i in range(n * downsample):\n        x_new = x + rv.draw()\n        accept_proba = func(x_new) * rv.pdf(x - x_new) / (func(x) * rv.pdf(x_new - x))\n        if random.random() < accept_proba:\n            x = x_new\n        if i % downsample == 0:\n            sample.append(x[0])\n    sample = np.asarray(sample)\n    assert sample.shape == (n, rv.ndim), sample.shape\n    return sample\n'"
books/PRML/PRML-master-Python/prml/sampling/rejection_sampling.py,0,"b'import random\nimport numpy as np\n\n\ndef rejection_sampling(func, rv, k, n):\n    """"""\n    perform rejection sampling n times\n\n    Parameters\n    ----------\n    func : callable\n        (un)normalized distribution to be sampled from\n    rv : RandomVariable\n        distribution to generate sample\n    k : float\n        constant to be multiplied with the distribution\n    n : int\n        number of samples to draw\n\n    Returns\n    -------\n    sample : (n, ndim) ndarray\n        generated sample\n    """"""\n    assert hasattr(rv, ""draw""), ""the distribution has no method to draw random samples""\n    sample = []\n    while len(sample) < n:\n        sample_candidate = rv.draw()\n        accept_proba = func(sample_candidate) / (k * rv.pdf(sample_candidate))\n        if random.random() < accept_proba:\n            sample.append(sample_candidate[0])\n    sample = np.asarray(sample)\n    assert sample.shape == (n, rv.ndim), sample.shape\n    return sample\n'"
books/PRML/PRML-master-Python/prml/sampling/sir.py,0,"b'import numpy as np\n\n\ndef sir(func, rv, n):\n    """"""\n    sampling-importance-resampling\n\n    Parameters\n    ----------\n    func : callable\n        (un)normalized distribution to be sampled from\n    rv : RandomVariable\n        distribution to generate sample\n    n : int\n        number of samples to draw\n\n    Returns\n    -------\n    sample : (n, ndim) ndarray\n        generated sample\n    """"""\n    assert hasattr(rv, ""draw""), ""the distribution has no method to draw random samples""\n    sample_candidate = rv.draw(n * 10)\n    weight = np.squeeze(func(sample_candidate) / rv.pdf(sample_candidate))\n    assert weight.shape == (n * 10,), weight.shape\n    weight /= np.sum(weight)\n    index = np.random.choice(n * 10, n, p=weight)\n    sample = sample_candidate[index]\n    return sample\n'"
books/PRML/PRML-master-Python/test/nn/__init__.py,0,b''
books/PRML/PRML-master-Python/test/test_bayesnet/__init__.py,0,b''
books/PRML/PRML-master-Python/test/test_bayesnet/test_discrete.py,0,"b'import unittest\nimport numpy as np\nfrom prml import bayesnet as bn\n\n\nclass TestDiscrete(unittest.TestCase):\n\n    def test_discrete(self):\n        a = bn.discrete([0.2, 0.8])\n        b = bn.discrete([[0.1, 0.2], [0.9, 0.8]], a)\n        self.assertTrue(np.allclose(b.proba, [0.18, 0.82]))\n        a.observe(0)\n        self.assertTrue(np.allclose(b.proba, [0.1, 0.9]))\n\n        a = bn.discrete([0.1, 0.9])\n        b = bn.discrete([[0.7, 0.2], [0.3, 0.8]], a)\n        c = bn.discrete([[0.8, 0.4], [0.2, 0.6]], b)\n        self.assertTrue(np.allclose(a.proba, [0.1, 0.9]))\n        self.assertTrue(np.allclose(b.proba, [0.25, 0.75]))\n        self.assertTrue(np.allclose(c.proba, [0.5, 0.5]))\n        c.observe(0)\n        self.assertTrue(np.allclose(a.proba, [0.136, 0.864]))\n        self.assertTrue(np.allclose(b.proba, [0.4, 0.6]))\n        self.assertTrue(np.allclose(c.proba, [1, 0]))\n\n        a = bn.discrete([0.1, 0.9], name=""p(a)"")\n        b = bn.discrete([0.1, 0.9], name=""p(b)"")\n        c = bn.discrete(\n            [[[0.9, 0.8],\n              [0.8, 0.2]],\n             [[0.1, 0.2],\n              [0.2, 0.8]]],\n            a, b, name=""p(c|a,b)""\n        )\n        c.observe(0)\n        self.assertTrue(np.allclose(a.proba, [0.25714286, 0.74285714]))\n        b.observe(0)\n        self.assertTrue(np.allclose(a.proba, [0.11111111, 0.88888888]))\n\n        a = bn.discrete([0.1, 0.9], name=""p(a)"")\n        b = bn.discrete([0.1, 0.9], name=""p(b)"")\n        c = bn.discrete(\n            [[[0.9, 0.8],\n              [0.8, 0.2]],\n             [[0.1, 0.2],\n              [0.2, 0.8]]],\n            a, b, name=""p(c|a,b)""\n        )\n        c.observe(0, proprange=0)\n        self.assertTrue(np.allclose(a.proba, [0.1, 0.9]))\n        b.observe(0, proprange=1)\n        self.assertTrue(np.allclose(a.proba, [0.1, 0.9]))\n        b.send_message(proprange=2)\n        self.assertTrue(np.allclose(a.proba, [0.11111111, 0.88888888]))\n        a.send_message()\n        c.send_message()\n        self.assertTrue(np.allclose(a.proba, [0.11111111, 0.88888888]), a.message_from)\n\n        a = bn.discrete([0.1, 0.9], name=""p(a)"")\n        b = bn.discrete([0.1, 0.9], name=""p(b)"")\n        c = bn.discrete(\n            [[[0.9, 0.8],\n              [0.8, 0.2]],\n             [[0.1, 0.2],\n              [0.2, 0.8]]],\n            a, b, name=""p(c|a,b)""\n        )\n        b.observe(0)\n        self.assertTrue(np.allclose(a.proba, [0.1, 0.9]))\n        c.send_message()\n        self.assertTrue(np.allclose(a.proba, [0.1, 0.9]), a.message_from)\n\n    def test_joint_discrete(self):\n        a = bn.DiscreteVariable(2)\n        b = bn.DiscreteVariable(2)\n        bn.discrete([[0.1, 0.2], [0.3, 0.4]], out=[a, b])\n        b.observe(1)\n        self.assertTrue(np.allclose(a.proba, [1/3, 2/3]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
books/PRML/PRML-master-Python/prml/nn/array/__init__.py,0,"b'from prml.nn.array.broadcast import broadcast_to\nfrom prml.nn.array.flatten import flatten\nfrom prml.nn.array.reshape import reshape, reshape_method\nfrom prml.nn.array.split import split\nfrom prml.nn.array.transpose import transpose, transpose_method\nfrom prml.nn.tensor.tensor import Tensor\n\n\nTensor.flatten = flatten\nTensor.reshape = reshape_method\nTensor.transpose = transpose_method\n\n__all__ = [\n    ""broadcast_to"",\n    ""flatten"",\n    ""reshape"",\n    ""split"",\n    ""transpose""\n]\n'"
books/PRML/PRML-master-Python/prml/nn/array/broadcast.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass BroadcastTo(Function):\n    """"""\n    Broadcast a tensor to an new shape\n    """"""\n\n    def forward(self, x, shape):\n        x = self._convert2tensor(x)\n        self.x = x\n        output = np.broadcast_to(x.value, shape)\n        if isinstance(self.x, Constant):\n            return Constant(output)\n        return Tensor(output, function=self)\n\n    def backward(self, delta):\n        dx = delta\n        if delta.ndim != self.x.ndim:\n            dx = dx.sum(axis=tuple(range(dx.ndim - self.x.ndim)))\n            if isinstance(dx, np.number):\n                dx = np.array(dx)\n        axis = tuple(i for i, len_ in enumerate(self.x.shape) if len_ == 1)\n        if axis:\n            dx = dx.sum(axis=axis, keepdims=True)\n        self.x.backward(dx)\n\n\ndef broadcast_to(x, shape):\n    """"""\n    Broadcast a tensor to an new shape\n    """"""\n    return BroadcastTo().forward(x, shape)\n'"
books/PRML/PRML-master-Python/prml/nn/array/flatten.py,0,"b'from prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Flatten(Function):\n    """"""\n    flatten array\n    """"""\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self._atleast_ndim(x, 2)\n        self.x = x\n        if isinstance(self.x, Constant):\n            return Constant(x.value.flatten())\n        return Tensor(x.value.flatten(), function=self)\n\n    def backward(self, delta):\n        dx = delta.reshape(*self.x.shape)\n        self.x.backward(dx)\n\n\ndef flatten(x):\n    """"""\n    flatten N-dimensional array (N >= 2)\n    """"""\n    return Flatten().forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/array/reshape.py,0,"b'from prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Reshape(Function):\n    """"""\n    reshape array\n    """"""\n\n    def forward(self, x, shape):\n        x = self._convert2tensor(x)\n        self._atleast_ndim(x, 1)\n        self.x = x\n        if isinstance(self.x, Constant):\n            return Constant(x.value.reshape(*shape))\n        return Tensor(x.value.reshape(*shape), function=self)\n\n    def backward(self, delta):\n        dx = delta.reshape(*self.x.shape)\n        self.x.backward(dx)\n\n\ndef reshape(x, shape):\n    """"""\n    reshape N-dimensional array (N >= 1)\n    """"""\n    return Reshape().forward(x, shape)\n\n\ndef reshape_method(x, *args):\n    return Reshape().forward(x, args)\n'"
books/PRML/PRML-master-Python/prml/nn/array/split.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Nth(Function):\n\n    def __init__(self, n):\n        self.n = n\n\n    def forward(self, x):\n        self.x = x\n        if isinstance(self.x, Constant):\n            return Constant(x.value)\n        return Tensor(x.value, function=self)\n\n    def backward(self, delta):\n        self.x.backward(delta, n=self.n)\n\n\nclass Split(Function):\n\n    def __init__(self, indices_or_sections, axis=-1):\n        self.indices_or_sections = indices_or_sections\n        self.axis = axis\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self._atleast_ndim(x, 1)\n        self.x = x\n        output = np.split(x.value, self.indices_or_sections, self.axis)\n        if isinstance(self.x, Constant):\n            return tuple([Constant(out) for out in output])\n        self.n_output = len(output)\n        self.delta = [None for _ in output]\n        return tuple([Tensor(out, function=self) for out in output])\n\n    def backward(self, delta, n):\n        self.delta[n] = delta\n        if all([d is not None for d in self.delta]):\n            dx = np.concatenate(self.delta, axis=self.axis)\n            self.x.backward(dx)\n\n\ndef split(x, indices_or_sections, axis=-1):\n    output = Split(indices_or_sections, axis).forward(x)\n    return tuple([Nth(i).forward(out) for i, out in enumerate(output)])\n'"
books/PRML/PRML-master-Python/prml/nn/array/transpose.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Transpose(Function):\n\n    def __init__(self, axes=None):\n        self.axes = axes\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        if self.axes is not None:\n            self._equal_ndim(x, len(self.axes))\n        self.x = x\n        if isinstance(self.x, Constant):\n            return Constant(np.transpose(x.value, self.axes))\n        return Tensor(np.transpose(x.value, self.axes), function=self)\n\n    def backward(self, delta):\n        if self.axes is None:\n            dx = np.transpose(delta)\n        else:\n            dx = np.transpose(delta, np.argsort(self.axes))\n        self.x.backward(dx)\n\n\ndef transpose(x, axes=None):\n    return Transpose(axes).forward(x)\n\n\ndef transpose_method(x, *args):\n    if args == ():\n        args = None\n    return Transpose(args).forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/image/__init__.py,0,"b'from prml.nn.image.convolve2d import convolve2d\nfrom prml.nn.image.max_pooling2d import max_pooling2d\n\n\n__all__ = [\n    ""convolve2d"",\n    ""max_pooling2d""\n]\n'"
books/PRML/PRML-master-Python/prml/nn/image/convolve2d.py,0,"b'import numpy as np\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\nfrom prml.nn.image.util import img2patch, patch2img\n\n\nclass Convolve2d(Function):\n\n    def __init__(self, stride, pad):\n        """"""\n        construct 2 dimensional convolution function\n\n        Parameters\n        ----------\n        stride : int or tuple of ints\n            stride of kernel application\n        pad : int or tuple of ints\n            padding image\n        """"""\n        self.stride = self._check_tuple(stride, ""stride"")\n        self.pad = self._check_tuple(pad, ""pad"")\n        self.pad = (0,) + self.pad + (0,)\n\n    def _check_tuple(self, tup, name):\n        if isinstance(tup, int):\n            tup = (tup,) * 2\n        if not isinstance(tup, tuple):\n            raise TypeError(\n                ""Unsupported type for {}: {}"".format(name, type(tup))\n            )\n        if len(tup) != 2:\n            raise ValueError(\n                ""the length of {} must be 2, not {}"".format(name, len(tup))\n            )\n        if not all([isinstance(n, int) for n in tup]):\n            raise TypeError(\n                ""Unsuported type for {}"".format(name)\n            )\n        if not all([n >= 0 for n in tup]):\n            raise ValueError(\n                ""{} must be non-negative values"".format(name)\n            )\n        return tup\n\n    def _check_input(self, x, y):\n        x = self._convert2tensor(x)\n        y = self._convert2tensor(y)\n        self._equal_ndim(x, 4)\n        self._equal_ndim(y, 4)\n        if x.shape[3] != y.shape[2]:\n            raise ValueError(\n                ""shapes {} and {} not aligned: {} (dim 3) != {} (dim 2)""\n                .format(x.shape, y.shape, x.shape[3], y.shape[2])\n            )\n        return x, y\n\n    def forward(self, x, y):\n        x, y = self._check_input(x, y)\n        self.x = x\n        self.y = y\n        img = np.pad(x.value, [(p,) for p in self.pad], ""constant"")\n        self.shape = img.shape\n        self.patch = img2patch(img, y.shape[:2], self.stride)\n        return Tensor(np.tensordot(self.patch, y.value, axes=((3, 4, 5), (0, 1, 2))), function=self)\n\n    def backward(self, delta):\n        dx = patch2img(\n            np.tensordot(delta, self.y.value, (3, 3)),\n            self.stride,\n            self.shape\n        )\n        slices = [slice(p, len_ - p) for p, len_ in zip(self.pad, self.shape)]\n        dx = dx[slices]\n        dy = np.tensordot(self.patch, delta, axes=((0, 1, 2),) * 2)\n        self.x.backward(dx)\n        self.y.backward(dy)\n\n\ndef convolve2d(x, y, stride=1, pad=0):\n    """"""\n    returns convolution of two tensors\n\n    Parameters\n    ----------\n    x : (n_batch, xlen, ylen, in_channel) Tensor\n        input tensor to be convolved\n    y : (kx, ky, in_channel, out_channel) Tensor\n        convolution kernel\n    stride : int or tuple of ints (sx, sy)\n        stride of kernel application\n    pad : int or tuple of ints (px, py)\n        padding image\n\n    Returns\n    -------\n    output : (n_batch, xlen\', ylen\', out_channel) Tensor\n        input convolved with kernel\n        len\' = (len + p - k) // s + 1\n    """"""\n    return Convolve2d(stride, pad).forward(x, y)\n'"
books/PRML/PRML-master-Python/prml/nn/image/max_pooling2d.py,0,"b'import numpy as np\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\nfrom prml.nn.image.util import img2patch, patch2img\n\n\nclass MaxPooling2d(Function):\n\n    def __init__(self, pool_size, stride, pad):\n        """"""\n        construct 2 dimensional max-pooling function\n\n        Parameters\n        ----------\n        pool_size : int or tuple of ints\n            pooling size\n        stride : int or tuple of ints\n            stride of kernel application\n        pad : int or tuple of ints\n            padding image\n        """"""\n        self.pool_size = self._check_tuple(pool_size, ""pool_size"")\n        self.stride = self._check_tuple(stride, ""stride"")\n        self.pad = self._check_tuple(pad, ""pad"")\n        self.pad = (0,) + self.pad + (0,)\n\n    def _check_tuple(self, tup, name):\n        if isinstance(tup, int):\n            tup = (tup,) * 2\n        if not isinstance(tup, tuple):\n            raise TypeError(\n                ""Unsupported type for {}: {}"".format(name, type(tup))\n            )\n        if len(tup) != 2:\n            raise ValueError(\n                ""the length of {} must be 2, not {}"".format(name, len(tup))\n            )\n        if not all([isinstance(n, int) for n in tup]):\n            raise TypeError(\n                ""Unsuported type for {}"".format(name)\n            )\n        if not all([n >= 0 for n in tup]):\n            raise ValueError(\n                ""{} must be non-negative values"".format(name)\n            )\n        return tup\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self._equal_ndim(x, 4)\n        self.x = x\n        img = np.pad(x.value, [(p,) for p in self.pad], ""constant"")\n        patch = img2patch(img, self.pool_size, self.stride)\n        n_batch, xlen_out, ylen_out, _, _, in_channels = patch.shape\n        patch = patch.reshape(n_batch, xlen_out, ylen_out, -1, in_channels)\n        self.shape = img.shape\n        self.index = patch.argmax(axis=3)\n        return Tensor(patch.max(axis=3), function=self)\n\n    def backward(self, delta):\n        delta_patch = np.zeros(delta.shape + (np.prod(self.pool_size),))\n        index = np.where(delta == delta) + (self.index.ravel(),)\n        delta_patch[index] = delta.ravel()\n        delta_patch = np.reshape(delta_patch, delta.shape + self.pool_size)\n        delta_patch = delta_patch.transpose(0, 1, 2, 4, 5, 3)\n        dx = patch2img(delta_patch, self.stride, self.shape)\n        slices = [slice(p, len_ - p) for p, len_ in zip(self.pad, self.shape)]\n        dx = dx[slices]\n        self.x.backward(dx)\n\n\ndef max_pooling2d(x, pool_size, stride=1, pad=0):\n    """"""\n    spatial max pooling\n\n    Parameters\n    ----------\n    x : (n_batch, xlen, ylen, in_channel) Tensor\n        input tensor\n    pool_size : int or tuple of ints (kx, ky)\n        pooling size\n    stride : int or tuple of ints (sx, sy)\n        stride of pooling application\n    pad : int or tuple of ints (px, py)\n        padding input\n\n    Returns\n    -------\n    output : (n_batch, xlen\', ylen\', out_channel) Tensor\n        max pooled image\n        len\' = (len + p - k) // s + 1\n    """"""\n    return MaxPooling2d(pool_size, stride, pad).forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/image/util.py,0,"b'import itertools\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\n\ndef img2patch(img, size, step=1):\n    """"""\n    convert batch of image array into patches\n    Parameters\n    ----------\n    img : (n_batch, xlen_in, ylen_in, in_channels) ndarray\n        batch of images\n    size : tuple or int\n        patch size\n    step : tuple or int\n        stride of patches\n    Returns\n    -------\n    patch : (n_batch, xlen_out, ylen_out, size, size, in_channels) ndarray\n        batch of patches at all points\n        len_out = (len_in - size) // step + 1\n    """"""\n    ndim = img.ndim\n    if isinstance(size, int):\n        size = (size,) * (ndim - 2)\n    if isinstance(step, int):\n        step = (step,) * (ndim - 2)\n\n    slices = [slice(None, None, s) for s in step]\n    window_strides = img.strides[1:]\n    index_strides = img[[slice(None)] + slices].strides[:-1]\n\n    out_shape = tuple(\n        np.subtract(img.shape[1: -1], size) // np.array(step) + 1)\n    out_shape = (len(img),) + out_shape + size + (np.size(img, -1),)\n    strides = index_strides + window_strides\n    patch = as_strided(img, shape=out_shape, strides=strides)\n    return patch\n\n\ndef patch2img(x, stride, shape):\n    """"""\n    sum up patches and form an image\n    Parameters\n    ----------\n    x : (n_batch, xlen_in, ylen_in, kx, ky, in_channels) ndarray\n        batch of patches at all points\n    stride : tuple\n        applied stride to take patches\n    shape : (n_batch, xlen_out, ylen_out, in_channels) tuple\n        output image shape\n    Returns\n    -------\n    img : (n_batch, len_out, ylen_out, in_channels) ndarray\n        image\n    """"""\n    img = np.zeros(shape, dtype=np.float32)\n    kx, ky = x.shape[3: 5]\n    for i, j in itertools.product(range(kx), range(ky)):\n        slices = [slice(b, b + s * len_, s) for b, s, len_ in zip([i, j], stride, x.shape[1: 3])]\n        img[[slice(None)] + slices] += x[..., i, j, :]\n    return img\n'"
books/PRML/PRML-master-Python/prml/nn/linalg/__init__.py,0,"b'from prml.nn.linalg.cholesky import cholesky\nfrom prml.nn.linalg.det import det\nfrom prml.nn.linalg.inv import inv\nfrom prml.nn.linalg.logdet import logdet\nfrom prml.nn.linalg.solve import solve\nfrom prml.nn.linalg.trace import trace\n\n\n__all__ = [\n    ""cholesky"",\n    ""det"",\n    ""inv"",\n    ""logdet"",\n    ""solve"",\n    ""trace""\n]\n'"
books/PRML/PRML-master-Python/prml/nn/linalg/cholesky.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Cholesky(Function):\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        self.output = np.linalg.cholesky(x.value)\n        if isinstance(self.x, Constant):\n            return Constant(self.output)\n        return Tensor(self.output, function=self)\n\n    def backward(self, delta):\n        delta_lower = np.tril(delta)\n        P = phi(self.output.T @ delta_lower)\n        S = np.linalg.solve(\n            self.output.T,\n            P @ np.linalg.inv(self.output)\n        )\n        dx = S + S.T + np.diag(np.diag(S))\n        self.x.backward(dx)\n\n\ndef phi(x):\n    return 0.5 * (np.tril(x) + np.tril(x, -1))\n\n\ndef cholesky(x):\n    """"""\n    cholesky decomposition of positive-definite matrix\n    x = LL^T\n    Parameters\n    ----------\n    x : (d, d) tensor_like\n        positive-definite matrix\n    Returns\n    -------\n    L : (d, d)\n        cholesky decomposition\n    """"""\n    return Cholesky().forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/linalg/det.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Determinant(Function):\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        self._equal_ndim(x, 2)\n        self.output = np.linalg.det(x.value)\n        if isinstance(self.x, Constant):\n            return Constant(self.output)\n        return Tensor(self.output, function=self)\n\n    def backward(self, delta):\n        dx = delta * self.output * np.linalg.inv(self.x.value.T)\n        self.x.backward(dx)\n\n\ndef det(x):\n    """"""\n    determinant of a matrix\n    Parameters\n    ----------\n    x : (d, d) tensor_like\n        a matrix to compute its determinant\n    Returns\n    -------\n    output : (d, d) tensor_like\n        determinant of the input matrix\n    """"""\n    return Determinant().forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/linalg/inv.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Inverse(Function):\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        self._equal_ndim(x, 2)\n        self.output = np.linalg.inv(x.value)\n        if isinstance(self.x, Constant):\n            return Constant(self.output)\n        return Tensor(self.output, function=self)\n\n    def backward(self, delta):\n        dx = -self.output.T @ delta @ self.output.T\n        self.x.backward(dx)\n\n\ndef inv(x):\n    """"""\n    inverse of a matrix\n    Parameters\n    ----------\n    x : (d, d) tensor_like\n        a matrix to be inverted\n    Returns\n    -------\n    output : (d, d) tensor_like\n        inverse of the input\n    """"""\n    return Inverse().forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/linalg/logdet.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass LogDeterminant(Function):\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        self._equal_ndim(x, 2)\n        sign, self.output = np.linalg.slogdet(x.value)\n        if sign != 1:\n            raise ValueError(""matrix has to be positive-definite"")\n        if isinstance(self.x, Constant):\n            return Constant(self.output)\n        return Tensor(self.output, function=self)\n\n    def backward(self, delta):\n        dx = delta * np.linalg.inv(self.x.value.T)\n        self.x.backward(dx)\n\n\ndef logdet(x):\n    """"""\n    log determinant of a matrix\n    Parameters\n    ----------\n    x : (d, d) tensor_like\n        a matrix to compute its log determinant\n    Returns\n    -------\n    output : (d, d) tensor_like\n        determinant of the input matrix\n    """"""\n    return LogDeterminant().forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/linalg/solve.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Solve(Function):\n\n    def forward(self, a, b):\n        a = self._convert2tensor(a)\n        b = self._convert2tensor(b)\n        self._equal_ndim(a, 2)\n        self._equal_ndim(b, 2)\n        self.a = a\n        self.b = b\n        self.output = np.linalg.solve(a.value, b.value)\n        if isinstance(self.a, Constant) and isinstance(self.b, Constant):\n            return Constant(self.output)\n        return Tensor(self.output, function=self)\n\n    def backward(self, delta):\n        db = np.linalg.solve(self.a.value.T, delta)\n        da = -db @ self.output.T\n        self.a.backward(da)\n        self.b.backward(db)\n\n\ndef solve(a, b):\n    """"""\n    solve a linear matrix equation\n    ax = b\n    Parameters\n    ----------\n    a : (d, d) tensor_like\n        coefficient matrix\n    b : (d, k) tensor_like\n        dependent variable\n    Returns\n    -------\n    output : (d, k) tensor_like\n        solution of the equation\n    """"""\n    return Solve().forward(a, b)\n'"
books/PRML/PRML-master-Python/prml/nn/linalg/trace.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Trace(Function):\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self._equal_ndim(x, 2)\n        self.x = x\n        output = np.trace(x.value)\n        if isinstance(self.x, Constant):\n            return Constant(output)\n        return Tensor(output, function=self)\n\n    def backward(self, delta):\n        dx = np.eye(self.x.shape[0], self.x.shape[1]) * delta\n        self.x.backward(dx)\n\n\ndef trace(x):\n    return Trace().forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/math/__init__.py,0,"b'from prml.nn.math.add import add\nfrom prml.nn.math.divide import divide, rdivide\nfrom prml.nn.math.exp import exp\nfrom prml.nn.math.log import log\nfrom prml.nn.math.matmul import matmul, rmatmul\nfrom prml.nn.math.mean import mean\nfrom prml.nn.math.multiply import multiply\nfrom prml.nn.math.negative import negative\nfrom prml.nn.math.power import power, rpower\nfrom prml.nn.math.product import prod\nfrom prml.nn.math.sqrt import sqrt\nfrom prml.nn.math.square import square\nfrom prml.nn.math.subtract import subtract, rsubtract\nfrom prml.nn.math.sum import sum\n\n\nfrom prml.nn.tensor.tensor import Tensor\nTensor.__add__ = add\nTensor.__radd__ = add\nTensor.__truediv__ = divide\nTensor.__rtruediv__ = rdivide\nTensor.mean = mean\nTensor.__matmul__ = matmul\nTensor.__rmatmul__ = rmatmul\nTensor.__mul__ = multiply\nTensor.__rmul__ = multiply\nTensor.__neg__ = negative\nTensor.__pow__ = power\nTensor.__rpow__ = rpower\nTensor.prod = prod\nTensor.__sub__ = subtract\nTensor.__rsub__ = rsubtract\nTensor.sum = sum\n\n\n__all__ = [\n    ""add"",\n    ""divide"",\n    ""exp"",\n    ""log"",\n    ""matmul"",\n    ""mean"",\n    ""multiply"",\n    ""power"",\n    ""prod"",\n    ""sqrt"",\n    ""square"",\n    ""subtract"",\n    ""sum""\n]\n'"
books/PRML/PRML-master-Python/prml/nn/math/abs.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Abs(Function):\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        self.output = np.abs(x.value)\n        if isinstance(x, Constant):\n            return Constant(self.output)\n        self.sign = np.sign(x.value)\n        return Tensor(self.output, function=self)\n\n    def backward(self, delta):\n        dx = self.sign * delta\n        self.x.backward(dx)\n\n\ndef abs(x):\n    """"""\n    element-wise absolute function\n    """"""\n    return Abs().forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/math/add.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\nfrom prml.nn.array.broadcast import broadcast_to\n\n\nclass Add(Function):\n    """"""\n    add arguments element-wise\n    """"""\n\n    def _check_input(self, x, y):\n        x = self._convert2tensor(x)\n        y = self._convert2tensor(y)\n        if x.shape != y.shape:\n            shape = np.broadcast(x.value, y.value).shape\n            if x.shape != shape:\n                x = broadcast_to(x, shape)\n            if y.shape != shape:\n                y = broadcast_to(y, shape)\n        return x, y\n\n    def forward(self, x, y):\n        x, y = self._check_input(x, y)\n        self.x = x\n        self.y = y\n        if isinstance(self.x, Constant) and isinstance(self.y, Constant):\n            return Constant(x.value + y.value)\n        return Tensor(x.value + y.value, function=self)\n\n    def backward(self, delta):\n        dx = delta\n        dy = delta\n        self.x.backward(dx)\n        self.y.backward(dy)\n\n\ndef add(x, y):\n    return Add().forward(x, y)\n'"
books/PRML/PRML-master-Python/prml/nn/math/divide.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\nfrom prml.nn.array.broadcast import broadcast_to\n\n\nclass Divide(Function):\n    """"""\n    divide arguments element-wise\n    """"""\n\n    def _check_input(self, x, y):\n        x = self._convert2tensor(x)\n        y = self._convert2tensor(y)\n        if x.shape != y.shape:\n            shape = np.broadcast(x.value, y.value).shape\n            if x.shape != shape:\n                x = broadcast_to(x, shape)\n            if y.shape != shape:\n                y = broadcast_to(y, shape)\n        return x, y\n\n    def forward(self, x, y):\n        x, y = self._check_input(x, y)\n        self.x = x\n        self.y = y\n        if isinstance(self.x, Constant) and isinstance(self.y, Constant):\n            return Constant(x.value / y.value)\n        return Tensor(x.value / y.value, function=self)\n\n    def backward(self, delta):\n        dx = delta / self.y.value\n        dy = - delta * self.x.value / self.y.value ** 2\n        self.x.backward(dx)\n        self.y.backward(dy)\n\n\ndef divide(x, y):\n    return Divide().forward(x, y)\n\n\ndef rdivide(x, y):\n    return Divide().forward(y, x)\n'"
books/PRML/PRML-master-Python/prml/nn/math/exp.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Exp(Function):\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        self.output = np.exp(x.value)\n        if isinstance(self.x, Constant):\n            return Constant(self.output)\n        return Tensor(self.output, function=self)\n\n    def backward(self, delta):\n        dx = self.output * delta\n        self.x.backward(dx)\n\n\ndef exp(x):\n    """"""\n    element-wise exponential function\n    """"""\n    return Exp().forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/math/gamma.py,0,"b'import scipy.special as sp\nfrom prml.nn.function import Function\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\n\n\nclass Gamma(Function):\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        self.output = sp.gamma(x.value)\n        if isinstance(x, Constant):\n            return Constant(self.output)\n        return Tensor(self.output, function=self)\n\n    def backward(self, delta):\n        dx = delta * self.output * sp.digamma(self.x.value)\n        self.x.backward(dx)\n\n\ndef gamma(x):\n    """"""\n    element-wise gamma function\n    """"""\n    return Gamma().forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/math/log.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Log(Function):\n    """"""\n    element-wise natural logarithm of the input\n    y = log(x)\n    """"""\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        output = np.log(self.x.value)\n        if isinstance(self.x, Constant):\n            return Constant(output)\n        return Tensor(output, function=self)\n\n    def backward(self, delta):\n        dx = delta / self.x.value\n        self.x.backward(dx)\n\n\ndef log(x):\n    """"""\n    element-wise natural logarithm of the input\n    y = log(x)\n    """"""\n    return Log().forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/math/matmul.py,0,"b'from prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass MatMul(Function):\n    """"""\n    Matrix multiplication function\n    """"""\n\n    def _check_input(self, x, y):\n        x = self._convert2tensor(x)\n        y = self._convert2tensor(y)\n        self._equal_ndim(x, 2)\n        self._equal_ndim(y, 2)\n        if x.shape[1] != y.shape[0]:\n            raise ValueError(\n                ""shapes {} and {} not aligned: {} (dim 1) != {} (dim 0)""\n                .format(x.shape, y.shape, x.shape[1], y.shape[0])\n            )\n        return x, y\n\n    def forward(self, x, y):\n        x, y = self._check_input(x, y)\n        self.x = x\n        self.y = y\n        if isinstance(self.x, Constant) and isinstance(self.y, Constant):\n            return Constant(x.value @ y.value)\n        return Tensor(x.value @ y.value, function=self)\n\n    def backward(self, delta):\n        dx = delta @ self.y.value.T\n        dy = self.x.value.T @ delta\n        self.x.backward(dx)\n        self.y.backward(dy)\n\n\ndef matmul(x, y):\n    return MatMul().forward(x, y)\n\n\ndef rmatmul(x, y):\n    return MatMul().forward(y, x)\n'"
books/PRML/PRML-master-Python/prml/nn/math/mean.py,0,"b'from prml.nn.math.sum import sum\n\n\ndef mean(x, axis=None, keepdims=False):\n    """"""\n    returns arithmetic mean of the elements along given axis\n    """"""\n    if axis is None:\n        return sum(x, axis=None, keepdims=keepdims) / x.size\n    elif isinstance(axis, int):\n        N = x.shape[axis]\n        return sum(x, axis=axis, keepdims=keepdims) / N\n    elif isinstance(axis, tuple):\n        N = 1\n        for ax in axis:\n            N *= x.shape[ax]\n        return sum(x, axis=axis, keepdims=keepdims) / N\n    else:\n        raise TypeError(\n            ""Unsupported type for axis: {}"".format(type(axis))\n        )\n'"
books/PRML/PRML-master-Python/prml/nn/math/multiply.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\nfrom prml.nn.array.broadcast import broadcast_to\n\n\nclass Multiply(Function):\n    """"""\n    multiply arguments element-wise\n    """"""\n\n    def _check_input(self, x, y):\n        x = self._convert2tensor(x)\n        y = self._convert2tensor(y)\n        if x.shape != y.shape:\n            shape = np.broadcast(x.value, y.value).shape\n            if x.shape != shape:\n                x = broadcast_to(x, shape)\n            if y.shape != shape:\n                y = broadcast_to(y, shape)\n        return x, y\n\n    def forward(self, x, y):\n        x, y = self._check_input(x, y)\n        self.x = x\n        self.y = y\n        if isinstance(self.x, Constant) and isinstance(self.y, Constant):\n            return Constant(x.value * y.value)\n        return Tensor(x.value * y.value, function=self)\n\n    def backward(self, delta):\n        dx = self.y.value * delta\n        dy = self.x.value * delta\n        self.x.backward(dx)\n        self.y.backward(dy)\n\n\ndef multiply(x, y):\n    return Multiply().forward(x, y)\n'"
books/PRML/PRML-master-Python/prml/nn/math/negative.py,0,"b'from prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Negative(Function):\n    """"""\n    element-wise negative\n    y = -x\n    """"""\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        if isinstance(self.x, Constant):\n            return Constant(-x.value)\n        return Tensor(-x.value, function=self)\n\n    def backward(self, delta):\n        dx = -delta\n        self.x.backward(dx)\n\n\ndef negative(x):\n    """"""\n    element-wise negative\n    """"""\n    return Negative().forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/math/power.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\nfrom prml.nn.array.broadcast import broadcast_to\n\n\nclass Power(Function):\n    """"""\n    First array elements raised to powers from second array\n    """"""\n\n    def _check_input(self, x, y):\n        x = self._convert2tensor(x)\n        y = self._convert2tensor(y)\n        if x.shape != y.shape:\n            shape = np.broadcast(x.value, y.value).shape\n            if x.shape != shape:\n                x = broadcast_to(x, shape)\n            if y.shape != shape:\n                y = broadcast_to(y, shape)\n        return x, y\n\n    def forward(self, x, y):\n        x, y = self._check_input(x, y)\n        self.x = x\n        self.y = y\n        self.output = np.power(x.value, y.value)\n        if isinstance(self.x, Constant) and isinstance(self.y, Constant):\n            return Constant(self.output)\n        return Tensor(self.output, function=self)\n\n    def backward(self, delta):\n        dx = self.y.value * np.power(self.x.value, self.y.value - 1) * delta\n        if self.x.size == 1:\n            if self.x.value > 0:\n                dy = self.output * np.log(self.x.value) * delta\n            else:\n                dy = None\n        else:\n            if (self.x.value > 0).all():\n                dy = self.output * np.log(self.x.value) * delta\n            else:\n                dy = None\n        self.x.backward(dx)\n        self.y.backward(dy)\n\n\ndef power(x, y):\n    """"""\n    First array elements raised to powers from second array\n    """"""\n    return Power().forward(x, y)\n\n\ndef rpower(x, y):\n    return Power().forward(y, x)\n'"
books/PRML/PRML-master-Python/prml/nn/math/product.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Product(Function):\n\n    def __init__(self, axis=None, keepdims=False):\n        if isinstance(axis, int):\n            axis = (axis,)\n        elif isinstance(axis, tuple):\n            axis = tuple(sorted(axis))\n        self.axis = axis\n        self.keepdims = keepdims\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        self.output = np.prod(self.x.value, axis=self.axis, keepdims=True)\n        if not self.keepdims:\n            output = np.squeeze(self.output)\n            if output.size == 1:\n                output = output.item()\n        else:\n            output = self.output\n        if isinstance(self.x, Constant):\n            return Constant(output)\n        return Tensor(output, function=self)\n\n    def backward(self, delta):\n        if not self.keepdims and self.axis is not None:\n            for ax in self.axis:\n                delta = np.expand_dims(delta, ax)\n        dx = delta * self.output / self.x.value\n        self.x.backward(dx)\n\n\ndef prod(x, axis=None, keepdims=False):\n    """"""\n    product of all element in the array\n    Parameters\n    ----------\n    x : tensor_like\n        input array\n    axis : int, tuple of ints\n        axis or axes along which a product is performed\n    keepdims : bool\n        keep dimensionality or not\n    Returns\n    -------\n    product : tensor_like\n        product of all element\n    """"""\n    return Product(axis=axis, keepdims=keepdims).forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/math/sqrt.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Sqrt(Function):\n    """"""\n    element-wise square root of the input\n    y = sqrt(x)\n    """"""\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        self.output = np.sqrt(x.value)\n        if isinstance(self.x, Constant):\n            return Constant(self.output)\n        return Tensor(self.output, function=self)\n\n    def backward(self, delta):\n        dx = 0.5 * delta / self.output\n        self.x.backward(dx)\n\n\ndef sqrt(x):\n    """"""\n    element-wise square root of the input\n    y = sqrt(x)\n    """"""\n    return Sqrt().forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/math/square.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Square(Function):\n    """"""\n    element-wise square of the input\n    y = x * x\n    """"""\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        if isinstance(self.x, Constant):\n            return Constant(np.square(x.value))\n        return Tensor(np.square(x.value), function=self)\n\n    def backward(self, delta):\n        dx = 2 * self.x.value * delta\n        self.x.backward(dx)\n\n\ndef square(x):\n    """"""\n    element-wise square of the input\n    y = x * x\n    """"""\n    return Square().forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/math/subtract.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\nfrom prml.nn.array.broadcast import broadcast_to\n\n\nclass Subtract(Function):\n    """"""\n    subtract arguments element-wise\n    """"""\n\n    def _check_input(self, x, y):\n        x = self._convert2tensor(x)\n        y = self._convert2tensor(y)\n        if x.shape != y.shape:\n            shape = np.broadcast(x.value, y.value).shape\n            if x.shape != shape:\n                x = broadcast_to(x, shape)\n            if y.shape != shape:\n                y = broadcast_to(y, shape)\n        return x, y\n\n    def forward(self, x, y):\n        x, y = self._check_input(x, y)\n        self.x = x\n        self.y = y\n        if isinstance(self.x, Constant) and isinstance(self.y, Constant):\n            return Constant(x.value - y.value)\n        return Tensor(x.value - y.value, function=self)\n\n    def backward(self, delta):\n        dx = delta\n        dy = -delta\n        self.x.backward(dx)\n        self.y.backward(dy)\n\n\ndef subtract(x, y):\n    return Subtract().forward(x, y)\n\n\ndef rsubtract(x, y):\n    return Subtract().forward(y, x)\n'"
books/PRML/PRML-master-Python/prml/nn/math/sum.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Sum(Function):\n    """"""\n    summation along given axis\n    y = sum_i=1^N x_i\n    """"""\n\n    def __init__(self, axis=None, keepdims=False):\n        if isinstance(axis, int):\n            axis = (axis,)\n        self.axis = axis\n        self.keepdims = keepdims\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        output = x.value.sum(axis=self.axis, keepdims=self.keepdims)\n        if isinstance(self.x, Constant):\n            return Constant(output)\n        return Tensor(output, function=self)\n\n    def backward(self, delta):\n        if isinstance(delta, np.ndarray) and (not self.keepdims) and (self.axis is not None):\n            axis_positive = []\n            for axis in self.axis:\n                if axis < 0:\n                    axis_positive.append(self.x.ndim + axis)\n                else:\n                    axis_positive.append(axis)\n            for axis in sorted(axis_positive):\n                delta = np.expand_dims(delta, axis)\n        dx = np.broadcast_to(delta, self.x.shape)\n        self.x.backward(dx)\n\n\ndef sum(x, axis=None, keepdims=False):\n    """"""\n    returns summation of the elements along given axis\n    y = sum_i=1^N x_i\n    """"""\n    return Sum(axis=axis, keepdims=keepdims).forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/nonlinear/__init__.py,0,b''
books/PRML/PRML-master-Python/prml/nn/nonlinear/log_softmax.py,0,"b'import numpy as np\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass LogSoftmax(Function):\n\n    def __init__(self, axis=-1):\n        self.axis = axis\n\n    def _logsumexp(self, x):\n        x_max = np.max(x, axis=self.axis, keepdims=True)\n        y = x - x_max\n        np.exp(y, out=y)\n        np.log(y.sum(axis=self.axis, keepdims=True), out=y)\n        y += x_max\n        return y\n\n    def _forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        self.output = x.value - self._logsumexp(x.value)\n        return Tensor(self.output, function=self)\n\n    def _backward(self, delta):\n        dx = delta\n        dx -= np.exp(self.output) * dx.sum(axis=self.axis, keepdims=True)\n        self.x.backward(dx)\n\n\ndef log_softmax(x, axis=-1):\n    return LogSoftmax(axis=axis).forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/nonlinear/relu.py,0,"b'from prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass ReLU(Function):\n    """"""\n    Rectified Linear Unit\n\n    y = max(x, 0)\n    """"""\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        output = x.value.clip(min=0)\n        if isinstance(x, Constant):\n            return Constant(output)\n        return Tensor(output, function=self)\n\n    def backward(self, delta):\n        dx = (self.x.value > 0) * delta\n        self.x.backward(dx)\n\n\ndef relu(x):\n    """"""\n    Rectified Linear Unit\n\n    y = max(x, 0)\n    """"""\n    return ReLU().forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/nonlinear/sigmoid.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Sigmoid(Function):\n    """"""\n    logistic sigmoid function\n    y = 1 / (1 + exp(-x))\n    """"""\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        self.output = np.tanh(x.value * 0.5) * 0.5 + 0.5\n        if isinstance(self.x, Constant):\n            return Constant(self.output)\n        return Tensor(self.output, function=self)\n\n    def backward(self, delta):\n        dx = self.output * (1 - self.output) * delta\n        self.x.backward(dx)\n\n\ndef sigmoid(x):\n    """"""\n    logistic sigmoid function\n    y = 1 / (1 + exp(-x))\n    """"""\n    return Sigmoid().forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/nonlinear/softmax.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Softmax(Function):\n\n    def __init__(self, axis=-1):\n        if not isinstance(axis, int):\n            raise TypeError(""axis must be int"")\n        self.axis = axis\n\n    def _softmax(self, array):\n        y = array - np.max(array, self.axis, keepdims=True)\n        np.exp(y, out=y)\n        y /= y.sum(self.axis, keepdims=True)\n        return y\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        self.output = self._softmax(x.value)\n        if isinstance(x, Constant):\n            return Constant(self.output)\n        return Tensor(self.output, function=self)\n\n    def backward(self, delta):\n        dx = self.output * delta\n        dx -= self.output * dx.sum(self.axis, keepdims=True)\n        self.x.backward(dx)\n\n\ndef softmax(x, axis=-1):\n    """"""\n    softmax function along specified axis\n    y_k = exp(x_k) / sum_i(exp(x_i))\n    """"""\n    return Softmax(axis=axis).forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/nonlinear/softplus.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Softplus(Function):\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        output = np.maximum(x.value, 0) + np.log1p(np.exp(-np.abs(x.value)))\n        if isinstance(x, Constant):\n            return Constant(output)\n        return Tensor(output, function=self)\n\n    def backward(self, delta):\n        dx = (np.tanh(0.5 * self.x.value) * 0.5 + 0.5) * delta\n        self.x.backward(dx)\n\n\ndef softplus(x):\n    """"""\n    smoothed rectified linear unit\n    log(1 + exp(x))\n    """"""\n    return Softplus().forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/nonlinear/tanh.py,0,"b'import numpy as np\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Tanh(Function):\n\n    def forward(self, x):\n        x = self._convert2tensor(x)\n        self.x = x\n        self.output = np.tanh(x.value)\n        if isinstance(self.x, Constant):\n            return Constant(self.output)\n        return Tensor(self.output, function=self)\n\n    def backward(self, delta):\n        dx = (1 - np.square(self.output)) * delta\n        self.x.backward(dx)\n\n\ndef tanh(x):\n    """"""\n    hyperbolic tangent function\n    y = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n    """"""\n    return Tanh().forward(x)\n'"
books/PRML/PRML-master-Python/prml/nn/optimizer/__init__.py,0,"b'from prml.nn.optimizer.ada_delta import AdaDelta\nfrom prml.nn.optimizer.ada_grad import AdaGrad\nfrom prml.nn.optimizer.adam import Adam\nfrom prml.nn.optimizer.gradient_ascent import GradientAscent\nfrom prml.nn.optimizer.momentum import Momentum\nfrom prml.nn.optimizer.rmsprop import RMSProp\n\n\n__all__ = [\n    ""AdaDelta"",\n    ""AdaGrad"",\n    ""Adam"",\n    ""GradientAscent"",\n    ""Momentum"",\n    ""RMSProp""\n]\n'"
books/PRML/PRML-master-Python/prml/nn/optimizer/ada_delta.py,0,"b'import numpy as np\nfrom prml.nn.optimizer.optimizer import Optimizer\n\n\nclass AdaDelta(Optimizer):\n    """"""\n    AdaDelta optimizer\n    """"""\n\n    def __init__(self, parameter, rho=0.95, epsilon=1e-8):\n        super().__init__(parameter, None)\n        self.rho = rho\n        self.epsilon = epsilon\n        self.mean_squared_deriv = []\n        self.mean_squared_update = []\n        for p in self.parameter:\n            self.mean_squared_deriv.append(np.zeros(p.shape))\n            self.mean_squared_update.append(np.zeros(p.shape))\n\n    def update(self):\n        self.increment_iteration()\n        for p, msd, msu in zip(self.parameter, self.mean_squared_deriv, self.mean_squared_update):\n            if p.grad is None:\n                continue\n            grad = p.grad\n            msd *= self.rho\n            msd += (1 - self.rho) * grad ** 2\n            delta = np.sqrt((msu + self.epsilon) / (msd + self.epsilon)) * grad\n            msu *= self.rho\n            msu *= (1 - self.rho) * delta ** 2\n            p.value += delta\n'"
books/PRML/PRML-master-Python/prml/nn/optimizer/ada_grad.py,0,"b'import numpy as np\nfrom prml.nn.optimizer.optimizer import Optimizer\n\n\nclass AdaGrad(Optimizer):\n    """"""\n    AdaGrad optimizer\n    initialization\n    G = 0\n    update rule\n    G += gradient ** 2\n    param -= learning_rate * gradient / sqrt(G + eps)\n    """"""\n\n    def __init__(self, parameter, learning_rate=0.001, epsilon=1e-8):\n        super().__init__(parameter, learning_rate)\n        self.epsilon = epsilon\n        self.G = []\n        for p in self.parameter:\n            self.G.append(np.zeros(p.shape))\n\n    def update(self):\n        """"""\n        update parameters\n        """"""\n        self.increment_iteration()\n        for p, G in zip(self.parameter, self.G):\n            if p.grad is None:\n                continue\n            grad = p.grad\n            G += grad ** 2\n            p.value += self.learning_rate * grad / (np.sqrt(G) + self.epsilon)\n'"
books/PRML/PRML-master-Python/prml/nn/optimizer/adam.py,0,"b'import numpy as np\nfrom prml.nn.optimizer.optimizer import Optimizer\n\n\nclass Adam(Optimizer):\n    """"""\n    Adam optimizer\n    initialization\n    m1 = 0 (Initial 1st moment of gradient)\n    m2 = 0 (Initial 2nd moment of gradient)\n    n_iter = 0\n    update rule\n    n_iter += 1\n    learning_rate *= sqrt(1 - beta2^n) / (1 - beta1^n)\n    m1 = beta1 * m1 + (1 - beta1) * gradient\n    m2 = beta2 * m2 + (1 - beta2) * gradient^2\n    param += learning_rate * m1 / (sqrt(m2) + epsilon)\n    """"""\n\n    def __init__(self, parameter, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n        """"""\n        construct Adam optimizer\n        Parameters\n        ----------\n        parameters : list\n            list of parameters to be optimized\n        learning_rate : float\n        beta1 : float\n            exponential decay rate for the 1st moment\n        beta2 : float\n            exponential decay rate for the 2nd moment\n        epsilon : float\n            small constant to be added to denominator for numerical stability\n        Attributes\n        ----------\n        n_iter : int\n            number of iterations performed\n        moment1 : dict\n            1st moment of each learnable parameter\n        moment2 : dict\n            2nd moment of each learnable parameter\n        """"""\n        super().__init__(parameter, learning_rate)\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.moment1 = []\n        self.moment2 = []\n        for p in self.parameter:\n            self.moment1.append(np.zeros(p.shape))\n            self.moment2.append(np.zeros(p.shape))\n\n    def update(self):\n        """"""\n        update parameter of the neural network\n        """"""\n        self.increment_iteration()\n        lr = (\n            self.learning_rate\n            * (1 - self.beta2 ** self.n_iter) ** 0.5\n            / (1 - self.beta1 ** self.n_iter))\n        for p, m1, m2 in zip(self.parameter, self.moment1, self.moment2):\n            if p.grad is None:\n                continue\n            m1 += (1 - self.beta1) * (p.grad - m1)\n            m2 += (1 - self.beta2) * (p.grad ** 2 - m2)\n            p.value += lr * m1 / (np.sqrt(m2) + self.epsilon)\n'"
books/PRML/PRML-master-Python/prml/nn/optimizer/eve.py,0,"b'import numpy as np\nfrom prml.nn.optimizer.optimizer import Optimizer\n\n\nclass Eve(Optimizer):\n    """"""\n    Eve optimizer\n\n    initialization\n    m1 = 0 (initial 1st moment of gradient)\n    m2 = 0 (initial 2nd moment of gradient)\n    n_iter = 0\n\n    update rule\n    n_iter += 1\n    learning\n    """"""\n\n    def __init__(self,\n                 network,\n                 learning_rate=0.001,\n                 beta1=0.9,\n                 beta2=0.999,\n                 beta3=0.999,\n                 lower_threshold=0.1,\n                 upper_threshold=10.,\n                 epsilon=1e-8):\n        """"""\n        construct Eve optimizer\n\n        Parameters\n        ----------\n        network : Network\n            neural network to be optmized\n        learning_rate : float\n        beta1 : float\n            exponential decay rate for the 1st moment\n        beta2 : float\n            exponential decay rate for the 2nd moment\n        beta3 : float\n            exponential decay rate for computing relative change\n        lower_threshold : float\n             lower threshold for relative change\n        upper_threshold : float\n             upper threshold for relative change\n        epsilon : float\n            small constant to be added to denominator for numerical stability\n\n        Attributes\n        ----------\n        n_iter : int\n            number of iterations performed\n        moment1 : dict\n            1st moment of each parameter\n        moment2 : dict\n            2nd moment of each parameter\n        """"""\n        super().__init__(network, learning_rate)\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.beta3 = beta3\n        self.lower_threshold = lower_threshold\n        self.upper_threshold = upper_threshold\n        self.epsilon = epsilon\n        self.moment1 = {}\n        self.moment2 = {}\n        self.f = 1.\n        self.d = 1.\n        for key, param in self.params.items():\n            self.moment1[key] = np.zeros(param.shape)\n            self.moment2[key] = np.zeros(param.shape)\n\n    def update(self, loss):\n        loss = float(loss)\n        self.increment_iteration()\n        if self.n_iter > 1:\n            if loss > self.f:\n                delta = self.lower_threshold + 1\n                Delta = self.upper_threshold + 1\n            else:\n                delta = 1 / (self.upper_threshold + 1)\n                Delta = 1 / (self.lower_threshold + 1)\n            c = min(max(delta, loss / self.f), Delta)\n            f = c * self.f\n            r = abs(f - self.f) / min(f, self.f)\n            self.d = self.beta3 * self.d * (1 - self.beta3) * r\n            self.f = f\n        else:\n            self.f = loss\n            self.d = 1\n        lr = (\n            self.learning_rate\n            * (1 - self.beta2 ** self.n_iter) ** 0.5\n            / (1 - self.beta1 ** self.n_iter)\n        )\n        for key, param in self.params.items():\n            m1 = self.moment1[key]\n            m2 = self.moment2[key]\n            m1 += (1 - self.beta1) * (param.grad - m1)\n            m2 += (1 - self.beta2) * (param.grad ** 2 - m2)\n            param.value -= lr * m1 / (self.d * np.sqrt(m2) + self.epsilon)\n'"
books/PRML/PRML-master-Python/prml/nn/optimizer/gradient_ascent.py,0,"b'from prml.nn.optimizer.optimizer import Optimizer\n\n\nclass GradientAscent(Optimizer):\n    """"""\n    gradient ascent optimizer\n    parameter += learning_rate * gradient\n    """"""\n\n    def update(self):\n        """"""\n        update parameters to be optimized\n        """"""\n        self.increment_iteration()\n        for p in self.parameter:\n            if p.grad is None:\n                continue\n            p.value += self.learning_rate * p.grad\n'"
books/PRML/PRML-master-Python/prml/nn/optimizer/momentum.py,0,"b'import numpy as np\nfrom prml.nn.optimizer.optimizer import Optimizer\n\n\nclass Momentum(Optimizer):\n    """"""\n    Momentum optimizer\n    initialization\n    v = 0\n    update rule\n    v = v * momentum - learning_rate * gradient\n    param += v\n    """"""\n\n    def __init__(self, parameter, learning_rate, momentum=0.9):\n        super().__init__(parameter, learning_rate)\n        self.momentum = momentum\n        self.inertia = []\n        for p in self.parameter:\n            self.inertia.append(np.zeros(p.shape))\n\n    def update(self):\n        self.increment_iteration()\n        for p, inertia in zip(self.parameter, self.inertia):\n            if p.grad is None:\n                continue\n            inertia *= self.momentum\n            inertia -= self.learning_rate * p.grad\n            p.value += inertia\n'"
books/PRML/PRML-master-Python/prml/nn/optimizer/optimizer.py,0,"b'from prml.nn.network import Network\n\n\nclass Optimizer(object):\n    """"""\n    Optimizer to train neural network\n    """"""\n\n    def __init__(self, parameter, learning_rate):\n        """"""\n        construct optimizer\n        Parameters\n        ----------\n        parameter : list, dict, Network\n            list of parameter to be optimized\n        learning_rate : float\n            update rate of parameter to be optimized\n        Attributes\n        ----------\n        n_iter : int\n            number of iterations performed\n        """"""\n        if isinstance(parameter, Network):\n            parameter = parameter.parameter\n        if isinstance(parameter, dict):\n            parameter = list(parameter.values())\n        self.parameter = parameter\n        self.learning_rate = learning_rate\n        self.n_iter = 0\n\n    def cleargrad(self):\n        for p in self.parameter:\n            p.cleargrad()\n\n    def set_decay(self, decay_rate, decay_step):\n        """"""\n        set exponential decay parameters\n        Parameters\n        ----------\n        decay_rate : float\n            dacay rate of the learning rate\n        decay_step : int\n            steps to decay the learning rate\n        """"""\n        self.decay_rate = decay_rate\n        self.decay_step = decay_step\n\n    def increment_iteration(self):\n        self.n_iter += 1\n        if hasattr(self, ""decay_rate""):\n            if self.n_iter % self.decay_step == 0:\n                self.learning_rate *= self.decay_rate\n'"
books/PRML/PRML-master-Python/prml/nn/optimizer/rmsprop.py,0,"b'import numpy as np\nfrom prml.nn.optimizer.optimizer import Optimizer\n\n\nclass RMSProp(Optimizer):\n    """"""\n    RMSProp optimizer\n    initial\n    msg = 0\n    update rule\n    msg = rho * msg + (1 - rho) * gradient ** 2\n    param -= learning_rate * gradient / (sqrt(msg) + eps)\n    """"""\n\n    def __init__(self, parameter, learning_rate=1e-3, rho=0.9, epsilon=1e-8):\n        super().__init__(parameter, learning_rate)\n        self.rho = rho\n        self.epsilon = epsilon\n        self.mean_squared_grad = []\n        for p in self.parameter:\n            self.mean_squared_grad.append(np.zeros(p.shape))\n\n    def update(self):\n        """"""\n        update parameters\n        """"""\n        self.increment_iteration()\n        for p, msg in zip(self.parameter, self.mean_squared_grad):\n            if p.grad is None:\n                continue\n            grad = p.grad\n            msg *= self.rho\n            msg += (1 - self.rho) * grad ** 2\n            p.value -= (\n                self.learning_rate * grad / (np.sqrt(msg) + self.epsilon)\n            )\n'"
books/PRML/PRML-master-Python/prml/nn/random/__init__.py,0,"b'from prml.nn.random.bernoulli import Bernoulli\nfrom prml.nn.random.categorical import Categorical\nfrom prml.nn.random.cauchy import Cauchy\nfrom prml.nn.random.dirichlet import Dirichlet\nfrom prml.nn.random.dropout import dropout\nfrom prml.nn.random.exponential import Exponential\nfrom prml.nn.random.gamma import Gamma\nfrom prml.nn.random.gaussian import Gaussian\nfrom prml.nn.random.gaussian_mixture import GaussianMixture\nfrom prml.nn.random.laplace import Laplace\nfrom prml.nn.random.multivariate_gaussian import MultivariateGaussian\n\n\n__all__ = [\n    ""Bernoulli"",\n    ""Categorical"",\n    ""Cauchy"",\n    ""Dirichlet"",\n    ""Exponential"",\n    ""Gamma"",\n    ""Gaussian"",\n    ""GaussianMixture"",\n    ""Laplace"",\n    ""MultivariateGaussian""\n]\n'"
books/PRML/PRML-master-Python/prml/nn/random/bernoulli.py,0,"b'import numpy as np\nfrom prml.nn.array.broadcast import broadcast_to\nfrom prml.nn.function import Function\nfrom prml.nn.math.log import log\nfrom prml.nn.nonlinear.sigmoid import sigmoid\nfrom prml.nn.random.random import RandomVariable\nfrom prml.nn.tensor.tensor import Tensor\n\n\nclass Bernoulli(RandomVariable):\n    """"""\n    Bernoulli distribution\n    p(x|mu) = mu^x (1 - mu)^(1 - x)\n    Parameters\n    ----------\n    mu : tensor_like\n        probability of value 1\n    logit : tensor_like\n        log-odd of value 1\n    data : tensor_like\n        observed data\n    p : RandomVariable\n        original distribution of a model\n    """"""\n\n    def __init__(self, mu=None, logit=None, data=None, p=None):\n        super().__init__(data, p)\n        if mu is not None and logit is None:\n            mu = self._convert2tensor(mu)\n            self.mu = mu\n        elif mu is None and logit is not None:\n            logit = self._convert2tensor(logit)\n            self.logit = logit\n        elif mu is None and logit is None:\n            raise ValueError(""Either mu or logit must not be None"")\n        else:\n            raise ValueError(""Cannot assign both mu and logit"")\n\n    @property\n    def mu(self):\n        try:\n            return self.parameter[""mu""]\n        except KeyError:\n            return sigmoid(self.logit)\n\n    @mu.setter\n    def mu(self, mu):\n        try:\n            inrange = (0 <= mu.value <= 1)\n        except ValueError:\n            inrange = ((mu.value >= 0).all() and (mu.value <= 1).all())\n\n        if not inrange:\n            raise ValueError(""value of mu must all be positive"")\n        self.parameter[""mu""] = mu\n\n    @property\n    def logit(self):\n        try:\n            return self.parameter[""logit""]\n        except KeyError:\n            raise AttributeError(""no attribute named logit"")\n\n    @logit.setter\n    def logit(self, logit):\n        self.parameter[""logit""] = logit\n\n    def forward(self):\n        return (np.random.uniform(size=self.mu.shape) < self.mu.value).astype(np.int)\n\n    def _pdf(self, x):\n        return self.mu ** x * (1 - self.mu) ** (1 - x)\n\n    def _log_pdf(self, x):\n        try:\n            return -SigmoidCrossEntropy().forward(self.logit, x)\n        except AttributeError:\n            return x * log(self.mu) + (1 - x) * log(1 - self.mu)\n\n\nclass SigmoidCrossEntropy(Function):\n    """"""\n    sum of cross entropies for binary data\n    logistic sigmoid\n    y_i = 1 / (1 + exp(-x_i))\n    cross_entropy_i = -t_i * log(y_i) - (1 - t_i) * log(1 - y_i)\n    Parameters\n    ----------\n    x : ndarary\n        input logit\n    y : ndarray\n        corresponding target binaries\n    """"""\n\n    def _check_input(self, x, t):\n        x = self._convert2tensor(x)\n        t = self._convert2tensor(t)\n        if x.shape != t.shape:\n            shape = np.broadcast(x.value, t.value).shape\n            if x.shape != shape:\n                x = broadcast_to(x, shape)\n            if t.shape != shape:\n                t = broadcast_to(t, shape)\n        return x, t\n\n\n    def forward(self, x, t):\n        x, t = self._check_input(x, t)\n        self.x = x\n        self.t = t\n        # y = self.forward(x)\n        # np.clip(y, 1e-10, 1 - 1e-10, out=y)\n        # return np.sum(-t * np.log(y) - (1 - t) * np.log(1 - y))\n        loss = (\n            np.maximum(x.value, 0)\n            - t.value * x.value\n            + np.log1p(np.exp(-np.abs(x.value)))\n        )\n        return Tensor(loss, function=self)\n\n    def backward(self, delta):\n        y = np.tanh(self.x.value * 0.5) * 0.5 + 0.5\n        dx = delta * (y - self.t.value)\n        dt = - delta * self.x.value\n        self.x.backward(dx)\n        self.t.backward(dt)\n'"
books/PRML/PRML-master-Python/prml/nn/random/categorical.py,0,"b'import numpy as np\nfrom prml.nn.array.broadcast import broadcast_to\nfrom prml.nn.function import Function\nfrom prml.nn.math.log import log\nfrom prml.nn.math.product import prod\nfrom prml.nn.nonlinear.softmax import softmax\nfrom prml.nn.random.random import RandomVariable\nfrom prml.nn.tensor.tensor import Tensor\n\n\nclass Categorical(RandomVariable):\n    """"""\n    Categorical distribution\n    Parameters\n    ----------\n    mu : (..., K) tensor_like\n        probability of each index\n    logit : (..., K) tensor_like\n        log-odd of each index\n    axis : int\n        axis along which represents each outcome\n    data : tensor_like\n        realization\n    p : RandomVariable\n        original distribution of a model\n    Attributes\n    ----------\n    n_category : int\n        number of categories\n    """"""\n\n    def __init__(self, mu=None, logit=None, axis=-1, data=None, p=None):\n        super().__init__(data, p)\n        assert axis == -1\n        self.axis = axis\n        if mu is not None and logit is None:\n            self.mu = self._convert2tensor(mu)\n        elif mu is None and logit is not None:\n            self.logit = self._convert2tensor(logit)\n        elif mu is None and logit is None:\n            raise ValueError(""Either mu or logit must not be None"")\n        else:\n            raise ValueError(""Cannot assign both mu and logit"")\n\n    @property\n    def mu(self):\n        try:\n            return self.parameter[""mu""]\n        except KeyError:\n            return softmax(self.parameter[""logit""])\n\n    @mu.setter\n    def mu(self, mu):\n        self._atleast_ndim(mu, 1)\n        if not ((mu.value >= 0).all() and (mu.value <= 1).all()):\n            raise ValueError(""values of mu must be in [0, 1]"")\n        if not np.allclose(mu.value.sum(axis=self.axis), 1):\n            raise ValueError(f""mu must be normalized along axis {self.axis}"")\n        self.parameter[""mu""] = mu\n        self.n_category = mu.shape[self.axis]\n\n    @property\n    def logit(self):\n        try:\n            return self.parameter[""logit""]\n        except KeyError:\n            raise AttributeError(""no attribute named logit"")\n\n    @logit.setter\n    def logit(self, logit):\n        self._atleast_ndim(logit, 1)\n        self.parameter[""logit""] = logit\n        self.n_category = logit.shape[self.axis]\n\n    def forward(self):\n        if self.mu.ndim == 1:\n            index = np.random.choice(self.n_category, p=self.mu.value)\n            return np.eye(self.n_category)[index]\n        elif self.mu.ndim == 2:\n            indices = np.array(\n                [np.random.choice(self.n_category, p=p.value) for p in self.mu.value]\n            )\n            return np.eye(self.n_category)[indices]\n        else:\n            raise NotImplementedError\n\n    def _pdf(self, x):\n        return prod(self.mu ** x, axis=self.axis)\n\n    def _log_pdf(self, x):\n        try:\n            return -SoftmaxCrossEntropy(axis=self.axis).forward(self.logit, x)\n        except (KeyError, AttributeError):\n            return (x * log(self.mu)).sum(axis=self.axis)\n\n\nclass SoftmaxCrossEntropy(Function):\n\n    def __init__(self, axis=-1):\n        self.axis = axis\n\n    def _check_input(self, x, t):\n        x = self._convert2tensor(x)\n        t = self._convert2tensor(t)\n        if x.shape != t.shape:\n            shape = np.broadcast(x.value, t.value).shape\n            if x.shape != shape:\n                x = broadcast_to(x, shape)\n            if t.shape != shape:\n                t = broadcast_to(t, shape)\n        return x, t\n\n    def _softmax(self, array):\n        y = np.exp(array - np.max(array, self.axis, keepdims=True))\n        y /= np.sum(y, self.axis, keepdims=True)\n        return y\n\n    def forward(self, x, t):\n        x, t = self._check_input(x, t)\n        self.x = x\n        self.t = t\n        self.y = self._softmax(x.value)\n        np.clip(self.y, 1e-10, 1, out=self.y)\n        loss = -t.value * np.log(self.y)\n        return Tensor(loss, function=self)\n\n    def backward(self, delta):\n        dx = delta * (self.y - self.t.value)\n        dt = - delta * np.log(self.y)\n        self.x.backward(dx)\n        self.t.backward(dt)\n'"
books/PRML/PRML-master-Python/prml/nn/random/cauchy.py,0,"b'import numpy as np\nfrom prml.nn.array.broadcast import broadcast_to\nfrom prml.nn.math.log import log\nfrom prml.nn.math.square import square\nfrom prml.nn.random.random import RandomVariable\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\n\n\nclass Cauchy(RandomVariable):\n    """"""\n    Cauchy distribution aka Lorentz distribution\n    p(x|x0(loc), scale)\n    = 1 / [pi*scale * {1 + (x - x0)^2 / scale^2}]\n    Parameters\n    ----------\n    loc : tensor_like\n        location parameter\n    scale : tensor_like\n        scale parameter\n    data : tensor_like\n        realization\n    p : RandomVariable\n        original distribution of a model\n    """"""\n\n    def __init__(self, loc, scale, data=None, p=None):\n        super().__init__(data, p)\n        self.loc, self.scale = self._check_input(loc, scale)\n\n    def _check_input(self, loc, scale):\n        loc = self._convert2tensor(loc)\n        scale = self._convert2tensor(scale)\n        if loc.shape != scale.shape:\n            shape = np.broadcast(loc.value, scale.value).shape\n            if loc.shape != shape:\n                loc = broadcast_to(loc, shape)\n            if scale.shape != shape:\n                scale = broadcast_to(scale, shape)\n        return loc, scale\n\n    @property\n    def loc(self):\n        return self.parameter[""loc""]\n\n    @loc.setter\n    def loc(self, loc):\n        self.parameter[""loc""] = loc\n\n    @property\n    def scale(self):\n        return self.parameter[""scale""]\n\n    @scale.setter\n    def scale(self, scale):\n        try:\n            ispositive = (scale.value > 0).all()\n        except AttributeError:\n            ispositive = (scale.value > 0)\n\n        if not ispositive:\n            raise ValueError(""value of scale must be positive"")\n        self.parameter[""scale""] = scale\n\n    def forward(self):\n        self.eps = np.random.standard_cauchy(size=self.loc.shape)\n        self.output = self.scale.value * self.eps + self.loc.value\n        if isinstance(self.loc, Constant):\n            return Constant(self.output)\n        return Tensor(self.output, function=self)\n\n    def backward(self, delta):\n        dloc = delta\n        dscale = delta * self.eps\n        self.loc.backward(dloc)\n        self.scale.backward(dscale)\n\n    def _pdf(self, x):\n        return (\n            1 / (np.pi * self.scale * (1 + square((x - self.loc) / self.scale)))\n        )\n\n    def _log_pdf(self, x):\n        return (\n            -np.log(np.pi)\n            - log(self.scale)\n            - log(1 + square((x - self.loc) / self.scale))\n        )\n'"
books/PRML/PRML-master-Python/prml/nn/random/dirichlet.py,0,"b'import numpy as np\nfrom prml.nn.math.gamma import gamma\nfrom prml.nn.math.log import log\nfrom prml.nn.math.product import prod\nfrom prml.nn.math.sum import sum\nfrom prml.nn.random.random import RandomVariable\nfrom prml.nn.tensor.tensor import Tensor\n\n\nclass Dirichlet(RandomVariable):\n    """"""\n    Dirichlet distribution\n    Parameters\n    ----------\n    alpha : (..., K) tensor_like\n        pseudo-count of each outcome\n    axis : int\n        axis along which represents each outcome\n    data : tensor_like\n        realization\n    p : RandomVariable\n        original distribution of a model\n    Attributes\n    ----------\n    n_category : int\n        number of categories\n    """"""\n\n    def __init__(self, alpha, axis=-1, data=None, p=None):\n        super().__init__(data, p)\n        assert axis == -1\n        self.axis = axis\n        self.alpha = self._convert2tensor(alpha)\n\n    @property\n    def alpha(self):\n        return self.parameter[""alpha""]\n\n    @alpha.setter\n    def alpha(self, alpha):\n        self._atleast_ndim(alpha, 1)\n        if (alpha.value <= 0).any():\n            raise ValueError(""alpha must all be positive"")\n        self.parameter[""alpha""] = alpha\n\n    def forward(self):\n        if self.alpha.ndim == 1:\n            return Tensor(np.random.dirichlet(self.alpha.value), function=self)\n        else:\n            raise NotImplementedError\n\n    def backward(self):\n        raise NotImplementedError\n\n    def _pdf(self, x):\n        return (\n            gamma(self.alpha.sum(axis=self.axis))\n            * prod(\n                x ** (self.alpha - 1)\n                / gamma(self.alpha),\n                axis=self.axis\n            )\n        )\n\n    def _log_pdf(self, x):\n        return (\n            log(gamma(self.alpha.sum(axis=self.axis)))\n            + sum(\n                (self.alpha - 1) * log(x) - log(gamma(self.alpha)),\n                axis=self.axis\n            )\n        )\n'"
books/PRML/PRML-master-Python/prml/nn/random/dropout.py,0,"b'import numpy as np\nfrom prml.nn.tensor.tensor import Tensor\nfrom prml.nn.function import Function\n\n\nclass Dropout(Function):\n\n    def __init__(self, prob):\n        """"""\n        construct dropout function\n\n        Parameters\n        ----------\n        prob : float\n            probability of dropping the input value\n        """"""\n        if not isinstance(prob, float):\n            raise TypeError(f""prob must be float value, not {type(prob)}"")\n        if prob < 0 or prob > 1:\n            raise ValueError(f""{prob} is out of the range [0, 1]"")\n        self.prob = prob\n        self.coef = 1 / (1 - prob)\n\n    def _forward(self, x, istraining=False):\n        x = self._convert2tensor(x)\n        if istraining:\n            self.x = x\n            self.mask = (np.random.rand(*x.shape) > self.prob) * self.coef\n            return Tensor(x.value * self.mask, function=self)\n        else:\n            return x\n\n    def _backward(self, delta):\n        dx = delta * self.mask\n        self.x.backward(dx)\n\n\ndef dropout(x, prob, istraining):\n    return Dropout(prob).forward(x, istraining)\n'"
books/PRML/PRML-master-Python/prml/nn/random/exponential.py,0,"b'import numpy as np\nfrom prml.nn.math.exp import exp\nfrom prml.nn.math.log import log\nfrom prml.nn.random.random import RandomVariable\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\n\n\nclass Exponential(RandomVariable):\n    """"""\n    Exponential distribution aka negative exponential distribution\n    p(x|rate) = rate * exp(-rate * x)\n    rate > 0\n    Parameters\n    ----------\n    rate : tensor_like\n        rate parameter\n    data : tensor_like\n        realization of this distribution\n    p : RandomVariable\n        original distribution of a model\n    """"""\n\n    def __init__(self, rate, data=None, p=None):\n        super().__init__(data, p)\n        rate = self._convert2tensor(rate)\n        self.rate = rate\n\n    @property\n    def rate(self):\n        return self.parameter[""rate""]\n\n    @rate.setter\n    def rate(self, rate):\n        try:\n            ispositive = (rate.value > 0).all()\n        except AttributeError:\n            ispositive = (rate.value > 0)\n\n        if not ispositive:\n            raise ValueError(""value of rate must be positive"")\n        self.parameter[""rate""] = rate\n\n    def forward(self):\n        eps = np.random.standard_exponential(size=self.rate.shape)\n        self.output = eps / self.rate.value\n        if isinstance(self.rate, Constant):\n            return Constant(self.output)\n        return Tensor(self.output, self)\n\n    def backward(self, delta):\n        drate = -delta * self.output / self.rate.value\n        self.rate.backward(drate)\n\n    def _pdf(self, x):\n        return self.rate * exp(-self.rate * x)\n\n    def _log_pdf(self, x):\n        return -self.rate * x + log(self.rate)\n'"
books/PRML/PRML-master-Python/prml/nn/random/gamma.py,0,"b'import numpy as np\nimport scipy.special as sp\nfrom prml.nn.array.broadcast import broadcast_to\nfrom prml.nn.math.exp import exp\nfrom prml.nn.math.gamma import gamma\nfrom prml.nn.math.log import log\nfrom prml.nn.random.random import RandomVariable\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\n\n\nclass Gamma(RandomVariable):\n    """"""\n    Gamma distribution\n    p(x|a(shape), b(rate))\n    = b^a * x^(a - 1) * e^(-bx) / Gamma(a)\n    Parameters\n    ----------\n    shape : tensor_like\n        shape parameter\n    rate : tensor_like\n        rate parameter\n    data : tensor_like\n        realization\n    p : RandomVariable\n        original distribution of a model\n    """"""\n\n    def __init__(self, shape, rate, data=None, p=None):\n        super().__init__(data, p)\n        shape, rate = self._check_input(shape, rate)\n        self.shape = shape\n        self.rate = rate\n\n    def _check_input(self, shape, rate):\n        shape = self._convert2tensor(shape)\n        rate = self._convert2tensor(rate)\n        if shape.shape != rate.shape:\n            shape_ = np.broadcast(shape.value, rate.value).shape\n            if shape.shape != shape_:\n                shape = broadcast_to(shape, shape_)\n            if rate.shape != shape_:\n                rate = broadcast_to(rate, shape_)\n        return shape, rate\n\n    @property\n    def shape(self):\n        return self.parameter[""shape""]\n\n    @shape.setter\n    def shape(self, shape):\n        try:\n            ispositive = (shape.value > 0).all()\n        except AttributeError:\n            ispositive = (shape.value > 0)\n\n        if not ispositive:\n            raise ValueError(""value of shape must be positive"")\n        self.parameter[""shape""] = shape\n\n    @property\n    def rate(self):\n        return self.parameter[""rate""]\n\n    @rate.setter\n    def rate(self, rate):\n        try:\n            ispositive = (rate.value > 0).all()\n        except AttributeError:\n            ispositive = (rate.value > 0)\n\n        if not ispositive:\n            raise ValueError(""value of rate must be positive"")\n        self.parameter[""rate""] = rate\n\n    def forward(self):\n        self.output = np.random.gamma(self.shape.value, 1 / self.rate.value)\n        if isinstance(self.shape, Constant) and isinstance(self.rate, Constant):\n            return Constant(self.output)\n        return Tensor(self.output, function=self)\n\n    def backward(self, delta):\n        a = self.shape.value\n        psia = sp.digamma(a)\n        psi1a = sp.polygamma(1, a)\n        sqrtpsi1a = np.sqrt(psi1a)\n        psi2a = sp.polygamma(2, a)\n        b = self.rate.value\n        eps = (np.log(self.output) - psia + np.log(b)) / sqrtpsi1a\n        dshape = self.output * (0.5 * eps * psi2a / sqrtpsi1a + psi1a) * delta\n        drate = -delta * self.output / b\n        self.shape.backward(dshape)\n        self.rate.backward(drate)\n\n    def _pdf(self, x):\n        return (\n            self.rate ** self.shape\n            * x ** (self.shape - 1)\n            * exp(-self.rate * x)\n            / gamma(self.shape)\n        )\n\n    def _log_pdf(self, x):\n        return (\n            self.shape * log(self.rate)\n            + (self.shape - 1) * log(x)\n            - self.rate * x\n            - log(gamma(self.shape))\n        )\n'"
books/PRML/PRML-master-Python/prml/nn/random/gaussian.py,0,"b'import numpy as np\nfrom prml.nn.array.broadcast import broadcast_to\nfrom prml.nn.function import Function\nfrom prml.nn.math.exp import exp\nfrom prml.nn.math.log import log\nfrom prml.nn.math.sqrt import sqrt\nfrom prml.nn.math.square import square\nfrom prml.nn.random.random import RandomVariable\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\n\n\nclass Gaussian(RandomVariable):\n    """"""\n    The Gaussian distribution\n    p(x|mu(mean), sigma(std))\n    = exp{-0.5 * (x - mu)^2 / sigma^2} / sqrt(2pi * sigma^2)\n    Parameters\n    ----------\n    mu : tensor_like\n        mean parameter\n    std : tensor_like\n        std parameter\n    var : tensor_like\n        variance parameter\n    tau : tensor_like\n        precision parameter\n    data : tensor_like\n        observed data\n    p : RandomVariable\n        original distribution of a model\n    """"""\n\n    def __init__(self, mu, std=None, var=None, tau=None, data=None, p=None):\n        super().__init__(data, p)\n        if std is not None and var is None and tau is None:\n            self.mu, self.std = self._check_input(mu, std)\n        elif std is None and var is not None and tau is None:\n            self.mu, self.var = self._check_input(mu, var)\n        elif std is None and var is None and tau is not None:\n            self.mu, self.tau = self._check_input(mu, tau)\n        elif std is None and var is None and tau is None:\n            raise ValueError(""Either std, var, or tau must be assigned"")\n        else:\n            raise ValueError(""Cannot assign more than two of these: std, var, tau"")\n\n    def _check_input(self, x, y):\n        x = self._convert2tensor(x)\n        y = self._convert2tensor(y)\n        if x.shape != y.shape:\n            shape = np.broadcast(x.value, y.value).shape\n            if x.shape != shape:\n                x = broadcast_to(x, shape)\n            if y.shape != shape:\n                y = broadcast_to(y, shape)\n        return x, y\n\n    @property\n    def mu(self):\n        return self.parameter[""mu""]\n\n    @mu.setter\n    def mu(self, mu):\n        self.parameter[""mu""] = mu\n\n    @property\n    def std(self):\n        return self.parameter[""std""]\n\n    @std.setter\n    def std(self, std):\n        try:\n            ispositive = (std.value > 0).all()\n        except AttributeError:\n            ispositive = (std.value > 0)\n\n        if not ispositive:\n            raise ValueError(""value of std must all be positive"")\n        self.parameter[""std""] = std\n\n    @property\n    def var(self):\n        try:\n            return self._var\n        except AttributeError:\n            return square(self.std)\n\n    @var.setter\n    def var(self, var):\n        try:\n            ispositive = (var.value > 0).all()\n        except AttributeError:\n            ispositive = (var.value > 0)\n\n        if not ispositive:\n            raise ValueError(""value of var must all be positive"")\n        self._var = var\n        self.parameter[""std""] = sqrt(var)\n\n    @property\n    def tau(self):\n        try:\n            return self._tau\n        except AttributeError:\n            return 1 / square(self.std)\n\n    @tau.setter\n    def tau(self, tau):\n        try:\n            ispositive = (tau.value > 0).all()\n        except AttributeError:\n            ispositive = (tau.value > 0)\n\n        if not ispositive:\n            raise ValueError(""value of tau must be positive"")\n        self._tau = tau\n        self.parameter[""std""] = 1 / sqrt(tau)\n\n    def forward(self):\n        self.eps = np.random.normal(size=self.mu.shape)\n        output = self.mu.value + self.std.value * self.eps\n        if isinstance(self.mu, Constant) and isinstance(self.var, Constant):\n            return Constant(output)\n        return Tensor(output, self)\n\n    def backward(self, delta):\n        dmu = delta\n        dstd = delta * self.eps\n        self.mu.backward(dmu)\n        self.std.backward(dstd)\n\n    def _pdf(self, x):\n        return (\n            exp(-0.5 * square((x - self.mu) / self.std))\n            / sqrt(2 * np.pi) / self.std\n        )\n\n    def _log_pdf(self, x):\n        return GaussianLogPDF().forward(x, self.mu, self.tau)\n\n\nclass GaussianLogPDF(Function):\n\n    def _check_input(self, x, mu, tau):\n        x = self._convert2tensor(x)\n        mu = self._convert2tensor(mu)\n        tau = self._convert2tensor(tau)\n        if not x.shape == mu.shape == tau.shape:\n            shape = np.broadcast(x.value, mu.value, tau.value).shape\n            if x.shape != shape:\n                x = broadcast_to(x, shape)\n            if mu.shape != shape:\n                mu = broadcast_to(mu, shape)\n            if tau.shape != shape:\n                tau = broadcast_to(tau, shape)\n        return x, mu, tau\n\n    def forward(self, x, mu, tau):\n        x, mu, tau = self._check_input(x, mu, tau)\n        self.x = x\n        self.mu = mu\n        self.tau = tau\n        output = (\n            -0.5 * np.square(x.value - mu.value) * tau.value\n            + 0.5 * np.log(tau.value)\n            - 0.5 * np.log(2 * np.pi)\n        )\n        return Tensor(output, function=self)\n\n    def backward(self, delta):\n        dx = -0.5 * delta * (self.x.value - self.mu.value) * self.tau.value\n        dmu = -0.5 * delta * (self.mu.value - self.x.value) * self.tau.value\n        dtau = 0.5 * delta * (\n            1 / self.tau.value\n            - (self.x.value - self.mu.value) ** 2\n        )\n        self.x.backward(dx)\n        self.mu.backward(dmu)\n        self.tau.backward(dtau)\n'"
books/PRML/PRML-master-Python/prml/nn/random/gaussian_mixture.py,0,"b'import numpy as np\nfrom prml.nn.array.broadcast import broadcast_to\nfrom prml.nn.math.exp import exp\nfrom prml.nn.math.log import log\nfrom prml.nn.math.sqrt import sqrt\nfrom prml.nn.math.square import square\nfrom prml.nn.random.random import RandomVariable\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\n\n\nclass GaussianMixture(RandomVariable):\n    """"""\n    Mixture of the Gaussian distribution\n    p(x|w, mu, std)\n    = w_1 * N(x|mu_1, std_1) + ... + w_K * N(x|mu_K, std_K)\n    Parameters\n    ----------\n    coef : tensor_like\n        mixing coefficient whose sum along specified axis should equal to 1\n    mu : tensor_like\n        mean parameter along specified axis for each component\n    std : tensor_like\n        std parameter along specified axis for each component\n    axis : int\n        axis along which represents each component\n    data : tensor_like\n        realization\n    p : RandomVariable\n        original distribution of a model\n    """"""\n\n    def __init__(self, coef, mu, std, axis=-1, data=None, p=None):\n        super().__init__(data, p)\n        assert axis == -1\n        self.axis = axis\n        self.coef, self.mu, self.std = self._check_input(coef, mu, std)\n\n    def _check_input(self, coef, mu, std):\n        coef = self._convert2tensor(coef)\n        mu = self._convert2tensor(mu)\n        std = self._convert2tensor(std)\n\n        if not coef.shape == mu.shape == std.shape:\n            shape = np.broadcast(coef.value, mu.value, std.value).shape\n            if coef.shape != shape:\n                coef = broadcast_to(coef, shape)\n            if mu.shape != shape:\n                mu = broadcast_to(mu, shape)\n            if std.shape != shape:\n                std = broadcast_to(std, shape)\n        self.n_component = coef.shape[self.axis]\n\n        return coef, mu, std\n\n    @property\n    def axis(self):\n        return self.parameter[""axis""]\n\n    @axis.setter\n    def axis(self, axis):\n        if not isinstance(axis, int):\n            raise TypeError(""axis must be int"")\n        self.parameter[""axis""] = axis\n\n    @property\n    def coef(self):\n        return self.parameter[""coef""]\n\n    @coef.setter\n    def coef(self, coef):\n        self._atleast_ndim(coef, 1)\n        if (coef.value < 0).any():\n            raise ValueError(""value of mixing coefficient must all be positive"")\n\n        if not np.allclose(coef.value.sum(axis=self.axis), 1):\n            raise ValueError(""sum of mixing coefficients must be 1"")\n        self.parameter[""coef""] = coef\n\n    @property\n    def mu(self):\n        return self.parameter[""mu""]\n\n    @mu.setter\n    def mu(self, mu):\n        self.parameter[""mu""] = mu\n\n    @property\n    def std(self):\n        return self.parameter[""std""]\n\n    @std.setter\n    def std(self, std):\n        self._atleast_ndim(std, 1)\n        if (std.value < 0).any():\n            raise ValueError(""value of std must all be positive"")\n        self.parameter[""std""] = std\n\n    @property\n    def var(self):\n        return square(self.parameter[""std""])\n\n    def forward(self):\n        if self.coef.ndim != 1:\n            raise NotImplementedError\n        indices = np.array(\n            [np.random.choice(self.n_component, p=c) for c in self.coef.value]\n        )\n        output = np.random.normal(\n            loc=self.mu.value[indices],\n            scale=self.std.value[indices]\n        )\n        if (\n                isinstance(self.coef, Constant)\n                and isinstance(self.mu, Constant)\n                and isinstance(self.std, Constant)\n        ):\n            return Constant(output)\n        return Tensor(output, function=self)\n\n    def backward(self):\n        raise NotImplementedError\n\n    def _pdf(self, x):\n        gauss = (\n            exp(-0.5 * square((x - self.mu) / self.std))\n            / sqrt(2 * np.pi) / self.std\n        )\n        return (self.coef * gauss).sum(axis=self.axis)\n\n    def _log_pdf(self, x):\n        return log(self.pdf(x))\n'"
books/PRML/PRML-master-Python/prml/nn/random/laplace.py,0,"b'import numpy as np\nfrom prml.nn.array.broadcast import broadcast_to\nfrom prml.nn.math.abs import abs\nfrom prml.nn.math.exp import exp\nfrom prml.nn.math.log import log\nfrom prml.nn.random.random import RandomVariable\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\n\n\nclass Laplace(RandomVariable):\n    """"""\n    Laplace distribution\n    p(x|loc, scale)\n    = exp(-|x - loc|/scale) / (2 * scale)\n    Parameters\n    ----------\n    loc : tensor_like\n        location parameter\n    scale : tensor_like\n        scale parameter\n    data : tensor_like\n        realization\n    p : RandomVariable\n        original distribution of a model\n    """"""\n\n    def __init__(self, loc, scale, data=None, p=None):\n        super().__init__(data, p)\n        self.loc, self.scale = self._check_input(loc, scale)\n\n    def _check_input(self, loc, scale):\n        loc = self._convert2tensor(loc)\n        scale = self._convert2tensor(scale)\n        if loc.shape != scale.shape:\n            shape = np.broadcast(loc.value, scale.value).shape\n            if loc.shape != shape:\n                loc = broadcast_to(loc, shape)\n            if scale.shape != shape:\n                scale = broadcast_to(scale, shape)\n        return loc, scale\n\n    @property\n    def loc(self):\n        return self.parameter[""loc""]\n\n    @loc.setter\n    def loc(self, loc):\n        self.parameter[""loc""] = loc\n\n    @property\n    def scale(self):\n        return self.parameter[""scale""]\n\n    @scale.setter\n    def scale(self, scale):\n        try:\n            ispositive = (scale.value > 0).all()\n        except AttributeError:\n            ispositive = (scale.value > 0)\n\n        if not ispositive:\n            raise ValueError(""value of scale must be positive"")\n        self.parameter[""scale""] = scale\n\n    def forward(self):\n        eps = 0.5 - np.random.uniform(size=self.loc.shape)\n        self.eps = np.sign(eps) * np.log(1 - 2 * np.abs(eps))\n        self.output = self.loc.value - self.scale.value * self.eps\n        if isinstance(self.loc, Constant) and isinstance(self.scale, Constant):\n            return Constant(self.output)\n        return Tensor(self.output, function=self)\n\n    def backward(self, delta):\n        dloc = delta\n        dscale = -delta * self.eps\n        self.loc.backward(dloc)\n        self.scale.backward(dscale)\n\n    def _pdf(self, x):\n        return 0.5 * exp(-abs(x - self.loc) / self.scale) / self.scale\n\n    def _log_pdf(self, x):\n        return np.log(0.5) - abs(x - self.loc) / self.scale - log(self.scale)\n'"
books/PRML/PRML-master-Python/prml/nn/random/multivariate_gaussian.py,0,"b'import numpy as np\nfrom prml.nn.array.broadcast import broadcast_to\nfrom prml.nn.linalg.cholesky import cholesky\nfrom prml.nn.linalg.det import det\nfrom prml.nn.linalg.logdet import logdet\nfrom prml.nn.linalg.solve import solve\nfrom prml.nn.linalg.trace import trace\nfrom prml.nn.math.exp import exp\nfrom prml.nn.math.log import log\nfrom prml.nn.math.sqrt import sqrt\nfrom prml.nn.random.random import RandomVariable\nfrom prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.tensor import Tensor\n\n\nclass MultivariateGaussian(RandomVariable):\n    """"""\n    Multivariate Gaussian distribution\n    p(x|mu, cov)\n    = exp{-0.5 * (x - mu)^T cov^-1 (x - mu)} * (1 / 2pi) ** (d / 2) * |cov^-1| ** 0.5\n    where d = dimensionality\n    Parameters\n    ----------\n    mu : (d,) tensor_like\n        mean parameter\n    cov : (d, d) tensor_like\n        variance-covariance matrix\n    data : (..., d) tensor_like\n        observed data\n    p : RandomVariable\n        original distribution of a model\n    """"""\n\n    def __init__(self, mu, cov, data=None, p=None):\n        super().__init__(data, p)\n        self.mu, self.cov = self._check_input(mu, cov)\n\n    def _check_input(self, mu, cov):\n        mu = self._convert2tensor(mu)\n        cov = self._convert2tensor(cov)\n        self._equal_ndim(mu, 1)\n        self._equal_ndim(cov, 2)\n        if cov.shape != (mu.size, mu.size):\n            raise ValueError(""Mismatching dimensionality of mu and cov"")\n        return mu, cov\n\n    @property\n    def mu(self):\n        return self.parameter[""mu""]\n\n    @mu.setter\n    def mu(self, mu):\n        self.parameter[""mu""] = mu\n\n    @property\n    def cov(self):\n        return self.parameter[""cov""]\n\n    @cov.setter\n    def cov(self, cov):\n        try:\n            self.L = cholesky(cov)\n        except np.linalg.LinAlgError:\n            raise ValueError(""cov must be positive-difinite matrix"")\n        self.parameter[""cov""] = cov\n\n    def forward(self):\n        self.eps = np.random.normal(size=self.mu.size)\n        output = self.mu.value + self.L.value @ self.eps\n        if isinstance(self.mu, Constant) and isinstance(self.cov, Constant):\n            return Constant(output)\n        return Tensor(output, self)\n\n    def backward(self, delta):\n        dmu = delta\n        dL = delta * self.eps[:, None]\n        self.mu.backward(dmu)\n        self.L.backward(dL)\n\n    def _pdf(self, x):\n        assert x.shape[-1] == self.mu.size\n        if x.ndim == 1:\n            squeeze = True\n            x = broadcast_to(x, (1, self.mu.size))\n        else:\n            squeeze = False\n        assert x.ndim == 2\n        d = x - self.mu\n        d = d.transpose()\n        p = (\n            exp(-0.5 * (solve(self.cov, d) * d).sum(axis=0))\n            / (2 * np.pi) ** (self.mu.size * 0.5)\n            / sqrt(det(self.cov))\n        )\n        if squeeze:\n            p = p.sum()\n\n        return p\n\n    def _log_pdf(self, x):\n        assert x.shape[-1] == self.mu.size\n        if x.ndim == 1:\n            squeeze = True\n            x = broadcast_to(x, (1, self.mu.size))\n        else:\n            squeeze = False\n        assert x.ndim == 2\n        d = x - self.mu\n        d = d.transpose()\n\n        logp = (\n            -0.5 * (solve(self.cov, d) * d).sum(axis=0)\n            - (self.mu.size * 0.5) * log(2 * np.pi)\n            - 0.5 * logdet(self.cov)\n        )\n        if squeeze:\n            logp = logp.sum()\n\n        return logp\n'"
books/PRML/PRML-master-Python/prml/nn/random/random.py,0,"b'from prml.nn.function import Function\nfrom prml.nn.tensor.constant import Constant\n\n\nclass RandomVariable(Function):\n    """"""\n    base class for random variables\n    """"""\n\n    def __init__(self, data=None, p=None):\n        """"""\n        construct a random variable\n        Parameters\n        ----------\n        data : tensor_like\n            observed data\n        p : RandomVariable\n            original distribution of a model\n        Returns\n        -------\n        parameter : dict\n            dictionary of parameters\n        observed : bool\n            flag of observed or not\n        """"""\n        if data is not None and p is not None:\n            raise ValueError(""Cannot assign both data and p at a time"")\n        if data is not None:\n            data = self._convert2tensor(data)\n        self.data = data\n        self.observed = isinstance(data, Constant)\n        self.p = p\n        self.parameter = dict()\n\n    @property\n    def p(self):\n        return self._p\n\n    @p.setter\n    def p(self, p):\n        if p is not None and not isinstance(p, RandomVariable):\n            raise TypeError(""p must be RandomVariable"")\n        self._p = p\n\n    def __repr__(self):\n        string = f""{self.__class__.__name__}(\\n""\n        for key, value in self.parameter.items():\n            string += ("" "" * 4)\n            string += f""{key}={value}""\n            string += ""\\n""\n        string += "")""\n        return string\n\n    def draw(self):\n        """"""\n        generate a sample\n        Returns\n        -------\n        sample : tensor\n            sample generated from this random variable\n        """"""\n        if self.observed:\n            raise ValueError(""draw method cannot be used for observed random variable"")\n        self.data = self.forward()\n        return self.data\n\n    def pdf(self, x=None):\n        """"""\n        compute probability density function\n        Parameters\n        ----------\n        x : tensor_like\n            observed data\n        Returns\n        -------\n        p : Tensor\n            value of probability density function for each input\n        """"""\n        if not hasattr(self, ""_pdf""):\n            raise NotImplementedError\n        if x is None:\n            if self.data is None:\n                raise ValueError(""There is no given or sampled data"")\n            return self._pdf(self.data)\n        return self._pdf(x)\n\n    def log_pdf(self, x=None):\n        """"""\n        logarithm of probability density function\n        Parameters\n        ----------\n        x : tensor_like\n            observed data\n        Returns\n        -------\n        output : Tensor\n            logarithm of probability density function\n        """"""\n        if not hasattr(self, ""_log_pdf""):\n            raise NotImplementedError\n        if x is None:\n            if self.data is None:\n                raise ValueError(""No given or sampled data"")\n            return self._log_pdf(self.data)\n        return self._log_pdf(x)\n\n    def KLqp(self):\n        r""""""\n        compute Kullback Leibler Divergence\n        KL(q(self)||p) = \\int q(x) ln(q(x) / p(x)) dx\n        Returns\n        -------\n        kl : Tensor\n            KL divergence\n        """"""\n        if self.p is None:\n            raise ValueError(""There is no assigned distribution p"")\n        if self.data is None:\n            raise ValueError(""There is no sampled data"")\n        return self.log_pdf() - self.p.log_pdf(self.data)\n'"
books/PRML/PRML-master-Python/prml/nn/tensor/__init__.py,0,"b'from prml.nn.tensor.constant import Constant\nfrom prml.nn.tensor.parameter import Parameter\nfrom prml.nn.tensor.tensor import Tensor\n\n\n__all__ = [\n    ""Constant"",\n    ""Parameter"",\n    ""Tensor""\n]\n'"
books/PRML/PRML-master-Python/prml/nn/tensor/constant.py,0,"b'from prml.nn.tensor.tensor import Tensor\n\n\nclass Constant(Tensor):\n    """"""\n    constant tensor class\n    """"""\n\n    def __init__(self, value):\n        super().__init__(value)\n'"
books/PRML/PRML-master-Python/prml/nn/tensor/parameter.py,0,"b'from prml.nn.tensor.tensor import Tensor\n\n\nclass Parameter(Tensor):\n    """"""\n    parameter to be optimized\n    """"""\n\n    def __init__(self, value, prior=None):\n        super().__init__(value, function=None)\n        self.grad = None\n        self.prior = prior\n\n    def _backward(self, delta, **kwargs):\n        if self.grad is None:\n            self.grad = delta\n        else:\n            self.grad += delta\n\n    def cleargrad(self):\n        self.grad = None\n        if self.prior is not None:\n            loss = -self.prior.log_pdf(self).sum()\n            loss.backward()\n'"
books/PRML/PRML-master-Python/prml/nn/tensor/tensor.py,0,"b'import numpy as np\n\n\nclass Tensor(object):\n    """"""\n    a base class for tensor object\n    """"""\n    __array_ufunc__ = None\n\n    def __init__(self, value, function=None):\n        """"""\n        construct Tensor object\n        Parameters\n        ----------\n        value : array_like\n            value of this tensor\n        function : Function\n            function output this tensor\n        """"""\n        if not isinstance(value, (int, float, np.number, np.ndarray)):\n            raise TypeError(\n                ""Unsupported class for Tensor: {}"".format(type(value))\n            )\n        self.value = value\n        self.function = function\n\n    def __format__(self, *args, **kwargs):\n        return self.__repr__()\n\n    def __repr__(self):\n        if isinstance(self.value, np.ndarray):\n            return (\n                ""{0}(shape={1.shape}, dtype={1.dtype})""\n                .format(self.__class__.__name__, self.value)\n            )\n        else:\n            return (\n                ""{0}(value={1})"".format(self.__class__.__name__, self.value)\n            )\n\n    @property\n    def ndim(self):\n        if hasattr(self.value, ""ndim""):\n            return self.value.ndim\n        else:\n            return 0\n\n    @property\n    def shape(self):\n        if hasattr(self.value, ""shape""):\n            return self.value.shape\n        else:\n            return ()\n\n    @property\n    def size(self):\n        if hasattr(self.value, ""size""):\n            return self.value.size\n        else:\n            return 1\n\n    def backward(self, delta=1, **kwargs):\n        """"""\n        back-propagate error\n        Parameters\n        ----------\n        delta : array_like\n            derivative with respect to this array\n        """"""\n        if isinstance(delta, np.ndarray):\n            if delta.shape != self.shape:\n                raise ValueError(\n                    ""shapes {} and {} not aligned""\n                    .format(delta.shape, self.shape)\n                )\n        elif isinstance(delta, (int, float, np.number)):\n            if self.shape != ():\n                raise ValueError(\n                    ""delta must be np.ndarray""\n                )\n        else:\n            raise TypeError(\n                ""unsupported class for delta""\n            )\n        self._backward(delta, **kwargs)\n\n    def _backward(self, delta, **kwargs):\n        if hasattr(self.function, ""backward""):\n            self.function.backward(delta, **kwargs)\n'"
books/PRML/PRML-master-Python/test/nn/array/__init__.py,0,b''
books/PRML/PRML-master-Python/test/nn/array/broadcast.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\nfrom prml.nn.array.broadcast import broadcast_to\n\n\nclass TestBroadcastTo(unittest.TestCase):\n\n    def test_broadcast(self):\n        x = nn.Parameter(np.ones((1, 1)))\n        shape = (5, 2, 3)\n        y = broadcast_to(x, shape)\n        self.assertEqual(y.shape, shape)\n        y.backward(np.ones(shape))\n        self.assertTrue((x.grad == np.ones((1, 1)) * 30).all())\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/array/flatten.py,0,"b'import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestFlatten(unittest.TestCase):\n\n    def test_flatten(self):\n        self.assertRaises(TypeError, nn.flatten, ""abc"")\n        self.assertRaises(ValueError, nn.flatten, np.ones(1))\n\n        x = np.random.rand(5, 4)\n        p = nn.Parameter(x)\n        y = p.flatten()\n        self.assertTrue((y.value == x.flatten()).all())\n        y.backward(np.ones(20))\n        self.assertTrue((p.grad == np.ones((5, 4))).all())\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
books/PRML/PRML-master-Python/test/nn/array/reshape.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestReshape(unittest.TestCase):\n\n    def test_reshape(self):\n        self.assertRaises(ValueError, nn.reshape, 1, (2, 3))\n\n        x = np.random.rand(2, 6)\n        p = nn.Parameter(x)\n        y = p.reshape(3, 4)\n        self.assertTrue((x.reshape(3, 4) == y.value).all())\n        y.backward(np.ones((3, 4)))\n        self.assertTrue((p.grad == np.ones((2, 6))).all())\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/array/split.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestSplit(unittest.TestCase):\n\n    def test_split(self):\n        x = np.random.rand(10, 7)\n        a = nn.Parameter(x)\n        b, c = nn.split(a, (3,), axis=-1)\n        self.assertTrue((b.value == x[:, :3]).all())\n        self.assertTrue((c.value == x[:, 3:]).all())\n        b.backward(np.ones((10, 3)))\n        self.assertIs(a.grad, None)\n        c.backward(np.ones((10, 4)))\n        self.assertTrue((a.grad == np.ones((10, 7))).all())\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/array/transpose.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestTranspose(unittest.TestCase):\n\n    def test_transpose(self):\n        arrays = [\n            np.random.normal(size=(2, 3)),\n            np.random.normal(size=(2, 3, 4))\n        ]\n        axes = [\n            None,\n            (2, 0, 1)\n        ]\n\n        for arr, ax in zip(arrays, axes):\n            arr = nn.Parameter(arr)\n            arr_t = nn.transpose(arr, ax)\n            self.assertEqual(arr_t.shape, np.transpose(arr.value, ax).shape)\n            da = np.random.normal(size=arr_t.shape)\n            arr_t.backward(da)\n            self.assertEqual(arr.grad.shape, arr.shape)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/linalg/__init__.py,0,b''
books/PRML/PRML-master-Python/test/nn/linalg/cholesky.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestCholesky(unittest.TestCase):\n\n    def test_cholesky(self):\n        A = np.array([\n            [2., -1],\n            [-1., 5.]\n        ])\n        L = np.linalg.cholesky(A)\n        Ap = nn.Parameter(A)\n        L_test = nn.linalg.cholesky(Ap)\n        self.assertTrue((L == L_test.value).all())\n\n        T = np.array([\n            [1., 0.],\n            [-1., 2.]\n        ])\n        for _ in range(1000):\n            Ap.cleargrad()\n            L_ = nn.linalg.cholesky(Ap)\n            loss = nn.square(T - L_).sum()\n            loss.backward()\n            Ap.value -= 0.1 * Ap.grad\n\n        self.assertTrue(np.allclose(Ap.value, T @ T.T))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/linalg/det.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestDeterminant(unittest.TestCase):\n\n    def test_determinant(self):\n        A = np.array([\n            [2., 1.],\n            [1., 3.]\n        ])\n        detA = np.linalg.det(A)\n        self.assertTrue((detA == nn.linalg.det(A).value).all())\n\n        A = nn.Parameter(A)\n        for _ in range(100):\n            A.cleargrad()\n            detA = nn.linalg.det(A)\n            loss = nn.square(detA - 1)\n            loss.backward()\n            A.value -= 0.1 * A.grad\n        self.assertAlmostEqual(detA.value, 1.)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/linalg/inv.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestInverse(unittest.TestCase):\n\n    def test_inverse(self):\n        A = np.array([\n            [2., 1.],\n            [1., 3.]\n        ])\n        Ainv = np.linalg.inv(A)\n        self.assertTrue((Ainv == nn.linalg.inv(A).value).all())\n\n        B = np.array([\n            [-1., 1.],\n            [1., 0.5]\n        ])\n        A = nn.Parameter(np.array([\n            [-0.4, 0.7],\n            [0.7, 0.7]\n        ]))\n        for _ in range(100):\n            A.cleargrad()\n            Ainv = nn.linalg.inv(A)\n            loss = nn.square(Ainv - B).sum()\n            loss.backward()\n            A.value -= 0.1 * A.grad\n\n        self.assertTrue(np.allclose(A.value, np.linalg.inv(B)))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/linalg/logdet.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestLogdet(unittest.TestCase):\n\n    def test_logdet(self):\n        A = np.array([\n            [2., 1.],\n            [1., 3.]\n        ])\n        logdetA = np.linalg.slogdet(A)[1]\n        self.assertTrue((logdetA == nn.linalg.logdet(A).value).all())\n\n        A = nn.Parameter(A)\n        for _ in range(100):\n            A.cleargrad()\n            logdetA = nn.linalg.logdet(A)\n            loss = nn.square(logdetA - 1)\n            loss.backward()\n            A.value -= 0.1 * A.grad\n        self.assertAlmostEqual(logdetA.value, 1)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/linalg/solve.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestSolve(unittest.TestCase):\n\n    def test_solve(self):\n        A = np.array([\n            [2., 1.],\n            [1., 3.]\n        ])\n        B = np.array([1., 2.])[:, None]\n        AinvB = np.linalg.solve(A, B)\n        self.assertTrue((AinvB == nn.linalg.solve(A, B).value).all())\n\n        A = nn.Parameter(A)\n        B = nn.Parameter(B)\n        for _ in range(100):\n            A.cleargrad()\n            B.cleargrad()\n            AinvB = nn.linalg.solve(A, B)\n            loss = nn.square(AinvB - 1).sum()\n            loss.backward()\n            A.value -= A.grad\n            B.value -= B.grad\n        self.assertTrue(np.allclose(AinvB.value, 1))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/linalg/trace.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestTrace(unittest.TestCase):\n\n    def test_trace(self):\n        arrays = [\n            np.random.normal(size=(2, 2)),\n            np.random.normal(size=(3, 4))\n        ]\n\n        for arr in arrays:\n            arr = nn.Parameter(arr)\n            tr_arr = nn.linalg.trace(arr)\n            self.assertEqual(tr_arr.value, np.trace(arr.value))\n\n        a = np.array([\n            [1.5, 0],\n            [-0.1, 1.1]\n        ])\n        a = nn.Parameter(a)\n        for _ in range(100):\n            a.cleargrad()\n            loss = nn.square(nn.linalg.trace(a) - 2)\n            loss.backward()\n            a.value -= 0.1 * a.grad\n        self.assertEqual(nn.linalg.trace(a).value, 2)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/math/__init__.py,0,b''
books/PRML/PRML-master-Python/test/nn/math/abs.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestAbs(unittest.TestCase):\n\n    def test_abs(self):\n        np.random.seed(1234)\n        x = nn.Parameter(np.random.randn(5, 7))\n        sign = np.sign(x.value)\n        y = nn.abs(x)\n        self.assertTrue((y.value == np.abs(x.value)).all())\n\n        for _ in range(10000):\n            x.cleargrad()\n            y = nn.abs(x)\n            nn.square(y - 0.01).sum().backward()\n            x.value -= x.grad * 0.001\n        self.assertTrue(np.allclose(x.value, 0.01 * sign))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/math/add.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestAdd(unittest.TestCase):\n\n    def test_add(self):\n        x = nn.Parameter(2)\n        z = x + 5\n        self.assertEqual(z.value, 7)\n        z.backward()\n        self.assertEqual(x.grad, 1)\n\n        x = np.random.rand(5, 4)\n        y = np.random.rand(4)\n        p = nn.Parameter(y)\n        z = x + p\n        self.assertTrue((z.value == x + y).all())\n        z.backward(np.ones((5, 4)))\n        self.assertTrue((p.grad == np.ones(4) * 5).all())\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/math/divide.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestDivide(unittest.TestCase):\n\n    def test_divide(self):\n        x = nn.Parameter(10.)\n        z = x / 2\n        self.assertEqual(z.value, 5)\n        z.backward()\n        self.assertEqual(x.grad, 0.5)\n\n        x = np.random.rand(5, 10, 3)\n        y = np.random.rand(10, 1)\n        p = nn.Parameter(y)\n        z = x / p\n        self.assertTrue((z.value == x / y).all())\n        z.backward(np.ones((5, 10, 3)))\n        d = np.sum(-x / y ** 2, axis=0).sum(axis=1, keepdims=True)\n        self.assertTrue((p.grad == d).all())\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/math/exp.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestExp(unittest.TestCase):\n\n    def test_exp(self):\n        x = nn.Parameter(2.)\n        y = nn.exp(x)\n        self.assertEqual(y.value, np.exp(2))\n        y.backward()\n        self.assertEqual(x.grad, np.exp(2))\n\n        x = np.random.rand(5, 3)\n        p = nn.Parameter(x)\n        y = nn.exp(p)\n        self.assertTrue((y.value == np.exp(x)).all())\n        y.backward(np.ones((5, 3)))\n        self.assertTrue((p.grad == np.exp(x)).all())\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/math/gamma.py,0,"b""import unittest\nfrom prml import nn\n\n\nclass TestGamma(unittest.TestCase):\n\n    def test_gamma(self):\n        self.assertEqual(24, nn.gamma(5).value)\n        a = nn.Parameter(2.5)\n        eps = 1e-5\n        b = nn.gamma(a)\n        b.backward()\n        num_grad = ((nn.gamma(a + eps) - nn.gamma(a - eps)) / (2 * eps)).value\n        self.assertAlmostEqual(a.grad, num_grad)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/math/log.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestLog(unittest.TestCase):\n\n    def test_log(self):\n        x = nn.Parameter(2.)\n        y = nn.log(x)\n        self.assertEqual(y.value, np.log(2))\n        y.backward()\n        self.assertEqual(x.grad, 0.5)\n\n        x = np.random.rand(4, 6)\n        p = nn.Parameter(x)\n        y = nn.log(p)\n        self.assertTrue((y.value == np.log(x)).all())\n        y.backward(np.ones((4, 6)))\n        self.assertTrue((p.grad == 1 / x).all())\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/math/matmul.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestMatMul(unittest.TestCase):\n\n    def test_matmul(self):\n        x = np.random.rand(10, 3)\n        y = np.random.rand(3, 5)\n        g = np.random.rand(10, 5)\n        xp = nn.Parameter(x)\n        z = xp @ y\n        self.assertTrue((z.value == x @ y).all())\n        z.backward(g)\n        self.assertTrue((xp.grad == g @ y.T).all())\n\n        yp = nn.Parameter(y)\n        z = x @ yp\n        self.assertTrue((z.value == x @ y).all())\n        z.backward(g)\n        self.assertTrue((yp.grad == x.T @ g).all())\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/math/mean.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestMean(unittest.TestCase):\n\n    def test_mean(self):\n        x = np.random.rand(5, 1, 2)\n        xp = nn.Parameter(x)\n        z = xp.mean()\n        self.assertEqual(z.value, x.mean())\n        z.backward()\n        self.assertTrue((xp.grad == np.ones((5, 1, 2)) / 10).all())\n        xp.cleargrad()\n\n        z = xp.mean(axis=0, keepdims=True)\n        self.assertEqual(z.shape, (1, 1, 2))\n        self.assertTrue((z.value == x.mean(axis=0, keepdims=True)).all())\n        z.backward(np.ones((1, 1, 2)))\n        self.assertTrue((xp.grad == np.ones((5, 1, 2)) / 5).all())\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/math/multiply.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestMultiply(unittest.TestCase):\n\n    def test_multiply(self):\n        x = nn.Parameter(2)\n        y = x * 5\n        self.assertEqual(y.value, 10)\n        y.backward()\n        self.assertEqual(x.grad, 5)\n\n        x = np.random.rand(5, 4)\n        y = np.random.rand(4)\n        yp = nn.Parameter(y)\n        z = x * yp\n        self.assertTrue((z.value == x * y).all())\n        z.backward(np.ones((5, 4)))\n        self.assertTrue((yp.grad == x.sum(axis=0)).all())\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/math/negative.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestNegative(unittest.TestCase):\n\n    def test_negative(self):\n        x = nn.Parameter(2.)\n        y = -x\n        self.assertEqual(y.value, -2)\n        y.backward()\n        self.assertEqual(x.grad, -1)\n\n        x = np.random.rand(2, 3)\n        xp = nn.Parameter(x)\n        y = -xp\n        self.assertTrue((y.value == -x).all())\n        y.backward(np.ones((2, 3)))\n        self.assertTrue((xp.grad == -np.ones((2, 3))).all())\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/math/power.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestPower(unittest.TestCase):\n\n    def test_power(self):\n        x = nn.Parameter(2.)\n        y = 2 ** x\n        self.assertEqual(y.value, 4)\n        y.backward()\n        self.assertEqual(x.grad, 4 * np.log(2))\n\n        x = np.random.rand(10, 2)\n        xp = nn.Parameter(x)\n        y = xp ** 3\n        self.assertTrue((y.value == x ** 3).all())\n        y.backward(np.ones((10, 2)))\n        self.assertTrue(np.allclose(xp.grad, 3 * x ** 2))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/math/sqrt.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestSqrt(unittest.TestCase):\n\n    def test_sqrt(self):\n        x = nn.Parameter(2.)\n        y = nn.sqrt(x)\n        self.assertEqual(y.value, np.sqrt(2))\n        y.backward()\n        self.assertEqual(x.grad, 0.5 / np.sqrt(2))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/math/square.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestSqrt(unittest.TestCase):\n\n    def test_sqrt(self):\n        x = nn.Parameter(2.)\n        y = nn.square(x)\n        self.assertEqual(y.value, 4)\n        y.backward()\n        self.assertEqual(x.grad, 4)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/math/subtract.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestSubtract(unittest.TestCase):\n\n    def test_forward_backward(self):\n        x = nn.Parameter(2)\n        z = x - 5\n        self.assertEqual(z.value, -3)\n        z.backward()\n        self.assertEqual(x.grad, 1)\n\n        x = np.random.rand(5, 4)\n        y = np.random.rand(4)\n        p = nn.Parameter(y)\n        z = x - p\n        self.assertTrue((z.value == x - y).all())\n        z.backward(np.ones((5, 4)))\n        self.assertTrue((p.grad == -np.ones(4) * 5).all())\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/math/sum.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestSum(unittest.TestCase):\n\n    def test_sum(self):\n        x = np.random.rand(5, 1, 2)\n        xp = nn.Parameter(x)\n        z = xp.sum()\n        self.assertEqual(z.value, x.sum())\n        z.backward()\n        self.assertTrue((xp.grad == np.ones((5, 1, 2))).all())\n        xp.cleargrad()\n\n        z = xp.sum(axis=0, keepdims=True)\n        self.assertEqual(z.shape, (1, 1, 2))\n        self.assertTrue((z.value == x.sum(axis=0, keepdims=True)).all())\n        z.backward(np.ones((1, 1, 2)))\n        self.assertTrue((xp.grad == np.ones((5, 1, 2))).all())\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/nonlinear/__init__.py,0,b''
books/PRML/PRML-master-Python/test/nn/nonlinear/sigmoid.py,0,"b""import unittest\nfrom prml import nn\n\n\nclass TestSigmoid(unittest.TestCase):\n\n    def test_sigmoid(self):\n        self.assertEqual(nn.sigmoid(0).value, 0.5)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/nonlinear/softmax.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestSoftmax(unittest.TestCase):\n\n    def test_softmax(self):\n        self.assertTrue(np.allclose(nn.softmax(np.ones(4)).value, 0.25))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/nonlinear/softplus.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestSoftplus(unittest.TestCase):\n\n    def test_softplus(self):\n        self.assertEqual(nn.softplus(0).value, np.log(2))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/nonlinear/tanh.py,0,"b""import unittest\nfrom prml import nn\n\n\nclass TestTanh(unittest.TestCase):\n\n    def test_tanh(self):\n        self.assertEqual(nn.tanh(0).value, 0)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/random/__init__.py,0,b''
books/PRML/PRML-master-Python/test/nn/random/bernoulli.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestBernoulli(unittest.TestCase):\n\n    def test_bernoulli(self):\n        np.random.seed(1234)\n        obs = np.random.choice(2, 1000, p=[0.1, 0.9])\n        a = nn.Parameter(0)\n        for _ in range(100):\n            a.cleargrad()\n            x = nn.random.Bernoulli(logit=a, data=obs)\n            x.log_pdf().sum().backward()\n            a.value += a.grad * 0.01\n        self.assertAlmostEqual(x.mu.value, np.mean(obs))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/random/categorical.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestCategorical(unittest.TestCase):\n\n    def test_categorical(self):\n        np.random.seed(1234)\n        obs = np.random.choice(3, 100, p=[0.2, 0.3, 0.5])\n        obs = np.eye(3)[obs]\n        a = nn.Parameter(np.zeros(3))\n        for _ in range(100):\n            a.cleargrad()\n            x = nn.random.Categorical(logit=a, data=obs)\n            x.log_pdf().sum().backward()\n            a.value += 0.01 * a.grad\n        self.assertTrue(np.allclose(np.mean(obs, 0), x.mu.value))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/random/cauchy.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestCauchy(unittest.TestCase):\n\n    def test_cauchy(self):\n        np.random.seed(1234)\n        obs = np.random.standard_cauchy(size=10000)\n        obs = 2 * obs + 1\n        loc = nn.Parameter(0)\n        s = nn.Parameter(1)\n        for _ in range(100):\n            loc.cleargrad()\n            s.cleargrad()\n            x = nn.random.Cauchy(loc, nn.softplus(s), data=obs)\n            x.log_pdf().sum().backward()\n            loc.value += loc.grad * 0.001\n            s.value += s.grad * 0.001\n        self.assertAlmostEqual(x.loc.value, 1, places=1)\n        self.assertAlmostEqual(x.scale.value, 2, places=1)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/random/dirichlet.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestDirichlet(unittest.TestCase):\n\n    def test_dirichlet(self):\n        np.random.seed(1234)\n        obs = np.random.choice(3, 100, p=[0.2, 0.3, 0.5])\n        obs = np.eye(3)[obs]\n        a = nn.Parameter(np.zeros(3))\n        for _ in range(100):\n            a.cleargrad()\n            mu = nn.softmax(a)\n            d = nn.random.Dirichlet(np.ones(3) * 10, data=mu)\n            x = nn.random.Categorical(mu, data=obs)\n            log_posterior = x.log_pdf().sum() + d.log_pdf().sum()\n            log_posterior.backward()\n            a.value += 0.01 * a.grad\n\n        count = np.sum(obs, 0) + 10\n        p = count / count.sum(keepdims=True)\n        self.assertTrue(np.allclose(p, mu.value, 1e-2, 1e-2))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/random/exponential.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestExponential(unittest.TestCase):\n\n    def test_exponential(self):\n        np.random.seed(1234)\n        obs = np.random.gamma(1, 1 / 0.5, size=1000)\n        a = nn.Parameter(0)\n        for _ in range(100):\n            a.cleargrad()\n            x = nn.random.Exponential(nn.softplus(a), data=obs)\n            x.log_pdf().sum().backward()\n            a.value += a.grad * 0.001\n        self.assertAlmostEqual(x.rate.value, 0.475135117)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/random/gaussian.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestGaussian(unittest.TestCase):\n\n    def test_gaussian(self):\n        self.assertRaises(ValueError, nn.random.Gaussian, 0, -1)\n        self.assertRaises(ValueError, nn.random.Gaussian, 0, np.array([1, -1]))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/random/laplace.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestLaplace(unittest.TestCase):\n\n    def test_laplace(self):\n        obs = np.arange(3)\n        loc = nn.Parameter(0)\n        s = nn.Parameter(1)\n        for _ in range(1000):\n            loc.cleargrad()\n            s.cleargrad()\n            x = nn.random.Laplace(loc, nn.softplus(s), data=obs)\n            x.log_pdf().sum().backward()\n            loc.value += loc.grad * 0.01\n            s.value += s.grad * 0.01\n        self.assertAlmostEqual(x.loc.value, np.median(obs), places=1)\n        self.assertAlmostEqual(x.scale.value, np.mean(np.abs(obs - x.loc.value)), places=1)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
books/PRML/PRML-master-Python/test/nn/random/multivariate_gaussian.py,0,"b""import unittest\nimport numpy as np\nfrom prml import nn\n\n\nclass TestMultivariateGaussian(unittest.TestCase):\n\n    def test_multivariate_gaussian(self):\n        self.assertRaises(ValueError, nn.random.MultivariateGaussian, np.zeros(2), np.eye(3))\n        self.assertRaises(ValueError, nn.random.MultivariateGaussian, np.zeros(2), np.eye(2) * -1)\n\n        x_train = np.array([\n            [1., 1.],\n            [1., -1],\n            [-1., 1.],\n            [-1., -2.]\n        ])\n        mu = nn.Parameter(np.ones(2))\n        cov = nn.Parameter(np.eye(2) * 2)\n        for _ in range(1000):\n            mu.cleargrad()\n            cov.cleargrad()\n            x = nn.random.MultivariateGaussian(mu, cov + cov.transpose(), data=x_train)\n            log_likelihood = x.log_pdf().sum()\n            log_likelihood.backward()\n            mu.value += 0.1 * mu.grad\n            cov.value += 0.1 * cov.grad\n        self.assertTrue(np.allclose(mu.value, x_train.mean(axis=0)))\n        self.assertTrue(np.allclose(np.cov(x_train, rowvar=False, bias=True), x.cov.value))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
