file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n\nlong_description = """"""\nThis repository is my project to bring velocity to deep-learning research, by providing tried and tested large pool of\nprebuilt components that are known to be working well together.\n\nI would like to minimize time to market of new projects, ease experimentation and provide bits of experiment management\nto bring some order to the data science workflow.\n\nIdeally, for most applications it should be enough to write a config file wiring existing components together.\nIf that\'s not the case writing bits of custom code shouldn\'t be unnecessarily complex.\n\nThis repository is still in an early stage of that journey but it will grow as I\'ll be putting some work into it.\n""""""\n\n\nsetup(\n    name=\'vel\',\n    version=\'0.3.0\',\n    description=""Velocity in deep-learning research"",\n    long_description=long_description,\n    url=\'https://github.com/MillionIntegrals/vel\',\n    author=\'Jerry Tworek\',\n    author_email=\'jerry@millionintegrals.com\',\n    license=\'MIT\',\n    packages=find_packages(exclude=[""*.tests"", ""*.tests.*"", ""tests.*"", ""tests""]),\n    python_requires=\'>=3.6\',\n    install_requires=[\n        \'attrs\',\n        \'cloudpickle\',\n        \'numpy\',\n        \'opencv-python\',\n        \'pandas\',\n        \'pyyaml\',\n        \'scikit-learn\',\n        \'torch ~= 1.0\',\n        \'torchvision\',\n        \'torchtext\',\n        \'tqdm\'\n    ],\n    extras_require={\n        \'visdom\': [\'visdom\'],\n        \'mongo\': [\'pymongo\', \'dnspython\'],\n        \'gym\': [\'gym[atari,box2d,classic_control]\'],\n        \'mujoco\': [\'gym[mujoco,robotics]\'],\n        \'dev\': [\'pytest\', \'ipython\', \'jupyter\'],\n        \'text\': [\'spacy\'],\n        \'all\': [\'visdom\', \'pymongo\', \'dnspython\', \'gym[all]\', \'pytest\', \'spacy\', \'ipython\', \'jupyter\']\n    },\n    tests_require=[\n        \'pytest\'\n    ],\n    entry_points={\n        \'console_scripts\': [\n            \'vel = vel.launcher:main\',\n        ],\n    },\n    classifiers=[\n        \'Development Status :: 3 - Alpha\',\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\'\n    ],\n    scripts=[]\n)\n'"
vel/__init__.py,0,b''
vel/exceptions.py,0,"b'\n\nclass VelException(Exception):\n    """""" Top-level Vel exception """"""\n    pass\n\n\nclass VelInitializationException(VelException):\n    """""" Exception during system initialization """"""\n    pass\n'"
vel/launcher.py,0,"b'#!/usr/bin/env python\nimport argparse\nimport multiprocessing\nimport sys\n\nfrom vel.internals.model_config import ModelConfig\nfrom vel.internals.parser import Parser\n\n\ndef main():\n    """""" Paperboy entry point - parse the arguments and run a command """"""\n    parser = argparse.ArgumentParser(description=\'Paperboy deep learning launcher\')\n\n    parser.add_argument(\'config\', metavar=\'FILENAME\', help=\'Configuration file for the run\')\n    parser.add_argument(\'command\', metavar=\'COMMAND\', help=\'A command to run\')\n    parser.add_argument(\'varargs\', nargs=\'*\', metavar=\'VARARGS\', help=\'Extra options to the command\')\n    parser.add_argument(\'-r\', \'--run_number\', type=int, default=0, help=""A run number"")\n    parser.add_argument(\'-d\', \'--device\', default=\'cuda\', help=""A device to run the model on"")\n    parser.add_argument(\'-s\', \'--seed\', type=int, default=None, help=""Random seed for the project"")\n    parser.add_argument(\n        \'-p\', \'--param\', type=str, metavar=\'NAME=VALUE\', action=\'append\', default=[],\n        help=""Configuration parameters""\n    )\n    parser.add_argument(\n        \'--continue\', action=\'store_true\', default=False, help=""Continue previously started learning process""\n    )\n    parser.add_argument(\n        \'--profile\', type=str, default=None, help=""Profiler output""\n    )\n\n    args = parser.parse_args()\n\n    model_config = ModelConfig.from_file(\n        args.config, args.run_number, continue_training=getattr(args, \'continue\'), device=args.device, seed=args.seed,\n        params={k: v for (k, v) in (Parser.parse_equality(eq) for eq in args.param)}\n    )\n\n    if model_config.project_dir not in sys.path:\n        sys.path.append(model_config.project_dir)\n\n    multiprocessing_setting = model_config.provide_with_default(\'multiprocessing\', default=None)\n\n    if multiprocessing_setting:\n        # This needs to be called before any of PyTorch module is imported\n        multiprocessing.set_start_method(multiprocessing_setting)\n\n    # Set seed already in the launcher\n    from vel.util.random import set_seed\n    set_seed(model_config.seed)\n\n    model_config.banner(args.command)\n\n    if args.profile:\n        print(""[PROFILER] Running Vel in profiling mode, output filename={}"".format(args.profile))\n        import cProfile\n        import pstats\n        profiler = cProfile.Profile()\n        profiler.enable()\n        model_config.run_command(args.command, args.varargs)\n        profiler.disable()\n\n        profiler.dump_stats(args.profile)\n        profiler.print_stats(sort=\'tottime\')\n\n        print(""======================================================================"")\n        pstats.Stats(profiler).strip_dirs().sort_stats(\'tottime\').print_stats(30)\n        print(""======================================================================"")\n        pstats.Stats(profiler).strip_dirs().sort_stats(\'cumtime\').print_stats(30)\n    else:\n        model_config.run_command(args.command, args.varargs)\n\n    model_config.quit_banner()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
vel/api/__init__.py,0,"b'from .callback import Callback\nfrom .info import BatchInfo, EpochInfo, TrainingInfo\nfrom .learner import Learner\nfrom .model import (\n    Model, BackboneModel, LinearBackboneModel, SupervisedModel, RnnLinearBackboneModel, RnnModel, RnnSupervisedModel\n)\nfrom .model_factory import ModelFactory\nfrom .optimizer import OptimizerFactory\nfrom .schedule import Schedule\nfrom .scheduler import SchedulerFactory\nfrom .source import Source, TrainingData, TextData\nfrom .storage import Storage\nfrom .train_phase import TrainPhase, EmptyTrainPhase\n\nfrom vel.internals.model_config import ModelConfig\n'"
vel/api/callback.py,0,"b'from .info import EpochInfo, BatchInfo, TrainingInfo\n\n\nclass Callback:\n    """"""\n    An abstract class that all callback classes extends from.\n    """"""\n\n    def on_initialization(self, training_info: TrainingInfo) -> None:\n        """"""\n        Runs for the first time a training process is started from scratch. Is guaranteed to be run only once\n        for the training process. Will be run before all other callbacks.\n        """"""\n        pass\n\n    def on_train_begin(self, training_info: TrainingInfo) -> None:\n        """"""\n        Beginning of a training process - is run every time a training process is started, even if it\'s restarted from\n        a checkpoint.\n        """"""\n        pass\n\n    def on_train_end(self, training_info: TrainingInfo) -> None:\n        """"""\n        Finalize training process. Runs each time at the end of a training process.\n        """"""\n        pass\n\n    def on_epoch_begin(self, epoch_info: EpochInfo) -> None:\n        """"""\n        Run for each epoch before an epoch will be trained\n        """"""\n        pass\n\n    def on_epoch_end(self, epoch_info: EpochInfo) -> None:\n        """"""\n        Runs for each epoch after an epoch is trained\n        """"""\n        pass\n\n    def on_batch_begin(self, batch_info: BatchInfo) -> None:\n        """"""\n        Runs for each batch before batch is evaluated\n        """"""\n        pass\n\n    def on_batch_end(self, batch_info: BatchInfo) -> None:\n        """"""\n        Runs for each batch after batch is evaluated\n        """"""\n        pass\n\n    def on_validation_batch_begin(self, batch_info: BatchInfo) -> None:\n        """"""\n        Supervised learning only - runs before validation batch\n        """"""\n        pass\n\n    def on_validation_batch_end(self, batch_info: BatchInfo) -> None:\n        """"""\n        Supervised learning only - runs after validation batch\n        """"""\n        pass\n\n    def write_state_dict(self, training_info: TrainingInfo, hidden_state_dict: dict) -> None:\n        """"""\n        Persist callback state to the state dictionary\n        """"""\n        pass\n\n    def load_state_dict(self, training_info: TrainingInfo, hidden_state_dict: dict) -> None:\n        """"""\n        Load callback state from the state dictionary\n        """"""\n        pass\n\n    # TODO(jerry): provide some info about current phase\n    # def on_phase_begin(self) -> None: pass\n    #\n    # def on_phase_end(self) -> None: pass\n'"
vel/api/info.py,2,"b'import collections.abc as abc\nimport numpy as np\nimport pandas as pd\nimport typing\n\nimport torch\n\nfrom vel.exceptions import VelException\n\n\nclass TrainingHistory:\n    """"""\n    Simple aggregator for the training history.\n\n    An output of training storing scalar metrics in a pandas dataframe.\n    """"""\n    def __init__(self):\n        self.data = []\n\n    def add(self, epoch_result):\n        """""" Add a datapoint to the history """"""\n        self.data.append(epoch_result)\n\n    def frame(self):\n        """""" Return history dataframe """"""\n        return pd.DataFrame(self.data).set_index(\'epoch_idx\')\n\n\nclass TrainingInfo(abc.MutableMapping):\n    """"""\n    Information that need to persist through the whole training process\n\n    Data dict is any extra information processes may want to store\n    """"""\n\n    def __init__(self, start_epoch_idx=0, run_name: typing.Optional[str]=None, metrics=None, callbacks=None):\n        self.data_dict = {}\n\n        self.start_epoch_idx = start_epoch_idx\n        self.metrics = metrics if metrics is not None else []\n        self.callbacks = callbacks if callbacks is not None else []\n        self.run_name = run_name\n        self.history = TrainingHistory()\n\n        self.optimizer_initial_state = None\n\n    def restore(self, hidden_state):\n        """""" Restore any state from checkpoint - currently not implemented but possible to do so in the future """"""\n        for callback in self.callbacks:\n            callback.load_state_dict(self, hidden_state)\n\n        if \'optimizer\' in hidden_state:\n            self.optimizer_initial_state = hidden_state[\'optimizer\']\n\n    def initialize(self):\n        """"""\n        Runs for the first time a training process is started from scratch. Is guaranteed to be run only once\n        for the training process. Will be run before all other callbacks.\n        """"""\n        for callback in self.callbacks:\n            callback.on_initialization(self)\n\n    def on_train_begin(self):\n        """"""\n        Beginning of a training process - is run every time a training process is started, even if it\'s restarted from\n        a checkpoint.\n        """"""\n        for callback in self.callbacks:\n            callback.on_train_begin(self)\n\n    def on_train_end(self):\n        """"""\n        Finalize training process. Runs each time at the end of a training process.\n        """"""\n        for callback in self.callbacks:\n            callback.on_train_end(self)\n\n    def __getitem__(self, item):\n        return self.data_dict[item]\n\n    def __setitem__(self, key, value):\n        self.data_dict[key] = value\n\n    def __delitem__(self, key):\n        del self.data_dict[key]\n\n    def __iter__(self):\n        return iter(self.data_dict)\n\n    def __len__(self):\n        return len(self.data_dict)\n\n    def __contains__(self, item):\n        return item in self.data_dict\n\n\nclass EpochResultAccumulator:\n    """""" Result of a single epoch of training -- accumulator """"""\n    def __init__(self, global_epoch_idx, metrics):\n        self.global_epoch_idx = global_epoch_idx\n        self.metrics = metrics\n\n        self.frozen_results = {}\n\n        self._reset_metrics()\n        self.metrics_by_name = {m.name: m for m in self.metrics}\n\n    @torch.no_grad()\n    def calculate(self, batch_info):\n        """""" Calculate metric values """"""\n        for m in self.metrics:\n            m.calculate(batch_info)\n\n    def _reset_metrics(self):\n        """""" Internal API : reset state of metrics """"""\n        for m in self.metrics:\n            m.reset()\n\n    def value(self):\n        """""" Return current value of the metrics """"""\n        return {m.name: m.value() for m in self.metrics}\n\n    def intermediate_value(self, metric):\n        """""" Return an intermediate (inter-epoch) value of a metric """"""\n        if \':\' in metric:\n            metric_name = metric.split(\':\')[-1]\n        else:\n            metric_name = metric\n\n        return self.metrics_by_name[metric_name].value()\n\n    def freeze_results(self, name=None):\n        new_results = self.value()\n\n        if name is None:\n            for key, value in new_results.items():\n                self.frozen_results[key] = value\n        else:\n            for key, value in new_results.items():\n                self.frozen_results[f\'{name}:{key}\'] = value\n\n        self._reset_metrics()\n\n    def result(self):\n        """""" Return the epoch result """"""\n        final_result = {\'epoch_idx\': self.global_epoch_idx}\n\n        for key, value in self.frozen_results.items():\n            final_result[key] = value\n\n        return final_result\n\n\nclass EpochInfo(abc.MutableMapping):\n    """"""\n    Information that need to persist through the single epoch.\n\n    Global epoch index - number of epoch from start of training until now\n    Local epoch index - number of epoch from start of current ""phase"" until now\n\n    Data dict is any extra information processes may want to store\n    """"""\n\n    def __init__(self, training_info: TrainingInfo, global_epoch_idx: int, batches_per_epoch: int,\n                 optimizer: torch.optim.Optimizer=None, local_epoch_idx: int = None, callbacks: list=None):\n        self.training_info = training_info\n        self.optimizer = optimizer\n        self.batches_per_epoch = batches_per_epoch\n\n        self.global_epoch_idx = global_epoch_idx\n\n        if local_epoch_idx is None:\n            self.local_epoch_idx = self.global_epoch_idx\n        else:\n            self.local_epoch_idx = local_epoch_idx\n\n        self.result_accumulator = EpochResultAccumulator(self.global_epoch_idx, self.training_info.metrics)\n        self._result = {}\n        self.data_dict = {}\n\n        if callbacks is None:\n            self.callbacks = self.training_info.callbacks\n        else:\n            self.callbacks = callbacks\n\n    def state_dict(self) -> dict:\n        """""" Calculate hidden state dictionary """"""\n        hidden_state = {}\n\n        if self.optimizer is not None:\n            hidden_state[\'optimizer\'] = self.optimizer.state_dict()\n\n        for callback in self.callbacks:\n            callback.write_state_dict(self.training_info, hidden_state)\n\n        return hidden_state\n\n    def on_epoch_begin(self) -> None:\n        """""" Initialize an epoch """"""\n        for callback in self.callbacks:\n            callback.on_epoch_begin(self)\n\n    def on_epoch_end(self):\n        """""" Finish epoch processing """"""\n        self.freeze_epoch_result()\n\n        for callback in self.callbacks:\n            callback.on_epoch_end(self)\n\n        self.training_info.history.add(self.result)\n\n    @property\n    def metrics(self):\n        """""" Just forward metrics from training_info """"""\n        return self.training_info.metrics\n\n    def freeze_epoch_result(self):\n        """""" Calculate epoch \'metrics\' result and store it in the internal variable """"""\n        self._result = self.result_accumulator.result()\n\n    @property\n    def result(self) -> dict:\n        """""" Result of the epoch """"""\n        if self._result is None:\n            raise VelException(""Result has not been frozen yet"")\n        else:\n            return self._result\n\n    def __repr__(self):\n        return f""EpochInfo(global_epoch_idx={self.global_epoch_idx}, local_epoch_idx={self.local_epoch_idx})""\n\n    def __getitem__(self, item):\n        return self.data_dict[item]\n\n    def __setitem__(self, key, value):\n        self.data_dict[key] = value\n\n    def __delitem__(self, key):\n        del self.data_dict[key]\n\n    def __iter__(self):\n        return iter(self.data_dict)\n\n    def __len__(self):\n        return len(self.data_dict)\n\n    def __contains__(self, item):\n        return item in self.data_dict\n\n\nclass BatchInfo(abc.MutableMapping):\n    """"""\n    Information about current batch.\n\n    Serves as a dictionary where all the information from the single run are accumulated\n    """"""\n    def __init__(self, epoch_info: EpochInfo, batch_number: int):\n        self.epoch_info = epoch_info\n        self.batch_number = batch_number\n        self.data_dict = {}\n\n    def on_batch_begin(self):\n        """""" Initialize batch processing """"""\n        for callback in self.callbacks:\n            callback.on_batch_begin(self)\n\n    def on_batch_end(self):\n        """""" Finalize batch processing """"""\n        for callback in self.callbacks:\n            callback.on_batch_end(self)\n\n        # Even with all the experience replay, we count the single rollout as a single batch\n        self.epoch_info.result_accumulator.calculate(self)\n\n    def on_validation_batch_begin(self):\n        """""" Initialize batch processing """"""\n        for callback in self.callbacks:\n            callback.on_validation_batch_begin(self)\n\n    def on_validation_batch_end(self):\n        """""" Finalize batch processing """"""\n        for callback in self.callbacks:\n            callback.on_validation_batch_end(self)\n\n        # Even with all the experience replay, we count the single rollout as a single batch\n        self.epoch_info.result_accumulator.calculate(self)\n\n    @property\n    def aggregate_batch_number(self):\n        return self.batch_number + self.epoch_info.batches_per_epoch * (self.epoch_info.global_epoch_idx - 1)\n\n    @property\n    def epoch_number(self):\n        return self.epoch_info.global_epoch_idx\n\n    @property\n    def batches_per_epoch(self):\n        return self.epoch_info.batches_per_epoch\n\n    @property\n    def local_epoch_number(self):\n        return self.epoch_info.local_epoch_idx\n\n    @property\n    def optimizer(self):\n        return self.epoch_info.optimizer\n\n    @property\n    def training_info(self):\n        return self.epoch_info.training_info\n\n    @property\n    def callbacks(self):\n        return self.epoch_info.callbacks\n\n    def aggregate_key(self, aggregate_key):\n        """""" Aggregate values from key and put them into the top-level dictionary """"""\n        aggregation = self.data_dict[aggregate_key]  # List of dictionaries of numpy arrays/scalars\n\n        # Aggregate sub batch data\n        data_dict_keys = {y for x in aggregation for y in x.keys()}\n\n        for key in data_dict_keys:\n            # Just average all the statistics from the loss function\n            stacked = np.stack([d[key] for d in aggregation], axis=0)\n            self.data_dict[key] = np.mean(stacked, axis=0)\n\n    def drop_key(self, key):\n        """""" Remove key from dictionary """"""\n        del self.data_dict[key]\n\n    def __getitem__(self, item):\n        return self.data_dict[item]\n\n    def __setitem__(self, key, value):\n        self.data_dict[key] = value\n\n    def __delitem__(self, key):\n        del self.data_dict[key]\n\n    def __iter__(self):\n        return iter(self.data_dict)\n\n    def __len__(self):\n        return len(self.data_dict)\n\n    def __contains__(self, item):\n        return item in self.data_dict\n\n    def __repr__(self):\n        return f""[BatchInfo epoch:{self.epoch_info.global_epoch_idx} batch:{self.batch_number}/{self.batches_per_epoch}]""\n'"
vel/api/learner.py,3,"b'import sys\nimport torch\nimport tqdm\nimport typing\n\nfrom .info import BatchInfo, EpochInfo, TrainingInfo\n\n\nclass Learner:\n    """""" Manages training process of a single model """"""\n    def __init__(self, device: torch.device, model, max_grad_norm: typing.Optional[float]=None):\n        self.device = device\n        self.model = model.to(device)\n        self.max_grad_norm = max_grad_norm\n\n    def metrics(self):\n        """""" Return metrics for given learner/model """"""\n        return self.model.metrics()\n\n    def summary(self):\n        """""" Print summary for given learner/model """"""\n        return self.model.summary()\n\n    def train(self):\n        """""" Set model in the training mode """"""\n        return self.model.train()\n\n    def eval(self):\n        """""" Set model in the evaluation mode """"""\n        return self.model.eval()\n\n    def number_of_parameters(self):\n        """""" Count model parameters """"""\n        return sum(p.numel() for p in self.model.parameters())\n\n    def initialize_training(self, training_info: TrainingInfo, model_state=None, hidden_state=None):\n        """""" Prepare for training """"""\n        if model_state is None:\n            self.model.reset_weights()\n        else:\n            self.model.load_state_dict(model_state)\n\n    def run_epoch(self, epoch_info: EpochInfo, source: \'vel.api.Source\'):\n        """""" Run full epoch of learning """"""\n        epoch_info.on_epoch_begin()\n\n        lr = epoch_info.optimizer.param_groups[-1][\'lr\']\n        print(""|-------- Epoch {:06} Lr={:.6f} ----------|"".format(epoch_info.global_epoch_idx, lr))\n\n        self.train_epoch(epoch_info, source)\n        epoch_info.result_accumulator.freeze_results(\'train\')\n\n        self.validation_epoch(epoch_info, source)\n        epoch_info.result_accumulator.freeze_results(\'val\')\n\n        epoch_info.on_epoch_end()\n\n    def train_epoch(self, epoch_info, source: \'vel.api.Source\', interactive=True):\n        """""" Run a single training epoch """"""\n        self.train()\n\n        if interactive:\n            iterator = tqdm.tqdm(source.train_loader(), desc=""Training"", unit=""iter"", file=sys.stdout)\n        else:\n            iterator = source.train_loader()\n\n        for batch_idx, (data, target) in enumerate(iterator):\n            batch_info = BatchInfo(epoch_info, batch_idx)\n\n            batch_info.on_batch_begin()\n            self.train_batch(batch_info, data, target)\n            batch_info.on_batch_end()\n\n            iterator.set_postfix(loss=epoch_info.result_accumulator.intermediate_value(\'loss\'))\n\n    def validation_epoch(self, epoch_info, source: \'vel.api.Source\'):\n        """""" Run a single evaluation epoch """"""\n        self.eval()\n\n        iterator = tqdm.tqdm(source.val_loader(), desc=""Validation"", unit=""iter"", file=sys.stdout)\n\n        with torch.no_grad():\n            for batch_idx, (data, target) in enumerate(iterator):\n                batch_info = BatchInfo(epoch_info, batch_idx)\n\n                batch_info.on_validation_batch_begin()\n                self.feed_batch(batch_info, data, target)\n                batch_info.on_validation_batch_end()\n\n    def feed_batch(self, batch_info, data, target):\n        """""" Run single batch of data """"""\n        data, target = data.to(self.device), target.to(self.device)\n        output, loss = self.model.loss(data, target)\n\n        # Store extra batch information for calculation of the statistics\n        batch_info[\'data\'] = data\n        batch_info[\'target\'] = target\n        batch_info[\'output\'] = output\n        batch_info[\'loss\'] = loss\n\n        return loss\n\n    def train_batch(self, batch_info, data, target):\n        """""" Train single batch of data """"""\n        batch_info.optimizer.zero_grad()\n        loss = self.feed_batch(batch_info, data, target)\n        loss.backward()\n\n        if self.max_grad_norm is not None:\n            batch_info[\'grad_norm\'] = torch.nn.utils.clip_grad_norm_(\n                filter(lambda p: p.requires_grad, self.model.parameters()),\n                max_norm=self.max_grad_norm\n            )\n\n        batch_info.optimizer.step()\n'"
vel/api/model.py,3,"b'import hashlib\nimport torch\nimport torch.nn as nn\n\nimport vel.util.module_util as mu\n\nfrom vel.metrics.loss_metric import Loss\nfrom vel.util.summary import summary\n\n\nclass Model(nn.Module):\n    """""" Class representing full neural network model """"""\n\n    def metrics(self) -> list:\n        """""" Set of metrics for this model """"""\n        return [Loss()]\n\n    def train(self, mode=True):\n        r""""""\n        Sets the module in training mode.\n\n        This has any effect only on certain modules. See documentations of\n        particular modules for details of their behaviors in training/evaluation\n        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n        etc.\n\n        Returns:\n            Module: self\n        """"""\n        super().train(mode)\n\n        if mode:\n            mu.apply_leaf(self, mu.set_train_mode)\n\n        return self\n\n    def summary(self, input_size=None, hashsummary=False):\n        """""" Print a model summary """"""\n\n        if input_size is None:\n            print(self)\n            print(""-"" * 120)\n            number = sum(p.numel() for p in self.model.parameters())\n            print(""Number of model parameters: {:,}"".format(number))\n            print(""-"" * 120)\n        else:\n            summary(self, input_size)\n\n        if hashsummary:\n            for idx, hashvalue in enumerate(self.hashsummary()):\n                print(f""{idx}: {hashvalue}"")\n\n    def hashsummary(self):\n        """""" Print a model summary - checksums of each layer parameters """"""\n        children = list(self.children())\n\n        result = []\n\n        for child in children:\n            result.extend(hashlib.sha256(x.detach().cpu().numpy().tobytes()).hexdigest() for x in child.parameters())\n\n        return result\n\n    def get_layer_groups(self):\n        """""" Return layers grouped """"""\n        return [self]\n\n    def reset_weights(self):\n        """""" Call proper initializers for the weights """"""\n        pass\n\n    @property\n    def is_recurrent(self) -> bool:\n        """""" If the network is recurrent and needs to be fed state as well as the observations """"""\n        return False\n\n\nclass RnnModel(Model):\n    """""" Class representing recurrent model """"""\n\n    @property\n    def is_recurrent(self) -> bool:\n        """""" If the network is recurrent and needs to be fed previous state """"""\n        return True\n\n    @property\n    def state_dim(self) -> int:\n        """""" Dimension of model state """"""\n        raise NotImplementedError\n\n    def zero_state(self, batch_size):\n        """""" Initial state of the network """"""\n        return torch.zeros(batch_size, self.state_dim)\n\n\nclass BackboneModel(Model):\n    """""" Model that serves as a backbone network to connect your heads to """"""\n\n\nclass RnnLinearBackboneModel(BackboneModel):\n    """"""\n    Model that serves as a backbone network to connect your heads to -\n    one that spits out a single-dimension output and is a recurrent neural network\n    """"""\n\n    @property\n    def is_recurrent(self) -> bool:\n        """""" If the network is recurrent and needs to be fed previous state """"""\n        return True\n\n    @property\n    def output_dim(self) -> int:\n        """""" Final dimension of model output """"""\n        raise NotImplementedError\n\n    @property\n    def state_dim(self) -> int:\n        """""" Dimension of model state """"""\n        raise NotImplementedError\n\n    def zero_state(self, batch_size):\n        """""" Initial state of the network """"""\n        return torch.zeros(batch_size, self.state_dim, dtype=torch.float32)\n\n\nclass LinearBackboneModel(BackboneModel):\n    """"""\n    Model that serves as a backbone network to connect your heads to - one that spits out a single-dimension output\n    """"""\n\n    @property\n    def output_dim(self) -> int:\n        """""" Final dimension of model output """"""\n        raise NotImplementedError\n\n\nclass SupervisedModel(Model):\n    """""" Model for a supervised learning problem """"""\n    def loss(self, x_data, y_true):\n        """""" Forward propagate network and return a value of loss function """"""\n        y_pred = self(x_data)\n        return y_pred, self.loss_value(x_data, y_true, y_pred)\n\n    def loss_value(self, x_data, y_true, y_pred):\n        """""" Calculate a value of loss function """"""\n        raise NotImplementedError\n\n\nclass RnnSupervisedModel(RnnModel):\n    """""" Model for a supervised learning problem """"""\n\n    def loss(self, x_data, y_true):\n        """""" Forward propagate network and return a value of loss function """"""\n        y_pred = self(x_data)\n        return y_pred, self.loss_value(x_data, y_true, y_pred)\n\n    def loss_value(self, x_data, y_true, y_pred):\n        """""" Calculate a value of loss function """"""\n        raise NotImplementedError\n'"
vel/api/model_factory.py,0,"b'from .model import Model\nfrom vel.internals.generic_factory import GenericFactory\n\n\nclass ModelFactory:\n    """""" Factory class for models """"""\n\n    def instantiate(self, **extra_args) -> Model:\n        raise NotImplementedError\n\n    @staticmethod\n    def generic(closure, **kwargs) -> \'ModelFactory\':\n        """""" Return a generic model factory """"""\n        # noinspection PyTypeChecker\n        return GenericFactory(closure, kwargs)\n'"
vel/api/optimizer.py,1,"b'from vel.api import Model\n\nfrom torch.optim import Optimizer\n\n\nclass OptimizerFactory:\n    """""" Base class for optimizer factories """"""\n\n    def instantiate(self, model: Model) -> Optimizer:\n        raise NotImplementedError\n'"
vel/api/schedule.py,0,"b'class Schedule:\n    """""" A schedule class encoding some kind of interpolation of a single value """"""\n    def value(self, progress_indicator):\n        """""" Value at given progress step """"""\n        raise NotImplementedError\n'"
vel/api/scheduler.py,0,"b'from .callback import Callback\n\n\nclass SchedulerFactory:\n    """""" Factory class for various schedulers """"""\n\n    def instantiate(self, optimizer, last_epoch=-1) -> Callback:\n        raise NotImplementedError\n'"
vel/api/source.py,1,"b'import torch.utils.data as data\n\n\nclass Source:\n    """""" Source of data for supervised learning algorithms """"""\n    def __init__(self):\n        pass\n\n    def train_loader(self):\n        """""" PyTorch loader of training data """"""\n        raise NotImplementedError\n\n    def val_loader(self):\n        """""" PyTorch loader of validation data """"""\n        raise NotImplementedError\n\n    def train_dataset(self):\n        """""" Return the training dataset """"""\n        raise NotImplementedError\n\n    def val_dataset(self):\n        """""" Return the validation dataset """"""\n        raise NotImplementedError\n\n    def train_iterations_per_epoch(self):\n        """""" Return number of iterations per epoch """"""\n        raise NotImplementedError\n\n    def val_iterations_per_epoch(self):\n        """""" Return number of iterations per epoch - validation """"""\n        raise NotImplementedError\n\n\nclass TextData(Source):\n    """""" An NLP torchtext data source """"""\n    def __init__(self, train_source, val_source, train_iterator, val_iterator, data_field, target_field):\n        super().__init__()\n\n        self.train_source = train_source\n        self.val_source = val_source\n        self.train_iterator = train_iterator\n        self.val_iterator = val_iterator\n        self.data_field = data_field\n        self.target_field = target_field\n\n    def train_loader(self):\n        """""" PyTorch loader of training data """"""\n        return self.train_iterator\n\n    def val_loader(self):\n        """""" PyTorch loader of validation data """"""\n        return self.val_iterator\n\n    def train_dataset(self):\n        """""" Return the training dataset """"""\n        return self.train_source\n\n    def val_dataset(self):\n        """""" Return the validation dataset """"""\n        return self.val_source\n\n    def train_iterations_per_epoch(self):\n        """""" Return number of iterations per epoch """"""\n        return len(self.train_iterator)\n\n    def val_iterations_per_epoch(self):\n        """""" Return number of iterations per epoch - validation """"""\n        return len(self.val_iterator)\n\n\nclass TrainingData(Source):\n    """""" Most common source of data combining a basic datasource and sampler """"""\n    def __init__(self, train_source, val_source, num_workers, batch_size, augmentations=None):\n        import vel.api.data as vel_data\n\n        super().__init__()\n\n        self.train_source = train_source\n        self.val_source = val_source\n\n        self.num_workers = num_workers\n        self.batch_size = batch_size\n\n        self.augmentations = augmentations\n\n        # Derived values\n        self.train_ds = vel_data.DataFlow(self.train_source, augmentations, tag=\'train\')\n        self.val_ds = vel_data.DataFlow(self.val_source, augmentations, tag=\'val\')\n\n        self._train_loader = data.DataLoader(\n            self.train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers\n        )\n\n        self._val_loader = data.DataLoader(\n            self.val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers\n        )\n\n    def train_loader(self):\n        """""" PyTorch loader of training data """"""\n        return self._train_loader\n\n    def val_loader(self):\n        """""" PyTorch loader of validation data """"""\n        return self._val_loader\n\n    def train_dataset(self):\n        """""" Return the training dataset """"""\n        return self.train_ds\n\n    def val_dataset(self):\n        """""" Return the validation dataset """"""\n        return self.val_ds\n\n    def train_iterations_per_epoch(self):\n        """""" Return number of iterations per epoch """"""\n        return len(self._train_loader)\n\n    def val_iterations_per_epoch(self):\n        """""" Return number of iterations per epoch - validation """"""\n        return len(self._val_loader)\n'"
vel/api/storage.py,0,"b'from vel.api import EpochInfo, TrainingInfo, Model\n\n\nclass Storage:\n    """""" Base class for Vel storage implementations """"""\n\n    def last_epoch_idx(self) -> int:\n        """""" Return last checkpointed epoch idx for given configuration. Returns 0 if no results have been stored """"""\n        raise NotImplementedError\n\n    def load(self, train_info: TrainingInfo) -> (dict, dict):\n        """"""\n        Resume learning process and return loaded hidden state dictionary\n        """"""\n        raise NotImplementedError\n\n    def reset(self, configuration: dict) -> None:\n        """"""\n        Whenever there was anything stored in the database or not, purge previous state and start\n        new training process from scratch.\n        """"""\n        raise NotImplementedError\n\n    def streaming_callbacks(self) -> list:\n        """""" Lift of callbacks for live streaming results """"""\n        return []\n\n    def get_metrics_frame(self):\n        """""" Get a frame of metrics from backend """"""\n        raise NotImplementedError\n\n    def checkpoint(self, epoch_info: EpochInfo, model: Model):\n        """""" When epoch is done, we persist the training state """"""\n        raise NotImplementedError\n'"
vel/api/train_phase.py,1,"b'from torch.optim import Optimizer\n\nfrom vel.api import TrainingInfo, EpochInfo, Learner, Model, Source\n\n\nclass TrainPhase:\n    """""" A single phase of training """"""\n\n    @property\n    def number_of_epochs(self) -> int:\n        """""" How many epochs does this phase take """"""\n        raise NotImplementedError\n\n    def set_up_phase(self, training_info: TrainingInfo, model: Model, source: Source) -> Optimizer:\n        """""" Prepare the phase for learning, returns phase optimizer """"""\n        pass\n\n    def restore(self, training_info: TrainingInfo, local_batch_idx: int, model: Model, hidden_state: dict):\n        """"""\n        Restore learning from intermediate state.\n        """"""\n        pass\n\n    def epoch_info(self, training_info: TrainingInfo, global_idx: int, local_idx: int) -> EpochInfo:\n        """""" Create Epoch info """"""\n        raise NotImplementedError\n\n    def execute_epoch(self, epoch_info: EpochInfo, learner: Learner):\n        """"""\n        Execute epoch training.\n        """"""\n        raise NotImplementedError\n\n    def tear_down_phase(self, training_info: TrainingInfo, model: Model):\n        """""" Clean up after phase is done """"""\n        pass\n\n    def state_dict(self):\n        """"""\n        State to save down\n        """"""\n        return {}\n\n    def banner(self) -> str:\n        """""" Return banner for the phase """"""\n        return f""|------> PHASE: {self.__class__.__name__} Length: {self.number_of_epochs}""\n\n\nclass EmptyTrainPhase(TrainPhase):\n    """""" A train phase that is a simple call, without any training """"""\n\n    @property\n    def number_of_epochs(self) -> int:\n        """""" How many epochs does this phase take """"""\n        return 0\n\n    def execute_epoch(self, epoch_info, learner):\n        """""" Prepare the phase for learning """"""\n        pass\n\n    def epoch_info(self, training_info: TrainingInfo, global_idx: int, local_idx: int) -> EpochInfo:\n        """""" Create Epoch info """"""\n        return EpochInfo(training_info, global_epoch_idx=global_idx, local_epoch_idx=local_idx, batches_per_epoch=0)\n'"
vel/augmentations/__init__.py,0,b''
vel/augmentations/center_crop.py,0,"b'""""""\nCode based on:\nhttps://github.com/fastai/fastai/blob/master/fastai/transforms.py\n""""""\n\nimport vel.api.data as data\n\n\nclass CenterCrop(data.Augmentation):\n    """""" A class that represents a Center Crop.\n\n    This transforms (optionally) transforms x,y at with the same parameters.\n    Arguments\n    ---------\n        sz: int\n            size of the crop.\n        tfm_y : TfmType\n            type of y transformation.\n    """"""\n    def __init__(self, size, mode=\'x\', tags=None):\n        super().__init__(mode, tags)\n\n        self.size = size\n\n    def __call__(self, x):\n        return data.center_crop(x, self.size)\n\n\ndef create(size, mode=\'x\', tags=None):\n    return CenterCrop(size, mode, tags)\n'"
vel/augmentations/normalize.py,0,"b'import numpy as np\n\nimport vel.api.data as data\n\n\nclass Normalize(data.Augmentation):\n    """""" Normalize input mean and standard deviation """"""\n\n    def __init__(self, mean, std, mode=\'x\', tags=None):\n        super().__init__(mode, tags)\n        self.mean = np.array(mean, dtype=np.float32)\n        self.std = np.array(std, dtype=np.float32)\n\n    def __call__(self, x_data):\n        return (x_data - self.mean) / self.std\n\n    def denormalize(self, x_data):\n        """""" Operation reverse to normalization """"""\n        return x_data * self.std + self.mean\n\n\ndef create(mean, std, mode=\'x\', tags=None):\n    """""" Vel factory function """"""\n    return Normalize(mean=mean, std=std, mode=mode, tags=tags)\n\n'"
vel/augmentations/random_crop.py,0,"b'""""""\nSlightly modified version of:\nhttps://github.com/pytorch/vision/blob/master/torchvision/transforms/transforms.py\n""""""\n\nimport numbers\nimport random\n\nimport vel.api.data as data\n\n\nclass RandomCrop(data.Augmentation):\n    """"""Crop the given PIL Image at a random location.\n\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n        padding (int or sequence, optional): Optional padding on each border\n            of the image. Default is 0, i.e no padding. If a sequence of length\n            4 is provided, it is used to pad left, top, right, bottom borders\n            respectively.\n        pad_if_needed (boolean): It will pad the image if smaller than the\n            desired size to avoid raising an exception.\n    """"""\n\n    def __init__(self, size, padding=0, padding_mode=\'constant\', pad_if_needed=False, mode=\'x\', tags=None):\n        super().__init__(mode, tags)\n\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n        self.padding = padding\n        self.padding_mode = padding_mode\n        self.padding_mode_cv = data.mode_to_cv2(self.padding_mode)\n        self.pad_if_needed = pad_if_needed\n\n    @staticmethod\n    def get_params(img, output_size):\n        """"""Get parameters for ``crop`` for a random crop.\n\n        Args:\n            img (PIL Image): Image to be cropped.\n            output_size (tuple): Expected output size of the crop.\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n        """"""\n        w, h, *_ = img.shape\n\n        th, tw = output_size\n        if w == tw and h == th:\n            return 0, 0, h, w\n\n        i = random.randint(0, h - th)\n        j = random.randint(0, w - tw)\n        return i, j, th, tw\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL Image): Image to be cropped.\n\n        Returns:\n            PIL Image: Cropped image.\n        """"""\n        if self.padding > 0:\n            img = data.pad(img, self.padding, mode=self.padding_mode_cv)\n\n        # pad the width if needed\n        if self.pad_if_needed and img.size[0] < self.size[1]:\n            img = data.pad(img, (int((1 + self.size[1] - img.size[0]) / 2), 0), mode=self.padding_mode_cv)\n\n        # pad the height if needed\n        if self.pad_if_needed and img.size[1] < self.size[0]:\n            img = data.pad(img, (0, int((1 + self.size[0] - img.size[1]) / 2)), mode=self.padding_mode_cv)\n\n        i, j, h, w = self.get_params(img, self.size)\n\n        return data.crop(img, j, i, w, h)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(size={0}, padding={1})\'.format(self.size, self.padding)\n\n\ndef create(width, height, padding=0, padding_mode=\'constant\', mode=\'x\', tags=None):\n    """""" Vel factory function """"""\n    return RandomCrop(size=(width, height), padding=padding, padding_mode=padding_mode, mode=mode, tags=tags)\n'"
vel/augmentations/random_horizontal_flip.py,0,"b'import random\nimport numpy as np\n\nimport vel.api.data as data\n\n\nclass RandomHorizontalFlip(data.Augmentation):\n    """""" Apply a horizontal flip randomly to input images """"""\n\n    def __init__(self, p=0.5, mode=\'x\', tags=None):\n        super().__init__(mode, tags)\n        self.p = p\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL Image): Image to be flipped.\n\n        Returns:\n            PIL Image: Randomly flipped image.\n        """"""\n        if random.random() < self.p:\n            return np.fliplr(img).copy()\n        return img\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(p={})\'.format(self.p)\n\n\ndef create(p=0.5):\n    return RandomHorizontalFlip(p)'"
vel/augmentations/random_lighting.py,0,"b'import random\n\nimport vel.api.data as data\n\n\nclass RandomLighting(data.Augmentation):\n    """""" Apply a horizontal flip randomly to input images """"""\n\n    def __init__(self, b, c, mode=\'x\', tags=None):\n        super().__init__(mode, tags)\n        self.b, self.c = b, c\n\n    def __call__(self, img):\n        """""" Adjust lighting """"""\n        rand_b = random.uniform(-self.b, self.b)\n        rand_c = random.uniform(-self.c, self.c)\n        rand_c = -1/(rand_c-1) if rand_c<0 else rand_c+1\n        return data.lighting(img, rand_b, rand_c)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(p={})\'.format(self.p)\n\n\ndef create(b, c, mode=\'x\', tags=None):\n    return RandomLighting(b, c, mode, tags)\n'"
vel/augmentations/random_rotate.py,0,"b'""""""\nCode based on:\nhttps://github.com/fastai/fastai/blob/master/fastai/transforms.py\n""""""\nimport cv2\nimport random\n\nimport vel.api.data as data\n\n\nclass RandomRotate(data.Augmentation):\n    """""" Rotate image randomly by an angle between (-deg, +deg) """"""\n    def __init__(self, deg, p=0.75, mode=\'x\', tags=None):\n        super().__init__(mode, tags)\n        self.deg = deg\n        self.p = p\n\n    def __call__(self, x_data):\n        if random.random() < self.p:\n            random_degree = random.uniform(-self.deg, self.deg)\n            return data.rotate_img(x_data, random_degree, mode=cv2.BORDER_REFLECT)\n        else:\n            # No, don\'t do it\n            return x_data\n\n\ndef create(deg, p=0.75, mode=\'x\', tags=None):\n    """""" Vel factory function """"""\n    return RandomRotate(deg, p, mode, tags)\n'"
vel/augmentations/random_scale.py,0,"b'""""""\nCode based on:\nhttps://github.com/fastai/fastai/blob/master/fastai/transforms.py\n""""""\nimport cv2\nimport collections.abc as abc\nimport random\n\nimport vel.api.data as data\n\n\nclass RandomScale(data.Augmentation):\n    """""" Scales the image so that the smallest axis is of \'size\' times a random number between 1.0 and max_zoom. """"""\n    def __init__(self, size, max_zoom, p=0.75, mode=\'x\', tags=None):\n        super().__init__(mode, tags)\n        self.size = size\n        self.max_zoom = max_zoom\n        self.p = p\n\n    def __call__(self, x_data):\n        if random.random() < self.p:\n            # Yes, do it\n            min_z = 1.\n            max_z = self.max_zoom\n            if isinstance(self.max_zoom, abc.Iterable):\n                min_z, max_z = self.max_zoom\n\n            mult = random.uniform(min_z, max_z)\n        else:\n            # No, don\'t do it\n            mult = 1.0\n\n        return data.scale_min(x_data, int(self.size * mult), cv2.INTER_AREA)\n\n\ndef create(size, max_zoom, p=0.75, mode=\'x\', tags=None):\n    return RandomScale(size, max_zoom, p, mode, tags)\n'"
vel/augmentations/scale_min_size.py,0,"b'""""""\nCode based on:\nhttps://github.com/fastai/fastai/blob/master/fastai/transforms.py\n""""""\nimport PIL.Image as Image\n\nimport vel.api.data as data\n\n\nclass ScaleMinSize(data.Augmentation):\n    """""" Scales the image so that the smallest axis is of \'size\'. """"""\n    def __init__(self, size, mode=\'x\', tags=None):\n        super().__init__(mode, tags)\n        self.size = size\n\n    def __call__(self, x_data):\n        return data.scale_min(x_data, self.size, Image.BILINEAR)\n\n\ndef create(size, mode=\'x\', tags=None):\n    """""" Vel factory function """"""\n    return ScaleMinSize(size, mode, tags)\n'"
vel/augmentations/to_array.py,0,"b'import numpy as np\n\nimport vel.api.data as data\n\n\nclass ToArray(data.Augmentation):\n    """""" Convert imate to an array of floats """"""\n    def __init__(self, mode=\'x\', tags=None):\n        super().__init__(mode, tags)\n\n    def __call__(self, x_data):\n        array = np.array(x_data)\n\n        if array.dtype == np.uint8:\n            return array.astype(np.float32) / 255.0\n        else:\n            return array\n\n\ndef create(mode=\'x\', tags=None):\n    return ToArray(mode, tags)\n'"
vel/augmentations/to_tensor.py,0,"b'import numpy as np\n\nimport torchvision.transforms.functional as F\n\nimport vel.api.data as data\n\n\nclass ToTensor(data.Augmentation):\n    """""" Convert image array to a tensor """"""\n    def __init__(self, mode=\'x\', tags=None):\n        super().__init__(mode, tags)\n\n    def __call__(self, datum):\n        if len(datum.shape) == 2:\n            # If the image has only one channel, it still needs to be specified\n            datum = datum.reshape(datum.shape[0], datum.shape[1], 1)\n\n        return F.to_tensor(datum)\n\n    def denormalize(self, datum):\n        return np.transpose(datum.numpy(), (1, 2, 0))\n\n\ndef create(mode=\'x\', tags=None):\n    """""" Vel factory function """"""\n    return ToTensor(mode, tags)\n'"
vel/callbacks/__init__.py,0,b''
vel/callbacks/time_tracker.py,0,"b'import time\n\nfrom vel.api import BatchInfo, TrainingInfo, Callback\n\n\nclass TimeTracker(Callback):\n    """""" Track training time - in seconds """"""\n    def __init__(self):\n        self.start_time = None\n\n    def on_initialization(self, training_info: TrainingInfo):\n        training_info[\'time\'] = 0.0\n\n    def on_train_begin(self, training_info: TrainingInfo):\n        self.start_time = time.time()\n\n    def on_batch_end(self, batch_info: BatchInfo):\n        current_time = time.time()\n        batch_time = current_time - self.start_time\n        self.start_time = current_time\n\n        batch_info[\'time\'] = batch_time\n        batch_info.training_info[\'time\'] += batch_info[\'time\']\n\n    def write_state_dict(self, training_info: TrainingInfo, hidden_state_dict: dict):\n        hidden_state_dict[\'time_tracker/time\'] = training_info[\'time\']\n\n    def load_state_dict(self, training_info: TrainingInfo, hidden_state_dict: dict):\n        training_info[\'time\'] = hidden_state_dict[\'time_tracker/time\']\n'"
vel/commands/__init__.py,0,b''
vel/commands/augvis_command.py,0,"b'import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom vel.api import Source\n\n\nclass AugmentationVisualizationCommand:\n    """""" Visualize augmentations """"""\n    def __init__(self, source: Source, samples, cases):\n        self.source = source\n        self.samples = samples\n        self.cases = cases\n\n    def run(self):\n        """""" Run the visualization """"""\n        dataset = self.source.train_dataset()\n        num_samples = len(dataset)\n\n        fig, ax = plt.subplots(self.cases, self.samples+1)\n\n        selected_sample = np.sort(np.random.choice(num_samples, self.cases, replace=False))\n\n        for i in range(self.cases):\n            raw_image, _ = dataset.get_raw(selected_sample[i])\n\n            ax[i, 0].imshow(raw_image)\n            ax[i, 0].set_title(""Original image"")\n\n            for j in range(self.samples):\n                augmented_image, _ = dataset[selected_sample[i]]\n                augmented_image = dataset.denormalize(augmented_image)\n                ax[i, j+1].imshow(augmented_image)\n\n        plt.show()\n\n\ndef create(source, samples, cases):\n    """""" Vel factory function """"""\n    return AugmentationVisualizationCommand(source, samples, cases)\n'"
vel/commands/lr_find_command.py,0,"b'""""""\nLearning rate finder.\nLoosely based on: https://github.com/fastai/fastai/blob/master/fastai/learner.py\n""""""\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tqdm\n\nimport vel.util.intepolate as interp\n\nfrom vel.api import Learner, TrainingInfo, EpochInfo, BatchInfo\nfrom vel.api.metrics.averaging_metric import AveragingNamedMetric\n\n\nclass LrFindCommand:\n    """"""Helps you find an optimal learning rate for a model.\n\n     It uses the technique developed in the 2015 paper\n     `Cyclical Learning Rates for Training Neural Networks`, where\n     we simply keep increasing the learning rate from a very small value,\n     until the loss starts decreasing.\n\n    Args:\n        start_lr (float/numpy array) : Passing in a numpy array allows you\n            to specify learning rates for a learner\'s layer_groups\n        end_lr (float) : The maximum learning rate to try.\n        wds (iterable/float)\n\n    Examples:\n        As training moves us closer to the optimal weights for a model,\n        the optimal learning rate will be smaller. We can take advantage of\n        that knowledge and provide lr_find() with a starting learning rate\n        1000x smaller than the model\'s current learning rate as such:\n\n        >> learn.lr_find(lr/1000)\n\n        >> lrs = np.array([ 1e-4, 1e-3, 1e-2 ])\n        >> learn.lr_find(lrs / 1000)\n\n    Notes:\n        lr_find() may finish before going through each batch of examples if\n        the loss decreases enough.\n\n    .. _Cyclical Learning Rates for Training Neural Networks:\n        http://arxiv.org/abs/1506.01186\n\n    """"""\n    def __init__(self, model_config, model, source, optimizer_factory, start_lr=1e-5, end_lr=10, num_it=100,\n                 interpolation=\'logscale\', freeze=False, stop_dv=True, divergence_threshold=4.0, metric=\'loss\'):\n        # Mandatory pieces\n        self.model = model\n        self.source = source\n        self.optimizer_factory = optimizer_factory\n        self.model_config = model_config\n        # Settings\n        self.start_lr = start_lr\n        self.end_lr = end_lr\n        self.num_it = num_it\n        self.interpolation = interpolation\n        self.freeze = freeze\n        self.stop_dv = stop_dv\n        self.divergence_threshold = divergence_threshold\n        self.metric = metric\n\n    def run(self):\n        """""" Run the command with supplied configuration """"""\n        device = self.model_config.torch_device()\n        learner = Learner(device, self.model.instantiate())\n\n        lr_schedule = interp.interpolate_series(self.start_lr, self.end_lr, self.num_it, self.interpolation)\n\n        if self.freeze:\n            learner.model.freeze()\n\n        # Optimizer shoudl be created after freeze\n        optimizer = self.optimizer_factory.instantiate(learner.model)\n\n        iterator = iter(self.source.train_loader())\n\n        # Metrics to track through this training\n        metrics = learner.metrics() + [AveragingNamedMetric(""lr"")]\n\n        learner.train()\n\n        best_value = None\n\n        training_info = TrainingInfo(start_epoch_idx=0, metrics=metrics)\n\n        # Treat it all as one epoch\n        epoch_info = EpochInfo(\n            training_info, global_epoch_idx=1, batches_per_epoch=1, optimizer=optimizer\n        )\n\n        for iteration_idx, lr in enumerate(tqdm.tqdm(lr_schedule)):\n            batch_info = BatchInfo(epoch_info, iteration_idx)\n\n            # First, set the learning rate, the same for each parameter group\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] = lr\n\n            try:\n                data, target = next(iterator)\n            except StopIteration:\n                iterator = iter(self.source.train_loader())\n                data, target = next(iterator)\n\n            learner.train_batch(batch_info, data, target)\n\n            batch_info[\'lr\'] = lr\n\n            # METRIC RECORDING PART\n            epoch_info.result_accumulator.calculate(batch_info)\n\n            current_value = epoch_info.result_accumulator.intermediate_value(self.metric)\n\n            final_metrics = {\'epoch_idx\': iteration_idx, self.metric: current_value, \'lr\': lr}\n\n            if best_value is None or current_value < best_value:\n                best_value = current_value\n\n            # Stop on divergence\n            if self.stop_dv and (np.isnan(current_value) or current_value > best_value * self.divergence_threshold):\n                break\n\n            training_info.history.add(final_metrics)\n\n        frame = training_info.history.frame()\n\n        fig, ax = plt.subplots(1, 2)\n\n        ax[0].plot(frame.index, frame.lr)\n        ax[0].set_title(""LR Schedule"")\n        ax[0].set_xlabel(""Num iterations"")\n        ax[0].set_ylabel(""Learning rate"")\n\n        if self.interpolation == \'logscale\':\n            ax[0].set_yscale(""log"", nonposy=\'clip\')\n\n        ax[1].plot(frame.lr, frame[self.metric], label=self.metric)\n        # ax[1].plot(frame.lr, frame[self.metric].ewm(com=20).mean(), label=self.metric + \' smooth\')\n        ax[1].set_title(self.metric)\n        ax[1].set_xlabel(""Learning rate"")\n        ax[1].set_ylabel(self.metric)\n        # ax[1].legend()\n\n        if self.interpolation == \'logscale\':\n            ax[1].set_xscale(""log"", nonposx=\'clip\')\n\n        plt.show()\n\n\ndef create(model_config, model, source, optimizer, start_lr=1e-5, end_lr=10, iterations=100, freeze=False,\n           interpolation=\'logscale\', stop_dv=True, divergence_threshold=4.0, metric=\'loss\'):\n    """""" Vel factory function """"""\n    return LrFindCommand(\n        model_config=model_config,\n        model=model,\n        source=source,\n        optimizer_factory=optimizer,\n        start_lr=start_lr,\n        end_lr=end_lr,\n        num_it=iterations,\n        interpolation=interpolation,\n        freeze=freeze,\n        stop_dv=stop_dv,\n        divergence_threshold=divergence_threshold,\n        metric=metric\n    )\n'"
vel/commands/phase_train_command.py,0,"b'import torch\nimport numpy as np\nimport bisect\nimport typing\n\nfrom vel.api import Learner, TrainingInfo, ModelConfig, TrainPhase\n\n\nclass PhaseTrainCommand:\n    """""" Training  command - learn according to a set of phases """"""\n\n    def __init__(self, model_config: ModelConfig, model_factory, source, storage, phases: typing.List[TrainPhase],\n                 callbacks=None, restart=True):\n        self.model_config = model_config\n        self.model_factory = model_factory\n        self.source = source\n        self.storage = storage\n        self.phases = phases\n        self.ladder = self._build_phase_ladder(phases)\n        self.full_number_of_epochs = sum(p.number_of_epochs for p in phases)\n        self.callbacks = callbacks if callbacks is not None else []\n        self.restart = restart\n\n    @staticmethod\n    def _build_phase_ladder(phases):\n        """""" Build a ladder of learning phases """"""\n        return [0] + np.cumsum([p.number_of_epochs for p in phases]).tolist()[:-1]\n\n    def _select_phase_left_bound(self, epoch_number):\n        """"""\n        Return number of current phase.\n        Return index of first phase not done after all up to epoch_number were done.\n        """"""\n        idx = bisect.bisect_left(self.ladder, epoch_number)\n\n        if idx >= len(self.ladder):\n            return len(self.ladder) - 1\n        elif self.ladder[idx] > epoch_number:\n            return idx - 1\n        else:\n            return idx\n\n    def _select_phase_right_bound(self, epoch_number):\n        """"""\n        Return number of current phase.\n        Return index of first phase not done after all up to epoch_number were done.\n        """"""\n        return bisect.bisect_right(self.ladder, epoch_number) - 1\n\n    def run(self):\n        """""" Run the command with supplied configuration """"""\n        device = self.model_config.torch_device()\n        learner = Learner(device, self.model_factory.instantiate())\n\n        # All callbacks useful for learning\n        callbacks = self.gather_callbacks()\n\n        # Metrics to track through this training\n        metrics = learner.metrics()\n\n        # Check if training was already started and potentially continue where we left off\n        training_info, hidden_state = self.resume_training(learner, callbacks, metrics)\n\n        # Prepare current training phase\n        current_phase_idx = self._select_phase_left_bound(training_info.start_epoch_idx)\n        current_phase = self.phases[current_phase_idx]\n        local_idx = training_info.start_epoch_idx - self.ladder[current_phase_idx]\n\n        current_phase.set_up_phase(training_info, learner.model, self.source)\n        print(current_phase.banner())\n\n        if training_info.start_epoch_idx > 0:\n            current_phase.restore(training_info, local_idx, learner.model, hidden_state)\n\n        training_info.on_train_begin()\n\n        for global_epoch_idx in range(training_info.start_epoch_idx + 1, self.full_number_of_epochs + 1):\n            iteration_phase_idx = self._select_phase_right_bound(global_epoch_idx-1)\n            local_idx = global_epoch_idx - self.ladder[iteration_phase_idx]\n\n            # Phase preparations\n            while current_phase_idx != iteration_phase_idx:\n                current_phase.tear_down_phase(training_info, learner.model)\n\n                current_phase_idx += 1\n                current_phase = self.phases[current_phase_idx]\n\n                current_phase.set_up_phase(training_info, learner.model, self.source)\n                print(current_phase.banner())\n\n            # Create epoch info\n            epoch_info = current_phase.epoch_info(training_info, global_epoch_idx, local_idx)\n\n            # Execute learning\n            current_phase.execute_epoch(epoch_info, learner)\n\n            # Epoch checkpoint\n            self.storage.checkpoint(epoch_info, learner.model)\n\n        # Tear down the last phase\n        if current_phase is not None:\n            current_phase.tear_down_phase(training_info, learner.model)\n\n        training_info.on_train_end()\n\n        return training_info\n\n    def gather_callbacks(self) -> list:\n        """""" Gather all the callbacks to be used in this training run """"""\n        callbacks = []\n\n        callbacks.extend(self.callbacks)\n        callbacks.extend(self.storage.streaming_callbacks())\n\n        return callbacks\n\n    def resume_training(self, learner, callbacks, metrics) -> (TrainingInfo, dict):\n        """""" Possibly resume training from a saved state from the storage """"""\n        if self.model_config.continue_training:\n            start_epoch = self.storage.last_epoch_idx()\n        else:\n            start_epoch = 0\n\n        training_info = TrainingInfo(\n            start_epoch_idx=start_epoch,\n            run_name=self.model_config.run_name,\n            metrics=metrics,\n            callbacks=callbacks\n        )\n\n        if start_epoch == 0:\n            self.storage.reset(self.model_config.render_configuration())\n            training_info.initialize()\n            learner.initialize_training(training_info)\n            hidden_state = None\n        else:\n            model_state, hidden_state = self.storage.load(training_info)\n            learner.initialize_training(training_info, model_state, hidden_state)\n\n        return training_info, hidden_state\n\n\ndef create(model_config, model, source, storage, phases, callbacks=None, restart=True):\n    """""" Vel factory function """"""\n    return PhaseTrainCommand(\n        model_config=model_config,\n        model_factory=model,\n        source=source,\n        storage=storage,\n        phases=phases,\n        callbacks=callbacks,\n        restart=restart\n    )\n'"
vel/commands/summary_command.py,0,"b'from vel.api import Source\n\n\nclass ModelSummary:\n    """""" Just print model summary """"""\n    def __init__(self, model, source: Source):\n        self.model = model\n        self.source = source\n\n    def run(self, *args):\n        """""" Print model summary """"""\n        if self.source is None:\n            self.model.summary()\n        else:\n            x_data, y_data = next(iter(self.source.train_loader()))\n            self.model.summary(input_size=x_data.shape[1:])\n\n\ndef create(model, source=None):\n    """""" Vel factory function """"""\n    return ModelSummary(model, source)\n'"
vel/commands/train_command.py,0,"b'import typing\n\nimport vel.api as api\n\nfrom vel.callbacks.time_tracker import TimeTracker\n\n\nclass SimpleTrainCommand:\n    """""" Very simple training command - just run the supplied generators """"""\n\n    def __init__(self, epochs: int, model_config: api.ModelConfig, model_factory: api.ModelFactory,\n                 optimizer_factory: api.OptimizerFactory, scheduler_factory: typing.Optional[api.SchedulerFactory],\n                 source: api.Source, storage: api.Storage, callbacks: typing.Optional[typing.List[api.Callback]],\n                 max_grad_norm: typing.Optional[float]):\n        self.epochs = epochs\n        self.model_config = model_config\n        self.model_factory = model_factory\n\n        self.optimizer_factory = optimizer_factory\n        self.scheduler_factory = scheduler_factory\n\n        self.source = source\n        self.storage = storage\n        self.callbacks = callbacks if callbacks is not None else []\n        self.max_grad_norm = max_grad_norm\n\n    def run(self):\n        """""" Run the command with supplied configuration """"""\n        device = self.model_config.torch_device()\n\n        learner = api.Learner(device, self.model_factory.instantiate(), self.max_grad_norm)\n        optimizer = self.optimizer_factory.instantiate(learner.model)\n\n        # All callbacks used for learning\n        callbacks = self.gather_callbacks(optimizer)\n\n        # Metrics to track through this training\n        metrics = learner.metrics()\n\n        # Check if training was already started and potentially continue where we left off\n        training_info = self.resume_training(learner, callbacks, metrics)\n\n        training_info.on_train_begin()\n\n        if training_info.optimizer_initial_state:\n            optimizer.load_state_dict(training_info.optimizer_initial_state)\n\n        for global_epoch_idx in range(training_info.start_epoch_idx + 1, self.epochs + 1):\n            epoch_info = api.EpochInfo(\n                training_info=training_info,\n                global_epoch_idx=global_epoch_idx,\n                batches_per_epoch=self.source.train_iterations_per_epoch(),\n                optimizer=optimizer\n            )\n\n            # Execute learning\n            learner.run_epoch(epoch_info, self.source)\n\n            self.storage.checkpoint(epoch_info, learner.model)\n\n        training_info.on_train_end()\n\n        return training_info\n\n    def gather_callbacks(self, optimizer) -> list:\n        """""" Gather all the callbacks to be used in this training run """"""\n        callbacks = [TimeTracker()]\n\n        if self.scheduler_factory is not None:\n            callbacks.append(self.scheduler_factory.instantiate(optimizer))\n\n        callbacks.extend(self.callbacks)\n        callbacks.extend(self.storage.streaming_callbacks())\n\n        return callbacks\n\n    def resume_training(self, learner, callbacks, metrics) -> api.TrainingInfo:\n        """""" Possibly resume training from a saved state from the storage """"""\n        if self.model_config.continue_training:\n            start_epoch = self.storage.last_epoch_idx()\n        else:\n            start_epoch = 0\n\n        training_info = api.TrainingInfo(\n            start_epoch_idx=start_epoch,\n            run_name=self.model_config.run_name,\n            metrics=metrics,\n            callbacks=callbacks\n        )\n\n        if start_epoch == 0:\n            self.storage.reset(self.model_config.render_configuration())\n            training_info.initialize()\n            learner.initialize_training(training_info)\n        else:\n            model_state, hidden_state = self.storage.load(training_info)\n            learner.initialize_training(training_info, model_state, hidden_state)\n\n        return training_info\n\n\ndef create(model_config, epochs, optimizer, model, source, storage, scheduler=None, callbacks=None, max_grad_norm=None):\n    """""" Vel factory function """"""\n    return SimpleTrainCommand(\n        epochs=epochs,\n        model_config=model_config,\n        model_factory=model,\n        optimizer_factory=optimizer,\n        scheduler_factory=scheduler,\n        source=source,\n        storage=storage,\n        callbacks=callbacks,\n        max_grad_norm=max_grad_norm\n    )\n'"
vel/commands/vis_store_command.py,0,"b'import visdom\n\n\nfrom vel.util.visdom import visdom_push_metrics, VisdomSettings\n\n\nclass VisdomCommand:\n    """""" Send metrics from database into VISDOM """"""\n    def __init__(self, model_config, storage, visdom_settings: VisdomSettings):\n        self.model_config = model_config\n        self.storage = storage\n        self.vis = visdom.Visdom(\n            server=visdom_settings.server,\n            endpoint=visdom_settings.endpoint,\n            port=visdom_settings.port,\n            env=self.model_config.run_name.replace(\'/\', \'_\')\n        )\n\n    def run(self):\n        metrics = self.storage.get_metrics_frame().drop(\'run_name\', axis=1)\n        visdom_push_metrics(self.vis, metrics)\n\n\ndef create(model_config, storage, visdom_settings):\n    """""" Vel factory function """"""\n    return VisdomCommand(model_config, storage, VisdomSettings(**visdom_settings))\n'"
vel/internals/__init__.py,0,b''
vel/internals/context.py,0,"b'import threading\nimport contextlib\n\n\nclass Context(threading.local):\n    """""" Context object maintaining argument stack """"""\n    def __init__(self):\n        self.stack = []\n\n    def push(self, **kwargs):\n        self.stack.append(kwargs)\n\n    def pop(self):\n        self.stack.pop()\n\n    def peek(self, name, default_value=None):\n        index = len(self.stack) - 1\n\n        while index >= 0:\n            if name in self.stack[index]:\n                return self.stack[index][name]\n\n        return default_value\n\n\n_THREAD_LOCAL_CONTEXT = Context()\n\n\n@contextlib.contextmanager\ndef with_context(**kwargs):\n    """""" Add given variables to the context """"""\n    _THREAD_LOCAL_CONTEXT.push(**kwargs)\n\n    try:\n        yield\n    finally:\n        _THREAD_LOCAL_CONTEXT.pop()\n\n\ndef push_context(**kwargs):\n    """""" Push custom context """"""\n    _THREAD_LOCAL_CONTEXT.push(**kwargs)\n\n\ndef pop_context():\n    """""" Pop context from the stack """"""\n    return _THREAD_LOCAL_CONTEXT.pop()\n\n\ndef get_context(name, default_value=None):\n    """""" Get context value (possibly default value) """"""\n    return _THREAD_LOCAL_CONTEXT.peek(name, default_value)\n'"
vel/internals/generic_factory.py,0,"b'class GenericFactory:\n    """""" Essentially a non-evaluated lambda function """"""\n    def __init__(self, function, arguments=None):\n        self.function = function\n        self.arguments = arguments if arguments is not None else {}\n\n    def instantiate(self, **kwargs):\n        """""" Create an instance """"""\n        # Unpack both arguments\n        return self.function(**{**kwargs, **self.arguments})\n'"
vel/internals/model_config.py,3,"b'import datetime as dtm\nimport os.path\n\nfrom vel.exceptions import VelInitializationException\nfrom vel.internals.parser import Parser\nfrom vel.internals.provider import Provider\n\n\nclass ModelConfig:\n    """"""\n    Read from YAML configuration of a model, specifying all details of the run.\n    Is a frontend for the provider, resolving all dependency-injection requests.\n    """"""\n\n    PROJECT_FILE_NAME = \'.velproject.yaml\'\n\n    @staticmethod\n    def find_project_directory(start_path) -> str:\n        """""" Locate top-level project directory  """"""\n        start_path = os.path.realpath(start_path)\n        possible_name = os.path.join(start_path, ModelConfig.PROJECT_FILE_NAME)\n\n        if os.path.exists(possible_name):\n            return start_path\n        else:\n            up_path = os.path.realpath(os.path.join(start_path, \'..\'))\n            if os.path.realpath(start_path) == up_path:\n                raise RuntimeError(f""Couldn\'t find project file starting from {start_path}"")\n            else:\n                return ModelConfig.find_project_directory(up_path)\n\n    @classmethod\n    def from_file(cls, filename: str, run_number: int, continue_training: bool = False, seed: int = None,\n                  device: str = \'cuda\', params=None):\n        """""" Create model config from file """"""\n        with open(filename, \'r\') as fp:\n            model_config_contents = Parser.parse(fp)\n\n        project_config_path = ModelConfig.find_project_directory(os.path.dirname(os.path.abspath(filename)))\n\n        with open(os.path.join(project_config_path, cls.PROJECT_FILE_NAME), \'r\') as fp:\n            project_config_contents = Parser.parse(fp)\n\n        aggregate_dictionary = {\n            **project_config_contents,\n            **model_config_contents\n        }\n\n        return ModelConfig(\n            filename=filename,\n            configuration=aggregate_dictionary,\n            run_number=run_number,\n            project_dir=project_config_path,\n            continue_training=continue_training,\n            seed=seed,\n            device=device,\n            parameters=params\n        )\n\n    @classmethod\n    def from_memory(cls, model_data: dict, run_number: int, project_dir: str,\n                    continue_training=False, seed: int = None, device: str = \'cuda\', params=None):\n        """""" Create model config from supplied data """"""\n        return ModelConfig(\n            filename=""[memory]"",\n            configuration=model_data,\n            run_number=run_number,\n            project_dir=project_dir,\n            continue_training=continue_training,\n            seed=seed,\n            device=device,\n            parameters=params\n        )\n\n    def __init__(self, filename: str, configuration: dict, run_number: int, project_dir: str,\n                 continue_training=False, seed: int = None, device: str = \'cuda\', parameters=None):\n        self.filename = filename\n        self.device = device\n        self.continue_training = continue_training\n        self.run_number = run_number\n        self.seed = seed if seed is not None else (dtm.date.today().year + self.run_number)\n\n        self.contents = configuration\n        self.project_dir = os.path.normpath(project_dir)\n\n        self.command_descriptors = self.contents.get(\'commands\', [])\n\n        # This one is special and needs to get removed\n        if \'commands\' in self.contents:\n            del self.contents[\'commands\']\n\n        self.provider = Provider(self._prepare_environment(), {\'model_config\': self}, parameters=parameters)\n        self._model_name = self.provider.get(""name"")\n\n    def _prepare_environment(self) -> dict:\n        """""" Return full environment for dependency injection """"""\n        return {**self.contents, \'run_number\': self.run_number}\n\n    def render_configuration(self) -> dict:\n        """""" Return a nice and picklable run configuration """"""\n        return self.provider.render_configuration()\n\n    ####################################################################################################################\n    # COMMAND UTILITIES\n    def get_command(self, command_name):\n        """""" Return object for given command """"""\n        return self.provider.instantiate_from_data(self.command_descriptors[command_name])\n\n    def run_command(self, command_name, varargs):\n        """""" Instantiate model class """"""\n        command_descriptor = self.get_command(command_name)\n        return command_descriptor.run(*varargs)\n\n    ####################################################################################################################\n    # MODEL DIRECTORIES\n    def checkpoint_dir(self, *args) -> str:\n        """""" Return checkpoint directory for this model """"""\n        return self.output_dir(\'checkpoints\', self.run_name, *args)\n\n    def data_dir(self, *args) -> str:\n        """""" Return data directory for given dataset """"""\n        return self.project_data_dir(*args)\n\n    def openai_dir(self) -> str:\n        """""" Return directory for openai output files for this model """"""\n        return self.output_dir(\'openai\', self.run_name)\n\n    def project_data_dir(self, *args) -> str:\n        """""" Directory where to store data """"""\n        return os.path.normpath(os.path.join(self.project_dir, \'data\', *args))\n\n    def output_dir(self, *args) -> str:\n        """""" Directory where to store output """"""\n        return os.path.join(self.project_dir, \'output\', *args)\n\n    def project_top_dir(self, *args) -> str:\n        """""" Project top-level directory """"""\n        return os.path.join(self.project_dir, *args)\n\n    ####################################################################################################################\n    # NAME UTILITIES\n    @property\n    def run_name(self) -> str:\n        """""" Return name of the run """"""\n        return ""{}/{}"".format(self._model_name, self.run_number)\n\n    @property\n    def name(self) -> str:\n        """""" Return name of the model """"""\n        return self._model_name\n\n    ####################################################################################################################\n    # MISC GETTERS\n    def torch_device(self):\n        """""" Return torch device object """"""\n        import torch\n        return torch.device(self.device)\n\n    ####################################################################################################################\n    # PROVIDER API\n    def provide(self, name):\n        """""" Return a dependency-injected instance """"""\n        return self.provider.instantiate_by_name(name)\n\n    def provide_with_default(self, name, default=None):\n        """""" Return a dependency-injected instance """"""\n        return self.provider.instantiate_by_name_with_default(name, default_value=default)\n\n    ####################################################################################################################\n    # BANNERS - Maybe shouldn\'t be here, but they are for now\n    def banner(self, command_name) -> None:\n        """""" Print a banner for running the system """"""\n        import torch\n        device = self.torch_device()\n\n        print(""="" * 80)\n        print(f""Pytorch version: {torch.__version__} cuda version {torch.version.cuda} cudnn version {torch.backends.cudnn.version()}"")\n        print(""Running model {}, run {} -- command {} -- device {}"".format(self._model_name, self.run_number, command_name, self.device))\n        if device.type == \'cuda\':\n            device_idx = 0 if device.index is None else device.index\n            print(f""CUDA Device name {torch.cuda.get_device_name(device_idx)}"")\n        print(dtm.datetime.now().strftime(""%Y/%m/%d - %H:%M:%S""))\n        print(""="" * 80)\n\n    def quit_banner(self) -> None:\n        """""" Print a banner for running the system """"""\n        print(""="" * 80)\n        print(""Done."")\n        print(dtm.datetime.now().strftime(""%Y/%m/%d - %H:%M:%S""))\n        print(""="" * 80)\n\n    ####################################################################################################################\n    # Small UI utils\n    def __repr__(self):\n        return f""<ModelConfig at {self.filename}>""\n'"
vel/internals/parser.py,0,"b'import os\nimport yaml\n\nfrom vel.exceptions import VelException\n\n\nclass Dummy:\n    """""" Dummy instance placeholder """"""\n    pass\n\n\nDUMMY_VALUE = Dummy()\n\n\nclass Variable:\n    """""" Project configuration variable to be popupated from command line """"""\n    @classmethod\n    def parameter_constructor(cls, loader, node):\n        """""" Construct variable instance from yaml node """"""\n        value = loader.construct_scalar(node)\n\n        if isinstance(value, str):\n            if \'=\' in value:\n                (varname, varvalue) = Parser.parse_equality(value)\n                return cls(varname, varvalue)\n            else:\n                return cls(value)\n        else:\n            return cls(value)\n\n    def __init__(self, name, default_value=DUMMY_VALUE):\n        self.name = name\n        self.default_value = default_value\n\n    def __repr__(self):\n        return f""{self.__class__.__name__}(name={self.name})""\n\n    def resolve(self, parameters):\n        """""" Resolve given variable """"""\n        raise NotImplementedError\n\n\nclass Parameter(Variable):\n    """""" Parameter that gets propagated from command line options """"""\n    def resolve(self, parameters):\n        """""" Resolve given variable """"""\n        if self.default_value == DUMMY_VALUE:\n            if self.name in parameters:\n                return parameters[self.name]\n            else:\n                raise VelException(f""Undefined parameter: {self.name}"")\n        else:\n            return parameters.get(self.name, self.default_value)\n\n\nclass EnvironmentVariable(Variable):\n    """""" Parameter that gets propagated from environment """"""\n    def resolve(self, _):\n        """""" Resolve given variable """"""\n        if self.default_value == DUMMY_VALUE:\n            if self.name in os.environ:\n                return os.environ[self.name]\n            else:\n                raise VelException(f""Undefined environment variable: {self.name}"")\n        else:\n            return os.environ.get(self.name, self.default_value)\n\n\nclass Parser:\n    """""" Parse configuration values """"""\n    IS_LOADED = False\n\n    @classmethod\n    def register(cls):\n        """""" Register variable handling in YAML """"""\n        if not cls.IS_LOADED:\n            cls.IS_LOADED = True\n\n            yaml.add_constructor(\'!param\', Parameter.parameter_constructor, Loader=yaml.SafeLoader)\n            yaml.add_constructor(\'!env\', EnvironmentVariable.parameter_constructor, Loader=yaml.SafeLoader)\n\n    @classmethod\n    def parse(cls, stream):\n        """""" Parse the stream into a Python object """"""\n        cls.register()\n        return yaml.safe_load(stream)\n\n    @classmethod\n    def parse_equality(cls, equality_string):\n        """""" Parse some simple equality statements """"""\n        cls.register()\n        assert \'=\' in equality_string, ""There must be an \'=\' sign in the equality""\n        [left_side, right_side] = equality_string.split(\'=\', 1)\n\n        left_side_value = yaml.safe_load(left_side.strip())\n        right_side_value = yaml.safe_load(right_side.strip())\n\n        assert isinstance(left_side_value, str), ""Left side of equality must be a string""\n\n        return left_side_value, right_side_value\n'"
vel/internals/provider.py,0,"b'import importlib\nimport inspect\n\nfrom vel.internals.parser import Variable\nfrom vel.internals.generic_factory import GenericFactory\n\n\nclass Provider:\n    """""" Dependency injection resolver for the configuration file """"""\n    def __init__(self, environment, instances=None, parameters=None):\n        self.environment = environment\n\n        self.parameters = parameters if parameters is not None else {}\n\n        self.instances = {\n            **(instances if instances is not None else {}),\n            \'vel_provider\': self\n        }\n\n    def inject(self, name, value):\n        """""" Inject an object into the provider """"""\n        self.instances[name] = value\n\n    def resolve_parameters(self, func, extra_env=None):\n        """""" Resolve parameter dictionary for the supplied function """"""\n        parameter_list = [\n            (k, v.default == inspect.Parameter.empty) for k, v in inspect.signature(func).parameters.items()\n        ]\n        extra_env = extra_env if extra_env is not None else {}\n        kwargs = {}\n\n        for parameter_name, is_required in parameter_list:\n            # extra_env is a \'local\' object data defined in-place\n            if parameter_name in extra_env:\n                kwargs[parameter_name] = self.instantiate_from_data(extra_env[parameter_name])\n                continue\n\n            if parameter_name in self.instances:\n                kwargs[parameter_name] = self.instances[parameter_name]\n                continue\n\n            if parameter_name in self.environment:\n                kwargs[parameter_name] = self.instantiate_by_name(parameter_name)\n                continue\n\n            if is_required:\n                funcname = f""{inspect.getmodule(func).__name__}.{func.__name__}""\n                raise RuntimeError(""Required argument \'{}\' cannot be resolved for function \'{}\'"".format(\n                    parameter_name, funcname\n                ))\n\n        return kwargs\n\n    def resolve_and_call(self, func, extra_env=None):\n        """""" Resolve function arguments and call them, possibily filling from the environment """"""\n        kwargs = self.resolve_parameters(func, extra_env=extra_env)\n        return func(**kwargs)\n\n    def instantiate_from_data(self, object_data):\n        """""" Instantiate object from the supplied data, additional args may come from the environment """"""\n        if isinstance(object_data, dict) and \'name\' in object_data:\n            name = object_data[\'name\']\n            module = importlib.import_module(name)\n            return self.resolve_and_call(module.create, extra_env=object_data)\n        if isinstance(object_data, dict) and \'factory\' in object_data:\n            factory = object_data[\'factory\']\n            module = importlib.import_module(factory)\n            params = self.resolve_parameters(module.create, extra_env=object_data)\n            return GenericFactory(module.create, params)\n        elif isinstance(object_data, dict):\n            return {k: self.instantiate_from_data(v) for k, v in object_data.items()}\n        elif isinstance(object_data, list):\n            return [self.instantiate_from_data(x) for x in object_data]\n        elif isinstance(object_data, Variable):\n            return object_data.resolve(self.parameters)\n        else:\n            return object_data\n\n    def render_configuration(self, configuration=None):\n        """""" Render variables in configuration object but don\'t instantiate anything """"""\n        if configuration is None:\n            configuration = self.environment\n\n        if isinstance(configuration, dict):\n            return {k: self.render_configuration(v) for k, v in configuration.items()}\n        elif isinstance(configuration, list):\n            return [self.render_configuration(x) for x in configuration]\n        elif isinstance(configuration, Variable):\n            return configuration.resolve(self.parameters)\n        else:\n            return configuration\n\n    def has_name(self, object_name):\n        """""" Check if given name is available in the provider """"""\n        return object_name in self.instances or object_name in self.environment\n\n    def instantiate_by_name(self, object_name):\n        """""" Instantiate object from the environment, possibly giving some extra arguments """"""\n        if object_name not in self.instances:\n            instance = self.instantiate_from_data(self.environment[object_name])\n\n            self.instances[object_name] = instance\n            return instance\n        else:\n            return self.instances[object_name]\n\n    def instantiate_by_name_with_default(self, object_name, default_value=None):\n        """""" Instantiate object from the environment, possibly giving some extra arguments """"""\n        if object_name not in self.instances:\n            if object_name not in self.environment:\n                return default_value\n            else:\n                instance = self.instantiate_from_data(self.environment[object_name])\n\n                self.instances[object_name] = instance\n                return instance\n        else:\n            return self.instances[object_name]\n\n    def get(self, name):\n        """""" Get object from given provider """"""\n        return self.instantiate_by_name(name)\n'"
vel/math/__init__.py,0,b''
vel/math/functions.py,1,"b'import torch\n\n\ndef explained_variance(returns, values):\n    """""" Calculate how much variance in returns do the values explain """"""\n    exp_var = 1 - torch.var(returns - values) / torch.var(returns)\n    return exp_var.item()\n'"
vel/math/processes.py,0,"b'import numpy as np\n\n\nclass OrnsteinUhlenbeckNoiseProcess:\n    """"""\n    Taken from https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py\n    """"""\n    def __init__(self, mu, sigma, theta=.15, dt=1e-2, x0=None):\n        self.theta = theta\n        self.mu = mu\n        self.sigma = sigma\n        self.dt = dt\n        self.x0 = x0\n        self.x_prev = None\n        self.reset()\n\n    def __call__(self):\n        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n        self.x_prev = x\n        return x\n\n    def reset(self):\n        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n\n    def __repr__(self):\n        return \'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})\'.format(self.mu, self.sigma)\n'"
vel/metrics/__init__.py,0,b''
vel/metrics/accuracy.py,0,"b'from vel.api.metrics.averaging_metric import AveragingSupervisedMetric\n\n\nclass Accuracy(AveragingSupervisedMetric):\n    """""" Classification accuracy """"""\n    def __init__(self):\n        super().__init__(""accuracy"")\n\n    def _value_function(self, x_input, y_true, y_pred):\n        """""" Return classification accuracy of input """"""\n        if len(y_true.shape) == 1:\n            return y_pred.argmax(1).eq(y_true).double().mean().item()\n        else:\n            raise NotImplementedError\n\n\ndef create():\n    """""" Vel factory function """"""\n    return Accuracy()\n'"
vel/metrics/loss_metric.py,0,"b'from vel.api.metrics.averaging_metric import AveragingMetric\n\n\nclass Loss(AveragingMetric):\n    """""" Just a loss function """"""\n    def __init__(self):\n        super().__init__(""loss"")\n\n    def _value_function(self, batch_info):\n        """""" Just forward a value of the loss""""""\n        return batch_info[\'loss\'].item()\n'"
vel/models/__init__.py,0,b''
vel/modules/__init__.py,0,b''
vel/modules/layers.py,2,"b'""""""\nCode (partially) based on:\nhttps://github.com/fastai/fastai/blob/master/fastai/layers.py\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom vel.util.tensor_util import one_hot_encoding\n\n\nclass AdaptiveConcatPool2d(nn.Module):\n    """""" Concat pooling - combined average pool and max pool """"""\n    def __init__(self, sz=None):\n        super().__init__()\n        sz = sz or (1, 1)\n        self.ap = nn.AdaptiveAvgPool2d(sz)\n        self.mp = nn.AdaptiveMaxPool2d(sz)\n\n    def forward(self, x):\n        return torch.cat([self.mp(x), self.ap(x)], 1)\n\n\nclass Lambda(nn.Module):\n    """""" Simple torch lambda layer """"""\n    def __init__(self, f):\n        super().__init__()\n        self.f = f\n\n    def forward(self, x):\n        return self.f(x)\n\n\nclass Flatten(nn.Module):\n    """""" Flatten input vector """"""\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\n\nclass OneHotEncode(nn.Module):\n    """""" One-hot encoding layer """"""\n    def __init__(self, num_classes):\n        super().__init__()\n        self.num_classes = num_classes\n\n    def forward(self, x):\n        return one_hot_encoding(x, self.num_classes)\n\n'"
vel/modules/resnet_v1.py,2,"b'""""""\nCode based on\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n""""""\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef conv3x3(in_channels, out_channels, stride=1):\n    """"""\n    3x3 convolution with padding.\n    Original code has had bias turned off, because Batch Norm would remove the bias either way\n    """"""\n    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    """"""\n    Single residual block consisting of two convolutional layers and a nonlinearity between them\n    """"""\n    def __init__(self, in_channels, out_channels, stride=1, divisor=None):\n        super().__init__()\n\n        self.stride = stride\n        self.divisor = divisor\n\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels),\n            )\n        else:\n            self.shortcut = None\n\n        self.conv1 = conv3x3(in_channels, out_channels, stride)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n\n        self.conv2 = conv3x3(out_channels, out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = F.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.shortcut is not None:\n            residual = self.shortcut(x)\n        else:\n            residual = x\n\n        out += residual\n        out = F.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    """"""\n    A \'bottleneck\' residual block consisting of three convolutional layers, where the first one is a downsampler,\n    then we have a 3x3 followed by an upsampler.\n    """"""\n    def __init__(self, in_channels, out_channels, stride=1, divisor=4):\n        super().__init__()\n\n        self.stride = stride\n        self.divisor = divisor\n\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels),\n            )\n        else:\n            self.shortcut = None\n\n        self.bottleneck_channels = out_channels // divisor\n\n        self.conv1 = nn.Conv2d(in_channels, self.bottleneck_channels, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.bottleneck_channels)\n\n        self.conv2 = conv3x3(self.bottleneck_channels, self.bottleneck_channels, stride)\n        self.bn2 = nn.BatchNorm2d(self.bottleneck_channels)\n\n        self.conv3 = nn.Conv2d(self.bottleneck_channels, out_channels, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = F.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = F.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.shortcut is not None:\n            residual = self.shortcut(x)\n        else:\n            residual = x\n\n        out += residual\n        out = F.relu(out)\n\n        return out\n'"
vel/modules/resnet_v2.py,2,"b'""""""\nCode based on\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n""""""\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef conv3x3(in_channels, out_channels, stride=1):\n    """"""\n    3x3 convolution with padding.\n    Original code has had bias turned off, because Batch Norm would remove the bias either way\n    """"""\n    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    """"""\n    Single residual block consisting of two convolutional layers and a nonlinearity between them\n    """"""\n    def __init__(self, in_channels, out_channels, stride=1, divisor=None):\n        super().__init__()\n\n        self.stride = stride\n        self.divisor = divisor\n\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n        else:\n            self.shortcut = None\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = conv3x3(in_channels, out_channels, stride)\n\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv2 = conv3x3(out_channels, out_channels)\n\n    def forward(self, x):\n        out = self.bn1(x)\n        out = F.relu(out)\n\n        # If we need to rescale the ""main path""\n        if self.shortcut:\n            residual = self.shortcut(out)\n        else:\n            residual = x\n\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = F.relu(out)\n        out = self.conv2(out)\n\n        out += residual\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    """"""\n    A \'bottleneck\' residual block consisting of three convolutional layers, where the first one is a downsampler,\n    then we have a 3x3 followed by an upsampler.\n    """"""\n    def __init__(self, in_channels, out_channels, stride=1, divisor=4):\n        super().__init__()\n\n        self.stride = stride\n        self.divisor = divisor\n\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n            )\n        else:\n            self.shortcut = None\n\n        self.bottleneck_channels = out_channels // divisor\n\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv1 = nn.Conv2d(in_channels, self.bottleneck_channels, kernel_size=1, bias=False)\n\n        self.bn2 = nn.BatchNorm2d(self.bottleneck_channels)\n        self.conv2 = nn.Conv2d(self.bottleneck_channels, self.bottleneck_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n\n        self.bn3 = nn.BatchNorm2d(self.bottleneck_channels)\n        self.conv3 = nn.Conv2d(self.bottleneck_channels, out_channels, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        # shortcut = self.shortcut(out) if hasattr(self, \'shortcut\') else x\n        # If we need to rescale the ""main path""\n\n        if self.shortcut:\n            residual = self.shortcut(out)\n        else:\n            residual = x\n\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = self.conv3(F.relu(self.bn3(out)))\n\n        out += residual\n\n        return out\n'"
vel/modules/resnext.py,2,"b'""""""\nCode based on\nhttps://github.com/fastai/fastai/blob/master/fastai/models/cifar10/resnext.py\n""""""\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ResNeXtBottleneck(nn.Module):\n    """"""\n    RexNeXt bottleneck type C (https://github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua)\n    """"""\n\n    def __init__(self, in_channels, out_channels, cardinality, divisor, stride=1):\n        super(ResNeXtBottleneck, self).__init__()\n\n        self.cardinality = cardinality\n        self.stride = stride\n        self.divisor = divisor\n\n        # D is a size of a single group\n        # Intermediate layers have D * C channels\n        D = out_channels // divisor\n        C = cardinality\n\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels),\n            )\n        else:\n            self.shortcut = None\n\n        self.conv_reduce = nn.Conv2d(in_channels, D * C, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_reduce = nn.BatchNorm2d(D*C)\n\n        self.conv_conv = nn.Conv2d(D*C, D*C, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n        self.bn = nn.BatchNorm2d(D*C)\n\n        self.conv_expand = nn.Conv2d(D * C, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_expand = nn.BatchNorm2d(out_channels)\n\n\n    def forward(self, x):\n        bottleneck = self.conv_reduce(x)\n        bottleneck = F.relu(self.bn_reduce(bottleneck), inplace=True)\n\n        bottleneck = self.conv_conv(bottleneck)\n        bottleneck = F.relu(self.bn(bottleneck), inplace=True)\n\n        bottleneck = self.conv_expand(bottleneck)\n        bottleneck = self.bn_expand(bottleneck)\n\n        if self.shortcut is not None:\n            residual = self.shortcut(x)\n        else:\n            residual = x\n\n        return F.relu(residual + bottleneck, inplace=True)\n'"
vel/modules/rnn_cell.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\n\nfrom vel.api import RnnLinearBackboneModel\n\n\nclass RnnCell(RnnLinearBackboneModel):\n    """""" Generalization of RNN cell (Simple RNN, LSTM or GRU) """"""\n\n    def __init__(self, input_size, hidden_size, rnn_type, bias=True, nonlinearity=\'tanh\'):\n        super().__init__()\n\n        assert rnn_type in {\'rnn\', \'lstm\', \'gru\'}, ""Rnn type {} is not supported"".format(rnn_type)\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.rnn_type = rnn_type\n\n        if self.rnn_type == \'rnn\':\n            self.rnn_cell = nn.RNNCell(\n                input_size=input_size, hidden_size=hidden_size, bias=bias, nonlinearity=nonlinearity\n            )\n        elif self.rnn_type == \'lstm\':\n            self.rnn_cell = nn.LSTMCell(input_size=input_size, hidden_size=hidden_size, bias=bias)\n        elif self.rnn_type == \'gru\':\n            self.rnn_cell = nn.GRUCell(input_size=input_size, hidden_size=hidden_size, bias=bias)\n\n    def reset_weights(self):\n        init.xavier_normal_(self.rnn_cell.weight_hh)\n        init.xavier_normal_(self.rnn_cell.weight_ih)\n        init.zeros_(self.rnn_cell.bias_ih)\n        init.zeros_(self.rnn_cell.bias_hh)\n\n    @property\n    def output_dim(self) -> int:\n        """""" Final dimension of model output """"""\n        return self.hidden_size\n\n    @property\n    def state_dim(self) -> int:\n        """""" Dimension of model state """"""\n        if self.rnn_type == \'lstm\':\n            return 2 * self.hidden_size\n        else:\n            return self.hidden_size\n\n    def forward(self, input_data, state):\n        if self.rnn_type == \'lstm\':\n            hidden_state, cell_state = torch.split(state, self.hidden_size, 1)\n            hidden_state, cell_state = self.rnn_cell(input_data, (hidden_state, cell_state))\n            new_state = torch.cat([hidden_state, cell_state], dim=1)\n            return hidden_state, new_state\n        else:\n            new_hidden_state = self.rnn_cell(input_data, state)\n            return new_hidden_state, new_hidden_state\n\n\n\n'"
vel/modules/rnn_layer.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\n\nfrom vel.api import RnnLinearBackboneModel\n\n\nclass RnnLayer(RnnLinearBackboneModel):\n    """""" Generalization of RNN layer (Simple RNN, LSTM or GRU) """"""\n\n    def __init__(self, input_size, hidden_size, rnn_type, bias=True, bidirectional=False, nonlinearity=\'tanh\'):\n        super().__init__()\n\n        assert rnn_type in {\'rnn\', \'lstm\', \'gru\'}, ""RNN type {} is not supported"".format(rnn_type)\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.rnn_type = rnn_type\n        self.bidirectional = bidirectional\n\n        if self.rnn_type == \'rnn\':\n            self.rnn_cell = nn.RNN(\n                input_size=input_size, hidden_size=hidden_size, bias=bias, nonlinearity=nonlinearity,\n                bidirectional=bidirectional, batch_first=True\n            )\n        elif self.rnn_type == \'lstm\':\n            self.rnn_cell = nn.LSTM(\n                input_size=input_size, hidden_size=hidden_size, bias=bias,\n                bidirectional=bidirectional, batch_first=True\n            )\n        elif self.rnn_type == \'gru\':\n            self.rnn_cell = nn.GRU(\n                input_size=input_size, hidden_size=hidden_size, bias=bias,\n                bidirectional=bidirectional, batch_first=True\n            )\n\n    def reset_weights(self):\n        init.xavier_normal_(self.rnn_cell.weight_hh)\n        init.xavier_normal_(self.rnn_cell.weight_ih)\n        init.zeros_(self.rnn_cell.bias_ih)\n        init.zeros_(self.rnn_cell.bias_hh)\n\n    @property\n    def output_dim(self) -> int:\n        """""" Final dimension of model output """"""\n        if self.bidirectional:\n            return 2.0 * self.hidden_size\n        else:\n            return self.hidden_size\n\n    @property\n    def state_dim(self) -> int:\n        """""" Dimension of model state """"""\n        if self.rnn_type == \'lstm\':\n            return 2 * self.hidden_size\n        else:\n            return self.hidden_size\n\n    def forward(self, input_data, state=None):\n        if state is None:\n            if self.bidirectional:\n                state = self.zero_state(input_data.size(0)).unsqueeze(0).repeat(2, 1, 1).to(input_data.device)\n            else:\n                state = self.zero_state(input_data.size(0)).unsqueeze(0).to(input_data.device)\n\n        if self.rnn_type == \'lstm\':\n            hidden_state, cell_state = torch.split(state, self.hidden_size, 2)\n            hidden_state = hidden_state.contiguous()\n            cell_state = cell_state.contiguous()\n            output, (hidden_state, cell_state) = self.rnn_cell(input_data, (hidden_state, cell_state))\n            new_state = torch.cat([hidden_state, cell_state], dim=2)\n            return output, new_state\n        else:\n            return self.rnn_cell(input_data, state)\n\n\n\n'"
vel/notebook/__init__.py,0,b'from .loader import load'
vel/notebook/loader.py,0,"b'from vel.api import ModelConfig\n\n\ndef load(config_path, run_number=0, device=\'cuda:0\'):\n    """""" Load a ModelConfig from filename """"""\n    model_config = ModelConfig.from_file(config_path, run_number, device=device)\n\n    return model_config\n'"
vel/openai/__init__.py,0,b''
vel/optimizers/__init__.py,0,b''
vel/optimizers/adadelta.py,3,"b'import torch.optim\n\nfrom vel.api import OptimizerFactory, Model\n\n\nclass AdadeltaFactory(OptimizerFactory):\n    """""" Adadelta optimizer factory """"""\n\n    def __init__(self, lr=1.0, rho=0.9, eps=1e-6, weight_decay=0):\n        self.lr = lr\n        self.rho = rho\n        self.eps = eps\n        self.weight_decay = weight_decay\n\n    def instantiate(self, model: Model) -> torch.optim.Adadelta:\n        return torch.optim.Adadelta(\n            filter(lambda p: p.requires_grad, model.parameters()),\n            lr=self.lr, rho=self.rho, eps=self.eps, weight_decay=self.weight_decay\n        )\n\n\ndef create():\n    """""" Vel factory function """"""\n    return AdadeltaFactory()\n'"
vel/optimizers/adam.py,4,"b'import collections\nimport torch.optim\n\nimport vel.util.module_util as mu\n\nfrom vel.api import OptimizerFactory, Model\n\n\nclass AdamFactory(OptimizerFactory):\n    """""" Adam optimizer factory """"""\n\n    def __init__(self, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False, layer_groups=False):\n        self.lr = lr\n        self.betas = betas\n        self.eps = eps\n        self.weight_decay = weight_decay\n        self.amsgrad = amsgrad\n        self.layer_groups = layer_groups\n\n    def instantiate(self, model: Model) -> torch.optim.Adam:\n        if self.layer_groups:\n            parameters = mu.to_parameter_groups(model.get_layer_groups())\n\n            if isinstance(self.lr, collections.Sequence):\n                for idx, lr in enumerate(self.lr):\n                    parameters[idx][\'lr\'] = lr\n\n                default_lr = self.lr[0]\n            else:\n                default_lr = float(self.lr)\n\n            if isinstance(self.weight_decay, collections.Sequence):\n                for idx, weight_decay in enumerate(self.weight_decay):\n                    parameters[idx][\'weight_decay\'] = weight_decay\n\n                default_weight_decay = self.weight_decay[0]\n            else:\n                default_weight_decay = self.weight_decay\n\n            return torch.optim.Adam(\n                parameters,\n                lr=default_lr, betas=self.betas, eps=self.eps, weight_decay=default_weight_decay, amsgrad=self.amsgrad\n            )\n        else:\n            parameters = filter(lambda p: p.requires_grad, model.parameters())\n\n            return torch.optim.Adam(\n                parameters,\n                lr=self.lr, betas=self.betas, eps=self.eps, weight_decay=self.weight_decay, amsgrad=self.amsgrad\n            )\n\n\ndef create(lr, betas=(0.9, 0.999), weight_decay=0, epsilon=1e-8, layer_groups=False):\n    """""" Vel factory function """"""\n    return AdamFactory(lr=lr, betas=betas, weight_decay=weight_decay, eps=epsilon, layer_groups=layer_groups)\n'"
vel/optimizers/rmsprop.py,3,"b'import torch.optim\n\nfrom vel.api import OptimizerFactory, Model\n\n\nclass RMSpropFactory(OptimizerFactory):\n    """""" RMSprop optimizer factory """"""\n\n    def __init__(self, lr=1e-2, alpha=0.99, eps=1e-8, weight_decay=0, momentum=0, centered=False):\n        self.lr = lr\n        self.alpha = alpha\n        self.eps = eps\n        self.weight_decay = weight_decay\n        self.momentum = momentum\n        self.centered = centered\n\n    def instantiate(self, model: Model) -> torch.optim.RMSprop:\n        return torch.optim.RMSprop(\n            filter(lambda p: p.requires_grad, model.parameters()),\n            lr=self.lr, alpha=self.alpha, eps=self.eps,\n            weight_decay=self.weight_decay, momentum=self.momentum, centered=self.centered\n        )\n\n\ndef create(lr, alpha, momentum=0, weight_decay=0, epsilon=1e-8):\n    """""" Vel factory function """"""\n    return RMSpropFactory(lr=lr, alpha=alpha, momentum=momentum, weight_decay=weight_decay, eps=float(epsilon))\n'"
vel/optimizers/rmsprop_tf.py,5,"b'import torch.optim\n\nfrom vel.api import OptimizerFactory, Model\n\n\nclass RMSpropTF(torch.optim.Optimizer):\n    """"""Implements RMSprop algorithm. A TensorFlow version with epsilon under the square root\n\n    Proposed by G. Hinton in his\n    `course <http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>`_.\n\n    The centered version first appears in `Generating Sequences\n    With Recurrent Neural Networks <https://arxiv.org/pdf/1308.0850v5.pdf>`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-2)\n        momentum (float, optional): momentum factor (default: 0)\n        alpha (float, optional): smoothing constant (default: 0.99)\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        centered (bool, optional) : if ``True``, compute the centered RMSProp,\n            the gradient is normalized by an estimation of its variance\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n\n    """"""\n\n    def __init__(self, params, lr=1e-2, alpha=0.99, eps=1e-8, weight_decay=0, momentum=0, centered=False):\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 <= momentum:\n            raise ValueError(""Invalid momentum value: {}"".format(momentum))\n        if not 0.0 <= weight_decay:\n            raise ValueError(""Invalid weight_decay value: {}"".format(weight_decay))\n        if not 0.0 <= alpha:\n            raise ValueError(""Invalid alpha value: {}"".format(alpha))\n\n        defaults = dict(lr=lr, momentum=momentum, alpha=alpha, eps=eps, centered=centered, weight_decay=weight_decay)\n        super(RMSpropTF, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RMSpropTF, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'momentum\', 0)\n            group.setdefault(\'centered\', False)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'RMSprop does not support sparse gradients\')\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # ANOTHER LINE I\'VE CHANGED\n                    state[\'square_avg\'] = torch.ones_like(p.data)\n\n                    if group[\'momentum\'] > 0:\n                        state[\'momentum_buffer\'] = torch.zeros_like(p.data)\n\n                    if group[\'centered\']:\n                        state[\'grad_avg\'] = torch.zeros_like(p.data)\n\n                square_avg = state[\'square_avg\']\n                alpha = group[\'alpha\']\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n\n                if group[\'centered\']:\n                    grad_avg = state[\'grad_avg\']\n                    grad_avg.mul_(alpha).add_(1 - alpha, grad)\n                    # THIS LINE IS EVERYTHING THAT I CHANGED IN THIS OPTIMIZER\n                    # avg = square_avg.addcmul(-1, grad_avg, grad_avg).sqrt().add_(group[\'eps\'])\n                    avg = square_avg.addcmul(-1, grad_avg, grad_avg).add(group[\'eps\']).sqrt()\n                else:\n                    # THIS LINE IS EVERYTHING THAT I CHANGED IN THIS OPTIMIZER\n                    # avg = square_avg.sqrt().add_(group[\'eps\'])\n                    avg = square_avg.add(group[\'eps\']).sqrt()\n\n                if group[\'momentum\'] > 0:\n                    buf = state[\'momentum_buffer\']\n                    buf.mul_(group[\'momentum\']).addcdiv_(grad, avg)\n                    p.data.add_(-group[\'lr\'], buf)\n                else:\n                    p.data.addcdiv_(-group[\'lr\'], grad, avg)\n\n        return loss\n\n\nclass RMSpropTFFactory(OptimizerFactory):\n    """""" RMSprop optimizer factory - A Tensorflow version with epsilon under square root """"""\n\n    def __init__(self, lr=1e-2, alpha=0.99, eps=1e-8, weight_decay=0, momentum=0, centered=False):\n        self.lr = lr\n        self.alpha = alpha\n        self.eps = eps\n        self.weight_decay = weight_decay\n        self.momentum = momentum\n        self.centered = centered\n\n    def instantiate(self, model: Model) -> RMSpropTF:\n        return RMSpropTF(\n            filter(lambda p: p.requires_grad, model.parameters()),\n            lr=self.lr, alpha=self.alpha, eps=self.eps,\n            weight_decay=self.weight_decay, momentum=self.momentum, centered=self.centered\n        )\n\n\ndef create(lr, alpha, momentum=0, weight_decay=0, epsilon=1e-8):\n    """""" Vel factory function """"""\n    return RMSpropTFFactory(lr=lr, alpha=alpha, momentum=momentum, weight_decay=weight_decay, eps=float(epsilon))\n'"
vel/optimizers/sgd.py,3,"b'import torch.optim\n\nimport vel.util.module_util as mu\n\nfrom vel.api import OptimizerFactory, Model\n\n\nclass SgdFactory(OptimizerFactory):\n    """""" SGD optimizer factory """"""\n\n    def __init__(self, lr, momentum=0, dampening=0, weight_decay=0, nesterov=False, layer_groups: bool=False):\n        self.lr = lr\n        self.momentum = momentum\n        self.dampening = dampening\n        self.weight_decay = weight_decay\n        self.nesterov = nesterov\n        self.layer_groups = layer_groups\n\n    def instantiate(self, model: Model) -> torch.optim.SGD:\n        if self.layer_groups:\n            parameters = mu.to_parameter_groups(model.get_layer_groups())\n        else:\n            parameters = filter(lambda p: p.requires_grad, model.parameters())\n\n        return torch.optim.SGD(\n            parameters,\n            lr=self.lr, momentum=self.momentum, dampening=self.dampening, weight_decay=self.weight_decay,\n            nesterov=self.nesterov\n        )\n\n\ndef create(lr, weight_decay=0, momentum=0, layer_groups=False):\n    """""" Vel factory function """"""\n    return SgdFactory(lr=lr, weight_decay=weight_decay, momentum=momentum, layer_groups=layer_groups)\n'"
vel/phase/__init__.py,0,b''
vel/phase/cycle.py,0,"b'import numpy as np\n\nimport vel.util.intepolate as interp\n\nfrom vel.api import BatchInfo, EpochInfo, TrainingInfo, Callback, TrainPhase\n\n\nclass CycleCallback(Callback):\n    """""" A callback that manages setting the proper learning rate """"""\n\n    def __init__(self, optimizer, max_lr, min_lr, cycles, cycle_len=1, cycle_mult=1, init_iter=0, init_lr=0,\n                 interpolate=\'linear\'):\n        self.max_lr = max_lr\n        self.min_lr = min_lr\n\n        self.cycles = cycles\n        self.cycle_len = cycle_len\n        self.cycle_mult = cycle_mult\n\n        self.init_iter = init_iter\n        self.init_lr = init_lr\n\n        if cycle_mult > 1:\n            self.epochs = self.cycle_len * (self.cycle_mult ** self.cycles - 1) // (self.cycle_mult - 1)\n        else:\n            self.epochs = self.cycle_mult * self.cycles\n\n        self.optimizer = optimizer\n        self.interpolate = interpolate\n\n        # self.current_cycle = None\n        self.cycle_dict, self.cycle_lengths, self.cycle_starts = self._init_cycle_dict()\n\n    def _init_cycle_dict(self):\n        """""" Populate a cycle dict """"""\n        dict_arr = np.zeros(self.epochs, dtype=int)\n        length_arr = np.zeros(self.epochs, dtype=int)\n        start_arr = np.zeros(self.epochs, dtype=int)\n\n        c_len = self.cycle_len\n        idx = 0\n\n        for i in range(self.cycles):\n            current_start = idx\n            for j in range(c_len):\n                dict_arr[idx] = i\n                length_arr[idx] = c_len\n                start_arr[idx] = current_start\n                idx += 1\n\n            c_len *= self.cycle_mult\n\n        return dict_arr, length_arr, start_arr\n\n    def on_batch_begin(self, batch_info: BatchInfo):\n        """""" Set proper learning rate """"""\n        cycle_length = self.cycle_lengths[batch_info.local_epoch_number - 1]\n        cycle_start = self.cycle_starts[batch_info.local_epoch_number - 1]\n\n        numerator = (batch_info.local_epoch_number - cycle_start - 1) * batch_info.batches_per_epoch + batch_info.batch_number\n        denominator = cycle_length * batch_info.batches_per_epoch\n\n        interpolation_number = numerator / denominator\n\n        if cycle_start == 0 and numerator < self.init_iter:\n            lr = self.init_lr\n        else:\n            if isinstance(self.max_lr, list):\n                lr = [interp.interpolate_single(max_lr, min_lr, interpolation_number, how=self.interpolate) for max_lr, min_lr in zip(self.max_lr, self.min_lr)]\n            else:\n                lr = interp.interpolate_single(self.max_lr, self.min_lr, interpolation_number, how=self.interpolate)\n\n        self.set_lr(lr)\n\n    def set_lr(self, lr):\n        """""" Set a learning rate for the optimizer """"""\n        if isinstance(lr, list):\n            for group_lr, param_group in zip(lr, self.optimizer.param_groups):\n                param_group[\'lr\'] = group_lr\n        else:\n            for param_group in self.optimizer.param_groups:\n                param_group[\'lr\'] = lr\n\n\nclass CyclePhase(TrainPhase):\n    """""" Most generic phase of training """"""\n\n    def __init__(self, optimizer_factory, max_lr, min_lr, cycles, cycle_len=1, cycle_mult=1, interpolate=\'linear\',\n                 init_lr=0, init_iter=0, freeze=False):\n        self.max_lr = max_lr\n        self.min_lr = min_lr\n\n        self.cycles = cycles\n        self.cycle_len = cycle_len\n        self.cycle_mult = cycle_mult\n\n        if cycle_mult > 1:\n            self.epochs = self.cycle_len * (self.cycle_mult ** self.cycles - 1) // (self.cycle_mult - 1)\n        else:\n            self.epochs = self.cycle_mult * self.cycles\n\n        self.interpolate = interpolate\n\n        self.init_iter = init_iter\n        self.init_lr = init_lr\n\n        self.optimizer_factory = optimizer_factory\n        self.freeze = freeze\n\n        self._optimizer_instance = None\n        self._source = None\n\n        self.special_callback = None\n\n    @property\n    def number_of_epochs(self) -> int:\n        return self.epochs\n\n    def set_up_phase(self, training_info, model, source):\n        """""" Prepare the phase for learning """"""\n        # To parameter groups handles properly filtering parameters that don\'t require gradient\n        self._optimizer_instance = self.optimizer_factory.instantiate(model)\n        self._source = source\n\n        self.special_callback = CycleCallback(\n            self._optimizer_instance,\n            max_lr=self.max_lr, min_lr=self.min_lr, cycles=self.cycles,\n            cycle_len=self.cycle_len, cycle_mult=self.cycle_mult, interpolate=self.interpolate,\n            init_iter=self.init_iter, init_lr=self.init_lr\n        )\n\n        return self._optimizer_instance\n\n    def epoch_info(self, training_info: TrainingInfo, global_idx: int, local_idx: int) -> EpochInfo:\n        """""" Create Epoch info """"""\n        return EpochInfo(\n            training_info=training_info,\n            global_epoch_idx=global_idx,\n            local_epoch_idx=local_idx,\n            batches_per_epoch=self._source.train_iterations_per_epoch(),\n            optimizer=self._optimizer_instance,\n            # Add special callback for this epoch\n            callbacks=[self.special_callback] + training_info.callbacks\n        )\n\n    def execute_epoch(self, epoch_info, learner):\n        """""" Prepare the phase for learning """"""\n        learner.run_epoch(epoch_info, self._source)\n\n\ndef create(optimizer, max_lr, min_lr, cycles, cycle_len=1, cycle_mult=1, interpolate=\'linear\', init_lr=0, init_iter=0):\n    """""" Vel factory function """"""\n    return CyclePhase(\n        max_lr=max_lr,\n        min_lr=min_lr,\n        cycles=cycles,\n        cycle_len=cycle_len,\n        cycle_mult=cycle_mult,\n        interpolate=interpolate,\n        optimizer_factory=optimizer,\n        init_lr=init_lr,\n        init_iter=init_iter,\n    )\n'"
vel/phase/freeze.py,0,"b'import vel.api as api\n\n\nclass FreezePhase(api.EmptyTrainPhase):\n    """""" Freeze the model """"""\n\n    def set_up_phase(self, training_info, model, source):\n        """""" Freeze the model """"""\n        model.freeze()\n\n\ndef create():\n    """""" Vel factory function """"""\n    return FreezePhase()\n'"
vel/phase/generic.py,0,"b'from vel.api import TrainingInfo, EpochInfo, TrainPhase, Source\n\n\nclass GenericPhase(TrainPhase):\n    """""" Most generic phase of training """"""\n\n    def __init__(self, lr, epochs, optimizer_factory):\n        self.lr = lr\n        self.epochs = epochs\n        self.optimizer_factory = optimizer_factory\n\n        self._optimizer_instance = None\n        self._source = None\n\n    @property\n    def number_of_epochs(self) -> int:\n        return self.epochs\n\n    def set_up_phase(self, training_info, model, source: Source):\n        """""" Prepare the phase for learning """"""\n        self._optimizer_instance = self.optimizer_factory.instantiate(model)\n        self._source = source\n\n    def epoch_info(self, training_info: TrainingInfo, global_idx: int, local_idx: int) -> EpochInfo:\n        """""" Create Epoch info """"""\n        return EpochInfo(\n            training_info=training_info,\n            global_epoch_idx=global_idx,\n            local_epoch_idx=local_idx,\n            batches_per_epoch=self._source.train_iterations_per_epoch(),\n            optimizer=self._optimizer_instance\n        )\n\n    def execute_epoch(self, epoch_info, learner):\n        """""" Prepare the phase for learning """"""\n        for param_group in epoch_info.optimizer.param_groups:\n            param_group[\'lr\'] = self.lr\n\n        epoch_result = learner.run_epoch(epoch_info, self._source)\n\n        return epoch_result\n\n\ndef create(lr, epochs, optimizer):\n    """""" Vel factory function """"""\n    return GenericPhase(\n        lr=lr,\n        epochs=epochs,\n        optimizer_factory=optimizer,\n    )\n'"
vel/phase/unfreeze.py,0,"b'import vel.api as api\n\n\nclass UnfreezePhase(api.EmptyTrainPhase):\n    """""" Freeze the model """"""\n\n    def set_up_phase(self, training_info, model, source):\n        """""" Freeze the model """"""\n        model.unfreeze()\n\n\ndef create():\n    """""" Vel factory function """"""\n    return UnfreezePhase()\n'"
vel/rl/__init__.py,0,b''
vel/rl/discount_bootstrap.py,2,"b'import torch\n\n\ndef discount_bootstrap(rewards_buffer, dones_buffer, final_values, discount_factor, number_of_steps):\n    """""" Calculate state values bootstrapping off the following state values """"""\n    true_value_buffer = torch.zeros_like(rewards_buffer)\n\n    # discount/bootstrap off value fn\n    current_value = final_values\n\n    for i in reversed(range(number_of_steps)):\n        current_value = rewards_buffer[i] + discount_factor * current_value * (1.0 - dones_buffer[i])\n        true_value_buffer[i] = current_value\n\n    return true_value_buffer\n\n\ndef discount_bootstrap_gae(rewards_buffer, dones_buffer, values_buffer, final_values, discount_factor, gae_lambda,\n                           number_of_steps):\n    """"""\n    Calculate state values bootstrapping off the following state values - Generalized Advantage Estimation\n    https://arxiv.org/abs/1506.02438\n    """"""\n    advantage_buffer = torch.zeros_like(rewards_buffer)\n\n    # Accmulate sums\n    sum_accumulator = 0\n\n    for i in reversed(range(number_of_steps)):\n        if i == number_of_steps - 1:\n            next_value = final_values\n        else:\n            next_value = values_buffer[i + 1]\n\n        bellman_delta = (\n                rewards_buffer[i] + discount_factor * next_value * (1.0 - dones_buffer[i]) - values_buffer[i]\n        )\n\n        advantage_buffer[i] = sum_accumulator = (\n                bellman_delta + discount_factor * gae_lambda * sum_accumulator * (1.0 - dones_buffer[i])\n        )\n\n    return advantage_buffer\n'"
vel/rl/metrics.py,1,"b'import collections\n\nimport numpy as np\nimport torch\n\nfrom vel.api import BatchInfo\nfrom vel.api.metrics import BaseMetric, AveragingMetric, ValueMetric\n\n\nclass FramesMetric(ValueMetric):\n    """""" Count the frames """"""\n    def __init__(self, name=""frames""):\n        super().__init__(name)\n\n    def _value_function(self, batch_info: BatchInfo):\n        return batch_info.training_info[\'frames\']\n\n\nclass FPSMetric(ValueMetric):\n    """""" Metric calculating FPS values """"""\n    def __init__(self, name=\'fps\'):\n        super().__init__(name)\n\n    def _value_function(self, batch_info):\n        frames = batch_info.training_info[\'frames\']\n        seconds = batch_info.training_info[\'time\']\n\n        fps = int(frames/seconds)\n\n        return fps\n\n\nclass EpisodeRewardMetric(BaseMetric):\n    def __init__(self, name):\n        super().__init__(name)\n        self.buffer = collections.deque(maxlen=100)\n\n    def calculate(self, batch_info):\n        """""" Calculate value of a metric based on supplied data """"""\n        self.buffer.extend(batch_info[\'episode_infos\'])\n\n    def reset(self):\n        """""" Reset value of a metric """"""\n        # Because it\'s a queue no need for reset..\n        pass\n\n    def value(self):\n        """""" Return current value for the metric """"""\n        if self.buffer:\n            return np.mean([ep[\'r\'] for ep in self.buffer])\n        else:\n            return 0.0\n\n\nclass EpisodeRewardMetricQuantile(BaseMetric):\n    def __init__(self, name, quantile, buf_size=100):\n        super().__init__(name)\n        self.buffer = collections.deque(maxlen=buf_size)\n        self.quantile = quantile\n\n    def calculate(self, batch_info):\n        """""" Calculate value of a metric based on supplied data """"""\n        self.buffer.extend(ep[\'r\'] for ep in batch_info[\'episode_infos\'])\n\n    def reset(self):\n        """""" Reset value of a metric """"""\n        # Because it\'s a queue no need for reset..\n        pass\n\n    def value(self):\n        """""" Return current value for the metric """"""\n        if self.buffer:\n            return np.quantile(self.buffer, self.quantile)\n        else:\n            return 0.0\n\n\nclass EpisodeLengthMetric(BaseMetric):\n    def __init__(self, name):\n        super().__init__(name)\n        self.buffer = collections.deque(maxlen=100)\n\n    def calculate(self, batch_info):\n        """""" Calculate value of a metric based on supplied data """"""\n        self.buffer.extend(batch_info[\'episode_infos\'])\n\n    def reset(self):\n        """""" Reset value of a metric """"""\n        # Because it\'s a queue no need for reset..\n        pass\n\n    def value(self):\n        """""" Return current value for the metric """"""\n        if self.buffer:\n            return np.mean([ep[\'l\'] for ep in self.buffer])\n        else:\n            return 0\n\n\nclass ExplainedVariance(AveragingMetric):\n    """""" How much value do rewards explain """"""\n    def __init__(self):\n        super().__init__(""explained_variance"")\n\n    def _value_function(self, batch_info):\n        values = batch_info[\'values\']\n        rewards = batch_info[\'rewards\']\n\n        explained_variance = 1 - torch.var(rewards - values) / torch.var(rewards)\n        return explained_variance.item()\n'"
vel/scheduler/__init__.py,0,b''
vel/scheduler/ladder.py,1,"b'import torch.optim.lr_scheduler as scheduler\nimport numpy as np\n\n\nfrom vel.api import Callback, SchedulerFactory\n\n\nclass LadderScheduler(Callback):\n    """""" Scheduler defined by a set of learning rates after reaching given number of iterations """"""\n    def __init__(self, optimizer, ladder, last_epoch):\n        self.schedule_limits = np.cumsum([x[0] for x in ladder])\n        self.schedule_numbers = np.array([float(x[1]) for x in ladder])\n        self.scheduler = scheduler.LambdaLR(optimizer, self.lambda_fn, last_epoch=last_epoch)\n\n    def lambda_fn(self, epoch_idx):\n        idx = np.minimum(np.searchsorted(self.schedule_limits, epoch_idx), len(self.schedule_limits) - 1)\n        return self.schedule_numbers[idx]\n\n    def on_epoch_begin(self, epoch_info):\n        self.scheduler.step(epoch=epoch_info.global_epoch_idx)\n\n\nclass LadderSchedulerFactory(SchedulerFactory):\n    """""" Factory class for ladder scheduler """"""\n    def __init__(self, ladder):\n        self.ladder = ladder\n\n    def instantiate(self, optimizer, last_epoch=-1) -> LadderScheduler:\n        return LadderScheduler(optimizer, self.ladder, last_epoch)\n\n\ndef create(ladder):\n    """""" Vel factory function """"""\n    return LadderSchedulerFactory(ladder)\n'"
vel/scheduler/linear_batch_scaler.py,0,"b'import vel.api as base\n\nfrom vel.api import BatchInfo, TrainingInfo\n\n\nclass LinearBatchScaler(base.Callback):\n    """""" Scales linearly LR from maximum value to 0 through every batch of training """"""\n    def __init__(self, optimizer):\n        self.optimizer = optimizer\n        self.starting_lr = None\n\n    def on_initialization(self, training_info: TrainingInfo):\n        self.starting_lr = [p[\'lr\'] for p in self.optimizer.param_groups]\n\n    def write_state_dict(self, training_info: TrainingInfo, hidden_state_dict: dict):\n        hidden_state_dict[\'linear_batch_scaler/starting_lr\'] = self.starting_lr\n\n    def load_state_dict(self, training_info: TrainingInfo, hidden_state_dict: dict):\n        self.starting_lr = hidden_state_dict[\'linear_batch_scaler/starting_lr\']\n\n    def on_batch_begin(self, batch_info: BatchInfo):\n        for starting_lr, param_group in zip(self.starting_lr, self.optimizer.param_groups):\n            param_group[\'lr\'] = starting_lr * (1.0 - batch_info[\'progress\'])\n\n\nclass LinearBatchScalerFactory(base.SchedulerFactory):\n    """""" Factory class for linear batch scaler scheduler """"""\n    def instantiate(self, optimizer, last_epoch=-1) -> LinearBatchScaler:\n        return LinearBatchScaler(optimizer)\n\n\ndef create():\n    """""" Vel factory function """"""\n    return LinearBatchScalerFactory()\n\n\n'"
vel/scheduler/multi_step.py,1,"b'import torch.optim.lr_scheduler as scheduler\n\n\n# class MultiStepScheduler:\n#     def __init__(self, optimizer, milestones, gamma, last_epoch):\n#         self.scheduler = scheduler.MultiStepLR(optimizer, milestones, gamma, last_epoch=last_epoch)\n#\n#     def get_lr(self):\n#         return self.scheduler.get_lr()\n#\n#     def pre_epoch_step(self, epoch_idx):\n#         self.scheduler.step()\n#\n#     def post_epoch_step(self, epoch_idx, metrics):\n#         pass\n#\n#\n# def create(milestones, gamma=0.1):\n#     """""" Create a multi-step scheduler """"""\n#     def scheduler_fn(optimizer, last_epoch=-1):\n#         return MultiStepScheduler(optimizer, milestones, gamma, last_epoch=last_epoch)\n#\n#     return scheduler_fn\n'"
vel/scheduler/reduce_lr_on_plateau.py,1,"b'import torch.optim.lr_scheduler as scheduler\n\n\n# class ReduceLrOnPlateau:\n#     def __init__(self, optimizer, metric_name, mode, factor, patience, threshold, threshold_mode, cooldown, min_lr, epsilon):\n#         self.metric_name = metric_name\n#         self.scheduler = scheduler.ReduceLROnPlateau(\n#             optimizer,\n#             mode=mode,\n#             factor=factor,\n#             patience=patience,\n#             threshold=threshold,\n#             threshold_mode=threshold_mode,\n#             cooldown=cooldown,\n#             min_lr=min_lr,\n#             eps=epsilon\n#         )\n#\n#     def get_lr(self):\n#         return [p[\'lr\'] for p in self.scheduler.optimizer.param_groups]\n#\n#     def pre_epoch_step(self):\n#         pass\n#\n#     def post_epoch_step(self, metrics):\n#         self.scheduler.step(metrics[self.metric_name])\n#\n#\n# def create(metric_name, mode=\'min\', factor=0.1, patience=10, threshold=1e-4, threshold_mode=\'rel\', cooldown=10,\n#            min_lr=0, epsilon=1e-8):\n#     """""" Create a scheduler that lowers the LR on metric plateau """"""\n#     def scheduler_fn(optimizer):\n#         return ReduceLrOnPlateau(optimizer, metric_name, mode, factor, patience, threshold, threshold_mode, cooldown, min_lr, epsilon)\n#\n#     return scheduler_fn\n\n'"
vel/schedules/__init__.py,0,b''
vel/schedules/constant.py,0,"b'from vel.api import Schedule\n\n\nclass ConstantSchedule(Schedule):\n    """""" Interpolate variable linearly between start value and final value """"""\n\n    def __init__(self, value):\n        self._value = value\n\n    def value(self, progress_indicator):\n        """""" Interpolate linearly between start and end """"""\n        return self._value\n\n\ndef create(value):\n    """""" Vel factory function """"""\n    return ConstantSchedule(value)\n'"
vel/schedules/linear.py,0,"b'import vel.util.intepolate as interpolate\n\nfrom vel.api import Schedule\n\n\nclass LinearSchedule(Schedule):\n    """""" Interpolate variable linearly between start value and final value """"""\n\n    def __init__(self, initial_value, final_value):\n        self.initial_value = initial_value\n        self.final_value = final_value\n\n    def value(self, progress_indicator):\n        """""" Interpolate linearly between start and end """"""\n        return interpolate.interpolate_linear_single(self.initial_value, self.final_value, progress_indicator)\n\n\ndef create(initial_value, final_value):\n    """""" Vel factory function """"""\n    return LinearSchedule(initial_value, final_value)\n\n'"
vel/schedules/linear_and_constant.py,0,"b'import vel.util.intepolate as interpolate\n\nfrom vel.api import Schedule\n\n\nclass LinearAndConstantSchedule(Schedule):\n    """"""\n    Interpolate variable linearly between start value and final values\n    only up to given point and return constant value afterwards.\n    """"""\n\n    def __init__(self, initial_value, final_value, end_of_interpolation):\n        self.initial_value = initial_value\n        self.final_value = final_value\n        self.end_of_interpolation = end_of_interpolation\n\n    def value(self, progress_indicator):\n        """""" Interpolate linearly between start and end """"""\n        if progress_indicator <= self.end_of_interpolation:\n            return interpolate.interpolate_linear_single(\n                self.initial_value, self.final_value, progress_indicator/self.end_of_interpolation\n            )\n        else:\n            return self.final_value\n\n\ndef create(initial_value, final_value, end_of_interpolation):\n    """""" Vel factory function """"""\n    return LinearAndConstantSchedule(initial_value, final_value, end_of_interpolation)\n'"
vel/sources/__init__.py,0,b''
vel/sources/img_dir_source.py,0,"b'import os.path\n\nimport torchvision.datasets as ds\n\nfrom vel.api import TrainingData\n\n\nclass ImageDirSource(ds.ImageFolder):\n    pass\n\n\ndef create(model_config, path, num_workers, batch_size, augmentations=None, tta=None):\n    """""" Create an ImageDirSource with supplied arguments """"""\n    if not os.path.isabs(path):\n        path = model_config.project_top_dir(path)\n\n    train_path = os.path.join(path, \'train\')\n    valid_path = os.path.join(path, \'valid\')\n\n    train_ds = ImageDirSource(train_path)\n    val_ds = ImageDirSource(valid_path)\n\n    return TrainingData(\n        train_ds,\n        val_ds,\n        num_workers=num_workers,\n        batch_size=batch_size,\n        augmentations=augmentations,\n        # test_time_augmentation=tta\n    )\n'"
vel/storage/__init__.py,0,b''
vel/storage/classic.py,5,"b'import os\nimport pathlib\nimport re\nimport torch\n\n\nfrom vel.api import ModelConfig, EpochInfo, TrainingInfo, Model, Storage\nfrom .strategy.checkpoint_strategy import CheckpointStrategy\n\n\nclass ClassicStorage(Storage):\n    """""" Model and metric persistence - classic implementation """"""\n\n    def __init__(self, model_config: ModelConfig, checkpoint_strategy: CheckpointStrategy, backend, streaming=None):\n        self.model_config = model_config\n        self.backend = backend\n        self.streaming = streaming if streaming is not None else []\n        self.checkpoint_strategy = checkpoint_strategy\n\n        self.cleaned = False\n\n    def last_epoch_idx(self):\n        """""" Return last checkpointed epoch idx for given configuration. Returns 0 if no results have been stored """"""\n        return self._persisted_last_epoch()\n\n    def reset(self, configuration: dict) -> None:\n        """"""\n        Whenever there was anything stored in the database or not, purge previous state and start\n        new training process from scratch.\n        """"""\n        self.clean(0)\n        self.backend.store_config(configuration)\n\n    def load(self, train_info: TrainingInfo) -> (dict, dict):\n        """"""\n        Resume learning process and return loaded hidden state dictionary\n        """"""\n        last_epoch = train_info.start_epoch_idx\n\n        model_state = torch.load(self.checkpoint_filename(last_epoch))\n        hidden_state = torch.load(self.checkpoint_hidden_filename(last_epoch))\n\n        self.checkpoint_strategy.restore(hidden_state)\n        train_info.restore(hidden_state)\n\n        return model_state, hidden_state\n\n    def get_metrics_frame(self):\n        """""" Get a frame of metrics from backend """"""\n        return self.backend.get_frame()\n\n    def clean(self, global_epoch_idx):\n        """""" Clean old checkpoints """"""\n        if self.cleaned:\n            return\n\n        self.cleaned = True\n        self.backend.clean(global_epoch_idx)\n\n        self._make_sure_dir_exists()\n\n        for x in os.listdir(self.model_config.checkpoint_dir()):\n            match = re.match(\'checkpoint_(\\\\d+)\\\\.data\', x)\n\n            if match:\n                idx = int(match[1])\n\n                if idx > global_epoch_idx:\n                    os.remove(os.path.join(self.model_config.checkpoint_dir(), x))\n\n            match = re.match(\'checkpoint_hidden_(\\\\d+)\\\\.data\', x)\n\n            if match:\n                idx = int(match[1])\n\n                if idx > global_epoch_idx:\n                    os.remove(os.path.join(self.model_config.checkpoint_dir(), x))\n\n            match = re.match(\'checkpoint_best_(\\\\d+)\\\\.data\', x)\n\n            if match:\n                idx = int(match[1])\n\n                if idx > global_epoch_idx:\n                    os.remove(os.path.join(self.model_config.checkpoint_dir(), x))\n\n    def checkpoint(self, epoch_info: EpochInfo, model: Model):\n        """""" When epoch is done, we persist the training state """"""\n        self.clean(epoch_info.global_epoch_idx - 1)\n\n        self._make_sure_dir_exists()\n\n        # Checkpoint latest\n        torch.save(model.state_dict(), self.checkpoint_filename(epoch_info.global_epoch_idx))\n\n        hidden_state = epoch_info.state_dict()\n        self.checkpoint_strategy.write_state_dict(hidden_state)\n\n        torch.save(hidden_state, self.checkpoint_hidden_filename(epoch_info.global_epoch_idx))\n\n        if epoch_info.global_epoch_idx > 1 and self.checkpoint_strategy.should_delete_previous_checkpoint(\n                                                   epoch_info.global_epoch_idx):\n            prev_epoch_idx = epoch_info.global_epoch_idx - 1\n\n            os.remove(self.checkpoint_filename(prev_epoch_idx))\n            os.remove(self.checkpoint_hidden_filename(prev_epoch_idx))\n\n        if self.checkpoint_strategy.should_store_best_checkpoint(epoch_info.global_epoch_idx, epoch_info.result):\n            best_checkpoint_idx = self.checkpoint_strategy.current_best_checkpoint_idx\n\n            if best_checkpoint_idx is not None:\n                os.remove(self.checkpoint_best_filename(best_checkpoint_idx))\n\n            torch.save(model.state_dict(), self.checkpoint_best_filename(epoch_info.global_epoch_idx))\n\n            self.checkpoint_strategy.store_best_checkpoint_idx(epoch_info.global_epoch_idx)\n\n        self.backend.store(epoch_info.result)\n\n    def streaming_callbacks(self) -> list:\n        """""" Lift of callbacks for live streaming results """"""\n        return self.streaming\n\n    ####################################################################################################################\n    # Filename helpers\n    def checkpoint_filename(self, epoch_idx) -> str:\n        """""" Return checkpoint filename for this model """"""\n        return self.model_config.checkpoint_dir(\'checkpoint_{:08}.data\'.format(epoch_idx))\n\n    def checkpoint_best_filename(self, epoch_idx) -> str:\n        """""" Return checkpoint filename for this model - best version """"""\n        return self.model_config.checkpoint_dir(\'checkpoint_best_{:08}.data\'.format(epoch_idx))\n\n    def checkpoint_hidden_filename(self, epoch_idx) -> str:\n        """""" Return checkpoint filename for this model - hidden state """"""\n        return self.model_config.checkpoint_dir(\'checkpoint_hidden_{:08}.data\'.format(epoch_idx))\n\n    ####################################################################################################################\n    # Internal interface\n    def _persisted_last_epoch(self) -> int:\n        """""" Return number of last epoch already calculated """"""\n        epoch_number = 0\n        self._make_sure_dir_exists()\n\n        for x in os.listdir(self.model_config.checkpoint_dir()):\n            match = re.match(\'checkpoint_(\\\\d+)\\\\.data\', x)\n            if match:\n                idx = int(match[1])\n\n                if idx > epoch_number:\n                    epoch_number = idx\n\n        return epoch_number\n\n    def _make_sure_dir_exists(self):\n        """""" Make sure directory exists """"""\n        filename = self.model_config.checkpoint_dir()\n        pathlib.Path(filename).mkdir(parents=True, exist_ok=True)\n\n\ndef create(model_config, backend, checkpoint_strategy, streaming=None):\n    """""" Vel factory function """"""\n    return ClassicStorage(\n        model_config=model_config,\n        backend=backend,\n        checkpoint_strategy=checkpoint_strategy,\n        streaming=streaming\n    )\n'"
vel/util/__init__.py,0,b''
vel/util/better.py,0,"b'import numpy as np\n\n\ndef better(old_value, new_value, mode):\n    """""" Check if new value is better than the old value""""""\n    if (old_value is None or np.isnan(old_value)) and (new_value is not None and not np.isnan(new_value)):\n        return True\n\n    if mode == \'min\':\n        return new_value < old_value\n    elif mode == \'max\':\n        return new_value > old_value\n    else:\n        raise RuntimeError(f""Mode \'{mode}\' value is not supported"")\n'"
vel/util/intepolate.py,0,"b'import numpy as np\nimport warnings\n\n\ndef interpolate_linear(start, end, steps):\n    """""" Interpolate series between start and end in given number of steps - linear interpolation """"""\n    return np.linspace(start, end, steps)\n\n\ndef interpolate_logscale(start, end, steps):\n    """""" Interpolate series between start and end in given number of steps - logscale interpolation """"""\n    if start <= 0.0:\n        warnings.warn(""Start of logscale interpolation must be positive!"")\n        start = 1e-5\n\n    return np.logspace(np.log10(float(start)), np.log10(float(end)), steps)\n\n\ndef interpolate_cosine_single(start, end, coefficient):\n    """""" Cosine interpolation """"""\n    cos_out = (np.cos(np.pi*coefficient) + 1) / 2.0\n    return end + (start - end) * cos_out\n\n\ndef interpolate_linear_single(start, end, coefficient):\n    """""" Cosine interpolation """"""\n    return start + (end - start) * coefficient\n\n\ndef interpolate_logscale_single(start, end, coefficient):\n    """""" Cosine interpolation """"""\n    return np.exp(np.log(start) + (np.log(end) - np.log(start)) * coefficient)\n\n\nINTERP_DICT = {\n    \'linear\': interpolate_linear,\n    \'logscale\': interpolate_logscale\n}\n\n\nINTERP_SINGLE_DICT = {\n    \'linear\': interpolate_linear_single,\n    \'logscale\': interpolate_logscale_single,\n    \'cosine\': interpolate_cosine_single\n}\n\n\ndef interpolate_series(start, end, steps, how=\'linear\'):\n    """""" Interpolate series between start and end in given number of steps """"""\n    return INTERP_DICT[how](start, end, steps)\n\n\ndef interpolate_single(start, end, coefficient, how=\'linear\'):\n    """""" Interpolate single value between start and end in given number of steps """"""\n    return INTERP_SINGLE_DICT[how](start, end, coefficient)\n\n'"
vel/util/math.py,0,"b'\n\ndef divide_ceiling(numerator, denominator):\n    """""" Determine the smallest number k such, that denominator * k >= numerator """"""\n    split_val = numerator // denominator\n    rest = numerator % denominator\n\n    if rest > 0:\n        return split_val + 1\n    else:\n        return split_val\n'"
vel/util/module_util.py,1,"b'""""""\nCode based on\nhttps://github.com/fastai/fastai/blob/master/fastai/model.py\n""""""\nimport torch.nn as nn\n\nimport collections\nimport itertools as it\n\n\ndef is_listy(x):\n    return isinstance(x, (list, tuple))\n\n\ndef model_children(module):\n    return module if isinstance(module, (list, tuple)) else list(module.children())\n\n\ndef apply_leaf(module, f):\n    if isinstance(module, nn.Module):\n        f(module)\n\n    children = model_children(module)\n\n    for l in children:\n        apply_leaf(l, f)\n\n\ndef module_apply_broadcast(m, broadcast_fn, args, kwargs):\n    if hasattr(m, broadcast_fn):\n        getattr(m, broadcast_fn)(*args, **kwargs)\n\n\ndef module_broadcast(m, broadcast_fn, *args, **kwargs):\n    """""" Call given function in all submodules with given parameters """"""\n    apply_leaf(m, lambda x: module_apply_broadcast(x, broadcast_fn, args, kwargs))\n\n\ndef set_train_mode(module):\n    # Only fix ones which we don\'t want to ""train""\n    if hasattr(module, \'running_mean\') and (getattr(module, \'bn_freeze\', False) or not getattr(module, \'trainable\', True)):\n        module.eval()\n    elif getattr(module, \'drop_freeze\', False) and hasattr(module, \'p\') and (\'drop\' in type(module).__name__.lower()):\n        module.eval()\n\n\ndef set_trainable_attr(module, trainable):\n    module.trainable = trainable\n\n\ndef set_requires_gradient(module, trainable):\n    for p in module.parameters():\n        p.requires_grad = trainable\n\n\ndef freeze_layer(module):\n    apply_leaf(module, lambda x: set_trainable_attr(x, trainable=False))\n    set_requires_gradient(module, trainable=False)\n\n\ndef unfreeze_layer(module):\n    apply_leaf(module, lambda x: set_trainable_attr(x, trainable=True))\n    set_requires_gradient(module, trainable=True)\n\n\ndef trainable_params_(m):\n    """""" Returns a list of trainable parameters in the model m. (i.e., those that require gradients.) """"""\n    if isinstance(m, collections.Sequence):\n        return [p for p in m if p.requires_grad]\n    else:\n        return [p for p in m.parameters() if p.requires_grad]\n\n\ndef chain_params(p):\n    if is_listy(p):\n        return list(it.chain(*[trainable_params_(o) for o in p]))\n    return trainable_params_(p)\n\n\ndef to_parameter_groups(layer_groups):\n    """""" Convert from list of layer groups into list of parameter settings for an optimizer """"""\n    return [{\'params\': chain_params(x)} for x in layer_groups]\n'"
vel/util/network.py,1,"b'import torch.nn as nn\n\n\nACTIVATION_DICT = {\n    \'relu\': nn.ReLU,\n    \'tanh\': nn.Tanh,\n    \'elu\': nn.ELU,\n    \'leaky_relu\': nn.LeakyReLU\n}\n\nNORMALIZATION_DICT = {\n    \'layer\': nn.LayerNorm,\n    \'layer-noscale\': lambda normalized_shape: nn.LayerNorm(normalized_shape, elementwise_affine=False),\n    \'batch1d\': nn.BatchNorm1d,\n    \'batch2d\': nn.BatchNorm2d\n}\n\n\ndef activation(name):\n    """""" Return activation block corresponding given name """"""\n    return ACTIVATION_DICT[name]\n\n\ndef normalization(name):\n    """""" Return activation block corresponding given name """"""\n    return NORMALIZATION_DICT[name]\n\n\ndef convolution_size_equation(size, filter_size, padding, stride):\n    """""" Output size of convolutional layer """"""\n    return (size - filter_size + 2 * padding) // stride + 1\n\n\ndef convolutional_layer_series(initial_size, layer_sequence):\n    """""" Execute a series of convolutional layer transformations to the size number """"""\n    size = initial_size\n\n    for filter_size, padding, stride in layer_sequence:\n        size = convolution_size_equation(size, filter_size, padding, stride)\n\n    return size\n'"
vel/util/random.py,1,"b'import numpy as np\nimport random\nimport torch\n\n\ndef set_seed(seed: int):\n    """""" Set random seed for python, numpy and pytorch RNGs """"""\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.random.manual_seed(seed)\n'"
vel/util/situational.py,0,"b'import typing\n\n\ndef process_environment_settings(default_dictionary: dict, settings: typing.Optional[dict]=None,\n                                 presets: typing.Optional[dict]=None):\n    """""" Process a dictionary of env settings """"""\n    settings = settings if settings is not None else {}\n    presets = presets if presets is not None else {}\n\n    env_keys = sorted(set(default_dictionary.keys()) | set(presets.keys()))\n\n    result_dict = {}\n\n    for key in env_keys:\n        if key in default_dictionary:\n            new_dict = default_dictionary[key].copy()\n        else:\n            new_dict = {}\n\n        new_dict.update(settings)\n\n        if key in presets:\n            new_dict.update(presets[key])\n\n        result_dict[key] = new_dict\n\n    return result_dict\n\n'"
vel/util/summary.py,9,"b'""""""\nCode based on: https://github.com/sksq96/pytorch-summary/blob/master/torchsummary/torchsummary.py\n""""""\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom collections import OrderedDict\n\n\ndef summary(model, input_size):\n    """""" Print summary of the model """"""\n    def register_hook(module):\n        def hook(module, input, output):\n            class_name = str(module.__class__).split(\'.\')[-1].split(""\'"")[0]\n            module_idx = len(summary)\n\n            m_key = \'%s-%i\' % (class_name, module_idx + 1)\n            summary[m_key] = OrderedDict()\n            summary[m_key][\'input_shape\'] = list(input[0].size())\n            summary[m_key][\'input_shape\'][0] = -1\n            if isinstance(output, (list, tuple)):\n                summary[m_key][\'output_shape\'] = [[-1] + list(o.size())[1:] for o in output]\n            else:\n                summary[m_key][\'output_shape\'] = list(output.size())\n                summary[m_key][\'output_shape\'][0] = -1\n\n            params = 0\n            if hasattr(module, \'weight\') and hasattr(module.weight, \'size\'):\n                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n                summary[m_key][\'trainable\'] = module.weight.requires_grad\n            if hasattr(module, \'bias\') and hasattr(module.bias, \'size\'):\n                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n            summary[m_key][\'nb_params\'] = params\n\n        if (not isinstance(module, nn.Sequential) and\n                not isinstance(module, nn.ModuleList) and\n                not (module == model)):\n            hooks.append(module.register_forward_hook(hook))\n\n    if torch.cuda.is_available():\n        dtype = torch.cuda.FloatTensor\n        model = model.cuda()\n    else:\n        dtype = torch.FloatTensor\n        model = model.cpu()\n\n    # check if there are multiple inputs to the network\n    if isinstance(input_size[0], (list, tuple)):\n        x = [Variable(torch.rand(2, *in_size)).type(dtype) for in_size in input_size]\n    else:\n        x = Variable(torch.rand(2, *input_size)).type(dtype)\n\n    # print(type(x[0]))\n    # create properties\n    summary = OrderedDict()\n    hooks = []\n    # register hook\n    model.apply(register_hook)\n    # make a forward pass\n    # print(x.shape)\n    model(x)\n    # remove these hooks\n    for h in hooks:\n        h.remove()\n\n    print(\'----------------------------------------------------------------\')\n    line_new = \'{:>20}  {:>25} {:>15}\'.format(\'Layer (type)\', \'Output Shape\', \'Param #\')\n    print(line_new)\n    print(\'================================================================\')\n    total_params = 0\n    trainable_params = 0\n    for layer in summary:\n        # input_shape, output_shape, trainable, nb_params\n        line_new = \'{:>20}  {:>25} {:>15}\'.format(layer, str(summary[layer][\'output_shape\']),\n                                                  \'{0:,}\'.format(summary[layer][\'nb_params\']))\n        total_params += summary[layer][\'nb_params\']\n        if \'trainable\' in summary[layer]:\n            if summary[layer][\'trainable\'] == True:\n                trainable_params += summary[layer][\'nb_params\']\n        print(line_new)\n    print(\'================================================================\')\n    print(\'Total params: {0:,}\'.format(total_params))\n    print(\'Trainable params: {0:,}\'.format(trainable_params))\n    print(\'Non-trainable params: {0:,}\'.format(total_params - trainable_params))\n    print(\'----------------------------------------------------------------\')\n    # return summary'"
vel/util/tensor_accumulator.py,1,"b'import collections\n\nimport torch\n\n\nclass TensorAccumulator:\n    """""" Buffer for tensors that will be stacked together """"""\n    def __init__(self):\n        self.accumulants = collections.defaultdict(list)\n\n    def add(self, name, tensor):\n        self.accumulants[name].append(tensor)\n\n    def result(self):\n        """""" Concatenate accumulated tensors """"""\n        return {k: torch.stack(v) for k, v in self.accumulants.items()}\n'"
vel/util/tensor_util.py,2,"b'import torch\n\n\ndef one_hot_encoding(input_tensor, num_labels):\n    """""" One-hot encode labels from input """"""\n    xview = input_tensor.view(-1, 1).to(torch.long)\n\n    onehot = torch.zeros(xview.size(0), num_labels, device=input_tensor.device, dtype=torch.float)\n    onehot.scatter_(1, xview, 1)\n    return onehot.view(list(input_tensor.shape) + [-1])\n\n\ndef merge_first_two_dims(tensor):\n    """""" Reshape tensor to merge first two dimensions """"""\n    shape = tensor.shape\n    batch_size = shape[0] * shape[1]\n    new_shape = tuple([batch_size] + list(shape[2:]))\n    return tensor.view(new_shape)\n'"
vel/util/visdom.py,0,"b'import attr\nimport itertools as it\n\n\n@attr.s(auto_attribs=True)\nclass VisdomSettings:\n    """""" Settings for connecting to the visdom server """"""\n    stream_lr: bool = False\n    server: str = \'http://localhost\'\n    endpoint: str = \'events\'\n    port: int = 8097\n\n\ndef _column_original_name(name):\n    """""" Return original name of the metric """"""\n    if \':\' in name:\n        return name.split(\':\')[-1]\n    else:\n        return name\n\n\ndef visdom_push_metrics(vis, metrics):\n    """""" Push metrics to visdom """"""\n    visdom_send_metrics(vis, metrics, \'replace\')\n\n\ndef visdom_send_metrics(vis, metrics, update=\'replace\'):\n    """""" Send set of metrics to visdom """"""\n    visited = {}\n\n    sorted_metrics = sorted(metrics.columns, key=_column_original_name)\n    for metric_basename, metric_list in it.groupby(sorted_metrics, key=_column_original_name):\n        metric_list = list(metric_list)\n\n        for metric in metric_list:\n            if vis.win_exists(metric_basename) and (not visited.get(metric, False)):\n                update = update\n            elif not vis.win_exists(metric_basename):\n                update = None\n            else:\n                update = \'append\'\n\n            vis.line(\n                metrics[metric].values,\n                metrics.index.values,\n                win=metric_basename,\n                name=metric,\n                opts={\n                    \'title\': metric_basename,\n                    \'showlegend\': True\n                },\n                update=update\n            )\n\n            if metric_basename != metric and len(metric_list) > 1:\n                if vis.win_exists(metric):\n                    update = update\n                else:\n                    update = None\n\n                vis.line(\n                    metrics[metric].values,\n                    metrics.index.values,\n                    win=metric,\n                    name=metric,\n                    opts={\n                        \'title\': metric,\n                        \'showlegend\': True\n                    },\n                    update=update\n                )\n\n\ndef visdom_append_metrics(vis, metrics, first_epoch=False):\n    """""" Append metrics to visdom """"""\n    visited = {}\n\n    sorted_metrics = sorted(metrics.columns, key=_column_original_name)\n    for metric_basename, metric_list in it.groupby(sorted_metrics, key=_column_original_name):\n        metric_list = list(metric_list)\n\n        for metric in metric_list:\n            if vis.win_exists(metric_basename) and (not visited.get(metric, False)) and first_epoch:\n                update = \'replace\'\n            elif not vis.win_exists(metric_basename):\n                update = None\n            else:\n                update = \'append\'\n\n            vis.line(\n                metrics[metric].values,\n                metrics.index.values,\n                win=metric_basename,\n                name=metric,\n                opts={\n                    \'title\': metric_basename,\n                    \'showlegend\': True\n                },\n                update=update\n            )\n\n            if metric_basename != metric and len(metric_list) > 1:\n                if vis.win_exists(metric) and first_epoch:\n                    update = \'replace\'\n                elif not vis.win_exists(metric):\n                    update = None\n                else:\n                    update = \'append\'\n\n                vis.line(\n                    metrics[metric].values,\n                    metrics.index.values,\n                    win=metric,\n                    name=metric,\n                    opts={\n                        \'title\': metric,\n                        \'showlegend\': True\n                    },\n                    update=update\n                )\n\n'"
vel/api/data/__init__.py,0,b'from .augmentation import Augmentation\nfrom .dataflow import DataFlow\nfrom .image_ops import *'
vel/api/data/augmentation.py,0,"b'\n\nclass Augmentation:\n    """""" Base class for all data augmentations """"""\n    def __init__(self, mode=\'x\', tags=None):\n        self.mode = mode\n        self.tags = tags or [\'train\', \'val\', \'test\']\n\n    def __call__(self, *args):\n        """""" Do the transformation """"""\n        print(self)\n        raise NotImplementedError\n\n    def denormalize(self, *args):\n        """""" Operation reverse to normalization """"""\n        if len(args) == 1:\n            return args[0]\n        else:\n            return args\n'"
vel/api/data/dataflow.py,1,"b'import torch.utils.data as data\n\n\nclass DataFlow(data.Dataset):\n    """""" A dataset wrapping underlying data source with transformations """"""\n    def __init__(self, dataset, transformations, tag):\n        self.dataset = dataset\n\n        if transformations is None:\n            self.transformations = []\n        else:\n            self.transformations = [t for t in transformations if tag in t.tags]\n\n        self.tag = tag\n\n    def get_raw(self, index):\n        return self.dataset[index]\n\n    def __getitem__(self, index):\n        raw_x, raw_y = self.dataset[index]\n\n        for t in self.transformations:\n            if t.mode == \'x\':\n                raw_x = t(raw_x)\n            elif t.mode == \'y\':\n                raw_y = t(raw_y)\n            elif t.mode == \'both\':\n                raw_x, raw_y = t(raw_x, raw_y)\n            else:\n                raise RuntimeError(f""Mode {t.mode} not recognized"")\n\n        return raw_x, raw_y\n\n    def denormalize(self, datum, mode=\'x\'):\n        for t in self.transformations[::-1]:\n            if t.mode == mode:\n                datum = t.denormalize(datum)\n\n        return datum\n\n    def __len__(self):\n        return len(self.dataset)\n'"
vel/api/data/image_ops.py,0,"b'import cv2\nimport math\nimport numpy as np\n\n\ndef crop_square(im, r, c, sz):\n    \'\'\'\n    crop image into a square of size sz,\n    \'\'\'\n    return im[r:r+sz, c:c+sz]\n\n\ndef crop(im, r, c, sz_h, sz_w):\n    \'\'\'\n    crop image into a square of size sz,\n    \'\'\'\n    return im[r:r+sz_h, c:c+sz_w]\n\n\ndef center_crop(im, min_sz=None):\n    """""" Returns a center crop of an image""""""\n    # return F.center_crop(im, min_sz)\n    r,c,*_ = im.shape\n    if min_sz is None: min_sz = min(r,c)\n    start_r = math.ceil((r-min_sz)/2)\n    start_c = math.ceil((c-min_sz)/2)\n    return crop_square(im, start_r, start_c, min_sz)\n\n\ndef scale_to(x, ratio, targ):\n    \'\'\'Calculate dimension of an image during scaling with aspect ratio\'\'\'\n    return max(math.floor(x*ratio), targ)\n\n\ndef scale_min(im, targ, interpolation=cv2.INTER_AREA):\n    """""" Scales the image so that the smallest axis is of size targ.\n\n    Arguments:\n        im (array): image\n        targ (int): target size\n    """"""\n    r,c,*_ = im.shape\n\n    ratio = targ/min(r,c)\n\n    sz = (scale_to(c, ratio, targ), scale_to(r, ratio, targ))\n\n    # return im.resize(sz, resample=interpolation)\n    return cv2.resize(im, sz, interpolation=interpolation)\n\n\n# def rotate_img(img, deg, interpolation=Image.BICUBIC):\n#     """""" Rotate image by given angle """"""\n#     return img.rotate(deg, resample=interpolation)\n\ndef rotate_img(im, deg, mode=cv2.BORDER_CONSTANT, interpolation=cv2.INTER_AREA):\n    """""" Rotates an image by deg degrees\n\n    Arguments:\n        deg (float): degree to rotate.\n    """"""\n    r,c,*_ = im.shape\n    M = cv2.getRotationMatrix2D((c//2,r//2),deg,1)\n    return cv2.warpAffine(im,M,(c,r), borderMode=mode, flags=cv2.WARP_FILL_OUTLIERS+interpolation)\n\n\ndef pad(img, pad, mode=cv2.BORDER_REFLECT):\n    return cv2.copyMakeBorder(img, pad, pad, pad, pad, mode)\n\n\ndef mode_to_cv2(mode=\'constant\'):\n    if mode == \'constant\':\n        return cv2.BORDER_CONSTANT\n    if mode == \'reflect\':\n        return cv2.BORDER_REFLECT\n\n    raise ValueError(f""Invalid mode {mode}"")\n\n\ndef lighting(im, b, c):\n    \'\'\' adjusts image\'s balance and contrast\'\'\'\n    if b==0 and c==1: return im\n    mu = np.average(im)\n    return np.clip((im-mu)*c+mu+b,0.,1.).astype(np.float32)\n'"
vel/api/metrics/__init__.py,0,"b'from .base_metric import BaseMetric\nfrom .averaging_metric import AveragingMetric, AveragingNamedMetric, AveragingSupervisedMetric\nfrom .value_metric import ValueMetric\nfrom .summing_metric import SummingMetric, SummingNamedMetric\n'"
vel/api/metrics/averaging_metric.py,0,"b'import numpy as np\n\nfrom .base_metric import BaseMetric\n\n\nclass AveragingMetric(BaseMetric):\n    """""" Base class for metrics that simply calculate the average over the epoch """"""\n    def __init__(self, name):\n        super().__init__(name)\n\n        self.storage = []\n\n    def calculate(self, batch_info):\n        """""" Calculate value of a metric """"""\n        value = self._value_function(batch_info)\n        self.storage.append(value)\n\n    def _value_function(self, batch_info):\n        raise NotImplementedError\n\n    def reset(self):\n        """""" Reset value of a metric """"""\n        self.storage = []\n\n    def value(self):\n        """""" Return current value for the metric """"""\n        return float(np.mean(self.storage))\n\n\nclass AveragingNamedMetric(AveragingMetric):\n    """""" Super simple averaging metric that just takes a value from dictionary and averages it over samples """"""\n    def __init__(self, name):\n        super().__init__(name)\n\n    def _value_function(self, batch_info):\n        return batch_info[self.name]\n\n\nclass AveragingSupervisedMetric(BaseMetric):\n    """""" Base class for metrics that simply calculate the average over the epoch """"""\n    def __init__(self, name):\n        super().__init__(name)\n\n        self.storage = []\n\n    def calculate(self, batch_info):\n        """""" Calculate value of a metric """"""\n        value = self._value_function(batch_info[\'data\'], batch_info[\'target\'], batch_info[\'output\'])\n        self.storage.append(value)\n\n    def _value_function(self, x_input, y_true, y_pred):\n        raise NotImplementedError\n\n    def reset(self):\n        """""" Reset value of a metric """"""\n        self.storage = []\n\n    def value(self):\n        """""" Return current value for the metric """"""\n        return np.mean(self.storage)\n'"
vel/api/metrics/base_metric.py,0,"b'from vel.api import TrainingInfo\n\n\nclass BaseMetric:\n    """""" Base class for all the metrics """"""\n\n    def __init__(self, name):\n        self.name = name\n\n    def calculate(self, batch_info):\n        """""" Calculate value of a metric based on supplied data """"""\n        raise NotImplementedError\n\n    def reset(self):\n        """""" Reset value of a metric """"""\n        raise NotImplementedError\n\n    def value(self):\n        """""" Return current value for the metric """"""\n        raise NotImplementedError\n\n    def write_state_dict(self, training_info: TrainingInfo, hidden_state_dict: dict) -> None:\n        """""" Potentially store some metric state to the checkpoint """"""\n        pass\n\n    def load_state_dict(self, training_info: TrainingInfo, hidden_state_dict: dict) -> None:\n        """""" Potentially load some metric state from the checkpoint """"""\n        pass\n'"
vel/api/metrics/summing_metric.py,0,"b'from .base_metric import BaseMetric\n\n\nclass SummingMetric(BaseMetric):\n    """""" Base class for metrics that simply calculate the sum over the epoch """"""\n    def __init__(self, name, reset_value=True):\n        super().__init__(name)\n\n        self.reset_value = reset_value\n        self.buffer = 0\n\n    def calculate(self, batch_info):\n        """""" Calculate value of a metric """"""\n        value = self._value_function(batch_info)\n        self.buffer += value\n\n    def _value_function(self, batch_info):\n        raise NotImplementedError\n\n    def reset(self):\n        """""" Reset value of a metric """"""\n\n        if self.reset_value:\n            self.buffer = 0\n\n    def value(self):\n        """""" Return current value for the metric """"""\n        return self.buffer\n\n\nclass SummingNamedMetric(SummingMetric):\n    """""" Super simple summing metric that just takes a value from dictionary and sums it over samples """"""\n    def __init__(self, name, reset_value=True):\n        super().__init__(name, reset_value=reset_value)\n\n    def _value_function(self, batch_info):\n        return batch_info[self.name].item()\n'"
vel/api/metrics/value_metric.py,0,"b'from .base_metric import BaseMetric\n\n\nclass ValueMetric(BaseMetric):\n    """""" Base class for metrics that don\'t have state and just calculate a simple value """"""\n\n    def __init__(self, name):\n        super().__init__(name)\n\n        self._metric_value = None\n\n    def calculate(self, batch_info):\n        """""" Calculate value of a metric based on supplied data """"""\n        self._metric_value = self._value_function(batch_info)\n\n    def reset(self):\n        """""" Reset value of a metric """"""\n        pass\n\n    def value(self):\n        """""" Return current value for the metric """"""\n        return self._metric_value\n\n    def _value_function(self, batch_info):\n        raise NotImplementedError\n\n'"
vel/augmentations/tta/__init__.py,0,b''
vel/augmentations/tta/train_tta.py,3,"b'# import torch\n# import torch.utils.data as tdata\n#\n# import vel.api.data as wdata\n#\n#\n# class TrainTTALoader:\n#     def __init__(self, n_augmentations, batch_size, data_source, augmentations, num_workers):\n#         self.n_augmentations = n_augmentations\n#         self.data_source = data_source\n#         self.augmentations = augmentations\n#\n#         self.val_ds = wdata.DataFlow(self.data_source, augmentations, tag=\'val\')\n#         self.train_ds = wdata.DataFlow(self.data_source, augmentations, tag=\'train\')\n#\n#         self.val_loader = tdata.DataLoader(\n#             self.val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers\n#         )\n#\n#         self.train_loader = tdata.DataLoader(\n#             self.train_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers\n#         )\n#\n#     def __len__(self):\n#         return (1 + self.n_augmentations) * len(self.val_loader)\n#\n#     def __iter__(self):\n#         iterlist = [iter(self.val_loader)]\n#\n#         for _ in range(self.n_augmentations):\n#             iterlist.append(iter(self.train_loader))\n#\n#         for _ in range(len(self.val_loader)):\n#             for iterator in iterlist:\n#                 yield next(iterator)\n#\n#\n# class TrainTTAAccumulator:\n#     def __init__(self, metric_accumulator, n_augmentations, data_source):\n#         self.metric_accumulator = metric_accumulator\n#\n#         self.source_elements = len(data_source)\n#         self.n_augmentations = n_augmentations\n#\n#         self.data = None\n#         self.target = None\n#\n#         self.accumulated_output = []\n#         self.accumulated_context = []\n#\n#         self.index = 0\n#\n#     # def calculate(self, data, target, output, context):\n#     def calculate(self, data_dict):\n#         """""" Accumulate results """"""\n#         data = data_dict[\'data\']\n#         target = data_dict[\'target\']\n#         output = data_dict[\'output\']\n#\n#         if self.index == 0:\n#             self.data = data\n#\n#         self.target = target\n#\n#         self.accumulated_output.append(output)\n#         self.accumulated_context.append(context)\n#\n#         self.index += 1\n#\n#         if self.index == (1 + self.n_augmentations):\n#             new_output = torch.mean(torch.stack(self.accumulated_output, dim=-1), dim=-1)\n#             new_context = {\n#                 k: torch.mean(torch.stack([c[k] for c in self.accumulated_context], dim=-1), dim=-1) for k in context.keys()\n#             }\n#\n#             self.metric_accumulator.calculate(self.data, self.target, new_output, new_context)\n#\n#             self.index = 0\n#             self.data = None\n#             self.target = None\n#             self.accumulated_output = []\n#             self.accumulated_context = []\n#\n#\n# class TrainTTA:\n#     """""" Test time augmentation that generates additional samples according to the training set augmentations """"""\n#     def __init__(self, n_augmentations):\n#         self.n_augmentations = n_augmentations\n#\n#     def loader(self, data_source, augmentations, batch_size, num_workers):\n#         """""" Return loader for the test-time-augmentation set """"""\n#         return TrainTTALoader(\n#             n_augmentations=self.n_augmentations,\n#             batch_size=batch_size,\n#             data_source=data_source,\n#             augmentations=augmentations,\n#             num_workers=num_workers\n#         )\n#\n#     def accumulator(self, metric_accumulator, val_source):\n#         """""" Reset internal state """"""\n#         return TrainTTAAccumulator(metric_accumulator, self.n_augmentations, val_source)\n#\n#\n# def create(n_augmentations):\n#     return TrainTTA(n_augmentations)\n'"
vel/commands/rnn/__init__.py,0,b''
vel/commands/rnn/generate_text.py,4,"b'import numpy as np\nimport tqdm\n\nimport torch\nimport torch.nn.functional as F\nimport torch.distributions as dist\n\nfrom vel.api import TrainingInfo\n\n\nclass GenerateTextCommand:\n    """""" Generate text using RNN model """"""\n\n    def __init__(self, model_config, model_factory, source, storage, start_letter, length, temperature):\n        self.model_config = model_config\n        self.model_factory = model_factory\n        self.source = source\n        self.storage = storage\n        self.start_letter = start_letter\n        self.length = length\n        self.temperature = temperature\n\n    @torch.no_grad()\n    def run(self):\n        device = self.model_config.torch_device()\n        model = self.model_factory.instantiate().to(device)\n\n        start_epoch = self.storage.last_epoch_idx()\n\n        training_info = TrainingInfo(\n            start_epoch_idx=start_epoch,\n            run_name=self.model_config.run_name,\n        )\n\n        model_state, hidden_state = self.storage.load(training_info)\n        model.load_state_dict(model_state)\n\n        model.eval()\n\n        current_char = self.start_letter\n        current_char_encoded = self.source.encode_character(self.start_letter)\n\n        generated_text = [current_char]\n\n        state = model.zero_state(1).to(device)\n\n        char_tensor = torch.from_numpy(np.array([current_char_encoded])).view(1, 1).to(device)\n\n        for _ in tqdm.trange(self.length):\n            prob_logits, state = model.forward_state(char_tensor, state)\n\n            # Apply temperature to the logits\n            prob_logits = F.log_softmax(prob_logits.view(-1).div(self.temperature), dim=0)\n\n            distribution = dist.Categorical(logits=prob_logits)\n\n            char_tensor = distribution.sample().view(1, 1)\n            current_char_encoded = char_tensor.item()\n\n            if current_char_encoded == 0:\n                # End of sequence marker\n                break\n\n            current_char = self.source.decode_character(current_char_encoded)\n\n            generated_text.append(current_char)\n\n        print(""============================ START GENERATED TEXT ================================================"")\n        print(\'\'.join(generated_text))\n        print(""============================ END GENERATED TEXT ================================================"")\n\n\ndef create(model_config, model, source, storage, start_letter, length, temperature):\n    """""" Vel factory function """"""\n    return GenerateTextCommand(model_config, model, source, storage, start_letter, length, temperature)\n'"
vel/internals/tests/__init__.py,0,b''
vel/internals/tests/fixture_a.py,0,"b""def create(a, b):\n    return {'a': a, 'b': b}\n"""
vel/internals/tests/fixture_b.py,0,"b""def create(one, d):\n    return {'one': one, 'd': d}\n"""
vel/internals/tests/test_parser.py,0,"b'import pytest\n\nimport vel.internals.parser as v\n\n\n@pytest.fixture\ndef setup_parser():\n    """""" Set up test environment """"""\n    v.Parser.register()\n\n\ndef test_variable_parsing(setup_parser):\n    yaml_text = """"""\nx:\n  y: !param xxx \n""""""\n\n    yaml_contents = v.Parser.parse(yaml_text)\n\n    assert isinstance(yaml_contents[\'x\'][\'y\'], v.Parameter)\n    assert yaml_contents[\'x\'][\'y\'].name == \'xxx\'\n\n\ndef test_env_variable_parsing(setup_parser):\n    yaml_text = """"""\nx:\n  y: !env xxx \n""""""\n\n    yaml_contents = v.Parser.parse(yaml_text)\n\n    assert isinstance(yaml_contents[\'x\'][\'y\'], v.EnvironmentVariable)\n    assert yaml_contents[\'x\'][\'y\'].name == \'xxx\'\n\n\ndef test_variable_default_values(setup_parser):\n    yaml_text = """"""\nx:\n  y: !param xxx = 5\n""""""\n\n    yaml_contents = v.Parser.parse(yaml_text)\n\n    assert yaml_contents[\'x\'][\'y\'].default_value == 5\n\n    yaml_text = """"""\nx:\n  y: !param xxx = abc\n""""""\n\n    yaml_contents = v.Parser.parse(yaml_text)\n\n    assert yaml_contents[\'x\'][\'y\'].default_value == \'abc\'\n\n    yaml_text = """"""\nx:\n  y: !param xxx = \'abc def\'\n""""""\n\n    yaml_contents = v.Parser.parse(yaml_text)\n\n    assert yaml_contents[\'x\'][\'y\'].default_value == \'abc def\'\n\n    yaml_text = """"""\nx:\n  y: !param xxx = null\n""""""\n\n    yaml_contents = v.Parser.parse(yaml_text)\n\n    assert yaml_contents[\'x\'][\'y\'].default_value == None\n\n    yaml_text = """"""\nx:\n  y: !param xxx\n""""""\n\n    yaml_contents = v.Parser.parse(yaml_text)\n\n    assert yaml_contents[\'x\'][\'y\'].default_value == v.DUMMY_VALUE\n\n\ndef test_parse_equality():\n    assert v.Parser.parse_equality(""x=5"") == (\'x\', 5)\n    assert v.Parser.parse_equality(""  x   =   5  "") == (\'x\', 5)\n\n    with pytest.raises(AssertionError):\n        v.Parser.parse_equality(""  1   =   2  "")\n\n    assert v.Parser.parse_equality(""  \'asd\'   =   \'www zzz\'  "") == (\'asd\', \'www zzz\')\n    assert v.Parser.parse_equality(""  \'asd\'   =   \'www=zzz\'  "") == (\'asd\', \'www=zzz\')\n'"
vel/internals/tests/test_provider.py,0,"b'import os\nimport pytest\n\nimport vel.internals.provider as v\nimport vel.internals.parser as p\nimport vel.exceptions as e\n\n\ndef data_function(a, b):\n    return a + b\n\n\ndef test_simple_instantiation():\n    provider = v.Provider({\n        \'a\': 1,\n        \'b\': 2,\n    })\n\n    assert provider.instantiate_from_data(1) == 1\n    assert provider.instantiate_from_data(""abc"") == ""abc""\n    assert provider.instantiate_from_data([1, 2, 3]) == [1, 2, 3]\n    assert provider.instantiate_from_data({""a"": ""a"", ""b"": ""b""}) == {""a"": ""a"", ""b"": ""b""}\n\n\ndef test_instantiate_function_call():\n    provider = v.Provider({\n        \'a\': 1,\n        \'b\': 2,\n    })\n\n    assert provider.resolve_and_call(data_function) == 3\n    assert provider.resolve_and_call(data_function, extra_env={\'b\': 4}) == 5\n\n\ndef test_simple_injection():\n    provider = v.Provider({\n        \'a\': 1,\n        \'b\': 2,\n        \'one\': {\n            \'name\': \'vel.internals.tests.fixture_a\'\n        },\n\n        \'two\': {\n            \'name\': \'vel.internals.tests.fixture_a\',\n            \'a\': 5,\n            \'b\': 6\n        },\n\n        \'three\': {\n            \'name\': \'vel.internals.tests.fixture_b\',\n            \'d\': \'d\'\n        }\n    })\n\n    one = provider.instantiate_by_name(\'one\')\n\n    assert isinstance(one, dict)\n    assert one[\'a\'] == 1\n    assert one[\'b\'] == 2\n\n    two = provider.instantiate_by_name(\'two\')\n\n    assert isinstance(two, dict)\n    assert two[\'a\'] == 5\n    assert two[\'b\'] == 6\n\n    three = provider.instantiate_by_name(\'three\')\n    assert isinstance(three, dict)\n    assert id(three[\'one\']) == id(one)\n    assert id(three[\'one\']) != id(two)\n    assert three[\'d\'] == \'d\'\n\n\ndef test_parameter_resolution():\n    os.environ[\'TEST_VAR\'] = \'10\'\n\n    provider = v.Provider({\n        \'a\': 1,\n        \'b\': p.Parameter(""xxx""),\n        \'one\': {\n            \'name\': \'vel.internals.tests.fixture_a\'\n        },\n        \'two\': {\n            \'name\': \'vel.internals.tests.fixture_a\',\n            \'b\': p.Parameter(\'yyy\')\n        },\n\n        \'three\': {\n            \'name\': \'vel.internals.tests.fixture_a\',\n            \'b\': p.Parameter(\'yyy\', 7)\n        },\n\n        \'four\': {\n            \'name\': \'vel.internals.tests.fixture_a\',\n            \'b\': p.EnvironmentVariable(\'TEST_VAR\')\n        },\n\n    }, parameters={\'xxx\': 5})\n\n    one = provider.instantiate_by_name(\'one\')\n\n    assert one[\'b\'] == 5\n\n    with pytest.raises(e.VelException):\n        provider.instantiate_by_name(\'two\')\n\n    three = provider.instantiate_by_name(\'three\')\n\n    assert three[\'b\'] == 7\n\n    four = provider.instantiate_by_name(\'four\')\n\n    assert four[\'b\'] == \'10\'\n\n\ndef test_render_configuration():\n    os.environ[\'TEST_VAR\'] = \'10\'\n\n    provider = v.Provider({\n        \'a\': 1,\n        \'b\': p.Parameter(""xxx""),\n        \'one\': {\n            \'name\': \'vel.internals.tests.fixture_a\'\n        },\n        \'two\': {\n            \'name\': \'vel.internals.tests.fixture_a\',\n            \'b\': p.Parameter(\'yyy\', 5)\n        },\n\n        \'three\': {\n            \'name\': \'vel.internals.tests.fixture_a\',\n            \'b\': p.Parameter(\'yyy\', 7)\n        },\n\n        \'four\': {\n            \'name\': \'vel.internals.tests.fixture_a\',\n            \'b\': p.EnvironmentVariable(\'TEST_VAR\')\n        },\n\n    }, parameters={\'xxx\': 5})\n\n    configuration = provider.render_configuration()\n\n    assert configuration == {\n        \'a\': 1,\n        \'b\': 5,\n        \'one\': {\n            \'name\': \'vel.internals.tests.fixture_a\'\n        },\n        \'two\': {\n            \'name\': \'vel.internals.tests.fixture_a\',\n            \'b\': 5\n        },\n\n        \'three\': {\n            \'name\': \'vel.internals.tests.fixture_a\',\n            \'b\': 7\n        },\n\n        \'four\': {\n            \'name\': \'vel.internals.tests.fixture_a\',\n            \'b\': \'10\'\n        },\n    }\n'"
vel/models/imagenet/__init__.py,0,b''
vel/models/imagenet/resnet34.py,2,"b'import torchvision.models.resnet as m\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport vel.modules.layers as l\nimport vel.util.module_util as mu\n\nfrom vel.api import SupervisedModel, ModelFactory\n\n\n# Because of concat pooling it\'s 2x 512\nNET_OUTPUT = 1024\n\n\nclass Resnet34(SupervisedModel):\n    """""" Resnet34 network model """"""\n\n    def __init__(self, fc_layers=None, dropout=None, pretrained=True):\n        super().__init__()\n\n        # Store settings, maybe someone will be interested to see them\n        self.fc_layers = fc_layers\n        self.dropout = dropout\n        self.pretrained = pretrained\n\n        self.head_layers = 8\n        self.group_cut_layers = (6, 10)\n\n        # Load backbbone\n        backbone = m.resnet34(pretrained=pretrained)\n\n        # If fc layers is set, let\'s put custom head\n        if fc_layers:\n            # Take out the old head and let\'s put the new head\n            valid_children = list(backbone.children())[:-2]\n\n            valid_children.extend([\n                l.AdaptiveConcatPool2d(),\n                l.Flatten()\n            ])\n\n            layer_inputs = [NET_OUTPUT] + fc_layers[:-1]\n\n            dropout = dropout or [None] * len(fc_layers)\n\n            for idx, (layer_input, layet_output, layer_dropout) in enumerate(zip(layer_inputs, fc_layers, dropout)):\n                valid_children.append(nn.BatchNorm1d(layer_input))\n\n                if layer_dropout:\n                    valid_children.append(nn.Dropout(layer_dropout))\n\n                valid_children.append(nn.Linear(layer_input, layet_output))\n\n                if idx == len(fc_layers) - 1:\n                    # Last layer\n                    valid_children.append(nn.LogSoftmax(dim=1))\n                else:\n                    valid_children.append(nn.ReLU())\n\n            final_model = nn.Sequential(*valid_children)\n        else:\n            final_model = backbone\n\n        self.model = final_model\n\n    def freeze(self, number=None):\n        """""" Freeze given number of layers in the model """"""\n        if number is None:\n            number = self.head_layers\n\n        for idx, child in enumerate(self.model.children()):\n            if idx < number:\n                mu.freeze_layer(child)\n\n    def unfreeze(self):\n        """""" Unfreeze model layers """"""\n        for idx, child in enumerate(self.model.children()):\n            mu.unfreeze_layer(child)\n\n    def get_layer_groups(self):\n        """""" Return layers grouped """"""\n        g1 = list(self.model[:self.group_cut_layers[0]])\n        g2 = list(self.model[self.group_cut_layers[0]:self.group_cut_layers[1]])\n        g3 = list(self.model[self.group_cut_layers[1]:])\n        return [g1, g2, g3]\n\n    def forward(self, x):\n        """""" Calculate model value """"""\n        return self.model(x)\n\n    def loss_value(self, x_data, y_true, y_pred):\n        """""" Calculate value of the loss function """"""\n        return F.nll_loss(y_pred, y_true)\n\n    def metrics(self):\n        """""" Set of metrics for this model """"""\n        from vel.metrics.loss_metric import Loss\n        from vel.metrics.accuracy import Accuracy\n        return [Loss(), Accuracy()]\n\n\ndef create(fc_layers=None, dropout=None, pretrained=True):\n    """""" Vel factory function """"""\n    def instantiate(**_):\n        return Resnet34(fc_layers, dropout, pretrained)\n\n    return ModelFactory.generic(instantiate)\n'"
vel/models/rnn/__init__.py,0,b''
vel/models/rnn/multilayer_rnn_sequence_classification.py,3,"b'import typing\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\n\nfrom vel.api import SupervisedModel, ModelFactory, LinearBackboneModel\nfrom vel.metrics.accuracy import Accuracy\nfrom vel.metrics.loss_metric import Loss\nfrom vel.modules.rnn_layer import RnnLayer\n\n\nclass MultilayerRnnSequenceClassification(SupervisedModel):\n    """""" Multilayer GRU network for sequence modeling (n:1) """"""\n\n    def __init__(self, input_block: LinearBackboneModel, rnn_type: str, output_dim: int,\n                 rnn_layers: typing.List[int], rnn_dropout: float=0.0, bidirectional: bool=False,\n                 linear_layers: typing.List[int]=None, linear_dropout: float=0.0):\n        super().__init__()\n\n        self.output_dim = output_dim\n\n        self.rnn_layers_sizes = rnn_layers\n        self.rnn_dropout = rnn_dropout\n        self.linear_layers_sizes = linear_layers\n        self.linear_dropout = linear_dropout\n\n        self.bidirectional = bidirectional\n        self.input_block = input_block\n\n        current_dim = self.input_block.output_dim\n\n        self.rnn_layers = []\n        self.rnn_dropout_layers = []\n\n        bidirectional_multiplier = 1\n\n        for idx, current_layer in enumerate(rnn_layers, 1):\n            rnn = RnnLayer(\n                input_size=current_dim * bidirectional_multiplier,\n                hidden_size=current_layer,\n                rnn_type=rnn_type,\n                bidirectional=bidirectional,\n            )\n\n            self.add_module(\'{}{:02}\'.format(rnn_type, idx), rnn)\n            self.rnn_layers.append(rnn)\n\n            if self.rnn_dropout > 0.0:\n                dropout_layer = nn.Dropout(p=self.rnn_dropout)\n\n                self.add_module(\'rnn_dropout{:02}\'.format(idx), dropout_layer)\n                self.rnn_dropout_layers.append(dropout_layer)\n\n            current_dim = current_layer\n\n            if self.bidirectional:\n                bidirectional_multiplier = 2\n            else:\n                bidirectional_multiplier = 1\n\n        self.linear_layers = []\n        self.linear_dropout_layers = []\n\n        for idx, current_layer in enumerate(linear_layers, 1):\n            linear_layer = nn.Linear(current_dim * bidirectional_multiplier, current_layer)\n\n            self.add_module(\'linear{:02}\'.format(idx), linear_layer)\n            self.linear_layers.append(linear_layer)\n\n            if self.linear_dropout > 0.0:\n                dropout_layer = nn.Dropout(p=self.linear_dropout)\n\n                self.add_module(\'linear_dropout{:02}\'.format(idx), dropout_layer)\n                self.linear_dropout_layers.append(dropout_layer)\n\n            bidirectional_multiplier = 1\n            current_dim = current_layer\n\n        if self.bidirectional:\n            self.output_layer = nn.Linear(bidirectional_multiplier * current_dim, output_dim)\n        else:\n            self.output_layer = nn.Linear(current_dim, output_dim)\n\n        self.output_activation = nn.LogSoftmax(dim=1)\n\n    def reset_weights(self):\n        self.input_block.reset_weights()\n\n        for layer in self.linear_layers:\n            nn.init.kaiming_normal_(layer.weight, nonlinearity=\'relu\')\n            nn.init.zeros_(layer.bias)\n\n        nn.init.kaiming_normal_(self.output_layer.weight, nonlinearity=\'relu\')\n        nn.init.zeros_(self.output_layer.bias)\n\n    def forward(self, sequence):\n        """""" Forward propagate batch of sequences through the network, without accounting for the state """"""\n        data = self.input_block(sequence)\n\n        for idx in range(len(self.rnn_layers)):\n            data, _ = self.rnn_layers[idx](data)\n\n            if self.rnn_dropout_layers:\n                data = self.rnn_dropout_layers[idx](data)\n\n        # We are interested only in the last element of the sequence\n        if self.bidirectional:\n            last_hidden_size = self.rnn_layers_sizes[-1]\n            data = torch.cat([data[:, -1, :last_hidden_size], data[:, 0, last_hidden_size:]], dim=1)\n        else:\n            data = data[:, -1]\n\n        for idx in range(len(self.linear_layers_sizes)):\n            data = F.relu(self.linear_layers[idx](data))\n\n            if self.linear_dropout_layers:\n                data = self.linear_dropout_layers[idx](data)\n\n        data = self.output_layer(data)\n\n        return self.output_activation(data)\n\n    def get_layer_groups(self):\n        return [\n            self.input_block,\n            self.rnn_layers,\n            self.linear_layers,\n            self.output_layer\n        ]\n\n    @property\n    def state_dim(self) -> int:\n        """""" Dimension of model state """"""\n        return sum(x.state_dim for x in self.gru_layers)\n\n    def loss_value(self, x_data, y_true, y_pred):\n        """""" Calculate a value of loss function """"""\n        return F.nll_loss(y_pred, y_true)\n\n    def metrics(self) -> list:\n        """""" Set of metrics for this model """"""\n        return [Loss(), Accuracy()]\n\n\ndef create(input_block: ModelFactory, rnn_type: str, output_dim: int,\n           rnn_layers: typing.List[int], rnn_dropout: float=0.0, bidirectional: bool=False,\n           linear_layers: typing.List[int]=None, linear_dropout: float=0.0):\n    """""" Vel factory function """"""\n    if linear_layers is None:\n        linear_layers = []\n\n    def instantiate(**_):\n        return MultilayerRnnSequenceClassification(\n            input_block=input_block.instantiate(), rnn_type=rnn_type, output_dim=output_dim,\n            rnn_layers=rnn_layers, rnn_dropout=rnn_dropout, bidirectional=bidirectional,\n            linear_layers=linear_layers, linear_dropout=linear_dropout\n        )\n\n    return ModelFactory.generic(instantiate)\n'"
vel/models/rnn/multilayer_rnn_sequence_model.py,5,"b'import typing\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\n\nfrom vel.api import RnnSupervisedModel, ModelFactory, LinearBackboneModel\nfrom vel.modules.rnn_layer import RnnLayer\n\n\nclass MultilayerRnnSequenceModel(RnnSupervisedModel):\n    """""" Multilayer GRU network for sequence modeling (n:n) """"""\n\n    def __init__(self, input_block: LinearBackboneModel, rnn_type: str, hidden_layers: typing.List[int],\n                 output_dim: int, dropout: float=0.0):\n        super().__init__()\n\n        self.output_dim = output_dim\n        self.hidden_layers = hidden_layers\n\n        self.input_block = input_block\n\n        current_dim = self.input_block.output_dim\n\n        self.recurrent_layers = []\n        self.dropout_layers = []\n\n        for idx, current_layer in enumerate(hidden_layers, 1):\n            rnn = RnnLayer(\n                input_size=current_dim,\n                hidden_size=current_layer,\n                rnn_type=rnn_type,\n            )\n\n            self.add_module(\'{}{:02}\'.format(rnn_type, idx), rnn)\n            self.recurrent_layers.append(rnn)\n\n            if dropout > 0.0:\n                dropout_layer = nn.Dropout(p=dropout)\n\n                self.add_module(\'rnn_dropout{:02}\'.format(idx), dropout_layer)\n                self.dropout_layers.append(dropout_layer)\n\n            current_dim = current_layer\n\n        self.output_layer = nn.Linear(current_dim, output_dim)\n        self.output_activation = nn.LogSoftmax(dim=2)\n\n    def reset_weights(self):\n        self.input_block.reset_weights()\n\n    def forward(self, sequence):\n        """""" Forward propagate batch of sequences through the network, without accounting for the state """"""\n        data = self.input_block(sequence)\n\n        for idx in range(len(self.recurrent_layers)):\n            data, _ = self.recurrent_layers[idx](data)\n\n            if self.dropout_layers:\n                data = self.dropout_layers[idx](data)\n\n        data = self.output_layer(data)\n\n        return self.output_activation(data)\n\n    def forward_state(self, sequence, state=None):\n        """""" Forward propagate a sequence through the network accounting for the state """"""\n        if state is None:\n            state = self.zero_state(sequence.size(0))\n\n        data = self.input_block(sequence)\n\n        state_outputs = []\n\n        # for layer_length, layer in zip(self.hidden_layers, self.recurrent_layers):\n        for idx in range(len(self.recurrent_layers)):\n            layer_length = self.recurrent_layers[idx].state_dim\n\n            # Partition hidden state, for each layer we have layer_length of h state and layer_length of c state\n            current_state = state[:, :, :layer_length]\n            state = state[:, :, layer_length:]\n\n            # Propagate through the GRU state\n            data, new_h = self.recurrent_layers[idx](data, current_state)\n\n            if self.dropout_layers:\n                data = self.dropout_layers[idx](data)\n\n            state_outputs.append(new_h)\n\n        output_data = self.output_activation(self.output_layer(data))\n\n        concatenated_hidden_output = torch.cat(state_outputs, dim=2)\n\n        return output_data, concatenated_hidden_output\n\n    @property\n    def state_dim(self) -> int:\n        """""" Dimension of model state """"""\n        return sum(x.state_dim for x in self.recurrent_layers)\n\n    def zero_state(self, batch_size):\n        """""" Initial state of the network """"""\n        return torch.zeros(1, batch_size, self.state_dim)\n\n    def loss_value(self, x_data, y_true, y_pred):\n        """""" Calculate a value of loss function """"""\n        y_pred = y_pred.view(-1, y_pred.size(2))\n        y_true = y_true.view(-1).to(torch.long)\n        return F.nll_loss(y_pred, y_true)\n\n\ndef create(input_block: ModelFactory, rnn_type: str, hidden_layers: typing.List[int],\n           output_dim: int, dropout=0.0):\n    """""" Vel factory function """"""\n    def instantiate(**_):\n        return MultilayerRnnSequenceModel(\n            input_block.instantiate(), rnn_type=rnn_type, hidden_layers=hidden_layers, output_dim=output_dim, dropout=dropout\n        )\n\n    return ModelFactory.generic(instantiate)\n'"
vel/models/vision/__init__.py,0,b''
vel/models/vision/cifar10_cnn_01.py,3,"b'""""""\nCode based loosely on implementation:\nhttps://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n\nUnder MIT license.\n""""""\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\nfrom vel.api import SupervisedModel, ModelFactory\nfrom vel.metrics.loss_metric import Loss\nfrom vel.metrics.accuracy import Accuracy\n\n\nclass Net(SupervisedModel):\n    """"""\n    A simple MNIST classification model.\n\n    Conv 3x3 - 32\n    Conv 3x3 - 64\n    MaxPool 2x2\n    Dropout 0.25\n    Flatten\n    Dense - 128\n    Dense - output (softmax)\n    """"""\n\n    @staticmethod\n    def _weight_initializer(tensor):\n        # init.xavier_uniform_(tensor.weight, gain=init.calculate_gain(\'relu\'))\n        init.kaiming_normal_(tensor.weight, nonlinearity=\'relu\')\n        init.constant_(tensor.bias, 0.0)\n\n    def __init__(self, img_rows, img_cols, img_channels, num_classes):\n        super(Net, self).__init__()\n\n        self.flattened_size = img_rows // 4 * img_cols // 4 * 64\n\n        self.conv1 = nn.Conv2d(in_channels=img_channels, out_channels=32, kernel_size=(3, 3), padding=1)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), padding=1)\n\n        self.dropout1 = nn.Dropout2d(p=0.25)\n\n        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1)\n        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), padding=1)\n\n        self.dropout2 = nn.Dropout2d(p=0.25)\n\n        self.fc1 = nn.Linear(self.flattened_size, 512)\n        self.dropout3 = nn.Dropout(p=0.5)\n\n        self.fc2 = nn.Linear(512, num_classes)\n\n        self._weight_initializer(self.conv1)\n        self._weight_initializer(self.conv2)\n        self._weight_initializer(self.conv3)\n        self._weight_initializer(self.conv4)\n        self._weight_initializer(self.fc1)\n        self._weight_initializer(self.fc2)\n\n    def forward(self, x):\n        # Block 1\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, kernel_size=(2, 2))\n        x = self.dropout1(x)\n\n        # Block 2\n\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = F.max_pool2d(x, kernel_size=(2, 2))\n        x = self.dropout2(x)\n\n        x = x.view(-1, self.flattened_size)\n        x = F.relu(self.fc1(x))\n        x = self.dropout3(x)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n    def loss_value(self, x_data, y_true, y_pred):\n        """""" Calculate a value of loss function """"""\n        return F.nll_loss(y_pred, y_true)\n\n    def metrics(self):\n        """""" Set of metrics for this model """"""\n        return [Loss(), Accuracy()]\n\n\ndef create(img_rows, img_cols, img_channels, num_classes):\n    """""" Vel factory function """"""\n    def instantiate(**_):\n        return Net(img_rows, img_cols, img_channels, num_classes)\n    return ModelFactory.generic(instantiate)\n'"
vel/models/vision/cifar_resnet_v1.py,2,"b'""""""\nCode based on\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n""""""\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom vel.api import SupervisedModel, ModelFactory\nfrom vel.modules.resnet_v1 import Bottleneck, BasicBlock\n\n\nclass ResNetV1(SupervisedModel):\n    """""" A ResNet V1 model as defined in the literature """"""\n\n    def __init__(self, block, layers, inplanes, divisor=4, img_channels=3, num_classes=1000):\n        super().__init__()\n\n        self.num_classess = num_classes\n        self.inplanes = inplanes\n        self.divisor = divisor\n\n        self.pre_conv = nn.Conv2d(img_channels, inplanes, kernel_size=(3, 3), padding=1, bias=False)\n        self.pre_bn = nn.BatchNorm2d(inplanes)\n\n        self.layer1 = self._make_layer(block, inplanes,     inplanes,     layers[0], stride=1)\n        self.layer2 = self._make_layer(block, inplanes,     inplanes * 2, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, inplanes * 2, inplanes * 4, layers[2], stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(inplanes * 4, num_classes)\n\n        # Weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                nn.init.constant_(m.bias, 0.0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1.0)\n                nn.init.constant_(m.bias, 0.0)\n\n    def _make_layer(self, block, in_channels, out_channels, blocks, stride=1):\n        layers = []\n\n        # First block is the one doing the rescaling\n        layers.append(block(in_channels, out_channels, stride, divisor=self.divisor))\n\n        for i in range(1, blocks):\n            layers.append(block(out_channels, out_channels, divisor=self.divisor))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.pre_conv(x)\n        x = self.pre_bn(x)\n        x = F.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        x = F.log_softmax(x, dim=1)\n\n        return x\n\n    def loss_value(self, x_data, y_true, y_pred):\n        """""" Calculate value of the loss function """"""\n        return F.nll_loss(y_pred, y_true)\n\n    def metrics(self):\n        """""" Set of metrics for this model """"""\n        from vel.metrics.loss_metric import Loss\n        from vel.metrics.accuracy import Accuracy\n        return [Loss(), Accuracy()]\n\n\ndef create(blocks, mode=\'basic\', inplanes=16, divisor=4, num_classes=1000):\n    """""" Vel factory function """"""\n    block_dict = {\n        \'basic\': BasicBlock,\n        \'bottleneck\': Bottleneck\n    }\n\n    def instantiate(**_):\n        return ResNetV1(block_dict[mode], blocks, inplanes=inplanes, divisor=divisor, num_classes=num_classes)\n\n    return ModelFactory.generic(instantiate)\n'"
vel/models/vision/cifar_resnet_v2.py,2,"b'""""""\nCode based on\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n""""""\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom vel.api import SupervisedModel, ModelFactory\nfrom vel.modules.resnet_v2 import Bottleneck, BasicBlock\n\n\nclass ResNetV2(SupervisedModel):\n    """""" A ResNet V2 (pre-activation resnet) model as defined in the literature """"""\n\n    def __init__(self, block, layers, inplanes, divisor=4, img_channels=3, num_classes=1000):\n        super().__init__()\n\n        self.num_classess = num_classes\n        self.inplanes = inplanes\n        self.divisor = divisor\n\n        self.pre_conv = nn.Conv2d(img_channels, inplanes, kernel_size=(3, 3), padding=1, bias=False)\n\n        # self.layer1 = self._make_layer(block, 16, layers[0])\n        # self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n        # self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n        self.layer1 = self._make_layer(block, inplanes,     inplanes,     layers[0], stride=1)\n        self.layer2 = self._make_layer(block, inplanes,     inplanes * 2, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, inplanes * 2, inplanes * 4, layers[2], stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(inplanes * 4, num_classes)\n\n        # Weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                nn.init.constant_(m.bias, 0.0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1.0)\n                nn.init.constant_(m.bias, 0.0)\n\n    def _make_layer(self, block, in_channels, out_channels, blocks, stride=1):\n        layers = []\n\n        # First block is the one doing the rescaling\n        layers.append(block(in_channels, out_channels, stride, divisor=self.divisor))\n\n        for i in range(1, blocks):\n            layers.append(block(out_channels, out_channels, divisor=self.divisor))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.pre_conv(x)\n        x = F.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = F.relu(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        x = F.log_softmax(x, dim=1)\n\n        return x\n\n    def loss_value(self, x_data, y_true, y_pred):\n        """""" Calculate value of the loss function """"""\n        return F.nll_loss(y_pred, y_true)\n\n    def metrics(self):\n        """""" Set of metrics for this model """"""\n        from vel.metrics.loss_metric import Loss\n        from vel.metrics.accuracy import Accuracy\n        return [Loss(), Accuracy()]\n\n    def summary(self):\n        """""" Print model summary """"""\n        # import torchsummary\n        # torchsummary.summary(self, input_size=(3, 32, 32))\n        print(self)\n\n\ndef create(blocks, mode=\'basic\', inplanes=16, divisor=4, num_classes=1000):\n    """""" Vel factory function """"""\n    block_dict = {\n        \'basic\': BasicBlock,\n        \'bottleneck\': Bottleneck\n    }\n\n    def instantiate(**_):\n        return ResNetV2(block_dict[mode], blocks, inplanes=inplanes, divisor=divisor, num_classes=num_classes)\n\n    return ModelFactory.generic(instantiate)\n'"
vel/models/vision/cifar_resnext.py,2,"b'""""""\nCode based on\nhttps://github.com/fastai/fastai/blob/master/fastai/models/cifar10/resnext.py\n""""""\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom vel.api import SupervisedModel, ModelFactory\nfrom vel.modules.resnext import ResNeXtBottleneck\n\n\nclass ResNeXt(SupervisedModel):\n    """""" A ResNext model as defined in the literature """"""\n\n    def __init__(self, block, layers, inplanes, image_features, cardinality=4, divisor=4, img_channels=3, num_classes=1000):\n        super().__init__()\n\n        self.num_classess = num_classes\n        self.inplanes = inplanes\n        self.divisor = divisor\n        self.cardinality = cardinality\n\n        self.pre_conv = nn.Conv2d(img_channels, image_features, kernel_size=(3, 3), padding=1, bias=False)\n        self.pre_bn = nn.BatchNorm2d(image_features)\n\n        self.layer1 = self._make_layer(block, image_features, inplanes,     layers[0], stride=1)\n        self.layer2 = self._make_layer(block, inplanes,       inplanes * 2, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, inplanes * 2,   inplanes * 4, layers[2], stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(inplanes * 4, num_classes)\n\n        # Weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                nn.init.constant_(m.bias, 0.0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1.0)\n                nn.init.constant_(m.bias, 0.0)\n\n    def _make_layer(self, block, in_channels, out_channels, blocks, stride=1):\n        layers = []\n        layers.append(block(in_channels, out_channels, self.cardinality, self.divisor, stride=stride))\n\n        for i in range(1, blocks):\n            layers.append(block(out_channels, out_channels, self.cardinality, self.divisor, stride=1))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.pre_conv(x)\n        x = self.pre_bn(x)\n        x = F.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        x = F.log_softmax(x, dim=1)\n\n        return x\n\n    def loss_value(self, x_data, y_true, y_pred):\n        """""" Calculate value of the loss function """"""\n        return F.nll_loss(y_pred, y_true)\n\n    def metrics(self):\n        """""" Set of metrics for this model """"""\n        from vel.metrics.loss_metric import Loss\n        from vel.metrics.accuracy import Accuracy\n        return [Loss(), Accuracy()]\n\n    def summary(self):\n        """""" Print model summary """"""\n        # import torchsummary\n\n        print(self)\n        # self.eval()\n        # torchsummary.summary(self, input_size=(3, 32, 32))\n\n\ndef create(blocks, mode=\'basic\', inplanes=64, cardinality=4, image_features=64, divisor=4, num_classes=1000):\n    """""" Vel factory function """"""\n    block_dict = {\n        # \'basic\': BasicBlock,\n        \'bottleneck\': ResNeXtBottleneck\n    }\n\n    def instantiate(**_):\n        return ResNeXt(block_dict[mode], blocks, inplanes=inplanes, image_features=image_features, cardinality=cardinality, divisor=divisor, num_classes=num_classes)\n\n    return ModelFactory.generic(instantiate)\n'"
vel/models/vision/mnist_cnn_01.py,3,"b'""""""\nCode based loosely on implementation:\nhttps://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n\nUnder MIT license.\n""""""\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\nfrom vel.api import SupervisedModel, ModelFactory\nfrom vel.metrics.loss_metric import Loss\nfrom vel.metrics.accuracy import Accuracy\n\n\nclass Net(SupervisedModel):\n    """"""\n    A simple MNIST classification model.\n\n    Conv 3x3 - 32\n    Conv 3x3 - 64\n    MaxPool 2x2\n    Dropout 0.25\n    Flatten\n    Dense - 128\n    Dense - output (softmax)\n    """"""\n\n    @staticmethod\n    def _weight_initializer(tensor):\n        init.xavier_uniform_(tensor.weight, gain=init.calculate_gain(\'relu\'))\n        init.constant_(tensor.bias, 0.0)\n\n    def __init__(self, img_rows, img_cols, img_channels, num_classes):\n        super(Net, self).__init__()\n\n        self.flattened_size = (img_rows - 4) // 2 * (img_cols - 4) // 2 * 64\n\n        self.conv1 = nn.Conv2d(in_channels=img_channels, out_channels=32, kernel_size=(3, 3))\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3))\n\n        self.dropout1 = nn.Dropout2d(p=0.25)\n\n        self.fc1 = nn.Linear(self.flattened_size, 128)\n        self.dropout2 = nn.Dropout(p=0.5)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def reset_weights(self):\n        self._weight_initializer(self.conv1)\n        self._weight_initializer(self.conv2)\n        self._weight_initializer(self.fc1)\n        self._weight_initializer(self.fc2)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, kernel_size=(2, 2))\n        x = self.dropout1(x)\n        x = x.view(-1, self.flattened_size)\n        x = F.relu(self.fc1(x))\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n    def loss_value(self, x_data, y_true, y_pred):\n        """""" Calculate a value of loss function """"""\n        return F.nll_loss(y_pred, y_true)\n\n    def metrics(self):\n        """""" Set of metrics for this model """"""\n        return [Loss(), Accuracy()]\n\n\ndef create(img_rows, img_cols, img_channels, num_classes):\n    """""" Vel factory function """"""\n    def instantiate(**_):\n        return Net(img_rows, img_cols, img_channels, num_classes)\n\n    return ModelFactory.generic(instantiate)\n'"
vel/modules/input/__init__.py,0,b''
vel/modules/input/embedding.py,1,"b'import torch.nn as nn\n\nfrom vel.api import LinearBackboneModel, TextData, ModelFactory\n\n\nclass EmbeddingInput(LinearBackboneModel):\n    """""" Learnable Embedding input layer """"""\n\n    def __init__(self, alphabet_size: int, output_dim: int, pretrained: bool=False, frozen: bool=False,\n                 source: TextData=None):\n        super().__init__()\n\n        self._output_dim = output_dim\n        self._alphabet_size = alphabet_size\n        self._pretrained = pretrained\n        self._frozen = frozen\n        self._source = source\n\n        self.layer = nn.Embedding(self._alphabet_size, self._output_dim)\n\n    def reset_weights(self):\n        if self._pretrained:\n            self.layer.weight.data.copy_(self._source.data_field.vocab.vectors)\n\n        if self._frozen:\n            self.layer.weight.requires_grad = False\n\n    @property\n    def output_dim(self) -> int:\n        """""" Final dimension of model output """"""\n        return self._output_dim\n\n    def forward(self, input_data):\n        return self.layer(input_data)\n\n\ndef create(alphabet_size: int, output_dim: int, pretrained: bool=False, frozen: bool=False, source: TextData=None):\n    """""" Vel factory function """"""\n    def instantiate(**_):\n        return EmbeddingInput(alphabet_size, output_dim, pretrained=pretrained, frozen=frozen, source=source)\n\n    return ModelFactory.generic(instantiate)\n\n\n# Scripting interface\nEmbeddingInputFactory = create\n\n'"
vel/modules/input/identity.py,0,"b'from vel.api import BackboneModel, ModelFactory\n\n\nclass Identity(BackboneModel):\n    """""" Identity transformation that doesn\'t do anything """"""\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x\n\n    def reset_weights(self):\n        pass\n\n\ndef create():\n    """""" Vel factory function """"""\n    def instantiate(**_):\n        return Identity()\n\n    return ModelFactory.generic(instantiate)\n\n\n# Scripting interface\nIdentityFactory = create\n'"
vel/modules/input/image_to_tensor.py,3,"b'import torch\n\nfrom vel.api import BackboneModel, ModelFactory\n\n\nclass ImageToTensor(BackboneModel):\n    """"""\n    Convert simple image to tensor.\n\n    Flip channels to a [C, W, H] order and potentially convert 8-bit color values to floats\n    """"""\n\n    def __init__(self):\n        super().__init__()\n\n    def reset_weights(self):\n        pass\n\n    def forward(self, image):\n        result = image.permute(0, 3, 1, 2).contiguous()\n\n        if result.dtype == torch.uint8:\n            result = result.type(torch.float) / 255.0\n        else:\n            result = result.type(torch.float)\n\n        return result\n\n\ndef create():\n    """""" Vel factory function """"""\n    return ModelFactory.generic(ImageToTensor)\n\n\n# Scripting interface\nImageToTensorFactory = create\n'"
vel/modules/input/normalize_observations.py,5,"b'import torch\nimport numbers\n\nfrom vel.api import BackboneModel, ModelFactory\n\n\nclass NormalizeObservations(BackboneModel):\n    """""" Normalize a vector of observations """"""\n\n    def __init__(self, input_shape, epsilon=1e-6):\n        super().__init__()\n\n        self.input_shape = input_shape\n        self.epsilon = epsilon\n\n        self.register_buffer(\'running_mean\', torch.zeros(input_shape, dtype=torch.float))\n        self.register_buffer(\'running_var\', torch.ones(input_shape, dtype=torch.float))\n        self.register_buffer(\'count\', torch.tensor(epsilon, dtype=torch.float))\n\n    def reset_weights(self):\n        self.running_mean.zero_()\n        self.running_var.fill_(1.0)\n        self.count.fill_(self.epsilon)\n\n    def forward(self, input_vector):\n        # Make sure input is float32\n        input_vector = input_vector.to(torch.float)\n\n        if self.training:\n            batch_mean = input_vector.mean(dim=0)\n            batch_var = input_vector.var(dim=0, unbiased=False)\n            batch_count = input_vector.size(0)\n\n            delta = batch_mean - self.running_mean\n            tot_count = self.count + batch_count\n\n            self.running_mean.add_(delta * batch_count / tot_count)\n\n            m_a = self.running_var * self.count\n            m_b = batch_var * batch_count\n            M2 = m_a + m_b + (delta ** 2) * self.count * batch_count / (self.count + batch_count)\n            new_var = M2 / (self.count + batch_count)\n\n            self.count.add_(batch_count)\n            self.running_var.copy_(new_var)\n\n        return (input_vector - self.running_mean.unsqueeze(0)) / torch.sqrt(self.running_var.unsqueeze(0))\n\n\ndef create(input_shape):\n    """""" Vel factory function """"""\n    if isinstance(input_shape, numbers.Number):\n        input_shape = (input_shape,)\n    elif not isinstance(input_shape, tuple):\n        input_shape = tuple(input_shape)\n\n    def instantiate(**_):\n        return NormalizeObservations(input_shape)\n\n    return ModelFactory.generic(instantiate)\n\n\n# Scripting interface\nNormalizeObservationsFactory = create\n'"
vel/modules/input/one_hot_encoding.py,0,"b'from vel.api import LinearBackboneModel, ModelFactory\nfrom vel.modules.layers import OneHotEncode\n\n\nclass OneHotEncodingInput(LinearBackboneModel):\n    """""" One-hot encoding input layer """"""\n\n    def __init__(self, alphabet_size: int):\n        super().__init__()\n\n        self._alphabet_size = alphabet_size\n\n        self.layer = OneHotEncode(self._alphabet_size)\n\n    @property\n    def output_dim(self) -> int:\n        """""" Final dimension of model output """"""\n        return self._alphabet_size\n\n    def forward(self, input_data):\n        return self.layer(input_data)\n\n\ndef create(alphabet_size: int):\n    """""" Vel factory function """"""\n    def instantiate(**_):\n        return OneHotEncodingInput(alphabet_size)\n\n    return ModelFactory.generic(instantiate)\n\n\n# Scripting interface\nOneHotEncodingInputFactory = create\n'"
vel/openai/baselines/__init__.py,0,b''
vel/openai/baselines/logger.py,0,"b'import os\nimport sys\nimport shutil\nimport os.path as osp\nimport json\nimport time\nimport datetime\nimport tempfile\nfrom collections import defaultdict\n\nDEBUG = 10\nINFO = 20\nWARN = 30\nERROR = 40\n\nDISABLED = 50\n\nclass KVWriter(object):\n    def writekvs(self, kvs):\n        raise NotImplementedError\n\nclass SeqWriter(object):\n    def writeseq(self, seq):\n        raise NotImplementedError\n\nclass HumanOutputFormat(KVWriter, SeqWriter):\n    def __init__(self, filename_or_file):\n        if isinstance(filename_or_file, str):\n            self.file = open(filename_or_file, \'wt\')\n            self.own_file = True\n        else:\n            assert hasattr(filename_or_file, \'read\'), \'expected file or str, got %s\'%filename_or_file\n            self.file = filename_or_file\n            self.own_file = False\n\n    def writekvs(self, kvs):\n        # Create strings for printing\n        key2str = {}\n        for (key, val) in sorted(kvs.items()):\n            if isinstance(val, float):\n                valstr = \'%-8.3g\' % (val,)\n            else:\n                valstr = str(val)\n            key2str[self._truncate(key)] = self._truncate(valstr)\n\n        # Find max widths\n        if len(key2str) == 0:\n            print(\'WARNING: tried to write empty key-value dict\')\n            return\n        else:\n            keywidth = max(map(len, key2str.keys()))\n            valwidth = max(map(len, key2str.values()))\n\n        # Write out the data\n        dashes = \'-\' * (keywidth + valwidth + 7)\n        lines = [dashes]\n        for (key, val) in sorted(key2str.items(), key=lambda kv: kv[0].lower()):\n            lines.append(\'| %s%s | %s%s |\' % (\n                key,\n                \' \' * (keywidth - len(key)),\n                val,\n                \' \' * (valwidth - len(val)),\n            ))\n        lines.append(dashes)\n        self.file.write(\'\\n\'.join(lines) + \'\\n\')\n\n        # Flush the output to the file\n        self.file.flush()\n\n    def _truncate(self, s):\n        return s[:20] + \'...\' if len(s) > 23 else s\n\n    def writeseq(self, seq):\n        seq = list(seq)\n        for (i, elem) in enumerate(seq):\n            self.file.write(elem)\n            if i < len(seq) - 1: # add space unless this is the last one\n                self.file.write(\' \')\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n    def close(self):\n        if self.own_file:\n            self.file.close()\n\nclass JSONOutputFormat(KVWriter):\n    def __init__(self, filename):\n        self.file = open(filename, \'wt\')\n\n    def writekvs(self, kvs):\n        for k, v in sorted(kvs.items()):\n            if hasattr(v, \'dtype\'):\n                v = v.tolist()\n                kvs[k] = float(v)\n        self.file.write(json.dumps(kvs) + \'\\n\')\n        self.file.flush()\n\n    def close(self):\n        self.file.close()\n\nclass CSVOutputFormat(KVWriter):\n    def __init__(self, filename):\n        self.file = open(filename, \'w+t\')\n        self.keys = []\n        self.sep = \',\'\n\n    def writekvs(self, kvs):\n        # Add our current row to the history\n        extra_keys = list(kvs.keys() - self.keys)\n        extra_keys.sort()\n        if extra_keys:\n            self.keys.extend(extra_keys)\n            self.file.seek(0)\n            lines = self.file.readlines()\n            self.file.seek(0)\n            for (i, k) in enumerate(self.keys):\n                if i > 0:\n                    self.file.write(\',\')\n                self.file.write(k)\n            self.file.write(\'\\n\')\n            for line in lines[1:]:\n                self.file.write(line[:-1])\n                self.file.write(self.sep * len(extra_keys))\n                self.file.write(\'\\n\')\n        for (i, k) in enumerate(self.keys):\n            if i > 0:\n                self.file.write(\',\')\n            v = kvs.get(k)\n            if v is not None:\n                self.file.write(str(v))\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n    def close(self):\n        self.file.close()\n\n\nclass TensorBoardOutputFormat(KVWriter):\n    """"""\n    Dumps key/value pairs into TensorBoard\'s numeric format.\n    """"""\n    def __init__(self, dir):\n        os.makedirs(dir, exist_ok=True)\n        self.dir = dir\n        self.step = 1\n        prefix = \'events\'\n        path = osp.join(osp.abspath(dir), prefix)\n        import tensorflow as tf\n        from tensorflow.python import pywrap_tensorflow\n        from tensorflow.core.util import event_pb2\n        from tensorflow.python.util import compat\n        self.tf = tf\n        self.event_pb2 = event_pb2\n        self.pywrap_tensorflow = pywrap_tensorflow\n        self.writer = pywrap_tensorflow.EventsWriter(compat.as_bytes(path))\n\n    def writekvs(self, kvs):\n        def summary_val(k, v):\n            kwargs = {\'tag\': k, \'simple_value\': float(v)}\n            return self.tf.Summary.Value(**kwargs)\n        summary = self.tf.Summary(value=[summary_val(k, v) for k, v in kvs.items()])\n        event = self.event_pb2.Event(wall_time=time.time(), summary=summary)\n        event.step = self.step # is there any reason why you\'d want to specify the step?\n        self.writer.WriteEvent(event)\n        self.writer.Flush()\n        self.step += 1\n\n    def close(self):\n        if self.writer:\n            self.writer.Close()\n            self.writer = None\n\ndef make_output_format(format, ev_dir, log_suffix=\'\'):\n    os.makedirs(ev_dir, exist_ok=True)\n    if format == \'stdout\':\n        return HumanOutputFormat(sys.stdout)\n    elif format == \'log\':\n        return HumanOutputFormat(osp.join(ev_dir, \'log%s.txt\' % log_suffix))\n    elif format == \'json\':\n        return JSONOutputFormat(osp.join(ev_dir, \'progress%s.json\' % log_suffix))\n    elif format == \'csv\':\n        return CSVOutputFormat(osp.join(ev_dir, \'progress%s.csv\' % log_suffix))\n    elif format == \'tensorboard\':\n        return TensorBoardOutputFormat(osp.join(ev_dir, \'tb%s\' % log_suffix))\n    else:\n        raise ValueError(\'Unknown format specified: %s\' % (format,))\n\n# ================================================================\n# API\n# ================================================================\n\ndef logkv(key, val):\n    """"""\n    Log a value of some diagnostic\n    Call this once for each diagnostic quantity, each iteration\n    If called many times, last value will be used.\n    """"""\n    Logger.CURRENT.logkv(key, val)\n\ndef logkv_mean(key, val):\n    """"""\n    The same as logkv(), but if called many times, values averaged.\n    """"""\n    Logger.CURRENT.logkv_mean(key, val)\n\ndef logkvs(d):\n    """"""\n    Log a dictionary of key-value pairs\n    """"""\n    for (k, v) in d.items():\n        logkv(k, v)\n\ndef dumpkvs():\n    """"""\n    Write all of the diagnostics from the current iteration\n\n    level: int. (see logger.py docs) If the global logger level is higher than\n                the level argument here, don\'t print to stdout.\n    """"""\n    Logger.CURRENT.dumpkvs()\n\ndef getkvs():\n    return Logger.CURRENT.name2val\n\n\ndef log(*args, level=INFO):\n    """"""\n    Write the sequence of args, with no separators, to the console and output files (if you\'ve configured an output file).\n    """"""\n    Logger.CURRENT.log(*args, level=level)\n\ndef debug(*args):\n    log(*args, level=DEBUG)\n\ndef info(*args):\n    log(*args, level=INFO)\n\ndef warn(*args):\n    log(*args, level=WARN)\n\ndef error(*args):\n    log(*args, level=ERROR)\n\n\ndef set_level(level):\n    """"""\n    Set logging threshold on current logger.\n    """"""\n    Logger.CURRENT.set_level(level)\n\ndef get_dir():\n    """"""\n    Get directory that log files are being written to.\n    will be None if there is no output directory (i.e., if you didn\'t call start)\n    """"""\n    return Logger.CURRENT.get_dir()\n\nrecord_tabular = logkv\ndump_tabular = dumpkvs\n\nclass ProfileKV:\n    """"""\n    Usage:\n    with logger.ProfileKV(""interesting_scope""):\n        code\n    """"""\n    def __init__(self, n):\n        self.n = ""wait_"" + n\n    def __enter__(self):\n        self.t1 = time.time()\n    def __exit__(self ,type, value, traceback):\n        Logger.CURRENT.name2val[self.n] += time.time() - self.t1\n\ndef profile(n):\n    """"""\n    Usage:\n    @profile(""my_func"")\n    def my_func(): code\n    """"""\n    def decorator_with_name(func):\n        def func_wrapper(*args, **kwargs):\n            with ProfileKV(n):\n                return func(*args, **kwargs)\n        return func_wrapper\n    return decorator_with_name\n\n\n# ================================================================\n# Backend\n# ================================================================\n\nclass Logger(object):\n    DEFAULT = None  # A logger with no output files. (See right below class definition)\n                    # So that you can still log to the terminal without setting up any output files\n    CURRENT = None  # Current logger being used by the free functions above\n\n    def __init__(self, dir, output_formats):\n        self.name2val = defaultdict(float)  # values this iteration\n        self.name2cnt = defaultdict(int)\n        self.level = INFO\n        self.dir = dir\n        self.output_formats = output_formats\n\n    # Logging API, forwarded\n    # ----------------------------------------\n    def logkv(self, key, val):\n        self.name2val[key] = val\n\n    def logkv_mean(self, key, val):\n        if val is None:\n            self.name2val[key] = None\n            return\n        oldval, cnt = self.name2val[key], self.name2cnt[key]\n        self.name2val[key] = oldval*cnt/(cnt+1) + val/(cnt+1)\n        self.name2cnt[key] = cnt + 1\n\n    def dumpkvs(self):\n        if self.level == DISABLED: return\n        for fmt in self.output_formats:\n            if isinstance(fmt, KVWriter):\n                fmt.writekvs(self.name2val)\n        self.name2val.clear()\n        self.name2cnt.clear()\n\n    def log(self, *args, level=INFO):\n        if self.level <= level:\n            self._do_log(args)\n\n    # Configuration\n    # ----------------------------------------\n    def set_level(self, level):\n        self.level = level\n\n    def get_dir(self):\n        return self.dir\n\n    def close(self):\n        for fmt in self.output_formats:\n            fmt.close()\n\n    # Misc\n    # ----------------------------------------\n    def _do_log(self, args):\n        for fmt in self.output_formats:\n            if isinstance(fmt, SeqWriter):\n                fmt.writeseq(map(str, args))\n\ndef configure(dir=None, format_strs=None):\n    if dir is None:\n        dir = os.getenv(\'OPENAI_LOGDIR\')\n    if dir is None:\n        dir = osp.join(tempfile.gettempdir(),\n            datetime.datetime.now().strftime(""openai-%Y-%m-%d-%H-%M-%S-%f""))\n    assert isinstance(dir, str)\n    os.makedirs(dir, exist_ok=True)\n\n    log_suffix = \'\'\n    rank = 0\n    # check environment variables here instead of importing mpi4py\n    # to avoid calling MPI_Init() when this module is imported\n    for varname in [\'PMI_RANK\', \'OMPI_COMM_WORLD_RANK\']:\n        if varname in os.environ:\n            rank = int(os.environ[varname])\n    if rank > 0:\n        log_suffix = ""-rank%03i"" % rank\n\n    if format_strs is None:\n        if rank == 0:\n            format_strs = os.getenv(\'OPENAI_LOG_FORMAT\', \'stdout,log,csv\').split(\',\')\n        else:\n            format_strs = os.getenv(\'OPENAI_LOG_FORMAT_MPI\', \'log\').split(\',\')\n    format_strs = filter(None, format_strs)\n    output_formats = [make_output_format(f, dir, log_suffix) for f in format_strs]\n\n    Logger.CURRENT = Logger(dir=dir, output_formats=output_formats)\n    # log(\'Logging to %s\'%dir)\n\ndef _configure_default_logger():\n    format_strs = None\n    # keep the old default of only writing to stdout\n    if \'OPENAI_LOG_FORMAT\' not in os.environ:\n        format_strs = [\'stdout\']\n    configure(format_strs=format_strs)\n    Logger.DEFAULT = Logger.CURRENT\n\ndef reset():\n    if Logger.CURRENT is not Logger.DEFAULT:\n        Logger.CURRENT.close()\n        Logger.CURRENT = Logger.DEFAULT\n        log(\'Reset logger\')\n\nclass scoped_configure(object):\n    def __init__(self, dir=None, format_strs=None):\n        self.dir = dir\n        self.format_strs = format_strs\n        self.prevlogger = None\n    def __enter__(self):\n        self.prevlogger = Logger.CURRENT\n        configure(dir=self.dir, format_strs=self.format_strs)\n    def __exit__(self, *args):\n        Logger.CURRENT.close()\n        Logger.CURRENT = self.prevlogger\n\n# ================================================================\n\ndef _demo():\n    info(""hi"")\n    debug(""shouldn\'t appear"")\n    set_level(DEBUG)\n    debug(""should appear"")\n    dir = ""/tmp/testlogging""\n    if os.path.exists(dir):\n        shutil.rmtree(dir)\n    configure(dir=dir)\n    logkv(""a"", 3)\n    logkv(""b"", 2.5)\n    dumpkvs()\n    logkv(""b"", -2.5)\n    logkv(""a"", 5.5)\n    dumpkvs()\n    info(""^^^ should see a = 5.5"")\n    logkv_mean(""b"", -22.5)\n    logkv_mean(""b"", -44.4)\n    logkv(""a"", 5.5)\n    dumpkvs()\n    info(""^^^ should see b = 33.3"")\n\n    logkv(""b"", -2.5)\n    dumpkvs()\n\n    logkv(""a"", ""longasslongasslongasslongasslongasslongassvalue"")\n    dumpkvs()\n\n\n# ================================================================\n# Readers\n# ================================================================\n\ndef read_json(fname):\n    import pandas\n    ds = []\n    with open(fname, \'rt\') as fh:\n        for line in fh:\n            ds.append(json.loads(line))\n    return pandas.DataFrame(ds)\n\ndef read_csv(fname):\n    import pandas\n    return pandas.read_csv(fname, index_col=None, comment=\'#\')\n\ndef read_tb(path):\n    """"""\n    path : a tensorboard file OR a directory, where we will find all TB files\n           of the form events.*\n    """"""\n    import pandas\n    import numpy as np\n    from glob import glob\n    from collections import defaultdict\n    import tensorflow as tf\n    if osp.isdir(path):\n        fnames = glob(osp.join(path, ""events.*""))\n    elif osp.basename(path).startswith(""events.""):\n        fnames = [path]\n    else:\n        raise NotImplementedError(""Expected tensorboard file or directory containing them. Got %s""%path)\n    tag2pairs = defaultdict(list)\n    maxstep = 0\n    for fname in fnames:\n        for summary in tf.train.summary_iterator(fname):\n            if summary.step > 0:\n                for v in summary.summary.value:\n                    pair = (summary.step, v.simple_value)\n                    tag2pairs[v.tag].append(pair)\n                maxstep = max(summary.step, maxstep)\n    data = np.empty((maxstep, len(tag2pairs)))\n    data[:] = np.nan\n    tags = sorted(tag2pairs.keys())\n    for (colidx,tag) in enumerate(tags):\n        pairs = tag2pairs[tag]\n        for (step, value) in pairs:\n            data[step-1, colidx] = value\n    return pandas.DataFrame(data, columns=tags)\n\n# configure the default logger on import\n_configure_default_logger()\n\nif __name__ == ""__main__"":\n    _demo()\n'"
vel/rl/algo/__init__.py,0,b''
vel/rl/algo/distributional_dqn.py,9,"b'import torch\nimport torch.nn.utils\n\nfrom vel.api import ModelFactory\nfrom vel.api.metrics.averaging_metric import AveragingNamedMetric\nfrom vel.rl.api import OptimizerAlgoBase\n\n\nclass DistributionalDeepQLearning(OptimizerAlgoBase):\n    """""" Deep Q-Learning algorithm """"""\n\n    def __init__(self, model_factory: ModelFactory, discount_factor: float, double_dqn: bool,\n                 target_update_frequency: int, max_grad_norm: float):\n        super().__init__(max_grad_norm)\n\n        self.model_factory = model_factory\n        self.discount_factor = discount_factor\n\n        self.double_dqn = double_dqn\n        self.target_update_frequency = target_update_frequency\n\n        self.target_model = None\n\n        self.vmin = None\n        self.vmax = None\n        self.num_atoms = None\n        self.support_atoms = None\n        self.atom_delta = None\n\n    def initialize(self, training_info, model, environment, device):\n        """""" Initialize policy gradient from reinforcer settings """"""\n        self.target_model = self.model_factory.instantiate(action_space=environment.action_space).to(device)\n        self.target_model.load_state_dict(model.state_dict())\n        self.target_model.eval()\n\n        histogram_info = model.histogram_info()\n\n        self.vmin = histogram_info[\'vmin\']\n        self.vmax = histogram_info[\'vmax\']\n\n        self.num_atoms = histogram_info[\'num_atoms\']\n\n        self.support_atoms = histogram_info[\'support_atoms\']\n        self.atom_delta = histogram_info[\'atom_delta\']\n\n    def calculate_gradient(self, batch_info, device, model, rollout):\n        """""" Calculate loss of the supplied rollout """"""\n        evaluator = model.evaluate(rollout)\n        batch_size = rollout.frames()\n\n        dones_tensor = evaluator.get(\'rollout:dones\')\n        rewards_tensor = evaluator.get(\'rollout:rewards\')\n\n        assert dones_tensor.dtype == torch.float32\n\n        with torch.no_grad():\n            target_evaluator = self.target_model.evaluate(rollout)\n\n            if self.double_dqn:\n                # DOUBLE DQN\n                # Histogram gets returned as logits initially, we need to exp it before projection\n                target_value_histogram_for_all_actions = target_evaluator.get(\'model:q_dist_next\').exp()\n                model_value_histogram_for_all_actions = evaluator.get(\'model:q_dist_next\').exp()\n\n                atoms_aligned = self.support_atoms.view(1, 1, self.num_atoms)\n\n                selected_action_indices = (\n                    (atoms_aligned * model_value_histogram_for_all_actions).sum(dim=-1).argmax(dim=1)\n                )\n\n                # Select largest \'target\' value based on action that \'model\' selects\n                next_value_histograms = (\n                    target_value_histogram_for_all_actions[range(batch_size), selected_action_indices]\n                )\n            else:\n                # REGULAR DQN\n                # Histogram gets returned as logits initially, we need to exp it before projection\n                target_value_histogram_for_all_actions = target_evaluator.get(\'model:q_dist_next\').exp()\n\n                atoms_aligned = self.support_atoms.view(1, 1, self.num_atoms)\n\n                selected_action_indices = (\n                    (atoms_aligned * target_value_histogram_for_all_actions).sum(dim=-1).argmax(dim=1)\n                )\n\n                next_value_histograms = (\n                    target_value_histogram_for_all_actions[range(batch_size), selected_action_indices]\n                )\n\n            # HISTOGRAM PROJECTION CODE\n            forward_steps = rollout.extra_data.get(\'forward_steps\', 1)\n\n            atoms_projected = (\n                rewards_tensor.unsqueeze(1) +\n                (self.discount_factor ** forward_steps) *\n                (1 - dones_tensor).unsqueeze(1) * self.support_atoms.unsqueeze(0)\n            )\n\n            atoms_projected = atoms_projected.clamp(min=self.vmin, max=self.vmax)\n            projection_indices = (atoms_projected - self.vmin) / self.atom_delta\n\n            index_floor = projection_indices.floor().long()\n            index_ceil = projection_indices.ceil().long()\n\n            # Fix corner case when index_floor == index_ceil\n            index_floor[(index_ceil > 0) * (index_floor == index_ceil)] -= 1\n            index_ceil[(index_floor < (self.num_atoms - 1)) * (index_floor == index_ceil)] += 1\n\n            value_histogram_projected = torch.zeros_like(next_value_histograms)\n\n            # Following part will be a bit convoluted, in an effort to fully vectorize projection operation\n\n            # Special offset index tensor\n            offsets = (\n                torch.arange(0, batch_size * self.num_atoms, self.num_atoms)\n                .unsqueeze(1)\n                .expand(batch_size, self.num_atoms)\n                .contiguous().view(-1).to(device)\n            )\n\n            # Linearize all the buffers\n            value_histogram_projected = value_histogram_projected.view(-1)\n            index_ceil = index_ceil.view(-1)\n            index_floor = index_floor.view(-1)\n            projection_indices = projection_indices.view(-1)\n\n            value_histogram_projected.index_add_(\n                0,\n                index_floor+offsets,\n                (next_value_histograms.view(-1) * (index_ceil.float() - projection_indices))\n            )\n\n            value_histogram_projected.index_add_(\n                0,\n                index_ceil+offsets,\n                (next_value_histograms.view(-1) * (projection_indices - index_floor.float()))\n            )\n\n            value_histogram_projected = value_histogram_projected.reshape(next_value_histograms.shape)\n\n        q_log_histogram_selected = evaluator.get(\'model:action:q_dist\')\n\n        # Cross-entropy loss as usual\n        original_losses = -(value_histogram_projected * q_log_histogram_selected).sum(dim=1)\n\n        if evaluator.is_provided(\'rollout:weights\'):\n            weights = evaluator.get(\'rollout:weights\')\n        else:\n            weights = torch.ones_like(rewards_tensor)\n\n        loss_value = torch.mean(weights * original_losses)\n        loss_value.backward()\n\n        with torch.no_grad():\n            mean_q_model = (self.support_atoms.unsqueeze(0) * torch.exp(q_log_histogram_selected)).sum(dim=1).mean()\n            mean_q_target = (self.support_atoms.unsqueeze(0) * value_histogram_projected).sum(dim=1).mean()\n\n        return {\n            \'loss\': loss_value.item(),\n            # We need it to update priorities in the replay buffer:\n            \'errors\': original_losses.detach().cpu().numpy(),\n            \'average_q_selected\': mean_q_model.item(),\n            \'average_q_target\': mean_q_target.item()\n        }\n\n    def post_optimization_step(self, batch_info, device, model, rollout):\n        """""" Steps to take after optimization has been done""""""\n        if batch_info.aggregate_batch_number % self.target_update_frequency == 0:\n            self.target_model.load_state_dict(model.state_dict())\n            self.target_model.eval()\n\n    def metrics(self) -> list:\n        """""" List of metrics to track for this learning process """"""\n        return [\n            AveragingNamedMetric(""loss""),\n            AveragingNamedMetric(""average_q_selected""),\n            AveragingNamedMetric(""average_q_target""),\n            AveragingNamedMetric(""grad_norm""),\n        ]\n\n\ndef create(model: ModelFactory, discount_factor: float, target_update_frequency: int,\n           max_grad_norm: float, double_dqn: bool = False):\n    """""" Vel factory function """"""\n    return DistributionalDeepQLearning(\n        model_factory=model,\n        discount_factor=discount_factor,\n        double_dqn=double_dqn,\n        target_update_frequency=target_update_frequency,\n        max_grad_norm=max_grad_norm\n    )\n'"
vel/rl/algo/dqn.py,8,"b'import torch\nimport torch.nn.functional as F\nimport torch.nn.utils\n\nfrom vel.api import ModelFactory\nfrom vel.api.metrics.averaging_metric import AveragingNamedMetric\nfrom vel.rl.api import OptimizerAlgoBase\n\n\nclass DeepQLearning(OptimizerAlgoBase):\n    """""" Deep Q-Learning algorithm """"""\n\n    def __init__(self, model_factory: ModelFactory, discount_factor: float, double_dqn: bool, target_update_frequency: int,\n                 max_grad_norm: float):\n        super().__init__(max_grad_norm)\n\n        self.model_factory = model_factory\n        self.discount_factor = discount_factor\n\n        self.double_dqn = double_dqn\n        self.target_update_frequency = target_update_frequency\n\n        self.target_model = None\n\n    def initialize(self, training_info, model, environment, device):\n        """""" Initialize policy gradient from reinforcer settings """"""\n        self.target_model = self.model_factory.instantiate(action_space=environment.action_space).to(device)\n        self.target_model.load_state_dict(model.state_dict())\n        self.target_model.eval()\n\n    def calculate_gradient(self, batch_info, device, model, rollout):\n        """""" Calculate loss of the supplied rollout """"""\n        evaluator = model.evaluate(rollout)\n\n        dones_tensor = evaluator.get(\'rollout:dones\')\n        rewards_tensor = evaluator.get(\'rollout:rewards\')\n\n        assert dones_tensor.dtype == torch.float32\n\n        with torch.no_grad():\n            target_evaluator = self.target_model.evaluate(rollout)\n\n            if self.double_dqn:\n                # DOUBLE DQN\n                target_q = target_evaluator.get(\'model:q_next\')\n                model_q = evaluator.get(\'model:q_next\')\n                # Select largest \'target\' value based on action that \'model\' selects\n                values = target_q.gather(1, model_q.argmax(dim=1, keepdim=True)).squeeze(1)\n            else:\n                # REGULAR DQN\n                # [0] is because in pytorch .max(...) returns tuple (max values, argmax)\n                values = target_evaluator.get(\'model:q_next\').max(dim=1)[0]\n\n            forward_steps = rollout.extra_data.get(\'forward_steps\', 1)\n            estimated_return = rewards_tensor + (self.discount_factor ** forward_steps) * values * (1 - dones_tensor)\n\n        q_selected = evaluator.get(\'model:action:q\')\n\n        if evaluator.is_provided(\'rollout:weights\'):\n            weights = evaluator.get(\'rollout:weights\')\n        else:\n            weights = torch.ones_like(rewards_tensor)\n\n        original_losses = F.smooth_l1_loss(q_selected, estimated_return, reduction=\'none\')\n\n        loss_value = torch.mean(weights * original_losses)\n        loss_value.backward()\n\n        return {\n            \'loss\': loss_value.item(),\n            # We need it to update priorities in the replay buffer:\n            \'errors\': original_losses.detach().cpu().numpy(),\n            \'average_q_selected\': torch.mean(q_selected).item(),\n            \'average_q_target\': torch.mean(estimated_return).item()\n        }\n\n    def post_optimization_step(self, batch_info, device, model, rollout):\n        """""" Steps to take after optimization has been done""""""\n        if batch_info.aggregate_batch_number % self.target_update_frequency == 0:\n            self.target_model.load_state_dict(model.state_dict())\n            self.target_model.eval()\n\n    def metrics(self) -> list:\n        """""" List of metrics to track for this learning process """"""\n        return [\n            AveragingNamedMetric(""loss""),\n            AveragingNamedMetric(""average_q_selected""),\n            AveragingNamedMetric(""average_q_target""),\n            AveragingNamedMetric(""grad_norm""),\n        ]\n\n\ndef create(model: ModelFactory, discount_factor: float, target_update_frequency: int,\n           max_grad_norm: float, double_dqn: bool=False):\n    """""" Vel factory function """"""\n    return DeepQLearning(\n        model_factory=model,\n        discount_factor=discount_factor,\n        double_dqn=double_dqn,\n        target_update_frequency=target_update_frequency,\n        max_grad_norm=max_grad_norm\n    )\n'"
vel/rl/api/__init__.py,0,"b'from .algo_base import AlgoBase, OptimizerAlgoBase\nfrom .env_base import EnvFactory, VecEnvFactory\nfrom .env_roller import EnvRollerBase, ReplayEnvRollerBase, EnvRollerFactoryBase, ReplayEnvRollerFactoryBase\nfrom .evaluator import Evaluator\nfrom .model import RlModel, RlRnnModel\nfrom .reinforcer_base import ReinforcerBase, ReinforcerFactory\nfrom .replay_buffer import ReplayBuffer, ReplayBufferFactory\nfrom .rollout import Rollout, Trajectories, Transitions\n'"
vel/rl/api/algo_base.py,2,"b'import torch.nn.utils\n\n\ndef clip_gradients(batch_result, model, max_grad_norm):\n    """""" Clip gradients to a given maximum length """"""\n    if max_grad_norm is not None:\n        grad_norm = torch.nn.utils.clip_grad_norm_(\n            filter(lambda p: p.requires_grad, model.parameters()),\n            max_norm=max_grad_norm\n        )\n    else:\n        grad_norm = 0.0\n\n    batch_result[\'grad_norm\'] = grad_norm\n\n\nclass AlgoBase:\n    """""" Base class for algo reinforcement calculations """"""\n\n    def initialize(self, training_info, model, environment, device):\n        """""" Initialize algo from reinforcer settings """"""\n        pass\n\n    def process_rollout(self, batch_info, rollout):\n        """""" Process rollout for ALGO before any chunking/shuffling  """"""\n        return rollout\n\n    def optimizer_step(self, batch_info, device, model, rollout):\n        """""" Single optimization step for a model """"""\n        raise NotImplementedError\n\n    def metrics(self) -> list:\n        """""" List of metrics to track for this learning process """"""\n        return []\n\n\nclass OptimizerAlgoBase(AlgoBase):\n    """""" RL algo that does a simple optimizer update """"""\n\n    def __init__(self, max_grad_norm):\n        self.max_grad_norm = max_grad_norm\n\n    def calculate_gradient(self, batch_info, device, model, rollout):\n        """""" Calculate loss of the supplied rollout """"""\n        raise NotImplementedError\n\n    def post_optimization_step(self, batch_info, device, model, rollout):\n        """""" Steps to take after optimization has been done""""""\n        pass\n\n    def optimizer_step(self, batch_info, device, model, rollout):\n        """""" Single optimization step for a model """"""\n        batch_info.optimizer.zero_grad()\n\n        batch_result = self.calculate_gradient(batch_info=batch_info, device=device, model=model, rollout=rollout)\n\n        clip_gradients(batch_result, model, self.max_grad_norm)\n\n        batch_info.optimizer.step(closure=None)\n\n        self.post_optimization_step(batch_info, device, model, rollout)\n\n        return batch_result\n\n    def metrics(self) -> list:\n        """""" List of metrics to track for this learning process """"""\n        return []\n'"
vel/rl/api/env_base.py,0,"b'from gym import Env\nfrom gym.envs.registration import EnvSpec\nfrom vel.openai.baselines.common.vec_env import VecEnv\n\n\nclass EnvFactory:\n    """""" Base class for environment factory """"""\n\n    def specification(self) -> EnvSpec:\n        """""" Return environment specification """"""\n        raise NotImplementedError\n\n    def instantiate(self, seed=0, serial_id=1, preset=\'default\', extra_args=None) -> Env:\n        """""" Create a new Env instance """"""\n        raise NotImplementedError\n\n\nclass VecEnvFactory:\n    """""" Base class for vector environment factory """"""\n\n    def instantiate(self, parallel_envs, seed=0, preset=\'default\') -> VecEnv:\n        """""" Create a new VecEnv instance """"""\n        raise NotImplementedError\n\n    def instantiate_single(self, seed=0, preset=\'default\') -> VecEnv:\n        """""" Create a new VecEnv instance - single """"""\n        raise NotImplementedError\n\n'"
vel/rl/api/env_roller.py,0,"b'import typing\nimport gym\n\nfrom vel.rl.api.rollout import Rollout\nfrom vel.api import BatchInfo, Model\nfrom vel.openai.baselines.common.vec_env import VecEnv\n\n\nclass EnvRollerBase:\n    """""" Class generating environment rollouts """"""\n\n    @property\n    def environment(self) -> typing.Union[gym.Env, VecEnv]:\n        """""" Reference to environment being evaluated """"""\n        raise NotImplementedError\n\n    def rollout(self, batch_info: BatchInfo, model: Model, number_of_steps: int) -> Rollout:\n        """""" Roll-out the environment and return it """"""\n        raise NotImplementedError\n\n    def metrics(self) -> list:\n        """""" List of metrics to track for this learning process """"""\n        return []\n\n\n# noinspection PyAbstractClass\nclass ReplayEnvRollerBase(EnvRollerBase):\n    """""" Class generating environment rollouts with experience replay """"""\n\n    def sample(self, batch_info: BatchInfo, model: Model, number_of_steps: int) -> Rollout:\n        """""" Sample experience from replay buffer and return a batch """"""\n        raise NotImplementedError\n\n    def is_ready_for_sampling(self) -> bool:\n        """""" If buffer is ready for drawing samples from it (usually checks if there is enough data) """"""\n        raise NotImplementedError\n\n    def initial_memory_size_hint(self) -> typing.Optional[int]:\n        """""" Hint how much data is needed to begin sampling, required only for diagnostics """"""\n        return None\n\n    def update(self, rollout, batch_info):\n        """""" Perform update of the internal state of the buffer - e.g. for the prioritized replay weights """"""\n        pass\n\n\nclass EnvRollerFactoryBase:\n    """""" Factory for env rollers """"""\n\n    def instantiate(self, environment, device) -> EnvRollerBase:\n        """""" Instantiate env roller """"""\n        raise NotImplementedError\n\n\nclass ReplayEnvRollerFactoryBase(EnvRollerFactoryBase):\n    """""" Factory for env rollers """"""\n\n    def instantiate(self, environment, device) -> ReplayEnvRollerBase:\n        """""" Instantiate env roller """"""\n        raise NotImplementedError\n'"
vel/rl/api/evaluator.py,0,"b'class EvaluatorMeta(type):\n    """""" Metaclass for Evaluator - gathers all provider methods in a class attribute """"""\n    def __new__(mcs, name, bases, attributes):\n        providers = {}\n\n        for name, attr in attributes.items():\n            if callable(attr):\n                proper_name = getattr(attr, \'_vel_evaluator_provides\', None)\n\n                if proper_name is not None:\n                    providers[proper_name] = attr\n\n        attributes[\'_providers\'] = providers\n\n        return super().__new__(mcs, name, bases, attributes)\n\n\nclass Evaluator(metaclass=EvaluatorMeta):\n    """"""\n    Different models may have different outputs and approach evaluating environment differently.\n\n    Evaluator is an object that abstracts over that, providing unified interface between algorithms\n    which just need certain outputs from models and models that may provide them in different ways.\n\n    I\'ll try to maintain here a dictionary of possible common values that can be requested from the evaluator.\n    Rollouts should communicate using the same names\n\n    - rollout:estimated_returns\n        - Bootstrapped return (sum of discounted future rewards) estimated using returns and value estimates\n    - rollout:values\n        - Value estimates from the model that was used to generate the rollout\n    - rollout:estimated_advantages\n        - Advantage of a rollout (state, action) pair by the model that was used to generate the rollout\n    - rollout:actions\n        - Actions performed in a rollout\n    - rollout:logprobs\n        - Logarithm of probability for **all** actions of a policy used to perform rollout\n        (defined only for finite action spaces)\n    - rollout:action:logprobs\n        - Logarithm of probability only for selected actions\n    - rollout:dones\n        - Whether given observation is last in a trajectory\n    - rollout:dones\n        - Raw rewards received from the environment in this learning process\n    - rollout:final_values\n        - Value estimates for observation after final observation in the rollout\n    - rollout:observations\n        - Observations of the rollout\n    - rollout:observations_next\n        - Next observations in the rollout\n    - rollout:weights\n        - Error weights of rollout samples\n    - rollout:q\n        - Action-values for each action in current space\n        (defined only for finite action spaces)\n\n    - model:logprobs\n        - Logarithm of probability of **all** actions in an environment as in current model policy\n        (defined only for finite action spaces)\n    - model:q\n        - Action-value for **all** actions\n        (defined only for finite action spaces)\n    - model:q_dist\n        - Action-value histogram for **all** actions\n        (defined only for finite action spaces)\n    - model:q_dist_next\n        - Action-value histogram for **all** actions from the \'next\' state in the rollout\n        (defined only for finite action spaces)\n    - model:q_next\n        - Action-value for **all** actions from the \'next\' state in the rollout\n        (defined only for finite action spaces)\n    - model:entropy\n        - Policy entropy for selected states\n    - model:action:q\n        - Action-value for actions selected in the rollout\n    - model:model_action:q\n        - Action-value for actions that model would perform (Deterministic policy only)\n    - model:actions\n        - Actions that model would perform (Deterministic policy only)\n    - model:action:logprobs\n        - Logarithm of probability for performed actions\n    - model:policy_params\n        - Parametrizations of policy for each state\n    - model:values\n        - Value estimates for each state, estimated by the current model\n    - model:values_next\n        - Value estimates for \'next\' state of each transition\n    """"""\n\n    @staticmethod\n    def provides(name):\n        """""" Function decorator - value provided by the evaluator """"""\n        def decorator(func):\n            func._vel_evaluator_provides = name\n            return func\n\n        return decorator\n\n    def __init__(self, rollout):\n        self._storage = {}\n        self.rollout = rollout\n\n    def is_provided(self, name):\n        """""" Capability check if evaluator provides given value """"""\n        if name in self._storage:\n            return True\n        elif name in self._providers:\n            return True\n        elif name.startswith(\'rollout:\'):\n            rollout_name = name[8:]\n        else:\n            return False\n\n    def get(self, name):\n        """"""\n        Return a value from this evaluator.\n\n        Because tensor calculated is cached, it may lead to suble bugs if the same value is used multiple times\n        with and without no_grad() context.\n\n        It is advised in such cases to not use no_grad and stick to .detach()\n        """"""\n        if name in self._storage:\n            return self._storage[name]\n        elif name in self._providers:\n            value = self._storage[name] = self._providers[name](self)\n            return value\n        elif name.startswith(\'rollout:\'):\n            rollout_name = name[8:]\n            value = self._storage[name] = self.rollout.batch_tensor(rollout_name)\n            return value\n        else:\n            raise RuntimeError(f""Key {name} is not provided by this evaluator"")\n\n    def provide(self, name, value):\n        """""" Provide given value under specified name """"""\n        self._storage[name] = value\n'"
vel/rl/api/model.py,1,"b'import torch\n\nfrom vel.api import Model\n\nfrom .rollout import Rollout\nfrom .evaluator import Evaluator\n\n\nclass RlModel(Model):\n    """""" Reinforcement learning model """"""\n\n    def step(self, observations) -> dict:\n        """"""\n        Evaluate environment on given observations, return actions and potentially some extra information\n        in a dictionary.\n        """"""\n        raise NotImplementedError\n\n    def evaluate(self, rollout: Rollout) -> Evaluator:\n        """""" Evaluate model on a rollout """"""\n        raise NotImplementedError\n\n\nclass RlRnnModel(Model):\n    """""" Reinforcement learning recurrent model """"""\n\n    @property\n    def is_recurrent(self) -> bool:\n        """""" If the network is recurrent and needs to be fed previous state """"""\n        return True\n\n    def step(self, observations, state) -> dict:\n        """"""\n        Evaluate environment on given observations, return actions and potentially some extra information\n        in a dictionary.\n        """"""\n        raise NotImplementedError\n\n    @property\n    def state_dim(self) -> int:\n        """""" Dimension of model state """"""\n        raise NotImplementedError\n\n    def zero_state(self, batch_size):\n        """""" Initial state of the network """"""\n        return torch.zeros(batch_size, self.state_dim)\n\n    def evaluate(self, rollout: Rollout) -> Evaluator:\n        """""" Evaluate model on a rollout """"""\n        raise NotImplementedError\n'"
vel/rl/api/reinforcer_base.py,1,"b'import torch\n\nfrom vel.api import TrainingInfo, EpochInfo, BatchInfo, Model\n\n\nclass ReinforcerBase:\n    """"""\n    Manages training process of a single model.\n    Learner version for reinforcement-learning problems.\n    """"""\n\n    def initialize_training(self, training_info: TrainingInfo, model_state=None, hidden_state=None):\n        """""" Run the initialization procedure """"""\n        pass\n\n    def train_epoch(self, epoch_info: EpochInfo):\n        """""" Train model on an epoch of a fixed number of batch updates """"""\n        raise NotImplementedError\n\n    def train_batch(self, batch_info: BatchInfo):\n        """""" Single, most atomic \'step\' of learning this reinforcer can perform """"""\n        raise NotImplementedError\n\n    def metrics(self) -> list:\n        """""" List of metrics to track for this learning process """"""\n        raise NotImplementedError\n\n    @property\n    def model(self) -> Model:\n        """""" Model trained by this reinforcer """"""\n        raise NotImplementedError\n\n\nclass ReinforcerFactory:\n    """""" A reinforcer factory """"""\n    def instantiate(self, device: torch.device) -> ReinforcerBase:\n        """""" Create new reinforcer instance """"""\n        raise NotImplementedError\n'"
vel/rl/api/replay_buffer.py,0,"b'import typing\n\nfrom vel.openai.baselines.common.vec_env import VecEnv\n\nfrom .rollout import Trajectories, Transitions\n\n\nclass ReplayBuffer:\n    """""" Base class for a replay buffer """"""\n    def __init__(self):\n        pass\n\n    def is_ready_for_sampling(self) -> bool:\n        """""" If buffer is ready for drawing samples from it (usually checks if there is enough data) """"""\n        raise NotImplementedError\n\n    def initial_memory_size_hint(self) -> typing.Optional[int]:\n        """""" Hint how much data is needed to begin sampling, required only for diagnostics """"""\n        return None\n\n    def update(self, rollout, batch_info):\n        """""" Perform update of the internal state of the buffer - e.g. for the prioritized replay weights """"""\n        raise NotImplementedError\n\n    def sample_transitions(self, batch_size, batch_info) -> Transitions:\n        """""" Sample transitions from replay buffer """"""\n        raise NotImplementedError\n\n    def sample_forward_transitions(self, batch_size, batch_info, forward_steps: int,\n                                   discount_factor: float) -> Transitions:\n        """"""\n        Sample transitions from replay buffer with _forward steps_.\n        That is, instead of getting a transition s_t -> s_t+1 with reward r,\n        get a transition s_t -> s_t+n with sum of intermediate rewards.\n\n        Used in a variant of Deep Q-Learning\n        """"""\n        raise NotImplementedError\n\n    def sample_trajectories(self, rollout_length, batch_info) -> Trajectories:\n        """""" Sample transitions from replay buffer """"""\n        raise NotImplementedError\n\n    def store_transition(self, frame, action, reward, done, extra_info=None):\n        """""" Store given transition in the backend """"""\n        raise NotImplementedError\n\n\nclass ReplayBufferFactory:\n    """""" Create a replay buffer based on supplied environment """"""\n\n    def instantiate(self, environment: VecEnv):\n        raise NotImplementedError\n'"
vel/rl/api/rollout.py,0,"b'import numpy as np\n\nimport vel.util.math as math_util\nimport vel.util.tensor_util as tensor_util\n\n\nclass Rollout:\n    """""" Base class for environment rollout data """"""\n\n    def to_transitions(self) -> \'Transitions\':\n        """""" Convert given rollout to Transitions """"""\n        raise NotImplementedError\n\n    def episode_information(self):\n        """""" List of information about finished episodes """"""\n        raise NotImplementedError\n\n    def frames(self) -> int:\n        """""" Number of frames in rollout """"""\n        raise NotImplementedError\n\n    def batch_tensor(self, name):\n        """""" A buffer of a given value in a \'flat\' (minibatch-indexed) format """"""\n        raise NotImplementedError\n\n    def has_tensor(self, name):\n        """""" Return true if rollout contains tensor with given name """"""\n        raise NotImplementedError\n\n    def to_device(self, device):\n        """""" Move a rollout to a selected device """"""\n        raise NotImplementedError\n\n\nclass Transitions(Rollout):\n    """"""\n    Rollout of random transitions, that don\'t necessarily have to be in any order\n\n    transition_tensors - tensors that have a row (multidimensional) per each transition. E.g. state, reward, done\n    """"""\n    def __init__(self, size, environment_information, transition_tensors, extra_data=None):\n        self.size = size\n        self.environment_information = environment_information\n        self.transition_tensors = transition_tensors\n        self.extra_data = extra_data if extra_data is not None else {}\n\n    def to_transitions(self) -> \'Transitions\':\n        """""" Convert given rollout to Transitions """"""\n        return self\n\n    def episode_information(self):\n        """""" List of information about finished episodes """"""\n        return [info.get(\'episode\') for info in self.environment_information if \'episode\' in info]\n\n    def frames(self):\n        """""" Number of frames in this rollout """"""\n        return self.size\n\n    def shuffled_batches(self, batch_size):\n        """""" Generate randomized batches of data """"""\n        if batch_size >= self.size:\n            yield self\n        else:\n            batch_splits = math_util.divide_ceiling(self.size, batch_size)\n            indices = list(range(self.size))\n            np.random.shuffle(indices)\n\n            for sub_indices in np.array_split(indices, batch_splits):\n                yield Transitions(\n                    size=len(sub_indices),\n                    environment_information=None,\n                    # Dont use it in batches for a moment, can be uncommented later if needed\n                    # environment_information=[info[sub_indices.tolist()] for info in self.environment_information]\n                    transition_tensors={k: v[sub_indices] for k, v in self.transition_tensors.items()}\n                    # extra_data does not go into batches\n                )\n\n    def has_tensor(self, name):\n        """""" Return true if rollout contains tensor with given name """"""\n        return name in self.transition_tensors\n\n    def batch_tensor(self, name):\n        """""" A buffer of a given value in a \'flat\' (minibatch-indexed) format """"""\n        return self.transition_tensors[name]\n\n    def to_device(self, device, non_blocking=True):\n        """""" Move a rollout to a selected device """"""\n        return Transitions(\n            size=self.size,\n            environment_information=self.environment_information,\n            transition_tensors={k: v.to(device, non_blocking=non_blocking) for k, v in self.transition_tensors.items()},\n            extra_data=self.extra_data\n        )\n\n\nclass Trajectories(Rollout):\n    """"""\n    Rollout of trajectories - a number of consecutive transitions\n\n    transition_tensors - tensors that have a row (multidimensional) per each transition. E.g. state, reward, done\n    rollout_tensors - tensors that have a row (multidimensional) per whole rollout. E.g. final_value, initial rnn state\n    """"""\n    def __init__(self, num_steps, num_envs, environment_information, transition_tensors, rollout_tensors, extra_data=None):\n        self.num_steps = num_steps\n        self.num_envs = num_envs\n        self.environment_information = environment_information\n        self.transition_tensors = transition_tensors\n        self.rollout_tensors = rollout_tensors\n        self.extra_data = extra_data if extra_data is not None else {}\n\n    def to_transitions(self) -> \'Transitions\':\n        """""" Convert given rollout to Transitions """"""\n        # No need to propagate \'rollout_tensors\' as they won\'t mean anything\n        return Transitions(\n            size=self.num_steps * self.num_envs,\n            environment_information=\n                [ei for l in self.environment_information for ei in l]\n                if self.environment_information is not None else None,\n            transition_tensors={\n                name: tensor_util.merge_first_two_dims(t) for name, t in self.transition_tensors.items()\n            },\n            extra_data=self.extra_data\n        )\n\n    def shuffled_batches(self, batch_size):\n        """""" Generate randomized batches of data - only sample whole trajectories """"""\n        if batch_size >= self.num_envs * self.num_steps:\n            yield self\n        else:\n            rollouts_in_batch = batch_size // self.num_steps\n\n            batch_splits = math_util.divide_ceiling(self.num_envs, rollouts_in_batch)\n\n            indices = list(range(self.num_envs))\n            np.random.shuffle(indices)\n\n            for sub_indices in np.array_split(indices, batch_splits):\n                yield Trajectories(\n                    num_steps=self.num_steps,\n                    num_envs=len(sub_indices),\n                    # Dont use it in batches for a moment, can be uncommented later if needed\n                    # environment_information=[x[sub_indices.tolist()] for x in self.environment_information],\n                    environment_information=None,\n                    transition_tensors={k: x[:, sub_indices] for k, x in self.transition_tensors.items()},\n                    rollout_tensors={k: x[sub_indices] for k, x in self.rollout_tensors.items()},\n                    # extra_data does not go into batches\n                )\n\n    def batch_tensor(self, name):\n        """""" A buffer of a given value in a \'flat\' (minibatch-indexed) format """"""\n        if name in self.transition_tensors:\n            return tensor_util.merge_first_two_dims(self.transition_tensors[name])\n        else:\n            return self.rollout_tensors[name]\n\n    def has_tensor(self, name):\n        """""" Return true if rollout contains tensor with given name """"""\n        return (name in self.transition_tensors) or (name in self.rollout_tensors)\n\n    def flatten_tensor(self, tensor):\n        """""" Merge first two dims of a tensor """"""\n        return tensor_util.merge_first_two_dims(tensor)\n\n    def episode_information(self):\n        """""" List of information about finished episodes """"""\n        return [\n            info.get(\'episode\') for infolist in self.environment_information for info in infolist if \'episode\' in info\n        ]\n\n    def frames(self):\n        """""" Number of frames in rollout """"""\n        return self.num_steps * self.num_envs\n\n    def to_device(self, device, non_blocking=True):\n        """""" Move a rollout to a selected device """"""\n        return Trajectories(\n            num_steps=self.num_steps,\n            num_envs=self.num_envs,\n            environment_information=self.environment_information,\n            transition_tensors={k: v.to(device, non_blocking=non_blocking) for k, v in self.transition_tensors.items()},\n            rollout_tensors={k: v.to(device, non_blocking=non_blocking) for k, v in self.rollout_tensors.items()},\n            extra_data=self.extra_data\n        )\n'"
vel/rl/buffers/__init__.py,0,b''
vel/rl/buffers/circular_replay_buffer.py,3,"b'import gym\nimport torch\nimport typing\n\nfrom vel.rl.api import ReplayBuffer, ReplayBufferFactory, Transitions, Trajectories\nfrom .backend.circular_vec_buffer_backend import CircularVecEnvBufferBackend\n\n\nclass CircularReplayBuffer(ReplayBuffer):\n    """"""\n    Replay buffer that uses a circular buffer - new experience overwrites the oldest one\n    Version supporting multiple environments.\n\n    Frame stack compensation - if environment has a framestack built in, we will store only the last frame\n    """"""\n\n    def __init__(self, buffer_capacity: int, buffer_initial_size: int, num_envs: int, observation_space: gym.Space,\n                 action_space: gym.Space, frame_stack_compensation: bool = False, frame_history: int = 1):\n        super().__init__()\n\n        self.buffer_initial_size = buffer_initial_size\n\n        self.backend = CircularVecEnvBufferBackend(\n            buffer_capacity=buffer_capacity,\n            num_envs=num_envs,\n            observation_space=observation_space,\n            action_space=action_space,\n            frame_stack_compensation=frame_stack_compensation,\n            frame_history=frame_history\n        )\n\n    def is_ready_for_sampling(self) -> bool:\n        """""" If buffer is ready for drawing samples from it (usually checks if there is enough data) """"""\n        return self.buffer_initial_size <= self.backend.current_size\n\n    def initial_memory_size_hint(self) -> typing.Optional[int]:\n        """""" Hint how much data is needed to begin sampling, required only for diagnostics """"""\n        return self.buffer_initial_size\n\n    def _get_transitions(self, indexes):\n        """""" Return batch with given indexes """"""\n        transition_tensors = self.backend.get_transitions(indexes)\n\n        return Trajectories(\n            num_steps=indexes.shape[0],\n            num_envs=indexes.shape[1],\n            environment_information=None,\n            transition_tensors=transition_tensors,\n            rollout_tensors={}\n        ).to_transitions()\n\n    def sample_trajectories(self, rollout_length, batch_info) -> Trajectories:\n        """""" Sample batch of trajectories and return them """"""\n        indexes = self.backend.sample_batch_trajectories(rollout_length)\n        transition_tensors = self.backend.get_trajectories(indexes, rollout_length)\n\n        return Trajectories(\n            num_steps=rollout_length,\n            num_envs=self.backend.num_envs,\n            environment_information=None,\n            transition_tensors={k: torch.from_numpy(v) for k, v in transition_tensors.items()},\n            rollout_tensors={}\n        )\n\n    def sample_transitions(self, batch_size, batch_info, discount_factor=None) -> Transitions:\n        """""" Sample batch of transitions and return them """"""\n        indexes = self.backend.sample_batch_transitions(batch_size)\n        transition_tensors = self.backend.get_transitions(indexes)\n\n        return Trajectories(\n            num_steps=batch_size,\n            num_envs=self.backend.num_envs,\n            environment_information=None,\n            transition_tensors={k: torch.from_numpy(v) for k, v in transition_tensors.items()},\n            rollout_tensors={}\n        ).to_transitions()\n\n    def sample_forward_transitions(self, batch_size, batch_info, forward_steps: int,\n                                   discount_factor: float) -> Transitions:\n        """"""\n        Sample transitions from replay buffer with _forward steps_.\n        That is, instead of getting a transition s_t -> s_t+1 with reward r,\n        get a transition s_t -> s_t+n with sum of intermediate rewards.\n\n        Used in a variant of Deep Q-Learning\n        """"""\n        indexes = self.backend.sample_batch_transitions(batch_size, forward_steps=forward_steps)\n        transition_tensors = self.backend.get_transitions_forward_steps(\n            indexes, forward_steps=forward_steps, discount_factor=discount_factor\n        )\n\n        return Trajectories(\n            num_steps=batch_size,\n            num_envs=self.backend.num_envs,\n            environment_information=None,\n            transition_tensors={k: torch.from_numpy(v) for k, v in transition_tensors.items()},\n            rollout_tensors={},\n            extra_data={\n                \'forward_steps\': forward_steps\n            }\n        ).to_transitions()\n\n    def store_transition(self, frame, action, reward, done, extra_info=None):\n        """""" Store given transition in the backend """"""\n        self.backend.store_transition(frame=frame, action=action, reward=reward, done=done, extra_info=extra_info)\n\n    def update(self, rollout, batch_info):\n        """""" Don\'t need to update anything """"""\n        pass\n\n\nclass CircularReplayBufferFactory(ReplayBufferFactory):\n    """""" Factory class for the CircularReplayBuffer """"""\n\n    def __init__(self, buffer_capacity: int, buffer_initial_size: int,\n                 frame_stack_compensation: bool = False, frame_history: int = 1):\n        self.buffer_capacity = buffer_capacity\n        self.buffer_initial_size = buffer_initial_size\n        self.frame_stack_compensation = frame_stack_compensation\n        self.frame_history = frame_history\n\n    def instantiate(self, environment):\n        return CircularReplayBuffer(\n            buffer_capacity=self.buffer_capacity,\n            buffer_initial_size=self.buffer_initial_size,\n            num_envs=environment.num_envs,\n            observation_space=environment.observation_space,\n            action_space=environment.action_space,\n            frame_stack_compensation=self.frame_stack_compensation,\n            frame_history=self.frame_history\n        )\n\n\ndef create(buffer_capacity: int, buffer_initial_size: int, frame_stack_compensation: bool = False,\n           frame_history: int = 1):\n    """""" Vel factory function """"""\n    return CircularReplayBufferFactory(\n        buffer_capacity=buffer_capacity,\n        buffer_initial_size=buffer_initial_size,\n        frame_stack_compensation=frame_stack_compensation,\n        frame_history=frame_history\n    )\n'"
vel/rl/buffers/prioritized_circular_replay_buffer.py,1,"b'import gym\nimport numpy as np\nimport torch\nimport typing\n\nfrom vel.api import Schedule\nfrom vel.rl.api import ReplayBuffer, Trajectories, Transitions\nfrom .backend.prioritized_vec_buffer_backend import PrioritizedCircularVecEnvBufferBackend\n\n\nclass PrioritizedCircularReplayBuffer(ReplayBuffer):\n    """"""\n    Replay buffer that supports prioritized experience replay as an overlay over a circular buffer.\n\n    Frame stack compensation - if environment has a framestack built in, we will store only the last frame\n    """"""\n\n    def __init__(self, buffer_capacity: int, buffer_initial_size: int, num_envs: int, observation_space: gym.Space,\n                 action_space: gym.Space, priority_exponent: float, priority_weight: Schedule, priority_epsilon: float,\n                 frame_stack_compensation: bool = False, frame_history: int = 1):\n        super().__init__()\n\n        self.buffer_initial_size = buffer_initial_size\n\n        self.priority_exponent = priority_exponent\n        self.priority_weight = priority_weight\n        self.priority_epsilon = priority_epsilon\n\n        self.backend = PrioritizedCircularVecEnvBufferBackend(\n            buffer_capacity=buffer_capacity,\n            num_envs=num_envs,\n            observation_space=observation_space,\n            action_space=action_space,\n            frame_stack_compensation=frame_stack_compensation,\n            frame_history=frame_history\n        )\n\n    def is_ready_for_sampling(self) -> bool:\n        """""" If buffer is ready for drawing samples from it (usually checks if there is enough data) """"""\n        return self.buffer_initial_size <= self.backend.current_size\n\n    def initial_memory_size_hint(self) -> typing.Optional[int]:\n        """""" Hint how much data is needed to begin sampling, required only for diagnostics """"""\n        return self.buffer_initial_size\n\n    def _get_transitions(self, probs, indexes, tree_idxs, batch_info, forward_steps=1, discount_factor=1.0):\n        """""" Return batch of frames for given indexes """"""\n        if forward_steps > 1:\n            transition_arrays = self.backend.get_transitions_forward_steps(indexes, forward_steps, discount_factor)\n        else:\n            transition_arrays = self.backend.get_transitions(indexes)\n\n        priority_weight = self.priority_weight.value(batch_info[\'progress\'])\n\n        # Normalize by sum of all probs\n        probs = probs / np.array([s.total() for s in self.backend.segment_trees], dtype=float).reshape(1, -1)\n        capacity = self.backend.current_size\n        weights = (capacity * probs) ** (-priority_weight)\n        weights = weights / weights.max(axis=0, keepdims=True)\n\n        transition_arrays[\'weights\'] = weights\n        transition_tensors = {k: torch.from_numpy(v) for k, v in transition_arrays.items()}\n\n        transitions = Trajectories(\n            num_steps=indexes.shape[0],\n            num_envs=indexes.shape[1],\n            environment_information=None,\n            transition_tensors=transition_tensors,\n            rollout_tensors={},\n            extra_data={\n                \'tree_idxs\': tree_idxs\n            }\n        )\n\n        return transitions.to_transitions()\n\n    def sample_transitions(self, batch_size, batch_info) -> Transitions:\n        """""" Sample batch of transitions and return them """"""\n        probs, indexes, tree_idxs = self.backend.sample_batch_transitions(batch_size)\n\n        return self._get_transitions(probs, indexes, tree_idxs, batch_info)\n\n    def sample_forward_transitions(self, batch_size, batch_info,\n                                   forward_steps: int, discount_factor: float) -> Transitions:\n        """"""\n        Sample transitions from replay buffer with _forward steps_.\n        That is, instead of getting a transition s_t -> s_t+1 with reward r,\n        get a transition s_t -> s_t+n with sum of intermediate rewards.\n\n        Used in a variant of Deep Q-Learning\n        """"""\n        probs, indexes, tree_idxs = self.backend.sample_batch_transitions(batch_size, forward_steps)\n\n        return self._get_transitions(\n            probs, indexes, tree_idxs, batch_info\n        )\n\n    def sample_trajectories(self, rollout_length, batch_info):\n        """""" Sample batch of trajectories and return them """"""\n        raise NotImplementedError(""There is no good idea so far how to sample trajectories in a prioritized way"")\n\n    def store_transition(self, frame, action, reward, done, extra_info=None):\n        """""" Store given transition in the backend """"""\n        self.backend.store_transition(frame=frame, action=action, reward=reward, done=done, extra_info=extra_info)\n\n    def update(self, rollout, batch_info):\n        tree_idxs = rollout.extra_data[\'tree_idxs\']\n        errors = batch_info[\'errors\'].reshape(tree_idxs.shape)\n\n        weights = (errors + self.priority_epsilon) ** self.priority_exponent\n\n        for idx, priority in zip(tree_idxs, weights):\n            self.backend.update_priority(idx, priority)\n\n\nclass PrioritizedCircularVecEnvBufferBackendFactory:\n    """""" Factory class for the CircularVecEnvBufferBackend """"""\n\n    def __init__(self, buffer_capacity: int, buffer_initial_size: int, priority_exponent: float,\n                 priority_weight: Schedule, priority_epsilon: float, frame_stack_compensation: bool = False,\n                 frame_history: int = 1):\n        self.buffer_capacity = buffer_capacity\n        self.buffer_initial_size = buffer_initial_size\n        self.frame_stack_compensation = frame_stack_compensation\n        self.frame_history = frame_history\n        self.priority_exponent = priority_exponent\n        self.priority_weight = priority_weight\n        self.priority_epsilon = priority_epsilon\n\n    def instantiate(self, environment):\n        return PrioritizedCircularReplayBuffer(\n            buffer_capacity=self.buffer_capacity,\n            buffer_initial_size=self.buffer_initial_size,\n            num_envs=environment.num_envs,\n            observation_space=environment.observation_space,\n            action_space=environment.action_space,\n            priority_exponent=self.priority_exponent,\n            priority_weight=self.priority_weight,\n            priority_epsilon=self.priority_epsilon,\n            frame_stack_compensation=self.frame_stack_compensation,\n            frame_history=self.frame_history\n        )\n\n\ndef create(buffer_capacity: int, buffer_initial_size: int, priority_exponent: float, priority_weight: Schedule,\n           priority_epsilon: float, frame_stack_compensation: bool = False, frame_history: int = 1):\n    """""" Vel factory function """"""\n    return PrioritizedCircularVecEnvBufferBackendFactory(\n        buffer_capacity=buffer_capacity,\n        buffer_initial_size=buffer_initial_size,\n        priority_exponent=priority_exponent,\n        priority_weight=priority_weight,\n        priority_epsilon=priority_epsilon,\n        frame_stack_compensation=frame_stack_compensation,\n        frame_history=frame_history\n    )\n'"
vel/rl/commands/__init__.py,0,b''
vel/rl/commands/enjoy.py,2,"b'import numpy as np\nimport torch\nimport typing\nimport time\n\nfrom vel.api import ModelConfig, TrainingInfo, Storage, ModelFactory\nfrom vel.rl.api import VecEnvFactory\n\n\nclass EnjoyCommand:\n    """""" Play render(""human"") in a loop for a human to enjoy """"""\n\n    def __init__(self, model_config: ModelConfig, model_factory: ModelFactory, vec_env_factory: VecEnvFactory,\n                 storage: Storage, fps: float, sample_args: typing.Optional[dict]):\n        self.model_config = model_config\n        self.model_factory = model_factory\n        self.vec_env_factory = vec_env_factory\n        self.storage = storage\n\n        self.fps = fps\n\n        self.sample_args = sample_args if sample_args is not None else {}\n\n    def run(self):\n        """""" Run the command """"""\n        device = self.model_config.torch_device()\n\n        env = self.vec_env_factory.instantiate_single(preset=\'record\', seed=self.model_config.seed)\n        model = self.model_factory.instantiate(action_space=env.action_space).to(device)\n\n        training_info = TrainingInfo(\n            start_epoch_idx=self.storage.last_epoch_idx(),\n            run_name=self.model_config.run_name\n        )\n\n        self.storage.load(training_info, model)\n\n        model.eval()\n\n        self.run_model(model, env, device)\n\n    @torch.no_grad()\n    def run_model(self, model, environment, device):\n        observation = environment.reset()\n        current_time = time.time()\n\n        seconds_per_frame = 1.0 / self.fps\n\n        if model.is_recurrent:\n            hidden_state = model.zero_state(1).to(device)\n\n        while True:\n            observation_array = np.expand_dims(np.array(observation), axis=0)\n            observation_tensor = torch.from_numpy(observation_array).to(device)\n\n            if model.is_recurrent:\n                output = model.step(observation_tensor, hidden_state, **self.sample_args)\n                hidden_state = output[\'state\']\n                actions = output[\'actions\']\n            else:\n                actions = model.step(observation_tensor, **self.sample_args)[\'actions\']\n\n            actions = actions.detach().cpu().numpy()\n\n            observation, reward, done, epinfo = environment.step(actions[0])\n\n            environment.render(\'human\')\n\n            frame_time = time.time()\n\n            if (frame_time - current_time) < seconds_per_frame:\n                time.sleep(seconds_per_frame - (frame_time - current_time))\n\n            current_time = frame_time\n\n            if \'episode\' in epinfo:\n                # End of an episode\n                break\n\n\ndef create(model_config, model, vec_env, storage, fps=30.0, sample_args=None):\n    """""" Vel factory function """"""\n    return EnjoyCommand(\n        model_config, model, vec_env, storage, float(fps), sample_args\n    )\n'"
vel/rl/commands/evaluate_env_command.py,4,"b'import numpy as np\nimport pandas as pd\nimport torch\nimport tqdm\nimport typing\n\nfrom vel.api import ModelConfig, TrainingInfo, Storage, ModelFactory\nfrom vel.rl.api import VecEnvFactory\n\n\nclass EvaluateEnvCommand:\n    """""" Record environment playthrough as a game  """"""\n    def __init__(self, model_config: ModelConfig, env_factory: VecEnvFactory, model_factory: ModelFactory,\n                 storage: Storage, parallel_envs: int, action_noise: typing.Optional[ModelFactory],  takes: int, sample_args: dict = None):\n        self.model_config = model_config\n        self.model_factory = model_factory\n        self.env_factory = env_factory\n        self.storage = storage\n        self.takes = takes\n        self.parallel_envs = parallel_envs\n        self.action_noise_factory = action_noise\n\n        self.sample_args = sample_args if sample_args is not None else {}\n\n    @torch.no_grad()\n    def run(self):\n        device = self.model_config.torch_device()\n\n        env = self.env_factory.instantiate(parallel_envs=self.parallel_envs, preset=\'record\', seed=self.model_config.seed)\n        model = self.model_factory.instantiate(action_space=env.action_space).to(device)\n\n        if self.action_noise_factory is not None:\n            action_noise = self.action_noise_factory.instantiate(environment=env).to(device)\n        else:\n            action_noise = None\n\n        training_info = TrainingInfo(\n            start_epoch_idx=self.storage.last_epoch_idx(), run_name=self.model_config.run_name\n        )\n\n        model_state, hidden_state = self.storage.load(training_info)\n        model.load_state_dict(model_state)\n\n        print(""Loading model trained for {} epochs"".format(training_info.start_epoch_idx))\n\n        model.eval()\n\n        episode_rewards = []\n        episode_lengths = []\n\n        observations = env.reset()\n        observations_tensor = torch.from_numpy(observations).to(device)\n\n        if model.is_recurrent:\n            hidden_state = model.zero_state(observations.shape[0]).to(device)\n\n        with tqdm.tqdm(total=self.takes) as progress_bar:\n            while len(episode_rewards) < self.takes:\n                if model.is_recurrent:\n                    output = model.step(observations_tensor, hidden_state, **self.sample_args)\n                    hidden_state = output[\'state\']\n                    actions = output[\'actions\']\n                else:\n                    actions = model.step(observations_tensor, **self.sample_args)[\'actions\']\n\n                if action_noise is not None:\n                    actions = action_noise(actions)\n\n                observations, rewards, dones, infos = env.step(actions.cpu().numpy())\n                observations_tensor = torch.from_numpy(observations).to(device)\n\n                for info in infos:\n                    if \'episode\' in info:\n                        episode_rewards.append(info[\'episode\'][\'r\'])\n                        episode_lengths.append(info[\'episode\'][\'l\'])\n                        progress_bar.update(1)\n\n                if model.is_recurrent:\n                    # Zero state belongiong to finished episodes\n                    dones_tensor = torch.from_numpy(dones.astype(np.float32)).to(device)\n                    hidden_state = hidden_state * (1.0 - dones_tensor.unsqueeze(-1))\n\n        print(pd.DataFrame({\'lengths\': episode_lengths, \'rewards\': episode_rewards}).describe())\n\n\ndef create(model_config, model, vec_env, storage, takes, parallel_envs, action_noise=None, sample_args=None):\n    """""" Vel factory function """"""\n    return EvaluateEnvCommand(\n        model_config=model_config,\n        model_factory=model,\n        env_factory=vec_env,\n        parallel_envs=parallel_envs,\n        action_noise=action_noise,\n        storage=storage,\n        takes=takes,\n        sample_args=sample_args\n    )\n'"
vel/rl/commands/record_movie_command.py,2,"b'import cv2\nimport numpy as np\nimport os.path\nimport pathlib\nimport sys\nimport torch\nimport tqdm\nimport typing\n\nfrom vel.api import ModelConfig, TrainingInfo, Storage, ModelFactory\nfrom vel.rl.api import VecEnvFactory\n\n\nclass RecordMovieCommand:\n    """""" Record environment playthrough as a game  """"""\n    def __init__(self, model_config: ModelConfig, env_factory: VecEnvFactory, model_factory: ModelFactory,\n                 storage: Storage, videoname: str, takes: int, fps: int, sample_args: typing.Optional[dict] = None):\n        self.model_config = model_config\n        self.model_factory = model_factory\n        self.env_factory = env_factory\n        self.storage = storage\n        self.takes = takes\n        self.videoname = videoname\n        self.sample_args = sample_args if sample_args is not None else {}\n        self.fps = fps\n\n    def run(self):\n        device = self.model_config.torch_device()\n\n        env = self.env_factory.instantiate_single(preset=\'record\', seed=self.model_config.seed)\n        model = self.model_factory.instantiate(action_space=env.action_space).to(device)\n\n        training_info = TrainingInfo(\n            start_epoch_idx=self.storage.last_epoch_idx(),\n            run_name=self.model_config.run_name\n        )\n\n        model_state, hidden_state = self.storage.load(training_info)\n        model.load_state_dict(model_state)\n\n        model.eval()\n\n        for i in range(self.takes):\n            self.record_take(model, env, device, take_number=i + 1)\n\n    @torch.no_grad()\n    def record_take(self, model, env_instance, device, take_number):\n        """""" Record a single movie and store it on hard drive """"""\n        frames = []\n\n        observation = env_instance.reset()\n\n        if model.is_recurrent:\n            hidden_state = model.zero_state(1).to(device)\n\n        frames.append(env_instance.render(\'rgb_array\'))\n\n        print(""Evaluating environment..."")\n\n        while True:\n            observation_array = np.expand_dims(np.array(observation), axis=0)\n            observation_tensor = torch.from_numpy(observation_array).to(device)\n\n            if model.is_recurrent:\n                output = model.step(observation_tensor, hidden_state, **self.sample_args)\n                hidden_state = output[\'state\']\n                actions = output[\'actions\']\n            else:\n                actions = model.step(observation_tensor, **self.sample_args)[\'actions\']\n\n            actions = actions.detach().cpu().numpy()\n\n            observation, reward, done, epinfo = env_instance.step(actions[0])\n\n            frames.append(env_instance.render(\'rgb_array\'))\n\n            if \'episode\' in epinfo:\n                # End of an episode\n                break\n\n        takename = self.model_config.output_dir(\'videos\', self.model_config.run_name, self.videoname.format(take_number))\n        pathlib.Path(os.path.dirname(takename)).mkdir(parents=True, exist_ok=True)\n\n        fourcc = cv2.VideoWriter_fourcc(\'M\', \'J\', \'P\', \'G\')\n        video = cv2.VideoWriter(takename, fourcc, self.fps, (frames[0].shape[1], frames[0].shape[0]))\n\n        for i in tqdm.trange(len(frames), file=sys.stdout):\n            video.write(cv2.cvtColor(frames[i], cv2.COLOR_RGB2BGR))\n\n        video.release()\n        print(""Written {}"".format(takename))\n\n\ndef create(model_config, model, vec_env, storage, takes, videoname, fps=30, sample_args=None):\n    """""" Vel factory function """"""\n    return RecordMovieCommand(\n        model_config=model_config,\n        model_factory=model,\n        env_factory=vec_env,\n        storage=storage,\n        videoname=videoname,\n        takes=takes,\n        fps=fps,\n        sample_args=sample_args\n    )\n'"
vel/rl/commands/rl_train_command.py,0,"b'import typing\n\nfrom vel.api import ModelConfig, EpochInfo, TrainingInfo, BatchInfo, OptimizerFactory, Storage, Callback\nfrom vel.rl.api import ReinforcerFactory\nfrom vel.callbacks.time_tracker import TimeTracker\n\nimport vel.openai.baselines.logger as openai_logger\n\n\nclass FrameTracker(Callback):\n    """""" Aggregate frame count from each batch to a global number """"""\n    def __init__(self, max_frames: typing.Optional[typing.Union[int, float]] = None):\n        self.max_frames = max_frames\n\n    def on_initialization(self, training_info: TrainingInfo):\n        if self.max_frames is not None:\n            training_info[\'total_frames\'] = int(self.max_frames)\n\n        training_info[\'frames\'] = 0\n\n    def on_batch_begin(self, batch_info: BatchInfo):\n        if \'total_frames\' in batch_info.training_info:\n            # Track progress during learning\n            batch_info[\'progress\'] = (\n                float(batch_info.training_info[\'frames\']) / batch_info.training_info[\'total_frames\']\n            )\n\n    def on_batch_end(self, batch_info: BatchInfo):\n        batch_info.training_info[\'frames\'] += batch_info[\'frames\']\n\n    def write_state_dict(self, training_info: TrainingInfo, hidden_state_dict: dict):\n        hidden_state_dict[\'frame_tracker/frames\'] = training_info[\'frames\']\n\n        if \'total_frames\' in training_info:\n            hidden_state_dict[\'frame_tracker/total_frames\'] = training_info[\'total_frames\']\n\n    def load_state_dict(self, training_info: TrainingInfo, hidden_state_dict: dict):\n        training_info[\'frames\'] = hidden_state_dict[\'frame_tracker/frames\']\n\n        if \'frame_tracker/total_frames\' in hidden_state_dict:\n            training_info[\'total_frames\'] = hidden_state_dict[\'frame_tracker/total_frames\']\n\n\nclass RlTrainCommand:\n    """""" Train a reinforcement learning algorithm by evaluating the environment and """"""\n    def __init__(self, model_config: ModelConfig, reinforcer: ReinforcerFactory,\n                 optimizer_factory: OptimizerFactory,\n                 storage: Storage, callbacks,\n                 total_frames: int, batches_per_epoch: int,\n                 scheduler_factory=None, openai_logging=False):\n        self.model_config = model_config\n        self.reinforcer = reinforcer\n        self.optimizer_factory = optimizer_factory\n        self.scheduler_factory = scheduler_factory\n        self.storage = storage\n        self.total_frames = total_frames\n        self.batches_per_epoch = batches_per_epoch\n        self.callbacks = callbacks if callbacks is not None else []\n\n        self.openai_logging = openai_logging\n\n    def run(self):\n        """""" Run reinforcement learning algorithm """"""\n        device = self.model_config.torch_device()\n\n        # Reinforcer is the learner for the reinforcement learning model\n        reinforcer = self.reinforcer.instantiate(device)\n        optimizer = self.optimizer_factory.instantiate(reinforcer.model)\n\n        # All callbacks used for learning\n        callbacks = self.gather_callbacks(optimizer)\n        # Metrics to track through this training\n        metrics = reinforcer.metrics()\n\n        training_info = self.resume_training(reinforcer, callbacks, metrics)\n\n        reinforcer.initialize_training(training_info)\n        training_info.on_train_begin()\n\n        if training_info.optimizer_initial_state:\n            optimizer.load_state_dict(training_info.optimizer_initial_state)\n\n        global_epoch_idx = training_info.start_epoch_idx + 1\n\n        while training_info[\'frames\'] < self.total_frames:\n            epoch_info = EpochInfo(\n                training_info,\n                global_epoch_idx=global_epoch_idx,\n                batches_per_epoch=self.batches_per_epoch,\n                optimizer=optimizer,\n            )\n\n            reinforcer.train_epoch(epoch_info)\n\n            if self.openai_logging:\n                self._openai_logging(epoch_info.result)\n\n            self.storage.checkpoint(epoch_info, reinforcer.model)\n\n            global_epoch_idx += 1\n\n        training_info.on_train_end()\n\n        return training_info\n\n    def gather_callbacks(self, optimizer) -> list:\n        """""" Gather all the callbacks to be used in this training run """"""\n        callbacks = [FrameTracker(self.total_frames), TimeTracker()]\n\n        if self.scheduler_factory is not None:\n            callbacks.append(self.scheduler_factory.instantiate(optimizer))\n\n        callbacks.extend(self.callbacks)\n        callbacks.extend(self.storage.streaming_callbacks())\n\n        return callbacks\n\n    def resume_training(self, reinforcer, callbacks, metrics) -> TrainingInfo:\n        """""" Possibly resume training from a saved state from the storage """"""\n        if self.model_config.continue_training:\n            start_epoch = self.storage.last_epoch_idx()\n        else:\n            start_epoch = 0\n\n        training_info = TrainingInfo(\n            start_epoch_idx=start_epoch,\n            run_name=self.model_config.run_name,\n            metrics=metrics, callbacks=callbacks\n        )\n\n        if start_epoch == 0:\n            self.storage.reset(self.model_config.render_configuration())\n            training_info.initialize()\n            reinforcer.initialize_training(training_info)\n        else:\n            model_state, hidden_state = self.storage.load(training_info)\n            reinforcer.initialize_training(training_info, model_state, hidden_state)\n\n        return training_info\n\n    def _openai_logging(self, epoch_result):\n        """""" Use OpenAI logging facilities for the same type of logging """"""\n        for key in sorted(epoch_result.keys()):\n            if key == \'fps\':\n                # Not super elegant, but I like nicer display of FPS\n                openai_logger.record_tabular(key, int(epoch_result[key]))\n            else:\n                openai_logger.record_tabular(key, epoch_result[key])\n\n        openai_logger.dump_tabular()\n\n\ndef create(model_config, reinforcer, optimizer, storage, total_frames, batches_per_epoch,\n           callbacks=None, scheduler=None, openai_logging=False):\n    """""" Vel factory function """"""\n    from vel.openai.baselines import logger\n    logger.configure(dir=model_config.openai_dir())\n\n    return RlTrainCommand(\n        model_config=model_config,\n        reinforcer=reinforcer,\n        optimizer_factory=optimizer,\n        scheduler_factory=scheduler,\n        storage=storage,\n        callbacks=callbacks,\n        total_frames=int(float(total_frames)),\n        batches_per_epoch=int(batches_per_epoch),\n        openai_logging=openai_logging\n    )\n'"
vel/rl/env/__init__.py,0,b''
vel/rl/env/classic_atari.py,0,"b'import gym\nimport os.path\n\nfrom gym.envs.registration import EnvSpec\n\n\nfrom vel.openai.baselines import logger\nfrom vel.openai.baselines.bench import Monitor\nfrom vel.openai.baselines.common.atari_wrappers import (\n    NoopResetEnv, MaxAndSkipEnv, FireResetEnv, EpisodicLifeEnv, WarpFrame, ClipRewardEnv,\n    ScaledFloatFrame, FrameStack, FireEpisodicLifeEnv\n)\n\nfrom vel.rl.api import EnvFactory\nfrom vel.rl.env.wrappers.clip_episode_length import ClipEpisodeLengthWrapper\nfrom vel.util.situational import process_environment_settings\n\n\nDEFAULT_SETTINGS = {\n    \'default\': {\n        \'disable_reward_clipping\': False,\n        \'disable_episodic_life\': False,\n        \'monitor\': False,\n        \'allow_early_resets\': False,\n        \'scale_float_frames\': False,\n        \'max_episode_frames\': 10000,\n        \'frame_stack\': None\n    },\n    \'record\': {\n        \'disable_reward_clipping\': False,\n        \'disable_episodic_life\': True,\n        \'monitor\': False,\n        \'allow_early_resets\': True,\n        \'scale_float_frames\': False,\n        \'max_episode_frames\': 10000,\n        \'frame_stack\': None\n    },\n}\n\n\ndef env_maker(environment_id):\n    """""" Create a relatively raw atari environment """"""\n    env = gym.make(environment_id)\n    assert \'NoFrameskip\' in env.spec.id\n\n    # Wait for between 1 and 30 rounds doing nothing on start\n    env = NoopResetEnv(env, noop_max=30)\n\n    # Do the same action for k steps. Return max of last 2 frames. Return sum of rewards\n    env = MaxAndSkipEnv(env, skip=4)\n\n    return env\n\n\ndef wrapped_env_maker(environment_id, seed, serial_id, disable_reward_clipping=False, disable_episodic_life=False,\n                      monitor=False, allow_early_resets=False, scale_float_frames=False,\n                      max_episode_frames=10000, frame_stack=None):\n    """""" Wrap atari environment so that it\'s nicer to learn RL algorithms """"""\n    env = env_maker(environment_id)\n    env.seed(seed + serial_id)\n\n    if max_episode_frames is not None:\n        env = ClipEpisodeLengthWrapper(env, max_episode_length=max_episode_frames)\n\n    # Monitoring the env\n    if monitor:\n        logdir = logger.get_dir() and os.path.join(logger.get_dir(), str(serial_id))\n    else:\n        logdir = None\n\n    env = Monitor(env, logdir, allow_early_resets=allow_early_resets)\n\n    if not disable_episodic_life:\n        # Make end-of-life == end-of-episode, but only reset on true game over.\n        # Done by DeepMind for the DQN and co. since it helps value estimation.\n        env = EpisodicLifeEnv(env)\n\n    if \'FIRE\' in env.unwrapped.get_action_meanings():\n        # Take action on reset for environments that are fixed until firing.\n        if disable_episodic_life:\n            env = FireEpisodicLifeEnv(env)\n        else:\n            env = FireResetEnv(env)\n\n    # Warp frames to 84x84 as done in the Nature paper and later work.\n    env = WarpFrame(env)\n\n    if scale_float_frames:\n        env = ScaledFloatFrame(env)\n\n    if not disable_reward_clipping:\n        # Bin reward to {+1, 0, -1} by its sign.\n        env = ClipRewardEnv(env)\n\n    if frame_stack is not None:\n        env = FrameStack(env, frame_stack)\n\n    return env\n\n\nclass ClassicAtariEnv(EnvFactory):\n    """""" Atari game environment wrapped in the same way as Deep Mind and OpenAI baselines """"""\n    def __init__(self, envname, settings=None, presets=None):\n        self.envname = envname\n        self.settings = process_environment_settings(DEFAULT_SETTINGS, settings, presets)\n\n    def specification(self) -> EnvSpec:\n        """""" Return environment specification """"""\n        return gym.spec(self.envname)\n\n    def get_preset(self, preset_key=\'default\') -> dict:\n        """""" Get env settings for given preset """"""\n        return self.settings[preset_key]\n\n    def instantiate(self, seed=0, serial_id=0, preset=\'default\', extra_args=None) -> gym.Env:\n        """""" Make a single environment compatible with the experiments """"""\n        settings = self.get_preset(preset)\n        return wrapped_env_maker(self.envname, seed, serial_id, **settings)\n\n\ndef create(game, settings=None, presets=None):\n    """""" Vel factory function """"""\n    return ClassicAtariEnv(game, settings, presets)\n'"
vel/rl/env/classic_control.py,0,"b'import gym\nimport os.path\n\nfrom gym.envs.registration import EnvSpec\n\nfrom vel.openai.baselines import logger\nfrom vel.openai.baselines.bench import Monitor\n\nfrom vel.rl.api import EnvFactory\nfrom vel.util.situational import process_environment_settings\n\n\nDEFAULT_SETTINGS = {\n    \'default\': {\n        \'monitor\': False,\n        \'allow_early_resets\': False,\n    },\n    \'record\': {\n        \'monitor\': False,\n        \'allow_early_resets\': True,\n    }\n}\n\n\ndef env_maker(environment_id, seed, serial_id, monitor=False, allow_early_resets=False):\n    """""" Create a classic control environment with basic set of wrappers """"""\n    env = gym.make(environment_id)\n    env.seed(seed + serial_id)\n\n    # Monitoring the env\n    if monitor:\n        logdir = logger.get_dir() and os.path.join(logger.get_dir(), str(serial_id))\n    else:\n        logdir = None\n\n    env = Monitor(env, logdir, allow_early_resets=allow_early_resets)\n\n    return env\n\n\nclass ClassicControlEnv(EnvFactory):\n    """""" A set of ""classic control"" environments from OpenAI gym """"""\n\n    def __init__(self, envname, settings=None, presets=None):\n        self.envname = envname\n        self.settings = process_environment_settings(DEFAULT_SETTINGS, settings, presets)\n\n    def specification(self) -> EnvSpec:\n        """""" Return environment specification """"""\n        return gym.spec(self.envname)\n\n    def get_preset(self, preset_key=\'default\') -> dict:\n        """""" Get env settings for given preset """"""\n        return self.settings[preset_key]\n\n    def instantiate(self, seed=0, serial_id=0, preset=\'default\', extra_args=None) -> gym.Env:\n        """""" Make a single environment compatible with the experiments """"""\n        settings = self.get_preset(preset)\n        return env_maker(self.envname, seed, serial_id, **settings)\n\n\ndef create(game, settings=None, presets=None):\n    """""" Vel factory function """"""\n    return ClassicControlEnv(\n        envname=game,\n        settings=settings,\n        presets=presets\n    )\n'"
vel/rl/env/mujoco.py,0,"b'import gym\nimport os.path\n\nfrom gym.envs.registration import EnvSpec\n\nfrom vel.openai.baselines import logger\nfrom vel.openai.baselines.bench import Monitor\nfrom vel.rl.api import EnvFactory\nfrom vel.rl.env.wrappers.env_normalize import EnvNormalize\nfrom vel.util.situational import process_environment_settings\n\n\nDEFAULT_SETTINGS = {\n    \'default\': {\n        \'monitor\': False,\n        \'allow_early_resets\': False,\n        \'normalize_observations\': False,\n        \'normalize_returns\': False,\n    },\n    \'record\': {\n        \'monitor\': False,\n        \'allow_early_resets\': True,\n        \'normalize_observations\': False,\n        \'normalize_returns\': False,\n    }\n}\n\n\ndef env_maker(environment_id, seed, serial_id, monitor=False, allow_early_resets=False, normalize_observations=False,\n              normalize_returns=False, normalize_gamma=0.99):\n    """""" Create a relatively raw atari environment """"""\n    env = gym.make(environment_id)\n    env.seed(seed + serial_id)\n\n    # Monitoring the env\n    if monitor:\n        logdir = logger.get_dir() and os.path.join(logger.get_dir(), str(serial_id))\n    else:\n        logdir = None\n\n    env = Monitor(env, logdir, allow_early_resets=allow_early_resets)\n\n    if normalize_observations or normalize_returns:\n        env = EnvNormalize(\n            env,\n            normalize_observations=normalize_observations,\n            normalize_returns=normalize_returns,\n            gamma=normalize_gamma\n        )\n\n    return env\n\n\nclass MujocoEnv(EnvFactory):\n    """""" Atari game environment wrapped in the same way as Deep Mind and OpenAI baselines """"""\n    def __init__(self, envname, normalize_observations=False, normalize_returns=False, settings=None, presets=None):\n        self.envname = envname\n\n        settings = settings if settings is not None else {}\n\n        if normalize_observations:\n            settings[\'normalize_observations\'] = True\n\n        if normalize_returns:\n            settings[\'normalize_returns\'] = True\n\n        self.settings = process_environment_settings(DEFAULT_SETTINGS, settings, presets)\n\n    def specification(self) -> EnvSpec:\n        """""" Return environment specification """"""\n        return gym.spec(self.envname)\n\n    def get_preset(self, preset_key=\'default\') -> dict:\n        """""" Get env settings for given preset """"""\n        return self.settings[preset_key]\n\n    def instantiate(self, seed=0, serial_id=0, preset=\'default\', extra_args=None) -> gym.Env:\n        """""" Make a single environment compatible with the experiments """"""\n        settings = self.get_preset(preset)\n        return env_maker(self.envname, seed, serial_id, **settings)\n\n\ndef create(game, normalize_returns=False, settings=None, presets=None):\n    """""" Vel factory function """"""\n    return MujocoEnv(\n        envname=game,\n        normalize_returns=normalize_returns,\n        settings=settings,\n        presets=presets\n    )\n'"
vel/rl/env_roller/__init__.py,0,b''
vel/rl/env_roller/step_env_roller.py,5,"b'import torch\nimport numpy as np\n\nfrom vel.api import BatchInfo, Model\nfrom vel.rl.api import Trajectories, Rollout, EnvRollerBase, EnvRollerFactoryBase\nfrom vel.util.tensor_accumulator import TensorAccumulator\n\n\nclass StepEnvRoller(EnvRollerBase):\n    """"""\n    Class calculating env rollouts.\n    """"""\n\n    def __init__(self, environment, device):\n        self._environment = environment\n        self.device = device\n\n        # Initial observation - kept on CPU\n        self.last_observation = torch.from_numpy(self.environment.reset()).clone()\n\n        # Relevant for RNN policies - kept on DEVICE\n        self.hidden_state = None\n\n    @property\n    def environment(self):\n        """""" Return environment of this env roller """"""\n        return self._environment\n\n    @torch.no_grad()\n    def rollout(self, batch_info: BatchInfo, model: Model, number_of_steps: int) -> Rollout:\n        """""" Calculate env rollout """"""\n        accumulator = TensorAccumulator()\n        episode_information = []  # List of dictionaries with episode information\n\n        if self.hidden_state is None and model.is_recurrent:\n            self.hidden_state = model.zero_state(self.last_observation.size(0)).to(self.device)\n\n        # Remember rollout initial state, we\'ll use that for training as well\n        initial_hidden_state = self.hidden_state\n\n        for step_idx in range(number_of_steps):\n            if model.is_recurrent:\n                step = model.step(self.last_observation.to(self.device), state=self.hidden_state)\n                self.hidden_state = step[\'state\']\n            else:\n                step = model.step(self.last_observation.to(self.device))\n\n            # Add step to the tensor accumulator\n            for name, tensor in step.items():\n                accumulator.add(name, tensor.cpu())\n\n            accumulator.add(\'observations\', self.last_observation)\n\n            actions_numpy = step[\'actions\'].detach().cpu().numpy()\n            new_obs, new_rewards, new_dones, new_infos = self.environment.step(actions_numpy)\n\n            # Done is flagged true when the episode has ended AND the frame we see is already a first frame from the\n            # next episode\n            dones_tensor = torch.from_numpy(new_dones.astype(np.float32)).clone()\n            accumulator.add(\'dones\', dones_tensor)\n\n            self.last_observation = torch.from_numpy(new_obs).clone()\n\n            if model.is_recurrent:\n                # Zero out state in environments that have finished\n                self.hidden_state = self.hidden_state * (1.0 - dones_tensor.unsqueeze(-1)).to(self.device)\n\n            accumulator.add(\'rewards\', torch.from_numpy(new_rewards.astype(np.float32)).clone())\n\n            episode_information.append(new_infos)\n\n        if model.is_recurrent:\n            final_values = model.value(self.last_observation.to(self.device), state=self.hidden_state).cpu()\n        else:\n            final_values = model.value(self.last_observation.to(self.device)).cpu()\n\n        accumulated_tensors = accumulator.result()\n\n        return Trajectories(\n            num_steps=accumulated_tensors[\'observations\'].size(0),\n            num_envs=accumulated_tensors[\'observations\'].size(1),\n            environment_information=episode_information,\n            transition_tensors=accumulated_tensors,\n            rollout_tensors={\n                \'initial_hidden_state\': initial_hidden_state.cpu() if initial_hidden_state is not None else None,\n                \'final_values\': final_values\n            }\n        )\n\n\nclass StepEnvRollerFactory(EnvRollerFactoryBase):\n    """""" Factory for the StepEnvRoller """"""\n    def __init__(self):\n        pass\n\n    def instantiate(self, environment, device):\n        return StepEnvRoller(\n            environment=environment,\n            device=device,\n        )\n\n\ndef create():\n    """""" Vel factory function """"""\n    return StepEnvRollerFactory()\n'"
vel/rl/env_roller/trajectory_replay_env_roller.py,5,"b'import torch\nimport numpy as np\n\nfrom vel.api import BatchInfo\nfrom vel.rl.api import (\n    Trajectories, Rollout, ReplayEnvRollerBase, ReplayEnvRollerFactoryBase, ReplayBuffer, ReplayBufferFactory, RlModel\n)\nfrom vel.util.tensor_accumulator import TensorAccumulator\n\n\nclass TrajectoryReplayEnvRoller(ReplayEnvRollerBase):\n    """"""\n    Calculate environment rollouts using a replay buffer for experience replay.\n    Replay buffer is parametrized.\n    Samples trajectories from the replay buffer (consecutive series of frames)\n    """"""\n\n    def __init__(self, environment, device, replay_buffer: ReplayBuffer):\n        self._environment = environment\n        self.device = device\n        self.replay_buffer = replay_buffer\n\n        # Initial observation\n        self.last_observation_cpu = torch.from_numpy(self.environment.reset()).clone()\n        self.last_observation = self.last_observation_cpu.to(self.device)\n\n    @property\n    def environment(self):\n        """""" Return environment of this env roller """"""\n        return self._environment\n\n    @torch.no_grad()\n    def rollout(self, batch_info: BatchInfo, model: RlModel, number_of_steps: int) -> Rollout:\n        """""" Calculate env rollout """"""\n        assert not model.is_recurrent, ""Replay env roller does not support recurrent models""\n\n        accumulator = TensorAccumulator()\n        episode_information = []  # List of dictionaries with episode information\n\n        for step_idx in range(number_of_steps):\n            step = model.step(self.last_observation)\n\n            replay_extra_information = {}\n\n            accumulator.add(\'observations\', self.last_observation_cpu)\n\n            # Add step to the tensor accumulator\n            for name, tensor in step.items():\n                tensor_cpu = tensor.cpu()\n                accumulator.add(name, tensor_cpu)\n\n                if name != \'actions\':\n                    replay_extra_information[name] = tensor_cpu.numpy()\n\n            actions_numpy = step[\'actions\'].detach().cpu().numpy()\n            new_obs, new_rewards, new_dones, new_infos = self.environment.step(actions_numpy)\n\n            # Store rollout in the experience replay buffer\n            self.replay_buffer.store_transition(\n                frame=self.last_observation_cpu.numpy(),\n                action=actions_numpy,\n                reward=new_rewards,\n                done=new_dones,\n                extra_info=replay_extra_information\n            )\n\n            # Done is flagged true when the episode has ended AND the frame we see is already a first frame from the\n            # next episode\n\n            dones_tensor = torch.from_numpy(new_dones.astype(np.float32)).clone()\n            accumulator.add(\'dones\', dones_tensor)\n\n            self.last_observation_cpu = torch.from_numpy(new_obs).clone()\n            self.last_observation = self.last_observation_cpu.to(self.device)\n            accumulator.add(\'rewards\', torch.from_numpy(new_rewards.astype(np.float32)).clone())\n\n            episode_information.append(new_infos)\n\n        accumulated_tensors = accumulator.result()\n\n        return Trajectories(\n            num_steps=accumulated_tensors[\'observations\'].size(0),\n            num_envs=accumulated_tensors[\'observations\'].size(1),\n            environment_information=episode_information,\n            transition_tensors=accumulated_tensors,\n            rollout_tensors={\n                \'final_values\': model.value(self.last_observation).cpu()\n            }\n        )\n\n    def sample(self, batch_info: BatchInfo, model: RlModel, number_of_steps: int) -> Rollout:\n        """""" Sample experience from replay buffer and return a batch """"""\n        # Sample trajectories\n        rollout = self.replay_buffer.sample_trajectories(rollout_length=number_of_steps, batch_info=batch_info)\n\n        last_observations = rollout.transition_tensors[\'observations_next\'][-1].to(self.device)\n        final_values = model.value(last_observations).cpu()\n\n        # Add \'final_values\' to the rollout\n        rollout.rollout_tensors[\'final_values\'] = final_values\n\n        return rollout\n\n    def is_ready_for_sampling(self) -> bool:\n        """""" If buffer is ready for drawing samples from it (usually checks if there is enough data) """"""\n        return self.replay_buffer.is_ready_for_sampling()\n\n    def update(self, rollout, batch_info):\n        """""" Perform update of the internal state of the buffer - e.g. for the prioritized replay weights """"""\n        self.replay_buffer.update(rollout, batch_info)\n\n\nclass TrajectoryReplayEnvRollerFactory(ReplayEnvRollerFactoryBase):\n    """""" Factory for the ReplayEnvRoller """"""\n\n    def __init__(self, replay_buffer_factory: ReplayBufferFactory):\n        self.replay_buffer_factory = replay_buffer_factory\n\n    def instantiate(self, environment, device):\n        replay_buffer = self.replay_buffer_factory.instantiate(environment)\n\n        return TrajectoryReplayEnvRoller(\n            environment=environment,\n            device=device,\n            replay_buffer=replay_buffer\n        )\n\n\ndef create(replay_buffer):\n    """""" Vel factory function """"""\n    return TrajectoryReplayEnvRollerFactory(replay_buffer_factory=replay_buffer)\n'"
vel/rl/env_roller/transition_replay_env_roller.py,7,"b'import torch\nimport torch.nn as nn\nimport typing\nimport numpy as np\n\nfrom vel.api import BatchInfo, ModelFactory\nfrom vel.openai.baselines.common.running_mean_std import RunningMeanStd\nfrom vel.rl.api import (\n    Trajectories, Rollout, ReplayEnvRollerBase, ReplayEnvRollerFactoryBase, RlModel, ReplayBuffer, ReplayBufferFactory\n)\nfrom vel.util.tensor_accumulator import TensorAccumulator\n\n\nclass TransitionReplayEnvRoller(ReplayEnvRollerBase):\n    """"""\n    Calculate environment rollouts using a replay buffer for experience replay.\n    Replay buffer is parametrized\n    Samples transitions from the replay buffer (individual frame transitions)\n    """"""\n\n    def __init__(self, environment, device, replay_buffer: ReplayBuffer, discount_factor: typing.Optional[float]=None,\n                 normalize_returns: bool=False, forward_steps: int=1, action_noise: typing.Optional[nn.Module]=None):\n        self._environment = environment\n        self.device = device\n        self.replay_buffer = replay_buffer\n        self.normalize_returns = normalize_returns\n        self.forward_steps = forward_steps\n        self.discount_factor = discount_factor\n        self.action_noise = action_noise.to(self.device) if action_noise is not None else None\n\n        if self.normalize_returns:\n            assert self.discount_factor is not None, \\\n                ""TransitionReplayEnvRoller must have a discount factor defined if normalize_returns is turned on""\n\n        if self.forward_steps > 1:\n            assert self.discount_factor is not None, \\\n                ""TransitionReplayEnvRoller must have a discount factor defined if forward_steps is larger than one""\n\n        self.ret_rms = RunningMeanStd(shape=()) if normalize_returns else None\n\n        # Initial observation\n        self.last_observation_cpu = torch.from_numpy(self.environment.reset()).clone()\n        self.last_observation = self.last_observation_cpu.to(self.device)\n\n        # Return normalization\n        self.clip_obs = 5.0\n        self.accumulated_returns = np.zeros(environment.num_envs, dtype=np.float32)\n\n    @property\n    def environment(self):\n        """""" Return environment of this env roller """"""\n        return self._environment\n\n    @torch.no_grad()\n    def rollout(self, batch_info: BatchInfo, model: RlModel, number_of_steps: int) -> Rollout:\n        """""" Calculate env rollout """"""\n        assert not model.is_recurrent, ""Replay env roller does not support recurrent models""\n\n        accumulator = TensorAccumulator()\n        episode_information = []  # List of dictionaries with episode information\n\n        for step_idx in range(number_of_steps):\n            step = model.step(self.last_observation)\n\n            if self.action_noise is not None:\n                step[\'actions\'] = self.action_noise(step[\'actions\'], batch_info=batch_info)\n\n            replay_extra_information = {}\n\n            accumulator.add(\'observations\', self.last_observation_cpu)\n\n            # Add step to the tensor accumulator\n            for name, tensor in step.items():\n                tensor_cpu = tensor.cpu()\n                accumulator.add(name, tensor_cpu)\n\n                if name != \'actions\':\n                    replay_extra_information[name] = tensor_cpu.numpy()\n\n            actions_numpy = step[\'actions\'].detach().cpu().numpy()\n            new_obs, new_rewards, new_dones, new_infos = self.environment.step(actions_numpy)\n\n            # Store rollout in the experience replay buffer\n            self.replay_buffer.store_transition(\n                frame=self.last_observation_cpu.numpy(),\n                action=actions_numpy,\n                reward=new_rewards,\n                done=new_dones,\n                extra_info=replay_extra_information\n            )\n\n            if self.ret_rms is not None:\n                self.accumulated_returns = new_rewards + self.discount_factor * self.accumulated_returns\n                self.ret_rms.update(self.accumulated_returns)\n\n            # Done is flagged true when the episode has ended AND the frame we see is already a first frame from the\n            # next episode\n            dones_tensor = torch.from_numpy(new_dones.astype(np.float32)).clone()\n            accumulator.add(\'dones\', dones_tensor)\n\n            if self.action_noise is not None:\n                self.action_noise.reset_training_state(dones_tensor, batch_info=batch_info)\n\n            self.accumulated_returns = self.accumulated_returns * (1.0 - new_dones.astype(np.float32))\n\n            self.last_observation_cpu = torch.from_numpy(new_obs).clone()\n            self.last_observation = self.last_observation_cpu.to(self.device)\n\n            if self.ret_rms is not None:\n                new_rewards = np.clip(new_rewards / np.sqrt(self.ret_rms.var + 1e-8), -self.clip_obs, self.clip_obs)\n\n            accumulator.add(\'rewards\', torch.from_numpy(new_rewards.astype(np.float32)).clone())\n\n            episode_information.append(new_infos)\n\n        accumulated_tensors = accumulator.result()\n\n        return Trajectories(\n            num_steps=accumulated_tensors[\'observations\'].size(0),\n            num_envs=accumulated_tensors[\'observations\'].size(1),\n            environment_information=episode_information,\n            transition_tensors=accumulated_tensors,\n            rollout_tensors={}\n        ).to_transitions()\n\n    def sample(self, batch_info: BatchInfo, model: RlModel, number_of_steps: int) -> Rollout:\n        """""" Sample experience from replay buffer and return a batch """"""\n        if self.forward_steps > 1:\n            transitions = self.replay_buffer.sample_forward_transitions(\n                batch_size=number_of_steps, batch_info=batch_info, forward_steps=self.forward_steps,\n                discount_factor=self.discount_factor\n            )\n        else:\n            transitions = self.replay_buffer.sample_transitions(batch_size=number_of_steps, batch_info=batch_info)\n\n        if self.ret_rms is not None:\n            rewards = transitions.transition_tensors[\'rewards\']\n            new_rewards = torch.clamp(rewards / np.sqrt(self.ret_rms.var + 1e-8), -self.clip_obs, self.clip_obs)\n            transitions.transition_tensors[\'rewards\'] = new_rewards\n\n        return transitions\n\n    def is_ready_for_sampling(self) -> bool:\n        """""" If buffer is ready for drawing samples from it (usually checks if there is enough data) """"""\n        return self.replay_buffer.is_ready_for_sampling()\n\n    def initial_memory_size_hint(self) -> typing.Optional[int]:\n        """""" Hint how much data is needed to begin sampling, required only for diagnostics """"""\n        return self.replay_buffer.initial_memory_size_hint()\n\n    def update(self, rollout, batch_info):\n        """""" Perform update of the internal state of the buffer - e.g. for the prioritized replay weights """"""\n        self.replay_buffer.update(rollout, batch_info)\n\n\nclass TransitionReplayEnvRollerFactory(ReplayEnvRollerFactoryBase):\n    """""" Factory for the ReplayEnvRoller """"""\n\n    def __init__(self, replay_buffer_factory: ReplayBufferFactory, discount_factor: typing.Optional[float]=None,\n                 normalize_returns: bool=False, forward_steps: int=1, action_noise: typing.Optional[ModelFactory]=None):\n        self.replay_buffer_factory = replay_buffer_factory\n        self.normalize_returns = normalize_returns\n        self.forward_steps = forward_steps\n        self.discount_factor = discount_factor\n        self.action_noise_factory = action_noise\n\n    def instantiate(self, environment, device):\n        replay_buffer = self.replay_buffer_factory.instantiate(environment)\n\n        if self.action_noise_factory is None:\n            action_noise = None\n        else:\n            action_noise = self.action_noise_factory.instantiate(environment=environment)\n\n        return TransitionReplayEnvRoller(\n            environment=environment,\n            device=device,\n            replay_buffer=replay_buffer,\n            discount_factor=self.discount_factor,\n            normalize_returns=self.normalize_returns,\n            forward_steps=self.forward_steps,\n            action_noise=action_noise\n        )\n\n\ndef create(replay_buffer, discount_factor: typing.Optional[float]=None, normalize_returns: bool=False,\n           forward_steps: int=1, action_noise: typing.Optional[ModelFactory]=None):\n    """""" Vel factory function """"""\n    return TransitionReplayEnvRollerFactory(\n        replay_buffer_factory=replay_buffer,\n        discount_factor=discount_factor,\n        forward_steps=forward_steps,\n        normalize_returns=normalize_returns,\n        action_noise=action_noise\n    )\n'"
vel/rl/models/__init__.py,0,b''
vel/rl/models/deterministic_policy_model.py,2,"b'import gym\nimport itertools as it\nimport torch\nimport typing\n\nfrom vel.api import LinearBackboneModel, ModelFactory, BackboneModel\nfrom vel.modules.input.identity import IdentityFactory\nfrom vel.rl.api import Rollout, Evaluator, RlModel\nfrom vel.rl.modules.deterministic_action_head import DeterministicActionHead\nfrom vel.rl.modules.deterministic_critic_head import DeterministicCriticHead\n\n\nclass DeterministicPolicyEvaluator(Evaluator):\n    """""" Evaluator for DeterministicPolicyModel """"""\n\n    def __init__(self, model: \'DeterministicPolicyModel\', rollout: Rollout):\n        super().__init__(rollout)\n\n        self.model = model\n\n    @Evaluator.provides(\'model:values_next\')\n    def model_estimated_values_next(self):\n        """""" Estimate state-value of the transition next state """"""\n        observations = self.get(\'rollout:observations_next\')\n        action, value = self.model(observations)\n        return value\n\n    @Evaluator.provides(\'model:actions\')\n    def model_actions(self):\n        """""" Estimate state-value of the transition next state """"""\n        observations = self.get(\'rollout:observations\')\n        model_action = self.model.action(observations)\n        return model_action\n\n    @Evaluator.provides(\'model:model_action:q\')\n    def model_model_action_q(self):\n        observations = self.get(\'rollout:observations\')\n        model_actions = self.get(\'model:actions\')\n        return self.model.value(observations, model_actions)\n\n    @Evaluator.provides(\'model:action:q\')\n    def model_action_q(self):\n        observations = self.get(\'rollout:observations\')\n        rollout_actions = self.get(\'rollout:actions\')\n        return self.model.value(observations, rollout_actions)\n\n\nclass DeterministicPolicyModel(RlModel):\n    """""" Deterministic Policy Gradient - model """"""\n\n    def __init__(self, input_block: BackboneModel, policy_backbone: LinearBackboneModel,\n                 value_backbone: LinearBackboneModel, action_space: gym.Space):\n        super().__init__()\n\n        self.input_block = input_block\n        self.policy_backbone = policy_backbone\n        self.value_backbone = value_backbone\n\n        self.action_head = DeterministicActionHead(self.policy_backbone.output_dim, action_space)\n        self.critic_head = DeterministicCriticHead(self.value_backbone.output_dim)\n\n    def reset_weights(self):\n        """""" Initialize properly model weights """"""\n        self.input_block.reset_weights()\n        self.policy_backbone.reset_weights()\n        self.value_backbone.reset_weights()\n        self.action_head.reset_weights()\n        self.critic_head.reset_weights()\n\n    def forward(self, observations, input_actions=None):\n        """""" Calculate model outputs """"""\n        observations = self.input_block(observations)\n\n        if input_actions is not None:\n            actions = input_actions\n\n            value_input = torch.cat([observations, actions], dim=1)\n            value_hidden = self.value_backbone(value_input)\n\n            values = self.critic_head(value_hidden)\n        else:\n            policy_hidden = self.policy_backbone(observations)\n            actions = self.action_head(policy_hidden)\n\n            value_input = torch.cat([observations, actions], dim=1)\n            value_hidden = self.value_backbone(value_input)\n\n            values = self.critic_head(value_hidden)\n\n        return actions, values\n\n    def policy_parameters(self):\n        """""" Parameters of policy """"""\n        return it.chain(self.policy_backbone.parameters(), self.action_head.parameters())\n\n    def value_parameters(self):\n        """""" Parameters of policy """"""\n        return it.chain(self.value_backbone.parameters(), self.critic_head.parameters())\n\n    def get_layer_groups(self):\n        """""" Return layers grouped """"""\n        return [\n            [self.policy_backbone, self.action_head],\n            [self.value_backbone, [y for (x, y) in self.critic_head.named_parameters() if x.endswith(\'bias\')]],\n            # OpenAI regularizes only weight on the last layer. I\'m just replicating that\n            [[y for (x, y) in self.critic_head.named_parameters() if x.endswith(\'weight\')]]\n        ]\n\n    def step(self, observations):\n        """""" Select actions based on model\'s output """"""\n        action, value = self(observations)\n\n        return {\n            \'actions\': action,\n            \'values\': value\n        }\n\n    def value(self, observation, input_actions=None):\n        """""" Calculate value for given state """"""\n        action, value = self(observation, input_actions)\n        return value\n\n    def action(self, observations):\n        """""" Calculate value for given state """"""\n        observations = self.input_block(observations)\n        policy_hidden = self.policy_backbone(observations)\n        action = self.action_head(policy_hidden)\n        return action\n\n    def evaluate(self, rollout: Rollout) -> Evaluator:\n        """""" Evaluate model on a rollout """"""\n        return DeterministicPolicyEvaluator(self, rollout)\n\n\nclass DeterministicPolicyModelFactory(ModelFactory):\n    """""" Factory class for policy gradient models """"""\n    def __init__(self, input_block: ModelFactory, policy_backbone: ModelFactory, value_backbone: ModelFactory):\n        self.input_block = input_block\n        self.policy_backbone = policy_backbone\n        self.value_backbone = value_backbone\n\n    def instantiate(self, **extra_args):\n        """""" Instantiate the model """"""\n        input_block = self.input_block.instantiate()\n        policy_backbone = self.policy_backbone.instantiate(**extra_args)\n        value_backbone = self.value_backbone.instantiate(**extra_args)\n\n        return DeterministicPolicyModel(\n            input_block=input_block,\n            policy_backbone=policy_backbone,\n            value_backbone=value_backbone,\n            action_space=extra_args[\'action_space\'],\n        )\n\n\ndef create(policy_backbone: ModelFactory, value_backbone: ModelFactory,\n           input_block: typing.Optional[ModelFactory]=None):\n    """""" Vel factory function """"""\n    if input_block is None:\n        input_block = IdentityFactory()\n\n    return DeterministicPolicyModelFactory(\n        input_block=input_block, policy_backbone=policy_backbone, value_backbone=value_backbone\n    )\n'"
vel/rl/models/q_distributional_model.py,0,"b'import gym\nimport typing\n\nfrom vel.api import LinearBackboneModel, ModelFactory, BackboneModel\nfrom vel.modules.input.identity import IdentityFactory\nfrom vel.rl.api import Rollout, RlModel, Evaluator\nfrom vel.rl.modules.q_distributional_head import QDistributionalHead\n\n\nclass QDistributionalModelEvaluator(Evaluator):\n    """""" Evaluate distributional q-model """"""\n    def __init__(self, model: \'QDistributionalModel\', rollout: Rollout):\n        super().__init__(rollout)\n        self.model = model\n\n    @Evaluator.provides(\'model:q\')\n    def model_q(self):\n        """""" Action values for all (discrete) actions """"""\n        # observations = self.get(\'rollout:observations\')\n        # # This mean of last dimension collapses the histogram/calculates mean reward\n        # return self.model(observations).mean(dim=-1)\n        raise NotImplementedError\n\n    @Evaluator.provides(\'model:q_dist\')\n    def model_q_dist(self):\n        """""" Action values for all (discrete) actions """"""\n        observations = self.get(\'rollout:observations\')\n        # This mean of last dimension collapses the histogram/calculates mean reward\n        return self.model(observations)\n\n    @Evaluator.provides(\'model:action:q\')\n    def model_action_q(self):\n        """""" Action values for selected actions in the rollout """"""\n        raise NotImplementedError\n\n    @Evaluator.provides(\'model:action:q_dist\')\n    def model_action_q_dist(self):\n        """""" Action values for selected actions in the rollout """"""\n        q = self.get(\'model:q_dist\')\n        actions = self.get(\'rollout:actions\')\n        return q[range(q.size(0)), actions]\n\n    @Evaluator.provides(\'model:q_next\')\n    def model_q_next(self):\n        """""" Action values for all (discrete) actions """"""\n        raise NotImplementedError\n\n    @Evaluator.provides(\'model:q_dist_next\')\n    def model_q_dist_next(self):\n        """""" Action values for all (discrete) actions """"""\n        observations = self.get(\'rollout:observations_next\')\n        # This mean of last dimension collapses the histogram/calculates mean reward\n        return self.model(observations)\n\n\nclass QDistributionalModel(RlModel):\n    """"""\n    A deterministic greedy action-value model that learns a value function distribution rather than\n    just an expectation.\n    Supports only discrete action spaces (ones that can be enumerated)\n    """"""\n    def __init__(self, input_block: BackboneModel, backbone: LinearBackboneModel, action_space: gym.Space,\n                 vmin: float, vmax: float, atoms: int=1):\n        super().__init__()\n\n        self.action_space = action_space\n\n        self.input_block = input_block\n        self.backbone = backbone\n\n        self.q_head = QDistributionalHead(\n            input_dim=backbone.output_dim, action_space=action_space,\n            vmin=vmin, vmax=vmax,\n            atoms=atoms\n        )\n\n    def reset_weights(self):\n        """""" Initialize weights to reasonable defaults """"""\n        self.input_block.reset_weights()\n        self.backbone.reset_weights()\n        self.q_head.reset_weights()\n\n    def forward(self, observations):\n        """""" Model forward pass """"""\n        input_data = self.input_block(observations)\n        base_output = self.backbone(input_data)\n        log_histogram = self.q_head(base_output)\n        return log_histogram\n\n    def histogram_info(self):\n        """""" Return extra information about histogram """"""\n        return self.q_head.histogram_info()\n\n    def step(self, observations):\n        """""" Sample action from an action space for given state """"""\n        log_histogram = self(observations)\n        actions = self.q_head.sample(log_histogram)\n\n        return {\n            \'actions\': actions,\n            \'log_histogram\': log_histogram\n        }\n\n    def evaluate(self, rollout: Rollout) -> Evaluator:\n        """""" Evaluate model on a rollout """"""\n        return QDistributionalModelEvaluator(self, rollout)\n\n\nclass QDistributionalModelFactory(ModelFactory):\n    """""" Factory class for q-learning models """"""\n    def __init__(self, input_block: ModelFactory, backbone: ModelFactory, vmin: float, vmax: float, atoms: int):\n        self.input_block = input_block\n        self.backbone = backbone\n        self.vmin = vmin\n        self.vmax = vmax\n        self.atoms = atoms\n\n    def instantiate(self, **extra_args):\n        """""" Instantiate the model """"""\n        input_block = self.input_block.instantiate()\n        backbone = self.backbone.instantiate(**extra_args)\n\n        return QDistributionalModel(\n            input_block=input_block,\n            backbone=backbone,\n            action_space=extra_args[\'action_space\'],\n            vmin=self.vmin,\n            vmax=self.vmax,\n            atoms=self.atoms\n        )\n\n\ndef create(backbone: ModelFactory, vmin: float, vmax: float, atoms: int,\n           input_block: typing.Optional[ModelFactory]=None):\n    """""" Vel factory function """"""\n    if input_block is None:\n        input_block = IdentityFactory()\n\n    return QDistributionalModelFactory(\n        input_block=input_block, backbone=backbone,\n        vmin=vmin,\n        vmax=vmax,\n        atoms=atoms\n    )\n'"
vel/rl/models/q_dueling_model.py,0,"b'import gym\nimport typing\n\nfrom vel.api import LinearBackboneModel, Model, ModelFactory, BackboneModel\nfrom vel.modules.input.identity import IdentityFactory\nfrom vel.rl.api import Rollout, Evaluator\nfrom vel.rl.modules.q_dueling_head import QDuelingHead\nfrom vel.rl.models.q_model import QModelEvaluator\n\n\nclass QDuelingModel(Model):\n    """"""\n    Deterministic greedy action-value model with dueling heads (kind of actor and critic)\n    Supports only discrete action spaces (ones that can be enumerated)\n    """"""\n\n    def __init__(self, input_block: BackboneModel, backbone: LinearBackboneModel, action_space: gym.Space):\n        super().__init__()\n\n        self.action_space = action_space\n\n        self.input_block = input_block\n        self.backbone = backbone\n        self.q_head = QDuelingHead(input_dim=backbone.output_dim, action_space=action_space)\n\n    def forward(self, observations):\n        """""" Model forward pass """"""\n        observations = self.input_block(observations)\n        advantage_features, value_features = self.backbone(observations)\n        q_values = self.q_head(advantage_features, value_features)\n\n        return q_values\n\n    def reset_weights(self):\n        """""" Initialize weights to reasonable defaults """"""\n        self.input_block.reset_weights()\n        self.backbone.reset_weights()\n        self.q_head.reset_weights()\n\n    def step(self, observations):\n        """""" Sample action from an action space for given state """"""\n        q_values = self(observations)\n\n        return {\n            \'actions\': self.q_head.sample(q_values),\n            \'q\': q_values\n        }\n\n    def evaluate(self, rollout: Rollout) -> Evaluator:\n        """""" Evaluate model on a rollout """"""\n        return QModelEvaluator(self, rollout)\n\n\nclass QDuelingModelFactory(ModelFactory):\n    """""" Factory class for policy gradient models """"""\n    def __init__(self, input_block: ModelFactory, backbone: ModelFactory):\n        self.input_block = input_block\n        self.backbone = backbone\n\n    def instantiate(self, **extra_args):\n        """""" Instantiate the model """"""\n        input_block = self.input_block.instantiate()\n        backbone = self.backbone.instantiate(**extra_args)\n\n        return QDuelingModel(input_block, backbone, extra_args[\'action_space\'])\n\n\ndef create(backbone: ModelFactory, input_block: typing.Optional[ModelFactory]=None):\n    """""" Vel factory function """"""\n    if input_block is None:\n        input_block = IdentityFactory()\n\n    return QDuelingModelFactory(input_block=input_block, backbone=backbone)\n'"
vel/rl/models/q_model.py,0,"b'import gym\nimport typing\n\nfrom vel.api import LinearBackboneModel, ModelFactory, BackboneModel\nfrom vel.modules.input.identity import IdentityFactory\nfrom vel.rl.api import Rollout, RlModel, Evaluator\nfrom vel.rl.modules.q_head import QHead\n\n\nclass QModelEvaluator(Evaluator):\n    """""" Evaluate simple q-model """"""\n    def __init__(self, model: \'QModel\', rollout: Rollout):\n        super().__init__(rollout)\n        self.model = model\n\n    @Evaluator.provides(\'model:q\')\n    def model_q(self):\n        """""" Action values for all (discrete) actions """"""\n        observations = self.get(\'rollout:observations\')\n        return self.model(observations)\n\n    @Evaluator.provides(\'model:action:q\')\n    def model_action_q(self):\n        """""" Action values for selected actions in the rollout """"""\n        q = self.get(\'model:q\')\n        actions = self.get(\'rollout:actions\')\n        return q.gather(1, actions.unsqueeze(1)).squeeze(1)\n\n    @Evaluator.provides(\'model:q_next\')\n    def model_q_next(self):\n        """""" Action values for all (discrete) actions """"""\n        observations = self.get(\'rollout:observations_next\')\n        return self.model(observations)\n\n\nclass QModel(RlModel):\n    """"""\n    Simple deterministic greedy action-value model.\n    Supports only discrete action spaces (ones that can be enumerated)\n    """"""\n    def __init__(self, input_block: BackboneModel, backbone: LinearBackboneModel, action_space: gym.Space):\n        super().__init__()\n\n        self.action_space = action_space\n\n        self.input_block = input_block\n        self.backbone = backbone\n        self.q_head = QHead(input_dim=backbone.output_dim, action_space=action_space)\n\n    def reset_weights(self):\n        """""" Initialize weights to reasonable defaults """"""\n        self.input_block.reset_weights()\n        self.backbone.reset_weights()\n        self.q_head.reset_weights()\n\n    def forward(self, observations):\n        """""" Model forward pass """"""\n        observations = self.input_block(observations)\n        base_output = self.backbone(observations)\n        q_values = self.q_head(base_output)\n        return q_values\n\n    def step(self, observations):\n        """""" Sample action from an action space for given state """"""\n        q_values = self(observations)\n        actions = self.q_head.sample(q_values)\n\n        return {\n            \'actions\': actions,\n            \'q\': q_values\n        }\n\n    def evaluate(self, rollout: Rollout) -> Evaluator:\n        """""" Evaluate model on a rollout """"""\n        return QModelEvaluator(self, rollout)\n\n\nclass QModelFactory(ModelFactory):\n    """""" Factory class for q-learning models """"""\n    def __init__(self, input_block: ModelFactory, backbone: ModelFactory):\n        self.input_block = input_block\n        self.backbone = backbone\n\n    def instantiate(self, **extra_args):\n        """""" Instantiate the model """"""\n        input_block = self.input_block.instantiate()\n        backbone = self.backbone.instantiate(**extra_args)\n\n        return QModel(input_block, backbone, extra_args[\'action_space\'])\n\n\ndef create(backbone: ModelFactory, input_block: typing.Optional[ModelFactory]=None):\n    """""" Vel factory function """"""\n    if input_block is None:\n        input_block = IdentityFactory()\n\n    return QModelFactory(input_block=input_block, backbone=backbone)\n'"
vel/rl/models/q_noisy_model.py,0,"b'import gym\nimport typing\n\nfrom vel.api import LinearBackboneModel, ModelFactory, BackboneModel\nfrom vel.modules.input.identity import IdentityFactory\nfrom vel.rl.api import Rollout, RlModel, Evaluator\nfrom vel.rl.models.q_model import QModelEvaluator\nfrom vel.rl.modules.q_noisy_head import QNoisyHead\n\n\nclass NoisyQModel(RlModel):\n    """"""\n    NoisyNets action-value model.\n    Supports only discrete action spaces (ones that can be enumerated)\n    """"""\n\n    def __init__(self, input_block: BackboneModel, backbone: LinearBackboneModel, action_space: gym.Space,\n                 initial_std_dev=0.4, factorized_noise=True):\n        super().__init__()\n\n        self.action_space = action_space\n\n        self.input_block = input_block\n        self.backbone = backbone\n        self.q_head = QNoisyHead(\n            input_dim=backbone.output_dim, action_space=action_space, initial_std_dev=initial_std_dev,\n            factorized_noise=factorized_noise\n        )\n\n    def reset_weights(self):\n        """""" Initialize weights to reasonable defaults """"""\n        self.input_block.reset_weights()\n        self.backbone.reset_weights()\n        self.q_head.reset_weights()\n\n    def forward(self, observations):\n        """""" Model forward pass """"""\n        observations = self.input_block(observations)\n        base_output = self.backbone(observations)\n        q_values = self.q_head(base_output)\n        return q_values\n\n    def step(self, observations):\n        """""" Sample action from an action space for given state """"""\n        q_values = self(observations)\n        actions = self.q_head.sample(q_values)\n\n        return {\n            \'actions\': actions,\n            \'q\': q_values\n        }\n\n    def evaluate(self, rollout: Rollout) -> Evaluator:\n        """""" Evaluate model on a rollout """"""\n        return QModelEvaluator(self, rollout)\n\n\nclass NoisyQModelFactory(ModelFactory):\n    """""" Factory class for q-learning models """"""\n    def __init__(self, input_block: ModelFactory, backbone: ModelFactory, initial_std_dev=0.4, factorized_noise=True):\n        self.initial_std_dev = initial_std_dev\n        self.factorized_noise = factorized_noise\n\n        self.input_block = input_block\n        self.backbone = backbone\n\n    def instantiate(self, **extra_args):\n        """""" Instantiate the model """"""\n        input_block = self.input_block.instantiate()\n        backbone = self.backbone.instantiate(**extra_args)\n\n        return NoisyQModel(\n            input_block, backbone, extra_args[\'action_space\'], initial_std_dev=self.initial_std_dev,\n            factorized_noise=self.factorized_noise\n        )\n\n\ndef create(backbone: ModelFactory, input_block: typing.Optional[ModelFactory]=None, initial_std_dev=0.4,\n           factorized_noise=True):\n    """""" Vel factory function """"""\n    if input_block is None:\n        input_block = IdentityFactory()\n\n    return NoisyQModelFactory(\n        input_block=input_block, backbone=backbone, initial_std_dev=initial_std_dev, factorized_noise=factorized_noise\n    )\n'"
vel/rl/models/q_rainbow_model.py,0,"b'import gym\nimport typing\n\nfrom vel.api import LinearBackboneModel, Model, ModelFactory, BackboneModel\nfrom vel.modules.input.identity import IdentityFactory\nfrom vel.rl.api import Rollout, Evaluator\nfrom vel.rl.models.q_distributional_model import QDistributionalModelEvaluator\nfrom vel.rl.modules.q_distributional_noisy_dueling_head import QDistributionalNoisyDuelingHead\n\n\nclass QRainbowModel(Model):\n    """"""\n    A deterministic greedy action-value model.\n    Includes following commonly known modifications:\n    - Distributional Q-Learning\n    - Dueling architecture\n    - Noisy Nets\n    """"""\n\n    def __init__(self, input_block: BackboneModel, backbone: LinearBackboneModel, action_space: gym.Space, vmin: float,\n                 vmax: float, atoms: int = 1, initial_std_dev: float = 0.4, factorized_noise: bool = True):\n        super().__init__()\n\n        self.action_space = action_space\n\n        self.input_block = input_block\n        self.backbone = backbone\n\n        self.q_head = QDistributionalNoisyDuelingHead(\n            input_dim=backbone.output_dim,\n            action_space=action_space,\n            vmin=vmin, vmax=vmax, atoms=atoms,\n            initial_std_dev=initial_std_dev, factorized_noise=factorized_noise\n        )\n\n    def reset_weights(self):\n        """""" Initialize weights to reasonable defaults """"""\n        self.input_block.reset_weights()\n        self.backbone.reset_weights()\n        self.q_head.reset_weights()\n\n    def forward(self, observations):\n        """""" Model forward pass """"""\n        input_data = self.input_block(observations)\n        advantage_features, value_features = self.backbone(input_data)\n        log_histogram = self.q_head(advantage_features, value_features)\n        return log_histogram\n\n    def histogram_info(self):\n        """""" Return extra information about histogram """"""\n        return self.q_head.histogram_info()\n\n    def step(self, observations):\n        """""" Sample action from an action space for given state """"""\n        log_histogram = self(observations)\n        actions = self.q_head.sample(log_histogram)\n\n        return {\n            \'actions\': actions,\n            \'log_histogram\': log_histogram\n        }\n\n    def evaluate(self, rollout: Rollout) -> Evaluator:\n        """""" Evaluate model on a rollout """"""\n        return QDistributionalModelEvaluator(self, rollout)\n\n\nclass QDistributionalModelFactory(ModelFactory):\n    """""" Factory class for q-learning models """"""\n    def __init__(self, input_block: ModelFactory, backbone: ModelFactory, vmin: float, vmax: float, atoms: int,\n                 initial_std_dev: float = 0.4, factorized_noise: bool = True):\n        self.input_block = input_block\n        self.backbone = backbone\n        self.vmin = vmin\n        self.vmax = vmax\n        self.atoms = atoms\n        self.initial_std_dev = initial_std_dev\n        self.factorized_noise = factorized_noise\n\n    def instantiate(self, **extra_args):\n        """""" Instantiate the model """"""\n        input_block = self.input_block.instantiate()\n        backbone = self.backbone.instantiate(**extra_args)\n\n        return QRainbowModel(\n            input_block=input_block,\n            backbone=backbone,\n            action_space=extra_args[\'action_space\'],\n            vmin=self.vmin,\n            vmax=self.vmax,\n            atoms=self.atoms,\n            initial_std_dev=self.initial_std_dev,\n            factorized_noise=self.factorized_noise\n        )\n\n\ndef create(backbone: ModelFactory, vmin: float, vmax: float, atoms: int, initial_std_dev: float = 0.4,\n           factorized_noise: bool = True, input_block: typing.Optional[ModelFactory] = None):\n    """""" Vel factory function """"""\n    if input_block is None:\n        input_block = IdentityFactory()\n\n    return QDistributionalModelFactory(\n        input_block=input_block, backbone=backbone,\n        vmin=vmin,\n        vmax=vmax,\n        atoms=atoms,\n        initial_std_dev=initial_std_dev,\n        factorized_noise=factorized_noise\n    )\n'"
vel/rl/models/q_stochastic_policy_model.py,1,"b'import gym\nimport torch\nimport typing\n\nfrom vel.api import LinearBackboneModel, Model, ModelFactory, BackboneModel\nfrom vel.modules.input.identity import IdentityFactory\nfrom vel.rl.api import Rollout, Evaluator\nfrom vel.rl.modules.action_head import ActionHead\nfrom vel.rl.modules.q_head import QHead\n\n\nclass QStochasticPolicyEvaluator(Evaluator):\n    """""" Evaluator for QPolicyGradientModel """"""\n    def __init__(self, model: \'QStochasticPolicyModel\', rollout: Rollout):\n        super().__init__(rollout)\n\n        self.model = model\n\n        observations = self.get(\'rollout:observations\')\n        logprobs, q = model(observations)\n\n        self.provide(\'model:logprobs\', logprobs)\n        self.provide(\'model:q\', q)\n\n    @Evaluator.provides(\'model:action:logprobs\')\n    def model_action_logprobs(self):\n        actions = self.get(\'rollout_actions\')\n        logprobs = self.get(\'model:logprobs\')\n        return self.model.action_head.logprob(actions, logprobs)\n\n\nclass QStochasticPolicyModel(Model):\n    """"""\n    A policy gradient model with an action-value critic head (instead of more common state-value critic head).\n    Supports only discrete action spaces (ones that can be enumerated)\n    """"""\n\n    def __init__(self, input_block: BackboneModel, backbone: LinearBackboneModel, action_space: gym.Space):\n        super().__init__()\n\n        assert isinstance(action_space, gym.spaces.Discrete)\n\n        self.input_block = input_block\n        self.backbone = backbone\n\n        self.action_head = ActionHead(\n            input_dim=self.backbone.output_dim,\n            action_space=action_space\n        )\n\n        self.q_head = QHead(\n            input_dim=self.backbone.output_dim,\n            action_space=action_space\n        )\n\n    def reset_weights(self):\n        """""" Initialize properly model weights """"""\n        self.input_block.reset_weights()\n        self.backbone.reset_weights()\n        self.action_head.reset_weights()\n        self.q_head.reset_weights()\n\n    def forward(self, observations):\n        """""" Calculate model outputs """"""\n        input_data = self.input_block(observations)\n\n        base_output = self.backbone(input_data)\n        policy_params = self.action_head(base_output)\n\n        q = self.q_head(base_output)\n\n        return policy_params, q\n\n    def step(self, observation, argmax_sampling=False):\n        """""" Select actions based on model\'s output """"""\n        policy_params, q = self(observation)\n        actions = self.action_head.sample(policy_params, argmax_sampling=argmax_sampling)\n\n        # log probability - we can do that, because we support only discrete action spaces\n        logprobs = self.action_head.logprob(actions, policy_params)\n\n        return {\n            \'actions\': actions,\n            \'q\': q,\n            \'logprobs\': policy_params,\n            \'action:logprobs\': logprobs\n        }\n\n    def evaluate(self, rollout: Rollout) -> QStochasticPolicyEvaluator:\n        """""" Evaluate model on a rollout """"""\n        return QStochasticPolicyEvaluator(self, rollout)\n\n    def value(self, observation):\n        """""" Calculate only value head for given state """"""\n        policy_params, q = self(observation)\n\n        # Expectation of Q value with respect to action\n        return (torch.exp(policy_params) * q).sum(dim=1)\n\n    def entropy(self, action_logits):\n        """""" Entropy of a probability distribution """"""\n        return self.action_head.entropy(action_logits)\n\n    def kl_divergence(self, logits_q, logits_p):\n        """""" Calculate KL-divergence between two probability distributions """"""\n        return self.action_head.kl_divergence(logits_q, logits_p)\n\n\nclass QStochasticPolicyModelFactory(ModelFactory):\n    """""" Factory  class for policy gradient models """"""\n    def __init__(self, input_block: IdentityFactory, backbone: ModelFactory):\n        self.backbone = backbone\n        self.input_block = input_block\n\n    def instantiate(self, **extra_args):\n        """""" Instantiate the model """"""\n        input_block = self.input_block.instantiate()\n        backbone = self.backbone.instantiate(**extra_args)\n\n        return QStochasticPolicyModel(input_block, backbone, extra_args[\'action_space\'])\n\n\ndef create(backbone: ModelFactory, input_block: typing.Optional[ModelFactory]=None):\n    """""" Vel factory function """"""\n    if input_block is None:\n        input_block = IdentityFactory()\n\n    return QStochasticPolicyModelFactory(input_block=input_block, backbone=backbone)\n'"
vel/rl/models/stochastic_policy_model.py,0,"b'import gym\nimport typing\n\nfrom vel.api import LinearBackboneModel, ModelFactory, BackboneModel\nfrom vel.modules.input.identity import IdentityFactory\nfrom vel.rl.api import Rollout, Evaluator, RlModel\nfrom vel.rl.modules.action_head import ActionHead\nfrom vel.rl.modules.value_head import ValueHead\n\n\nclass StochasticPolicyEvaluator(Evaluator):\n    """""" Evaluator for a policy gradient model """"""\n\n    def __init__(self, model: \'StochasticPolicyModel\', rollout: Rollout):\n        super().__init__(rollout)\n\n        self.model = model\n\n        policy_params, estimated_values = model(self.rollout.batch_tensor(\'observations\'))\n\n        self.provide(\'model:policy_params\',  policy_params)\n        self.provide(\'model:values\', estimated_values)\n\n    @Evaluator.provides(\'model:action:logprobs\')\n    def model_action_logprobs(self):\n        actions = self.get(\'rollout:actions\')\n        policy_params = self.get(\'model:policy_params\')\n        return self.model.action_head.logprob(actions, policy_params)\n\n    @Evaluator.provides(\'model:entropy\')\n    def model_entropy(self):\n        policy_params = self.get(\'model:policy_params\')\n        return self.model.entropy(policy_params)\n\n\nclass StochasticPolicyModel(RlModel):\n    """"""\n    Most generic policy gradient model class with a set of common actor-critic heads that share a single backbone\n    """"""\n\n    def __init__(self, input_block: BackboneModel, backbone: LinearBackboneModel, action_space: gym.Space):\n        super().__init__()\n\n        self.input_block = input_block\n        self.backbone = backbone\n        self.action_head = ActionHead(\n            action_space=action_space,\n            input_dim=self.backbone.output_dim\n        )\n        self.value_head = ValueHead(input_dim=self.backbone.output_dim)\n\n    def reset_weights(self):\n        """""" Initialize properly model weights """"""\n        self.input_block.reset_weights()\n        self.backbone.reset_weights()\n        self.action_head.reset_weights()\n        self.value_head.reset_weights()\n\n    def forward(self, observations):\n        """""" Calculate model outputs """"""\n        input_data = self.input_block(observations)\n\n        base_output = self.backbone(input_data)\n\n        action_output = self.action_head(base_output)\n        value_output = self.value_head(base_output)\n\n        return action_output, value_output\n\n    def step(self, observation, argmax_sampling=False):\n        """""" Select actions based on model\'s output """"""\n        action_pd_params, value_output = self(observation)\n        actions = self.action_head.sample(action_pd_params, argmax_sampling=argmax_sampling)\n\n        # log likelihood of selected action\n        logprobs = self.action_head.logprob(actions, action_pd_params)\n\n        return {\n            \'actions\': actions,\n            \'values\': value_output,\n            \'action:logprobs\': logprobs\n        }\n\n    def evaluate(self, rollout: Rollout) -> Evaluator:\n        """""" Evaluate model on a rollout """"""\n        return StochasticPolicyEvaluator(self, rollout)\n\n    def logprob(self, action_sample, policy_params):\n        """""" Calculate - log(prob) of selected actions """"""\n        return self.action_head.logprob(action_sample, policy_params)\n\n    def value(self, observations):\n        """""" Calculate only value head for given state """"""\n        input_data = self.input_block(observations)\n        base_output = self.backbone(input_data)\n        value_output = self.value_head(base_output)\n        return value_output\n\n    def entropy(self, policy_params):\n        """""" Entropy of a probability distribution """"""\n        return self.action_head.entropy(policy_params)\n\n\nclass StochasticPolicyModelFactory(ModelFactory):\n    """""" Factory class for policy gradient models """"""\n    def __init__(self, input_block: IdentityFactory, backbone: ModelFactory):\n        self.backbone = backbone\n        self.input_block = input_block\n\n    def instantiate(self, **extra_args):\n        """""" Instantiate the model """"""\n        input_block = self.input_block.instantiate()\n        backbone = self.backbone.instantiate(**extra_args)\n\n        return StochasticPolicyModel(input_block, backbone, extra_args[\'action_space\'])\n\n\ndef create(backbone: ModelFactory, input_block: typing.Optional[ModelFactory]=None):\n    """""" Vel factory function """"""\n    if input_block is None:\n        input_block = IdentityFactory()\n\n    return StochasticPolicyModelFactory(input_block=input_block, backbone=backbone)\n'"
vel/rl/models/stochastic_policy_model_separate.py,0,"b'import gym\nimport itertools as it\nimport typing\n\nfrom vel.api import LinearBackboneModel, ModelFactory, BackboneModel\nfrom vel.modules.input.identity import IdentityFactory\nfrom vel.rl.api import Rollout, RlModel, Evaluator\nfrom vel.rl.modules.action_head import ActionHead\nfrom vel.rl.modules.value_head import ValueHead\nfrom vel.rl.models.stochastic_policy_model import StochasticPolicyEvaluator\n\n\nclass StochasticPolicyModelSeparate(RlModel):\n    """"""\n    Policy gradient model class with an actor and critic heads that don\'t share a backbone\n    """"""\n\n    def __init__(self, input_block: BackboneModel,\n                 policy_backbone: LinearBackboneModel, value_backbone: LinearBackboneModel,\n                 action_space: gym.Space):\n        super().__init__()\n\n        self.input_block = input_block\n        self.policy_backbone = policy_backbone\n        self.value_backbone = value_backbone\n\n        self.action_head = ActionHead(\n            action_space=action_space,\n            input_dim=self.policy_backbone.output_dim\n        )\n\n        self.value_head = ValueHead(input_dim=self.value_backbone.output_dim)\n\n    def reset_weights(self):\n        """""" Initialize properly model weights """"""\n        self.input_block.reset_weights()\n\n        self.policy_backbone.reset_weights()\n        self.value_backbone.reset_weights()\n\n        self.action_head.reset_weights()\n        self.value_head.reset_weights()\n\n    def forward(self, observations):\n        """""" Calculate model outputs """"""\n        input_data = self.input_block(observations)\n\n        policy_base_output = self.policy_backbone(input_data)\n        value_base_output = self.value_backbone(input_data)\n\n        action_output = self.action_head(policy_base_output)\n        value_output = self.value_head(value_base_output)\n\n        return action_output, value_output\n\n    def step(self, observation, argmax_sampling=False):\n        """""" Select actions based on model\'s output """"""\n        policy_params, values = self(observation)\n        actions = self.action_head.sample(policy_params, argmax_sampling=argmax_sampling)\n\n        # log likelihood of selected action\n        logprobs = self.action_head.logprob(actions, policy_params)\n\n        return {\n            \'actions\': actions,\n            \'values\': values,\n            \'action:logprobs\': logprobs\n        }\n\n    def policy_parameters(self):\n        """""" Parameters of policy """"""\n        return it.chain(self.policy_backbone.parameters(), self.action_head.parameters())\n\n    def logprob(self, action_sample, policy_params):\n        """""" Calculate - log(prob) of selected actions """"""\n        return self.action_head.logprob(action_sample, policy_params)\n\n    def value(self, observations):\n        """""" Calculate only value head for given state """"""\n        input_data = self.input_block(observations)\n        base_output = self.value_backbone(input_data)\n        value_output = self.value_head(base_output)\n        return value_output\n\n    def policy(self, observations):\n        """""" Calculate only action head for given state """"""\n        input_data = self.input_block(observations)\n        policy_base_output = self.policy_backbone(input_data)\n        policy_params = self.action_head(policy_base_output)\n        return policy_params\n\n    def evaluate(self, rollout: Rollout) -> Evaluator:\n        """""" Evaluate model on a rollout """"""\n        return StochasticPolicyEvaluator(self, rollout)\n\n    def entropy(self, policy_params):\n        """""" Entropy of a probability distribution """"""\n        return self.action_head.entropy(policy_params)\n\n    def kl_divergence(self, pd_q, pd_p):\n        """""" Calculate KL-divergence between two probability distributions """"""\n        return self.action_head.kl_divergence(pd_q, pd_p)\n\n\nclass StochasticPolicyModelSeparateFactory(ModelFactory):\n    """""" Factory class for policy gradient models """"""\n    def __init__(self, input_block: ModelFactory, policy_backbone: ModelFactory, value_backbone: ModelFactory):\n        self.input_block = input_block\n        self.policy_backbone = policy_backbone\n        self.value_backbone = value_backbone\n\n    def instantiate(self, **extra_args):\n        """""" Instantiate the model """"""\n        input_block = self.input_block.instantiate()\n        policy_backbone = self.policy_backbone.instantiate(**extra_args)\n        value_backbone = self.value_backbone.instantiate(**extra_args)\n\n        return StochasticPolicyModelSeparate(input_block, policy_backbone, value_backbone, extra_args[\'action_space\'])\n\n\ndef create(policy_backbone: ModelFactory, value_backbone: ModelFactory,\n           input_block: typing.Optional[ModelFactory] = None):\n    """""" Vel factory function """"""\n    if input_block is None:\n        input_block = IdentityFactory()\n\n    return StochasticPolicyModelSeparateFactory(\n        input_block=input_block,\n        policy_backbone=policy_backbone,\n        value_backbone=value_backbone\n    )\n'"
vel/rl/models/stochastic_policy_rnn_model.py,2,"b'import gym\nimport torch\nimport typing\n\nfrom vel.api import RnnLinearBackboneModel, ModelFactory, BackboneModel\nfrom vel.modules.input.identity import IdentityFactory\nfrom vel.rl.api import Rollout, Trajectories, Evaluator, RlRnnModel\nfrom vel.rl.modules.action_head import ActionHead\nfrom vel.rl.modules.value_head import ValueHead\n\n\nclass StochasticPolicyRnnEvaluator(Evaluator):\n    """""" Evaluate recurrent model from initial state """"""\n\n    def __init__(self, model: \'StochasticPolicyRnnModel\', rollout: Rollout):\n        assert isinstance(rollout, Trajectories), ""For an RNN model, we must evaluate trajectories""\n        super().__init__(rollout)\n\n        self.model = model\n\n        observation_trajectories = rollout.transition_tensors[\'observations\']\n        hidden_state = rollout.rollout_tensors[\'initial_hidden_state\']\n\n        action_accumulator = []\n        value_accumulator = []\n\n        # Evaluate recurrent network step by step\n        for i in range(observation_trajectories.size(0)):\n            action_output, value_output, hidden_state = model(observation_trajectories[i], hidden_state)\n            action_accumulator.append(action_output)\n            value_accumulator.append(value_output)\n\n        policy_params = torch.cat(action_accumulator, dim=0)\n        estimated_values = torch.cat(value_accumulator, dim=0)\n\n        self.provide(\'model:policy_params\', policy_params)\n        self.provide(\'model:values\', estimated_values)\n\n    @Evaluator.provides(\'model:action:logprobs\')\n    def model_action_logprobs(self):\n        actions = self.get(\'rollout:actions\')\n        policy_params = self.get(\'model:policy_params\')\n        return self.model.action_head.logprob(actions, policy_params)\n\n    @Evaluator.provides(\'model:entropy\')\n    def model_entropy(self):\n        policy_params = self.get(\'model:policy_params\')\n        return self.model.entropy(policy_params)\n\n\nclass StochasticPolicyRnnModel(RlRnnModel):\n    """"""\n    Most generic policy gradient model class with a set of common actor-critic heads that share a single backbone\n    RNN version\n    """"""\n\n    def __init__(self, input_block: BackboneModel, backbone: RnnLinearBackboneModel, action_space: gym.Space):\n        super().__init__()\n\n        self.input_block = input_block\n        self.backbone = backbone\n\n        self.action_head = ActionHead(\n            action_space=action_space,\n            input_dim=self.backbone.output_dim\n        )\n        self.value_head = ValueHead(input_dim=self.backbone.output_dim)\n\n        assert self.backbone.is_recurrent, ""Backbone must be a recurrent model""\n\n    @property\n    def state_dim(self) -> int:\n        """""" Dimension of model state """"""\n        return self.backbone.state_dim\n\n    def reset_weights(self):\n        """""" Initialize properly model weights """"""\n        self.input_block.reset_weights()\n        self.backbone.reset_weights()\n        self.action_head.reset_weights()\n        self.value_head.reset_weights()\n\n    def forward(self, observations, state):\n        """""" Calculate model outputs """"""\n        input_data = self.input_block(observations)\n        base_output, new_state = self.backbone(input_data, state=state)\n\n        action_output = self.action_head(base_output)\n        value_output = self.value_head(base_output)\n\n        return action_output, value_output, new_state\n\n    def step(self, observations, state, argmax_sampling=False):\n        """""" Select actions based on model\'s output """"""\n        action_pd_params, value_output, new_state = self(observations, state)\n        actions = self.action_head.sample(action_pd_params, argmax_sampling=argmax_sampling)\n\n        # log likelihood of selected action\n        logprobs = self.action_head.logprob(actions, action_pd_params)\n\n        return {\n            \'actions\': actions,\n            \'values\': value_output,\n            \'action:logprobs\': logprobs,\n            \'state\': new_state\n        }\n\n    def evaluate(self, rollout: Rollout) -> Evaluator:\n        """""" Evaluate model on a rollout """"""\n        return StochasticPolicyRnnEvaluator(self, rollout)\n\n    def logprob(self, action_sample, policy_params):\n        """""" Calculate - log(prob) of selected actions """"""\n        return self.action_head.logprob(action_sample, policy_params)\n\n    def value(self, observations, state):\n        """""" Calculate only value head for given state """"""\n        input_data = self.input_block(observations)\n\n        base_output, new_state = self.backbone(input_data, state)\n        value_output = self.value_head(base_output)\n\n        return value_output\n\n    def entropy(self, action_pd_params):\n        """""" Entropy of a probability distribution """"""\n        return self.action_head.entropy(action_pd_params)\n\n\nclass PolicyGradientRnnModelFactory(ModelFactory):\n    """""" Factory class for policy gradient models """"""\n    def __init__(self, input_block: ModelFactory, backbone: ModelFactory):\n        self.input_block = input_block\n        self.backbone = backbone\n\n    def instantiate(self, **extra_args):\n        """""" Instantiate the model """"""\n        input_block = self.input_block.instantiate()\n        backbone = self.backbone.instantiate(**extra_args)\n\n        return StochasticPolicyRnnModel(input_block, backbone, extra_args[\'action_space\'])\n\n\ndef create(backbone: ModelFactory, input_block: typing.Optional[ModelFactory]=None):\n    """""" Vel factory function """"""\n    if input_block is None:\n        input_block = IdentityFactory()\n\n    return PolicyGradientRnnModelFactory(\n        input_block=input_block,\n        backbone=backbone\n    )\n'"
vel/rl/modules/__init__.py,0,b''
vel/rl/modules/action_head.py,15,"b'import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\nimport gym.spaces as spaces\n\n\nclass DiagGaussianActionHead(nn.Module):\n    """"""\n    Action head where actions are normally distibuted uncorrelated variables with specific means and variances.\n\n    Means are calculated directly from the network while standard deviation are a parameter of this module\n    """"""\n\n    LOG2PI = np.log(2.0 * np.pi)\n\n    def __init__(self, input_dim, num_dimensions):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.num_dimensions = num_dimensions\n\n        self.linear_layer = nn.Linear(input_dim, num_dimensions)\n        self.log_std = nn.Parameter(torch.zeros(1, num_dimensions))\n\n    def forward(self, input_data):\n        means = self.linear_layer(input_data)\n        log_std_tile = self.log_std.repeat(means.size(0), 1)\n\n        return torch.stack([means, log_std_tile], dim=-1)\n\n    def sample(self, params, argmax_sampling=False):\n        """""" Sample from a probability space of all actions """"""\n        means = params[:, :, 0]\n        log_std = params[:, :, 1]\n\n        if argmax_sampling:\n            return means\n        else:\n            return torch.randn_like(means) * torch.exp(log_std) + means\n\n    def logprob(self, action_sample, pd_params):\n        """""" Log-likelihood """"""\n        means = pd_params[:, :, 0]\n        log_std = pd_params[:, :, 1]\n\n        std = torch.exp(log_std)\n\n        z_score = (action_sample - means) / std\n\n        return - (0.5 * ((z_score**2 + self.LOG2PI).sum(dim=-1)) + log_std.sum(dim=-1))\n\n    def reset_weights(self):\n        init.orthogonal_(self.linear_layer.weight, gain=0.01)\n        init.constant_(self.linear_layer.bias, 0.0)\n\n    def entropy(self, params):\n        """"""\n        Categorical distribution entropy calculation - sum probs * log(probs).\n        In case of diagonal gaussian distribution - 1/2 log(2 pi e sigma^2)\n        """"""\n        log_std = params[:, :, 1]\n        return (log_std + 0.5 * (self.LOG2PI + 1)).sum(dim=-1)\n\n    def kl_divergence(self, params_q, params_p):\n        """"""\n        Categorical distribution KL divergence calculation\n        KL(Q || P) = sum Q_i log (Q_i / P_i)\n\n        Formula is:\n        log(sigma_p) - log(sigma_q) + (sigma_q^2 + (mu_q - mu_p)^2))/(2 * sigma_p^2)\n        """"""\n        means_q = params_q[:, :, 0]\n        log_std_q = params_q[:, :, 1]\n\n        means_p = params_p[:, :, 0]\n        log_std_p = params_p[:, :, 1]\n\n        std_q = torch.exp(log_std_q)\n        std_p = torch.exp(log_std_p)\n\n        kl_div = log_std_p - log_std_q + (std_q ** 2 + (means_q - means_p) ** 2) / (2.0 * std_p ** 2) - 0.5\n\n        return kl_div.sum(dim=-1)\n\n\nclass CategoricalActionHead(nn.Module):\n    """""" Action head with categorical actions """"""\n    def __init__(self, input_dim, num_actions):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.num_actions = num_actions\n\n        self.linear_layer = nn.Linear(input_dim, num_actions)\n\n    def forward(self, input_data):\n        return F.log_softmax(self.linear_layer(input_data), dim=1)\n\n    def logprob(self, actions, action_logits):\n        """""" Logarithm of probability of given sample """"""\n        neg_log_prob = F.nll_loss(action_logits, actions, reduction=\'none\')\n        return -neg_log_prob\n\n    def sample(self, logits, argmax_sampling=False):\n        """""" Sample from a probability space of all actions """"""\n        if argmax_sampling:\n            return torch.argmax(logits, dim=-1)\n        else:\n            u = torch.rand_like(logits)\n            return torch.argmax(logits - torch.log(-torch.log(u)), dim=-1)\n\n    def reset_weights(self):\n        init.orthogonal_(self.linear_layer.weight, gain=0.01)\n        init.constant_(self.linear_layer.bias, 0.0)\n\n    def entropy(self, logits):\n        """""" Categorical distribution entropy calculation - sum probs * log(probs) """"""\n        probs = torch.exp(logits)\n        entropy = - torch.sum(probs * logits, dim=-1)\n        return entropy\n\n    def kl_divergence(self, logits_q, logits_p):\n        """"""\n        Categorical distribution KL divergence calculation\n        KL(Q || P) = sum Q_i log (Q_i / P_i)\n        When talking about logits this is:\n        sum exp(Q_i) * (Q_i - P_i)\n        """"""\n        return (torch.exp(logits_q) * (logits_q - logits_p)).sum(1, keepdim=True)\n\n\nclass ActionHead(nn.Module):\n    """"""\n    Network head for action determination. Returns probability distribution parametrization\n    """"""\n\n    def __init__(self, input_dim, action_space):\n        super().__init__()\n\n        self.action_space = action_space\n\n        if isinstance(action_space, spaces.Box):\n            assert len(action_space.shape) == 1\n            self.head = DiagGaussianActionHead(input_dim, action_space.shape[0])\n        elif isinstance(action_space, spaces.Discrete):\n            self.head = CategoricalActionHead(input_dim, action_space.n)\n        # elif isinstance(action_space, spaces.MultiDiscrete):\n        #     return MultiCategoricalPdType(action_space.nvec)\n        # elif isinstance(action_space, spaces.MultiBinary):\n        #     return BernoulliPdType(action_space.n)\n        else:\n            raise NotImplementedError\n\n    def forward(self, input_data):\n        return self.head(input_data)\n\n    def sample(self, policy_params, **kwargs):\n        """""" Sample from a probability space of all actions """"""\n        return self.head.sample(policy_params, **kwargs)\n\n    def reset_weights(self):\n        """""" Initialize weights to sane defaults """"""\n        self.head.reset_weights()\n\n    def entropy(self, policy_params):\n        """""" Entropy calculation - sum probs * log(probs) """"""\n        return self.head.entropy(policy_params)\n\n    def kl_divergence(self, params_q, params_p):\n        """""" Kullback\xe2\x80\x93Leibler divergence between two sets of parameters """"""\n        return self.head.kl_divergence(params_q, params_p)\n\n    def logprob(self, action_sample, policy_params):\n        """""" - log probabilty of selected actions """"""\n        return self.head.logprob(action_sample, policy_params)\n'"
vel/rl/modules/deterministic_action_head.py,4,"b'import numpy as np\nimport gym.spaces as spaces\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\n\nclass DeterministicActionHead(nn.Module):\n    """"""\n    Network head for action determination. Returns deterministic action depending on the inputs\n    """"""\n\n    def __init__(self, input_dim, action_space):\n        super().__init__()\n\n        self.action_space = action_space\n\n        assert isinstance(action_space, spaces.Box)\n        assert len(action_space.shape) == 1\n\n        assert (np.abs(action_space.low) == action_space.high).all()  # we assume symmetric actions.\n        self.register_buffer(\'max_action\', torch.from_numpy(action_space.high))\n\n        self.linear_layer = nn.Linear(input_dim, action_space.shape[0])\n\n    def forward(self, input_data):\n        return torch.tanh(self.linear_layer(input_data)) * self.max_action\n\n    def sample(self, params, **_):\n        """""" Sample from a probability space of all actions """"""\n        return {\n            \'actions\': self(params)\n        }\n\n    def reset_weights(self):\n        """""" Initialize weights to sane defaults """"""\n        init.orthogonal_(self.linear_layer.weight, gain=0.01)\n        init.constant_(self.linear_layer.bias, 0.0)\n'"
vel/rl/modules/deterministic_critic_head.py,2,"b'import torch.nn as nn\nimport torch.nn.init as init\n\n\nclass DeterministicCriticHead(nn.Module):\n    """"""\n    Network head for action-dependent critic.\n    Returns deterministic action-value for given combination of action and state.\n    """"""\n\n    def __init__(self, input_dim):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.linear = nn.Linear(input_dim, 1)\n\n    def forward(self, input_data):\n        return self.linear(input_data)[:, 0]\n\n    def reset_weights(self):\n        """""" Initialize weights to sane defaults """"""\n        init.uniform_(self.linear.weight, -3e-3, 3e-3)\n        init.zeros_(self.linear.bias)\n'"
vel/rl/modules/noisy_linear.py,9,"b'""""""\nCode based on:\nhttps://github.com/Kaixhin/Rainbow/blob/master/model.py\n""""""\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\n\n\ndef scaled_noise(size, device):\n    x = torch.randn(size, device=device)\n    return x.sign().mul_(x.abs().sqrt_())\n\n\ndef factorized_gaussian_noise(in_features, out_features, device):\n    """"""\n    Factorised (cheaper) gaussian noise from ""Noisy Networks for Exploration""\n    by Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot and others\n    """"""\n    in_noise = scaled_noise(in_features, device=device)\n    out_noise = scaled_noise(out_features, device=device)\n\n    return out_noise.ger(in_noise), out_noise\n\n\ndef gaussian_noise(in_features, out_features, device):\n    """""" Normal gaussian N(0, 1) noise """"""\n    return torch.randn((in_features, out_features), device=device), torch.randn(out_features, device=device)\n\n\nclass NoisyLinear(nn.Module):\n    """""" NoisyNets noisy linear layer """"""\n    def __init__(self, in_features, out_features, initial_std_dev: float = 0.4, factorized_noise: bool = True):\n        super(NoisyLinear, self).__init__()\n\n        self.in_features = in_features\n        self.out_features = out_features\n        self.initial_std_dev = initial_std_dev\n        self.factorized_noise = factorized_noise\n\n        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n\n        self.bias_mu = nn.Parameter(torch.empty(out_features))\n        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n\n    def reset_weights(self):\n        init.orthogonal_(self.weight_mu, gain=math.sqrt(2))\n        init.constant_(self.bias_mu, 0.0)\n\n        # Initialize the ""random weights"" to constants\n        self.weight_sigma.data.fill_(self.initial_std_dev / math.sqrt(self.in_features))\n        self.bias_sigma.data.fill_(self.initial_std_dev / math.sqrt(self.out_features))\n\n    def forward(self, input_data):\n        if self.training:\n            if self.factorized_noise:\n                weight_epsilon, bias_epsilon = factorized_gaussian_noise(\n                    self.in_features, self.out_features, device=input_data.device\n                )\n            else:\n                weight_epsilon, bias_epsilon = gaussian_noise(\n                    self.in_features, self.out_features, device=input_data.device\n                )\n\n            return F.linear(\n                input_data,\n                self.weight_mu + self.weight_sigma * weight_epsilon,\n                self.bias_mu + self.bias_sigma * bias_epsilon\n            )\n        else:\n            return F.linear(input_data, self.weight_mu, self.bias_mu)\n\n    def extra_repr(self):\n        r""""""Set the extra representation of the module\n\n        To print customized extra information, you should reimplement\n        this method in your own modules. Both single-line and multi-line\n        strings are acceptable.\n        """"""\n        return (\n            f\'{self.in_features}, {self.out_features}, initial_std_dev={self.initial_std_dev}, \'\n            \'factorized_noise={self.factorized_noise} \'\n        )\n'"
vel/rl/modules/q_distributional_head.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\n\nimport gym.spaces as spaces\n\n\nclass QDistributionalHead(nn.Module):\n    """""" Network head calculating Q-function value for each (discrete) action. """"""\n    def __init__(self, input_dim, action_space, vmin: float, vmax: float, atoms: int = 1):\n        super().__init__()\n\n        # Q-function requires a discrete action space\n        assert isinstance(action_space, spaces.Discrete)\n        assert vmax > vmin\n\n        self.atoms = atoms\n        self.vmin = vmin\n        self.vmax = vmax\n\n        self.action_space = action_space\n        self.action_size = action_space.n\n\n        self.atom_delta = (self.vmax - self.vmin) / (self.atoms - 1)\n\n        self.linear_layer = nn.Linear(input_dim, self.action_size * self.atoms)\n\n        self.register_buffer(\'support_atoms\', torch.linspace(self.vmin, self.vmax, self.atoms))\n\n    def histogram_info(self) -> dict:\n        """""" Return extra information about histogram """"""\n        return {\n            \'support_atoms\': self.support_atoms,\n            \'atom_delta\': self.atom_delta,\n            \'vmin\': self.vmin,\n            \'vmax\': self.vmax,\n            \'num_atoms\': self.atoms\n        }\n\n    def reset_weights(self):\n        init.orthogonal_(self.linear_layer.weight, gain=1.0)\n        init.constant_(self.linear_layer.bias, 0.0)\n\n    def forward(self, input_data):\n        histogram_logits = self.linear_layer(input_data).view(input_data.size(0), self.action_size, self.atoms)\n        histogram_log = F.log_softmax(histogram_logits, dim=2)\n\n        # Calculate log-softmax to establish log-probability distribution\n        return histogram_log\n\n    def sample(self, histogram_logits):\n        """""" Sample from a greedy strategy with given q-value histogram """"""\n        histogram_probs = histogram_logits.exp()  # Batch size * actions * atoms\n        atoms = self.support_atoms.view(1, 1, self.atoms)  # Need to introduce two new dimensions\n        return (histogram_probs * atoms).sum(dim=-1).argmax(dim=1)  # Argmax of expectation\n'"
vel/rl/modules/q_distributional_noisy_dueling_head.py,3,"b'import gym.spaces as spaces\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nfrom vel.rl.modules.noisy_linear import NoisyLinear\n\n\nclass QDistributionalNoisyDuelingHead(nn.Module):\n    """""" Network head calculating Q-function value for each (discrete) action. """"""\n    def __init__(self, input_dim, action_space, vmin: float, vmax: float, atoms: int = 1,\n                 initial_std_dev: float = 0.4, factorized_noise: bool = True):\n        super().__init__()\n\n        # Q-function requires a discrete action space\n        assert isinstance(action_space, spaces.Discrete)\n        assert vmax > vmin\n\n        self.atoms = atoms\n        self.vmin = vmin\n        self.vmax = vmax\n\n        self.action_size = action_space.n\n        self.action_space = action_space\n\n        self.atom_delta = (self.vmax - self.vmin) / (self.atoms - 1)\n\n        self.linear_layer_advantage = NoisyLinear(\n            input_dim, self.action_size * self.atoms, initial_std_dev=initial_std_dev, factorized_noise=factorized_noise\n        )\n\n        self.linear_layer_value = NoisyLinear(\n            input_dim, self.atoms, initial_std_dev=initial_std_dev, factorized_noise=factorized_noise\n        )\n\n        self.register_buffer(\'support_atoms\', torch.linspace(self.vmin, self.vmax, self.atoms))\n\n    def histogram_info(self) -> dict:\n        """""" Return extra information about histogram """"""\n        return {\n            \'support_atoms\': self.support_atoms,\n            \'atom_delta\': self.atom_delta,\n            \'vmin\': self.vmin,\n            \'vmax\': self.vmax,\n            \'num_atoms\': self.atoms\n        }\n\n    def reset_weights(self):\n        self.linear_layer_advantage.reset_weights()\n        self.linear_layer_value.reset_weights()\n\n    def forward(self, advantage_features, value_features):\n        adv = self.linear_layer_advantage(advantage_features).view(-1, self.action_size, self.atoms)\n        val = self.linear_layer_value(value_features).view(-1, 1, self.atoms)\n\n        # I\'m quite unsure if this is the right way to combine these, but this is what paper seems to be suggesting\n        # and I don\'t know any better way.\n        histogram_output = val + adv - adv.mean(dim=1, keepdim=True)\n\n        return F.log_softmax(histogram_output, dim=2)\n\n    def sample(self, histogram_logits):\n        """""" Sample from a greedy strategy with given q-value histogram """"""\n        histogram_probs = histogram_logits.exp()  # Batch size * actions * atoms\n        atoms = self.support_atoms.view(1, 1, self.atoms)  # Need to introduce two new dimensions\n        return (histogram_probs * atoms).sum(dim=-1).argmax(dim=1)  # Argmax of expectation\n'"
vel/rl/modules/q_dueling_head.py,2,"b'import torch.nn as nn\nimport torch.nn.init as init\n\nimport gym.spaces as spaces\n\n\nclass QDuelingHead(nn.Module):\n    """""" Network head calculating Q-function value for each (discrete) action using two separate inputs. """"""\n    def __init__(self, input_dim, action_space):\n        super().__init__()\n\n        # Q-function requires a discrete action space\n        assert isinstance(action_space, spaces.Discrete)\n\n        self.linear_layer_advantage = nn.Linear(input_dim, action_space.n)\n        self.linear_layer_value = nn.Linear(input_dim, 1)\n        self.action_space = action_space\n\n    def reset_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                # init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                init.orthogonal_(m.weight, gain=1.0)\n                init.constant_(m.bias, 0.0)\n\n    def forward(self, advantage_data, value_data):\n        adv = self.linear_layer_advantage(advantage_data)\n        value = self.linear_layer_value(value_data)\n        # Advantage must be 0-centered\n\n        return (adv - adv.mean(dim=1, keepdim=True)) + value\n\n    def sample(self, q_values):\n        """""" Sample from greedy strategy with given q-values """"""\n        return q_values.argmax(dim=1)\n'"
vel/rl/modules/q_head.py,2,"b'import torch.nn as nn\nimport torch.nn.init as init\n\nimport gym.spaces as spaces\n\n\nclass QHead(nn.Module):\n    """""" Network head calculating Q-function value for each (discrete) action. """"""\n    def __init__(self, input_dim, action_space):\n        super().__init__()\n\n        # Q-function requires a discrete action space\n        assert isinstance(action_space, spaces.Discrete)\n\n        self.action_space = action_space\n\n        self.linear_layer = nn.Linear(input_dim, action_space.n)\n\n    def reset_weights(self):\n        init.orthogonal_(self.linear_layer.weight, gain=1.0)\n        init.constant_(self.linear_layer.bias, 0.0)\n\n    def forward(self, input_data):\n        return self.linear_layer(input_data)\n\n    def sample(self, q_values):\n        """""" Sample from epsilon-greedy strategy with given q-values """"""\n        return q_values.argmax(dim=1)\n\n'"
vel/rl/modules/q_noisy_head.py,1,"b'import torch.nn as nn\n\nimport gym.spaces as spaces\n\nfrom vel.rl.modules.noisy_linear import NoisyLinear\n\n\nclass QNoisyHead(nn.Module):\n    """""" Network head calculating Q-function value for each (discrete) action. """"""\n    def __init__(self, input_dim, action_space, initial_std_dev=0.4, factorized_noise=True):\n        super().__init__()\n\n        # Q-function requires a discrete action space\n        assert isinstance(action_space, spaces.Discrete)\n\n        self.action_space = action_space\n\n        self.linear_layer = NoisyLinear(\n            input_dim, action_space.n, initial_std_dev=initial_std_dev, factorized_noise=factorized_noise\n        )\n\n    def reset_weights(self):\n        self.linear_layer.reset_weights()\n\n    def forward(self, input_data):\n        return self.linear_layer(input_data)\n\n    def sample(self, q_values):\n        """""" Sample from epsilon-greedy strategy with given q-values """"""\n        return q_values.argmax(dim=1)\n'"
vel/rl/modules/value_head.py,2,"b'import torch.nn as nn\nimport torch.nn.init as init\n\n\nclass ValueHead(nn.Module):\n    """""" Network head for value determination """"""\n    def __init__(self, input_dim):\n        super().__init__()\n\n        self.linear_layer = nn.Linear(input_dim, 1)\n\n    def reset_weights(self):\n        # init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n        init.orthogonal_(self.linear_layer.weight, gain=1.0)\n        init.constant_(self.linear_layer.bias, 0.0)\n\n    def forward(self, input_data):\n        return self.linear_layer(input_data)[:, 0]\n'"
vel/rl/reinforcers/__init__.py,0,b''
vel/rl/reinforcers/buffered_mixed_policy_iteration_reinforcer.py,2,"b'import attr\nimport numpy as np\nimport sys\nimport torch\nimport tqdm\n\nfrom vel.api import TrainingInfo, EpochInfo, BatchInfo, Model, ModelFactory\nfrom vel.openai.baselines.common.vec_env import VecEnv\nfrom vel.rl.api import (\n    ReinforcerBase, ReinforcerFactory, VecEnvFactory, ReplayEnvRollerBase, AlgoBase, ReplayEnvRollerFactoryBase\n)\nfrom vel.rl.metrics import (\n    FPSMetric, EpisodeLengthMetric, EpisodeRewardMetricQuantile, EpisodeRewardMetric, FramesMetric\n)\n\n\n@attr.s(auto_attribs=True)\nclass BufferedMixedPolicyIterationReinforcerSettings:\n    """""" Settings dataclass for a policy gradient reinforcer """"""\n    number_of_steps: int\n    experience_replay: int = 1\n    stochastic_experience_replay: bool = True\n\n\nclass BufferedMixedPolicyIterationReinforcer(ReinforcerBase):\n    """"""\n    A \'mixed\' reinforcer that does both, on-policy learning from environment rollouts and off-policy learning\n    from a replay buffer.\n\n    Environments are rolled out in parallel and used as normally in off-policy learning.\n    After that, each rollout is stored in a buffer that is sampled specified number of times per-environment\n    for replay batches.\n    """"""\n\n    def __init__(self, device: torch.device, settings: BufferedMixedPolicyIterationReinforcerSettings, env: VecEnv,\n                 model: Model, env_roller: ReplayEnvRollerBase, algo: AlgoBase) -> None:\n        self.device = device\n        self.settings = settings\n\n        self.environment = env\n        self._trained_model = model.to(self.device)\n\n        self.env_roller = env_roller\n        self.algo = algo\n\n    def metrics(self) -> list:\n        """""" List of metrics to track for this learning process """"""\n        my_metrics = [\n            FramesMetric(""frames""),\n            FPSMetric(""fps""),\n            EpisodeRewardMetric(\'PMM:episode_rewards\'),\n            EpisodeRewardMetricQuantile(\'P09:episode_rewards\', quantile=0.9),\n            EpisodeRewardMetricQuantile(\'P01:episode_rewards\', quantile=0.1),\n            EpisodeLengthMetric(""episode_length"")\n        ]\n\n        return my_metrics + self.algo.metrics() + self.env_roller.metrics()\n\n    @property\n    def model(self) -> Model:\n        """""" Model trained by this reinforcer """"""\n        return self._trained_model\n\n    def initialize_training(self, training_info: TrainingInfo, model_state=None, hidden_state=None):\n        """""" Prepare models for training """"""\n        if model_state is not None:\n            self.model.load_state_dict(model_state)\n        else:\n            self.model.reset_weights()\n\n        self.algo.initialize(\n            training_info=training_info, model=self.model, environment=self.environment, device=self.device\n        )\n\n    def train_epoch(self, epoch_info: EpochInfo, interactive=True):\n        """""" Train model on an epoch of a fixed number of batch updates """"""\n        epoch_info.on_epoch_begin()\n\n        if interactive:\n            iterator = tqdm.trange(epoch_info.batches_per_epoch, file=sys.stdout, desc=""Training"", unit=""batch"")\n        else:\n            iterator = range(epoch_info.batches_per_epoch)\n\n        for batch_idx in iterator:\n            batch_info = BatchInfo(epoch_info, batch_idx)\n\n            batch_info.on_batch_begin()\n            self.train_batch(batch_info)\n            batch_info.on_batch_end()\n\n        epoch_info.result_accumulator.freeze_results()\n        epoch_info.on_epoch_end()\n\n    def train_batch(self, batch_info: BatchInfo):\n        """""" Single, most atomic \'step\' of learning this reinforcer can perform """"""\n        batch_info[\'sub_batch_data\'] = []\n\n        self.on_policy_train_batch(batch_info)\n\n        if self.settings.experience_replay > 0 and self.env_roller.is_ready_for_sampling():\n            if self.settings.stochastic_experience_replay:\n                experience_replay_count = np.random.poisson(self.settings.experience_replay)\n            else:\n                experience_replay_count = self.settings.experience_replay\n\n            for i in range(experience_replay_count):\n                self.off_policy_train_batch(batch_info)\n\n        # Even with all the experience replay, we count the single rollout as a single batch\n        batch_info.aggregate_key(\'sub_batch_data\')\n\n    def on_policy_train_batch(self, batch_info: BatchInfo):\n        """""" Perform an \'on-policy\' training step of evaluating an env and a single backpropagation step """"""\n        self.model.train()\n\n        rollout = self.env_roller.rollout(batch_info, self.model, self.settings.number_of_steps).to_device(self.device)\n\n        batch_result = self.algo.optimizer_step(\n            batch_info=batch_info,\n            device=self.device,\n            model=self.model,\n            rollout=rollout\n        )\n\n        batch_info[\'sub_batch_data\'].append(batch_result)\n        batch_info[\'frames\'] = rollout.frames()\n        batch_info[\'episode_infos\'] = rollout.episode_information()\n\n    def off_policy_train_batch(self, batch_info: BatchInfo):\n        """""" Perform an \'off-policy\' training step of sampling the replay buffer and gradient descent """"""\n        self.model.train()\n\n        rollout = self.env_roller.sample(batch_info, self.model, self.settings.number_of_steps).to_device(self.device)\n\n        batch_result = self.algo.optimizer_step(\n            batch_info=batch_info,\n            device=self.device,\n            model=self.model,\n            rollout=rollout\n        )\n\n        batch_info[\'sub_batch_data\'].append(batch_result)\n\n\nclass BufferedMixedPolicyIterationReinforcerFactory(ReinforcerFactory):\n    """""" Factory class for the PolicyGradientReplayBuffer factory """"""\n    def __init__(self, settings, env_factory: VecEnvFactory, model_factory: ModelFactory,\n                 env_roller_factory: ReplayEnvRollerFactoryBase, algo: AlgoBase, parallel_envs: int, seed: int):\n        self.settings = settings\n\n        self.model_factory = model_factory\n        self.env_factory = env_factory\n        self.parallel_envs = parallel_envs\n        self.env_roller_factory = env_roller_factory\n        self.algo = algo\n        self.seed = seed\n\n    def instantiate(self, device: torch.device) -> ReinforcerBase:\n        env = self.env_factory.instantiate(parallel_envs=self.parallel_envs, seed=self.seed)\n        model = self.model_factory.instantiate(action_space=env.action_space)\n        env_roller = self.env_roller_factory.instantiate(env, device)\n\n        return BufferedMixedPolicyIterationReinforcer(device, self.settings, env, model, env_roller, self.algo)\n\n\ndef create(model_config, model, vec_env, algo, env_roller,\n           parallel_envs, number_of_steps,\n           experience_replay=1, stochastic_experience_replay=True):\n    """""" Vel factory function """"""\n    settings = BufferedMixedPolicyIterationReinforcerSettings(\n        experience_replay=experience_replay,\n        stochastic_experience_replay=stochastic_experience_replay,\n        number_of_steps=number_of_steps\n    )\n\n    return BufferedMixedPolicyIterationReinforcerFactory(\n        settings,\n        env_factory=vec_env,\n        model_factory=model,\n        parallel_envs=parallel_envs,\n        env_roller_factory=env_roller,\n        algo=algo,\n        seed=model_config.seed\n    )\n'"
vel/rl/reinforcers/buffered_off_policy_iteration_reinforcer.py,2,"b'import attr\nimport sys\nimport tqdm\n\nimport torch\n\nfrom vel.api import TrainingInfo, EpochInfo, BatchInfo, Model, ModelFactory\nfrom vel.openai.baselines.common.vec_env import VecEnv\nfrom vel.rl.api import (\n    ReinforcerBase, ReinforcerFactory, ReplayEnvRollerBase, AlgoBase, VecEnvFactory, ReplayEnvRollerFactoryBase\n)\nfrom vel.rl.metrics import (\n    FPSMetric, EpisodeLengthMetric, EpisodeRewardMetricQuantile, EpisodeRewardMetric, FramesMetric,\n)\n\n\n@attr.s(auto_attribs=True)\nclass BufferedOffPolicyIterationReinforcerSettings:\n    """""" Settings class for deep Q-Learning """"""\n    # How many steps to roll out for each env\n    rollout_steps: int\n\n    # How many steps to generate as a roll out from replay buffer\n    training_steps: int\n    # How many times to roll out\n    training_rounds: int = 1\n\n\nclass BufferedOffPolicyIterationReinforcer(ReinforcerBase):\n    """"""\n    An off-policy reinforcer that rolls out environment and stores transitions in a buffer.\n    Afterwards, it samples experience batches from this buffer to train the policy.\n    """"""\n    def __init__(self, device: torch.device, settings: BufferedOffPolicyIterationReinforcerSettings,\n                 environment: VecEnv, model: Model, algo: AlgoBase, env_roller: ReplayEnvRollerBase):\n        self.device = device\n        self.settings = settings\n        self.environment = environment\n\n        self._trained_model = model.to(self.device)\n        self.algo = algo\n\n        self.env_roller = env_roller\n\n    def metrics(self) -> list:\n        """""" List of metrics to track for this learning process """"""\n        my_metrics = [\n            FramesMetric(""frames""),\n            FPSMetric(""fps""),\n            EpisodeRewardMetric(\'PMM:episode_rewards\'),\n            EpisodeRewardMetricQuantile(\'P09:episode_rewards\', quantile=0.9),\n            EpisodeRewardMetricQuantile(\'P01:episode_rewards\', quantile=0.1),\n            EpisodeLengthMetric(""episode_length"")\n        ]\n\n        return my_metrics + self.algo.metrics() + self.env_roller.metrics()\n\n    @property\n    def model(self) -> Model:\n        return self._trained_model\n\n    def initialize_training(self, training_info: TrainingInfo, model_state=None, hidden_state=None):\n        """""" Prepare models for training """"""\n        if model_state is not None:\n            self.model.load_state_dict(model_state)\n        else:\n            self.model.reset_weights()\n\n        self.algo.initialize(\n            training_info=training_info, model=self.model, environment=self.environment, device=self.device\n        )\n\n    def train_epoch(self, epoch_info: EpochInfo, interactive=True) -> None:\n        """""" Train model for a single epoch  """"""\n        epoch_info.on_epoch_begin()\n\n        if interactive:\n            iterator = tqdm.trange(epoch_info.batches_per_epoch, file=sys.stdout, desc=""Training"", unit=""batch"")\n        else:\n            iterator = range(epoch_info.batches_per_epoch)\n\n        for batch_idx in iterator:\n            batch_info = BatchInfo(epoch_info, batch_idx)\n\n            batch_info.on_batch_begin()\n            self.train_batch(batch_info)\n            batch_info.on_batch_end()\n\n        epoch_info.result_accumulator.freeze_results()\n        epoch_info.on_epoch_end()\n\n    def train_batch(self, batch_info: BatchInfo) -> None:\n        """"""\n        Batch - the most atomic unit of learning.\n\n        For this reinforforcer, that involves:\n\n        1. Roll out environment and store out experience in the buffer\n        2. Sample the buffer and train the algo on sample batch\n        """"""\n        # For each reinforcer batch:\n\n        # 1. Roll out environment and store out experience in the buffer\n        self.roll_out_and_store(batch_info)\n\n        # 2. Sample the buffer and train the algo on sample batch\n        self.train_on_replay_memory(batch_info)\n\n    def roll_out_and_store(self, batch_info):\n        """""" Roll out environment and store result in the replay buffer """"""\n        self.model.train()\n\n        if self.env_roller.is_ready_for_sampling():\n            rollout = self.env_roller.rollout(batch_info, self.model, self.settings.rollout_steps).to_device(self.device)\n\n            # Store some information about the rollout, no training phase\n            batch_info[\'frames\'] = rollout.frames()\n            batch_info[\'episode_infos\'] = rollout.episode_information()\n        else:\n            frames = 0\n            episode_infos = []\n\n            with tqdm.tqdm(desc=""Populating memory"", total=self.env_roller.initial_memory_size_hint()) as pbar:\n                while not self.env_roller.is_ready_for_sampling():\n                    rollout = self.env_roller.rollout(batch_info, self.model, self.settings.rollout_steps).to_device(self.device)\n\n                    new_frames = rollout.frames()\n                    frames += new_frames\n                    episode_infos.extend(rollout.episode_information())\n\n                    pbar.update(new_frames)\n\n            # Store some information about the rollout, no training phase\n            batch_info[\'frames\'] = frames\n            batch_info[\'episode_infos\'] = episode_infos\n\n    def train_on_replay_memory(self, batch_info):\n        """""" Train agent on a memory gotten from replay buffer """"""\n        self.model.train()\n\n        # Algo will aggregate data into this list:\n        batch_info[\'sub_batch_data\'] = []\n\n        for i in range(self.settings.training_rounds):\n            sampled_rollout = self.env_roller.sample(batch_info, self.model, self.settings.training_steps)\n\n            batch_result = self.algo.optimizer_step(\n                batch_info=batch_info,\n                device=self.device,\n                model=self.model,\n                rollout=sampled_rollout.to_device(self.device)\n            )\n\n            self.env_roller.update(rollout=sampled_rollout, batch_info=batch_result)\n\n            batch_info[\'sub_batch_data\'].append(batch_result)\n\n        batch_info.aggregate_key(\'sub_batch_data\')\n\n\nclass BufferedOffPolicyIterationReinforcerFactory(ReinforcerFactory):\n    """""" Factory class for the DQN reinforcer """"""\n\n    def __init__(self, settings, env_factory: VecEnvFactory, model_factory: ModelFactory,\n                 algo: AlgoBase, env_roller_factory: ReplayEnvRollerFactoryBase, parallel_envs: int, seed: int):\n        self.settings = settings\n\n        self.env_factory = env_factory\n        self.model_factory = model_factory\n        self.algo = algo\n        self.env_roller_factory = env_roller_factory\n        self.parallel_envs = parallel_envs\n        self.seed = seed\n\n    def instantiate(self, device: torch.device) -> BufferedOffPolicyIterationReinforcer:\n        env = self.env_factory.instantiate(parallel_envs=self.parallel_envs, seed=self.seed)\n        env_roller = self.env_roller_factory.instantiate(env, device)\n        model = self.model_factory.instantiate(action_space=env.action_space)\n\n        return BufferedOffPolicyIterationReinforcer(\n            device=device,\n            settings=self.settings,\n            environment=env,\n            model=model,\n            algo=self.algo,\n            env_roller=env_roller\n        )\n\n\ndef create(model_config, vec_env, model, algo, env_roller, parallel_envs: int,\n           rollout_steps: int, training_steps: int, training_rounds: int=1):\n    """""" Vel factory function """"""\n    settings = BufferedOffPolicyIterationReinforcerSettings(\n        rollout_steps=rollout_steps,\n        training_steps=training_steps,\n        training_rounds=training_rounds\n    )\n\n    return BufferedOffPolicyIterationReinforcerFactory(\n        settings=settings,\n        env_factory=vec_env,\n        model_factory=model,\n        algo=algo,\n        env_roller_factory=env_roller,\n        parallel_envs=parallel_envs,\n        seed=model_config.seed\n    )\n'"
vel/rl/reinforcers/on_policy_iteration_reinforcer.py,2,"b'import attr\nimport numpy as np\nimport sys\nimport torch\nimport tqdm\n\nfrom vel.api import Model, ModelFactory, TrainingInfo, EpochInfo, BatchInfo\nfrom vel.rl.api import ReinforcerBase, ReinforcerFactory, VecEnvFactory, EnvRollerFactoryBase, EnvRollerBase, AlgoBase\nfrom vel.rl.metrics import (\n    FPSMetric, EpisodeLengthMetric, EpisodeRewardMetricQuantile,\n    EpisodeRewardMetric, FramesMetric\n)\n\n\n@attr.s(auto_attribs=True)\nclass OnPolicyIterationReinforcerSettings:\n    """""" Settings dataclass for a policy gradient reinforcer """"""\n    number_of_steps: int\n\n    batch_size: int = 256\n    experience_replay: int = 1\n    stochastic_experience_replay: bool = False\n\n    # For each experience replay loop, shuffle transitions to randomize gradient calculations\n    # That means, disregarding actual trajectory order\n    # Does not work with RNN policies\n    shuffle_transitions: bool = True\n\n\nclass OnPolicyIterationReinforcer(ReinforcerBase):\n    """"""\n    A reinforcer that calculates on-policy environment rollouts and uses them to train policy directly.\n    May split the sample into multiple batches and may replay batches a few times.\n    """"""\n    def __init__(self, device: torch.device, settings: OnPolicyIterationReinforcerSettings, model: Model,\n                 algo: AlgoBase, env_roller: EnvRollerBase) -> None:\n        self.device = device\n        self.settings = settings\n\n        self._trained_model = model.to(self.device)\n\n        self.env_roller = env_roller\n        self.algo = algo\n\n    def metrics(self) -> list:\n        """""" List of metrics to track for this learning process """"""\n        my_metrics = [\n            FramesMetric(""frames""),\n            FPSMetric(""fps""),\n            EpisodeRewardMetric(\'PMM:episode_rewards\'),\n            EpisodeRewardMetricQuantile(\'P09:episode_rewards\', quantile=0.9),\n            EpisodeRewardMetricQuantile(\'P01:episode_rewards\', quantile=0.1),\n            EpisodeLengthMetric(""episode_length""),\n        ]\n\n        return my_metrics + self.algo.metrics() + self.env_roller.metrics()\n\n    @property\n    def model(self) -> Model:\n        """""" Model trained by this reinforcer """"""\n        return self._trained_model\n\n    def initialize_training(self, training_info: TrainingInfo, model_state=None, hidden_state=None):\n        """""" Prepare models for training """"""\n        if model_state is not None:\n            self.model.load_state_dict(model_state)\n        else:\n            self.model.reset_weights()\n\n        self.algo.initialize(\n            training_info=training_info, model=self.model, environment=self.env_roller.environment, device=self.device\n        )\n\n    def train_epoch(self, epoch_info: EpochInfo, interactive=True) -> None:\n        """""" Train model on an epoch of a fixed number of batch updates """"""\n        epoch_info.on_epoch_begin()\n\n        if interactive:\n            iterator = tqdm.trange(epoch_info.batches_per_epoch, file=sys.stdout, desc=""Training"", unit=""batch"")\n        else:\n            iterator = range(epoch_info.batches_per_epoch)\n\n        for batch_idx in iterator:\n            batch_info = BatchInfo(epoch_info, batch_idx)\n\n            batch_info.on_batch_begin()\n            self.train_batch(batch_info)\n            batch_info.on_batch_end()\n\n        epoch_info.result_accumulator.freeze_results()\n        epoch_info.on_epoch_end()\n\n    def train_batch(self, batch_info: BatchInfo) -> None:\n        """"""\n        Batch - the most atomic unit of learning.\n\n        For this reinforforcer, that involves:\n\n        1. Roll out the environmnent using current policy\n        2. Use that rollout to train the policy\n        """"""\n        # Calculate environment rollout on the evaluation version of the model\n        self.model.train()\n\n        rollout = self.env_roller.rollout(batch_info, self.model, self.settings.number_of_steps)\n\n        # Process rollout by the \'algo\' (e.g. perform the advantage estimation)\n        rollout = self.algo.process_rollout(batch_info, rollout)\n\n        # Perform the training step\n        # Algo will aggregate data into this list:\n        batch_info[\'sub_batch_data\'] = []\n\n        if self.settings.shuffle_transitions:\n            rollout = rollout.to_transitions()\n\n        if self.settings.stochastic_experience_replay:\n            # Always play experience at least once\n            experience_replay_count = max(np.random.poisson(self.settings.experience_replay), 1)\n        else:\n            experience_replay_count = self.settings.experience_replay\n\n        # Repeat the experience N times\n        for i in range(experience_replay_count):\n            # We may potentially need to split rollout into multiple batches\n            if self.settings.batch_size >= rollout.frames():\n                batch_result = self.algo.optimizer_step(\n                    batch_info=batch_info,\n                    device=self.device,\n                    model=self.model,\n                    rollout=rollout.to_device(self.device)\n                )\n\n                batch_info[\'sub_batch_data\'].append(batch_result)\n            else:\n                # Rollout too big, need to split in batches\n                for batch_rollout in rollout.shuffled_batches(self.settings.batch_size):\n                    batch_result = self.algo.optimizer_step(\n                        batch_info=batch_info,\n                        device=self.device,\n                        model=self.model,\n                        rollout=batch_rollout.to_device(self.device)\n                    )\n\n                    batch_info[\'sub_batch_data\'].append(batch_result)\n\n        batch_info[\'frames\'] = rollout.frames()\n        batch_info[\'episode_infos\'] = rollout.episode_information()\n\n        # Even with all the experience replay, we count the single rollout as a single batch\n        batch_info.aggregate_key(\'sub_batch_data\')\n\n\nclass OnPolicyIterationReinforcerFactory(ReinforcerFactory):\n    """""" Vel factory class for the PolicyGradientReinforcer """"""\n    def __init__(self, settings, parallel_envs: int, env_factory: VecEnvFactory, model_factory: ModelFactory,\n                 algo: AlgoBase, env_roller_factory: EnvRollerFactoryBase, seed: int):\n        self.settings = settings\n        self.parallel_envs = parallel_envs\n\n        self.env_factory = env_factory\n        self.model_factory = model_factory\n        self.algo = algo\n        self.env_roller_factory = env_roller_factory\n        self.seed = seed\n\n    def instantiate(self, device: torch.device) -> ReinforcerBase:\n        env = self.env_factory.instantiate(parallel_envs=self.parallel_envs, seed=self.seed)\n        env_roller = self.env_roller_factory.instantiate(environment=env, device=device)\n        model = self.model_factory.instantiate(action_space=env.action_space)\n\n        return OnPolicyIterationReinforcer(device, self.settings, model, self.algo, env_roller)\n\n\ndef create(model_config, model, vec_env, algo, env_roller, parallel_envs, number_of_steps,\n           batch_size=256, experience_replay=1, stochastic_experience_replay=False, shuffle_transitions=True):\n    """""" Vel factory function """"""\n    settings = OnPolicyIterationReinforcerSettings(\n        number_of_steps=number_of_steps,\n        batch_size=batch_size,\n        experience_replay=experience_replay,\n        stochastic_experience_replay=stochastic_experience_replay,\n        shuffle_transitions=shuffle_transitions\n    )\n\n    return OnPolicyIterationReinforcerFactory(\n        settings=settings,\n        parallel_envs=parallel_envs,\n        env_factory=vec_env,\n        model_factory=model,\n        algo=algo,\n        env_roller_factory=env_roller,\n        seed=model_config.seed\n    )\n'"
vel/rl/test/__init__.py,0,b''
vel/rl/test/test_integration.py,8,"b'import torch\nimport torch.optim as optim\n\nfrom vel.modules.input.image_to_tensor import ImageToTensorFactory\nfrom vel.modules.input.normalize_observations import NormalizeObservationsFactory\nfrom vel.rl.buffers.circular_replay_buffer import CircularReplayBuffer\nfrom vel.rl.buffers.prioritized_circular_replay_buffer import PrioritizedCircularReplayBuffer\nfrom vel.rl.commands.rl_train_command import FrameTracker\nfrom vel.rl.env_roller.step_env_roller import StepEnvRoller\nfrom vel.rl.env_roller.trajectory_replay_env_roller import TrajectoryReplayEnvRoller\nfrom vel.rl.env_roller.transition_replay_env_roller import TransitionReplayEnvRoller\nfrom vel.rl.metrics import EpisodeRewardMetric\nfrom vel.rl.modules.noise.eps_greedy import EpsGreedy\nfrom vel.rl.modules.noise.ou_noise import OuNoise\nfrom vel.schedules.linear import LinearSchedule\nfrom vel.schedules.linear_and_constant import LinearAndConstantSchedule\nfrom vel.util.random import set_seed\n\nfrom vel.rl.env.classic_atari import ClassicAtariEnv\nfrom vel.rl.env.mujoco import MujocoEnv\nfrom vel.rl.vecenv.subproc import SubprocVecEnvWrapper\nfrom vel.rl.vecenv.dummy import DummyVecEnvWrapper\n\nfrom vel.rl.models.stochastic_policy_model import StochasticPolicyModelFactory\nfrom vel.rl.models.q_stochastic_policy_model import QStochasticPolicyModelFactory\nfrom vel.rl.models.q_model import QModelFactory\nfrom vel.rl.models.deterministic_policy_model import DeterministicPolicyModelFactory\nfrom vel.rl.models.stochastic_policy_model_separate import StochasticPolicyModelSeparateFactory\n\nfrom vel.rl.models.backbone.nature_cnn import NatureCnnFactory\nfrom vel.rl.models.backbone.mlp import MLPFactory\n\nfrom vel.rl.reinforcers.on_policy_iteration_reinforcer import (\n    OnPolicyIterationReinforcer, OnPolicyIterationReinforcerSettings\n)\n\nfrom vel.rl.reinforcers.buffered_off_policy_iteration_reinforcer import (\n    BufferedOffPolicyIterationReinforcer, BufferedOffPolicyIterationReinforcerSettings\n)\n\nfrom vel.rl.reinforcers.buffered_mixed_policy_iteration_reinforcer import (\n    BufferedMixedPolicyIterationReinforcer, BufferedMixedPolicyIterationReinforcerSettings\n)\n\nfrom vel.rl.algo.dqn import DeepQLearning\nfrom vel.rl.algo.policy_gradient.a2c import A2CPolicyGradient\nfrom vel.rl.algo.policy_gradient.ppo import PpoPolicyGradient\nfrom vel.rl.algo.policy_gradient.trpo import TrpoPolicyGradient\nfrom vel.rl.algo.policy_gradient.acer import AcerPolicyGradient\nfrom vel.rl.algo.policy_gradient.ddpg import DeepDeterministicPolicyGradient\n\nfrom vel.api.info import TrainingInfo, EpochInfo\n\n\nCPU_DEVICE = torch.device(\'cpu\')\n\n\ndef test_a2c_breakout():\n    """"""\n    Simple 1 iteration of a2c breakout\n    """"""\n    seed = 1001\n\n    # Set random seed in python std lib, numpy and pytorch\n    set_seed(seed)\n\n    # Create 16 environments evaluated in parallel in sub processess with all usual DeepMind wrappers\n    # These are just helper functions for that\n    vec_env = SubprocVecEnvWrapper(\n        ClassicAtariEnv(\'BreakoutNoFrameskip-v4\'), frame_history=4\n    ).instantiate(parallel_envs=16, seed=seed)\n\n    # Again, use a helper to create a model\n    # But because model is owned by the reinforcer, model should not be accessed using this variable\n    # but from reinforcer.model property\n    model = StochasticPolicyModelFactory(\n        input_block=ImageToTensorFactory(),\n        backbone=NatureCnnFactory(input_width=84, input_height=84, input_channels=4)\n    ).instantiate(action_space=vec_env.action_space)\n\n    # Reinforcer - an object managing the learning process\n    reinforcer = OnPolicyIterationReinforcer(\n        device=CPU_DEVICE,\n        settings=OnPolicyIterationReinforcerSettings(\n            batch_size=256,\n            number_of_steps=5\n        ),\n        model=model,\n        algo=A2CPolicyGradient(\n            entropy_coefficient=0.01,\n            value_coefficient=0.5,\n            discount_factor=0.99,\n            max_grad_norm=0.5\n        ),\n        env_roller=StepEnvRoller(\n            environment=vec_env,\n            device=CPU_DEVICE\n        )\n    )\n\n    # Model optimizer\n    optimizer = optim.RMSprop(reinforcer.model.parameters(), lr=7.0e-4, eps=1e-3)\n\n    # Overall information store for training information\n    training_info = TrainingInfo(\n        metrics=[\n            EpisodeRewardMetric(\'episode_rewards\'),  # Calculate average reward from episode\n        ],\n        callbacks=[]  # Print live metrics every epoch to standard output\n    )\n\n    # A bit of training initialization bookkeeping...\n    training_info.initialize()\n    reinforcer.initialize_training(training_info)\n    training_info.on_train_begin()\n\n    # Let\'s make 100 batches per epoch to average metrics nicely\n    num_epochs = 1\n\n    # Normal handrolled training loop\n    for i in range(1, num_epochs+1):\n        epoch_info = EpochInfo(\n            training_info=training_info,\n            global_epoch_idx=i,\n            batches_per_epoch=1,\n            optimizer=optimizer\n        )\n\n        reinforcer.train_epoch(epoch_info, interactive=False)\n\n    training_info.on_train_end()\n\n\ndef test_ppo_breakout():\n    """"""\n    Simple 1 iteration of ppo breakout\n    """"""\n    device = torch.device(\'cpu\')\n    seed = 1001\n\n    # Set random seed in python std lib, numpy and pytorch\n    set_seed(seed)\n\n    # Create 16 environments evaluated in parallel in sub processess with all usual DeepMind wrappers\n    # These are just helper functions for that\n    vec_env = SubprocVecEnvWrapper(\n        ClassicAtariEnv(\'BreakoutNoFrameskip-v4\'), frame_history=4\n    ).instantiate(parallel_envs=8, seed=seed)\n\n    # Again, use a helper to create a model\n    # But because model is owned by the reinforcer, model should not be accessed using this variable\n    # but from reinforcer.model property\n    model = StochasticPolicyModelFactory(\n        input_block=ImageToTensorFactory(),\n        backbone=NatureCnnFactory(input_width=84, input_height=84, input_channels=4)\n    ).instantiate(action_space=vec_env.action_space)\n\n    # Reinforcer - an object managing the learning process\n    reinforcer = OnPolicyIterationReinforcer(\n        device=device,\n        settings=OnPolicyIterationReinforcerSettings(\n            number_of_steps=12,\n            batch_size=4,\n            experience_replay=2,\n        ),\n        model=model,\n        algo=PpoPolicyGradient(\n            entropy_coefficient=0.01,\n            value_coefficient=0.5,\n            max_grad_norm=0.5,\n            cliprange=LinearSchedule(0.1, 0.0),\n            discount_factor=0.99,\n            normalize_advantage=True\n        ),\n        env_roller=StepEnvRoller(\n            environment=vec_env,\n            device=device,\n        )\n    )\n\n    # Model optimizer\n    # optimizer = optim.RMSprop(reinforcer.model.parameters(), lr=7.0e-4, eps=1e-3)\n    optimizer = optim.Adam(reinforcer.model.parameters(), lr=2.5e-4, eps=1e-5)\n\n    # Overall information store for training information\n    training_info = TrainingInfo(\n        metrics=[\n            EpisodeRewardMetric(\'episode_rewards\'),  # Calculate average reward from episode\n        ],\n        callbacks=[\n            FrameTracker(100_000)\n        ]  # Print live metrics every epoch to standard output\n    )\n\n    # A bit of training initialization bookkeeping...\n    training_info.initialize()\n    reinforcer.initialize_training(training_info)\n    training_info.on_train_begin()\n\n    # Let\'s make 100 batches per epoch to average metrics nicely\n    num_epochs = 1\n\n    # Normal handrolled training loop\n    for i in range(1, num_epochs+1):\n        epoch_info = EpochInfo(\n            training_info=training_info,\n            global_epoch_idx=i,\n            batches_per_epoch=1,\n            optimizer=optimizer\n        )\n\n        reinforcer.train_epoch(epoch_info, interactive=False)\n\n    training_info.on_train_end()\n\n\ndef test_dqn_breakout():\n    """"""\n    Simple 1 iteration of DQN breakout\n    """"""\n    device = torch.device(\'cpu\')\n    seed = 1001\n\n    # Set random seed in python std lib, numpy and pytorch\n    set_seed(seed)\n\n    # Only single environment for DQN\n    vec_env = DummyVecEnvWrapper(\n        ClassicAtariEnv(\'BreakoutNoFrameskip-v4\'), frame_history=4\n    ).instantiate(parallel_envs=1, seed=seed)\n\n    # Again, use a helper to create a model\n    # But because model is owned by the reinforcer, model should not be accessed using this variable\n    # but from reinforcer.model property\n    model_factory = QModelFactory(\n        input_block=ImageToTensorFactory(),\n        backbone=NatureCnnFactory(input_width=84, input_height=84, input_channels=4)\n    )\n\n    # Reinforcer - an object managing the learning process\n    reinforcer = BufferedOffPolicyIterationReinforcer(\n        device=device,\n        settings=BufferedOffPolicyIterationReinforcerSettings(\n            rollout_steps=4,\n            training_steps=1,\n        ),\n        environment=vec_env,\n        algo=DeepQLearning(\n            model_factory=model_factory,\n            double_dqn=False,\n            target_update_frequency=10_000,\n            discount_factor=0.99,\n            max_grad_norm=0.5\n        ),\n        model=model_factory.instantiate(action_space=vec_env.action_space),\n        env_roller=TransitionReplayEnvRoller(\n            environment=vec_env,\n            device=device,\n            replay_buffer=CircularReplayBuffer(\n                buffer_capacity=100,\n                buffer_initial_size=100,\n                num_envs=vec_env.num_envs,\n                observation_space=vec_env.observation_space,\n                action_space=vec_env.action_space,\n                frame_stack_compensation=True,\n                frame_history=4\n            ),\n            action_noise=EpsGreedy(\n                epsilon=LinearAndConstantSchedule(\n                    initial_value=1.0, final_value=0.1, end_of_interpolation=0.1\n                ),\n                environment=vec_env\n            )\n        )\n    )\n\n    # Model optimizer\n    optimizer = optim.RMSprop(reinforcer.model.parameters(), lr=2.5e-4, alpha=0.95, momentum=0.95, eps=1e-3)\n\n    # Overall information store for training information\n    training_info = TrainingInfo(\n        metrics=[\n            EpisodeRewardMetric(\'episode_rewards\'),  # Calculate average reward from episode\n        ],\n        callbacks=[\n            FrameTracker(100_000)\n        ]  # Print live metrics every epoch to standard output\n    )\n\n    # A bit of training initialization bookkeeping...\n    training_info.initialize()\n    reinforcer.initialize_training(training_info)\n    training_info.on_train_begin()\n\n    # Let\'s make 100 batches per epoch to average metrics nicely\n    num_epochs = 1\n\n    # Normal handrolled training loop\n    for i in range(1, num_epochs+1):\n        epoch_info = EpochInfo(\n            training_info=training_info,\n            global_epoch_idx=i,\n            batches_per_epoch=1,\n            optimizer=optimizer\n        )\n\n        reinforcer.train_epoch(epoch_info, interactive=False)\n\n    training_info.on_train_end()\n\n\ndef test_prioritized_dqn_breakout():\n    """"""\n    Simple 1 iteration of DQN prioritized replay breakout\n    """"""\n    device = torch.device(\'cpu\')\n    seed = 1001\n\n    # Set random seed in python std lib, numpy and pytorch\n    set_seed(seed)\n\n    # Only single environment for DQN\n    vec_env = DummyVecEnvWrapper(\n        ClassicAtariEnv(\'BreakoutNoFrameskip-v4\'), frame_history=4\n    ).instantiate(parallel_envs=1, seed=seed)\n\n    # Again, use a helper to create a model\n    # But because model is owned by the reinforcer, model should not be accessed using this variable\n    # but from reinforcer.model property\n    model_factory = QModelFactory(\n        input_block=ImageToTensorFactory(),\n        backbone=NatureCnnFactory(input_width=84, input_height=84, input_channels=4)\n    )\n\n    # Reinforcer - an object managing the learning process\n    reinforcer = BufferedOffPolicyIterationReinforcer(\n        device=device,\n        settings=BufferedOffPolicyIterationReinforcerSettings(\n            rollout_steps=4,\n            training_steps=1,\n        ),\n        environment=vec_env,\n        algo=DeepQLearning(\n            model_factory=model_factory,\n            double_dqn=False,\n            target_update_frequency=10_000,\n            discount_factor=0.99,\n            max_grad_norm=0.5\n        ),\n        model=model_factory.instantiate(action_space=vec_env.action_space),\n        env_roller=TransitionReplayEnvRoller(\n            environment=vec_env,\n            device=device,\n            replay_buffer=PrioritizedCircularReplayBuffer(\n                buffer_capacity=100,\n                buffer_initial_size=100,\n                num_envs=vec_env.num_envs,\n                observation_space=vec_env.observation_space,\n                action_space=vec_env.action_space,\n                priority_exponent=0.6,\n                priority_weight=LinearSchedule(\n                    initial_value=0.4,\n                    final_value=1.0\n                ),\n                priority_epsilon=1.0e-6,\n                frame_stack_compensation=True,\n                frame_history=4\n            ),\n            action_noise=EpsGreedy(\n                epsilon=LinearAndConstantSchedule(\n                    initial_value=1.0, final_value=0.1, end_of_interpolation=0.1\n                ),\n                environment=vec_env\n            )\n        )\n    )\n\n    # Model optimizer\n    optimizer = optim.RMSprop(reinforcer.model.parameters(), lr=2.5e-4, alpha=0.95, momentum=0.95, eps=1e-3)\n\n    # Overall information store for training information\n    training_info = TrainingInfo(\n        metrics=[\n            EpisodeRewardMetric(\'episode_rewards\'),  # Calculate average reward from episode\n        ],\n        callbacks=[\n            FrameTracker(100_000)\n        ]  # Print live metrics every epoch to standard output\n    )\n\n    # A bit of training initialization bookkeeping...\n    training_info.initialize()\n    reinforcer.initialize_training(training_info)\n    training_info.on_train_begin()\n\n    # Let\'s make 100 batches per epoch to average metrics nicely\n    num_epochs = 1\n\n    # Normal handrolled training loop\n    for i in range(1, num_epochs+1):\n        epoch_info = EpochInfo(\n            training_info=training_info,\n            global_epoch_idx=i,\n            batches_per_epoch=1,\n            optimizer=optimizer\n        )\n\n        reinforcer.train_epoch(epoch_info, interactive=False)\n\n    training_info.on_train_end()\n\n\ndef test_ddpg_bipedal_walker():\n    """"""\n    1 iteration of DDPG bipedal walker environment\n    """"""\n    device = torch.device(\'cpu\')\n    seed = 1001\n\n    # Set random seed in python std lib, numpy and pytorch\n    set_seed(seed)\n\n    # Only single environment for DDPG\n\n    vec_env = DummyVecEnvWrapper(\n        MujocoEnv(\'BipedalWalker-v2\')\n    ).instantiate(parallel_envs=1, seed=seed)\n\n    # Again, use a helper to create a model\n    # But because model is owned by the reinforcer, model should not be accessed using this variable\n    # but from reinforcer.model property\n    model_factory = DeterministicPolicyModelFactory(\n        input_block=NormalizeObservationsFactory(input_shape=24),\n        policy_backbone=MLPFactory(input_length=24, hidden_layers=[64, 64], normalization=\'layer\'),\n        value_backbone=MLPFactory(input_length=28, hidden_layers=[64, 64], normalization=\'layer\')\n    )\n\n    # Reinforcer - an object managing the learning process\n    reinforcer = BufferedOffPolicyIterationReinforcer(\n        device=device,\n        settings=BufferedOffPolicyIterationReinforcerSettings(\n            rollout_steps=4,\n            training_steps=1,\n        ),\n        environment=vec_env,\n        algo=DeepDeterministicPolicyGradient(\n            model_factory=model_factory,\n            tau=0.01,\n            discount_factor=0.99,\n            max_grad_norm=0.5\n        ),\n        model=model_factory.instantiate(action_space=vec_env.action_space),\n        env_roller=TransitionReplayEnvRoller(\n            environment=vec_env,\n            device=device,\n            action_noise=OuNoise(std_dev=0.2, environment=vec_env),\n            replay_buffer=CircularReplayBuffer(\n                buffer_capacity=100,\n                buffer_initial_size=100,\n                num_envs=vec_env.num_envs,\n                observation_space=vec_env.observation_space,\n                action_space=vec_env.action_space\n            ),\n            normalize_returns=True,\n            discount_factor=0.99\n        ),\n    )\n\n    # Model optimizer\n    optimizer = optim.Adam(reinforcer.model.parameters(), lr=2.5e-4, eps=1e-4)\n\n    # Overall information store for training information\n    training_info = TrainingInfo(\n        metrics=[\n            EpisodeRewardMetric(\'episode_rewards\'),  # Calculate average reward from episode\n        ],\n        callbacks=[\n            FrameTracker(100_000)\n        ]  # Print live metrics every epoch to standard output\n    )\n\n    # A bit of training initialization bookkeeping...\n    training_info.initialize()\n    reinforcer.initialize_training(training_info)\n    training_info.on_train_begin()\n\n    # Let\'s make 100 batches per epoch to average metrics nicely\n    num_epochs = 1\n\n    # Normal handrolled training loop\n    for i in range(1, num_epochs+1):\n        epoch_info = EpochInfo(\n            training_info=training_info,\n            global_epoch_idx=i,\n            batches_per_epoch=1,\n            optimizer=optimizer\n        )\n\n        reinforcer.train_epoch(epoch_info, interactive=False)\n\n    training_info.on_train_end()\n\n\ndef test_trpo_bipedal_walker():\n    """"""\n    1 iteration of TRPO on bipedal walker\n    """"""\n    device = torch.device(\'cpu\')\n    seed = 1001\n\n    # Set random seed in python std lib, numpy and pytorch\n    set_seed(seed)\n\n    vec_env = DummyVecEnvWrapper(\n        MujocoEnv(\'BipedalWalker-v2\', normalize_returns=True),\n    ).instantiate(parallel_envs=8, seed=seed)\n\n    # Again, use a helper to create a model\n    # But because model is owned by the reinforcer, model should not be accessed using this variable\n    # but from reinforcer.model property\n    model_factory = StochasticPolicyModelSeparateFactory(\n        input_block=NormalizeObservationsFactory(input_shape=24),\n        policy_backbone=MLPFactory(input_length=24, hidden_layers=[32, 32]),\n        value_backbone=MLPFactory(input_length=24, hidden_layers=[32])\n    )\n\n    # Reinforcer - an object managing the learning process\n    reinforcer = OnPolicyIterationReinforcer(\n        device=device,\n        settings=OnPolicyIterationReinforcerSettings(\n            number_of_steps=12,\n        ),\n        model=model_factory.instantiate(action_space=vec_env.action_space),\n        algo=TrpoPolicyGradient(\n            max_kl=0.01,\n            cg_iters=10,\n            line_search_iters=10,\n            improvement_acceptance_ratio=0.1,\n            cg_damping=0.1,\n            vf_iters=5,\n            entropy_coef=0.0,\n            discount_factor=0.99,\n            max_grad_norm=0.5,\n            gae_lambda=1.0\n        ),\n        env_roller=StepEnvRoller(\n            environment=vec_env,\n            device=device,\n        )\n    )\n\n    # Model optimizer\n    optimizer = optim.Adam(reinforcer.model.parameters(), lr=1.0e-3, eps=1e-4)\n\n    # Overall information store for training information\n    training_info = TrainingInfo(\n        metrics=[\n            EpisodeRewardMetric(\'episode_rewards\'),  # Calculate average reward from episode\n        ],\n        callbacks=[\n            FrameTracker(100_000)\n        ]  # Print live metrics every epoch to standard output\n    )\n\n    # A bit of training initialization bookkeeping...\n    training_info.initialize()\n    reinforcer.initialize_training(training_info)\n    training_info.on_train_begin()\n\n    # Let\'s make 100 batches per epoch to average metrics nicely\n    num_epochs = 1\n\n    # Normal handrolled training loop\n    for i in range(1, num_epochs+1):\n        epoch_info = EpochInfo(\n            training_info=training_info,\n            global_epoch_idx=i,\n            batches_per_epoch=1,\n            optimizer=optimizer\n        )\n\n        reinforcer.train_epoch(epoch_info, interactive=False)\n\n    training_info.on_train_end()\n\n\ndef test_acer_breakout():\n    """"""\n    1 iteration of ACER on breakout environment\n    """"""\n    device = torch.device(\'cpu\')\n    seed = 1001\n\n    # Set random seed in python std lib, numpy and pytorch\n    set_seed(seed)\n\n    # Create 16 environments evaluated in parallel in sub processess with all usual DeepMind wrappers\n    # These are just helper functions for that\n    vec_env = SubprocVecEnvWrapper(\n        ClassicAtariEnv(\'BreakoutNoFrameskip-v4\'), frame_history=4\n    ).instantiate(parallel_envs=16, seed=seed)\n\n    # Again, use a helper to create a model\n    # But because model is owned by the reinforcer, model should not be accessed using this variable\n    # but from reinforcer.model property\n    model_factory = QStochasticPolicyModelFactory(\n        input_block=ImageToTensorFactory(),\n        backbone=NatureCnnFactory(input_width=84, input_height=84, input_channels=4)\n    )\n\n    # Reinforcer - an object managing the learning process\n    reinforcer = BufferedMixedPolicyIterationReinforcer(\n        device=device,\n        settings=BufferedMixedPolicyIterationReinforcerSettings(\n            experience_replay=2,\n            number_of_steps=12,\n            stochastic_experience_replay=False\n        ),\n        model=model_factory.instantiate(action_space=vec_env.action_space),\n        env=vec_env,\n        algo=AcerPolicyGradient(\n            model_factory=model_factory,\n            entropy_coefficient=0.01,\n            q_coefficient=0.5,\n            rho_cap=10.0,\n            retrace_rho_cap=1.0,\n            trust_region=True,\n            trust_region_delta=1.0,\n            discount_factor=0.99,\n            max_grad_norm=10.0,\n        ),\n        env_roller=TrajectoryReplayEnvRoller(\n            environment=vec_env,\n            device=device,\n            replay_buffer=CircularReplayBuffer(\n                buffer_capacity=100,\n                buffer_initial_size=100,\n                num_envs=vec_env.num_envs,\n                action_space=vec_env.action_space,\n                observation_space=vec_env.observation_space,\n                frame_stack_compensation=True,\n                frame_history=4,\n            )\n        ),\n    )\n\n    # Model optimizer\n    optimizer = optim.RMSprop(reinforcer.model.parameters(), lr=7.0e-4, eps=1e-3, alpha=0.99)\n\n    # Overall information store for training information\n    training_info = TrainingInfo(\n        metrics=[\n            EpisodeRewardMetric(\'episode_rewards\'),  # Calculate average reward from episode\n        ],\n        callbacks=[]  # Print live metrics every epoch to standard output\n    )\n\n    # A bit of training initialization bookkeeping...\n    training_info.initialize()\n    reinforcer.initialize_training(training_info)\n    training_info.on_train_begin()\n\n    # Let\'s make 100 batches per epoch to average metrics nicely\n    num_epochs = 1\n\n    # Normal handrolled training loop\n    for i in range(1, num_epochs+1):\n        epoch_info = EpochInfo(\n            training_info=training_info,\n            global_epoch_idx=i,\n            batches_per_epoch=1,\n            optimizer=optimizer\n        )\n\n        reinforcer.train_epoch(epoch_info, interactive=False)\n\n    training_info.on_train_end()\n'"
vel/rl/vecenv/__init__.py,0,b''
vel/rl/vecenv/dummy.py,0,"b'from vel.openai.baselines.common.vec_env import VecEnv\nfrom vel.openai.baselines.common.atari_wrappers import FrameStack\nfrom vel.openai.baselines.common.vec_env.dummy_vec_env import DummyVecEnv\nfrom vel.openai.baselines.common.vec_env.vec_frame_stack import VecFrameStack\n\nfrom vel.rl.api import VecEnvFactory\n\n\nclass DummyVecEnvWrapper(VecEnvFactory):\n    """""" Wraps a single-threaded environment into a one-element vector environment """"""\n\n    def __init__(self, env, frame_history=None):\n        self.env = env\n        self.frame_history = frame_history\n\n    def instantiate(self, parallel_envs, seed=0, preset=\'default\') -> VecEnv:\n        """""" Create vectorized environments """"""\n        envs = DummyVecEnv([self._creation_function(i, seed, preset) for i in range(parallel_envs)])\n\n        if self.frame_history is not None:\n            envs = VecFrameStack(envs, self.frame_history)\n\n        return envs\n\n    def instantiate_single(self, seed=0, preset=\'default\'):\n        """""" Create a new Env instance - single """"""\n        env = self.env.instantiate(seed=seed, serial_id=0, preset=preset)\n\n        if self.frame_history is not None:\n            env = FrameStack(env, self.frame_history)\n\n        return env\n\n    def _creation_function(self, idx, seed, preset):\n        """""" Helper function to create a proper closure around supplied values """"""\n        return lambda: self.env.instantiate(seed=seed, serial_id=idx, preset=preset)\n\n\ndef create(env, frame_history=None):\n    """""" Vel factory function """"""\n    return DummyVecEnvWrapper(env, frame_history=frame_history)\n'"
vel/rl/vecenv/shared_mem.py,0,"b'from vel.openai.baselines.common.vec_env import VecEnv\nfrom vel.openai.baselines.common.atari_wrappers import FrameStack\nfrom vel.openai.baselines.common.vec_env.shmem_vec_env import ShmemVecEnv\nfrom vel.openai.baselines.common.vec_env.vec_frame_stack import VecFrameStack\n\nfrom vel.rl.api import VecEnvFactory\n\n\nclass SharedMemVecEnvWrapper(VecEnvFactory):\n    """"""\n    Optimized version of SubprocVecEnv that uses shared variables to communicate observations.\n    """"""\n\n    def __init__(self, env, frame_history=None):\n        self.env = env\n        self.frame_history = frame_history\n\n    def instantiate(self, parallel_envs, seed=0, preset=\'default\') -> VecEnv:\n        """""" Create vectorized environments """"""\n        envs = ShmemVecEnv([self._creation_function(i, seed, preset) for i in range(parallel_envs)])\n\n        if self.frame_history is not None:\n            envs = VecFrameStack(envs, self.frame_history)\n\n        return envs\n\n    def instantiate_single(self, seed=0, preset=\'default\'):\n        """""" Create a new Env instance - single """"""\n        env = self.env.instantiate(seed=seed, serial_id=0, preset=preset)\n\n        if self.frame_history is not None:\n            env = FrameStack(env, self.frame_history)\n\n        return env\n\n    def _creation_function(self, idx, seed, preset):\n        """""" Helper function to create a proper closure around supplied values """"""\n        return lambda: self.env.instantiate(seed=seed, serial_id=idx, preset=preset)\n\n\ndef create(env, frame_history=None):\n    """""" Vel factory function """"""\n    return SharedMemVecEnvWrapper(env, frame_history=frame_history)\n'"
vel/rl/vecenv/subproc.py,0,"b'from vel.openai.baselines.common.vec_env import VecEnv\nfrom vel.openai.baselines.common.atari_wrappers import FrameStack\nfrom vel.openai.baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\nfrom vel.openai.baselines.common.vec_env.vec_frame_stack import VecFrameStack\n\nfrom vel.rl.api import VecEnvFactory\n\n\nclass SubprocVecEnvWrapper(VecEnvFactory):\n    """""" Wrapper for an environment to create sub-process vector environment """"""\n\n    def __init__(self, env, frame_history=None):\n        self.env = env\n        self.frame_history = frame_history\n\n    def instantiate(self, parallel_envs, seed=0, preset=\'default\') -> VecEnv:\n        """""" Create vectorized environments """"""\n        envs = SubprocVecEnv([self._creation_function(i, seed, preset) for i in range(parallel_envs)])\n\n        if self.frame_history is not None:\n            envs = VecFrameStack(envs, self.frame_history)\n\n        return envs\n\n    def instantiate_single(self, seed=0, preset=\'default\'):\n        """""" Create a new Env instance - single """"""\n        env = self.env.instantiate(seed=seed, serial_id=0, preset=preset)\n\n        if self.frame_history is not None:\n            env = FrameStack(env, self.frame_history)\n\n        return env\n\n    def _creation_function(self, idx, seed, preset):\n        """""" Helper function to create a proper closure around supplied values """"""\n        return lambda: self.env.instantiate(seed=seed, serial_id=idx, preset=preset)\n\n\ndef create(env, frame_history=None):\n    """""" Vel factory function """"""\n    return SubprocVecEnvWrapper(env, frame_history=frame_history)\n'"
vel/sources/nlp/__init__.py,0,b''
vel/sources/nlp/imdb.py,0,"b'import os\nimport glob\nimport io\nimport pickle\n\nimport torchtext.datasets.imdb as imdb\nimport torchtext.data as data\n\n\nfrom vel.api import TextData\n\n\nclass IMDBCached(imdb.IMDB):\n    """""" Cached version of the IMDB dataset (to save time on tokenization) """"""\n\n    def __init__(self, path, text_field, label_field, **kwargs):\n        """"""Create an IMDB dataset instance given a path and fields.\n\n        Arguments:\n            path: Path to the dataset\'s highest level directory\n            text_field: The field that will be used for text data.\n            label_field: The field that will be used for label data.\n            Remaining keyword arguments: Passed to the constructor of\n                data.Dataset.\n        """"""\n        cache_file = os.path.join(path, \'examples_cache.pk\')\n\n        fields = [(\'text\', text_field), (\'label\', label_field)]\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fp:\n                examples = pickle.load(fp)\n        else:\n            examples = []\n\n            for label in [\'pos\', \'neg\']:\n                for fname in glob.iglob(os.path.join(path, label, \'*.txt\')):\n                    with io.open(fname, \'r\', encoding=""utf-8"") as f:\n                        text = f.readline()\n                    examples.append(data.Example.fromlist([text, label], fields))\n\n            with open(cache_file, \'wb\') as fp:\n                pickle.dump(examples, file=fp)\n\n        data.Dataset.__init__(self, examples, fields, **kwargs)\n\n\ndef create(model_config, batch_size, vectors=None):\n    """""" Create an IMDB dataset """"""\n    path = model_config.data_dir(\'imdb\')\n\n    text_field = data.Field(lower=True, tokenize=\'spacy\', batch_first=True)\n    label_field = data.LabelField(is_target=True)\n\n    train_source, test_source = IMDBCached.splits(\n        root=path,\n        text_field=text_field,\n        label_field=label_field\n    )\n\n    text_field.build_vocab(train_source, max_size=25_000, vectors=vectors)\n    label_field.build_vocab(train_source)\n\n    train_iterator, test_iterator = data.BucketIterator.splits(\n        (train_source, test_source),\n        batch_size=batch_size,\n        device=model_config.torch_device(),\n        shuffle=True\n    )\n\n    return TextData(\n        train_source, test_source, train_iterator, test_iterator, text_field, label_field\n    )\n'"
vel/sources/nlp/text_url.py,5,"b'import certifi\nimport numpy as np\nimport os\nimport pathlib\nimport urllib3\n\nimport torch\n\nfrom vel.api import Source\n\n\nclass TextIterator:\n    """""" Iterator over a text dataset """"""\n    def __init__(self, padded_sequence, sequence_length, batch_size, alphabet_size, num_batches):\n        self.sequence_length = sequence_length\n        self.batch_size = batch_size\n        self.num_batches = num_batches\n        self.alphabet_size = alphabet_size\n\n        self.padded_sequence = padded_sequence[:-1].reshape(self.num_batches * self.batch_size, self.sequence_length)\n        self.padded_sequence_next = padded_sequence[1:].reshape(self.num_batches * self.batch_size, self.sequence_length)\n\n        self.sequence_indices = np.arange(self.num_batches * self.batch_size)\n\n        np.random.shuffle(self.sequence_indices)\n\n        self.sequence_indices = self.sequence_indices.reshape(self.num_batches, self.batch_size)\n\n        self.batch_idx = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.batch_idx == self.num_batches:\n            raise StopIteration\n        else:\n            input_data = torch.from_numpy(self.padded_sequence[self.sequence_indices[self.batch_idx]])\n            target_data = torch.from_numpy(self.padded_sequence_next[self.sequence_indices[self.batch_idx]])\n\n            self.batch_idx += 1\n\n            return input_data.to(torch.long), target_data.to(torch.long)\n\n\nclass TextLoader:\n    """""" Loader of sequential text data """"""\n    def __init__(self, sequence, sequence_length, batch_size, alphabet_size):\n        self.sequence = sequence\n        self.sequence_length = sequence_length\n        self.batch_size = batch_size\n        self.alphabet_size = alphabet_size\n\n        # 1 is for the last element as the target needs to be shifted by 1\n        residual_length = (len(self.sequence) - self.sequence_length - 1)\n        full_size = self.sequence_length * self.batch_size\n\n        rest = residual_length % full_size\n        self.num_batches = residual_length // full_size\n\n        if rest > 0:\n            self.sequence = np.pad(self.sequence, (0, full_size - rest), mode=\'constant\')\n            self.num_batches += 1\n\n    def __iter__(self):\n        initial_offset = np.random.randint(self.sequence_length)\n        relevant_subsequence = self.sequence[\n           # 1 is for the last element as the target needs to be shifted by 1\n           initial_offset:self.num_batches * self.sequence_length * self.batch_size + initial_offset + 1\n        ]\n\n        return TextIterator(\n            relevant_subsequence, self.sequence_length, self.batch_size,\n            alphabet_size=self.alphabet_size,\n            num_batches=self.num_batches\n        )\n\n    def __len__(self):\n        """""" Number of batches in this loader """"""\n        return self.num_batches\n\n\nclass TextUrlSource(Source):\n    """""" Download text from source and model it character by character """"""\n    def __init__(self, url, absolute_data_path, sequence_length, batch_size, train_val_split=0.8):\n        super().__init__()\n\n        self.url = url\n        self.data_path = absolute_data_path\n        self.sequence_length = sequence_length\n        self.batch_size = batch_size\n        self.train_val_split = train_val_split\n\n        self.text_path = os.path.join(self.data_path, \'text.txt\')\n        self.processed_path = os.path.join(self.data_path, \'processed.data\')\n\n        self.data_dict = self.download()\n\n        content_encoded = self.data_dict[\'content_encoded\']\n        alphabet_size = len(self.data_dict[\'alphabet\'])\n\n        split_idx = int(len(content_encoded) * train_val_split)\n\n        self._train_loader = TextLoader(\n            sequence=content_encoded[:split_idx],\n            sequence_length=sequence_length,\n            batch_size=batch_size,\n            alphabet_size=alphabet_size,\n        )\n\n        self._val_loader = TextLoader(\n            sequence=content_encoded[split_idx:],\n            sequence_length=sequence_length,\n            batch_size=batch_size,\n            alphabet_size=alphabet_size,\n        )\n\n    def encode_character(self, char):\n        return self.data_dict[\'character_to_index\'][char]\n\n    def decode_character(self, index):\n        return self.data_dict[\'index_to_character\'][index]\n\n    def train_loader(self):\n        """""" PyTorch loader of training data """"""\n        return self._train_loader\n\n    def val_loader(self):\n        """""" PyTorch loader of validation data """"""\n        return self._val_loader\n\n    def train_dataset(self):\n        """""" Return the training dataset """"""\n        return None\n\n    def val_dataset(self):\n        """""" Return the validation dataset """"""\n        return None\n\n    def train_iterations_per_epoch(self):\n        """""" Return number of iterations per epoch """"""\n        return len(self._train_loader)\n\n    def val_iterations_per_epoch(self):\n        """""" Return number of iterations per epoch - validation """"""\n        return len(self._val_loader)\n\n    def download(self):\n        """""" Make sure data file is downloaded and stored properly """"""\n        if not os.path.exists(self.data_path):\n            # Create if it doesn\'t exist\n            pathlib.Path(self.data_path).mkdir(parents=True, exist_ok=True)\n\n        if not os.path.exists(self.text_path):\n            http = urllib3.PoolManager(cert_reqs=\'CERT_REQUIRED\', ca_certs=certifi.where())\n\n            with open(self.text_path, \'wt\') as fp:\n                request = http.request(\'GET\', self.url)\n                content = request.data.decode(\'utf8\')\n                fp.write(content)\n\n        if not os.path.exists(self.processed_path):\n            with open(self.text_path, \'rt\') as fp:\n                content = fp.read()\n\n            alphabet = sorted(set(content))\n\n            index_to_character = {idx: c for idx, c in enumerate(alphabet, 1)}\n            character_to_index = {c: idx for idx, c in enumerate(alphabet, 1)}\n\n            content_encoded = np.array([character_to_index[c] for c in content], dtype=np.uint8)\n\n            data_dict = {\n                \'alphabet\': alphabet,\n                \'index_to_character\': index_to_character,\n                \'character_to_index\': character_to_index,\n                \'content_encoded\': content_encoded\n            }\n\n            with open(self.processed_path, \'wb\') as fp:\n                torch.save(data_dict, fp)\n        else:\n            with open(self.processed_path, \'rb\') as fp:\n                data_dict = torch.load(fp)\n\n        return data_dict\n\n\ndef create(model_config, url, local_dir, sequence_length=64, batch_size=64, train_val_split=0.8):\n    """""" Vel factory function """"""\n    if not os.path.isabs(local_dir):\n        local_dir = model_config.project_data_dir(local_dir)\n\n    return TextUrlSource(\n        url,\n        absolute_data_path=local_dir,\n        sequence_length=sequence_length,\n        batch_size=batch_size,\n        train_val_split=train_val_split,\n    )\n'"
vel/sources/vision/__init__.py,0,b''
vel/sources/vision/cifar10.py,0,"b'from torchvision import datasets\n\nfrom vel.api import TrainingData\n\nfrom vel.augmentations.normalize import Normalize\nfrom vel.augmentations.to_tensor import ToTensor\nfrom vel.augmentations.to_array import ToArray\n\n\ndef create(model_config, batch_size, normalize=True, num_workers=0, augmentations=None):\n    """"""\n    Create a CIFAR10 dataset, normalized.\n    Augmentations are the same as in the literature benchmarking CIFAR performance.\n    """"""\n    path = model_config.data_dir(\'cifar10\')\n\n    train_dataset = datasets.CIFAR10(path, train=True, download=True)\n    test_dataset = datasets.CIFAR10(path, train=False, download=True)\n\n    augmentations = [ToArray()] + (augmentations if augmentations is not None else [])\n    \n    if normalize:\n        train_data = train_dataset.train_data\n        mean_value = (train_data / 255).mean(axis=(0, 1, 2))\n        std_value = (train_data / 255).std(axis=(0, 1, 2))\n\n        augmentations.append(Normalize(mean=mean_value, std=std_value, tags=[\'train\', \'val\']))\n\n    augmentations.append(ToTensor())\n\n    return TrainingData(\n        train_dataset,\n        test_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        augmentations=augmentations\n    )\n'"
vel/sources/vision/mnist.py,0,"b'from torchvision import datasets\n\n\nfrom vel.api import TrainingData\n\nfrom vel.augmentations.normalize import Normalize\nfrom vel.augmentations.to_tensor import ToTensor\nfrom vel.augmentations.to_array import ToArray\n\n\ndef create(model_config, batch_size, normalize=True, num_workers=0, augmentations=None):\n    """""" Create a MNIST dataset, normalized """"""\n    path = model_config.data_dir(\'mnist\')\n\n    train_dataset = datasets.MNIST(path, train=True, download=True)\n    test_dataset = datasets.MNIST(path, train=False, download=True)\n\n    augmentations = [ToArray()] + (augmentations if augmentations is not None else [])\n\n    if normalize:\n        train_data = train_dataset.train_data\n        mean_value = (train_data.double() / 255).mean().item()\n        std_value = (train_data.double() / 255).std().item()\n\n        augmentations.append(Normalize(mean=mean_value, std=std_value, tags=[\'train\', \'val\']))\n\n    augmentations.append(ToTensor())\n\n    return TrainingData(\n        train_dataset,\n        test_dataset,\n        num_workers=num_workers,\n        batch_size=batch_size,\n        augmentations=augmentations\n    )\n'"
vel/storage/backend/__init__.py,0,b''
vel/storage/backend/dummy.py,0,"b'import pandas as pd\n\n\nclass DummyBackend:\n    """""" Storage backend to store all experiment data in /dev/null """"""\n\n    def __init__(self):\n        pass\n\n    def clean(self, initial_epoch):\n        """""" Remove entries from database that would get overwritten """"""\n        pass\n\n    def get_frame(self):\n        """""" Get a dataframe of metrics from this storage """"""\n        return pd.DataFrame()\n\n    def store(self, metrics):\n        """""" Store metrics in a datastore """"""\n        pass\n\n    def store_config(self, configuration):\n        """""" Store model parameters """"""\n        pass\n\n\ndef create():\n    """""" Vel factory function """"""\n    return DummyBackend()\n'"
vel/storage/backend/mongodb.py,0,"b'import pymongo\nimport pandas as pd\n\n\nclass MongoDbBackend:\n    """""" Storage backend to store all experiment results in a MongoDB database """"""\n\n    def __init__(self, model_config, uri, database):\n        self.model_config = model_config\n        self.client = pymongo.MongoClient(uri)\n        self.db = self.client[database]\n\n    def clean(self, initial_epoch):\n        """""" Remove entries from database that would get overwritten """"""\n        self.db.metrics.delete_many({\'run_name\': self.model_config.run_name, \'epoch_idx\': {\'$gt\': initial_epoch}})\n\n    def store_config(self, configuration):\n        """""" Store model parameters in the database """"""\n        run_name = self.model_config.run_name\n\n        self.db.configs.delete_many({\'run_name\': self.model_config.run_name})\n\n        configuration = configuration.copy()\n        configuration[\'run_name\'] = run_name\n\n        self.db.configs.insert_one(configuration)\n\n    def get_frame(self):\n        """""" Get a dataframe of metrics from this storage """"""\n        metric_items = list(self.db.metrics.find({\'run_name\': self.model_config.run_name}).sort(\'epoch_idx\'))\n        if len(metric_items) == 0:\n            return pd.DataFrame(columns=[\'run_name\'])\n        else:\n            return pd.DataFrame(metric_items).drop([\'_id\', \'model_name\'], axis=1).set_index(\'epoch_idx\')\n\n    def store(self, metrics):\n        augmented_metrics = metrics.copy()\n\n        model_name = self.model_config.name\n        run_name = self.model_config.run_name\n\n        augmented_metrics[\'model_name\'] = model_name\n        augmented_metrics[\'run_name\'] = run_name\n\n        self.db.metrics.insert_one(augmented_metrics)\n\n\ndef create(model_config, uri, database):\n    """""" Vel factory function """"""\n    return MongoDbBackend(model_config, uri, database)\n\n'"
vel/storage/strategy/__init__.py,0,b''
vel/storage/strategy/checkpoint_strategy.py,0,"b'import typing\n\n\nclass CheckpointStrategy:\n    """""" Base class for various checkpoint strategies """"""\n    def should_delete_previous_checkpoint(self, epoch_idx) -> bool:\n        """""" Should previous checkpoint be deleted or stored """"""\n        return True\n\n    def should_store_best_checkpoint(self, epoch_idx, metrics) -> bool:\n        """""" Should we store current checkpoint as the best """"""\n        return False\n\n    def store_best_checkpoint_idx(self, epoch_idx) -> None:\n        """""" Should we store current checkpoint as the best """"""\n        pass\n\n    @property\n    def current_best_checkpoint_idx(self) -> typing.Union[int, None]:\n        return None\n\n    def write_state_dict(self, hidden_state_dict): pass\n\n    def restore(self, hidden_state_dict): pass\n\n\n'"
vel/storage/strategy/classic_checkpoint_strategy.py,0,"b'import typing\n\nfrom .checkpoint_strategy import CheckpointStrategy\nfrom vel.util.better import better\n\n\nclass ClassicCheckpointStrategy(CheckpointStrategy):\n    """""" Classic checkpoint strategy """"""\n    def __init__(self, checkpoint_frequency=0, metric=None, metric_mode=\'min\', store_best=False):\n        self.checkpoint_frequency = checkpoint_frequency\n        self.metric = metric or \'val:loss\'\n        self.metric_mode = metric_mode\n        self.store_best = store_best\n\n        # TODO(jerry) initialize these values from hidden state\n        self._current_best_metric_value = None\n        self._current_best_checkpoint_idx = None\n\n    def should_delete_previous_checkpoint(self, epoch_idx) -> bool:\n        prev_epoch_idx = epoch_idx - 1\n\n        if self.checkpoint_frequency > 0 and prev_epoch_idx % self.checkpoint_frequency == 0:\n            return False\n        else:\n            return True\n\n    def should_store_best_checkpoint(self, epoch_idx, metrics) -> bool:\n        """""" Should we store current checkpoint as the best """"""\n        if not self.store_best:\n            return False\n\n        metric = metrics[self.metric]\n\n        if better(self._current_best_metric_value, metric, self.metric_mode):\n            self._current_best_metric_value = metric\n            return True\n\n        return False\n\n    def store_best_checkpoint_idx(self, epoch_idx) -> None:\n        """""" Should we store current checkpoint as the best """"""\n        self._current_best_checkpoint_idx = epoch_idx\n\n    @property\n    def current_best_checkpoint_idx(self) -> typing.Union[int, None]:\n        return self._current_best_checkpoint_idx\n\n\ndef create(checkpoint_frequency=0, metric=None, metric_mode=\'min\', store_best=False):\n    """""" Vel factory function """"""\n    return ClassicCheckpointStrategy(\n        checkpoint_frequency=checkpoint_frequency,\n        metric=metric,\n        metric_mode=metric_mode,\n        store_best=store_best\n    )\n'"
vel/storage/streaming/__init__.py,0,b''
vel/storage/streaming/stdout.py,0,"b'from vel.api import EpochInfo, Callback\n\n\nclass StdoutStreaming(Callback):\n    """""" Stream results to stdout """"""\n    def on_epoch_end(self, epoch_info: EpochInfo):\n        if epoch_info.training_info.run_name:\n            print(f""=>>>>>>>>>> EPOCH {epoch_info.global_epoch_idx} [{epoch_info.training_info.run_name}]"")\n        else:\n            print(f""=>>>>>>>>>> EPOCH {epoch_info.global_epoch_idx}"")\n\n        if any(\':\' not in x for x in epoch_info.result.keys()):\n            self._print_metrics_line(epoch_info.result, head=None)\n\n        head_set = sorted({x.split(\':\')[0] + \':\' for x in epoch_info.result.keys() if \':\' in x})\n\n        for head in head_set:\n            if any(x.startswith(head) for x in epoch_info.result.keys()):\n                self._print_metrics_line(epoch_info.result, head)\n\n        print(f""=>>>>>>>>>> DONE"")\n\n    @staticmethod\n    def _print_metrics_line(metrics, head=None):\n        if head is None:\n            head = \'Metrics:\'\n\n            metrics_list = [\n                ""{} {:.06f}"".format(k, metrics[k])\n                for k in sorted([k for k in metrics.keys() if \':\' not in k])\n            ]\n        else:\n            metrics_list = [\n                ""{} {:.06f}"".format(k.split(\':\')[1], metrics[k])\n                for k in sorted([k for k in metrics.keys() if k.startswith(head)])\n            ]\n\n        print(\'{0: <10}\'.format(head.capitalize()), "" "".join(metrics_list))\n\n\ndef create():\n    """""" Vel factory function """"""\n    return StdoutStreaming()\n'"
vel/storage/streaming/visdom.py,0,"b'import visdom\nimport pandas as pd\n\n\nfrom vel.api import ModelConfig, Callback\nfrom vel.util.visdom import visdom_append_metrics, VisdomSettings\n\n\nclass VisdomStreaming(Callback):\n    """""" Stream live results to visdom from training """"""\n    def __init__(self, model_config: ModelConfig, visdom_settings: VisdomSettings):\n        self.model_config = model_config\n        self.settings = visdom_settings\n\n        self.vis = visdom.Visdom(\n            server=visdom_settings.server,\n            endpoint=visdom_settings.endpoint,\n            port=visdom_settings.port,\n            env=self.model_config.run_name.replace(\'/\', \'_\')\n        )\n\n    def on_epoch_end(self, epoch_info):\n        """""" Update data in visdom on push """"""\n        metrics_df = pd.DataFrame([epoch_info.result]).set_index(\'epoch_idx\')\n\n        visdom_append_metrics(\n            self.vis,\n            metrics_df,\n            first_epoch=epoch_info.global_epoch_idx == 1\n        )\n\n    def on_batch_end(self, batch_info):\n        """""" Stream LR to visdom """"""\n        if self.settings.stream_lr:\n            iteration_idx = (\n                    float(batch_info.epoch_number) +\n                    float(batch_info.batch_number) / batch_info.batches_per_epoch\n            )\n            \n            lr = batch_info.optimizer.param_groups[-1][\'lr\']\n\n            metrics_df = pd.DataFrame([lr], index=[iteration_idx], columns=[\'lr\'])\n\n            visdom_append_metrics(\n                self.vis,\n                metrics_df,\n                first_epoch=(batch_info.epoch_number == 1) and (batch_info.batch_number == 0)\n            )\n\n\ndef create(model_config, visdom_settings):\n    """""" Vel factory function """"""\n    return VisdomStreaming(model_config, VisdomSettings(**visdom_settings))\n'"
examples-scripts/rl/atari/a2c/breakout_a2c.py,2,"b""import torch\nimport torch.optim as optim\n\nfrom vel.rl.metrics import EpisodeRewardMetric\nfrom vel.storage.streaming.stdout import StdoutStreaming\nfrom vel.util.random import set_seed\n\nfrom vel.rl.env.classic_atari import ClassicAtariEnv\nfrom vel.rl.vecenv.subproc import SubprocVecEnvWrapper\n\nfrom vel.modules.input.image_to_tensor import ImageToTensorFactory\nfrom vel.rl.models.stochastic_policy_model import StochasticPolicyModelFactory\nfrom vel.rl.models.backbone.nature_cnn import NatureCnnFactory\n\n\nfrom vel.rl.reinforcers.on_policy_iteration_reinforcer import (\n    OnPolicyIterationReinforcer, OnPolicyIterationReinforcerSettings\n)\n\nfrom vel.rl.algo.policy_gradient.a2c import A2CPolicyGradient\nfrom vel.rl.env_roller.step_env_roller import StepEnvRoller\n\nfrom vel.api.info import TrainingInfo, EpochInfo\n\n\ndef breakout_a2c():\n    device = torch.device('cuda:0')\n    seed = 1001\n\n    # Set random seed in python std lib, numpy and pytorch\n    set_seed(seed)\n\n    # Create 16 environments evaluated in parallel in sub processess with all usual DeepMind wrappers\n    # These are just helper functions for that\n    vec_env = SubprocVecEnvWrapper(\n        ClassicAtariEnv('BreakoutNoFrameskip-v4'), frame_history=4\n    ).instantiate(parallel_envs=16, seed=seed)\n\n    # Again, use a helper to create a model\n    # But because model is owned by the reinforcer, model should not be accessed using this variable\n    # but from reinforcer.model property\n    model = StochasticPolicyModelFactory(\n        input_block=ImageToTensorFactory(),\n        backbone=NatureCnnFactory(input_width=84, input_height=84, input_channels=4)\n    ).instantiate(action_space=vec_env.action_space)\n\n    # Reinforcer - an object managing the learning process\n    reinforcer = OnPolicyIterationReinforcer(\n        device=device,\n        settings=OnPolicyIterationReinforcerSettings(\n            batch_size=256,\n            number_of_steps=5,\n        ),\n        model=model,\n        algo=A2CPolicyGradient(\n            entropy_coefficient=0.01,\n            value_coefficient=0.5,\n            max_grad_norm=0.5,\n            discount_factor=0.99,\n        ),\n        env_roller=StepEnvRoller(\n            environment=vec_env,\n            device=device,\n        )\n    )\n\n    # Model optimizer\n    optimizer = optim.RMSprop(reinforcer.model.parameters(), lr=7.0e-4, eps=1e-3)\n\n    # Overall information store for training information\n    training_info = TrainingInfo(\n        metrics=[\n            EpisodeRewardMetric('episode_rewards'),  # Calculate average reward from episode\n        ],\n        callbacks=[StdoutStreaming()]  # Print live metrics every epoch to standard output\n    )\n\n    # A bit of training initialization bookkeeping...\n    training_info.initialize()\n    reinforcer.initialize_training(training_info)\n    training_info.on_train_begin()\n\n    # Let's make 100 batches per epoch to average metrics nicely\n    num_epochs = int(1.1e7 / (5 * 16) / 100)\n\n    # Normal handrolled training loop\n    for i in range(1, num_epochs+1):\n        epoch_info = EpochInfo(\n            training_info=training_info,\n            global_epoch_idx=i,\n            batches_per_epoch=100,\n            optimizer=optimizer\n        )\n\n        reinforcer.train_epoch(epoch_info)\n\n    training_info.on_train_end()\n\n\nif __name__ == '__main__':\n    breakout_a2c()\n"""
examples-scripts/rl/atari/a2c/breakout_a2c_evaluate.py,4,"b'import torch\nimport pandas as pd\nimport numpy as np\n\nfrom vel.modules.input.image_to_tensor import ImageToTensorFactory\nfrom vel.openai.baselines.common.atari_wrappers import FrameStack\nfrom vel.rl.env.classic_atari import ClassicAtariEnv\nfrom vel.rl.models.backbone.nature_cnn import NatureCnnFactory\nfrom vel.rl.models.stochastic_policy_model import StochasticPolicyModelFactory\n\n\ndef breakout_a2c_evaluate(checkpoint_file_path, takes=10):\n    model_checkpoint = torch.load(checkpoint_file_path)\n    device = torch.device(\'cuda:0\')\n\n    env = FrameStack(\n        ClassicAtariEnv(\'BreakoutNoFrameskip-v4\').instantiate(preset=\'record\'), k=4\n    )\n\n    model = StochasticPolicyModelFactory(\n        input_block=ImageToTensorFactory(),\n        backbone=NatureCnnFactory(input_width=84, input_height=84, input_channels=4)\n    ).instantiate(action_space=env.action_space)\n\n    model.load_state_dict(model_checkpoint)\n    model = model.to(device)\n\n    model.eval()\n\n    rewards = []\n    lengths = []\n\n    for i in range(takes):\n        result = record_take(model, env, device)\n        rewards.append(result[\'r\'])\n        lengths.append(result[\'l\'])\n\n    print(pd.DataFrame({\'lengths\': lengths, \'rewards\': rewards}).describe())\n\n\n@torch.no_grad()\ndef record_take(model, env_instance, device):\n    frames = []\n\n    observation = env_instance.reset()\n\n    frames.append(env_instance.render(\'rgb_array\'))\n\n    print(""Evaluating environment..."")\n\n    while True:\n        observation_array = np.expand_dims(np.array(observation), axis=0)\n        observation_tensor = torch.from_numpy(observation_array).to(device)\n        actions = model.step(observation_tensor, argmax_sampling=True)[\'actions\']\n\n        observation, reward, done, epinfo = env_instance.step(actions.item())\n\n        frames.append(env_instance.render(\'rgb_array\'))\n\n        if \'episode\' in epinfo:\n            # End of an episode\n            return epinfo[\'episode\']\n\n\nif __name__ == \'__main__\':\n    breakout_a2c_evaluate(""checkpoint_00001375.data"", takes=2)\n'"
examples-scripts/rl/atari/ppo/qbert_ppo.py,2,"b""import torch\nimport torch.optim as optim\n\nfrom vel.rl.metrics import EpisodeRewardMetric\nfrom vel.storage.streaming.stdout import StdoutStreaming\nfrom vel.util.random import set_seed\nfrom vel.api.info import TrainingInfo, EpochInfo\n\nfrom vel.modules.input.image_to_tensor import ImageToTensorFactory\nfrom vel.rl.env.classic_atari import ClassicAtariEnv\nfrom vel.rl.vecenv.subproc import SubprocVecEnvWrapper\nfrom vel.rl.models.stochastic_policy_model import StochasticPolicyModelFactory\nfrom vel.rl.models.backbone.nature_cnn import NatureCnnFactory\n\nfrom vel.rl.reinforcers.on_policy_iteration_reinforcer import (\n    OnPolicyIterationReinforcer, OnPolicyIterationReinforcerSettings\n)\n\nfrom vel.rl.algo.policy_gradient.ppo import PpoPolicyGradient\nfrom vel.rl.env_roller.step_env_roller import StepEnvRoller\nfrom vel.rl.commands.rl_train_command import FrameTracker\n\nfrom vel.schedules.linear import LinearSchedule\n\n\ndef qbert_ppo():\n    device = torch.device('cuda:0')\n    seed = 1001\n\n    # Set random seed in python std lib, numpy and pytorch\n    set_seed(seed)\n\n    # Create 16 environments evaluated in parallel in sub processess with all usual DeepMind wrappers\n    # These are just helper functions for that\n    vec_env = SubprocVecEnvWrapper(\n        ClassicAtariEnv('QbertNoFrameskip-v4'), frame_history=4\n    ).instantiate(parallel_envs=8, seed=seed)\n\n    # Again, use a helper to create a model\n    # But because model is owned by the reinforcer, model should not be accessed using this variable\n    # but from reinforcer.model property\n    model = StochasticPolicyModelFactory(\n        input_block=ImageToTensorFactory(),\n        backbone=NatureCnnFactory(input_width=84, input_height=84, input_channels=4)\n    ).instantiate(action_space=vec_env.action_space)\n\n    # Set schedule for gradient clipping.\n    cliprange = LinearSchedule(\n        initial_value=0.1,\n        final_value=0.0\n    )\n\n    # Reinforcer - an object managing the learning process\n    reinforcer = OnPolicyIterationReinforcer(\n        device=device,\n        settings=OnPolicyIterationReinforcerSettings(\n            batch_size=256,\n            experience_replay=4,\n            number_of_steps=128\n        ),\n        model=model,\n        algo=PpoPolicyGradient(\n            entropy_coefficient=0.01,\n            value_coefficient=0.5,\n            max_grad_norm=0.5,\n            discount_factor=0.99,\n            gae_lambda=0.95,\n            cliprange=cliprange\n        ),\n        env_roller=StepEnvRoller(\n            environment=vec_env,\n            device=device,\n        )\n    )\n\n    # Model optimizer\n    optimizer = optim.Adam(reinforcer.model.parameters(), lr=2.5e-4, eps=1.0e-5)\n\n    # Overall information store for training information\n    training_info = TrainingInfo(\n        metrics=[\n            EpisodeRewardMetric('episode_rewards'),  # Calculate average reward from episode\n        ],\n        callbacks=[\n            StdoutStreaming(),   # Print live metrics every epoch to standard output\n            FrameTracker(1.1e7)      # We need frame tracker to track the progress of learning\n        ]\n    )\n\n    # A bit of training initialization bookkeeping...\n    training_info.initialize()\n    reinforcer.initialize_training(training_info)\n    training_info.on_train_begin()\n\n    # Let's make 10 batches per epoch to average metrics nicely\n    # Rollout size is 8 environments times 128 steps\n    num_epochs = int(1.1e7 / (128 * 8) / 10)\n\n    # Normal handrolled training loop\n    for i in range(1, num_epochs+1):\n        epoch_info = EpochInfo(\n            training_info=training_info,\n            global_epoch_idx=i,\n            batches_per_epoch=10,\n            optimizer=optimizer\n        )\n\n        reinforcer.train_epoch(epoch_info)\n\n    training_info.on_train_end()\n\n\nif __name__ == '__main__':\n    qbert_ppo()\n"""
examples-scripts/rl/mujoco/ddpg/half_cheetah_ddpg.py,2,"b""import torch\nimport torch.optim\n\nfrom vel.api import TrainingInfo, EpochInfo\nfrom vel.modules.input.normalize_observations import NormalizeObservationsFactory\nfrom vel.rl.buffers.circular_replay_buffer import CircularReplayBuffer\nfrom vel.rl.env_roller.transition_replay_env_roller import TransitionReplayEnvRoller\nfrom vel.rl.metrics import EpisodeRewardMetric\nfrom vel.rl.modules.noise.ou_noise import OuNoise\nfrom vel.storage.streaming.stdout import StdoutStreaming\nfrom vel.util.random import set_seed\nfrom vel.rl.env.mujoco import MujocoEnv\nfrom vel.rl.models.deterministic_policy_model import DeterministicPolicyModelFactory\nfrom vel.rl.models.backbone.mlp import MLPFactory\nfrom vel.rl.reinforcers.buffered_off_policy_iteration_reinforcer import (\n    BufferedOffPolicyIterationReinforcer, BufferedOffPolicyIterationReinforcerSettings\n)\nfrom vel.rl.algo.policy_gradient.ddpg import DeepDeterministicPolicyGradient\nfrom vel.rl.vecenv.dummy import DummyVecEnvWrapper\nfrom vel.optimizers.adam import AdamFactory\n\n\ndef half_cheetah_ddpg():\n    device = torch.device('cuda:0')\n    seed = 1002\n\n    # Set random seed in python std lib, numpy and pytorch\n    set_seed(seed)\n\n    vec_env = DummyVecEnvWrapper(\n        MujocoEnv('HalfCheetah-v2')\n    ).instantiate(parallel_envs=1, seed=seed)\n\n    model_factory = DeterministicPolicyModelFactory(\n        input_block=NormalizeObservationsFactory(input_shape=17),\n        policy_backbone=MLPFactory(input_length=17, hidden_layers=[64, 64], activation='tanh'),\n        value_backbone=MLPFactory(input_length=23, hidden_layers=[64, 64], activation='tanh'),\n    )\n\n    model = model_factory.instantiate(action_space=vec_env.action_space)\n\n    reinforcer = BufferedOffPolicyIterationReinforcer(\n        device=device,\n        environment=vec_env,\n        settings=BufferedOffPolicyIterationReinforcerSettings(\n            rollout_steps=2,\n            training_steps=64,\n        ),\n        model=model,\n        algo=DeepDeterministicPolicyGradient(\n            model_factory=model_factory,\n            discount_factor=0.99,\n            tau=0.01,\n        ),\n        env_roller=TransitionReplayEnvRoller(\n            environment=vec_env,\n            device=device,\n            action_noise=OuNoise(std_dev=0.2, environment=vec_env),\n            replay_buffer=CircularReplayBuffer(\n                buffer_capacity=1_000_000,\n                buffer_initial_size=2_000,\n                num_envs=vec_env.num_envs,\n                observation_space=vec_env.observation_space,\n                action_space=vec_env.action_space\n            ),\n            normalize_returns=True,\n            discount_factor=0.99\n        ),\n    )\n\n    # Optimizer helper - A weird regularization settings I've copied from OpenAI code\n    adam_optimizer = AdamFactory(\n        lr=[1.0e-4, 1.0e-3, 1.0e-3],\n        weight_decay=[0.0, 0.0, 0.001],\n        eps=1.0e-4,\n        layer_groups=True\n    ).instantiate(model)\n\n    # Overall information store for training information\n    training_info = TrainingInfo(\n        metrics=[\n            EpisodeRewardMetric('episode_rewards'),  # Calculate average reward from episode\n        ],\n        callbacks=[StdoutStreaming()]  # Print live metrics every epoch to standard output\n    )\n\n    # A bit of training initialization bookkeeping...\n    training_info.initialize()\n    reinforcer.initialize_training(training_info)\n    training_info.on_train_begin()\n\n    # Let's make 20 batches per epoch to average metrics nicely\n    num_epochs = int(1.0e6 / 2 / 1000)\n\n    # Normal handrolled training loop\n    for i in range(1, num_epochs+1):\n        epoch_info = EpochInfo(\n            training_info=training_info,\n            global_epoch_idx=i,\n            batches_per_epoch=1000,\n            optimizer=adam_optimizer\n        )\n\n        reinforcer.train_epoch(epoch_info)\n\n    training_info.on_train_end()\n\n\nif __name__ == '__main__':\n    half_cheetah_ddpg()\n"""
vel/openai/baselines/bench/__init__.py,0,b'from vel.openai.baselines.bench.benchmarks import *\nfrom vel.openai.baselines.bench.monitor import *\n'
vel/openai/baselines/bench/benchmarks.py,0,"b'import re\nimport os.path as osp\nimport os\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n\n_atari7 = [\'BeamRider\', \'Breakout\', \'Enduro\', \'Pong\', \'Qbert\', \'Seaquest\', \'SpaceInvaders\']\n_atariexpl7 = [\'Freeway\', \'Gravitar\', \'MontezumaRevenge\', \'Pitfall\', \'PrivateEye\', \'Solaris\', \'Venture\']\n\n_BENCHMARKS = []\n\nremove_version_re = re.compile(r\'-v\\d+$\')\n\n\ndef register_benchmark(benchmark):\n    for b in _BENCHMARKS:\n        if b[\'name\'] == benchmark[\'name\']:\n            raise ValueError(\'Benchmark with name %s already registered!\' % b[\'name\'])\n\n    # automatically add a description if it is not present\n    if \'tasks\' in benchmark:\n        for t in benchmark[\'tasks\']:\n            if \'desc\' not in t:\n                t[\'desc\'] = remove_version_re.sub(\'\', t[\'env_id\'])\n    _BENCHMARKS.append(benchmark)\n\n\ndef list_benchmarks():\n    return [b[\'name\'] for b in _BENCHMARKS]\n\n\ndef get_benchmark(benchmark_name):\n    for b in _BENCHMARKS:\n        if b[\'name\'] == benchmark_name:\n            return b\n    raise ValueError(\'%s not found! Known benchmarks: %s\' % (benchmark_name, list_benchmarks()))\n\n\ndef get_task(benchmark, env_id):\n    """"""Get a task by env_id. Return None if the benchmark doesn\'t have the env""""""\n    return next(filter(lambda task: task[\'env_id\'] == env_id, benchmark[\'tasks\']), None)\n\n\ndef find_task_for_env_id_in_any_benchmark(env_id):\n    for bm in _BENCHMARKS:\n        for task in bm[""tasks""]:\n            if task[""env_id""] == env_id:\n                return bm, task\n    return None, None\n\n\n_ATARI_SUFFIX = \'NoFrameskip-v4\'\n\nregister_benchmark({\n    \'name\': \'Atari50M\',\n    \'description\': \'7 Atari games from Mnih et al. (2013), with pixel observations, 50M timesteps\',\n    \'tasks\': [{\'desc\': _game, \'env_id\': _game + _ATARI_SUFFIX, \'trials\': 2, \'num_timesteps\': int(50e6)} for _game in _atari7]\n})\n\nregister_benchmark({\n    \'name\': \'Atari10M\',\n    \'description\': \'7 Atari games from Mnih et al. (2013), with pixel observations, 10M timesteps\',\n    \'tasks\': [{\'desc\': _game, \'env_id\': _game + _ATARI_SUFFIX, \'trials\': 2, \'num_timesteps\': int(10e6)} for _game in _atari7]\n})\n\nregister_benchmark({\n    \'name\': \'Atari1Hr\',\n    \'description\': \'7 Atari games from Mnih et al. (2013), with pixel observations, 1 hour of walltime\',\n    \'tasks\': [{\'desc\': _game, \'env_id\': _game + _ATARI_SUFFIX, \'trials\': 2, \'num_seconds\': 60 * 60} for _game in _atari7]\n})\n\nregister_benchmark({\n    \'name\': \'AtariExploration10M\',\n    \'description\': \'7 Atari games emphasizing exploration, with pixel observations, 10M timesteps\',\n    \'tasks\': [{\'desc\': _game, \'env_id\': _game + _ATARI_SUFFIX, \'trials\': 2, \'num_timesteps\': int(10e6)} for _game in _atariexpl7]\n})\n\n\n# MuJoCo\n\n_mujocosmall = [\n    \'InvertedDoublePendulum-v2\', \'InvertedPendulum-v2\',\n    \'HalfCheetah-v2\', \'Hopper-v2\', \'Walker2d-v2\',\n    \'Reacher-v2\', \'Swimmer-v2\']\nregister_benchmark({\n    \'name\': \'Mujoco1M\',\n    \'description\': \'Some small 2D MuJoCo tasks, run for 1M timesteps\',\n    \'tasks\': [{\'env_id\': _envid, \'trials\': 3, \'num_timesteps\': int(1e6)} for _envid in _mujocosmall]\n})\nregister_benchmark({\n    \'name\': \'MujocoWalkers\',\n    \'description\': \'MuJoCo forward walkers, run for 8M, humanoid 100M\',\n    \'tasks\': [\n        {\'env_id\': ""Hopper-v1"", \'trials\': 4, \'num_timesteps\': 8 * 1000000},\n        {\'env_id\': ""Walker2d-v1"", \'trials\': 4, \'num_timesteps\': 8 * 1000000},\n        {\'env_id\': ""Humanoid-v1"", \'trials\': 4, \'num_timesteps\': 100 * 1000000},\n    ]\n})\n\n# Roboschool\n\nregister_benchmark({\n    \'name\': \'Roboschool8M\',\n    \'description\': \'Small 2D tasks, up to 30 minutes to complete on 8 cores\',\n    \'tasks\': [\n        {\'env_id\': ""RoboschoolReacher-v1"", \'trials\': 4, \'num_timesteps\': 2 * 1000000},\n        {\'env_id\': ""RoboschoolAnt-v1"", \'trials\': 4, \'num_timesteps\': 8 * 1000000},\n        {\'env_id\': ""RoboschoolHalfCheetah-v1"", \'trials\': 4, \'num_timesteps\': 8 * 1000000},\n        {\'env_id\': ""RoboschoolHopper-v1"", \'trials\': 4, \'num_timesteps\': 8 * 1000000},\n        {\'env_id\': ""RoboschoolWalker2d-v1"", \'trials\': 4, \'num_timesteps\': 8 * 1000000},\n    ]\n})\nregister_benchmark({\n    \'name\': \'RoboschoolHarder\',\n    \'description\': \'Test your might!!! Up to 12 hours on 32 cores\',\n    \'tasks\': [\n        {\'env_id\': ""RoboschoolHumanoid-v1"", \'trials\': 4, \'num_timesteps\': 100 * 1000000},\n        {\'env_id\': ""RoboschoolHumanoidFlagrun-v1"", \'trials\': 4, \'num_timesteps\': 200 * 1000000},\n        {\'env_id\': ""RoboschoolHumanoidFlagrunHarder-v1"", \'trials\': 4, \'num_timesteps\': 400 * 1000000},\n    ]\n})\n\n# Other\n\n_atari50 = [  # actually 47\n    \'Alien\', \'Amidar\', \'Assault\', \'Asterix\', \'Asteroids\',\n    \'Atlantis\', \'BankHeist\', \'BattleZone\', \'BeamRider\', \'Bowling\',\n    \'Breakout\', \'Centipede\', \'ChopperCommand\', \'CrazyClimber\',\n    \'DemonAttack\', \'DoubleDunk\', \'Enduro\', \'FishingDerby\', \'Freeway\',\n    \'Frostbite\', \'Gopher\', \'Gravitar\', \'IceHockey\', \'Jamesbond\',\n    \'Kangaroo\', \'Krull\', \'KungFuMaster\', \'MontezumaRevenge\', \'MsPacman\',\n    \'NameThisGame\', \'Pitfall\', \'Pong\', \'PrivateEye\', \'Qbert\',\n    \'RoadRunner\', \'Robotank\', \'Seaquest\', \'SpaceInvaders\', \'StarGunner\',\n    \'Tennis\', \'TimePilot\', \'Tutankham\', \'UpNDown\', \'Venture\',\n    \'VideoPinball\', \'WizardOfWor\', \'Zaxxon\',\n]\n\nregister_benchmark({\n    \'name\': \'Atari50_10M\',\n    \'description\': \'47 Atari games from Mnih et al. (2013), with pixel observations, 10M timesteps\',\n    \'tasks\': [{\'desc\': _game, \'env_id\': _game + _ATARI_SUFFIX, \'trials\': 2, \'num_timesteps\': int(10e6)} for _game in _atari50]\n})\n\n# HER DDPG\n\nregister_benchmark({\n    \'name\': \'HerDdpg\',\n    \'description\': \'Smoke-test only benchmark of HER\',\n    \'tasks\': [{\'trials\': 1, \'env_id\': \'FetchReach-v1\'}]\n})\n'"
vel/openai/baselines/bench/monitor.py,0,"b'__all__ = [\'Monitor\', \'get_monitor_files\', \'load_results\']\n\nimport gym\nfrom gym.core import Wrapper\nimport time\nfrom glob import glob\nimport csv\nimport os\nimport os.path as osp\nimport json\nimport uuid\nimport numpy as np\nimport pandas\n\nclass Monitor(Wrapper):\n    EXT = ""monitor.csv""\n    f = None\n\n    def __init__(self, env, filename, allow_early_resets=False, reset_keywords=(), info_keywords=()):\n        Wrapper.__init__(self, env=env)\n        self.tstart = time.time()\n        if filename is None:\n            self.f = None\n            self.logger = None\n        else:\n            if not filename.endswith(Monitor.EXT):\n                if osp.isdir(filename):\n                    filename = osp.join(filename, Monitor.EXT)\n                else:\n                    filename = filename + ""."" + Monitor.EXT\n            self.f = open(filename, ""wt"")\n            self.f.write(\'#%s\\n\'%json.dumps({""t_start"": self.tstart, \'env_id\' : env.spec and env.spec.id}))\n            self.logger = csv.DictWriter(self.f, fieldnames=(\'r\', \'l\', \'t\')+reset_keywords+info_keywords)\n            self.logger.writeheader()\n            self.f.flush()\n\n        self.reset_keywords = reset_keywords\n        self.info_keywords = info_keywords\n        self.allow_early_resets = allow_early_resets\n        self.rewards = None\n        self.needs_reset = True\n        self.episode_rewards = []\n        self.episode_lengths = []\n        self.episode_times = []\n        self.total_steps = 0\n        self.current_reset_info = {} # extra info about the current episode, that was passed in during reset()\n\n    def reset(self, **kwargs):\n        if not self.allow_early_resets and not self.needs_reset:\n            raise RuntimeError(""Tried to reset an environment before done. If you want to allow early resets, wrap your env with Monitor(env, path, allow_early_resets=True)"")\n        self.rewards = []\n        self.needs_reset = False\n        for k in self.reset_keywords:\n            v = kwargs.get(k)\n            if v is None:\n                raise ValueError(\'Expected you to pass kwarg %s into reset\'%k)\n            self.current_reset_info[k] = v\n        return self.env.reset(**kwargs)\n\n    def step(self, action):\n        if self.needs_reset:\n            raise RuntimeError(""Tried to step environment that needs reset"")\n        ob, rew, done, info = self.env.step(action)\n        self.rewards.append(rew)\n        if done:\n            self.needs_reset = True\n            eprew = sum(self.rewards)\n            eplen = len(self.rewards)\n            epinfo = {""r"": round(eprew, 6), ""l"": eplen, ""t"": round(time.time() - self.tstart, 6)}\n            for k in self.info_keywords:\n                epinfo[k] = info[k]\n            self.episode_rewards.append(eprew)\n            self.episode_lengths.append(eplen)\n            self.episode_times.append(time.time() - self.tstart)\n            epinfo.update(self.current_reset_info)\n            if self.logger:\n                self.logger.writerow(epinfo)\n                self.f.flush()\n            info[\'episode\'] = epinfo\n        self.total_steps += 1\n        return (ob, rew, done, info)\n\n    def close(self):\n        if self.f is not None:\n            self.f.close()\n\n    def get_total_steps(self):\n        return self.total_steps\n\n    def get_episode_rewards(self):\n        return self.episode_rewards\n\n    def get_episode_lengths(self):\n        return self.episode_lengths\n\n    def get_episode_times(self):\n        return self.episode_times\n\nclass LoadMonitorResultsError(Exception):\n    pass\n\ndef get_monitor_files(dir):\n    return glob(osp.join(dir, ""*"" + Monitor.EXT))\n\ndef load_results(dir):\n    import pandas\n    monitor_files = (\n            glob(osp.join(dir, ""*monitor.json"")) +\n            glob(osp.join(dir, ""*monitor.csv""))) # get both csv and (old) json files\n    if not monitor_files:\n        raise LoadMonitorResultsError(""no monitor files of the form *%s found in %s"" % (Monitor.EXT, dir))\n    dfs = []\n    headers = []\n    for fname in monitor_files:\n        with open(fname, \'rt\') as fh:\n            if fname.endswith(\'csv\'):\n                firstline = fh.readline()\n                assert firstline[0] == \'#\'\n                header = json.loads(firstline[1:])\n                df = pandas.read_csv(fh, index_col=None)\n                headers.append(header)\n            elif fname.endswith(\'json\'): # Deprecated json format\n                episodes = []\n                lines = fh.readlines()\n                header = json.loads(lines[0])\n                headers.append(header)\n                for line in lines[1:]:\n                    episode = json.loads(line)\n                    episodes.append(episode)\n                df = pandas.DataFrame(episodes)\n            else:\n                assert 0, \'unreachable\'\n            df[\'t\'] += header[\'t_start\']\n        dfs.append(df)\n    df = pandas.concat(dfs)\n    df.sort_values(\'t\', inplace=True)\n    df.reset_index(inplace=True)\n    df[\'t\'] -= min(header[\'t_start\'] for header in headers)\n    df.headers = headers # HACK to preserve backwards compatibility\n    return df\n\ndef test_monitor():\n    env = gym.make(""CartPole-v1"")\n    env.seed(0)\n    mon_file = ""/tmp/baselines-test-%s.monitor.csv"" % uuid.uuid4()\n    menv = Monitor(env, mon_file)\n    menv.reset()\n    for _ in range(1000):\n        _, _, done, _ = menv.step(0)\n        if done:\n            menv.reset()\n\n    f = open(mon_file, \'rt\')\n\n    firstline = f.readline()\n    assert firstline.startswith(\'#\')\n    metadata = json.loads(firstline[1:])\n    assert metadata[\'env_id\'] == ""CartPole-v1""\n    assert set(metadata.keys()) == {\'env_id\', \'gym_version\', \'t_start\'},  ""Incorrect keys in monitor metadata""\n\n    last_logline = pandas.read_csv(f, index_col=None)\n    assert set(last_logline.keys()) == {\'l\', \'t\', \'r\'}, ""Incorrect keys in monitor logline""\n    f.close()\n    os.remove(mon_file)\n'"
vel/openai/baselines/common/__init__.py,0,b''
vel/openai/baselines/common/atari_wrappers.py,0,"b'import numpy as np\nfrom collections import deque\nimport gym\nfrom gym import spaces\nimport cv2\ncv2.ocl.setUseOpenCL(False)\n\nclass NoopResetEnv(gym.Wrapper):\n    def __init__(self, env, noop_max=30):\n        """"""Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.noop_max = noop_max\n        self.override_num_noops = None\n        self.noop_action = 0\n        assert env.unwrapped.get_action_meanings()[0] == \'NOOP\'\n\n    def reset(self, **kwargs):\n        """""" Do no-op action for a number of steps in [1, noop_max].""""""\n        self.env.reset(**kwargs)\n        if self.override_num_noops is not None:\n            noops = self.override_num_noops\n        else:\n            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n        assert noops > 0\n        obs = None\n        for _ in range(noops):\n            obs, _, done, _ = self.env.step(self.noop_action)\n            if done:\n                obs = self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Take action on reset for environments that are fixed until firing.""""""\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == \'FIRE\'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done  = True\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if lives < self.lives and lives > 0:\n            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n            # so it\'s important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        """"""Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        """"""\n        if self.was_real_done:\n            obs = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs\n\n\nclass FireEpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n\n        if self.lives > lives > 0:\n            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n            # so its important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            # done = True\n            obs, _, done, _ = self.env.step(1)\n            if done:\n                self.env.reset()\n            obs, _, done, _ = self.env.step(2)\n            if done:\n                self.env.reset()\n\n        self.lives = lives\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs\n\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        """"""Return only every `skip`-th frame""""""\n        gym.Wrapper.__init__(self, env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n        self._skip       = skip\n\n    def step(self, action):\n        """"""Repeat action, sum reward, and max over last observations.""""""\n        total_reward = 0.0\n        done = None\n        for i in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            if i == self._skip - 2: self._obs_buffer[0] = obs\n            if i == self._skip - 1: self._obs_buffer[1] = obs\n            total_reward += reward\n            if done:\n                break\n        # Note that the observation on the done=True frame\n        # doesn\'t matter\n        max_frame = self._obs_buffer.max(axis=0)\n\n        return max_frame, total_reward, done, info\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\n    def render(self, mode=\'human\'):\n        if mode == \'rgb_array\':\n            return self._obs_buffer.max(axis=0)\n        else:\n            return self.env.render(mode)\n\n\nclass ClipRewardEnv(gym.RewardWrapper):\n    def __init__(self, env):\n        gym.RewardWrapper.__init__(self, env)\n\n    def reward(self, reward):\n        """"""Bin reward to {+1, 0, -1} by its sign.""""""\n        return np.sign(reward)\n\nclass WarpFrame(gym.ObservationWrapper):\n    def __init__(self, env, width=84, height=84, grayscale=True):\n        """"""Warp frames to 84x84 as done in the Nature paper and later work.""""""\n        gym.ObservationWrapper.__init__(self, env)\n        self.width = width\n        self.height = height\n        self.grayscale = grayscale\n        if self.grayscale:\n            self.observation_space = spaces.Box(low=0, high=255,\n                shape=(self.height, self.width, 1), dtype=np.uint8)\n        else:\n            self.observation_space = spaces.Box(low=0, high=255,\n                shape=(self.height, self.width, 3), dtype=np.uint8)\n\n    def observation(self, frame):\n        if self.grayscale:\n            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n        if self.grayscale:\n            frame = np.expand_dims(frame, -1)\n        return frame\n\nclass FrameStack(gym.Wrapper):\n    def __init__(self, env, k):\n        """"""Stack k last frames.\n\n        Returns lazy array, which is much more memory efficient.\n\n        See Also\n        --------\n        baselines.common.atari_wrappers.LazyFrames\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.k = k\n        self.frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[:-1] + (shp[-1] * k,)), dtype=env.observation_space.dtype)\n\n    def reset(self):\n        ob = self.env.reset()\n        for _ in range(self.k):\n            self.frames.append(ob)\n        return self._get_ob()\n\n    def step(self, action):\n        ob, reward, done, info = self.env.step(action)\n        self.frames.append(ob)\n        return self._get_ob(), reward, done, info\n\n    def _get_ob(self):\n        assert len(self.frames) == self.k\n        return LazyFrames(list(self.frames))\n\nclass ScaledFloatFrame(gym.ObservationWrapper):\n    def __init__(self, env):\n        gym.ObservationWrapper.__init__(self, env)\n        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n\n    def observation(self, observation):\n        # careful! This undoes the memory optimization, use\n        # with smaller replay buffers only.\n        return np.array(observation).astype(np.float32) / 255.0\n\nclass LazyFrames(object):\n    def __init__(self, frames):\n        """"""This object ensures that common frames between the observations are only stored once.\n        It exists purely to optimize memory usage which can be huge for DQN\'s 1M frames replay\n        buffers.\n\n        This object should only be converted to numpy array before being passed to the model.\n\n        You\'d not believe how complex the previous solution was.""""""\n        self._frames = frames\n        self._out = None\n\n    def _force(self):\n        if self._out is None:\n            self._out = np.concatenate(self._frames, axis=-1)\n            self._frames = None\n        return self._out\n\n    def __array__(self, dtype=None):\n        out = self._force()\n        if dtype is not None:\n            out = out.astype(dtype)\n        return out\n\n    def __len__(self):\n        return len(self._force())\n\n    def __getitem__(self, i):\n        return self._force()[i]\n\ndef make_atari(env_id, timelimit=True):\n    # XXX(john): remove timelimit argument after gym is upgraded to allow double wrapping\n    env = gym.make(env_id)\n    if not timelimit:\n        env = env.env\n    assert \'NoFrameskip\' in env.spec.id\n    env = NoopResetEnv(env, noop_max=30)\n    env = MaxAndSkipEnv(env, skip=4)\n    return env\n\ndef wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\n    """"""Configure environment for DeepMind-style Atari.\n    """"""\n    if episode_life:\n        env = EpisodicLifeEnv(env)\n    if \'FIRE\' in env.unwrapped.get_action_meanings():\n        env = FireResetEnv(env)\n    env = WarpFrame(env)\n    if scale:\n        env = ScaledFloatFrame(env)\n    if clip_rewards:\n        env = ClipRewardEnv(env)\n    if frame_stack:\n        env = FrameStack(env, 4)\n    return env\n'"
vel/openai/baselines/common/retro_wrappers.py,0,"b' # flake8: noqa F403, F405\nfrom .atari_wrappers import *\nimport numpy as np\nimport gym\n\n\nclass TimeLimit(gym.Wrapper):\n    def __init__(self, env, max_episode_steps=None):\n        super(TimeLimit, self).__init__(env)\n        self._max_episode_steps = max_episode_steps\n        self._elapsed_steps = 0\n\n    def step(self, ac):\n        observation, reward, done, info = self.env.step(ac)\n        self._elapsed_steps += 1\n        if self._elapsed_steps >= self._max_episode_steps:\n            done = True\n            info[\'TimeLimit.truncated\'] = True\n        return observation, reward, done, info\n\n    def reset(self, **kwargs):\n        self._elapsed_steps = 0\n        return self.env.reset(**kwargs)\n\n\nclass StochasticFrameSkip(gym.Wrapper):\n    def __init__(self, env, n, stickprob):\n        gym.Wrapper.__init__(self, env)\n        self.n = n\n        self.stickprob = stickprob\n        self.curac = None\n        self.rng = np.random.RandomState()\n        self.supports_want_render = hasattr(env, ""supports_want_render"")\n\n    def reset(self, **kwargs):\n        self.curac = None\n        return self.env.reset(**kwargs)\n\n    def step(self, ac):\n        done = False\n        totrew = 0\n        for i in range(self.n):\n            # First step after reset, use action\n            if self.curac is None:\n                self.curac = ac\n            # First substep, delay with probability=stickprob\n            elif i==0:\n                if self.rng.rand() > self.stickprob:\n                    self.curac = ac\n            # Second substep, new action definitely kicks in\n            elif i==1:\n                self.curac = ac\n            if self.supports_want_render and i<self.n-1:\n                ob, rew, done, info = self.env.step(self.curac, want_render=False)\n            else:\n                ob, rew, done, info = self.env.step(self.curac)\n            totrew += rew\n            if done: break\n        return ob, totrew, done, info\n\n    def seed(self, s):\n        self.rng.seed(s)\n\n\nclass PartialFrameStack(gym.Wrapper):\n    def __init__(self, env, k, channel=1):\n        """"""\n        Stack one channel (channel keyword) from previous frames\n        """"""\n        gym.Wrapper.__init__(self, env)\n        shp = env.observation_space.shape\n        self.channel = channel\n        self.observation_space = gym.spaces.Box(low=0, high=255,\n                                                shape=(shp[0], shp[1], shp[2] + k - 1),\n                                                dtype=env.observation_space.dtype)\n        self.k = k\n        self.frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n\n    def reset(self):\n        ob = self.env.reset()\n        assert ob.shape[2] > self.channel\n        for _ in range(self.k):\n            self.frames.append(ob)\n        return self._get_ob()\n\n    def step(self, ac):\n        ob, reward, done, info = self.env.step(ac)\n        self.frames.append(ob)\n        return self._get_ob(), reward, done, info\n\n    def _get_ob(self):\n        assert len(self.frames) == self.k\n        return np.concatenate([frame if i==self.k-1 else frame[:,:,self.channel:self.channel+1]\n                               for (i, frame) in enumerate(self.frames)], axis=2)\n\n\nclass Downsample(gym.ObservationWrapper):\n    def __init__(self, env, ratio):\n        """"""\n        Downsample images by a factor of ratio\n        """"""\n        gym.ObservationWrapper.__init__(self, env)\n        (oldh, oldw, oldc) = env.observation_space.shape\n        newshape = (oldh//ratio, oldw//ratio, oldc)\n        self.observation_space = spaces.Box(\n            low=0, high=255,\n            shape=newshape, dtype=np.uint8\n        )\n\n    def observation(self, frame):\n        height, width, _ = self.observation_space.shape\n        frame = cv2.resize(frame, (width, height), interpolation=cv2.INTER_AREA)\n        if frame.ndim == 2:\n            frame = frame[:,:,None]\n        return frame\n\n\nclass Rgb2gray(gym.ObservationWrapper):\n    def __init__(self, env):\n        """"""\n        Downsample images by a factor of ratio\n        """"""\n        gym.ObservationWrapper.__init__(self, env)\n        (oldh, oldw, _oldc) = env.observation_space.shape\n        self.observation_space = spaces.Box(\n            low=0, high=255,\n            shape=(oldh, oldw, 1), dtype=np.uint8\n        )\n\n    def observation(self, frame):\n        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        return frame[:,:,None]\n\n\nclass MovieRecord(gym.Wrapper):\n    def __init__(self, env, savedir, k):\n        gym.Wrapper.__init__(self, env)\n        self.savedir = savedir\n        self.k = k\n        self.epcount = 0\n    def reset(self):\n        if self.epcount % self.k == 0:\n            self.env.unwrapped.movie_path = self.savedir\n        else:\n            self.env.unwrapped.movie_path = None\n            self.env.unwrapped.movie = None\n        self.epcount += 1\n        return self.env.reset()\n\n\nclass AppendTimeout(gym.Wrapper):\n    def __init__(self, env):\n        gym.Wrapper.__init__(self, env)\n        self.action_space = env.action_space\n        self.timeout_space = gym.spaces.Box(low=np.array([0.0]), high=np.array([1.0]), dtype=np.float32)\n        self.original_os = env.observation_space\n        if isinstance(self.original_os, gym.spaces.Dict):\n            import copy\n            ordered_dict = copy.deepcopy(self.original_os.spaces)\n            ordered_dict[\'value_estimation_timeout\'] = self.timeout_space\n            self.observation_space = gym.spaces.Dict(ordered_dict)\n            self.dict_mode = True\n        else:\n            self.observation_space = gym.spaces.Dict({\n                \'original\': self.original_os,\n                \'value_estimation_timeout\': self.timeout_space\n            })\n            self.dict_mode = False\n        self.ac_count = None\n        while 1:\n            if not hasattr(env, ""_max_episode_steps""):  # Looking for TimeLimit wrapper that has this field\n                env = env.env\n                continue\n            break\n        self.timeout = env._max_episode_steps\n\n    def step(self, ac):\n        self.ac_count += 1\n        ob, rew, done, info = self.env.step(ac)\n        return self._process(ob), rew, done, info\n\n    def reset(self):\n        self.ac_count = 0\n        return self._process(self.env.reset())\n\n    def _process(self, ob):\n        fracmissing = 1 - self.ac_count / self.timeout\n        if self.dict_mode:\n            ob[\'value_estimation_timeout\'] = fracmissing\n        else:\n            return { \'original\': ob, \'value_estimation_timeout\': fracmissing }\n\n\nclass StartDoingRandomActionsWrapper(gym.Wrapper):\n    """"""\n    Warning: can eat info dicts, not good if you depend on them\n    """"""\n    def __init__(self, env, max_random_steps, on_startup=True, every_episode=False):\n        gym.Wrapper.__init__(self, env)\n        self.on_startup = on_startup\n        self.every_episode = every_episode\n        self.random_steps = max_random_steps\n        self.last_obs = None\n        if on_startup:\n            self.some_random_steps()\n\n    def some_random_steps(self):\n        self.last_obs = self.env.reset()\n        n = np.random.randint(self.random_steps)\n        #print(""running for random %i frames"" % n)\n        for _ in range(n):\n            self.last_obs, _, done, _ = self.env.step(self.env.action_space.sample())\n            if done: self.last_obs = self.env.reset()\n\n    def reset(self):\n        return self.last_obs\n\n    def step(self, a):\n        self.last_obs, rew, done, info = self.env.step(a)\n        if done:\n            self.last_obs = self.env.reset()\n            if self.every_episode:\n                self.some_random_steps()\n        return self.last_obs, rew, done, info\n\n\ndef make_retro(*, game, state, max_episode_steps, **kwargs):\n    import retro\n    env = retro.make(game, state, **kwargs)\n    env = StochasticFrameSkip(env, n=4, stickprob=0.25)\n    if max_episode_steps is not None:\n        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n    return env\n\n\ndef wrap_deepmind_retro(env, scale=True, frame_stack=4):\n    """"""\n    Configure environment for retro games, using config similar to DeepMind-style Atari in wrap_deepmind\n    """"""\n    env = WarpFrame(env)\n    env = ClipRewardEnv(env)\n    env = FrameStack(env, frame_stack)\n    if scale:\n        env = ScaledFloatFrame(env)\n    return env\n\n\nclass SonicDiscretizer(gym.ActionWrapper):\n    """"""\n    Wrap a gym-retro environment and make it use discrete\n    actions for the Sonic game.\n    """"""\n    def __init__(self, env):\n        super(SonicDiscretizer, self).__init__(env)\n        buttons = [""B"", ""A"", ""MODE"", ""START"", ""UP"", ""DOWN"", ""LEFT"", ""RIGHT"", ""C"", ""Y"", ""X"", ""Z""]\n        actions = [[\'LEFT\'], [\'RIGHT\'], [\'LEFT\', \'DOWN\'], [\'RIGHT\', \'DOWN\'], [\'DOWN\'],\n                   [\'DOWN\', \'B\'], [\'B\']]\n        self._actions = []\n        for action in actions:\n            arr = np.array([False] * 12)\n            for button in action:\n                arr[buttons.index(button)] = True\n            self._actions.append(arr)\n        self.action_space = gym.spaces.Discrete(len(self._actions))\n\n    def action(self, a): # pylint: disable=W0221\n        return self._actions[a].copy()\n\n\nclass RewardScaler(gym.RewardWrapper):\n    """"""\n    Bring rewards to a reasonable scale for PPO.\n    This is incredibly important and effects performance\n    drastically.\n    """"""\n    def __init__(self, env, scale=0.01):\n        super(RewardScaler, self).__init__(env)\n        self.scale = scale\n\n    def reward(self, reward):\n        return reward * self.scale\n\n\nclass AllowBacktracking(gym.Wrapper):\n    """"""\n    Use deltas in max(X) as the reward, rather than deltas\n    in X. This way, agents are not discouraged too heavily\n    from exploring backwards if there is no way to advance\n    head-on in the level.\n    """"""\n    def __init__(self, env):\n        super(AllowBacktracking, self).__init__(env)\n        self._cur_x = 0\n        self._max_x = 0\n\n    def reset(self, **kwargs): # pylint: disable=E0202\n        self._cur_x = 0\n        self._max_x = 0\n        return self.env.reset(**kwargs)\n\n    def step(self, action): # pylint: disable=E0202\n        obs, rew, done, info = self.env.step(action)\n        self._cur_x += rew\n        rew = max(0, self._cur_x - self._max_x)\n        self._max_x = max(self._max_x, self._cur_x)\n        return obs, rew, done, info\n'"
vel/openai/baselines/common/running_mean_std.py,0,"b""import numpy as np\n\n\nclass RunningMeanStd(object):\n    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n    def __init__(self, epsilon=1e-4, shape=()):\n        self.mean = np.zeros(shape, 'float64')\n        self.var = np.ones(shape, 'float64')\n        self.count = epsilon\n\n    def update(self, x):\n        batch_mean = np.mean(x, axis=0)\n        batch_var = np.var(x, axis=0)\n        batch_count = x.shape[0]\n        self.update_from_moments(batch_mean, batch_var, batch_count)\n\n    def update_from_moments(self, batch_mean, batch_var, batch_count):\n        delta = batch_mean - self.mean\n        tot_count = self.count + batch_count\n\n        new_mean = self.mean + delta * batch_count / tot_count\n        m_a = self.var * self.count\n        m_b = batch_var * batch_count\n        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)\n        new_var = M2 / (self.count + batch_count)\n\n        new_count = batch_count + self.count\n\n        self.mean = new_mean\n        self.var = new_var\n        self.count = new_count\n\n"""
vel/openai/baselines/common/tile_images.py,0,"b'import numpy as np\n\n\ndef tile_images(img_nhwc):\n    """"""\n    Tile N images into one big PxQ image\n    (P,Q) are chosen to be as close as possible, and if N\n    is square, then P=Q.\n\n    input: img_nhwc, list or array of images, ndim=4 once turned into array\n        n = batch index, h = height, w = width, c = channel\n    returns:\n        bigim_HWc, ndarray with ndim=3\n    """"""\n    img_nhwc = np.asarray(img_nhwc)\n    N, h, w, c = img_nhwc.shape\n    H = int(np.ceil(np.sqrt(N)))\n    W = int(np.ceil(float(N)/H))\n    img_nhwc = np.array(list(img_nhwc) + [img_nhwc[0]*0 for _ in range(N, H*W)])\n    img_HWhwc = img_nhwc.reshape(H, W, h, w, c)\n    img_HhWwc = img_HWhwc.transpose(0, 2, 1, 3, 4)\n    img_Hh_Ww_c = img_HhWwc.reshape(H*h, W*w, c)\n    return img_Hh_Ww_c\n'"
vel/rl/algo/policy_gradient/__init__.py,0,b''
vel/rl/algo/policy_gradient/a2c.py,4,"b'import torch\nimport torch.nn.functional as F\n\nfrom vel.api.metrics.averaging_metric import AveragingNamedMetric\nfrom vel.math.functions import explained_variance\nfrom vel.rl.api import OptimizerAlgoBase, Rollout, Trajectories\nfrom vel.rl.discount_bootstrap import discount_bootstrap_gae\n\n\nclass A2CPolicyGradient(OptimizerAlgoBase):\n    """""" Simplest policy gradient - calculate loss as an advantage of an actor versus value function """"""\n    def __init__(self, entropy_coefficient, value_coefficient, max_grad_norm, discount_factor: float, gae_lambda=1.0):\n        super().__init__(max_grad_norm)\n\n        self.entropy_coefficient = entropy_coefficient\n        self.value_coefficient = value_coefficient\n        self.gae_lambda = gae_lambda\n        self.discount_factor = discount_factor\n\n    def process_rollout(self, batch_info, rollout: Rollout):\n        """""" Process rollout for ALGO before any chunking/shuffling  """"""\n        assert isinstance(rollout, Trajectories), ""A2C requires trajectory rollouts""\n\n        advantages = discount_bootstrap_gae(\n            rewards_buffer=rollout.transition_tensors[\'rewards\'],\n            dones_buffer=rollout.transition_tensors[\'dones\'],\n            values_buffer=rollout.transition_tensors[\'values\'],\n            final_values=rollout.rollout_tensors[\'final_values\'],\n            discount_factor=self.discount_factor,\n            gae_lambda=self.gae_lambda,\n            number_of_steps=rollout.num_steps\n        )\n\n        returns = advantages + rollout.transition_tensors[\'values\']\n\n        rollout.transition_tensors[\'advantages\'] = advantages\n        rollout.transition_tensors[\'returns\'] = returns\n\n        return rollout\n\n    def calculate_gradient(self, batch_info, device, model, rollout):\n        """""" Calculate loss of the supplied rollout """"""\n        evaluator = model.evaluate(rollout)\n\n        # Use evaluator interface to get the what we are interested in from the model\n        advantages = evaluator.get(\'rollout:advantages\')\n        returns = evaluator.get(\'rollout:returns\')\n        rollout_values = evaluator.get(\'rollout:values\')\n\n        logprobs = evaluator.get(\'model:action:logprobs\')\n        values = evaluator.get(\'model:values\')\n        entropy = evaluator.get(\'model:entropy\')\n\n        # Actual calculations. Pretty trivial\n        policy_loss = -torch.mean(advantages * logprobs)\n        value_loss = 0.5 * F.mse_loss(values, returns)\n        policy_entropy = torch.mean(entropy)\n\n        loss_value = (\n            policy_loss - self.entropy_coefficient * policy_entropy + self.value_coefficient * value_loss\n        )\n\n        loss_value.backward()\n\n        return {\n            \'policy_loss\': policy_loss.item(),\n            \'value_loss\': value_loss.item(),\n            \'policy_entropy\': policy_entropy.item(),\n            \'advantage_norm\': torch.norm(advantages).item(),\n            \'explained_variance\': explained_variance(returns, rollout_values)\n        }\n\n    def metrics(self) -> list:\n        """""" List of metrics to track for this learning process """"""\n        return [\n            AveragingNamedMetric(""value_loss""),\n            AveragingNamedMetric(""policy_entropy""),\n            AveragingNamedMetric(""policy_loss""),\n            AveragingNamedMetric(""grad_norm""),\n            AveragingNamedMetric(""advantage_norm""),\n            AveragingNamedMetric(""explained_variance"")\n        ]\n\n\ndef create(entropy_coefficient, value_coefficient, max_grad_norm, discount_factor, gae_lambda=1.0):\n    """""" Vel factory function """"""\n    return A2CPolicyGradient(\n        entropy_coefficient,\n        value_coefficient,\n        max_grad_norm,\n        discount_factor,\n        gae_lambda\n    )\n'"
vel/rl/algo/policy_gradient/acer.py,18,"b'import torch\nimport torch.nn.functional as F\n\nfrom vel.api.metrics.averaging_metric import AveragingNamedMetric\nfrom vel.rl.api import Trajectories, OptimizerAlgoBase\n\n\ndef select_indices(tensor, indices):\n    """""" Select indices from tensor """"""\n    return tensor.gather(1, indices.unsqueeze(1)).squeeze()\n\n\nclass AcerPolicyGradient(OptimizerAlgoBase):\n    """""" Actor-Critic with Experience Replay - policy gradient calculations """"""\n\n    def __init__(self, model_factory, discount_factor, trust_region: bool = True, entropy_coefficient: float = 0.01,\n                 q_coefficient: float = 0.5, rho_cap: float = 10.0, retrace_rho_cap: float = 1.0,\n                 max_grad_norm: float = None, average_model_alpha: float = 0.99, trust_region_delta: float = 1.0):\n        super().__init__(max_grad_norm)\n\n        self.discount_factor = discount_factor\n\n        self.trust_region = trust_region\n        self.model_factory = model_factory\n\n        self.entropy_coefficient = entropy_coefficient\n        self.q_coefficient = q_coefficient\n\n        self.rho_cap = rho_cap\n        self.retrace_rho_cap = retrace_rho_cap\n\n        # Trust region settings\n        self.average_model = None\n        self.average_model_alpha = average_model_alpha\n        self.trust_region_delta = trust_region_delta\n\n    def initialize(self, training_info, model, environment, device):\n        """""" Initialize policy gradient from reinforcer settings """"""\n        if self.trust_region:\n            self.average_model = self.model_factory.instantiate(action_space=environment.action_space).to(device)\n            self.average_model.load_state_dict(model.state_dict())\n\n    def update_average_model(self, model):\n        """""" Update weights of the average model with new model observation """"""\n        for model_param, average_param in zip(model.parameters(), self.average_model.parameters()):\n            # EWMA average model update\n            average_param.data.mul_(self.average_model_alpha).add_(model_param.data * (1 - self.average_model_alpha))\n\n    def calculate_gradient(self, batch_info, device, model, rollout):\n        """""" Calculate loss of the supplied rollout """"""\n        assert isinstance(rollout, Trajectories), ""ACER algorithm requires trajectory input""\n\n        local_epsilon = 1e-6\n\n        evaluator = model.evaluate(rollout)\n\n        actions = evaluator.get(\'rollout:actions\')\n        rollout_probabilities = torch.exp(evaluator.get(\'rollout:logprobs\'))\n\n        # We calculate the trust-region update with respect to the average model\n        if self.trust_region:\n            self.update_average_model(model)\n\n        logprobs = evaluator.get(\'model:logprobs\')\n        q = evaluator.get(\'model:q\')\n\n        # Selected action values\n        action_logprobs = select_indices(logprobs, actions)\n        action_q = select_indices(q, actions)\n\n        # We only want to propagate gradients through specific variables\n        with torch.no_grad():\n            model_probabilities = torch.exp(logprobs)\n\n            # Importance sampling correction - we must find the quotient of probabilities\n            rho = model_probabilities / (rollout_probabilities + local_epsilon)\n\n            # Probability quotient only for selected actions\n            actions_rho = select_indices(rho, actions)\n\n            # Calculate policy state values\n            model_state_values = (model_probabilities * q).sum(dim=1)\n\n            trajectory_rewards = rollout.transition_tensors[\'rewards\']\n            trajectory_dones = rollout.transition_tensors[\'dones\']\n\n            q_retraced = self.retrace(\n                trajectory_rewards,\n                trajectory_dones,\n                action_q.reshape(trajectory_rewards.size()),\n                model_state_values.reshape(trajectory_rewards.size()),\n                actions_rho.reshape(trajectory_rewards.size()),\n                rollout.rollout_tensors[\'final_values\']\n            ).flatten()\n\n            advantages = q_retraced - model_state_values\n            importance_sampling_coefficient = torch.min(actions_rho, self.rho_cap * torch.ones_like(actions_rho))\n\n            explained_variance = 1 - torch.var(q_retraced - action_q) / torch.var(q_retraced)\n\n        # Entropy of the policy distribution\n        policy_entropy = torch.mean(model.entropy(logprobs))\n        policy_gradient_loss = -torch.mean(advantages * importance_sampling_coefficient * action_logprobs)\n\n        # Policy gradient bias correction\n        with torch.no_grad():\n            advantages_bias_correction = q - model_state_values.view(model_probabilities.size(0), 1)\n            bias_correction_coefficient = F.relu(1.0 - self.rho_cap / (rho + local_epsilon))\n\n        # This sum is an expectation with respect to action probabilities according to model policy\n        policy_gradient_bias_correction_gain = torch.sum(\n            logprobs * bias_correction_coefficient * advantages_bias_correction * model_probabilities,\n            dim=1\n        )\n\n        policy_gradient_bias_correction_loss = - torch.mean(policy_gradient_bias_correction_gain)\n\n        policy_loss = policy_gradient_loss + policy_gradient_bias_correction_loss\n\n        q_function_loss = 0.5 * F.mse_loss(action_q, q_retraced)\n\n        if self.trust_region:\n            with torch.no_grad():\n                average_evaluator = self.average_model.evaluate(rollout)\n                average_action_logits = average_evaluator.get(\'model:logprobs\')\n\n            actor_loss = policy_loss - self.entropy_coefficient * policy_entropy\n            q_loss = self.q_coefficient * q_function_loss\n\n            actor_gradient = torch.autograd.grad(-actor_loss, logprobs, retain_graph=True)[0]\n\n            # kl_divergence = model.kl_divergence(average_action_logits, action_logits).mean()\n            # kl_divergence_grad = torch.autograd.grad(kl_divergence, action_logits, retain_graph=True)\n\n            # Analytically calculated derivative of KL divergence on logits\n            # That makes it hardcoded for discrete action spaces\n            kl_divergence_grad_symbolic = - torch.exp(average_action_logits) / logprobs.size(0)\n\n            k_dot_g = (actor_gradient * kl_divergence_grad_symbolic).sum(dim=-1)\n            k_dot_k = (kl_divergence_grad_symbolic ** 2).sum(dim=-1)\n\n            adjustment = (k_dot_g - self.trust_region_delta) / k_dot_k\n            adjustment_clipped = adjustment.clamp(min=0.0)\n\n            actor_gradient_updated = actor_gradient - adjustment_clipped.view(adjustment_clipped.size(0), 1)\n\n            # Populate gradient from the newly updated fn\n            logprobs.backward(gradient=-actor_gradient_updated, retain_graph=True)\n            q_loss.backward(retain_graph=True)\n        else:\n            # Just populate gradient from the loss\n            loss = policy_loss + self.q_coefficient * q_function_loss - self.entropy_coefficient * policy_entropy\n\n            loss.backward()\n\n        return {\n            \'policy_loss\': policy_loss.item(),\n            \'policy_gradient_loss\': policy_gradient_loss.item(),\n            \'policy_gradient_bias_correction\': policy_gradient_bias_correction_loss.item(),\n            \'avg_q_selected\': action_q.mean().item(),\n            \'avg_q_retraced\': q_retraced.mean().item(),\n            \'q_loss\': q_function_loss.item(),\n            \'policy_entropy\': policy_entropy.item(),\n            \'advantage_norm\': torch.norm(advantages).item(),\n            \'explained_variance\': explained_variance.item(),\n            \'model_prob_std\': model_probabilities.std().item(),\n            \'rollout_prob_std\': rollout_probabilities.std().item()\n        }\n\n    def retrace(self, rewards, dones, q_values, state_values, rho, final_values):\n        """""" Calculate Q retraced targets """"""\n        rho_bar = torch.min(torch.ones_like(rho) * self.retrace_rho_cap, rho)\n\n        q_retraced_buffer = torch.zeros_like(rewards)\n\n        next_value = final_values\n\n        for i in reversed(range(rewards.size(0))):\n            q_retraced = rewards[i] + self.discount_factor * next_value * (1.0 - dones[i])\n\n            # Next iteration\n            next_value = rho_bar[i] * (q_retraced - q_values[i]) + state_values[i]\n\n            q_retraced_buffer[i] = q_retraced\n\n        return q_retraced_buffer\n\n    def metrics(self) -> list:\n        """""" List of metrics to track for this learning process """"""\n        return [\n            AveragingNamedMetric(""q_loss""),\n            AveragingNamedMetric(""policy_entropy""),\n            AveragingNamedMetric(""policy_loss""),\n            AveragingNamedMetric(""policy_gradient_loss""),\n            AveragingNamedMetric(""policy_gradient_bias_correction""),\n            AveragingNamedMetric(""explained_variance""),\n            AveragingNamedMetric(""advantage_norm""),\n            AveragingNamedMetric(""grad_norm""),\n            AveragingNamedMetric(""model_prob_std""),\n            AveragingNamedMetric(""rollout_prob_std""),\n            AveragingNamedMetric(""avg_q_selected""),\n            AveragingNamedMetric(""avg_q_retraced"")\n        ]\n\n\ndef create(model, trust_region, entropy_coefficient, q_coefficient, max_grad_norm, discount_factor,\n           rho_cap=10.0, retrace_rho_cap=1.0, average_model_alpha=0.99, trust_region_delta=1.0):\n    """""" Vel factory function """"""\n    return AcerPolicyGradient(\n        trust_region=trust_region,\n        model_factory=model,\n        entropy_coefficient=entropy_coefficient,\n        q_coefficient=q_coefficient,\n        rho_cap=rho_cap,\n        retrace_rho_cap=retrace_rho_cap,\n        max_grad_norm=max_grad_norm,\n        discount_factor=discount_factor,\n        average_model_alpha=average_model_alpha,\n        trust_region_delta=trust_region_delta\n    )\n'"
vel/rl/algo/policy_gradient/ddpg.py,4,"b'import torch\nimport typing\nimport torch.autograd\nimport torch.nn.functional as F\n\nfrom vel.rl.api import OptimizerAlgoBase\nfrom vel.api.metrics.averaging_metric import AveragingNamedMetric\n\n\nclass DeepDeterministicPolicyGradient(OptimizerAlgoBase):\n    """""" Deep Deterministic Policy Gradient (DDPG) - policy gradient calculations """"""\n\n    def __init__(self, model_factory, discount_factor: float, tau: float, max_grad_norm: typing.Optional[float]=None):\n        super().__init__(max_grad_norm)\n\n        self.model_factory = model_factory\n        self.tau = tau\n        self.discount_factor = discount_factor\n\n        self.target_model = None\n\n    def initialize(self, training_info, model, environment, device):\n        """""" Initialize algo from reinforcer settings """"""\n        self.target_model = self.model_factory.instantiate(action_space=environment.action_space).to(device)\n        self.target_model.load_state_dict(model.state_dict())\n        self.target_model.eval()\n\n    def calculate_gradient(self, batch_info, device, model, rollout):\n        """""" Calculate loss of the supplied rollout """"""\n        rollout = rollout.to_transitions()\n\n        dones = rollout.batch_tensor(\'dones\')\n        rewards = rollout.batch_tensor(\'rewards\')\n        observations_next = rollout.batch_tensor(\'observations_next\')\n        actions = rollout.batch_tensor(\'actions\')\n        observations = rollout.batch_tensor(\'observations\')\n\n        # Calculate value loss - or critic loss\n        with torch.no_grad():\n            target_next_value = self.target_model.value(observations_next)\n            target_value = rewards + (1.0 - dones) * self.discount_factor * target_next_value\n\n        # Value estimation error vs the target network\n        model_value = model.value(observations, actions)\n        value_loss = F.mse_loss(model_value, target_value)\n\n        # It may seem a bit tricky what I\'m doing here, but the underlying idea is simple\n        # All other implementations I found keep two separate optimizers for actor and critic\n        # and update them separately\n        # What I\'m trying to do is to optimize them both with a single optimizer\n        # but I need to make sure gradients flow correctly\n        # From critic loss to critic network only and from actor loss to actor network only\n\n        # Backpropagate value loss to critic only\n        value_loss.backward()\n\n        model_action = model.action(observations)\n        model_action_value = model.value(observations, model_action)\n\n        policy_loss = -model_action_value.mean()\n\n        model_action_grad = torch.autograd.grad(policy_loss, model_action)[0]\n\n        # Backpropagate actor loss to actor only\n        model_action.backward(gradient=model_action_grad)\n\n        return {\n            \'policy_loss\': policy_loss.item(),\n            \'value_loss\': value_loss.item(),\n        }\n\n    def post_optimization_step(self, batch_info, device, model, rollout):\n        """""" Steps to take after optimization has been done""""""\n        # Update target model\n        for model_param, target_param in zip(model.parameters(), self.target_model.parameters()):\n            # EWMA average model update\n            target_param.data.mul_(1 - self.tau).add_(model_param.data * self.tau)\n\n    def metrics(self) -> list:\n        """""" List of metrics to track for this learning process """"""\n        return [\n            AveragingNamedMetric(""value_loss""),\n            AveragingNamedMetric(""policy_loss""),\n        ]\n\n\ndef create(model, discount_factor: float, tau: float, max_grad_norm: float=None):\n    """""" Vel factory function """"""\n    return DeepDeterministicPolicyGradient(\n        tau=tau,\n        discount_factor=discount_factor,\n        model_factory=model,\n        max_grad_norm=max_grad_norm\n    )\n'"
vel/rl/algo/policy_gradient/ppo.py,10,"b'import torch\n\nimport numbers\n\nfrom vel.api.metrics.averaging_metric import AveragingNamedMetric\nfrom vel.math.functions import explained_variance\nfrom vel.rl.api import OptimizerAlgoBase, Rollout, Trajectories\nfrom vel.rl.discount_bootstrap import discount_bootstrap_gae\nfrom vel.schedules.constant import ConstantSchedule\n\n\nclass PpoPolicyGradient(OptimizerAlgoBase):\n    """""" Proximal Policy Optimization - https://arxiv.org/abs/1707.06347 """"""\n    def __init__(self, entropy_coefficient, value_coefficient, cliprange, max_grad_norm, discount_factor: float,\n                 normalize_advantage: bool=True, gae_lambda: float=1.0):\n        super().__init__(max_grad_norm)\n\n        self.entropy_coefficient = entropy_coefficient\n        self.value_coefficient = value_coefficient\n        self.normalize_advantage = normalize_advantage\n\n        if isinstance(cliprange, numbers.Number):\n            self.cliprange = ConstantSchedule(cliprange)\n        else:\n            self.cliprange = cliprange\n\n        self.gae_lambda = gae_lambda\n        self.discount_factor = discount_factor\n\n    def process_rollout(self, batch_info, rollout: Rollout):\n        """""" Process rollout for ALGO before any chunking/shuffling  """"""\n        assert isinstance(rollout, Trajectories), ""PPO requires trajectory rollouts""\n\n        advantages = discount_bootstrap_gae(\n            rewards_buffer=rollout.transition_tensors[\'rewards\'],\n            dones_buffer=rollout.transition_tensors[\'dones\'],\n            values_buffer=rollout.transition_tensors[\'values\'],\n            final_values=rollout.rollout_tensors[\'final_values\'],\n            discount_factor=self.discount_factor,\n            gae_lambda=self.gae_lambda,\n            number_of_steps=rollout.num_steps\n        )\n\n        returns = advantages + rollout.transition_tensors[\'values\']\n\n        rollout.transition_tensors[\'advantages\'] = advantages\n        rollout.transition_tensors[\'returns\'] = returns\n\n        return rollout\n\n    def calculate_gradient(self, batch_info, device, model, rollout):\n        """""" Calculate loss of the supplied rollout """"""\n        evaluator = model.evaluate(rollout)\n\n        # Part 0.0 - Rollout values\n        advantages = evaluator.get(\'rollout:advantages\')\n        rollout_values = evaluator.get(\'rollout:values\')\n        rollout_action_logprobs = evaluator.get(\'rollout:action:logprobs\')\n        returns = evaluator.get(\'rollout:returns\')\n\n        # PART 0.1 - Model evaluation\n        entropy = evaluator.get(\'model:entropy\')\n        model_values = evaluator.get(\'model:values\')\n        model_action_logprobs = evaluator.get(\'model:action:logprobs\')\n\n        # Select the cliprange\n        current_cliprange = self.cliprange.value(batch_info[\'progress\'])\n\n        # Normalize the advantages?\n        if self.normalize_advantage:\n            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        # PART 1 - policy entropy\n        policy_entropy = torch.mean(entropy)\n\n        # PART 2 - value function\n        value_output_clipped = rollout_values + torch.clamp(\n            model_values - rollout_values, -current_cliprange, current_cliprange\n        )\n        value_loss_part1 = (model_values - returns).pow(2)\n        value_loss_part2 = (value_output_clipped - returns).pow(2)\n        value_loss = 0.5 * torch.mean(torch.max(value_loss_part1, value_loss_part2))\n\n        # PART 3 - policy gradient loss\n        ratio = torch.exp(model_action_logprobs - rollout_action_logprobs)\n\n        pg_loss_part1 = -advantages * ratio\n        pg_loss_part2 = -advantages * torch.clamp(ratio, 1.0 - current_cliprange, 1.0 + current_cliprange)\n        policy_loss = torch.mean(torch.max(pg_loss_part1, pg_loss_part2))\n\n        loss_value = (\n            policy_loss - self.entropy_coefficient * policy_entropy + self.value_coefficient * value_loss\n        )\n\n        loss_value.backward()\n\n        with torch.no_grad():\n            approx_kl_divergence = 0.5 * torch.mean((model_action_logprobs - rollout_action_logprobs).pow(2))\n            clip_fraction = torch.mean((torch.abs(ratio - 1.0) > current_cliprange).to(dtype=torch.float))\n\n        return {\n            \'policy_loss\': policy_loss.item(),\n            \'value_loss\': value_loss.item(),\n            \'policy_entropy\': policy_entropy.item(),\n            \'approx_kl_divergence\': approx_kl_divergence.item(),\n            \'clip_fraction\': clip_fraction.item(),\n            \'advantage_norm\': torch.norm(advantages).item(),\n            \'explained_variance\': explained_variance(returns, rollout_values)\n        }\n\n    def metrics(self) -> list:\n        """""" List of metrics to track for this learning process """"""\n        return [\n            AveragingNamedMetric(""policy_loss""),\n            AveragingNamedMetric(""value_loss""),\n            AveragingNamedMetric(""policy_entropy""),\n            AveragingNamedMetric(""approx_kl_divergence""),\n            AveragingNamedMetric(""clip_fraction""),\n            AveragingNamedMetric(""grad_norm""),\n            AveragingNamedMetric(""advantage_norm""),\n            AveragingNamedMetric(""explained_variance"")\n        ]\n\n\ndef create(entropy_coefficient, value_coefficient, cliprange, max_grad_norm, discount_factor,\n           normalize_advantage=True, gae_lambda=1.0):\n    """""" Vel factory function """"""\n    return PpoPolicyGradient(\n        entropy_coefficient, value_coefficient, cliprange, max_grad_norm,\n        discount_factor=discount_factor,\n        normalize_advantage=normalize_advantage,\n        gae_lambda=gae_lambda\n    )\n'"
vel/rl/algo/policy_gradient/trpo.py,21,"b'import numpy as np\nimport torch\nimport torch.autograd as autograd\nimport torch.nn.functional as F\nimport torch.nn.utils\n\nfrom vel.api.metrics.averaging_metric import AveragingNamedMetric\nfrom vel.math.functions import explained_variance\nfrom vel.rl.api import AlgoBase, Rollout, Trajectories\nfrom vel.rl.discount_bootstrap import discount_bootstrap_gae\n\n\ndef p2v(params):\n    """""" Parameters to vector - shorthand utility version """"""\n    return torch.nn.utils.parameters_to_vector(params)\n\n\ndef v2p(vector, params):\n    """""" Vector to parameters - shorthand utility version """"""\n    return torch.nn.utils.vector_to_parameters(vector, params)\n\n\ndef conjugate_gradient_method(matrix_vector_operator, loss_gradient, nsteps, rdotr_tol=1e-10):\n    """""" Conjugate gradient algorithm """"""\n    x = torch.zeros_like(loss_gradient)\n\n    r = loss_gradient.clone()\n    p = loss_gradient.clone()\n\n    rdotr = torch.dot(r, r)\n\n    for i in range(nsteps):\n        Avp = matrix_vector_operator(p)\n        alpha = rdotr / torch.dot(p, Avp)\n\n        x += alpha * p\n        r -= alpha * Avp\n\n        new_rdotr = torch.dot(r, r)\n        betta = new_rdotr / rdotr\n        p = r + betta * p\n        rdotr = new_rdotr\n\n        if rdotr < rdotr_tol:\n            break\n\n    return x\n\n\nclass TrpoPolicyGradient(AlgoBase):\n    """""" Trust Region Policy Optimization - https://arxiv.org/abs/1502.05477 """"""\n\n    def __init__(self, max_kl, cg_iters, line_search_iters, cg_damping, entropy_coef, vf_iters,\n                 discount_factor, gae_lambda, improvement_acceptance_ratio, max_grad_norm):\n        self.mak_kl = max_kl\n        self.cg_iters = cg_iters\n        self.line_search_iters = line_search_iters\n        self.cg_damping = cg_damping\n        self.entropy_coef = entropy_coef\n        self.vf_iters = vf_iters\n        self.discount_factor = discount_factor\n        self.gae_lambda = gae_lambda\n        self.improvement_acceptance_ratio = improvement_acceptance_ratio\n        self.max_grad_norm = max_grad_norm\n\n    def process_rollout(self, batch_info, rollout: Rollout):\n        """""" Process rollout for ALGO before any chunking/shuffling  """"""\n        assert isinstance(rollout, Trajectories), ""TRPO requires trajectory rollouts""\n\n        advantages = discount_bootstrap_gae(\n            rewards_buffer=rollout.transition_tensors[\'rewards\'],\n            dones_buffer=rollout.transition_tensors[\'dones\'],\n            values_buffer=rollout.transition_tensors[\'values\'],\n            final_values=rollout.rollout_tensors[\'final_values\'],\n            discount_factor=self.discount_factor,\n            gae_lambda=self.gae_lambda,\n            number_of_steps=rollout.num_steps\n        )\n\n        returns = advantages + rollout.transition_tensors[\'values\']\n\n        rollout.transition_tensors[\'advantages\'] = advantages\n        rollout.transition_tensors[\'returns\'] = returns\n\n        return rollout\n\n    def optimizer_step(self, batch_info, device, model, rollout):\n        """""" Single optimization step for a model """"""\n        rollout = rollout.to_transitions()\n\n        # This algorithm makes quote strong assumptions about how does the model look\n        # so it does not make that much sense to switch to the evaluator interface\n        # As it would be more of a problem than actual benefit\n\n        observations = rollout.batch_tensor(\'observations\')\n        returns = rollout.batch_tensor(\'returns\')\n\n        # Evaluate model on the observations\n        policy_params = model.policy(observations)\n        policy_entropy = torch.mean(model.entropy(policy_params))\n\n        policy_loss = self.calc_policy_loss(model, policy_params, policy_entropy, rollout)\n        policy_grad = p2v(autograd.grad(policy_loss, model.policy_parameters(), retain_graph=True)).detach()\n\n        # Calculate gradient of KL divergence of model with fixed version of itself\n        # Value of kl_divergence will be 0, but what we need is the gradient, actually the 2nd derivarive\n        kl_divergence = torch.mean(model.kl_divergence(policy_params.detach(), policy_params))\n        kl_divergence_gradient = p2v(torch.autograd.grad(kl_divergence, model.policy_parameters(), create_graph=True))\n\n        step_direction = conjugate_gradient_method(\n            matrix_vector_operator=lambda x: self.fisher_vector_product(x, kl_divergence_gradient, model),\n            # Because we want to decrease the loss, we want to go into the direction of -gradient\n            loss_gradient=-policy_grad,\n            nsteps=self.cg_iters\n        )\n\n        shs = 0.5 * step_direction @ self.fisher_vector_product(step_direction, kl_divergence_gradient, model)\n        lm = torch.sqrt(shs / self.mak_kl)\n        full_step = step_direction / lm\n\n        # Because we want to decrease the loss, we want to go into the direction of -gradient\n        expected_improvement = (-policy_grad) @ full_step\n        original_parameter_vec = p2v(model.policy_parameters()).detach_()\n\n        policy_optimization_success, ratio, policy_loss_improvement, new_policy_loss, kl_divergence_step = self.line_search(\n            model, rollout, policy_loss, policy_params, original_parameter_vec, full_step, expected_improvement\n        )\n\n        gradient_norms = []\n\n        for i in range(self.vf_iters):\n            batch_info.optimizer.zero_grad()\n            value_loss = self.value_loss(model, observations, returns)\n\n            value_loss.backward()\n\n            # Gradient clipping\n            if self.max_grad_norm is not None:\n                grad_norm = torch.nn.utils.clip_grad_norm_(\n                    filter(lambda p: p.requires_grad, model.parameters()),\n                    max_norm=self.max_grad_norm\n                )\n\n                gradient_norms.append(grad_norm)\n\n            batch_info.optimizer.step(closure=None)\n\n        if gradient_norms:\n            gradient_norm = np.mean(gradient_norms)\n        else:\n            gradient_norm = 0.0\n\n        # noinspection PyUnboundLocalVariable\n        return {\n            \'new_policy_loss\': new_policy_loss.item(),\n            \'policy_entropy\': policy_entropy.item(),\n            \'value_loss\': value_loss.item(),\n            \'policy_optimization_success\': float(policy_optimization_success),\n            \'policy_improvement_ratio\': ratio.item(),\n            \'kl_divergence_step\': kl_divergence_step.item(),\n            \'policy_loss_improvement\': policy_loss_improvement.item(),\n            \'grad_norm\': gradient_norm,\n            \'advantage_norm\': torch.norm(rollout.batch_tensor(\'advantages\')).item(),\n            \'explained_variance\': explained_variance(returns, rollout.batch_tensor(\'values\'))\n        }\n\n    def line_search(self, model, rollout, original_policy_loss, original_policy_params, original_parameter_vec,\n                    full_step, expected_improvement_full):\n        """""" Find the right stepsize to make sure policy improves """"""\n        current_parameter_vec = original_parameter_vec.clone()\n\n        for idx in range(self.line_search_iters):\n            stepsize = 0.5 ** idx\n\n            new_parameter_vec = current_parameter_vec + stepsize * full_step\n\n            # Update model parameters\n            v2p(new_parameter_vec, model.policy_parameters())\n\n            # Calculate new loss\n            with torch.no_grad():\n                policy_params = model.policy(rollout.batch_tensor(\'observations\'))\n                policy_entropy = torch.mean(model.entropy(policy_params))\n                kl_divergence = torch.mean(model.kl_divergence(original_policy_params, policy_params))\n\n                new_loss = self.calc_policy_loss(model, policy_params, policy_entropy, rollout)\n\n                actual_improvement = original_policy_loss - new_loss\n                expected_improvement = expected_improvement_full * stepsize\n\n                ratio = actual_improvement / expected_improvement\n\n            if kl_divergence.item() > self.mak_kl * 1.5:\n                # KL divergence bound exceeded\n                continue\n            elif ratio < expected_improvement:\n                # Not enough loss improvement\n                continue\n            else:\n                # Optimization successful\n                return True, ratio, actual_improvement, new_loss, kl_divergence\n\n        # Optimization failed, revert to initial parameters\n        v2p(original_parameter_vec, model.policy_parameters())\n        return False, torch.tensor(0.0), torch.tensor(0.0), torch.tensor(0.0), torch.tensor(0.0)\n\n    def fisher_vector_product(self, vector, kl_divergence_gradient, model):\n        """""" Calculate product Hessian @ vector """"""\n        assert not vector.requires_grad, ""Vector must not propagate gradient""\n        dot_product = vector @ kl_divergence_gradient\n\n        # at least one dimension spans across two contiguous subspaces\n        double_gradient = torch.autograd.grad(dot_product, model.policy_parameters(), retain_graph=True)\n        fvp = p2v(x.contiguous() for x in double_gradient)\n\n        return fvp + vector * self.cg_damping\n\n    def value_loss(self, model, observations, discounted_rewards):\n        """""" Loss of value estimator """"""\n        value_outputs = model.value(observations)\n        value_loss = 0.5 * F.mse_loss(value_outputs, discounted_rewards)\n        return value_loss\n\n    def calc_policy_loss(self, model, policy_params, policy_entropy, rollout):\n        """"""\n        Policy gradient loss - calculate from probability distribution\n\n        Calculate surrogate loss - advantage * policy_probability / fixed_initial_policy_probability\n\n        Because we operate with logarithm of -probability (neglogp) we do\n        - advantage * exp(fixed_neglogps - model_neglogps)\n        """"""\n        actions = rollout.batch_tensor(\'actions\')\n        advantages = rollout.batch_tensor(\'advantages\')\n        fixed_logprobs = rollout.batch_tensor(\'action:logprobs\')\n\n        model_logprobs = model.logprob(actions, policy_params)\n\n        # Normalize advantages\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        # We put - in front because we want to maximize the surrogate objective\n        policy_loss = -advantages * torch.exp(model_logprobs - fixed_logprobs)\n\n        return policy_loss.mean() - policy_entropy * self.entropy_coef\n\n    def metrics(self) -> list:\n        """""" List of metrics to track for this learning process """"""\n        return [\n            AveragingNamedMetric(""new_policy_loss""),\n            AveragingNamedMetric(""policy_entropy""),\n            AveragingNamedMetric(""value_loss""),\n            AveragingNamedMetric(""policy_optimization_success""),\n            AveragingNamedMetric(""policy_improvement_ratio""),\n            AveragingNamedMetric(""kl_divergence_step""),\n            AveragingNamedMetric(""policy_loss_improvement""),\n            AveragingNamedMetric(""grad_norm""),\n            AveragingNamedMetric(""advantage_norm""),\n            AveragingNamedMetric(""explained_variance"")\n        ]\n\n\ndef create(max_kl, cg_iters, line_search_iters, cg_damping, entropy_coef, vf_iters, discount_factor,\n           gae_lambda=1.0, improvement_acceptance_ratio=0.1, max_grad_norm=0.5):\n    """""" Vel factory function """"""\n    return TrpoPolicyGradient(\n        max_kl, int(cg_iters), int(line_search_iters), cg_damping, entropy_coef, vf_iters,\n        discount_factor=discount_factor,\n        gae_lambda=gae_lambda,\n        improvement_acceptance_ratio=improvement_acceptance_ratio,\n        max_grad_norm=max_grad_norm\n    )\n'"
vel/rl/buffers/backend/__init__.py,0,b''
vel/rl/buffers/backend/circular_buffer_backend.py,0,"b'import gym\nimport numpy as np\n\nfrom vel.exceptions import VelException\n\n\nclass CircularBufferBackend:\n    """""" Backend class for replay buffer that uses a circular buffer - new experience overwrites the oldest one """"""\n\n    def __init__(self, buffer_capacity: int, observation_space: gym.Space, action_space: gym.Space, extra_data=None):\n        # Maximum number of items in the buffer\n        self.buffer_capacity = buffer_capacity\n\n        # How many elements have been inserted in the buffer\n        self.current_size = 0\n\n        # Index of last inserted element\n        self.current_idx = -1\n\n        # Data buffers\n        self.state_buffer = np.zeros(\n            [self.buffer_capacity] + list(observation_space.shape),\n            dtype=observation_space.dtype\n        )\n\n        self.action_buffer = np.zeros([self.buffer_capacity] + list(action_space.shape), dtype=action_space.dtype)\n        self.reward_buffer = np.zeros([self.buffer_capacity], dtype=np.float32)\n        self.dones_buffer = np.zeros([self.buffer_capacity], dtype=bool)\n\n        self.extra_data = {} if extra_data is None else extra_data\n\n        # Just a sentinel to simplify further calculations\n        self.dones_buffer[self.current_idx] = True\n\n    def store_transition(self, frame, action, reward, done, extra_info=None):\n        """""" Store given transition in the backend """"""\n        self.current_idx = (self.current_idx + 1) % self.buffer_capacity\n\n        self.state_buffer[self.current_idx] = frame\n        self.action_buffer[self.current_idx] = action\n        self.reward_buffer[self.current_idx] = reward\n        self.dones_buffer[self.current_idx] = done\n\n        for name in self.extra_data:\n            self.extra_data[name][self.current_idx] = extra_info[name]\n\n        if self.current_size < self.buffer_capacity:\n            self.current_size += 1\n\n        return self.current_idx\n\n    def get_frame(self, idx, history_length=1):\n        """""" Return frame from the buffer """"""\n        if idx >= self.current_size:\n            raise VelException(""Requested frame beyond the size of the buffer"")\n\n        if history_length > 1:\n            assert self.state_buffer.shape[-1] == 1, \\\n                ""State buffer must have last dimension of 1 if we want frame history""\n\n        accumulator = []\n\n        last_frame = self.state_buffer[idx]\n        accumulator.append(last_frame)\n\n        for i in range(history_length - 1):\n            prev_idx = (idx - 1) % self.buffer_capacity\n\n            if prev_idx == self.current_idx:\n                raise VelException(""Cannot provide enough history for the frame"")\n            elif self.dones_buffer[prev_idx]:\n                # If previous frame was done - just append zeroes\n                accumulator.append(np.zeros_like(last_frame))\n            else:\n                idx = prev_idx\n                accumulator.append(self.state_buffer[idx])\n\n        # We\'re pushing the elements in reverse order\n        return np.concatenate(accumulator[::-1], axis=-1)\n\n    def get_transition(self, frame_idx, history_length=1):\n        """""" Single transition with given index """"""\n        past_frame, future_frame = self.get_frame_with_future(frame_idx, history_length)\n\n        data_dict = {\n            \'observations\': past_frame,\n            \'observations_next\': future_frame,\n            \'actions\': self.action_buffer[frame_idx],\n            \'rewards\': self.reward_buffer[frame_idx],\n            \'dones\': self.dones_buffer[frame_idx],\n        }\n\n        for name in self.extra_data:\n            data_dict[name] = self.extra_data[name][frame_idx]\n\n        return data_dict\n\n    def get_frame_with_future(self, frame_idx, history_length=1):\n        """""" Return frame from the buffer together with the next frame """"""\n        if frame_idx == self.current_idx:\n            raise VelException(""Cannot provide enough future for the frame"")\n\n        past_frame = self.get_frame(frame_idx, history_length)\n\n        if history_length > 1:\n            assert self.state_buffer.shape[-1] == 1, \\\n                ""State buffer must have last dimension of 1 if we want frame history""\n\n        if not self.dones_buffer[frame_idx]:\n            next_idx = (frame_idx + 1) % self.buffer_capacity\n            next_frame = self.state_buffer[next_idx]\n        else:\n            next_idx = (frame_idx + 1) % self.buffer_capacity\n            next_frame = np.zeros_like(self.state_buffer[next_idx])\n\n        if history_length > 1:\n            future_frame = np.concatenate([\n                past_frame.take(indices=np.arange(1, past_frame.shape[-1]), axis=-1), next_frame\n            ], axis=-1)\n        else:\n            future_frame = next_frame\n\n        return past_frame, future_frame\n\n    def get_transitions(self, indexes, history_length=1):\n        """""" Return batch with given indexes """"""\n        frame_batch_shape = (\n                [indexes.shape[0]]\n                + list(self.state_buffer.shape[1:-1])\n                + [self.state_buffer.shape[-1] * history_length]\n        )\n\n        past_frame_buffer = np.zeros(frame_batch_shape, dtype=self.state_buffer.dtype)\n        future_frame_buffer = np.zeros(frame_batch_shape, dtype=self.state_buffer.dtype)\n\n        for buffer_idx, frame_idx in enumerate(indexes):\n            past_frame_buffer[buffer_idx], future_frame_buffer[buffer_idx] = self.get_frame_with_future(\n                frame_idx, history_length\n            )\n\n        actions = self.action_buffer[indexes]\n        rewards = self.reward_buffer[indexes]\n        dones = self.dones_buffer[indexes]\n\n        data_dict = {\n            \'observations\': past_frame_buffer,\n            \'actions\': actions,\n            \'rewards\': rewards,\n            \'observations_next\': future_frame_buffer,\n            \'dones\': dones,\n        }\n\n        for name in self.extra_data:\n            data_dict[name] = self.extra_data[name][indexes]\n\n        return data_dict\n\n    def get_trajectories(self, index, rollout_length, history_length):\n        """""" Return batch consisting of *consecutive* transitions """"""\n        indexes = np.arange(index - rollout_length + 1, index + 1, dtype=int)\n        return self.get_transitions(indexes, history_length)\n\n    def sample_batch_transitions(self, batch_size, history_length):\n        """""" Return indexes of next sample""""""\n        # Sample from up to total size\n        if self.current_size < self.buffer_capacity:\n            # -1 because we cannot take the last one\n            return np.random.choice(self.current_size - 1, batch_size, replace=False)\n        else:\n            candidate = np.random.choice(self.buffer_capacity, batch_size, replace=False)\n\n            forbidden_ones = (\n                    np.arange(self.current_idx, self.current_idx + history_length)\n                    % self.buffer_capacity\n            )\n\n            # Exclude these frames for learning as they may have some part of history overwritten\n            while any(x in candidate for x in forbidden_ones):\n                candidate = np.random.choice(self.buffer_capacity, batch_size, replace=False)\n\n            return candidate\n\n    def sample_batch_trajectories(self, rollout_length, history_length):\n        """""" Return indexes of next sample """"""\n        # Sample from up to total size\n        if self.current_size < self.buffer_capacity:\n            if rollout_length + 1 > self.current_size:\n                raise VelException(""Not enough elements in the buffer to sample the rollout"")\n\n            # -1 because we cannot take the last one\n            return np.random.choice(self.current_size - rollout_length) + rollout_length - 1\n        else:\n            if rollout_length + history_length > self.current_size:\n                raise VelException(""Not enough elements in the buffer to sample the rollout"")\n\n            candidate = np.random.choice(self.buffer_capacity)\n\n            # These are the elements we cannot draw, as then we don\'t have enough history\n            forbidden_ones = (\n                    np.arange(self.current_idx, self.current_idx + history_length + rollout_length - 1)\n                    % self.buffer_capacity\n            )\n\n            # Exclude these frames for learning as they may have some part of history overwritten\n            while candidate in forbidden_ones:\n                candidate = np.random.choice(self.buffer_capacity)\n\n            return candidate\n'"
vel/rl/buffers/backend/circular_vec_buffer_backend.py,0,"b'import gym\nimport numpy as np\n\nfrom vel.exceptions import VelException\n\n\ndef take_along_axis(large_array, indexes):\n    """""" Take along axis """"""\n    # Reshape indexes into the right shape\n    if len(large_array.shape) > len(indexes.shape):\n        indexes = indexes.reshape(indexes.shape + tuple([1] * (len(large_array.shape) - len(indexes.shape))))\n\n    return np.take_along_axis(large_array, indexes, axis=0)\n\n\nclass CircularVecEnvBufferBackend:\n    """"""\n    Backend class for replay buffer that uses a circular buffer - new experience overwrites the oldest one\n    Version supporting multiple environments.\n\n    Frame stack compensation - if environment has a framestack built in, we will store only the last action\n    """"""\n\n    def __init__(self, buffer_capacity: int, num_envs: int, observation_space: gym.Space, action_space: gym.Space,\n                 frame_stack_compensation: bool=False, frame_history: int=1):\n        # Maximum number of items in the buffer\n        self.buffer_capacity = buffer_capacity\n\n        self.frame_stack_compensation = frame_stack_compensation\n\n        # Number of parallel envs to record\n        self.num_envs = num_envs\n\n        # How many elements have been inserted in the buffer\n        self.current_size = 0\n\n        # Index of last inserted element\n        self.current_idx = -1\n\n        # How many frames are stacked into each observation\n        self.frame_history = frame_history\n\n        # Data buffers\n        if self.frame_stack_compensation:\n            self.state_buffer = np.zeros(\n                [self.buffer_capacity, self.num_envs] + list(observation_space.shape)[:-1] +\n                [observation_space.shape[-1] // self.frame_history],\n                dtype=observation_space.dtype\n            )\n        else:\n            self.state_buffer = np.zeros(\n                [self.buffer_capacity, self.num_envs] + list(observation_space.shape),\n                dtype=observation_space.dtype\n            )\n\n        self.action_buffer = np.zeros(\n            [self.buffer_capacity, self.num_envs] + list(action_space.shape), dtype=action_space.dtype\n        )\n        self.reward_buffer = np.zeros([self.buffer_capacity, self.num_envs], dtype=np.float32)\n        self.dones_buffer = np.zeros([self.buffer_capacity, self.num_envs], dtype=bool)\n\n        self.extra_data = {}\n\n        # Just a sentinel to simplify further calculations\n        self.dones_buffer[self.current_idx] = True\n\n    def store_transition(self, frame, action, reward, done, extra_info=None):\n        """""" Store given transition in the backend """"""\n        self.current_idx = (self.current_idx + 1) % self.buffer_capacity\n\n        if self.frame_stack_compensation:\n            # Compensate for frame stack built into the environment\n            idx_range = np.arange(-frame.shape[-1] // self.frame_history, 0)\n            frame = np.take(frame, indices=idx_range, axis=-1)\n\n        self.state_buffer[self.current_idx] = frame\n\n        self.action_buffer[self.current_idx] = action\n        self.reward_buffer[self.current_idx] = reward\n        self.dones_buffer[self.current_idx] = done\n\n        if extra_info is not None:\n            for name in extra_info:\n                if name not in self.extra_data:\n                    assert self.current_size == 0, f""New data {name} encountered in the middle of the training""\n                    array = extra_info[name]\n\n                    self.extra_data[name] = np.zeros([self.buffer_capacity] + list(array.shape), dtype=array.dtype)\n\n                self.extra_data[name][self.current_idx] = extra_info[name]\n\n        if self.current_size < self.buffer_capacity:\n            self.current_size += 1\n\n        return self.current_idx\n\n    def get_frame_with_future(self, frame_idx, env_idx):\n        """""" Return frame from the buffer together with the next frame """"""\n        if frame_idx == self.current_idx:\n            raise VelException(""Cannot provide enough future for the frame"")\n\n        past_frame = self.get_frame(frame_idx, env_idx)\n\n        if not self.dones_buffer[frame_idx, env_idx]:\n            # We\'re not done\n            next_idx = (frame_idx + 1) % self.buffer_capacity\n            next_frame = self.state_buffer[next_idx, env_idx]\n\n            if self.frame_history > 1:\n                future_frame = np.concatenate([\n                    past_frame.take(indices=np.arange(1, past_frame.shape[-1]), axis=-1), next_frame\n                ], axis=-1)\n            else:\n                future_frame = next_frame\n        else:\n            # We are done\n            future_frame = np.zeros_like(past_frame)\n\n        return past_frame, future_frame\n\n    def get_frame_with_future_forward_steps(self, frame_idx, env_idx, forward_steps, discount_factor):\n        """""" Return frame from the buffer together with the next frame """"""\n        index_array = np.arange(frame_idx, frame_idx+forward_steps) % self.current_size\n\n        if self.current_idx in index_array:\n            raise VelException(""Cannot provide enough future for the frame"")\n\n        past_frame = self.get_frame(frame_idx, env_idx)\n        dones_array = self.dones_buffer[index_array, env_idx]\n\n        rewards_array = self.reward_buffer[index_array, env_idx]\n        discounted_rewards_array = rewards_array * (discount_factor ** np.arange(forward_steps))\n\n        if dones_array.any():\n            # Are we done between current frame and frame + n\n            done = True\n\n            dones_shifted = np.zeros_like(dones_array)\n            dones_shifted[1:] = dones_array[:-1]\n            reward = discounted_rewards_array[~np.logical_or.accumulate(dones_shifted)].sum()\n            future_frame = np.zeros_like(past_frame)\n        else:\n            done = False\n            reward = discounted_rewards_array.sum()\n\n            if forward_steps >= self.frame_history:\n                frame_indices = (index_array[:self.frame_history] + 1) % self.buffer_capacity\n                future_frame = np.moveaxis(self.state_buffer[frame_indices, env_idx], 0, -2).reshape(past_frame.shape)\n            else:\n                frame_candidate = np.moveaxis(\n                    self.state_buffer[(index_array + 1) % self.buffer_capacity, env_idx], 0, -2\n                )\n                frame_candidate_target_shape = (\n                    list(frame_candidate.shape[:-2]) + [frame_candidate.shape[-2] * frame_candidate.shape[-1]]\n                )\n\n                future_frame = np.concatenate([\n                    past_frame[..., (frame_candidate_target_shape[-1] - past_frame.shape[-1]):],\n                    frame_candidate.reshape(frame_candidate_target_shape)\n                ], -1)\n\n        return past_frame, future_frame, reward, done\n\n    def get_frame(self, frame_idx, env_idx):\n        """""" Return frame from the buffer """"""\n        if frame_idx >= self.current_size:\n            raise VelException(""Requested frame beyond the size of the buffer"")\n\n        accumulator = []\n\n        last_frame = self.state_buffer[frame_idx, env_idx]\n\n        accumulator.append(last_frame)\n\n        for i in range(self.frame_history - 1):\n            prev_idx = (frame_idx - 1) % self.buffer_capacity\n\n            if prev_idx == self.current_idx:\n                raise VelException(""Cannot provide enough history for the frame"")\n            elif self.dones_buffer[prev_idx, env_idx]:\n                # If previous frame was done - just append zeroes\n                accumulator.append(np.zeros_like(last_frame))\n            else:\n                frame_idx = prev_idx\n                accumulator.append(self.state_buffer[frame_idx, env_idx])\n\n        # We\'re pushing the elements in reverse order\n        return np.concatenate(accumulator[::-1], axis=-1)\n\n    def get_transition(self, frame_idx, env_idx):\n        """""" Single transition with given index """"""\n        past_frame, future_frame = self.get_frame_with_future(frame_idx, env_idx)\n\n        data_dict = {\n            \'observations\': past_frame,\n            \'observations_next\': future_frame,\n            \'actions\': self.action_buffer[frame_idx, env_idx],\n            \'rewards\': self.reward_buffer[frame_idx, env_idx],\n            \'dones\': self.dones_buffer[frame_idx, env_idx],\n        }\n\n        for name in self.extra_data:\n            data_dict[name] = self.extra_data[name][frame_idx, env_idx]\n\n        return data_dict\n\n    def get_transitions(self, indexes):\n        """""" Get dictionary of transition data """"""\n        assert indexes.shape[1] == self.state_buffer.shape[1], \\\n            ""Must have the same number of indexes as there are environments""\n\n        frame_batch_shape = (\n                [indexes.shape[0], indexes.shape[1]]\n                + list(self.state_buffer.shape[2:-1])\n                + [self.state_buffer.shape[-1] * self.frame_history]\n        )\n\n        past_frame_buffer = np.zeros(frame_batch_shape, dtype=self.state_buffer.dtype)\n        future_frame_buffer = np.zeros(frame_batch_shape, dtype=self.state_buffer.dtype)\n\n        for buffer_idx, frame_row in enumerate(indexes):\n            for env_idx, frame_idx in enumerate(frame_row):\n                past_frame_buffer[buffer_idx, env_idx], future_frame_buffer[buffer_idx, env_idx] = (\n                    self.get_frame_with_future(frame_idx, env_idx)\n                )\n\n        actions = take_along_axis(self.action_buffer, indexes)\n        rewards = take_along_axis(self.reward_buffer, indexes)\n        dones = take_along_axis(self.dones_buffer, indexes)\n\n        transition_tensors = {\n            \'observations\': past_frame_buffer,\n            \'actions\': actions,\n            \'rewards\': rewards,\n            \'observations_next\': future_frame_buffer,\n            \'dones\': dones.astype(np.float32),\n        }\n\n        for name in self.extra_data:\n            transition_tensors[name] = take_along_axis(self.extra_data[name], indexes)\n\n        return transition_tensors\n\n    def get_transitions_forward_steps(self, indexes, forward_steps, discount_factor):\n        """"""\n        Get dictionary of a transition data - where the target of a transition is\n        n steps forward along the trajectory. Rewards are properly aggregated according to the discount factor,\n        and the process stops when trajectory is done.\n        """"""\n        frame_batch_shape = (\n                [indexes.shape[0], indexes.shape[1]]\n                + list(self.state_buffer.shape[2:-1])\n                + [self.state_buffer.shape[-1] * self.frame_history]\n        )\n\n        simple_batch_shape = [indexes.shape[0], indexes.shape[1]]\n\n        past_frame_buffer = np.zeros(frame_batch_shape, dtype=self.state_buffer.dtype)\n        future_frame_buffer = np.zeros(frame_batch_shape, dtype=self.state_buffer.dtype)\n\n        reward_buffer = np.zeros(simple_batch_shape, dtype=np.float32)\n        dones_buffer = np.zeros(simple_batch_shape, dtype=bool)\n\n        for buffer_idx, frame_row in enumerate(indexes):\n            for env_idx, frame_idx in enumerate(frame_row):\n                past_frame, future_frame, reward, done = self.get_frame_with_future_forward_steps(\n                    frame_idx, env_idx, forward_steps=forward_steps, discount_factor=discount_factor\n                )\n\n                past_frame_buffer[buffer_idx, env_idx] = past_frame\n                future_frame_buffer[buffer_idx, env_idx] = future_frame\n                reward_buffer[buffer_idx, env_idx] = reward\n                dones_buffer[buffer_idx, env_idx] = done\n\n        actions = take_along_axis(self.action_buffer, indexes)\n\n        transition_tensors = {\n            \'observations\': past_frame_buffer,\n            \'actions\': actions,\n            \'rewards\': reward_buffer,\n            \'observations_next\': future_frame_buffer,\n            \'dones\': dones_buffer.astype(np.float32),\n        }\n\n        for name in self.extra_data:\n            transition_tensors[name] = take_along_axis(self.extra_data[name], indexes)\n\n        return transition_tensors\n\n    def get_trajectories(self, indexes, rollout_length):\n        """""" Return batch consisting of *consecutive* transitions """"""\n        # assert indexes.shape[0] > 1, ""There must be multiple indexes supplied""\n        assert rollout_length > 1, ""Rollout length must be greater than 1""\n\n        batch_indexes = (\n                indexes.reshape(1, indexes.shape[0]) - np.arange(rollout_length - 1, -1, -1).reshape(rollout_length, 1)\n        )\n\n        return self.get_transitions(batch_indexes)\n\n    def sample_batch_transitions(self, batch_size, forward_steps=1):\n        """""" Return indexes of next sample""""""\n        results = []\n\n        for i in range(self.num_envs):\n            results.append(self.sample_frame_single_env(batch_size, forward_steps=forward_steps))\n\n        return np.stack(results, axis=-1)\n\n    def sample_batch_trajectories(self, rollout_length):\n        """""" Return indexes of next random rollout """"""\n        results = []\n\n        for i in range(self.num_envs):\n            results.append(self.sample_rollout_single_env(rollout_length))\n\n        return np.stack(results, axis=-1)\n\n    def sample_rollout_single_env(self, rollout_length):\n        """""" Return indexes of next sample""""""\n        # Sample from up to total size\n        if self.current_size < self.buffer_capacity:\n            if rollout_length + 1 > self.current_size:\n                raise VelException(""Not enough elements in the buffer to sample the rollout"")\n\n            # -1 because we cannot take the last one\n            return np.random.choice(self.current_size - rollout_length) + rollout_length - 1\n        else:\n            if rollout_length + self.frame_history > self.current_size:\n                raise VelException(""Not enough elements in the buffer to sample the rollout"")\n\n            candidate = np.random.choice(self.buffer_capacity)\n\n            # These are the elements we cannot draw, as then we don\'t have enough history\n            forbidden_ones = (\n                    np.arange(self.current_idx, self.current_idx + self.frame_history + rollout_length - 1)\n                    % self.buffer_capacity\n            )\n\n            # Exclude these frames for learning as they may have some part of history overwritten\n            while candidate in forbidden_ones:\n                candidate = np.random.choice(self.buffer_capacity)\n\n            return candidate\n\n    def sample_frame_single_env(self, batch_size, forward_steps=1):\n        """""" Return an in index of a random set of frames from a buffer, that have enough history and future """"""\n        # Whole idea of this function is to make sure that sample we take is far away from the point which we are\n        # currently writing to the buffer, which is \'discontinuous\'\n\n        if self.current_size < self.buffer_capacity:\n            # Sample from up to total size of the buffer\n            # -1 because we cannot take the last one\n            return np.random.choice(self.current_size - forward_steps, batch_size, replace=False)\n        else:\n            candidate = np.random.choice(self.buffer_capacity, batch_size, replace=False)\n\n            forbidden_ones = (\n                np.arange(self.current_idx - forward_steps + 1, self.current_idx + self.frame_history)\n                % self.buffer_capacity\n            )\n\n            # Exclude these frames for learning as they may have some part of history overwritten\n            while any(x in candidate for x in forbidden_ones):\n                candidate = np.random.choice(self.buffer_capacity, batch_size, replace=False)\n\n            return candidate\n'"
vel/rl/buffers/backend/prioritized_buffer_backend.py,0,"b'import gym\nimport numpy as np\nimport random\n\nfrom .circular_buffer_backend import CircularBufferBackend\nfrom .segment_tree import SegmentTree\n\n\nclass PrioritizedCircularBufferBackend:\n    """""" Backend behind the prioritized replay buffer using circular buffer """"""\n    def __init__(self, buffer_capacity: int, observation_space: gym.Space, action_space: gym.Space, extra_data=None):\n        self.deque = CircularBufferBackend(buffer_capacity, observation_space, action_space, extra_data=extra_data)\n        self.segment_tree = SegmentTree(buffer_capacity)\n\n    def store_transition(self, frame, action, reward, done, extra_info=None):\n        """""" Store given transition in the backend """"""\n        self.deque.store_transition(frame, action, reward, done, extra_info=extra_info)\n\n        # We add element with max priority to the tree\n        self.segment_tree.append(self.segment_tree.max)\n\n    def get_frame(self, idx, history):\n        """""" Return frame from the buffer """"""\n        return self.deque.get_frame(idx, history)\n\n    def get_frame_with_future(self, idx, history):\n        """""" Return frame from the buffer together with the next frame """"""\n        return self.deque.get_frame_with_future(idx, history)\n\n    def get_batch(self, indexes, history):\n        """""" Return batch of frames for given indexes """"""\n        return self.deque.get_transitions(indexes, history)\n\n    def update_priority(self, tree_idx, priority):\n        """""" Update priorities of the elements in the tree """"""\n        self.segment_tree.update(tree_idx, priority)\n\n    def sample_batch_prioritized(self, batch_size, history):\n        """""" Return indexes of the next sample in from prioritized distribution """"""\n        p_total = self.segment_tree.total()\n        segment = p_total / batch_size\n\n        # Get batch of valid samples\n        batch = [self._get_sample_from_segment(segment, i, history) for i in range(batch_size)]\n        probs, idxs, tree_idxs = zip(*batch)\n        return probs, np.array(idxs), tree_idxs\n\n    def _get_sample_from_segment(self, segment, i, history):\n        valid = False\n\n        prob = None\n        idx = None\n        tree_idx = None\n\n        while not valid:\n            # Uniformly sample an element from within a segment\n            sample = random.uniform(i * segment, (i + 1) * segment)\n\n            # Retrieve sample from tree with un-normalised probability\n            prob, idx, tree_idx = self.segment_tree.find(sample)\n\n            # Resample if transition straddled current index or probablity 0\n            if (self.segment_tree.index - idx) % self.segment_tree.size > 1 and \\\n                    (idx - self.segment_tree.index) % self.segment_tree.size >= history and prob != 0:\n                valid = True  # Note that conditions are valid but extra conservative around buffer index 0\n\n        return prob, idx, tree_idx\n\n    @property\n    def current_size(self):\n        """""" Return current size of the replay buffer """"""\n        return self.deque.current_size\n\n    @property\n    def current_idx(self):\n        """""" Return current index """"""\n        return self.deque.current_idx\n'"
vel/rl/buffers/backend/prioritized_vec_buffer_backend.py,0,"b'import gym\nimport random\nimport numpy as np\n\nfrom .circular_vec_buffer_backend import CircularVecEnvBufferBackend\nfrom .segment_tree import SegmentTree\n\n\nclass PrioritizedCircularVecEnvBufferBackend:\n    """"""\n    Prioritized replay buffer version of CircularVecEnvBufferBackend\n    """"""\n\n    def __init__(self, buffer_capacity: int, num_envs: int, observation_space: gym.Space, action_space: gym.Space,\n                 frame_stack_compensation: bool = False, frame_history: int = 1):\n        self.deque = CircularVecEnvBufferBackend(\n            buffer_capacity=buffer_capacity,\n            num_envs=num_envs,\n            observation_space=observation_space,\n            action_space=action_space,\n            frame_stack_compensation=frame_stack_compensation,\n            frame_history=frame_history\n        )\n\n        self.segment_trees = [SegmentTree(buffer_capacity) for _ in range(num_envs)]\n\n    def store_transition(self, frame, action, reward, done, extra_info=None):\n        """""" Store given transition in the backend """"""\n        self.deque.store_transition(frame, action, reward, done, extra_info=extra_info)\n\n        # We add element with max priority to all the trees\n        for segment_tree in self.segment_trees:\n            segment_tree.append(segment_tree.max)\n\n    def get_frame_with_future(self, frame_idx, env_idx):\n        """""" Return frame from the buffer together with the next frame """"""\n        return self.deque.get_frame_with_future(frame_idx, env_idx)\n\n    def get_frame(self, frame_idx, env_idx):\n        """""" Return frame from the buffer """"""\n        return self.deque.get_frame(frame_idx, env_idx)\n\n    def get_transition(self, frame_idx, env_idx):\n        """""" Single transition with given index """"""\n        return self.deque.get_transition(frame_idx, env_idx)\n\n    def get_trajectories(self, indexes, rollout_length):\n        """""" Return batch consisting of *consecutive* transitions """"""\n        return self.deque.get_trajectories(indexes, rollout_length)\n\n    def get_transitions(self, indexes):\n        """""" Get dictionary of transition data """"""\n        return self.deque.get_transitions(indexes)\n\n    def get_transitions_forward_steps(self, indexes, forward_steps, discount_factor):\n        """""" Get dictionary of transition data """"""\n        return self.deque.get_transitions_forward_steps(indexes, forward_steps, discount_factor)\n\n    def sample_batch_transitions(self, batch_size, forward_steps=1):\n        """""" Return indexes of next sample""""""\n        batches = [\n            self._sample_batch_prioritized(\n                segment_tree, batch_size, self.deque.frame_history, forward_steps=forward_steps\n            )\n            for segment_tree in self.segment_trees\n        ]\n\n        probs, idxs, tree_idxs = zip(*batches)\n\n        return np.stack(probs, axis=1).astype(float), np.stack(idxs, axis=1), np.stack(tree_idxs, axis=1)\n\n    def update_priority(self, tree_idx_list, priority_list):\n        """""" Update priorities of the elements in the tree """"""\n        for tree_idx, priority, segment_tree in zip(tree_idx_list, priority_list, self.segment_trees):\n            segment_tree.update(tree_idx, priority)\n\n    @property\n    def current_size(self):\n        """""" Return current size of the replay buffer """"""\n        return self.deque.current_size\n\n    @property\n    def current_idx(self):\n        """""" Return current index """"""\n        return self.deque.current_idx\n\n    def _sample_batch_prioritized(self, segment_tree, batch_size, history, forward_steps=1):\n        """""" Return indexes of the next sample in from prioritized distribution """"""\n        p_total = segment_tree.total()\n        segment = p_total / batch_size\n\n        # Get batch of valid samples\n        batch = [\n            self._get_sample_from_segment(segment_tree, segment, i, history, forward_steps)\n            for i in range(batch_size)\n        ]\n\n        probs, idxs, tree_idxs = zip(*batch)\n        return np.array(probs), np.array(idxs), np.array(tree_idxs)\n\n    def _get_sample_from_segment(self, segment_tree, segment, i, history, forward_steps):\n        valid = False\n\n        prob = None\n        idx = None\n        tree_idx = None\n\n        while not valid:\n            # Uniformly sample an element from within a segment\n            sample = random.uniform(i * segment, (i + 1) * segment)\n\n            # Retrieve sample from tree with un-normalised probability\n            prob, idx, tree_idx = segment_tree.find(sample)\n\n            # Resample if transition straddled current index or probablity 0\n            if (segment_tree.index - idx) % segment_tree.size > forward_steps and \\\n                    (idx - segment_tree.index) % segment_tree.size >= history and prob != 0:\n                valid = True  # Note that conditions are valid but extra conservative around buffer index 0\n\n        return prob, idx, tree_idx\n'"
vel/rl/buffers/backend/segment_tree.py,0,"b'# Segment tree implementation taken from https://github.com/Kaixhin/Rainbow/blob/master/memory.py\nclass SegmentTree:\n    """"""\n    Segment tree data structure where parent node values are sum/max of children node values\n    """"""\n    def __init__(self, size):\n        self.index = 0\n        self.size = size\n        self.full = False  # Used to track actual capacity\n        self.sum_tree = [0] * (2 * size - 1)  # Initialise fixed size tree with all (priority) zeros\n        self.max = 1  # Initial max value to return (1 = 1^\xcf\x89)\n\n    # Propagates value up tree given a tree index\n    def _propagate(self, index, value):\n        parent = (index - 1) // 2\n        left, right = 2 * parent + 1, 2 * parent + 2\n        self.sum_tree[parent] = self.sum_tree[left] + self.sum_tree[right]\n        if parent != 0:\n            self._propagate(parent, value)\n\n    # Updates value given a tree index\n    def update(self, index, value):\n        self.sum_tree[index] = value  # Set new value\n        self._propagate(index, value)  # Propagate value\n        self.max = max(value, self.max)\n\n    # def append(self, data, value):\n    def append(self, value):\n        # self.data[self.index] = data  # Store data in underlying data structure\n        self.update(self.index + self.size - 1, value)  # Update tree\n        self.index = (self.index + 1) % self.size  # Update index\n        self.full = self.full or self.index == 0  # Save when capacity reached\n        self.max = max(value, self.max)\n\n    # Searches for the location of a value in sum tree\n    def _retrieve(self, index, value):\n        left, right = 2 * index + 1, 2 * index + 2\n        if left >= len(self.sum_tree):\n            return index\n        elif value <= self.sum_tree[left]:\n            return self._retrieve(left, value)\n        else:\n            return self._retrieve(right, value - self.sum_tree[left])\n\n    # Searches for a value in sum tree and returns value, data index and tree index\n    def find(self, value):\n        index = self._retrieve(0, value)  # Search for index of item from root\n        data_index = index - self.size + 1\n        return self.sum_tree[index], data_index, index  # Return value, data index, tree index\n\n    def tree_index_for_index(self, index):\n        return index + self.size - 1\n\n    def total(self):\n        return self.sum_tree[0]\n'"
vel/rl/buffers/tests/__init__.py,0,b''
vel/rl/buffers/tests/test_circular_buffer_backend.py,0,"b'import gym\nimport gym.spaces\nimport numpy as np\nimport numpy.testing as nt\nimport pytest\n\nfrom vel.exceptions import VelException\nfrom vel.rl.buffers.backend.circular_buffer_backend import CircularBufferBackend\n\n\ndef get_half_filled_buffer():\n    """""" Return simple preinitialized buffer """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=np.uint8)\n    action_space = gym.spaces.Discrete(4)\n\n    buffer = CircularBufferBackend(20, observation_space, action_space)\n\n    v1 = np.ones(4).reshape((2, 2, 1))\n\n    for i in range(10):\n        buffer.store_transition(v1 * (i+1), 0, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer():\n    """""" Return simple preinitialized buffer """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=np.uint8)\n    action_space = gym.spaces.Discrete(4)\n\n    buffer = CircularBufferBackend(20, observation_space, action_space)\n\n    v1 = np.ones(4).reshape((2, 2, 1))\n\n    for i in range(30):\n        buffer.store_transition(v1 * (i+1), 0, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer1x1():\n    """""" Return simple preinitialized buffer """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2,), dtype=int)\n    action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=float)\n\n    buffer = CircularBufferBackend(20, observation_space=observation_space, action_space=action_space)\n\n    v1 = np.ones(2).reshape((2,))\n    a1 = np.arange(2).reshape((2,))\n\n    for i in range(30):\n        item = v1.copy()\n        item[0] *= (i+1)\n        item[1] *= 10 * (i+1)\n\n        buffer.store_transition(item, a1 * i, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer2x2():\n    """""" Return simple preinitialized buffer """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2), dtype=int)\n    action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(2, 2), dtype=float)\n\n    buffer = CircularBufferBackend(20, observation_space=observation_space, action_space=action_space)\n\n    v1 = np.ones(4).reshape((2, 2))\n    a1 = np.arange(4).reshape((2, 2))\n\n    for i in range(30):\n        item = v1.copy()\n        item[0] *= (i+1)\n        item[1] *= 10 * (i+1)\n\n        buffer.store_transition(item, a1 * i, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer3x3():\n    """""" Return simple preinitialized buffer """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 2), dtype=int)\n    action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(2, 2, 2), dtype=float)\n\n    buffer = CircularBufferBackend(20, observation_space=observation_space, action_space=action_space)\n\n    v1 = np.ones(8).reshape((2, 2, 2))\n    a1 = np.arange(8).reshape((2, 2, 2))\n\n    for i in range(30):\n        item = v1.copy()\n        item[0] *= (i+1)\n        item[1] *= 10 * (i+1)\n\n        buffer.store_transition(item, i * a1, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer1x1_history():\n    """""" Return simple preinitialized buffer """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 1), dtype=int)\n    action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=float)\n\n    buffer = CircularBufferBackend(20, observation_space=observation_space, action_space=action_space)\n\n    v1 = np.ones(2).reshape((2, 1))\n    a1 = np.arange(2).reshape((2,))\n\n    for i in range(30):\n        item = v1.copy()\n        item[0] *= (i+1)\n        item[1] *= 10 * (i+1)\n\n        buffer.store_transition(item, a1 * i, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer2x2_history():\n    """""" Return simple preinitialized buffer """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=int)\n    action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(2, 2), dtype=float)\n\n    buffer = CircularBufferBackend(20, observation_space=observation_space, action_space=action_space)\n\n    v1 = np.ones(4).reshape((2, 2, 1))\n    a1 = np.arange(4).reshape((2, 2))\n\n    for i in range(30):\n        item = v1.copy()\n        item[0] *= (i+1)\n        item[1] *= 10 * (i+1)\n\n        buffer.store_transition(item, a1 * i, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer3x3_history():\n    """""" Return simple preinitialized buffer """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 2, 1), dtype=int)\n    action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(2, 2, 2), dtype=float)\n\n    buffer = CircularBufferBackend(20, observation_space=observation_space, action_space=action_space)\n\n    v1 = np.ones(8).reshape((2, 2, 2, 1))\n    a1 = np.arange(8).reshape((2, 2, 2))\n\n    for i in range(30):\n        item = v1.copy()\n        item[0] *= (i+1)\n        item[1] *= 10 * (i+1)\n\n        buffer.store_transition(item, i * a1, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer_extra_info():\n    """""" Return simple preinitialized buffer """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=np.uint8)\n    action_space = gym.spaces.Discrete(4)\n\n    buffer = CircularBufferBackend(20, observation_space, action_space, extra_data={\n        \'neglogp\': np.zeros(20, dtype=float)\n    })\n\n    v1 = np.ones(4).reshape((2, 2, 1))\n\n    for i in range(30):\n        buffer.store_transition(v1 * (i+1), 0, float(i)/2, False, extra_info={\'neglogp\': i / 30.0})\n\n    return buffer\n\n\ndef get_filled_buffer_with_dones():\n    """""" Return simple preinitialized buffer with some done\'s in there """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=np.uint8)\n    action_space = gym.spaces.Discrete(4)\n    buffer = CircularBufferBackend(20, observation_space, action_space)\n\n    v1 = np.ones(4).reshape((2, 2, 1))\n\n    done_set = {2, 5, 10, 13, 18, 22, 28}\n\n    for i in range(30):\n        if i in done_set:\n            buffer.store_transition(v1 * (i+1), 0, float(i)/2, True)\n        else:\n            buffer.store_transition(v1 * (i+1), 0, float(i)/2, False)\n\n    return buffer\n\n\ndef test_simple_get_frame():\n    """""" Check if get_frame returns frames from a buffer partially full """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=np.uint8)\n    action_space = gym.spaces.Discrete(4)\n    buffer = CircularBufferBackend(20, observation_space, action_space)\n\n    v1 = np.ones(4).reshape((2, 2, 1))\n    v2 = v1 * 2\n    v3 = v1 * 3\n\n    buffer.store_transition(v1, 0, 0, False)\n    buffer.store_transition(v2, 0, 0, False)\n    buffer.store_transition(v3, 0, 0, False)\n\n    assert np.all(buffer.get_frame(0, 4).max(0).max(0) == np.array([0, 0, 0, 1]))\n    assert np.all(buffer.get_frame(1, 4).max(0).max(0) == np.array([0, 0, 1, 2]))\n    assert np.all(buffer.get_frame(2, 4).max(0).max(0) == np.array([0, 1, 2, 3]))\n\n    with pytest.raises(VelException):\n        buffer.get_frame(3, 4)\n\n    with pytest.raises(VelException):\n        buffer.get_frame(4, 4)\n\n\ndef test_full_buffer_get_frame():\n    """""" Check if get_frame returns frames for full buffer """"""\n    buffer = get_filled_buffer()\n\n    nt.assert_array_equal(buffer.get_frame(0, 4).max(0).max(0), np.array([18, 19, 20, 21]))\n    nt.assert_array_equal(buffer.get_frame(1, 4).max(0).max(0), np.array([19, 20, 21, 22]))\n    nt.assert_array_equal(buffer.get_frame(9, 4).max(0).max(0), np.array([27, 28, 29, 30]))\n\n    with pytest.raises(VelException):\n        buffer.get_frame(10, 4)\n\n    with pytest.raises(VelException):\n        buffer.get_frame(11, 4)\n\n    with pytest.raises(VelException):\n        buffer.get_frame(12, 4)\n\n    nt.assert_array_equal(buffer.get_frame(13, 4).max(0).max(0), np.array([11, 12, 13, 14]))\n    nt.assert_array_equal(buffer.get_frame(19, 4).max(0).max(0), np.array([17, 18, 19, 20]))\n\n\ndef test_full_buffer_get_future_frame():\n    """""" Check if get_frame_with_future works with full buffer """"""\n    buffer = get_filled_buffer()\n\n    nt.assert_array_equal(buffer.get_frame_with_future(0, 4)[1].max(0).max(0), np.array([19, 20, 21, 22]))\n    nt.assert_array_equal(buffer.get_frame_with_future(1, 4)[1].max(0).max(0), np.array([20, 21, 22, 23]))\n\n    with pytest.raises(VelException):\n        buffer.get_frame_with_future(9, 4)\n\n    with pytest.raises(VelException):\n        buffer.get_frame_with_future(10, 4)\n\n    with pytest.raises(VelException):\n        buffer.get_frame_with_future(11, 4)\n\n    with pytest.raises(VelException):\n        buffer.get_frame_with_future(12, 4)\n\n    nt.assert_array_equal(buffer.get_frame_with_future(13, 4)[1].max(0).max(0), np.array([12, 13, 14, 15]))\n    nt.assert_array_equal(buffer.get_frame_with_future(19, 4)[1].max(0).max(0), np.array([18, 19, 20, 21]))\n\n\ndef test_buffer_filling_size():\n    """""" Check if buffer size is properly updated when we add items """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=np.uint8)\n    action_space = gym.spaces.Discrete(4)\n    buffer = CircularBufferBackend(20, observation_space, action_space)\n\n    v1 = np.ones(4).reshape((2, 2, 1))\n\n    assert buffer.current_size == 0\n\n    buffer.store_transition(v1, 0, 0, False)\n    buffer.store_transition(v1, 0, 0, False)\n\n    assert buffer.current_size == 2\n\n    for i in range(30):\n        buffer.store_transition(v1 * (i+1), 0, float(i)/2, False)\n\n    assert buffer.current_size == buffer.buffer_capacity\n\n\ndef test_get_frame_with_dones():\n    """""" Check if get_frame works properly in case there are multiple sequences in buffer """"""\n    buffer = get_filled_buffer_with_dones()\n\n    nt.assert_array_equal(buffer.get_frame(0, 4).max(0).max(0), np.array([0, 0, 20, 21]))\n    nt.assert_array_equal(buffer.get_frame(1, 4).max(0).max(0), np.array([0, 20, 21, 22]))\n    nt.assert_array_equal(buffer.get_frame(2, 4).max(0).max(0), np.array([20, 21, 22, 23]))\n    nt.assert_array_equal(buffer.get_frame(3, 4).max(0).max(0), np.array([0, 0, 0, 24]))\n\n    nt.assert_array_equal(buffer.get_frame(8, 4).max(0).max(0), np.array([26, 27, 28, 29]))\n\n    nt.assert_array_equal(buffer.get_frame(9, 4).max(0).max(0), np.array([0, 0, 0, 30]))\n\n    with pytest.raises(VelException):\n        buffer.get_frame(10, 4)\n\n    nt.assert_array_equal(buffer.get_frame(11, 4).max(0).max(0), np.array([0, 0, 0, 12]))\n    nt.assert_array_equal(buffer.get_frame(12, 4).max(0).max(0), np.array([0, 0, 12, 13]))\n\n\ndef test_get_frame_future_with_dones():\n    """""" Check if get_frame_with_future works properly in case there are multiple sequences in buffer """"""\n    buffer = get_filled_buffer_with_dones()\n\n    nt.assert_array_equal(buffer.get_frame_with_future(0, 4)[1].max(0).max(0), np.array([0, 20, 21, 22]))\n    nt.assert_array_equal(buffer.get_frame_with_future(1, 4)[1].max(0).max(0), np.array([20, 21, 22, 23]))\n    nt.assert_array_equal(buffer.get_frame_with_future(2, 4)[1].max(0).max(0), np.array([21, 22, 23, 0]))\n\n    nt.assert_array_equal(buffer.get_frame_with_future(3, 4)[1].max(0).max(0), np.array([0, 0, 24, 25]))\n\n    nt.assert_array_equal(buffer.get_frame_with_future(8, 4)[1].max(0).max(0), np.array([27, 28, 29, 0]))\n\n    with pytest.raises(VelException):\n        buffer.get_frame_with_future(9, 4)\n\n    with pytest.raises(VelException):\n        buffer.get_frame_with_future(10, 4)\n\n    nt.assert_array_equal(buffer.get_frame_with_future(11, 4)[1].max(0).max(0), np.array([0, 0, 12, 13]))\n    nt.assert_array_equal(buffer.get_frame_with_future(12, 4)[1].max(0).max(0), np.array([0, 12, 13, 14]))\n\n\ndef test_get_batch():\n    """""" Check if get_batch works properly for buffers """"""\n    buffer = get_filled_buffer_with_dones()\n\n    batch = buffer.get_transitions(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8]), history_length=4)\n\n    obs = batch[\'observations\']\n    act = batch[\'actions\']\n    rew = batch[\'rewards\']\n    obs_tp1 = batch[\'observations_next\']\n    dones = batch[\'dones\']\n\n    nt.assert_array_equal(dones, np.array([False, False, True, False, False, False, False, False, True]))\n    nt.assert_array_equal(obs.max(1).max(1), np.array([\n        [0, 0, 20, 21],\n        [0, 20, 21, 22],\n        [20, 21, 22, 23],\n        [0, 0, 0, 24],\n        [0, 0, 24, 25],\n        [0, 24, 25, 26],\n        [24, 25, 26, 27],\n        [25, 26, 27, 28],\n        [26, 27, 28, 29],\n    ]))\n\n    nt.assert_array_equal(act, np.array([0, 0, 0, 0, 0, 0, 0, 0, 0]))\n    nt.assert_array_equal(rew, np.array([10.0, 10.5, 11.0, 11.5, 12.0, 12.5, 13.0, 13.5, 14.0]))\n\n    nt.assert_array_equal(obs_tp1.max(1).max(1), np.array([\n        [0, 20, 21, 22],\n        [20, 21, 22, 23],\n        [21, 22, 23, 0],\n        [0, 0, 24, 25],\n        [0, 24, 25, 26],\n        [24, 25, 26, 27],\n        [25, 26, 27, 28],\n        [26, 27, 28, 29],\n        [27, 28, 29, 0],\n    ]))\n\n    with pytest.raises(VelException):\n        buffer.get_transitions(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), history_length=4)\n\n\ndef test_sample_and_get_batch():\n    """""" Check if batch sampling works properly """"""\n    buffer = get_filled_buffer_with_dones()\n\n    for i in range(100):\n        indexes = buffer.sample_batch_transitions(batch_size=5, history_length=4)\n        batch = buffer.get_transitions(indexes, history_length=4)\n\n        obs = batch[\'observations\']\n        act = batch[\'actions\']\n        rew = batch[\'rewards\']\n        obs_tp1 = batch[\'observations_next\']\n        dones = batch[\'dones\']\n\n        assert obs.shape[0] == 5\n        assert act.shape[0] == 5\n        assert rew.shape[0] == 5\n        assert obs_tp1.shape[0] == 5\n        assert dones.shape[0] == 5\n\n\ndef test_storing_extra_info():\n    """""" Make sure additional information are stored and recovered properly """"""\n    buffer = get_filled_buffer_extra_info()\n\n    batch = buffer.get_transitions(np.array([0, 1, 2, 17, 18, 19]), history_length=4)\n\n    nt.assert_equal(batch[\'neglogp\'][0], 20.0/30)\n    nt.assert_equal(batch[\'neglogp\'][1], 21.0/30)\n    nt.assert_equal(batch[\'neglogp\'][2], 22.0/30)\n    nt.assert_equal(batch[\'neglogp\'][3], 17.0/30)\n    nt.assert_equal(batch[\'neglogp\'][4], 18.0/30)\n    nt.assert_equal(batch[\'neglogp\'][5], 19.0/30)\n\n\ndef test_sample_rollout_half_filled():\n    """""" Test if sampling rollout is correct and returns proper results """"""\n    buffer = get_half_filled_buffer()\n\n    indexes = []\n\n    for i in range(1000):\n        rollout_idx = buffer.sample_batch_trajectories(rollout_length=5, history_length=4)\n        rollout = buffer.get_trajectories(index=rollout_idx, rollout_length=5, history_length=4)\n\n        assert rollout[\'observations\'].shape[0] == 5  # Rollout length\n        assert rollout[\'observations\'].shape[-1] == 4  # History length\n\n        indexes.append(rollout_idx)\n\n    assert np.min(indexes) == 4\n    assert np.max(indexes) == 8\n\n    with pytest.raises(VelException):\n        buffer.sample_batch_trajectories(rollout_length=10, history_length=4)\n\n    rollout_idx = buffer.sample_batch_trajectories(rollout_length=9, history_length=4)\n    rollout = buffer.get_trajectories(index=rollout_idx, rollout_length=9, history_length=4)\n\n    assert rollout_idx == 8\n\n    nt.assert_array_equal(rollout[\'rewards\'], np.array([\n        0., 0.5, 1., 1.5, 2., 2.5, 3., 3.5, 4.\n    ]))\n\n\ndef test_sample_rollout_filled():\n    """""" Test if sampling rollout is correct and returns proper results """"""\n    buffer = get_filled_buffer()\n\n    indexes = []\n\n    for i in range(1000):\n        rollout_idx = buffer.sample_batch_trajectories(rollout_length=5, history_length=4)\n        rollout = buffer.get_trajectories(index=rollout_idx, rollout_length=5, history_length=4)\n\n        assert rollout[\'observations\'].shape[0] == 5  # Rollout length\n        assert rollout[\'observations\'].shape[-1] == 4  # History length\n\n        indexes.append(rollout_idx)\n\n    assert np.min(indexes) == 0\n    assert np.max(indexes) == 19\n\n    with pytest.raises(VelException):\n        buffer.sample_batch_trajectories(rollout_length=17, history_length=4)\n\n    max_rollout = buffer.sample_batch_trajectories(rollout_length=16, history_length=4)\n\n    rollout = buffer.get_trajectories(max_rollout, rollout_length=16, history_length=4)\n\n    assert max_rollout == 8\n    assert np.sum(rollout[\'rewards\']) == pytest.approx(164.0, 1e-5)\n\n\ndef test_buffer_flexible_obs_action_sizes():\n    b1x1 = get_filled_buffer1x1()\n    b2x2 = get_filled_buffer2x2()\n    b3x3 = get_filled_buffer3x3()\n\n    nt.assert_array_almost_equal(b1x1.get_frame(0), np.array([21, 210]))\n    nt.assert_array_almost_equal(b2x2.get_frame(0), np.array([[21, 21], [210, 210]]))\n    nt.assert_array_almost_equal(b3x3.get_frame(0), np.array([[[21, 21], [21, 21]], [[210, 210], [210, 210]]]))\n\n    nt.assert_array_almost_equal(b1x1.get_transition(0, 0)[\'actions\'], np.array([0, 20]))\n    nt.assert_array_almost_equal(b2x2.get_transition(0, 0)[\'actions\'], np.array([[0, 20], [40, 60]]))\n    nt.assert_array_almost_equal(b3x3.get_transition(0, 0)[\'actions\'], np.array(\n        [[[0, 20], [40, 60]],\n         [[80, 100], [120, 140]]]\n    ))\n\n    with pytest.raises(AssertionError):\n        b1x1.get_frame(0, history_length=2)\n\n    with pytest.raises(AssertionError):\n        b1x1.get_transition(0, history_length=2)\n\n    with pytest.raises(AssertionError):\n        b2x2.get_frame(0,  history_length=2)\n\n    with pytest.raises(AssertionError):\n        b2x2.get_transition(0, history_length=2)\n\n    with pytest.raises(AssertionError):\n        b3x3.get_frame(0, history_length=2)\n\n    with pytest.raises(AssertionError):\n        b3x3.get_transition(0, history_length=2)\n\n\ndef test_buffer_flexible_obs_action_sizes_with_history():\n    b1x1 = get_filled_buffer1x1_history()\n    b2x2 = get_filled_buffer2x2_history()\n    b3x3 = get_filled_buffer3x3_history()\n\n    nt.assert_array_almost_equal(b1x1.get_frame(0, history_length=2), np.array([[20, 21], [200, 210]]))\n    nt.assert_array_almost_equal(b2x2.get_frame(0, history_length=2), np.array(\n        [[[20, 21], [20, 21]], [[200, 210], [200, 210]]]\n    ))\n    nt.assert_array_almost_equal(b3x3.get_frame(0, history_length=2), np.array(\n        [[[[20, 21], [20, 21]], [[20, 21], [20, 21]]], [[[200, 210], [200, 210]], [[200, 210], [200, 210]]]]\n    ))\n\n    nt.assert_array_almost_equal(\n        b1x1.get_transition(0, history_length=2)[\'observations_next\'], np.array([[21, 22], [210, 220]])\n    )\n\n    nt.assert_array_almost_equal(b2x2.get_transition(0, history_length=2)[\'observations_next\'], np.array(\n        [[[21, 22], [21, 22]], [[210, 220], [210, 220]]]\n    ))\n    nt.assert_array_almost_equal(b3x3.get_transition(0, history_length=2)[\'observations_next\'], np.array(\n        [[[[21, 22], [21, 22]], [[21, 22], [21, 22]]],\n         [[[210, 220], [210, 220]], [[210, 220], [210, 220]]]]\n    ))\n'"
vel/rl/buffers/tests/test_circular_vec_env_buffer_backend.py,0,"b'import gym\nimport gym.spaces\nimport numpy as np\nimport numpy.testing as nt\nimport pytest\n\nfrom vel.exceptions import VelException\nfrom vel.rl.buffers.circular_replay_buffer import CircularVecEnvBufferBackend\n\n\ndef get_half_filled_buffer(frame_history=1):\n    """""" Return simple preinitialized buffer """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=int)\n    action_space = gym.spaces.Discrete(4)\n\n    buffer = CircularVecEnvBufferBackend(\n        20, num_envs=2, observation_space=observation_space, action_space=action_space, frame_history=frame_history\n    )\n\n    v1 = np.ones(8).reshape((2, 2, 2, 1))\n\n    for i in range(10):\n        item = v1.copy()\n        item[0] *= (i+1)\n        item[1] *= 10 * (i+1)\n\n        buffer.store_transition(item, 0, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer(frame_history=1):\n    """""" Return simple preinitialized buffer """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=int)\n    action_space = gym.spaces.Discrete(4)\n\n    buffer = CircularVecEnvBufferBackend(\n        20, num_envs=2, observation_space=observation_space, action_space=action_space, frame_history=frame_history\n    )\n\n    v1 = np.ones(8).reshape((2, 2, 2, 1))\n\n    for i in range(30):\n        item = v1.copy()\n        item[0] *= (i+1)\n        item[1] *= 10 * (i+1)\n\n        buffer.store_transition(item, 0, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer1x1(frame_history=1):\n    """""" Return simple preinitialized buffer """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2,), dtype=int)\n    action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=float)\n\n    buffer = CircularVecEnvBufferBackend(\n        20, num_envs=2, observation_space=observation_space, action_space=action_space, frame_history=frame_history\n    )\n\n    v1 = np.ones(4).reshape((2, 2))\n    a1 = np.arange(4).reshape((2, 2))\n\n    for i in range(30):\n        item = v1.copy()\n        item[:, 0] *= (i+1)\n        item[:, 1] *= 10 * (i+1)\n\n        buffer.store_transition(item, a1 * i, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer2x2(frame_history=1):\n    """""" Return simple preinitialized buffer """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2), dtype=int)\n    action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(2, 2), dtype=float)\n\n    buffer = CircularVecEnvBufferBackend(\n        20, num_envs=2, observation_space=observation_space, action_space=action_space, frame_history=frame_history\n    )\n\n    v1 = np.ones(8).reshape((2, 2, 2))\n    a1 = np.arange(8).reshape((2, 2, 2))\n\n    for i in range(30):\n        item = v1.copy()\n        item[:, 0] *= (i+1)\n        item[:, 1] *= 10 * (i+1)\n\n        buffer.store_transition(item, a1 * i, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer3x3(frame_history=1):\n    """""" Return simple preinitialized buffer """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 2), dtype=int)\n    action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(2, 2, 2), dtype=float)\n\n    buffer = CircularVecEnvBufferBackend(\n        20, num_envs=2, observation_space=observation_space, action_space=action_space, frame_history=frame_history\n    )\n\n    v1 = np.ones(16).reshape((2, 2, 2, 2))\n    a1 = np.arange(16).reshape((2, 2, 2, 2))\n\n    for i in range(30):\n        item = v1.copy()\n        item[:, 0] *= (i+1)\n        item[:, 1] *= 10 * (i+1)\n\n        buffer.store_transition(item, i * a1, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer1x1_history(frame_history=1):\n    """""" Return simple preinitialized buffer """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 1), dtype=int)\n    action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=float)\n\n    buffer = CircularVecEnvBufferBackend(\n        20, num_envs=2, observation_space=observation_space, action_space=action_space, frame_history=frame_history\n    )\n\n    v1 = np.ones(4).reshape((2, 2, 1))\n    a1 = np.arange(4).reshape((2, 2))\n\n    for i in range(30):\n        item = v1.copy()\n        item[:, 0] *= (i+1)\n        item[:, 1] *= 10 * (i+1)\n\n        buffer.store_transition(item, a1 * i, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer2x2_history(frame_history=1):\n    """""" Return simple preinitialized buffer """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=int)\n    action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(2, 2), dtype=float)\n\n    buffer = CircularVecEnvBufferBackend(\n        20, num_envs=2, observation_space=observation_space, action_space=action_space, frame_history=frame_history\n    )\n\n    v1 = np.ones(8).reshape((2, 2, 2, 1))\n    a1 = np.arange(8).reshape((2, 2, 2))\n\n    for i in range(30):\n        item = v1.copy()\n        item[:, 0] *= (i+1)\n        item[:, 1] *= 10 * (i+1)\n\n        buffer.store_transition(item, a1 * i, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer3x3_history(frame_history=1):\n    """""" Return simple preinitialized buffer """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 2, 1), dtype=int)\n    action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(2, 2, 2), dtype=float)\n\n    buffer = CircularVecEnvBufferBackend(\n        20, num_envs=2, observation_space=observation_space, action_space=action_space, frame_history=frame_history\n    )\n\n    v1 = np.ones(16).reshape((2, 2, 2, 2, 1))\n    a1 = np.arange(16).reshape((2, 2, 2, 2))\n\n    for i in range(30):\n        item = v1.copy()\n        item[:, 0] *= (i+1)\n        item[:, 1] *= 10 * (i+1)\n\n        buffer.store_transition(item, i * a1, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer_extra_info(frame_history=1):\n    """""" Return simple preinitialized buffer """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=int)\n    action_space = gym.spaces.Discrete(4)\n\n    buffer = CircularVecEnvBufferBackend(\n        20, num_envs=2, observation_space=observation_space, action_space=action_space, frame_history=frame_history\n    )\n\n    v1 = np.ones(8).reshape((2, 2, 2, 1))\n\n    for i in range(30):\n        item = v1.copy()\n        item[0] *= (i+1)\n        item[1] *= 10 * (i+1)\n        buffer.store_transition(item, 0, float(i)/2, False, extra_info={\n            \'neglogp\': np.array([i / 30.0, (i+1) / 30.0])\n        })\n\n    return buffer\n\n\ndef get_filled_buffer_with_dones(frame_history=1):\n    """""" Return simple preinitialized buffer with some done\'s in there """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=int)\n    action_space = gym.spaces.Discrete(4)\n\n    buffer = CircularVecEnvBufferBackend(\n        20, num_envs=2, observation_space=observation_space, action_space=action_space, frame_history=frame_history\n    )\n\n    v1 = np.ones(8).reshape((2, 2, 2, 1))\n\n    done_set = {2, 5, 10, 13, 18, 22, 28}\n\n    for i in range(30):\n        item = v1.copy()\n        item[0] *= (i+1)\n        item[1] *= 10 * (i+1)\n\n        done_array = np.array([i in done_set, (i+1) in done_set], dtype=bool)\n        buffer.store_transition(item, 0, float(i)/2, done_array)\n\n    return buffer\n\n\ndef get_filled_buffer_frame_stack(frame_stack=4, frame_dim=1):\n    """""" Return a preinitialized buffer with frame stack implemented """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, frame_dim * frame_stack), dtype=int)\n    action_space = gym.spaces.Discrete(4)\n\n    buffer = CircularVecEnvBufferBackend(\n        buffer_capacity=20, num_envs=2, observation_space=observation_space, action_space=action_space,\n        frame_stack_compensation=True, frame_history=frame_stack\n    )\n\n    v1 = np.ones(8 * frame_dim).reshape((2, 2, 2, frame_dim))\n    done_set = {2, 5, 10, 13, 18, 22, 28}\n\n    # simple buffer of previous frames to simulate frame stack\n    item_array = []\n\n    for i in range(30):\n        item = v1.copy()\n\n        item[:, 0] *= (i+1)\n        item[:, 1] *= 10 * (i+1)\n\n        done_array = np.array([i in done_set, (i+1) in done_set], dtype=bool)\n\n        item_array.append(item)\n\n        if len(item_array) < frame_stack:\n            item_concatenated = np.concatenate([item] * frame_stack, axis=-1)\n        else:\n            item_concatenated = np.concatenate(item_array[-frame_stack:], axis=-1)\n\n        buffer.store_transition(item_concatenated, 0, float(i) / 2, done_array)\n\n    return buffer\n\n\ndef test_simple_get_frame():\n    """""" Check if get_frame returns frames from a buffer partially full """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=int)\n    action_space = gym.spaces.Discrete(4)\n    buffer = CircularVecEnvBufferBackend(\n        20, num_envs=2, observation_space=observation_space, action_space=action_space, frame_history=4\n    )\n\n    v1 = np.ones(8).reshape((2, 2, 2, 1))\n    v1[1] *= 2\n\n    v2 = v1 * 2\n    v3 = v1 * 3\n\n    buffer.store_transition(v1, 0, 0, False)\n    buffer.store_transition(v2, 0, 0, False)\n    buffer.store_transition(v3, 0, 0, False)\n\n    assert np.all(buffer.get_frame(0, 0).max(0).max(0) == np.array([0, 0, 0, 1]))\n    assert np.all(buffer.get_frame(1, 0).max(0).max(0) == np.array([0, 0, 1, 2]))\n    assert np.all(buffer.get_frame(2, 0).max(0).max(0) == np.array([0, 1, 2, 3]))\n\n    assert np.all(buffer.get_frame(0, 1).max(0).max(0) == np.array([0, 0, 0, 2]))\n    assert np.all(buffer.get_frame(1, 1).max(0).max(0) == np.array([0, 0, 2, 4]))\n    assert np.all(buffer.get_frame(2, 1).max(0).max(0) == np.array([0, 2, 4, 6]))\n\n    with pytest.raises(VelException):\n        buffer.get_frame(3, 0)\n\n    with pytest.raises(VelException):\n        buffer.get_frame(4, 0)\n\n    with pytest.raises(VelException):\n        buffer.get_frame(3, 1)\n\n    with pytest.raises(VelException):\n        buffer.get_frame(4, 1)\n\n\ndef test_full_buffer_get_frame():\n    """""" Check if get_frame returns frames for full buffer """"""\n    buffer = get_filled_buffer(frame_history=4)\n\n    nt.assert_array_equal(buffer.get_frame(0, 0).max(0).max(0), np.array([18, 19, 20, 21]))\n    nt.assert_array_equal(buffer.get_frame(1, 0).max(0).max(0), np.array([19, 20, 21, 22]))\n    nt.assert_array_equal(buffer.get_frame(9, 0).max(0).max(0), np.array([27, 28, 29, 30]))\n\n    nt.assert_array_equal(buffer.get_frame(0, 1).max(0).max(0), np.array([180, 190, 200, 210]))\n    nt.assert_array_equal(buffer.get_frame(1, 1).max(0).max(0), np.array([190, 200, 210, 220]))\n    nt.assert_array_equal(buffer.get_frame(9, 1).max(0).max(0), np.array([270, 280, 290, 300]))\n\n    with pytest.raises(VelException):\n        buffer.get_frame(10, 0)\n\n    with pytest.raises(VelException):\n        buffer.get_frame(11, 0)\n\n    with pytest.raises(VelException):\n        buffer.get_frame(12, 0)\n\n    with pytest.raises(VelException):\n        buffer.get_frame(10, 1)\n\n    with pytest.raises(VelException):\n        buffer.get_frame(11, 1)\n\n    with pytest.raises(VelException):\n        buffer.get_frame(12, 1)\n\n    nt.assert_array_equal(buffer.get_frame(13, 0).max(0).max(0), np.array([11, 12, 13, 14]))\n    nt.assert_array_equal(buffer.get_frame(19, 0).max(0).max(0), np.array([17, 18, 19, 20]))\n\n    nt.assert_array_equal(buffer.get_frame(13, 1).max(0).max(0), np.array([110, 120, 130, 140]))\n    nt.assert_array_equal(buffer.get_frame(19, 1).max(0).max(0), np.array([170, 180, 190, 200]))\n\n\ndef test_full_buffer_get_future_frame():\n    """""" Check if get_frame_with_future works with full buffer """"""\n    buffer = get_filled_buffer(frame_history=4)\n\n    nt.assert_array_equal(buffer.get_frame_with_future(0, 0)[1].max(0).max(0), np.array([19, 20, 21, 22]))\n    nt.assert_array_equal(buffer.get_frame_with_future(1, 0)[1].max(0).max(0), np.array([20, 21, 22, 23]))\n\n    nt.assert_array_equal(buffer.get_frame_with_future(0, 1)[1].max(0).max(0), np.array([190, 200, 210, 220]))\n    nt.assert_array_equal(buffer.get_frame_with_future(1, 1)[1].max(0).max(0), np.array([200, 210, 220, 230]))\n\n    with pytest.raises(VelException):\n        buffer.get_frame_with_future(9, 0)\n\n    with pytest.raises(VelException):\n        buffer.get_frame_with_future(10, 0)\n\n    with pytest.raises(VelException):\n        buffer.get_frame_with_future(11, 0)\n\n    with pytest.raises(VelException):\n        buffer.get_frame_with_future(12, 0)\n\n    with pytest.raises(VelException):\n        buffer.get_frame_with_future(9, 1)\n\n    with pytest.raises(VelException):\n        buffer.get_frame_with_future(10, 1)\n\n    with pytest.raises(VelException):\n        buffer.get_frame_with_future(11, 1)\n\n    with pytest.raises(VelException):\n        buffer.get_frame_with_future(12, 1)\n\n    nt.assert_array_equal(buffer.get_frame_with_future(13, 0)[1].max(0).max(0), np.array([12, 13, 14, 15]))\n    nt.assert_array_equal(buffer.get_frame_with_future(19, 0)[1].max(0).max(0), np.array([18, 19, 20, 21]))\n\n    nt.assert_array_equal(buffer.get_frame_with_future(13, 1)[1].max(0).max(0), np.array([120, 130, 140, 150]))\n    nt.assert_array_equal(buffer.get_frame_with_future(19, 1)[1].max(0).max(0), np.array([180, 190, 200, 210]))\n\n\ndef test_buffer_filling_size():\n    """""" Check if buffer size is properly updated when we add items """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=int)\n    action_space = gym.spaces.Discrete(4)\n    buffer = CircularVecEnvBufferBackend(20, num_envs=2, observation_space=observation_space, action_space=action_space)\n\n    v1 = np.ones(8).reshape((2, 2, 2, 1))\n\n    assert buffer.current_size == 0\n\n    buffer.store_transition(v1, 0, 0, False)\n    buffer.store_transition(v1, 0, 0, False)\n\n    assert buffer.current_size == 2\n\n    for i in range(30):\n        buffer.store_transition(v1 * (i+1), 0, float(i)/2, False)\n\n    assert buffer.current_size == buffer.buffer_capacity\n\n\ndef test_get_frame_with_dones():\n    """""" Check if get_frame works properly in case there are multiple sequences in buffer """"""\n    buffer = get_filled_buffer_with_dones(frame_history=4)\n\n    nt.assert_array_equal(buffer.get_frame(0, 0).max(0).max(0), np.array([0, 0, 20, 21]))\n    nt.assert_array_equal(buffer.get_frame(1, 0).max(0).max(0), np.array([0, 20, 21, 22]))\n    nt.assert_array_equal(buffer.get_frame(2, 0).max(0).max(0), np.array([20, 21, 22, 23]))\n    nt.assert_array_equal(buffer.get_frame(3, 0).max(0).max(0), np.array([0, 0, 0, 24]))\n\n    nt.assert_array_equal(buffer.get_frame(8, 0).max(0).max(0), np.array([26, 27, 28, 29]))\n    nt.assert_array_equal(buffer.get_frame(9, 0).max(0).max(0), np.array([0, 0, 0, 30]))\n\n    nt.assert_array_equal(buffer.get_frame(0, 1).max(0).max(0), np.array([0, 190, 200, 210]))\n    nt.assert_array_equal(buffer.get_frame(1, 1).max(0).max(0), np.array([190, 200, 210, 220]))\n    nt.assert_array_equal(buffer.get_frame(2, 1).max(0).max(0), np.array([0, 0, 0, 230]))\n    nt.assert_array_equal(buffer.get_frame(3, 1).max(0).max(0), np.array([0, 0, 230, 240]))\n\n    nt.assert_array_equal(buffer.get_frame(8, 1).max(0).max(0), np.array([0, 0, 0, 290]))\n    nt.assert_array_equal(buffer.get_frame(9, 1).max(0).max(0), np.array([0, 0, 290, 300]))\n\n    with pytest.raises(VelException):\n        buffer.get_frame(10, 0)\n\n    with pytest.raises(VelException):\n        buffer.get_frame(10, 1)\n\n    nt.assert_array_equal(buffer.get_frame(11, 0).max(0).max(0), np.array([0, 0, 0, 12]))\n    nt.assert_array_equal(buffer.get_frame(12, 0).max(0).max(0), np.array([0, 0, 12, 13]))\n\n    with pytest.raises(VelException):\n        buffer.get_frame(11, 1)\n\n    with pytest.raises(VelException):\n        buffer.get_frame(12, 1)\n\n\ndef test_get_frame_future_with_dones():\n    """""" Check if get_frame_with_future works properly in case there are multiple sequences in buffer """"""\n    buffer = get_filled_buffer_with_dones(frame_history=4)\n\n    nt.assert_array_equal(buffer.get_frame_with_future(0, 0)[1].max(0).max(0), np.array([0, 20, 21, 22]))\n    nt.assert_array_equal(buffer.get_frame_with_future(1, 0)[1].max(0).max(0), np.array([20, 21, 22, 23]))\n    nt.assert_array_equal(buffer.get_frame_with_future(2, 0)[1].max(0).max(0), np.array([0, 0, 0, 0]))\n\n    nt.assert_array_equal(buffer.get_frame_with_future(3, 0)[1].max(0).max(0), np.array([0, 0, 24, 25]))\n    nt.assert_array_equal(buffer.get_frame_with_future(8, 0)[1].max(0).max(0), np.array([0, 0, 0, 0]))\n\n    nt.assert_array_equal(buffer.get_frame_with_future(0, 1)[1].max(0).max(0), np.array([190, 200, 210, 220]))\n    nt.assert_array_equal(buffer.get_frame_with_future(1, 1)[1].max(0).max(0), np.array([0, 0, 0, 0]))\n    nt.assert_array_equal(buffer.get_frame_with_future(2, 1)[1].max(0).max(0), np.array([0, 0, 230, 240]))\n\n    nt.assert_array_equal(buffer.get_frame_with_future(3, 1)[1].max(0).max(0), np.array([0, 230, 240, 250]))\n    nt.assert_array_equal(buffer.get_frame_with_future(7, 1)[1].max(0).max(0), np.array([0, 0, 0, 0]))\n\n    with pytest.raises(VelException):\n        buffer.get_frame_with_future(9, 0)\n\n    with pytest.raises(VelException):\n        buffer.get_frame_with_future(10, 0)\n\n    nt.assert_array_equal(buffer.get_frame_with_future(11, 0)[1].max(0).max(0), np.array([0, 0, 12, 13]))\n    nt.assert_array_equal(buffer.get_frame_with_future(12, 0)[1].max(0).max(0), np.array([0, 12, 13, 14]))\n\n    with pytest.raises(VelException):\n        buffer.get_frame_with_future(9, 1)\n\n    with pytest.raises(VelException):\n        buffer.get_frame_with_future(10, 1)\n\n    with pytest.raises(VelException):\n        buffer.get_frame(11, 1)\n\n    with pytest.raises(VelException):\n        buffer.get_frame(12, 1)\n\n    nt.assert_array_equal(buffer.get_frame_with_future(13, 1)[1].max(0).max(0), np.array([0, 0, 140, 150]))\n\n\ndef test_get_batch():\n    """""" Check if get_batch works properly for buffers """"""\n    buffer = get_filled_buffer_with_dones(frame_history=4)\n\n    batch = buffer.get_transitions(np.array([\n        [0, 1, 2, 3, 4, 5, 6, 7],  # Frames for env=0\n        [1, 2, 3, 4, 5, 6, 7, 8],  # Frames for env=1\n    ]).T)\n\n    obs = batch[\'observations\']\n    act = batch[\'actions\']\n    rew = batch[\'rewards\']\n    obs_tp1 = batch[\'observations_next\']\n    dones = batch[\'dones\']\n\n    nt.assert_array_equal(dones[:, 0], np.array([False, False, True, False, False, False, False, False]))\n    nt.assert_array_equal(dones[:, 1], np.array([True, False, False, False, False, False, True, False]))\n\n    nt.assert_array_equal(obs[:, 0].max(1).max(1), np.array([\n        [0, 0, 20, 21],\n        [0, 20, 21, 22],\n        [20, 21, 22, 23],\n        [0, 0, 0, 24],\n        [0, 0, 24, 25],\n        [0, 24, 25, 26],\n        [24, 25, 26, 27],\n        [25, 26, 27, 28],\n    ]))\n\n    nt.assert_array_equal(obs[:, 1].max(1).max(1), np.array([\n        [190, 200, 210, 220],\n        [0, 0, 0, 230],\n        [0, 0, 230, 240],\n        [0, 230, 240, 250],\n        [230, 240, 250, 260],\n        [240, 250, 260, 270],\n        [250, 260, 270, 280],\n        [0, 0, 0, 290],\n    ]))\n\n    nt.assert_array_equal(act[:, 0], np.array([0, 0, 0, 0, 0, 0, 0, 0]))\n    nt.assert_array_equal(rew[:, 0], np.array([10.0, 10.5, 11.0, 11.5, 12.0, 12.5, 13.0, 13.5]))\n\n    nt.assert_array_equal(act[:, 1], np.array([0, 0, 0, 0, 0, 0, 0, 0]))\n    nt.assert_array_equal(rew[:, 1], np.array([10.5, 11.0, 11.5, 12.0, 12.5, 13.0, 13.5, 14.0]))\n\n    nt.assert_array_equal(obs_tp1[:, 0].max(1).max(1), np.array([\n        [0, 20, 21, 22],\n        [20, 21, 22, 23],\n        [0, 0, 0, 0],\n        [0, 0, 24, 25],\n        [0, 24, 25, 26],\n        [24, 25, 26, 27],\n        [25, 26, 27, 28],\n        [26, 27, 28, 29]\n    ]))\n\n    nt.assert_array_equal(obs_tp1[:, 1].max(1).max(1), np.array([\n        [0, 0, 0, 0],\n        [0, 0, 230, 240],\n        [0, 230, 240, 250],\n        [230, 240, 250, 260],\n        [240, 250, 260, 270],\n        [250, 260, 270, 280],\n        [0, 0, 0, 0],\n        [0, 0, 290, 300],\n    ]))\n\n    with pytest.raises(VelException):\n        buffer.get_transitions(np.array([\n            [0, 1, 2, 3, 4, 5, 6, 7, 8],\n            [1, 2, 3, 4, 5, 6, 7, 8, 9]\n        ]).T)\n\n\ndef test_sample_and_get_batch():\n    """""" Check if batch sampling works properly """"""\n    buffer = get_filled_buffer_with_dones(frame_history=4)\n\n    for i in range(100):\n        indexes = buffer.sample_batch_transitions(batch_size=5)\n        batch = buffer.get_transitions(indexes)\n\n        obs = batch[\'observations\']\n        act = batch[\'actions\']\n        rew = batch[\'rewards\']\n        obs_tp1 = batch[\'observations_next\']\n        dones = batch[\'dones\']\n\n        with pytest.raises(AssertionError):\n            nt.assert_array_equal(indexes[:, 0], indexes[:, 1])\n\n        assert obs.shape[0] == 5\n        assert act.shape[0] == 5\n        assert rew.shape[0] == 5\n        assert obs_tp1.shape[0] == 5\n        assert dones.shape[0] == 5\n\n\ndef test_storing_extra_info():\n    """""" Make sure additional information are stored and recovered properly """"""\n    buffer = get_filled_buffer_extra_info(frame_history=4)\n\n    indexes = np.array([\n        [0, 1, 2, 17, 18, 19],\n        [0, 1, 2, 17, 18, 19],\n    ]).T\n\n    batch = buffer.get_transitions(indexes)\n\n    nt.assert_equal(batch[\'neglogp\'][0, 0], 20.0/30)\n    nt.assert_equal(batch[\'neglogp\'][1, 0], 21.0/30)\n    nt.assert_equal(batch[\'neglogp\'][2, 0], 22.0/30)\n    nt.assert_equal(batch[\'neglogp\'][3, 0], 17.0/30)\n    nt.assert_equal(batch[\'neglogp\'][4, 0], 18.0/30)\n    nt.assert_equal(batch[\'neglogp\'][5, 0], 19.0/30)\n\n    nt.assert_equal(batch[\'neglogp\'][0, 1], 21.0/30)\n    nt.assert_equal(batch[\'neglogp\'][1, 1], 22.0/30)\n    nt.assert_equal(batch[\'neglogp\'][2, 1], 23.0/30)\n    nt.assert_equal(batch[\'neglogp\'][3, 1], 18.0/30)\n    nt.assert_equal(batch[\'neglogp\'][4, 1], 19.0/30)\n    nt.assert_equal(batch[\'neglogp\'][5, 1], 20.0/30)\n\n\ndef test_sample_rollout_half_filled():\n    """""" Test if sampling rollout is correct and returns proper results """"""\n    buffer = get_half_filled_buffer(frame_history=4)\n\n    indexes = []\n\n    for i in range(1000):\n        rollout_idx = buffer.sample_batch_trajectories(rollout_length=5)\n        rollout = buffer.get_trajectories(indexes=rollout_idx, rollout_length=5)\n\n        assert rollout[\'observations\'].shape[0] == 5  # Rollout length\n        assert rollout[\'observations\'].shape[-1] == 4  # History length\n\n        indexes.append(rollout_idx)\n\n    assert np.min(indexes) == 4\n    assert np.max(indexes) == 8\n\n    with pytest.raises(VelException):\n        buffer.sample_batch_trajectories(rollout_length=10)\n\n    rollout_idx = buffer.sample_batch_trajectories(rollout_length=9)\n    rollout = buffer.get_trajectories(indexes=rollout_idx, rollout_length=9)\n\n    nt.assert_array_equal(rollout_idx, np.array([8, 8]))\n\n    nt.assert_array_equal(rollout[\'rewards\'], np.array([\n        [0., 0.5, 1., 1.5, 2., 2.5, 3., 3.5, 4.],\n        [0., 0.5, 1., 1.5, 2., 2.5, 3., 3.5, 4.],\n    ]).T)\n\n\ndef test_sample_rollout_filled():\n    """""" Test if sampling rollout is correct and returns proper results """"""\n    buffer = get_filled_buffer(frame_history=4)\n\n    indexes = []\n\n    for i in range(1000):\n        rollout_idx = buffer.sample_batch_trajectories(rollout_length=5)\n        rollout = buffer.get_trajectories(indexes=rollout_idx, rollout_length=5)\n\n        assert rollout[\'observations\'].shape[0] == 5  # Rollout length\n        assert rollout[\'observations\'].shape[-1] == 4  # History length\n\n        indexes.append(rollout_idx)\n\n    assert np.min(indexes) == 0\n    assert np.max(indexes) == 19\n\n    with pytest.raises(VelException):\n        buffer.sample_batch_trajectories(rollout_length=17)\n\n    max_rollout = buffer.sample_batch_trajectories(rollout_length=16)\n\n    rollout = buffer.get_trajectories(max_rollout, rollout_length=16)\n\n    nt.assert_array_equal(max_rollout, np.array([8, 8]))\n    assert np.sum(rollout[\'rewards\']) == pytest.approx(164.0 * 2, 1e-5)\n\n\ndef test_buffer_flexible_obs_action_sizes():\n    b1x1 = get_filled_buffer1x1(frame_history=1)\n    b2x2 = get_filled_buffer2x2(frame_history=1)\n    b3x3 = get_filled_buffer3x3(frame_history=1)\n\n    nt.assert_array_almost_equal(b1x1.get_frame(0, 0), np.array([21, 210]))\n    nt.assert_array_almost_equal(b2x2.get_frame(0, 0), np.array([[21, 21], [210, 210]]))\n    nt.assert_array_almost_equal(b3x3.get_frame(0, 0), np.array([[[21, 21], [21, 21]], [[210, 210], [210, 210]]]))\n\n    nt.assert_array_almost_equal(b1x1.get_transition(0, 0)[\'actions\'], np.array([0, 20]))\n    nt.assert_array_almost_equal(b2x2.get_transition(0, 0)[\'actions\'], np.array([[0, 20], [40, 60]]))\n    nt.assert_array_almost_equal(b3x3.get_transition(0, 0)[\'actions\'], np.array(\n        [\n            [[0, 20], [40, 60]],\n            [[80, 100], [120, 140]]\n        ]\n    ))\n\n\ndef test_buffer_flexible_obs_action_sizes_with_history():\n    b1x1 = get_filled_buffer1x1_history(frame_history=2)\n    b2x2 = get_filled_buffer2x2_history(frame_history=2)\n    b3x3 = get_filled_buffer3x3_history(frame_history=2)\n\n    nt.assert_array_almost_equal(b1x1.get_frame(0, 0), np.array([[20, 21], [200, 210]]))\n    nt.assert_array_almost_equal(b2x2.get_frame(0, 0), np.array([[[20, 21], [20, 21]], [[200, 210], [200, 210]]]))\n    nt.assert_array_almost_equal(b3x3.get_frame(0, 0), np.array(\n        [[[[20, 21], [20, 21]], [[20, 21], [20, 21]]], [[[200, 210], [200, 210]], [[200, 210], [200, 210]]]]\n    ))\n\n    nt.assert_array_almost_equal(b1x1.get_transition(0, 0)[\'observations_next\'], np.array([[21, 22], [210, 220]]))\n    nt.assert_array_almost_equal(b2x2.get_transition(0, 0)[\'observations_next\'], np.array(\n        [[[21, 22], [21, 22]], [[210, 220], [210, 220]]]\n    ))\n    nt.assert_array_almost_equal(b3x3.get_transition(0, 0)[\'observations_next\'], np.array(\n        [[[[21, 22], [21, 22]], [[21, 22], [21, 22]]],\n         [[[210, 220], [210, 220]], [[210, 220], [210, 220]]]]\n    ))\n\n\ndef test_frame_stack_compensation_single_dim():\n    buffer = get_filled_buffer_frame_stack(frame_stack=4, frame_dim=1)\n\n    observations_1 = buffer.get_transition(frame_idx=0, env_idx=0)[\'observations\']\n    observations_2 = buffer.get_transition(frame_idx=1, env_idx=0)[\'observations\']\n    observations_3 = buffer.get_transition(frame_idx=2, env_idx=0)[\'observations\']\n\n    nt.assert_array_almost_equal(\n        observations_1, np.array([[[0,   0,  20,  21],\n                                   [0,   0,  20,  21]],\n                                  [[0,   0, 200, 210],\n                                   [0,   0, 200, 210]]])\n    )\n\n    nt.assert_array_almost_equal(\n        observations_2, np.array([[[0,  20,  21,  22],\n                                   [0,  20,  21,  22]],\n                                  [[0, 200, 210, 220],\n                                   [0, 200, 210, 220]]])\n    )\n\n    nt.assert_array_almost_equal(\n        observations_3, np.array([[[20,  21,  22,  23],\n                                   [20,  21,  22,  23]],\n                                  [[200, 210, 220, 230],\n                                   [200, 210, 220, 230]]])\n    )\n\n\ndef test_frame_stack_compensation_multi_dim():\n    buffer = get_filled_buffer_frame_stack(frame_stack=4, frame_dim=2)\n\n    observations_1 = buffer.get_transition(frame_idx=0, env_idx=0)[\'observations\']\n    observations_2 = buffer.get_transition(frame_idx=1, env_idx=0)[\'observations\']\n    observations_3 = buffer.get_transition(frame_idx=2, env_idx=0)[\'observations\']\n\n    nt.assert_array_almost_equal(\n        observations_1, np.array([[[0,   0,   0,   0,  20,  20,  21,  21],\n                                   [0,   0,   0,   0,  20,  20,  21,  21]],\n                                  [[0,   0,   0,   0, 200, 200, 210, 210],\n                                   [0,   0,   0,   0, 200, 200, 210, 210]]])\n    )\n\n    nt.assert_array_almost_equal(\n        observations_2, np.array([[[0, 0, 20, 20, 21, 21, 22, 22],\n                                   [0, 0, 20, 20, 21, 21, 22, 22]],\n                                  [[0, 0, 200, 200, 210, 210, 220, 220],\n                                   [0, 0, 200, 200, 210, 210, 220, 220]]])\n    )\n\n    nt.assert_array_almost_equal(\n        observations_3, np.array([[[20, 20, 21, 21, 22, 22, 23, 23],\n                                   [20, 20, 21, 21, 22, 22, 23, 23]],\n                                  [[200, 200, 210, 210, 220, 220, 230, 230],\n                                   [200, 200, 210, 210, 220, 220, 230, 230]]])\n    )\n\n\ndef test_get_frame_with_future_forward_steps_exceptions():\n    """"""\n    Test function get_frame_with_future_forward_steps.\n\n    Does it throw vel exception properly if and only if cannot provide enough future frames.\n    """"""\n    buffer = get_filled_buffer_frame_stack(frame_stack=4, frame_dim=2)\n\n    buffer.get_frame_with_future_forward_steps(0, 0, forward_steps=2, discount_factor=0.9)\n    buffer.get_frame_with_future_forward_steps(1, 0, forward_steps=2, discount_factor=0.9)\n    buffer.get_frame_with_future_forward_steps(2, 0, forward_steps=2, discount_factor=0.9)\n    buffer.get_frame_with_future_forward_steps(3, 0, forward_steps=2, discount_factor=0.9)\n    buffer.get_frame_with_future_forward_steps(4, 0, forward_steps=2, discount_factor=0.9)\n    buffer.get_frame_with_future_forward_steps(5, 0, forward_steps=2, discount_factor=0.9)\n    buffer.get_frame_with_future_forward_steps(6, 0, forward_steps=2, discount_factor=0.9)\n    buffer.get_frame_with_future_forward_steps(7, 0, forward_steps=2, discount_factor=0.9)\n\n    with pytest.raises(VelException):\n        # No future for the frame\n        buffer.get_frame_with_future_forward_steps(8, 0, forward_steps=2, discount_factor=0.9)\n\n    with pytest.raises(VelException):\n        # No future for the frame\n        buffer.get_frame_with_future_forward_steps(9, 0, forward_steps=2, discount_factor=0.9)\n\n    with pytest.raises(VelException):\n        # No history for the frame\n        buffer.get_frame_with_future_forward_steps(10, 0, forward_steps=2, discount_factor=0.9)\n\n    buffer.get_frame_with_future_forward_steps(11, 0, forward_steps=2, discount_factor=0.9)\n    buffer.get_frame_with_future_forward_steps(12, 0, forward_steps=2, discount_factor=0.9)\n    buffer.get_frame_with_future_forward_steps(13, 0, forward_steps=2, discount_factor=0.9)\n    buffer.get_frame_with_future_forward_steps(14, 0, forward_steps=2, discount_factor=0.9)\n    buffer.get_frame_with_future_forward_steps(15, 0, forward_steps=2, discount_factor=0.9)\n    buffer.get_frame_with_future_forward_steps(16, 0, forward_steps=2, discount_factor=0.9)\n    buffer.get_frame_with_future_forward_steps(17, 0, forward_steps=2, discount_factor=0.9)\n    buffer.get_frame_with_future_forward_steps(18, 0, forward_steps=2, discount_factor=0.9)\n    buffer.get_frame_with_future_forward_steps(19, 0, forward_steps=2, discount_factor=0.9)\n\n    with pytest.raises(VelException):\n        # Index beyond buffer size\n        buffer.get_frame_with_future_forward_steps(20, 0, forward_steps=2, discount_factor=0.9)\n\n\ndef test_get_frame_with_future_forward_steps_with_dones():\n    """"""\n    Test function get_frame_with_future_forward_steps.\n\n    Does it return empty frame if there is a done in between.\n    Does it return correct rewards if there is a done in between\n    Does it return correct rewards if there is no done in between\n    """"""\n    buffer = get_filled_buffer_frame_stack(frame_stack=4, frame_dim=2)\n\n    # Just a check to be sure\n    nt.assert_array_equal(\n        buffer.dones_buffer[:, 0],\n        np.array([\n            False, False, True, False, False, False, False, False, True, False,\n            True, False, False, True, False, False, False, False, True, False\n        ])\n    )\n\n    nt.assert_array_equal(\n        buffer.reward_buffer[:, 0],\n        np.array([\n            10., 10.5, 11., 11.5, 12., 12.5, 13., 13.5, 14., 14.5, 5., 5.5, 6., 6.5, 7., 7.5, 8., 8.5, 9., 9.5\n        ])\n    )\n\n    for i in [0, 3, 4, 5, 6, 11, 14, 15, 19]:\n        result = buffer.get_frame_with_future_forward_steps(i, 0, forward_steps=2, discount_factor=0.9)\n\n        next_frame = result[1]\n        reward = result[2]\n        done = result[3]\n\n        assert next_frame.max() != 0\n        assert done is False\n        assert reward == buffer.reward_buffer[i, 0] + 0.9 * buffer.reward_buffer[(i+1) % 20, 0]\n\n    for i in [1, 2, 7,  12, 13, 17, 18]:\n        result = buffer.get_frame_with_future_forward_steps(i, 0, forward_steps=2, discount_factor=0.9)\n\n        next_frame = result[1]\n        done = result[3]\n\n        assert next_frame.max() == 0\n        assert done is True\n\n    for i in [1, 7, 12, 17]:\n        result = buffer.get_frame_with_future_forward_steps(i, 0, forward_steps=2, discount_factor=0.9)\n        reward = result[2]\n        assert reward == buffer.reward_buffer[i, 0] + 0.9 * buffer.reward_buffer[(i+1) % 20, 0]\n\n    for i in [2, 13, 18]:\n        result = buffer.get_frame_with_future_forward_steps(i, 0, forward_steps=2, discount_factor=0.9)\n        reward = result[2]\n        assert reward == buffer.reward_buffer[i, 0]\n\n\ndef test_get_frame_with_future_forward_steps_without_dones():\n    """"""\n    Test function get_frame_with_future_forward_steps.\n\n    Does it return correct frame if there is no done in between\n    """"""\n    buffer = get_filled_buffer_frame_stack(frame_stack=4, frame_dim=2)\n\n    result = buffer.get_frame_with_future_forward_steps(0, 0, forward_steps=2, discount_factor=0.9)\n\n    frame = result[0]\n    future_frame = result[1]\n\n    nt.assert_array_equal(\n        frame,\n        np.array([[[0, 0, 0, 0, 20, 20, 21, 21],\n                   [0, 0, 0, 0, 20, 20, 21, 21]],\n                  [[0, 0, 0, 0, 200, 200, 210, 210],\n                   [0, 0, 0, 0, 200, 200, 210, 210]]])\n    )\n\n    nt.assert_array_equal(\n        future_frame,\n        np.array([[[20, 20, 21, 21, 22, 22, 23, 23],\n                   [20, 20, 21, 21, 22, 22, 23, 23]],\n                  [[200, 200, 210, 210, 220, 220, 230, 230],\n                   [200, 200, 210, 210, 220, 220, 230, 230]]])\n    )\n\n    result = buffer.get_frame_with_future_forward_steps(3, 0, forward_steps=4, discount_factor=0.9)\n\n    frame = result[0]\n    future_frame = result[1]\n\n    nt.assert_array_equal(\n        frame,\n        np.array([[[0, 0, 0, 0, 0, 0, 24, 24],\n                   [0, 0, 0, 0, 0, 0, 24, 24]],\n                  [[0, 0, 0, 0, 0, 0, 240, 240],\n                   [0, 0, 0, 0, 0, 0, 240, 240]]])\n    )\n\n    nt.assert_array_equal(\n        future_frame,\n        np.array([[[25, 25, 26, 26, 27, 27, 28, 28],\n                   [25, 25, 26, 26, 27, 27, 28, 28]],\n                  [[250, 250, 260, 260, 270, 270, 280, 280],\n                   [250, 250, 260, 260, 270, 270, 280, 280]]])\n    )\n\n    result = buffer.get_frame_with_future_forward_steps(19, 0, forward_steps=2, discount_factor=0.9)\n\n    frame = result[0]\n    future_frame = result[1]\n\n    nt.assert_array_equal(\n        frame,\n        np.array([[[0, 0, 0, 0, 0, 0, 20, 20],\n                   [0, 0, 0, 0, 0, 0, 20, 20]],\n                  [[0, 0, 0, 0, 0, 0, 200, 200],\n                   [0, 0, 0, 0, 0, 0, 200, 200]]])\n    )\n\n    nt.assert_array_equal(\n        future_frame,\n        np.array([[[0, 0, 20, 20, 21, 21, 22, 22],\n                   [0, 0, 20, 20, 21, 21, 22, 22]],\n                  [[0, 0, 200, 200, 210, 210, 220, 220],\n                   [0, 0, 200, 200, 210, 210, 220, 220]]])\n    )\n'"
vel/rl/buffers/tests/test_prioritized_circular_buffer_backend.py,0,"b'import collections\nimport gym\nimport gym.spaces\nimport numpy as np\nimport numpy.testing as nt\nimport pytest\n\nfrom vel.exceptions import VelException\nfrom vel.rl.buffers.backend.prioritized_buffer_backend import PrioritizedCircularBufferBackend\n\n\ndef get_halfempty_buffer_with_dones():\n    """""" Return simple preinitialized buffer with some done\'s in there """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=np.uint8)\n    action_space = gym.spaces.Discrete(4)\n    buffer = PrioritizedCircularBufferBackend(20, observation_space, action_space)\n\n    v1 = np.ones(4).reshape((2, 2, 1))\n\n    done_set = {2, 5, 10, 13, 18, 22, 28}\n\n    for i in range(10):\n        if i in done_set:\n            buffer.store_transition(v1 * (i+1), 0, float(i)/2, True)\n        else:\n            buffer.store_transition(v1 * (i+1), 0, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer_with_dones():\n    """""" Return simple preinitialized buffer with some done\'s in there """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=np.uint8)\n    action_space = gym.spaces.Discrete(4)\n    buffer = PrioritizedCircularBufferBackend(20, observation_space, action_space)\n\n    v1 = np.ones(4).reshape((2, 2, 1))\n\n    done_set = {2, 5, 10, 13, 18, 22, 28}\n\n    for i in range(30):\n        if i in done_set:\n            buffer.store_transition(v1 * (i+1), 0, float(i)/2, True)\n        else:\n            buffer.store_transition(v1 * (i+1), 0, float(i)/2, False)\n\n    return buffer\n\n\ndef get_large_filled_buffer_with_dones():\n    """""" Return simple preinitialized buffer with some done\'s in there """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=np.uint8)\n    action_space = gym.spaces.Discrete(4)\n    buffer = PrioritizedCircularBufferBackend(2000, observation_space, action_space)\n\n    v1 = np.ones(4).reshape((2, 2, 1))\n\n    done_increment = 2\n    done_index = 2\n\n    for i in range(3000):\n        if i == done_index:\n            done_increment += 1\n            done_index += done_increment\n\n            buffer.store_transition(v1 * (i+1), 0, float(i)/2, True)\n        else:\n            buffer.store_transition(v1 * (i+1), 0, float(i)/2, False)\n\n    return buffer\n\n\ndef test_sampling_is_correct():\n    """""" Check if sampling multiple times we don\'t get incorrect values""""""\n    buffer = get_filled_buffer_with_dones()\n\n    for i in range(100):\n        probs, idxs, tree_idxs = buffer.sample_batch_prioritized(6, history=4)\n        buffer.get_batch(idxs, history=4)\n\n        nt.assert_array_equal(np.array(probs), np.ones(6))\n\n    with pytest.raises(VelException):\n        buffer.get_batch(np.array([10]), history=4)\n\n\ndef test_sampling_is_correct_small_buffer():\n    """""" Check if sampling multiple times we don\'t get incorrect values""""""\n    buffer = get_halfempty_buffer_with_dones()\n\n    for i in range(100):\n        probs, idxs, tree_idxs = buffer.sample_batch_prioritized(6, history=4)\n        buffer.get_batch(idxs, history=4)\n\n        nt.assert_array_equal(np.array(probs), np.ones(6))\n        assert np.all(idxs <= 10)\n\n    with pytest.raises(VelException):\n        buffer.get_batch(np.array([10]), history=4)\n\n\ndef test_prioritized_sampling_probabilities():\n    """""" Check if sampling probabilities are more or less correct in the sampling results """"""\n    buffer = get_large_filled_buffer_with_dones()\n\n    zero_tree_idx = buffer.segment_tree.tree_index_for_index(0)\n\n    # Give much more priority to specified element\n    high_prio = 100.0\n    buffer.update_priority(zero_tree_idx, high_prio)\n\n    counter = collections.Counter()\n\n    for i in range(1000):\n        probs, idxs, tree_idxs = buffer.sample_batch_prioritized(6, history=4)\n        counter.update(idxs)\n\n    # Element with the highest priority is the one that happens the most often\n    assert counter[0] == max(counter.values())\n\n    # Make priority low now\n    buffer.update_priority(zero_tree_idx, 0.01)\n\n    counter = collections.Counter()\n\n    for i in range(1000):\n        probs, idxs, tree_idxs = buffer.sample_batch_prioritized(6, history=4)\n        counter.update(idxs)\n\n    # At least half of the element have greater counts than zero\n    assert np.mean([1 if counter.get(i, 0) > counter.get(0, 0) else 0 for i in range(2000)]) > 0.5\n'"
vel/rl/buffers/tests/test_prioritized_vec_buffer_backend.py,0,"b'import collections\nimport gym\nimport gym.spaces\nimport numpy as np\nimport numpy.testing as nt\nimport pytest\n\nfrom vel.exceptions import VelException\nfrom vel.rl.buffers.backend.prioritized_vec_buffer_backend import PrioritizedCircularVecEnvBufferBackend\n\n\ndef get_halfempty_buffer_with_dones(frame_history=1):\n    """""" Return simple preinitialized buffer with some done\'s in there """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=np.uint8)\n    action_space = gym.spaces.Discrete(4)\n\n    buffer = PrioritizedCircularVecEnvBufferBackend(\n        buffer_capacity=20, num_envs=2, observation_space=observation_space, action_space=action_space,\n        frame_history=frame_history\n    )\n\n    v1 = np.ones(8).reshape((2, 2, 2, 1))\n\n    done_set = {2, 5, 10, 13, 18, 22, 28}\n\n    for i in range(10):\n        if i in done_set:\n            buffer.store_transition(v1 * (i+1), 0, float(i)/2, True)\n        else:\n            buffer.store_transition(v1 * (i+1), 0, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer_with_dones(frame_history=1):\n    """""" Return simple preinitialized buffer with some done\'s in there """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=np.uint8)\n    action_space = gym.spaces.Discrete(4)\n\n    buffer = PrioritizedCircularVecEnvBufferBackend(\n        buffer_capacity=20, num_envs=2, observation_space=observation_space, action_space=action_space,\n        frame_history=frame_history\n    )\n\n    v1 = np.ones(8).reshape((2, 2, 2, 1))\n\n    done_set = {2, 5, 10, 13, 18, 22, 28}\n\n    for i in range(30):\n        if i in done_set:\n            buffer.store_transition(v1 * (i+1), 0, float(i)/2, True)\n        else:\n            buffer.store_transition(v1 * (i+1), 0, float(i)/2, False)\n\n    return buffer\n\n\ndef get_large_filled_buffer_with_dones(frame_history=1):\n    """""" Return simple preinitialized buffer with some done\'s in there """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, 1), dtype=np.uint8)\n    action_space = gym.spaces.Discrete(4)\n\n    buffer = PrioritizedCircularVecEnvBufferBackend(\n        buffer_capacity=2000, num_envs=2, observation_space=observation_space, action_space=action_space,\n        frame_history=frame_history\n    )\n\n    v1 = np.ones(8).reshape((2, 2, 2, 1))\n\n    done_increment = 2\n    done_index = 2\n\n    for i in range(3000):\n        if i == done_index:\n            done_increment += 1\n            done_index += done_increment\n\n            buffer.store_transition(v1 * (i+1), 0, float(i)/2, True)\n        else:\n            buffer.store_transition(v1 * (i+1), 0, float(i)/2, False)\n\n    return buffer\n\n\ndef get_filled_buffer_frame_stack(frame_stack=4, frame_dim=1):\n    """""" Return a preinitialized buffer with frame stack implemented """"""\n    observation_space = gym.spaces.Box(low=0, high=255, shape=(2, 2, frame_dim * frame_stack), dtype=int)\n    action_space = gym.spaces.Discrete(4)\n\n    buffer = PrioritizedCircularVecEnvBufferBackend(\n        buffer_capacity=20, num_envs=2, observation_space=observation_space, action_space=action_space,\n        frame_stack_compensation=True, frame_history=frame_stack\n    )\n\n    v1 = np.ones(8 * frame_dim).reshape((2, 2, 2, frame_dim))\n    done_set = {2, 5, 10, 13, 18, 22, 28}\n\n    # simple buffer of previous frames to simulate frame stack\n    item_array = []\n\n    for i in range(30):\n        item = v1.copy()\n\n        item[:, 0] *= (i+1)\n        item[:, 1] *= 10 * (i+1)\n\n        done_array = np.array([i in done_set, (i+1) in done_set], dtype=bool)\n\n        item_array.append(item)\n\n        if len(item_array) < frame_stack:\n            item_concatenated = np.concatenate([item] * frame_stack, axis=-1)\n        else:\n            item_concatenated = np.concatenate(item_array[-frame_stack:], axis=-1)\n\n        buffer.store_transition(item_concatenated, 0, float(i) / 2, done_array)\n\n    return buffer\n\n\ndef test_sampling_is_correct():\n    """""" Check if sampling multiple times we don\'t get incorrect values""""""\n    buffer = get_filled_buffer_with_dones(frame_history=4)\n\n    for i in range(100):\n        probs, idxs, tree_idxs = buffer.sample_batch_transitions(6)\n        buffer.get_transitions(idxs)\n\n        nt.assert_array_equal(probs, np.ones((6, 2)))\n\n    with pytest.raises(VelException):\n        buffer.get_transitions(np.array([[10, 10]]))\n\n\ndef test_sampling_is_correct_small_buffer():\n    """""" Check if sampling multiple times we don\'t get incorrect values""""""\n    buffer = get_halfempty_buffer_with_dones(frame_history=4)\n\n    for i in range(100):\n        probs, idxs, tree_idxs = buffer.sample_batch_transitions(6)\n        buffer.get_transitions(idxs)\n\n        nt.assert_array_equal(np.array(probs), np.ones((6, 2)))\n        assert np.all(idxs <= 10)\n\n    with pytest.raises(VelException):\n        buffer.get_transitions(np.array([[10, 10]]))\n\n\ndef test_prioritized_sampling_probabilities():\n    """""" Check if sampling probabilities are more or less correct in the sampling results """"""\n    buffer = get_large_filled_buffer_with_dones(frame_history=4)\n\n    zero_tree_idx = [tree.tree_index_for_index(0) for tree in buffer.segment_trees]\n\n    # Give much more priority to specified element\n    high_prio = [100.0 for _ in zero_tree_idx]\n    buffer.update_priority(zero_tree_idx, high_prio)\n\n    counter = collections.Counter()\n\n    for i in range(1000):\n        probs, idxs, tree_idxs = buffer.sample_batch_transitions(6)\n\n        for idx in idxs:\n            counter.update(idx)\n\n    # Element with the highest priority is the one that happens the most often\n    assert counter[0] == max(counter.values())\n\n    # Make priority low now\n    buffer.update_priority(zero_tree_idx, [0.01 for _ in zero_tree_idx])\n\n    counter = collections.Counter()\n\n    for i in range(1000):\n        probs, idxs, tree_idxs = buffer.sample_batch_transitions(6)\n\n        for idx in idxs:\n            counter.update(idx)\n\n    # At least half of the element have greater counts than zero\n    assert np.mean([1 if counter.get(i, 0) > counter.get(0, 0) else 0 for i in range(2000)]) > 0.5\n\n\ndef test_frame_stack_compensation_single_dim():\n    buffer = get_filled_buffer_frame_stack(frame_stack=4, frame_dim=1)\n\n    observations_1 = buffer.get_transition(frame_idx=0, env_idx=0)[\'observations\']\n    observations_2 = buffer.get_transition(frame_idx=1, env_idx=0)[\'observations\']\n    observations_3 = buffer.get_transition(frame_idx=2, env_idx=0)[\'observations\']\n\n    nt.assert_array_almost_equal(\n        observations_1, np.array([[[0,   0,  20,  21],\n                                   [0,   0,  20,  21]],\n                                  [[0,   0, 200, 210],\n                                   [0,   0, 200, 210]]])\n    )\n\n    nt.assert_array_almost_equal(\n        observations_2, np.array([[[0,  20,  21,  22],\n                                   [0,  20,  21,  22]],\n                                  [[0, 200, 210, 220],\n                                   [0, 200, 210, 220]]])\n    )\n\n    nt.assert_array_almost_equal(\n        observations_3, np.array([[[ 20,  21,  22,  23],\n                                   [ 20,  21,  22,  23]],\n                                  [[200, 210, 220, 230],\n                                   [200, 210, 220, 230]]])\n    )\n\n\ndef test_frame_stack_compensation_multi_dim():\n    buffer = get_filled_buffer_frame_stack(frame_stack=4, frame_dim=2)\n\n    observations_1 = buffer.get_transition(frame_idx=0, env_idx=0)[\'observations\']\n    observations_2 = buffer.get_transition(frame_idx=1, env_idx=0)[\'observations\']\n    observations_3 = buffer.get_transition(frame_idx=2, env_idx=0)[\'observations\']\n\n    nt.assert_array_almost_equal(\n        observations_1, np.array([[[0,   0,   0,   0,  20,  20,  21,  21],\n                                   [0,   0,   0,   0,  20,  20,  21,  21]],\n                                  [[0,   0,   0,   0, 200, 200, 210, 210],\n                                   [0,   0,   0,   0, 200, 200, 210, 210]]])\n    )\n\n    nt.assert_array_almost_equal(\n        observations_2, np.array([[[0, 0, 20, 20, 21, 21, 22, 22],\n                                   [0, 0, 20, 20, 21, 21, 22, 22]],\n                                  [[0, 0, 200, 200, 210, 210, 220, 220],\n                                   [0, 0, 200, 200, 210, 210, 220, 220]]])\n    )\n\n    nt.assert_array_almost_equal(\n        observations_3, np.array([[[20, 20, 21, 21, 22, 22, 23, 23],\n                                   [20, 20, 21, 21, 22, 22, 23, 23]],\n                                  [[200, 200, 210, 210, 220, 220, 230, 230],\n                                   [200, 200, 210, 210, 220, 220, 230, 230]]])\n    )\n'"
vel/rl/env/wrappers/__init__.py,0,b''
vel/rl/env/wrappers/clip_episode_length.py,0,"b'import gym\n\n\nclass ClipEpisodeLengthWrapper(gym.Wrapper):\n    """""" Env wrapper that clips number of frames an episode can last """"""\n    def __init__(self, env, max_episode_length):\n        super().__init__(env)\n\n        self.max_episode_length = max_episode_length\n        self.current_episode_length = 0\n\n    def reset(self, **kwargs):\n        self.current_episode_length = 0\n        return self.env.reset(**kwargs)\n\n    def step(self, action):\n        self.current_episode_length += 1\n        ob, reward, done, info = self.env.step(action)\n\n        if self.current_episode_length > self.max_episode_length:\n            done = True\n            info[\'clipped_length\'] = True\n\n        return ob, reward, done, info\n'"
vel/rl/env/wrappers/env_normalize.py,0,"b'import gym\nimport numpy as np\n\nfrom vel.openai.baselines.common.running_mean_std import RunningMeanStd\n\n\nclass EnvNormalize(gym.Wrapper):\n    """"""\n    Single environment normalization based on VecNormalize from OpenAI baselines\n    """"""\n    def __init__(self, env, normalize_observations=True, normalize_returns=True,\n                 clip_observations=10., clip_rewards=10., gamma=0.99, epsilon=1e-8):\n        super().__init__(env)\n\n        self.ob_rms = RunningMeanStd(shape=self.observation_space.shape) if normalize_observations else None\n        self.ret_rms = RunningMeanStd(shape=()) if normalize_returns else None\n        self.clipob = clip_observations\n        self.cliprew = clip_rewards\n        self.ret = 0.0\n        self.gamma = gamma\n        self.epsilon = epsilon\n\n    def step(self, action):\n        """"""\n        Apply sequence of actions to sequence of environments\n        actions -> (observations, rewards, news)\n\n        where \'news\' is a boolean vector indicating whether each element is new.\n        """"""\n        obs, rews, news, infos = self.env.step(action)\n\n        self.ret = self.ret * self.gamma + rews\n\n        obs = self._filter_observation(obs)\n\n        if self.ret_rms:\n            self.ret_rms.update(np.array([self.ret]))\n            rews = np.clip(rews / np.sqrt(self.ret_rms.var + self.epsilon), -self.cliprew, self.cliprew)\n\n        return obs, rews, news, infos\n\n    def _filter_observation(self, obs):\n        if self.ob_rms:\n            self.ob_rms.update(obs[None])\n            obs = np.clip((obs - self.ob_rms.mean) / np.sqrt(self.ob_rms.var + self.epsilon), -self.clipob, self.clipob)\n\n            return obs.astype(np.float32)\n        else:\n            return obs\n\n    def reset(self):\n        """"""\n        Reset all environments\n        """"""\n        obs = self.env.reset()\n        return self._filter_observation(obs)\n'"
vel/rl/models/backbone/__init__.py,0,b''
vel/rl/models/backbone/double_nature_cnn.py,3,"b'""""""\nCode based loosely on implementation:\nhttps://github.com/openai/baselines/blob/master/baselines/ppo2/policies.py\n\nUnder MIT license.\n""""""\nimport numpy as np\n\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\nimport vel.util.network as net_util\n\nfrom vel.api import LinearBackboneModel, ModelFactory\n\n\nclass DoubleNatureCnn(LinearBackboneModel):\n    """"""\n    Neural network as defined in the paper \'Human-level control through deep reinforcement learning\'\n    but with two separate heads.\n    """"""\n    def __init__(self, input_width, input_height, input_channels, output_dim=512):\n        super().__init__()\n\n        self._output_dim = output_dim\n\n        self.conv1 = nn.Conv2d(\n            in_channels=input_channels,\n            out_channels=32,\n            kernel_size=(8, 8),\n            stride=4\n        )\n\n        self.conv2 = nn.Conv2d(\n            in_channels=32,\n            out_channels=64,\n            kernel_size=(4, 4),\n            stride=2\n        )\n\n        self.conv3 = nn.Conv2d(\n            in_channels=64,\n            out_channels=64,\n            kernel_size=(3, 3),\n            stride=1\n        )\n\n        self.final_width = net_util.convolutional_layer_series(input_width, [\n            (8, 0, 4),\n            (4, 0, 2),\n            (3, 0, 1)\n        ])\n\n        self.final_height = net_util.convolutional_layer_series(input_height, [\n            (8, 0, 4),\n            (4, 0, 2),\n            (3, 0, 1)\n        ])\n\n        self.linear_layer_one = nn.Linear(\n            self.final_width * self.final_height * 64,\n            self.output_dim\n        )\n\n        self.linear_layer_two = nn.Linear(\n            self.final_width * self.final_height * 64,\n            self.output_dim\n        )\n\n    @property\n    def output_dim(self) -> int:\n        """""" Final dimension of model output """"""\n        return self._output_dim\n\n    def reset_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                init.orthogonal_(m.weight, gain=np.sqrt(2))\n                init.constant_(m.bias, 0.0)\n            elif isinstance(m, nn.Linear):\n                # init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                init.orthogonal_(m.weight, gain=np.sqrt(2))\n                init.constant_(m.bias, 0.0)\n\n    def forward(self, image):\n        result = image\n        result = F.relu(self.conv1(result))\n        result = F.relu(self.conv2(result))\n        result = F.relu(self.conv3(result))\n        flattened = result.view(result.size(0), -1)\n\n        output_one = F.relu(self.linear_layer_one(flattened))\n        output_two = F.relu(self.linear_layer_two(flattened))\n\n        return output_one, output_two\n\n\ndef create(input_width, input_height, input_channels=1):\n    """""" Vel factory function """"""\n    def instantiate(**_):\n        return DoubleNatureCnn(input_width=input_width, input_height=input_height, input_channels=input_channels)\n\n    return ModelFactory.generic(instantiate)\n\n\nDoubleNatureCnnFactory = create\n'"
vel/rl/models/backbone/double_noisy_nature_cnn.py,3,"b'""""""\nCode based loosely on implementation:\nhttps://github.com/openai/baselines/blob/master/baselines/ppo2/policies.py\n\nUnder MIT license.\n""""""\nimport numpy as np\n\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\nimport vel.util.network as net_util\n\nfrom vel.api import LinearBackboneModel, ModelFactory\nfrom vel.rl.modules.noisy_linear import NoisyLinear\n\n\nclass DoubleNoisyNatureCnn(LinearBackboneModel):\n    """"""\n    Neural network as defined in the paper \'Human-level control through deep reinforcement learning\'\n    but with two separate heads and ""noisy"" linear layer.\n    """"""\n    def __init__(self, input_width, input_height, input_channels, output_dim=512, initial_std_dev=0.4,\n                 factorized_noise=True):\n        super().__init__()\n\n        self._output_dim = output_dim\n\n        self.conv1 = nn.Conv2d(\n            in_channels=input_channels,\n            out_channels=32,\n            kernel_size=(8, 8),\n            stride=4\n        )\n\n        self.conv2 = nn.Conv2d(\n            in_channels=32,\n            out_channels=64,\n            kernel_size=(4, 4),\n            stride=2\n        )\n\n        self.conv3 = nn.Conv2d(\n            in_channels=64,\n            out_channels=64,\n            kernel_size=(3, 3),\n            stride=1\n        )\n\n        self.final_width = net_util.convolutional_layer_series(input_width, [\n            (8, 0, 4),\n            (4, 0, 2),\n            (3, 0, 1)\n        ])\n\n        self.final_height = net_util.convolutional_layer_series(input_height, [\n            (8, 0, 4),\n            (4, 0, 2),\n            (3, 0, 1)\n        ])\n\n        self.linear_layer_one = NoisyLinear(\n            # 64 is the number of channels of the last conv layer\n            self.final_width * self.final_height * 64,\n            self.output_dim,\n            initial_std_dev=initial_std_dev,\n            factorized_noise=factorized_noise\n        )\n\n        self.linear_layer_two = NoisyLinear(\n            # 64 is the number of channels of the last conv layer\n            self.final_width * self.final_height * 64,\n            self.output_dim,\n            initial_std_dev=initial_std_dev,\n            factorized_noise=factorized_noise\n        )\n\n    @property\n    def output_dim(self) -> int:\n        """""" Final dimension of model output """"""\n        return self._output_dim\n\n    def reset_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                init.orthogonal_(m.weight, gain=np.sqrt(2))\n                init.constant_(m.bias, 0.0)\n            elif isinstance(m, nn.Linear):\n                # init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                init.orthogonal_(m.weight, gain=np.sqrt(2))\n                init.constant_(m.bias, 0.0)\n            elif isinstance(m, NoisyLinear):\n                m.reset_weights()\n\n    def forward(self, image):\n        result = image\n        result = F.relu(self.conv1(result))\n        result = F.relu(self.conv2(result))\n        result = F.relu(self.conv3(result))\n        flattened = result.view(result.size(0), -1)\n\n        output_one = F.relu(self.linear_layer_one(flattened))\n        output_two = F.relu(self.linear_layer_two(flattened))\n\n        return output_one, output_two\n\n\ndef create(input_width, input_height, input_channels=1, output_dim=512, initial_std_dev=0.4, factorized_noise=True):\n    """""" Vel factory function """"""\n    def instantiate(**_):\n        return DoubleNoisyNatureCnn(\n            input_width=input_width, input_height=input_height, input_channels=input_channels,\n            output_dim=output_dim, initial_std_dev=initial_std_dev, factorized_noise=factorized_noise\n        )\n\n    return ModelFactory.generic(instantiate)\n\n\nDoubleNoisyNatureCnnFactory = create\n'"
vel/rl/models/backbone/lstm.py,0,"b'from vel.api import RnnLinearBackboneModel, ModelFactory\n\n\nclass LstmBackbone(RnnLinearBackboneModel):\n    """"""\n    Simple \'LSTM\' model backbone\n    """"""\n\n    def __init__(self, input_size, hidden_units):\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_units = hidden_units\n\n    def forward(self, input_data, masks, state):\n        raise NotImplementedError\n\n    def initial_state(self):\n        """""" Initial state of the network """"""\n        raise NotImplementedError\n'"
vel/rl/models/backbone/mlp.py,2,"b'""""""\nCode based loosely on the implementation:\nhttps://github.com/openai/baselines/blob/master/baselines/common/models.py\n\nUnder MIT license.\n""""""\nimport typing\nimport numpy as np\n\nimport torch.nn as nn\nimport torch.nn.init as init\n\nimport vel.util.network as net_util\n\nfrom vel.api import LinearBackboneModel, ModelFactory\n\n\nclass MLP(LinearBackboneModel):\n    """""" Simple Multi-Layer-Perceptron network """"""\n    def __init__(self, input_length: int, hidden_layers: typing.List[int], activation: str=\'tanh\',\n                 normalization: typing.Optional[str]=None):\n        super().__init__()\n\n        self.input_length = input_length\n        self.hidden_layers = hidden_layers\n        self.activation = activation\n        self.normalization = normalization\n\n        layer_objects = []\n        layer_sizes = zip([input_length] + hidden_layers, hidden_layers)\n\n        for input_size, output_size in layer_sizes:\n            layer_objects.append(nn.Linear(input_size, output_size))\n\n            if self.normalization:\n                layer_objects.append(net_util.normalization(normalization)(output_size))\n\n            layer_objects.append(net_util.activation(activation)())\n\n        self.model = nn.Sequential(*layer_objects)\n        self.hidden_units = hidden_layers[-1] if hidden_layers else input_length\n\n    @property\n    def output_dim(self) -> int:\n        """""" Final dimension of model output """"""\n        return self.hidden_units\n\n    def reset_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                # init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                init.orthogonal_(m.weight, gain=np.sqrt(2))\n                init.constant_(m.bias, 0.0)\n\n    def forward(self, input_data):\n        input_data = input_data.float()\n        return self.model(input_data)\n\n\ndef create(input_length, hidden_layers, activation=\'tanh\', normalization=None):\n    """""" Vel factory function """"""\n    def instantiate(**_):\n        return MLP(\n            input_length=input_length,\n            hidden_layers=hidden_layers,\n            activation=activation,\n            normalization=normalization\n        )\n\n    return ModelFactory.generic(instantiate)\n\n\nMLPFactory = create\n'"
vel/rl/models/backbone/nature_cnn.py,3,"b'""""""\nCode based loosely on implementation:\nhttps://github.com/openai/baselines/blob/master/baselines/ppo2/policies.py\n\nUnder MIT license.\n""""""\nimport numpy as np\n\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\nimport vel.util.network as net_util\n\nfrom vel.api import LinearBackboneModel, ModelFactory\n\n\nclass NatureCnn(LinearBackboneModel):\n    """""" Neural network as defined in the paper \'Human-level control through deep reinforcement learning\' """"""\n    def __init__(self, input_width, input_height, input_channels, output_dim=512):\n        super().__init__()\n\n        self._output_dim = output_dim\n\n        self.conv1 = nn.Conv2d(\n            in_channels=input_channels,\n            out_channels=32,\n            kernel_size=(8, 8),\n            stride=4\n        )\n\n        self.conv2 = nn.Conv2d(\n            in_channels=32,\n            out_channels=64,\n            kernel_size=(4, 4),\n            stride=2\n        )\n\n        self.conv3 = nn.Conv2d(\n            in_channels=64,\n            out_channels=64,\n            kernel_size=(3, 3),\n            stride=1\n        )\n\n        layer_series = [\n            (8, 0, 4),\n            (4, 0, 2),\n            (3, 0, 1)\n        ]\n\n        self.final_width = net_util.convolutional_layer_series(input_width, layer_series)\n        self.final_height = net_util.convolutional_layer_series(input_height, layer_series)\n\n        self.linear_layer = nn.Linear(\n            self.final_width * self.final_height * 64,  # 64 is the number of channels of the last conv layer\n            self.output_dim\n        )\n\n    @property\n    def output_dim(self) -> int:\n        """""" Final dimension of model output """"""\n        return self._output_dim\n\n    def reset_weights(self):\n        """""" Call proper initializers for the weights """"""\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                init.orthogonal_(m.weight, gain=np.sqrt(2))\n                init.constant_(m.bias, 0.0)\n            elif isinstance(m, nn.Linear):\n                # init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                init.orthogonal_(m.weight, gain=np.sqrt(2))\n                init.constant_(m.bias, 0.0)\n\n    def forward(self, image):\n        result = image\n        result = F.relu(self.conv1(result))\n        result = F.relu(self.conv2(result))\n        result = F.relu(self.conv3(result))\n        flattened = result.view(result.size(0), -1)\n        return F.relu(self.linear_layer(flattened))\n\n\ndef create(input_width, input_height, input_channels=1, output_dim=512):\n    """""" Vel factory function """"""\n    def instantiate(**_):\n        return NatureCnn(\n            input_width=input_width, input_height=input_height, input_channels=input_channels,\n            output_dim=output_dim\n        )\n\n    return ModelFactory.generic(instantiate)\n\n\n# Scripting interface\nNatureCnnFactory = create\n'"
vel/rl/models/backbone/nature_cnn_rnn.py,0,"b'from vel.api import RnnLinearBackboneModel, ModelFactory\nfrom vel.rl.models.backbone.nature_cnn import NatureCnn\nfrom vel.modules.rnn_cell import RnnCell\n\n\nclass NatureCnnRnnBackbone(RnnLinearBackboneModel):\n    """"""\n    Long-Short-Term Memory rnn cell together with DeepMind-style \'Nature\' cnn preprocessing\n    """"""\n\n    def __init__(self, input_width: int, input_height: int, input_channels: int, rnn_type=\'lstm\',\n                 cnn_output_dim: int=512, hidden_units: int=128):\n        super().__init__()\n\n        self.hidden_units = hidden_units\n\n        self.nature_cnn = NatureCnn(input_width, input_height, input_channels, cnn_output_dim)\n        self.rnn_cell = RnnCell(input_size=self.nature_cnn.output_dim, hidden_size=self.hidden_units, rnn_type=rnn_type)\n\n    def reset_weights(self):\n        """""" Call proper initializers for the weights """"""\n        self.nature_cnn.reset_weights()\n        self.rnn_cell.reset_weights()\n\n    @property\n    def output_dim(self) -> int:\n        return self.rnn_cell.output_dim\n\n    @property\n    def state_dim(self) -> int:\n        """""" Initial state of the network """"""\n        return self.rnn_cell.state_dim\n\n    def forward(self, input_image, state):\n        cnn_output = self.nature_cnn(input_image)\n        hidden_state, new_state = self.rnn_cell(cnn_output, state)\n\n        return hidden_state, new_state\n\n\ndef create(input_width, input_height, input_channels=1, rnn_type=\'lstm\', cnn_output_dim=512, hidden_units=128):\n    """""" Vel factory function """"""\n    def instantiate(**_):\n        return NatureCnnRnnBackbone(\n            input_width=input_width, input_height=input_height, input_channels=input_channels,\n            rnn_type=rnn_type, cnn_output_dim=cnn_output_dim, hidden_units=hidden_units\n        )\n\n    return ModelFactory.generic(instantiate)\n\n\n# Add this to make nicer scripting interface\nNatureCnnFactory = create\n'"
vel/rl/models/backbone/nature_cnn_small.py,3,"b'""""""\nCode based loosely on implementation:\nhttps://github.com/openai/baselines/blob/master/baselines/ppo2/policies.py\n\nUnder MIT license.\n""""""\nimport numpy as np\n\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\nimport vel.util.network as net_util\n\nfrom vel.api import LinearBackboneModel, ModelFactory\n\n\nclass NatureCnnSmall(LinearBackboneModel):\n    """"""\n    Neural network as defined in the paper \'Human-level control through deep reinforcement learning\'\n    Smaller version.\n    """"""\n    def __init__(self, input_width, input_height, input_channels, output_dim=128):\n        super().__init__()\n\n        self._output_dim = output_dim\n\n        self.conv1 = nn.Conv2d(\n            in_channels=input_channels,\n            out_channels=8,\n            kernel_size=(8, 8),\n            stride=4\n        )\n\n        self.conv2 = nn.Conv2d(\n            in_channels=8,\n            out_channels=16,\n            kernel_size=(4, 4),\n            stride=2\n        )\n\n        self.final_width = net_util.convolutional_layer_series(input_width, [\n            (8, 0, 4),\n            (4, 0, 2),\n        ])\n\n        self.final_height = net_util.convolutional_layer_series(input_height, [\n            (8, 0, 4),\n            (4, 0, 2),\n        ])\n\n        self.linear_layer = nn.Linear(\n            self.final_width * self.final_height * 16,\n            self.output_dim\n        )\n\n    @property\n    def output_dim(self) -> int:\n        """""" Final dimension of model output """"""\n        return self._output_dim\n\n    def reset_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                init.orthogonal_(m.weight, gain=np.sqrt(2))\n                init.constant_(m.bias, 0.0)\n            elif isinstance(m, nn.Linear):\n                # init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                init.orthogonal_(m.weight, gain=np.sqrt(2))\n                init.constant_(m.bias, 0.0)\n\n    def forward(self, image):\n        result = image\n        result = F.relu(self.conv1(result))\n        result = F.relu(self.conv2(result))\n        flattened = result.view(result.size(0), -1)\n        return F.relu(self.linear_layer(flattened))\n\n\ndef create(input_width, input_height, input_channels=1):\n    """""" Vel factory function """"""\n    def instantiate(**_):\n        return NatureCnnSmall(input_width=input_width, input_height=input_height, input_channels=input_channels)\n\n    return ModelFactory.generic(instantiate)\n\n\nNatureCnnSmallFactory = create\n'"
vel/rl/models/backbone/noisy_nature_cnn.py,3,"b'""""""\nCode based loosely on implementation:\nhttps://github.com/openai/baselines/blob/master/baselines/ppo2/policies.py\n\nUnder MIT license.\n""""""\nimport numpy as np\n\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\nimport vel.util.network as net_util\n\nfrom vel.api import LinearBackboneModel, ModelFactory\nfrom vel.rl.modules.noisy_linear import NoisyLinear\n\n\nclass NoisyNatureCnn(LinearBackboneModel):\n    """"""\n    Neural network as defined in the paper \'Human-level control through deep reinforcement learning\'\n    implemented via ""Noisy Networks for Exploration""\n    """"""\n    def __init__(self, input_width, input_height, input_channels, output_dim=512, initial_std_dev=0.4,\n                 factorized_noise=True):\n        super().__init__()\n\n        self._output_dim = output_dim\n\n        self.conv1 = nn.Conv2d(\n            in_channels=input_channels,\n            out_channels=32,\n            kernel_size=(8, 8),\n            stride=4\n        )\n\n        self.conv2 = nn.Conv2d(\n            in_channels=32,\n            out_channels=64,\n            kernel_size=(4, 4),\n            stride=2\n        )\n\n        self.conv3 = nn.Conv2d(\n            in_channels=64,\n            out_channels=64,\n            kernel_size=(3, 3),\n            stride=1\n        )\n\n        layer_series = [\n            (8, 0, 4),\n            (4, 0, 2),\n            (3, 0, 1)\n        ]\n\n        self.final_width = net_util.convolutional_layer_series(input_width, layer_series)\n        self.final_height = net_util.convolutional_layer_series(input_height, layer_series)\n\n        self.linear_layer = NoisyLinear(\n            self.final_width * self.final_height * 64,  # 64 is the number of channels of the last conv layer\n            self.output_dim,\n            initial_std_dev=initial_std_dev,\n            factorized_noise=factorized_noise\n        )\n\n    @property\n    def output_dim(self) -> int:\n        """""" Final dimension of model output """"""\n        return self._output_dim\n\n    def reset_weights(self):\n        """""" Call proper initializers for the weights """"""\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                init.orthogonal_(m.weight, gain=np.sqrt(2))\n                init.constant_(m.bias, 0.0)\n            elif isinstance(m, nn.Linear):\n                # init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                init.orthogonal_(m.weight, gain=np.sqrt(2))\n                init.constant_(m.bias, 0.0)\n            elif isinstance(m, NoisyLinear):\n                m.reset_weights()\n\n    def forward(self, image):\n        result = image\n        result = F.relu(self.conv1(result))\n        result = F.relu(self.conv2(result))\n        result = F.relu(self.conv3(result))\n        flattened = result.view(result.size(0), -1)\n        return F.relu(self.linear_layer(flattened))\n\n\ndef create(input_width, input_height, input_channels=1, output_dim=512, initial_std_dev=0.4, factorized_noise=True):\n    """""" Vel factory function """"""\n    def instantiate(**_):\n        return NoisyNatureCnn(\n            input_width=input_width, input_height=input_height, input_channels=input_channels,\n            output_dim=output_dim, initial_std_dev=initial_std_dev, factorized_noise=factorized_noise\n        )\n\n    return ModelFactory.generic(instantiate)\n\n\n# Scripting interface\nNatureCnnFactory = create\n'"
vel/rl/modules/noise/__init__.py,0,b''
vel/rl/modules/noise/eps_greedy.py,4,"b'import typing\n\nimport torch\nimport torch.nn as nn\n\nfrom vel.api import Schedule\nfrom vel.internals.generic_factory import GenericFactory\nfrom vel.schedules.constant import ConstantSchedule\n\n\nclass EpsGreedy(nn.Module):\n    """""" Epsilon-greedy action selection """"""\n    def __init__(self, epsilon: typing.Union[Schedule, float], environment):\n        super().__init__()\n\n        if isinstance(epsilon, Schedule):\n            self.epsilon_schedule = epsilon\n        else:\n            self.epsilon_schedule = ConstantSchedule(epsilon)\n\n        self.action_space = environment.action_space\n\n    def forward(self, actions, batch_info=None):\n        if batch_info is None:\n            # Just take final value if there is no batch info\n            epsilon = self.epsilon_schedule.value(1.0)\n        else:\n            epsilon = self.epsilon_schedule.value(batch_info[\'progress\'])\n\n        random_samples = torch.randint_like(actions, self.action_space.n)\n        selector = torch.rand_like(random_samples, dtype=torch.float32)\n\n        # Actions with noise applied\n        noisy_actions = torch.where(selector > epsilon, actions, random_samples)\n\n        return noisy_actions\n\n    def reset_training_state(self, dones, batch_info):\n        """""" A hook for a model to react when during training episode is finished """"""\n        pass\n\n\ndef create(epsilon: typing.Union[Schedule, float]):\n    """""" Vel factory function """"""\n    return GenericFactory(EpsGreedy, arguments={\'epsilon\': epsilon})\n'"
vel/rl/modules/noise/ou_noise.py,5,"b'import torch\nimport numpy as np\nimport torch.nn as nn\n\nfrom vel.math.processes import OrnsteinUhlenbeckNoiseProcess\nfrom vel.internals.generic_factory import GenericFactory\n\n\nclass OuNoise(nn.Module):\n    """""" Ornstein\xe2\x80\x93Uhlenbeck noise process for action noise """"""\n\n    def __init__(self, std_dev, environment):\n        super().__init__()\n\n        self.std_dev = std_dev\n        self.action_space = environment.action_space\n        self.processes = []\n\n        self.register_buffer(\'low_tensor\', torch.from_numpy(self.action_space.low).unsqueeze(0))\n        self.register_buffer(\'high_tensor\', torch.from_numpy(self.action_space.high).unsqueeze(0))\n\n    def reset_training_state(self, dones, batch_info):\n        """""" A hook for a model to react when during training episode is finished """"""\n        for idx, done in enumerate(dones):\n            if done > 0.5:\n                self.processes[idx].reset()\n\n    def forward(self, actions, batch_info):\n        """""" Return model step after applying noise """"""\n        while len(self.processes) < actions.shape[0]:\n            len_action_space = self.action_space.shape[-1]\n\n            self.processes.append(\n                OrnsteinUhlenbeckNoiseProcess(\n                    np.zeros(len_action_space), float(self.std_dev) * np.ones(len_action_space)\n                )\n            )\n\n        noise = torch.from_numpy(np.stack([x() for x in self.processes])).float().to(actions.device)\n\n        return torch.min(torch.max(actions + noise, self.low_tensor), self.high_tensor)\n\n\ndef create(std_dev: float):\n    """""" Vel factory function """"""\n    return GenericFactory(OuNoise, arguments={\'std_dev\': std_dev})\n'"
vel/rl/modules/test/__init__.py,0,b''
vel/rl/modules/test/test_action_head.py,20,"b'import numpy as np\nimport pytest\n\nimport numpy.testing as nt\n\nimport torch\nimport torch.nn.functional as F\nimport torch.distributions as d\n\nfrom vel.rl.modules.action_head import DiagGaussianActionHead, CategoricalActionHead\n\n\ndef test_sample_diag_gaussian():\n    """""" Test sampling from a multivariate gaussian distribution with a diagonal covariance matrix """"""\n    head = DiagGaussianActionHead(1, 5)\n\n    array = np.zeros((10000, 5, 2))\n\n    sample = head.sample(torch.from_numpy(array))\n\n    result_array = sample.detach().cpu().numpy()\n\n    nt.assert_array_less(np.abs(result_array.mean(axis=0)), 0.1)\n    nt.assert_array_less(result_array.std(axis=0), 1.1)\n    nt.assert_array_less(0.9, result_array.std(axis=0))\n\n    array2 = np.zeros((10000, 5, 2))\n    array2[:, 0, 0] = 5.0\n    array2[:, 0, 1] = np.log(10)\n\n    sample2 = head.sample(torch.from_numpy(array2))\n\n    result_array2 = sample2.detach().cpu().numpy()\n    nt.assert_array_less(result_array2.mean(axis=0), np.array([5.3, 0.1, 0.1, 0.1, 0.1]))\n    nt.assert_array_less(np.array([4.7, -0.1, -0.1, -0.1, -0.1]), result_array2.mean(axis=0))\n\n\ndef test_neglogp_diag_gaussian():\n    """"""\n    Test negative logarithm of likelihood of a multivariate gaussian distribution with a diagonal covariance matrix\n    """"""\n    head = DiagGaussianActionHead(1, 5)\n    distrib = d.MultivariateNormal(torch.tensor([1.0, -1.0]), covariance_matrix=torch.tensor([[2.0, 0.0], [0.0, 0.5]]))\n\n    pd_params = torch.tensor([[1.0, -1.0], [np.log(np.sqrt(2.0)), np.log(np.sqrt(0.5))]]).t()\n    sample = head.sample(pd_params[None])\n\n    log_prob1 = distrib.log_prob(sample)\n    log_prob2 = head.logprob(sample, pd_params[None])\n\n    nt.assert_allclose(log_prob1.detach().cpu().numpy(), log_prob2.detach().cpu().numpy(), rtol=1e-5)\n\n\ndef test_entropy_diag_gaussian():\n    """"""\n    Test entropy of a multivariate gaussian distribution with a diagonal covariance matrix\n    """"""\n    head = DiagGaussianActionHead(1, 5)\n    distrib = d.MultivariateNormal(torch.tensor([1.0, -1.0]), covariance_matrix=torch.tensor([[2.0, 0.0], [0.0, 0.5]]))\n\n    pd_params = torch.tensor([[1.0, -1.0], [np.log(np.sqrt(2.0)), np.log(np.sqrt(0.5))]]).t()\n\n    entropy1 = distrib.entropy()\n    entropy2 = head.entropy(pd_params[None])\n\n    nt.assert_allclose(entropy1.detach().cpu().numpy(), entropy2.detach().cpu().numpy())\n\n\ndef test_kl_divergence_diag_gaussian():\n    """"""\n    Test kl divergence between multivariate gaussian distributions with a diagonal covariance matrix\n    """"""\n    head = DiagGaussianActionHead(1, 5)\n\n    distrib1 = d.MultivariateNormal(torch.tensor([1.0, -1.0]), covariance_matrix=torch.tensor([[2.0, 0.0], [0.0, 0.5]]))\n    distrib2 = d.MultivariateNormal(torch.tensor([0.3, 0.7]), covariance_matrix=torch.tensor([[1.8, 0.0], [0.0, 5.5]]))\n\n    pd_params1 = torch.tensor([[1.0, -1.0], [np.log(np.sqrt(2.0)), np.log(np.sqrt(0.5))]]).t()\n    pd_params2 = torch.tensor([[0.3, 0.7], [np.log(np.sqrt(1.8)), np.log(np.sqrt(5.5))]]).t()\n\n    kl_div_1 = d.kl_divergence(distrib1, distrib2)\n    kl_div_2 = head.kl_divergence(pd_params1[None], pd_params2[None])\n\n    assert kl_div_1.item() == pytest.approx(kl_div_2.item(), 0.001)\n\n\ndef test_sample_categorical():\n    """"""\n    Test sampling from a categorical distribution\n    """"""\n    head = CategoricalActionHead(1, 5)\n\n    array = np.zeros((10000, 5))\n\n    sample = head.sample(torch.from_numpy(array))\n\n    result_array = sample.detach().cpu().numpy()\n\n    nt.assert_array_less(np.abs(result_array.mean(axis=0)), 2.1)\n    nt.assert_array_less(1.9, np.abs(result_array.mean(axis=0)))\n\n    array2 = np.zeros((10000, 5))\n    array2[:, 0:4] = -10.0\n    array2[:, 4] = 10.0\n\n    sample2 = head.sample(F.log_softmax(torch.from_numpy(array2), dim=1))\n    result_array2 = sample2.detach().cpu().numpy()\n\n    nt.assert_array_less(np.abs(result_array2.mean(axis=0)), 4.1)\n    nt.assert_array_less(3.9, np.abs(result_array2.mean(axis=0)))\n\n\ndef test_neglogp_categorical():\n    """"""\n    Test negative logarithm of likelihood of a categorical distribution\n    """"""\n    head = CategoricalActionHead(1, 5)\n\n    logits = F.log_softmax(torch.tensor([0.0, 1.0, 2.0, 3.0, 4.0]), dim=0)\n\n    distrib = d.Categorical(logits=logits)\n\n    actions = torch.tensor([0, 1, 2, 3, 4])\n\n    log_p_1 = distrib.log_prob(actions)\n    log_p_2 = head.logprob(actions, torch.stack([logits, logits, logits, logits, logits], dim=0))\n\n    nt.assert_allclose(log_p_1.detach().cpu().numpy(), log_p_2.detach().cpu().numpy(), rtol=1e-5)\n\n\ndef test_entropy_categorical():\n    """"""\n    Test entropy of a categorical distribution\n    """"""\n    head = CategoricalActionHead(1, 5)\n\n    logits = F.log_softmax(torch.tensor([0.0, 1.0, 2.0, 3.0, 4.0]), dim=0)\n\n    distrib = d.Categorical(logits=logits)\n\n    entropy1 = distrib.entropy()\n    entropy2 = head.entropy(logits[None])\n\n    nt.assert_allclose(entropy1.item(), entropy2.item())\n\n\ndef test_kl_divergence_categorical():\n    """"""\n    Test KL divergence between categorical distributions\n    """"""\n    head = CategoricalActionHead(1, 5)\n\n    logits1 = F.log_softmax(torch.tensor([0.0, 1.0, 2.0, 3.0, 4.0]), dim=0)\n    logits2 = F.log_softmax(torch.tensor([-1.0, 0.2, 5.0, 2.0, 8.0]), dim=0)\n\n    distrib1 = d.Categorical(logits=logits1)\n    distrib2 = d.Categorical(logits=logits2)\n\n    kl_div_1 = d.kl_divergence(distrib1, distrib2)\n    kl_div_2 = head.kl_divergence(logits1[None], logits2[None])\n\n    nt.assert_allclose(kl_div_1.item(), kl_div_2.item(), rtol=1e-5)\n'"
vel/openai/baselines/common/vec_env/__init__.py,0,"b'from abc import ABC, abstractmethod\nfrom vel.openai.baselines.common.tile_images import tile_images\n\n\nclass AlreadySteppingError(Exception):\n    """"""\n    Raised when an asynchronous step is running while\n    step_async() is called again.\n    """"""\n\n    def __init__(self):\n        msg = \'already running an async step\'\n        Exception.__init__(self, msg)\n\n\nclass NotSteppingError(Exception):\n    """"""\n    Raised when an asynchronous step is not running but\n    step_wait() is called.\n    """"""\n\n    def __init__(self):\n        msg = \'not running an async step\'\n        Exception.__init__(self, msg)\n\n\nclass VecEnv(ABC):\n    """"""\n    An abstract asynchronous, vectorized environment.\n    Used to batch data from multiple copies of an environment, so that\n    each observation becomes an batch of observations, and expected action is a batch of actions to\n    be applied per-environment.\n    """"""\n    closed = False\n    viewer = None\n\n    metadata = {\n        \'render.modes\': [\'human\', \'rgb_array\']\n    }\n\n    def __init__(self, num_envs, observation_space, action_space):\n        self.num_envs = num_envs\n        self.observation_space = observation_space\n        self.action_space = action_space\n\n    @abstractmethod\n    def reset(self):\n        """"""\n        Reset all the environments and return an array of\n        observations, or a dict of observation arrays.\n\n        If step_async is still doing work, that work will\n        be cancelled and step_wait() should not be called\n        until step_async() is invoked again.\n        """"""\n        pass\n\n    @abstractmethod\n    def step_async(self, actions):\n        """"""\n        Tell all the environments to start taking a step\n        with the given actions.\n        Call step_wait() to get the results of the step.\n\n        You should not call this if a step_async run is\n        already pending.\n        """"""\n        pass\n\n    @abstractmethod\n    def step_wait(self):\n        """"""\n        Wait for the step taken with step_async().\n\n        Returns (obs, rews, dones, infos):\n         - obs: an array of observations, or a dict of\n                arrays of observations.\n         - rews: an array of rewards\n         - dones: an array of ""episode done"" booleans\n         - infos: a sequence of info objects\n        """"""\n        pass\n\n    def close_extras(self):\n        """"""\n        Clean up the  extra resources, beyond what\'s in this base class.\n        Only runs when not self.closed.\n        """"""\n        pass\n\n    def close(self):\n        if self.closed:\n            return\n        if self.viewer is not None:\n            self.viewer.close()\n        self.close_extras()\n        self.closed = True\n\n    def step(self, actions):\n        """"""\n        Step the environments synchronously.\n\n        This is available for backwards compatibility.\n        """"""\n        self.step_async(actions)\n        return self.step_wait()\n\n    def render(self, mode=\'human\'):\n        imgs = self.get_images()\n        bigimg = tile_images(imgs)\n        if mode == \'human\':\n            self.get_viewer().imshow(bigimg)\n            return self.get_viewer().isopen\n        elif mode == \'rgb_array\':\n            return bigimg\n        else:\n            raise NotImplementedError\n\n    def get_images(self):\n        """"""\n        Return RGB images from each environment\n        """"""\n        raise NotImplementedError\n\n    @property\n    def unwrapped(self):\n        if isinstance(self, VecEnvWrapper):\n            return self.venv.unwrapped\n        else:\n            return self\n\n    def get_viewer(self):\n        if self.viewer is None:\n            from gym.envs.classic_control import rendering\n            self.viewer = rendering.SimpleImageViewer()\n        return self.viewer\n\n\nclass VecEnvWrapper(VecEnv):\n    """"""\n    An environment wrapper that applies to an entire batch\n    of environments at once.\n    """"""\n\n    def __init__(self, venv, observation_space=None, action_space=None):\n        self.venv = venv\n        VecEnv.__init__(self,\n                        num_envs=venv.num_envs,\n                        observation_space=observation_space or venv.observation_space,\n                        action_space=action_space or venv.action_space)\n\n    def step_async(self, actions):\n        self.venv.step_async(actions)\n\n    @abstractmethod\n    def reset(self):\n        pass\n\n    @abstractmethod\n    def step_wait(self):\n        pass\n\n    def close(self):\n        return self.venv.close()\n\n    def render(self, mode=\'human\'):\n        return self.venv.render(mode=mode)\n\n    def get_images(self):\n        return self.venv.get_images()\n\n\nclass CloudpickleWrapper(object):\n    """"""\n    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n    """"""\n\n    def __init__(self, x):\n        self.x = x\n\n    def __getstate__(self):\n        import cloudpickle\n        return cloudpickle.dumps(self.x)\n\n    def __setstate__(self, ob):\n        import pickle\n        self.x = pickle.loads(ob)\n'"
vel/openai/baselines/common/vec_env/dummy_vec_env.py,0,"b'import numpy as np\nfrom gym import spaces\nfrom . import VecEnv\nfrom .util import copy_obs_dict, dict_to_obs, obs_space_info\n\nclass DummyVecEnv(VecEnv):\n    """"""\n    VecEnv that does runs multiple environments sequentially, that is,\n    the step and reset commands are send to one environment at a time.\n    Useful when debugging and when num_env == 1 (in the latter case,\n    avoids communication overhead)\n    """"""\n    def __init__(self, env_fns):\n        """"""\n        Arguments:\n        env_fns: iterable of callables      functions that build environments\n        """"""\n        self.envs = [fn() for fn in env_fns]\n        env = self.envs[0]\n        VecEnv.__init__(self, len(env_fns), env.observation_space, env.action_space)\n        obs_space = env.observation_space\n        self.keys, shapes, dtypes = obs_space_info(obs_space)\n\n        self.buf_obs = { k: np.zeros((self.num_envs,) + tuple(shapes[k]), dtype=dtypes[k]) for k in self.keys }\n        self.buf_dones = np.zeros((self.num_envs,), dtype=np.bool)\n        self.buf_rews  = np.zeros((self.num_envs,), dtype=np.float32)\n        self.buf_infos = [{} for _ in range(self.num_envs)]\n        self.actions = None\n        self.specs = [e.spec for e in self.envs]\n\n    def step_async(self, actions):\n        listify = True\n        try:\n            if len(actions) == self.num_envs:\n                listify = False\n        except TypeError:\n            pass\n\n        if not listify:\n            self.actions = actions\n        else:\n            assert self.num_envs == 1, ""actions {} is either not a list or has a wrong size - cannot match to {} environments"".format(actions, self.num_envs)\n            self.actions = [actions]\n\n    def step_wait(self):\n        for e in range(self.num_envs):\n            action = self.actions[e]\n            if isinstance(self.envs[e].action_space, spaces.Discrete):\n                action = int(action)\n\n            obs, self.buf_rews[e], self.buf_dones[e], self.buf_infos[e] = self.envs[e].step(action)\n            if self.buf_dones[e]:\n                obs = self.envs[e].reset()\n            self._save_obs(e, obs)\n        return (self._obs_from_buf(), np.copy(self.buf_rews), np.copy(self.buf_dones),\n                self.buf_infos.copy())\n\n    def reset(self):\n        for e in range(self.num_envs):\n            obs = self.envs[e].reset()\n            self._save_obs(e, obs)\n        return self._obs_from_buf()\n\n    def _save_obs(self, e, obs):\n        for k in self.keys:\n            if k is None:\n                self.buf_obs[k][e] = obs\n            else:\n                self.buf_obs[k][e] = obs[k]\n\n    def _obs_from_buf(self):\n        return dict_to_obs(copy_obs_dict(self.buf_obs))\n\n    def get_images(self):\n        return [env.render(mode=\'rgb_array\') for env in self.envs]\n\n    def render(self, mode=\'human\'):\n        if self.num_envs == 1:\n            return self.envs[0].render(mode=mode)\n        else:\n            return super().render(mode=mode)\n'"
vel/openai/baselines/common/vec_env/shmem_vec_env.py,0,"b'""""""\nAn interface for asynchronous vectorized environments.\n""""""\n\nfrom multiprocessing import Pipe, Array, Process\n\nimport numpy as np\nfrom . import VecEnv, CloudpickleWrapper\nimport ctypes\nfrom vel.openai.baselines import logger\n\nfrom .util import dict_to_obs, obs_space_info, obs_to_dict\n\n_NP_TO_CT = {np.float32: ctypes.c_float,\n             np.int32: ctypes.c_int32,\n             np.int8: ctypes.c_int8,\n             np.uint8: ctypes.c_char,\n             np.bool: ctypes.c_bool}\n\n\nclass ShmemVecEnv(VecEnv):\n    """"""\n    Optimized version of SubprocVecEnv that uses shared variables to communicate observations.\n    """"""\n\n    def __init__(self, env_fns, spaces=None):\n        """"""\n        If you don\'t specify observation_space, we\'ll have to create a dummy\n        environment to get it.\n        """"""\n        if spaces:\n            observation_space, action_space = spaces\n        else:\n            logger.log(\'Creating dummy env object to get spaces\')\n            with logger.scoped_configure(format_strs=[]):\n                dummy = env_fns[0]()\n                observation_space, action_space = dummy.observation_space, dummy.action_space\n                dummy.close()\n                del dummy\n        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n        self.obs_keys, self.obs_shapes, self.obs_dtypes = obs_space_info(observation_space)\n        self.obs_bufs = [\n            {k: Array(_NP_TO_CT[self.obs_dtypes[k].type], int(np.prod(self.obs_shapes[k]))) for k in self.obs_keys}\n            for _ in env_fns]\n        self.parent_pipes = []\n        self.procs = []\n        for env_fn, obs_buf in zip(env_fns, self.obs_bufs):\n            wrapped_fn = CloudpickleWrapper(env_fn)\n            parent_pipe, child_pipe = Pipe()\n            proc = Process(target=_subproc_worker,\n                           args=(child_pipe, parent_pipe, wrapped_fn, obs_buf, self.obs_shapes, self.obs_dtypes, self.obs_keys))\n            proc.daemon = True\n            self.procs.append(proc)\n            self.parent_pipes.append(parent_pipe)\n            proc.start()\n            child_pipe.close()\n        self.waiting_step = False\n        self.specs = [f().spec for f in env_fns]\n        self.viewer = None\n\n    def reset(self):\n        if self.waiting_step:\n            logger.warn(\'Called reset() while waiting for the step to complete\')\n            self.step_wait()\n        for pipe in self.parent_pipes:\n            pipe.send((\'reset\', None))\n        return self._decode_obses([pipe.recv() for pipe in self.parent_pipes])\n\n    def step_async(self, actions):\n        assert len(actions) == len(self.parent_pipes)\n        for pipe, act in zip(self.parent_pipes, actions):\n            pipe.send((\'step\', act))\n\n    def step_wait(self):\n        outs = [pipe.recv() for pipe in self.parent_pipes]\n        obs, rews, dones, infos = zip(*outs)\n        return self._decode_obses(obs), np.array(rews), np.array(dones), infos\n\n    def close_extras(self):\n        if self.waiting_step:\n            self.step_wait()\n        for pipe in self.parent_pipes:\n            pipe.send((\'close\', None))\n        for pipe in self.parent_pipes:\n            pipe.recv()\n            pipe.close()\n        for proc in self.procs:\n            proc.join()\n\n    def get_images(self, mode=\'human\'):\n        for pipe in self.parent_pipes:\n            pipe.send((\'render\', None))\n        return [pipe.recv() for pipe in self.parent_pipes]\n\n    def _decode_obses(self, obs):\n        result = {}\n        for k in self.obs_keys:\n\n            bufs = [b[k] for b in self.obs_bufs]\n            o = [np.frombuffer(b.get_obj(), dtype=self.obs_dtypes[k]).reshape(self.obs_shapes[k]) for b in bufs]\n            result[k] = np.array(o)\n        return dict_to_obs(result)\n\n\ndef _subproc_worker(pipe, parent_pipe, env_fn_wrapper, obs_bufs, obs_shapes, obs_dtypes, keys):\n    """"""\n    Control a single environment instance using IPC and\n    shared memory.\n    """"""\n    def _write_obs(maybe_dict_obs):\n        flatdict = obs_to_dict(maybe_dict_obs)\n        for k in keys:\n            dst = obs_bufs[k].get_obj()\n            dst_np = np.frombuffer(dst, dtype=obs_dtypes[k]).reshape(obs_shapes[k])  # pylint: disable=W0212\n            np.copyto(dst_np, flatdict[k])\n\n    env = env_fn_wrapper.x()\n    parent_pipe.close()\n    try:\n        while True:\n            cmd, data = pipe.recv()\n            if cmd == \'reset\':\n                pipe.send(_write_obs(env.reset()))\n            elif cmd == \'step\':\n                obs, reward, done, info = env.step(data)\n                if done:\n                    obs = env.reset()\n                pipe.send((_write_obs(obs), reward, done, info))\n            elif cmd == \'render\':\n                pipe.send(env.render(mode=\'rgb_array\'))\n            elif cmd == \'close\':\n                pipe.send(None)\n                break\n            else:\n                raise RuntimeError(\'Got unrecognized cmd %s\' % cmd)\n    except KeyboardInterrupt:\n        print(\'ShmemVecEnv worker: got KeyboardInterrupt\')\n    finally:\n        env.close()\n'"
vel/openai/baselines/common/vec_env/subproc_vec_env.py,0,"b'import numpy as np\nfrom multiprocessing import Process, Pipe\nfrom . import VecEnv, CloudpickleWrapper\n\n\ndef worker(remote, parent_remote, env_fn_wrapper):\n    parent_remote.close()\n    env = env_fn_wrapper.x()\n    try:\n        while True:\n            cmd, data = remote.recv()\n            if cmd == \'step\':\n                ob, reward, done, info = env.step(data)\n                if done:\n                    ob = env.reset()\n                remote.send((ob, reward, done, info))\n            elif cmd == \'reset\':\n                ob = env.reset()\n                remote.send(ob)\n            elif cmd == \'render\':\n                remote.send(env.render(mode=\'rgb_array\'))\n            elif cmd == \'close\':\n                remote.close()\n                break\n            elif cmd == \'get_spaces\':\n                remote.send((env.observation_space, env.action_space))\n            else:\n                raise NotImplementedError\n    except KeyboardInterrupt:\n        print(\'SubprocVecEnv worker: got KeyboardInterrupt\')\n    finally:\n        env.close()\n\n\nclass SubprocVecEnv(VecEnv):\n    """"""\n    VecEnv that runs multiple environments in parallel in subproceses and communicates with them via pipes.\n    Recommended to use when num_envs > 1 and step() can be a bottleneck.\n    """"""\n    def __init__(self, env_fns, spaces=None):\n        """"""\n        Arguments:\n\n        env_fns: iterable of callables -  functions that create environments to run in subprocesses. Need to be cloud-pickleable\n        """"""\n        self.waiting = False\n        self.closed = False\n        nenvs = len(env_fns)\n        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n        self.ps = [Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n                   for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n        for p in self.ps:\n            p.daemon = True  # if the main process crashes, we should not cause things to hang\n            p.start()\n        for remote in self.work_remotes:\n            remote.close()\n\n        self.remotes[0].send((\'get_spaces\', None))\n        observation_space, action_space = self.remotes[0].recv()\n        self.viewer = None\n        self.specs = [f().spec for f in env_fns]\n        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n\n    def step_async(self, actions):\n        self._assert_not_closed()\n        for remote, action in zip(self.remotes, actions):\n            remote.send((\'step\', action))\n        self.waiting = True\n\n    def step_wait(self):\n        self._assert_not_closed()\n        results = [remote.recv() for remote in self.remotes]\n        self.waiting = False\n        obs, rews, dones, infos = zip(*results)\n        return _flatten_obs(obs), np.stack(rews), np.stack(dones), infos\n\n    def reset(self):\n        self._assert_not_closed()\n        for remote in self.remotes:\n            remote.send((\'reset\', None))\n        return _flatten_obs([remote.recv() for remote in self.remotes])\n\n    def close_extras(self):\n        self.closed = True\n        if self.waiting:\n            for remote in self.remotes:\n                remote.recv()\n        for remote in self.remotes:\n            remote.send((\'close\', None))\n        for p in self.ps:\n            p.join()\n\n    def get_images(self):\n        self._assert_not_closed()\n        for pipe in self.remotes:\n            pipe.send((\'render\', None))\n        imgs = [pipe.recv() for pipe in self.remotes]\n        return imgs\n\n    def _assert_not_closed(self):\n        assert not self.closed, ""Trying to operate on a SubprocVecEnv after calling close()""\n\n\ndef _flatten_obs(obs):\n    assert isinstance(obs, list) or isinstance(obs, tuple)\n    assert len(obs) > 0\n\n    if isinstance(obs[0], dict):\n        import collections\n        assert isinstance(obs, collections.OrderedDict)\n        keys = obs[0].keys()\n        return {k: np.stack([o[k] for o in obs]) for k in keys}\n    else:\n        return np.stack(obs)\n\n'"
vel/openai/baselines/common/vec_env/util.py,0,"b'""""""\nHelpers for dealing with vectorized environments.\n""""""\n\nfrom collections import OrderedDict\n\nimport gym\nimport numpy as np\n\n\ndef copy_obs_dict(obs):\n    """"""\n    Deep-copy an observation dict.\n    """"""\n    return {k: np.copy(v) for k, v in obs.items()}\n\n\ndef dict_to_obs(obs_dict):\n    """"""\n    Convert an observation dict into a raw array if the\n    original observation space was not a Dict space.\n    """"""\n    if set(obs_dict.keys()) == {None}:\n        return obs_dict[None]\n    return obs_dict\n\n\ndef obs_space_info(obs_space):\n    """"""\n    Get dict-structured information about a gym.Space.\n\n    Returns:\n      A tuple (keys, shapes, dtypes):\n        keys: a list of dict keys.\n        shapes: a dict mapping keys to shapes.\n        dtypes: a dict mapping keys to dtypes.\n    """"""\n    if isinstance(obs_space, gym.spaces.Dict):\n        assert isinstance(obs_space.spaces, OrderedDict)\n        subspaces = obs_space.spaces\n    else:\n        subspaces = {None: obs_space}\n    keys = []\n    shapes = {}\n    dtypes = {}\n    for key, box in subspaces.items():\n        keys.append(key)\n        shapes[key] = box.shape\n        dtypes[key] = box.dtype\n    return keys, shapes, dtypes\n\n\ndef obs_to_dict(obs):\n    """"""\n    Convert an observation into a dict.\n    """"""\n    if isinstance(obs, dict):\n        return obs\n    return {None: obs}\n'"
vel/openai/baselines/common/vec_env/vec_frame_stack.py,0,"b'from . import VecEnvWrapper\nimport numpy as np\nfrom gym import spaces\n\n\nclass VecFrameStack(VecEnvWrapper):\n    def __init__(self, venv, nstack):\n        self.venv = venv\n        self.nstack = nstack\n        wos = venv.observation_space  # wrapped ob space\n        low = np.repeat(wos.low, self.nstack, axis=-1)\n        high = np.repeat(wos.high, self.nstack, axis=-1)\n        self.stackedobs = np.zeros((venv.num_envs,) + low.shape, low.dtype)\n        observation_space = spaces.Box(low=low, high=high, dtype=venv.observation_space.dtype)\n        VecEnvWrapper.__init__(self, venv, observation_space=observation_space)\n\n    def step_wait(self):\n        obs, rews, news, infos = self.venv.step_wait()\n        self.stackedobs = np.roll(self.stackedobs, shift=-1, axis=-1)\n        for (i, new) in enumerate(news):\n            if new:\n                self.stackedobs[i] = 0\n        self.stackedobs[..., -obs.shape[-1]:] = obs\n        return self.stackedobs, rews, news, infos\n\n    def reset(self):\n        obs = self.venv.reset()\n        self.stackedobs[...] = 0\n        self.stackedobs[..., -obs.shape[-1]:] = obs\n        return self.stackedobs\n'"
vel/openai/baselines/common/vec_env/vec_normalize.py,0,"b'from . import VecEnvWrapper\nfrom vel.openai.baselines.common.running_mean_std import RunningMeanStd\nimport numpy as np\n\n\nclass VecNormalize(VecEnvWrapper):\n    """"""\n    A vectorized wrapper that normalizes the observations\n    and returns from an environment.\n    """"""\n\n    def __init__(self, venv, ob=True, ret=True, clipob=10., cliprew=10., gamma=0.99, epsilon=1e-8):\n        VecEnvWrapper.__init__(self, venv)\n        self.ob_rms = RunningMeanStd(shape=self.observation_space.shape) if ob else None\n        self.ret_rms = RunningMeanStd(shape=()) if ret else None\n        self.clipob = clipob\n        self.cliprew = cliprew\n        self.ret = np.zeros(self.num_envs)\n        self.gamma = gamma\n        self.epsilon = epsilon\n\n    def step_wait(self):\n        obs, rews, news, infos = self.venv.step_wait()\n        self.ret = self.ret * self.gamma + rews\n        obs = self._obfilt(obs)\n        if self.ret_rms:\n            self.ret_rms.update(self.ret)\n            rews = np.clip(rews / np.sqrt(self.ret_rms.var + self.epsilon), -self.cliprew, self.cliprew)\n        self.ret[news] = 0.\n        return obs, rews, news, infos\n\n    def _obfilt(self, obs):\n        if self.ob_rms:\n            self.ob_rms.update(obs)\n            obs = np.clip((obs - self.ob_rms.mean) / np.sqrt(self.ob_rms.var + self.epsilon), -self.clipob, self.clipob)\n            return obs\n        else:\n            return obs\n\n    def reset(self):\n        self.ret = np.zeros(self.num_envs)\n        obs = self.venv.reset()\n        return self._obfilt(obs)\n'"
