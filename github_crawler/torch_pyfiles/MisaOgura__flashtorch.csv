file_path,api_count,code
setup.py,1,"b'#!/usr/bin/env python\n"""""" FlashTorch is a feature visualization toolkit.\nIt is built with PyTorch, for neural networks in PyTorch.\n\nNeural networks are often described as ""black box"". The lack of understanding\non how neural networks make predictions enables unpredictable/biased models,\ncausing real harm to society and a loss of trust in AI-assisted systems.\n\nFeature visualization is an area of research, which aims to understand how\nneural networks perceive images. However, implementing such techniques is often\ncomplicated.\n\nFlashTorch was created to solve this problem!\n\nYou can apply feature visualization techniques such as saliency maps and\nactivation maximization on your model, with as little as a few lines of code.\n\nIt is compatible with pre-trained models that come with torchvision, and\nseamlessly integrates with other custom models built in PyTorch.\n\nAll FlashTorch wheels on PyPI are distrubuted with the MIT License.\n""""""\n\nfrom setuptools import setup, find_packages\n\nDOCLINES = (__doc__ or \'\').split(""\\n"")\nlong_description = ""\\n"".join(DOCLINES[2:])\n\nversion = \'0.1.3\'\n\nsetup(\n    name=\'flashtorch\',\n    version=version,\n    author=\'Misa Ogura\',\n    author_email=\'misa.ogura01@gmail.com\',\n    description=\'Visualization toolkit for neural networks in PyTorch\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    url=\'https://github.com/MisaOgura/flashtorch\',\n    packages=find_packages(exclude=[\'tests*\']),\n    include_package_data=True,\n    package_data={\'flashtorch.utils.resources\': [\'imagenet_class_index.json\']},\n    install_requires=[\n        \'matplotlib\',\n        \'numpy\',\n        \'Pillow\',\n        \'torch\',\n        \'torchvision\',\n        \'importlib_resources\'\n    ],\n    classifiers=[\n        \'Programming Language :: Python :: 3 :: Only\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Operating System :: OS Independent\',\n    ],\n)\n'"
flashtorch/__init__.py,0,"b""__version__ = '0.1.3'\n"""
tests/test_backprop.py,26,"b'import inspect\nimport pytest\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchvision.models as models\n\nfrom flashtorch.saliency import Backprop\n\n\n#####################\n# Utility functions #\n#####################\n\n\ndef find_first_conv_layer(model, layer_type, in_channels):\n    for _, module in model.named_modules():\n        if isinstance(module, layer_type) and \\\n                module.in_channels == in_channels:\n            return module\n\n\ndef find_relu_layers(model, layer_type):\n    modules = []\n\n    for _, module in model.named_modules():\n        if isinstance(module, layer_type):\n            modules.append(module)\n\n    return modules\n\n\n# Mock the output from the neural network\ndef make_mock_output(mocker, model, top_class):\n    num_classes = 10\n\n    mock_tensor = torch.zeros((1, num_classes))\n    mock_tensor[0][top_class] = 1\n    mock_output = mocker.Mock(spec=mock_tensor, shape=(1, num_classes))\n\n    mocker.patch.object(model, \'forward\', return_value=mock_output)\n\n    # Mock the return value of output.topk()\n\n    mock_topk = (None, top_class)\n    mocker.patch.object(mock_output, \'topk\', return_value=mock_topk)\n\n    return mock_output\n\n\n# Make expected target of the gradient calculation\ndef make_expected_gradient_target(top_class):\n    num_classes = 10\n\n    target = torch.zeros((1, num_classes))\n    target[0][top_class] = 1\n\n    return target\n\n\nclass CnnGrayscale(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=3, padding=1)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(64, 10, kernel_size=3, stride=3, padding=1)\n        self.fc1 = nn.Linear(10 * 25 * 25, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n\n        return F.softmax(self.fc1(x.view(-1, 10 * 25 * 25)), dim=1)\n\n\n#################\n# Test fixtures #\n#################\n\n\n@pytest.fixture\ndef model():\n    return models.alexnet()\n\n\n@pytest.fixture\ndef model_grayscale():\n    return CnnGrayscale()\n\n\n##############\n# Test cases #\n##############\n\n\ndef test_set_model_to_eval_mode(mocker, model):\n    mocker.spy(model, \'eval\')\n    Backprop(model)\n\n    model.eval.assert_called_once()\n\n\ndef test_zero_out_gradients(mocker, model):\n    backprop = Backprop(model)\n    mocker.spy(model, \'zero_grad\')\n\n    target_class = 5\n    input_ = torch.zeros([1, 3, 224, 224])\n\n    make_mock_output(mocker, model, target_class)\n\n    backprop.calculate_gradients(input_, target_class)\n\n    model.zero_grad.assert_called_once()\n\n\ndef test_handle_binary_classifier(mocker, model):\n    backprop = Backprop(model)\n\n    target_class = 0\n    input_ = torch.zeros([1, 3, 224, 224])\n\n    mock_output = torch.tensor([0.8])\n    mock_output.requires_grad = True\n    mocker.patch.object(model, \'forward\', return_value=mock_output)\n\n    backprop.calculate_gradients(input_, target_class)\n\n\ndef test_calculate_gradients_of_target_class_only(mocker, model):\n    backprop = Backprop(model)\n\n    top_class = 5\n    target_class = 5\n    input_ = torch.zeros([1, 3, 224, 224])\n\n    target = make_expected_gradient_target(top_class)\n\n    mock_output = make_mock_output(mocker, model, target_class)\n\n    backprop.calculate_gradients(input_, target_class)\n\n    args, kwargs = mock_output.backward.call_args\n\n    assert torch.all(kwargs[\'gradient\'].eq(target))\n\n\ndef test_calc_gradients_of_top_class_if_target_not_provided(mocker, model):\n    backprop = Backprop(model)\n\n    top_class = 5\n    input_ = torch.zeros([1, 3, 224, 224])\n\n    target = make_expected_gradient_target(top_class)\n\n    mock_output = make_mock_output(mocker, model, top_class)\n\n    backprop.calculate_gradients(input_)\n\n    args, kwargs = mock_output.backward.call_args\n\n    assert torch.all(kwargs[\'gradient\'].eq(target))\n\n\ndef test_calc_gradients_of_top_class_if_prediction_is_wrong(mocker, model):\n    backprop = Backprop(model)\n\n    top_class = torch.tensor(5)\n    target_class = 7\n    input_ = torch.zeros([1, 3, 224, 224])\n\n    target = make_expected_gradient_target(top_class)\n\n    mock_output = make_mock_output(mocker, model, top_class)\n\n    with pytest.warns(UserWarning):\n        backprop.calculate_gradients(input_, target_class)\n\n    args, kwargs = mock_output.backward.call_args\n\n    assert torch.all(kwargs[\'gradient\'].eq(target))\n\n\ndef test_handle_greyscale_input(mocker, model_grayscale):\n    backprop = Backprop(model_grayscale)\n\n    input_ = torch.zeros([1, 1, 224, 224], requires_grad=True)\n\n    gradients = backprop.calculate_gradients(input_)\n\n    assert gradients.shape == (1, 224, 224)\n\n\ndef test_return_max_across_color_channels_if_specified(mocker, model):\n    backprop = Backprop(model)\n\n    target_class = 5\n    input_ = torch.zeros([1, 3, 224, 224])\n\n    make_mock_output(mocker, model, target_class)\n\n    gradients = backprop.calculate_gradients(input_,\n                                             target_class,\n                                             take_max=True)\n\n    assert gradients.shape == (1, 224, 224)\n\n\ndef test_checks_input_size_for_inception_model(mocker):\n    with pytest.raises(ValueError) as error:\n        model = models.inception_v3()\n        backprop = Backprop(model)\n\n        target_class = 5\n        input_ = torch.zeros([1, 3, 224, 224])\n\n        backprop.calculate_gradients(input_, target_class)\n\n    assert \'Image must be 299x299 for Inception models.\' in str(error.value)\n\n\ndef test_warn_when_prediction_is_wrong(mocker, model):\n    backprop = Backprop(model)\n\n    top_class = torch.tensor(1)\n    target_class = 5\n\n    input_ = torch.zeros([1, 3, 224, 224])\n\n    make_mock_output(mocker, model, top_class)\n\n    with pytest.warns(UserWarning):\n        backprop.calculate_gradients(input_, target_class)\n\n\n# Test visualize method\n\n\ndef test_visualize_calls_calculate_gradients_twice(mocker, model):\n    backprop = Backprop(model)\n    mocker.spy(backprop, \'calculate_gradients\')\n\n    top_class = 5\n    target_class = 5\n    input_ = torch.zeros([1, 3, 224, 224])\n\n    make_expected_gradient_target(top_class)\n    make_mock_output(mocker, model, target_class)\n\n    backprop.visualize(input_, target_class, use_gpu=True)\n\n    assert backprop.calculate_gradients.call_count == 2\n\n\ndef test_visualize_passes_gpu_flag(mocker, model):\n    backprop = Backprop(model)\n    mocker.spy(backprop, \'calculate_gradients\')\n\n    top_class = 5\n    target_class = 5\n    input_ = torch.zeros([1, 3, 224, 224])\n\n    make_expected_gradient_target(top_class)\n    make_mock_output(mocker, model, target_class)\n\n    backprop.visualize(input_, target_class, use_gpu=True)\n\n    _, _, kwargs = backprop.calculate_gradients.mock_calls[0]\n\n    assert kwargs[\'use_gpu\']\n\n\n# Test compatibilities with torchvision models\n\navailable_models = inspect.getmembers(models, inspect.isfunction)\n\n\n@pytest.mark.parametrize(""name, model_module"", available_models)\ndef test_register_hook_to_first_conv_layer(mocker, name, model_module):\n    model = model_module()\n\n    conv_layer = find_first_conv_layer(model, nn.modules.conv.Conv2d, 3)\n    mocker.spy(conv_layer, \'register_backward_hook\')\n\n    Backprop(model)\n\n    conv_layer.register_backward_hook.assert_called_once()\n\n\n@pytest.mark.parametrize(""name, model_module"", available_models)\ndef test_register_hooks_to_relu_layers(mocker, name, model_module):\n    model = model_module()\n    relu_layers = find_relu_layers(model, nn.ReLU)\n\n    for layer in relu_layers:\n        mocker.spy(layer, \'register_forward_hook\')\n        mocker.spy(layer, \'register_backward_hook\')\n\n    backprop = Backprop(model)\n\n    target_class = 5\n    input_ = torch.zeros([1, 3, 224, 224])\n\n    if \'inception\' in name:\n        input_ = torch.zeros([1, 3, 299, 299])\n\n    make_mock_output(mocker, model, target_class)\n\n    backprop.calculate_gradients(input_, target_class, guided=True)\n\n    for layer in relu_layers:\n\n        layer.register_forward_hook.assert_called_once()\n        layer.register_backward_hook.assert_called_once()\n\n\n@pytest.mark.parametrize(""name, model_module"", available_models)\ndef test_calculate_gradients_for_all_models(mocker, name, model_module):\n    model = model_module()\n    backprop = Backprop(model)\n\n    target_class = 5\n    input_ = torch.zeros([1, 3, 224, 224])\n\n    if \'inception\' in name:\n        input_ = torch.zeros([1, 3, 299, 299])\n\n    make_mock_output(mocker, model, target_class)\n\n    gradients = backprop.calculate_gradients(input_,\n                                             target_class,\n                                             use_gpu=True)\n\n    assert gradients.shape == input_.size()[1:]\n\n\nif __name__ == \'__main__\':\n    pytest.main([__file__])\n'"
tests/test_gradient_ascent.py,2,"b""\nimport inspect\nimport pytest\n\nfrom os import path\n\nimport numpy as np\nimport torchvision.models as models\n\nfrom flashtorch.activmax import GradientAscent\n\nfrom flashtorch.utils import apply_transforms\n\n\n#################\n# Test fixtures #\n#################\n\n\n@pytest.fixture\ndef model():\n    return models.alexnet().features\n\n\n@pytest.fixture\ndef conv_layer(model):\n    return model[0]\n\n\n@pytest.fixture\ndef available_models():\n    return inspect.getmembers(models, inspect.isfunction)\n\n\n@pytest.fixture\ndef g_ascent(model):\n    g_ascent = GradientAscent(model)\n    g_ascent.img_size = 64  # to reduce test time\n    return g_ascent\n\n\n##############\n# Test cases #\n##############\n\n\ndef test_optimize(g_ascent, conv_layer):\n    output = g_ascent.optimize(conv_layer, 0, num_iter=2)\n\n    assert len(output) == 2  # num_iter\n    assert output[0].shape == (1, 3, g_ascent.img_size, g_ascent.img_size)\n\n\ndef test_optimize_with_custom_input(mocker, conv_layer, model):\n    mocker.spy(model, 'forward')\n    g_ascent = GradientAscent(model)\n\n    custom_input = np.uint8(np.random.uniform(150, 180, (64, 64, 3)))\n    custom_input = apply_transforms(custom_input, size=64)\n\n    g_ascent.optimize(conv_layer, 0, input_=custom_input, num_iter=1)\n\n    model.forward.assert_called_with(custom_input)\n\n\ndef test_set_custom_img_size(conv_layer, g_ascent):\n    custom_img_size = 64\n    g_ascent.img_size = custom_img_size\n    assert g_ascent.img_size == custom_img_size\n\n    output = g_ascent.optimize(conv_layer, 0, num_iter=2)\n\n    assert output[0].shape == (1, 3, custom_img_size, custom_img_size)\n\n\ndef test_invalid_layer_str(g_ascent):\n    with pytest.raises(TypeError):\n        g_ascent.optimize('first', 0, num_iter=2)\n\n\ndef test_invalid_layer_int(g_ascent):\n    with pytest.raises(TypeError):\n        g_ascent.optimize(0, 0, num_iter=2)\n\n\ndef test_invalid_layer_not_conv(model, g_ascent):\n    with pytest.raises(TypeError):\n        g_ascent.optimize(model[1], 0, num_iter=2)  # model[1] is  ReLU layer\n\n\ndef test_invalid_filter_idx_not_int(conv_layer, g_ascent):\n    with pytest.raises(TypeError):\n        g_ascent.optimize(conv_layer, 'first', num_iter=2)\n\n\ndef test_invalid_filter_idx_negative(conv_layer, g_ascent):\n    with pytest.raises(ValueError):\n        g_ascent.optimize(conv_layer, -1, num_iter=2)\n\n\ndef test_invalid_filter_idx_too_large(conv_layer, g_ascent):\n    with pytest.raises(ValueError):\n        # Target conv layer has 64 filters\n        g_ascent.optimize(conv_layer, 70, num_iter=2)\n\n\ndef test_register_forward_hook_to_target_layer(mocker, conv_layer, model):\n    mocker.spy(conv_layer, 'register_forward_hook')\n\n    g_ascent = GradientAscent(model)\n    g_ascent.optimize(conv_layer, 0, num_iter=2)\n\n    conv_layer.register_forward_hook.assert_called_once()\n\n\ndef test_register_backward_hook_to_first_conv_layer(mocker, conv_layer, model):\n    mocker.spy(conv_layer, 'register_backward_hook')\n\n    g_ascent = GradientAscent(model)\n    g_ascent.optimize(conv_layer, 0, num_iter=2)\n\n    conv_layer.register_backward_hook.assert_called_once()\n\n\ndef test_remove_any_hooks_before_registering(mocker, conv_layer, model):\n    mocker.spy(conv_layer, 'register_forward_hook')\n    mocker.spy(conv_layer, 'register_backward_hook')\n\n    another_conv_layer = model[10]\n    mocker.spy(another_conv_layer, 'register_forward_hook')\n    mocker.spy(another_conv_layer, 'register_backward_hook')\n\n    g_ascent = GradientAscent(model)\n\n    # Optimize for the first conv layer\n\n    g_ascent.optimize(conv_layer, 0, num_iter=2)\n\n    # Optimize for another\n\n    g_ascent.optimize(another_conv_layer, 1, num_iter=2)\n\n    # Backward hook is registered twice, as we always retrieve\n    # gradients from it, but forward hook is registered only once\n\n    conv_layer.register_forward_hook.assert_called_once()\n    assert conv_layer.register_backward_hook.call_count == 2\n\n    # Instead forward hook is registered on the target layer\n\n    another_conv_layer.register_forward_hook.assert_called_once()\n\n\ndef test_visualize_one_filter(conv_layer, g_ascent):\n    output = g_ascent.visualize(conv_layer, 0, 2, return_output=True)\n\n    assert output[-1].shape == (1, 3, g_ascent.img_size, g_ascent.img_size)\n\n\ndef test_visualize_random_filters_from_one_layer(conv_layer, g_ascent):\n    num_subplots = 3\n\n    output = g_ascent.visualize(conv_layer, num_iter=2,\n                                num_subplots=num_subplots,\n                                return_output=True)\n\n    assert len(output) == num_subplots\n    assert len(output[0]) == 2  # num_iter\n    assert output[0][-1].shape == (1, 3, g_ascent.img_size, g_ascent.img_size)\n\n\ndef test_max_num_of_subplots_is_total_num_of_filters(conv_layer, g_ascent):\n    num_subplots = 100\n\n    output = g_ascent.visualize(\n        conv_layer, num_iter=2, num_subplots=num_subplots, return_output=True)\n\n    assert len(output) == conv_layer.out_channels\n\n\ndef test_visualize_specified_filters_from_one_layer(conv_layer, g_ascent):\n    filter_idxs = np.random.choice(range(64), size=5)\n\n    output = g_ascent.visualize(\n        conv_layer, filter_idxs, num_iter=2, return_output=True)\n\n    assert len(output) == len(filter_idxs)\n\n\ndef test_deepdream(conv_layer, g_ascent):\n    img_path = path.join(path.dirname(__file__), 'resources', 'test_image.jpg')\n\n    output = g_ascent.deepdream(img_path, conv_layer, 0, return_output=True)\n\n    assert len(output) == 20  # default num_iter for deepdream\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/test_imagenet.py,1,"b""import pytest\nfrom collections.abc import Iterable\n\nfrom flashtorch.utils import ImageNetIndex\n\n\n#################\n# Test fixtures #\n#################\n\n\n@pytest.fixture\ndef imagenet():\n    return ImageNetIndex()\n\n\ndef test_is_iterable(imagenet):\n    assert isinstance(imagenet, Iterable)\n    assert isinstance(iter(imagenet), Iterable)\n\n\ndef test_return_length(imagenet):\n    assert len(imagenet) == 1000\n\n\ndef test_list_keys(imagenet):\n    assert len(imagenet.keys()) == 1000\n\n\ndef test_list_items(imagenet):\n    assert len(imagenet.items()) == 1000\n\n\ndef test_return_true_when_target_class_exists(imagenet):\n    assert 'dalmatian' in imagenet\n\n\ndef test_return_false_when_target_class_does_not_exist(imagenet):\n    assert 'invalid class' not in imagenet\n\n\ndef test_find_class_index(imagenet):\n    class_index = imagenet['jay']\n\n    assert class_index == 17\n\n\ndef test_find_whole_match_first(imagenet):\n    class_index = imagenet['king penguin']\n\n    assert class_index == 145\n\n\ndef test_handle_multi_word_target_class(imagenet):\n    class_index = imagenet['dalmatian dog']\n\n    assert class_index == 251\n\n\ndef test_handle_partial_match(imagenet):\n    class_index = imagenet['foxhound']\n\n    assert class_index == 167\n\n\ndef test_return_none_for_invalid_class_name(imagenet):\n    class_index = imagenet['invalid class name']\n\n    assert class_index is None\n\n\ndef test_raise_on_invalid_argument_type(imagenet):\n    with pytest.raises(TypeError) as error:\n        _ = imagenet[1]\n\n    assert 'Target class needs to be a string' in str(error.value)\n\n\ndef test_raise_on_multiple_matches(imagenet):\n    with pytest.raises(ValueError) as error:\n        _ = imagenet['dog']\n\n    assert 'Multiple potential matches found' in str(error.value)\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
tests/test_notebooks.py,0,"b'import subprocess\nimport glob\nimport tempfile\nimport pytest\n\nnotebooks = [nb for nb in glob.glob(""tests/*.ipynb"")]\n\n\n@pytest.mark.parametrize(""nb"", notebooks)\ndef test_execute_notebooks(nb):\n    with tempfile.NamedTemporaryFile(suffix="".ipynb"") as fout:\n        args = [""jupyter"", ""nbconvert"", ""--to"", ""notebook"",\n                ""--execute"", ""--ExecutePreprocessor.timeout=1000"",\n                ""--output"", fout.name, nb]\n\n        subprocess.check_call(args)\n'"
tests/test_utils.py,15,"b""import pytest\n\nfrom os import path\nfrom PIL import Image\n\nimport numpy as np\nimport torch\n\nfrom flashtorch.utils import (load_image,\n                              apply_transforms,\n                              denormalize,\n                              standardize_and_clip,\n                              format_for_plotting)\n\n\n#################\n# Test fixtures #\n#################\n\n\n@pytest.fixture\ndef image():\n    image_path = path.join(path.dirname(__file__),\n                           'resources',\n                           'test_image.jpg')\n\n    return load_image(image_path)\n\n\n##############\n# Test cases #\n##############\n\n\ndef test_convert_image_to_rgb_when_loading_image(image):\n    assert isinstance(image, Image.Image)\n    assert image.mode == 'RGB'\n\n\ndef test_handle_non_pil_as_input():\n    non_pil_input = np.uint8(np.random.uniform(150, 180, (3, 224, 224)))\n\n    transformed = apply_transforms(non_pil_input)\n\n    assert isinstance(transformed, torch.Tensor)\n    assert transformed.requires_grad\n\n\ndef test_handle_non_pil_input_with_channel_last():\n    non_pil_input = np.uint8(np.random.uniform(150, 180, (224, 224, 3)))\n\n    transformed = apply_transforms(non_pil_input)\n\n    assert isinstance(transformed, torch.Tensor)\n    assert transformed.shape == (1, 3, 224, 224)\n\n\ndef test_transform_image_to_tensor(image):\n    transformed = apply_transforms(image)\n\n    assert isinstance(transformed, torch.Tensor)\n\n\ndef test_crop_to_224_by_default(image):\n    transformed = apply_transforms(image)\n\n    assert transformed.shape == (1, 3, 224, 224)\n\n\ndef test_crop_to_custom_size(image):\n    transformed = apply_transforms(image, 299)\n\n    assert transformed.shape == (1, 3, 299, 299)\n\n\ndef test_denormalize_tensor(image):\n    transformed = apply_transforms(image)\n    denormalized = denormalize(transformed)\n\n    assert denormalized.shape == transformed.shape\n    assert denormalized.min() >= 0.0 and denormalized.max() <= 1.0\n\n\ndef test_format_multi_channel_tensor_with_batch_dimension():\n    input_ = torch.zeros((1, 3, 224, 224))\n\n    formatted = format_for_plotting(input_)\n\n    assert formatted.shape == (224, 224, 3)\n\n\ndef test_format_mono_channel_tensor_with_batch_dimension():\n    input_ = torch.zeros((1, 1, 224, 224))\n    formatted = format_for_plotting(input_)\n\n    assert formatted.shape == (224, 224)\n\n\ndef test_format_multi_channel_tensor_without_batch_dimension():\n    input_ = torch.zeros((3, 224, 224))\n    formatted = format_for_plotting(input_)\n\n    assert formatted.shape == (224, 224, 3)\n\n\ndef test_format_mono_channel_tensor_without_batch_dimension():\n    input_ = torch.zeros((1, 224, 224))\n    formatted = format_for_plotting(input_)\n\n    assert formatted.shape == (224, 224)\n\n\ndef test_detach_tensor_from_computational_graph():\n    input_ = torch.zeros((1, 224, 224))\n    input_.requires_grad = True\n\n    formatted = format_for_plotting(input_)\n\n    assert not formatted.requires_grad\n\n\ndef test_standardize_and_clip_tensor():\n    default_min = 0.0\n    default_max = 1.0\n\n    input_ = torch.randint(low=-1000, high=1000, size=(224, 224)).float()\n    normalized = standardize_and_clip(input_)\n\n    assert normalized.shape == input_.shape\n    assert normalized.min().item() >= default_min\n    assert normalized.max().item() <= default_max\n\n\ndef test_standardize_and_clip_detach_input_from_graph():\n\n    input_ = torch.randint(low=-1000, high=1000, size=(224, 224)).float()\n    input_.requires_grad = True\n    normalized = standardize_and_clip(input_)\n\n    assert not normalized.requires_grad\n\n\ndef test_standardize_and_clip_with_custom_min_max():\n    custom_min = 2.0\n    custom_max = 3.0\n\n    input_ = torch.randint(low=-1000, high=1000, size=(224, 224)).float()\n    normalized = standardize_and_clip(input_,\n                                      min_value=custom_min,\n                                      max_value=custom_max)\n\n    assert normalized.shape == input_.shape\n    assert normalized.min() >= custom_min\n    assert normalized.max() <= custom_max\n\n\ndef test_standardize_and_clip_mono_channel_tensor():\n    default_min = 0.0\n    default_max = 1.0\n\n    input_ = torch.randint(low=-1000, high=1000, size=(1, 224, 224)).float()\n    normalized = standardize_and_clip(input_)\n\n    assert normalized.shape == input_.shape\n    assert normalized.min().item() >= default_min\n    assert normalized.max().item() <= default_max\n\n\ndef test_standardize_and_clip_multi_channel_tensor():\n    default_min = 0.0\n    default_max = 1.0\n\n    input_ = torch.randint(low=-1000, high=1000, size=(3, 224, 224)).float()\n    normalized = standardize_and_clip(input_)\n\n    assert normalized.shape == input_.shape\n\n    for channel in normalized:\n        assert channel.min().item() >= default_min\n        assert channel.max().item() <= default_max\n\n\ndef test_standardize_and_clip_add_eplison_when_std_is_zero():\n    default_min = 0.0\n    default_max = 1.0\n\n    input_ = torch.zeros(1, 244, 244)\n    normalized = standardize_and_clip(input_)\n\n    assert normalized.shape == input_.shape\n\n    for channel in normalized:\n        assert channel.min().item() >= default_min\n        assert channel.max().item() <= default_max\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"""
flashtorch/activmax/__init__.py,0,b'# flake8: noqa\nfrom .gradient_ascent import *\n'
flashtorch/activmax/gradient_ascent.py,17,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\n\nfrom flashtorch.utils import (apply_transforms,\n                              format_for_plotting,\n                              load_image,\n                              standardize_and_clip)\n\n\nclass GradientAscent:\n    """"""Provides an interface for activation maximization via gradient descent.\n\n    This class implements the gradient ascent algorithm in order to perform\n    activation maximization with convolutional neural networks (CNN).\n\n    `Activation maximization <https://pdfs.semanticscholar.org/65d9/94fb778a8d9e0f632659fb33a082949a50d3.pdf>`_\n    is one form of feature visualization that allows us to visualize what CNN\n    filters are ""looking for"", by applying each filter to an input image and\n    updating the input image so as to maximize the activation of the filter of\n    interest (i.e. treating it as a gradient ascent task with activation as the\n    loss). The implementation is inspired by `this demo <https://blog.keras.io/category/demo.html>`_\n    by Francois Chollet.\n\n    Args:\n        model: A neural network model from `torchvision.models\n            <https://pytorch.org/docs/stable/torchvision/models.html>`_,\n            typically without the fully-connected part of the network.\n            e.g. torchvisions.alexnet(pretrained=True).features\n        img_size (int, optional, default=224): The size of an input image to be\n            optimized.\n        lr (float, optional, default=1.): The step size (or learning rate) of\n            the gradient ascent.\n        use_gpu (bool, optional, default=False): Use GPU if set to True and\n            `torch.cuda.is_available()`.\n\n    """""" # noqa\n\n    ####################\n    # Public interface #\n    ####################\n\n    def __init__(self, model, img_size=224, lr=1., use_gpu=False):\n        self.model = model\n        self._img_size = img_size\n        self._lr = lr\n        self._use_gpu = use_gpu\n\n        self.num_layers = len(list(self.model.named_children()))\n        self.activation = None\n        self.gradients = None\n\n        self.handlers = []\n\n        self.output = None\n\n    @property\n    def lr(self):\n        return self._lr\n\n    @lr.setter\n    def lr(self, lr):\n        self._lr = lr\n\n    @property\n    def img_size(self):\n        return self._img_size\n\n    @img_size.setter\n    def img_size(self, img_size):\n        self._img_size = img_size\n\n    @property\n    def use_gpu(self):\n        return self._use_gpu\n\n    @use_gpu.setter\n    def use_gpu(self, use_gpu):\n        self._use_gpu = use_gpu\n\n    def optimize(self, layer, filter_idx, input_=None, num_iter=30):\n        """"""Generates an image that maximally activates the target filter.\n\n        Args:\n            layer (torch.nn.modules.conv.Conv2d): The target Conv2d layer from\n                which the filter to be chosen, based on `filter_idx`.\n            filter_idx (int): The index of the target filter.\n            num_iter (int, optional, default=30): The number of iteration for\n                the gradient ascent operation.\n\n        Returns:\n            output (list of torch.Tensor): With dimentions\n                :math:`(num_iter, C, H, W)`. The size of the image is\n                determined by `img_size` attribute which defaults to 224.\n\n        """"""\n\n        # Validate the type of the layer\n\n        if type(layer) != nn.modules.conv.Conv2d:\n            raise TypeError(\'The layer must be nn.modules.conv.Conv2d.\')\n\n        # Validate filter index\n\n        num_total_filters = layer.out_channels\n        self._validate_filter_idx(num_total_filters, filter_idx)\n\n        # Inisialize input (as noise) if not provided\n\n        if input_ is None:\n            input_ = np.uint8(np.random.uniform(\n                150, 180, (self._img_size, self._img_size, 3)))\n            input_ = apply_transforms(input_, size=self._img_size)\n\n        if torch.cuda.is_available() and self.use_gpu:\n            self.model = self.model.to(\'cuda\')\n            input_ = input_.to(\'cuda\')\n\n        # Remove previous hooks if any\n\n        while len(self.handlers) > 0:\n            self.handlers.pop().remove()\n\n        # Register hooks to record activation and gradients\n\n        self.handlers.append(self._register_forward_hooks(layer, filter_idx))\n        self.handlers.append(self._register_backward_hooks())\n\n        # Inisialize gradients\n\n        self.gradients = torch.zeros(input_.shape)\n\n        # Optimize\n\n        return self._ascent(input_, num_iter)\n\n    def visualize(self, layer, filter_idxs=None, lr=1., num_iter=30,\n                  num_subplots=4, figsize=(4, 4), title=\'Conv2d\',\n                  return_output=False):\n        """"""Optimizes for the target layer/filter and visualizes the output.\n\n        A method that combines optimization and visualization. There are\n        mainly 3 types of operations, given a target layer:\n\n        1. If `filter_idxs` is provided as an integer, it optimizes for the\n            filter specified and plots the output.\n        2. If `filter_idxs` is provided as a list of integers, it optimizes for\n            all the filters specified and plots the output.\n        3. if `filter_idx` is not provided, i.e. None, it randomly chooses\n            `num_subplots` number of filters from the layer provided and\n            plots the output.\n\n        It also returns the output of the optimization, if specified with\n        `return_output=True`.\n\n        Args:\n            layer (torch.nn.modules.conv.Conv2d): The target Conv2d layer from\n                which the filter to be chosen, based on `filter_idx`.\n            filter_idxs (int or list of int, optional, default=None): The index\n                or indecies of the target filter(s).\n            lr (float, optional, default=.1): The step size of optimization.\n            num_iter (int, optional, default=30): The number of iteration for\n                the gradient ascent operation.\n            num_subplots (int, optional, default=4): The number of filters to\n                optimize for and visualize. Relevant in case 3 above.\n            figsize (tuple, optional, default=(4, 4)): The size of the plot.\n                Relevant in case 1 above.\n            title (str, optional default=\'Conv2d\'): The title of the plot.\n            return_output (bool, optional, default=False): Returns the\n                output(s) of optimization if set to True.\n\n\n        Returns:\n            For a single optimization (i.e. case 1 above):\n                output (list of torch.Tensor): With dimentions\n                    :math:`(num_iter, C, H, W)`. The size of the image is\n                    determined by `img_size` attribute which defaults to 224.\n            For multiple optimization (i.e. case 2 or 3 above):\n                output (list of list of torch.Tensor): With dimentions\n                    :math:`(num_subplots, num_iter, C, H, W)`. The size of the\n                    image is determined by `img_size` attribute which defaults\n                    to 224.\n\n        """"""\n\n        self._lr = lr\n\n        if (type(filter_idxs) == int):\n            self._visualize_filter(layer,\n                                   filter_idxs,\n                                   num_iter=num_iter,\n                                   figsize=figsize,\n                                   title=title)\n        else:\n            num_total_filters = layer.out_channels\n\n            if filter_idxs is None:\n                num_subplots = min(num_total_filters, num_subplots)\n                filter_idxs = np.random.choice(range(num_total_filters),\n                                               size=num_subplots)\n\n            self._visualize_filters(layer,\n                                    filter_idxs,\n                                    num_iter,\n                                    len(filter_idxs),\n                                    title=title)\n\n        if return_output:\n            return self.output\n\n    def deepdream(self, img_path, layer, filter_idx, lr=.1, num_iter=20,\n                  figsize=(4, 4), title=\'DeepDream\', return_output=False):\n        """"""Creates DeepDream.\n\n        It applies the optimization on the image provided. The image is loaded\n        and made into a torch.Tensor that is compatible as the input to the\n        network.\n\n        Read the original blog post by Google for more information on\n        `DeepDream <https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html>`_.\n\n        Args:\n            img_path (str): A path to the image you want to apply DeepDream on\n            layer (torch.nn.modules.conv.Conv2d): The target Conv2d layer from\n                which the filter to be chosen, based on `filter_idx`.\n            filter_idx (int): The index of the target filter.\n            lr (float, optional, default=.1): The step size of optimization.\n            num_iter (int, optional, default=30): The number of iteration for\n                the gradient ascent operation.\n            figsize (tuple, optional, default=(4, 4)): The size of the plot.\n                Relevant in case 1 above.\n            title (str, optional default=\'Conv2d\'): The title of the plot.\n            return_output (bool, optional, default=False): Returns the\n                output(s) of optimization if set to True.\n\n        Returns:\n            output (list of torch.Tensor): With dimentions\n                :math:`(num_iter, C, H, W)`. The size of the image is\n                determined by `img_size` attribute which defaults to 224.\n\n        """""" # noqa\n\n        input_ = apply_transforms(load_image(img_path), self.img_size)\n\n        self._lr = lr\n        output = self.optimize(layer, filter_idx, input_, num_iter=num_iter)\n\n        plt.figure(figsize=figsize)\n        plt.axis(\'off\')\n        plt.title(title)\n\n        plt.imshow(format_for_plotting(\n            standardize_and_clip(output[-1],\n                                 saturation=0.15,\n                                 brightness=0.7))); # noqa\n\n        if return_output:\n            return output\n\n    #####################\n    # Private interface #\n    #####################\n\n    def _register_forward_hooks(self, layer, filter_idx):\n        def _record_activation(module, input_, output):\n            self.activation = torch.mean(output[:, filter_idx, :, :])\n\n        return layer.register_forward_hook(_record_activation)\n\n    def _register_backward_hooks(self):\n        def _record_gradients(module, grad_in, grad_out):\n            if self.gradients.shape == grad_in[0].shape:\n                self.gradients = grad_in[0]\n\n        for _, module in self.model.named_modules():\n            if isinstance(module, nn.modules.conv.Conv2d) and \\\n                    module.in_channels == 3:\n                return module.register_backward_hook(_record_gradients)\n\n    def _ascent(self, x, num_iter):\n        output = []\n\n        for i in range(num_iter):\n            self.model(x)\n\n            self.activation.backward()\n\n            self.gradients /= (torch.sqrt(torch.mean(\n                torch.mul(self.gradients, self.gradients))) + 1e-5)\n\n            x = x + self.gradients * self._lr\n            output.append(x)\n\n        return output\n\n    def _validate_filter_idx(self, num_filters, filter_idx):\n        if not np.issubdtype(type(filter_idx), np.integer):\n            raise TypeError(\'Indecies must be integers.\')\n        elif (filter_idx < 0) or (filter_idx > num_filters):\n            raise ValueError(f\'Filter index must be between 0 and \\\n                             {num_filters - 1}.\')\n\n    def _visualize_filter(self, layer, filter_idx, num_iter, figsize, title):\n        self.output = self.optimize(layer, filter_idx, num_iter=num_iter)\n\n        plt.figure(figsize=figsize)\n        plt.axis(\'off\')\n        plt.title(title)\n\n        plt.imshow(format_for_plotting(\n            standardize_and_clip(self.output[-1],\n                                 saturation=0.15,\n                                 brightness=0.7))); # noqa\n\n    def _visualize_filters(self, layer, filter_idxs, num_iter, num_subplots,\n                           title):\n        # Prepare the main plot\n\n        num_cols = 4\n        num_rows = int(np.ceil(num_subplots / num_cols))\n\n        fig = plt.figure(figsize=(16, num_rows * 5))\n        plt.title(title)\n        plt.axis(\'off\')\n\n        self.output = []\n\n        # Plot subplots\n\n        for i, filter_idx in enumerate(filter_idxs):\n            output = self.optimize(layer, filter_idx, num_iter=num_iter)\n\n            self.output.append(output)\n\n            ax = fig.add_subplot(num_rows, num_cols, i+1)\n            ax.set_xticks([])\n            ax.set_yticks([])\n            ax.set_title(f\'filter {filter_idx}\')\n\n            ax.imshow(format_for_plotting(\n                standardize_and_clip(output[-1],\n                                     saturation=0.15,\n                                     brightness=0.7)))\n\n        plt.subplots_adjust(wspace=0, hspace=0); # noqa\n'"
flashtorch/saliency/__init__.py,0,b'# flake8: noqa\nfrom .backprop import *\n'
flashtorch/saliency/backprop.py,13,"b'import warnings\n\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\n\n\nfrom flashtorch.utils import (denormalize,\n                              format_for_plotting,\n                              standardize_and_clip)\n\n\nclass Backprop:\n    """"""Provides an interface to perform backpropagation.\n\n    This class provids a way to calculate the gradients of a target class\n    output w.r.t. an input image, by performing a single backprobagation.\n\n    The gradients obtained can be used to visualise an image-specific class\n    saliency map, which can gives some intuition on regions within the input\n    image that contribute the most (and least) to the corresponding output.\n\n    More details on saliency maps: `Deep Inside Convolutional Networks:\n    Visualising Image Classification Models and Saliency Maps\n    <https://arxiv.org/pdf/1312.6034.pdf>`_.\n\n    Args:\n        model: A neural network model from `torchvision.models\n            <https://pytorch.org/docs/stable/torchvision/models.html>`_.\n\n    """""" # noqa\n\n    ####################\n    # Public interface #\n    ####################\n\n    def __init__(self, model):\n        self.model = model\n        self.model.eval()\n        self.gradients = None\n        self._register_conv_hook()\n\n    def calculate_gradients(self,\n                            input_,\n                            target_class=None,\n                            take_max=False,\n                            guided=False,\n                            use_gpu=False):\n\n        """"""Calculates gradients of the target_class output w.r.t. an input_.\n\n        The gradients is calculated for each colour channel. Then, the maximum\n        gradients across colour channels is returned.\n\n        Args:\n            input_ (torch.Tensor): With shape :math:`(N, C, H, W)`.\n            target_class (int, optional, default=None)\n            take_max (bool, optional, default=False): If True, take the maximum\n                gradients across colour channels for each pixel.\n            guided (bool, optional, default=Fakse): If True, perform guided\n                backpropagation. See `Striving for Simplicity: The All\n                Convolutional Net <https://arxiv.org/pdf/1412.6806.pdf>`_.\n            use_gpu (bool, optional, default=False): Use GPU if set to True and\n                `torch.cuda.is_available()`.\n\n        Returns:\n            gradients (torch.Tensor): With shape :math:`(C, H, W)`.\n\n        """""" # noqa\n\n        if \'inception\' in self.model.__class__.__name__.lower():\n            if input_.size()[1:] != (3, 299, 299):\n                raise ValueError(\'Image must be 299x299 for Inception models.\')\n\n        if guided:\n            self.relu_outputs = []\n            self._register_relu_hooks()\n\n        if torch.cuda.is_available() and use_gpu:\n            self.model = self.model.to(\'cuda\')\n            input_ = input_.to(\'cuda\')\n\n        self.model.zero_grad()\n\n        self.gradients = torch.zeros(input_.shape)\n\n        # Get a raw prediction value (logit) from the last linear layer\n\n        output = self.model(input_)\n\n        # Don\'t set the gradient target if the model is a binary classifier\n        # i.e. has one class prediction\n\n        if len(output.shape) == 1:\n            target = None\n        else:\n            _, top_class = output.topk(1, dim=1)\n\n            # Create a 2D tensor with shape (1, num_classes) and\n            # set all element to zero\n\n            target = torch.FloatTensor(1, output.shape[-1]).zero_()\n\n            if torch.cuda.is_available() and use_gpu:\n                target = target.to(\'cuda\')\n\n            if (target_class is not None) and (top_class != target_class):\n                warnings.warn(UserWarning(\n                    f\'The predicted class index {top_class.item()} does not\' +\n                    f\'equal the target class index {target_class}. \' +\n                    \'Calculating the gradient w.r.t. the predicted class.\'\n                ))\n\n            # Set the element at top class index to be 1\n\n            target[0][top_class] = 1\n\n        # Calculate gradients of the target class output w.r.t. input_\n\n        output.backward(gradient=target)\n\n        # Detach the gradients from the graph and move to cpu\n\n        gradients = self.gradients.detach().cpu()[0]\n\n        if take_max:\n            # Take the maximum across colour channels\n\n            gradients = gradients.max(dim=0, keepdim=True)[0]\n\n        return gradients\n\n    def visualize(self, input_, target_class, guided=False, use_gpu=False,\n                  figsize=(16, 4), cmap=\'viridis\', alpha=.5,\n                  return_output=False):\n        """"""Calculates gradients and visualizes the output.\n\n        A method that combines the backprop operation and visualization.\n\n        It also returns the gradients, if specified with `return_output=True`.\n\n        Args:\n            input_ (torch.Tensor): With shape :math:`(N, C, H, W)`.\n            target_class (int, optional, default=None)\n            take_max (bool, optional, default=False): If True, take the maximum\n                gradients across colour channels for each pixel.\n            guided (bool, optional, default=Fakse): If True, perform guided\n                backpropagation. See `Striving for Simplicity: The All\n                Convolutional Net <https://arxiv.org/pdf/1412.6806.pdf>`_.\n            use_gpu (bool, optional, default=False): Use GPU if set to True and\n                `torch.cuda.is_available()`.\n            figsize (tuple, optional, default=(16, 4)): The size of the plot.\n            cmap (str, optional, default=\'viridis): The color map of the\n                gradients plots. See avaialable color maps `here <https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html>`_.\n            alpha (float, optional, default=.5): The alpha value of the max\n                gradients to be jaxaposed on top of the input image.\n            return_output (bool, optional, default=False): Returns the\n                output(s) of optimization if set to True.\n\n        Returns:\n            gradients (torch.Tensor): With shape :math:`(C, H, W)`.\n        """""" # noqa\n\n        # Calculate gradients\n\n        gradients = self.calculate_gradients(input_,\n                                             target_class,\n                                             guided=guided,\n                                             use_gpu=use_gpu)\n        max_gradients = self.calculate_gradients(input_,\n                                                 target_class,\n                                                 guided=guided,\n                                                 take_max=True,\n                                                 use_gpu=use_gpu)\n\n        # Setup subplots\n\n        subplots = [\n            # (title, [(image1, cmap, alpha), (image2, cmap, alpha)])\n            (\'Input image\',\n             [(format_for_plotting(denormalize(input_)), None, None)]),\n            (\'Gradients across RGB channels\',\n             [(format_for_plotting(standardize_and_clip(gradients)),\n              None,\n              None)]),\n            (\'Max gradients\',\n             [(format_for_plotting(standardize_and_clip(max_gradients)),\n              cmap,\n              None)]),\n            (\'Overlay\',\n             [(format_for_plotting(denormalize(input_)), None, None),\n              (format_for_plotting(standardize_and_clip(max_gradients)),\n               cmap,\n               alpha)])\n        ]\n\n        fig = plt.figure(figsize=figsize)\n\n        for i, (title, images) in enumerate(subplots):\n            ax = fig.add_subplot(1, len(subplots), i + 1)\n            ax.set_axis_off()\n            ax.set_title(title)\n\n            for image, cmap, alpha in images:\n                ax.imshow(image, cmap=cmap, alpha=alpha)\n\n        if return_output:\n            return gradients, max_gradients\n\n    #####################\n    # Private interface #\n    #####################\n\n    def _register_conv_hook(self):\n        def _record_gradients(module, grad_in, grad_out):\n            if self.gradients.shape == grad_in[0].shape:\n                self.gradients = grad_in[0]\n\n        for _, module in self.model.named_modules():\n            if isinstance(module, nn.modules.conv.Conv2d):\n                module.register_backward_hook(_record_gradients)\n                break\n\n    def _register_relu_hooks(self):\n        def _record_output(module, input_, output):\n            self.relu_outputs.append(output)\n\n        def _clip_gradients(module, grad_in, grad_out):\n            relu_output = self.relu_outputs.pop()\n            clippled_grad_out = grad_out[0].clamp(0.0)\n\n            return (clippled_grad_out.mul(relu_output),)\n\n        for _, module in self.model.named_modules():\n            if isinstance(module, nn.ReLU):\n                module.register_forward_hook(_record_output)\n                module.register_backward_hook(_clip_gradients)\n'"
flashtorch/utils/__init__.py,9,"b'#!/usr/bin/env python\n""""""flashtorch.utils\n\nThis module provides utility functions for image handling and tensor\ntransformation.\n\n""""""\nfrom PIL import Image\n\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as F\n\nfrom .imagenet import * # noqa\n\n\ndef load_image(image_path):\n    """"""Loads image as a PIL RGB image.\n\n    Args:\n        image_path (str): A path to the image\n\n    Returns:\n        An instance of PIL.Image.Image in RGB\n\n    """"""\n\n    return Image.open(image_path).convert(\'RGB\')\n\n\ndef apply_transforms(image, size=224):\n    """"""Transforms a PIL image to torch.Tensor.\n\n    Applies a series of tranformations on PIL image including a conversion\n    to a tensor. The returned tensor has a shape of :math:`(N, C, H, W)` and\n    is ready to be used as an input to neural networks.\n\n    First the image is resized to 256, then cropped to 224. The `means` and\n    `stds` for normalisation are taken from numbers used in ImageNet, as\n    currently developing the package for visualizing pre-trained models.\n\n    The plan is to to expand this to handle custom size/mean/std.\n\n    Args:\n        image (PIL.Image.Image or numpy array)\n        size (int, optional, default=224): Desired size (width/height) of the\n            output tensor\n\n    Shape:\n        Input: :math:`(C, H, W)` for numpy array\n        Output: :math:`(N, C, H, W)`\n\n    Returns:\n        torch.Tensor (torch.float32): Transformed image tensor\n\n    Note:\n        Symbols used to describe dimensions:\n            - N: number of images in a batch\n            - C: number of channels\n            - H: height of the image\n            - W: width of the image\n\n    """"""\n\n    if not isinstance(image, Image.Image):\n        image = F.to_pil_image(image)\n\n    means = [0.485, 0.456, 0.406]\n    stds = [0.229, 0.224, 0.225]\n\n    transform = transforms.Compose([\n        transforms.Resize(size),\n        transforms.CenterCrop(size),\n        transforms.ToTensor(),\n        transforms.Normalize(means, stds)\n    ])\n\n    tensor = transform(image).unsqueeze(0)\n\n    tensor.requires_grad = True\n\n    return tensor\n\n\ndef denormalize(tensor):\n    """"""Reverses the normalisation on a tensor.\n\n    Performs a reverse operation on a tensor, so the pixel value range is\n    between 0 and 1. Useful for when plotting a tensor into an image.\n\n    Normalisation: (image - mean) / std\n    Denormalisation: image * std + mean\n\n    Args:\n        tensor (torch.Tensor, dtype=torch.float32): Normalized image tensor\n\n    Shape:\n        Input: :math:`(N, C, H, W)`\n        Output: :math:`(N, C, H, W)` (same shape as input)\n\n    Return:\n        torch.Tensor (torch.float32): Demornalised image tensor with pixel\n            values between [0, 1]\n\n    Note:\n        Symbols used to describe dimensions:\n            - N: number of images in a batch\n            - C: number of channels\n            - H: height of the image\n            - W: width of the image\n\n    """"""\n\n    means = [0.485, 0.456, 0.406]\n    stds = [0.229, 0.224, 0.225]\n\n    denormalized = tensor.clone()\n\n    for channel, mean, std in zip(denormalized[0], means, stds):\n        channel.mul_(std).add_(mean)\n\n    return denormalized\n\n\ndef standardize_and_clip(tensor, min_value=0.0, max_value=1.0,\n                         saturation=0.1, brightness=0.5):\n\n    """"""Standardizes and clips input tensor.\n\n    Standardizes the input tensor (mean = 0.0, std = 1.0). The color saturation\n    and brightness are adjusted, before tensor values are clipped to min/max\n    (default: 0.0/1.0).\n\n    Args:\n        tensor (torch.Tensor):\n        min_value (float, optional, default=0.0)\n        max_value (float, optional, default=1.0)\n        saturation (float, optional, default=0.1)\n        brightness (float, optional, default=0.5)\n\n    Shape:\n        Input: :math:`(C, H, W)`\n        Output: Same as the input\n\n    Return:\n        torch.Tensor (torch.float32): Normalised tensor with values between\n            [min_value, max_value]\n\n    """"""\n\n    tensor = tensor.detach().cpu()\n\n    mean = tensor.mean()\n    std = tensor.std()\n\n    if std == 0:\n        std += 1e-7\n\n    standardized = tensor.sub(mean).div(std).mul(saturation)\n    clipped = standardized.add(brightness).clamp(min_value, max_value)\n\n    return clipped\n\n\ndef format_for_plotting(tensor):\n    """"""Formats the shape of tensor for plotting.\n\n    Tensors typically have a shape of :math:`(N, C, H, W)` or :math:`(C, H, W)`\n    which is not suitable for plotting as images. This function formats an\n    input tensor :math:`(H, W, C)` for RGB and :math:`(H, W)` for mono-channel\n    data.\n\n    Args:\n        tensor (torch.Tensor, torch.float32): Image tensor\n\n    Shape:\n        Input: :math:`(N, C, H, W)` or :math:`(C, H, W)`\n        Output: :math:`(H, W, C)` or :math:`(H, W)`, respectively\n\n    Return:\n        torch.Tensor (torch.float32): Formatted image tensor (detached)\n\n    Note:\n        Symbols used to describe dimensions:\n            - N: number of images in a batch\n            - C: number of channels\n            - H: height of the image\n            - W: width of the image\n\n    """"""\n\n    has_batch_dimension = len(tensor.shape) == 4\n    formatted = tensor.clone()\n\n    if has_batch_dimension:\n        formatted = tensor.squeeze(0)\n\n    if formatted.shape[0] == 1:\n        return formatted.squeeze(0).detach()\n    else:\n        return formatted.permute(1, 2, 0).detach()\n'"
flashtorch/utils/imagenet.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n""""""\n""""""\n\nimport json\n\nfrom collections.abc import Mapping\nfrom importlib_resources import path\n\nfrom . import resources\n\n\nclass ImageNetIndex(Mapping):\n    """"""Interface to retrieve ImageNet class indeces from class names.\n\n    This class implements a dictionary like object, aiming to provide an\n    easy-to-use look-up table for finding a target class index from an ImageNet\n    class name.\n\n    Reference:\n        - ImageNet class index: https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\n        - Synsets: http://image-net.org/challenges/LSVRC/2015/browse-synsets\n\n    Note:\n        Class names in `imagenet_class_index.json` has been slightly modified\n        from the source due to duplicated class names (e.g. crane). This helps\n        make the use of this tool simpler.\n    """""" # noqa\n\n    def __init__(self):\n        self._index = {}\n\n        with path(resources, \'imagenet_class_index.json\') as source_path:\n            with open(str(source_path), \'r\') as source:\n                data = json.load(source)\n\n        for index, (_, class_name) in data.items():\n            class_name = class_name.lower().replace(\'_\', \' \')\n            self._index[class_name] = int(index)\n\n    def __len__(self):\n        return len(self._index)\n\n    def __iter__(self):\n        return iter(self._index)\n\n    def __getitem__(self, phrase):\n        if type(phrase) != str:\n            raise TypeError(\'Target class needs to be a string.\')\n\n        if phrase in self._index:\n            return self._index[phrase]\n\n        partial_matches = self._find_partial_matches(phrase)\n\n        if not any(partial_matches):\n            return None\n        elif len(partial_matches) > 1:\n            raise ValueError(\'Multiple potential matches found: {}\'\n                             .format(\', \'.join(map(str, partial_matches))))\n\n        target_class = partial_matches.pop()\n\n        return self._index[target_class]\n\n    def __contains__(self, key):\n        return any(key in name for name in self._index)\n\n    def keys(self):\n        return self._index.keys()\n\n    def items(self):\n        return self._index.items()\n\n    def _find_partial_matches(self, phrase):\n        words = phrase.lower().split(\' \')\n\n        # Find the intersection between search words and class names to\n        # prioritise whole word matches\n        # e.g. If words = {\'dalmatian\', \'dog\'} then matches \'dalmatian\'\n\n        matches = set(words).intersection(set(self.keys()))\n\n        if not any(matches):\n            # Find substring matches between search words and class names to\n            # accommodate for fuzzy matches to some extend\n            # e.g. If words = {\'foxhound\'} then matches \'english foxhound\'\n\n            matches = [key for word in words for key in self.keys()\n                       if word in key]\n\n        return matches\n'"
flashtorch/utils/resources/__init__.py,0,b''
