file_path,api_count,code
tutorial-contents/201_torch_numpy.py,9,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.1.11\nnumpy\n""""""\nimport torch\nimport numpy as np\n\n# details about math operation in torch can be found in: http://pytorch.org/docs/torch.html#math-operations\n\n# convert numpy to tensor or vise versa\nnp_data = np.arange(6).reshape((2, 3))\ntorch_data = torch.from_numpy(np_data)\ntensor2array = torch_data.numpy()\nprint(\n    \'\\nnumpy array:\', np_data,          # [[0 1 2], [3 4 5]]\n    \'\\ntorch tensor:\', torch_data,      #  0  1  2 \\n 3  4  5    [torch.LongTensor of size 2x3]\n    \'\\ntensor to array:\', tensor2array, # [[0 1 2], [3 4 5]]\n)\n\n\n# abs\ndata = [-1, -2, 1, 2]\ntensor = torch.FloatTensor(data)  # 32-bit floating point\nprint(\n    \'\\nabs\',\n    \'\\nnumpy: \', np.abs(data),          # [1 2 1 2]\n    \'\\ntorch: \', torch.abs(tensor)      # [1 2 1 2]\n)\n\n# sin\nprint(\n    \'\\nsin\',\n    \'\\nnumpy: \', np.sin(data),      # [-0.84147098 -0.90929743  0.84147098  0.90929743]\n    \'\\ntorch: \', torch.sin(tensor)  # [-0.8415 -0.9093  0.8415  0.9093]\n)\n\n# mean\nprint(\n    \'\\nmean\',\n    \'\\nnumpy: \', np.mean(data),         # 0.0\n    \'\\ntorch: \', torch.mean(tensor)     # 0.0\n)\n\n# matrix multiplication\ndata = [[1,2], [3,4]]\ntensor = torch.FloatTensor(data)  # 32-bit floating point\n# correct method\nprint(\n    \'\\nmatrix multiplication (matmul)\',\n    \'\\nnumpy: \', np.matmul(data, data),     # [[7, 10], [15, 22]]\n    \'\\ntorch: \', torch.mm(tensor, tensor)   # [[7, 10], [15, 22]]\n)\n# incorrect method\ndata = np.array(data)\nprint(\n    \'\\nmatrix multiplication (dot)\',\n    \'\\nnumpy: \', data.dot(data),        # [[7, 10], [15, 22]]\n    \'\\ntorch: \', tensor.dot(tensor)     # this will convert tensor to [1,2,3,4], you\'ll get 30.0\n)'"
tutorial-contents/202_variable.py,8,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.1.11\n""""""\nimport torch\nfrom torch.autograd import Variable\n\n# Variable in torch is to build a computational graph,\n# but this graph is dynamic compared with a static graph in Tensorflow or Theano.\n# So torch does not have placeholder, torch can just pass variable to the computational graph.\n\ntensor = torch.FloatTensor([[1,2],[3,4]])            # build a tensor\nvariable = Variable(tensor, requires_grad=True)      # build a variable, usually for compute gradients\n\nprint(tensor)       # [torch.FloatTensor of size 2x2]\nprint(variable)     # [torch.FloatTensor of size 2x2]\n\n# till now the tensor and variable seem the same.\n# However, the variable is a part of the graph, it\'s a part of the auto-gradient.\n\nt_out = torch.mean(tensor*tensor)       # x^2\nv_out = torch.mean(variable*variable)   # x^2\nprint(t_out)\nprint(v_out)    # 7.5\n\nv_out.backward()    # backpropagation from v_out\n# v_out = 1/4 * sum(variable*variable)\n# the gradients w.r.t the variable, d(v_out)/d(variable) = 1/4*2*variable = variable/2\nprint(variable.grad)\n\'\'\'\n 0.5000  1.0000\n 1.5000  2.0000\n\'\'\'\n\nprint(variable)     # this is data in variable format\n""""""\nVariable containing:\n 1  2\n 3  4\n[torch.FloatTensor of size 2x2]\n""""""\n\nprint(variable.data)    # this is data in tensor format\n""""""\n 1  2\n 3  4\n[torch.FloatTensor of size 2x2]\n""""""\n\nprint(variable.data.numpy())    # numpy format\n""""""\n[[ 1.  2.]\n [ 3.  4.]]\n""""""'"
tutorial-contents/203_activation.py,7,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.4\nmatplotlib\n""""""\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\n\n# fake data\nx = torch.linspace(-5, 5, 200)  # x data (tensor), shape=(100, 1)\nx = Variable(x)\nx_np = x.data.numpy()   # numpy array for plotting\n\n# following are popular activation functions\ny_relu = torch.relu(x).data.numpy()\ny_sigmoid = torch.sigmoid(x).data.numpy()\ny_tanh = torch.tanh(x).data.numpy()\ny_softplus = F.softplus(x).data.numpy() # there\'s no softplus in torch\n# y_softmax = torch.softmax(x, dim=0).data.numpy() softmax is a special kind of activation function, it is about probability\n\n# plt to visualize these activation function\nplt.figure(1, figsize=(8, 6))\nplt.subplot(221)\nplt.plot(x_np, y_relu, c=\'red\', label=\'relu\')\nplt.ylim((-1, 5))\nplt.legend(loc=\'best\')\n\nplt.subplot(222)\nplt.plot(x_np, y_sigmoid, c=\'red\', label=\'sigmoid\')\nplt.ylim((-0.2, 1.2))\nplt.legend(loc=\'best\')\n\nplt.subplot(223)\nplt.plot(x_np, y_tanh, c=\'red\', label=\'tanh\')\nplt.ylim((-1.2, 1.2))\nplt.legend(loc=\'best\')\n\nplt.subplot(224)\nplt.plot(x_np, y_softplus, c=\'red\', label=\'softplus\')\nplt.ylim((-0.2, 6))\nplt.legend(loc=\'best\')\n\nplt.show()\n'"
tutorial-contents/301_regression.py,9,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.4\nmatplotlib\n""""""\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n# torch.manual_seed(1)    # reproducible\n\nx = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)\ny = x.pow(2) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)\n\n# torch can only train on Variable, so convert them to Variable\n# The code below is deprecated in Pytorch 0.4. Now, autograd directly supports tensors\n# x, y = Variable(x), Variable(y)\n\n# plt.scatter(x.data.numpy(), y.data.numpy())\n# plt.show()\n\n\nclass Net(torch.nn.Module):\n    def __init__(self, n_feature, n_hidden, n_output):\n        super(Net, self).__init__()\n        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n\n    def forward(self, x):\n        x = F.relu(self.hidden(x))      # activation function for hidden layer\n        x = self.predict(x)             # linear output\n        return x\n\nnet = Net(n_feature=1, n_hidden=10, n_output=1)     # define the network\nprint(net)  # net architecture\n\noptimizer = torch.optim.SGD(net.parameters(), lr=0.2)\nloss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n\nplt.ion()   # something about plotting\n\nfor t in range(200):\n    prediction = net(x)     # input x and predict based on x\n\n    loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n\n    optimizer.zero_grad()   # clear gradients for next train\n    loss.backward()         # backpropagation, compute gradients\n    optimizer.step()        # apply gradients\n\n    if t % 5 == 0:\n        # plot and show learning process\n        plt.cla()\n        plt.scatter(x.data.numpy(), y.data.numpy())\n        plt.plot(x.data.numpy(), prediction.data.numpy(), \'r-\', lw=5)\n        plt.text(0.5, 0, \'Loss=%.4f\' % loss.data.numpy(), fontdict={\'size\': 20, \'color\':  \'red\'})\n        plt.pause(0.1)\n\nplt.ioff()\nplt.show()\n'"
tutorial-contents/302_classification.py,15,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.4\nmatplotlib\n""""""\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n# torch.manual_seed(1)    # reproducible\n\n# make fake data\nn_data = torch.ones(100, 2)\nx0 = torch.normal(2*n_data, 1)      # class0 x data (tensor), shape=(100, 2)\ny0 = torch.zeros(100)               # class0 y data (tensor), shape=(100, 1)\nx1 = torch.normal(-2*n_data, 1)     # class1 x data (tensor), shape=(100, 2)\ny1 = torch.ones(100)                # class1 y data (tensor), shape=(100, 1)\nx = torch.cat((x0, x1), 0).type(torch.FloatTensor)  # shape (200, 2) FloatTensor = 32-bit floating\ny = torch.cat((y0, y1), ).type(torch.LongTensor)    # shape (200,) LongTensor = 64-bit integer\n\n# The code below is deprecated in Pytorch 0.4. Now, autograd directly supports tensors\n# x, y = Variable(x), Variable(y)\n\n# plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=y.data.numpy(), s=100, lw=0, cmap=\'RdYlGn\')\n# plt.show()\n\n\nclass Net(torch.nn.Module):\n    def __init__(self, n_feature, n_hidden, n_output):\n        super(Net, self).__init__()\n        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n        self.out = torch.nn.Linear(n_hidden, n_output)   # output layer\n\n    def forward(self, x):\n        x = F.relu(self.hidden(x))      # activation function for hidden layer\n        x = self.out(x)\n        return x\n\nnet = Net(n_feature=2, n_hidden=10, n_output=2)     # define the network\nprint(net)  # net architecture\n\noptimizer = torch.optim.SGD(net.parameters(), lr=0.02)\nloss_func = torch.nn.CrossEntropyLoss()  # the target label is NOT an one-hotted\n\nplt.ion()   # something about plotting\n\nfor t in range(100):\n    out = net(x)                 # input x and predict based on x\n    loss = loss_func(out, y)     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n\n    optimizer.zero_grad()   # clear gradients for next train\n    loss.backward()         # backpropagation, compute gradients\n    optimizer.step()        # apply gradients\n\n    if t % 2 == 0:\n        # plot and show learning process\n        plt.cla()\n        prediction = torch.max(out, 1)[1]\n        pred_y = prediction.data.numpy()\n        target_y = y.data.numpy()\n        plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap=\'RdYlGn\')\n        accuracy = float((pred_y == target_y).astype(int).sum()) / float(target_y.size)\n        plt.text(1.5, -4, \'Accuracy=%.2f\' % accuracy, fontdict={\'size\': 20, \'color\':  \'red\'})\n        plt.pause(0.1)\n\nplt.ioff()\nplt.show()\n'"
tutorial-contents/303_build_nn_quickly.py,8,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.1.11\n""""""\nimport torch\nimport torch.nn.functional as F\n\n\n# replace following class code with an easy sequential network\nclass Net(torch.nn.Module):\n    def __init__(self, n_feature, n_hidden, n_output):\n        super(Net, self).__init__()\n        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n\n    def forward(self, x):\n        x = F.relu(self.hidden(x))      # activation function for hidden layer\n        x = self.predict(x)             # linear output\n        return x\n\nnet1 = Net(1, 10, 1)\n\n# easy and fast way to build your network\nnet2 = torch.nn.Sequential(\n    torch.nn.Linear(1, 10),\n    torch.nn.ReLU(),\n    torch.nn.Linear(10, 1)\n)\n\n\nprint(net1)     # net1 architecture\n""""""\nNet (\n  (hidden): Linear (1 -> 10)\n  (predict): Linear (10 -> 1)\n)\n""""""\n\nprint(net2)     # net2 architecture\n""""""\nSequential (\n  (0): Linear (1 -> 10)\n  (1): ReLU ()\n  (2): Linear (10 -> 1)\n)\n""""""'"
tutorial-contents/304_save_reload.py,17,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.4\nmatplotlib\n""""""\nimport torch\nimport matplotlib.pyplot as plt\n\n# torch.manual_seed(1)    # reproducible\n\n# fake data\nx = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)\ny = x.pow(2) + 0.2*torch.rand(x.size())  # noisy y data (tensor), shape=(100, 1)\n\n# The code below is deprecated in Pytorch 0.4. Now, autograd directly supports tensors\n# x, y = Variable(x, requires_grad=False), Variable(y, requires_grad=False)\n\n\ndef save():\n    # save net1\n    net1 = torch.nn.Sequential(\n        torch.nn.Linear(1, 10),\n        torch.nn.ReLU(),\n        torch.nn.Linear(10, 1)\n    )\n    optimizer = torch.optim.SGD(net1.parameters(), lr=0.5)\n    loss_func = torch.nn.MSELoss()\n\n    for t in range(100):\n        prediction = net1(x)\n        loss = loss_func(prediction, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # plot result\n    plt.figure(1, figsize=(10, 3))\n    plt.subplot(131)\n    plt.title(\'Net1\')\n    plt.scatter(x.data.numpy(), y.data.numpy())\n    plt.plot(x.data.numpy(), prediction.data.numpy(), \'r-\', lw=5)\n\n    # 2 ways to save the net\n    torch.save(net1, \'net.pkl\')  # save entire net\n    torch.save(net1.state_dict(), \'net_params.pkl\')   # save only the parameters\n\n\ndef restore_net():\n    # restore entire net1 to net2\n    net2 = torch.load(\'net.pkl\')\n    prediction = net2(x)\n\n    # plot result\n    plt.subplot(132)\n    plt.title(\'Net2\')\n    plt.scatter(x.data.numpy(), y.data.numpy())\n    plt.plot(x.data.numpy(), prediction.data.numpy(), \'r-\', lw=5)\n\n\ndef restore_params():\n    # restore only the parameters in net1 to net3\n    net3 = torch.nn.Sequential(\n        torch.nn.Linear(1, 10),\n        torch.nn.ReLU(),\n        torch.nn.Linear(10, 1)\n    )\n\n    # copy net1\'s parameters into net3\n    net3.load_state_dict(torch.load(\'net_params.pkl\'))\n    prediction = net3(x)\n\n    # plot result\n    plt.subplot(133)\n    plt.title(\'Net3\')\n    plt.scatter(x.data.numpy(), y.data.numpy())\n    plt.plot(x.data.numpy(), prediction.data.numpy(), \'r-\', lw=5)\n    plt.show()\n\n# save net1\nsave()\n\n# restore entire net (may slow)\nrestore_net()\n\n# restore only the net parameters\nrestore_params()\n'"
tutorial-contents/305_batch_train.py,4,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.1.11\n""""""\nimport torch\nimport torch.utils.data as Data\n\ntorch.manual_seed(1)    # reproducible\n\nBATCH_SIZE = 5\n# BATCH_SIZE = 8\n\nx = torch.linspace(1, 10, 10)       # this is x data (torch tensor)\ny = torch.linspace(10, 1, 10)       # this is y data (torch tensor)\n\ntorch_dataset = Data.TensorDataset(x, y)\nloader = Data.DataLoader(\n    dataset=torch_dataset,      # torch TensorDataset format\n    batch_size=BATCH_SIZE,      # mini batch size\n    shuffle=True,               # random shuffle for training\n    num_workers=2,              # subprocesses for loading data\n)\n\n\ndef show_batch():\n    for epoch in range(3):   # train entire dataset 3 times\n        for step, (batch_x, batch_y) in enumerate(loader):  # for each training step\n            # train your data...\n            print(\'Epoch: \', epoch, \'| Step: \', step, \'| batch x: \',\n                  batch_x.numpy(), \'| batch y: \', batch_y.numpy())\n\n\nif __name__ == \'__main__\':\n    show_batch()\n\n'"
tutorial-contents/306_optimizer.py,13,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.4\nmatplotlib\n""""""\nimport torch\nimport torch.utils.data as Data\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\n# torch.manual_seed(1)    # reproducible\n\nLR = 0.01\nBATCH_SIZE = 32\nEPOCH = 12\n\n# fake dataset\nx = torch.unsqueeze(torch.linspace(-1, 1, 1000), dim=1)\ny = x.pow(2) + 0.1*torch.normal(torch.zeros(*x.size()))\n\n# plot dataset\nplt.scatter(x.numpy(), y.numpy())\nplt.show()\n\n# put dateset into torch dataset\ntorch_dataset = Data.TensorDataset(x, y)\nloader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)\n\n\n# default network\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.hidden = torch.nn.Linear(1, 20)   # hidden layer\n        self.predict = torch.nn.Linear(20, 1)   # output layer\n\n    def forward(self, x):\n        x = F.relu(self.hidden(x))      # activation function for hidden layer\n        x = self.predict(x)             # linear output\n        return x\n\nif __name__ == \'__main__\':\n    # different nets\n    net_SGD         = Net()\n    net_Momentum    = Net()\n    net_RMSprop     = Net()\n    net_Adam        = Net()\n    nets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]\n\n    # different optimizers\n    opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)\n    opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)\n    opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)\n    opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))\n    optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]\n\n    loss_func = torch.nn.MSELoss()\n    losses_his = [[], [], [], []]   # record loss\n\n    # training\n    for epoch in range(EPOCH):\n        print(\'Epoch: \', epoch)\n        for step, (b_x, b_y) in enumerate(loader):          # for each training step\n            for net, opt, l_his in zip(nets, optimizers, losses_his):\n                output = net(b_x)              # get output for every net\n                loss = loss_func(output, b_y)  # compute loss for every net\n                opt.zero_grad()                # clear gradients for next train\n                loss.backward()                # backpropagation, compute gradients\n                opt.step()                     # apply gradients\n                l_his.append(loss.data.numpy())     # loss recoder\n\n    labels = [\'SGD\', \'Momentum\', \'RMSprop\', \'Adam\']\n    for i, l_his in enumerate(losses_his):\n        plt.plot(l_his, label=labels[i])\n    plt.legend(loc=\'best\')\n    plt.xlabel(\'Steps\')\n    plt.ylabel(\'Loss\')\n    plt.ylim((0, 0.2))\n    plt.show()\n'"
tutorial-contents/401_CNN.py,8,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.4\ntorchvision\nmatplotlib\n""""""\n# library\n# standard library\nimport os\n\n# third-party library\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as Data\nimport torchvision\nimport matplotlib.pyplot as plt\n\n# torch.manual_seed(1)    # reproducible\n\n# Hyper Parameters\nEPOCH = 1               # train the training data n times, to save time, we just train 1 epoch\nBATCH_SIZE = 50\nLR = 0.001              # learning rate\nDOWNLOAD_MNIST = False\n\n\n# Mnist digits dataset\nif not(os.path.exists(\'./mnist/\')) or not os.listdir(\'./mnist/\'):\n    # not mnist dir or mnist is empyt dir\n    DOWNLOAD_MNIST = True\n\ntrain_data = torchvision.datasets.MNIST(\n    root=\'./mnist/\',\n    train=True,                                     # this is training data\n    transform=torchvision.transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to\n                                                    # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\n    download=DOWNLOAD_MNIST,\n)\n\n# plot one example\nprint(train_data.train_data.size())                 # (60000, 28, 28)\nprint(train_data.train_labels.size())               # (60000)\nplt.imshow(train_data.train_data[0].numpy(), cmap=\'gray\')\nplt.title(\'%i\' % train_data.train_labels[0])\nplt.show()\n\n# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)\ntrain_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n\n# pick 2000 samples to speed up testing\ntest_data = torchvision.datasets.MNIST(root=\'./mnist/\', train=False)\ntest_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)[:2000]/255.   # shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)\ntest_y = test_data.test_labels[:2000]\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Sequential(         # input shape (1, 28, 28)\n            nn.Conv2d(\n                in_channels=1,              # input height\n                out_channels=16,            # n_filters\n                kernel_size=5,              # filter size\n                stride=1,                   # filter movement/step\n                padding=2,                  # if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1\n            ),                              # output shape (16, 28, 28)\n            nn.ReLU(),                      # activation\n            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area, output shape (16, 14, 14)\n        )\n        self.conv2 = nn.Sequential(         # input shape (16, 14, 14)\n            nn.Conv2d(16, 32, 5, 1, 2),     # output shape (32, 14, 14)\n            nn.ReLU(),                      # activation\n            nn.MaxPool2d(2),                # output shape (32, 7, 7)\n        )\n        self.out = nn.Linear(32 * 7 * 7, 10)   # fully connected layer, output 10 classes\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = x.view(x.size(0), -1)           # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n        output = self.out(x)\n        return output, x    # return x for visualization\n\n\ncnn = CNN()\nprint(cnn)  # net architecture\n\noptimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\nloss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted\n\n# following function (plot_with_labels) is for visualization, can be ignored if not interested\nfrom matplotlib import cm\ntry: from sklearn.manifold import TSNE; HAS_SK = True\nexcept: HAS_SK = False; print(\'Please install sklearn for layer visualization\')\ndef plot_with_labels(lowDWeights, labels):\n    plt.cla()\n    X, Y = lowDWeights[:, 0], lowDWeights[:, 1]\n    for x, y, s in zip(X, Y, labels):\n        c = cm.rainbow(int(255 * s / 9)); plt.text(x, y, s, backgroundcolor=c, fontsize=9)\n    plt.xlim(X.min(), X.max()); plt.ylim(Y.min(), Y.max()); plt.title(\'Visualize last layer\'); plt.show(); plt.pause(0.01)\n\nplt.ion()\n# training and testing\nfor epoch in range(EPOCH):\n    for step, (b_x, b_y) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\n\n        output = cnn(b_x)[0]               # cnn output\n        loss = loss_func(output, b_y)   # cross entropy loss\n        optimizer.zero_grad()           # clear gradients for this training step\n        loss.backward()                 # backpropagation, compute gradients\n        optimizer.step()                # apply gradients\n\n        if step % 50 == 0:\n            test_output, last_layer = cnn(test_x)\n            pred_y = torch.max(test_output, 1)[1].data.numpy()\n            accuracy = float((pred_y == test_y.data.numpy()).astype(int).sum()) / float(test_y.size(0))\n            print(\'Epoch: \', epoch, \'| train loss: %.4f\' % loss.data.numpy(), \'| test accuracy: %.2f\' % accuracy)\n            if HAS_SK:\n                # Visualization of trained flatten layer (T-SNE)\n                tsne = TSNE(perplexity=30, n_components=2, init=\'pca\', n_iter=5000)\n                plot_only = 500\n                low_dim_embs = tsne.fit_transform(last_layer.data.numpy()[:plot_only, :])\n                labels = test_y.numpy()[:plot_only]\n                plot_with_labels(low_dim_embs, labels)\nplt.ioff()\n\n# print 10 predictions from test data\ntest_output, _ = cnn(test_x[:10])\npred_y = torch.max(test_output, 1)[1].data.numpy()\nprint(pred_y, \'prediction number\')\nprint(test_y[:10].numpy(), \'real number\')\n'"
tutorial-contents/402_RNN_classifier.py,7,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.4\nmatplotlib\ntorchvision\n""""""\nimport torch\nfrom torch import nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n\n# torch.manual_seed(1)    # reproducible\n\n# Hyper Parameters\nEPOCH = 1               # train the training data n times, to save time, we just train 1 epoch\nBATCH_SIZE = 64\nTIME_STEP = 28          # rnn time step / image height\nINPUT_SIZE = 28         # rnn input size / image width\nLR = 0.01               # learning rate\nDOWNLOAD_MNIST = True   # set to True if haven\'t download the data\n\n\n# Mnist digital dataset\ntrain_data = dsets.MNIST(\n    root=\'./mnist/\',\n    train=True,                         # this is training data\n    transform=transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to\n                                        # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\n    download=DOWNLOAD_MNIST,            # download it if you don\'t have it\n)\n\n# plot one example\nprint(train_data.train_data.size())     # (60000, 28, 28)\nprint(train_data.train_labels.size())   # (60000)\nplt.imshow(train_data.train_data[0].numpy(), cmap=\'gray\')\nplt.title(\'%i\' % train_data.train_labels[0])\nplt.show()\n\n# Data Loader for easy mini-batch return in training\ntrain_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n\n# convert test data into Variable, pick 2000 samples to speed up testing\ntest_data = dsets.MNIST(root=\'./mnist/\', train=False, transform=transforms.ToTensor())\ntest_x = test_data.test_data.type(torch.FloatTensor)[:2000]/255.   # shape (2000, 28, 28) value in range(0,1)\ntest_y = test_data.test_labels.numpy()[:2000]    # covert to numpy array\n\n\nclass RNN(nn.Module):\n    def __init__(self):\n        super(RNN, self).__init__()\n\n        self.rnn = nn.LSTM(         # if use nn.RNN(), it hardly learns\n            input_size=INPUT_SIZE,\n            hidden_size=64,         # rnn hidden unit\n            num_layers=1,           # number of rnn layer\n            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n        )\n\n        self.out = nn.Linear(64, 10)\n\n    def forward(self, x):\n        # x shape (batch, time_step, input_size)\n        # r_out shape (batch, time_step, output_size)\n        # h_n shape (n_layers, batch, hidden_size)\n        # h_c shape (n_layers, batch, hidden_size)\n        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n\n        # choose r_out at the last time step\n        out = self.out(r_out[:, -1, :])\n        return out\n\n\nrnn = RNN()\nprint(rnn)\n\noptimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters\nloss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted\n\n# training and testing\nfor epoch in range(EPOCH):\n    for step, (b_x, b_y) in enumerate(train_loader):        # gives batch data\n        b_x = b_x.view(-1, 28, 28)              # reshape x to (batch, time_step, input_size)\n\n        output = rnn(b_x)                               # rnn output\n        loss = loss_func(output, b_y)                   # cross entropy loss\n        optimizer.zero_grad()                           # clear gradients for this training step\n        loss.backward()                                 # backpropagation, compute gradients\n        optimizer.step()                                # apply gradients\n\n        if step % 50 == 0:\n            test_output = rnn(test_x)                   # (samples, time_step, input_size)\n            pred_y = torch.max(test_output, 1)[1].data.numpy()\n            accuracy = float((pred_y == test_y).astype(int).sum()) / float(test_y.size)\n            print(\'Epoch: \', epoch, \'| train loss: %.4f\' % loss.data.numpy(), \'| test accuracy: %.2f\' % accuracy)\n\n# print 10 predictions from test data\ntest_output = rnn(test_x[:10].view(-1, 28, 28))\npred_y = torch.max(test_output, 1)[1].data.numpy()\nprint(pred_y, \'prediction number\')\nprint(test_y[:10], \'real number\')\n\n'"
tutorial-contents/403_RNN_regressor.py,5,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.4\nmatplotlib\nnumpy\n""""""\nimport torch\nfrom torch import nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# torch.manual_seed(1)    # reproducible\n\n# Hyper Parameters\nTIME_STEP = 10      # rnn time step\nINPUT_SIZE = 1      # rnn input size\nLR = 0.02           # learning rate\n\n# show data\nsteps = np.linspace(0, np.pi*2, 100, dtype=np.float32)  # float32 for converting torch FloatTensor\nx_np = np.sin(steps)\ny_np = np.cos(steps)\nplt.plot(steps, y_np, \'r-\', label=\'target (cos)\')\nplt.plot(steps, x_np, \'b-\', label=\'input (sin)\')\nplt.legend(loc=\'best\')\nplt.show()\n\n\nclass RNN(nn.Module):\n    def __init__(self):\n        super(RNN, self).__init__()\n\n        self.rnn = nn.RNN(\n            input_size=INPUT_SIZE,\n            hidden_size=32,     # rnn hidden unit\n            num_layers=1,       # number of rnn layer\n            batch_first=True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n        )\n        self.out = nn.Linear(32, 1)\n\n    def forward(self, x, h_state):\n        # x (batch, time_step, input_size)\n        # h_state (n_layers, batch, hidden_size)\n        # r_out (batch, time_step, hidden_size)\n        r_out, h_state = self.rnn(x, h_state)\n\n        outs = []    # save all predictions\n        for time_step in range(r_out.size(1)):    # calculate output for each time step\n            outs.append(self.out(r_out[:, time_step, :]))\n        return torch.stack(outs, dim=1), h_state\n\n        # instead, for simplicity, you can replace above codes by follows\n        # r_out = r_out.view(-1, 32)\n        # outs = self.out(r_out)\n        # outs = outs.view(-1, TIME_STEP, 1)\n        # return outs, h_state\n        \n        # or even simpler, since nn.Linear can accept inputs of any dimension \n        # and returns outputs with same dimension except for the last\n        # outs = self.out(r_out)\n        # return outs\n\nrnn = RNN()\nprint(rnn)\n\noptimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters\nloss_func = nn.MSELoss()\n\nh_state = None      # for initial hidden state\n\nplt.figure(1, figsize=(12, 5))\nplt.ion()           # continuously plot\n\nfor step in range(100):\n    start, end = step * np.pi, (step+1)*np.pi   # time range\n    # use sin predicts cos\n    steps = np.linspace(start, end, TIME_STEP, dtype=np.float32, endpoint=False)  # float32 for converting torch FloatTensor\n    x_np = np.sin(steps)\n    y_np = np.cos(steps)\n\n    x = torch.from_numpy(x_np[np.newaxis, :, np.newaxis])    # shape (batch, time_step, input_size)\n    y = torch.from_numpy(y_np[np.newaxis, :, np.newaxis])\n\n    prediction, h_state = rnn(x, h_state)   # rnn output\n    # !! next step is important !!\n    h_state = h_state.data        # repack the hidden state, break the connection from last iteration\n\n    loss = loss_func(prediction, y)         # calculate loss\n    optimizer.zero_grad()                   # clear gradients for this training step\n    loss.backward()                         # backpropagation, compute gradients\n    optimizer.step()                        # apply gradients\n\n    # plotting\n    plt.plot(steps, y_np.flatten(), \'r-\')\n    plt.plot(steps, prediction.data.numpy().flatten(), \'b-\')\n    plt.draw(); plt.pause(0.05)\n\nplt.ioff()\nplt.show()\n'"
tutorial-contents/404_autoencoder.py,7,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.4\nmatplotlib\nnumpy\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as Data\nimport torchvision\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport numpy as np\n\n\n# torch.manual_seed(1)    # reproducible\n\n# Hyper Parameters\nEPOCH = 10\nBATCH_SIZE = 64\nLR = 0.005         # learning rate\nDOWNLOAD_MNIST = False\nN_TEST_IMG = 5\n\n# Mnist digits dataset\ntrain_data = torchvision.datasets.MNIST(\n    root=\'./mnist/\',\n    train=True,                                     # this is training data\n    transform=torchvision.transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to\n                                                    # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\n    download=DOWNLOAD_MNIST,                        # download it if you don\'t have it\n)\n\n# plot one example\nprint(train_data.train_data.size())     # (60000, 28, 28)\nprint(train_data.train_labels.size())   # (60000)\nplt.imshow(train_data.train_data[2].numpy(), cmap=\'gray\')\nplt.title(\'%i\' % train_data.train_labels[2])\nplt.show()\n\n# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)\ntrain_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n\n\nclass AutoEncoder(nn.Module):\n    def __init__(self):\n        super(AutoEncoder, self).__init__()\n\n        self.encoder = nn.Sequential(\n            nn.Linear(28*28, 128),\n            nn.Tanh(),\n            nn.Linear(128, 64),\n            nn.Tanh(),\n            nn.Linear(64, 12),\n            nn.Tanh(),\n            nn.Linear(12, 3),   # compress to 3 features which can be visualized in plt\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(3, 12),\n            nn.Tanh(),\n            nn.Linear(12, 64),\n            nn.Tanh(),\n            nn.Linear(64, 128),\n            nn.Tanh(),\n            nn.Linear(128, 28*28),\n            nn.Sigmoid(),       # compress to a range (0, 1)\n        )\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return encoded, decoded\n\n\nautoencoder = AutoEncoder()\n\noptimizer = torch.optim.Adam(autoencoder.parameters(), lr=LR)\nloss_func = nn.MSELoss()\n\n# initialize figure\nf, a = plt.subplots(2, N_TEST_IMG, figsize=(5, 2))\nplt.ion()   # continuously plot\n\n# original data (first row) for viewing\nview_data = train_data.train_data[:N_TEST_IMG].view(-1, 28*28).type(torch.FloatTensor)/255.\nfor i in range(N_TEST_IMG):\n    a[0][i].imshow(np.reshape(view_data.data.numpy()[i], (28, 28)), cmap=\'gray\'); a[0][i].set_xticks(()); a[0][i].set_yticks(())\n\nfor epoch in range(EPOCH):\n    for step, (x, b_label) in enumerate(train_loader):\n        b_x = x.view(-1, 28*28)   # batch x, shape (batch, 28*28)\n        b_y = x.view(-1, 28*28)   # batch y, shape (batch, 28*28)\n\n        encoded, decoded = autoencoder(b_x)\n\n        loss = loss_func(decoded, b_y)      # mean square error\n        optimizer.zero_grad()               # clear gradients for this training step\n        loss.backward()                     # backpropagation, compute gradients\n        optimizer.step()                    # apply gradients\n\n        if step % 100 == 0:\n            print(\'Epoch: \', epoch, \'| train loss: %.4f\' % loss.data.numpy())\n\n            # plotting decoded image (second row)\n            _, decoded_data = autoencoder(view_data)\n            for i in range(N_TEST_IMG):\n                a[1][i].clear()\n                a[1][i].imshow(np.reshape(decoded_data.data.numpy()[i], (28, 28)), cmap=\'gray\')\n                a[1][i].set_xticks(()); a[1][i].set_yticks(())\n            plt.draw(); plt.pause(0.05)\n\nplt.ioff()\nplt.show()\n\n# visualize in 3D plot\nview_data = train_data.train_data[:200].view(-1, 28*28).type(torch.FloatTensor)/255.\nencoded_data, _ = autoencoder(view_data)\nfig = plt.figure(2); ax = Axes3D(fig)\nX, Y, Z = encoded_data.data[:, 0].numpy(), encoded_data.data[:, 1].numpy(), encoded_data.data[:, 2].numpy()\nvalues = train_data.train_labels[:200].numpy()\nfor x, y, z, s in zip(X, Y, Z, values):\n    c = cm.rainbow(int(255*s/9)); ax.text(x, y, z, s, backgroundcolor=c)\nax.set_xlim(X.min(), X.max()); ax.set_ylim(Y.min(), Y.max()); ax.set_zlim(Z.min(), Z.max())\nplt.show()\n'"
tutorial-contents/405_DQN_Reinforcement_learning.py,9,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\nMore about Reinforcement learning: https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/\n\nDependencies:\ntorch: 0.4\ngym: 0.8.1\nnumpy\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport gym\n\n# Hyper Parameters\nBATCH_SIZE = 32\nLR = 0.01                   # learning rate\nEPSILON = 0.9               # greedy policy\nGAMMA = 0.9                 # reward discount\nTARGET_REPLACE_ITER = 100   # target update frequency\nMEMORY_CAPACITY = 2000\nenv = gym.make(\'CartPole-v0\')\nenv = env.unwrapped\nN_ACTIONS = env.action_space.n\nN_STATES = env.observation_space.shape[0]\nENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape     # to confirm the shape\n\n\nclass Net(nn.Module):\n    def __init__(self, ):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(N_STATES, 50)\n        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n        self.out = nn.Linear(50, N_ACTIONS)\n        self.out.weight.data.normal_(0, 0.1)   # initialization\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.relu(x)\n        actions_value = self.out(x)\n        return actions_value\n\n\nclass DQN(object):\n    def __init__(self):\n        self.eval_net, self.target_net = Net(), Net()\n\n        self.learn_step_counter = 0                                     # for target updating\n        self.memory_counter = 0                                         # for storing memory\n        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory\n        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n        self.loss_func = nn.MSELoss()\n\n    def choose_action(self, x):\n        x = torch.unsqueeze(torch.FloatTensor(x), 0)\n        # input only one sample\n        if np.random.uniform() < EPSILON:   # greedy\n            actions_value = self.eval_net.forward(x)\n            action = torch.max(actions_value, 1)[1].data.numpy()\n            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n        else:   # random\n            action = np.random.randint(0, N_ACTIONS)\n            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n        return action\n\n    def store_transition(self, s, a, r, s_):\n        transition = np.hstack((s, [a, r], s_))\n        # replace the old memory with new memory\n        index = self.memory_counter % MEMORY_CAPACITY\n        self.memory[index, :] = transition\n        self.memory_counter += 1\n\n    def learn(self):\n        # target parameter update\n        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n            self.target_net.load_state_dict(self.eval_net.state_dict())\n        self.learn_step_counter += 1\n\n        # sample batch transitions\n        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n        b_memory = self.memory[sample_index, :]\n        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n\n        # q_eval w.r.t the action in experience\n        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n        q_next = self.target_net(b_s_).detach()     # detach from graph, don\'t backpropagate\n        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n        loss = self.loss_func(q_eval, q_target)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\ndqn = DQN()\n\nprint(\'\\nCollecting experience...\')\nfor i_episode in range(400):\n    s = env.reset()\n    ep_r = 0\n    while True:\n        env.render()\n        a = dqn.choose_action(s)\n\n        # take action\n        s_, r, done, info = env.step(a)\n\n        # modify the reward\n        x, x_dot, theta, theta_dot = s_\n        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n        r = r1 + r2\n\n        dqn.store_transition(s, a, r, s_)\n\n        ep_r += r\n        if dqn.memory_counter > MEMORY_CAPACITY:\n            dqn.learn()\n            if done:\n                print(\'Ep: \', i_episode,\n                      \'| Ep_r: \', round(ep_r, 2))\n\n        if done:\n            break\n        s = s_'"
tutorial-contents/406_GAN.py,8,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.4\nnumpy\nmatplotlib\n""""""\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# torch.manual_seed(1)    # reproducible\n# np.random.seed(1)\n\n# Hyper Parameters\nBATCH_SIZE = 64\nLR_G = 0.0001           # learning rate for generator\nLR_D = 0.0001           # learning rate for discriminator\nN_IDEAS = 5             # think of this as number of ideas for generating an art work (Generator)\nART_COMPONENTS = 15     # it could be total point G can draw in the canvas\nPAINT_POINTS = np.vstack([np.linspace(-1, 1, ART_COMPONENTS) for _ in range(BATCH_SIZE)])\n\n# show our beautiful painting range\n# plt.plot(PAINT_POINTS[0], 2 * np.power(PAINT_POINTS[0], 2) + 1, c=\'#74BCFF\', lw=3, label=\'upper bound\')\n# plt.plot(PAINT_POINTS[0], 1 * np.power(PAINT_POINTS[0], 2) + 0, c=\'#FF9359\', lw=3, label=\'lower bound\')\n# plt.legend(loc=\'upper right\')\n# plt.show()\n\n\ndef artist_works():     # painting from the famous artist (real target)\n    a = np.random.uniform(1, 2, size=BATCH_SIZE)[:, np.newaxis]\n    paintings = a * np.power(PAINT_POINTS, 2) + (a-1)\n    paintings = torch.from_numpy(paintings).float()\n    return paintings\n\nG = nn.Sequential(                      # Generator\n    nn.Linear(N_IDEAS, 128),            # random ideas (could from normal distribution)\n    nn.ReLU(),\n    nn.Linear(128, ART_COMPONENTS),     # making a painting from these random ideas\n)\n\nD = nn.Sequential(                      # Discriminator\n    nn.Linear(ART_COMPONENTS, 128),     # receive art work either from the famous artist or a newbie like G\n    nn.ReLU(),\n    nn.Linear(128, 1),\n    nn.Sigmoid(),                       # tell the probability that the art work is made by artist\n)\n\nopt_D = torch.optim.Adam(D.parameters(), lr=LR_D)\nopt_G = torch.optim.Adam(G.parameters(), lr=LR_G)\n\nplt.ion()   # something about continuous plotting\n\nfor step in range(10000):\n    artist_paintings = artist_works()  # real painting from artist\n    G_ideas = torch.randn(BATCH_SIZE, N_IDEAS, requires_grad=True)  # random ideas\\n\n    G_paintings = G(G_ideas)                    # fake painting from G (random ideas)\n    prob_artist1 = D(G_paintings)               # D try to reduce this prob\n    G_loss = torch.mean(torch.log(1. - prob_artist1))  \n    opt_G.zero_grad()\n    G_loss.backward()\n    opt_G.step()\n     \n    prob_artist0 = D(artist_paintings)          # D try to increase this prob\n    prob_artist1 = D(G_paintings.detach())  # D try to reduce this prob\n    D_loss = - torch.mean(torch.log(prob_artist0) + torch.log(1. - prob_artist1))\n    opt_D.zero_grad()\n    D_loss.backward(retain_graph=True)      # reusing computational graph\n    opt_D.step()\n\n    if step % 50 == 0:  # plotting\n        plt.cla()\n        plt.plot(PAINT_POINTS[0], G_paintings.data.numpy()[0], c=\'#4AD631\', lw=3, label=\'Generated painting\',)\n        plt.plot(PAINT_POINTS[0], 2 * np.power(PAINT_POINTS[0], 2) + 1, c=\'#74BCFF\', lw=3, label=\'upper bound\')\n        plt.plot(PAINT_POINTS[0], 1 * np.power(PAINT_POINTS[0], 2) + 0, c=\'#FF9359\', lw=3, label=\'lower bound\')\n        plt.text(-.5, 2.3, \'D accuracy=%.2f (0.5 for D to converge)\' % prob_artist0.data.numpy().mean(), fontdict={\'size\': 13})\n        plt.text(-.5, 2, \'D score= %.2f (-1.38 for G to converge)\' % -D_loss.data.numpy(), fontdict={\'size\': 13})\n        plt.ylim((0, 3));plt.legend(loc=\'upper right\', fontsize=10);plt.draw();plt.pause(0.01)\n\nplt.ioff()\nplt.show()'"
tutorial-contents/406_conditional_GAN.py,17,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.4\nnumpy\nmatplotlib\n""""""\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# torch.manual_seed(1)    # reproducible\n# np.random.seed(1)\n\n# Hyper Parameters\nBATCH_SIZE = 64\nLR_G = 0.0001           # learning rate for generator\nLR_D = 0.0001           # learning rate for discriminator\nN_IDEAS = 5             # think of this as number of ideas for generating an art work (Generator)\nART_COMPONENTS = 15     # it could be total point G can draw in the canvas\nPAINT_POINTS = np.vstack([np.linspace(-1, 1, ART_COMPONENTS) for _ in range(BATCH_SIZE)])\n\n# show our beautiful painting range\n# plt.plot(PAINT_POINTS[0], 2 * np.power(PAINT_POINTS[0], 2) + 1, c=\'#74BCFF\', lw=3, label=\'upper bound\')\n# plt.plot(PAINT_POINTS[0], 1 * np.power(PAINT_POINTS[0], 2) + 0, c=\'#FF9359\', lw=3, label=\'lower bound\')\n# plt.legend(loc=\'upper right\')\n# plt.show()\n\n\ndef artist_works_with_labels():     # painting from the famous artist (real target)\n    a = np.random.uniform(1, 2, size=BATCH_SIZE)[:, np.newaxis]\n    paintings = a * np.power(PAINT_POINTS, 2) + (a-1)\n    labels = (a-1) > 0.5            # upper paintings (1), lower paintings (0), two classes\n    paintings = torch.from_numpy(paintings).float()\n    labels = torch.from_numpy(labels.astype(np.float32))\n    return paintings, labels\n\n\nG = nn.Sequential(                      # Generator\n    nn.Linear(N_IDEAS+1, 128),          # random ideas (could from normal distribution) + class label\n    nn.ReLU(),\n    nn.Linear(128, ART_COMPONENTS),     # making a painting from these random ideas\n)\n\nD = nn.Sequential(                      # Discriminator\n    nn.Linear(ART_COMPONENTS+1, 128),   # receive art work either from the famous artist or a newbie like G with label\n    nn.ReLU(),\n    nn.Linear(128, 1),\n    nn.Sigmoid(),                       # tell the probability that the art work is made by artist\n)\n\nopt_D = torch.optim.Adam(D.parameters(), lr=LR_D)\nopt_G = torch.optim.Adam(G.parameters(), lr=LR_G)\n\nplt.ion()   # something about continuous plotting\n\nfor step in range(10000):\n    artist_paintings, labels = artist_works_with_labels()           # real painting, label from artist\n    G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)                      # random ideas\n    G_inputs = torch.cat((G_ideas, labels), 1)                      # ideas with labels\n    G_paintings = G(G_inputs)                                       # fake painting w.r.t label from G\n\n    D_inputs0 = torch.cat((artist_paintings, labels), 1)            # all have their labels\n    D_inputs1 = torch.cat((G_paintings, labels), 1)\n    prob_artist0 = D(D_inputs0)                 # D try to increase this prob\n    prob_artist1 = D(D_inputs1)                 # D try to reduce this prob\n\n    D_score0 = torch.log(prob_artist0)          # maximise this for D\n    D_score1 = torch.log(1. - prob_artist1)     # maximise this for D\n    D_loss = - torch.mean(D_score0 + D_score1)  # minimise the negative of both two above for D\n    G_loss = torch.mean(D_score1)               # minimise D score w.r.t G\n\n    opt_D.zero_grad()\n    D_loss.backward(retain_graph=True)      # reusing computational graph\n    opt_D.step()\n\n    opt_G.zero_grad()\n    G_loss.backward()\n    opt_G.step()\n\n    if step % 200 == 0:  # plotting\n        plt.cla()\n        plt.plot(PAINT_POINTS[0], G_paintings.data.numpy()[0], c=\'#4AD631\', lw=3, label=\'Generated painting\',)\n        bound = [0, 0.5] if labels.data[0, 0] == 0 else [0.5, 1]\n        plt.plot(PAINT_POINTS[0], 2 * np.power(PAINT_POINTS[0], 2) + bound[1], c=\'#74BCFF\', lw=3, label=\'upper bound\')\n        plt.plot(PAINT_POINTS[0], 1 * np.power(PAINT_POINTS[0], 2) + bound[0], c=\'#FF9359\', lw=3, label=\'lower bound\')\n        plt.text(-.5, 2.3, \'D accuracy=%.2f (0.5 for D to converge)\' % prob_artist0.data.numpy().mean(), fontdict={\'size\': 13})\n        plt.text(-.5, 2, \'D score= %.2f (-1.38 for G to converge)\' % -D_loss.data.numpy(), fontdict={\'size\': 13})\n        plt.text(-.5, 1.7, \'Class = %i\' % int(labels.data[0, 0]), fontdict={\'size\': 13})\n        plt.ylim((0, 3));plt.legend(loc=\'upper right\', fontsize=10);plt.draw();plt.pause(0.1)\n\nplt.ioff()\nplt.show()\n\n# plot a generated painting for upper class\nz = torch.randn(1, N_IDEAS)\nlabel = torch.FloatTensor([[1.]])     # for upper class\nG_inputs = torch.cat((z, label), 1)\nG_paintings = G(G_inputs)\nplt.plot(PAINT_POINTS[0], G_paintings.data.numpy()[0], c=\'#4AD631\', lw=3, label=\'G painting for upper class\',)\nplt.plot(PAINT_POINTS[0], 2 * np.power(PAINT_POINTS[0], 2) + bound[1], c=\'#74BCFF\', lw=3, label=\'upper bound (class 1)\')\nplt.plot(PAINT_POINTS[0], 1 * np.power(PAINT_POINTS[0], 2) + bound[0], c=\'#FF9359\', lw=3, label=\'lower bound (class 1)\')\nplt.ylim((0, 3));plt.legend(loc=\'upper right\', fontsize=10);plt.show()'"
tutorial-contents/501_why_torch_dynamic_graph.py,5,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.4\nmatplotlib\nnumpy\n""""""\nimport torch\nfrom torch import nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# torch.manual_seed(1)    # reproducible\n\n# Hyper Parameters\nINPUT_SIZE = 1          # rnn input size / image width\nLR = 0.02               # learning rate\n\n\nclass RNN(nn.Module):\n    def __init__(self):\n        super(RNN, self).__init__()\n\n        self.rnn = nn.RNN(\n            input_size=1,\n            hidden_size=32,     # rnn hidden unit\n            num_layers=1,       # number of rnn layer\n            batch_first=True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n        )\n        self.out = nn.Linear(32, 1)\n\n    def forward(self, x, h_state):\n        # x (batch, time_step, input_size)\n        # h_state (n_layers, batch, hidden_size)\n        # r_out (batch, time_step, output_size)\n        r_out, h_state = self.rnn(x, h_state)\n\n        outs = []                                   # this is where you can find torch is dynamic\n        for time_step in range(r_out.size(1)):      # calculate output for each time step\n            outs.append(self.out(r_out[:, time_step, :]))\n        return torch.stack(outs, dim=1), h_state\n\n\nrnn = RNN()\nprint(rnn)\n\noptimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters\nloss_func = nn.MSELoss()                                # the target label is not one-hotted\n\nh_state = None   # for initial hidden state\n\nplt.figure(1, figsize=(12, 5))\nplt.ion()   # continuously plot\n\n########################  Below is different #########################\n\n################ static time steps ##########\n# for step in range(60):\n#     start, end = step * np.pi, (step+1)*np.pi   # time steps\n#     # use sin predicts cos\n#     steps = np.linspace(start, end, 10, dtype=np.float32)\n\n################ dynamic time steps #########\nstep = 0\nfor i in range(60):\n    dynamic_steps = np.random.randint(1, 4)  # has random time steps\n    start, end = step * np.pi, (step + dynamic_steps) * np.pi  # different time steps length\n    step += dynamic_steps\n\n    # use sin predicts cos\n    steps = np.linspace(start, end, 10 * dynamic_steps, dtype=np.float32)\n\n#######################  Above is different ###########################\n\n    print(len(steps))       # print how many time step feed to RNN\n\n    x_np = np.sin(steps)    # float32 for converting torch FloatTensor\n    y_np = np.cos(steps)\n\n    x = torch.from_numpy(x_np[np.newaxis, :, np.newaxis])    # shape (batch, time_step, input_size)\n    y = torch.from_numpy(y_np[np.newaxis, :, np.newaxis])\n\n    prediction, h_state = rnn(x, h_state)   # rnn output\n    # !! next step is important !!\n    h_state = h_state.data        # repack the hidden state, break the connection from last iteration\n\n    loss = loss_func(prediction, y)         # cross entropy loss\n    optimizer.zero_grad()                   # clear gradients for this training step\n    loss.backward()                         # backpropagation, compute gradients\n    optimizer.step()                        # apply gradients\n\n    # plotting\n    plt.plot(steps, y_np.flatten(), \'r-\')\n    plt.plot(steps, prediction.data.numpy().flatten(), \'b-\')\n    plt.draw()\n    plt.pause(0.05)\n\nplt.ioff()\nplt.show()\n'"
tutorial-contents/502_GPU.py,8,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.4\ntorchvision\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as Data\nimport torchvision\n\n# torch.manual_seed(1)\n\nEPOCH = 1\nBATCH_SIZE = 50\nLR = 0.001\nDOWNLOAD_MNIST = False\n\ntrain_data = torchvision.datasets.MNIST(root=\'./mnist/\', train=True, transform=torchvision.transforms.ToTensor(), download=DOWNLOAD_MNIST,)\ntrain_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n\ntest_data = torchvision.datasets.MNIST(root=\'./mnist/\', train=False)\n\n# !!!!!!!! Change in here !!!!!!!!! #\ntest_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)[:2000].cuda()/255.   # Tensor on GPU\ntest_y = test_data.test_labels[:2000].cuda()\n\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2,),\n                                   nn.ReLU(), nn.MaxPool2d(kernel_size=2),)\n        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, 5, 1, 2), nn.ReLU(), nn.MaxPool2d(2),)\n        self.out = nn.Linear(32 * 7 * 7, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = x.view(x.size(0), -1)\n        output = self.out(x)\n        return output\n\ncnn = CNN()\n\n# !!!!!!!! Change in here !!!!!!!!! #\ncnn.cuda()      # Moves all model parameters and buffers to the GPU.\n\noptimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\nloss_func = nn.CrossEntropyLoss()\n\nfor epoch in range(EPOCH):\n    for step, (x, y) in enumerate(train_loader):\n\n        # !!!!!!!! Change in here !!!!!!!!! #\n        b_x = x.cuda()    # Tensor on GPU\n        b_y = y.cuda()    # Tensor on GPU\n\n        output = cnn(b_x)\n        loss = loss_func(output, b_y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if step % 50 == 0:\n            test_output = cnn(test_x)\n\n            # !!!!!!!! Change in here !!!!!!!!! #\n            pred_y = torch.max(test_output, 1)[1].cuda().data  # move the computation in GPU\n\n            accuracy = torch.sum(pred_y == test_y).type(torch.FloatTensor) / test_y.size(0)\n            print(\'Epoch: \', epoch, \'| train loss: %.4f\' % loss.data.cpu().numpy(), \'| test accuracy: %.2f\' % accuracy)\n\n\ntest_output = cnn(test_x[:10])\n\n# !!!!!!!! Change in here !!!!!!!!! #\npred_y = torch.max(test_output, 1)[1].cuda().data # move the computation in GPU\n\nprint(pred_y, \'prediction number\')\nprint(test_y[:10], \'real number\')\n'"
tutorial-contents/503_dropout.py,22,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.4\nmatplotlib\n""""""\nimport torch\nimport matplotlib.pyplot as plt\n\n# torch.manual_seed(1)    # reproducible\n\nN_SAMPLES = 20\nN_HIDDEN = 300\n\n# training data\nx = torch.unsqueeze(torch.linspace(-1, 1, N_SAMPLES), 1)\ny = x + 0.3*torch.normal(torch.zeros(N_SAMPLES, 1), torch.ones(N_SAMPLES, 1))\n\n# test data\ntest_x = torch.unsqueeze(torch.linspace(-1, 1, N_SAMPLES), 1)\ntest_y = test_x + 0.3*torch.normal(torch.zeros(N_SAMPLES, 1), torch.ones(N_SAMPLES, 1))\n\n# show data\nplt.scatter(x.data.numpy(), y.data.numpy(), c=\'magenta\', s=50, alpha=0.5, label=\'train\')\nplt.scatter(test_x.data.numpy(), test_y.data.numpy(), c=\'cyan\', s=50, alpha=0.5, label=\'test\')\nplt.legend(loc=\'upper left\')\nplt.ylim((-2.5, 2.5))\nplt.show()\n\nnet_overfitting = torch.nn.Sequential(\n    torch.nn.Linear(1, N_HIDDEN),\n    torch.nn.ReLU(),\n    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n    torch.nn.ReLU(),\n    torch.nn.Linear(N_HIDDEN, 1),\n)\n\nnet_dropped = torch.nn.Sequential(\n    torch.nn.Linear(1, N_HIDDEN),\n    torch.nn.Dropout(0.5),  # drop 50% of the neuron\n    torch.nn.ReLU(),\n    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n    torch.nn.Dropout(0.5),  # drop 50% of the neuron\n    torch.nn.ReLU(),\n    torch.nn.Linear(N_HIDDEN, 1),\n)\n\nprint(net_overfitting)  # net architecture\nprint(net_dropped)\n\noptimizer_ofit = torch.optim.Adam(net_overfitting.parameters(), lr=0.01)\noptimizer_drop = torch.optim.Adam(net_dropped.parameters(), lr=0.01)\nloss_func = torch.nn.MSELoss()\n\nplt.ion()   # something about plotting\n\nfor t in range(500):\n    pred_ofit = net_overfitting(x)\n    pred_drop = net_dropped(x)\n    loss_ofit = loss_func(pred_ofit, y)\n    loss_drop = loss_func(pred_drop, y)\n\n    optimizer_ofit.zero_grad()\n    optimizer_drop.zero_grad()\n    loss_ofit.backward()\n    loss_drop.backward()\n    optimizer_ofit.step()\n    optimizer_drop.step()\n\n    if t % 10 == 0:\n        # change to eval mode in order to fix drop out effect\n        net_overfitting.eval()\n        net_dropped.eval()  # parameters for dropout differ from train mode\n\n        # plotting\n        plt.cla()\n        test_pred_ofit = net_overfitting(test_x)\n        test_pred_drop = net_dropped(test_x)\n        plt.scatter(x.data.numpy(), y.data.numpy(), c=\'magenta\', s=50, alpha=0.3, label=\'train\')\n        plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c=\'cyan\', s=50, alpha=0.3, label=\'test\')\n        plt.plot(test_x.data.numpy(), test_pred_ofit.data.numpy(), \'r-\', lw=3, label=\'overfitting\')\n        plt.plot(test_x.data.numpy(), test_pred_drop.data.numpy(), \'b--\', lw=3, label=\'dropout(50%)\')\n        plt.text(0, -1.2, \'overfitting loss=%.4f\' % loss_func(test_pred_ofit, test_y).data.numpy(), fontdict={\'size\': 20, \'color\':  \'red\'})\n        plt.text(0, -1.5, \'dropout loss=%.4f\' % loss_func(test_pred_drop, test_y).data.numpy(), fontdict={\'size\': 20, \'color\': \'blue\'})\n        plt.legend(loc=\'upper left\'); plt.ylim((-2.5, 2.5));plt.pause(0.1)\n\n        # change back to train mode\n        net_overfitting.train()\n        net_dropped.train()\n\nplt.ioff()\nplt.show()'"
tutorial-contents/504_batch_normalization.py,9,"b'""""""\nView more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\nMy Youtube Channel: https://www.youtube.com/user/MorvanZhou\n\nDependencies:\ntorch: 0.4\nmatplotlib\nnumpy\n""""""\nimport torch\nfrom torch import nn\nfrom torch.nn import init\nimport torch.utils.data as Data\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# torch.manual_seed(1)    # reproducible\n# np.random.seed(1)\n\n# Hyper parameters\nN_SAMPLES = 2000\nBATCH_SIZE = 64\nEPOCH = 12\nLR = 0.03\nN_HIDDEN = 8\nACTIVATION = torch.tanh\nB_INIT = -0.2   # use a bad bias constant initializer\n\n# training data\nx = np.linspace(-7, 10, N_SAMPLES)[:, np.newaxis]\nnoise = np.random.normal(0, 2, x.shape)\ny = np.square(x) - 5 + noise\n\n# test data\ntest_x = np.linspace(-7, 10, 200)[:, np.newaxis]\nnoise = np.random.normal(0, 2, test_x.shape)\ntest_y = np.square(test_x) - 5 + noise\n\ntrain_x, train_y = torch.from_numpy(x).float(), torch.from_numpy(y).float()\ntest_x = torch.from_numpy(test_x).float()\ntest_y = torch.from_numpy(test_y).float()\n\ntrain_dataset = Data.TensorDataset(train_x, train_y)\ntrain_loader = Data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)\n\n# show data\nplt.scatter(train_x.numpy(), train_y.numpy(), c=\'#FF9359\', s=50, alpha=0.2, label=\'train\')\nplt.legend(loc=\'upper left\')\n\n\nclass Net(nn.Module):\n    def __init__(self, batch_normalization=False):\n        super(Net, self).__init__()\n        self.do_bn = batch_normalization\n        self.fcs = []\n        self.bns = []\n        self.bn_input = nn.BatchNorm1d(1, momentum=0.5)   # for input data\n\n        for i in range(N_HIDDEN):               # build hidden layers and BN layers\n            input_size = 1 if i == 0 else 10\n            fc = nn.Linear(input_size, 10)\n            setattr(self, \'fc%i\' % i, fc)       # IMPORTANT set layer to the Module\n            self._set_init(fc)                  # parameters initialization\n            self.fcs.append(fc)\n            if self.do_bn:\n                bn = nn.BatchNorm1d(10, momentum=0.5)\n                setattr(self, \'bn%i\' % i, bn)   # IMPORTANT set layer to the Module\n                self.bns.append(bn)\n\n        self.predict = nn.Linear(10, 1)         # output layer\n        self._set_init(self.predict)            # parameters initialization\n\n    def _set_init(self, layer):\n        init.normal_(layer.weight, mean=0., std=.1)\n        init.constant_(layer.bias, B_INIT)\n\n    def forward(self, x):\n        pre_activation = [x]\n        if self.do_bn: x = self.bn_input(x)     # input batch normalization\n        layer_input = [x]\n        for i in range(N_HIDDEN):\n            x = self.fcs[i](x)\n            pre_activation.append(x)\n            if self.do_bn: x = self.bns[i](x)   # batch normalization\n            x = ACTIVATION(x)\n            layer_input.append(x)\n        out = self.predict(x)\n        return out, layer_input, pre_activation\n\nnets = [Net(batch_normalization=False), Net(batch_normalization=True)]\n\n# print(*nets)    # print net architecture\n\nopts = [torch.optim.Adam(net.parameters(), lr=LR) for net in nets]\n\nloss_func = torch.nn.MSELoss()\n\n\ndef plot_histogram(l_in, l_in_bn, pre_ac, pre_ac_bn):\n    for i, (ax_pa, ax_pa_bn, ax, ax_bn) in enumerate(zip(axs[0, :], axs[1, :], axs[2, :], axs[3, :])):\n        [a.clear() for a in [ax_pa, ax_pa_bn, ax, ax_bn]]\n        if i == 0:\n            p_range = (-7, 10);the_range = (-7, 10)\n        else:\n            p_range = (-4, 4);the_range = (-1, 1)\n        ax_pa.set_title(\'L\' + str(i))\n        ax_pa.hist(pre_ac[i].data.numpy().ravel(), bins=10, range=p_range, color=\'#FF9359\', alpha=0.5);ax_pa_bn.hist(pre_ac_bn[i].data.numpy().ravel(), bins=10, range=p_range, color=\'#74BCFF\', alpha=0.5)\n        ax.hist(l_in[i].data.numpy().ravel(), bins=10, range=the_range, color=\'#FF9359\');ax_bn.hist(l_in_bn[i].data.numpy().ravel(), bins=10, range=the_range, color=\'#74BCFF\')\n        for a in [ax_pa, ax, ax_pa_bn, ax_bn]: a.set_yticks(());a.set_xticks(())\n        ax_pa_bn.set_xticks(p_range);ax_bn.set_xticks(the_range)\n        axs[0, 0].set_ylabel(\'PreAct\');axs[1, 0].set_ylabel(\'BN PreAct\');axs[2, 0].set_ylabel(\'Act\');axs[3, 0].set_ylabel(\'BN Act\')\n    plt.pause(0.01)\n\n\nif __name__ == ""__main__"":\n    f, axs = plt.subplots(4, N_HIDDEN + 1, figsize=(10, 5))\n    plt.ion()  # something about plotting\n    plt.show()\n\n    # training\n    losses = [[], []]  # recode loss for two networks\n\n    for epoch in range(EPOCH):\n        print(\'Epoch: \', epoch)\n        layer_inputs, pre_acts = [], []\n        for net, l in zip(nets, losses):\n            net.eval()              # set eval mode to fix moving_mean and moving_var\n            pred, layer_input, pre_act = net(test_x)\n            l.append(loss_func(pred, test_y).data.item())\n            layer_inputs.append(layer_input)\n            pre_acts.append(pre_act)\n            net.train()             # free moving_mean and moving_var\n        plot_histogram(*layer_inputs, *pre_acts)     # plot histogram\n\n        for step, (b_x, b_y) in enumerate(train_loader):\n            for net, opt in zip(nets, opts):     # train for each network\n                pred, _, _ = net(b_x)\n                loss = loss_func(pred, b_y)\n                opt.zero_grad()\n                loss.backward()\n                opt.step()    # it will also learns the parameters in Batch Normalization\n\n    plt.ioff()\n\n    # plot training loss\n    plt.figure(2)\n    plt.plot(losses[0], c=\'#FF9359\', lw=3, label=\'Original\')\n    plt.plot(losses[1], c=\'#74BCFF\', lw=3, label=\'Batch Normalization\')\n    plt.xlabel(\'step\');plt.ylabel(\'test loss\');plt.ylim((0, 2000));plt.legend(loc=\'best\')\n\n    # evaluation\n    # set net to eval mode to freeze the parameters in batch normalization layers\n    [net.eval() for net in nets]    # set eval mode to fix moving_mean and moving_var\n    preds = [net(test_x)[0] for net in nets]\n    plt.figure(3)\n    plt.plot(test_x.data.numpy(), preds[0].data.numpy(), c=\'#FF9359\', lw=4, label=\'Original\')\n    plt.plot(test_x.data.numpy(), preds[1].data.numpy(), c=\'#74BCFF\', lw=4, label=\'Batch Normalization\')\n    plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c=\'r\', s=50, alpha=0.2, label=\'train\')\n    plt.legend(loc=\'best\')\n    plt.show()\n'"
