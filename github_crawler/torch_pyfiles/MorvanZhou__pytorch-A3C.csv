file_path,api_count,code
continuous_A3C.py,5,"b'""""""\nReinforcement Learning (A3C) using Pytroch + multiprocessing.\nThe most simple implementation for continuous action.\n\nView more on my Chinese tutorial page [\xe8\x8e\xab\xe7\x83\xa6Python](https://morvanzhou.github.io/).\n""""""\n\nimport torch\nimport torch.nn as nn\nfrom utils import v_wrap, set_init, push_and_pull, record\nimport torch.nn.functional as F\nimport torch.multiprocessing as mp\nfrom shared_adam import SharedAdam\nimport gym\nimport math, os\nos.environ[""OMP_NUM_THREADS""] = ""1""\n\nUPDATE_GLOBAL_ITER = 5\nGAMMA = 0.9\nMAX_EP = 3000\nMAX_EP_STEP = 200\n\nenv = gym.make(\'Pendulum-v0\')\nN_S = env.observation_space.shape[0]\nN_A = env.action_space.shape[0]\n\n\nclass Net(nn.Module):\n    def __init__(self, s_dim, a_dim):\n        super(Net, self).__init__()\n        self.s_dim = s_dim\n        self.a_dim = a_dim\n        self.a1 = nn.Linear(s_dim, 200)\n        self.mu = nn.Linear(200, a_dim)\n        self.sigma = nn.Linear(200, a_dim)\n        self.c1 = nn.Linear(s_dim, 100)\n        self.v = nn.Linear(100, 1)\n        set_init([self.a1, self.mu, self.sigma, self.c1, self.v])\n        self.distribution = torch.distributions.Normal\n\n    def forward(self, x):\n        a1 = F.relu6(self.a1(x))\n        mu = 2 * F.tanh(self.mu(a1))\n        sigma = F.softplus(self.sigma(a1)) + 0.001      # avoid 0\n        c1 = F.relu6(self.c1(x))\n        values = self.v(c1)\n        return mu, sigma, values\n\n    def choose_action(self, s):\n        self.training = False\n        mu, sigma, _ = self.forward(s)\n        m = self.distribution(mu.view(1, ).data, sigma.view(1, ).data)\n        return m.sample().numpy()\n\n    def loss_func(self, s, a, v_t):\n        self.train()\n        mu, sigma, values = self.forward(s)\n        td = v_t - values\n        c_loss = td.pow(2)\n\n        m = self.distribution(mu, sigma)\n        log_prob = m.log_prob(a)\n        entropy = 0.5 + 0.5 * math.log(2 * math.pi) + torch.log(m.scale)  # exploration\n        exp_v = log_prob * td.detach() + 0.005 * entropy\n        a_loss = -exp_v\n        total_loss = (a_loss + c_loss).mean()\n        return total_loss\n\n\nclass Worker(mp.Process):\n    def __init__(self, gnet, opt, global_ep, global_ep_r, res_queue, name):\n        super(Worker, self).__init__()\n        self.name = \'w%i\' % name\n        self.g_ep, self.g_ep_r, self.res_queue = global_ep, global_ep_r, res_queue\n        self.gnet, self.opt = gnet, opt\n        self.lnet = Net(N_S, N_A)           # local network\n        self.env = gym.make(\'Pendulum-v0\').unwrapped\n\n    def run(self):\n        total_step = 1\n        while self.g_ep.value < MAX_EP:\n            s = self.env.reset()\n            buffer_s, buffer_a, buffer_r = [], [], []\n            ep_r = 0.\n            for t in range(MAX_EP_STEP):\n                if self.name == \'w0\':\n                    self.env.render()\n                a = self.lnet.choose_action(v_wrap(s[None, :]))\n                s_, r, done, _ = self.env.step(a.clip(-2, 2))\n                if t == MAX_EP_STEP - 1:\n                    done = True\n                ep_r += r\n                buffer_a.append(a)\n                buffer_s.append(s)\n                buffer_r.append((r+8.1)/8.1)    # normalize\n\n                if total_step % UPDATE_GLOBAL_ITER == 0 or done:  # update global and assign to local net\n                    # sync\n                    push_and_pull(self.opt, self.lnet, self.gnet, done, s_, buffer_s, buffer_a, buffer_r, GAMMA)\n                    buffer_s, buffer_a, buffer_r = [], [], []\n\n                    if done:  # done and print information\n                        record(self.g_ep, self.g_ep_r, ep_r, self.res_queue, self.name)\n                        break\n                s = s_\n                total_step += 1\n\n        self.res_queue.put(None)\n\n\nif __name__ == ""__main__"":\n    gnet = Net(N_S, N_A)        # global network\n    gnet.share_memory()         # share the global parameters in multiprocessing\n    opt = SharedAdam(gnet.parameters(), lr=1e-4, betas=(0.95, 0.999))  # global optimizer\n    global_ep, global_ep_r, res_queue = mp.Value(\'i\', 0), mp.Value(\'d\', 0.), mp.Queue()\n\n    # parallel training\n    workers = [Worker(gnet, opt, global_ep, global_ep_r, res_queue, i) for i in range(mp.cpu_count())]\n    [w.start() for w in workers]\n    res = []                    # record episode reward to plot\n    while True:\n        r = res_queue.get()\n        if r is not None:\n            res.append(r)\n        else:\n            break\n    [w.join() for w in workers]\n\n    import matplotlib.pyplot as plt\n    plt.plot(res)\n    plt.ylabel(\'Moving average ep reward\')\n    plt.xlabel(\'Step\')\n    plt.show()\n'"
discrete_A3C.py,6,"b'""""""\nReinforcement Learning (A3C) using Pytroch + multiprocessing.\nThe most simple implementation for continuous action.\n\nView more on my Chinese tutorial page [\xe8\x8e\xab\xe7\x83\xa6Python](https://morvanzhou.github.io/).\n""""""\n\nimport torch\nimport torch.nn as nn\nfrom utils import v_wrap, set_init, push_and_pull, record\nimport torch.nn.functional as F\nimport torch.multiprocessing as mp\nfrom shared_adam import SharedAdam\nimport gym\nimport os\nos.environ[""OMP_NUM_THREADS""] = ""1""\n\nUPDATE_GLOBAL_ITER = 5\nGAMMA = 0.9\nMAX_EP = 3000\n\nenv = gym.make(\'CartPole-v0\')\nN_S = env.observation_space.shape[0]\nN_A = env.action_space.n\n\n\nclass Net(nn.Module):\n    def __init__(self, s_dim, a_dim):\n        super(Net, self).__init__()\n        self.s_dim = s_dim\n        self.a_dim = a_dim\n        self.pi1 = nn.Linear(s_dim, 128)\n        self.pi2 = nn.Linear(128, a_dim)\n        self.v1 = nn.Linear(s_dim, 128)\n        self.v2 = nn.Linear(128, 1)\n        set_init([self.pi1, self.pi2, self.v1, self.v2])\n        self.distribution = torch.distributions.Categorical\n\n    def forward(self, x):\n        pi1 = torch.tanh(self.pi1(x))\n        logits = self.pi2(pi1)\n        v1 = torch.tanh(self.v1(x))\n        values = self.v2(v1)\n        return logits, values\n\n    def choose_action(self, s):\n        self.eval()\n        logits, _ = self.forward(s)\n        prob = F.softmax(logits, dim=1).data\n        m = self.distribution(prob)\n        return m.sample().numpy()[0]\n\n    def loss_func(self, s, a, v_t):\n        self.train()\n        logits, values = self.forward(s)\n        td = v_t - values\n        c_loss = td.pow(2)\n        \n        probs = F.softmax(logits, dim=1)\n        m = self.distribution(probs)\n        exp_v = m.log_prob(a) * td.detach().squeeze()\n        a_loss = -exp_v\n        total_loss = (c_loss + a_loss).mean()\n        return total_loss\n\n\nclass Worker(mp.Process):\n    def __init__(self, gnet, opt, global_ep, global_ep_r, res_queue, name):\n        super(Worker, self).__init__()\n        self.name = \'w%02i\' % name\n        self.g_ep, self.g_ep_r, self.res_queue = global_ep, global_ep_r, res_queue\n        self.gnet, self.opt = gnet, opt\n        self.lnet = Net(N_S, N_A)           # local network\n        self.env = gym.make(\'CartPole-v0\').unwrapped\n\n    def run(self):\n        total_step = 1\n        while self.g_ep.value < MAX_EP:\n            s = self.env.reset()\n            buffer_s, buffer_a, buffer_r = [], [], []\n            ep_r = 0.\n            while True:\n                if self.name == \'w00\':\n                    self.env.render()\n                a = self.lnet.choose_action(v_wrap(s[None, :]))\n                s_, r, done, _ = self.env.step(a)\n                if done: r = -1\n                ep_r += r\n                buffer_a.append(a)\n                buffer_s.append(s)\n                buffer_r.append(r)\n\n                if total_step % UPDATE_GLOBAL_ITER == 0 or done:  # update global and assign to local net\n                    # sync\n                    push_and_pull(self.opt, self.lnet, self.gnet, done, s_, buffer_s, buffer_a, buffer_r, GAMMA)\n                    buffer_s, buffer_a, buffer_r = [], [], []\n\n                    if done:  # done and print information\n                        record(self.g_ep, self.g_ep_r, ep_r, self.res_queue, self.name)\n                        break\n                s = s_\n                total_step += 1\n        self.res_queue.put(None)\n\n\nif __name__ == ""__main__"":\n    gnet = Net(N_S, N_A)        # global network\n    gnet.share_memory()         # share the global parameters in multiprocessing\n    opt = SharedAdam(gnet.parameters(), lr=1e-4, betas=(0.92, 0.999))      # global optimizer\n    global_ep, global_ep_r, res_queue = mp.Value(\'i\', 0), mp.Value(\'d\', 0.), mp.Queue()\n\n    # parallel training\n    workers = [Worker(gnet, opt, global_ep, global_ep_r, res_queue, i) for i in range(mp.cpu_count())]\n    [w.start() for w in workers]\n    res = []                    # record episode reward to plot\n    while True:\n        r = res_queue.get()\n        if r is not None:\n            res.append(r)\n        else:\n            break\n    [w.join() for w in workers]\n\n    import matplotlib.pyplot as plt\n    plt.plot(res)\n    plt.ylabel(\'Moving average ep reward\')\n    plt.xlabel(\'Step\')\n    plt.show()\n'"
shared_adam.py,3,"b'""""""\nShared optimizer, the parameters in the optimizer will shared in the multiprocessors.\n""""""\n\nimport torch\n\n\nclass SharedAdam(torch.optim.Adam):\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), eps=1e-8,\n                 weight_decay=0):\n        super(SharedAdam, self).__init__(params, lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        # State initialization\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                state[\'step\'] = 0\n                state[\'exp_avg\'] = torch.zeros_like(p.data)\n                state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                # share in memory\n                state[\'exp_avg\'].share_memory_()\n                state[\'exp_avg_sq\'].share_memory_()\n\n'"
utils.py,1,"b'""""""\nFunctions that use multiple times\n""""""\n\nfrom torch import nn\nimport torch\nimport numpy as np\n\n\ndef v_wrap(np_array, dtype=np.float32):\n    if np_array.dtype != dtype:\n        np_array = np_array.astype(dtype)\n    return torch.from_numpy(np_array)\n\n\ndef set_init(layers):\n    for layer in layers:\n        nn.init.normal_(layer.weight, mean=0., std=0.1)\n        nn.init.constant_(layer.bias, 0.)\n\n\ndef push_and_pull(opt, lnet, gnet, done, s_, bs, ba, br, gamma):\n    if done:\n        v_s_ = 0.               # terminal\n    else:\n        v_s_ = lnet.forward(v_wrap(s_[None, :]))[-1].data.numpy()[0, 0]\n\n    buffer_v_target = []\n    for r in br[::-1]:    # reverse buffer r\n        v_s_ = r + gamma * v_s_\n        buffer_v_target.append(v_s_)\n    buffer_v_target.reverse()\n\n    loss = lnet.loss_func(\n        v_wrap(np.vstack(bs)),\n        v_wrap(np.array(ba), dtype=np.int64) if ba[0].dtype == np.int64 else v_wrap(np.vstack(ba)),\n        v_wrap(np.array(buffer_v_target)[:, None]))\n\n    # calculate local gradients and push local parameters to global\n    opt.zero_grad()\n    loss.backward()\n    for lp, gp in zip(lnet.parameters(), gnet.parameters()):\n        gp._grad = lp.grad\n    opt.step()\n\n    # pull global parameters\n    lnet.load_state_dict(gnet.state_dict())\n\n\ndef record(global_ep, global_ep_r, ep_r, res_queue, name):\n    with global_ep.get_lock():\n        global_ep.value += 1\n    with global_ep_r.get_lock():\n        if global_ep_r.value == 0.:\n            global_ep_r.value = ep_r\n        else:\n            global_ep_r.value = global_ep_r.value * 0.99 + ep_r * 0.01\n    res_queue.put(global_ep_r.value)\n    print(\n        name,\n        ""Ep:"", global_ep.value,\n        ""| Ep_r: %.0f"" % global_ep_r.value,\n    )'"
