file_path,api_count,code
config.py,0,"b'\n\n\n\n\n\n\nclass VinConfig():\n    No = 3  # the number of object\n    img_folder = ""./vin/img/""  # image folder\n    data_folder = ""./vin/data/""  # data folder\n    frame_num = 50  # The Number of Saved Frame per each simul\n    frame_step = 1  # The difference between saved frames\n    roll_num = 20  # The Number of Rollout\n    set_num = 10  # The Number of set\n    weight=32\n    height=32\n    col_dim=4\n    batch_size=4\n    checkpoint_dir=""./checkpoint/""\n    load=False\n    log_dir=""./log""'"
logger.py,0,"b'# Code referenced from https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\n\nimport numpy as np\nimport scipy.misc\nimport os\nimport tensorflow as tf\ntry:\n    from StringIO import StringIO\nexcept ImportError:\n    from io import BytesIO\n\n\nclass Logger(object):\n    def __init__(self, log_dir):\n        import tensorflow as tf\n\n        self.summary_writer = tf.summary.FileWriter(log_dir)\n        self.summary_ops = {}\n\n    def graph_summary(self, graph):\n        self.summary_writer.add_graph(graph)\n        self.summary_writer.flush()\n\n    def scalar_summary(self, tag, value, step, scope):\n        summary = tf.Summary(value=[tf.Summary.Value(tag=os.path.join(scope, tag), simple_value=value)])\n        self.summary_writer.add_summary(summary, step)\n        self.summary_writer.flush()\n\n    def image_summary(self, tag, images, step, scope, max_output=4, random_summarization=False):\n        """"""Log a list of images.""""""\n        assert len(images.shape) == 4, ""the input image shape should be in form [batch,hight,width,channels]""\n        img_summaries = []\n        if random_summarization:\n            idxs = np.random.choice(images.shape[0], min(max_output, images.shape[0]))\n            images = images[idxs]\n        else:\n            images = images[:max_output]\n        if images.shape[-1]==1:\n            images=np.squeeze(images)\n        for i in range(images.shape[0]):\n            img=images[i]\n            try:\n                s = StringIO()\n            except:\n                s = BytesIO()\n            scipy.misc.toimage(img).save(s, format=""png"")\n\n            # Create an Image object\n            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n                                       height=img.shape[0],\n                                       width=img.shape[1])\n            # Create a Summary value\n            img_summaries.append(tf.Summary.Value(tag=os.path.join(scope, \'%s/%d\' % (tag, i)), image=img_sum))\n\n        # Create and write Summary\n        summary = tf.Summary(value=img_summaries)\n        self.summary_writer.add_summary(summary, step)\n        self.summary_writer.flush()\n\n    def histo_summary(self, tag, values, step, scope, bins=1000, ):\n        """"""Log a histogram of the tensor of values.""""""\n        counts, bin_edges = np.histogram(values, bins=bins)\n        # Fill the fields of the histogram proto\n        hist = tf.HistogramProto()\n        hist.min = float(np.min(values))\n        hist.max = float(np.max(values))\n        hist.num = int(np.prod(values.shape))\n        hist.sum = float(np.sum(values))\n        hist.sum_squares = float(np.sum(values ** 2))\n\n        # Drop the start of the first bin\n        bin_edges = bin_edges[1:]\n\n        # Add bin edges and counts\n        for edge in bin_edges:\n            hist.bucket_limit.append(edge)\n        for c in counts:\n            hist.bucket.append(c)\n\n        # Create and write Summary\n        summary = tf.Summary(value=[tf.Summary.Value(tag=os.path.join(scope, tag), histo=hist)])\n        self.summary_writer.add_summary(summary, step)\n        self.summary_writer.flush()\n\n    # summarize tenorflow tenosrs or images or merged summary, but this requires tensorflow session run\n    def summarize(self, sess, step, scope=\'train\', summaries_dict=None, summaries_merged=None):\n        """"""\n        Add the summaries to tensorboard\n        :param step:\n        :param summaries_dict:\n        :param summaries_merged:\n        :return:\n        """"""\n        with tf.variable_scope(scope):\n            if summaries_dict is not None:\n                summary_list = []\n                for tag, value in summaries_dict.items():\n                    if tag not in self.summary_ops:\n                        self.summary_placeholders[tag] = tf.placeholder(\'float32\', value.shape, name=tag)\n                        if len(value.shape) <= 1:\n                            self.summary_ops[tag] = tf.summary.scalar(tag, self.summary_placeholders[tag])\n                        else:\n                            self.summary_ops[tag] = tf.summary.image(tag, self.summary_placeholders[tag])\n\n                    summary_list.append(sess.run(self.summary_ops[tag], {self.summary_placeholders[tag]: value}))\n\n                for summary in summary_list:\n                    self.summary_writer.add_summary(summary, step)\n            if summaries_merged is not None:\n                self.summary_writer.add_summary(summaries_merged, step)\n            self.summary_writer.flush()'"
model.py,29,"b'from __future__ import print_function\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self, config):\n        super(Net, self).__init__()\n        self.config = config\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # visual encoder modules\n        self.conv1 = nn.Conv2d(10, 128, 3, padding=1)\n        self.conv2 = nn.Conv2d(128, 128, 3, padding=1)\n        self.conv3 = nn.Conv2d(128, 128, 3, padding=1)\n        self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n        self.conv5 = nn.Conv2d(128, 128, 3, padding=1)\n        # shared linear layer to convert the tensors to shape N_obj*64\n        self.fc1 = nn.Linear(128, 3 * 64)\n        # shared MLP layer to output the encoded state code N_obj*64\n        self.fc2 = nn.Linear(64 * 2, 64)\n        self.fc3 = nn.Linear(64, 64)\n        self.fc4 = nn.Linear(64, 64)\n        # end of visual encoder\n\n        # dynamic predictor modules\n        self.self_cores = {}\n        for i in range(3):\n            self.self_cores[i] = []\n            self.self_cores[i].append(nn.Linear(64, 64).double().cuda())\n            self.self_cores[i].append(nn.Linear(64, 64).double().cuda())\n        self.rel_cores = {}\n        for i in range(3):\n            self.rel_cores[i] = []\n            self.rel_cores[i].append(nn.Linear(64 * 2, 64).double().cuda())\n            self.rel_cores[i].append(nn.Linear(64, 64).double().cuda())\n            self.rel_cores[i].append(nn.Linear(64, 64).double().cuda())\n\n        self.affector = {}\n        for i in range(3):\n            self.affector[i] = []\n            self.affector[i].append(nn.Linear(64, 64).double().cuda())\n            self.affector[i].append(nn.Linear(64, 64).double().cuda())\n            self.affector[i].append(nn.Linear(64, 64).double().cuda())\n        self.out = {}\n        for i in range(3):\n            self.out[i] = []\n            self.out[i].append(nn.Linear(64 + 64, 64).double().cuda())\n            self.out[i].append(nn.Linear(64, 64).double().cuda())\n        self.aggregator1 = nn.Linear(64 * 3, 64)\n        self.aggregator2 = nn.Linear(64, 64)\n        self.state_decoder = nn.Linear(64, 4)\n\n    def core(self, s, core_idx):\n        objects = torch.chunk(s, 3, 1)\n\n        s_reshaped = s.view(-1, 64)\n        self_sd_h1 = F.relu(self.self_cores[core_idx][0](s_reshaped))\n        self_sd_h2 = self.self_cores[core_idx][1](self_sd_h1) + self_sd_h1\n        self_dynamic = self_sd_h2.view(-1, 3, 64)\n\n        rel_combination = []\n        for i in range(6):\n            row_idx = int(i / (2));\n            col_idx = int(i % (2));\n            rel_combination.append(torch.cat([objects[row_idx], objects[col_idx]], 1))\n        rel_combination = torch.cat(rel_combination)\n        rel_combination=rel_combination.view(-1,64*2)\n        rel_sd_h1 = F.relu(self.rel_cores[core_idx][0](rel_combination))\n        rel_sd_h2 = F.relu(self.rel_cores[core_idx][1](rel_sd_h1) + rel_sd_h1)\n        rel_sd_h3 = self.rel_cores[core_idx][2](rel_sd_h2) + rel_sd_h2\n        rel_objects = torch.chunk(rel_sd_h3, 6)\n        obj1 = rel_objects[0] + rel_objects[1]\n        obj2 = rel_objects[2] + rel_objects[3]\n        obj3 = rel_objects[4] + rel_objects[5]\n        rel_dynamic = torch.stack([obj1, obj2, obj3], 1)\n        dynamic_pred = self_dynamic + rel_dynamic\n        dynamic_pred = dynamic_pred.view(-1, 64)\n        aff1 = F.relu(self.affector[core_idx][0](dynamic_pred))\n        aff2 = F.relu(self.affector[core_idx][1](aff1) + aff1)\n        aff3 = self.affector[core_idx][2](aff2) + aff2\n        aff3 = aff3.view(-1, 3, 64)\n        aff_s = torch.cat([aff3, s], 2)\n        aff_s = aff_s.view(-1, 64 + 64)\n\n        out1 = F.relu(self.out[core_idx][0](aff_s))\n        out2 = self.out[core_idx][1](out1) + out1\n        out2 = out2.view(-1, 3, 64)\n\n        return out2\n\n    def forward(self, x, x_cor, y_cor):\n        f1, f2, f3, f4, f5, f6 = torch.chunk(x, 6,1)\n        f1, f2, f3, f4, f5, f6=f1.squeeze(), f2.squeeze(), f3.squeeze(), f4.squeeze(), f5.squeeze(), f6.squeeze()\n        f1f2 = torch.cat([f1, f2], 1)\n        f2f3 = torch.cat([f2, f3], 1)\n        f3f4 = torch.cat([f3, f4], 1)\n        f4f5 = torch.cat([f4, f5], 1)\n        f5f6 = torch.cat([f5, f6], 1)\n\n        pairs = torch.cat([f1f2, f2f3, f3f4, f4f5, f5f6])\n        pairs = torch.cat([pairs, x_cor, y_cor], dim=1)\n        ve_h1 = F.relu(self.conv1(pairs))\n        ve_h1 = self.pool(ve_h1)\n        ve_h2 = F.relu(self.conv2(ve_h1) + ve_h1)\n        ve_h2 = self.pool(ve_h2)\n        ve_h3 = F.relu(self.conv3(ve_h2) + ve_h2)\n        ve_h3 = self.pool(ve_h3)\n        ve_h4 = F.relu(self.conv4(ve_h3) + ve_h3)\n        ve_h4 = self.pool(ve_h4)\n        ve_h5 = F.relu(self.conv5(ve_h4) + ve_h4)\n        ve_h5 = self.pool(ve_h5)\n        unit_pairs = ve_h5.view(-1, 128)\n        # p1,p2,p3,p4,p5=torch.chunk(unit_pairs, 5)\n        encoded_pairs = self.fc1(unit_pairs)\n        p1, p2, p3, p4, p5 = torch.chunk(encoded_pairs, 5)\n        p1 = p1.view(-1, 3, 64)\n        p2 = p2.view(-1, 3, 64)\n        p3 = p3.view(-1, 3, 64)\n        p4 = p4.view(-1, 3, 64)\n        p5 = p5.view(-1, 3, 64)\n\n        pair1 = torch.cat([p1, p2], 2)\n        pair2 = torch.cat([p2, p3], 2)\n        pair3 = torch.cat([p3, p4], 2)\n        pair4 = torch.cat([p4, p5], 2)\n\n        diff_pairs = torch.cat([pair1, pair2, pair3, pair4])\n        diff_pairs = diff_pairs.view(-1, 64 * 2)\n        shared_h1 = F.relu(self.fc2(diff_pairs))\n        shared_h2 = F.relu(self.fc3(shared_h1) + shared_h1)\n        shared_h3 = self.fc4(shared_h2) + shared_h2\n        state_codes = shared_h3.view(-1, 3, 64)\n        s1, s2, s3, s4 = torch.chunk(state_codes, 4)\n        rolls = []\n        for i in range(20):\n            c1 = self.core(s1, 0)\n            c2 = self.core(s2, 1)\n            c3 = self.core(s3, 2)\n            all_c = torch.cat([c1, c2, c3], 2)\n            all_c = all_c.view(-1, 64 * 3)\n            aggregator1 = F.relu(self.aggregator1(all_c))\n            aggregator2 = self.aggregator2(aggregator1)\n            aggregator2 = aggregator2.view(-1, 3, 64)\n            rolls.append(aggregator2)\n            s1, s2, s3, s4 = s2, s3, s4, aggregator2\n        rollouts=torch.stack(rolls)\n        rollouts=rollouts.view(80,3,64)\n        rollouts=torch.cat([s1,s2,s3,s4,rollouts])\n        rollouts=rollouts.view(-1,64)\n        state_decoder=self.state_decoder(rollouts)\n        state_decoder=state_decoder.view(-1,3,4)\n        aux_out=state_decoder[:4*4]\n        aux_out=aux_out.view(4,4,3,4)\n\n\n\n        net_out=state_decoder[4*4:]\n        net_out=net_out.view(-1,20,3,4)\n        net_out2=torch.chunk(net_out, 20,1)\n\n        net_out2=net_out2[:8]\n        return net_out2,aux_out,net_out\n\n\n    def num_flat_features(self, x):\n        size = x.size()[1:]  # all dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n'"
train.py,7,"b'from vin_dataset import VinDataset, VinTestDataset, ToTensor, ToTensorV2\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torch import nn\nimport numpy as np\nimport torch\nfrom logger import Logger\nfrom utils import create_dir\nimport os\nfrom utils import make_image2\n\n\nclass Trainer():\n    def __init__(self, config, net):\n        self.net = net\n        self.config = config\n        create_dir(self.config.checkpoint_dir)\n\n        dataset = VinDataset(self.config, transform=ToTensor())\n        test_dataset = VinTestDataset(self.config, transform=ToTensorV2())\n        self.dataloader = DataLoader(dataset, batch_size=self.config.batch_size,\n                                     shuffle=True, num_workers=4)\n        self.test_dataloader = DataLoader(test_dataset, batch_size=1,\n                                          shuffle=True, num_workers=1)\n        self.optimizer = optim.Adam(self.net.parameters(), lr=0.0005)\n        self.logger = Logger(self.config.log_dir)\n        self.construct_cors()\n        self.save()\n        if config.load:\n            self.load()\n\n    def save(self):\n        torch.save(self.net.state_dict(), os.path.join(self.config.checkpoint_dir, ""checkpoint""))\n\n    def load(self):\n        self.net.load_state_dict(torch.load(os.path.join(self.config.checkpoint_dir, ""checkpoint"")))\n\n    def construct_cors(self):\n        # x-cor and y-cor setting\n        nx, ny = (self.config.weight, self.config.height)\n        x = np.linspace(0, 1, nx)\n        y = np.linspace(0, 1, ny)\n        xv, yv = np.meshgrid(x, y)\n        xv = np.reshape(xv, [self.config.height, self.config.weight, 1])\n        yv = np.reshape(yv, [self.config.height, self.config.weight, 1])\n        xcor = np.zeros((self.config.batch_size * 5, self.config.height, self.config.weight, 1), dtype=float)\n        ycor = np.zeros((self.config.batch_size * 5, self.config.height, self.config.weight, 1), dtype=float)\n        for i in range(self.config.batch_size * 5):\n            xcor[i] = xv\n            ycor[i] = yv\n        xcor = xcor.transpose((0, 3, 1, 2))\n        ycor = ycor.transpose((0, 3, 1, 2))\n        self.xcor, self.ycor = Variable(torch.from_numpy(xcor)).cuda(), Variable(torch.from_numpy(ycor)).cuda()\n\n    def train(self):\n        total_step = 0\n        df = Variable(torch.ones(1)).double().cuda()\n        for epoch in range(100):  # loop over the dataset multiple times\n            running_loss = 0.0\n            for i, data in enumerate(self.dataloader, 0):\n                total_step += 1\n                data = data[0]\n                if data[\'image\'].size()[0] < self.config.batch_size:\n                    print(data[\'image\'].size()[0])\n                    continue\n                inputs, output_label, output_S_label = data[\'image\'], data[\'output_label\'], data[\n                    \'output_S_label\']\n\n                inputs, output_label, output_S_label = Variable(inputs).cuda(), Variable(output_label).cuda(), Variable(\n                    output_S_label).cuda()\n\n                self.optimizer.zero_grad()\n\n                net_out, aux_out, _ = self.net(inputs, self.xcor, self.ycor)\n\n                # loss and optimizer\n                loss = nn.MSELoss()\n                mse = df * loss(net_out[0], output_label[:, 0])\n\n                for i in range(1, 8):\n                    mse += (df ** (i + 1)) * loss(net_out[i], output_label[:, i])\n                mse = mse / 8;\n                ve_loss = loss(aux_out, output_S_label)\n                total_loss = mse + ve_loss\n                total_loss.backward()\n\n                self.optimizer.step()\n                # tensorboard_logging\n                self.logger.scalar_summary(""mse"", mse.data[0], total_step, ""train"")\n                self.logger.scalar_summary(""ve_loss"", ve_loss.data[0], total_step, ""train"")\n                self.logger.scalar_summary(""total_loss"", total_loss.data[0], total_step, ""train"")\n            print(""epoch "", epoch, "" Finished"")\n            print(""testing................"")\n            self.test()\n        print(\'Finished Training\')\n\n    def test(self):\n        test_data = None\n        for i, data in enumerate(self.test_dataloader, 0):\n            test_data = data\n\n            data = test_data[0]\n            inputs, output_label, output_S_label, xy_origin, xy_estimated = data[\'image\'][0], data[\'output_label\'], data[\n                \'output_S_label\'], data[\'xy_origin\'], data[\'xy_estimated\']\n            xy_origin = Variable(xy_origin).data.cpu().numpy()[0]\n            xy_estimated = Variable(xy_estimated).data.cpu().numpy()[0]\n            inputs, output_label, output_S_label = Variable(inputs).cuda(), Variable(output_label).cuda(), Variable(\n                output_S_label).cuda()\n\n            out, aux_out, posi = self.net(inputs[:4], self.xcor, self.ycor)\n            velo = posi[0][:, :, 2:4];\n            xy_estimated[0] = output_S_label[0][3][3][:, :2].data.cpu().numpy() + (velo[0] * 0.01).data.cpu().numpy()\n            for i in range(1, len([0])):\n                xy_estimated[i] = xy_estimated[i - 1] + velo[i] * 0.01\n\n            # Saving\n            print(""Image Making"")\n            make_image2(xy_origin, self.config.img_folder + ""../results/"", ""true"")\n            make_image2(xy_estimated, self.config.img_folder + ""../results/"", ""modeling"")\n            print(""Done"")\n            yield'"
utils.py,0,"b'import os\nimport matplotlib.pyplot as plt\ndef create_dir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\ndef make_image2(xy,img_folder,prefix):\n  if not os.path.exists(img_folder):\n    os.makedirs(img_folder);\n  fig_num=len(xy);\n  mydpi=100;\n  for i in range(fig_num):\n    #fig = plt.figure(figsize=(32/mydpi,32/mydpi))\n    fig = plt.figure(figsize=(128/mydpi,128/mydpi))\n    plt.xlim(-200, 200)\n    plt.ylim(-200, 200)\n    plt.axis(\'off\');\n    color=[\'r\',\'b\',\'g\',\'k\',\'y\',\'m\',\'c\'];\n    for j in range(len(xy[0])):\n      #plt.scatter(xy[i,j,1],xy[i,j,0],c=color[j%len(color)],s=0.5);\n      plt.scatter(xy[i,j,1],xy[i,j,0],c=color[j%len(color)],s=5);\n    fig.savefig(img_folder+prefix+""_""+str(i)+"".png"",dpi=mydpi);'"
vin.py,0,"b""from model import Net\nfrom config import VinConfig\nimport tensorflow as tf\n\n\ndef main():\n    from  train import Trainer\n    net=Net(VinConfig)\n    net=net.cuda()\n    net=net.double()\n    trainer=Trainer(VinConfig,net)\n    trainer.train()\n\n\n\n\nif __name__ == '__main__':\n    main()"""
vin_dataset.py,7,"b'from torch.utils.data import Dataset\nimport os\nimport numpy as np\nimport matplotlib.image as mpimg\nimport torch\n\n\nclass VinDataset(Dataset):\n    """"""Face Landmarks dataset.""""""\n\n    def __init__(self, config, transform=None):\n\n        self.config = config\n        self.transform = transform\n\n        total_img = np.zeros((self.config.set_num, int(self.config.frame_num), self.config.height, self.config.weight,\n                              self.config.col_dim))\n        for i in range(self.config.set_num):\n            for j in range(int(self.config.frame_num)):\n                total_img[i, j] = mpimg.imread(self.config.img_folder + ""train/"" + str(i) + \'_\' + str(j) + \'.png\')[:, :,\n                                  :self.config.col_dim]\n\n        total_data = np.zeros((self.config.set_num, int(self.config.frame_num), self.config.No * 5))\n        for i in range(self.config.set_num):\n            f = open(self.config.data_folder + ""train/"" + str(i) + "".csv"", ""r"")\n            total_data[i] = [line[:-1].split("","") for line in f.readlines()]\n        total_data = np.reshape(total_data, [self.config.set_num, int(self.config.frame_num), self.config.No, 5])\n\n        # reshape img and data\n        input_img = np.zeros((self.config.set_num * (int(self.config.frame_num) - 14 + 1), 6, self.config.height,\n                              self.config.weight, self.config.col_dim)\n                             )\n        output_label = np.zeros((self.config.set_num * (int(self.config.frame_num) - 14 + 1), 8, self.config.No, 4)\n                                )\n        output_S_label = np.zeros((self.config.set_num * (int(self.config.frame_num) - 14 + 1), 4, self.config.No, 4)\n                                  )\n        for i in range(self.config.set_num):\n            for j in range(int(self.config.frame_num) - 14 + 1):\n                input_img[i * (int(self.config.frame_num) - 14 + 1) + j] = total_img[i, j:j + 6]\n                output_label[i * (int(self.config.frame_num) - 14 + 1) + j] = np.reshape(total_data[i, j + 6:j + 14],\n                                                                                         [8, self.config.No, 5])[\n                                                                              :, :, 1:5]\n                output_S_label[i * (int(self.config.frame_num) - 14 + 1) + j] = np.reshape(total_data[i, j + 2:j + 6],\n                                                                                           [4, self.config.No, 5])[:, :,\n                                                                                1:5]\n\n        # shuffle\n        tr_data_num = int(len(input_img) * 1)\n        total_idx = np.arange(len(input_img))\n        np.random.shuffle(total_idx)\n        self.tr_data = input_img[total_idx]\n        self.tr_label = output_label[total_idx]\n        self.tr_S_label = output_S_label[total_idx]\n\n    def __len__(self):\n        return len(self.tr_data)\n\n    def __getitem__(self, idx):\n\n        sample = {\'image\': self.tr_data[idx], \'output_label\': self.tr_label[idx],\n                  \'output_S_label\': self.tr_S_label[idx]}\n\n        if self.transform:\n            sample = self.transform(sample),\n\n        return sample\n\n\nclass ToTensor(object):\n    """"""Convert ndarrays in sample to Tensors.""""""\n\n    def __call__(self, sample):\n        image, output_label, output_S_label = sample[\'image\'], sample[\'output_label\'], sample[\n            \'output_S_label\']\n\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        image = image.transpose((0, 3, 1, 2))\n        sample[\'image\']=torch.from_numpy(image)\n        sample[\'output_label\']=torch.from_numpy(output_label)\n        sample[\'output_S_label\']=torch.from_numpy(output_S_label)\n        return sample\n\nclass ToTensorV2(object):\n    """"""Convert ndarrays in sample to Tensors.""""""\n\n    def __call__(self, sample):\n        image, output_label, output_S_label = sample[\'image\'], sample[\'output_label\'], sample[\n            \'output_S_label\']\n\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        image = image.transpose((0, 1, 4, 2,3))\n        sample[\'image\']=torch.from_numpy(image)\n        sample[\'output_label\']=torch.from_numpy(output_label)\n        sample[\'output_S_label\']=torch.from_numpy(output_S_label)\n        return sample\n\n\nclass VinTestDataset(Dataset):\n    """"""Face Landmarks dataset.""""""\n\n    def __init__(self, config, transform=None):\n\n        self.config = config\n        self.transform = transform\n\n    def __len__(self):\n        return 1\n\n    def __getitem__(self, idx):\n\n        total_img = np.zeros((self.config.set_num, int(self.config.frame_num), self.config.height, self.config.weight,\n                              self.config.col_dim))\n        for i in range(self.config.set_num):\n            for j in range(int(self.config.frame_num)):\n                total_img[i, j] = mpimg.imread(self.config.img_folder + ""train/"" + str(i) + \'_\' + str(j) + \'.png\')[:, :,\n                                  :self.config.col_dim]\n        ts_img = np.zeros(\n            (1, int(self.config.frame_num), self.config.height, self.config.weight, self.config.col_dim),\n            dtype=float)\n        for i in range(1):\n            for j in range(int(self.config.frame_num)):\n                ts_img[i, j] = mpimg.imread(self.config.img_folder + ""test/"" + str(i) + ""_"" + str(j) + \'.png\')[:, :,\n                               :self.config.col_dim]\n        ts_data = np.zeros((1, int(self.config.frame_num), self.config.No * 5), dtype=float)\n        for i in range(1):\n            f = open(self.config.data_folder + ""test/"" + str(i) + "".csv"", ""r"")\n            ts_data[i] = [line[:-1].split("","") for line in f.readlines()]\n\n        # reshape img and data\n        input_img = np.zeros(\n            (1 * (int(self.config.frame_num) - 14 + 1), 6, self.config.height, self.config.weight,\n             self.config.col_dim),\n            dtype=float);\n        output_label = np.zeros((1 * (int(self.config.frame_num) - 14 + 1), 8, self.config.No, 4), dtype=float)\n        output_S_label = np.zeros((1 * (int(self.config.frame_num) - 14 + 1), 4, self.config.No, 4), dtype=float)\n        for i in range(1):\n            for j in range(int(self.config.frame_num) - 14 + 1):\n                input_img[i * (int(self.config.frame_num) - 14 + 1) + j] = total_img[i, j:j + 6]\n                output_label[i * (int(self.config.frame_num) - 14 + 1) + j] = np.reshape(ts_data[i, j + 6:j + 14],\n                                                                                         [8, self.config.No, 5])[:,\n                                                                              :, 1:5]\n                output_S_label[i * (int(self.config.frame_num) - 14 + 1) + j] = np.reshape(ts_data[i, j + 2:j + 6],\n                                                                                           [4, self.config.No, 5])[\n                                                                                :,\n                                                                                :, 1:5]\n\n        xy_origin = output_label[:(int(self.config.frame_num) - 14 + 1 - 4 + 1), 0, :, 0:2]\n        xy_estimated = np.zeros((self.config.roll_num, self.config.No, 2), dtype=float)\n\n        sample = {\'image\': input_img, \'output_label\': output_label,\n                  \'output_S_label\': output_S_label,\'xy_origin\':xy_origin,\'xy_estimated\':xy_estimated}\n\n        if self.transform:\n            sample = self.transform(sample),\n\n        return sample\n\n'"
