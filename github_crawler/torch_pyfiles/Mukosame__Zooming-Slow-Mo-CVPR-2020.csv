file_path,api_count,code
codes/test.py,13,"b""'''\ntest Zooming Slow-Mo models on arbitrary datasets\nwrite to txt log file\n[kosame] TODO: update the test script to the newest version\n'''\n\nimport os\nimport os.path as osp\nimport glob\nimport logging\nimport numpy as np\nimport cv2\nimport torch\n\nimport utils.util as util\nimport data.util as data_util\nimport models.modules.Sakuya_arch as Sakuya_arch\n\ndef main():\n    scale = 4\n    N_ot = 7 #3\n    N_in = 1+ N_ot // 2\n    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\n    #### model \n    #### TODO: change your model path here\n    model_path = '../experiments/pretrained_models/xiang2020zooming.pth'\n    model = Sakuya_arch.LunaTokis(64, N_ot, 8, 5, 40)\n\n    #### dataset\n    data_mode = 'Custom' #'Vid4' #'SPMC'#'Middlebury'#\n\n    if data_mode == 'Vid4':\n        test_dataset_folder = '/data/xiang/SR/Vid4/LR/*'\n    if data_mode == 'SPMC':\n        test_dataset_folder = '/data/xiang/SR/spmc/*'\n    if data_mode == 'Custom':\n        test_dataset_folder = '../test_example/*' # TODO: put your own data path here\n\n    #### evaluation\n    flip_test = False #True#\n    crop_border = 0\n\n    # temporal padding mode\n    padding = 'replicate'\n    save_imgs = False #True#\n    if 'Custom' in data_mode: save_imgs = True\n    ############################################################################\n    if torch.cuda.is_available():\n        device = torch.device('cuda') \n    else:\n        device = torch.device('cpu')\n    save_folder = '../results/{}'.format(data_mode)\n    util.mkdirs(save_folder)\n    util.setup_logger('base', save_folder, 'test', level=logging.INFO, screen=True, tofile=True)\n    logger = logging.getLogger('base')\n    model_params = util.get_model_total_params(model)\n\n    #### log info\n    logger.info('Data: {} - {}'.format(data_mode, test_dataset_folder))\n    logger.info('Padding mode: {}'.format(padding))\n    logger.info('Model path: {}'.format(model_path))\n    logger.info('Model parameters: {} M'.format(model_params))\n    logger.info('Save images: {}'.format(save_imgs))\n    logger.info('Flip Test: {}'.format(flip_test))\n   \n\n    def single_forward(model, imgs_in):\n        with torch.no_grad():\n            # imgs_in.size(): [1,n,3,h,w]\n            b,n,c,h,w = imgs_in.size()\n            h_n = int(4*np.ceil(h/4))\n            w_n = int(4*np.ceil(w/4))\n            imgs_temp = imgs_in.new_zeros(b,n,c,h_n,w_n)\n            imgs_temp[:,:,:,0:h,0:w] = imgs_in\n\n            model_output = model(imgs_temp)\n            # model_output.size(): torch.Size([1, 3, 4h, 4w])\n            model_output = model_output[:, :, :, 0:scale*h, 0:scale*w]\n            if isinstance(model_output, list) or isinstance(model_output, tuple):\n                output = model_output[0]\n            else:\n                output = model_output\n        return output\n\n    sub_folder_l = sorted(glob.glob(test_dataset_folder))\n\n    model.load_state_dict(torch.load(model_path), strict=True)\n\n    model.eval()\n    model = model.to(device)\n\n    avg_psnr_l = []\n    avg_psnr_y_l = []\n    sub_folder_name_l = []\n    # total_time = []\n    # for each sub-folder\n    for sub_folder in sub_folder_l:\n        gt_tested_list = []\n        sub_folder_name = sub_folder.split('/')[-1]\n        sub_folder_name_l.append(sub_folder_name)\n        save_sub_folder = osp.join(save_folder, sub_folder_name)\n\n        if data_mode == 'SPMC':\n            sub_folder = sub_folder + '/LR/'\n        img_LR_l = sorted(glob.glob(sub_folder + '/*'))\n\n        if save_imgs:\n            util.mkdirs(save_sub_folder)\n\n        #### read LR images\n        imgs = util.read_seq_imgs(sub_folder)\n        #### read GT images\n        img_GT_l = []\n        if data_mode == 'SPMC':\n            sub_folder_GT = osp.join(sub_folder.replace('/LR/', '/truth/'))\n        else:\n            sub_folder_GT = osp.join(sub_folder.replace('/LR/', '/HR/'))\n\n        if 'Custom' not in data_mode:\n            for img_GT_path in sorted(glob.glob(osp.join(sub_folder_GT,'*'))):\n                img_GT_l.append(util.read_image(img_GT_path))\n\n        avg_psnr, avg_psnr_sum, cal_n = 0,0,0\n        avg_psnr_y, avg_psnr_sum_y = 0,0\n        \n        if len(img_LR_l) == len(img_GT_l):\n            skip = True\n        else:\n            skip = False\n        \n        if 'Custom' in data_mode:\n            select_idx_list = util.test_index_generation(False, N_ot, len(img_LR_l))\n        else:\n            select_idx_list = util.test_index_generation(skip, N_ot, len(img_LR_l))\n        # process each image\n        for select_idxs in select_idx_list:\n            # get input images\n            select_idx = select_idxs[0]\n            gt_idx = select_idxs[1]\n            imgs_in = imgs.index_select(0, torch.LongTensor(select_idx)).unsqueeze(0).to(device)\n\n            output = single_forward(model, imgs_in)\n\n            outputs = output.data.float().cpu().squeeze(0)            \n\n            if flip_test:\n                # flip W\n                output = single_forward(model, torch.flip(imgs_in, (-1, )))\n                output = torch.flip(output, (-1, ))\n                output = output.data.float().cpu().squeeze(0)\n                outputs = outputs + output\n                # flip H\n                output = single_forward(model, torch.flip(imgs_in, (-2, )))\n                output = torch.flip(output, (-2, ))\n                output = output.data.float().cpu().squeeze(0)\n                outputs = outputs + output\n                # flip both H and W\n                output = single_forward(model, torch.flip(imgs_in, (-2, -1)))\n                output = torch.flip(output, (-2, -1))\n                output = output.data.float().cpu().squeeze(0)\n                outputs = outputs + output\n\n                outputs = outputs / 4\n\n            # save imgs\n            for idx, name_idx in enumerate(gt_idx):\n                if name_idx in gt_tested_list:\n                    continue\n                gt_tested_list.append(name_idx)\n                output_f = outputs[idx,:,:,:].squeeze(0)\n\n                output = util.tensor2img(output_f)\n                if save_imgs:                \n                    cv2.imwrite(osp.join(save_sub_folder, '{:08d}.png'.format(name_idx+1)), output)\n\n                if 'Custom' not in data_mode:\n                    #### calculate PSNR\n                    output = output / 255.\n\n                    GT = np.copy(img_GT_l[name_idx])\n\n                    if crop_border == 0:\n                        cropped_output = output\n                        cropped_GT = GT\n                    else:\n                        cropped_output = output[crop_border:-crop_border, crop_border:-crop_border, :]\n                        cropped_GT = GT[crop_border:-crop_border, crop_border:-crop_border, :]\n                    crt_psnr = util.calculate_psnr(cropped_output * 255, cropped_GT * 255)\n                    cropped_GT_y = data_util.bgr2ycbcr(cropped_GT, only_y=True)\n                    cropped_output_y = data_util.bgr2ycbcr(cropped_output, only_y=True)\n                    crt_psnr_y = util.calculate_psnr(cropped_output_y * 255, cropped_GT_y * 255)\n                    logger.info('{:3d} - {:25}.png \\tPSNR: {:.6f} dB  PSNR-Y: {:.6f} dB'.format(name_idx + 1, name_idx+1, crt_psnr, crt_psnr_y))\n                    avg_psnr_sum += crt_psnr\n                    avg_psnr_sum_y += crt_psnr_y\n                    cal_n += 1\n\n        if 'Custom' not in data_mode:\n            avg_psnr = avg_psnr_sum / cal_n\n            avg_psnr_y = avg_psnr_sum_y / cal_n\n    \n            logger.info('Folder {} - Average PSNR: {:.6f} dB PSNR-Y: {:.6f} dB for {} frames; '.format(sub_folder_name, avg_psnr, avg_psnr_y, cal_n))\n    \n            avg_psnr_l.append(avg_psnr)\n            avg_psnr_y_l.append(avg_psnr_y)\n\n    if 'Custom' not in data_mode:\n        logger.info('################ Tidy Outputs ################')\n        for name, psnr, psnr_y in zip(sub_folder_name_l, avg_psnr_l, avg_psnr_y_l):\n            logger.info('Folder {} - Average PSNR: {:.6f} dB PSNR-Y: {:.6f} dB. '\n                       .format(name, psnr, psnr_y))\n        logger.info('################ Final Results ################')\n        logger.info('Data: {} - {}'.format(data_mode, test_dataset_folder))\n        logger.info('Padding mode: {}'.format(padding))\n        logger.info('Model path: {}'.format(model_path))\n        logger.info('Save images: {}'.format(save_imgs))\n        logger.info('Flip Test: {}'.format(flip_test))\n        logger.info('Total Average PSNR: {:.6f} dB PSNR-Y: {:.6f} dB for {} clips. '\n                    .format(\n                        sum(avg_psnr_l) / len(avg_psnr_l), sum(avg_psnr_y_l) / len(avg_psnr_y_l), len(sub_folder_l)))\n        # logger.info('Total Runtime: {:.6f} s Average Runtime: {:.6f} for {} images.'\n                    # .format(sum(total_time), sum(total_time)/171, 171))\n\nif __name__ == '__main__':\n    main()"""
codes/train.py,12,"b""import os\nimport math\nimport argparse\nimport random\nimport logging\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom data.data_sampler import DistIterSampler\n\nimport options.options as option\nfrom utils import util\nfrom data import create_dataloader, create_dataset\nfrom models import create_model\n\n\ndef init_dist(backend='nccl', **kwargs):\n    ''' initialization for distributed training'''\n    # if mp.get_start_method(allow_none=True) is None:\n    if mp.get_start_method(allow_none=True) != 'spawn':\n        mp.set_start_method('spawn')\n    rank = int(os.environ['RANK'])\n    num_gpus = torch.cuda.device_count()\n    torch.cuda.set_device(rank % num_gpus)\n    dist.init_process_group(backend=backend, **kwargs)\n\n\ndef main():\n    #### options\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-opt', type=str, help='Path to option YAML file.')\n    parser.add_argument('--launcher', choices=['none', 'pytorch'], default='none',\n                        help='job launcher')\n    parser.add_argument('--local_rank', type=int, default=0)\n    args = parser.parse_args()\n    opt = option.parse(args.opt, is_train=True)\n\n    #### distributed training settings\n    if args.launcher == 'none':  # disabled distributed training\n        opt['dist'] = False\n        rank = -1\n        print('Disabled distributed training.')\n    else:\n        opt['dist'] = True\n        init_dist()\n        world_size = torch.distributed.get_world_size()\n        rank = torch.distributed.get_rank()\n\n    #### loading resume state if exists\n    if opt['path'].get('resume_state', None):\n        # distributed resuming: all load into default GPU\n        device_id = torch.cuda.current_device()\n        resume_state = torch.load(opt['path']['resume_state'],\n                                  map_location=lambda storage, loc: storage.cuda(device_id))\n        option.check_resume(opt, resume_state['iter'])  # check resume options\n    else:\n        resume_state = None\n\n    #### mkdir and loggers\n    if rank <= 0:  # normal training (rank -1) OR distributed training (rank 0)\n        if resume_state is None:\n            util.mkdir_and_rename(\n                opt['path']['experiments_root'])  # rename experiment folder if exists\n            util.mkdirs((path for key, path in opt['path'].items() if not key == 'experiments_root'\n                         and 'pretrain_model' not in key and 'resume' not in key))\n\n        # config loggers. Before it, the log will not work\n        util.setup_logger('base', opt['path']['log'], 'train_' + opt['name'], level=logging.INFO,\n                          screen=True, tofile=True)\n        logger = logging.getLogger('base')\n        logger.info(option.dict2str(opt))\n        # tensorboard logger\n        if opt['use_tb_logger'] and 'debug' not in opt['name']:\n            version = float(torch.__version__[0:3])\n            if version >= 1.1:  # PyTorch 1.1\n                from torch.utils.tensorboard import SummaryWriter\n            else:\n                logger.info(\n                    'You are using PyTorch {}. Tensorboard will use [tensorboardX]'.format(version))\n                from tensorboardX import SummaryWriter\n            tb_logger = SummaryWriter(log_dir='../tb_logger/' + opt['name'])\n    else:\n        util.setup_logger('base', opt['path']['log'], 'train', level=logging.INFO, screen=True)\n        logger = logging.getLogger('base')\n\n    # convert to NoneDict, which returns None for missing keys\n    opt = option.dict_to_nonedict(opt)\n\n    #### random seed\n    seed = opt['train']['manual_seed']\n    if seed is None:\n        seed = random.randint(1, 10000)\n    if rank <= 0:\n        logger.info('Random seed: {}'.format(seed))\n    util.set_random_seed(seed)\n\n    torch.backends.cudnn.benckmark = True\n    # torch.backends.cudnn.deterministic = True\n\n    #### create train and val dataloader\n    dataset_ratio = 200  # enlarge the size of each epoch\n    for phase, dataset_opt in opt['datasets'].items():\n        if phase == 'train':\n            train_set = create_dataset(dataset_opt)\n            train_size = int(math.ceil(len(train_set) / dataset_opt['batch_size']))\n            total_iters = int(opt['train']['niter'])\n            total_epochs = int(math.ceil(total_iters / train_size))\n            if opt['dist']:\n                train_sampler = DistIterSampler(train_set, world_size, rank, dataset_ratio)\n                total_epochs = int(math.ceil(total_iters / (train_size * dataset_ratio)))\n            else:\n                train_sampler = None\n            train_loader = create_dataloader(train_set, dataset_opt, opt, train_sampler)\n            if rank <= 0:\n                logger.info('Number of train images: {:,d}, iters: {:,d}'.format(\n                    len(train_set), train_size))\n                logger.info('Total epochs needed: {:d} for iters {:,d}'.format(\n                    total_epochs, total_iters))\n        elif phase == 'val':\n            pass\n            '''\n            val_set = create_dataset(dataset_opt)\n            val_loader = create_dataloader(val_set, dataset_opt, opt, None)\n            if rank <= 0:\n                logger.info('Number of val images in [{:s}]: {:d}'.format(\n                    dataset_opt['name'], len(val_set)))\n            '''\n        else:\n            raise NotImplementedError('Phase [{:s}] is not recognized.'.format(phase))\n    assert train_loader is not None\n\n    #### create model\n    model = create_model(opt)\n\n    #### resume training\n    if resume_state:\n        logger.info('Resuming training from epoch: {}, iter: {}.'.format(\n            resume_state['epoch'], resume_state['iter']))\n\n        start_epoch = resume_state['epoch']\n        current_step = resume_state['iter']\n        model.resume_training(resume_state)  # handle optimizers and schedulers\n    else:\n        current_step = 0\n        start_epoch = 0\n\n    #### training\n    logger.info('Start training from epoch: {:d}, iter: {:d}'.format(start_epoch, current_step))\n    for epoch in range(start_epoch, total_epochs + 1):\n        if opt['dist']:\n            train_sampler.set_epoch(epoch)\n        for _, train_data in enumerate(train_loader):\n            current_step += 1\n            if current_step > total_iters:\n                break\n            #### update learning rate\n            model.update_learning_rate(current_step, warmup_iter=opt['train']['warmup_iter'])\n\n            #### training\n            model.feed_data(train_data)\n            model.optimize_parameters(current_step)\n\n            #### log\n            if current_step % opt['logger']['print_freq'] == 0:\n                logs = model.get_current_log()\n                message = '<epoch:{:3d}, iter:{:8,d}, lr:('.format(epoch, current_step)\n                for v in model.get_current_learning_rate():\n                    message += '{:.3e},'.format(v)\n                message += ')>'\n                for k, v in logs.items():\n                    message += '{:s}: {:.4e} '.format(k, v)\n                    # tensorboard logger\n                    if opt['use_tb_logger'] and 'debug' not in opt['name']:\n                        if rank <= 0:\n                            tb_logger.add_scalar(k, v, current_step)\n                if rank <= 0:\n                    logger.info(message)\n            #### validation\n            # currently, it does not support validation during training\n\n            #### save models and training states\n            if current_step % opt['logger']['save_checkpoint_freq'] == 0:\n                if rank <= 0:\n                    logger.info('Saving models and training states.')\n                    model.save(current_step)\n                    model.save_training_state(epoch, current_step)\n\n    if rank <= 0:\n        logger.info('Saving the final model.')\n        model.save('latest')\n        logger.info('End of training.')\n\n\nif __name__ == '__main__':\n    main()\n"""
codes/video_to_zsm.py,6,"b'#!/usr/bin/env python3\nimport os\nimport os.path as osp\nimport glob\nimport logging\nimport numpy as np\nimport cv2\nimport torch\n\nimport utils.util as util\nimport data.util as data_util\nimport models.modules.Sakuya_arch as Sakuya_arch\n\nimport argparse\nfrom shutil import rmtree\n\n# For parsing commandline arguments\nparser = argparse.ArgumentParser()\nparser.add_argument(""--ffmpeg_dir"", type=str, default="""", help=\'path to ffmpeg.exe\')\nparser.add_argument(""--video"", type=str, required=True, help=\'path of video to be converted\')\nparser.add_argument(""--model"", type=str, required=True, help=\'path of pretrained model\')\nparser.add_argument(""--fps"", type=float, default=24, help=\'specify fps of output video. Default: 24.\')\nparser.add_argument(""--N_out"", type=int, default=7, help=\'Specify size of output frames of the network for faster conversion. This will depend on your cpu/gpu memory. Default: 7\')\nparser.add_argument(""--output"", type=str, default=""output.mp4"", help=\'Specify output file name. Default: output.mp4\')\nargs = parser.parse_args()\n\ndef check():\n    """"""\n    Checks the validity of commandline arguments.\n    Parameters\n    ----------\n        None\n    Returns\n    -------\n        error : string\n            Error message if error occurs otherwise blank string.\n    """"""\n\n    error = """"\n    if (args.batch_size not in [3, 5, 7]):\n        error = ""Error: --N_out has to be 3 or 5 or 7""\n    # if "".mkv"" not in args.output:\n        # error = ""output needs to have a video container""\n    return error\n\ndef main():\n    scale = 4\n    N_ot = args.N_out\n    N_in = 1 + N_ot//2\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0\'\n\n    #### model\n    model_path = args.model\n    model = Sakuya_arch.LunaTokis(64, N_ot, 8, 5, 40)\n\n    #### extract the input video to temporary folder\n    save_folder = osp.join(osp.dirname(args.output), \'.delme\')\n    save_out_folder = osp.join(osp.dirname(args.output), \'.hr_delme\')\n    util.mkdirs(save_folder)\n    util.mkdirs(save_out_folder)\n    error = util.extract_frames(args.ffmpeg_dir, args.video, save_folder)\n    if error:\n        print(error)\n        exit(1)\n\n    # temporal padding mode\n    padding = \'replicate\'\n    save_imgs = True\n\n    ############################################################################\n    if torch.cuda.is_available():\n        device = torch.device(\'cuda\') \n    else:\n        device = torch.device(\'cpu\')\n    \n    def single_forward(model, imgs_in):\n        with torch.no_grad():\n            # print(imgs_in.size()) # [1,5,3,270,480]\n            b,n,c,h,w = imgs_in.size()\n            h_n = int(4*np.ceil(h/4))\n            w_n = int(4*np.ceil(w/4))\n            imgs_temp = imgs_in.new_zeros(b,n,c,h_n,w_n)\n            imgs_temp[:,:,:,0:h,0:w] = imgs_in\n            model_output = model(imgs_temp)\n            model_output = model_output[:, :, :, 0:scale*h, 0:scale*w]\n            if isinstance(model_output, list) or isinstance(model_output, tuple):\n                output = model_output[0]\n            else:\n                output = model_output\n        return output\n\n    model.load_state_dict(torch.load(model_path), strict=True)\n\n    model.eval()\n    model = model.to(device)\n    #### zsm images\n    imgs = util.read_seq_imgs(save_folder)\n    select_idx_list = util.test_index_generation(False, N_ot, len(imgs))\n    for select_idxs in select_idx_list:\n        # get input images\n        select_idx = select_idxs[0]\n        imgs_in = imgs.index_select(0, torch.LongTensor(select_idx)).unsqueeze(0).to(device)\n        output = single_forward(model, imgs_in)\n        outputs = output.data.float().cpu().squeeze(0)            \n        # save imgs\n        out_idx = select_idxs[1]\n        for idx,name_idx in enumerate(out_idx):\n            output_f = outputs[idx,...].squeeze(0)\n            if save_imgs:     \n                output = util.tensor2img(output_f)\n                cv2.imwrite(osp.join(save_out_folder, \'{:06d}.png\'.format(name_idx)), output)\n\n    # now turn output images to video\n    # generate mp4\n    util.create_video(args.ffmpeg_dir, save_out_folder, args.output, args.fps)\n\n    # remove tmp folder    \n    rmtree(save_folder)\n    rmtree(save_out_folder)\n    \n    exit(0)\n\nif __name__ == \'__main__\':\n    main()'"
codes/data/Vimeo7_dataset.py,3,"b""'''\nVimeo7 dataset\nsupport reading images from lmdb, image folder and memcached\n'''\nimport os.path as osp\nimport random\nimport pickle\nimport logging\nimport numpy as np\nimport cv2\nimport lmdb\nimport torch\nimport torch.utils.data as data\nimport data.util as util\ntry:\n    import mc  # import memcached\nexcept ImportError:\n    pass\n\nlogger = logging.getLogger('base')\n\n\nclass Vimeo7Dataset(data.Dataset):\n    '''\n    Reading the training Vimeo dataset\n    key example: train/00001/0001/im1.png\n    GT: Ground-Truth;\n    LQ: Low-Quality, e.g., low-resolution frames\n    support reading N HR frames, N = 3, 5, 7\n    '''\n\n    def __init__(self, opt):\n        super(Vimeo7Dataset, self).__init__()\n        self.opt = opt\n        # temporal augmentation\n        self.interval_list = opt['interval_list']\n        self.random_reverse = opt['random_reverse']\n        logger.info('Temporal augmentation interval list: [{}], with random reverse is {}.'.format(\n            ','.join(str(x) for x in opt['interval_list']), self.random_reverse))\n        self.half_N_frames = opt['N_frames'] // 2\n        self.LR_N_frames = 1 + self.half_N_frames\n        assert self.LR_N_frames > 1, 'Error: Not enough LR frames to interpolate'\n        #### determine the LQ frame list\n        '''\n        N | frames\n        1 | error\n        3 | 0,2\n        5 | 0,2,4\n        7 | 0,2,4,6\n        '''\n        self.LR_index_list = []\n        for i in range(self.LR_N_frames):\n            self.LR_index_list.append(i*2)\n\n        self.GT_root, self.LQ_root = opt['dataroot_GT'], opt['dataroot_LQ']\n        self.data_type = self.opt['data_type']\n        self.LR_input = False if opt['GT_size'] == opt['LQ_size'] else True  # low resolution inputs\n        #### directly load image keys\n        if opt['cache_keys']:\n            logger.info('Using cache keys: {}'.format(opt['cache_keys']))\n            cache_keys = opt['cache_keys']\n        else:\n            cache_keys = 'Vimeo7_train_keys.pkl'\n        logger.info('Using cache keys - {}.'.format(cache_keys))\n        self.paths_GT = pickle.load(open('./data/{}'.format(cache_keys), 'rb'))\n     \n        assert self.paths_GT, 'Error: GT path is empty.'\n\n        if self.data_type == 'lmdb':\n            self.GT_env, self.LQ_env = None, None\n        elif self.data_type == 'mc':  # memcached\n            self.mclient = None\n        elif self.data_type == 'img':\n            pass\n        else:\n            raise ValueError('Wrong data type: {}'.format(self.data_type))\n\n    def _init_lmdb(self):\n        # https://github.com/chainer/chainermn/issues/129\n        self.GT_env = lmdb.open(self.opt['dataroot_GT'], readonly=True, lock=False, readahead=False,\n                                meminit=False)\n        self.LQ_env = lmdb.open(self.opt['dataroot_LQ'], readonly=True, lock=False, readahead=False,\n                                meminit=False)\n\n    def _ensure_memcached(self):\n        if self.mclient is None:\n            # specify the config files\n            server_list_config_file = None\n            client_config_file = None\n            self.mclient = mc.MemcachedClient.GetInstance(server_list_config_file,\n                                                          client_config_file)\n\n    def _read_img_mc(self, path):\n        ''' Return BGR, HWC, [0, 255], uint8'''\n        value = mc.pyvector()\n        self.mclient.Get(path, value)\n        value_buf = mc.ConvertBuffer(value)\n        img_array = np.frombuffer(value_buf, np.uint8)\n        img = cv2.imdecode(img_array, cv2.IMREAD_UNCHANGED)\n        return img\n\n    def _read_img_mc_BGR(self, path, name_a, name_b):\n        ''' Read BGR channels separately and then combine for 1M limits in cluster'''\n        img_B = self._read_img_mc(osp.join(path + '_B', name_a, name_b + '.png'))\n        img_G = self._read_img_mc(osp.join(path + '_G', name_a, name_b + '.png'))\n        img_R = self._read_img_mc(osp.join(path + '_R', name_a, name_b + '.png'))\n        img = cv2.merge((img_B, img_G, img_R))\n        return img\n\n    def __getitem__(self, index):\n        if self.data_type == 'mc':\n            self._ensure_memcached()\n        elif self.data_type == 'lmdb':\n            if (self.GT_env is None) or (self.LQ_env is None):\n                self._init_lmdb()\n\n        scale = self.opt['scale']\n        # print(scale)\n        N_frames = self.opt['N_frames']\n        GT_size = self.opt['GT_size']\n        key = self.paths_GT[index]\n        name_a, name_b = key.split('_')\n\n        center_frame_idx = random.randint(2,6) # 2<= index <=6\n\n        #### determine the neighbor frames\n        interval = random.choice(self.interval_list)\n        if self.opt['border_mode']:\n            direction = 1  # 1: forward; 0: backward\n            if self.random_reverse and random.random() < 0.5:\n                direction = random.choice([0, 1])\n            if center_frame_idx + interval * (N_frames - 1) > 7:\n                direction = 0\n            elif center_frame_idx - interval * (N_frames - 1) < 1:\n                direction = 1\n            # get the neighbor list\n            if direction == 1:\n                neighbor_list = list(\n                    range(center_frame_idx, center_frame_idx + interval * N_frames, interval))\n            else:\n                neighbor_list = list(\n                    range(center_frame_idx, center_frame_idx - interval * N_frames, -interval))\n        else:\n            # ensure not exceeding the borders\n            while (center_frame_idx + self.half_N_frames * interval >\n                   7) or (center_frame_idx - self.half_N_frames * interval < 1):\n                center_frame_idx = random.randint(2, 6)\n            # get the neighbor list\n            neighbor_list = list(\n                range(center_frame_idx - self.half_N_frames * interval,\n                      center_frame_idx + self.half_N_frames * interval + 1, interval))\n            if self.random_reverse and random.random() < 0.5:\n                neighbor_list.reverse()\n\n        self.LQ_frames_list = []\n        for i in self.LR_index_list:\n            self.LQ_frames_list.append(neighbor_list[i])\n\n        assert len(\n            neighbor_list) == self.opt['N_frames'], 'Wrong length of neighbor list: {}'.format(\n                len(neighbor_list))\n\n        #### get the GT image (as the center frame)\n        img_GT_l = []\n        for v in neighbor_list:\n            if self.data_type == 'mc':\n                img_GT = self._read_img_mc_BGR(self.GT_root, name_a, name_b, '{}.png'.format(v))\n                img_GT = img_GT.astype(np.float32) / 255.\n            elif self.data_type == 'lmdb':\n                img_GT = util.read_img(self.GT_env, key + '_{}'.format(v), (3, 256, 448))\n            else:               \n                img_GT = util.read_img(None, osp.join(self.GT_root, name_a, name_b, 'im{}.png'.format(v)))\n            img_GT_l.append(img_GT)\n                \n       #### get LQ images\n        LQ_size_tuple = (3, 64, 112) if self.LR_input else (3, 256, 448)\n        img_LQ_l = []\n        for v in self.LQ_frames_list:\n            if self.data_type == 'mc':\n                img_LQ = self._read_img_mc(\n                    osp.join(self.LQ_root, name_a, name_b, '/{}.png'.format(v)))\n                img_LQ = img_LQ.astype(np.float32) / 255.\n            elif self.data_type == 'lmdb':\n                img_LQ = util.read_img(self.LQ_env, key + '_{}'.format(v), LQ_size_tuple)\n            else:\n                img_LQ = util.read_img(None,\n                                       osp.join(self.LQ_root, name_a, name_b, 'im{}.png'.format(v)))\n            img_LQ_l.append(img_LQ)\n\n        if self.opt['phase'] == 'train':\n            C, H, W = LQ_size_tuple  # LQ size\n            # randomly crop\n            if self.LR_input:\n                LQ_size = GT_size // scale\n                rnd_h = random.randint(0, max(0, H - LQ_size))\n                rnd_w = random.randint(0, max(0, W - LQ_size))\n                img_LQ_l = [v[rnd_h:rnd_h + LQ_size, rnd_w:rnd_w + LQ_size, :] for v in img_LQ_l]\n                rnd_h_HR, rnd_w_HR = int(rnd_h * scale), int(rnd_w * scale)\n                img_GT_l = [v[rnd_h_HR:rnd_h_HR + GT_size, rnd_w_HR:rnd_w_HR + GT_size, :] for v in img_GT_l]\n            else:\n                rnd_h = random.randint(0, max(0, H - GT_size))\n                rnd_w = random.randint(0, max(0, W - GT_size))\n                img_LQ_l = [v[rnd_h:rnd_h + GT_size, rnd_w:rnd_w + GT_size, :] for v in img_LQ_l]\n                img_GT_l = [v[rnd_h:rnd_h + GT_size, rnd_w:rnd_w + GT_size, :] for v in img_GT_l]\n\n            # augmentation - flip, rotate\n            img_LQ_l = img_LQ_l + img_GT_l\n            rlt = util.augment(img_LQ_l, self.opt['use_flip'], self.opt['use_rot'])\n            img_LQ_l = rlt[0:-N_frames]\n            img_GT_l = rlt[-N_frames:]\n\n        # stack LQ images to NHWC, N is the frame number\n        img_LQs = np.stack(img_LQ_l, axis=0)\n        img_GTs = np.stack(img_GT_l, axis=0)\n        # BGR to RGB, HWC to CHW, numpy to tensor\n        img_GTs = img_GTs[:, :, :, [2, 1, 0]]\n        img_LQs = img_LQs[:, :, :, [2, 1, 0]]\n        img_GTs = torch.from_numpy(np.ascontiguousarray(np.transpose(img_GTs, (0, 3, 1, 2)))).float()\n        img_LQs = torch.from_numpy(np.ascontiguousarray(np.transpose(img_LQs,\n                                                                     (0, 3, 1, 2)))).float()\n        return {'LQs': img_LQs, 'GT': img_GTs, 'key': key}\n\n    def __len__(self):\n        return len(self.paths_GT)\n"""
codes/data/__init__.py,4,"b""'''create dataset and dataloader'''\nimport logging\nimport torch\nimport torch.utils.data\n\n\ndef create_dataloader(dataset, dataset_opt, opt, sampler):\n    phase = dataset_opt['phase']\n    if phase == 'train':\n        if opt['dist']:\n            world_size = torch.distributed.get_world_size()\n            num_workers = dataset_opt['n_workers']\n            assert dataset_opt['batch_size'] % world_size == 0\n            batch_size = dataset_opt['batch_size'] // world_size\n            shuffle = False\n        else:\n            num_workers = dataset_opt['n_workers'] * len(opt['gpu_ids'])\n            batch_size = dataset_opt['batch_size']\n            shuffle = True\n        return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n                                           num_workers=num_workers, sampler=sampler, drop_last=True,\n                                           pin_memory=False)\n    else:\n        return torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1,\n                                           pin_memory=True)\n\n\ndef create_dataset(dataset_opt):\n    mode = dataset_opt['mode']\n    if mode == 'Vimeo7':\n        from data.Vimeo7_dataset import Vimeo7Dataset as D\n    else:\n        raise NotImplementedError('Dataset [{:s}] is not recognized.'.format(mode))\n    dataset = D(dataset_opt)\n\n    logger = logging.getLogger('base')\n    logger.info('Dataset [{:s} - {:s}] is created.'.format(dataset.__class__.__name__,\n                                                           dataset_opt['name']))\n    return dataset\n"""
codes/data/data_sampler.py,6,"b'""""""\nModified from torch.utils.data.distributed.DistributedSampler\nSupport enlarging the dataset for *iter-oriented* training, for saving time when restart the\ndataloader after each epoch\n""""""\nimport math\nimport torch\nfrom torch.utils.data.sampler import Sampler\nimport torch.distributed as dist\n\n\nclass DistIterSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n\n    .. note::\n        Dataset is assumed to be of constant size.\n\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self, dataset, num_replicas=None, rank=None, ratio=100):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * ratio / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        g = torch.Generator()\n        g.manual_seed(self.epoch)\n        indices = torch.randperm(self.total_size, generator=g).tolist()\n\n        dsize = len(self.dataset)\n        indices = [v % dsize for v in indices]\n\n        # subsample\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n'"
codes/data/util.py,15,"b""import os\nimport pickle\nimport random\nimport numpy as np\nimport cv2\nimport math\nimport torch\n\n####################\n# Files & IO\n####################\n\n###################### get image path list ######################\nIMG_EXTENSIONS = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP']\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef _get_paths_from_images(path):\n    '''get image path list from image folder'''\n    assert os.path.isdir(path), '{:s} is not a valid directory'.format(path)\n    images = []\n    for dirpath, _, fnames in sorted(os.walk(path)):\n        for fname in sorted(fnames):\n            if is_image_file(fname):\n                img_path = os.path.join(dirpath, fname)\n                images.append(img_path)\n    assert images, '{:s} has no valid image file'.format(path)\n    return images\n\n\ndef _get_paths_from_lmdb(dataroot):\n    '''get image path list from lmdb meta info'''\n    meta_info = pickle.load(open(os.path.join(dataroot, 'meta_info.pkl'), 'rb'))\n    paths = meta_info['keys']\n    sizes = meta_info['resolution']\n    if len(sizes) == 1:\n        sizes = sizes * len(paths)\n    return paths, sizes\n\n\ndef get_image_paths(data_type, dataroot):\n    '''get image path list\n    support lmdb or image files'''\n    paths, sizes = None, None\n    if dataroot is not None:\n        if data_type == 'lmdb':\n            paths, sizes = _get_paths_from_lmdb(dataroot)\n        elif data_type == 'img':\n            paths = sorted(_get_paths_from_images(dataroot))\n        else:\n            raise NotImplementedError('data_type [{:s}] is not recognized.'.format(data_type))\n    return paths, sizes\n\n\n###################### read images ######################\ndef _read_img_lmdb(env, key, size):\n    '''read image from lmdb with key (w/ and w/o fixed size)\n    size: (C, H, W) tuple'''\n    with env.begin(write=False) as txn:\n        buf = txn.get(key.encode('ascii'))\n    img_flat = np.frombuffer(buf, dtype=np.uint8)\n    C, H, W = size\n    img = img_flat.reshape(H, W, C)\n    return img\n\n\ndef read_img(env, path, size=None):\n    '''read image by cv2 or from lmdb\n    return: Numpy float32, HWC, BGR, [0,1]'''\n    if env is None:  # img\n        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n    else:\n        img = _read_img_lmdb(env, path, size)\n    img = img.astype(np.float32) / 255.\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n    # some images have 4 channels\n    if img.shape[2] > 3:\n        img = img[:, :, :3]\n    return img\n\n\n####################\n# image processing\n# process on numpy image\n####################\n\n\ndef augment(img_list, hflip=True, rot=True):\n    # horizontal flip OR rotate\n    hflip = hflip and random.random() < 0.5\n    vflip = rot and random.random() < 0.5\n    rot90 = rot and random.random() < 0.5\n\n    def _augment(img):\n        if hflip:\n            img = img[:, ::-1, :]\n        if vflip:\n            img = img[::-1, :, :]\n        if rot90:\n            img = img.transpose(1, 0, 2)\n        return img\n\n    return [_augment(img) for img in img_list]\n\ndef channel_convert(in_c, tar_type, img_list):\n    # conversion among BGR, gray and y\n    if in_c == 3 and tar_type == 'gray':  # BGR to gray\n        gray_list = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in img_list]\n        return [np.expand_dims(img, axis=2) for img in gray_list]\n    elif in_c == 3 and tar_type == 'y':  # BGR to y\n        y_list = [bgr2ycbcr(img, only_y=True) for img in img_list]\n        return [np.expand_dims(img, axis=2) for img in y_list]\n    elif in_c == 1 and tar_type == 'RGB':  # gray/y to BGR\n        return [cv2.cvtColor(img, cv2.COLOR_GRAY2BGR) for img in img_list]\n    else:\n        return img_list\n\n\ndef rgb2ycbcr(img, only_y=True):\n    '''same as matlab rgb2ycbcr\n    only_y: only return Y channel\n    Input:\n        uint8, [0, 255]\n        float, [0, 1]\n    '''\n    in_img_type = img.dtype\n    img.astype(np.float32)\n    if in_img_type != np.uint8:\n        img *= 255.\n    # convert\n    if only_y:\n        rlt = np.dot(img, [65.481, 128.553, 24.966]) / 255.0 + 16.0\n    else:\n        rlt = np.matmul(img, [[65.481, -37.797, 112.0], [128.553, -74.203, -93.786],\n                              [24.966, 112.0, -18.214]]) / 255.0 + [16, 128, 128]\n    if in_img_type == np.uint8:\n        rlt = rlt.round()\n    else:\n        rlt /= 255.\n    return rlt.astype(in_img_type)\n\n\ndef bgr2ycbcr(img, only_y=True):\n    '''bgr version of rgb2ycbcr\n    only_y: only return Y channel\n    Input:\n        uint8, [0, 255]\n        float, [0, 1]\n    '''\n    in_img_type = img.dtype\n    img.astype(np.float32)\n    if in_img_type != np.uint8:\n        img *= 255.\n    # convert\n    if only_y:\n        rlt = np.dot(img, [24.966, 128.553, 65.481]) / 255.0 + 16.0\n    else:\n        rlt = np.matmul(img, [[24.966, 112.0, -18.214], [128.553, -74.203, -93.786],\n                              [65.481, -37.797, 112.0]]) / 255.0 + [16, 128, 128]\n    if in_img_type == np.uint8:\n        rlt = rlt.round()\n    else:\n        rlt /= 255.\n    return rlt.astype(in_img_type)\n\n\ndef ycbcr2rgb(img):\n    '''same as matlab ycbcr2rgb\n    Input:\n        uint8, [0, 255]\n        float, [0, 1]\n    '''\n    in_img_type = img.dtype\n    img.astype(np.float32)\n    if in_img_type != np.uint8:\n        img *= 255.\n    # convert\n    rlt = np.matmul(img, [[0.00456621, 0.00456621, 0.00456621], [0, -0.00153632, 0.00791071],\n                          [0.00625893, -0.00318811, 0]]) * 255.0 + [-222.921, 135.576, -276.836]\n    if in_img_type == np.uint8:\n        rlt = rlt.round()\n    else:\n        rlt /= 255.\n    return rlt.astype(in_img_type)\n\n\ndef modcrop(img_in, scale):\n    # img_in: Numpy, HWC or HW\n    img = np.copy(img_in)\n    if img.ndim == 2:\n        H, W = img.shape\n        H_r, W_r = H % scale, W % scale\n        img = img[:H - H_r, :W - W_r]\n    elif img.ndim == 3:\n        H, W, C = img.shape\n        H_r, W_r = H % scale, W % scale\n        img = img[:H - H_r, :W - W_r, :]\n    else:\n        raise ValueError('Wrong img ndim: [{:d}].'.format(img.ndim))\n    return img\n\ndef cubic(x):\n    absx = torch.abs(x)\n    absx2 = absx**2\n    absx3 = absx**3\n    return (1.5 * absx3 - 2.5 * absx2 + 1) * (\n        (absx <= 1).type_as(absx)) + (-0.5 * absx3 + 2.5 * absx2 - 4 * absx + 2) * ((\n            (absx > 1) * (absx <= 2)).type_as(absx))\n\ndef calculate_weights_indices(in_length, out_length, scale, kernel, kernel_width, antialiasing):\n    if (scale < 1) and (antialiasing):\n        # Use a modified kernel to simultaneously interpolate and antialias- larger kernel width\n        kernel_width = kernel_width / scale\n\n    # Output-space coordinates\n    x = torch.linspace(1, out_length, out_length)\n\n    # Input-space coordinates. Calculate the inverse mapping such that 0.5\n    # in output space maps to 0.5 in input space, and 0.5+scale in output\n    # space maps to 1.5 in input space.\n    u = x / scale + 0.5 * (1 - 1 / scale)\n\n    # What is the left-most pixel that can be involved in the computation?\n    left = torch.floor(u - kernel_width / 2)\n\n    # What is the maximum number of pixels that can be involved in the\n    # computation?  Note: it's OK to use an extra pixel here; if the\n    # corresponding weights are all zero, it will be eliminated at the end\n    # of this function.\n    P = math.ceil(kernel_width) + 2\n\n    # The indices of the input pixels involved in computing the k-th output\n    # pixel are in row k of the indices matrix.\n    indices = left.view(out_length, 1).expand(out_length, P) + torch.linspace(0, P - 1, P).view(\n        1, P).expand(out_length, P)\n\n    # The weights used to compute the k-th output pixel are in row k of the\n    # weights matrix.\n    distance_to_center = u.view(out_length, 1).expand(out_length, P) - indices\n    # apply cubic kernel\n    if (scale < 1) and (antialiasing):\n        weights = scale * cubic(distance_to_center * scale)\n    else:\n        weights = cubic(distance_to_center)\n    # Normalize the weights matrix so that each row sums to 1.\n    weights_sum = torch.sum(weights, 1).view(out_length, 1)\n    weights = weights / weights_sum.expand(out_length, P)\n\n    # If a column in weights is all zero, get rid of it. only consider the first and last column.\n    weights_zero_tmp = torch.sum((weights == 0), 0)\n    if not math.isclose(weights_zero_tmp[0], 0, rel_tol=1e-6):\n        indices = indices.narrow(1, 1, P - 2)\n        weights = weights.narrow(1, 1, P - 2)\n    if not math.isclose(weights_zero_tmp[-1], 0, rel_tol=1e-6):\n        indices = indices.narrow(1, 0, P - 2)\n        weights = weights.narrow(1, 0, P - 2)\n    weights = weights.contiguous()\n    indices = indices.contiguous()\n    sym_len_s = -indices.min() + 1\n    sym_len_e = indices.max() - in_length\n    indices = indices + sym_len_s - 1\n    return weights, indices, int(sym_len_s), int(sym_len_e)\n\ndef imresize_np(img, scale, antialiasing=True):\n    # Now the scale should be the same for H and W\n    # input: img: Numpy, HWC BGR [0,1]\n    # output: HWC BGR [0,1] w/o round\n    img = torch.from_numpy(img)\n\n    in_H, in_W, in_C = img.size()\n    _, out_H, out_W = in_C, math.ceil(in_H * scale), math.ceil(in_W * scale)\n    kernel_width = 4\n    kernel = 'cubic'\n\n    # Return the desired dimension order for performing the resize.  The\n    # strategy is to perform the resize first along the dimension with the\n    # smallest scale factor.\n    # Now we do not support this.\n\n    # get weights and indices\n    weights_H, indices_H, sym_len_Hs, sym_len_He = calculate_weights_indices(\n        in_H, out_H, scale, kernel, kernel_width, antialiasing)\n    weights_W, indices_W, sym_len_Ws, sym_len_We = calculate_weights_indices(\n        in_W, out_W, scale, kernel, kernel_width, antialiasing)\n    # process H dimension\n    # symmetric copying\n    img_aug = torch.FloatTensor(in_H + sym_len_Hs + sym_len_He, in_W, in_C)\n    img_aug.narrow(0, sym_len_Hs, in_H).copy_(img)\n\n    sym_patch = img[:sym_len_Hs, :, :]\n    inv_idx = torch.arange(sym_patch.size(0) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(0, inv_idx)\n    img_aug.narrow(0, 0, sym_len_Hs).copy_(sym_patch_inv)\n\n    sym_patch = img[-sym_len_He:, :, :]\n    inv_idx = torch.arange(sym_patch.size(0) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(0, inv_idx)\n    img_aug.narrow(0, sym_len_Hs + in_H, sym_len_He).copy_(sym_patch_inv)\n\n    out_1 = torch.FloatTensor(out_H, in_W, in_C)\n    kernel_width = weights_H.size(1)\n    for i in range(out_H):\n        idx = int(indices_H[i][0])\n        out_1[i, :, 0] = img_aug[idx:idx + kernel_width, :, 0].transpose(0, 1).mv(weights_H[i])\n        out_1[i, :, 1] = img_aug[idx:idx + kernel_width, :, 1].transpose(0, 1).mv(weights_H[i])\n        out_1[i, :, 2] = img_aug[idx:idx + kernel_width, :, 2].transpose(0, 1).mv(weights_H[i])\n\n    # process W dimension\n    # symmetric copying\n    out_1_aug = torch.FloatTensor(out_H, in_W + sym_len_Ws + sym_len_We, in_C)\n    out_1_aug.narrow(1, sym_len_Ws, in_W).copy_(out_1)\n\n    sym_patch = out_1[:, :sym_len_Ws, :]\n    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n    out_1_aug.narrow(1, 0, sym_len_Ws).copy_(sym_patch_inv)\n\n    sym_patch = out_1[:, -sym_len_We:, :]\n    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n    out_1_aug.narrow(1, sym_len_Ws + in_W, sym_len_We).copy_(sym_patch_inv)\n\n    out_2 = torch.FloatTensor(out_H, out_W, in_C)\n    kernel_width = weights_W.size(1)\n    for i in range(out_W):\n        idx = int(indices_W[i][0])\n        out_2[:, i, 0] = out_1_aug[:, idx:idx + kernel_width, 0].mv(weights_W[i])\n        out_2[:, i, 1] = out_1_aug[:, idx:idx + kernel_width, 1].mv(weights_W[i])\n        out_2[:, i, 2] = out_1_aug[:, idx:idx + kernel_width, 2].mv(weights_W[i])\n\n    return out_2.numpy()\n"""
codes/data_scripts/create_lmdb_mp.py,0,"b'\'\'\'create lmdb files for Vimeo90K-7 frames training dataset (multiprocessing)\nWill read all the images to the memory\n\'\'\'\n\nimport os,sys\nimport os.path as osp\nimport glob\nimport pickle\nfrom multiprocessing import Pool\nimport numpy as np\nimport lmdb\nimport cv2\ntry:\n    sys.path.append(osp.dirname(osp.dirname(osp.abspath(__file__))))\n    import data.util as data_util\n    import utils.util as util\nexcept ImportError:\n    pass\n\n\ndef reading_image_worker(path, key):\n    \'\'\'worker for reading images\'\'\'\n    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n    return (key, img)\n\ndef vimeo7():\n    \'\'\'create lmdb for the Vimeo90K-7 frames dataset, each image with fixed size\n    GT: [3, 256, 448]\n        Only need the 4th frame currently, e.g., 00001_0001_4\n    LR: [3, 64, 112]\n        With 1st - 7th frames, e.g., 00001_0001_1, ..., 00001_0001_7\n    key:\n        Use the folder and subfolder names, w/o the frame index, e.g., 00001_0001\n    \'\'\'\n    #### configurations\n    mode = \'GT\'  # GT | LR\n    batch = 3000 # TODO: depending on your mem size\n    if mode == \'GT\':\n        img_folder = \'/data/datasets/SR/vimeo_septuplet/sequences/train\'\n        lmdb_save_path = \'/data/datasets/SR/vimeo_septuplet/vimeo7_train_GT.lmdb\'\n        txt_file = \'/data/datasets/SR/vimeo_septuplet/sep_trainlist.txt\'\n        H_dst, W_dst = 256, 448\n    elif mode == \'LR\':\n        img_folder = \'/data/datasets/SR/vimeo_septuplet/sequences_LR/LR/x4/train\'\n        lmdb_save_path = \'/data/datasets/SR/vimeo_septuplet/vimeo7_train_LR7.lmdb\'\n        txt_file = \'/data/datasets/SR/vimeo_septuplet/sep_trainlist.txt\'\n        H_dst, W_dst = 64, 112\n    n_thread = 40\n    ########################################################\n    if not lmdb_save_path.endswith(\'.lmdb\'):\n        raise ValueError(""lmdb_save_path must end with \\\'lmdb\\\'."")\n    #### whether the lmdb file exist\n    if osp.exists(lmdb_save_path):\n        print(\'Folder [{:s}] already exists. Exit...\'.format(lmdb_save_path))\n        sys.exit(1)\n\n    #### read all the image paths to a list\n    print(\'Reading image path list ...\')\n    with open(txt_file) as f:\n        train_l = f.readlines()\n        train_l = [v.strip() for v in train_l]\n    all_img_list = []\n    keys = []\n    for line in train_l:\n        folder = line.split(\'/\')[0]\n        sub_folder = line.split(\'/\')[1]\n        file_l = glob.glob(osp.join(img_folder, folder, sub_folder) + \'/*\')\n        all_img_list.extend(file_l)\n        for j in range(7):\n            keys.append(\'{}_{}_{}\'.format(folder, sub_folder, j + 1))\n    all_img_list = sorted(all_img_list)\n    keys = sorted(keys)\n    if mode == \'GT\': \n        all_img_list = [v for v in all_img_list if v.endswith(\'.png\')]\n        keys = [v for v in keys]\n    print(\'Calculating the total size of images...\')\n    data_size = sum(os.stat(v).st_size for v in all_img_list)\n\n    #### read all images to memory (multiprocessing)\n    print(\'Read images with multiprocessing, #thread: {} ...\'.format(n_thread))\n    \n    #### create lmdb environment\n    env = lmdb.open(lmdb_save_path, map_size=data_size * 10)\n    txn = env.begin(write=True)  # txn is a Transaction object\n\n    #### write data to lmdb\n    pbar = util.ProgressBar(len(all_img_list))\n\n    i = 0\n    for path, key in zip(all_img_list, keys):\n        pbar.update(\'Write {}\'.format(key))\n        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n        key_byte = key.encode(\'ascii\')\n        H, W, C = img.shape  # fixed shape\n        assert H == H_dst and W == W_dst and C == 3, \'different shape.\'\n        txn.put(key_byte, img)\n        i += 1\n        if  i % batch == 1:\n            txn.commit()\n            txn = env.begin(write=True)\n\n    txn.commit()\n    env.close()\n    print(\'Finish reading and writing {} images.\'.format(len(all_img_list)))\n            \n    print(\'Finish writing lmdb.\')\n\n    #### create meta information\n    meta_info = {}\n    if mode == \'GT\':\n        meta_info[\'name\'] = \'Vimeo7_train_GT\'\n    elif mode == \'LR\':\n        meta_info[\'name\'] = \'Vimeo7_train_LR7\'\n    meta_info[\'resolution\'] = \'{}_{}_{}\'.format(3, H_dst, W_dst)\n    key_set = set()\n    for key in keys:\n        a, b, _ = key.split(\'_\')\n        key_set.add(\'{}_{}\'.format(a, b))\n    meta_info[\'keys\'] = key_set\n    pickle.dump(meta_info, open(osp.join(lmdb_save_path, \'Vimeo7_train_keys.pkl\'), ""wb""))\n    print(\'Finish creating lmdb meta info.\')\n\ndef test_lmdb(dataroot, dataset=\'vimeo7\'):\n    env = lmdb.open(dataroot, readonly=True, lock=False, readahead=False, meminit=False)\n    meta_info = pickle.load(open(osp.join(dataroot, \'meta_info.pkl\'), ""rb""))\n    print(\'Name: \', meta_info[\'name\'])\n    print(\'Resolution: \', meta_info[\'resolution\'])\n    print(\'# keys: \', len(meta_info[\'keys\']))\n    # read one image\n    if dataset == \'vimeo7\':\n        key = \'00001_0001_4\'\n    else:\n        raise NameError(\'Please check the filename format.\')\n    print(\'Reading {} for test.\'.format(key))\n    with env.begin(write=False) as txn:\n        buf = txn.get(key.encode(\'ascii\'))\n    img_flat = np.frombuffer(buf, dtype=np.uint8)\n    C, H, W = [int(s) for s in meta_info[\'resolution\'].split(\'_\')]\n    img = img_flat.reshape(H, W, C)\n    cv2.imwrite(\'test.png\', img)\n\n\nif __name__ == ""__main__"":\n    vimeo7()\n    test_lmdb(\'/data/datasets/SR/vimeo_septuplet/vimeo7_train_GT.lmdb\', \'vimeo7\')\n'"
codes/data_scripts/generate_mod_LR_bic.py,0,"b'import os\nimport sys\nimport cv2\nimport numpy as np\n\ntry:\n    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n    from data.util import imresize_np\nexcept ImportError:\n    pass\n\n\ndef generate_mod_LR_bic(up_scale, sourcedir, savedir):\n    # params: upscale factor, input directory, output directory\n    saveHRpath = os.path.join(savedir, \'HR\', \'x\' + str(up_scale))\n    saveLRpath = os.path.join(savedir, \'LR\', \'x\' + str(up_scale))\n    saveBicpath = os.path.join(savedir, \'Bic\', \'x\' + str(up_scale))\n\n    if not os.path.isdir(sourcedir):\n        print(\'Error: No source data found\')\n        exit(0)\n    if not os.path.isdir(savedir):\n        os.mkdir(savedir)\n\n    if not os.path.isdir(os.path.join(savedir, \'HR\')):\n        os.mkdir(os.path.join(savedir, \'HR\'))\n    if not os.path.isdir(os.path.join(savedir, \'LR\')):\n        os.mkdir(os.path.join(savedir, \'LR\'))\n    if not os.path.isdir(os.path.join(savedir, \'Bic\')):\n        os.mkdir(os.path.join(savedir, \'Bic\'))\n\n    if not os.path.isdir(saveHRpath):\n        os.mkdir(saveHRpath)\n    else:\n        print(\'It will cover \' + str(saveHRpath))\n\n    if not os.path.isdir(saveLRpath):\n        os.mkdir(saveLRpath)\n    else:\n        print(\'It will cover \' + str(saveLRpath))\n\n    if not os.path.isdir(saveBicpath):\n        os.mkdir(saveBicpath)\n    else:\n        print(\'It will cover \' + str(saveBicpath))\n\n    filepaths = [f for f in os.listdir(sourcedir) if f.endswith(\'.png\')]\n    num_files = len(filepaths)\n\n    # prepare data with augementation\n    for i in range(num_files):\n        filename = filepaths[i]\n        print(\'No.{} -- Processing {}\'.format(i, filename))\n        # read image\n        image = cv2.imread(os.path.join(sourcedir, filename))\n\n        width = int(np.floor(image.shape[1] / up_scale))\n        height = int(np.floor(image.shape[0] / up_scale))\n        # modcrop\n        if len(image.shape) == 3:\n            image_HR = image[0:up_scale * height, 0:up_scale * width, :]\n        else:\n            image_HR = image[0:up_scale * height, 0:up_scale * width]\n        # LR\n        image_LR = imresize_np(image_HR, 1 / up_scale, True)\n        # bic\n        image_Bic = imresize_np(image_LR, up_scale, True)\n\n        cv2.imwrite(os.path.join(saveHRpath, filename), image_HR)\n        cv2.imwrite(os.path.join(saveLRpath, filename), image_LR)\n        cv2.imwrite(os.path.join(saveBicpath, filename), image_Bic)\n\n\nif __name__ == ""__main__"":\n    generate_mod_LR_bic(4, \'inPath\', \'outPath\')\n'"
codes/data_scripts/sep_vimeo_list.py,0,"b'import os, shutil\n\nif __name__ ==  ""__main__"":\n    inPath = \'/data/datasets/SR/vimeo_septuplet/sequences/\'\n    outPath = \'/data/datasets/SR/vimeo_septuplet/sequences/test/\'\n    guide = \'/data/datasets/SR/vimeo_septuplet/sep_testlist.txt\'\n    \n    f = open(guide, ""r"")\n    lines = f.readlines()\n    \n    if not os.path.isdir(outPath):\n        os.mkdir(outPath)\n\n    for l in lines:\n        line = l.replace(\'\\n\',\'\')\n        this_folder = os.path.join(inPath, line)\n        dest_folder = os.path.join(outPath, line)\n        print(this_folder)\n        shutil.copytree(this_folder, dest_folder)\n    print(\'Done\')\n'"
codes/models/VideoSR_base_model.py,6,"b""import logging\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parallel import DataParallel, DistributedDataParallel\nimport models.networks as networks\nimport models.lr_scheduler as lr_scheduler\nfrom .base_model import BaseModel\nfrom models.modules.loss import CharbonnierLoss, LapLoss\n\nlogger = logging.getLogger('base')\n\n\nclass VideoSRBaseModel(BaseModel):\n    def __init__(self, opt):\n        super(VideoSRBaseModel, self).__init__(opt)\n\n        if opt['dist']:\n            self.rank = torch.distributed.get_rank()\n        else:\n            self.rank = -1  # non dist training\n        train_opt = opt['train']\n\n        # define network and load pretrained models\n        self.netG = networks.define_G(opt).to(self.device)\n\n        if opt['dist']:\n            self.netG = DistributedDataParallel(self.netG, device_ids=[torch.cuda.current_device()])\n        else:\n            self.netG = DataParallel(self.netG)\n        # print network\n        self.print_network()\n        self.load()\n\n        if self.is_train:\n            self.netG.train()\n\n            #### loss\n            loss_type = train_opt['pixel_criterion']\n            if loss_type == 'l1':\n                self.cri_pix = nn.L1Loss(reduction='sum').to(self.device)\n            elif loss_type == 'l2':\n                self.cri_pix = nn.MSELoss(reduction='sum').to(self.device)\n            elif loss_type == 'cb':\n                self.cri_pix = CharbonnierLoss().to(self.device)\n            elif loss_type == 'lp':\n                self.cri_pix = LapLoss(max_levels=5).to(self.device)\n            else:\n                raise NotImplementedError('Loss type [{:s}] is not recognized.'.format(loss_type))\n            self.l_pix_w = train_opt['pixel_weight']\n\n            #### optimizers\n            wd_G = train_opt['weight_decay_G'] if train_opt['weight_decay_G'] else 0\n            optim_params = []\n            for k, v in self.netG.named_parameters():\n                if v.requires_grad:\n                    optim_params.append(v)\n                else:\n                    if self.rank <= 0:\n                        logger.warning('Params [{:s}] will not optimize.'.format(k))\n\n            self.optimizer_G = torch.optim.Adam(optim_params, lr=train_opt['lr_G'],\n                                                weight_decay=wd_G,\n                                                betas=(train_opt['beta1'], train_opt['beta2']))\n            self.optimizers.append(self.optimizer_G)\n            #### schedulers\n            if train_opt['lr_scheme'] == 'MultiStepLR':\n                for optimizer in self.optimizers:\n                    self.schedulers.append(\n                        lr_scheduler.MultiStepLR_Restart(optimizer, train_opt['lr_steps'],\n                                                         restarts=train_opt['restarts'],\n                                                         weights=train_opt['restart_weights'],\n                                                         gamma=train_opt['lr_gamma'],\n                                                         clear_state=train_opt['clear_state']))\n            elif train_opt['lr_scheme'] == 'CosineAnnealingLR_Restart':\n                for optimizer in self.optimizers:\n                    self.schedulers.append(\n                        lr_scheduler.CosineAnnealingLR_Restart(\n                            optimizer, train_opt['T_period'], eta_min=train_opt['eta_min'],\n                            restarts=train_opt['restarts'], weights=train_opt['restart_weights']))\n            else:\n                raise NotImplementedError()\n\n            self.log_dict = OrderedDict()\n\n    def feed_data(self, data, need_GT=True):\n        self.var_L = data['LQs'].to(self.device)\n        if need_GT:\n            self.real_H = data['GT'].to(self.device)\n\n    def set_params_lr_zero(self):\n        # fix normal module\n        self.optimizers[0].param_groups[0]['lr'] = 0\n\n    def optimize_parameters(self, step):\n        self.optimizer_G.zero_grad()\n        self.fake_H = self.netG(self.var_L)\n        l_pix = self.l_pix_w * self.cri_pix(self.fake_H, self.real_H)\n        l_pix.backward()\n        self.optimizer_G.step()\n\n        # set log\n        self.log_dict['l_pix'] = l_pix.item()\n\n    def test(self):\n        self.netG.eval()\n        with torch.no_grad():\n            self.fake_H = self.netG(self.var_L)\n        self.netG.train()\n\n    def get_current_log(self):\n        return self.log_dict\n\n    def get_current_visuals(self, need_GT=True):\n        out_dict = OrderedDict()\n        out_dict['LQ'] = self.var_L.detach()[0].float().cpu()\n        out_dict['restore'] = self.fake_H.detach()[0].float().cpu()\n        if need_GT:\n            out_dict['GT'] = self.real_H.detach()[0].float().cpu()\n        return out_dict\n\n    def print_network(self):\n        s, n = self.get_network_description(self.netG)\n        if isinstance(self.netG, nn.DataParallel):\n            net_struc_str = '{} - {}'.format(self.netG.__class__.__name__,\n                                             self.netG.module.__class__.__name__)\n        else:\n            net_struc_str = '{}'.format(self.netG.__class__.__name__)\n        if self.rank <= 0:\n            logger.info('Network G structure: {}, with parameters: {:,d}'.format(net_struc_str, n))\n            logger.info(s)\n\n    def load(self):\n        load_path_G = self.opt['path']['pretrain_model_G']\n        if load_path_G is not None:\n            logger.info('Loading model for G [{:s}] ...'.format(load_path_G))\n            self.load_network(load_path_G, self.netG, self.opt['path']['strict_load'])\n\n    def save(self, iter_label):\n        self.save_network(self.netG, 'G', iter_label)\n"""
codes/models/__init__.py,0,"b""import logging\nlogger = logging.getLogger('base')\n\n\ndef create_model(opt):\n    model = opt['model']\n    if model == 'VideoSR_base':\n        from .VideoSR_base_model import VideoSRBaseModel as M\n    else:\n        raise NotImplementedError('Model [{:s}] not recognized.'.format(model))\n    m = M(opt)\n    logger.info('Model [{:s}] is created.'.format(m.__class__.__name__))\n    return m\n"""
codes/models/base_model.py,6,"b""import os\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parallel import DistributedDataParallel\n\n\nclass BaseModel():\n    def __init__(self, opt):\n        self.opt = opt\n        self.device = torch.device('cuda' if opt['gpu_ids'] is not None else 'cpu')\n        self.is_train = opt['is_train']\n        self.schedulers = []\n        self.optimizers = []\n\n    def feed_data(self, data):\n        pass\n\n    def optimize_parameters(self):\n        pass\n\n    def get_current_visuals(self):\n        pass\n\n    def get_current_losses(self):\n        pass\n\n    def print_network(self):\n        pass\n\n    def save(self, label):\n        pass\n\n    def load(self):\n        pass\n\n    def _set_lr(self, lr_groups_l):\n        ''' set learning rate for warmup,\n        lr_groups_l: list for lr_groups. each for a optimizer'''\n        for optimizer, lr_groups in zip(self.optimizers, lr_groups_l):\n            for param_group, lr in zip(optimizer.param_groups, lr_groups):\n                param_group['lr'] = lr\n\n    def _get_init_lr(self):\n        # get the initial lr, which is set by the scheduler\n        init_lr_groups_l = []\n        for optimizer in self.optimizers:\n            init_lr_groups_l.append([v['initial_lr'] for v in optimizer.param_groups])\n        return init_lr_groups_l\n\n    def update_learning_rate(self, cur_iter, warmup_iter=-1):\n        for scheduler in self.schedulers:\n            scheduler.step()\n        #### set up warm up learning rate\n        if cur_iter < warmup_iter:\n            # get initial lr for each group\n            init_lr_g_l = self._get_init_lr()\n            # modify warming-up learning rates\n            warm_up_lr_l = []\n            for init_lr_g in init_lr_g_l:\n                warm_up_lr_l.append([v / warmup_iter * cur_iter for v in init_lr_g])\n            # set learning rate\n            self._set_lr(warm_up_lr_l)\n\n    def get_current_learning_rate(self):\n        lr_l = []\n        for param_group in self.optimizers[0].param_groups:\n            lr_l.append(param_group['lr'])\n        return lr_l\n\n    def get_network_description(self, network):\n        '''Get the string and total parameters of the network'''\n        if isinstance(network, nn.DataParallel) or isinstance(network, DistributedDataParallel):\n            network = network.module\n        s = str(network)\n        n = sum(map(lambda x: x.numel(), network.parameters()))\n        return s, n\n\n    def save_network(self, network, network_label, iter_label):\n        save_filename = '{}_{}.pth'.format(iter_label, network_label)\n        save_path = os.path.join(self.opt['path']['models'], save_filename)\n        if isinstance(network, nn.DataParallel) or isinstance(network, DistributedDataParallel):\n            network = network.module\n        state_dict = network.state_dict()\n        for key, param in state_dict.items():\n            state_dict[key] = param.cpu()\n        torch.save(state_dict, save_path)\n\n    def load_network(self, load_path, network, strict=True):\n        if isinstance(network, nn.DataParallel) or isinstance(network, DistributedDataParallel):\n            network = network.module\n        load_net = torch.load(load_path)\n        load_net_clean = OrderedDict()  # remove unnecessary 'module.'\n        for k, v in load_net.items():\n            if k.startswith('module.'):\n                load_net_clean[k[7:]] = v\n            else:\n                load_net_clean[k] = v\n        network.load_state_dict(load_net_clean, strict=strict)\n\n    def save_training_state(self, epoch, iter_step):\n        '''Saves training state during training, which will be used for resuming'''\n        state = {'epoch': epoch, 'iter': iter_step, 'schedulers': [], 'optimizers': []}\n        for s in self.schedulers:\n            state['schedulers'].append(s.state_dict())\n        for o in self.optimizers:\n            state['optimizers'].append(o.state_dict())\n        save_filename = '{}.state'.format(iter_step)\n        save_path = os.path.join(self.opt['path']['training_state'], save_filename)\n        torch.save(state, save_path)\n\n    def resume_training(self, resume_state):\n        '''Resume the optimizers and schedulers for training'''\n        resume_optimizers = resume_state['optimizers']\n        resume_schedulers = resume_state['schedulers']\n        assert len(resume_optimizers) == len(self.optimizers), 'Wrong lengths of optimizers'\n        assert len(resume_schedulers) == len(self.schedulers), 'Wrong lengths of schedulers'\n        for i, o in enumerate(resume_optimizers):\n            self.optimizers[i].load_state_dict(o)\n        for i, s in enumerate(resume_schedulers):\n            self.schedulers[i].load_state_dict(s)\n"""
codes/models/lr_scheduler.py,2,"b'import math\nfrom collections import Counter\nfrom collections import defaultdict\nimport torch\nfrom torch.optim.lr_scheduler import _LRScheduler\n\n\nclass MultiStepLR_Restart(_LRScheduler):\n    def __init__(self, optimizer, milestones, restarts=None, weights=None, gamma=0.1,\n                 clear_state=False, last_epoch=-1):\n        self.milestones = Counter(milestones)\n        self.gamma = gamma\n        self.clear_state = clear_state\n        self.restarts = restarts if restarts else [0]\n        self.restart_weights = weights if weights else [1]\n        assert len(self.restarts) == len(\n            self.restart_weights), \'restarts and their weights do not match.\'\n        super(MultiStepLR_Restart, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch in self.restarts:\n            if self.clear_state:\n                self.optimizer.state = defaultdict(dict)\n            weight = self.restart_weights[self.restarts.index(self.last_epoch)]\n            return [group[\'initial_lr\'] * weight for group in self.optimizer.param_groups]\n        if self.last_epoch not in self.milestones:\n            return [group[\'lr\'] for group in self.optimizer.param_groups]\n        return [\n            group[\'lr\'] * self.gamma**self.milestones[self.last_epoch]\n            for group in self.optimizer.param_groups\n        ]\n\n\nclass CosineAnnealingLR_Restart(_LRScheduler):\n    def __init__(self, optimizer, T_period, restarts=None, weights=None, eta_min=0, last_epoch=-1):\n        self.T_period = T_period\n        self.T_max = self.T_period[0]  # current T period\n        self.eta_min = eta_min\n        self.restarts = restarts if restarts else [0]\n        self.restart_weights = weights if weights else [1]\n        self.last_restart = 0\n        assert len(self.restarts) == len(\n            self.restart_weights), \'restarts and their weights do not match.\'\n        super(CosineAnnealingLR_Restart, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch == 0:\n            return self.base_lrs\n        elif self.last_epoch in self.restarts:\n            self.last_restart = self.last_epoch\n            self.T_max = self.T_period[self.restarts.index(self.last_epoch) + 1]\n            weight = self.restart_weights[self.restarts.index(self.last_epoch)]\n            return [group[\'initial_lr\'] * weight for group in self.optimizer.param_groups]\n        elif (self.last_epoch - self.last_restart - 1 - self.T_max) % (2 * self.T_max) == 0:\n            return [\n                group[\'lr\'] + (base_lr - self.eta_min) * (1 - math.cos(math.pi / self.T_max)) / 2\n                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n            ]\n        return [(1 + math.cos(math.pi * (self.last_epoch - self.last_restart) / self.T_max)) /\n                (1 + math.cos(math.pi * ((self.last_epoch - self.last_restart) - 1) / self.T_max)) *\n                (group[\'lr\'] - self.eta_min) + self.eta_min\n                for group in self.optimizer.param_groups]\n\n\nif __name__ == ""__main__"":\n    optimizer = torch.optim.Adam([torch.zeros(3, 64, 3, 3)], lr=2e-4, weight_decay=0,\n                                 betas=(0.9, 0.99))\n    ##############################\n    # MultiStepLR_Restart\n    ##############################\n    ## Original\n    lr_steps = [200000, 400000, 600000, 800000]\n    restarts = None\n    restart_weights = None\n\n    ## two\n    lr_steps = [100000, 200000, 300000, 400000, 490000, 600000, 700000, 800000, 900000, 990000]\n    restarts = [500000]\n    restart_weights = [1]\n\n    ## four\n    lr_steps = [\n        50000, 100000, 150000, 200000, 240000, 300000, 350000, 400000, 450000, 490000, 550000,\n        600000, 650000, 700000, 740000, 800000, 850000, 900000, 950000, 990000\n    ]\n    restarts = [250000, 500000, 750000]\n    restart_weights = [1, 1, 1]\n\n    scheduler = MultiStepLR_Restart(optimizer, lr_steps, restarts, restart_weights, gamma=0.5,\n                                    clear_state=False)\n\n    ##############################\n    # Cosine Annealing Restart\n    ##############################\n    ## two\n    T_period = [500000, 500000]\n    restarts = [500000]\n    restart_weights = [1]\n\n    ## four\n    T_period = [250000, 250000, 250000, 250000]\n    restarts = [250000, 500000, 750000]\n    restart_weights = [1, 1, 1]\n\n    scheduler = CosineAnnealingLR_Restart(optimizer, T_period, eta_min=1e-7, restarts=restarts,\n                                          weights=restart_weights)\n\n    ##############################\n    # Draw figure\n    ##############################\n    N_iter = 1000000\n    lr_l = list(range(N_iter))\n    for i in range(N_iter):\n        scheduler.step()\n        current_lr = optimizer.param_groups[0][\'lr\']\n        lr_l[i] = current_lr\n\n    import matplotlib as mpl\n    from matplotlib import pyplot as plt\n    import matplotlib.ticker as mtick\n    mpl.style.use(\'default\')\n    import seaborn\n    seaborn.set(style=\'whitegrid\')\n    seaborn.set_context(\'paper\')\n\n    plt.figure(1)\n    plt.subplot(111)\n    plt.ticklabel_format(style=\'sci\', axis=\'x\', scilimits=(0, 0))\n    plt.title(\'Title\', fontsize=16, color=\'k\')\n    plt.plot(list(range(N_iter)), lr_l, linewidth=1.5, label=\'learning rate scheme\')\n    legend = plt.legend(loc=\'upper right\', shadow=False)\n    ax = plt.gca()\n    labels = ax.get_xticks().tolist()\n    for k, v in enumerate(labels):\n        labels[k] = str(int(v / 1000)) + \'K\'\n    ax.set_xticklabels(labels)\n    ax.yaxis.set_major_formatter(mtick.FormatStrFormatter(\'%.1e\'))\n\n    ax.set_ylabel(\'Learning rate\')\n    ax.set_xlabel(\'Iteration\')\n    fig = plt.gcf()\n    plt.show()\n'"
codes/models/networks.py,0,"b""import models.modules.Sakuya_arch as Sakuya_arch\n\n####################\n# define network\n####################\n# Generator\ndef define_G(opt):\n    opt_net = opt['network_G']\n    which_model = opt_net['which_model_G']\n\n    if which_model == 'LunaTokis':\n        netG = Sakuya_arch.LunaTokis(nf=opt_net['nf'], nframes=opt_net['nframes'],\n                              groups=opt_net['groups'], front_RBs=opt_net['front_RBs'],\n                              back_RBs=opt_net['back_RBs'])                              \n    else:\n        raise NotImplementedError('Generator model [{:s}] not recognized'.format(which_model))\n\n    return netG\n"""
codes/options/__init__.py,0,b''
codes/options/options.py,0,"b""import os\nimport os.path as osp\nimport logging\nimport yaml\nfrom utils.util import OrderedYaml\nLoader, Dumper = OrderedYaml()\n\n\ndef parse(opt_path, is_train=True):\n    with open(opt_path, mode='r') as f:\n        opt = yaml.load(f, Loader=Loader)\n    # export CUDA_VISIBLE_DEVICES\n    gpu_list = ','.join(str(x) for x in opt['gpu_ids'])\n    os.environ['CUDA_VISIBLE_DEVICES'] = gpu_list\n    print('export CUDA_VISIBLE_DEVICES=' + gpu_list)\n\n    opt['is_train'] = is_train\n    if opt['distortion'] == 'sr' or opt['distortion'] == 'isr':\n        scale = opt['scale']\n\n    # datasets\n    for phase, dataset in opt['datasets'].items():\n        phase = phase.split('_')[0]\n        dataset['phase'] = phase\n        if opt['distortion'] == 'sr' or opt['distortion'] == 'isr':\n            dataset['scale'] = scale\n        is_lmdb = False\n        if dataset.get('dataroot_GT', None) is not None:\n            dataset['dataroot_GT'] = os.path.expanduser(dataset['dataroot_GT'])\n            if dataset['dataroot_GT'].endswith('lmdb'):\n                is_lmdb = True\n        if dataset.get('dataroot_LQ', None) is not None:\n            dataset['dataroot_LQ'] = os.path.expanduser(dataset['dataroot_LQ'])\n            if dataset['dataroot_LQ'].endswith('lmdb'):\n                is_lmdb = True\n        dataset['data_type'] = 'lmdb' if is_lmdb else 'img'\n        if dataset['mode'].endswith('mc'):  # for memcached\n            dataset['data_type'] = 'mc'\n            dataset['mode'] = dataset['mode'].replace('_mc', '')\n\n    # path\n    for key, path in opt['path'].items():\n        if path and key in opt['path'] and key != 'strict_load':\n            opt['path'][key] = osp.expanduser(path)\n    opt['path']['root'] = osp.abspath(osp.join(__file__, osp.pardir, osp.pardir, osp.pardir))\n    if is_train:\n        experiments_root = os.path.join(opt['path']['root'], 'experiments', opt['name'])\n        opt['path']['experiments_root'] = experiments_root\n        opt['path']['models'] = os.path.join(experiments_root, 'models')\n        opt['path']['training_state'] = os.path.join(experiments_root, 'training_state')\n        opt['path']['log'] = experiments_root\n        opt['path']['val_images'] = os.path.join(experiments_root, 'val_images')\n\n        # change some options for debug mode\n        if 'debug' in opt['name']:\n            opt['train']['val_freq'] = 8\n            opt['logger']['print_freq'] = 1\n            opt['logger']['save_checkpoint_freq'] = 8\n    else:  # test\n        results_root = os.path.join(opt['path']['root'], 'results', opt['name'])\n        opt['path']['results_root'] = results_root\n        opt['path']['log'] = results_root\n\n    # network\n    if opt['distortion'] == 'sr' or opt['distortion'] == 'isr':\n        opt['network_G']['scale'] = scale\n\n    return opt\n\n\ndef dict2str(opt, indent_l=1):\n    '''dict to string for logger'''\n    msg = ''\n    for k, v in opt.items():\n        if isinstance(v, dict):\n            msg += ' ' * (indent_l * 2) + k + ':[\\n'\n            msg += dict2str(v, indent_l + 1)\n            msg += ' ' * (indent_l * 2) + ']\\n'\n        else:\n            msg += ' ' * (indent_l * 2) + k + ': ' + str(v) + '\\n'\n    return msg\n\n\n# convert to NoneDict, which return None for missing key.\nclass NoneDict(dict):\n    def __missing__(self, key):\n        return None\n\n\ndef dict_to_nonedict(opt):\n    if isinstance(opt, dict):\n        new_opt = dict()\n        for key, sub_opt in opt.items():\n            new_opt[key] = dict_to_nonedict(sub_opt)\n        return NoneDict(**new_opt)\n    elif isinstance(opt, list):\n        return [dict_to_nonedict(sub_opt) for sub_opt in opt]\n    else:\n        return opt\n\n\ndef check_resume(opt, resume_iter):\n    '''Check resume states and pretrain_model paths'''\n    logger = logging.getLogger('base')\n    if opt['path']['resume_state']:\n        if opt['path'].get('pretrain_model_G', None) is not None or opt['path'].get(\n                'pretrain_model_D', None) is not None:\n            logger.warning('pretrain_model path will be ignored when resuming training.')\n\n        opt['path']['pretrain_model_G'] = osp.join(opt['path']['models'],\n                                                   '{}_G.pth'.format(resume_iter))\n        logger.info('Set [pretrain_model_G] to ' + opt['path']['pretrain_model_G'])\n        if 'gan' in opt['model']:\n            opt['path']['pretrain_model_D'] = osp.join(opt['path']['models'],\n                                                       '{}_D.pth'.format(resume_iter))\n            logger.info('Set [pretrain_model_D] to ' + opt['path']['pretrain_model_D'])\n"""
codes/utils/__init__.py,0,b''
codes/utils/make_video.py,0,"b'import cv2\nimport numpy as np\nimport os\n\nfrom os.path import isfile, join\n\ndef convert_frames_to_video(pathIn,pathOut,fps):\n    frame_array = []\n    files = [f for f in os.listdir(pathIn) if isfile(join(pathIn, f))]\n\n    # for sorting the file names properly\n    # TODO: can be changed depending on your data\n    files.sort(key = lambda x: int(x[2:-4]))\n\n    for i in range(len(files)):\n        filename=pathIn + files[i]\n        #reading each file\n        img = cv2.imread(filename)\n        height, width, layers = img.shape\n        size = (width,height)\n        #inserting the frames into an image array\n        frame_array.append(img)\n\n    out = cv2.VideoWriter(pathOut,cv2.VideoWriter_fourcc(*\'MP4V\'), fps, size)\n\n    for i in range(len(frame_array)):\n        # writing to a image array\n        out.write(frame_array[i])\n    out.release()\n\ndef main():\n    # TODO: change your input & output path here\n    pathIn= \'./input_path\'\n    pathOut = \'./output_path/2kcut_hfrsr_slow.mp4\'\n    fps = 23.98\n    convert_frames_to_video(pathIn, pathOut, fps)\n\nif __name__==""__main__"":\n    main()'"
codes/utils/util.py,4,"b'# this code is modified from https://github.com/xinntao/EDVR/blob/master/codes/utils/util.py\nimport os\nimport sys\nimport time\nimport math\nimport torch.nn.functional as F\nfrom datetime import datetime\nimport random\nimport logging\nfrom collections import OrderedDict\nimport numpy as np\nimport cv2\nimport torch\nfrom torchvision.utils import make_grid\nfrom shutil import get_terminal_size\nimport glob\nimport re\n\nimport yaml\ntry:\n    from yaml import CLoader as Loader, CDumper as Dumper\nexcept ImportError:\n    from yaml import Loader, Dumper\n\n\ndef OrderedYaml():\n    \'\'\'yaml orderedDict support\'\'\'\n    _mapping_tag = yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG\n\n    def dict_representer(dumper, data):\n        return dumper.represent_dict(data.items())\n\n    def dict_constructor(loader, node):\n        return OrderedDict(loader.construct_pairs(node))\n\n    Dumper.add_representer(OrderedDict, dict_representer)\n    Loader.add_constructor(_mapping_tag, dict_constructor)\n    return Loader, Dumper\n\n\n####################\n# miscellaneous\n####################\ndef get_model_total_params(model):\n    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    return (1.0*params/(1000*1000))\n\ndef get_timestamp():\n    return datetime.now().strftime(\'%y%m%d-%H%M%S\')\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef mkdirs(paths):\n    if isinstance(paths, str):\n        mkdir(paths)\n    else:\n        for path in paths:\n            mkdir(path)\n\n\ndef mkdir_and_rename(path):\n    if os.path.exists(path):\n        new_name = path + \'_archived_\' + get_timestamp()\n        print(\'Path already exists. Rename it to [{:s}]\'.format(new_name))\n        logger = logging.getLogger(\'base\')\n        logger.info(\'Path already exists. Rename it to [{:s}]\'.format(new_name))\n        os.rename(path, new_name)\n    os.makedirs(path)\n\n\ndef set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef setup_logger(logger_name, root, phase, level=logging.INFO, screen=False, tofile=False):\n    \'\'\'set up logger\'\'\'\n    lg = logging.getLogger(logger_name)\n    formatter = logging.Formatter(\'%(asctime)s.%(msecs)03d - %(levelname)s: %(message)s\',\n                                  datefmt=\'%y-%m-%d %H:%M:%S\')\n    lg.setLevel(level)\n    if tofile:\n        log_file = os.path.join(root, phase + \'_{}.log\'.format(get_timestamp()))\n        fh = logging.FileHandler(log_file, mode=\'w\')\n        fh.setFormatter(formatter)\n        lg.addHandler(fh)\n    if screen:\n        sh = logging.StreamHandler()\n        sh.setFormatter(formatter)\n        lg.addHandler(sh)\n\n\n####################\n# image convert\n####################\n\n\ndef tensor2img(tensor, out_type=np.uint8, min_max=(0, 1)):\n    \'\'\'\n    Converts a torch Tensor into an image Numpy array\n    Input: 4D(B,(3/1),H,W), 3D(C,H,W), or 2D(H,W), any range, RGB channel order\n    Output: 3D(H,W,C) or 2D(H,W), [0,255], np.uint8 (default)\n    \'\'\'\n    tensor = tensor.squeeze().float().cpu().clamp_(*min_max)  # clamp\n    tensor = (tensor - min_max[0]) / (min_max[1] - min_max[0])  # to range [0,1]\n    n_dim = tensor.dim()\n    if n_dim == 4:\n        n_img = len(tensor)\n        img_np = make_grid(tensor, nrow=int(math.sqrt(n_img)), normalize=False).numpy()\n        img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))  # HWC, BGR\n    elif n_dim == 3:\n        img_np = tensor.numpy()\n        img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))  # HWC, BGR\n    elif n_dim == 2:\n        img_np = tensor.numpy()\n    else:\n        raise TypeError(\n            \'Only support 4D, 3D and 2D tensor. But received with dimension: {:d}\'.format(n_dim))\n    if out_type == np.uint8:\n        img_np = (img_np * 255.0).round()\n        # Important. Unlike matlab, numpy.unit8() WILL NOT round by default.\n    return img_np.astype(out_type)\n\n\ndef save_img(img, img_path, mode=\'RGB\'):\n    cv2.imwrite(img_path, img)\n\n####################\n# metric\n####################\n\n\ndef calculate_psnr(img1, img2):\n    # img1 and img2 have range [0, 255]\n    img1 = img1.astype(np.float64)\n    img2 = img2.astype(np.float64)\n    # print(img1)\n    # print(\'img1-2\')\n    # print(img2)\n    mse = np.mean((img1 - img2)**2)\n    # print(mse)\n    if mse == 0:\n        return float(\'inf\')\n    return 20 * math.log10(255.0 / math.sqrt(mse))\n\n\ndef ssim(img1, img2):\n    C1 = (0.01 * 255)**2\n    C2 = (0.03 * 255)**2\n\n    img1 = img1.astype(np.float64)\n    img2 = img2.astype(np.float64)\n    kernel = cv2.getGaussianKernel(11, 1.5)\n    window = np.outer(kernel, kernel.transpose())\n\n    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]  # valid\n    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]\n    mu1_sq = mu1**2\n    mu2_sq = mu2**2\n    mu1_mu2 = mu1 * mu2\n    sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq\n    sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq\n    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2\n\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *\n                                                            (sigma1_sq + sigma2_sq + C2))\n    return ssim_map.mean()\n\n\ndef calculate_ssim(img1, img2):\n    \'\'\'calculate SSIM\n    the same outputs as MATLAB\'s\n    img1, img2: [0, 255]\n    \'\'\'\n    if not img1.shape == img2.shape:\n        raise ValueError(\'Input images must have the same dimensions.\')\n    if img1.ndim == 2:\n        return ssim(img1, img2)\n    elif img1.ndim == 3:\n        if img1.shape[2] == 3:\n            ssims = []\n            for i in range(3):\n                ssims.append(ssim(img1, img2))\n            return np.array(ssims).mean()\n        elif img1.shape[2] == 1:\n            return ssim(np.squeeze(img1), np.squeeze(img2))\n    else:\n        raise ValueError(\'Wrong input image dimensions.\')\n\n\nclass ProgressBar(object):\n    \'\'\'A progress bar which can print the progress\n    modified from https://github.com/hellock/cvbase/blob/master/cvbase/progress.py\n    \'\'\'\n\n    def __init__(self, task_num=0, bar_width=50, start=True):\n        self.task_num = task_num\n        max_bar_width = self._get_max_bar_width()\n        self.bar_width = (bar_width if bar_width <= max_bar_width else max_bar_width)\n        self.completed = 0\n        if start:\n            self.start()\n\n    def _get_max_bar_width(self):\n        terminal_width, _ = get_terminal_size()\n        max_bar_width = min(int(terminal_width * 0.6), terminal_width - 50)\n        if max_bar_width < 10:\n            print(\'terminal width is too small ({}), please consider widen the terminal for better \'\n                  \'progressbar visualization\'.format(terminal_width))\n            max_bar_width = 10\n        return max_bar_width\n\n    def start(self):\n        if self.task_num > 0:\n            sys.stdout.write(\'[{}] 0/{}, elapsed: 0s, ETA:\\n{}\\n\'.format(\n                \' \' * self.bar_width, self.task_num, \'Start...\'))\n        else:\n            sys.stdout.write(\'completed: 0, elapsed: 0s\')\n        sys.stdout.flush()\n        self.start_time = time.time()\n\n    def update(self, msg=\'In progress...\'):\n        self.completed += 1\n        elapsed = time.time() - self.start_time\n        fps = self.completed / elapsed\n        if self.task_num > 0:\n            percentage = self.completed / float(self.task_num)\n            eta = int(elapsed * (1 - percentage) / percentage + 0.5)\n            mark_width = int(self.bar_width * percentage)\n            bar_chars = \'>\' * mark_width + \'-\' * (self.bar_width - mark_width)\n            sys.stdout.write(\'\\033[2F\')  # cursor up 2 lines\n            sys.stdout.write(\'\\033[J\')  # clean the output (remove extra chars since last display)\n            sys.stdout.write(\'[{}] {}/{}, {:.1f} task/s, elapsed: {}s, ETA: {:5}s\\n{}\\n\'.format(\n                bar_chars, self.completed, self.task_num, fps, int(elapsed + 0.5), eta, msg))\n        else:\n            sys.stdout.write(\'completed: {}, elapsed: {}s, {:.1f} tasks/s\'.format(\n                self.completed, int(elapsed + 0.5), fps))\n        sys.stdout.flush()\n\n####################\n# read image\n####################\n\ndef read_image(img_path):\n    \'\'\'read one image from img_path\n    Return img: HWC, BGR, [0,1], numpy\n    \'\'\'\n    img_GT = cv2.imread(img_path)\n    img = img_GT.astype(np.float32) / 255.\n    return img\n\ndef read_seq_imgs(img_seq_path):\n    \'\'\'read a sequence of images\'\'\'\n    img_path_l = glob.glob(img_seq_path + \'/*\')\n    # img_path_l.sort(key=lambda x: int(os.path.basename(x)[:-4]))\n    img_path_l.sort(key=lambda x: int(re.search(r\'\\d+\', os.path.basename(x)).group()))\n    img_l = [read_image(v) for v in img_path_l]\n    # stack to TCHW, RGB, [0,1], torch\n    imgs = np.stack(img_l, axis=0)\n    imgs = imgs[:, :, :, [2, 1, 0]]\n    imgs = torch.from_numpy(np.ascontiguousarray(np.transpose(imgs, (0, 3, 1, 2)))).float()\n    return imgs\n\n\ndef test_index_generation(skip, N_out, len_in):\n    \'\'\'\n    params: \n    skip: if skip even number; \n    N_out: number of frames of the network; \n    len_in: length of input frames\n\n    example:\n  len_in | N_out  | times | (no skip)                  |   (skip)\n    5    |   3    |  4/2  | [0,1], [1,2], [2,3], [3,4] | [0,2],[2,4]\n    7    |   3    |  5/3  | [0,1],[1,2][2,3]...[5,6]   | [0,2],[2,4],[4,6] \n    5    |   5    |  2/1  | [0,1,2] [2,3,4]            | [0,2,4]\n    \'\'\'\n    # number of input frames for the network\n    N_in = 1 + N_out // 2\n    # input length should be enough to generate the output frames\n    assert N_in <= len_in\n\n    sele_list = []\n    if skip: \n        right = N_out # init\n        while (right <= len_in):\n            h_list = [right-N_out+x for x in range(N_out)]\n            l_list = h_list[::2]\n            right += (N_out - 1)\n            sele_list.append([l_list,h_list])\n    else:\n        right = N_out # init\n        right_in = N_in\n        while (right_in <= len_in):\n            h_list = [right-N_out+x for x in range(N_out)]\n            l_list = [right_in-N_in+x for x in range(N_in)]\n            right += (N_out - 1)\n            right_in += (N_in - 1)\n            sele_list.append([l_list,h_list])\n    # check if it covers the last image, if not, we should cover it \n    if (skip) and (right < len_in - 1):\n        h_list = [len_in - N_out + x for x in range(N_out)]\n        l_list = h_list[::2]\n        sele_list.append([l_list,h_list])\n    if (not skip) and (right_in < len_in - 1):\n        right = len_in * 2 - 1;\n        h_list = [right-N_out+x for x in range(N_out)]\n        l_list = [len_in - N_in + x for x in range(N_in)]\n        sele_list.append([l_list,h_list])        \n    return sele_list\n\n\n####################\n# video\n####################\n\ndef extract_frames(ffmpeg_dir, video, outDir):\n    """"""\n    Converts the `video` to images.\n    Parameters\n    ----------\n        video : string\n            full path to the video file.\n        outDir : string\n            path to directory to output the extracted images.\n    Returns\n    -------\n        error : string\n            Error message if error occurs otherwise blank string.\n    """"""\n\n    error = """"\n    print(\'{} -i {} -vsync 0 {}/%06d.png\'.format(os.path.join(ffmpeg_dir, ""ffmpeg""), video, outDir))\n    retn = os.system(\'{} -i ""{}"" -vsync 0 {}/%06d.png\'.format(os.path.join(ffmpeg_dir, ""ffmpeg""), video, outDir))\n    if retn:\n        error = ""Error converting file:{}. Exiting."".format(video)\n    return error\n\ndef create_video(ffmpeg_dir, dir, output, fps):\n    error = """"\n    # print(\'{} -r {} -i {}/%6d.png -vcodec ffvhuff {}\'.format(os.path.join(ffmpeg_dir, ""ffmpeg""), fps, dir, output))\n    # retn = os.system(\'{} -r {} -i {}/%6d.png -vcodec ffvhuff ""{}""\'.format(os.path.join(ffmpeg_dir, ""ffmpeg""), fps, dir, output))\n    print(\'{} -r {} -f image2 -i {}/%6d.png {}\'.format(os.path.join(ffmpeg_dir, ""ffmpeg""), fps, dir, output))\n    retn = os.system(\'{} -r {} -f image2 -i {}/%6d.png {}\'.format(os.path.join(ffmpeg_dir, ""ffmpeg""), fps, dir, output))\n    if retn:\n        error = ""Error creating output video. Exiting.""\n    return error\n\n# if __name__ == \'__main__\':\n'"
codes/models/modules/Sakuya_arch.py,21,"b""''' network architecture for Sakuya '''\nimport functools\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport models.modules.module_util as mutil\nfrom models.modules.convlstm import ConvLSTM, ConvLSTMCell\ntry:\n    from models.modules.DCNv2.dcn_v2 import DCN_sep\nexcept ImportError:\n    raise ImportError('Failed to import DCNv2 module.')\n\nclass PCD_Align(nn.Module):\n    ''' Alignment module using Pyramid, Cascading and Deformable convolution\n    with 3 pyramid levels.\n    '''\n    def __init__(self, nf=64, groups=8):\n        super(PCD_Align, self).__init__()\n\n        # fea1\n        # L3: level 3, 1/4 spatial size\n        self.L3_offset_conv1_1 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for diff\n        self.L3_offset_conv2_1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.L3_dcnpack_1 = DCN_sep(nf, nf, 3, stride=1, padding=1, dilation=1,\n                                  deformable_groups=groups)\n        # L2: level 2, 1/2 spatial size\n        self.L2_offset_conv1_1 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for diff\n        self.L2_offset_conv2_1 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for offset\n        self.L2_offset_conv3_1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.L2_dcnpack_1 = DCN_sep(nf, nf, 3, stride=1, padding=1, dilation=1,\n                                  deformable_groups=groups)\n        self.L2_fea_conv_1 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for fea\n        # L1: level 1, original spatial size\n        self.L1_offset_conv1_1 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for diff\n        self.L1_offset_conv2_1 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for offset\n        self.L1_offset_conv3_1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.L1_dcnpack_1 = DCN_sep(nf, nf, 3, stride=1, padding=1, dilation=1,\n                                  deformable_groups=groups)\n        self.L1_fea_conv_1 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for fea\n\n        # fea2 \n        # L3: level 3, 1/4 spatial size\n        self.L3_offset_conv1_2 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for diff\n        self.L3_offset_conv2_2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.L3_dcnpack_2 = DCN_sep(nf, nf, 3, stride=1, padding=1, dilation=1,\n                                  deformable_groups=groups)\n        # L2: level 2, 1/2 spatial size\n        self.L2_offset_conv1_2 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for diff\n        self.L2_offset_conv2_2 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for offset\n        self.L2_offset_conv3_2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.L2_dcnpack_2 = DCN_sep(nf, nf, 3, stride=1, padding=1, dilation=1,\n                                  deformable_groups=groups)\n        self.L2_fea_conv_2 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for fea\n        # L1: level 1, original spatial size\n        self.L1_offset_conv1_2 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for diff\n        self.L1_offset_conv2_2 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for offset\n        self.L1_offset_conv3_2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.L1_dcnpack_2 = DCN_sep(nf, nf, 3, stride=1, padding=1, dilation=1,\n                                  deformable_groups=groups)\n        self.L1_fea_conv_2 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for fea\n\n        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\n    def forward(self, fea1, fea2):\n        '''align other neighboring frames to the reference frame in the feature level\n        fea1, fea2: [L1, L2, L3], each with [B,C,H,W] features\n        estimate offset bidirectionally\n        '''\n        y = []\n        # param. of fea1\n        # L3\n        L3_offset = torch.cat([fea1[2], fea2[2]], dim=1)\n        L3_offset = self.lrelu(self.L3_offset_conv1_1(L3_offset))\n        L3_offset = self.lrelu(self.L3_offset_conv2_1(L3_offset))\n        L3_fea = self.lrelu(self.L3_dcnpack_1(fea1[2], L3_offset))\n        # L2\n        L2_offset = torch.cat([fea1[1], fea2[1]], dim=1)\n        L2_offset = self.lrelu(self.L2_offset_conv1_1(L2_offset))\n        L3_offset = F.interpolate(L3_offset, scale_factor=2, mode='bilinear', align_corners=False)\n        L2_offset = self.lrelu(self.L2_offset_conv2_1(torch.cat([L2_offset, L3_offset * 2], dim=1)))\n        L2_offset = self.lrelu(self.L2_offset_conv3_1(L2_offset))\n        L2_fea = self.L2_dcnpack_1(fea1[1], L2_offset)\n        L3_fea = F.interpolate(L3_fea, scale_factor=2, mode='bilinear', align_corners=False)\n        L2_fea = self.lrelu(self.L2_fea_conv_1(torch.cat([L2_fea, L3_fea], dim=1)))\n        # L1\n        L1_offset = torch.cat([fea1[0], fea2[0]], dim=1)\n        L1_offset = self.lrelu(self.L1_offset_conv1_1(L1_offset))\n        L2_offset = F.interpolate(L2_offset, scale_factor=2, mode='bilinear', align_corners=False)\n        L1_offset = self.lrelu(self.L1_offset_conv2_1(torch.cat([L1_offset, L2_offset * 2], dim=1)))\n        L1_offset = self.lrelu(self.L1_offset_conv3_1(L1_offset))\n        L1_fea = self.L1_dcnpack_1(fea1[0], L1_offset)\n        L2_fea = F.interpolate(L2_fea, scale_factor=2, mode='bilinear', align_corners=False)\n        L1_fea = self.L1_fea_conv_1(torch.cat([L1_fea, L2_fea], dim=1))\n        y.append(L1_fea)\n\n        # param. of fea2\n        # L3\n        L3_offset = torch.cat([fea2[2], fea1[2]], dim=1)\n        L3_offset = self.lrelu(self.L3_offset_conv1_2(L3_offset))\n        L3_offset = self.lrelu(self.L3_offset_conv2_2(L3_offset))\n        L3_fea = self.lrelu(self.L3_dcnpack_2(fea2[2], L3_offset))\n        # L2\n        L2_offset = torch.cat([fea2[1], fea1[1]], dim=1)\n        L2_offset = self.lrelu(self.L2_offset_conv1_2(L2_offset))\n        L3_offset = F.interpolate(L3_offset, scale_factor=2, mode='bilinear', align_corners=False)\n        L2_offset = self.lrelu(self.L2_offset_conv2_2(torch.cat([L2_offset, L3_offset * 2], dim=1)))\n        L2_offset = self.lrelu(self.L2_offset_conv3_2(L2_offset))\n        L2_fea = self.L2_dcnpack_2(fea2[1], L2_offset)\n        L3_fea = F.interpolate(L3_fea, scale_factor=2, mode='bilinear', align_corners=False)\n        L2_fea = self.lrelu(self.L2_fea_conv_2(torch.cat([L2_fea, L3_fea], dim=1)))\n        # L1\n        L1_offset = torch.cat([fea2[0], fea1[0]], dim=1)\n        L1_offset = self.lrelu(self.L1_offset_conv1_2(L1_offset))\n        L2_offset = F.interpolate(L2_offset, scale_factor=2, mode='bilinear', align_corners=False)\n        L1_offset = self.lrelu(self.L1_offset_conv2_2(torch.cat([L1_offset, L2_offset * 2], dim=1)))\n        L1_offset = self.lrelu(self.L1_offset_conv3_2(L1_offset))\n        L1_fea = self.L1_dcnpack_2(fea2[0], L1_offset)\n        L2_fea = F.interpolate(L2_fea, scale_factor=2, mode='bilinear', align_corners=False)\n        L1_fea = self.L1_fea_conv_2(torch.cat([L1_fea, L2_fea], dim=1))\n        y.append(L1_fea)\n        \n        y = torch.cat(y, dim=1)\n        return y\n\nclass Easy_PCD(nn.Module):\n    def __init__(self, nf=64, groups=8):\n        super(Easy_PCD, self).__init__()\n\n        self.fea_L2_conv1 = nn.Conv2d(nf, nf, 3, 2, 1, bias=True)\n        self.fea_L2_conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.fea_L3_conv1 = nn.Conv2d(nf, nf, 3, 2, 1, bias=True)\n        self.fea_L3_conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.pcd_align = PCD_Align(nf=nf, groups=groups)\n        self.fusion = nn.Conv2d(2 * nf, nf, 1, 1, bias=True)\n        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\n    def forward(self, f1, f2):\n        # input: extracted features\n        # feature size: f1 = f2 = [B, N, C, H, W]\n        # print(f1.size())\n        L1_fea = torch.stack([f1, f2], dim=1)\n        B, N, C, H, W = L1_fea.size()\n        L1_fea = L1_fea.view(-1, C, H, W)\n        # L2\n        L2_fea = self.lrelu(self.fea_L2_conv1(L1_fea))\n        L2_fea = self.lrelu(self.fea_L2_conv2(L2_fea))\n        # L3\n        L3_fea = self.lrelu(self.fea_L3_conv1(L2_fea))\n        L3_fea = self.lrelu(self.fea_L3_conv2(L3_fea))\n\n        L1_fea = L1_fea.view(B, N, -1, H, W)\n        L2_fea = L2_fea.view(B, N, -1, H // 2, W // 2)\n        L3_fea = L3_fea.view(B, N, -1, H // 4, W // 4)\n\n        fea1 = [L1_fea[:, 0, :, :, :].clone(), L2_fea[:, 0, :, :, :].clone(), L3_fea[:, 0, :, :, :].clone()]\n        fea2 = [L1_fea[:, 1, :, :, :].clone(), L2_fea[:, 1, :, :, :].clone(), L3_fea[:, 1, :, :, :].clone()]\n        aligned_fea = self.pcd_align(fea1, fea2)\n        fusion_fea = self.fusion(aligned_fea) # [B, N, C, H, W]\n        return fusion_fea\n\nclass DeformableConvLSTM(ConvLSTM):\n    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, num_layers, front_RBs, groups,\n                 batch_first=False, bias=True, return_all_layers=False):\n        ConvLSTM.__init__(self, input_size, input_dim, hidden_dim, kernel_size, num_layers, \n              batch_first=batch_first, bias=bias, return_all_layers=return_all_layers)\n        #### extract features (for each frame)\n        nf = input_dim\n\n        self.pcd_h = Easy_PCD(nf=nf, groups=groups)\n        self.pcd_c = Easy_PCD(nf=nf, groups=groups)        \n\n        cell_list = []\n        for i in range(0, num_layers):\n            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i-1]\n            cell_list.append(ConvLSTMCell(input_size=(self.height, self.width),\n                                          input_dim=cur_input_dim,\n                                          hidden_dim=self.hidden_dim[i],\n                                          kernel_size=self.kernel_size[i],\n                                          bias=self.bias))\n        self.cell_list = nn.ModuleList(cell_list)\n\n        #### activation function\n        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\n    def forward(self, input_tensor, hidden_state = None):\n        '''        \n        Parameters\n        ----------\n        input_tensor: \n            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n        hidden_state: \n            None. \n            \n        Returns\n        -------\n        last_state_list, layer_output\n        '''\n        if not self.batch_first:\n            # (t, b, c, h, w) -> (b, t, c, h, w)\n            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n\n        if hidden_state is not None:\n            raise NotImplementedError()\n        else:\n            tensor_size = (input_tensor.size(3),input_tensor.size(4))\n            hidden_state = self._init_hidden(batch_size=input_tensor.size(0),tensor_size=tensor_size)\n        \n        layer_output_list = []\n        last_state_list = []\n\n        seq_len = input_tensor.size(1)\n        cur_layer_input = input_tensor\n\n        for layer_idx in range(self.num_layers):\n            h, c = hidden_state[layer_idx]\n            output_inner = []\n            for t in range(seq_len):\n                in_tensor = cur_layer_input[:, t, :, :, :] \n                h_temp = self.pcd_h(in_tensor, h)\n                c_temp = self.pcd_c(in_tensor, c)\n                h, c = self.cell_list[layer_idx](input_tensor=in_tensor,\n                                                 cur_state=[h_temp, c_temp])\n                output_inner.append(h)\n\n            layer_output = torch.stack(output_inner, dim=1)\n            cur_layer_input = layer_output\n\n            layer_output_list.append(layer_output)\n            last_state_list.append([h, c])\n\n        if not self.return_all_layers:\n            layer_output_list = layer_output_list[-1:]\n            last_state_list   = last_state_list[-1:]\n\n        return layer_output_list, last_state_list\n    \n    def _init_hidden(self, batch_size, tensor_size):\n        return super()._init_hidden(batch_size, tensor_size)\n\nclass BiDeformableConvLSTM(nn.Module):\n    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, num_layers, front_RBs, groups,\n                 batch_first=False, bias=True, return_all_layers=False):\n        super(BiDeformableConvLSTM, self).__init__()\n        self.forward_net = DeformableConvLSTM(input_size=input_size, input_dim=input_dim, hidden_dim=hidden_dim,\n                                           kernel_size=kernel_size, num_layers=num_layers, front_RBs=front_RBs,\n                                           groups=groups, batch_first=batch_first, bias=bias, return_all_layers=return_all_layers)\n        self.conv_1x1 = nn.Conv2d(2*input_dim, input_dim, 1, 1, bias=True)\n\n    def forward(self, x):\n        reversed_idx = list(reversed(range(x.shape[1])))\n        x_rev = x[:, reversed_idx, ...]\n        out_fwd, _ = self.forward_net(x)\n        out_rev, _ = self.forward_net(x_rev)\n        rev_rev = out_rev[0][:, reversed_idx, ...]\n        B, N, C, H, W = out_fwd[0].size()\n        result = torch.cat((out_fwd[0], rev_rev), dim=2)\n        result = result.view(B*N,-1,H,W)\n        result = self.conv_1x1(result)\n        return result.view(B, -1, C, H, W) \n\nclass LunaTokis(nn.Module):\n    def __init__(self, nf=64, nframes=3, groups=8, front_RBs=5, back_RBs=10):\n        super(LunaTokis, self).__init__()\n        self.nf = nf\n        self.in_frames = 1 + nframes // 2\n        self.ot_frames = nframes\n        p_size = 48 # a place holder, not so useful\n        patch_size = (p_size, p_size) \n        n_layers = 1\n        hidden_dim = []\n        for i in range(n_layers):\n            hidden_dim.append(nf)\n\n        ResidualBlock_noBN_f = functools.partial(mutil.ResidualBlock_noBN, nf=nf)\n        self.conv_first = nn.Conv2d(3, nf, 3, 1, 1, bias=True)\n        self.feature_extraction = mutil.make_layer(ResidualBlock_noBN_f, front_RBs)\n        self.fea_L2_conv1 = nn.Conv2d(nf, nf, 3, 2, 1, bias=True)\n        self.fea_L2_conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.fea_L3_conv1 = nn.Conv2d(nf, nf, 3, 2, 1, bias=True)\n        self.fea_L3_conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.pcd_align = PCD_Align(nf=nf, groups=groups)\n        self.fusion = nn.Conv2d(2 * nf, nf, 1, 1, bias=True)\n        self.ConvBLSTM = BiDeformableConvLSTM(input_size=patch_size, input_dim=nf, hidden_dim=hidden_dim, \\\n            kernel_size=(3,3), num_layers=1, batch_first=True, front_RBs=front_RBs, groups=groups)\n        #### reconstruction\n        self.recon_trunk = mutil.make_layer(ResidualBlock_noBN_f, back_RBs)\n        #### upsampling\n        self.upconv1 = nn.Conv2d(nf, nf * 4, 3, 1, 1, bias=True)\n        self.upconv2 = nn.Conv2d(nf, 64 * 4, 3, 1, 1, bias=True)\n        self.pixel_shuffle = nn.PixelShuffle(2)\n        self.HRconv = nn.Conv2d(64, 64, 3, 1, 1, bias=True)\n        self.conv_last = nn.Conv2d(64, 3, 3, 1, 1, bias=True)\n\n        #### activation function\n        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\n    def forward(self, x):\n        B, N, C, H, W = x.size()  # N input video frames\n        #### extract LR features\n        # L1\n        L1_fea = self.lrelu(self.conv_first(x.view(-1, C, H, W)))\n        L1_fea = self.feature_extraction(L1_fea)\n        # L2\n        L2_fea = self.lrelu(self.fea_L2_conv1(L1_fea))\n        L2_fea = self.lrelu(self.fea_L2_conv2(L2_fea))\n        # L3\n        L3_fea = self.lrelu(self.fea_L3_conv1(L2_fea))\n        L3_fea = self.lrelu(self.fea_L3_conv2(L3_fea))\n        L1_fea = L1_fea.view(B, N, -1, H, W)\n        L2_fea = L2_fea.view(B, N, -1, H // 2, W // 2)\n        L3_fea = L3_fea.view(B, N, -1, H // 4, W // 4)\n\n        #### align using pcd\n        to_lstm_fea = []\n        '''\n        0: + fea1, fusion_fea, fea2\n        1: + ...    ...        ...  fusion_fea, fea2\n        2: + ...    ...        ...    ...       ...   fusion_fea, fea2\n        '''\n        for idx in range(N-1):\n            fea1 = [\n                L1_fea[:, idx, :, :, :].clone(), L2_fea[:, idx, :, :, :].clone(), L3_fea[:, idx, :, :, :].clone()\n            ]\n            fea2 = [\n                L1_fea[:, idx+1, :, :, :].clone(), L2_fea[:, idx+1, :, :, :].clone(), L3_fea[:, idx+1, :, :, :].clone()\n            ]\n            aligned_fea = self.pcd_align(fea1, fea2)\n\n            fusion_fea = self.fusion(aligned_fea) # [B, N, C, H, W]\n            if idx == 0:\n                to_lstm_fea.append(fea1[0])\n            to_lstm_fea.append(fusion_fea)\n            to_lstm_fea.append(fea2[0])\n        lstm_feats = torch.stack(to_lstm_fea, dim = 1)\n        #### align using bidirectional deformable conv-lstm\n        feats = self.ConvBLSTM(lstm_feats) \n        B, T, C, H, W = feats.size()\n\n        feats = feats.view(B*T, C, H, W)\n        out = self.recon_trunk(feats)\n        out = self.lrelu(self.pixel_shuffle(self.upconv1(out)))\n        out = self.lrelu(self.pixel_shuffle(self.upconv2(out)))\n\n        out = self.lrelu(self.HRconv(out))\n        out = self.conv_last(out)\n        _, _, K, G = out.size()\n        outs = out.view(B, T, -1, K, G)\n        return outs"""
codes/models/modules/__init__.py,0,b''
codes/models/modules/convlstm.py,13,"b'import torch.nn as nn\nfrom torch.autograd import Variable\nimport torch\n\n\nclass ConvLSTMCell(nn.Module):\n\n    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, bias):\n        """"""\n        Initialize ConvLSTM cell.\n        \n        Parameters\n        ----------\n        input_size: (int, int)\n            Height and width of input tensor as (height, width).\n        input_dim: int\n            Number of channels of input tensor.\n        hidden_dim: int\n            Number of channels of hidden state.\n        kernel_size: (int, int)\n            Size of the convolutional kernel.\n        bias: bool\n            Whether or not to add the bias.\n        """"""\n\n        super(ConvLSTMCell, self).__init__()\n\n        self.height, self.width = input_size\n        self.input_dim  = input_dim\n        self.hidden_dim = hidden_dim\n\n        self.kernel_size = kernel_size\n        self.padding     = kernel_size[0] // 2, kernel_size[1] // 2\n        self.bias        = bias\n        \n        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n                              out_channels=4 * self.hidden_dim,\n                              kernel_size=self.kernel_size,\n                              padding=self.padding,\n                              bias=self.bias)\n\n    def forward(self, input_tensor, cur_state):\n        \n        h_cur, c_cur = cur_state\n\n        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n        \n        combined_conv = self.conv(combined)\n        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1) \n        i = torch.sigmoid(cc_i)\n        f = torch.sigmoid(cc_f)\n        o = torch.sigmoid(cc_o)\n        g = torch.tanh(cc_g)\n\n        c_next = f * c_cur + i * g\n        h_next = o * torch.tanh(c_next)\n        \n        return h_next, c_next\n\n    def init_hidden(self, batch_size, tensor_size):\n        height, width = tensor_size\n        return (Variable(torch.zeros(batch_size, self.hidden_dim, height, width)).cuda(),\n                Variable(torch.zeros(batch_size, self.hidden_dim, height, width)).cuda())\n\n\nclass ConvLSTM(nn.Module):\n\n    def __init__(self, input_size, input_dim, hidden_dim, kernel_size, num_layers,\n                 batch_first=False, bias=True, return_all_layers=False):\n        super(ConvLSTM, self).__init__()\n\n        self._check_kernel_size_consistency(kernel_size)\n\n        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n        hidden_dim  = self._extend_for_multilayer(hidden_dim, num_layers)\n        if not len(kernel_size) == len(hidden_dim) == num_layers:\n            raise ValueError(\'Inconsistent list length.\')\n\n        self.height, self.width = input_size\n\n        self.input_dim  = input_dim\n        self.hidden_dim = hidden_dim\n        self.kernel_size = kernel_size\n        self.num_layers = num_layers\n        self.batch_first = batch_first\n        self.bias = bias\n        self.return_all_layers = return_all_layers\n\n        cell_list = []\n        for i in range(0, self.num_layers):\n            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i-1]\n\n            cell_list.append(ConvLSTMCell(input_size=(self.height, self.width),\n                                          input_dim=cur_input_dim,\n                                          hidden_dim=self.hidden_dim[i],\n                                          kernel_size=self.kernel_size[i],\n                                          bias=self.bias))\n\n        self.cell_list = nn.ModuleList(cell_list)\n\n    def forward(self, input_tensor, hidden_state=None):\n        """"""\n        \n        Parameters\n        ----------\n        input_tensor: todo \n            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n        hidden_state: todo\n            None. todo implement stateful\n            \n        Returns\n        -------\n        last_state_list, layer_output\n        """"""\n        if not self.batch_first:\n            # (t, b, c, h, w) -> (b, t, c, h, w)\n            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n\n        # Implement stateful ConvLSTM\n        if hidden_state is not None:\n            raise NotImplementedError()\n        else:\n            tensor_size = (input_tensor.size(3),input_tensor.size(4))\n            hidden_state = self._init_hidden(batch_size=input_tensor.size(0),tensor_size=tensor_size)\n\n        layer_output_list = []\n        last_state_list   = []\n\n        seq_len = input_tensor.size(1)\n        cur_layer_input = input_tensor\n\n        for layer_idx in range(self.num_layers):\n\n            h, c = hidden_state[layer_idx]\n            output_inner = []\n            for t in range(seq_len):\n\n                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n                                                 cur_state=[h, c])\n                output_inner.append(h)\n\n            layer_output = torch.stack(output_inner, dim=1)\n            cur_layer_input = layer_output\n\n            layer_output_list.append(layer_output)\n            last_state_list.append([h, c])\n\n        if not self.return_all_layers:\n            layer_output_list = layer_output_list[-1:]\n            last_state_list   = last_state_list[-1:]\n\n        return layer_output_list, last_state_list\n\n    def _init_hidden(self, batch_size, tensor_size):\n        init_states = []\n        for i in range(self.num_layers):\n            init_states.append(self.cell_list[i].init_hidden(batch_size, tensor_size))\n        return init_states\n\n    @staticmethod\n    def _check_kernel_size_consistency(kernel_size):\n        if not (isinstance(kernel_size, tuple) or\n                    (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n            raise ValueError(\'`kernel_size` must be tuple or list of tuples\')\n\n    @staticmethod\n    def _extend_for_multilayer(param, num_layers):\n        if not isinstance(param, list):\n            param = [param] * num_layers\n        return param\n    \nclass ConvBLSTM(nn.Module):\n    # Constructor\n    def __init__(self, input_size, input_dim, hidden_dim,\n                 kernel_size, num_layers, batch_first=False, bias=True, return_all_layers=False):\n\n        super(ConvBLSTM, self).__init__()\n        self.forward_net = ConvLSTM(input_size, input_dim, hidden_dims//2, kernel_size,\n                                    num_layers, batch_first=batch_first, bias=bias, \n                                    return_all_layers=return_all_layers)\n        self.reverse_net = ConvLSTM(input_size, input_dim, hidden_dims//2, kernel_size,\n                                    num_layers, batch_first=batch_first, bias=bias, \n                                    return_all_layers=return_all_layers)\n        \n    def forward(self, xforward, xreverse):\n        """"""\n        xforward, xreverse = B T C H W tensors.\n        """"""\n\n        y_out_fwd, _ = self.forward_net(xforward)\n        y_out_rev, _ = self.reverse_net(xreverse)\n        \n        if not self.return_all_layers:\n            y_out_fwd = y_out_fwd[-1] # outputs of last CLSTM layer = B, T, C, H, W\n            y_out_rev = y_out_rev[-1] # outputs of last CLSTM layer = B, T, C, H, W\n\n        reversed_idx = list(reversed(range(y_out_rev.shape[1])))\n        y_out_rev = y_out_rev[:, reversed_idx, ...] # reverse temporal outputs.\n        ycat = torch.cat((y_out_fwd, y_out_rev), dim=2)\n        \n        return ycat\n'"
codes/models/modules/loss.py,5,"b'import torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as fnn\nfrom torch.autograd import Variable\n\nclass CharbonnierLoss(nn.Module):\n    """"""Charbonnier Loss (L1)""""""\n\n    def __init__(self, eps=1e-6):\n        super(CharbonnierLoss, self).__init__()\n        self.eps = eps\n\n    def forward(self, x, y):\n        diff = x - y\n        loss = torch.sum(torch.sqrt(diff * diff + self.eps))\n        return loss\n\ndef build_gauss_kernel(size=5, sigma=1.0, n_channels=1, cuda=False):\n    if size % 2 != 1:\n        raise ValueError(""kernel size must be uneven"")\n    grid = np.float32(np.mgrid[0:size,0:size].T)\n    gaussian = lambda x: np.exp((x - size//2)**2/(-2*sigma**2))**2\n    kernel = np.sum(gaussian(grid), axis=2)\n    kernel /= np.sum(kernel)\n    # repeat same kernel across depth dimension\n    kernel = np.tile(kernel, (n_channels, 1, 1))\n    # conv weight should be (out_channels, groups/in_channels, h, w), \n    # and since we have depth-separable convolution we want the groups dimension to be 1\n    kernel = torch.FloatTensor(kernel[:, None, :, :])\n    if cuda:\n        kernel = kernel.cuda()\n    return Variable(kernel, requires_grad=False)\n\n\ndef conv_gauss(img, kernel):\n    """""" convolve img with a gaussian kernel that has been built with build_gauss_kernel """"""\n    n_channels, _, kw, kh = kernel.shape\n    img = fnn.pad(img, (kw//2, kh//2, kw//2, kh//2), mode=\'replicate\')\n    return fnn.conv2d(img, kernel, groups=n_channels)\n\n\ndef laplacian_pyramid(img, kernel, max_levels=5):\n    current = img\n    pyr = []\n\n    for level in range(max_levels):\n        filtered = conv_gauss(current, kernel)\n        diff = current - filtered\n        pyr.append(diff)\n        current = fnn.avg_pool2d(filtered, 2)\n\n    pyr.append(current)\n    return pyr\n\nclass LapLoss(nn.Module):\n    def __init__(self, max_levels=5, k_size=5, sigma=2.0):\n        super(LapLoss, self).__init__()\n        self.max_levels = max_levels\n        self.k_size = k_size\n        self.sigma = sigma\n        self._gauss_kernel = None\n        \n    def forward(self, input, target):\n        # input shape :[B, N, C, H, W]\n        if len(input.shape) == 5:\n            B,N,C,H,W = input.size()\n            input = input.view(-1, C, H , W)\n            target = target.view(-1, C, H, W)\n        if self._gauss_kernel is None or self._gauss_kernel.shape[1] != input.shape[1]:\n            self._gauss_kernel = build_gauss_kernel(\n                size=self.k_size, sigma=self.sigma, \n                n_channels=input.shape[1], cuda=input.is_cuda\n            )\n        pyr_input  = laplacian_pyramid(input, self._gauss_kernel, self.max_levels)\n        pyr_target = laplacian_pyramid(target, self._gauss_kernel, self.max_levels)\n        return sum(fnn.l1_loss(a, b) for a, b in zip(pyr_input, pyr_target))\n\n# if __name__ == ""__main__"":  \n'"
codes/models/modules/module_util.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\n\ndef initialize_weights(net_l, scale=1):\n    if not isinstance(net_l, list):\n        net_l = [net_l]\n    for net in net_l:\n        for m in net.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, a=0, mode=\'fan_in\')\n                m.weight.data *= scale  # for residual block\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                init.kaiming_normal_(m.weight, a=0, mode=\'fan_in\')\n                m.weight.data *= scale\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias.data, 0.0)\n\n\ndef make_layer(block, n_layers):\n    layers = []\n    for _ in range(n_layers):\n        layers.append(block())\n    return nn.Sequential(*layers)\n\n\nclass ResidualBlock_noBN(nn.Module):\n    \'\'\'Residual block w/o BN\n    ---Conv-ReLU-Conv-+-\n     |________________|\n    \'\'\'\n\n    def __init__(self, nf=64):\n        super(ResidualBlock_noBN, self).__init__()\n        self.conv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n\n        # initialization\n        initialize_weights([self.conv1, self.conv2], 0.1)\n\n    def forward(self, x):\n        identity = x\n        out = F.relu(self.conv1(x), inplace=True)\n        out = self.conv2(out)\n        return identity + out\n\n\ndef flow_warp(x, flow, interp_mode=\'bilinear\', padding_mode=\'zeros\'):\n    """"""Warp an image or feature map with optical flow\n    Args:\n        x (Tensor): size (N, C, H, W)\n        flow (Tensor): size (N, H, W, 2), normal value\n        interp_mode (str): \'nearest\' or \'bilinear\'\n        padding_mode (str): \'zeros\' or \'border\' or \'reflection\'\n\n    Returns:\n        Tensor: warped image or feature map\n    """"""\n    print(x.size()[-2:])\n    print(flow.size()[1:3])\n    assert x.size()[-2:] == flow.size()[1:3]\n    B, C, H, W = x.size()\n    # mesh grid\n    grid_y, grid_x = torch.meshgrid(torch.arange(0, H), torch.arange(0, W))\n    grid = torch.stack((grid_x, grid_y), 2).float()  # W(x), H(y), 2\n    grid.requires_grad = False\n    grid = grid.type_as(x)\n    vgrid = grid + flow\n    # scale grid to [-1,1]\n    vgrid_x = 2.0 * vgrid[:, :, :, 0] / max(W - 1, 1) - 1.0\n    vgrid_y = 2.0 * vgrid[:, :, :, 1] / max(H - 1, 1) - 1.0\n    vgrid_scaled = torch.stack((vgrid_x, vgrid_y), dim=3)\n    output = F.grid_sample(x, vgrid_scaled, mode=interp_mode, padding_mode=padding_mode)\n    return output\n'"
codes/models/modules/DCNv2/__init__.py,0,b''
codes/models/modules/DCNv2/dcn_v2.py,15,"b""#!/usr/bin/env python\n\nimport math\nimport logging\nimport torch\nfrom torch import nn\nfrom torch.autograd import Function\nfrom torch.nn.modules.utils import _pair\nfrom torch.autograd.function import once_differentiable\n\nimport _ext as _backend\nlogger = logging.getLogger('base')\n\n\nclass _DCNv2(Function):\n    @staticmethod\n    def forward(ctx, input, offset, mask, weight, bias, stride, padding, dilation,\n                deformable_groups):\n        ctx.stride = _pair(stride)\n        ctx.padding = _pair(padding)\n        ctx.dilation = _pair(dilation)\n        ctx.kernel_size = _pair(weight.shape[2:4])\n        ctx.deformable_groups = deformable_groups\n        output = _backend.dcn_v2_forward(input, weight, bias, offset, mask, ctx.kernel_size[0],\n                                         ctx.kernel_size[1], ctx.stride[0], ctx.stride[1],\n                                         ctx.padding[0], ctx.padding[1], ctx.dilation[0],\n                                         ctx.dilation[1], ctx.deformable_groups)\n        ctx.save_for_backward(input, offset, mask, weight, bias)\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        input, offset, mask, weight, bias = ctx.saved_tensors\n        grad_input, grad_offset, grad_mask, grad_weight, grad_bias = \\\n            _backend.dcn_v2_backward(input, weight,\n                                     bias,\n                                     offset, mask,\n                                     grad_output,\n                                     ctx.kernel_size[0], ctx.kernel_size[1],\n                                     ctx.stride[0], ctx.stride[1],\n                                     ctx.padding[0], ctx.padding[1],\n                                     ctx.dilation[0], ctx.dilation[1],\n                                     ctx.deformable_groups)\n\n        return grad_input, grad_offset, grad_mask, grad_weight, grad_bias,\\\n            None, None, None, None,\n\n\ndcn_v2_conv = _DCNv2.apply\n\n\nclass DCNv2(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation=1,\n                 deformable_groups=1):\n        super(DCNv2, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _pair(kernel_size)\n        self.stride = _pair(stride)\n        self.padding = _pair(padding)\n        self.dilation = _pair(dilation)\n        self.deformable_groups = deformable_groups\n\n        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels, *self.kernel_size))\n        self.bias = nn.Parameter(torch.Tensor(out_channels))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n        self.bias.data.zero_()\n\n    def forward(self, input, offset, mask):\n        assert 2 * self.deformable_groups * self.kernel_size[0] * self.kernel_size[1] == \\\n            offset.shape[1]\n        assert self.deformable_groups * self.kernel_size[0] * self.kernel_size[1] == \\\n            mask.shape[1]\n        return dcn_v2_conv(input, offset, mask, self.weight, self.bias, self.stride, self.padding,\n                           self.dilation, self.deformable_groups)\n\n\nclass DCN(DCNv2):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation=1,\n                 deformable_groups=1):\n        super(DCN, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation,\n                                  deformable_groups)\n\n        channels_ = self.deformable_groups * 3 * self.kernel_size[0] * self.kernel_size[1]\n        self.conv_offset_mask = nn.Conv2d(self.in_channels, channels_, kernel_size=self.kernel_size,\n                                          stride=self.stride, padding=self.padding, bias=True)\n        self.init_offset()\n\n    def init_offset(self):\n        self.conv_offset_mask.weight.data.zero_()\n        self.conv_offset_mask.bias.data.zero_()\n\n    def forward(self, input):\n        out = self.conv_offset_mask(input)\n        o1, o2, mask = torch.chunk(out, 3, dim=1)\n        offset = torch.cat((o1, o2), dim=1)\n        mask = torch.sigmoid(mask)\n        return dcn_v2_conv(input, offset, mask, self.weight, self.bias, self.stride, self.padding,\n                           self.dilation, self.deformable_groups)\n\n\nclass DCN_sep(DCNv2):\n    '''Use other features to generate offsets and masks'''\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation=1,\n                 deformable_groups=1):\n        super(DCN_sep, self).__init__(in_channels, out_channels, kernel_size, stride, padding,\n                                      dilation, deformable_groups)\n\n        channels_ = self.deformable_groups * 3 * self.kernel_size[0] * self.kernel_size[1]\n        self.conv_offset_mask = nn.Conv2d(self.in_channels, channels_, kernel_size=self.kernel_size,\n                                          stride=self.stride, padding=self.padding, bias=True)\n        self.init_offset()\n\n    def init_offset(self):\n        self.conv_offset_mask.weight.data.zero_()\n        self.conv_offset_mask.bias.data.zero_()\n\n    def forward(self, input, fea):\n        '''input: input features for deformable conv\n        fea: other features used for generating offsets and mask'''\n        out = self.conv_offset_mask(fea)\n        o1, o2, mask = torch.chunk(out, 3, dim=1)\n        offset = torch.cat((o1, o2), dim=1)\n\n        offset_mean = torch.mean(torch.abs(offset))\n        if offset_mean > 100:\n            logger.warning('Offset mean is {}, larger than 100.'.format(offset_mean))\n\n        mask = torch.sigmoid(mask)\n        return dcn_v2_conv(input, offset, mask, self.weight, self.bias, self.stride, self.padding,\n                           self.dilation, self.deformable_groups)\n\n\nclass _DCNv2Pooling(Function):\n    @staticmethod\n    def forward(ctx, input, rois, offset, spatial_scale, pooled_size, output_dim, no_trans,\n                group_size=1, part_size=None, sample_per_part=4, trans_std=.0):\n        ctx.spatial_scale = spatial_scale\n        ctx.no_trans = int(no_trans)\n        ctx.output_dim = output_dim\n        ctx.group_size = group_size\n        ctx.pooled_size = pooled_size\n        ctx.part_size = pooled_size if part_size is None else part_size\n        ctx.sample_per_part = sample_per_part\n        ctx.trans_std = trans_std\n\n        output, output_count = \\\n            _backend.dcn_v2_psroi_pooling_forward(input, rois, offset,\n                                                  ctx.no_trans, ctx.spatial_scale,\n                                                  ctx.output_dim, ctx.group_size,\n                                                  ctx.pooled_size, ctx.part_size,\n                                                  ctx.sample_per_part, ctx.trans_std)\n        ctx.save_for_backward(input, rois, offset, output_count)\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        input, rois, offset, output_count = ctx.saved_tensors\n        grad_input, grad_offset = \\\n            _backend.dcn_v2_psroi_pooling_backward(grad_output,\n                                                   input,\n                                                   rois,\n                                                   offset,\n                                                   output_count,\n                                                   ctx.no_trans,\n                                                   ctx.spatial_scale,\n                                                   ctx.output_dim,\n                                                   ctx.group_size,\n                                                   ctx.pooled_size,\n                                                   ctx.part_size,\n                                                   ctx.sample_per_part,\n                                                   ctx.trans_std)\n\n        return grad_input, None, grad_offset, \\\n            None, None, None, None, None, None, None, None\n\n\ndcn_v2_pooling = _DCNv2Pooling.apply\n\n\nclass DCNv2Pooling(nn.Module):\n    def __init__(self, spatial_scale, pooled_size, output_dim, no_trans, group_size=1,\n                 part_size=None, sample_per_part=4, trans_std=.0):\n        super(DCNv2Pooling, self).__init__()\n        self.spatial_scale = spatial_scale\n        self.pooled_size = pooled_size\n        self.output_dim = output_dim\n        self.no_trans = no_trans\n        self.group_size = group_size\n        self.part_size = pooled_size if part_size is None else part_size\n        self.sample_per_part = sample_per_part\n        self.trans_std = trans_std\n\n    def forward(self, input, rois, offset):\n        assert input.shape[1] == self.output_dim\n        if self.no_trans:\n            offset = input.new()\n        return dcn_v2_pooling(input, rois, offset, self.spatial_scale, self.pooled_size,\n                              self.output_dim, self.no_trans, self.group_size, self.part_size,\n                              self.sample_per_part, self.trans_std)\n\n\nclass DCNPooling(DCNv2Pooling):\n    def __init__(self, spatial_scale, pooled_size, output_dim, no_trans, group_size=1,\n                 part_size=None, sample_per_part=4, trans_std=.0, deform_fc_dim=1024):\n        super(DCNPooling, self).__init__(spatial_scale, pooled_size, output_dim, no_trans,\n                                         group_size, part_size, sample_per_part, trans_std)\n\n        self.deform_fc_dim = deform_fc_dim\n\n        if not no_trans:\n            self.offset_mask_fc = nn.Sequential(\n                nn.Linear(self.pooled_size * self.pooled_size * self.output_dim,\n                          self.deform_fc_dim), nn.ReLU(inplace=True),\n                nn.Linear(self.deform_fc_dim, self.deform_fc_dim), nn.ReLU(inplace=True),\n                nn.Linear(self.deform_fc_dim, self.pooled_size * self.pooled_size * 3))\n            self.offset_mask_fc[4].weight.data.zero_()\n            self.offset_mask_fc[4].bias.data.zero_()\n\n    def forward(self, input, rois):\n        offset = input.new()\n\n        if not self.no_trans:\n\n            # do roi_align first\n            n = rois.shape[0]\n            roi = dcn_v2_pooling(\n                input,\n                rois,\n                offset,\n                self.spatial_scale,\n                self.pooled_size,\n                self.output_dim,\n                True,  # no trans\n                self.group_size,\n                self.part_size,\n                self.sample_per_part,\n                self.trans_std)\n\n            # build mask and offset\n            offset_mask = self.offset_mask_fc(roi.view(n, -1))\n            offset_mask = offset_mask.view(n, 3, self.pooled_size, self.pooled_size)\n            o1, o2, mask = torch.chunk(offset_mask, 3, dim=1)\n            offset = torch.cat((o1, o2), dim=1)\n            mask = torch.sigmoid(mask)\n\n            # do pooling with offset and mask\n            return dcn_v2_pooling(input, rois, offset, self.spatial_scale, self.pooled_size,\n                                  self.output_dim, self.no_trans, self.group_size, self.part_size,\n                                  self.sample_per_part, self.trans_std) * mask\n        # only roi_align\n        return dcn_v2_pooling(input, rois, offset, self.spatial_scale, self.pooled_size,\n                              self.output_dim, self.no_trans, self.group_size, self.part_size,\n                              self.sample_per_part, self.trans_std)\n"""
codes/models/modules/DCNv2/setup.py,5,"b'#!/usr/bin/env python\n\nimport os\nimport glob\n\nimport torch\n\nfrom torch.utils.cpp_extension import CUDA_HOME\nfrom torch.utils.cpp_extension import CppExtension\nfrom torch.utils.cpp_extension import CUDAExtension\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nrequirements = [""torch"", ""torchvision""]\n\n\ndef get_extensions():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    extensions_dir = os.path.join(this_dir, ""src"")\n\n    main_file = glob.glob(os.path.join(extensions_dir, ""*.cpp""))\n    source_cpu = glob.glob(os.path.join(extensions_dir, ""cpu"", ""*.cpp""))\n    source_cuda = glob.glob(os.path.join(extensions_dir, ""cuda"", ""*.cu""))\n\n    sources = main_file + source_cpu\n    extension = CppExtension\n    extra_compile_args = {""cxx"": []}\n    define_macros = []\n\n    if torch.cuda.is_available() and CUDA_HOME is not None:\n        extension = CUDAExtension\n        sources += source_cuda\n        define_macros += [(""WITH_CUDA"", None)]\n        extra_compile_args[""nvcc""] = [\n            ""-DCUDA_HAS_FP16=1"",\n            ""-D__CUDA_NO_HALF_OPERATORS__"",\n            ""-D__CUDA_NO_HALF_CONVERSIONS__"",\n            ""-D__CUDA_NO_HALF2_OPERATORS__"",\n        ]\n    else:\n        raise NotImplementedError(\'Cuda is not availabel\')\n\n    sources = [os.path.join(extensions_dir, s) for s in sources]\n    include_dirs = [extensions_dir]\n    ext_modules = [\n        extension(\n            ""_ext"",\n            sources,\n            include_dirs=include_dirs,\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args,\n        )\n    ]\n    return ext_modules\n\n\nsetup(\n    name=""DCNv2"",\n    version=""0.1"",\n    author=""charlesshang"",\n    url=""https://github.com/charlesshang/DCNv2"",\n    description=""deformable convolutional networks"",\n    packages=find_packages(exclude=(\n        ""configs"",\n        ""tests"",\n    )),\n    # install_requires=requirements,\n    ext_modules=get_extensions(),\n    cmdclass={""build_ext"": torch.utils.cpp_extension.BuildExtension},\n)'"
codes/models/modules/DCNv2/test.py,37,"b'#!/usr/bin/env python\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport time\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import gradcheck\n\nfrom dcn_v2 import dcn_v2_conv, DCNv2, DCN\nfrom dcn_v2 import dcn_v2_pooling, DCNv2Pooling, DCNPooling\n\ndeformable_groups = 1\nN, inC, inH, inW = 2, 2, 4, 4\noutC = 2\nkH, kW = 3, 3\n\n\ndef conv_identify(weight, bias):\n    weight.data.zero_()\n    bias.data.zero_()\n    o, i, h, w = weight.shape\n    y = h//2\n    x = w//2\n    for p in range(i):\n        for q in range(o):\n            if p == q:\n                weight.data[q, p, y, x] = 1.0\n\n\ndef check_zero_offset():\n    conv_offset = nn.Conv2d(inC, deformable_groups * 2 * kH * kW,\n                            kernel_size=(kH, kW),\n                            stride=(1, 1),\n                            padding=(1, 1),\n                            bias=True).cuda()\n\n    conv_mask = nn.Conv2d(inC, deformable_groups * 1 * kH * kW,\n                          kernel_size=(kH, kW),\n                          stride=(1, 1),\n                          padding=(1, 1),\n                          bias=True).cuda()\n\n    dcn_v2 = DCNv2(inC, outC, (kH, kW),\n                   stride=1, padding=1, dilation=1,\n                   deformable_groups=deformable_groups).cuda()\n\n    conv_offset.weight.data.zero_()\n    conv_offset.bias.data.zero_()\n    conv_mask.weight.data.zero_()\n    conv_mask.bias.data.zero_()\n    conv_identify(dcn_v2.weight, dcn_v2.bias)\n\n    input = torch.randn(N, inC, inH, inW).cuda()\n    offset = conv_offset(input)\n    mask = conv_mask(input)\n    mask = torch.sigmoid(mask)\n    output = dcn_v2(input, offset, mask)\n    output *= 2\n    d = (input - output).abs().max()\n    if d < 1e-10:\n        print(\'Zero offset passed\')\n    else:\n        print(\'Zero offset failed\')\n        print(input)\n        print(output)\n\ndef check_gradient_dconv():\n\n    input = torch.rand(N, inC, inH, inW).cuda() * 0.01\n    input.requires_grad = True\n\n    offset = torch.randn(N, deformable_groups * 2 * kW * kH, inH, inW).cuda() * 2\n    # offset.data.zero_()\n    # offset.data -= 0.5\n    offset.requires_grad = True\n\n    mask = torch.rand(N, deformable_groups * 1 * kW * kH, inH, inW).cuda()\n    # mask.data.zero_()\n    mask.requires_grad = True\n    mask = torch.sigmoid(mask)\n\n    weight = torch.randn(outC, inC, kH, kW).cuda()\n    weight.requires_grad = True\n\n    bias = torch.rand(outC).cuda()\n    bias.requires_grad = True\n\n    stride = 1\n    padding = 1\n    dilation = 1\n\n    print(\'check_gradient_dconv: \',\n          gradcheck(dcn_v2_conv, (input, offset, mask, weight, bias,\n                    stride, padding, dilation, deformable_groups),\n                    eps=1e-3, atol=1e-4, rtol=1e-2))\n\n\ndef check_pooling_zero_offset():\n\n    input = torch.randn(2, 16, 64, 64).cuda().zero_()\n    input[0, :, 16:26, 16:26] = 1.\n    input[1, :, 10:20, 20:30] = 2.\n    rois = torch.tensor([\n        [0, 65, 65, 103, 103],\n        [1, 81, 41, 119, 79],\n    ]).cuda().float()\n    pooling = DCNv2Pooling(spatial_scale=1.0 / 4,\n                           pooled_size=7,\n                           output_dim=16,\n                           no_trans=True,\n                           group_size=1,\n                           trans_std=0.0).cuda()\n\n    out = pooling(input, rois, input.new())\n    s = \', \'.join([\'%f\' % out[i, :, :, :].mean().item()\n                   for i in range(rois.shape[0])])\n    print(s)\n\n    dpooling = DCNv2Pooling(spatial_scale=1.0 / 4,\n                            pooled_size=7,\n                            output_dim=16,\n                            no_trans=False,\n                            group_size=1,\n                            trans_std=0.0).cuda()\n    offset = torch.randn(20, 2, 7, 7).cuda().zero_()\n    dout = dpooling(input, rois, offset)\n    s = \', \'.join([\'%f\' % dout[i, :, :, :].mean().item()\n                   for i in range(rois.shape[0])])\n    print(s)\n\n\ndef check_gradient_dpooling():\n    input = torch.randn(2, 3, 5, 5).cuda() * 0.01\n    N = 4\n    batch_inds = torch.randint(2, (N, 1)).cuda().float()\n    x = torch.rand((N, 1)).cuda().float() * 15\n    y = torch.rand((N, 1)).cuda().float() * 15\n    w = torch.rand((N, 1)).cuda().float() * 10\n    h = torch.rand((N, 1)).cuda().float() * 10\n    rois = torch.cat((batch_inds, x, y, x + w, y + h), dim=1)\n    offset = torch.randn(N, 2, 3, 3).cuda()\n    input.requires_grad = True\n    offset.requires_grad = True\n\n    spatial_scale = 1.0 / 4\n    pooled_size = 3\n    output_dim = 3\n    no_trans = 0\n    group_size = 1\n    trans_std = 0.0\n    sample_per_part = 4\n    part_size = pooled_size\n\n    print(\'check_gradient_dpooling:\',\n          gradcheck(dcn_v2_pooling, (input, rois, offset,\n                                     spatial_scale,\n                                     pooled_size,\n                                     output_dim,\n                                     no_trans,\n                                     group_size,\n                                     part_size,\n                                     sample_per_part,\n                                     trans_std),\n                    eps=1e-4))\n\n\ndef example_dconv():\n    input = torch.randn(2, 64, 128, 128).cuda()\n    # wrap all things (offset and mask) in DCN\n    dcn = DCN(64, 64, kernel_size=(3, 3), stride=1,\n              padding=1, deformable_groups=2).cuda()\n    # print(dcn.weight.shape, input.shape)\n    output = dcn(input)\n    targert = output.new(*output.size())\n    targert.data.uniform_(-0.01, 0.01)\n    error = (targert - output).mean()\n    error.backward()\n    print(output.shape)\n\n\ndef example_dpooling():\n    input = torch.randn(2, 32, 64, 64).cuda()\n    batch_inds = torch.randint(2, (20, 1)).cuda().float()\n    x = torch.randint(256, (20, 1)).cuda().float()\n    y = torch.randint(256, (20, 1)).cuda().float()\n    w = torch.randint(64, (20, 1)).cuda().float()\n    h = torch.randint(64, (20, 1)).cuda().float()\n    rois = torch.cat((batch_inds, x, y, x + w, y + h), dim=1)\n    offset = torch.randn(20, 2, 7, 7).cuda()\n    input.requires_grad = True\n    offset.requires_grad = True\n\n    # normal roi_align\n    pooling = DCNv2Pooling(spatial_scale=1.0 / 4,\n                           pooled_size=7,\n                           output_dim=32,\n                           no_trans=True,\n                           group_size=1,\n                           trans_std=0.1).cuda()\n\n    # deformable pooling\n    dpooling = DCNv2Pooling(spatial_scale=1.0 / 4,\n                            pooled_size=7,\n                            output_dim=32,\n                            no_trans=False,\n                            group_size=1,\n                            trans_std=0.1).cuda()\n\n    out = pooling(input, rois, offset)\n    dout = dpooling(input, rois, offset)\n    print(out.shape)\n    print(dout.shape)\n\n    target_out = out.new(*out.size())\n    target_out.data.uniform_(-0.01, 0.01)\n    target_dout = dout.new(*dout.size())\n    target_dout.data.uniform_(-0.01, 0.01)\n    e = (target_out - out).mean()\n    e.backward()\n    e = (target_dout - dout).mean()\n    e.backward()\n\n\ndef example_mdpooling():\n    input = torch.randn(2, 32, 64, 64).cuda()\n    input.requires_grad = True\n    batch_inds = torch.randint(2, (20, 1)).cuda().float()\n    x = torch.randint(256, (20, 1)).cuda().float()\n    y = torch.randint(256, (20, 1)).cuda().float()\n    w = torch.randint(64, (20, 1)).cuda().float()\n    h = torch.randint(64, (20, 1)).cuda().float()\n    rois = torch.cat((batch_inds, x, y, x + w, y + h), dim=1)\n\n    # mdformable pooling (V2)\n    dpooling = DCNPooling(spatial_scale=1.0 / 4,\n                          pooled_size=7,\n                          output_dim=32,\n                          no_trans=False,\n                          group_size=1,\n                          trans_std=0.1,\n                          deform_fc_dim=1024).cuda()\n\n    dout = dpooling(input, rois)\n    target = dout.new(*dout.size())\n    target.data.uniform_(-0.1, 0.1)\n    error = (target - dout).mean()\n    error.backward()\n    print(dout.shape)\n\n\nif __name__ == \'__main__\':\n\n    example_dconv()\n    example_dpooling()\n    example_mdpooling()\n\n    check_pooling_zero_offset()\n    # zero offset check\n    if inC == outC:\n        check_zero_offset()\n\n    check_gradient_dpooling()\n    check_gradient_dconv()\n    # """"""\n    # ****** Note: backward is not reentrant error may not be a serious problem,\n    # ****** since the max error is less than 1e-7,\n    # ****** Still looking for what trigger this problem\n    # """"""\n'"
