file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\nfrom codecs import open\nfrom os import path\n\nfrom mushroom_rl import __version__\n\nhere = path.abspath(path.dirname(__file__))\n\nrequires_list = []\nwith open(path.join(here, \'requirements.txt\'), encoding=\'utf-8\') as f:\n    for line in f:\n        requires_list.append(str(line))\n\nextras = {\n    \'gym\': [\'gym\'],\n    \'atari\': [\'atari_py~=0.2.0\', \'Pillow\', \'opencv-python\'],\n    \'box2d\': [\'box2d-py~=2.3.5\'],\n    \'bullet\': [\'pybullet\'],\n    \'mujoco\': [\'mujoco_py\'],\n    \'plots\': [\'pyqtgraph\']\n}\n\nall_deps = []\nfor group_name in extras:\n    if group_name not in [\'mujoco\', \'plots\']:\n        all_deps += extras[group_name]\nextras[\'all\'] = all_deps\n\nlong_description = \'Mushroom is a Python Reinforcement Learning (RL) library\' \\\n                   \' whose modularity allows to easily use well-known Python\' \\\n                   \' libraries for tensor computation (e.g. PyTorch, Tensorflow)\' \\\n                   \' and RL benchmarks (e.g. OpenAI Gym, PyBullet, Deepmind\' \\\n                   \' Control Suite). It allows to perform RL experiments in a\' \\\n                   \' simple way providing classical RL algorithms\' \\\n                   \' (e.g. Q-Learning, SARSA, FQI), and deep RL algorithms\' \\\n                   \' (e.g. DQN, DDPG, SAC, TD3, TRPO, PPO). Full documentation\' \\\n                   \' available at http://mushroomrl.readthedocs.io/en/latest/.\'\n\nsetup(\n    name=\'mushroom-rl\',\n    version=__version__,\n    description=\'A Python toolkit for Reinforcement Learning experiments.\',\n    long_description=long_description,\n    url=\'https://github.com/MushroomRL/mushroom-rl\',\n    author=""Carlo D\'Eramo"",\n    author_email=\'carlo.deramo@gmail.com\',\n    license=\'MIT\',\n    packages=[package for package in find_packages()\n              if package.startswith(\'mushroom_rl\')],\n    zip_safe=False,\n    install_requires=requires_list,\n    extras_require=extras,\n    classifiers=[""Programming Language :: Python :: 3"",\n                 ""License :: OSI Approved :: MIT License"",\n                 ""Operating System :: OS Independent"",\n                 ]\n)\n'"
docs/__init__.py,0,b''
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Mushroom documentation build configuration file, created by\n# sphinx-quickstart on Wed Dec  6 10:51:04 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\nfrom mushroom_rl import __version__\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.napoleon\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'MushroomRL\'\ncopyright = u\'2018, 2019, 2020 Carlo D\\\'Eramo, Davide Tateo\'\nauthor = u\'Carlo D\\\'Eramo\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'.\'.join(__version__.split(\'.\')[:-1])\n\n# The full version, including alpha/beta/rc tags.\nrelease = __version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\nhtml_sidebars = {\n    \'**\': [\n        \'relations.html\',  # needs \'show_related\': True theme option to display\n        \'searchbox.html\',\n    ]\n}\n\nhtml_show_sourcelink=False\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'MushroomRLdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'MushroomRL.tex\', u\'MushroomRL Documentation\',\n     u\'Carlo D\\\'Eramo, Davide Tateo\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'mushroom_rl\', u\'MushroomRL Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'MushroomRL\', u\'MushroomRL Documentation\',\n     author, \'MushroomRL\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n\n# -- Options for Epub output ----------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\nepub_author = author\nepub_publisher = author\nepub_copyright = copyright\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n\n# -- Options for autodoc ---------------------------------------------------\n\nautodoc_member_order = \'bysource\'\nautodoc_mock_imports = [\'torch\', \'pybullet\', \'dm_control\', \'mujoco_py\', \'glfw\']\n\ndef skip(app, what, name, obj, skip, options):\n    if name == ""__init__"" or name == ""__call__"":\n        return False\n    return skip\n\ndef setup(app):\n    app.connect(""autodoc-skip-member"", skip)\n\n\n'"
examples/__init__.py,0,b''
examples/acrobot_dqn.py,5,"b""import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom mushroom_rl.algorithms.value import DQN\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import *\nfrom mushroom_rl.policy import EpsGreedy\nfrom mushroom_rl.approximators.parametric.torch_approximator import *\nfrom mushroom_rl.utils.dataset import compute_J\nfrom mushroom_rl.utils.parameters import Parameter, LinearParameter\n\n\nclass Network(nn.Module):\n    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n        super().__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._h1 = nn.Linear(n_input, n_features)\n        self._h2 = nn.Linear(n_features, n_features)\n        self._h3 = nn.Linear(n_features, n_output)\n\n        nn.init.xavier_uniform_(self._h1.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h2.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h3.weight,\n                                gain=nn.init.calculate_gain('linear'))\n\n    def forward(self, state, action=None):\n        features1 = F.relu(self._h1(torch.squeeze(state, 1).float()))\n        features2 = F.relu(self._h2(features1))\n        q = self._h3(features2)\n\n        if action is None:\n            return q\n        else:\n            action = action.long()\n            q_acted = torch.squeeze(q.gather(1, action))\n\n            return q_acted\n\n\ndef experiment(n_epochs, n_steps, n_steps_test):\n    np.random.seed()\n\n    # MDP\n    horizon = 1000\n    gamma = 0.99\n    gamma_eval = 1.\n    mdp = Gym('Acrobot-v1', horizon, gamma)\n\n    # Policy\n    epsilon = LinearParameter(value=1., threshold_value=.01, n=5000)\n    epsilon_test = Parameter(value=0.)\n    epsilon_random = Parameter(value=1.)\n    pi = EpsGreedy(epsilon=epsilon_random)\n\n    # Settings\n    initial_replay_size = 500\n    max_replay_size = 5000\n    target_update_frequency = 100\n    batch_size = 200\n    n_features = 80\n    train_frequency = 1\n\n    # Approximator\n    input_shape = mdp.info.observation_space.shape\n    approximator_params = dict(network=Network,\n                               optimizer={'class': optim.Adam,\n                                          'params': {'lr': .001}},\n                               loss=F.smooth_l1_loss,\n                               n_features=n_features,\n                               input_shape=input_shape,\n                               output_shape=mdp.info.action_space.size,\n                               n_actions=mdp.info.action_space.n)\n\n    # Agent\n    agent = DQN(mdp.info, pi, TorchApproximator,\n                approximator_params=approximator_params, batch_size=batch_size,\n                n_approximators=1, initial_replay_size=initial_replay_size,\n                max_replay_size=max_replay_size,\n                target_update_frequency=target_update_frequency)\n\n    # Algorithm\n    core = Core(agent, mdp)\n\n    core.learn(n_steps=initial_replay_size, n_steps_per_fit=initial_replay_size)\n\n    # RUN\n    pi.set_epsilon(epsilon_test)\n    dataset = core.evaluate(n_steps=n_steps_test, render=False)\n    J = compute_J(dataset, gamma_eval)\n    print('J: ', np.mean(J))\n\n    for n in range(n_epochs):\n        print('Epoch: ', n)\n        pi.set_epsilon(epsilon)\n        core.learn(n_steps=n_steps, n_steps_per_fit=train_frequency)\n        pi.set_epsilon(epsilon_test)\n        dataset = core.evaluate(n_steps=n_steps_test, render=False)\n        J = compute_J(dataset, gamma_eval)\n        print('J: ', np.mean(J))\n\n    print('Press a button to visualize acrobot')\n    input()\n    core.evaluate(n_episodes=5, render=True)\n\n\nif __name__ == '__main__':\n    experiment(n_epochs=20, n_steps=1000, n_steps_test=2000)\n"""
examples/atari_dqn.py,5,"b'import argparse\nimport datetime\nimport pathlib\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom mushroom_rl.algorithms.value import AveragedDQN, CategoricalDQN, DQN, DoubleDQN\nfrom mushroom_rl.approximators.parametric import TorchApproximator\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import *\nfrom mushroom_rl.policy import EpsGreedy\nfrom mushroom_rl.utils.dataset import compute_metrics\nfrom mushroom_rl.utils.parameters import LinearParameter, Parameter\nfrom mushroom_rl.utils.replay_memory import PrioritizedReplayMemory\n\n""""""\nThis script runs Atari experiments with DQN, and some of its variants, as\npresented in:\n""Human-Level Control Through Deep Reinforcement Learning"". Mnih V. et al.. 2015.\n\n""""""\n\n\nclass Network(nn.Module):\n    n_features = 512\n\n    def __init__(self, input_shape, output_shape, **kwargs):\n        super().__init__()\n\n        n_input = input_shape[0]\n        n_output = output_shape[0]\n\n        self._h1 = nn.Conv2d(n_input, 32, kernel_size=8, stride=4)\n        self._h2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n        self._h3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n        self._h4 = nn.Linear(3136, self.n_features)\n        self._h5 = nn.Linear(self.n_features, n_output)\n\n        nn.init.xavier_uniform_(self._h1.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n        nn.init.xavier_uniform_(self._h2.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n        nn.init.xavier_uniform_(self._h3.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n        nn.init.xavier_uniform_(self._h4.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n        nn.init.xavier_uniform_(self._h5.weight,\n                                gain=nn.init.calculate_gain(\'linear\'))\n\n    def forward(self, state, action=None):\n        h = F.relu(self._h1(state.float() / 255.))\n        h = F.relu(self._h2(h))\n        h = F.relu(self._h3(h))\n        h = F.relu(self._h4(h.view(-1, 3136)))\n        q = self._h5(h)\n\n        if action is None:\n            return q\n        else:\n            q_acted = torch.squeeze(q.gather(1, action.long()))\n\n            return q_acted\n\n\nclass FeatureNetwork(nn.Module):\n    def __init__(self, input_shape, output_shape, **kwargs):\n        super().__init__()\n\n        n_input = input_shape[0]\n\n        self._h1 = nn.Conv2d(n_input, 32, kernel_size=8, stride=4)\n        self._h2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n        self._h3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n        self._h4 = nn.Linear(3136, Network.n_features)\n\n        nn.init.xavier_uniform_(self._h1.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n        nn.init.xavier_uniform_(self._h2.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n        nn.init.xavier_uniform_(self._h3.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n        nn.init.xavier_uniform_(self._h4.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n\n    def forward(self, state, action=None):\n        h = F.relu(self._h1(state.float() / 255.))\n        h = F.relu(self._h2(h))\n        h = F.relu(self._h3(h))\n        h = F.relu(self._h4(h.view(-1, 3136)))\n\n        return h\n\n\ndef print_epoch(epoch):\n    print(\'################################################################\')\n    print(\'Epoch: \', epoch)\n    print(\'----------------------------------------------------------------\')\n\n\ndef get_stats(dataset):\n    score = compute_metrics(dataset)\n    print((\'min_reward: %f, max_reward: %f, mean_reward: %f,\'\n          \' games_completed: %d\' % score))\n\n    return score\n\n\ndef experiment():\n    np.random.seed()\n\n    # Argument parser\n    parser = argparse.ArgumentParser()\n\n    arg_game = parser.add_argument_group(\'Game\')\n    arg_game.add_argument(""--name"",\n                          type=str,\n                          default=\'BreakoutDeterministic-v4\',\n                          help=\'Gym ID of the Atari game.\')\n    arg_game.add_argument(""--screen-width"", type=int, default=84,\n                          help=\'Width of the game screen.\')\n    arg_game.add_argument(""--screen-height"", type=int, default=84,\n                          help=\'Height of the game screen.\')\n\n    arg_mem = parser.add_argument_group(\'Replay Memory\')\n    arg_mem.add_argument(""--initial-replay-size"", type=int, default=50000,\n                         help=\'Initial size of the replay memory.\')\n    arg_mem.add_argument(""--max-replay-size"", type=int, default=500000,\n                         help=\'Max size of the replay memory.\')\n    arg_mem.add_argument(""--prioritized"", action=\'store_true\',\n                         help=\'Whether to use prioritized memory or not.\')\n\n    arg_net = parser.add_argument_group(\'Deep Q-Network\')\n    arg_net.add_argument(""--optimizer"",\n                         choices=[\'adadelta\',\n                                  \'adam\',\n                                  \'rmsprop\',\n                                  \'rmspropcentered\'],\n                         default=\'rmsprop\',\n                         help=\'Name of the optimizer to use.\')\n    arg_net.add_argument(""--learning-rate"", type=float, default=.00025,\n                         help=\'Learning rate value of the optimizer.\')\n    arg_net.add_argument(""--decay"", type=float, default=.95,\n                         help=\'Discount factor for the history coming from the\'\n                              \'gradient momentum in rmspropcentered and\'\n                              \'rmsprop\')\n    arg_net.add_argument(""--epsilon"", type=float, default=1e-8,\n                         help=\'Epsilon term used in rmspropcentered and\'\n                              \'rmsprop\')\n\n    arg_alg = parser.add_argument_group(\'Algorithm\')\n    arg_alg.add_argument(""--algorithm"", choices=[\'dqn\', \'ddqn\', \'adqn\', \'cdqn\'],\n                         default=\'dqn\',\n                         help=\'Name of the algorithm. dqn is for standard\'\n                              \'DQN, ddqn is for Double DQN and adqn is for\'\n                              \'Averaged DQN.\')\n    arg_alg.add_argument(""--n-approximators"", type=int, default=1,\n                         help=""Number of approximators used in the ensemble for""\n                              ""Averaged DQN."")\n    arg_alg.add_argument(""--batch-size"", type=int, default=32,\n                         help=\'Batch size for each fit of the network.\')\n    arg_alg.add_argument(""--history-length"", type=int, default=4,\n                         help=\'Number of frames composing a state.\')\n    arg_alg.add_argument(""--target-update-frequency"", type=int, default=10000,\n                         help=\'Number of collected samples before each update\'\n                              \'of the target network.\')\n    arg_alg.add_argument(""--evaluation-frequency"", type=int, default=250000,\n                         help=\'Number of collected samples before each\'\n                              \'evaluation. An epoch ends after this number of\'\n                              \'steps\')\n    arg_alg.add_argument(""--train-frequency"", type=int, default=4,\n                         help=\'Number of collected samples before each fit of\'\n                              \'the neural network.\')\n    arg_alg.add_argument(""--max-steps"", type=int, default=50000000,\n                         help=\'Total number of collected samples.\')\n    arg_alg.add_argument(""--final-exploration-frame"", type=int, default=1000000,\n                         help=\'Number of collected samples until the exploration\'\n                              \'rate stops decreasing.\')\n    arg_alg.add_argument(""--initial-exploration-rate"", type=float, default=1.,\n                         help=\'Initial value of the exploration rate.\')\n    arg_alg.add_argument(""--final-exploration-rate"", type=float, default=.1,\n                         help=\'Final value of the exploration rate. When it\'\n                              \'reaches this values, it stays constant.\')\n    arg_alg.add_argument(""--test-exploration-rate"", type=float, default=.05,\n                         help=\'Exploration rate used during evaluation.\')\n    arg_alg.add_argument(""--test-samples"", type=int, default=125000,\n                         help=\'Number of collected samples for each\'\n                              \'evaluation.\')\n    arg_alg.add_argument(""--max-no-op-actions"", type=int, default=30,\n                         help=\'Maximum number of no-op actions performed at the\'\n                              \'beginning of the episodes.\')\n    arg_alg.add_argument(""--n-atoms"", type=int, default=51,\n                         help=\'Number of atoms for Categorical DQN.\')\n    arg_alg.add_argument(""--v-min"", type=int, default=-10,\n                         help=\'Minimum action-value for Categorical DQN.\')\n    arg_alg.add_argument(""--v-max"", type=int, default=10,\n                         help=\'Maximum action-value for Categorical DQN.\')\n\n    arg_utils = parser.add_argument_group(\'Utils\')\n    arg_utils.add_argument(\'--use-cuda\', action=\'store_true\',\n                           help=\'Flag specifying whether to use the GPU.\')\n    arg_utils.add_argument(\'--load-path\', type=str,\n                           help=\'Path of the model to be loaded.\')\n    arg_utils.add_argument(\'--save\', action=\'store_true\',\n                           help=\'Flag specifying whether to save the model.\')\n    arg_utils.add_argument(\'--render\', action=\'store_true\',\n                           help=\'Flag specifying whether to render the game.\')\n    arg_utils.add_argument(\'--quiet\', action=\'store_true\',\n                           help=\'Flag specifying whether to hide the progress\'\n                                \'bar.\')\n    arg_utils.add_argument(\'--debug\', action=\'store_true\',\n                           help=\'Flag specifying whether the script has to be\'\n                                \'run in debug mode.\')\n\n    args = parser.parse_args()\n\n    scores = list()\n\n    optimizer = dict()\n    if args.optimizer == \'adam\':\n        optimizer[\'class\'] = optim.Adam\n        optimizer[\'params\'] = dict(lr=args.learning_rate,\n                                   eps=args.epsilon)\n    elif args.optimizer == \'adadelta\':\n        optimizer[\'class\'] = optim.Adadelta\n        optimizer[\'params\'] = dict(lr=args.learning_rate,\n                                   eps=args.epsilon)\n    elif args.optimizer == \'rmsprop\':\n        optimizer[\'class\'] = optim.RMSprop\n        optimizer[\'params\'] = dict(lr=args.learning_rate,\n                                   alpha=args.decay,\n                                   eps=args.epsilon)\n    elif args.optimizer == \'rmspropcentered\':\n        optimizer[\'class\'] = optim.RMSprop\n        optimizer[\'params\'] = dict(lr=args.learning_rate,\n                                   alpha=args.decay,\n                                   eps=args.epsilon,\n                                   centered=True)\n    else:\n        raise ValueError\n\n    # Evaluation of the model provided by the user.\n    if args.load_path:\n        # MDP\n        mdp = Atari(args.name, args.screen_width, args.screen_height,\n                    ends_at_life=False, history_length=args.history_length,\n                    max_no_op_actions=args.max_no_op_actions)\n\n        # Policy\n        epsilon_test = Parameter(value=args.test_exploration_rate)\n        pi = EpsGreedy(epsilon=epsilon_test)\n\n        # Approximator\n        input_shape = (args.history_length, args.screen_height,\n                       args.screen_width)\n        approximator_params = dict(\n            network=Network,\n            input_shape=input_shape,\n            output_shape=(mdp.info.action_space.n,),\n            n_actions=mdp.info.action_space.n,\n            load_path=args.load_path,\n            optimizer=optimizer,\n            loss=F.smooth_l1_loss,\n            use_cuda=args.use_cuda\n        )\n\n        approximator = TorchApproximator\n\n        # Agent\n        algorithm_params = dict(\n            batch_size=1,\n            train_frequency=1,\n            target_update_frequency=1,\n            initial_replay_size=0,\n            max_replay_size=0\n        )\n        agent = DQN(mdp.info, pi, approximator,\n                    approximator_params=approximator_params, **algorithm_params)\n\n        # Algorithm\n        core_test = Core(agent, mdp)\n\n        # Evaluate model\n        pi.set_epsilon(epsilon_test)\n        dataset = core_test.evaluate(n_steps=args.test_samples,\n                                     render=args.render,\n                                     quiet=args.quiet)\n        get_stats(dataset)\n    else:\n        # DQN learning run\n\n        # Summary folder\n        folder_name = \'./logs/atari_\' + args.algorithm + \'_\' + args.name +\\\n            \'_\' + datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\')\n        pathlib.Path(folder_name).mkdir(parents=True)\n\n        # Settings\n        if args.debug:\n            initial_replay_size = 50\n            max_replay_size = 500\n            train_frequency = 5\n            target_update_frequency = 10\n            test_samples = 20\n            evaluation_frequency = 50\n            max_steps = 1000\n        else:\n            initial_replay_size = args.initial_replay_size\n            max_replay_size = args.max_replay_size\n            train_frequency = args.train_frequency\n            target_update_frequency = args.target_update_frequency\n            test_samples = args.test_samples\n            evaluation_frequency = args.evaluation_frequency\n            max_steps = args.max_steps\n\n        # MDP\n        mdp = Atari(args.name, args.screen_width, args.screen_height,\n                    ends_at_life=True, history_length=args.history_length,\n                    max_no_op_actions=args.max_no_op_actions)\n\n        # Policy\n        epsilon = LinearParameter(value=args.initial_exploration_rate,\n                                  threshold_value=args.final_exploration_rate,\n                                  n=args.final_exploration_frame)\n        epsilon_test = Parameter(value=args.test_exploration_rate)\n        epsilon_random = Parameter(value=1)\n        pi = EpsGreedy(epsilon=epsilon_random)\n\n        class CategoricalLoss(nn.Module):\n            def forward(self, input, target):\n                input = input.clamp(1e-5)\n\n                return -torch.sum(target * torch.log(input))\n\n        # Approximator\n        input_shape = (args.history_length, args.screen_height,\n                       args.screen_width)\n        approximator_params = dict(\n            network=Network if args.algorithm != \'cdqn\' else FeatureNetwork,\n            input_shape=input_shape,\n            output_shape=(mdp.info.action_space.n,),\n            n_actions=mdp.info.action_space.n,\n            n_features=Network.n_features,\n            optimizer=optimizer,\n            loss=F.smooth_l1_loss if args.algorithm != \'cdqn\' else CategoricalLoss(),\n            use_cuda=args.use_cuda\n        )\n\n        approximator = TorchApproximator\n\n        if args.prioritized:\n            replay_memory = PrioritizedReplayMemory(\n                initial_replay_size, max_replay_size, alpha=.6,\n                beta=LinearParameter(.4, threshold_value=1,\n                                     n=max_steps // train_frequency)\n            )\n        else:\n            replay_memory = None\n\n            # Agent\n        algorithm_params = dict(\n            batch_size=args.batch_size,\n            n_approximators=args.n_approximators,\n            target_update_frequency=target_update_frequency // train_frequency,\n            replay_memory=replay_memory,\n            initial_replay_size=initial_replay_size,\n            max_replay_size=max_replay_size\n        )\n\n        if args.algorithm == \'dqn\':\n            agent = DQN(mdp.info, pi, approximator,\n                        approximator_params=approximator_params,\n                        **algorithm_params)\n        elif args.algorithm == \'ddqn\':\n            agent = DoubleDQN(mdp.info, pi, approximator,\n                              approximator_params=approximator_params,\n                              **algorithm_params)\n        elif args.algorithm == \'adqn\':\n            agent = AveragedDQN(mdp.info, pi, approximator,\n                                approximator_params=approximator_params,\n                                **algorithm_params)\n        elif args.algorithm == \'cdqn\':\n            agent = CategoricalDQN(mdp.info, pi,\n                                   approximator_params=approximator_params,\n                                   n_atoms=args.n_atoms, v_min=args.v_min,\n                                   v_max=args.v_max, **algorithm_params)\n\n        # Algorithm\n        core = Core(agent, mdp)\n\n        # RUN\n\n        # Fill replay memory with random dataset\n        print_epoch(0)\n        core.learn(n_steps=initial_replay_size,\n                   n_steps_per_fit=initial_replay_size, quiet=args.quiet)\n\n        if args.save:\n            np.save(folder_name + \'/weights-exp-0-0.npy\',\n                    agent.approximator.get_weights())\n\n        # Evaluate initial policy\n        pi.set_epsilon(epsilon_test)\n        mdp.set_episode_end(False)\n        dataset = core.evaluate(n_steps=test_samples, render=args.render,\n                                quiet=args.quiet)\n        scores.append(get_stats(dataset))\n\n        np.save(folder_name + \'/scores.npy\', scores)\n        for n_epoch in range(1, max_steps // evaluation_frequency + 1):\n            print_epoch(n_epoch)\n            print(\'- Learning:\')\n            # learning step\n            pi.set_epsilon(epsilon)\n            mdp.set_episode_end(True)\n            core.learn(n_steps=evaluation_frequency,\n                       n_steps_per_fit=train_frequency, quiet=args.quiet)\n\n            if args.save:\n                np.save(folder_name + \'/weights-exp-0-\' + str(n_epoch) + \'.npy\',\n                        agent.approximator.get_weights())\n\n            print(\'- Evaluation:\')\n            # evaluation step\n            pi.set_epsilon(epsilon_test)\n            mdp.set_episode_end(False)\n            dataset = core.evaluate(n_steps=test_samples, render=args.render,\n                                    quiet=args.quiet)\n            scores.append(get_stats(dataset))\n\n            np.save(folder_name + \'/scores.npy\', scores)\n\n    return scores\n\n\nif __name__ == \'__main__\':\n    experiment()\n'"
examples/car_on_hill_fqi.py,0,"b'import numpy as np\nfrom joblib import Parallel, delayed\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nfrom mushroom_rl.algorithms.value import FQI\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import *\nfrom mushroom_rl.policy import EpsGreedy\nfrom mushroom_rl.utils.dataset import compute_J\nfrom mushroom_rl.utils.parameters import Parameter\n\n""""""\nThis script aims to replicate the experiments on the Car on Hill MDP as\npresented in:\n""Tree-Based Batch Mode Reinforcement Learning"", Ernst D. et al.. 2005. \n\n""""""\n\n\ndef experiment():\n    np.random.seed()\n\n    # MDP\n    mdp = CarOnHill()\n\n    # Policy\n    epsilon = Parameter(value=1.)\n    pi = EpsGreedy(epsilon=epsilon)\n\n    # Approximator\n    approximator_params = dict(input_shape=mdp.info.observation_space.shape,\n                               n_actions=mdp.info.action_space.n,\n                               n_estimators=50,\n                               min_samples_split=5,\n                               min_samples_leaf=2)\n    approximator = ExtraTreesRegressor\n\n    # Agent\n    algorithm_params = dict(n_iterations=20)\n    agent = FQI(mdp.info, pi, approximator,\n                approximator_params=approximator_params, **algorithm_params)\n\n    # Algorithm\n    core = Core(agent, mdp)\n\n    # Render\n    core.evaluate(n_episodes=1, render=True)\n\n    # Train\n    core.learn(n_episodes=1000, n_episodes_per_fit=1000)\n\n    # Test\n    test_epsilon = Parameter(0.)\n    agent.policy.set_epsilon(test_epsilon)\n\n    initial_states = np.zeros((289, 2))\n    cont = 0\n    for i in range(-8, 9):\n        for j in range(-8, 9):\n            initial_states[cont, :] = [0.125 * i, 0.375 * j]\n            cont += 1\n\n    dataset = core.evaluate(initial_states=initial_states)\n\n    # Render\n    core.evaluate(n_episodes=3, render=True)\n\n    return np.mean(compute_J(dataset, mdp.info.gamma))\n\n\nif __name__ == \'__main__\':\n    n_experiment = 1\n\n    Js = Parallel(n_jobs=-1)(delayed(experiment)() for _ in range(n_experiment))\n    print((np.mean(Js)))\n'"
examples/cartpole_lspi.py,0,"b'import numpy as np\nfrom joblib import Parallel, delayed\n\nfrom mushroom_rl.algorithms.value import LSPI\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import *\nfrom mushroom_rl.features import Features\nfrom mushroom_rl.features.basis import PolynomialBasis, GaussianRBF\nfrom mushroom_rl.policy import EpsGreedy\nfrom mushroom_rl.utils.dataset import episodes_length\nfrom mushroom_rl.utils.parameters import Parameter\n\n\n""""""\nThis script aims to replicate the experiments on the Inverted Pendulum MDP as\npresented in:\n""Least-Squares Policy Iteration"". Lagoudakis M. G. and Parr R.. 2003.\n\n""""""\n\n\ndef experiment():\n    np.random.seed()\n\n    # MDP\n    mdp = CartPole()\n\n    # Policy\n    epsilon = Parameter(value=1.)\n    pi = EpsGreedy(epsilon=epsilon)\n\n    # Agent\n    basis = [PolynomialBasis()]\n\n    s1 = np.array([-np.pi, 0, np.pi]) * .25\n    s2 = np.array([-1, 0, 1])\n    for i in s1:\n        for j in s2:\n            basis.append(GaussianRBF(np.array([i, j]), np.array([1.])))\n    features = Features(basis_list=basis)\n\n    fit_params = dict()\n    approximator_params = dict(input_shape=(features.size,),\n                               output_shape=(mdp.info.action_space.n,),\n                               n_actions=mdp.info.action_space.n)\n    agent = LSPI(mdp.info, pi, approximator_params=approximator_params,\n                 fit_params=fit_params, features=features)\n\n    # Algorithm\n    core = Core(agent, mdp)\n    core.evaluate(n_episodes=3, render=True)\n\n    # Train\n    core.learn(n_episodes=100, n_episodes_per_fit=100)\n\n    # Test\n    test_epsilon = Parameter(0.)\n    agent.policy.set_epsilon(test_epsilon)\n\n    dataset = core.evaluate(n_episodes=1, quiet=True)\n\n    core.evaluate(n_steps=100, render=True)\n\n    return np.mean(episodes_length(dataset))\n\n\nif __name__ == \'__main__\':\n    n_experiment = 1\n\n    steps = experiment()\n    print(\'Final episode lenght: \', steps)\n'"
examples/grid_world_td.py,0,"b'import matplotlib\nmatplotlib.use(\'Agg\')\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom joblib import Parallel, delayed\n\nfrom mushroom_rl.algorithms.value import QLearning, DoubleQLearning,\\\n    WeightedQLearning, SpeedyQLearning, SARSA\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import *\nfrom mushroom_rl.policy import EpsGreedy\nfrom mushroom_rl.utils.callbacks import CollectDataset, CollectMaxQ\nfrom mushroom_rl.utils.dataset import parse_dataset\nfrom mushroom_rl.utils.parameters import ExponentialParameter\n\n\n""""""\nThis script aims to replicate the experiments on the Grid World MDP as\npresented in:\n""Double Q-Learning"", Hasselt H. V.. 2010.\n\nSARSA and many variants of Q-Learning are used. \n\n""""""\n\n\ndef experiment(algorithm_class, exp):\n    np.random.seed()\n\n    # MDP\n    mdp = GridWorldVanHasselt()\n\n    # Policy\n    epsilon = ExponentialParameter(value=1, exp=.5, size=mdp.info.observation_space.size)\n    pi = EpsGreedy(epsilon=epsilon)\n\n    # Agent\n    learning_rate = ExponentialParameter(value=1, exp=exp, size=mdp.info.size)\n    algorithm_params = dict(learning_rate=learning_rate)\n    agent = algorithm_class(mdp.info, pi, **algorithm_params)\n\n    # Algorithm\n    start = mdp.convert_to_int(mdp._start, mdp._width)\n    collect_max_Q = CollectMaxQ(agent.approximator, start)\n    collect_dataset = CollectDataset()\n    callbacks = [collect_dataset, collect_max_Q]\n    core = Core(agent, mdp, callbacks)\n\n    # Train\n    core.learn(n_steps=10000, n_steps_per_fit=1, quiet=True)\n\n    _, _, reward, _, _, _ = parse_dataset(collect_dataset.get())\n    max_Qs = collect_max_Q.get()\n\n    return reward, max_Qs\n\n\nif __name__ == \'__main__\':\n    n_experiment = 10000\n\n    names = {1: \'1\', .8: \'08\', QLearning: \'Q\', DoubleQLearning: \'DQ\',\n             WeightedQLearning: \'WQ\', SpeedyQLearning: \'SPQ\', SARSA: \'SARSA\'}\n\n    for e in [1, .8]:\n        print(\'Exp: \', e)\n        fig = plt.figure()\n        plt.suptitle(names[e])\n        legend_labels = []\n        for a in [QLearning, DoubleQLearning, WeightedQLearning,\n                  SpeedyQLearning, SARSA]:\n            print(\'Alg: \', names[a])\n            out = Parallel(n_jobs=-1)(\n                delayed(experiment)(a, e) for _ in range(n_experiment))\n            r = np.array([o[0] for o in out])\n            max_Qs = np.array([o[1] for o in out])\n\n            r = np.convolve(np.mean(r, 0), np.ones(100) / 100., \'valid\')\n            max_Qs = np.mean(max_Qs, 0)\n\n            np.save(names[a] + \'_\' + names[e] + \'_r.npy\', r)\n            np.save(names[a] + \'_\' + names[e] + \'_maxQ.npy\', max_Qs)\n\n            plt.subplot(2, 1, 1)\n            plt.plot(r)\n            plt.subplot(2, 1, 2)\n            plt.plot(max_Qs)\n            legend_labels.append(names[a])\n        plt.legend(legend_labels)\n        fig.savefig(\'test_\' + names[e] + \'.png\')\n'"
examples/humanoid_sac.py,7,"b'import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom mushroom_rl.utils.callbacks import PlotDataset\nfrom mushroom_rl.utils.preprocessors import MinMaxPreprocessor\n\nfrom mushroom_rl.algorithms.actor_critic import SAC\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.utils.dataset import compute_J, episodes_length\n\nfrom mushroom_rl.environments.mujoco_envs import HumanoidGait\nfrom mushroom_rl.environments.mujoco_envs.humanoid_gait import \\\n    VelocityProfile3D, RandomConstantVelocityProfile, ConstantVelocityProfile\n\n\nclass CriticNetwork(nn.Module):\n    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n        super(CriticNetwork, self).__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._in = nn.Linear(n_input, n_features[0])\n        self._h1 = nn.Linear(n_features[0], n_features[1])\n        self._out = nn.Linear(n_features[1], n_output)\n\n        nn.init.xavier_uniform_(self._in.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n        nn.init.xavier_uniform_(self._h1.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n        nn.init.xavier_uniform_(self._out.weight,\n                                gain=nn.init.calculate_gain(\'linear\'))\n\n    def forward(self, state, action):\n        in_feats = torch.cat((state.float(), action.float()), dim=1)\n        feats = F.relu(self._in(in_feats))\n        feats = F.relu(self._h1(feats))\n\n        out = self._out(feats)\n        return torch.squeeze(out)\n\n\nclass ActorNetwork(nn.Module):\n    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n        super(ActorNetwork, self).__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._in = nn.Linear(n_input, n_features[0])\n        self._h1 = nn.Linear(n_features[0], n_features[1])\n        self._out = nn.Linear(n_features[1], n_output)\n\n        nn.init.xavier_uniform_(self._in.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n        nn.init.xavier_uniform_(self._h1.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n        nn.init.xavier_uniform_(self._out.weight,\n                                gain=nn.init.calculate_gain(\'linear\'))\n\n    def forward(self, state):\n        in_feats = torch.squeeze(state, 1).float()\n\n        feats = F.relu(self._in(in_feats))\n        feats = F.relu(self._h1(feats))\n\n        out = self._out(feats)\n        return out\n\n\ndef create_SAC_agent(mdp, use_cuda=None):\n    if use_cuda is None:\n        use_cuda = torch.cuda.is_available()\n\n    # Settings\n    actor_mu_network = ActorNetwork\n    actor_sigma_network = ActorNetwork\n    network_layers_actor_mu = (512, 256)\n    network_layers_actor_sigma = (512, 256)\n    network_layers_critic = (512, 256)\n\n    initial_replay_size = 3000\n    max_replay_size = 100000\n    batch_size = 256\n    warmup_transitions = 5000\n    tau = 0.005\n\n    lr_alpha = 2e-6\n    lr_actor = 2e-5\n    lr_critic = 4e-5\n    weight_decay_actor = 0.0\n    weight_decay_critic = 0.0\n\n    target_entropy = -22.0\n\n    # Approximator\n    actor_input_shape = mdp.info.observation_space.shape\n    actor_mu_params = dict(network=actor_mu_network,\n                           n_features=network_layers_actor_mu,\n                           input_shape=actor_input_shape,\n                           output_shape=mdp.info.action_space.shape,\n                           use_cuda=use_cuda)\n\n    actor_sigma_params = dict(network=actor_sigma_network,\n                              n_features=network_layers_actor_sigma,\n                              input_shape=actor_input_shape,\n                              output_shape=mdp.info.action_space.shape,\n                              use_cuda=use_cuda)\n\n    actor_optimizer = {\'class\': optim.Adam,\n                       \'params\': {\'lr\': lr_actor, \'weight_decay\': weight_decay_actor}}\n\n    critic_input_shape = (actor_input_shape[0] + mdp.info.action_space.shape[0],)\n    critic_params = dict(network=CriticNetwork,\n                         optimizer={\'class\': optim.Adam,\n                                    \'params\': {\'lr\': lr_critic, \'weight_decay\': weight_decay_critic}},\n                         loss=F.mse_loss,\n                         n_features=network_layers_critic,\n                         input_shape=critic_input_shape,\n                         output_shape=(1,),\n                         use_cuda=use_cuda)\n\n    # create SAC agent\n    agent = SAC(mdp_info=mdp.info,\n                batch_size=batch_size, initial_replay_size=initial_replay_size,\n                max_replay_size=max_replay_size,\n                warmup_transitions=warmup_transitions, tau=tau, lr_alpha=lr_alpha,\n                actor_mu_params=actor_mu_params, actor_sigma_params=actor_sigma_params,\n                actor_optimizer=actor_optimizer, critic_params=critic_params,\n                target_entropy=target_entropy, critic_fit_params=None)\n\n    return agent\n\n\ndef create_mdp(gamma, horizon, goal, use_muscles):\n    if goal == ""trajectory"":\n        mdp = HumanoidGait(gamma=gamma, horizon=horizon, n_intermediate_steps=10,\n                           goal_reward=""trajectory"",\n                           goal_reward_params=dict(use_error_terminate=True),\n                           use_muscles=use_muscles,\n                           obs_avg_window=1, act_avg_window=1)\n\n    elif goal == ""max_vel"":\n        mdp = HumanoidGait(gamma=gamma, horizon=horizon, n_intermediate_steps=10,\n                           goal_reward=""max_vel"",\n                           goal_reward_params=dict(traj_start=True),\n                           use_muscles=use_muscles,\n                           obs_avg_window=1, act_avg_window=1)\n\n    elif goal == ""vel_profile"":\n        velocity_profile = dict(profile_instance=VelocityProfile3D([\n                RandomConstantVelocityProfile(min=0.5, max=2.0),\n                ConstantVelocityProfile(0),\n                ConstantVelocityProfile(0)]))\n\n        mdp = HumanoidGait(gamma=gamma, horizon=horizon, n_intermediate_steps=10,\n                           goal_reward=""vel_profile"",\n                           goal_reward_params=dict(traj_start=True,\n                                                   **velocity_profile),\n                           use_muscles=use_muscles,\n                           obs_avg_window=1, act_avg_window=1)\n    else:\n        raise NotImplementedError(""Invalid goal selected, try one of ""\n                                  ""[\'trajectory\', \'vel_profile\', \'max_vel\']"")\n    return mdp\n\n\ndef experiment(goal, use_muscles, n_epochs, n_steps, n_episodes_test):\n    np.random.seed(1)\n\n    # MDP\n    gamma = 0.99\n    horizon = 2000\n    mdp = create_mdp(gamma, horizon, goal, use_muscles=use_muscles)\n\n    # Agent\n    agent = create_SAC_agent(mdp)\n\n    # normalization callback\n    normalizer = MinMaxPreprocessor(mdp_info=mdp.info)\n\n    # plotting callback\n    plotter = PlotDataset(mdp.info)\n\n    # Algorithm(with normalization and plotting)\n    core = Core(agent, mdp, callback_step=plotter, preprocessors=[normalizer])\n\n    # training loop\n    for n in range(n_epochs):\n        core.learn(n_steps=n_steps, n_steps_per_fit=1)\n        dataset = core.evaluate(n_episodes=n_episodes_test, render=True)\n        print(\'Epoch: \', n,\n              \'  J: \', np.mean(compute_J(dataset, gamma)),\n              \'  Len_ep: \', int(np.round(np.mean(episodes_length(dataset))))\n              )\n\n    print(\'Press a button to visualize humanoid\')\n    input()\n    core.evaluate(n_episodes=10, render=True)\n\n\nif __name__ == \'__main__\':\n    goal = [""trajectory"", ""vel_profile"", ""max_vel""]\n    experiment(goal=goal[0], use_muscles=True,\n               n_epochs=250, n_steps=10000, n_episodes_test=10)'"
examples/lqr_bbo.py,0,"b'import numpy as np\nfrom tqdm import tqdm\n\nfrom mushroom_rl.algorithms.policy_search import RWR, PGPE, REPS\nfrom mushroom_rl.approximators.parametric import LinearApproximator\nfrom mushroom_rl.approximators.regressor import Regressor\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.distributions import GaussianCholeskyDistribution\nfrom mushroom_rl.environments import LQR\nfrom mushroom_rl.policy import DeterministicPolicy\nfrom mushroom_rl.utils.dataset import compute_J\nfrom mushroom_rl.utils.parameters import AdaptiveParameter\n\n\n""""""\nThis script aims to replicate the experiments on the LQR MDP using episode-based\npolicy search algorithms, also known as Black Box policy search algorithms.\n\n""""""\n\ntqdm.monitor_interval = 0\n\n\ndef experiment(alg, params, n_epochs, fit_per_run, ep_per_run):\n    np.random.seed()\n\n    # MDP\n    mdp = LQR.generate(dimensions=1)\n\n    approximator = Regressor(LinearApproximator,\n                             input_shape=mdp.info.observation_space.shape,\n                             output_shape=mdp.info.action_space.shape)\n\n    policy = DeterministicPolicy(mu=approximator)\n\n    mu = np.zeros(policy.weights_size)\n    sigma = 1e-3 * np.eye(policy.weights_size)\n    distribution = GaussianCholeskyDistribution(mu, sigma)\n\n    # Agent\n    agent = alg(mdp.info, distribution, policy, **params)\n\n    # Train\n    core = Core(agent, mdp)\n    dataset_eval = core.evaluate(n_episodes=ep_per_run)\n    print(\'distribution parameters: \', distribution.get_parameters())\n    J = compute_J(dataset_eval, gamma=mdp.info.gamma)\n    print(\'J at start : \' + str(np.mean(J)))\n\n    for i in range(n_epochs):\n        core.learn(n_episodes=fit_per_run * ep_per_run,\n                   n_episodes_per_fit=ep_per_run)\n        dataset_eval = core.evaluate(n_episodes=ep_per_run)\n        print(\'distribution parameters: \', distribution.get_parameters())\n        J = compute_J(dataset_eval, gamma=mdp.info.gamma)\n        print(\'J at iteration \' + str(i) + \': \' + str(np.mean(J)))\n\n\nif __name__ == \'__main__\':\n    learning_rate = AdaptiveParameter(value=0.05)\n\n    algs = [REPS, RWR, PGPE]\n    params = [{\'eps\': 0.5}, {\'beta\': 0.7}, {\'learning_rate\': learning_rate}]\n\n    for alg, params in zip(algs, params):\n        print(alg.__name__)\n        experiment(alg, params, n_epochs=4, fit_per_run=10, ep_per_run=100)\n'"
examples/lqr_pg.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.policy_search import REINFORCE, GPOMDP, eNAC\nfrom mushroom_rl.approximators.parametric import LinearApproximator\nfrom mushroom_rl.approximators.regressor import Regressor\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import LQR\nfrom mushroom_rl.policy import StateStdGaussianPolicy\nfrom mushroom_rl.utils.dataset import compute_J\nfrom mushroom_rl.utils.parameters import AdaptiveParameter\n\nfrom tqdm import tqdm\n\n\n""""""\nThis script aims to replicate the experiments on the LQR MDP using policy\ngradient algorithms.\n\n""""""\n\ntqdm.monitor_interval = 0\n\n\ndef experiment(alg, n_epochs, n_iterations, ep_per_run):\n    np.random.seed()\n\n    # MDP\n    mdp = LQR.generate(dimensions=1)\n\n    approximator = Regressor(LinearApproximator,\n                             input_shape=mdp.info.observation_space.shape,\n                             output_shape=mdp.info.action_space.shape)\n\n    sigma = Regressor(LinearApproximator,\n                      input_shape=mdp.info.observation_space.shape,\n                      output_shape=mdp.info.action_space.shape)\n\n    sigma_weights = 2 * np.ones(sigma.weights_size)\n    sigma.set_weights(sigma_weights)\n\n    policy = StateStdGaussianPolicy(approximator, sigma)\n\n    # Agent\n    learning_rate = AdaptiveParameter(value=.01)\n    algorithm_params = dict(learning_rate=learning_rate)\n    agent = alg(mdp.info, policy, **algorithm_params)\n\n    # Train\n    core = Core(agent, mdp)\n    dataset_eval = core.evaluate(n_episodes=ep_per_run)\n    print(\'policy parameters: \', policy.get_weights())\n    J = compute_J(dataset_eval, gamma=mdp.info.gamma)\n    print(\'J at start : \' + str(np.mean(J)))\n\n    for i in range(n_epochs):\n        core.learn(n_episodes=n_iterations * ep_per_run,\n                   n_episodes_per_fit=ep_per_run)\n        dataset_eval = core.evaluate(n_episodes=ep_per_run)\n        print(\'policy parameters: \', policy.get_weights())\n        J = compute_J(dataset_eval, gamma=mdp.info.gamma)\n        print(\'J at iteration \' + str(i) + \': \' + str(np.mean(J)))\n\n\nif __name__ == \'__main__\':\n\n    algs = [REINFORCE, GPOMDP, eNAC]\n\n    for alg in algs:\n        print(alg.__name__)\n        experiment(alg, n_epochs=10, n_iterations=4, ep_per_run=100)\n'"
examples/mountain_car_sarsa.py,0,"b'import numpy as np\nfrom joblib import Parallel, delayed\n\nfrom mushroom_rl.algorithms.value import TrueOnlineSARSALambda\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import Gym\nfrom mushroom_rl.features import Features\nfrom mushroom_rl.features.tiles import Tiles\nfrom mushroom_rl.policy import EpsGreedy\nfrom mushroom_rl.utils.dataset import compute_J\nfrom mushroom_rl.utils.parameters import Parameter\n\n""""""\nThis script aims to replicate the experiments on the Mountain Car MDP as\npresented in:\n""True Online TD(lambda)"". Seijen H. V. et al.. 2014.\n\n""""""\n\n\ndef experiment(alpha):\n    np.random.seed()\n\n    # MDP\n    mdp = Gym(name=\'MountainCar-v0\', horizon=np.inf, gamma=1.)\n\n    # Policy\n    epsilon = Parameter(value=0.)\n    pi = EpsGreedy(epsilon=epsilon)\n\n    # Agent\n    n_tilings = 10\n    tilings = Tiles.generate(n_tilings, [10, 10],\n                             mdp.info.observation_space.low,\n                             mdp.info.observation_space.high)\n    features = Features(tilings=tilings)\n\n    learning_rate = Parameter(alpha / n_tilings)\n\n    approximator_params = dict(input_shape=(features.size,),\n                               output_shape=(mdp.info.action_space.n,),\n                               n_actions=mdp.info.action_space.n)\n    algorithm_params = {\'learning_rate\': learning_rate,\n                        \'lambda_coeff\': .9}\n\n    agent = TrueOnlineSARSALambda(mdp.info, pi,\n                                  approximator_params=approximator_params,\n                                  features=features, **algorithm_params)\n\n    # Algorithm\n    core = Core(agent, mdp)\n\n    # Train\n    core.learn(n_episodes=40, n_steps_per_fit=1, render=False)\n    dataset = core.evaluate(n_episodes=1, render=True)\n\n    return np.mean(compute_J(dataset, 1.))\n\n\nif __name__ == \'__main__\':\n    n_experiment = 1\n\n    alpha = .1\n    Js = Parallel(\n        n_jobs=-1)(delayed(experiment)(alpha) for _ in range(n_experiment))\n\n    print((np.mean(Js)))\n'"
examples/pendulum_a2c.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nfrom tqdm import tqdm, trange\n\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import Gym\nfrom mushroom_rl.algorithms.actor_critic import A2C\n\nfrom mushroom_rl.policy import GaussianTorchPolicy\nfrom mushroom_rl.utils.dataset import compute_J\n\n\nclass Network(nn.Module):\n    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n        super(Network, self).__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._h1 = nn.Linear(n_input, n_features)\n        self._h2 = nn.Linear(n_features, n_features)\n        self._h3 = nn.Linear(n_features, n_output)\n\n        nn.init.xavier_uniform_(self._h1.weight,\n                                gain=nn.init.calculate_gain('tanh'))\n        nn.init.xavier_uniform_(self._h2.weight,\n                                gain=nn.init.calculate_gain('tanh'))\n        nn.init.xavier_uniform_(self._h3.weight,\n                                gain=nn.init.calculate_gain('linear'))\n\n    def forward(self, state, **kwargs):\n        features1 = torch.tanh(self._h1(torch.squeeze(state, 1).float()))\n        features2 = torch.tanh(self._h2(features1))\n        a = self._h3(features2)\n\n        return a\n\n\ndef experiment(alg, env_id, horizon, gamma, n_epochs, n_steps, n_steps_per_fit,\n               n_step_test, alg_params, policy_params):\n    print(alg.__name__)\n\n    mdp = Gym(env_id, horizon, gamma)\n\n    critic_params = dict(network=Network,\n                         optimizer={'class': optim.RMSprop,\n                                    'params': {'lr': 7e-4,\n                                               'eps': 1e-5}},\n                         loss=F.mse_loss,\n                         n_features=64,\n                         input_shape=mdp.info.observation_space.shape,\n                         output_shape=(1,))\n\n    alg_params['critic_params'] = critic_params\n\n    policy = GaussianTorchPolicy(Network,\n                                 mdp.info.observation_space.shape,\n                                 mdp.info.action_space.shape,\n                                 **policy_params)\n\n    agent = alg(mdp.info, policy, **alg_params)\n\n    core = Core(agent, mdp)\n\n    for it in trange(n_epochs):\n        core.learn(n_steps=n_steps, n_steps_per_fit=n_steps_per_fit)\n        dataset = core.evaluate(n_steps=n_step_test, render=False)\n\n        J = np.mean(compute_J(dataset, mdp.info.gamma))\n        R = np.mean(compute_J(dataset))\n        E = agent.policy.entropy()\n\n        tqdm.write('END OF EPOCH ' + str(it))\n        tqdm.write('J: {}, R: {}, entropy: {}'.format(J, R, E))\n        tqdm.write('##################################################################################################')\n\n    print('Press a button to visualize')\n    input()\n    core.evaluate(n_episodes=5, render=True)\n\n\nif __name__ == '__main__':\n    policy_params = dict(\n        std_0=1.,\n        n_features=64,\n        use_cuda=False\n    )\n\n    a2c_params = dict(actor_optimizer={'class': optim.RMSprop,\n                                       'params': {'lr': 7e-4,\n                                                  'eps': 3e-3}},\n                      max_grad_norm=0.5,\n                      ent_coeff=0.01)\n\n    algs_params = [\n        (A2C, 'a2c', a2c_params)\n     ]\n\n    for alg, alg_name, alg_params in algs_params:\n        experiment(alg=alg, env_id='Pendulum-v0', horizon=200, gamma=.99,\n                   n_epochs=40, n_steps=30000, n_steps_per_fit=5,\n                   n_step_test=5000, alg_params=alg_params,\n                   policy_params=policy_params)\n"""
examples/pendulum_ac.py,0,"b""import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom mushroom_rl.algorithms.actor_critic import StochasticAC_AVG\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import *\nfrom mushroom_rl.features import Features\nfrom mushroom_rl.features.tiles import Tiles\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.approximators.parametric import LinearApproximator\nfrom mushroom_rl.policy import StateLogStdGaussianPolicy\nfrom mushroom_rl.utils.dataset import compute_J\nfrom mushroom_rl.utils.callbacks import CollectDataset\nfrom mushroom_rl.utils.parameters import Parameter\n\nfrom tqdm import tqdm\ntqdm.monitor_interval = 0\n\n\nclass Display:\n    def __init__(self, V, mu, std, low, high, phi, psi):\n        plt.ion()\n\n        self._V = V\n        self._mu = mu\n        self._std = std\n        self._phi = phi\n        self._psi = psi\n\n        fig = plt.figure(figsize=(15, 5))\n        ax1 = fig.add_subplot(1, 3, 1)\n        ax2 = fig.add_subplot(1, 3, 2)\n        ax3 = fig.add_subplot(1, 3, 3)\n\n        self._theta = np.linspace(low[0], high[0], 100)\n        self._omega = np.linspace(low[1], high[1], 100)\n\n        vv, mm, ss = self._compute_data()\n\n        ext = [low[0], high[0],\n               low[1], high[1]]\n\n        ax1.set_title('V')\n        im1 = ax1.imshow(vv, cmap=cm.coolwarm, extent=ext, aspect='auto')\n        fig.colorbar(im1, ax=ax1)\n\n        ax2.set_title('mean')\n        im2 = ax2.imshow(mm, cmap=cm.coolwarm, extent=ext, aspect='auto')\n        fig.colorbar(im2, ax=ax2)\n\n        ax3.set_title('sigma')\n        im3 = ax3.imshow(ss, cmap=cm.coolwarm, extent=ext, aspect='auto')\n        fig.colorbar(im3, ax=ax3)\n\n        self._im = [im1, im2, im3]\n\n        self._counter = 0\n\n        plt.draw()\n        plt.pause(.1)\n\n    def __call__(self, *args, **kwargs):\n        vv, mm, ss = self._compute_data()\n\n        self._im[0].set_data(vv)\n        self._im[0].autoscale()\n        self._im[1].set_data(mm)\n        self._im[1].autoscale()\n        self._im[2].set_data(ss)\n        self._im[2].autoscale()\n\n        self._counter = 0\n\n        plt.draw()\n        plt.pause(.1)\n\n    def _compute_data(self):\n        n_points = len(self._theta) * len(self._omega)\n        vv = np.empty(n_points)\n        mm = np.empty(n_points)\n        ss = np.empty(n_points)\n\n        c = 0\n        for y in self._omega:\n            for x in self._theta:\n                s = self._phi(np.array([x, y]))\n                s_v = self._psi(np.array([x, y]))\n                vv[c] = self._V(s_v)\n                mm[c] = self._mu(s)\n                ss[c] = np.exp(self._std(s)) ** 2\n                c += 1\n\n        shape = (len(self._theta), len(self._omega))\n        vv = vv.reshape(shape)\n        mm = mm.reshape(shape)\n        ss = ss.reshape(shape)\n\n        return vv, mm, ss\n\n\ndef experiment(n_epochs, n_episodes):\n    np.random.seed()\n\n    # MDP\n    n_steps = 5000\n    mdp = InvertedPendulum(horizon=n_steps)\n\n    # Agent\n    n_tilings = 11\n    alpha_r = Parameter(.0001)\n    alpha_theta = Parameter(.001 / n_tilings)\n    alpha_v = Parameter(.1 / n_tilings)\n    tilings = Tiles.generate(n_tilings-1, [10, 10],\n                             mdp.info.observation_space.low,\n                             mdp.info.observation_space.high + 1e-3)\n\n    phi = Features(tilings=tilings)\n\n    tilings_v = tilings + Tiles.generate(1, [1, 1],\n                                         mdp.info.observation_space.low,\n                                         mdp.info.observation_space.high + 1e-3)\n    psi = Features(tilings=tilings_v)\n\n    input_shape = (phi.size,)\n\n    mu = Regressor(LinearApproximator, input_shape=input_shape,\n                   output_shape=mdp.info.action_space.shape)\n\n    std = Regressor(LinearApproximator, input_shape=input_shape,\n                    output_shape=mdp.info.action_space.shape)\n\n    std_0 = np.sqrt(1.)\n    std.set_weights(np.log(std_0) / n_tilings * np.ones(std.weights_size))\n\n    policy = StateLogStdGaussianPolicy(mu, std)\n\n    agent = StochasticAC_AVG(mdp.info, policy,\n                             alpha_theta, alpha_v, alpha_r,\n                             lambda_par=.5,\n                             value_function_features=psi,\n                             policy_features=phi)\n\n    # Train\n    dataset_callback = CollectDataset()\n    display_callback = Display(agent._V, mu, std,\n                               mdp.info.observation_space.low,\n                               mdp.info.observation_space.high,\n                               phi, psi)\n    core = Core(agent, mdp, callbacks_episode=[dataset_callback])\n\n    for i in range(n_epochs):\n        core.learn(n_episodes=n_episodes,\n                   n_steps_per_fit=1, render=False)\n        J = compute_J(dataset_callback.get(), gamma=1.)\n        dataset_callback.clean()\n        display_callback()\n        print('Mean Reward at iteration ' + str(i) + ': ' +\n              str(np.sum(J) / n_steps/n_episodes))\n\n    print('Press a button to visualize the pendulum...')\n    input()\n    core.evaluate(n_steps=n_steps, render=True)\n\n\nif __name__ == '__main__':\n    n_epochs = 24\n    n_episodes = 5\n\n    experiment(n_epochs, n_episodes)\n"""
examples/pendulum_ddpg.py,7,"b""import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom mushroom_rl.algorithms.actor_critic import DDPG, TD3\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments.gym_env import Gym\nfrom mushroom_rl.policy import OrnsteinUhlenbeckPolicy\nfrom mushroom_rl.utils.dataset import compute_J\n\n\nclass CriticNetwork(nn.Module):\n    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n        super().__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._h1 = nn.Linear(n_input, n_features)\n        self._h2 = nn.Linear(n_features, n_features)\n        self._h3 = nn.Linear(n_features, n_output)\n\n        nn.init.xavier_uniform_(self._h1.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h2.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h3.weight,\n                                gain=nn.init.calculate_gain('linear'))\n\n    def forward(self, state, action):\n        state_action = torch.cat((state.float(), action.float()), dim=1)\n        features1 = F.relu(self._h1(state_action))\n        features2 = F.relu(self._h2(features1))\n        q = self._h3(features2)\n\n        return torch.squeeze(q)\n\n\nclass ActorNetwork(nn.Module):\n    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n        super(ActorNetwork, self).__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._h1 = nn.Linear(n_input, n_features)\n        self._h2 = nn.Linear(n_features, n_features)\n        self._h3 = nn.Linear(n_features, n_output)\n\n        nn.init.xavier_uniform_(self._h1.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h2.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h3.weight,\n                                gain=nn.init.calculate_gain('linear'))\n\n    def forward(self, state):\n        features1 = F.relu(self._h1(torch.squeeze(state, 1).float()))\n        features2 = F.relu(self._h2(features1))\n        a = self._h3(features2)\n\n        return a\n\n\ndef experiment(alg, n_epochs, n_steps, n_steps_test):\n    np.random.seed()\n\n    use_cuda = torch.cuda.is_available()\n\n    # MDP\n    horizon = 200\n    gamma = 0.99\n    mdp = Gym('Pendulum-v0', horizon, gamma)\n\n    # Policy\n    policy_class = OrnsteinUhlenbeckPolicy\n    policy_params = dict(sigma=np.ones(1) * .2, theta=.15, dt=1e-2)\n\n    # Settings\n    initial_replay_size = 500\n    max_replay_size = 5000\n    batch_size = 200\n    n_features = 80\n    tau = .001\n\n    # Approximator\n    actor_input_shape = mdp.info.observation_space.shape\n    actor_params = dict(network=ActorNetwork,\n                        n_features=n_features,\n                        input_shape=actor_input_shape,\n                        output_shape=mdp.info.action_space.shape,\n                        use_cuda=use_cuda)\n\n    actor_optimizer = {'class': optim.Adam,\n                       'params': {'lr': .001}}\n\n    critic_input_shape = (actor_input_shape[0] + mdp.info.action_space.shape[0],)\n    critic_params = dict(network=CriticNetwork,\n                         optimizer={'class': optim.Adam,\n                                    'params': {'lr': .001}},\n                         loss=F.mse_loss,\n                         n_features=n_features,\n                         input_shape=critic_input_shape,\n                         output_shape=(1,),\n                         use_cuda=use_cuda)\n\n    # Agent\n    agent = alg(mdp.info, policy_class, policy_params,\n                actor_params, actor_optimizer, critic_params, batch_size,\n                initial_replay_size, max_replay_size, tau)\n\n    # Algorithm\n    core = Core(agent, mdp)\n\n    core.learn(n_steps=initial_replay_size, n_steps_per_fit=initial_replay_size)\n\n    # RUN\n    dataset = core.evaluate(n_steps=n_steps_test, render=False)\n    J = compute_J(dataset, gamma)\n    print('J: ', np.mean(J))\n\n    for n in range(n_epochs):\n        print('Epoch: ', n)\n        core.learn(n_steps=n_steps, n_steps_per_fit=1)\n        dataset = core.evaluate(n_steps=n_steps_test, render=False)\n        J = compute_J(dataset, gamma)\n        print('J: ', np.mean(J))\n\n    print('Press a button to visualize pendulum')\n    input()\n    core.evaluate(n_episodes=5, render=True)\n\n\nif __name__ == '__main__':\n    algs = [DDPG, TD3]\n\n    for alg in algs:\n        print('Algorithm: ', alg.__name__)\n        experiment(alg=alg, n_epochs=40, n_steps=1000, n_steps_test=2000)\n"""
examples/pendulum_dpg.py,0,"b""import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom mushroom_rl.algorithms.actor_critic import COPDAC_Q\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import *\nfrom mushroom_rl.features import Features\nfrom mushroom_rl.features.tiles import Tiles\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.approximators.parametric import LinearApproximator\nfrom mushroom_rl.policy import GaussianPolicy\nfrom mushroom_rl.utils.dataset import compute_J\nfrom mushroom_rl.utils.parameters import Parameter\nfrom mushroom_rl.utils.callbacks import CollectDataset\n\nfrom tqdm import tqdm\ntqdm.monitor_interval = 0\n\n\nclass Display:\n    def __init__(self, V, mu, low, high, phi, psi):\n        plt.ion()\n\n        self._V = V\n        self._mu = mu\n        self._phi = phi\n        self._psi = psi\n\n        fig = plt.figure(figsize=(10, 5))\n        ax1 = fig.add_subplot(1, 2, 1)\n        ax2 = fig.add_subplot(1, 2, 2)\n\n        self._theta = np.linspace(low[0], high[0], 100)\n        self._omega = np.linspace(low[1], high[1], 100)\n\n        vv, mm = self._compute_data()\n\n        ext = [low[0], high[0],\n               low[1], high[1]]\n\n        ax1.set_title('V')\n        im1 = ax1.imshow(vv, cmap=cm.coolwarm, extent=ext, aspect='auto')\n        fig.colorbar(im1, ax=ax1)\n\n        ax2.set_title('mean')\n        im2 = ax2.imshow(mm, cmap=cm.coolwarm, extent=ext, aspect='auto')\n        fig.colorbar(im2, ax=ax2)\n\n        self._im = [im1, im2]\n\n        self._counter = 0\n\n        plt.draw()\n        plt.pause(0.1)\n\n    def __call__(self, *args, **kwargs):\n        vv, mm = self._compute_data()\n\n        self._im[0].set_data(vv)\n        self._im[0].autoscale()\n        self._im[1].set_data(mm)\n        self._im[1].autoscale()\n\n        self._counter = 0\n\n        plt.draw()\n        plt.pause(.1)\n\n    def _compute_data(self):\n        n_points = len(self._theta) * len(self._omega)\n        vv = np.empty(n_points)\n        mm = np.empty(n_points)\n\n        c = 0\n        for y in self._omega:\n            for x in self._theta:\n                s = self._phi(np.array([x, y]))\n                s_v = self._psi(np.array([x, y]))\n                vv[c] = self._V(s_v)\n                mm[c] = self._mu(s)\n                c += 1\n\n        shape = (len(self._theta), len(self._omega))\n        vv = vv.reshape(shape)\n        mm = mm.reshape(shape)\n\n        return vv, mm\n\n\ndef experiment(n_epochs, n_episodes):\n    np.random.seed()\n\n    # MDP\n    n_steps = 5000\n    mdp = InvertedPendulum(horizon=n_steps)\n\n    # Agent\n    n_tilings = 10\n    alpha_theta = Parameter(5e-3 / n_tilings)\n    alpha_omega = Parameter(0.5 / n_tilings)\n    alpha_v = Parameter(0.5 / n_tilings)\n    tilings = Tiles.generate(n_tilings, [10, 10],\n                             mdp.info.observation_space.low,\n                             mdp.info.observation_space.high + 1e-3)\n\n    phi = Features(tilings=tilings)\n\n    input_shape = (phi.size,)\n\n    mu = Regressor(LinearApproximator, input_shape=input_shape,\n                   output_shape=mdp.info.action_space.shape)\n\n    sigma = 1e-1 * np.eye(1)\n    policy = GaussianPolicy(mu, sigma)\n\n    agent = COPDAC_Q(mdp.info, policy, mu,\n                     alpha_theta, alpha_omega, alpha_v,\n                     value_function_features=phi,\n                     policy_features=phi)\n\n    # Train\n    dataset_callback = CollectDataset()\n    visualization_callback = Display(agent._V, mu,\n                                     mdp.info.observation_space.low,\n                                     mdp.info.observation_space.high,\n                                     phi, phi)\n    core = Core(agent, mdp, callbacks_episode=[dataset_callback])\n\n    for i in range(n_epochs):\n        core.learn(n_episodes=n_episodes,\n                   n_steps_per_fit=1, render=False)\n        J = compute_J(dataset_callback.get(), gamma=1.0)\n        dataset_callback.clean()\n        visualization_callback()\n        print('Mean Reward at iteration ' + str(i) + ': ' +\n              str(np.sum(J) / n_steps / n_episodes))\n\n    print('Press a button to visualize the pendulum...')\n    input()\n    sigma = 1e-8 * np.eye(1)\n    policy.set_sigma(sigma)\n    core.evaluate(n_steps=n_steps, render=True)\n\n\nif __name__ == '__main__':\n    n_epochs = 24\n    n_episodes = 5\n\n    experiment(n_epochs, n_episodes)\n"""
examples/pendulum_sac.py,7,"b""import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom mushroom_rl.algorithms.actor_critic import SAC\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments.gym_env import Gym\nfrom mushroom_rl.utils.dataset import compute_J\n\n\nclass CriticNetwork(nn.Module):\n    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n        super().__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._h1 = nn.Linear(n_input, n_features)\n        self._h2 = nn.Linear(n_features, n_features)\n        self._h3 = nn.Linear(n_features, n_output)\n\n        nn.init.xavier_uniform_(self._h1.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h2.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h3.weight,\n                                gain=nn.init.calculate_gain('linear'))\n\n    def forward(self, state, action):\n        state_action = torch.cat((state.float(), action.float()), dim=1)\n        features1 = F.relu(self._h1(state_action))\n        features2 = F.relu(self._h2(features1))\n        q = self._h3(features2)\n\n        return torch.squeeze(q)\n\n\nclass ActorNetwork(nn.Module):\n    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n        super(ActorNetwork, self).__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._h1 = nn.Linear(n_input, n_features)\n        self._h2 = nn.Linear(n_features, n_features)\n        self._h3 = nn.Linear(n_features, n_output)\n\n        nn.init.xavier_uniform_(self._h1.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h2.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h3.weight,\n                                gain=nn.init.calculate_gain('linear'))\n\n    def forward(self, state):\n        features1 = F.relu(self._h1(torch.squeeze(state, 1).float()))\n        features2 = F.relu(self._h2(features1))\n        a = self._h3(features2)\n\n        return a\n\n\ndef experiment(alg, n_epochs, n_steps, n_steps_test):\n    np.random.seed()\n\n    # MDP\n    horizon = 200\n    gamma = 0.99\n    mdp = Gym('Pendulum-v0', horizon, gamma)\n\n    # Settings\n    initial_replay_size = 64\n    max_replay_size = 50000\n    batch_size = 64\n    n_features = 64\n    warmup_transitions = 100\n    tau = 0.005\n    lr_alpha = 3e-4\n\n    use_cuda = torch.cuda.is_available()\n\n    # Approximator\n    actor_input_shape = mdp.info.observation_space.shape\n    actor_mu_params = dict(network=ActorNetwork,\n                           n_features=n_features,\n                           input_shape=actor_input_shape,\n                           output_shape=mdp.info.action_space.shape,\n                           use_cuda=use_cuda)\n    actor_sigma_params = dict(network=ActorNetwork,\n                              n_features=n_features,\n                              input_shape=actor_input_shape,\n                              output_shape=mdp.info.action_space.shape,\n                              use_cuda=use_cuda)\n\n    actor_optimizer = {'class': optim.Adam,\n                       'params': {'lr': 3e-4}}\n\n    critic_input_shape = (actor_input_shape[0] + mdp.info.action_space.shape[0],)\n    critic_params = dict(network=CriticNetwork,\n                         optimizer={'class': optim.Adam,\n                                    'params': {'lr': 3e-4}},\n                         loss=F.mse_loss,\n                         n_features=n_features,\n                         input_shape=critic_input_shape,\n                         output_shape=(1,),\n                         use_cuda=use_cuda)\n\n    # Agent\n    agent = alg(mdp.info, actor_mu_params, actor_sigma_params,\n                actor_optimizer, critic_params, batch_size, initial_replay_size,\n                max_replay_size, warmup_transitions, tau, lr_alpha,\n                critic_fit_params=None)\n\n    # Algorithm\n    core = Core(agent, mdp)\n\n    core.learn(n_steps=initial_replay_size, n_steps_per_fit=initial_replay_size)\n\n    # RUN\n    dataset = core.evaluate(n_steps=n_steps_test, render=False)\n    J = compute_J(dataset, gamma)\n    print('J: ', np.mean(J))\n\n    for n in range(n_epochs):\n        print('Epoch: ', n)\n        core.learn(n_steps=n_steps, n_steps_per_fit=1)\n        dataset = core.evaluate(n_steps=n_steps_test, render=False)\n        J = compute_J(dataset, gamma)\n        print('J: ', np.mean(J))\n\n    print('Press a button to visualize pendulum')\n    input()\n    core.evaluate(n_episodes=5, render=True)\n\n\nif __name__ == '__main__':\n    algs = [\n        SAC\n    ]\n\n    for alg in algs:\n        print('Algorithm: ', alg.__name__)\n        experiment(alg=alg, n_epochs=40, n_steps=1000, n_steps_test=2000)\n"""
examples/pendulum_trust_region.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nfrom tqdm import tqdm, trange\n\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import Gym\nfrom mushroom_rl.algorithms.actor_critic import TRPO, PPO\n\nfrom mushroom_rl.policy import GaussianTorchPolicy\nfrom mushroom_rl.utils.dataset import compute_J\n\n\nclass Network(nn.Module):\n    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n        super(Network, self).__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._h1 = nn.Linear(n_input, n_features)\n        self._h2 = nn.Linear(n_features, n_features)\n        self._h3 = nn.Linear(n_features, n_output)\n\n        nn.init.xavier_uniform_(self._h1.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h2.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h3.weight,\n                                gain=nn.init.calculate_gain('linear'))\n\n    def forward(self, state, **kwargs):\n        features1 = F.relu(self._h1(torch.squeeze(state, 1).float()))\n        features2 = F.relu(self._h2(features1))\n        a = self._h3(features2)\n\n        return a\n\n\ndef experiment(alg, env_id, horizon, gamma, n_epochs, n_steps, n_steps_per_fit, n_episodes_test,\n               alg_params, policy_params):\n    print(alg.__name__)\n\n    mdp = Gym(env_id, horizon, gamma)\n\n    critic_params = dict(network=Network,\n                         optimizer={'class': optim.Adam,\n                                    'params': {'lr': 3e-4}},\n                         loss=F.mse_loss,\n                         n_features=64,\n                         input_shape=mdp.info.observation_space.shape,\n                         output_shape=(1,))\n\n    policy = GaussianTorchPolicy(Network,\n                                 mdp.info.observation_space.shape,\n                                 mdp.info.action_space.shape,\n                                 **policy_params)\n\n    alg_params['critic_params'] = critic_params\n\n    agent = alg(mdp.info, policy, **alg_params)\n\n    core = Core(agent, mdp)\n\n    for it in trange(n_epochs):\n        core.learn(n_steps=n_steps, n_steps_per_fit=n_steps_per_fit)\n        dataset = core.evaluate(n_episodes=n_episodes_test, render=False)\n\n        J = np.mean(compute_J(dataset, mdp.info.gamma))\n        R = np.mean(compute_J(dataset))\n        E = agent.policy.entropy()\n\n        tqdm.write('END OF EPOCH ' + str(it))\n        tqdm.write('J: {}, R: {}, entropy: {}'.format(J, R, E))\n        tqdm.write('##################################################################################################')\n\n    print('Press a button to visualize')\n    input()\n    core.evaluate(n_episodes=5, render=True)\n\n\nif __name__ == '__main__':\n    max_kl = .015\n\n    policy_params = dict(\n        std_0=1.,\n        n_features=64,\n        use_cuda=torch.cuda.is_available()\n\n    )\n\n    ppo_params = dict(actor_optimizer={'class': optim.Adam,\n                                       'params': {'lr': 3e-4}},\n                      n_epochs_policy=4,\n                      batch_size=64,\n                      eps_ppo=.2,\n                      lam=.95,\n                      quiet=True)\n\n    trpo_params = dict(ent_coeff=0.0,\n                       max_kl=.001,\n                       lam=.98,\n                       n_epochs_line_search=10,\n                       n_epochs_cg=10,\n                       cg_damping=1e-2,\n                       cg_residual_tol=1e-10,\n                       quiet=True)\n\n    algs_params = [\n        (TRPO, 'trpo', trpo_params),\n        (PPO, 'ppo', ppo_params)\n     ]\n\n    for alg, alg_name, alg_params in algs_params:\n        experiment(alg=alg, env_id='Pendulum-v0', horizon=200, gamma=.99,\n                   n_epochs=40, n_steps=30000, n_steps_per_fit=3000,\n                   n_episodes_test=25, alg_params=alg_params,\n                   policy_params=policy_params)\n"""
examples/plotting_and_normalization.py,0,"b'import os\n\nfrom mushroom_rl.utils.preprocessors import MinMaxPreprocessor\nfrom mushroom_rl.utils.callbacks import PlotDataset\n\nimport numpy as np\n\nfrom mushroom_rl.algorithms.policy_search import REINFORCE\nfrom mushroom_rl.approximators.parametric import LinearApproximator\nfrom mushroom_rl.approximators.regressor import Regressor\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import LQR\nfrom mushroom_rl.policy import StateStdGaussianPolicy\nfrom mushroom_rl.utils.dataset import compute_J\nfrom mushroom_rl.utils.parameters import AdaptiveParameter\n\nfrom tqdm import tqdm\n\n\n""""""\nThis script shows how to use preprocessors and plot callback.\n\n""""""\n\ntqdm.monitor_interval = 0\n\n\ndef experiment(n_epochs, n_iterations, ep_per_run, save_states_to_disk):\n    np.random.seed()\n\n    # MDP\n    mdp = LQR.generate(dimensions=2, max_pos=10., max_action=5., episodic=True)\n\n    approximator = Regressor(LinearApproximator,\n                             input_shape=mdp.info.observation_space.shape,\n                             output_shape=mdp.info.action_space.shape)\n\n    sigma = Regressor(LinearApproximator,\n                      input_shape=mdp.info.observation_space.shape,\n                      output_shape=mdp.info.action_space.shape)\n\n    sigma_weights = 2 * np.ones(sigma.weights_size)\n    sigma.set_weights(sigma_weights)\n\n    policy = StateStdGaussianPolicy(approximator, sigma)\n\n    # Agent\n    learning_rate = AdaptiveParameter(value=.01)\n    algorithm_params = dict(learning_rate=learning_rate)\n    agent = REINFORCE(mdp.info, policy, **algorithm_params)\n\n    # normalization callback\n    prepro = MinMaxPreprocessor(mdp_info=mdp.info)\n\n    # plotting callback\n    plotter = PlotDataset(mdp.info, obs_normalized=True)\n\n    # Train\n    core = Core(agent, mdp, callback_step=plotter, preprocessors=[prepro])\n\n    # training loop\n    for n in range(n_epochs):\n        core.learn(n_episodes=n_iterations * ep_per_run,\n                   n_episodes_per_fit=ep_per_run)\n        dataset = core.evaluate(n_episodes=ep_per_run, render=False)\n        print(\'Epoch: \', n, \'  J: \', np.mean(compute_J(dataset,\n                                                       mdp.info.gamma)))\n\n    if save_states_to_disk:\n        # save normalization / plot states to disk path\n        os.makedirs(""./temp/"", exist_ok=True)\n        prepro.save_state(""./temp/normalization_state"")\n        plotter.save_state(""./temp/plotting_state"")\n\n        # load states from disk path\n        prepro.load_state(""./temp/normalization_state"")\n        plotter.load_state(""./temp/plotting_state"")\n\n\nif __name__ == \'__main__\':\n    experiment(n_epochs=10, n_iterations=10, ep_per_run=100,\n               save_states_to_disk=False)\n'"
examples/segway_test_bbo.py,0,"b""import numpy as np\n\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments.segway import Segway\nfrom mushroom_rl.algorithms.policy_search import *\nfrom mushroom_rl.policy import DeterministicPolicy\nfrom mushroom_rl.distributions import GaussianDiagonalDistribution\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.approximators.parametric import LinearApproximator\nfrom mushroom_rl.utils.dataset import compute_J\nfrom mushroom_rl.utils.callbacks import CollectDataset\nfrom mushroom_rl.utils.parameters import AdaptiveParameter\n\nfrom tqdm import tqdm\ntqdm.monitor_interval = 0\n\n\ndef experiment(alg, params, n_epochs, n_episodes, n_ep_per_fit):\n    np.random.seed()\n\n    # MDP\n    mdp = Segway()\n\n    # Policy\n    approximator = Regressor(LinearApproximator,\n                             input_shape=mdp.info.observation_space.shape,\n                             output_shape=mdp.info.action_space.shape)\n\n    n_weights = approximator.weights_size\n    mu = np.zeros(n_weights)\n    sigma = 2e-0 * np.ones(n_weights)\n    policy = DeterministicPolicy(approximator)\n    dist = GaussianDiagonalDistribution(mu, sigma)\n\n    agent = alg(mdp.info, dist, policy, **params)\n\n    # Train\n    print(alg.__name__)\n    dataset_callback = CollectDataset()\n    core = Core(agent, mdp, callbacks_episode=[dataset_callback])\n\n    for i in range(n_epochs):\n        core.learn(n_episodes=n_episodes,\n                   n_episodes_per_fit=n_ep_per_fit, render=False)\n        J = compute_J(dataset_callback.get(), gamma=mdp.info.gamma)\n        dataset_callback.clean()\n\n        p = dist.get_parameters()\n\n        print('mu:    ', p[:n_weights])\n        print('sigma: ', p[n_weights:])\n        print('Reward at iteration ' + str(i) + ': ' +\n              str(np.mean(J)))\n\n    print('Press a button to visualize the segway...')\n    input()\n    core.evaluate(n_episodes=3, render=True)\n\n\nif __name__ == '__main__':\n    algs_params = [\n        (REPS, {'eps': 0.05}),\n        (RWR, {'beta': 0.01}),\n        (PGPE, {'learning_rate':  AdaptiveParameter(value=0.3)}),\n        ]\n    for alg, params in algs_params:\n        experiment(alg, params, n_epochs=20, n_episodes=100, n_ep_per_fit=25)\n"""
examples/ship_steering_bbo.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.policy_search import REPS, RWR, PGPE\nfrom mushroom_rl.approximators.parametric import LinearApproximator\nfrom mushroom_rl.approximators.regressor import Regressor\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import ShipSteering\nfrom mushroom_rl.features.tiles import Tiles\nfrom mushroom_rl.features.features import Features\nfrom mushroom_rl.distributions import GaussianDiagonalDistribution\nfrom mushroom_rl.policy import DeterministicPolicy\nfrom mushroom_rl.utils.dataset import compute_J\nfrom mushroom_rl.utils.parameters import AdaptiveParameter\nfrom tqdm import tqdm\n\n\n""""""\nThis script aims to replicate the experiments on the Ship Steering MDP \nusing policy gradient algorithms.\n\n""""""\n\ntqdm.monitor_interval = 0\n\n\ndef experiment(alg, params, n_epochs, n_iterations, ep_per_run):\n    np.random.seed()\n\n    # MDP\n    mdp = ShipSteering()\n\n    # Policy\n    high = [150, 150, np.pi]\n    low = [0, 0, -np.pi]\n    n_tiles = [5, 5, 6]\n    low = np.array(low, dtype=np.float)\n    high = np.array(high, dtype=np.float)\n    n_tilings = 1\n\n    tilings = Tiles.generate(n_tilings=n_tilings, n_tiles=n_tiles, low=low,\n                             high=high)\n\n    phi = Features(tilings=tilings)\n    input_shape = (phi.size,)\n\n    approximator = Regressor(LinearApproximator, input_shape=input_shape,\n                             output_shape=mdp.info.action_space.shape)\n\n    policy = DeterministicPolicy(approximator)\n\n    mu = np.zeros(policy.weights_size)\n    sigma = 4e-1 * np.ones(policy.weights_size)\n    distribution = GaussianDiagonalDistribution(mu, sigma)\n\n    # Agent\n    agent = alg(mdp.info, distribution, policy, features=phi, **params)\n\n    # Train\n    print(alg.__name__)\n    core = Core(agent, mdp)\n    dataset_eval = core.evaluate(n_episodes=ep_per_run)\n    J = compute_J(dataset_eval, gamma=mdp.info.gamma)\n    print(\'J at start : \' + str(np.mean(J)))\n\n    for i in range(n_epochs):\n        core.learn(n_episodes=n_iterations * ep_per_run,\n                   n_episodes_per_fit=ep_per_run)\n        dataset_eval = core.evaluate(n_episodes=ep_per_run)\n        J = compute_J(dataset_eval, gamma=mdp.info.gamma)\n        print(\'J at iteration \' + str(i) + \': \' + str(np.mean(J)))\n\n\nif __name__ == \'__main__\':\n\n    algs_params = [\n        (REPS, {\'eps\': 1.0}),\n        (RWR, {\'beta\': 0.7}),\n        (PGPE, {\'learning_rate\': AdaptiveParameter(value=1.5)}),\n        ]\n\n    for alg, params in algs_params:\n        experiment(alg, params, n_epochs=25, n_iterations=10, ep_per_run=20)\n'"
examples/simple_chain_qlearning.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.value import QLearning\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import *\nfrom mushroom_rl.policy import EpsGreedy\nfrom mushroom_rl.utils.parameters import Parameter\n\nfrom mushroom_rl.utils.dataset import compute_J\n\n\n""""""\nSimple script to solve a simple chain with Q-Learning.\n\n""""""\n\n\ndef experiment():\n    np.random.seed()\n\n    # MDP\n    mdp = generate_simple_chain(state_n=5, goal_states=[2], prob=.8, rew=1,\n                                gamma=.9)\n\n    # Policy\n    epsilon = Parameter(value=.15)\n    pi = EpsGreedy(epsilon=epsilon)\n\n    # Agent\n    learning_rate = Parameter(value=.2)\n    algorithm_params = dict(learning_rate=learning_rate)\n    agent = QLearning(mdp.info, pi, **algorithm_params)\n\n    # Core\n    core = Core(agent, mdp)\n\n    # Initial policy Evaluation\n    dataset = core.evaluate(n_steps=1000)\n    J = np.mean(compute_J(dataset, mdp.info.gamma))\n    print(\'J start:\', J)\n\n    # Train\n    core.learn(n_steps=10000, n_steps_per_fit=1)\n\n    # Final Policy Evaluation\n    dataset = core.evaluate(n_steps=1000)\n    J = np.mean(compute_J(dataset, mdp.info.gamma))\n    print(\'J final:\', J)\n\n\nif __name__ == \'__main__\':\n    experiment()\n'"
mushroom_rl/__init__.py,0,"b""__version__ = '1.4.0'\n"""
tests/test_imports.py,0,b'\n\ndef test_imports():\n    import mushroom_rl\n\n    import mushroom_rl.algorithms\n    import mushroom_rl.algorithms.actor_critic\n    import mushroom_rl.algorithms.actor_critic.classic_actor_critic\n    import mushroom_rl.algorithms.actor_critic.deep_actor_critic\n    import mushroom_rl.algorithms.policy_search\n    import mushroom_rl.algorithms.policy_search.black_box_optimization\n    import mushroom_rl.algorithms.policy_search.policy_gradient\n    import mushroom_rl.algorithms.value\n    import mushroom_rl.algorithms.value.batch_td\n    import mushroom_rl.algorithms.value.td\n    import mushroom_rl.algorithms.value.dqn\n\n    import mushroom_rl.approximators\n    import mushroom_rl.approximators._implementations\n    import mushroom_rl.approximators.parametric\n\n    import mushroom_rl.core\n\n    import mushroom_rl.distributions\n\n    import mushroom_rl.environments\n    import mushroom_rl.environments.generators\n\n    try:\n        import mujoco_py\n    except ImportError:\n        pass\n    else:\n        import mushroom_rl.environments.mujoco_envs\n\n    import mushroom_rl.features\n    import mushroom_rl.features._implementations\n    import mushroom_rl.features.basis\n    import mushroom_rl.features.tensors\n    import mushroom_rl.features.tiles\n\n    import mushroom_rl.policy\n\n    import mushroom_rl.solvers\n\n    import mushroom_rl.utils\n\n'
examples/double_chain_q_learning/__init__.py,0,b''
examples/double_chain_q_learning/double_chain.py,0,"b'import numpy as np\nfrom joblib import Parallel, delayed\n\nfrom mushroom_rl.algorithms.value import QLearning, DoubleQLearning,\\\n    WeightedQLearning, SpeedyQLearning\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import *\nfrom mushroom_rl.policy import EpsGreedy\nfrom mushroom_rl.utils.callbacks import CollectQ\nfrom mushroom_rl.utils.parameters import Parameter, ExponentialParameter\n\n\n""""""\nSimple script to solve a double chain with Q-Learning and some of its variants.\nThe considered double chain is the one presented in:\n""Relative Entropy Policy Search"". Peters J. et al.. 2010.\n\n""""""\n\n\ndef experiment(algorithm_class, exp):\n    np.random.seed()\n\n    # MDP\n    p = np.load(\'chain_structure/p.npy\')\n    rew = np.load(\'chain_structure/rew.npy\')\n    mdp = FiniteMDP(p, rew, gamma=.9)\n\n    # Policy\n    epsilon = Parameter(value=1.)\n    pi = EpsGreedy(epsilon=epsilon)\n\n    # Agent\n    learning_rate = ExponentialParameter(value=1., exp=exp, size=mdp.info.size)\n    algorithm_params = dict(learning_rate=learning_rate)\n    agent = algorithm_class(mdp.info, pi, **algorithm_params)\n\n    # Algorithm\n    collect_Q = CollectQ(agent.approximator)\n    callbacks = [collect_Q]\n    core = Core(agent, mdp, callbacks)\n\n    # Train\n    core.learn(n_steps=20000, n_steps_per_fit=1, quiet=True)\n\n    Qs = collect_Q.get()\n\n    return Qs\n\n\nif __name__ == \'__main__\':\n    n_experiment = 500\n\n    names = {1: \'1\', .51: \'51\', QLearning: \'Q\', DoubleQLearning: \'DQ\',\n             WeightedQLearning: \'WQ\', SpeedyQLearning: \'SPQ\'}\n\n    for e in [1, .51]:\n        for a in [QLearning, DoubleQLearning, WeightedQLearning,\n                  SpeedyQLearning]:\n            out = Parallel(n_jobs=-1)(\n                delayed(experiment)(a, e) for _ in range(n_experiment))\n            Qs = np.array([o for o in out])\n\n            Qs = np.mean(Qs, 0)\n\n            np.save(names[a] + names[e] + \'.npy\', Qs[:, 0, 0])\n'"
examples/taxi_mellow_sarsa/taxi_mellow.py,0,"b'import numpy as np\nfrom joblib import Parallel, delayed\n\nfrom mushroom_rl.algorithms.value import SARSA\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments.generators.taxi import generate_taxi\nfrom mushroom_rl.policy import Boltzmann, EpsGreedy, Mellowmax\nfrom mushroom_rl.utils.callbacks import CollectDataset\nfrom mushroom_rl.utils.parameters import Parameter\n\n\n""""""\nThis script aims to replicate the experiments on the Taxi MDP as presented in:\n""An Alternative Softmax Operator for Reinforcement Learning"", Asadi K. et al..\n2017. \n\n""""""\n\n\ndef experiment(policy, value):\n    np.random.seed()\n\n    # MDP\n    mdp = generate_taxi(\'grid.txt\')\n\n    # Policy\n    pi = policy(Parameter(value=value))\n\n    # Agent\n    learning_rate = Parameter(value=.15)\n    algorithm_params = dict(learning_rate=learning_rate)\n    agent = SARSA(mdp.info, pi, **algorithm_params)\n\n    # Algorithm\n    collect_dataset = CollectDataset()\n    callbacks = [collect_dataset]\n    core = Core(agent, mdp, callbacks)\n\n    # Train\n    n_steps = 300000\n    core.learn(n_steps=n_steps, n_steps_per_fit=1, quiet=True)\n\n    return np.sum(np.array(collect_dataset.get())[:, 2]) / float(n_steps)\n\n\nif __name__ == \'__main__\':\n    n_experiment = 25\n\n    algs = {EpsGreedy: \'epsilon\', Boltzmann: \'boltzmann\', Mellowmax: \'mellow\'}\n    ranges = {EpsGreedy: np.linspace(.05, .5, 10),\n              Boltzmann: np.linspace(.5, 10, 10),\n              Mellowmax: np.linspace(.5, 10, 10)}\n\n    for p in [EpsGreedy, Boltzmann, Mellowmax]:\n        print(\'Policy: \', algs[p])\n        Js = list()\n        for v in ranges[p]:\n            out = Parallel(n_jobs=-1)(\n                delayed(experiment)(p, v) for _ in range(n_experiment))\n            J = [np.mean(o) for o in out]\n            Js.append(np.mean(J))\n\n        np.save(\'r_%s.npy\' % algs[p], Js)\n'"
mushroom_rl/algorithms/__init__.py,0,"b""from .agent import Agent\n\n__all__ = ['Agent']\n"""
mushroom_rl/algorithms/agent.py,2,"b'import json\nimport torch\nimport pickle\nimport numpy as np\nfrom copy import deepcopy\nfrom pathlib import Path, PurePath\n\n\nclass Agent(object):\n    """"""\n    This class implements the functions to manage the agent (e.g. move the agent\n    following its policy).\n\n    """"""\n\n    def __init__(self, mdp_info, policy, features=None):\n        """"""\n        Constructor.\n\n        Args:\n            mdp_info (MDPInfo): information about the MDP;\n            policy (Policy): the policy followed by the agent;\n            features (object, None): features to extract from the state.\n\n        """"""\n        self.mdp_info = mdp_info\n        self.policy = policy\n\n        self.phi = features\n\n        self.next_action = None\n\n        self._add_save_attr(\n            mdp_info=\'pickle\',\n            policy=\'pickle\',\n            phi=\'pickle\',\n            next_action=\'numpy\'\n        )\n\n    def fit(self, dataset):\n        """"""\n        Fit step.\n\n        Args:\n            dataset (list): the dataset.\n\n        """"""\n        raise NotImplementedError(\'Agent is an abstract class\')\n\n    def draw_action(self, state):\n        """"""\n        Return the action to execute in the given state. It is the action\n        returned by the policy or the action set by the algorithm (e.g. in the\n        case of SARSA).\n\n        Args:\n            state (np.ndarray): the state where the agent is.\n\n        Returns:\n            The action to be executed.\n\n        """"""\n        if self.phi is not None:\n            state = self.phi(state)\n\n        if self.next_action is None:\n            return self.policy.draw_action(state)\n        else:\n            action = self.next_action\n            self.next_action = None\n\n            return action\n\n    def episode_start(self):\n        """"""\n        Called by the agent when a new episode starts.\n\n        """"""\n        self.policy.reset()\n\n    def stop(self):\n        """"""\n        Method used to stop an agent. Useful when dealing with real world\n        environments, simulators, or to cleanup environments internals after\n        a core learn/evaluate to enforce consistency.\n\n        """"""\n        pass\n\n    @classmethod\n    def load(cls, path):\n        """"""\n        Load and deserialize the agent from the given location on disk.\n\n        Args:\n            path (string): Relative or absolute path to the agents save\n                location.\n\n        Returns:\n            The loaded agent.\n\n        """"""\n        if not isinstance(path, str): \n            raise ValueError(\'path has to be of type string\')\n        if not Path(path).is_dir():\n            raise NotADirectoryError(""Path to load agent is not valid"")\n\n        agent_type, save_attributes = cls._load_pickle(\n            PurePath(path, \'agent.config\')).values()\n\n        agent = agent_type.__new__(agent_type)\n\n        for att, method in save_attributes.items():\n            load_path = Path(path, \'{}.{}\'.format(att, method))\n            \n            if load_path.is_file():\n                load_method = getattr(cls, \'_load_{}\'.format(method))\n                if load_method is None:\n                    raise NotImplementedError(\'Method _load_{} is not\'\n                                              \'implemented\'.format(method))\n                att_val = load_method(load_path.resolve())\n                setattr(agent, att, att_val)\n            else:\n                setattr(agent, att, None)\n\n        agent._post_load()\n\n        return agent\n\n    def save(self, path):\n        """"""\n        Serialize and save the agent to the given path on disk.\n\n        Args:\n            path (string): Relative or absolute path to the agents save\n                location.\n\n        """"""\n        if not isinstance(path, str):\n            raise ValueError(\'path has to be of type string\')\n\n        path_obj = Path(path)\n        path_obj.mkdir(parents=True, exist_ok=True)\n\n        # Save algorithm type and save_attributes\n        agent_config = dict(\n            type=type(self),\n            save_attributes=self._save_attributes\n        )\n        self._save_pickle(PurePath(path, \'agent.config\'), agent_config)\n\n        for att, method in self._save_attributes.items():\n            attribute = getattr(self, att) if hasattr(self, att) else None\n            save_method = getattr(self, \'_save_{}\'.format(method)) if hasattr(\n                self, \'_save_{}\'.format(method)) else None\n            if attribute is None:\n                continue\n            elif save_method is None:\n                raise NotImplementedError(\n                    ""Method _save_{} is not implemented for class \'{}\'"".format(\n                        method, self.__class__.__name__)\n                )\n            else:\n                save_method(PurePath(path, ""{}.{}"".format(att, method)),\n                            attribute)\n\n    def copy(self):\n        """"""\n        Returns:\n             A deepcopy of the agent.\n\n        """"""\n        return deepcopy(self)\n\n    def _add_save_attr(self, **attr_dict):\n        """"""\n        Add attributes that should be saved for an agent.\n\n        Args:\n            attr_dict (dict): dictionary of attributes mapped to the method that\n                should be used to save and load them.\n\n        """"""\n        if not hasattr(self, \'_save_attributes\'):\n            self._save_attributes = dict(_save_attributes=\'json\')\n        self._save_attributes.update(attr_dict)\n\n    def _post_load(self):\n        """"""\n        This method can be overwritten to implement logic that is executed after\n        the loading of the agent.\n\n        """"""\n        pass\n\n    @staticmethod\n    def _load_pickle(path):\n        with Path(path).open(\'rb\') as f:\n            return pickle.load(f)\n    \n    @staticmethod\n    def _load_numpy(path):\n        with Path(path).open(\'rb\') as f:\n            return np.load(f)\n    \n    @staticmethod\n    def _load_torch(path):\n        return torch.load(path)\n    \n    @staticmethod\n    def _load_json(path):\n        with Path(path).open(\'r\') as f:\n            return json.load(f)\n\n    @staticmethod\n    def _save_pickle(path, obj):\n        with Path(path).open(\'wb\') as f:\n            pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n    \n    @staticmethod\n    def _save_numpy(path, obj):\n        with Path(path).open(\'wb\') as f:\n            np.save(f, obj)\n    \n    @staticmethod\n    def _save_torch(path, obj):\n        torch.save(obj, path)\n    \n    @staticmethod\n    def _save_json(path, obj):\n        with Path(path).open(\'w\') as f:\n            json.dump(obj, f)\n'"
mushroom_rl/approximators/__init__.py,0,"b""from ._implementations.ensemble import Ensemble\nfrom .regressor import Regressor\n\n__all__ = ['Ensemble', 'Regressor']\n"""
mushroom_rl/approximators/regressor.py,0,"b'import numpy as np\n\nfrom ._implementations.q_regressor import QRegressor\nfrom ._implementations.action_regressor import ActionRegressor\nfrom ._implementations.ensemble import Ensemble\nfrom ._implementations.generic_regressor import GenericRegressor\n\n\nclass Regressor:\n    """"""\n    This class implements the function to manage a function approximator. This\n    class selects the appropriate kind of regressor to implement according to\n    the parameters provided by the user; this makes this class the only one to\n    use for each kind of task that has to be performed.\n    The inference of the implementation to choose is done checking the provided\n    values of parameters ``n_actions``.\n    If ``n_actions`` is provided, it means that the user wants to implement an\n    approximator of the Q-function: if the value of ``n_actions`` is equal to\n    the ``output_shape`` then a ``QRegressor`` is created, else\n    (``output_shape`` should be (1,)) an ``ActionRegressor`` is created.\n    Otherwise a ``GenericRegressor`` is created.\n    An ``Ensemble`` model can be used for all the previous implementations\n    listed before simply providing a ``n_models`` parameter greater than 1.\n\n    """"""\n    def __init__(self, approximator, input_shape, output_shape=(1,),\n                 n_actions=None, n_models=1, **params):\n        """"""\n        Constructor.\n\n        Args:\n            approximator (object): the approximator class to use to create\n                the model;\n            input_shape (tuple): the shape of the input of the model;\n            output_shape (tuple, (1,)): the shape of the output of the model;\n            n_actions (int, None): number of actions considered to create a\n                ``QRegressor`` or an ``ActionRegressor``;\n            n_models (int, 1): number of models to create;\n            **params (dict): other parameters to create each model.\n\n        """"""\n        if not approximator.__module__.startswith(\'sklearn\'):\n            params[\'input_shape\'] = input_shape\n            params[\'output_shape\'] = output_shape\n\n        self._input_shape = input_shape\n        self._output_shape = output_shape\n\n        self.n_actions = n_actions\n\n        assert n_models >= 1\n        self._n_models = n_models\n\n        if self._n_models > 1:\n            params[\'model\'] = approximator\n            params[\'n_models\'] = n_models\n            approximator = Ensemble\n\n        if n_actions is not None:\n            assert len(self._output_shape) == 1 and n_actions >= 2\n            if n_actions == self._output_shape[0]:\n                self._impl = QRegressor(approximator, **params)\n            else:\n                self._impl = ActionRegressor(approximator, n_actions, **params)\n        else:\n            self._impl = GenericRegressor(approximator,\n                                          len(self.input_shape),\n                                          **params)\n\n    def __call__(self, *z, **predict_params):\n        return self.predict(*z, **predict_params)\n\n    def _ndim(self):\n        if isinstance(self._input_shape[0], tuple):\n            ndim = len(self._input_shape[0])\n        else:\n            ndim = len(self._input_shape)\n\n        return ndim\n\n    def fit(self, *z, **fit_params):\n        """"""\n        Fit the model.\n\n        Args:\n            *z (list): list of input of the model;\n            **fit_params (dict): parameters to use to fit the model.\n\n        """"""\n        ndim = self._ndim()\n\n        if z[0].ndim == ndim:\n            z = [np.expand_dims(z_i, axis=0) for z_i in z]\n        self._impl.fit(*z, **fit_params)\n\n    def predict(self, *z, **predict_params):\n        """"""\n        Predict the output of the model given an input.\n\n        Args:\n            *z (list): list of input of the model;\n            **predict_params(dict): parameters to use to predict with the model.\n\n        Returns:\n            The model prediction.\n\n        """"""\n        ndim = self._ndim()\n\n        if z[0].ndim == ndim:\n            z = [np.expand_dims(z_i, axis=0) for z_i in z]\n\n            return self._impl.predict(*z, **predict_params)[0]\n        else:\n            return self._impl.predict(*z, **predict_params)\n\n    @property\n    def model(self):\n        """"""\n        Returns:\n             The model object.\n\n        """"""\n        return self._impl.model\n\n    def reset(self):\n        """"""\n        Reset the model parameters.\n\n        """"""\n        try:\n            self._impl.reset()\n        except AttributeError:\n            raise NotImplementedError(\'Attempt to reset weights of a\'\n                                      \'non-parametric regressor.\')\n\n    @property\n    def input_shape(self):\n        """"""\n        Returns:\n             The shape of the input of the model.\n\n        """"""\n        return self._input_shape\n\n    @property\n    def output_shape(self):\n        """"""\n        Returns:\n             The shape of the output of the model.\n\n        """"""\n        return self._output_shape\n\n    @property\n    def weights_size(self):\n        """"""\n        Returns:\n             The shape of the weights of the model.\n\n        """"""\n        try:\n            return self._impl.weights_size\n        except AttributeError:\n            raise NotImplementedError(\'Attempt to get shape of weights of a\'\n                                      \' non-parametric regressor.\')\n\n    def get_weights(self):\n        """"""\n        Returns:\n             The weights of the model.\n\n        """"""\n        try:\n            return self._impl.get_weights()\n        except AttributeError:\n            raise NotImplementedError(\'Attempt to get weights of a\'\n                                      \' non-parametric regressor.\')\n\n    def set_weights(self, w):\n        """"""\n        Args:\n            w (list): list of weights to be set in the model.\n\n        """"""\n        try:\n            self._impl.set_weights(w)\n        except AttributeError:\n            raise NotImplementedError(\'Attempt to set weights of a\'\n                                      \' non-parametric regressor.\')\n\n    def diff(self, *z):\n        """"""\n        Args:\n            *z (list): the input of the model.\n\n        Returns:\n             The derivative of the model.\n\n        """"""\n        try:\n            return self._impl.diff(*z)\n        except AttributeError:\n            raise NotImplementedError(\'Attempt to compute derivative of a\'\n                                      \' non-differentiable regressor.\')\n\n    def __len__(self):\n        return 1 if self._n_models == 1 else len(self._impl)\n\n    def __getitem__(self, item):\n        if len(self) == 1:\n            assert item == 0\n\n            return self.model\n        else:\n            return self.model[item]\n'"
mushroom_rl/core/__init__.py,0,"b""from .core import Core\n\n__all__ = ['Core']\n"""
mushroom_rl/core/core.py,0,"b'from tqdm import tqdm\n\n\nclass Core(object):\n    """"""\n    Implements the functions to run a generic algorithm.\n\n    """"""\n    def __init__(self, agent, mdp, callbacks_episode=None, callback_step=None,\n                 preprocessors=None):\n        """"""\n        Constructor.\n\n        Args:\n            agent (Agent): the agent moving according to a policy;\n            mdp (Environment): the environment in which the agent moves;\n            callbacks_episode (list): list of callbacks to execute at the end of\n                each learn iteration;\n            callback_step (Callback): callback to execute after each step;\n            preprocessors (list): list of state preprocessors to be\n                applied to state variables before feeding them to the\n                agent.\n\n        """"""\n        self.agent = agent\n        self.mdp = mdp\n        self.callbacks_episode = callbacks_episode if callbacks_episode is not None else list()\n        self.callback_step = callback_step if callback_step is not None else lambda x: None\n        self._preprocessors = preprocessors if preprocessors is not None else list()\n\n        self._state = None\n\n        self._total_episodes_counter = 0\n        self._total_steps_counter = 0\n        self._current_episodes_counter = 0\n        self._current_steps_counter = 0\n        self._episode_steps = None\n        self._n_episodes = None\n        self._n_steps_per_fit = None\n        self._n_episodes_per_fit = None\n\n    def learn(self, n_steps=None, n_episodes=None, n_steps_per_fit=None,\n              n_episodes_per_fit=None, render=False, quiet=False):\n        """"""\n        This function moves the agent in the environment and fits the policy\n        using the collected samples. The agent can be moved for a given number\n        of steps or a given number of episodes and, independently from this\n        choice, the policy can be fitted after a given number of steps or a\n        given number of episodes. By default, the environment is reset.\n\n        Args:\n            n_steps (int, None): number of steps to move the agent;\n            n_episodes (int, None): number of episodes to move the agent;\n            n_steps_per_fit (int, None): number of steps between each fit of the\n                policy;\n            n_episodes_per_fit (int, None): number of episodes between each fit\n                of the policy;\n            render (bool, False): whether to render the environment or not;\n            quiet (bool, False): whether to show the progress bar or not.\n\n        """"""\n        assert (n_episodes_per_fit is not None and n_steps_per_fit is None)\\\n            or (n_episodes_per_fit is None and n_steps_per_fit is not None)\n\n        self._n_steps_per_fit = n_steps_per_fit\n        self._n_episodes_per_fit = n_episodes_per_fit\n\n        if n_steps_per_fit is not None:\n            fit_condition =\\\n                lambda: self._current_steps_counter >= self._n_steps_per_fit\n        else:\n            fit_condition = lambda: self._current_episodes_counter\\\n                                     >= self._n_episodes_per_fit\n\n        self._run(n_steps, n_episodes, fit_condition, render, quiet)\n\n    def evaluate(self, initial_states=None, n_steps=None, n_episodes=None,\n                 render=False, quiet=False):\n        """"""\n        This function moves the agent in the environment using its policy.\n        The agent is moved for a provided number of steps, episodes, or from\n        a set of initial states for the whole episode. By default, the\n        environment is reset.\n\n        Args:\n            initial_states (np.ndarray, None): the starting states of each\n                episode;\n            n_steps (int, None): number of steps to move the agent;\n            n_episodes (int, None): number of episodes to move the agent;\n            render (bool, False): whether to render the environment or not;\n            quiet (bool, False): whether to show the progress bar or not.\n\n        """"""\n        fit_condition = lambda: False\n\n        return self._run(n_steps, n_episodes, fit_condition, render, quiet,\n                         initial_states)\n\n    def _run(self, n_steps, n_episodes, fit_condition, render, quiet,\n             initial_states=None):\n        assert n_episodes is not None and n_steps is None and initial_states is None\\\n            or n_episodes is None and n_steps is not None and initial_states is None\\\n            or n_episodes is None and n_steps is None and initial_states is not None\n\n        self._n_episodes = len(\n            initial_states) if initial_states is not None else n_episodes\n\n        if n_steps is not None:\n            move_condition =\\\n                lambda: self._total_steps_counter < n_steps\n\n            steps_progress_bar = tqdm(total=n_steps,\n                                      dynamic_ncols=True, disable=quiet,\n                                      leave=False)\n            episodes_progress_bar = tqdm(disable=True)\n        else:\n            move_condition =\\\n                lambda: self._total_episodes_counter < self._n_episodes\n\n            steps_progress_bar = tqdm(disable=True)\n            episodes_progress_bar = tqdm(total=self._n_episodes,\n                                         dynamic_ncols=True, disable=quiet,\n                                         leave=False)\n\n        return self._run_impl(move_condition, fit_condition, steps_progress_bar,\n                              episodes_progress_bar, render, initial_states)\n\n    def _run_impl(self, move_condition, fit_condition, steps_progress_bar,\n                  episodes_progress_bar, render, initial_states):\n        self._total_episodes_counter = 0\n        self._total_steps_counter = 0\n        self._current_episodes_counter = 0\n        self._current_steps_counter = 0\n\n        dataset = list()\n        last = True\n        while move_condition():\n            if last:\n                self.reset(initial_states)\n\n            sample = self._step(render)\n            dataset.append(sample)\n\n            self.callback_step([sample])\n\n            self._total_steps_counter += 1\n            self._current_steps_counter += 1\n            steps_progress_bar.update(1)\n\n            if sample[-1]:\n                self._total_episodes_counter += 1\n                self._current_episodes_counter += 1\n                episodes_progress_bar.update(1)\n\n            if fit_condition():\n                self.agent.fit(dataset)\n                self._current_episodes_counter = 0\n                self._current_steps_counter = 0\n\n                for c in self.callbacks_episode:\n                    c(dataset)\n\n                dataset = list()\n\n            last = sample[-1]\n\n        self.agent.stop()\n        self.mdp.stop()\n\n        steps_progress_bar.close()\n        episodes_progress_bar.close()\n\n        return dataset\n\n    def _step(self, render):\n        """"""\n        Single step.\n\n        Args:\n            render (bool): whether to render or not.\n\n        Returns:\n            A tuple containing the previous state, the action sampled by the\n            agent, the reward obtained, the reached state, the absorbing flag\n            of the reached state and the last step flag.\n\n        """"""\n        action = self.agent.draw_action(self._state)\n        next_state, reward, absorbing, _ = self.mdp.step(action)\n\n        self._episode_steps += 1\n\n        if render:\n            self.mdp.render()\n\n        last = not(\n            self._episode_steps < self.mdp.info.horizon and not absorbing)\n\n        state = self._state\n        next_state = self._preprocess(next_state.copy())\n        self._state = next_state\n\n        return state, action, reward, next_state, absorbing, last\n\n    def reset(self, initial_states=None):\n        """"""\n        Reset the state of the agent.\n\n        """"""\n        if initial_states is None\\\n            or self._total_episodes_counter == self._n_episodes:\n            initial_state = None\n        else:\n            initial_state = initial_states[self._total_episodes_counter]\n\n        self._state = self._preprocess(self.mdp.reset(initial_state).copy())\n        self.agent.episode_start()\n        self.agent.next_action = None\n        self._episode_steps = 0\n\n    def _preprocess(self, state):\n        """"""\n        Method to apply state preprocessors.\n\n        Args:\n            state (np.ndarray): the state to be preprocessed.\n\n        Returns:\n             The preprocessed state.\n\n        """"""\n        for p in self._preprocessors:\n            state = p(state)\n\n        return state\n'"
mushroom_rl/distributions/__init__.py,0,"b""from .distribution import Distribution\nfrom .gaussian import GaussianDistribution, GaussianDiagonalDistribution, \\\n    GaussianCholeskyDistribution\n\n__all__ = ['Distribution',\n           'GaussianDistribution', 'GaussianDiagonalDistribution',\n           'GaussianCholeskyDistribution']\n"""
mushroom_rl/distributions/distribution.py,0,"b'class Distribution(object):\n    """"""\n    Interface for Distributions to represent a generic probability distribution.\n    Probability distributions are often used by black box optimization\n    algorithms in order to perform exploration in parameter space. In\n    literature, they are also known as high level policies.\n\n    """"""\n\n    def sample(self):\n        """"""\n        Draw a sample from the distribution.\n\n        Returns:\n            A random vector sampled from the distribution.\n\n        """"""\n        raise NotImplementedError\n\n    def log_pdf(self, theta):\n        """"""\n        Compute the logarithm of the probability density function in the\n        specified point\n\n        Args:\n            theta (np.ndarray): the point where the log pdf is calculated\n\n        Returns:\n            The value of the log pdf in the specified point.\n\n        """"""\n        raise NotImplementedError\n\n    def __call__(self, theta):\n        """"""\n        Compute the probability density function in the specified point\n\n        Args:\n            theta (np.ndarray): the point where the pdf is calculated\n\n        Returns:\n            The value of the pdf in the specified point.\n\n        """"""\n        raise NotImplementedError\n\n    def mle(self, theta, weights=None):\n        """"""\n        Compute the (weighted) maximum likelihood estimate of the points,\n        and update the distribution accordingly.\n\n        Args:\n            theta (np.ndarray): a set of points, every row is a sample\n            weights (np.ndarray, None): a vector of weights. If specified\n                                        the weighted maximum likelihood\n                                        estimate is computed instead of the\n                                        plain maximum likelihood. The number of\n                                        elements of this vector must be equal\n                                        to the number of rows of the theta\n                                        matrix.\n\n        """"""\n        raise NotImplementedError\n\n    def diff_log(self, theta):\n        """"""\n        Compute the derivative of the gradient of the probability denstity\n        function in the specified point.\n\n        Args:\n            theta (np.ndarray): the point where the gradient of the log pdf is\n            calculated\n\n        Returns:\n            The gradient of the log pdf in the specified point.\n\n        """"""\n        raise NotImplementedError\n\n    def diff(self, theta):\n        """"""\n        Compute the derivative of the probability density function, in the\n        specified point. Normally it is computed w.r.t. the\n        derivative of the logarithm of the probability density function,\n        exploiting the likelihood ratio trick, i.e.:\n\n        .. math::\n            \\\\nabla_{\\\\rho}p(\\\\theta)=p(\\\\theta)\\\\nabla_{\\\\rho}\\\\log p(\\\\theta)\n\n        Args:\n            theta (np.ndarray): the point where the gradient of the pdf is\n            calculated.\n\n        Returns:\n            The gradient of the pdf in the specified point.\n\n        """"""\n        return self(theta) * self.diff_log(theta)\n\n    def get_parameters(self):\n        """"""\n        Getter.\n\n        Returns:\n             The current distribution parameters.\n\n        """"""\n        raise NotImplementedError\n\n    def set_parameters(self, rho):\n        """"""\n        Setter.\n\n        Args:\n            rho (np.ndarray): the vector of the new parameters to be used by\n                              the distribution\n\n        """"""\n        raise NotImplementedError\n\n    @property\n    def parameters_size(self):\n        """"""\n        Property.\n\n        Returns:\n             The size of the distribution parameters.\n\n        """"""\n        raise NotImplementedError\n'"
mushroom_rl/distributions/gaussian.py,0,"b'import numpy as np\nfrom .distribution import Distribution\nfrom scipy.stats import multivariate_normal\n\n\nclass GaussianDistribution(Distribution):\n    """"""\n    Gaussian distribution with fixed covariance matrix. The parameters\n    vector represents only the mean.\n    """"""\n    def __init__(self, mu, sigma):\n        self._mu = mu\n        self._sigma = sigma\n        self.inv_sigma = np.linalg.inv(sigma)\n\n    def sample(self):\n        return np.random.multivariate_normal(self._mu, self._sigma)\n\n    def log_pdf(self, theta):\n        return multivariate_normal.logpdf(theta, self._mu, self._sigma)\n\n    def __call__(self, theta):\n        return multivariate_normal.pdf(theta, self._mu, self._sigma)\n\n    def mle(self, theta, weights=None):\n        if weights is None:\n            self._mu = np.mean(theta, axis=0)\n        else:\n            self._mu = weights.dot(theta) / np.sum(weights)\n\n    def diff_log(self, theta):\n        delta = theta - self._mu\n        g = self.inv_sigma.dot(delta)\n\n        return g\n\n    def get_parameters(self):\n        return self._mu\n\n    def set_parameters(self, rho):\n        self._mu = rho\n\n    @property\n    def parameters_size(self):\n        return len(self._mu)\n\n\nclass GaussianDiagonalDistribution(Distribution):\n    """"""\n    Gaussian distribution with diagonal covariance matrix. The parameters\n    vector represents the mean and the standard deviation for each dimension.\n    """"""\n    def __init__(self, mu, std):\n        assert(len(std.shape) == 1)\n        self._mu = mu\n        self._std = std\n\n    def sample(self):\n        sigma = np.diag(self._std**2)\n        return np.random.multivariate_normal(self._mu, sigma)\n\n    def log_pdf(self, theta):\n        sigma = np.diag(self._std ** 2)\n        return multivariate_normal.logpdf(theta, self._mu, sigma)\n\n    def __call__(self, theta):\n        sigma = np.diag(self._std ** 2)\n        return multivariate_normal.pdf(theta, self._mu, sigma)\n\n    def mle(self, theta, weights=None):\n        if weights is None:\n            self._mu = np.mean(theta, axis=0)\n            self._std = np.std(theta, axis=0)\n        else:\n            sumD = np.sum(weights)\n            sumD2 = np.sum(weights**2)\n            Z = sumD - sumD2 / sumD\n\n            self._mu = weights.dot(theta) / sumD\n\n            delta2 = (theta - self._mu)**2\n            self._std = np.sqrt(weights.dot(delta2) / Z)\n\n    def diff_log(self, theta):\n        n_dims = len(self._mu)\n\n        sigma = self._std**2\n\n        g = np.empty(self.parameters_size)\n\n        delta = theta - self._mu\n\n        g_mean = delta / sigma\n        g_cov = delta**2 / (self._std**3) - 1 / self._std\n\n        g[:n_dims] = g_mean\n        g[n_dims:] = g_cov\n        return g\n\n    def get_parameters(self):\n        rho = np.empty(self.parameters_size)\n        n_dims = len(self._mu)\n\n        rho[:n_dims] = self._mu\n        rho[n_dims:] = self._std\n\n        return rho\n\n    def set_parameters(self, rho):\n        n_dims = len(self._mu)\n        self._mu = rho[:n_dims]\n        self._std = rho[n_dims:]\n\n    @property\n    def parameters_size(self):\n        return 2 * len(self._mu)\n\n\nclass GaussianCholeskyDistribution(Distribution):\n    """"""\n    Gaussian distribution with full covariance matrix. The parameters\n    vector represents the mean and the Cholesky decomposition of the\n    covariance matrix. This parametrization enforce the covariance matrix to be\n    positive definite.\n    """"""\n    def __init__(self, mu, sigma):\n        self._mu = mu\n        self._chol_sigma = np.linalg.cholesky(sigma)\n\n    def sample(self):\n        sigma = self._chol_sigma.dot(self._chol_sigma.T)\n        return np.random.multivariate_normal(self._mu, sigma)\n\n    def log_pdf(self, theta):\n        sigma = self._chol_sigma.dot(self._chol_sigma.T)\n        return multivariate_normal.logpdf(theta, self._mu, sigma)\n\n    def __call__(self, theta):\n        sigma = self._chol_sigma.dot(self._chol_sigma.T)\n        return multivariate_normal.pdf(theta, self._mu, sigma)\n\n    def mle(self, theta, weights=None):\n        if weights is None:\n            self._mu = np.mean(theta, axis=0)\n            sigma = np.cov(theta, rowvar=False)\n        else:\n            sumD = np.sum(weights)\n            sumD2 = np.sum(weights**2)\n            Z = sumD - sumD2 / sumD\n\n            self._mu = weights.dot(theta) / sumD\n\n            delta = theta - self._mu\n\n            sigma = delta.T.dot(np.diag(weights)).dot(delta) / Z\n\n        self._chol_sigma = np.linalg.cholesky(sigma)\n\n    def diff_log(self, theta):\n        n_dims = len(self._mu)\n        inv_chol = np.linalg.inv(self._chol_sigma)\n        inv_sigma = inv_chol.T.dot(inv_chol)\n\n        g = np.empty(self.parameters_size)\n\n        delta = theta - self._mu\n        g_mean = inv_sigma.dot(delta)\n\n        delta_a = np.reshape(delta, (-1, 1))\n        delta_b = np.reshape(delta, (1, -1))\n\n        S = inv_chol.dot(delta_a).dot(delta_b).dot(inv_sigma)\n\n        g_cov = S - np.diag(np.diag(inv_chol))\n\n        g[:n_dims] = g_mean\n        g[n_dims:] = g_cov.T[np.tril_indices(n_dims)]\n\n        return g\n\n    def get_parameters(self):\n        rho = np.empty(self.parameters_size)\n        n_dims = len(self._mu)\n\n        rho[:n_dims] = self._mu\n        rho[n_dims:] = self._chol_sigma[np.tril_indices(n_dims)]\n\n        return rho\n\n    def set_parameters(self, rho):\n        n_dims = len(self._mu)\n        self._mu = rho[:n_dims]\n        self._chol_sigma[np.tril_indices(n_dims)] = rho[n_dims:]\n\n    @property\n    def parameters_size(self):\n        n_dims = len(self._mu)\n\n        return 2 * n_dims + (n_dims * n_dims - n_dims) // 2\n'"
mushroom_rl/environments/__init__.py,0,"b""__extras__ = []\n\nfrom .environment import Environment, MDPInfo\ntry:\n    Atari = None\n    from .atari import Atari\n    __extras__.append('Atari')\nexcept ImportError:\n    pass\n\ntry:\n    Gym = None\n    from .gym_env import Gym\n    __extras__.append('Gym')\nexcept ImportError:\n    pass\n\ntry:\n    DMControl = None\n    from .dm_control_env import DMControl\n    __extras__.append('DMControl')\nexcept ImportError:\n    pass\n\ntry:\n    Mujoco = None\n    from .mujoco import MuJoCo\n    __extras__.append('Mujoco')\nexcept ImportError:\n    pass\n\nfrom .car_on_hill import CarOnHill\nfrom .generators.simple_chain import generate_simple_chain\nfrom .grid_world import GridWorld, GridWorldVanHasselt\nfrom .finite_mdp import FiniteMDP\nfrom .inverted_pendulum import InvertedPendulum\nfrom .cart_pole import CartPole\nfrom .puddle_world import PuddleWorld\nfrom .ship_steering import ShipSteering\nfrom .lqr import LQR\n\n__all__ = ['CarOnHill', 'Environment', 'MDPInfo',\n           'FiniteMDP', 'InvertedPendulum', 'CartPole',\n           'GridWorld', 'generate_simple_chain', 'GridWorldVanHasselt',\n           'PuddleWorld', 'ShipSteering', 'LQR'] + __extras__\n"""
mushroom_rl/environments/atari.py,0,"b'from copy import deepcopy\nfrom collections import deque\n\nimport cv2\ncv2.ocl.setUseOpenCL(False)\nimport gym\n\nfrom mushroom_rl.environments import Environment, MDPInfo\nfrom mushroom_rl.utils.spaces import *\n\n\nclass MaxAndSkip(gym.Wrapper):\n    def __init__(self, env, skip, max_pooling=True):\n        gym.Wrapper.__init__(self, env)\n        self._obs_buffer = np.zeros((2,) + env.observation_space.shape,\n                                    dtype=np.uint8)\n        self._skip = skip\n        self._max_pooling = max_pooling\n\n    def reset(self):\n        return self.env.reset()\n\n    def step(self, action):\n        total_reward = 0.\n        for i in range(self._skip):\n            obs, reward, absorbing, info = self.env.step(action)\n            if i == self._skip - 2:\n                self._obs_buffer[0] = obs\n            if i == self._skip - 1:\n                self._obs_buffer[1] = obs\n            total_reward += reward\n            if absorbing:\n                break\n        if self._max_pooling:\n            frame = self._obs_buffer.max(axis=0)\n        else:\n            frame = self._obs_buffer.mean(axis=0)\n\n        return frame, total_reward, absorbing, info\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\n\nclass LazyFrames(object):\n    """"""\n    From OpenAI Baseline.\n    https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n\n    """"""\n    def __init__(self, frames, history_length):\n        self._frames = frames\n\n        assert len(self._frames) == history_length\n\n    def __array__(self, dtype=None):\n        out = np.array(self._frames)\n        if dtype is not None:\n            out = out.astype(dtype)\n\n        return out\n\n    def copy(self):\n        return self\n\n\nclass Atari(Environment):\n    """"""\n    The Atari environment as presented in:\n    ""Human-level control through deep reinforcement learning"". Mnih et. al..\n    2015.\n\n    """"""\n    def __init__(self, name, width=84, height=84, ends_at_life=False,\n                 max_pooling=True, history_length=4, max_no_op_actions=30):\n        """"""\n        Constructor.\n\n        Args:\n            name (str): id name of the Atari game in Gym;\n            width (int, 84): width of the screen;\n            height (int, 84): height of the screen;\n            ends_at_life (bool, False): whether the episode ends when a life is\n               lost or not;\n            max_pooling (bool, True): whether to do max-pooling or\n                average-pooling of the last two frames when using NoFrameskip;\n            history_length (int, 4): number of frames to form a state;\n            max_no_op_actions (int, 30): maximum number of no-op action to\n                execute at the beginning of an episode.\n\n        """"""\n        # MPD creation\n        if \'NoFrameskip\' in name:\n            self.env = MaxAndSkip(gym.make(name), history_length, max_pooling)\n        else:\n            self.env = gym.make(name)\n\n        # MDP parameters\n        self.img_size = (width, height)\n        self._episode_ends_at_life = ends_at_life\n        self._max_lives = self.env.unwrapped.ale.lives()\n        self._lives = self._max_lives\n        self._force_fire = None\n        self._real_reset = True\n        self._max_no_op_actions = max_no_op_actions\n        self._history_length = history_length\n        self._current_no_op = None\n\n        assert self.env.unwrapped.get_action_meanings()[0] == \'NOOP\'\n\n        # MDP properties\n        action_space = Discrete(self.env.action_space.n)\n        observation_space = Box(\n            low=0., high=255., shape=(self.img_size[1], self.img_size[0]))\n        horizon = np.inf  # the gym time limit is used.\n        gamma = .99\n        mdp_info = MDPInfo(observation_space, action_space, gamma, horizon)\n\n        super().__init__(mdp_info)\n\n    def reset(self, state=None):\n        if self._real_reset:\n            self._state = self._preprocess_observation(self.env.reset())\n            self._state = deque([deepcopy(\n                self._state) for _ in range(self._history_length)],\n                maxlen=self._history_length\n            )\n            self._lives = self._max_lives\n\n        self._force_fire = self.env.unwrapped.get_action_meanings()[1] == \'FIRE\'\n\n        self._current_no_op = np.random.randint(self._max_no_op_actions + 1)\n\n        return LazyFrames(list(self._state), self._history_length)\n\n    def step(self, action):\n        # Force FIRE action to start episodes in games with lives\n        if self._force_fire:\n            obs, _, _, _ = self.env.env.step(1)\n            self._force_fire = False\n        while self._current_no_op > 0:\n            obs, _, _, _ = self.env.env.step(0)\n            self._current_no_op -= 1\n\n        obs, reward, absorbing, info = self.env.step(action)\n        self._real_reset = absorbing\n        if info[\'ale.lives\'] != self._lives:\n            if self._episode_ends_at_life:\n                absorbing = True\n            self._lives = info[\'ale.lives\']\n            self._force_fire = self.env.unwrapped.get_action_meanings()[\n                1] == \'FIRE\'\n\n        self._state.append(self._preprocess_observation(obs))\n\n        return LazyFrames(list(self._state),\n                          self._history_length), reward, absorbing, info\n\n    def render(self, mode=\'human\'):\n        self.env.render(mode=mode)\n\n    def stop(self):\n        self.env.close()\n        self._real_reset = True\n\n    def set_episode_end(self, ends_at_life):\n        """"""\n        Setter.\n\n        Args:\n            ends_at_life (bool): whether the episode ends when a life is\n                lost or not.\n\n        """"""\n        self._episode_ends_at_life = ends_at_life\n\n    def _preprocess_observation(self, obs):\n        image = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n        image = cv2.resize(image, self.img_size,\n                           interpolation=cv2.INTER_LINEAR)\n\n        return np.array(image, dtype=np.uint8)\n'"
mushroom_rl/environments/car_on_hill.py,0,"b'import numpy as np\nfrom scipy.integrate import odeint\n\nfrom mushroom_rl.environments import Environment, MDPInfo\nfrom mushroom_rl.utils import spaces\nfrom mushroom_rl.utils.viewer import Viewer\n\n\nclass CarOnHill(Environment):\n    """"""\n    The Car On Hill environment as presented in:\n    ""Tree-Based Batch Mode Reinforcement Learning"". Ernst D. et al.. 2005.\n\n    """"""\n    def __init__(self, horizon=100, gamma=.95):\n        """"""\n        Constructor.\n\n        """"""\n        # MDP parameters\n        self.max_pos = 1.\n        self.max_velocity = 3.\n        high = np.array([self.max_pos, self.max_velocity])\n        self._g = 9.81\n        self._m = 1.\n        self._dt = .1\n        self._discrete_actions = [-4., 4.]\n\n        # MDP properties\n        observation_space = spaces.Box(low=-high, high=high)\n        action_space = spaces.Discrete(2)\n        mdp_info = MDPInfo(observation_space, action_space, gamma, horizon)\n\n        # Visualization\n        self._viewer = Viewer(1, 1)\n\n        super().__init__(mdp_info)\n\n    def reset(self, state=None):\n        if state is None:\n            self._state = np.array([-0.5, 0])\n        else:\n            self._state = state\n\n        return self._state\n\n    def step(self, action):\n        action = self._discrete_actions[action[0]]\n        sa = np.append(self._state, action)\n        new_state = odeint(self._dpds, sa, [0, self._dt])\n\n        self._state = new_state[-1, :-1]\n\n        if self._state[0] < -self.max_pos or \\\n                np.abs(self._state[1]) > self.max_velocity:\n            reward = -1\n            absorbing = True\n        elif self._state[0] > self.max_pos and \\\n                np.abs(self._state[1]) <= self.max_velocity:\n            reward = 1\n            absorbing = True\n        else:\n            reward = 0\n            absorbing = False\n\n        return self._state, reward, absorbing, {}\n\n    def render(self):\n        # Slope\n        self._viewer.function(0, 1, self._height)\n\n        # Car\n        car_body = [\n            [-3e-2, 0],\n            [-3e-2, 2e-2],\n            [-2e-2, 2e-2],\n            [-1e-2, 3e-2],\n            [1e-2, 3e-2],\n            [2e-2, 2e-2],\n            [3e-2, 2e-2],\n            [3e-2, 0]\n        ]\n\n        x_car = (self._state[0] + 1) / 2\n        y_car = self._height(x_car)\n        c_car = [x_car, y_car]\n        angle = self._angle(x_car)\n        self._viewer.polygon(c_car, angle, car_body, color=(32, 193, 54))\n\n        self._viewer.display(self._dt)\n\n    @staticmethod\n    def _angle(x):\n        if x < 0.5:\n            m = 4 * x - 1\n        else:\n            m = 1 / ((20 * x**2 - 20 * x + 6) ** 1.5)\n\n        return np.arctan(m)\n\n    @staticmethod\n    def _height(x):\n        y_neg = 4 * x**2 - 2 * x\n        y_pos = (2 * x - 1) / np.sqrt(5 * (2 * x - 1)**2 + 1)\n        y = np.zeros_like(x)\n\n        mask = x < .5\n        neg_mask = np.logical_not(mask)\n        y[mask] = y_neg[mask]\n        y[neg_mask] = y_pos[neg_mask]\n\n        y_norm = (y + 1) / 2\n\n        return y_norm\n\n    def _dpds(self, state_action, t):\n        position = state_action[0]\n        velocity = state_action[1]\n        u = state_action[-1]\n\n        if position < 0.:\n            diff_hill = 2 * position + 1\n            diff_2_hill = 2\n        else:\n            diff_hill = 1 / ((1 + 5 * position ** 2) ** 1.5)\n            diff_2_hill = (-15 * position) / ((1 + 5 * position ** 2) ** 2.5)\n\n        dp = velocity\n        ds = (u - self._g * self._m * diff_hill - velocity ** 2 * self._m *\n              diff_hill * diff_2_hill) / (self._m * (1 + diff_hill ** 2))\n\n        return dp, ds, 0.\n'"
mushroom_rl/environments/cart_pole.py,0,"b'import numpy as np\nfrom scipy.integrate import odeint\n\nfrom mushroom_rl.environments import Environment, MDPInfo\nfrom mushroom_rl.utils import spaces\nfrom mushroom_rl.utils.angles import normalize_angle\nfrom mushroom_rl.utils.viewer import Viewer\n\n\nclass CartPole(Environment):\n    """"""\n    The Inverted Pendulum on a Cart environment as presented in:\n    ""Least-Squares Policy Iteration"". Lagoudakis M. G. and Parr R.. 2003.\n\n    """"""\n    def __init__(self, m=2., M=8., l=.5, g=9.8, mu=1e-2, max_u=50., noise_u=10.,\n                 horizon=3000, gamma=.95):\n        """"""\n        Constructor.\n\n        Args:\n            m (float, 2.0): mass of the pendulum;\n            M (float, 8.0): mass of the cart;\n            l (float, .5): length of the pendulum;\n            g (float, 9.8): gravity acceleration constant;\n            mu (float, 1e-2): friction constant of the pendulum;\n            max_u (float, 50.): maximum allowed input torque;\n            noise_u (float, 10.): maximum noise on the action;\n            horizon (int, 3000): horizon of the problem;\n            gamma (int, .95): discount factor.\n\n        """"""\n        # MDP parameters\n        self._m = m\n        self._M = M\n        self._l = l\n        self._g = g\n        self._alpha = 1 / (self._m + self._M)\n        self._mu = mu\n        self._dt = .1\n        self._max_u = max_u\n        self._noise_u = noise_u\n        high = np.array([np.inf, np.inf])\n\n        # MDP properties\n        observation_space = spaces.Box(low=-high, high=high)\n        action_space = spaces.Discrete(3)\n        mdp_info = MDPInfo(observation_space, action_space, gamma, horizon)\n\n        # Visualization\n        self._viewer = Viewer(2.5 * l, 2.5 * l)\n        self._last_u = None\n        self._state = None\n\n        super().__init__(mdp_info)\n\n    def reset(self, state=None):\n        if state is None:\n            angle = np.random.uniform(-np.pi / 8., np.pi / 8.)\n\n            self._state = np.array([angle, 0.])\n        else:\n            self._state = state\n            self._state[0] = normalize_angle(self._state[0])\n\n        self._last_u = 0\n        return self._state\n\n    def step(self, action):\n        if action == 0:\n            u = -self._max_u\n        elif action == 1:\n            u = 0.\n        else:\n            u = self._max_u\n\n        self._last_u = u\n\n        u += np.random.uniform(-self._noise_u, self._noise_u)\n        new_state = odeint(self._dynamics, self._state, [0, self._dt],\n                           (u,))\n\n        self._state = np.array(new_state[-1])\n        self._state[0] = normalize_angle(self._state[0])\n\n        if np.abs(self._state[0]) > np.pi * .5:\n            reward = -1.\n            absorbing = True\n        else:\n            reward = 0.\n            absorbing = False\n\n        return self._state, reward, absorbing, {}\n\n    def render(self, mode=\'human\'):\n        start = 1.25 * self._l * np.ones(2)\n        end = 1.25 * self._l * np.ones(2)\n\n        end[0] += self._l * np.sin(self._state[0])\n        end[1] += self._l * np.cos(self._state[0])\n\n        self._viewer.line(start, end)\n        self._viewer.square(start, 0,  self._l / 10)\n        self._viewer.circle(end, self._l / 20)\n\n        direction = np.array([1, 0])\n        self._viewer.force_arrow(start, direction, -self._last_u,\n                                 self._max_u, self._l / 5)\n\n        self._viewer.display(self._dt)\n\n    def stop(self):\n        self._viewer.close()\n\n    def _dynamics(self, state, t, u):\n        theta = state[0]\n        omega = state[1]\n\n        d_theta = omega\n        d_omega = (self._g * np.sin(theta) - self._alpha * self._m * self._l *\n                   d_theta ** 2 * np.sin(2 * theta) * .5 - self._alpha * np.cos(\n                    theta) * u) / (4 / 3 * self._l - self._alpha * self._m *\n                                   self._l * np.cos(theta) ** 2)\n\n        return d_theta, d_omega\n'"
mushroom_rl/environments/dm_control_env.py,0,"b'from dm_control import suite\n\nfrom mushroom_rl.environments import Environment, MDPInfo\nfrom mushroom_rl.utils.spaces import *\nfrom mushroom_rl.utils.viewer import ImageViewer\n\n\nclass DMControl(Environment):\n    """"""\n    Interface for dm_control suite Mujoco environments. It makes it possible to\n    use every dm_control suite Mujoco environment just providing the necessary\n    information.\n\n    """"""\n    def __init__(self, domain_name, task_name, horizon, gamma, task_kwargs=None,\n                 dt=.01, width_screen=480, height_screen=480, camera_id=0):\n        """"""\n        Constructor.\n\n        Args:\n             domain_name (str): name of the environment;\n             task_name (str): name of the task of the environment;\n             horizon (int): the horizon;\n             gamma (float): the discount factor;\n             task_kwargs (dict, None): parameters of the task;\n             dt (float, .01): duration of a control step;\n             width_screen (int, 480): width of the screen;\n             height_screen (int, 480): height of the screen;\n             camera_id (int, 0): position of camera to render the environment;\n\n        """"""\n        # MDP creation\n        if task_kwargs is None:\n            task_kwargs = dict()\n        task_kwargs[\'time_limit\'] = np.inf  # Hack to ignore dm_control time limit.\n\n        self.env = suite.load(domain_name, task_name, task_kwargs=task_kwargs)\n\n        # MDP properties\n        action_space = self._convert_action_space(self.env.action_spec())\n        observation_space = self._convert_observation_space(self.env.observation_spec())\n        mdp_info = MDPInfo(observation_space, action_space, gamma, horizon)\n\n        self._viewer = ImageViewer((width_screen, height_screen), dt)\n        self._camera_id = camera_id\n\n        super().__init__(mdp_info)\n\n    def reset(self, state=None):\n        if state is None:\n            self._state = self._convert_observation(self.env.reset().observation)\n        else:\n            raise NotImplementedError\n\n        return self._state\n\n    def step(self, action):\n        step = self.env.step(action)\n\n        reward = step.reward\n        self._state = self._convert_observation(step.observation)\n        absorbing = step.last()\n\n        return self._state, reward, absorbing, {}\n\n    def render(self):\n        img = self.env.physics.render(self._viewer.size[1],\n                                      self._viewer.size[0],\n                                      self._camera_id)\n        self._viewer.display(img)\n\n    def stop(self):\n        pass\n\n    @staticmethod\n    def _convert_observation_space(observation_space):\n        observation_shape = 0\n        for i in observation_space:\n            shape = observation_space[i].shape\n            if len(shape) > 0:\n                observation_shape += shape[0]\n            else:\n                observation_shape += 1\n\n        return Box(low=-np.inf, high=np.inf, shape=(observation_shape,))\n\n    @staticmethod\n    def _convert_action_space(action_space):\n        low = action_space.minimum\n        high = action_space.maximum\n\n        return Box(low=np.array(low), high=np.array(high))\n\n    @staticmethod\n    def _convert_observation(observation):\n        obs = list()\n        for i in observation:\n            obs.append(np.atleast_1d(observation[i]))\n\n        return np.concatenate(obs)\n'"
mushroom_rl/environments/environment.py,0,"b'import numpy as np\n\n\nclass MDPInfo:\n    """"""\n    This class is used to store the information of the environment.\n\n    """"""\n    def __init__(self, observation_space, action_space, gamma, horizon):\n        """"""\n        Constructor.\n\n        Args:\n             observation_space ([Box, Discrete]): the state space;\n             action_space ([Box, Discrete]): the action space;\n             gamma (float): the discount factor;\n             horizon (int): the horizon.\n\n        """"""\n        self.observation_space = observation_space\n        self.action_space = action_space\n        self.gamma = gamma\n        self.horizon = horizon\n\n    @property\n    def size(self):\n        """"""\n        Returns:\n            The sum of the number of discrete states and discrete actions. Only\n            works for discrete spaces.\n\n        """"""\n        return self.observation_space.size + self.action_space.size\n\n    @property\n    def shape(self):\n        """"""\n        Returns:\n            The concatenation of the shape tuple of the state and action\n            spaces.\n\n        """"""\n        return self.observation_space.shape + self.action_space.shape\n\n\nclass Environment(object):\n    """"""\n    Basic interface used by any mushroom environment.\n\n    """"""\n    def __init__(self, mdp_info):\n        """"""\n        Constructor.\n\n        Args:\n             mdp_info (MDPInfo): an object containing the info of the\n                environment.\n\n        """"""\n        self._mdp_info = mdp_info\n\n    def seed(self, seed):\n        """"""\n        Set the seed of the environment.\n\n        Args:\n            seed (float): the value of the seed.\n\n        """"""\n        if hasattr(self, \'env\'):\n            self.env.seed(seed)\n        else:\n            raise NotImplementedError\n\n    def reset(self, state=None):\n        """"""\n        Reset the current state.\n\n        Args:\n            state (np.ndarray, None): the state to set to the current state.\n\n        Returns:\n            The current state.\n\n        """"""\n        raise NotImplementedError\n\n    def step(self, action):\n        """"""\n        Move the agent from its current state according to the action.\n\n        Args:\n            action (np.ndarray): the action to execute.\n\n        Returns:\n            The state reached by the agent executing ``action`` in its current\n            state, the reward obtained in the transition and a flag to signal\n            if the next state is absorbing. Also an additional dictionary is\n            returned (possibly empty).\n\n        """"""\n        raise NotImplementedError\n\n    def render(self):\n        raise NotImplementedError\n\n    def stop(self):\n        """"""\n        Method used to stop an mdp. Useful when dealing with real world\n        environments, simulators, or when using openai-gym rendering\n\n        """"""\n        pass\n\n    @property\n    def info(self):\n        """"""\n        Returns:\n             An object containing the info of the environment.\n\n        """"""\n        return self._mdp_info\n\n    @staticmethod\n    def _bound(x, min_value, max_value):\n        """"""\n        Method used to bound state and action variables.\n\n        Args:\n            x: the variable to bound;\n            min_value: the minimum value;\n            max_value: the maximum value;\n\n        Returns:\n            The bounded variable.\n\n        """"""\n        return np.maximum(min_value, np.minimum(x, max_value))\n'"
mushroom_rl/environments/finite_mdp.py,0,"b'import numpy as np\n\nfrom .environment import Environment, MDPInfo\nfrom mushroom_rl.utils import spaces\n\n\nclass FiniteMDP(Environment):\n    """"""\n    Finite Markov Decision Process.\n\n    """"""\n    def __init__(self, p, rew, mu=None, gamma=.9, horizon=np.inf):\n        """"""\n        Constructor.\n\n        Args:\n            p (np.ndarray): transition probability matrix;\n            rew (np.ndarray): reward matrix;\n            mu (np.ndarray, None): initial state probability distribution;\n            gamma (float, .9): discount factor;\n            horizon (int, np.inf): the horizon.\n\n        """"""\n        assert p.shape == rew.shape\n        assert mu is None or p.shape[0] == mu.size\n\n        # MDP parameters\n        self.p = p\n        self.r = rew\n        self.mu = mu\n\n        # MDP properties\n        observation_space = spaces.Discrete(p.shape[0])\n        action_space = spaces.Discrete(p.shape[1])\n        horizon = horizon\n        gamma = gamma\n        mdp_info = MDPInfo(observation_space, action_space, gamma, horizon)\n\n        super().__init__(mdp_info)\n\n    def reset(self, state=None):\n        if state is None:\n            if self.mu is not None:\n                self._state = np.array(\n                    [np.random.choice(self.mu.size, p=self.mu)])\n            else:\n                self._state = np.array([np.random.choice(self.p.shape[0])])\n        else:\n            self._state = state\n\n        return self._state\n\n    def step(self, action):\n        p = self.p[self._state[0], action[0], :]\n        next_state = np.array([np.random.choice(p.size, p=p)])\n        absorbing = not np.any(self.p[next_state[0]])\n        reward = self.r[self._state[0], action[0], next_state[0]]\n\n        self._state = next_state\n\n        return self._state, reward, absorbing, {}\n'"
mushroom_rl/environments/grid_world.py,0,"b'import numpy as np\n\nfrom mushroom_rl.environments import Environment, MDPInfo\nfrom mushroom_rl.utils import spaces\nfrom mushroom_rl.utils.viewer import Viewer\n\n\nclass AbstractGridWorld(Environment):\n    """"""\n    Abstract class to build a grid world.\n\n    """"""\n    def __init__(self, mdp_info, height, width, start, goal):\n        """"""\n        Constructor.\n\n        Args:\n            height (int): height of the grid;\n            width (int): width of the grid;\n            start (tuple): x-y coordinates of the goal;\n            goal (tuple): x-y coordinates of the goal.\n\n        """"""\n        assert not np.array_equal(start, goal)\n\n        assert goal[0] < height and goal[1] < width,\\\n            \'Goal position not suitable for the grid world dimension.\'\n\n        self._state = None\n        self._height = height\n        self._width = width\n        self._start = start\n        self._goal = goal\n\n        # Visualization\n        self._viewer = Viewer(self._width, self._height, 500,\n                              self._height * 500 // self._width)\n\n        super().__init__(mdp_info)\n\n    def reset(self, state=None):\n        if state is None:\n            state = self.convert_to_int(self._start, self._width)\n\n        self._state = state\n\n        return self._state\n\n    def step(self, action):\n        state = self.convert_to_grid(self._state, self._width)\n\n        new_state, reward, absorbing, info = self._step(state, action)\n        self._state = self.convert_to_int(new_state, self._width)\n\n        return self._state, reward, absorbing, info\n\n    def render(self):\n        for row in range(1, self._height):\n            for col in range(1, self._width):\n                self._viewer.line(np.array([col, 0]),\n                                  np.array([col, self._height]))\n                self._viewer.line(np.array([0, row]),\n                                  np.array([self._width, row]))\n\n        goal_center = np.array([.5 + self._goal[1],\n                                self._height - (.5 + self._goal[0])])\n        self._viewer.square(goal_center, 0, 1, (0, 255, 0))\n\n        start_grid = self.convert_to_grid(self._start, self._width)\n        start_center = np.array([.5 + start_grid[1],\n                                 self._height - (.5 + start_grid[0])])\n        self._viewer.square(start_center, 0, 1, (255, 0, 0))\n\n        state_grid = self.convert_to_grid(self._state, self._width)\n        state_center = np.array([.5 + state_grid[1],\n                                 self._height - (.5 + state_grid[0])])\n        self._viewer.circle(state_center, .4, (0, 0, 255))\n\n        self._viewer.display(.1)\n\n    def _step(self, state, action):\n        raise NotImplementedError(\'AbstractGridWorld is an abstract class.\')\n\n    def _grid_step(self, state, action):\n        if action == 0:\n            if state[0] > 0:\n                state[0] -= 1\n        elif action == 1:\n            if state[0] + 1 < self._height:\n                state[0] += 1\n        elif action == 2:\n            if state[1] > 0:\n                state[1] -= 1\n        elif action == 3:\n            if state[1] + 1 < self._width:\n                state[1] += 1\n\n    @staticmethod\n    def convert_to_grid(state, width):\n        return np.array([state[0] // width, state[0] % width])\n\n    @staticmethod\n    def convert_to_int(state, width):\n        return np.array([state[0] * width + state[1]])\n\n\nclass GridWorld(AbstractGridWorld):\n    """"""\n    Standard grid world.\n\n    """"""\n    def __init__(self, height, width, goal, start=(0, 0)):\n        # MDP properties\n        observation_space = spaces.Discrete(height * width)\n        action_space = spaces.Discrete(4)\n        horizon = 100\n        gamma = .9\n        mdp_info = MDPInfo(observation_space, action_space, gamma, horizon)\n\n        super().__init__(mdp_info, height, width, start, goal)\n\n    def _step(self, state, action):\n        self._grid_step(state, action)\n\n        if np.array_equal(state, self._goal):\n            reward = 10\n            absorbing = True\n        else:\n            reward = 0\n            absorbing = False\n\n        return state, reward, absorbing, {}\n\n\nclass GridWorldVanHasselt(AbstractGridWorld):\n    """"""\n    A variant of the grid world as presented in:\n    ""Double Q-Learning"". Hasselt H. V.. 2010.\n\n    """"""\n    def __init__(self, height=3, width=3, goal=(0, 2), start=(2, 0)):\n        # MDP properties\n        observation_space = spaces.Discrete(height * width)\n        action_space = spaces.Discrete(4)\n        horizon = np.inf\n        gamma = .95\n        mdp_info = MDPInfo(observation_space, action_space, gamma, horizon)\n\n        super().__init__(mdp_info, height, width, start, goal)\n\n    def _step(self, state, action):\n        if np.array_equal(state, self._goal):\n            reward = 5\n            absorbing = True\n        else:\n            self._grid_step(state, action)\n\n            reward = np.random.choice([-12, 10])\n            absorbing = False\n\n        return state, reward, absorbing, {}\n'"
mushroom_rl/environments/gym_env.py,0,"b'import gym\n\ntry:\n    import pybullet_envs\n    import time\nexcept ImportError:\n    pass\n\nfrom gym import spaces as gym_spaces\nfrom mushroom_rl.environments import Environment, MDPInfo\nfrom mushroom_rl.utils.spaces import *\n\n\nclass Gym(Environment):\n    """"""\n    Interface for OpenAI Gym environments. It makes it possible to use every\n    Gym environment just providing the id, except for the Atari games that\n    are managed in a separate class.\n\n    """"""\n    def __init__(self, name, horizon, gamma):\n        """"""\n        Constructor.\n\n        Args:\n             name (str): gym id of the environment;\n             horizon (int): the horizon;\n             gamma (float): the discount factor.\n\n        """"""\n        # MDP creation\n        self._close_at_stop = True\n        if \'- \' + name in pybullet_envs.getList():\n            import pybullet\n            pybullet.connect(pybullet.DIRECT)\n            self._close_at_stop = False\n\n        self.env = gym.make(name)\n\n        self.env._max_episode_steps = np.inf  # Hack to ignore gym time limit.\n\n        # MDP properties\n        assert not isinstance(self.env.observation_space,\n                              gym_spaces.MultiDiscrete)\n        assert not isinstance(self.env.action_space, gym_spaces.MultiDiscrete)\n\n        action_space = self._convert_gym_space(self.env.action_space)\n        observation_space = self._convert_gym_space(self.env.observation_space)\n        mdp_info = MDPInfo(observation_space, action_space, gamma, horizon)\n\n        if isinstance(action_space, Discrete):\n            self._convert_action = lambda a: a[0]\n        else:\n            self._convert_action = lambda a: a\n\n        super().__init__(mdp_info)\n\n    def reset(self, state=None):\n        if state is None:\n            return self.env.reset()\n        else:\n            self.env.reset()\n            self.env.state = state\n\n            return state\n\n    def step(self, action):\n        action = self._convert_action(action)\n\n        return self.env.step(action)\n\n    def render(self, mode=\'human\'):\n        self.env.render(mode=mode)\n\n    def stop(self):\n        try:\n            if self._close_at_stop:\n                self.env.close()\n        except:\n            pass\n\n    @staticmethod\n    def _convert_gym_space(space):\n        if isinstance(space, gym_spaces.Discrete):\n            return Discrete(space.n)\n        elif isinstance(space, gym_spaces.Box):\n            return Box(low=space.low, high=space.high, shape=space.shape)\n        else:\n            raise ValueError\n'"
mushroom_rl/environments/inverted_pendulum.py,0,"b'import numpy as np\nfrom scipy.integrate import odeint\n\nfrom mushroom_rl.environments import Environment, MDPInfo\nfrom mushroom_rl.utils import spaces\nfrom mushroom_rl.utils.angles import normalize_angle\nfrom mushroom_rl.utils.viewer import Viewer\n\n\nclass InvertedPendulum(Environment):\n    """"""\n    The Inverted Pendulum environment (continuous version) as presented in:\n    ""Reinforcement Learning In Continuous Time and Space"". Doya K.. 2000.\n    ""Off-Policy Actor-Critic"". Degris T. et al.. 2012.\n    ""Deterministic Policy Gradient Algorithms"". Silver D. et al. 2014.\n\n    """"""\n    def __init__(self, random_start=False, m=1., l=1., g=9.8, mu=1e-2,\n                 max_u=5., horizon=5000, gamma=.99):\n        """"""\n        Constructor.\n\n        Args:\n            random_start (bool, False): whether to start from a random position\n                or from the horizontal one;\n            m (float, 1.0): mass of the pendulum;\n            l (float, 1.0): length of the pendulum;\n            g (float, 9.8): gravity acceleration constant;\n            mu (float, 1e-2): friction constant of the pendulum;\n            max_u (float, 5.0): maximum allowed input torque;\n            horizon (int, 5000): horizon of the problem;\n            gamma (int, .99): discount factor.\n\n        """"""\n        # MDP parameters\n        self._m = m\n        self._l = l\n        self._g = g\n        self._mu = mu\n        self._random = random_start\n        self._dt = .01\n        self._max_u = max_u\n        self._max_omega = 5 / 2 * np.pi\n        high = np.array([np.pi, self._max_omega])\n\n        # MDP properties\n        observation_space = spaces.Box(low=-high, high=high)\n        action_space = spaces.Box(low=np.array([-max_u]),\n                                  high=np.array([max_u]))\n        mdp_info = MDPInfo(observation_space, action_space, gamma, horizon)\n\n        # Visualization\n        self._viewer = Viewer(2.5 * l, 2.5 * l)\n        self._last_u = None\n\n        super().__init__(mdp_info)\n\n    def reset(self, state=None):\n        if state is None:\n            if self._random:\n                angle = np.random.uniform(-np.pi, np.pi)\n            else:\n                angle = np.pi / 2\n\n            self._state = np.array([angle, 0.])\n        else:\n            self._state = state\n            self._state[0] = normalize_angle(self._state[0])\n            self._state[1] = self._bound(self._state[1], -self._max_omega,\n                                         self._max_omega)\n\n        self._last_u = 0.0\n        return self._state\n\n    def step(self, action):\n        u = self._bound(action[0], -self._max_u, self._max_u)\n        new_state = odeint(self._dynamics, self._state, [0, self._dt],\n                           (u,))\n\n        self._state = np.array(new_state[-1])\n        self._state[0] = normalize_angle(self._state[0])\n        self._state[1] = self._bound(self._state[1], -self._max_omega,\n                                     self._max_omega)\n\n        reward = np.cos(self._state[0])\n\n        self._last_u = u.item()\n\n        return self._state, reward, False, {}\n\n    def render(self, mode=\'human\'):\n        start = 1.25 * self._l * np.ones(2)\n        end = 1.25 * self._l * np.ones(2)\n\n        end[0] += self._l * np.sin(self._state[0])\n        end[1] += self._l * np.cos(self._state[0])\n\n        self._viewer.line(start, end)\n        self._viewer.circle(start, self._l / 40)\n        self._viewer.circle(end, self._l / 20)\n        self._viewer.torque_arrow(start, -self._last_u, self._max_u,\n                                  self._l / 5)\n\n        self._viewer.display(self._dt)\n\n    def stop(self):\n        self._viewer.close()\n\n    def _dynamics(self, state, t, u):\n        theta = state[0]\n        omega = self._bound(state[1], -self._max_omega, self._max_omega)\n\n        d_theta = omega\n        d_omega = (-self._mu * omega + self._m * self._g * self._l * np.sin(\n            theta) + u) / (self._m * self._l**2)\n\n        return d_theta, d_omega\n'"
mushroom_rl/environments/lqr.py,0,"b'import numpy as np\n\nfrom mushroom_rl.environments import Environment, MDPInfo\nfrom mushroom_rl.utils import spaces\n\n\nclass LQR(Environment):\n    """"""\n    This class implements a Linear-Quadratic Regulator.\n    This task aims to minimize the undesired deviations from nominal values of\n    some controller settings in control problems.\n    The system equations in this task are:\n\n    .. math::\n        x_{t+1} = Ax_t + Bu_t\n\n    where x is the state and u is the control signal.\n\n    The reward function is given by:\n\n    .. math::\n        r_t = -\\\\left( x_t^TQx_t + u_t^TRu_t \\\\right)\n\n    ""Policy gradient approaches for multi-objective sequential decision making"".\n    Parisi S., Pirotta M., Smacchia N., Bascetta L., Restelli M.. 2014\n\n    """"""\n    def __init__(self, A, B, Q, R, max_pos=np.inf, max_action=np.inf,\n                 random_init=False, episodic=False, gamma=0.9, horizon=50):\n        """"""\n        Constructor.\n\n            Args:\n                A (np.ndarray): the state dynamics matrix;\n                B (np.ndarray): the action dynamics matrix;\n                Q (np.ndarray): reward weight matrix for state;\n                R (np.ndarray): reward weight matrix for action;\n                max_pos (float, np.inf): maximum value of the state;\n                max_action (float, np.inf): maximum value of the action;\n                random_init (bool, False): start from a random state;\n                episodic (bool, False): end the episode when the state goes over\n                the threshold;\n                gamma (float, 0.9): discount factor;\n                horizon (int, 50): horizon of the mdp.\n\n        """"""\n        self.A = A\n        self.B = B\n        self.Q = Q\n        self.R = R\n        self._max_pos = max_pos\n        self._max_action = max_action\n        self._episodic = episodic\n        self.random_init = random_init\n\n        # MDP properties\n        high_x = self._max_pos * np.ones(A.shape[0])\n        low_x = -high_x\n\n        high_u = self._max_action * np.ones(B.shape[1])\n        low_u = -high_u\n\n        observation_space = spaces.Box(low=low_x, high=high_x)\n        action_space = spaces.Box(low=low_u, high=high_u)\n        mdp_info = MDPInfo(observation_space, action_space, gamma, horizon)\n\n        super().__init__(mdp_info)\n\n    @staticmethod\n    def generate(dimensions, max_pos=np.inf, max_action=np.inf, eps=.1,\n                 index=0, random_init=False, episodic=False, gamma=.9,\n                 horizon=50):\n        """"""\n        Factory method that generates an lqr with identity dynamics and\n        symmetric reward matrices.\n\n        Args:\n            dimensions (int): number of state-action dimensions;\n            max_pos (float, np.inf): maximum value of the state;\n            max_action (float, np.inf): maximum value of the action;\n            eps (double, .1): reward matrix weights specifier;\n            index (int, 0): selector for the principal state;\n            random_init (bool, False): start from a random state;\n            episodic (bool, False): end the episode when the state goes over the\n                threshold;\n            gamma (float, .9): discount factor;\n            horizon (int, 50): horizon of the mdp.\n\n        """"""\n        assert dimensions >= 1\n\n        A = np.eye(dimensions)\n        B = np.eye(dimensions)\n        Q = eps * np.eye(dimensions)\n        R = (1. - eps) * np.eye(dimensions)\n\n        Q[index, index] = 1. - eps\n        R[index, index] = eps\n\n        return LQR(A, B, Q, R, max_pos, max_action, random_init, episodic,\n                   gamma, horizon)\n\n    def reset(self, state=None):\n        if state is None:\n            if self.random_init:\n                self._state = self._bound(\n                    np.random.uniform(-3, 3, size=self.A.shape[0]),\n                    self.info.observation_space.low,\n                    self.info.observation_space.high\n                )\n            else:\n                init_value = .9 * self._max_pos if np.isfinite(\n                    self._max_pos) else 10\n                self._state = init_value * np.ones(self.A.shape[0])\n        else:\n            self._state = state\n\n        return self._state\n\n    def step(self, action):\n        x = self._state\n        u = self._bound(action, self.info.action_space.low,\n                        self.info.action_space.high)\n\n        reward = -(x.dot(self.Q).dot(x) + u.dot(self.R).dot(u))\n        self._state = self.A.dot(x) + self.B.dot(u)\n\n        if np.any(np.abs(self._state) > self._max_pos):\n            if self._episodic:\n                reward = -self._max_pos ** 2 * 10\n                absorbing = True\n            else:\n                self._state = self._bound(self._state,\n                                          self.info.observation_space.low,\n                                          self.info.observation_space.high)\n        else:\n            absorbing = False\n\n        return self._state, reward, absorbing, {}\n'"
mushroom_rl/environments/mujoco.py,0,"b'import mujoco_py\nfrom mujoco_py import functions as mj_fun\nimport numpy as np\nfrom enum import Enum\nfrom mushroom_rl.environments import Environment, MDPInfo\nfrom mushroom_rl.utils.spaces import Box\nimport glfw\n\nclass ObservationType(Enum):\n    """"""\n    An enum indicating the type of data that should be added to the observation\n    of the environment, can be Joint-/Body-/Site- positions and velocities.\n\n    """"""\n    __order__ = ""BODY_POS BODY_VEL JOINT_POS JOINT_VEL SITE_POS SITE_VEL""\n    BODY_POS = 0\n    BODY_VEL = 1\n    JOINT_POS = 2\n    JOINT_VEL = 3\n    SITE_POS = 4\n    SITE_VEL = 5\n\n\nclass MuJoCo(Environment):\n    """"""\n    Class to create a Mushroom environment using the MuJoCo simulator.\n\n    """"""\n    def __init__(self, file_name, actuation_spec, observation_spec, gamma,\n                 horizon, n_substeps=1, n_intermediate_steps=1, additional_data_spec=None,\n                 collision_groups=None):\n        """"""\n        Constructor.\n\n        Args:\n            file_name (string): The path to the XML file with which the\n                environment should be created;\n            actuation_spec (list): A list specifying the names of the joints\n                which should be controllable by the agent. Can be left empty\n                when all actuators should be used;\n            observation_spec (list): A list containing the names of data that\n                should be made available to the agent as an observation and\n                their type (ObservationType). An entry in the list is given by:\n                (name, type);\n            gamma (float): The discounting factor of the environment;\n            horizon (int): The maximum horizon for the environment;\n            n_substeps (int): The number of substeps to use by the MuJoCo\n                simulator. An action given by the agent will be applied for\n                n_substeps before the agent receives the next observation and\n                can act accordingly;\n            n_intermediate_steps (int): The number of steps between every action\n                taken by the agent. Similar to n_substeps but allows the user\n                to modify, control and access intermediate states.\n            additional_data_spec (list): A list containing the data fields of\n                interest, which should be read from or written to during\n                simulation. The entries are given as the following tuples:\n                (key, name, type) key is a string for later referencing in the\n                ""read_data"" and ""write_data"" methods. The name is the name of\n                the object in the XML specification and the type is the\n                ObservationType;\n            collision_groups (list): A list containing groups of geoms for\n                which collisions should be checked during simulation via\n                ``check_collision``. The entries are given as:\n                ``(key, geom_names)``, where key is a string for later\n                referencing in the ""check_collision"" method, and geom_names is\n                a list of geom names in the XML specification.\n        """"""\n        # Create the simulation\n        self.sim = mujoco_py.MjSim(mujoco_py.load_model_from_path(file_name),\n                                   nsubsteps=n_substeps)\n\n        self.n_intermediate_steps = n_intermediate_steps\n        self.viewer = None\n        self._state = None\n\n        # Read the actuation spec and build the mapping between actions and ids\n        # as well as their limits\n        if len(actuation_spec) == 0:\n            self.action_indices = [i for i in range(0, len(self.sim.model._actuator_name2id))]\n        else:\n            self.action_indices = []\n            for name in actuation_spec:\n                self.action_indices.append(self.sim.model._actuator_name2id[name])\n\n        low = []\n        high = []\n        for index in self.action_indices:\n            if self.sim.model.actuator_ctrllimited[index]:\n                low.append(self.sim.model.actuator_ctrlrange[index][0])\n                high.append(self.sim.model.actuator_ctrlrange[index][1])\n            else:\n                low.append(-np.inf)\n                high.append(np.inf)\n        action_space = Box(np.array(low), np.array(high))\n\n        # Read the observation spec to build a mapping at every step. It is\n        # ensured that the values appear in the order they are specified.\n        if len(observation_spec) == 0:\n            raise AttributeError(""No Environment observations were specified. ""\n                                 ""Add at least one observation to the observation_spec."")\n        else:\n            self.observation_map = observation_spec\n\n        # We can only specify limits for the joint positions, all other\n        # information can be potentially unbounded\n        low = []\n        high = []\n        for name, ot in self.observation_map:\n            obs_count = len(self._observation_map(name, ot))\n            if obs_count == 1 and ot == ObservationType.JOINT_POS:\n                joint_range = self.sim.model.jnt_range[self.sim.model._joint_name2id[name]]\n                if not(joint_range[0] == joint_range[1] == 0.0):\n                    low.append(joint_range[0])\n                    high.append(joint_range[1])\n                else:\n                    low.extend(-np.inf)\n                    high.extend(np.inf)\n            else:\n                low.extend([-np.inf] * obs_count)\n                high.extend([np.inf] * obs_count)\n        observation_space = Box(np.array(low), np.array(high))\n\n        # Pre-process the additional data to allow easier writing and reading\n        # to and from arrays in MuJoCo\n        self.additional_data = {}\n        for key, name, ot in additional_data_spec:\n            self.additional_data[key] = (name, ot)\n\n        # Pre-process the collision groups for ""fast"" detection of contacts\n        self.collision_groups = {}\n        if self.collision_groups is not None:\n            for name, geom_names in collision_groups:\n                self.collision_groups[name] = {self.sim.model._geom_name2id[geom_name]\n                                               for geom_name in geom_names}\n\n        # Finally, we create the MDP information and call the constructor of\n        # the parent class\n        mdp_info = MDPInfo(observation_space, action_space, gamma, horizon)\n        super().__init__(mdp_info)\n\n    def seed(self, seed):\n        np.random.seed(seed)\n\n    def reset(self, state=None):\n        mj_fun.mj_resetData(self.sim.model, self.sim.data)\n        self.setup()\n\n        self._state = self._create_observation()\n        return self._state\n\n    def render(self):\n        if self.viewer is None:\n            self.viewer = mujoco_py.MjViewer(self.sim)\n\n        self.viewer.render()\n\n    def stop(self):\n        if self.viewer is not None:\n            v = self.viewer\n            self.viewer = None\n            glfw.destroy_window(v.window)\n            del v\n\n    def _observation_map(self, name, otype):\n        if otype == ObservationType.BODY_POS:\n            data = self.sim.data.get_body_xpos(name)\n        elif otype == ObservationType.BODY_VEL:\n            data = self.sim.data.get_body_xvelp(name)\n        elif otype == ObservationType.JOINT_POS:\n            data = self.sim.data.get_joint_qpos(name)\n        elif otype == ObservationType.JOINT_VEL:\n            data = self.sim.data.get_joint_qvel(name)\n        elif otype == ObservationType.SITE_POS:\n            data = self.sim.data.get_site_xpos(name)\n        else:\n            data = self.sim.data.get_site_xvelp(name)\n\n        if hasattr(data, ""__len__""):\n            return data\n        else:\n            return [data]\n\n    def _create_observation(self):\n        data_obs = [self._observation_map(name, ot)\n                    for name, ot in self.observation_map]\n        return np.concatenate(data_obs)\n\n    def step(self, action):\n        cur_obs = self._state\n\n        action = self._preprocess_action(action)\n\n        self._step_init(cur_obs, action)\n\n        for i in range(self.n_intermediate_steps):\n\n            ctrl_action = self._compute_action(action)\n            self.sim.data.ctrl[self.action_indices] = ctrl_action\n\n            self._simulation_pre_step()\n\n            self.sim.step()\n\n            self._simulation_post_step()\n\n        self._state = self._create_observation()\n\n        self._step_finalize()\n\n        reward = self.reward(cur_obs, action, self._state)\n\n        return self._state, reward, self.is_absorbing(self._state), {}\n\n    def _preprocess_action(self, action):\n        """"""\n        Compute a transformation of the action provided to the\n        environment.\n\n        Args:\n            action (np.ndarray): numpy array with the actions\n                provided to the environment.\n\n        Returns:\n            The action to be used for the current step\n        """"""\n        return action\n\n    def _step_init(self, state, action):\n        """"""\n        Allows information to be initialized at the start of a step.\n        """"""\n        pass\n\n    def _compute_action(self, action):\n        """"""\n        Compute a transformation of the action at every intermediate step.\n        Useful to add control signals simulated directly in python.\n\n        Args:\n            action (np.ndarray): numpy array with the actions\n                provided at every step.\n\n        Returns:\n            The action to be set in the actual mujoco simulation.\n\n        """"""\n        return action\n\n    def _simulation_pre_step(self):\n        """"""\n        Allows information to be accesed and changed at every intermediate step\n            before taking a step in the mujoco simulation.\n            Can be usefull to apply an external force/torque to the specified bodies.\n\n        ex: apply a force over X to the torso:\n            force = [200, 0, 0]\n            torque = [0, 0, 0]\n            self.sim.data.xfrc_applied[self.sim.model._body_name2id[""torso""],:] = force + torque\n        """"""\n        pass\n\n    def _simulation_post_step(self):\n        """"""\n        Allows information to be accesed at every intermediate step\n            after taking a step in the mujoco simulation.\n            Can be usefull to average forces over all intermediate steps.\n        """"""\n        pass\n\n    def _step_finalize(self):\n        """"""\n        Allows information to be accesed at the end of a step.\n        """"""\n        pass\n\n    def read_data(self, name):\n        """"""\n        Read data form the MuJoCo data structure.\n\n        Args:\n            name (string): A name referring to an entry contained the\n                additional_data_spec list handed to the constructor.\n\n        Returns:\n            The desired data as a one-dimensional numpy array.\n\n        """"""\n        data_id, otype = self.additional_data[name]\n        return np.array(self._observation_map(data_id, otype))\n\n    def write_data(self, name, value):\n        """"""\n        Write data to the MuJoCo data structure.\n\n        Args:\n            name (string): A name referring to an entry contained in the\n                additional_data_spec list handed to the constructor;\n            value (ndarray): The data that should be written.\n\n        """"""\n\n        data_id, otype = self.additional_data[name]\n\n        if otype == ObservationType.JOINT_POS:\n            self.sim.data.set_joint_qpos(data_id, value)\n        elif otype == ObservationType.JOINT_VEL:\n            self.sim.data.set_joint_qvel(data_id, value)\n        else:\n            data_buffer = self._observation_map(data_id, otype)\n            data_buffer[:] = value\n\n    def check_collision(self, group1, group2):\n        """"""\n        Check for collision between the specified groups.\n\n        Args:\n            group1 (string): A name referring to an entry contained in the\n                collision_groups list handed to the constructor;\n            group2 (string): A name referring to an entry contained in the\n                collision_groups list handed to the constructor.\n\n        Returns:\n            A flag indicating whether a collision occurred between the given\n            groups or not.\n\n        """"""\n        ids1 = self.collision_groups[group1]\n        ids2 = self.collision_groups[group2]\n\n        for coni in range(0, self.sim.data.ncon):\n            con = self.sim.data.contact[coni]\n\n            collision = con.geom1 in ids1 and con.geom2 in ids2\n            collision_trans = con.geom1 in ids2 and con.geom2 in ids1\n\n            if collision or collision_trans:\n                return True\n        return False\n\n    def get_collision_force(self, group1, group2):\n        """"""\n        Returns the collision force and torques between the specified groups.\n\n        Args:\n            group1 (string): A name referring to an entry contained in the\n                collision_groups list handed to the constructor;\n            group2 (string): A name referring to an entry contained in the\n                collision_groups list handed to the constructor.\n\n        Returns:\n            A 6D vector specifying the collision forces/torques[3D force + 3D torque]\n            between the given groups. Vector of 0\'s in case there was no collision.\n            http://mujoco.org/book/programming.html#siContact\n\n        """"""\n        ids1 = self.collision_groups[group1]\n        ids2 = self.collision_groups[group2]\n\n        c_array = np.zeros(6, dtype=np.float64)\n        for con_i in range(0, self.sim.data.ncon):\n            con = self.sim.data.contact[con_i]\n\n            if (con.geom1 in ids1 and con.geom2 in ids2 or\n               con.geom1 in ids2 and con.geom2 in ids1):\n\n                mujoco_py.functions.mj_contactForce(self.sim.model, self.sim.data,\n                                                    con_i, c_array)\n                return c_array\n\n        return c_array\n\n    def reward(self, state, action, next_state):\n        """"""\n        Compute the reward based on the given transition.\n\n        Args:\n            state (np.array): the current state of the system;\n            action (np.array): the action that is applied in the current state;\n            next_state (np.array): the state reached after applying the given\n                action.\n\n        Returns:\n            The reward as a floating point scalar value.\n\n        """"""\n        raise NotImplementedError\n\n    def is_absorbing(self, state):\n        """"""\n        Check whether the given state is an absorbing state or not.\n\n        Args:\n            state (np.array): the state of the system.\n\n        Returns:\n            A boolean flag indicating whether this state is absorbing or not.\n\n        """"""\n        raise NotImplementedError\n\n    def setup(self):\n        """"""\n        A function that allows to execute setup code after an environment\n        reset.\n\n        """"""\n        pass\n'"
mushroom_rl/environments/puddle_world.py,0,"b'import numpy as np\nfrom scipy.stats import norm\n\nfrom mushroom_rl.environments import Environment, MDPInfo\nfrom mushroom_rl.utils.spaces import Discrete, Box\nfrom mushroom_rl.utils.viewer import Viewer\n\n\nclass PuddleWorld(Environment):\n    """"""\n    Puddle world as presented in:\n    ""Off-Policy Actor-Critic"". Degris T. et al.. 2012.\n\n    """"""\n    def __init__(self, start=None, goal=None, goal_threshold=.1, noise_step=.025,\n                 noise_reward=0, reward_goal=0., thrust=.05, puddle_center=None,\n                 puddle_width=None, gamma=.99, horizon=5000):\n        """"""\n        Constructor.\n\n        Args:\n            start (np.array, None): starting position of the agent;\n            goal (np.array, None): goal position;\n            goal_threshold (float, .1): distance threshold of the agent from the\n                goal to consider it reached;\n            noise_step (float, .025): noise in actions;\n            noise_reward (float, 0): standard deviation of gaussian noise in reward;\n            reward_goal (float, 0): reward obtained reaching goal state;\n            thrust (float, .05): distance walked during each action;\n            puddle_center (np.array, None): center of the puddle;\n            puddle_width (np.array, None): width of the puddle;\n\n        """"""\n        # MDP parameters\n        self._start = np.array([.2, .4]) if start is None else start\n        self._goal = np.array([1., 1.]) if goal is None else goal\n        self._goal_threshold = goal_threshold\n        self._noise_step = noise_step\n        self._noise_reward = noise_reward\n        self._reward_goal = reward_goal\n        self._thrust = thrust\n        puddle_center = [[.3, .6], [.4, .5], [.8, .9]] if puddle_center is None else puddle_center\n        self._puddle_center = [np.array(center) for center in puddle_center]\n        puddle_width = [[.1, .03], [.03, .1], [.03, .1]] if puddle_width is None else puddle_width\n        self._puddle_width = [np.array(width) for width in puddle_width]\n\n        self._actions = [np.zeros(2) for _ in range(5)]\n        for i in range(4):\n            self._actions[i][i // 2] = thrust * (i % 2 * 2 - 1)\n\n        # MDP properties\n        action_space = Discrete(5)\n        observation_space = Box(0., 1., shape=(2,))\n        mdp_info = MDPInfo(observation_space, action_space, gamma, horizon)\n\n        # Visualization\n        self._pixels = None\n        self._viewer = Viewer(1.0, 1.0)\n\n        super().__init__(mdp_info)\n\n    def reset(self, state=None):\n        if state is None:\n            self._state = self._start.copy()\n        else:\n            self._state = state\n\n        return self._state\n\n    def step(self, action):\n        idx = action[0]\n        self._state += self._actions[idx] + np.random.uniform(\n            low=-self._noise_step, high=self._noise_step, size=(2,))\n        self._state = np.clip(self._state, 0., 1.)\n\n        absorbing = np.linalg.norm((self._state - self._goal),\n                                   ord=1) < self._goal_threshold\n\n        if not absorbing:\n            reward = np.random.randn() * self._noise_reward + self._get_reward(\n                self._state)\n        else:\n            reward = self._reward_goal\n\n        return self._state, reward, absorbing, {}\n\n    def render(self):\n        if self._pixels is None:\n            img_size = 100\n            pixels = np.zeros((img_size, img_size, 3))\n            for i in range(img_size):\n                for j in range(img_size):\n                    x = i / img_size\n                    y = j / img_size\n                    pixels[i, img_size - 1 - j] = self._get_reward(\n                        np.array([x, y]))\n\n            pixels -= pixels.min()\n            pixels *= 255. / pixels.max()\n            self._pixels = np.floor(255 - pixels)\n\n        self._viewer.background_image(self._pixels)\n        self._viewer.circle(self._state, 0.01,\n                            color=(0, 255, 0))\n\n        goal_area = [\n            [-self._goal_threshold, 0],\n            [0, self._goal_threshold],\n            [self._goal_threshold, 0],\n            [0, -self._goal_threshold]\n        ]\n        self._viewer.polygon(self._goal, 0, goal_area,\n                             color=(255, 0, 0), width=1)\n\n        self._viewer.display(0.1)\n\n    def stop(self):\n        if self._viewer is not None:\n            self._viewer.close()\n\n    def _get_reward(self, state):\n        reward = -1.\n        for cen, wid in zip(self._puddle_center, self._puddle_width):\n            reward -= 2. * norm.pdf(state[0], cen[0], wid[0]) * norm.pdf(\n                state[1], cen[1], wid[1])\n\n        return reward\n'"
mushroom_rl/environments/segway.py,0,"b'import numpy as np\nfrom scipy.integrate import odeint\n\nfrom mushroom_rl.environments import Environment, MDPInfo\nfrom mushroom_rl.utils import spaces\nfrom mushroom_rl.utils.angles import normalize_angle\nfrom mushroom_rl.utils.viewer import Viewer\n\n\nclass Segway(Environment):\n    """"""\n    The Segway environment (continuous version) as presented in:\n    ""Deep Learning for Actor-Critic Reinforcement Learning"". Xueli Jia. 2015.\n\n    """"""\n    def __init__(self, random_start=False):\n        """"""\n        Constructor.\n\n        Args:\n            random_start (bool, False): whether to start from a random position\n                or from the horizontal one.\n\n        """"""\n        # MDP parameters\n        gamma = 0.97\n\n        self._Mr = 0.3 * 2\n        self._Mp = 2.55\n        self._Ip = 2.6e-2\n        self._Ir = 4.54e-4 * 2\n        self._l = 13.8e-2\n        self._r = 5.5e-2\n        self._dt = 1e-2\n        self._g = 9.81\n        self._max_u = 5\n\n        self._random = random_start\n\n        high = np.array([-np.pi / 2, 15, 75])\n\n        # MDP properties\n        observation_space = spaces.Box(low=-high, high=high)\n        action_space = spaces.Box(low=np.array([-self._max_u]),\n                                  high=np.array([self._max_u]))\n        horizon = 300\n        mdp_info = MDPInfo(observation_space, action_space, gamma, horizon)\n\n        # Visualization\n        self._viewer = Viewer(5 * self._l, 5 * self._l)\n        self._last_x = 0\n\n        super().__init__(mdp_info)\n\n    def reset(self, state=None):\n        if state is None:\n            if self._random:\n                angle = np.random.uniform(-np.pi / 2, np.pi / 2)\n            else:\n                angle = -np.pi/8\n\n            self._state = np.array([angle, 0., 0.])\n        else:\n            self._state = state\n            self._state[0] = normalize_angle(self._state[0])\n\n        self._last_x = 0\n\n        return self._state\n\n    def step(self, action):\n        u = self._bound(action[0], -self._max_u, self._max_u)\n        new_state = odeint(self._dynamics, self._state, [0, self._dt],\n                           (u,))\n\n        self._state = np.array(new_state[-1])\n        self._state[0] = normalize_angle(self._state[0])\n\n        if abs(self._state[0]) > np.pi / 2:\n            absorbing = True\n            reward = -10000\n        else:\n            absorbing = False\n            Q = np.diag([3.0, 0.1, 0.1])\n\n            x = self._state\n\n            J = x.dot(Q).dot(x)\n\n            reward = -J\n\n        return self._state, reward, absorbing, {}\n\n    def _dynamics(self, state, t, u):\n\n        alpha = state[0]\n        d_alpha = state[1]\n\n        h1 = (self._Mr + self._Mp) * (self._r ** 2) + self._Ir\n        h2 = self._Mp * self._r * self._l * np.cos(alpha)\n        h3 = self._l ** 2 * self._Mp + self._Ip\n\n        omegaP = d_alpha\n\n        dOmegaP = -(h2 * self._l * self._Mp * self._r * np.sin(\n            alpha) * omegaP**2 - self._g * h1 * self._l * self._Mp * np.sin(\n            alpha) + (h2 + h1) * u) / (h1 * h3 - h2**2)\n        dOmegaR = (h3 * self._l * self._Mp * self._r * np.sin(\n            alpha) * omegaP**2 - self._g * h2 * self._l * self._Mp * np.sin(\n            alpha) + (h3 + h2) * u) / (h1 * h3 - h2**2)\n\n        dx = list()\n        dx.append(omegaP)\n        dx.append(dOmegaP)\n        dx.append(dOmegaR)\n\n        return dx\n\n    def render(self, mode=\'human\'):\n        start = 2.5 * self._l * np.ones(2)\n        end = 2.5 * self._l * np.ones(2)\n\n        dx = -self._state[2] * self._r * self._dt\n\n        self._last_x += dx\n\n        if self._last_x > 2.5 * self._l or self._last_x < -2.5 * self._l:\n            self._last_x = (2.5 * self._l + self._last_x) % (\n                5 * self._l) - 2.5 * self._l\n\n        start[0] += self._last_x\n        end[0] += -2 * self._l * np.sin(self._state[0]) + self._last_x\n        end[1] += 2 * self._l * np.cos(self._state[0])\n\n        if (start[0] > 5 * self._l and end[0] > 5 * self._l) \\\n                or (start[0] < 0 and end[0] < 0):\n            start[0] = start[0] % 5 * self._l\n            end[0] = end[0] % 5 * self._l\n\n        self._viewer.line(start, end)\n        self._viewer.circle(start, self._r)\n\n        self._viewer.display(self._dt)\n\n\n\n'"
mushroom_rl/environments/ship_steering.py,0,"b'import numpy as np\n\nfrom mushroom_rl.environments import Environment, MDPInfo\nfrom mushroom_rl.utils import spaces\nfrom mushroom_rl.utils.angles import normalize_angle\nfrom mushroom_rl.utils.viewer import Viewer\n\n\nclass ShipSteering(Environment):\n    """"""\n    The Ship Steering environment as presented in:\n    ""Hierarchical Policy Gradient Algorithms"". Ghavamzadeh M. and Mahadevan S..\n    2013.\n\n    """"""\n    def __init__(self, small=True, n_steps_action=3):\n        """"""\n        Constructor.\n\n        Args:\n             small (bool, True): whether to use a small state space or not.\n             n_steps_action (int, 3): number of integration intervals for each\n                                      step of the mdp.\n\n        """"""\n        # MDP parameters\n        self.field_size = 150 if small else 1000\n        low = np.array([0, 0, -np.pi, -np.pi / 12.])\n        high = np.array([self.field_size, self.field_size, np.pi, np.pi / 12.])\n        self.omega_max = np.array([np.pi / 12.])\n        self._v = 3.\n        self._T = 5.\n        self._dt = .2\n        self._gate_s = np.empty(2)\n        self._gate_e = np.empty(2)\n        self._gate_s[0] = 100 if small else 350\n        self._gate_s[1] = 120 if small else 400\n        self._gate_e[0] = 120 if small else 450\n        self._gate_e[1] = 100 if small else 400\n        self._out_reward = -100\n        self._success_reward = 0\n        self._small = small\n        self._state = None\n        self.n_steps_action = n_steps_action\n\n        # MDP properties\n        observation_space = spaces.Box(low=low, high=high)\n        action_space = spaces.Box(low=-self.omega_max, high=self.omega_max)\n        horizon = 5000\n        gamma = .99\n        mdp_info = MDPInfo(observation_space, action_space, gamma, horizon)\n\n        # Visualization\n        self._viewer = Viewer(self.field_size, self.field_size,\n                              background=(66, 131, 237))\n\n        super().__init__(mdp_info)\n\n    def reset(self, state=None):\n        if state is None:\n            if self._small:\n                self._state = np.zeros(4)\n                self._state[2] = np.pi/2\n            else:\n                low = self.info.observation_space.low\n                high = self.info.observation_space.high\n                self._state = (high-low)*np.random.rand(4) + low\n        else:\n            self._state = state\n\n        return self._state\n\n    def step(self, action):\n\n        r = self._bound(action[0], -self.omega_max, self.omega_max)\n\n        new_state = self._state\n\n        for _ in range(self.n_steps_action):\n            state = new_state\n            new_state = np.empty(4)\n            new_state[0] = state[0] + self._v * np.cos(state[2]) * self._dt\n            new_state[1] = state[1] + self._v * np.sin(state[2]) * self._dt\n            new_state[2] = normalize_angle(state[2] + state[3] * self._dt)\n            new_state[3] = state[3] + (r - state[3]) * self._dt / self._T\n\n            if new_state[0] > self.field_size \\\n               or new_state[1] > self.field_size \\\n               or new_state[0] < 0 or new_state[1] < 0:\n\n                new_state[0] = self._bound(new_state[0], 0, self.field_size)\n                new_state[1] = self._bound(new_state[1], 0, self.field_size)\n\n                reward = self._out_reward\n                absorbing = True\n                break\n\n            elif self._through_gate(state[:2], new_state[:2]):\n                reward = self._success_reward\n                absorbing = True\n                break\n            else:\n                reward = -1\n                absorbing = False\n\n        self._state = new_state\n\n        return self._state, reward, absorbing, {}\n\n    def render(self, mode=\'human\'):\n        self._viewer.line(self._gate_s, self._gate_e,\n                          width=3)\n\n        boat = [\n            [-4, -4],\n            [-4, 4],\n            [4, 4],\n            [8, 0.0],\n            [4, -4]\n        ]\n        self._viewer.polygon(self._state[:2], self._state[2], boat,\n                             color=(32, 193, 54))\n\n        self._viewer.display(self._dt)\n\n    def stop(self):\n        self._viewer.close()\n\n    def _through_gate(self, start, end):\n        r = self._gate_e - self._gate_s\n        s = end - start\n        den = self._cross_2d(vecr=r, vecs=s)\n\n        if den == 0:\n            return False\n\n        t = self._cross_2d((start - self._gate_s), s) / den\n        u = self._cross_2d((start - self._gate_s), r) / den\n\n        return 1 >= u >= 0 and 1 >= t >= 0\n\n    @staticmethod\n    def _cross_2d(vecr, vecs):\n        return vecr[0] * vecs[1] - vecr[1] * vecs[0]\n'"
mushroom_rl/features/__init__.py,0,"b""from .features import Features, get_action_features\n\n__all__ = ['Features', 'get_action_features']\n"""
mushroom_rl/features/features.py,0,"b'import numpy as np\n\nfrom ._implementations.basis_features import BasisFeatures\nfrom ._implementations.functional_features import FunctionalFeatures\nfrom ._implementations.tiles_features import TilesFeatures\nfrom ._implementations.pytorch_features import PyTorchFeatures\n\n\ndef Features(basis_list=None, tilings=None, tensor_list=None,\n             n_outputs=None, function=None, device=None):\n    """"""\n    Factory method to build the requested type of features. The types are\n    mutually exclusive.\n\n    Possible features are tilings (``tilings``), basis functions\n    (``basis_list``), tensor basis (``tensor_list``), and functional mappings\n    (``n_outputs`` and ``function``).\n\n    The difference between ``basis_list`` and ``tensor_list`` is that the\n    former is a list of python classes each one evaluating a single element of\n    the feature vector, while the latter consists in a list  of PyTorch modules\n    that can be used to build a PyTorch network. The use of ``tensor_list`` is\n    a faster way to compute features than `basis_list` and is suggested when\n    the computation of the requested features is slow (see the Gaussian radial\n    basis function implementation as an example). A functional mapping applies\n    a function to the input computing an ``n_outputs``-dimensional vector,\n    where the mapping is expressed by ``function``. If ``function`` is not\n    provided, the identity is used.\n\n    Args:\n        basis_list (list, None): list of basis functions;\n        tilings ([object, list], None): single object or list of tilings;\n        tensor_list (list, None): list of dictionaries containing the\n            instructions to build the requested tensors;\n        n_outputs (int, None): dimensionality of the feature mapping;\n        function (object, None): a callable function to be used as feature\n            mapping. Only needed when using a functional mapping.\n        device (int, None): where to run the group of tensors. Only\n            needed when using a list of tensors.\n\n    Returns:\n        The class implementing the requested type of features.\n\n    """"""\n    if basis_list is not None and tilings is None and tensor_list is None and n_outputs is None:\n        return BasisFeatures(basis_list)\n    elif basis_list is None and tilings is not None and tensor_list is None and n_outputs is None:\n        return TilesFeatures(tilings)\n    elif basis_list is None and tilings is None and tensor_list is not None and n_outputs is None:\n        return PyTorchFeatures(tensor_list, device=device)\n    elif basis_list is None and tilings is None and tensor_list is None and n_outputs is not None:\n        return FunctionalFeatures(n_outputs, function)\n    else:\n        raise ValueError(\'You must specify a list of basis or a list of tilings\'\n                         \'or a list of tensors.\')\n\n\ndef get_action_features(phi_state, action, n_actions):\n    """"""\n    Compute an array of size ``len(phi_state)`` * ``n_actions`` filled with\n    zeros, except for elements from ``len(phi_state)`` * ``action`` to\n    ``len(phi_state)`` * (``action`` + 1) that are filled with `phi_state`. This\n    is used to compute state-action features.\n\n    Args:\n        phi_state (np.ndarray): the feature of the state;\n        action (np.ndarray): the action whose features have to be computed;\n        n_actions (int): the number of actions.\n\n    Returns:\n        The state-action features.\n\n    """"""\n    if len(phi_state.shape) > 1:\n        assert phi_state.shape[0] == action.shape[0]\n\n        phi = np.ones((phi_state.shape[0], n_actions * phi_state[0].size))\n        i = 0\n        for s, a in zip(phi_state, action):\n            start = s.size * int(a[0])\n            stop = start + s.size\n\n            phi_sa = np.zeros(n_actions * s.size)\n            phi_sa[start:stop] = s\n\n            phi[i] = phi_sa\n\n            i += 1\n    else:\n        start = phi_state.size * action[0]\n        stop = start + phi_state.size\n\n        phi = np.zeros(n_actions * phi_state.size)\n        phi[start:stop] = phi_state\n\n    return phi\n'"
mushroom_rl/policy/__init__.py,0,"b""from .policy import Policy, ParametricPolicy\nfrom .noise_policy import OrnsteinUhlenbeckPolicy\nfrom .td_policy import TDPolicy, Boltzmann, EpsGreedy, Mellowmax\nfrom .gaussian_policy import GaussianPolicy, DiagonalGaussianPolicy, \\\n     StateStdGaussianPolicy, StateLogStdGaussianPolicy\nfrom .deterministic_policy import DeterministicPolicy\nfrom .torch_policy import TorchPolicy, GaussianTorchPolicy\n\n\n__all_td__ = ['TDPolicy', 'Boltzmann', 'EpsGreedy', 'Mellowmax']\n__all_parametric__ = ['ParametricPolicy', 'GaussianPolicy',\n                      'DiagonalGaussianPolicy', 'StateStdGaussianPolicy',\n                      'StateLogStdGaussianPolicy']\n__all_torch__ = ['TorchPolicy', 'GaussianTorchPolicy']\n\n__all__ = ['Policy',  'DeterministicPolicy', 'OrnsteinUhlenbeckPolicy'] \\\n          + __all_td__ + __all_parametric__ + __all_torch__\n"""
mushroom_rl/policy/deterministic_policy.py,0,"b'import numpy as np\nfrom .policy import ParametricPolicy\n\n\nclass DeterministicPolicy(ParametricPolicy):\n    """"""\n    Simple parametric policy representing a deterministic policy. As\n    deterministic policies are degenerate probability functions where all\n    the probability mass is on the deterministic action,they are not\n    differentiable, even if the mean value approximator is differentiable.\n\n    """"""\n    def __init__(self, mu):\n        """"""\n        Constructor.\n\n        Args:\n            mu (Regressor): the regressor representing the action to select\n                in each state.\n\n        """"""\n        self._approximator = mu\n\n    def get_regressor(self):\n        """"""\n        Getter.\n\n        Returns:\n            the regressor that is used to map state to actions.\n\n        """"""\n        return self._approximator\n\n    def __call__(self, state, action):\n        policy_action = self._approximator.predict(state)\n\n        return 1. if np.array_equal(action, policy_action) else 0.\n\n    def draw_action(self, state):\n        return self._approximator.predict(state)\n\n    def set_weights(self, weights):\n        self._approximator.set_weights(weights)\n\n    def get_weights(self):\n        return self._approximator.get_weights()\n\n    @property\n    def weights_size(self):\n        return self._approximator.weights_size\n'"
mushroom_rl/policy/gaussian_policy.py,0,"b'import numpy as np\n\nfrom .policy import ParametricPolicy\nfrom scipy.stats import multivariate_normal\n\n\nclass AbstractGaussianPolicy(ParametricPolicy):\n    """"""\n    Abstract class of Gaussian policies.\n\n    """"""\n    def __call__(self, state, action):\n        mu, sigma = self._compute_multivariate_gaussian(state)[:2]\n\n        return multivariate_normal.pdf(action, mu, sigma)\n\n    def draw_action(self, state):\n        mu, sigma = self._compute_multivariate_gaussian(state)[:2]\n\n        return np.random.multivariate_normal(mu, sigma)\n\n\nclass GaussianPolicy(AbstractGaussianPolicy):\n    """"""\n    Gaussian policy.\n    This is a differentiable policy for continuous action spaces.\n    The policy samples an action in every state following a gaussian\n    distribution, where the mean is computed in the state and the covariance\n    matrix is fixed.\n\n    """"""\n    def __init__(self, mu, sigma):\n        """"""\n        Constructor.\n\n        Args:\n            mu (Regressor): the regressor representing the mean w.r.t. the\n                state;\n            sigma (np.ndarray): a square positive definite matrix representing\n                the covariance matrix. The size of this matrix must be n x n,\n                where n is the action dimensionality.\n\n        """"""\n        self._approximator = mu\n        self._inv_sigma = np.linalg.inv(sigma)\n        self._sigma = sigma\n\n    def set_sigma(self, sigma):\n        """"""\n        Setter.\n\n        Args:\n            sigma (np.ndarray): the new covariance matrix. Must be a square\n                positive definite matrix.\n\n        """"""\n        self._sigma = sigma\n        self._inv_sigma = np.linalg.inv(sigma)\n\n    def diff_log(self, state, action):\n\n        mu, _, inv_sigma = self._compute_multivariate_gaussian(state)\n\n        delta = action - mu\n\n        j_mu = self._approximator.diff(state)\n\n        if len(j_mu.shape) == 1:\n            j_mu = np.expand_dims(j_mu, axis=1)\n\n        g = .5 * j_mu.dot(inv_sigma + inv_sigma.T).dot(delta.T)\n\n        return g\n\n    def set_weights(self, weights):\n        self._approximator.set_weights(weights)\n\n    def get_weights(self):\n        return self._approximator.get_weights()\n\n    @property\n    def weights_size(self):\n        return self._approximator.weights_size\n\n    def _compute_multivariate_gaussian(self, state):\n        mu = np.reshape(self._approximator.predict(np.expand_dims(state,\n                                                                  axis=0)), -1)\n\n        return mu, self._sigma, self._inv_sigma\n\n\nclass DiagonalGaussianPolicy(AbstractGaussianPolicy):\n    """"""\n    Gaussian policy with learnable standard deviation.\n    The Covariance matrix is\n    constrained to be a diagonal matrix, where the diagonal is the squared\n    standard deviation vector.\n    This is a differentiable policy for continuous action spaces.\n    This policy is similar to the gaussian policy, but the weights includes\n    also the standard deviation.\n\n    """"""\n    def __init__(self, mu, std):\n        """"""\n        Constructor.\n\n        Args:\n            mu (Regressor): the regressor representing the mean w.r.t. the\n                state;\n            std (np.ndarray): a vector of standard deviations. The length of\n                this vector must be equal to the action dimensionality.\n\n        """"""\n        self._approximator = mu\n        self._std = std\n\n    def set_std(self, std):\n        """"""\n        Setter.\n\n        Args:\n            std (np.ndarray): the new standard deviation. Must be a square\n                positive definite matrix.\n\n        """"""\n        self._std = std\n\n    def diff_log(self, state, action):\n        mu, _, inv_sigma = self._compute_multivariate_gaussian(state)\n\n        delta = action - mu\n\n        # Compute mean derivative\n        j_mu = self._approximator.diff(state)\n\n        if len(j_mu.shape) == 1:\n            j_mu = np.expand_dims(j_mu, axis=1)\n\n        g_mu = .5 * j_mu.dot(inv_sigma + inv_sigma.T).dot(delta.T)\n\n        # Compute standard deviation derivative\n        g_sigma = -1. / self._std + delta**2 / self._std**3\n\n        return np.concatenate((g_mu, g_sigma), axis=0)\n\n    def set_weights(self, weights):\n        self._approximator.set_weights(\n            weights[0:self._approximator.weights_size])\n        self._std = weights[self._approximator.weights_size:]\n\n    def get_weights(self):\n        return np.concatenate((self._approximator.get_weights(), self._std),\n                              axis=0)\n\n    @property\n    def weights_size(self):\n        return self._approximator.weights_size + self._std.size\n\n    def _compute_multivariate_gaussian(self, state):\n        mu = np.reshape(self._approximator.predict(np.expand_dims(state,\n                                                                  axis=0)), -1)\n\n        sigma = self._std**2\n\n        return mu, np.diag(sigma), np.diag(1. / sigma)\n\n\nclass StateStdGaussianPolicy(AbstractGaussianPolicy):\n    """"""\n    Gaussian policy with learnable standard deviation.\n    The Covariance matrix is\n    constrained to be a diagonal matrix, where the diagonal is the squared\n    standard deviation, which is computed for each state.\n    This is a differentiable policy for continuous action spaces.\n    This policy is similar to the diagonal gaussian policy, but a parametric\n    regressor is used to compute the standard deviation, so the standard\n    deviation depends on the current state.\n\n    """"""\n    def __init__(self, mu, std, eps=1e-6):\n        """"""\n        Constructor.\n\n        Args:\n            mu (Regressor): the regressor representing the mean w.r.t. the\n                state;\n            std (Regressor): the regressor representing the standard\n                deviations w.r.t. the state. The output dimensionality of the\n                regressor must be equal to the action dimensionality;\n            eps(float, 1e-6): A positive constant added to the variance to\n                ensure that is always greater than zero.\n\n        """"""\n        assert(eps > 0)\n\n        self._mu_approximator = mu\n        self._std_approximator = std\n        self._eps = eps\n\n    def diff_log(self, state, action):\n\n        mu, sigma, std = self._compute_multivariate_gaussian(state)\n        diag_sigma = np.diag(sigma)\n\n        delta = action - mu\n\n        # Compute mean derivative\n        j_mu = self._mu_approximator.diff(state)\n\n        if len(j_mu.shape) == 1:\n            j_mu = np.expand_dims(j_mu, axis=1)\n\n        sigma_inv = np.diag(1 / diag_sigma)\n\n        g_mu = j_mu.dot(sigma_inv).dot(delta.T)\n\n        # Compute variance derivative\n        w = (delta**2 - diag_sigma) * std / diag_sigma**2\n        j_sigma = np.atleast_2d(self._std_approximator.diff(state).T)\n        g_sigma = np.atleast_1d(w.dot(j_sigma))\n\n        return np.concatenate((g_mu, g_sigma), axis=0)\n\n    def set_weights(self, weights):\n        mu_weights = weights[0:self._mu_approximator.weights_size]\n        std_weights = weights[self._mu_approximator.weights_size:]\n\n        self._mu_approximator.set_weights(mu_weights)\n        self._std_approximator.set_weights(std_weights)\n\n    def get_weights(self):\n        mu_weights = self._mu_approximator.get_weights()\n        std_weights = self._std_approximator.get_weights()\n\n        return np.concatenate((mu_weights, std_weights), axis=0)\n\n    @property\n    def weights_size(self):\n        return self._mu_approximator.weights_size + \\\n               self._std_approximator.weights_size\n\n    def _compute_multivariate_gaussian(self, state):\n        mu = np.reshape(self._mu_approximator.predict(\n            np.expand_dims(state, axis=0)), -1)\n\n        std = np.reshape(self._std_approximator.predict(\n            np.expand_dims(state, axis=0)), -1)\n\n        sigma = std**2 + self._eps\n\n        return mu, np.diag(sigma), std\n\n\nclass StateLogStdGaussianPolicy(AbstractGaussianPolicy):\n    """"""\n    Gaussian policy with learnable standard deviation.\n    The Covariance matrix is\n    constrained to be a diagonal matrix, the diagonal is computed by an\n    exponential transformation of the logarithm of the standard deviation\n    computed in each state.\n    This is a differentiable policy for continuous action spaces.\n    This policy is similar to the State std gaussian policy, but here the\n    regressor represents the logarithm of the standard deviation.\n\n    """"""\n    def __init__(self, mu, log_std):\n        """"""\n        Constructor.\n\n        Args:\n            mu (Regressor): the regressor representing the mean w.r.t. the\n                state;\n            log_std (Regressor): a regressor representing the logarithm of the\n                variance w.r.t. the state. The output dimensionality of the\n                regressor must be equal to the action dimensionality.\n\n        """"""\n        self._mu_approximator = mu\n        self._log_std_approximator = log_std\n\n    def diff_log(self, state, action):\n\n        mu, sigma = self._compute_multivariate_gaussian(state)\n        diag_sigma = np.diag(sigma)\n\n        delta = action - mu\n\n        # Compute mean derivative\n        j_mu = self._mu_approximator.diff(state)\n\n        if len(j_mu.shape) == 1:\n            j_mu = np.expand_dims(j_mu, axis=1)\n\n        sigma_inv = np.diag(1 / diag_sigma)\n\n        g_mu = j_mu.dot(sigma_inv).dot(delta.T)\n\n        # Compute variance derivative\n        w = delta**2 / diag_sigma\n        j_sigma = np.atleast_2d(self._log_std_approximator.diff(state).T)\n        g_sigma = np.atleast_1d(w.dot(j_sigma)) - np.sum(j_sigma, axis=0)\n\n        return np.concatenate((g_mu, g_sigma), axis=0)\n\n    def set_weights(self, weights):\n        mu_weights = weights[0:self._mu_approximator.weights_size]\n        log_std_weights = weights[self._mu_approximator.weights_size:]\n\n        self._mu_approximator.set_weights(mu_weights)\n        self._log_std_approximator.set_weights(log_std_weights)\n\n    def get_weights(self):\n        mu_weights = self._mu_approximator.get_weights()\n        log_std_weights = self._log_std_approximator.get_weights()\n\n        return np.concatenate((mu_weights, log_std_weights), axis=0)\n\n    @property\n    def weights_size(self):\n        return self._mu_approximator.weights_size + \\\n               self._log_std_approximator.weights_size\n\n    def _compute_multivariate_gaussian(self, state):\n        mu = np.reshape(self._mu_approximator.predict(\n            np.expand_dims(state, axis=0)), -1)\n\n        log_std = np.reshape(self._log_std_approximator.predict(\n            np.expand_dims(state, axis=0)), -1)\n\n        sigma = np.exp(log_std)**2\n\n        return mu, np.diag(sigma)\n'"
mushroom_rl/policy/noise_policy.py,0,"b'import numpy as np\n\nfrom .policy import ParametricPolicy\n\n\nclass OrnsteinUhlenbeckPolicy(ParametricPolicy):\n    """"""\n    Ornstein-Uhlenbeck process as implemented in:\n    https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py.\n\n    This policy is commonly used in the Deep Deterministic Policy Gradient\n    algorithm.\n\n    """"""\n    def __init__(self, mu, sigma, theta, dt, x0=None):\n        """"""\n        Constructor.\n\n        Args:\n            mu (Regressor): the regressor representing the mean w.r.t. the\n                state;\n            sigma (np.ndarray): average magnitude of the random flactations per\n                square-root time;\n            theta (float): rate of mean reversion;\n            dt (float): time interval;\n            x0 (np.ndarray, None): initial values of noise.\n\n        """"""\n        self._approximator = mu\n        self._sigma = sigma\n        self._theta = theta\n        self._dt = dt\n        self._x0 = x0\n\n        self.reset()\n\n    def __call__(self, state, action):\n        raise NotImplementedError\n\n    def draw_action(self, state):\n        mu = self._approximator.predict(state)\n\n        x = self._x_prev - self._theta * self._x_prev * self._dt +\\\n            self._sigma * np.sqrt(self._dt) * np.random.normal(\n                size=self._approximator.output_shape\n            )\n        self._x_prev = x\n\n        return mu + x\n\n    def set_weights(self, weights):\n        self._approximator.set_weights(weights)\n\n    def get_weights(self):\n        return self._approximator.get_weights()\n\n    @property\n    def weights_size(self):\n        return self._approximator.weights_size\n\n    def reset(self):\n        self._x_prev = self._x0 if self._x0 is not None else np.zeros(self._approximator.output_shape)\n'"
mushroom_rl/policy/policy.py,0,"b'class Policy(object):\n    """"""\n    Interface representing a generic policy.\n    A policy is a probability distribution that gives the probability of taking\n    an action given a specified state.\n    A policy is used by mushroom agents to interact with the environment.\n    """"""\n    def __call__(self, *args):\n        """"""\n        Compute the probability of taking action in a certain state following\n        the policy.\n\n        Args:\n            *args (list): list containing a state or a state and an action.\n\n        Returns:\n            The probability of all actions following the policy in the given\n            state if the list contains only the state, else the probability\n            of the given action in the given state following the policy. If\n            the action space is continuous, state and action must be provided\n\n        """"""\n        raise NotImplementedError\n\n    def draw_action(self, state):\n        """"""\n        Sample an action in ``state`` using the policy.\n\n        Args:\n            state (np.ndarray): the state where the agent is.\n\n        Returns:\n            The action sampled from the policy.\n\n        """"""\n        raise NotImplementedError\n\n    def reset(self):\n        """"""\n        Useful when the policy needs a special initialization at the beginning\n        of an episode.\n\n        """"""\n        pass\n\n\nclass ParametricPolicy(Policy):\n    """"""\n    Interface for a generic parametric policy.\n    A parametric policy is a policy that depends on set of parameters,\n    called the policy weights.\n    If the policy is differentiable, the derivative of the probability for a\n    specified state-action pair can be provided.\n    """"""\n\n    def diff_log(self, state, action):\n        """"""\n        Compute the gradient of the logarithm of the probability density\n        function, in the specified state and action pair, i.e.:\n\n        .. math::\n            \\\\nabla_{\\\\theta}\\\\log p(s,a)\n\n\n        Args:\n            state (np.ndarray): the state where the gradient is computed\n            action (np.ndarray): the action where the gradient is computed\n\n        Returns:\n            The gradient of the logarithm of the pdf w.r.t. the policy weights\n        """"""\n        raise RuntimeError(\'The policy is not differentiable\')\n\n    def diff(self, state, action):\n        """"""\n        Compute the derivative of the probability density function, in the\n        specified state and action pair. Normally it is computed w.r.t. the\n        derivative of the logarithm of the probability density function,\n        exploiting the likelihood ratio trick, i.e.:\n\n        .. math::\n            \\\\nabla_{\\\\theta}p(s,a)=p(s,a)\\\\nabla_{\\\\theta}\\\\log p(s,a)\n\n\n        Args:\n            state (np.ndarray): the state where the derivative is computed\n            action (np.ndarray): the action where the derivative is computed\n\n        Returns:\n            The derivative w.r.t. the  policy weights\n        """"""\n        return self(state, action) * self.diff_log(state, action)\n\n    def set_weights(self, weights):\n        """"""\n        Setter.\n\n        Args:\n            weights (np.ndarray): the vector of the new weights to be used by\n                the policy.\n\n        """"""\n        raise NotImplementedError\n\n    def get_weights(self):\n        """"""\n        Getter.\n\n        Returns:\n             The current policy weights.\n\n        """"""\n        raise NotImplementedError\n\n    @property\n    def weights_size(self):\n        """"""\n        Property.\n\n        Returns:\n             The size of the policy weights.\n\n        """"""\n        raise NotImplementedError\n'"
mushroom_rl/policy/td_policy.py,0,"b'import numpy as np\nfrom scipy.optimize import brentq\nfrom scipy.special import logsumexp\nfrom .policy import Policy\n\nfrom mushroom_rl.utils.parameters import Parameter\n\n\nclass TDPolicy(Policy):\n    def __init__(self):\n        """"""\n        Constructor.\n\n        """"""\n        self._approximator = None\n\n    def set_q(self, approximator):\n        """"""\n        Args:\n            approximator (object): the approximator to use.\n\n        """"""\n        self._approximator = approximator\n\n    def get_q(self):\n        """"""\n        Returns:\n             The approximator used by the policy.\n\n        """"""\n        return self._approximator\n\n\nclass EpsGreedy(TDPolicy):\n    """"""\n    Epsilon greedy policy.\n\n    """"""\n    def __init__(self, epsilon):\n        """"""\n        Constructor.\n\n        Args:\n            epsilon (Parameter): the exploration coefficient. It indicates\n                the probability of performing a random actions in the current\n                step.\n\n        """"""\n        super().__init__()\n\n        assert isinstance(epsilon, Parameter)\n        self._epsilon = epsilon\n\n    def __call__(self, *args):\n        state = args[0]\n        q = self._approximator.predict(np.expand_dims(state, axis=0)).ravel()\n        max_a = np.argwhere(q == np.max(q)).ravel()\n\n        p = self._epsilon.get_value(state) / self._approximator.n_actions\n\n        if len(args) == 2:\n            action = args[1]\n            if action in max_a:\n                return p + (1. - self._epsilon.get_value(state)) / len(max_a)\n            else:\n                return p\n        else:\n            probs = np.ones(self._approximator.n_actions) * p\n            probs[max_a] += (1. - self._epsilon.get_value(state)) / len(max_a)\n\n            return probs\n\n    def draw_action(self, state):\n        if not np.random.uniform() < self._epsilon(state):\n            q = self._approximator.predict(state)\n            max_a = np.argwhere(q == np.max(q)).ravel()\n\n            if len(max_a) > 1:\n                max_a = np.array([np.random.choice(max_a)])\n\n            return max_a\n\n        return np.array([np.random.choice(self._approximator.n_actions)])\n\n    def set_epsilon(self, epsilon):\n        """"""\n        Setter.\n\n        Args:\n            epsilon (Parameter): the exploration coefficient. It indicates the\n            probability of performing a random actions in the current step.\n\n        """"""\n        assert isinstance(epsilon, Parameter)\n\n        self._epsilon = epsilon\n\n    def update(self, *idx):\n        """"""\n        Update the value of the epsilon parameter at the provided index (e.g. in\n        case of different values of epsilon for each visited state according to\n        the number of visits).\n\n        Args:\n            *idx (list): index of the parameter to be updated.\n\n        """"""\n        self._epsilon.update(*idx)\n\n\nclass Boltzmann(TDPolicy):\n    """"""\n    Boltzmann softmax policy.\n\n    """"""\n    def __init__(self, beta):\n        """"""\n        Constructor.\n\n        Args:\n            beta (Parameter): the inverse of the temperature distribution. As\n            the temperature approaches infinity, the policy becomes more and\n            more random. As the temperature approaches 0.0, the policy becomes\n            more and more greedy.\n\n        """"""\n        super().__init__()\n        self._beta = beta\n\n    def __call__(self, *args):\n        state = args[0]\n        q_beta = self._approximator.predict(state) * self._beta(state)\n        q_beta -= q_beta.max()\n        qs = np.exp(q_beta)\n\n        if len(args) == 2:\n            action = args[1]\n\n            return qs[action] / np.sum(qs)\n        else:\n            return qs / np.sum(qs)\n\n    def draw_action(self, state):\n        return np.array([np.random.choice(self._approximator.n_actions,\n                                          p=self(state))])\n\n    def set_beta(self, beta):\n        """"""\n        Setter.\n\n        Args:\n            beta (Parameter): the inverse of the temperature distribution.\n\n        """"""\n        assert isinstance(beta, Parameter)\n\n        self._beta = beta\n\n    def update(self, *idx):\n        """"""\n        Update the value of the beta parameter at the provided index (e.g. in\n        case of different values of beta for each visited state according to\n        the number of visits).\n\n        Args:\n            *idx (list): index of the parameter to be updated.\n\n        """"""\n        self._beta.update(*idx)\n\n\nclass Mellowmax(Boltzmann):\n    """"""\n    Mellowmax policy.\n    ""An Alternative Softmax Operator for Reinforcement Learning"". Asadi K. and\n    Littman M.L.. 2017.\n\n    """"""\n\n    class MellowmaxParameter:\n        def __init__(self, outer, omega, beta_min, beta_max):\n            self._omega = omega\n            self._outer = outer\n            self._beta_min = beta_min\n            self._beta_max = beta_max\n\n        def __call__(self, state):\n            q = self._outer._approximator.predict(state)\n            mm = (logsumexp(q * self._omega(state)) - np.log(\n                q.size)) / self._omega(state)\n\n            def f(beta):\n                v = q - mm\n                beta_v = beta * v\n                beta_v -= beta_v.max()\n\n                return np.sum(np.exp(beta_v) * v)\n\n            try:\n                beta = brentq(f, a=self._beta_min, b=self._beta_max)\n                assert not (np.isnan(beta) or np.isinf(beta))\n\n                return beta\n            except ValueError:\n                return 0.\n\n    def __init__(self, omega, beta_min=-10., beta_max=10.):\n        """"""\n        Constructor.\n\n        Args:\n            omega (Parameter): the omega parameter of the policy from which beta\n                of the Boltzmann policy is computed;\n            beta_min (float, -10.): one end of the bracketing interval for\n                minimization with Brent\'s method;\n            beta_max (float, 10.): the other end of the bracketing interval for\n                minimization with Brent\'s method.\n\n        """"""\n        beta_mellow = self.MellowmaxParameter(self, omega, beta_min, beta_max)\n\n        super().__init__(beta_mellow)\n\n    def set_beta(self, beta):\n        raise RuntimeError(\'Cannot change the beta parameter of Mellowmax policy\')\n\n    def update(self, *idx):\n        raise RuntimeError(\'Cannot update the beta parameter of Mellowmax policy\')'"
mushroom_rl/policy/torch_policy.py,13,"b'import numpy as np\n\nimport torch\nimport torch.nn as nn\n\nfrom mushroom_rl.policy import Policy\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.approximators.parametric import TorchApproximator\nfrom mushroom_rl.utils.torch import to_float_tensor\n\nfrom itertools import chain\n\n\nclass TorchPolicy(Policy):\n    """"""\n    Interface for a generic PyTorch policy.\n    A PyTorch policy is a policy implemented as a neural network using PyTorch.\n    Functions ending with \'_t\' use tensors as input, and also as output when\n    required.\n\n    """"""\n    def __init__(self, use_cuda):\n        """"""\n        Constructor.\n\n        Args:\n            use_cuda (bool): whether to use cuda or not.\n\n        """"""\n        self._use_cuda = use_cuda\n\n    def __call__(self, state, action):\n        s = to_float_tensor(np.atleast_2d(state), self._use_cuda)\n        a = to_float_tensor(np.atleast_2d(action), self._use_cuda)\n\n        return np.exp(self.log_prob_t(s, a).item())\n\n    def draw_action(self, state):\n        with torch.no_grad():\n            s = to_float_tensor(np.atleast_2d(state), self._use_cuda)\n            a = self.draw_action_t(s)\n\n        return torch.squeeze(a, dim=0).detach().cpu().numpy()\n\n    def distribution(self, state):\n        """"""\n        Compute the policy distribution in the given states.\n\n        Args:\n            state (np.ndarray): the set of states where the distribution is\n                computed.\n\n        Returns:\n            The torch distribution for the provided states.\n\n        """"""\n        s = to_float_tensor(state, self._use_cuda)\n\n        return self.distribution_t(s)\n\n    def entropy(self, state=None):\n        """"""\n        Compute the entropy of the policy.\n\n        Args:\n            state (np.ndarray, None): the set of states to consider. If the\n                entropy of the policy can be computed in closed form, then\n                ``state`` can be None.\n\n        Returns:\n            The value of the entropy of the policy.\n\n        """"""\n        s = to_float_tensor(state, self._use_cuda) if state is not None else None\n\n        return self.entropy_t(s).detach().cpu().numpy().item()\n\n    def draw_action_t(self, state):\n        """"""\n        Draw an action given a tensor.\n\n        Args:\n            state (torch.Tensor): set of states.\n\n        Returns:\n            The tensor of the actions to perform in each state.\n\n        """"""\n        raise NotImplementedError\n\n    def log_prob_t(self, state, action):\n        """"""\n        Compute the logarithm of the probability of taking ``action`` in\n        ``state``.\n\n        Args:\n            state (torch.Tensor): set of states.\n            action (torch.Tensor): set of actions.\n\n        Returns:\n            The tensor of log-probability.\n\n        """"""\n        raise NotImplementedError\n\n    def entropy_t(self, state=None):\n        """"""\n        Compute the entropy of the policy.\n\n        Args:\n            state (torch.Tensor): the set of states to consider. If the\n                entropy of the policy can be computed in closed form, then\n                ``state`` can be None.\n\n        Returns:\n            The tensor value of the entropy of the policy.\n\n        """"""\n        raise NotImplementedError\n\n    def distribution_t(self, state):\n        """"""\n        Compute the policy distribution in the given states.\n\n        Args:\n            state (torch.Tensor): the set of states where the distribution is\n                computed.\n\n        Returns:\n            The torch distribution for the provided states.\n\n        """"""\n        raise NotImplementedError\n\n    def set_weights(self, weights):\n        """"""\n        Setter.\n\n        Args:\n            weights (np.ndarray): the vector of the new weights to be used by\n                the policy.\n\n        """"""\n        raise NotImplementedError\n\n    def get_weights(self):\n        """"""\n        Getter.\n\n        Returns:\n             The current policy weights.\n\n        """"""\n        raise NotImplementedError\n\n    def parameters(self):\n        """"""\n        Returns the trainable policy parameters, as expected by torch\n        optimizers.\n\n        Returns:\n            List of parameters to be optimized.\n\n        """"""\n        raise NotImplementedError\n\n    def reset(self):\n        pass\n\n    @property\n    def use_cuda(self):\n        """"""\n        True if the policy is using cuda_tensors.\n        """"""\n        return self._use_cuda\n\n\nclass GaussianTorchPolicy(TorchPolicy):\n    """"""\n    Torch policy implementing a Gaussian policy with trainable standard\n    deviation. The standard deviation is not state-dependent.\n\n    """"""\n    def __init__(self, network, input_shape, output_shape, std_0=1.,\n                 use_cuda=False, **params):\n        """"""\n        Constructor.\n\n        Args:\n            network (object): the network class used to implement the mean\n                regressor;\n            input_shape (tuple): the shape of the state space;\n            output_shape (tuple): the shape of the action space;\n            std_0 (float, 1.): initial standard deviation;\n            params (dict): parameters used by the network constructor.\n\n        """"""\n        super().__init__(use_cuda)\n\n        self._action_dim = output_shape[0]\n\n        self._mu = Regressor(TorchApproximator, input_shape, output_shape,\n                             network=network, use_cuda=use_cuda, **params)\n\n        log_sigma_init = torch.ones(self._action_dim) * np.log(std_0)\n\n        if self._use_cuda:\n            log_sigma_init = log_sigma_init.cuda()\n\n        self._log_sigma = nn.Parameter(log_sigma_init)\n\n    def draw_action_t(self, state):\n        return self.distribution_t(state).sample().detach()\n\n    def log_prob_t(self, state, action):\n        return self.distribution_t(state).log_prob(action)[:, None]\n\n    def entropy_t(self, state=None):\n        return self._action_dim / 2 * np.log(2 * np.pi * np.e) + torch.sum(self._log_sigma)\n\n    def distribution_t(self, state):\n        mu, sigma = self.get_mean_and_covariance(state)\n        return torch.distributions.MultivariateNormal(loc=mu, covariance_matrix=sigma)\n\n    def get_mean_and_covariance(self, state):\n        return self._mu(state, output_tensor=True), torch.diag(torch.exp(2 * self._log_sigma))\n\n    def set_weights(self, weights):\n        log_sigma_data = torch.from_numpy(weights[-self._action_dim:])\n        if self.use_cuda:\n            log_sigma_data = log_sigma_data.cuda()\n        self._log_sigma.data = log_sigma_data\n\n        self._mu.set_weights(weights[:-self._action_dim])\n\n    def get_weights(self):\n        mu_weights = self._mu.get_weights()\n        sigma_weights = self._log_sigma.data.detach().cpu().numpy()\n\n        return np.concatenate([mu_weights, sigma_weights])\n\n    def parameters(self):\n        return chain(self._mu.model.network.parameters(), [self._log_sigma])\n'"
mushroom_rl/solvers/__init__.py,0,b''
mushroom_rl/solvers/car_on_hill.py,0,"b'def step(mdp, state, action):\n    """"""\n    Perform a step in the tree.\n\n    Args:\n        mdp (CarOnHill): the Car-On-Hill environment;\n        state (np.array): the state;\n        action (np.array): the action.\n\n    Returns:\n        The resulting transition executing ``action`` in ``state``.\n\n    """"""\n    mdp.reset(state)\n\n    return mdp.step(action)\n\n\ndef bfs(mdp, frontier, k, max_k):\n    """"""\n    Perform Breadth-First tree search.\n\n    Args:\n        mdp (CarOnHill): the Car-On-Hill environment;\n        frontier (list): the state at the frontier of the BFS;\n        k (int): the current depth of the tree;\n        max_k (int): maximum depth to consider.\n\n    Returns:\n        A tuple containing a flag for the algorithm ending, and the updated\n        depth of the tree.\n\n    """"""\n    if len(frontier) == 0 or k == max_k:\n        return False, k\n\n    new_frontier = list()\n    for f in frontier:\n        s, r, _, _ = step(mdp, f, [0])\n        if r == 1:\n            return True, k\n        elif r == 0:\n            new_frontier.append(s)\n\n        s, r, _, _ = step(mdp, f, [1])\n        if r == 1:\n            return True, k\n        elif r == 0:\n            new_frontier.append(s)\n\n    return bfs(mdp, new_frontier, k + 1, max_k)\n\n\ndef solve_car_on_hill(mdp, states, actions, gamma, max_k=50):\n    """"""\n    Solver of the Car-On-Hill environment.\n\n    Args:\n        mdp (CarOnHill): the Car-On-Hill environment;\n        states (np.ndarray): the states;\n        actions (np.ndarray): the actions;\n        gamma (float): the discount factor;\n        max_k (int, 50): maximum depth to consider.\n\n    Returns:\n        The Q-value for each ``state``-``action`` tuple.\n\n    """"""\n\n    q = list()\n    for s, a in zip(states, actions):\n        mdp.reset(s)\n        state, reward, _, _ = mdp.step(a)\n\n        if reward == 1:\n            k = 1\n            success = True\n        elif reward == -1:\n            k = 1\n            success = False\n        else:\n            success, k = bfs(mdp, [state], 2, max_k)\n\n        if success:\n            q.append(gamma ** (k - 1))\n        else:\n            q.append(-gamma ** (k - 1))\n\n    return q\n'"
mushroom_rl/solvers/dynamic_programming.py,0,"b'import numpy as np\nfrom copy import deepcopy\n\n\ndef value_iteration(prob, reward, gamma, eps):\n    """"""\n    Value iteration algorithm to solve a dynamic programming problem.\n\n    Args:\n        prob (np.ndarray): transition probability matrix;\n        reward (np.ndarray): reward matrix;\n        gamma (float): discount factor;\n        eps (float): accuracy threshold.\n\n    Returns:\n        The optimal value of each state.\n\n    """"""\n    n_states = prob.shape[0]\n    n_actions = prob.shape[1]\n\n    value = np.zeros(n_states)\n\n    while True:\n        value_old = deepcopy(value)\n\n        for state in range(n_states):\n            vmax = -np.inf\n            for action in range(n_actions):\n                prob_state_action = prob[state, action, :]\n                reward_state_action = reward[state, action, :]\n                va = prob_state_action.T.dot(\n                    reward_state_action + gamma * value_old)\n                vmax = max(va, vmax)\n\n            value[state] = vmax\n        if np.linalg.norm(value - value_old) <= eps:\n            break\n\n    return value\n\n\ndef policy_iteration(prob, reward, gamma):\n    """"""\n    Policy iteration algorithm to solve a dynamic programming problem.\n\n    Args:\n        prob (np.ndarray): transition probability matrix;\n        reward (np.ndarray): reward matrix;\n        gamma (float): discount factor.\n\n    Returns:\n        The optimal value of each state and the optimal policy.\n\n    """"""\n    n_states = prob.shape[0]\n    n_actions = prob.shape[1]\n\n    policy = np.zeros(n_states, dtype=int)\n    value = np.zeros(n_states)\n\n    changed = True\n    while changed:\n        p_pi = np.zeros((n_states, n_states))\n        r_pi = np.zeros(n_states)\n        i = np.eye(n_states)\n\n        for state in range(n_states):\n            action = policy[state]\n            p_pi_s = prob[state, action, :]\n            r_pi_s = reward[state, action, :]\n\n            p_pi[state, :] = p_pi_s.T\n            r_pi[state] = p_pi_s.T.dot(r_pi_s)\n\n        value = np.linalg.inv(i - gamma * p_pi).dot(r_pi)\n\n        changed = False\n\n        for state in range(n_states):\n            vmax = value[state]\n            for action in range(n_actions):\n                if action != policy[state]:\n                    p_sa = prob[state, action]\n                    r_sa = reward[state, action]\n                    va = p_sa.T.dot(r_sa + gamma * value)\n                    if va > vmax and not np.isclose(va, vmax):\n                        policy[state] = action\n                        vmax = va\n                        changed = True\n\n    return value, policy\n'"
mushroom_rl/utils/__init__.py,0,b''
mushroom_rl/utils/angles.py,0,"b'# *********************************************************************\n# Software License Agreement (BSD License)\n#\n#  Copyright (c) 2015, Bossa Nova Robotics\n#  All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification, are permitted provided that the following conditions\n#  are met:\n#\n#   * Redistributions of source code must retain the above copyright\n#     notice, this list of conditions and the following disclaimer.\n#   * Redistributions in binary form must reproduce the above\n#     copyright notice, this list of conditions and the following\n#     disclaimer in the documentation and/or other materials provided\n#     with the distribution.\n#   * Neither the name of the Bossa Nova Robotics nor the names of its\n#     contributors may be used to endorse or promote products derived\n#     from this software without specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n#  ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n#  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\n#  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE\n#  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n#  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n#  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES\n#  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n#  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN\n#  ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n#  POSSIBILITY OF SUCH DAMAGE.\n# ********************************************************************/\n\nfrom math import fmod\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\n\ndef normalize_angle_positive(angle):\n    """"""\n    Wrap the angle between 0 and 2 * pi.\n\n    Args:\n        angle (float): angle to wrap.\n\n    Returns:\n         The wrapped angle.\n\n    """"""\n    pi_2 = 2. * np.pi\n\n    return fmod(fmod(angle, pi_2) + pi_2, pi_2)\n\n\ndef normalize_angle(angle):\n    """"""\n    Wrap the angle between -pi and pi.\n\n    Args:\n        angle (float): angle to wrap.\n\n    Returns:\n         The wrapped angle.\n\n    """"""\n    a = normalize_angle_positive(angle)\n    if a > np.pi:\n        a -= 2. * np.pi\n\n    return a\n\n\ndef shortest_angular_distance(from_angle, to_angle):\n    """"""\n    Compute the shortest distance between two angles\n\n    Args:\n        from_angle (float): starting angle;\n        to_angle (float): final angle.\n\n    Returns:\n        The shortest distance between from_angle and to_angle.\n\n    """"""\n    return normalize_angle(to_angle - from_angle)\n\n\ndef quat_to_euler(quat):\n    """"""\n    Convert a quaternion to euler angles.\n\n    Args:\n        quat (np.ndarray):  quaternion to be converted, must be in format [w, x, y, z]\n\n    Returns:\n        The euler angles [x, y, z] representation of the quaternion\n\n    """"""\n    if len(quat.shape) < 2:\n        return R.from_quat(quat[[1, 2, 3, 0]]).as_euler(\'xyz\')\n    else:\n        return R.from_quat(quat[[1, 2, 3, 0], :].T).as_euler(\'xyz\').T\n\n\ndef euler_to_quat(euler):\n    """"""\n    Convert euler angles into a quaternion.\n\n    Args:\n        euler (np.ndarray):  euler angles to be converted\n\n    Returns:\n        Quaternion in format [w, x, y, z]\n\n    """"""\n    if len(euler.shape) < 2:\n        return R.from_euler(\'xyz\', euler).as_quat()[[3, 0, 1, 2]]\n    else:\n        return R.from_euler(\'xyz\', euler.T).as_quat()[:, [3, 0, 1, 2]].T\n'"
mushroom_rl/utils/dataset.py,0,"b'import numpy as np\n\n\ndef parse_dataset(dataset, features=None):\n    """"""\n    Split the dataset in its different components and return them.\n\n    Args:\n        dataset (list): the dataset to parse;\n        features (object, None): features to apply to the states.\n\n    Returns:\n        The np.ndarray of state, action, reward, next_state, absorbing flag and\n        last step flag. Features are applied to ``state`` and ``next_state``,\n        when provided.\n\n    """"""\n    assert len(dataset) > 0\n\n    shape = dataset[0][0].shape if features is None else (features.size,)\n\n    state = np.ones((len(dataset),) + shape)\n    action = np.ones((len(dataset),) + dataset[0][1].shape)\n    reward = np.ones(len(dataset))\n    next_state = np.ones((len(dataset),) + shape)\n    absorbing = np.ones(len(dataset))\n    last = np.ones(len(dataset))\n\n    if features is not None:\n        for i in range(len(dataset)):\n            state[i, ...] = features(dataset[i][0])\n            action[i, ...] = dataset[i][1]\n            reward[i] = dataset[i][2]\n            next_state[i, ...] = features(dataset[i][3])\n            absorbing[i] = dataset[i][4]\n            last[i] = dataset[i][5]\n    else:\n        for i in range(len(dataset)):\n            state[i, ...] = dataset[i][0]\n            action[i, ...] = dataset[i][1]\n            reward[i] = dataset[i][2]\n            next_state[i, ...] = dataset[i][3]\n            absorbing[i] = dataset[i][4]\n            last[i] = dataset[i][5]\n\n    return np.array(state), np.array(action), np.array(reward), np.array(\n        next_state), np.array(absorbing), np.array(last)\n\n\ndef arrays_as_dataset(states, actions, rewards, next_states, absorbings, lasts):\n    """"""\n    Creates a dataset of transitions from the provided arrays.\n\n    Args:\n        states (np.ndarray): array of states;\n        actions (np.ndarray): array of actions;\n        rewards (np.ndarray): array of rewards;\n        next_states (np.ndarray): array of next_states;\n        absorbings (np.ndarray): array of absorbing flags;\n        lasts (np.ndarray): array of last flags.\n\n    Returns:\n        The list of transitions.\n\n    """"""\n    assert (len(states) == len(actions) == len(rewards)\n            == len(next_states) == len(absorbings) == len(lasts))\n\n    dataset = list()\n    for s, a, r, ss, ab, last in zip(states, actions, rewards, next_states,\n                                     absorbings.astype(bool), lasts.astype(bool)\n                                     ):\n        dataset.append((s, a, r.item(0), ss, ab.item(0), last.item(0)))\n\n    return dataset\n\n\ndef episodes_length(dataset):\n    """"""\n    Compute the length of each episode in the dataset.\n\n    Args:\n        dataset (list): the dataset to consider.\n\n    Returns:\n        A list of length of each episode in the dataset.\n\n    """"""\n    lengths = list()\n    l = 0\n    for sample in dataset:\n        l += 1\n        if sample[-1] == 1:\n            lengths.append(l)\n            l = 0\n\n    return lengths\n\n\ndef select_first_episodes(dataset, n_episodes, parse=False):\n    """"""\n    Return the first ``n_episodes`` episodes in the provided dataset.\n\n    Args:\n        dataset (list): the dataset to consider;\n        n_episodes (int): the number of episodes to pick from the dataset;\n        parse (bool, False): whether to parse the dataset to return.\n\n    Returns:\n        A subset of the dataset containing the first ``n_episodes`` episodes.\n\n    """"""\n    assert n_episodes >= 0, \'Number of episodes must be greater than or equal\' \\\n                            \'to zero.\'\n    if n_episodes == 0:\n        return np.array([[]])\n\n    dataset = np.array(dataset)\n    last_idxs = np.argwhere(dataset[:, -1] == 1).ravel()\n    sub_dataset = dataset[:last_idxs[n_episodes - 1] + 1, :]\n\n    return sub_dataset if not parse else parse_dataset(sub_dataset)\n\n\ndef select_random_samples(dataset, n_samples, parse=False):\n    """"""\n    Return the randomly picked desired number of samples in the provided\n    dataset.\n\n    Args:\n        dataset (list): the dataset to consider;\n        n_samples (int): the number of samples to pick from the dataset;\n        parse (bool, False): whether to parse the dataset to return.\n\n    Returns:\n        A subset of the dataset containing randomly picked ``n_samples``\n        samples.\n\n    """"""\n    assert n_samples >= 0, \'Number of samples must be greater than or equal\' \\\n                           \'to zero.\'\n    if n_samples == 0:\n        return np.array([[]])\n\n    dataset = np.array(dataset)\n    idxs = np.random.randint(dataset.shape[0], size=n_samples)\n    sub_dataset = dataset[idxs, ...]\n\n    return sub_dataset if not parse else parse_dataset(sub_dataset)\n\n\ndef compute_J(dataset, gamma=1.):\n    """"""\n    Compute the cumulative discounted reward of each episode in the dataset.\n\n    Args:\n        dataset (list): the dataset to consider;\n        gamma (float, 1.): discount factor.\n\n    Returns:\n        The cumulative discounted reward of each episode in the dataset.\n\n    """"""\n    js = list()\n\n    j = 0.\n    episode_steps = 0\n    for i in range(len(dataset)):\n        j += gamma ** episode_steps * dataset[i][2]\n        episode_steps += 1\n        if dataset[i][-1] or i == len(dataset) - 1:\n            js.append(j)\n            j = 0.\n            episode_steps = 0\n\n    if len(js) == 0:\n        return [0.]\n    return js\n\n\ndef compute_metrics(dataset, gamma=1.):\n    """"""\n    Compute the metrics of each complete episode in the dataset.\n\n    Args:\n        dataset (list): the dataset to consider;\n        gamma (float, 1.): the discount factor.\n\n    Returns:\n        The minimum score reached in an episode,\n        the maximum score reached in an episode,\n        the mean score reached,\n        the number of completed games.\n\n        If episode has not been completed, it returns 0 for all values.\n\n    """"""\n    for i in reversed(range(len(dataset))):\n        if dataset[i][-1]:\n            i += 1\n            break\n\n    dataset = dataset[:i]\n\n    if len(dataset) > 0:\n        J = compute_J(dataset, gamma)\n        return np.min(J), np.max(J), np.mean(J), len(J)\n    else:\n        return 0, 0, 0, 0\n'"
mushroom_rl/utils/eligibility_trace.py,0,"b'from mushroom_rl.utils.table import Table\n\n\ndef EligibilityTrace(shape, name=\'replacing\'):\n    """"""\n    Factory method to create an eligibility trace of the provided type.\n\n    Args:\n        shape (list): shape of the eligibility trace table;\n        name (str, \'replacing\'): type of the eligibility trace.\n\n    Returns:\n        The eligibility trace table of the provided shape and type.\n\n    """"""\n    if name == \'replacing\':\n        return ReplacingTrace(shape)\n    elif name == \'accumulating\':\n        return AccumulatingTrace(shape)\n    else:\n        raise ValueError(\'Unknown type of trace.\')\n\n\nclass ReplacingTrace(Table):\n    """"""\n    Replacing trace.\n\n    """"""\n    def reset(self):\n        self.table[:] = 0.\n\n    def update(self, state, action):\n        self.table[state, action] = 1.\n\n\nclass AccumulatingTrace(Table):\n    """"""\n    Accumulating trace.\n\n    """"""\n    def reset(self):\n        self.table[:] = 0.\n\n    def update(self, state, action):\n        self.table[state, action] += 1.\n'"
mushroom_rl/utils/features.py,0,"b'import numpy as np\n\n\ndef uniform_grid(n_centers, low, high):\n    """"""\n    This function is used to create the parameters of uniformly spaced radial\n    basis functions with 25% of overlap. It creates a uniformly spaced grid of\n    ``n_centers[i]`` points in each ``ranges[i]``. Also returns a vector\n    containing the appropriate scales of the radial basis functions.\n\n    Args:\n         n_centers (list): number of centers of each dimension;\n         low (np.ndarray): lowest value for each dimension;\n         high (np.ndarray): highest value for each dimension.\n\n    Returns:\n        The uniformly spaced grid and the scale vector.\n\n    """"""\n    n_features = len(low)\n    b = np.zeros(n_features)\n    c = list()\n    tot_points = 1\n    for i, n in enumerate(n_centers):\n        start = low[i]\n        end = high[i]\n\n        b[i] = (end - start) ** 2 / n ** 3\n        m = abs(start - end) / n\n        if n == 1:\n            c_i = (start + end) / 2.\n            c.append(np.array([c_i]))\n        else:\n            c_i = np.linspace(start - m * .1, end + m * .1, n)\n            c.append(c_i)\n        tot_points *= n\n\n    n_rows = 1\n    n_cols = 0\n\n    grid = np.zeros((tot_points, n_features))\n\n    for discrete_values in c:\n        i1 = 0\n        dim = len(discrete_values)\n\n        for i in range(dim):\n            for r in range(n_rows):\n                idx_r = r + i * n_rows\n                for c in range(n_cols):\n                    grid[idx_r, c] = grid[r, c]\n                grid[idx_r, n_cols] = discrete_values[i1]\n\n            i1 += 1\n\n        n_cols += 1\n        n_rows *= len(discrete_values)\n\n    return grid, b\n'"
mushroom_rl/utils/folder.py,0,"b'import errno\nimport os\nfrom os.path import join as join_paths\n\n\ndef mk_dir_recursive(dir_path):\n    """"""\n    Create a directory and, if needed, all the directory tree. Differently from\n    os.mkdir, this function does not raise exception when the directory already\n    exists.\n\n    Args:\n        dir_path (str): the path of the directory to create.\n\n    """"""\n    if os.path.isdir(dir_path):\n        return\n    h, t = os.path.split(dir_path)  # head/tail\n    if not os.path.isdir(h):\n        mk_dir_recursive(h)\n\n    new_path = join_paths(h, t)\n    if not os.path.isdir(new_path):\n        os.mkdir(new_path)\n\n\ndef force_symlink(src, dst):\n    """"""\n    Create a symlink deleting the previous one, if it already exists.\n\n    Args:\n        src (str): source;\n        dst (str): destination.\n\n    """"""\n    src = src.rstrip(\'/\')\n    dst = dst.rstrip(\'/\')\n\n    try:\n        os.symlink(src, dst)\n    except OSError as e:\n        if e.errno == errno.EEXIST:\n            os.remove(dst)\n            os.symlink(src, dst)\n'"
mushroom_rl/utils/minibatches.py,0,"b'import numpy as np\n\n\ndef minibatch_number(size, batch_size):\n    """"""\n    Function to retrieve the number of batches, given a batch sizes.\n\n    Args:\n        size (int): size of the dataset;\n        batch_size (int): size of the batches.\n\n    Returns:\n        The number of minibatches in the dataset.\n\n    """"""\n    return int(np.ceil(size / batch_size))\n\n\ndef minibatch_generator(batch_size, *dataset):\n    """"""\n    Generator that creates a minibatch from the full dataset.\n\n    Args:\n        batch_size (int): the maximum size of each minibatch;\n        dataset: the dataset to be splitted.\n\n    Returns:\n        The current minibatch.\n\n    """"""\n    size = len(dataset[0])\n    num_batches = minibatch_number(size, batch_size)\n    indexes = np.arange(0, size, 1)\n    np.random.shuffle(indexes)\n    batches = [(i * batch_size, min(size, (i + 1) * batch_size))\n               for i in range(0, num_batches)]\n\n    for (batch_start, batch_end) in batches:\n        batch = []\n        for i in range(len(dataset)):\n            batch.append(dataset[i][indexes[batch_start:batch_end]])\n        yield batch\n'"
mushroom_rl/utils/numerical_gradient.py,0,"b'import numpy as np\n\n\ndef numerical_diff_policy(policy, state, action, eps=1e-6):\n    """"""\n    Compute the gradient of a policy in (``state``, ``action``) numerically.\n\n    Args:\n        policy (Policy): the policy whose gradient has to be returned;\n        state (np.ndarray): the state;\n        action (np.ndarray): the action;\n        eps (float, 1e-6): the value of the perturbation.\n\n    Returns:\n        The gradient of the provided policy in (``state``, ``action``)\n        computed numerically.\n\n    """"""\n    w_start = policy.get_weights()\n\n    g = np.zeros(policy.weights_size)\n    for i in range(len(w_start)):\n        perturb = np.zeros(policy.weights_size)\n        perturb[i] = eps\n\n        policy.set_weights(w_start - perturb)\n        v1 = policy(state, action)\n\n        policy.set_weights(w_start + perturb)\n        v2 = policy(state, action)\n\n        g[i] = (v2 - v1) / (2 * eps)\n\n    policy.set_weights(w_start)\n\n    return g\n\n\ndef numerical_diff_dist(dist, theta, eps=1e-6):\n    """"""\n    Compute the gradient of a distribution in ``theta`` numerically.\n\n    Args:\n        dist (Distribution): the distribution whose gradient has to be returned;\n        theta (np.ndarray): the parametrization where to compute the gradient;\n        eps (float, 1e-6): the value of the perturbation.\n\n    Returns:\n        The gradient of the provided distribution ``theta`` computed\n        numerically.\n\n    """"""\n    rho_start = dist.get_parameters()\n\n    g = np.zeros(dist.parameters_size)\n    for i in range(len(rho_start)):\n        perturb = np.zeros(dist.parameters_size)\n        perturb[i] = eps\n\n        dist.set_parameters(rho_start - perturb)\n        v1 = dist(theta)\n\n        dist.set_parameters(rho_start + perturb)\n        v2 = dist(theta)\n\n        g[i] = (v2 - v1) / (2 * eps)\n\n    dist.set_parameters(rho_start)\n\n    return g'"
mushroom_rl/utils/parameters.py,0,"b'from mushroom_rl.utils.table import Table\nimport numpy as np\n\n\nclass Parameter(object):\n    """"""\n    This class implements function to manage parameters, such as learning rate.\n    It also allows to have a single parameter for each state of state-action\n    tuple.\n\n    """"""\n    def __init__(self, value, min_value=None, max_value=None, size=(1,)):\n        """"""\n        Constructor.\n\n        Args:\n            value (float): initial value of the parameter;\n            min_value (float, None): minimum value that the parameter can reach\n                when decreasing;\n            max_value (float, None): maximum value that the parameter can reach\n                when increasing;\n            size (tuple, (1,)): shape of the matrix of parameters; this shape\n                can be used to have a single parameter for each state or\n                state-action tuple.\n\n        """"""\n        self._initial_value = value\n        self._min_value = min_value\n        self._max_value = max_value\n        self._n_updates = Table(size)\n\n    def __call__(self, *idx, **kwargs):\n        """"""\n        Update and return the parameter in the provided index.\n\n        Args:\n             *idx (list): index of the parameter to return.\n\n        Returns:\n            The updated parameter in the provided index.\n\n        """"""\n        if self._n_updates.table.size == 1:\n            idx = list()\n\n        self.update(*idx, **kwargs)\n\n        return self.get_value(*idx, **kwargs)\n\n    def get_value(self, *idx, **kwargs):\n        """"""\n        Return the current value of the parameter in the provided index.\n\n        Args:\n            *idx (list): index of the parameter to return.\n\n        Returns:\n            The current value of the parameter in the provided index.\n\n        """"""\n        new_value = self._compute(*idx, **kwargs)\n\n        if self._min_value is None and self._max_value is None:\n            return new_value\n        else:\n            return np.clip(new_value, self._min_value, self._max_value)\n\n    def _compute(self, *idx, **kwargs):\n        """"""\n        Returns:\n            The value of the parameter in the provided index.\n\n        """"""\n        return self._initial_value\n\n    def update(self, *idx, **kwargs):\n        """"""\n        Updates the number of visit of the parameter in the provided index.\n\n        Args:\n            *idx (list): index of the parameter whose number of visits has to be\n                updated.\n\n        """"""\n        self._n_updates[idx] += 1\n\n    @property\n    def shape(self):\n        """"""\n        Returns:\n            The shape of the table of parameters.\n\n        """"""\n        return self._n_updates.table.shape\n\n\nclass LinearParameter(Parameter):\n    """"""\n    This class implements a linearly changing parameter according to the number\n    of times it has been used.\n\n    """"""\n    def __init__(self, value, threshold_value, n, size=(1,)):\n        self._coeff = (threshold_value - value) / n\n\n        if self._coeff >= 0:\n            super().__init__(value, None, threshold_value, size)\n        else:\n            super().__init__(value, threshold_value, None, size)\n\n    def _compute(self, *idx, **kwargs):\n        return self._coeff * self._n_updates[idx] + self._initial_value\n\n\nclass ExponentialParameter(Parameter):\n    """"""\n    This class implements a exponentially changing parameter according to the\n    number of times it has been used.\n\n    """"""\n    def __init__(self, value, exp=1., min_value=None, max_value=None,\n                 size=(1,)):\n        self._exp = exp\n\n        super().__init__(value, min_value, max_value, size)\n\n    def _compute(self, *idx, **kwargs):\n        n = np.maximum(self._n_updates[idx], 1)\n\n        return self._initial_value / n ** self._exp\n\n\nclass AdaptiveParameter(object):\n    """"""\n    This class implements a basic adaptive gradient step. Instead of moving of\n    a step proportional to the gradient, takes a step limited by a given metric.\n    To specify the metric, the natural gradient has to be provided. If natural\n    gradient is not provided, the identity matrix is used.\n\n    The step rule is:\n\n    .. math::\n        \\\\Delta\\\\theta=\\\\underset{\\\\Delta\\\\vartheta}{argmax}\\\\Delta\\\\vartheta^{t}\\\\nabla_{\\\\theta}J\n\n        s.t.:\\\\Delta\\\\vartheta^{T}M\\\\Delta\\\\vartheta\\\\leq\\\\varepsilon\n\n    Lecture notes, Neumann G.\n    http://www.ias.informatik.tu-darmstadt.de/uploads/Geri/lecture-notes-constraint.pdf\n\n    """"""\n    def __init__(self, value):\n        self._eps = value\n\n    def __call__(self, *args, **kwargs):\n        return self.get_value(*args, **kwargs)\n\n    def get_value(self, *args, **kwargs):\n        if len(args) == 2:\n            gradient = args[0]\n            nat_gradient = args[1]\n            tmp = (gradient.dot(nat_gradient)).item()\n            lambda_v = np.sqrt(tmp / (4. * self._eps))\n            # For numerical stability\n            lambda_v = max(lambda_v, 1e-8)\n            step_length = 1. / (2. * lambda_v)\n\n            return step_length\n        elif len(args) == 1:\n            return self.get_value(args[0], args[0], **kwargs)\n        else:\n            raise ValueError(\'Adaptive parameters needs gradient or gradient\'\n                             \'and natural gradient\')\n\n    @property\n    def shape(self):\n        return None\n'"
mushroom_rl/utils/preprocessors.py,0,"b'import pickle\nimport numpy as np\n\nfrom mushroom_rl.utils.running_stats import RunningStandardization\n\n\nclass StandardizationPreprocessor(object):\n    """"""\n    Preprocess observations from the environment using a running\n    standardization.\n\n    """"""\n    def __init__(self, mdp_info, clip_obs=10., alpha=1e-32):\n        """"""\n        Constructor.\n\n        Args:\n            mdp_info (MDPInfo): information of the MDP;\n            clip_obs (float, 10.): values to clip the normalized observations;\n            alpha (float, 1e-32): moving average catchup parameter for the\n                normalization.\n\n        """"""\n        self.clip_obs = clip_obs\n        self.obs_shape = mdp_info.observation_space.shape\n        self.obs_runstand = RunningStandardization(shape=self.obs_shape,\n                                                   alpha=alpha)\n\n    def __call__(self, obs):\n        """"""\n        Call function to normalize the observation.\n\n        Args:\n            obs (np.ndarray): observation to be normalized.\n\n        Returns:\n            Normalized observation array with the same shape.\n\n        """"""\n        assert obs.shape == self.obs_shape, \\\n            ""Values given to running_norm have incorrect shape "" \\\n            ""(obs shape: {},  expected shape: {})"" \\\n            .format(obs.shape, self.obs_shape)\n\n        self.obs_runstand.update_stats(obs)\n        norm_obs = np.clip(\n            (obs - self.obs_runstand.mean) / self.obs_runstand.std,\n            -self.clip_obs, self.clip_obs\n        )\n\n        return norm_obs\n\n    def get_state(self):\n        """"""\n        Returns:\n            A dictionary with the normalization state.\n\n        """"""\n        return self.obs_runstand.get_state()\n\n    def set_state(self, data):\n        """"""\n        Set the current normalization state from the data dict.\n\n        """"""\n        self.obs_runstand.set_state(data)\n\n    def save_state(self, path):\n        """"""\n        Save the running normalization state to path.\n\n        Args:\n            path (str): path to save the running normalization state.\n\n        """"""\n        with open(path, \'wb\') as f:\n            pickle.dump(self.get_state(), f, protocol=3)\n\n    def load_state(self, path):\n        """"""\n        Load the running normalization state from path.\n\n        Args:\n            path (string): path to load the running normalization state from.\n\n        """"""\n        with open(path, \'rb\') as f:\n            data = pickle.load(f)\n            self.set_state(data)\n\n\nclass MinMaxPreprocessor(StandardizationPreprocessor):\n    """"""\n    Preprocess observations from the environment using the bounds of the\n    observation space of the environment. For observations that are not limited\n    falls back to using running mean standardization.\n\n    """"""\n    def __init__(self, mdp_info, clip_obs=10., alpha=1e-32):\n        """"""\n        Constructor.\n\n        Args:\n            mdp_info (MDPInfo): information of the MDP;\n            clip_obs (float, 10.): values to clip the normalized observations;\n            alpha (float, 1e-32): moving average catchup parameter for the\n                normalization.\n\n        """"""\n        super(MinMaxPreprocessor, self).__init__(mdp_info, clip_obs,\n                                                 alpha)\n\n        obs_low, obs_high = (mdp_info.observation_space.low.copy(),\n                             mdp_info.observation_space.high.copy())\n\n        self.stand_obs_mask = np.where(\n            ~(np.isinf(obs_low) | np.isinf(obs_high))\n        )\n\n        assert np.squeeze(self.stand_obs_mask).size > 0, \\\n            ""All observations have unlimited range, you should use "" \\\n            ""StandardizationPreprocessor directly instead.""\n\n        self.run_norm_obs = len(np.squeeze(self.stand_obs_mask)) != obs_low.shape[0]\n\n        self.obs_mean = np.zeros_like(obs_low)\n        self.obs_delta = np.ones_like(obs_low)\n        self.obs_mean[self.stand_obs_mask] = (\n            obs_high[self.stand_obs_mask] + obs_low[self.stand_obs_mask]) / 2.\n        self.obs_delta[self.stand_obs_mask] = (\n            obs_high[self.stand_obs_mask] - obs_low[self.stand_obs_mask]) / 2.\n\n    def __call__(self, obs):\n        """"""\n        Call function to normalize the observation.\n\n        Args:\n            obs (np.ndarray): observation to be normalized.\n\n        Returns:\n            Normalized observation array with the same shape.\n\n        """"""\n        orig_obs = obs.copy()\n\n        if self.run_norm_obs:\n            obs = super(MinMaxPreprocessor, self).__call__(obs)\n\n        obs[self.stand_obs_mask] = \\\n            ((orig_obs - self.obs_mean) / self.obs_delta)[self.stand_obs_mask]\n\n        return obs\n'"
mushroom_rl/utils/replay_memory.py,0,"b'import numpy as np\n\n\nclass ReplayMemory(object):\n    """"""\n    This class implements function to manage a replay memory as the one used in\n    ""Human-Level Control Through Deep Reinforcement Learning"" by Mnih V. et al..\n\n    """"""\n    def __init__(self, initial_size, max_size):\n        """"""\n        Constructor.\n\n        Args:\n            initial_size (int): initial number of elements in the replay memory;\n            max_size (int): maximum number of elements that the replay memory\n                can contain.\n\n        """"""\n        self._initial_size = initial_size\n        self._max_size = max_size\n\n        self.reset()\n\n    def add(self, dataset):\n        """"""\n        Add elements to the replay memory.\n\n        Args:\n            dataset (list): list of elements to add to the replay memory.\n\n        """"""\n        for i in range(len(dataset)):\n            self._states[self._idx] = dataset[i][0]\n            self._actions[self._idx] = dataset[i][1]\n            self._rewards[self._idx] = dataset[i][2]\n            self._next_states[self._idx] = dataset[i][3]\n            self._absorbing[self._idx] = dataset[i][4]\n            self._last[self._idx] = dataset[i][5]\n\n            self._idx += 1\n            if self._idx == self._max_size:\n                self._full = True\n                self._idx = 0\n\n    def get(self, n_samples):\n        """"""\n        Returns the provided number of states from the replay memory.\n\n        Args:\n            n_samples (int): the number of samples to return.\n\n        Returns:\n            The requested number of samples.\n\n        """"""\n        s = list()\n        a = list()\n        r = list()\n        ss = list()\n        ab = list()\n        last = list()\n        for i in np.random.randint(self.size, size=n_samples):\n            s.append(np.array(self._states[i]))\n            a.append(self._actions[i])\n            r.append(self._rewards[i])\n            ss.append(np.array(self._next_states[i]))\n            ab.append(self._absorbing[i])\n            last.append(self._last[i])\n\n        return np.array(s), np.array(a), np.array(r), np.array(ss),\\\n            np.array(ab), np.array(last)\n\n    def reset(self):\n        """"""\n        Reset the replay memory.\n\n        """"""\n        self._idx = 0\n        self._full = False\n        self._states = [None for _ in range(self._max_size)]\n        self._actions = [None for _ in range(self._max_size)]\n        self._rewards = [None for _ in range(self._max_size)]\n        self._next_states = [None for _ in range(self._max_size)]\n        self._absorbing = [None for _ in range(self._max_size)]\n        self._last = [None for _ in range(self._max_size)]\n\n    @property\n    def initialized(self):\n        """"""\n        Returns:\n            Whether the replay memory has reached the number of elements that\n            allows it to be used.\n\n        """"""\n        return self.size > self._initial_size\n\n    @property\n    def size(self):\n        """"""\n        Returns:\n            The number of elements contained in the replay memory.\n\n        """"""\n        return self._idx if not self._full else self._max_size\n\n\nclass SumTree(object):\n    """"""\n    This class implements a sum tree data structure.\n    This is used, for instance, by ``PrioritizedReplayMemory``.\n\n    """"""\n    def __init__(self, max_size):\n        """"""\n        Constructor.\n\n        Args:\n            max_size (int): maximum size of the tree.\n\n        """"""\n        self._max_size = max_size\n        self._tree = np.zeros(2 * max_size - 1)\n        self._data = [None for _ in range(max_size)]\n        self._idx = 0\n        self._full = False\n\n    def add(self, dataset, priority):\n        """"""\n        Add elements to the tree.\n\n        Args:\n            dataset (list): list of elements to add to the tree;\n            p (np.ndarray): priority of each sample in the dataset.\n\n        """"""\n        for d, p in zip(dataset, priority):\n            idx = self._idx + self._max_size - 1\n\n            self._data[self._idx] = d\n            self.update([idx], [p])\n\n            self._idx += 1\n            if self._idx == self._max_size:\n                self._idx = 0\n                self._full = True\n\n    def get(self, s):\n        """"""\n        Returns the provided number of states from the replay memory.\n\n        Args:\n            s (float): the value of the samples to return.\n\n        Returns:\n            The requested sample.\n\n        """"""\n        idx = self._retrieve(s, 0)\n        data_idx = idx - self._max_size + 1\n\n        return idx, self._tree[idx], self._data[data_idx]\n\n    def update(self, idx, priorities):\n        """"""\n        Update the priority of the sample at the provided index in the dataset.\n\n        Args:\n            idx (np.ndarray): indexes of the transitions in the dataset;\n            priorities (np.ndarray): priorities of the transitions.\n\n        """"""\n        for i, p in zip(idx, priorities):\n            delta = p - self._tree[i]\n\n            self._tree[i] = p\n            self._propagate(delta, i)\n\n    def _propagate(self, delta, idx):\n        parent_idx = (idx - 1) // 2\n\n        self._tree[parent_idx] += delta\n\n        if parent_idx != 0:\n            self._propagate(delta, parent_idx)\n\n    def _retrieve(self, s, idx):\n        left = 2 * idx + 1\n        right = left + 1\n\n        if left >= len(self._tree):\n            return idx\n\n        if self._tree[left] == self._tree[right]:\n            return self._retrieve(s, np.random.choice([left, right]))\n\n        if s <= self._tree[left]:\n            return self._retrieve(s, left)\n        else:\n            return self._retrieve(s - self._tree[left], right)\n\n    @property\n    def size(self):\n        """"""\n        Returns:\n            The current size of the tree.\n\n        """"""\n        return self._idx if not self._full else self._max_size\n\n    @property\n    def max_p(self):\n        """"""\n        Returns:\n            The maximum priority among the ones in the tree.\n\n        """"""\n        return self._tree[-self._max_size:].max()\n\n    @property\n    def total_p(self):\n        """"""\n        Returns:\n            The sum of the priorities in the tree, i.e. the value of the root\n            node.\n\n        """"""\n        return self._tree[0]\n\n\nclass PrioritizedReplayMemory(object):\n    """"""\n    This class implements function to manage a prioritized replay memory as the\n    one used in ""Prioritized Experience Replay"" by Schaul et al., 2015.\n\n    """"""\n    def __init__(self, initial_size, max_size, alpha, beta, epsilon=.01):\n        """"""\n        Constructor.\n\n        Args:\n            initial_size (int): initial number of elements in the replay\n                memory;\n            max_size (int): maximum number of elements that the replay memory\n                can contain;\n            alpha (float): prioritization coefficient;\n            beta (float): importance sampling coefficient;\n            epsilon (float, .01): small value to avoid zero probabilities.\n\n        """"""\n        self._initial_size = initial_size\n        self._max_size = max_size\n        self._alpha = alpha\n        self._beta = beta\n        self._epsilon = epsilon\n\n        self._tree = SumTree(max_size)\n\n    def add(self, dataset, p):\n        """"""\n        Add elements to the replay memory.\n\n        Args:\n            dataset (list): list of elements to add to the replay memory;\n            p (np.ndarray): priority of each sample in the dataset.\n\n        """"""\n        self._tree.add(dataset, p)\n\n    def get(self, n_samples):\n        """"""\n        Returns the provided number of states from the replay memory.\n\n        Args:\n            n_samples (int): the number of samples to return.\n\n        Returns:\n            The requested number of samples.\n\n        """"""\n        states = [None for _ in range(n_samples)]\n        actions = [None for _ in range(n_samples)]\n        rewards = [None for _ in range(n_samples)]\n        next_states = [None for _ in range(n_samples)]\n        absorbing = [None for _ in range(n_samples)]\n        last = [None for _ in range(n_samples)]\n\n        idxs = np.zeros(n_samples, dtype=np.int)\n        priorities = np.zeros(n_samples)\n\n        total_p = self._tree.total_p\n        segment = total_p / n_samples\n\n        a = np.arange(n_samples) * segment\n        b = np.arange(1, n_samples + 1) * segment\n        samples = np.random.uniform(a, b)\n        for i, s in enumerate(samples):\n            idx, p, data = self._tree.get(s)\n\n            idxs[i] = idx\n            priorities[i] = p\n            states[i], actions[i], rewards[i], next_states[i], absorbing[i],\\\n                last[i] = data\n            states[i] = np.array(states[i])\n            next_states[i] = np.array(next_states[i])\n\n        sampling_probabilities = priorities / self._tree.total_p\n        is_weight = (self._tree.size * sampling_probabilities) ** -self._beta()\n        is_weight /= is_weight.max()\n\n        return np.array(states), np.array(actions), np.array(rewards),\\\n            np.array(next_states), np.array(absorbing), np.array(last),\\\n            idxs, is_weight\n\n    def update(self, error, idx):\n        """"""\n        Update the priority of the sample at the provided index in the dataset.\n\n        Args:\n            error (np.ndarray): errors to consider to compute the priorities;\n            idx (np.ndarray): indexes of the transitions in the dataset.\n\n        """"""\n        p = self._get_priority(error)\n        self._tree.update(idx, p)\n\n    def _get_priority(self, error):\n        return (np.abs(error) + self._epsilon) ** self._alpha\n\n    @property\n    def initialized(self):\n        """"""\n        Returns:\n            Whether the replay memory has reached the number of elements that\n            allows it to be used.\n\n        """"""\n        return self._tree.size > self._initial_size\n\n    @property\n    def max_priority(self):\n        """"""\n        Returns:\n            The maximum value of priority inside the replay memory.\n\n        """"""\n        return self._tree.max_p if self.initialized else 1.\n'"
mushroom_rl/utils/running_stats.py,0,"b'import numpy as np\nfrom collections import deque\n\n\nclass RunningStandardization:\n    """"""\n    Compute a running standardization of values according to Welford\'s online\n    algorithm: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford\'s_online_algorithm\n\n    """"""\n    def __init__(self, shape, alpha=1e-32):\n        """"""\n        Constructor.\n\n        Args:\n            shape (tuple): shape of the data to standardize;\n            alpha (float, 1e-32): minimum learning rate.\n\n        """"""\n        self._shape = shape\n\n        assert 0. < alpha < 1.\n        self._alpha = alpha\n\n        self._n = 1\n        self._m = np.zeros(self._shape)\n        self._s = np.ones(self._shape)\n\n    def reset(self):\n        """"""\n        Reset the mean and standard deviation.\n\n        """"""\n        self._n = 1\n        self._m = np.zeros(self._shape)\n        self._s = np.ones(self._shape)\n\n    def update_stats(self, value):\n        """"""\n        Update the statistics with the current data value.\n\n        Args:\n            value (np.ndarray): current data value to use for the update.\n\n        """"""\n        self._n += 1\n        alpha = max(1. / self._n, self._alpha)\n        new_m = (1 - alpha) * self._m + alpha * value\n        new_s = self._s + (value - self._m) * (value - new_m)\n        self._m, self._s = new_m, new_s\n\n    def get_state(self):\n        """"""\n        Returns:\n            A dictionary containing the state of the filter.\n\n        """"""\n        return dict(mean=self._m, var=self._s, count=self._n)\n\n    def set_state(self, state):\n        """"""\n        Set the state of the filter.\n\n        """"""\n        self._m = state[""mean""]\n        self._s = state[""var""]\n        self._n = state[""count""]\n\n    @property\n    def mean(self):\n        """"""\n        Returns:\n            The estimated mean value.\n\n        """"""\n        return self._m\n\n    @property\n    def std(self):\n        """"""\n        Returns:\n            The estimated standard deviation value.\n\n        """"""\n        return np.sqrt(self._s / self._n)\n\n\nclass RunningExpWeightedAverage:\n    """"""\n    Compute an exponentially weighted moving average.\n\n    """"""\n    def __init__(self, shape, alpha, init_value=None):\n        """"""\n        Constructor.\n\n        Args:\n             shape (tuple): shape of the data to standardize;\n             alpha (float): learning rate;\n             init_value (np.ndarray): initial value of the filter.\n\n        """"""\n        self._shape = shape\n        self._alpha = alpha\n        self.reset(init_value)\n\n    def reset(self, init_value=None):\n        """"""\n        Reset the mean and standard deviation.\n\n        Args:\n            init_value (np.ndarray): initial value of the filter.\n\n        """"""\n        if init_value is None:\n            self._avg_value = np.zeros(self._shape)\n        else:\n            self._avg_value = init_value\n\n    def update_stats(self, value):\n        """"""\n        Update the statistics with the current data value.\n\n        Args:\n            value (np.ndarray): current data value to use for the update.\n\n        """"""\n        self._avg_value = (\n            1. - self._alpha) * self._avg_value + self._alpha * value\n\n    @property\n    def mean(self):\n        """"""\n        Returns:\n            The estimated mean value.\n\n        """"""\n        return self._avg_value\n\n\nclass RunningAveragedWindow:\n    """"""\n    Compute the running average using a window of fixed size.\n\n    """"""\n    def __init__(self, shape, window_size, init_value=None):\n        """"""\n        Constructor.\n\n        Args:\n             shape (tuple): shape of the data to standardize;\n             window_size (int): size of the windos;\n             init_value (np.ndarray): initial value of the filter.\n\n        """"""\n        self.shape = shape\n        self.window_size = window_size\n        self.reset(init_value)\n\n    def reset(self, init_value=None):\n        """"""\n        Reset the window.\n\n        Args:\n            init_value (np.ndarray): initial value of the filter.\n\n        """"""\n        if init_value is None:\n            self.avg_buffer = deque(np.zeros((1, *self.shape)),\n                                    maxlen=self.window_size)\n        else:\n            self.avg_buffer = deque([init_value], maxlen=self.window_size)\n\n    def update_stats(self, value):\n        """"""\n        Update the statistics with the current data value.\n\n        Args:\n            value (np.ndarray): current data value to use for the update.\n\n        """"""\n        self.avg_buffer.append(value)\n\n    @property\n    def mean(self):\n        """"""\n        Returns:\n            The estimated mean value.\n\n        """"""\n        return np.mean(self.avg_buffer, axis=0)\n'"
mushroom_rl/utils/spaces.py,0,"b'import numpy as np\n\n\nclass Box:\n    """"""\n    This class implements functions to manage continuous states and action\n    spaces. It is similar to the ``Box`` class in ``gym.spaces.box``.\n\n    """"""\n    def __init__(self, low, high, shape=None):\n        """"""\n        Constructor.\n\n        Args:\n            low ([float, np.ndarray]): the minimum value of each dimension of\n                the space. If a scalar value is provided, this value is\n                considered as the minimum one for each dimension. If a\n                np.ndarray is provided, each i-th element is considered the\n                minimum value of the i-th dimension;\n            high ([float, np.ndarray]): the maximum value of dimensions of the\n                space. If a scalar value is provided, this value is considered\n                as the maximum one for each dimension. If a np.ndarray is\n                provided, each i-th element is considered the maximum value\n                of the i-th dimension;\n            shape (np.ndarray, None): the dimension of the space. Must match\n                the shape of ``low`` and ``high``, if they are np.ndarray.\n\n        """"""\n        if shape is None:\n            self._low = low\n            self._high = high\n            self._shape = low.shape\n        else:\n            self._low = low\n            self._high = high\n            self._shape = shape\n            if np.isscalar(low) and np.isscalar(high):\n                self._low += np.zeros(shape)\n                self._high += np.zeros(shape)\n\n        assert self._low.shape == self._high.shape\n\n    @property\n    def low(self):\n        """"""\n        Returns:\n             The minimum value of each dimension of the space.\n\n        """"""\n        return self._low\n\n    @property\n    def high(self):\n        """"""\n        Returns:\n             The maximum value of each dimension of the space.\n\n        """"""\n        return self._high\n\n    @property\n    def shape(self):\n        """"""\n        Returns:\n            The dimensions of the space.\n\n        """"""\n        return self._shape\n\n\nclass Discrete:\n    """"""\n    This class implements functions to manage discrete states and action\n    spaces. It is similar to the ``Discrete`` class in ``gym.spaces.discrete``.\n\n    """"""\n    def __init__(self, n):\n        """"""\n        Constructor.\n\n        Args:\n            n (int): the number of values of the space.\n\n        """"""\n        self.values = np.arange(n)\n        self.n = n\n\n    @property\n    def size(self):\n        """"""\n        Returns:\n            The number of elements of the space.\n\n        """"""\n        return self.n,\n\n    @property\n    def shape(self):\n        """"""\n        Returns:\n            The shape of the space that is always (1,).\n\n        """"""\n        return 1,\n'"
mushroom_rl/utils/table.py,0,"b'import numpy as np\n\nfrom mushroom_rl.approximators import Ensemble\n\n\nclass Table:\n    """"""\n    Table regressor. Used for discrete state and action spaces.\n\n    """"""\n    def __init__(self, shape, initial_value=0., dtype=None):\n        """"""\n        Constructor.\n\n        Args:\n            shape (tuple): the shape of the tabular regressor.\n            initial_value (float, 0.): the initial value for each entry of the\n                tabular regressor.\n            dtype ([int, float], None): the dtype of the table array.\n\n        """"""\n        self.table = np.ones(shape, dtype=dtype) * initial_value\n\n    def __getitem__(self, args):\n        if self.table.size == 1:\n            return self.table[0]\n        else:\n            idx = tuple([\n                a[0] if isinstance(a, np.ndarray) else a for a in args])\n\n            return self.table[idx]\n\n    def __setitem__(self, args, value):\n        if self.table.size == 1:\n            self.table[0] = value\n        else:\n            idx = tuple([\n                a[0] if isinstance(a, np.ndarray) else a for a in args])\n            self.table[idx] = value\n\n    def fit(self, x, y):\n        """"""\n        Args:\n            x (int): index of the table to be filled;\n            y (float): value to fill in the table.\n\n        """"""\n        self[x] = y\n\n    def predict(self, *z):\n        """"""\n        Predict the output of the table given an input.\n\n        Args:\n            *z (list): list of input of the model. If the table is a Q-table,\n            this list may contain states or states and actions depending\n                on whether the call requires to predict all q-values or only\n                one q-value corresponding to the provided action;\n\n        Returns:\n            The table prediction.\n\n        """"""\n        if z[0].ndim == 1:\n            z = [np.expand_dims(z_i, axis=0) for z_i in z]\n        state = z[0]\n\n        values = list()\n        if len(z) == 2:\n            action = z[1]\n            for i in range(len(state)):\n                val = self[state[i], action[i]]\n                values.append(val)\n        else:\n            for i in range(len(state)):\n                val = self[state[i], :]\n                values.append(val)\n\n        if len(values) == 1:\n            return values[0]\n        else:\n            return np.array(values)\n\n    @property\n    def n_actions(self):\n        """"""\n        Returns:\n            The number of actions considered by the table.\n\n        """"""\n        return self.table.shape[-1]\n\n    @property\n    def shape(self):\n        """"""\n        Returns:\n            The shape of the table.\n\n        """"""\n        return self.table.shape\n\n\nclass EnsembleTable(Ensemble):\n    """"""\n    This class implements functions to manage table ensembles.\n\n    """"""\n    def __init__(self, n_models, shape):\n        """"""\n        Constructor.\n\n        Args:\n            n_models (int): number of models in the ensemble;\n            shape (np.ndarray): shape of each table in the ensemble.\n\n        """"""\n        approximator_params = dict(shape=shape)\n        super(EnsembleTable, self).__init__(Table, n_models,\n                                            **approximator_params)\n\n    @property\n    def n_actions(self):\n        return self._model[0].shape[-1]\n'"
mushroom_rl/utils/torch.py,4,"b'import torch\nimport numpy as np\n\n\ndef set_weights(parameters, weights, use_cuda):\n    """"""\n    Function used to set the value of a set of torch parameters given a\n    vector of values.\n\n    Args:\n        parameters (list): list of parameters to be considered;\n        weights (numpy.ndarray): array of the new values for\n            the parameters;\n        use_cuda (bool): whether the parameters are cuda tensors or not;\n\n    """"""\n    idx = 0\n    for p in parameters:\n        shape = p.data.shape\n\n        c = 1\n        for s in shape:\n            c *= s\n\n        w = np.reshape(weights[idx:idx + c], shape)\n\n        if not use_cuda:\n            w_tensor = torch.from_numpy(w).type(p.data.dtype)\n        else:\n            w_tensor = torch.from_numpy(w).type(p.data.dtype).cuda()\n\n        p.data = w_tensor\n        idx += c\n\n    assert idx == weights.size\n\n\ndef get_weights(parameters):\n    """"""\n    Function used to get the value of a set of torch parameters as\n    a single vector of values.\n\n    Args:\n        parameters (list): list of parameters to be considered.\n\n    Returns:\n        A numpy vector consisting of all the values of the vectors.\n\n    """"""\n    weights = list()\n\n    for p in parameters:\n        w = p.data.detach().cpu().numpy()\n        weights.append(w.flatten())\n\n    weights = np.concatenate(weights, 0)\n\n    return weights\n\n\ndef zero_grad(parameters):\n    """"""\n    Function used to set to zero the value of the gradient of a set\n    of torch parameters.\n\n    Args:\n        parameters (list): list of parameters to be considered.\n\n    """"""\n\n    for p in parameters:\n        if p.grad is not None:\n           p.grad.detach_()\n           p.grad.zero_()\n\n\ndef get_gradient(params):\n    """"""\n    Function used to get the value of the gradient of a set of\n    torch parameters.\n\n    Args:\n        parameters (list): list of parameters to be considered.\n\n    """"""\n    views = []\n    for p in params:\n        if p.grad is None:\n            view = p.new(p.numel()).zero_()\n        else:\n            view = p.grad.view(-1)\n        views.append(view)\n    return torch.cat(views, 0)\n\n\ndef to_float_tensor(x, use_cuda=False):\n    """"""\n    Function used to convert a numpy array to a float torch tensor.\n\n    Args:\n        x (np.ndarray): numpy array to be converted as torch tensor;\n        use_cuda (bool): whether to build a cuda tensors or not.\n\n    Returns:\n        A float tensor build from the values contained in the input array.\n\n    """"""\n    x = torch.tensor(x, dtype=torch.float)\n    return x.cuda() if use_cuda else x\n'"
mushroom_rl/utils/value_functions.py,0,"b'import numpy as np\n\n\ndef compute_advantage_montecarlo(V, s, ss, r, absorbing, gamma):\n    """"""\n    Function to estimate the advantage and new value function target\n    over a dataset. The value function is estimated using rollouts\n    (monte carlo estimation).\n\n    Args:\n        V (Regressor): the current value function regressor;\n        s (numpy.ndarray): the set of states in which we want\n            to evaluate the advantage;\n        ss (numpy.ndarray): the set of next states in which we want\n            to evaluate the advantage;\n        r (numpy.ndarray): the reward obtained in each transition\n            from state s to state ss;\n        absorbing (numpy.ndarray): an array of boolean flags indicating\n            if the reached state is absorbing;\n        gamma (float): the discount factor of the considered problem.\n    Returns:\n        The new estimate for the value function of the next state\n        and the advantage function.\n    """"""\n    r = r.squeeze()\n    q = np.zeros(len(r))\n    v = V(s).squeeze()\n\n    q_next = V(ss[-1]).squeeze().item()\n    for rev_k in range(len(r)):\n        k = len(r) - rev_k - 1\n        q_next = r[k] + gamma * q_next * (1. - absorbing[k])\n        q[k] = q_next\n\n    adv = q - v\n    return q[:, np.newaxis], adv[:, np.newaxis]\n\n\ndef compute_advantage(V, s, ss, r, absorbing, gamma):\n    """"""\n    Function to estimate the advantage and new value function target\n    over a dataset. The value function is estimated using bootstrapping.\n\n    Args:\n        V (Regressor): the current value function regressor;\n        s (numpy.ndarray): the set of states in which we want\n            to evaluate the advantage;\n        ss (numpy.ndarray): the set of next states in which we want\n            to evaluate the advantage;\n        r (numpy.ndarray): the reward obtained in each transition\n            from state s to state ss;\n        absorbing (numpy.ndarray): an array of boolean flags indicating\n            if the reached state is absorbing;\n        gamma (float): the discount factor of the considered problem.\n    Returns:\n        The new estimate for the value function of the next state\n        and the advantage function.\n    """"""\n    v = V(s).squeeze()\n    v_next = V(ss).squeeze() * (1 - absorbing)\n\n    q = r + gamma * v_next\n    adv = q - v\n    return q[:, np.newaxis], adv[:, np.newaxis]\n\n\ndef compute_gae(V, s, ss, r, absorbing, last, gamma, lam):\n    """"""\n    Function to compute Generalized Advantage Estimation (GAE)\n    and new value function target over a dataset.\n\n    ""High-Dimensional Continuous Control Using Generalized\n    Advantage Estimation"".\n    Schulman J. et al.. 2016.\n\n    Args:\n        V (Regressor): the current value function regressor;\n        s (numpy.ndarray): the set of states in which we want\n            to evaluate the advantage;\n        ss (numpy.ndarray): the set of next states in which we want\n            to evaluate the advantage;\n        r (numpy.ndarray): the reward obtained in each transition\n            from state s to state ss;\n        absorbing (numpy.ndarray): an array of boolean flags indicating\n            if the reached state is absorbing;\n        last (numpy.ndarray): an array of boolean flags indicating\n            if the reached state is the last of the trajectory;\n        gamma (float): the discount factor of the considered problem;\n        lam (float): the value for the lamba coefficient used by GEA\n            algorithm.\n    Returns:\n        The new estimate for the value function of the next state\n        and the estimated generalized advantage.\n    """"""\n    v = V(s)\n    v_next = V(ss)\n    gen_adv = np.empty_like(v)\n    for rev_k in range(len(v)):\n        k = len(v) - rev_k - 1\n        if last[k] or rev_k == 0:\n            gen_adv[k] = r[k] - v[k]\n            if not absorbing[k]:\n                gen_adv[k] += gamma * v_next[k]\n        else:\n            gen_adv[k] = r[k] + gamma * v_next[k] - v[k] + gamma * lam * gen_adv[k + 1]\n    return gen_adv + v, gen_adv'"
mushroom_rl/utils/variance_parameters.py,0,"b'import numpy as np\n\nfrom mushroom_rl.utils.parameters import Parameter\nfrom mushroom_rl.utils.table import Table\n\n\nclass VarianceParameter(Parameter):\n    """"""\n    Abstract class to implement variance-dependent parameters. A ``target``\n    parameter is expected.\n\n    """"""\n    def __init__(self, value, exponential=False, min_value=None, tol=1.,\n                 size=(1,)):\n        """"""\n        Constructor.\n\n        Args:\n            tol (float): value of the variance of the target variable such that\n                The parameter value is 0.5.\n\n        """"""\n        self._exponential = exponential\n        self._tol = tol\n        self._weights_var = Table(size)\n        self._x = Table(size)\n        self._x2 = Table(size)\n        self._parameter_value = Table(size)\n\n        super().__init__(value, min_value, size)\n\n    def _compute(self, *idx, **kwargs):\n        return self._parameter_value[idx]\n\n    def update(self, *idx, **kwargs):\n        """"""\n        Updates the value of the parameter in the provided index.\n\n        Args:\n            *idx (list): index of the parameter whose number of visits has to be\n                updated.\n            target (float): Value of the target variable;\n            factor (float): Multiplicative factor for the parameter value, useful\n                when the parameter depend on another parameter value.\n\n        """"""\n        x = kwargs[\'target\']\n        factor = kwargs.get(\'factor\', 1.)\n\n        # compute parameter value\n        n = self._n_updates[idx]\n        self._n_updates[idx] += 1\n\n        if n < 2:\n            parameter_value = self._initial_value\n        else:\n            var = n * (self._x2[idx] - self._x[idx] ** 2) / (n - 1.)\n            var_estimator = var * self._weights_var[idx]\n            parameter_value = self._compute_parameter(var_estimator,\n                                                      sigma_process=var,\n                                                      index=idx)\n\n        # update state\n        self._x[idx] += (x - self._x[idx]) / self._n_updates[idx]\n        self._x2[idx] += (x ** 2 - self._x2[idx]) / self._n_updates[idx]\n        self._weights_var[idx] = (\n            1. - factor * parameter_value) ** 2 * self._weights_var[idx] + (\n            factor * parameter_value) ** 2\n        self._parameter_value[idx] = parameter_value\n\n    def _compute_parameter(self, sigma, **kwargs):\n        raise NotImplementedError(\'VarianceParameter is an abstract class.\')\n\n\nclass VarianceIncreasingParameter(VarianceParameter):\n    """"""\n    Class implementing a parameter that increases with the target\n    variance.\n\n    """"""\n    def _compute_parameter(self, sigma, **kwargs):\n        if self._exponential:\n            return 1 - np.exp(sigma * np.log(.5) / self._tol)\n        else:\n            return sigma / (sigma + self._tol)\n\n\nclass VarianceDecreasingParameter(VarianceParameter):\n    """"""\n    Class implementing a parameter that decreases with the target\n    variance.\n\n    """"""\n    def _compute_parameter(self, sigma, **kwargs):\n        if self._exponential:\n            return np.exp(sigma * np.log(.5) / self._tol)\n        else:\n            return 1. / (sigma + self._tol)\n\n\nclass WindowedVarianceParameter(Parameter):\n    """"""\n    Abstract class to implement variance-dependent parameters. A ``target``\n    parameter is expected. differently from the ""Variance Parameter"" class\n    the variance is computed in a window interval.\n\n    """"""\n    def __init__(self, value, exponential=False, min_value=None, tol=1.,\n                 window=100, size=(1,)):\n        """"""\n        Constructor.\n\n        Args:\n            tol (float): value of the variance of the target variable such that the\n                parameter value is 0.5.\n            window (int):\n        """"""\n        self._exponential = exponential\n        self._tol = tol\n        self._weights_var = Table(size)\n        self._samples = Table(size + (window,))\n        self._index = Table(size, dtype=int)\n        self._window = window\n        self._parameter_value = Table(size)\n\n        super(WindowedVarianceParameter, self).__init__(value, min_value, size)\n\n    def _compute(self, *idx, **kwargs):\n        return self._parameter_value[idx]\n\n    def update(self, *idx, **kwargs):\n        """"""\n        Updates the value of the parameter in the provided index.\n\n        Args:\n            *idx (list): index of the parameter whose number of visits has to be\n                updated.\n            target (float): Value of the target variable;\n            factor (float): Multiplicative factor for the parameter value, useful\n                when the parameter depend on another parameter value.\n\n        """"""\n        x = kwargs[\'target\']\n        factor = kwargs.get(\'factor\', 1.)\n\n        # compute parameter value\n        n = self._n_updates[idx]\n        self._n_updates[idx] += 1\n\n        if n < 2:\n            parameter_value = self._initial_value\n        else:\n            samples = self._samples[idx]\n\n            if n < self._window:\n                samples = samples[:int(n)]\n\n            var = np.var(samples)\n            var_estimator = var * self._weights_var[idx]\n            parameter_value = self._compute_parameter(var_estimator,\n                                                      sigma_process=var,\n                                                      index=idx)\n\n        # update state\n        index = np.array([self._index[idx]], dtype=int)\n        self._samples[idx + (index,)] = x\n        self._index[idx] += 1\n        if self._index[idx] >= self._window:\n            self._index[idx] = 0\n\n        self._weights_var[idx] = (\n            1. - factor*parameter_value) ** 2 * self._weights_var[idx] + (\n            factor * parameter_value) ** 2\n        self._parameter_value[idx] = parameter_value\n\n    def _compute_parameter(self, sigma, **kwargs):\n        raise NotImplementedError(\n            \'WindowedVarianceParameter is an abstract class.\')\n\n\nclass WindowedVarianceIncreasingParameter(WindowedVarianceParameter):\n    """"""\n    Class implementing a parameter that decreases with the target\n    variance, where the variance is computed in a fixed length\n    window.\n\n    """"""\n    def _compute_parameter(self, sigma, **kwargs):\n        if self._exponential:\n            return 1 - np.exp(sigma * np.log(.5) / self._tol)\n        else:\n            return sigma / (sigma + self._tol)\n'"
mushroom_rl/utils/viewer.py,0,"b'import pygame\nimport time\nimport numpy as np\n\n\nclass ImageViewer:\n    """"""\n    Interface to pygame for visualizing plain images.\n\n    """"""\n    def __init__(self, size, dt):\n        """"""\n        Constructor.\n\n        Args:\n            size ([list, tuple]): size of the displayed image;\n            dt (float): duration of a control step.\n\n        """"""\n        self._size = size\n        self._dt = dt\n        self._initialized = False\n        self._screen = None\n\n    def display(self, img):\n        """"""\n        Display given frame.\n\n        Args:\n            img: image to display.\n\n        """"""\n        if not self._initialized:\n            pygame.init()\n            self._initialized = True\n\n        if self._screen is None:\n            self._screen = pygame.display.set_mode(self._size)\n\n        img = np.transpose(img, (1, 0, 2))\n        surf = pygame.surfarray.make_surface(img)\n        self._screen.blit(surf, (0, 0))\n        pygame.display.flip()\n        time.sleep(self._dt)\n\n    @property\n    def size(self):\n        return self._size\n\n\nclass Viewer:\n    """"""\n    Interface to pygame for visualizing mushroom native environments.\n\n    """"""\n    def __init__(self, env_width, env_height, width=500, height=500,\n                 background=(0, 0, 0)):\n        """"""\n        Constructor.\n\n        Args:\n            env_width (int): The x dimension limit of the desired environment;\n            env_height (int): The y dimension limit of the desired environment;\n            width (int, 500): width of the environment window;\n            height (int, 500): height of the environment window;\n            background (tuple, (0, 0, 0)): background color of the screen.\n\n        """"""\n        self._size = (width, height)\n        self._width = width\n        self._height = height\n        self._screen = None\n        self._ratio = np.array([width / env_width, height / env_height])\n        self._background = background\n\n        self._initialized = False\n\n    @property\n    def screen(self):\n        """"""\n        Property.\n\n        Returns:\n            The screen created by this viewer.\n\n        """"""\n        if not self._initialized:\n            pygame.init()\n            self._initialized = True\n\n        if self._screen is None:\n            self._screen = pygame.display.set_mode(self._size)\n\n        return self._screen\n\n    @property\n    def size(self):\n        """"""\n        Property.\n\n        Returns:\n            The size of the screen.\n\n        """"""\n        return self._size\n\n    def line(self, start, end, color=(255, 255, 255), width=1):\n        """"""\n        Draw a line on the screen.\n\n        Args:\n            start (np.ndarray): starting point of the line;\n            end (np.ndarray): end point of the line;\n            color (tuple (255, 255, 255)): color of the line;\n            width (int, 1): width of the line.\n\n        """"""\n        start = self._transform(start)\n        end = self._transform(end)\n\n        pygame.draw.line(self.screen, color, start, end, width)\n\n    def square(self, center, angle, edge, color=(255, 255, 255), width=0):\n        """"""\n        Draw a square on the screen and apply a roto-translation to it.\n\n        Args:\n            center (np.ndarray): the center of the polygon;\n            angle (float): the rotation to apply to the polygon;\n            edge (float): length of an edge;\n            color (tuple, (255, 255, 255)) : the color of the polygon;\n            width (int, 0): the width of the polygon line, 0 to fill the\n                polygon.\n\n        """"""\n        edge_2 = edge / 2\n        points = [[edge_2, edge_2],\n                  [edge_2, -edge_2],\n                  [-edge_2, -edge_2],\n                  [-edge_2, edge_2]]\n\n        self.polygon(center, angle, points, color, width)\n\n    def polygon(self, center, angle, points, color=(255, 255, 255), width=0):\n        """"""\n        Draw a polygon on the screen and apply a roto-translation to it.\n\n        Args:\n            center (np.ndarray): the center of the polygon;\n            angle (float): the rotation to apply to the polygon;\n            points (list): the points of the polygon w.r.t. the center;\n            color (tuple, (255, 255, 255)) : the color of the polygon;\n            width (int, 0): the width of the polygon line, 0 to fill the\n                polygon.\n\n        """"""\n        poly = list()\n\n        for point in points:\n            point = self._rotate(point, angle)\n            point += center\n            point = self._transform(point)\n            poly.append(point)\n\n        pygame.draw.polygon(self.screen, color, poly, width)\n\n    def circle(self, center, radius, color=(255, 255, 255), width=0):\n        """"""\n        Draw a circle on the screen.\n\n        Args:\n            center (np.ndarray): the center of the circle;\n            radius (float): the radius of the circle;\n            color (tuple, (255, 255, 255)): the color of the circle;\n            width (int, 0): the width of the circle line, 0 to fill the circle.\n\n        """"""\n        center = self._transform(center)\n        radius = int(radius * self._ratio[0])\n        pygame.draw.circle(self.screen, color, center, radius, width)\n\n    def arrow_head(self, center, scale, angle, color=(255, 255, 255)):\n        """"""\n        Draw an harrow head.\n\n        Args:\n            center (np.ndarray): the position of the arrow head;\n            scale (float): scale of the arrow, correspond to the length;\n            angle (float): the angle of rotation of the angle head;\n            color (tuple, (255, 255, 255)): the color of the arrow.\n\n        """"""\n        points = [[-0.5 * scale, -0.5 * scale],\n                  [-0.5 * scale, 0.5 * scale],\n                  [0.5 * scale, 0]]\n\n        self.polygon(center, angle, points, color)\n\n    def force_arrow(self, center, direction, force, max_force,\n                    max_length, color=(255, 255, 255), width=1):\n        """"""\n        Draw a force arrow, i.e. an arrow representing a force. The\n        length of the arrow is directly proportional to the force value.\n\n        Args:\n            center (np.ndarray): the point where the force is applied;\n            direction (np.ndarray): the direction of the force;\n            force (float): the applied force value;\n            max_force (float): the maximum force value;\n            max_length (float): the length to use for the maximum force;\n            color (tuple, (255, 255, 255)): the color of the arrow;\n            width (int, 1): the width of the force arrow.\n\n        """"""\n        length = force / max_force * max_length\n\n        if length != 0:\n\n            c = self._transform(center)\n            direction = direction / np.linalg.norm(direction)\n            end = center + length * direction\n            e = self._transform(end)\n            delta = e - c\n\n            pygame.draw.line(self.screen, color, c, e, width)\n            self.arrow_head(end, max_length / 4, np.arctan2(delta[1], delta[0]),\n                            color)\n\n    def torque_arrow(self, center, torque, max_torque,\n                     max_radius, color=(255, 255, 255), width=1):\n        """"""\n        Draw a torque arrow, i.e. a circular arrow representing a torque. The\n        radius of the arrow is directly proportional to the torque value.\n\n        Args:\n            center (np.ndarray): the point where the torque is applied;\n            torque (float): the applied torque value;\n            max_torque (float): the maximum torque value;\n            max_radius (float): the radius to use for the maximum torque;\n            color (tuple, (255, 255, 255)): the color of the arrow;\n            width (int, 1): the width of the torque arrow.\n\n        """"""\n        angle_end = 3 * np.pi / 2 if torque > 0 else 0\n        angle_start = 0 if torque > 0 else np.pi / 2\n        radius = abs(torque) / max_torque * max_radius\n\n        r = int(radius * self._ratio[0])\n        if r != 0:\n            c = self._transform(center)\n            rect = pygame.Rect(c[0] - r, c[1] - r, 2 * r, 2 * r)\n            pygame.draw.arc(self.screen, color, rect,\n                            angle_start, angle_end, width)\n\n            arrow_center = center\n            arrow_center[1] -= radius * np.sign(torque)\n            arrow_scale = radius / 4\n            self.arrow_head(arrow_center, arrow_scale, 0, color)\n\n    def background_image(self, img):\n        """"""\n        Use the given image as background for the window, rescaling it\n        appropriately.\n\n        Args:\n            img: the image to be used.\n\n        """"""\n        surf = pygame.surfarray.make_surface(img)\n        surf = pygame.transform.smoothscale(surf, self.size)\n        self.screen.blit(surf, (0, 0))\n\n    def function(self, x_s, x_e, f, n_points=100,  width=1, color=(255, 255, 255)):\n        """"""\n        Draw the graph of a function in the image.\n\n        Args:\n            x_s (float): starting x coordinate;\n            x_e (float): final x coordinate;\n            f (function): the function that maps x coorinates into y\n                coordinates;\n            n_points (int, 100): the number of segments used to approximate the\n                function to draw;\n            width (int, 1): thw width of the line drawn;\n            color (tuple, (255,255,255)): the color of the line.\n\n        """"""\n        x = np.linspace(x_s, x_e, n_points)\n        y = f(x)\n\n        points = [self._transform([a, b]) for a, b in zip(x,y)]\n        pygame.draw.lines(self.screen, color, False, points, width)\n\n    def display(self, s):\n        """"""\n        Display current frame and initialize the next frame to the background\n        color.\n\n        Args:\n            s: time to wait in visualization.\n\n        """"""\n        pygame.display.flip()\n        time.sleep(s)\n\n        self.screen.fill(self._background)\n\n    def close(self):\n        """"""\n        Close the viewer, destroy the window.\n\n        """"""\n        self._screen = None\n        pygame.display.quit()\n\n    def _transform(self, p):\n        return np.array([p[0] * self._ratio[0],\n                         self._height - p[1] * self._ratio[1]]).astype(int)\n\n    @staticmethod\n    def _rotate(p, theta):\n        return np.array([np.cos(theta) * p[0] - np.sin(theta) * p[1],\n                         np.sin(theta) * p[0] + np.cos(theta) * p[1]])\n'"
tests/algorithms/test_a2c.py,6,"b'# import sys\n# import os\n# sys.path = [os.getcwd()] + sys.path\n\n# import sys\n# print(\'\\n\'.join(sys.path))\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nimport shutil\nfrom datetime import datetime\nfrom helper.utils import TestUtils as tu\n\nfrom mushroom_rl.algorithms import Agent\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import Gym\nfrom mushroom_rl.algorithms.actor_critic import A2C\n\nfrom mushroom_rl.policy import GaussianTorchPolicy\n\n\nclass Network(nn.Module):\n    def __init__(self, input_shape, output_shape, **kwargs):\n        super().__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._h = nn.Linear(n_input, n_output)\n\n        nn.init.xavier_uniform_(self._h.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n\n    def forward(self, state):\n        return F.relu(self._h(torch.squeeze(state, 1).float()))\n\ndef learn_a2c():\n    mdp = Gym(name=\'Pendulum-v0\', horizon=200, gamma=.99)\n    mdp.seed(1)\n    np.random.seed(1)\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    policy_params = dict(\n        std_0=1.,\n        n_features=64,\n        use_cuda=False\n    )\n\n    critic_params = dict(network=Network,\n                         optimizer={\'class\': optim.RMSprop,\n                                    \'params\': {\'lr\': 7e-4,\n                                               \'eps\': 1e-5}},\n                         loss=F.mse_loss,\n                         input_shape=mdp.info.observation_space.shape,\n                         output_shape=(1,))\n\n    algorithm_params = dict(critic_params=critic_params,\n                            actor_optimizer={\'class\': optim.RMSprop,\n                                             \'params\': {\'lr\': 7e-4,\n                                                        \'eps\': 3e-3}},\n                            max_grad_norm=0.5,\n                            ent_coeff=0.01)\n\n    policy = GaussianTorchPolicy(Network,\n                                 mdp.info.observation_space.shape,\n                                 mdp.info.action_space.shape,\n                                 **policy_params)\n\n    agent = A2C(mdp.info, policy, **algorithm_params)\n\n    core = Core(agent, mdp)\n    core.learn(n_episodes=10, n_episodes_per_fit=5)\n\n    return agent\n\n\ndef test_a2c():\n    \n    agent = learn_a2c()\n\n    w = agent.policy.get_weights()\n    w_test = np.array([-1.6307759, 1.0356185, -0.34508315, 0.27108294,\n                       -0.01047843])\n\n    assert np.allclose(w, w_test)\n\n\ndef test_a2c_save():\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save = learn_a2c()\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n'"
tests/algorithms/test_batch_td.py,0,"b'import numpy as np\nimport shutil\nfrom datetime import datetime\nfrom helper.utils import TestUtils as tu\n\nfrom mushroom_rl.algorithms import Agent\nfrom mushroom_rl.algorithms.value import LSPI\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import *\nfrom mushroom_rl.features import Features\nfrom mushroom_rl.features.basis import PolynomialBasis\nfrom mushroom_rl.policy import EpsGreedy\nfrom mushroom_rl.utils.parameters import Parameter\n\n\ndef learn_lspi():\n    mdp = CartPole()\n    np.random.seed(1)\n\n    # Policy\n    epsilon = Parameter(value=1.)\n    pi = EpsGreedy(epsilon=epsilon)\n\n    # Agent\n    basis = [PolynomialBasis()]\n    features = Features(basis_list=basis)\n\n    approximator_params = dict(input_shape=(features.size,),\n                               output_shape=(mdp.info.action_space.n,),\n                               n_actions=mdp.info.action_space.n)\n    agent = LSPI(mdp.info, pi, fit_params=dict(),\n                 approximator_params=approximator_params, features=features)\n\n    # Algorithm\n    core = Core(agent, mdp)\n\n    # Train\n    core.learn(n_episodes=100, n_episodes_per_fit=100)\n    return agent\n\n\ndef test_lspi():\n\n    w = learn_lspi().approximator.get_weights()\n    w_test = np.array([-2.23880597, -2.27427603, -2.25])\n\n    assert np.allclose(w, w_test)\n\n\ndef test_lspi_save():\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save = learn_lspi()\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n'"
tests/algorithms/test_black_box.py,1,"b'import numpy as np\nimport torch\nimport shutil\nfrom datetime import datetime\nfrom helper.utils import TestUtils as tu\n\nfrom mushroom_rl.algorithms import Agent\nfrom mushroom_rl.algorithms.policy_search import PGPE, REPS, RWR\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.approximators.parametric import LinearApproximator\nfrom mushroom_rl.distributions import GaussianDiagonalDistribution\nfrom mushroom_rl.environments import LQR\nfrom mushroom_rl.policy import DeterministicPolicy\nfrom mushroom_rl.utils.parameters import AdaptiveParameter\n\n\ndef learn(alg, **alg_params):\n    np.random.seed(1)\n    torch.manual_seed(1)\n\n    # MDP\n    mdp = LQR.generate(dimensions=2)\n\n    approximator = Regressor(LinearApproximator,\n                             input_shape=mdp.info.observation_space.shape,\n                             output_shape=mdp.info.action_space.shape)\n\n    policy = DeterministicPolicy(mu=approximator)\n\n    mu = np.zeros(policy.weights_size)\n    sigma = 1e-3 * np.ones(policy.weights_size)\n    distribution = GaussianDiagonalDistribution(mu, sigma)\n\n    agent = alg(mdp.info, distribution, policy, **alg_params)\n    core = Core(agent, mdp)\n\n    core.learn(n_episodes=5, n_episodes_per_fit=5)\n\n    return agent\n\n\ndef test_RWR():\n    distribution = learn(RWR, beta=1.).distribution\n    w = distribution.get_parameters()\n    w_test = np.array([0.00086195, -0.00229678, 0.00173919, -0.0007568,\n                       0.00073533, 0.00101203, 0.00119701, 0.00094453])\n\n    assert np.allclose(w, w_test)\n\n\ndef test_RWR_save():\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save = learn(RWR, beta=1.)\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_REPS():\n    distribution = learn(REPS, eps=.7).distribution\n    w = distribution.get_parameters()\n    w_test = np.array([0.00050246, -0.00175432, 0.00128979, -0.00050779,\n                       0.00071795, 0.00108254, 0.00098966, 0.00086633])\n\n    assert np.allclose(w, w_test)\n\n\ndef test_REPS_save():\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save = learn(REPS, eps=.7)\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_PGPE():\n    distribution = learn(PGPE, learning_rate=AdaptiveParameter(1.5)).distribution\n    w = distribution.get_parameters()\n    w_test = np.array([0.02489092, 0.31062211, 0.2051433, 0.05959651,\n                       -0.78302236, 0.77381954, 0.23676176, -0.29855654])\n\n    assert np.allclose(w, w_test)\n\n\ndef test_PGPE_save():\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save = learn(PGPE, learning_rate=AdaptiveParameter(1.5))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n\n        tu.assert_eq(save_attr, load_attr)\n'"
tests/algorithms/test_ddpg.py,8,"b'# import sys\n# import os\n# sys.path = [os.getcwd()] + sys.path\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport shutil\nimport pathlib\nimport itertools\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom helper.utils import TestUtils as tu\n\nimport mushroom_rl\nfrom mushroom_rl.algorithms import Agent\nfrom mushroom_rl.algorithms.actor_critic import DDPG, TD3\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments.gym_env import Gym\nfrom mushroom_rl.policy import OrnsteinUhlenbeckPolicy\n\n\nclass CriticNetwork(nn.Module):\n    def __init__(self, input_shape, output_shape, **kwargs):\n        super().__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._h = nn.Linear(n_input, n_output)\n\n        nn.init.xavier_uniform_(self._h.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n\n    def forward(self, state, action):\n        state_action = torch.cat((state.float(), action.float()), dim=1)\n        q = F.relu(self._h(state_action))\n\n        return torch.squeeze(q)\n\n\nclass ActorNetwork(nn.Module):\n    def __init__(self, input_shape, output_shape, **kwargs):\n        super(ActorNetwork, self).__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._h = nn.Linear(n_input, n_output)\n\n        nn.init.xavier_uniform_(self._h.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n\n    def forward(self, state):\n        return F.relu(self._h(torch.squeeze(state, 1).float()))\n\n\ndef learn(alg):\n    mdp = Gym(\'Pendulum-v0\', 200, .99)\n    mdp.seed(1)\n    np.random.seed(1)\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    # Policy\n    policy_class = OrnsteinUhlenbeckPolicy\n    policy_params = dict(sigma=np.ones(1) * .2, theta=.15, dt=1e-2)\n\n    # Settings\n    initial_replay_size = 500\n    max_replay_size = 5000\n    batch_size = 200\n    n_features = 80\n    tau = .001\n\n    # Approximator\n    actor_input_shape = mdp.info.observation_space.shape\n    actor_params = dict(network=ActorNetwork,\n                        n_features=n_features,\n                        input_shape=actor_input_shape,\n                        output_shape=mdp.info.action_space.shape,\n                        use_cuda=False)\n\n    actor_optimizer = {\'class\': optim.Adam,\n                       \'params\': {\'lr\': .001}}\n\n    critic_input_shape = (actor_input_shape[0] + mdp.info.action_space.shape[0],)\n    critic_params = dict(network=CriticNetwork,\n                         optimizer={\'class\': optim.Adam,\n                                    \'params\': {\'lr\': .001}},\n                         loss=F.mse_loss,\n                         n_features=n_features,\n                         input_shape=critic_input_shape,\n                         output_shape=(1,),\n                         use_cuda=False)\n\n    # Agent\n    agent = alg(mdp.info, policy_class, policy_params,\n                actor_params, actor_optimizer, critic_params, batch_size,\n                initial_replay_size, max_replay_size, tau)\n\n    # Algorithm\n    core = Core(agent, mdp)\n\n    core.learn(n_episodes=10, n_episodes_per_fit=5)\n\n    return agent\n\n\ndef test_ddpg():\n    policy = learn(DDPG).policy\n    w = policy.get_weights()\n    w_test = np.array([-0.28865, -0.7487735, -0.5533644, -0.34702766])\n\n    assert np.allclose(w, w_test)\n\n\ndef test_ddpg_save():\n    \n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save = learn(DDPG)\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_td3():\n    policy = learn(TD3).policy\n    w = policy.get_weights()\n    w_test = np.array([1.7005192, -0.73382795, 1.2999079, -0.26730126])\n\n    assert np.allclose(w, w_test)\n\n\ndef test_td3_save():\n    \n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save = learn(TD3)\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n'"
tests/algorithms/test_dpg.py,2,"b'import numpy as np\nimport torch\nimport shutil\nfrom datetime import datetime\nfrom helper.utils import TestUtils as tu\n\nfrom mushroom_rl.algorithms import Agent\nfrom mushroom_rl.algorithms.actor_critic import COPDAC_Q\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import *\nfrom mushroom_rl.features import Features\nfrom mushroom_rl.features.tiles import Tiles\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.approximators.parametric import LinearApproximator\nfrom mushroom_rl.policy import GaussianPolicy\nfrom mushroom_rl.utils.parameters import Parameter\n\ndef learn_copdac_q():\n    n_steps = 50\n    mdp = InvertedPendulum(horizon=n_steps)\n    np.random.seed(1)\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    # Agent\n    n_tilings = 1\n    alpha_theta = Parameter(5e-3 / n_tilings)\n    alpha_omega = Parameter(0.5 / n_tilings)\n    alpha_v = Parameter(0.5 / n_tilings)\n    tilings = Tiles.generate(n_tilings, [2, 2],\n                             mdp.info.observation_space.low,\n                             mdp.info.observation_space.high + 1e-3)\n\n    phi = Features(tilings=tilings)\n\n    input_shape = (phi.size,)\n\n    mu = Regressor(LinearApproximator, input_shape=input_shape,\n                   output_shape=mdp.info.action_space.shape)\n\n    sigma = 1e-1 * np.eye(1)\n    policy = GaussianPolicy(mu, sigma)\n\n    agent = COPDAC_Q(mdp.info, policy, mu,\n                     alpha_theta, alpha_omega, alpha_v,\n                     value_function_features=phi,\n                     policy_features=phi)\n\n    # Train\n    core = Core(agent, mdp)\n\n    core.learn(n_episodes=2, n_episodes_per_fit=1)\n    \n    return agent\n\ndef test_copdac_q():\n\n    policy = learn_copdac_q().policy\n    w = policy.get_weights()\n    w_test = np.array([0, -6.62180045e-7, 0, -4.23972882e-2])\n\n    assert np.allclose(w, w_test)\n\ndef test_copdac_q_save():\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save = learn_copdac_q()\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n'"
tests/algorithms/test_dqn.py,8,"b'import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport shutil\nfrom datetime import datetime\nfrom helper.utils import TestUtils as tu\n\nfrom mushroom_rl.algorithms import Agent\nfrom mushroom_rl.algorithms.value import DQN, DoubleDQN, AveragedDQN, CategoricalDQN\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import *\nfrom mushroom_rl.policy import EpsGreedy\nfrom mushroom_rl.approximators.parametric.torch_approximator import *\nfrom mushroom_rl.utils.parameters import Parameter, LinearParameter\nfrom mushroom_rl.utils.replay_memory import PrioritizedReplayMemory\n\n\nclass Network(nn.Module):\n    def __init__(self, input_shape, output_shape, **kwargs):\n        super().__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._h1 = nn.Linear(n_input, n_output)\n\n        nn.init.xavier_uniform_(self._h1.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n\n    def forward(self, state, action=None):\n        q = F.relu(self._h1(torch.squeeze(state, 1).float()))\n\n        if action is None:\n            return q\n        else:\n            action = action.long()\n            q_acted = torch.squeeze(q.gather(1, action))\n\n            return q_acted\n\n\nclass FeatureNetwork(nn.Module):\n    def __init__(self, input_shape, output_shape, **kwargs):\n        super().__init__()\n\n    def forward(self, state, action=None):\n        return torch.squeeze(state, 1).float()\n\n\ndef learn(alg, alg_params):\n    # MDP\n    mdp = CartPole()\n    np.random.seed(1)\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    # Policy\n    epsilon_random = Parameter(value=1.)\n    pi = EpsGreedy(epsilon=epsilon_random)\n\n    # Approximator\n    input_shape = mdp.info.observation_space.shape\n    approximator_params = dict(network=Network if alg is not CategoricalDQN else FeatureNetwork,\n                               optimizer={\'class\': optim.Adam,\n                                          \'params\': {\'lr\': .001}},\n                               loss=F.smooth_l1_loss,\n                               input_shape=input_shape,\n                               output_shape=mdp.info.action_space.size,\n                               n_actions=mdp.info.action_space.n,\n                               n_features=2, use_cuda=False)\n\n    # Agent\n    if alg is not CategoricalDQN:\n        agent = alg(mdp.info, pi, TorchApproximator,\n                    approximator_params=approximator_params, **alg_params)\n    else:\n        agent = alg(mdp.info, pi, approximator_params=approximator_params,\n                    n_atoms=2, v_min=-1, v_max=1, **alg_params)\n\n    # Algorithm\n    core = Core(agent, mdp)\n\n    core.learn(n_steps=500, n_steps_per_fit=5)\n\n    return agent\n\n\ndef test_dqn():\n    params = dict(batch_size=50, n_approximators=1, initial_replay_size=50,\n                  max_replay_size=500, target_update_frequency=50)\n    approximator = learn(DQN, params).approximator\n\n    w = approximator.get_weights()\n    w_test = np.array([-0.15894288, 0.47257397, 0.05482405, 0.5442066,\n                       -0.56469935, -0.07374532, -0.0706185, 0.40790945,\n                       0.12486243])\n\n    assert np.allclose(w, w_test)\n\n\ndef test_dqn_save():\n    params = dict(batch_size=50, n_approximators=1, initial_replay_size=50,\n                  max_replay_size=500, target_update_frequency=50)\n    agent_save = learn(DQN, params)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_prioritized_dqn():\n    replay_memory = PrioritizedReplayMemory(\n        50, 500, alpha=.6,\n        beta=LinearParameter(.4, threshold_value=1, n=500 // 5)\n    )\n    params = dict(batch_size=50, n_approximators=1, initial_replay_size=50,\n                  max_replay_size=500, target_update_frequency=50,\n                  replay_memory=replay_memory)\n    approximator = learn(DQN, params).approximator\n\n    w = approximator.get_weights()\n    w_test = np.array([-0.1384063, 0.48957556, 0.02254359, 0.50994426,\n                       -0.56277484, -0.075808, -0.06829552, 0.3642576,\n                       0.15519235])\n\n    assert np.allclose(w, w_test)\n\n\ndef test_prioritized_dqn_save():\n    replay_memory = PrioritizedReplayMemory(\n        50, 500, alpha=.6,\n        beta=LinearParameter(.4, threshold_value=1, n=500 // 5)\n    )\n    params = dict(batch_size=50, n_approximators=1, initial_replay_size=50,\n                  max_replay_size=500, target_update_frequency=50,\n                  replay_memory=replay_memory)\n    agent_save = learn(DQN, params)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_double_dqn():\n    params = dict(batch_size=50, n_approximators=1, initial_replay_size=50,\n                  max_replay_size=500, target_update_frequency=50)\n    approximator = learn(DoubleDQN, params).approximator\n\n    w = approximator.get_weights()\n    w_test = np.array([-0.15894286, 0.47257394, 0.05482561, 0.54420704,\n                       -0.5646987, -0.07374918, -0.07061853, 0.40789905,\n                       0.12482855])\n\n    assert np.allclose(w, w_test)\n\n\ndef test_double_dqn_save():\n    params = dict(batch_size=50, n_approximators=1, initial_replay_size=50,\n                  max_replay_size=500, target_update_frequency=50)\n    agent_save = learn(DoubleDQN, params)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_averaged_dqn():\n    params = dict(batch_size=50, n_approximators=5, initial_replay_size=50,\n                  max_replay_size=5000, target_update_frequency=50)\n    approximator = learn(AveragedDQN, params).approximator\n\n    w = approximator.get_weights()\n    w_test = np.array([-0.15889995, 0.47253257, 0.05424322, 0.5434766,\n                       -0.56529117, -0.0743931, -0.07070775, 0.4055584,\n                       0.12588869])\n\n    assert np.allclose(w, w_test)\n\n\ndef test_averaged_dqn_save():\n    params = dict(batch_size=50, n_approximators=5, initial_replay_size=50,\n                  max_replay_size=5000, target_update_frequency=50)\n    agent_save = learn(AveragedDQN, params)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_categorical_dqn():\n    params = dict(batch_size=50, n_approximators=1, initial_replay_size=50,\n                  max_replay_size=5000, target_update_frequency=50)\n    approximator = learn(CategoricalDQN, params).approximator\n\n    w = approximator.get_weights()\n    w_test = np.array([1.0035713, 0.30592525, -0.38904265, -0.66449565,\n                       -0.71816885, 0.47653696, -0.12593754, -0.44365975,\n                       -0.47181657, -0.02598009, 0.11935875, 0.11164782,\n                       0.659329, 0.5941985, -1.1264751, 0.8307397, 0.01681535,\n                       0.08285073])\n\n    assert np.allclose(w, w_test, rtol=1e-4)\n\n\ndef test_categorical_dqn_save():\n    params = dict(batch_size=50, n_approximators=1, initial_replay_size=50,\n                  max_replay_size=5000, target_update_frequency=50)\n    agent_save = learn(CategoricalDQN, params)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n'"
tests/algorithms/test_fqi.py,0,"b'import numpy as np\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nimport shutil\nfrom datetime import datetime\nfrom helper.utils import TestUtils as tu\n\nfrom mushroom_rl.algorithms import Agent\nfrom mushroom_rl.algorithms.value import DoubleFQI, FQI\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import *\nfrom mushroom_rl.policy import EpsGreedy\nfrom mushroom_rl.utils.dataset import compute_J\nfrom mushroom_rl.utils.parameters import Parameter\n\n\ndef learn(alg, alg_params):\n    mdp = CarOnHill()\n    np.random.seed(1)\n\n    # Policy\n    epsilon = Parameter(value=1.)\n    pi = EpsGreedy(epsilon=epsilon)\n\n    # Approximator\n    approximator_params = dict(input_shape=mdp.info.observation_space.shape,\n                               n_actions=mdp.info.action_space.n,\n                               n_estimators=50,\n                               min_samples_split=5,\n                               min_samples_leaf=2)\n    approximator = ExtraTreesRegressor\n\n    # Agent\n    agent = alg(mdp.info, pi, approximator,\n                approximator_params=approximator_params, **alg_params)\n\n    # Algorithm\n    core = Core(agent, mdp)\n\n    # Train\n    core.learn(n_episodes=5, n_episodes_per_fit=5)\n\n    test_epsilon = Parameter(0.75)\n    agent.policy.set_epsilon(test_epsilon)\n    dataset = core.evaluate(n_episodes=2)\n\n    return agent, np.mean(compute_J(dataset, mdp.info.gamma))\n\n\ndef test_fqi():\n    params = dict(n_iterations=10)\n    _, j = learn(FQI, params)\n    j_test = -0.0874123073618985\n\n    assert j == j_test\n\n\ndef test_fqi_save():\n    params = dict(n_iterations=10)\n    agent_save, _ = learn(FQI, params)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_fqi_boosted():\n    params = dict(n_iterations=10, boosted=True)\n    _, j = learn(FQI, params)\n    j_test = -0.09201295511778791\n\n    assert j == j_test\n\n\ndef test_fqi_boosted_save():\n    params = dict(n_iterations=10, boosted=True)\n    agent_save, _ = learn(FQI, params)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_double_fqi():\n    params = dict(n_iterations=10)\n    _, j = learn(DoubleFQI, params)\n    j_test = -0.19933233708925654\n\n    assert j == j_test\n\n\ndef test_double_fqi_save():\n    params = dict(n_iterations=10)\n    agent_save, _ = learn(DoubleFQI, params)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n'"
tests/algorithms/test_lspi.py,0,"b'import numpy as np\n\nimport shutil\nfrom datetime import datetime\nfrom helper.utils import TestUtils as tu\n\nfrom mushroom_rl.algorithms import Agent\nfrom mushroom_rl.algorithms.value import LSPI\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import *\nfrom mushroom_rl.features import Features\nfrom mushroom_rl.features.basis import PolynomialBasis\nfrom mushroom_rl.policy import EpsGreedy\nfrom mushroom_rl.utils.parameters import Parameter\n\ndef learn_lspi():\n    np.random.seed(1)\n\n    # MDP\n    mdp = CartPole()\n\n    # Policy\n    epsilon = Parameter(value=1.)\n    pi = EpsGreedy(epsilon=epsilon)\n\n    # Agent\n    basis = [PolynomialBasis()]\n    features = Features(basis_list=basis)\n\n    fit_params = dict()\n    approximator_params = dict(input_shape=(features.size,),\n                               output_shape=(mdp.info.action_space.n,),\n                               n_actions=mdp.info.action_space.n)\n    agent = LSPI(mdp.info, pi, approximator_params=approximator_params,\n                 fit_params=fit_params, features=features)\n\n    # Algorithm\n    core = Core(agent, mdp)\n\n    # Train\n    core.learn(n_episodes=10, n_episodes_per_fit=10)\n\n    return agent\n\n\ndef test_lspi():\n\n    w = learn_lspi().approximator.get_weights()\n    w_test = np.array([-1.00749128, -1.13444655, -0.96620322])\n\n    assert np.allclose(w, w_test)\n\n\ndef test_lspi_save():\n\n    agent_save = learn_lspi()\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n'"
tests/algorithms/test_policy_gradient.py,2,"b'import numpy as np\nimport torch\nimport shutil\nfrom datetime import datetime\nfrom helper.utils import TestUtils as tu\n\nfrom mushroom_rl.algorithms import Agent\nfrom mushroom_rl.algorithms.policy_search import *\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.approximators.parametric import LinearApproximator\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments.lqr import LQR\nfrom mushroom_rl.policy.gaussian_policy import StateStdGaussianPolicy\nfrom mushroom_rl.utils.parameters import AdaptiveParameter\n\n\ndef learn(alg, alg_params):\n    mdp = LQR.generate(dimensions=1)\n    np.random.seed(1)\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    approximator_params = dict(input_dim=mdp.info.observation_space.shape)\n    approximator = Regressor(LinearApproximator,\n                             input_shape=mdp.info.observation_space.shape,\n                             output_shape=mdp.info.action_space.shape,\n                             params=approximator_params)\n\n    sigma = Regressor(LinearApproximator,\n                      input_shape=mdp.info.observation_space.shape,\n                      output_shape=mdp.info.action_space.shape,\n                      params=approximator_params)\n\n    sigma_weights = 2 * np.ones(sigma.weights_size)\n    sigma.set_weights(sigma_weights)\n\n    policy = StateStdGaussianPolicy(approximator, sigma)\n\n    agent = alg(mdp.info, policy, **alg_params)\n\n    core = Core(agent, mdp)\n\n    core.learn(n_episodes=10, n_episodes_per_fit=5)\n\n    return agent\n\n\ndef test_REINFORCE():\n    params = dict(learning_rate=AdaptiveParameter(value=.01))\n    policy = learn(REINFORCE, params).policy\n    w = np.array([-0.0084793 ,  2.00536528])\n\n    assert np.allclose(w, policy.get_weights())\n\n\ndef test_REINFORCE_save():\n    params = dict(learning_rate=AdaptiveParameter(value=.01))\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save = learn(REINFORCE, params)\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_GPOMDP():\n    params = dict(learning_rate=AdaptiveParameter(value=.01))\n    policy = learn(GPOMDP, params).policy\n    w = np.array([-0.07623939,  2.05232858])\n\n    assert np.allclose(w, policy.get_weights())\n\n\ndef test_GPOMDP_save():\n    params = dict(learning_rate=AdaptiveParameter(value=.01))\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save = learn(GPOMDP, params)\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_eNAC():\n    params = dict(learning_rate=AdaptiveParameter(value=.01))\n    policy = learn(eNAC, params).policy\n    w = np.array([-0.03668018,  2.05112355])\n\n    assert np.allclose(w, policy.get_weights())\n\n\ndef test_eNAC_save():\n    params = dict(learning_rate=AdaptiveParameter(value=.01))\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save = learn(eNAC, params)\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n\n        tu.assert_eq(save_attr, load_attr)\n'"
tests/algorithms/test_sac.py,8,"b'import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport shutil\nfrom datetime import datetime\nfrom helper.utils import TestUtils as tu\n\nfrom mushroom_rl.algorithms import Agent\nfrom mushroom_rl.algorithms.actor_critic import SAC\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments.gym_env import Gym\n\n\nclass CriticNetwork(nn.Module):\n    def __init__(self, input_shape, output_shape, **kwargs):\n        super().__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._h = nn.Linear(n_input, n_output)\n\n        nn.init.xavier_uniform_(self._h.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n\n    def forward(self, state, action):\n        state_action = torch.cat((state.float(), action.float()), dim=1)\n        q = F.relu(self._h(state_action))\n\n        return torch.squeeze(q)\n\n\nclass ActorNetwork(nn.Module):\n    def __init__(self, input_shape, output_shape, **kwargs):\n        super(ActorNetwork, self).__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._h = nn.Linear(n_input, n_output)\n\n        nn.init.xavier_uniform_(self._h.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n\n    def forward(self, state):\n        return F.relu(self._h(torch.squeeze(state, 1).float()))\n\ndef learn_sac():\n\n    # MDP\n    horizon = 200\n    gamma = 0.99\n    mdp = Gym(\'Pendulum-v0\', horizon, gamma)\n    mdp.seed(1)\n    np.random.seed(1)\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    # Settings\n    initial_replay_size = 64\n    max_replay_size = 50000\n    batch_size = 64\n    n_features = 64\n    warmup_transitions = 10\n    tau = 0.005\n    lr_alpha = 3e-4\n\n    # Approximator\n    actor_input_shape = mdp.info.observation_space.shape\n    actor_mu_params = dict(network=ActorNetwork,\n                           n_features=n_features,\n                           input_shape=actor_input_shape,\n                           output_shape=mdp.info.action_space.shape,\n                           use_cuda=False)\n    actor_sigma_params = dict(network=ActorNetwork,\n                              n_features=n_features,\n                              input_shape=actor_input_shape,\n                              output_shape=mdp.info.action_space.shape,\n                              use_cuda=False)\n\n    actor_optimizer = {\'class\': optim.Adam,\n                       \'params\': {\'lr\': 3e-4}}\n\n    critic_input_shape = (\n    actor_input_shape[0] + mdp.info.action_space.shape[0],)\n    critic_params = dict(network=CriticNetwork,\n                         optimizer={\'class\': optim.Adam,\n                                    \'params\': {\'lr\': 3e-4}},\n                         loss=F.mse_loss,\n                         n_features=n_features,\n                         input_shape=critic_input_shape,\n                         output_shape=(1,),\n                         use_cuda=False)\n\n    # Agent\n    agent = SAC(mdp.info, actor_mu_params, actor_sigma_params, actor_optimizer,\n                critic_params, batch_size, initial_replay_size, max_replay_size,\n                warmup_transitions, tau, lr_alpha,\n                critic_fit_params=None)\n\n    # Algorithm\n    core = Core(agent, mdp)\n\n    core.learn(n_steps=2 * initial_replay_size,\n               n_steps_per_fit=initial_replay_size)\n    \n    return agent\n\ndef test_sac():\n    policy = learn_sac().policy\n    w = policy.get_weights()\n    w_test = np.array([ 1.6998193, -0.732528, 1.2986078, -0.26860124,\n                        0.5094043, -0.5001421, -0.18989229, -0.30646914])\n\n    assert np.allclose(w, w_test)\n\ndef test_sac_save():\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save = learn_sac()\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n'"
tests/algorithms/test_stochastic_ac.py,2,"b'import numpy as np\nimport torch\nimport shutil\nfrom datetime import datetime\nfrom helper.utils import TestUtils as tu\n\nfrom mushroom_rl.algorithms import Agent\nfrom mushroom_rl.algorithms.actor_critic import StochasticAC, StochasticAC_AVG\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import *\nfrom mushroom_rl.features import Features\nfrom mushroom_rl.features.tiles import Tiles\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.approximators.parametric import LinearApproximator\nfrom mushroom_rl.policy import StateLogStdGaussianPolicy\nfrom mushroom_rl.utils.parameters import Parameter\n\n\ndef learn(alg):\n    n_steps = 50\n    mdp = InvertedPendulum(horizon=n_steps)\n    np.random.seed(1)\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    # Agent\n    n_tilings = 2\n    alpha_r = Parameter(.0001)\n    alpha_theta = Parameter(.001 / n_tilings)\n    alpha_v = Parameter(.1 / n_tilings)\n    tilings = Tiles.generate(n_tilings-1, [1, 1],\n                             mdp.info.observation_space.low,\n                             mdp.info.observation_space.high + 1e-3)\n\n    phi = Features(tilings=tilings)\n\n    tilings_v = tilings + Tiles.generate(1, [1, 1],\n                                         mdp.info.observation_space.low,\n                                         mdp.info.observation_space.high + 1e-3)\n    psi = Features(tilings=tilings_v)\n\n    input_shape = (phi.size,)\n\n    mu = Regressor(LinearApproximator, input_shape=input_shape,\n                   output_shape=mdp.info.action_space.shape)\n\n    std = Regressor(LinearApproximator, input_shape=input_shape,\n                    output_shape=mdp.info.action_space.shape)\n\n    std_0 = np.sqrt(1.)\n    std.set_weights(np.log(std_0) / n_tilings * np.ones(std.weights_size))\n\n    policy = StateLogStdGaussianPolicy(mu, std)\n\n    if alg is StochasticAC:\n        agent = alg(mdp.info, policy, alpha_theta, alpha_v, lambda_par=.5,\n                    value_function_features=psi, policy_features=phi)\n    elif alg is StochasticAC_AVG:\n        agent = alg(mdp.info, policy, alpha_theta, alpha_v, alpha_r,\n                    lambda_par=.5, value_function_features=psi,\n                    policy_features=phi)\n\n    core = Core(agent, mdp)\n\n    core.learn(n_episodes=2, n_episodes_per_fit=1)\n\n    return agent\n\n\ndef test_stochastic_ac():\n    policy = learn(StochasticAC).policy\n\n    w = policy.get_weights()\n    w_test = np.array([-0.0026135, 0.01222979])\n\n    assert np.allclose(w, w_test)\n\n\ndef test_stochastic_ac_save():\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save = learn(StochasticAC)\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_stochastic_ac_avg():\n    policy = learn(StochasticAC_AVG).policy\n\n    w = policy.get_weights()\n    w_test = np.array([-0.00295433, 0.01325534])\n\n    assert np.allclose(w, w_test)\n\n\ndef test_stochastic_ac_avg_save():\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save = learn(StochasticAC_AVG)\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n'"
tests/algorithms/test_td.py,5,"b'import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport shutil\nfrom datetime import datetime\nfrom helper.utils import TestUtils as tu\n\nfrom mushroom_rl.algorithms import Agent\nfrom mushroom_rl.algorithms.value import *\nfrom mushroom_rl.approximators.parametric import LinearApproximator, TorchApproximator\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments.grid_world import GridWorld\nfrom mushroom_rl.environments.gym_env import Gym\nfrom mushroom_rl.features import Features\nfrom mushroom_rl.features.tiles import Tiles\nfrom mushroom_rl.policy.td_policy import EpsGreedy\nfrom mushroom_rl.utils.parameters import Parameter\n\nclass Network(nn.Module):\n    def __init__(self, input_shape, output_shape, **kwargs):\n        super().__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._h1 = nn.Linear(n_input, n_output)\n\n        nn.init.xavier_uniform_(self._h1.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n\n    def forward(self, state, action=None):\n        q = F.relu(self._h1(torch.squeeze(state, 1).float()))\n\n        if action is None:\n            return q\n        else:\n            action = action.long()\n            q_acted = torch.squeeze(q.gather(1, action))\n\n            return q_acted\n\n\ndef initialize():\n    np.random.seed(1)\n    torch.manual_seed(1)\n    return EpsGreedy(Parameter(1)), GridWorld(2, 2, start=(0, 0), goal=(1, 1)),\\\n           Gym(name=\'MountainCar-v0\', horizon=np.inf, gamma=1.)\n\n\ndef test_q_learning():\n    pi, mdp, _ = initialize()\n    agent = QLearning(mdp.info, pi, Parameter(.5))\n\n    core = Core(agent, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    test_q = np.array([[7.82042542, 8.40151978, 7.64961548, 8.82421875],\n                       [8.77587891, 9.921875, 7.29316406, 8.68359375],\n                       [7.7203125, 7.69921875, 4.5, 9.84375],\n                       [0., 0., 0., 0.]])\n\n    assert np.allclose(agent.Q.table, test_q)\n\n\ndef test_q_learning_save():\n    pi, mdp, _ = initialize()\n    agent_save = QLearning(mdp.info, pi, Parameter(.5))\n\n    core = Core(agent_save, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_double_q_learning():\n    pi, mdp, _ = initialize()\n    agent = DoubleQLearning(mdp.info, pi, Parameter(.5))\n\n    core = Core(agent, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    test_q_0 = np.array([[2.6578125, 6.94757812, 3.73359375, 7.171875],\n                         [2.25, 7.5, 3.0375, 3.375],\n                         [3.0375, 5.4140625, 2.08265625, 8.75],\n                         [0., 0., 0., 0.]])\n    test_q_1 = np.array([[2.72109375, 4.5, 4.36640625, 6.609375],\n                         [4.5, 9.375, 4.49296875, 4.5],\n                         [1.0125, 5.0625, 5.625, 8.75],\n                         [0., 0., 0., 0.]])\n\n    assert np.allclose(agent.Q[0].table, test_q_0)\n    assert np.allclose(agent.Q[1].table, test_q_1)\n\n\ndef test_double_q_learning_save():\n    pi, mdp, _ = initialize()\n    agent_save = DoubleQLearning(mdp.info, pi, Parameter(.5))\n\n    core = Core(agent_save, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_weighted_q_learning():\n    pi, mdp, _ = initialize()\n    agent = WeightedQLearning(mdp.info, pi, Parameter(.5))\n\n    core = Core(agent, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    test_q = np.array([[7.1592415, 4.07094744, 7.10518702, 8.5467274],\n                       [8.08689916, 9.99023438, 5.77871216, 7.51059129],\n                       [6.52294537, 0.86087671, 3.70431496, 9.6875],\n                       [0., 0., 0., 0.]])\n\n    assert np.allclose(agent.Q.table, test_q)\n\n\ndef test_weighted_q_learning_save():\n    pi, mdp, _ = initialize()\n    agent_save = WeightedQLearning(mdp.info, pi, Parameter(.5))\n\n    core = Core(agent_save, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_speedy_q_learning():\n    pi, mdp, _ = initialize()\n    agent = SpeedyQLearning(mdp.info, pi, Parameter(.5))\n\n    core = Core(agent, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    test_q = np.array([[7.82042542, 8.40151978, 7.64961548, 8.82421875],\n                       [8.77587891, 9.921875, 7.29316406, 8.68359375],\n                       [7.7203125, 7.69921875, 4.5, 9.84375],\n                       [0., 0., 0., 0.]])\n\n    assert np.allclose(agent.Q.table, test_q)\n\n\ndef test_speedy_q_learning_save():\n    pi, mdp, _ = initialize()\n    agent_save = SpeedyQLearning(mdp.info, pi, Parameter(.5))\n\n    core = Core(agent_save, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_sarsa():\n    pi, mdp, _ = initialize()\n    agent = SARSA(mdp.info, pi, Parameter(.1))\n\n    core = Core(agent, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    test_q = np.array([[4.31368701e-2, 3.68037689e-1, 4.14040445e-2, 1.64007642e-1],\n                       [6.45491436e-1, 4.68559000, 8.07603735e-2, 1.67297938e-1],\n                       [4.21445838e-2, 3.71538042e-3, 0., 3.439],\n                       [0., 0., 0., 0.]])\n\n    assert np.allclose(agent.Q.table, test_q)\n\n\ndef test_sarsa_save():\n    pi, mdp, _ = initialize()\n    agent_save = SARSA(mdp.info, pi, Parameter(.1))\n\n    core = Core(agent_save, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_sarsa_lambda_discrete():\n    pi, mdp, _ = initialize()\n    agent = SARSALambda(mdp.info, pi, Parameter(.1), .9)\n\n    core = Core(agent, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    test_q = np.array([[1.88093529, 2.42467354, 1.07390687, 2.39288988],\n                       [2.46058746, 4.68559, 1.5661933, 2.56586018],\n                       [1.24808966, 0.91948465, 0.47734152, 3.439],\n                       [0., 0., 0., 0.]])\n\n    assert np.allclose(agent.Q.table, test_q)\n\n\ndef test_sarsa_lambda_discrete_save():\n    pi, mdp, _ = initialize()\n    agent_save = SARSALambda(mdp.info, pi, Parameter(.1), .9)\n\n    core = Core(agent_save, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_sarsa_lambda_continuous_linear():\n    pi, _, mdp_continuous = initialize()\n    mdp_continuous.seed(1)\n    n_tilings = 1\n    tilings = Tiles.generate(n_tilings, [2, 2],\n                             mdp_continuous.info.observation_space.low,\n                             mdp_continuous.info.observation_space.high)\n    features = Features(tilings=tilings)\n\n    approximator_params = dict(\n        input_shape=(features.size,),\n        output_shape=(mdp_continuous.info.action_space.n,),\n        n_actions=mdp_continuous.info.action_space.n\n    )\n    agent = SARSALambdaContinuous(mdp_continuous.info, pi, LinearApproximator,\n                                  Parameter(.1), .9, features=features,\n                                  approximator_params=approximator_params)\n\n    core = Core(agent, mdp_continuous)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    test_w = np.array([-16.38428419, 0., -14.31250136, 0., -15.68571525, 0.,\n                       -10.15663821, 0., -15.0545445, 0., -8.3683605, 0.])\n\n    assert np.allclose(agent.Q.get_weights(), test_w)\n\n\ndef test_sarsa_lambda_continuous_linear_save():\n    pi, _, mdp_continuous = initialize()\n    mdp_continuous.seed(1)\n    n_tilings = 1\n    tilings = Tiles.generate(n_tilings, [2, 2],\n                             mdp_continuous.info.observation_space.low,\n                             mdp_continuous.info.observation_space.high)\n    features = Features(tilings=tilings)\n\n    approximator_params = dict(\n        input_shape=(features.size,),\n        output_shape=(mdp_continuous.info.action_space.n,),\n        n_actions=mdp_continuous.info.action_space.n\n    )\n    agent_save = SARSALambdaContinuous(mdp_continuous.info, pi, LinearApproximator,\n                                  Parameter(.1), .9, features=features,\n                                  approximator_params=approximator_params)\n\n    core = Core(agent_save, mdp_continuous)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_sarsa_lambda_continuous_nn():\n    pi, _, mdp_continuous = initialize()\n    mdp_continuous.seed(1)\n    \n    features = Features(\n        n_outputs=mdp_continuous.info.observation_space.shape[0]\n    )\n\n    approximator_params = dict(\n        input_shape=(features.size,),\n        output_shape=(mdp_continuous.info.action_space.n,),\n        network=Network,\n        n_actions=mdp_continuous.info.action_space.n\n    )\n    agent = SARSALambdaContinuous(mdp_continuous.info, pi, TorchApproximator,\n                                  Parameter(.1), .9, features=features,\n                                  approximator_params=approximator_params)\n\n    core = Core(agent, mdp_continuous)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    test_w = np.array([-0.18968964, 0.4296857, 0.52967095, 0.5674884,\n                       -0.12784956, -0.10572472, -0.14546978, -0.67001086,\n                       -0.93925357])\n\n    assert np.allclose(agent.Q.get_weights(), test_w)\n\n\ndef test_sarsa_lambda_continuous_nn_save():\n    pi, _, mdp_continuous = initialize()\n    mdp_continuous.seed(1)\n\n    features = Features(\n        n_outputs=mdp_continuous.info.observation_space.shape[0]\n    )\n\n    approximator_params = dict(\n        input_shape=(features.size,),\n        output_shape=(mdp_continuous.info.action_space.n,),\n        network=Network,\n        n_actions=mdp_continuous.info.action_space.n\n    )\n    agent_save = SARSALambdaContinuous(mdp_continuous.info, pi, TorchApproximator,\n                                  Parameter(.1), .9, features=features,\n                                  approximator_params=approximator_params)\n\n    core = Core(agent_save, mdp_continuous)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_expected_sarsa():\n    pi, mdp, _ = initialize()\n    agent = ExpectedSARSA(mdp.info, pi, Parameter(.1))\n\n    core = Core(agent, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    test_q = np.array([[0.10221208, 0.48411449, 0.07688765, 0.64002317],\n                       [0.58525881, 5.217031, 0.06047094, 0.48214145],\n                       [0.08478224, 0.28873536, 0.06543094, 4.68559],\n                       [0., 0., 0., 0.]])\n\n    assert np.allclose(agent.Q.table, test_q)\n\n\ndef test_expected_sarsa_save():\n    pi, mdp, _ = initialize()\n    agent_save = ExpectedSARSA(mdp.info, pi, Parameter(.1))\n\n    core = Core(agent_save, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_true_online_sarsa_lambda():\n    pi, _, mdp_continuous = initialize()\n    mdp_continuous.seed(1)\n    n_tilings = 1\n    tilings = Tiles.generate(n_tilings, [2, 2],\n                             mdp_continuous.info.observation_space.low,\n                             mdp_continuous.info.observation_space.high)\n    features = Features(tilings=tilings)\n\n    approximator_params = dict(\n        input_shape=(features.size,),\n        output_shape=(mdp_continuous.info.action_space.n,),\n        n_actions=mdp_continuous.info.action_space.n\n    )\n    agent = TrueOnlineSARSALambda(mdp_continuous.info, pi,\n                                  Parameter(.1), .9, features=features,\n                                  approximator_params=approximator_params)\n\n    core = Core(agent, mdp_continuous)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    test_w = np.array([-17.27410736, 0., -15.04386343, 0., -16.6551805, 0.,\n                       -11.31383707, 0., -16.11782002, 0., -9.6927357, 0.])\n\n    assert np.allclose(agent.Q.get_weights(), test_w)\n\n\ndef test_true_online_sarsa_lambda_save():\n    pi, _, mdp_continuous = initialize()\n    mdp_continuous.seed(1)\n    n_tilings = 1\n    tilings = Tiles.generate(n_tilings, [2, 2],\n                             mdp_continuous.info.observation_space.low,\n                             mdp_continuous.info.observation_space.high)\n    features = Features(tilings=tilings)\n\n    approximator_params = dict(\n        input_shape=(features.size,),\n        output_shape=(mdp_continuous.info.action_space.n,),\n        n_actions=mdp_continuous.info.action_space.n\n    )\n    agent_save = TrueOnlineSARSALambda(mdp_continuous.info, pi,\n                                  Parameter(.1), .9, features=features,\n                                  approximator_params=approximator_params)\n\n    core = Core(agent_save, mdp_continuous)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_r_learning():\n    pi, mdp, _ = initialize()\n    agent = RLearning(mdp.info, pi, Parameter(.1), Parameter(.5))\n\n    core = Core(agent, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    test_q = np.array([[-6.19137991, -3.9368055, -5.11544257, -3.43673781],\n                       [-2.52319391, 1.92201829, -2.77602918, -2.45972955],\n                       [-5.38824415, -2.43019918, -1.09965936, 2.04202511],\n                       [0., 0., 0., 0.]])\n\n    assert np.allclose(agent.Q.table, test_q)\n\n\ndef test_r_learning_save():\n    pi, mdp, _ = initialize()\n    agent_save = RLearning(mdp.info, pi, Parameter(.1), Parameter(.5))\n\n    core = Core(agent_save, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_rq_learning():\n    pi, mdp, _ = initialize()\n\n    agent = RQLearning(mdp.info, pi, Parameter(.1), beta=Parameter(.5))\n\n    core = Core(agent, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    test_q = np.array([[0.32411217, 2.9698436, 0.46474438, 1.10269504],\n                       [2.99505139, 5.217031, 0.40933461, 0.37687883],\n                       [0.41942675, 0.32363486, 0., 4.68559],\n                       [0., 0., 0., 0.]])\n\n    assert np.allclose(agent.Q.table, test_q)\n\n    agent = RQLearning(mdp.info, pi, Parameter(.1), delta=Parameter(.5))\n\n    core = Core(agent, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    test_q = np.array([[1.04081115e-2, 5.14662188e-1, 1.73951634e-2, 1.24081875e-01],\n                       [0., 2.71, 1.73137500e-4, 4.10062500e-6],\n                       [0., 4.50000000e-2, 0., 4.68559],\n                       [0., 0., 0., 0.]])\n\n    assert np.allclose(agent.Q.table, test_q)\n\n    agent = RQLearning(mdp.info, pi, Parameter(.1), off_policy=True,\n                       beta=Parameter(.5))\n\n    core = Core(agent, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    test_q = np.array([[3.55204022, 4.54235939, 3.42601165, 2.95170908],\n                       [2.73877031, 3.439, 2.42031528, 2.86634531],\n                       [3.43274708, 3.8592342, 3.72637395, 5.217031],\n                       [0., 0., 0., 0.]])\n\n    assert np.allclose(agent.Q.table, test_q)\n\n    agent = RQLearning(mdp.info, pi, Parameter(.1), off_policy=True,\n                       delta=Parameter(.5))\n\n    core = Core(agent, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    test_q = np.array([[0.18947806, 1.57782254, 0.21911489, 1.05197011],\n                       [0.82309759, 5.217031, 0.04167492, 0.61472604],\n                       [0.23620541, 0.59828262, 1.25299991, 5.217031],\n                       [0., 0., 0., 0.]])\n\n    assert np.allclose(agent.Q.table, test_q)\n\n\ndef test_rq_learning_save():\n    pi, mdp, _ = initialize()\n\n    agent_save = RQLearning(mdp.info, pi, Parameter(.1), beta=Parameter(.5))\n\n    core = Core(agent_save, mdp)\n\n    # Train\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n        tu.assert_eq(save_attr, load_attr)'"
tests/algorithms/test_trust_region.py,6,"b'# import sys\n# import os\n# sys.path = [os.getcwd()] + sys.path\n\nimport gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport shutil\nimport pathlib\nimport itertools\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom helper.utils import TestUtils as tu\n\nimport mushroom_rl\nfrom mushroom_rl.algorithms import Agent\n\nfrom mushroom_rl.algorithms.actor_critic import PPO, TRPO\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import Gym\nfrom mushroom_rl.policy import GaussianTorchPolicy\n\nclass Network(nn.Module):\n    def __init__(self, input_shape, output_shape, **kwargs):\n        super(Network, self).__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._h = nn.Linear(n_input, n_output)\n\n        nn.init.xavier_uniform_(self._h.weight,\n                                gain=nn.init.calculate_gain(\'relu\'))\n\n    def forward(self, state, **kwargs):\n        return F.relu(self._h(torch.squeeze(state, 1).float()))\n\ndef learn(alg, alg_params):\n\n    mdp = Gym(\'Pendulum-v0\', 200, .99)\n    mdp.seed(1)\n    np.random.seed(1)\n    torch.manual_seed(1)\n    torch.cuda.manual_seed(1)\n\n    critic_params = dict(network=Network,\n                         optimizer={\'class\': optim.Adam,\n                                    \'params\': {\'lr\': 3e-4}},\n                         loss=F.mse_loss,\n                         input_shape=mdp.info.observation_space.shape,\n                         output_shape=(1,))\n\n    policy_params = dict(std_0=1., use_cuda=False)\n\n    policy = GaussianTorchPolicy(Network,\n                                 mdp.info.observation_space.shape,\n                                 mdp.info.action_space.shape,\n                                 **policy_params)\n\n    alg_params[\'critic_params\'] = critic_params\n\n    agent = alg(mdp.info, policy, **alg_params)\n\n    core = Core(agent, mdp)\n\n    core.learn(n_episodes=2, n_episodes_per_fit=1)\n\n    return agent\n\n\ndef test_PPO():\n    params = dict(actor_optimizer={\'class\': optim.Adam,\n                                   \'params\': {\'lr\': 3e-4}},\n                  n_epochs_policy=4, batch_size=64, eps_ppo=.2, lam=.95,\n                  quiet=True)\n    policy = learn(PPO, params).policy\n    w = policy.get_weights()\n    w_test = np.array([-1.6293062, 1.0408604, -3.5757786e-1, 2.6958251e-1,\n                       -8.7002787e-4])\n\n    assert np.allclose(w, w_test)\n\n\ndef test_PPO_save():\n    params = dict(actor_optimizer={\'class\': optim.Adam,\n                                   \'params\': {\'lr\': 3e-4}},\n                  n_epochs_policy=4, batch_size=64, eps_ppo=.2, lam=.95,\n                  quiet=True)\n    \n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save = learn(PPO, params)\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n        tu.assert_eq(save_attr, load_attr)\n\n\ndef test_TRPO():\n    params = dict(ent_coeff=0.0, max_kl=.001, lam=.98, n_epochs_line_search=10,\n                  n_epochs_cg=10, cg_damping=1e-2, cg_residual_tol=1e-10,\n                  quiet=True)\n    policy = learn(TRPO, params).policy\n    w = policy.get_weights()\n    w_test = np.array([-1.5759772, 1.0822705, -0.37794656, 0.29728204,\n                       -0.0396419])\n\n    assert np.allclose(w, w_test)\n\n\ndef test_TRPO_save():\n    params = dict(ent_coeff=0.0, max_kl=.001, lam=.98, n_epochs_line_search=10,\n                  n_epochs_cg=10, cg_damping=1e-2, cg_residual_tol=1e-10,\n                  quiet=True)\n    \n    agent_path = \'./agentdir{}/\'.format(datetime.now().strftime(""%H%M%S%f""))\n\n    agent_save = learn(TRPO, params)\n\n    agent_save.save(agent_path)\n    agent_load = Agent.load(agent_path)\n\n    shutil.rmtree(agent_path)\n\n    for att, method in agent_save.__dict__.items():\n        save_attr = getattr(agent_save, att)\n        load_attr = getattr(agent_load, att)\n        #print(\'{}: {}\'.format(att, type(save_attr)))\n        tu.assert_eq(save_attr, load_attr)\n'"
tests/approximators/__init__.py,0,b''
tests/approximators/test_linear_approximator.py,0,"b""import numpy as np\n\nfrom mushroom_rl.approximators.regressor import Regressor\nfrom mushroom_rl.approximators.parametric import *\n\n\ndef test_linear_approximator():\n    np.random.seed(1)\n\n    # Generic regressor\n    a = np.random.rand(1000, 3)\n\n    k = np.random.rand(3, 2)\n    b = a.dot(k) + np.random.randn(1000, 2)\n\n    approximator = Regressor(LinearApproximator, input_shape=(3,),\n                             output_shape=(2,))\n\n    approximator.fit(a, b)\n\n    x = np.random.rand(2, 3)\n    y = approximator.predict(x)\n    y_test = np.array([[0.57638247, 0.1573216],\n                       [0.11388247, 0.24123678]])\n\n    assert np.allclose(y, y_test)\n\n    point = np.random.randn(3,)\n    derivative = approximator.diff(point)\n\n    lp = len(point)\n    for i in range(derivative.shape[1]):\n        assert (derivative[i * lp:(i + 1) * lp, i] == point).all()\n\n    old_weights = approximator.get_weights()\n    approximator.set_weights(old_weights)\n    new_weights = approximator.get_weights()\n\n    assert np.array_equal(new_weights, old_weights)\n\n    random_weights = np.random.randn(*old_weights.shape).astype(np.float32)\n    approximator.set_weights(random_weights)\n    random_weight_new = approximator.get_weights()\n\n    assert np.array_equal(random_weights, random_weight_new)\n    assert not np.any(np.equal(random_weights, old_weights))\n\n    # Action regressor + Ensemble\n    n_actions = 2\n    s = np.random.rand(1000, 3)\n    a = np.random.randint(n_actions, size=(1000, 1))\n    q = np.random.rand(1000)\n\n    approximator = Regressor(LinearApproximator, input_shape=(3,),\n                             n_actions=n_actions, n_models=5)\n\n    approximator.fit(s, a, q)\n\n    x_s = np.random.rand(2, 3)\n    x_a = np.random.randint(n_actions, size=(2, 1))\n    y = approximator.predict(x_s, x_a, prediction='mean')\n    y_test = np.array([0.49225698, 0.69660881])\n    assert np.allclose(y, y_test)\n\n    y = approximator.predict(x_s, x_a, prediction='sum')\n    y_test = np.array([2.46128492, 3.48304404])\n    assert np.allclose(y, y_test)\n\n    y = approximator.predict(x_s, x_a, prediction='min')\n    y_test = np.array([[0.49225698, 0.69660881]])\n    assert np.allclose(y, y_test)\n\n    y = approximator.predict(x_s)\n    y_test = np.array([[0.49225698, 0.44154141],\n                       [0.69660881, 0.69060195]])\n    assert np.allclose(y, y_test)\n\n    approximator = Regressor(LinearApproximator, input_shape=(3,),\n                             n_actions=n_actions)\n\n    approximator.fit(s, a, q)\n\n    gradient = approximator.diff(x_s[0], x_a[0])\n    gradient_test = np.array([0.88471362, 0.11666548, 0.45466254, 0., 0., 0.])\n\n    assert np.allclose(gradient, gradient_test)\n"""
tests/approximators/test_torch_approximator.py,5,"b""import numpy as np\n\nfrom mushroom_rl.approximators.regressor import Regressor\nfrom mushroom_rl.approximators.parametric import *\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\n\nclass ExampleNet(nn.Module):\n    def __init__(self, input_shape, output_shape,\n                 **kwargs):\n        super(ExampleNet, self).__init__()\n\n        self._q = nn.Linear(input_shape[0], output_shape[0])\n\n        nn.init.xavier_uniform_(self._q.weight,\n                                gain=nn.init.calculate_gain('linear'))\n\n    def forward(self, x, a=None):\n        x = x.float()\n        q = self._q(x)\n\n        if a is None:\n            return q\n        else:\n            action = a.long()\n            q_acted = torch.squeeze(q.gather(1, action))\n\n            return q_acted\n\n\ndef test_pytorch_approximator():\n    np.random.seed(1)\n    torch.manual_seed(1)\n\n    n_actions = 2\n    s = np.random.rand(1000, 4)\n    a = np.random.randint(n_actions, size=(1000, 1))\n    q = np.random.rand(1000)\n\n    approximator = Regressor(TorchApproximator, input_shape=(4,),\n                             output_shape=(2,), n_actions=n_actions,\n                             network=ExampleNet,\n                             optimizer={'class': optim.Adam,\n                                        'params': {}}, loss=F.mse_loss,\n                             batch_size=100, quiet=True)\n\n    approximator.fit(s, a, q, n_epochs=20)\n\n    x_s = np.random.rand(2, 4)\n    x_a = np.random.randint(n_actions, size=(2, 1))\n    y = approximator.predict(x_s, x_a)\n    y_test = np.array([0.37191153, 0.5920861])\n\n    assert np.allclose(y, y_test)\n\n    y = approximator.predict(x_s)\n    y_test = np.array([[0.47908658, 0.37191153],\n                       [0.5920861, 0.27575058]])\n\n    assert np.allclose(y, y_test)\n\n    gradient = approximator.diff(x_s[0], x_a[0])\n    gradient_test = np.array([0., 0., 0., 0., 0.02627479, 0.76513696,\n                              0.6672573, 0.35979462, 0., 1.])\n    assert np.allclose(gradient, gradient_test)\n\n    gradient = approximator.diff(x_s[0])\n    gradient_test = np.array([[0.02627479, 0.], [0.76513696, 0.],\n                              [0.6672573, 0.], [0.35979462, 0.],\n                              [0., 0.02627479], [0., 0.76513696],\n                              [0., 0.6672573], [0., 0.35979462], [1, 0.],\n                              [0., 1.]])\n    assert np.allclose(gradient, gradient_test)\n\n    old_weights = approximator.get_weights()\n    approximator.set_weights(old_weights)\n    new_weights = approximator.get_weights()\n\n    assert np.array_equal(new_weights, old_weights)\n\n    random_weights = np.random.randn(*old_weights.shape).astype(np.float32)\n    approximator.set_weights(random_weights)\n    random_weight_new = approximator.get_weights()\n\n    assert np.array_equal(random_weights, random_weight_new)\n    assert not np.any(np.equal(random_weights, old_weights))\n"""
tests/distributions/test_distribution_interface.py,0,"b'from mushroom_rl.distributions import Distribution\n\n\ndef abstract_method_tester(f, *args):\n    try:\n        f(*args)\n    except NotImplementedError:\n        pass\n    else:\n        assert False\n\n\ndef test_distribution_interface():\n    tmp = Distribution()\n\n    abstract_method_tester(tmp.sample)\n    abstract_method_tester(tmp.log_pdf, None)\n    abstract_method_tester(tmp.__call__, None)\n    abstract_method_tester(tmp.mle, None)\n    abstract_method_tester(tmp.diff_log, None)\n    abstract_method_tester(tmp.diff, None)\n\n    abstract_method_tester(tmp.get_parameters)\n    abstract_method_tester(tmp.set_parameters, None)\n\n    try:\n        tmp.parameters_size\n    except NotImplementedError:\n        pass\n    else:\n        assert False\n'"
tests/distributions/test_gaussian_distribution.py,0,"b'import numpy as np\nfrom mushroom_rl.distributions import *\nfrom mushroom_rl.utils.numerical_gradient import numerical_diff_dist\n\n\ndef test_gaussian():\n    np.random.seed(88)\n    n_dims = 6\n\n    random_matrix = np.random.rand(n_dims, n_dims)\n    sigma = random_matrix.dot(random_matrix.T)\n\n    mu = np.random.randn(n_dims)\n\n    dist = GaussianDistribution(mu, sigma)\n\n    for i in range(20):\n        theta = dist.sample()\n        exact_diff = dist.diff(theta)\n        numerical_diff = numerical_diff_dist(dist, theta)\n\n        assert np.allclose(exact_diff, numerical_diff)\n\n    theta = dist.sample()\n    log_p = dist.log_pdf(theta)\n    log_p_test = -5.622792079250801\n    assert np.isclose(log_p, log_p_test)\n\n    theta = np.random.randn(100, n_dims)\n\n    weights = np.random.rand(100)\n\n\n    dist.mle(theta)\n    assert np.array_equal(dist.get_parameters(), theta.mean(axis=0))\n\n    dist.mle(theta, weights)\n    assert np.array_equal(dist.get_parameters(), weights.dot(theta) / np.sum(weights))\n\n\ndef test_diagonal_gaussian():\n    np.random.seed(88)\n    n_dims = 6\n\n    std = np.abs(np.random.rand(n_dims))\n    mu = np.random.randn(n_dims)\n\n    dist = GaussianDiagonalDistribution(mu, std)\n\n    for i in range(20):\n        theta = dist.sample()\n        exact_diff = dist.diff(theta)\n        numerical_diff = numerical_diff_dist(dist, theta)\n\n        assert np.allclose(exact_diff, numerical_diff)\n\n    theta = dist.sample()\n    log_p = dist.log_pdf(theta)\n    log_p_test = -7.818947754486631\n    assert np.isclose(log_p, log_p_test)\n\n    theta = np.random.randn(100, n_dims)\n\n    weights = np.random.rand(100)\n\n    dist.mle(theta)\n    assert np.array_equal(dist.get_parameters()[:n_dims], theta.mean(axis=0))\n    assert np.array_equal(dist.get_parameters()[n_dims:], theta.std(axis=0))\n\n    dist.mle(theta, weights)\n    wmle_test = np.array([0.14420612, -0.02660736,  0.07439633, -0.00424596,\n                          0.2495252 , 0.20968329,  0.97527594,  1.08187144,\n                          1.04907019,  1.0634484 , 1.03453275,  1.03961039])\n    assert np.allclose(dist.get_parameters(), wmle_test)\n\n\ndef test_cholesky_gaussian():\n    np.random.seed(88)\n    n_dims = 6\n\n    random_matrix = np.random.rand(n_dims, n_dims)\n    sigma = random_matrix.dot(random_matrix.T)\n\n    mu = np.random.randn(n_dims)\n\n    dist = GaussianCholeskyDistribution(mu, sigma)\n\n    for i in range(20):\n        theta = dist.sample()\n        exact_diff = dist.diff(theta)\n        numerical_diff = numerical_diff_dist(dist, theta)\n\n        assert np.allclose(exact_diff, numerical_diff)\n\n    theta = dist.sample()\n    log_p = dist.log_pdf(theta)\n    log_p_test = -5.622792079250836\n    assert np.isclose(log_p, log_p_test)\n\n    theta = np.random.randn(100, n_dims)\n\n    weights = np.random.rand(100)\n\n    dist.mle(theta)\n\n    mle_test = np.array([0.03898376, 0.07252868,  0.26070238,  0.13782173,  0.18927999, -0.02548812,\n                         1.00855691, 0.19697324,  1.06381216, -0.07439629,  0.0656251 ,  1.02907183,\n                         0.03779866,-0.00504703, -0.14902275,  0.99917335, -0.09132656, -0.03225904,\n                        -0.13589437, 0.1419549,  0.94040997, -0.00145945,  0.00326904,  0.00291136,\n                        -0.07456335, 0.04581934,  1.02750578])\n    assert np.allclose(dist.get_parameters(), mle_test)\n\n    dist.mle(theta, weights)\n    wmle_test = np.array([-0.07797052, 0.08518447,  0.36272218, 0.17409145,  0.26339453, -0.02891896,\n                           0.98529941, 0.26728657,  1.09177517, -0.03838698, -0.08395759,  0.98168805,\n                           0.0150622, 0.05611417, -0.09351055,  1.0166716, -0.06390746, -0.05409177,\n                          -0.08944413, 0.17745539,  1.01277413, 0.00923361,  0.05694206, -0.02457328,\n                          -0.14141649, 0.1117947,  1.03121418])\n    assert np.allclose(dist.get_parameters(), wmle_test)\n'"
tests/environments/__init__.py,0,b''
tests/environments/test_all_envs.py,0,"b'import numpy as np\n\nfrom mushroom_rl.environments.atari import Atari\nfrom mushroom_rl.environments.car_on_hill import CarOnHill\nfrom mushroom_rl.environments.cart_pole import CartPole\nfrom mushroom_rl.environments.generators import generate_grid_world,\\\n    generate_simple_chain, generate_taxi\nfrom mushroom_rl.environments.grid_world import GridWorld, GridWorldVanHasselt\nfrom mushroom_rl.environments.gym_env import Gym\nfrom mushroom_rl.environments.inverted_pendulum import InvertedPendulum\nfrom mushroom_rl.environments.lqr import LQR\nfrom mushroom_rl.environments.puddle_world import PuddleWorld\nfrom mushroom_rl.environments.segway import Segway\nfrom mushroom_rl.environments.ship_steering import ShipSteering\n\nimport os\nos.environ[""SDL_VIDEODRIVER""] = ""dummy""\n\n\ndef test_atari():\n    np.random.seed(1)\n    mdp = Atari(name=\'PongDeterministic-v4\')\n    mdp.reset()\n    for i in range(10):\n        ns, r, ab, _ = mdp.step([np.random.randint(mdp.info.action_space.n)])\n    ns_test = np.load(\'tests/environments/test_atari_1.npy\')\n\n    assert np.allclose(ns, ns_test)\n\n    mdp = Atari(name=\'PongNoFrameskip-v4\')\n    mdp.reset()\n    for i in range(10):\n        ns, r, ab, _ = mdp.step([np.random.randint(mdp.info.action_space.n)])\n    ns_test = np.load(\'tests/environments/test_atari_2.npy\')\n\n    assert np.allclose(ns, ns_test)\n\n\ndef test_car_on_hill():\n    np.random.seed(1)\n    mdp = CarOnHill()\n    mdp.reset()\n    mdp.render()\n    for i in range(10):\n        ns, r, ab, _ = mdp.step([np.random.randint(mdp.info.action_space.n)])\n    ns_test = np.array([-0.29638141, -0.05527507])\n    mdp.render()\n\n    angle = mdp._angle(ns_test[0])\n    angle_test = -1.141676764064636\n\n    height = mdp._height(ns_test[0])\n    height_test = 0.9720652903871763\n\n    assert np.allclose(ns, ns_test)\n    assert np.allclose(angle, angle_test)\n    assert np.allclose(height, height_test)\n\n\ndef test_cartpole():\n    np.random.seed(1)\n    mdp = CartPole()\n    mdp.reset()\n    mdp.render()\n    for i in range(10):\n        ns, r, ab, _ = mdp.step([np.random.randint(mdp.info.action_space.n)])\n    ns_test = np.array([1.5195833, -2.82335548])\n    mdp.render()\n\n    assert np.allclose(ns, ns_test)\n\n\ndef test_finite_mdp():\n    np.random.seed(1)\n    mdp = generate_simple_chain(state_n=5, goal_states=[2], prob=.8, rew=1,\n                                gamma=.9)\n    mdp.reset()\n    for i in range(10):\n        ns, r, ab, _ = mdp.step([np.random.randint(mdp.info.action_space.n)])\n\n    assert ns == 4\n\n\ndef test_grid_world():\n    np.random.seed(1)\n    mdp = GridWorld(start=(0, 0), goal=(2, 2), height=3, width=3)\n    mdp.reset()\n    mdp.render()\n    for i in range(10):\n        ns, r, ab, _ = mdp.step([np.random.randint(mdp.info.action_space.n)])\n    mdp.render()\n\n    assert ns == 0\n\n    np.random.seed(1)\n    mdp = GridWorldVanHasselt()\n    mdp.reset()\n    for i in range(10):\n        ns, r, ab, _ = mdp.step([np.random.randint(mdp.info.action_space.n)])\n\n    assert ns == 6\n\n    np.random.seed(5)\n    mdp = generate_grid_world(\'tests/environments/grid.txt\', .9, 1, -1)\n    mdp.reset()\n    for i in range(10):\n        ns, r, ab, _ = mdp.step([np.random.randint(mdp.info.action_space.n)])\n\n    assert ns == 4\n\n\ndef test_gym():\n    np.random.seed(1)\n    mdp = Gym(\'Acrobot-v1\', 1000, .99)\n    mdp.seed(1)\n    mdp.reset()\n    for i in range(10):\n        ns, r, ab, _ = mdp.step([np.random.randint(mdp.info.action_space.n)])\n    ns_test = np.array([0.99989477, 0.01450661, 0.97517825, -0.22142128,\n                        -0.02323116, 0.40630765])\n\n    assert np.allclose(ns, ns_test)\n\n\ndef test_inverted_pendulum():\n    np.random.seed(1)\n    mdp = InvertedPendulum()\n    mdp.reset()\n    mdp.render()\n    for i in range(10):\n        ns, r, ab, _ = mdp.step([np.random.rand(mdp.info.action_space.shape[0])])\n    ns_test = np.array([1.62134054, 1.0107062])\n    mdp.render()\n\n    assert np.allclose(ns, ns_test)\n\n\ndef test_lqr():\n    np.random.seed(1)\n    mdp = LQR.generate(2)\n    mdp.reset()\n    for i in range(10):\n        ns, r, ab, _ = mdp.step(np.random.rand(mdp.info.action_space.shape[0]))\n    ns_test = np.array([12.35564605, 14.98996889])\n\n    assert np.allclose(ns, ns_test)\n\n    A = np.eye(3)\n    B = np.array([[2 / 3, 0], [1 / 3, 1 / 3], [0, 2 / 3]])\n    Q = np.array([[0.1, 0., 0.], [0., 0.9, 0.], [0., 0., 0.1]])\n    R = np.array([[0.1, 0.], [0., 0.9]])\n    mdp = LQR(A, B, Q, R, max_pos=11.0, max_action=0.5, episodic=True)\n    mdp.reset()\n\n    a_test = np.array([1.0, 0.3])\n    ns, r, ab, _ = mdp.step(a_test)\n    ns_test = np.array([10.23333333, 10.16666667, 10.1])\n    assert np.allclose(ns, ns_test) and np.allclose(r, -107.917) and not ab\n\n    a_test = np.array([0.4, -0.1])\n    ns, r, ab, _ = mdp.step(a_test)\n    ns_test = np.array([10.5, 10.26666667, 10.03333333])\n    assert np.allclose(ns, ns_test) and np.allclose(r, -113.72311111111117) and not ab\n\n    a_test = np.array([0.5, 0.6])\n    ns, r, ab, _ = mdp.step(a_test)\n    ns_test = np.array([10.83333333, 10.6, 10.36666667])\n    assert np.allclose(ns, ns_test) and np.allclose(r, -116.20577777777778) and not ab\n\n    a_test = np.array([0.3, -0.7])\n    ns, r, ab, _ = mdp.step(a_test)\n    ns_test = np.array([11.03333333, 10.53333333, 10.03333333])\n    assert np.allclose(ns, ns_test) and np.allclose(r, -1210.0) and ab\n\n\ndef test_puddle_world():\n    np.random.seed(1)\n    mdp = PuddleWorld()\n    mdp.reset()\n    mdp.render()\n    for i in range(10):\n        ns, r, ab, _ = mdp.step([np.random.randint(mdp.info.action_space.n)])\n    ns_test = np.array([0.41899424, 0.4022506])\n    mdp.render()\n\n    assert np.allclose(ns, ns_test)\n\n\ndef test_segway():\n    np.random.seed(1)\n    mdp = Segway()\n    mdp.reset()\n    mdp.render()\n    for i in range(10):\n        ns, r, ab, _ = mdp.step([np.random.rand()])\n    ns_test = np.array([-0.64112019, -4.92869367, 10.33970413])\n    mdp.render()\n\n    assert np.allclose(ns, ns_test)\n\n\ndef test_ship_steering():\n    np.random.seed(1)\n    mdp = ShipSteering()\n    mdp.reset()\n    mdp.render()\n    for i in range(10):\n        ns, r, ab, _ = mdp.step([np.random.rand()])\n    ns_test = np.array([0., 7.19403055, 1.66804923, 0.08134399])\n    mdp.render()\n\n    assert np.allclose(ns, ns_test)\n\n\ndef test_taxi():\n    np.random.seed(1)\n    mdp = generate_taxi(\'tests/environments/taxi.txt\')\n    mdp.reset()\n    for i in range(10):\n        ns, r, ab, _ = mdp.step([np.random.randint(mdp.info.action_space.n)])\n    ns_test = 5\n\n    assert np.allclose(ns, ns_test)\n'"
tests/environments/test_mujoco.py,0,"b""try:\n    from mushroom_rl.environments.dm_control_env import DMControl\n    import numpy as np\n\n    def test_dm_control():\n        np.random.seed(1)\n        mdp = DMControl('hopper', 'hop', 1000, .99, task_kwargs={'random': 1})\n        mdp.reset()\n        for i in range(10):\n            ns, r, ab, _ = mdp.step(\n                np.random.rand(mdp.info.action_space.shape[0]))\n        ns_test = np.array([-0.26244546, -2.33917271, 0.50130095, -0.50937527,\n                            0.55561752, -0.21111919, -0.55516933, -2.03929087,\n                            -18.22893801, 5.89523326, 22.07483625, -2.21756007,\n                            3.95695223, 0., 0.])\n\n        assert np.allclose(ns, ns_test)\nexcept ImportError:\n    pass\n"""
tests/features/test_features.py,0,"b'import numpy as np\n\nfrom mushroom_rl.features import Features\nfrom mushroom_rl.features.tiles import Tiles\nfrom mushroom_rl.features.basis import GaussianRBF, FourierBasis\nfrom mushroom_rl.features.tensors import PyTorchGaussianRBF\n\n\ndef test_tiles():\n    tilings = Tiles.generate(3, [3, 3],\n                             np.array([0., -.5]),\n                             np.array([1., .5]))\n    features = Features(tilings=tilings)\n\n    x = np.random.rand(10, 2) + [0., -.5]\n\n    y = features(x)\n\n    for i, x_i in enumerate(x):\n        assert np.all(features(x_i) == y[i])\n\n    x_1 = x[:, 0].reshape(-1, 1)\n    x_2 = x[:, 1].reshape(-1, 1)\n\n    assert np.all(features(x_1, x_2) == y)\n\n    for i, x_i in enumerate(zip(x_1, x_2)):\n        assert np.all(features(x_i[0], x_i[1]) == y[i])\n\n\ndef test_basis():\n    low = np.array([0., -.5])\n    high = np.array([1., .5])\n    rbf = GaussianRBF.generate([3, 3], high, low)\n    features = Features(basis_list=rbf)\n\n    x = np.random.rand(10, 2) + [0., -.5]\n\n    y = features(x)\n\n    for i, x_i in enumerate(x):\n        assert np.all(features(x_i) == y[i])\n\n    x_1 = x[:, 0].reshape(-1, 1)\n    x_2 = x[:, 1].reshape(-1, 1)\n\n    assert np.all(features(x_1, x_2) == y)\n\n    for i, x_i in enumerate(zip(x_1, x_2)):\n        assert np.all(features(x_i[0], x_i[1]) == y[i])\n\n\ndef test_tensor():\n    low = np.array([0., -.5])\n    high = np.array([1., .5])\n    rbf = PyTorchGaussianRBF.generate([3, 3], low, high)\n    features = Features(tensor_list=rbf)\n\n    x = np.random.rand(10, 2) + [0., -.5]\n\n    y = features(x)\n\n    for i, x_i in enumerate(x):\n        assert np.allclose(features(x_i), y[i])\n\n    x_1 = x[:, 0].reshape(-1, 1)\n    x_2 = x[:, 1].reshape(-1, 1)\n\n    assert np.allclose(features(x_1, x_2), y)\n\n    for i, x_i in enumerate(zip(x_1, x_2)):\n        assert np.allclose(features(x_i[0], x_i[1]), y[i])\n\n\ndef test_basis_and_tensors():\n    low = np.array([0., -.5])\n    high = np.array([1., .5])\n    basis_rbf = GaussianRBF.generate([3, 3], low, high)\n    tensor_rbf = PyTorchGaussianRBF.generate([3, 3], low, high)\n    features_1 = Features(tensor_list=tensor_rbf)\n    features_2 = Features(basis_list=basis_rbf)\n\n    x = np.random.rand(10, 2) + [0., -.5]\n\n    y_1 = features_1(x)\n    y_2 = features_2(x)\n\n    assert np.allclose(y_1, y_2)\n\n\ndef test_fourier():\n    low = np.array([-1.0, 0.5])\n    high = np.array([1.0, 2.5])\n    basis_list = FourierBasis.generate(low, high, 5)\n\n    features = Features(basis_list=basis_list)\n\n    x = np.array([0.1, 1.4])\n\n    res = np.array([1., -0.15643447, -0.95105652,  0.4539905, 0.80901699,\n                    -0.70710678, 0.15643447, -1., 0.15643447, 0.95105652,\n                    -0.4539905, -0.80901699, -0.95105652, -0.15643447, 1.,\n                    -0.15643447, -0.95105652, 0.4539905, -0.4539905,\n                    0.95105652, 0.15643447, -1., 0.15643447, 0.95105652,\n                    0.80901699, 0.4539905, -0.95105652, -0.15643447,  1.,\n                    -0.15643447, 0.70710678, -0.80901699, -0.4539905,\n                    0.95105652, 0.15643447, -1.])\n\n    assert np.allclose(features(x), res)\n'"
tests/policy/test_deterministic_policy.py,0,"b'from mushroom_rl.policy import DeterministicPolicy\nfrom mushroom_rl.approximators.regressor import Regressor\nfrom mushroom_rl.approximators.parametric import LinearApproximator\nfrom mushroom_rl.utils.numerical_gradient import numerical_diff_policy\n\nimport numpy as np\n\n\ndef test_deterministic_policy():\n    np.random.seed(88)\n\n    n_dims = 5\n\n    approximator = Regressor(LinearApproximator,\n                             input_shape=(n_dims,),\n                             output_shape=(2,))\n\n    pi = DeterministicPolicy(approximator)\n\n    w_new = np.random.rand(pi.weights_size)\n\n    w_old = pi.get_weights()\n    pi.set_weights(w_new)\n\n    assert np.array_equal(w_new, approximator.get_weights())\n    assert not np.array_equal(w_old, w_new)\n    assert np.array_equal(w_new, pi.get_weights())\n\n    s_test_1 = np.random.randn(5)\n    s_test_2 = np.random.randn(5)\n    a_test = approximator.predict(s_test_1)\n\n    assert pi.get_regressor() == approximator\n\n    assert pi(s_test_1, a_test) == 1\n    assert pi(s_test_2, a_test) == 0\n\n    a_stored = np.array([-1.86941072, -0.1789696])\n    assert np.allclose(pi.draw_action(s_test_1), a_stored)\n\n'"
tests/policy/test_gaussian_policy.py,0,"b'from mushroom_rl.policy.gaussian_policy import *\nfrom mushroom_rl.approximators.regressor import Regressor\nfrom mushroom_rl.approximators.parametric import LinearApproximator\nfrom mushroom_rl.utils.numerical_gradient import numerical_diff_policy\n\n\ndef test_univariate_gaussian():\n    np.random.seed(88)\n    sigma = 1e-3 * np.eye(1)\n\n    n_dims = 5\n\n    approximator = Regressor(LinearApproximator,\n                             input_shape=(n_dims,),\n                             output_shape=(1,))\n\n    pi = GaussianPolicy(approximator, sigma)\n    mu_weights = np.random.rand(pi.weights_size)\n    pi.set_weights(mu_weights)\n\n    x = np.random.randn(20, n_dims)\n\n    for x_i in x:\n        state = np.atleast_1d(x_i)\n        action = pi.draw_action(state)\n        exact_diff = pi.diff(state, action)\n        numerical_diff = numerical_diff_policy(pi, state, action)\n\n        assert np.allclose(exact_diff, numerical_diff)\n\n\ndef test_multivariate_gaussian():\n    np.random.seed(88)\n    n_dims = 5\n    n_outs = 3\n\n    random_matrix = np.random.rand(n_outs, n_outs)\n\n    sigma = random_matrix.dot(random_matrix.T)\n\n    approximator = Regressor(LinearApproximator,\n                             input_shape=(n_dims,),\n                             output_shape=(n_outs,))\n\n    pi = GaussianPolicy(approximator, sigma)\n    mu_weights = np.random.rand(pi.weights_size)\n    pi.set_weights(mu_weights)\n\n    x = np.random.randn(20, n_dims)\n\n    for x_i in x:\n        state = np.atleast_1d(x_i)\n        action = pi.draw_action(state)\n        exact_diff = pi.diff(state, action)\n        numerical_diff = numerical_diff_policy(pi, state, action)\n\n        assert np.allclose(exact_diff, numerical_diff)\n\n\ndef test_multivariate_diagonal_gaussian():\n    np.random.seed(88)\n    n_dims = 5\n    n_outs = 3\n\n    std = np.random.randn(n_outs)\n\n    approximator = Regressor(LinearApproximator,\n                             input_shape=(n_dims,),\n                             output_shape=(n_outs,))\n\n    pi = DiagonalGaussianPolicy(approximator, std)\n    mu_weights = np.random.rand(pi.weights_size)\n    pi.set_weights(mu_weights)\n\n    x = np.random.randn(20, n_dims)\n\n    for x_i in x:\n        state = np.atleast_1d(x_i)\n        action = pi.draw_action(state)\n        exact_diff = pi.diff(state, action)\n        numerical_diff = numerical_diff_policy(pi, state, action)\n\n        assert np.allclose(exact_diff, numerical_diff)\n\n\ndef test_multivariate_state_std_gaussian():\n    np.random.seed(88)\n    n_dims = 5\n    n_outs = 3\n\n    mu_approximator = Regressor(LinearApproximator,\n                                input_shape=(n_dims,),\n                                output_shape=(n_outs,))\n\n    std_approximator = Regressor(LinearApproximator,\n                                 input_shape=(n_dims,),\n                                 output_shape=(n_outs,))\n\n    pi = StateStdGaussianPolicy(mu_approximator, std_approximator)\n    weights = np.random.rand(pi.weights_size) + .1\n    pi.set_weights(weights)\n\n    x = np.random.randn(20, n_dims)\n\n    for x_i in x:\n        state = np.atleast_1d(x_i)\n        action = pi.draw_action(state)\n        exact_diff = pi.diff(state, action)\n        numerical_diff = numerical_diff_policy(pi, state, action)\n\n        assert np.allclose(exact_diff, numerical_diff)\n\n\ndef test_multivariate_state_log_std_gaussian():\n    np.random.seed(88)\n    n_dims = 5\n    n_outs = 3\n\n    mu_approximator = Regressor(LinearApproximator,\n                                input_shape=(n_dims,),\n                                output_shape=(n_outs,))\n\n    log_var_approximator = Regressor(LinearApproximator,\n                                     input_shape=(n_dims,),\n                                     output_shape=(n_outs,))\n\n    pi = StateLogStdGaussianPolicy(mu_approximator, log_var_approximator)\n    weights = np.random.rand(pi.weights_size)\n    pi.set_weights(weights)\n\n    x = np.random.randn(20, n_dims)\n\n    for x_i in x:\n        state = np.atleast_1d(x_i)\n        action = pi.draw_action(state)\n        exact_diff = pi.diff(state, action)\n        numerical_diff = numerical_diff_policy(pi, state, action)\n\n        assert np.allclose(exact_diff, numerical_diff)\n'"
tests/policy/test_noise_policy.py,0,"b'import numpy as np\n\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.approximators.parametric import LinearApproximator\nfrom mushroom_rl.policy import OrnsteinUhlenbeckPolicy\n\n\ndef test_ornstein_uhlenbeck_policy():\n    np.random.seed(88)\n\n    mu = Regressor(LinearApproximator,  input_shape=(5,), output_shape=(2,))\n    pi = OrnsteinUhlenbeckPolicy(mu, sigma=np.ones(1) * .2, theta=.15, dt=1e-2)\n\n    w = np.random.randn(pi.weights_size)\n    pi.set_weights(w)\n    assert np.array_equal(pi.get_weights(), w)\n\n    state = np.random.randn(5)\n\n    action = pi.draw_action(state)\n    action_test = np.array([-1.95896171,  1.91292747])\n    assert np.allclose(action, action_test)\n\n    pi.reset()\n    action = pi.draw_action(state)\n    action_test = np.array([-1.94161061,  1.92233358])\n    assert np.allclose(action, action_test)\n\n    try:\n        pi(state, action)\n    except NotImplementedError:\n        pass\n    else:\n        assert False\n\n'"
tests/policy/test_policy_interface.py,0,"b'from mushroom_rl.policy import Policy, ParametricPolicy\n\n\ndef abstract_method_tester(f, ex, *args):\n    try:\n        f(*args)\n    except ex:\n        pass\n    else:\n        assert False\n\n\ndef test_policy_interface():\n    tmp = Policy()\n    abstract_method_tester(tmp.__call__, NotImplementedError)\n    abstract_method_tester(tmp.draw_action, NotImplementedError, None)\n    tmp.reset()\n\n\ndef test_parametric_policy():\n    tmp = ParametricPolicy()\n    abstract_method_tester(tmp.diff_log, RuntimeError, None, None)\n    abstract_method_tester(tmp.diff, RuntimeError, None, None)\n    abstract_method_tester(tmp.set_weights, NotImplementedError, None)\n    abstract_method_tester(tmp.get_weights, NotImplementedError)\n    try:\n        tmp.weights_size\n    except NotImplementedError:\n        pass\n    else:\n        assert False\n'"
tests/policy/test_td_policy.py,0,"b'from mushroom_rl.policy.td_policy import *\nfrom mushroom_rl.utils.table import Table\nfrom mushroom_rl.utils.parameters import Parameter, LinearParameter\n\n\ndef test_td_policy():\n    Q = Table((10, 3))\n    pi = TDPolicy()\n\n    pi.set_q(Q)\n\n    assert Q == pi.get_q()\n\n\ndef test_eps_greedy():\n    np.random.seed(88)\n    eps = Parameter(0.1)\n    pi = EpsGreedy(eps)\n\n    Q = Table((10, 3))\n    Q.table = np.random.randn(10, 3)\n\n    pi.set_q(Q)\n\n    s = np.array([2])\n    a = np.array([1])\n\n    p_s = pi(s)\n    p_s_test = np.array([0.03333333, 0.93333333, 0.03333333])\n    assert np.allclose(p_s, p_s_test)\n\n    p_sa = pi(s, a)\n    p_sa_test = np.array([0.93333333])\n    assert np.allclose(p_sa, p_sa_test)\n\n    a = pi.draw_action(s)\n    a_test = 1\n    assert a.item() == a_test\n\n    eps_2 = LinearParameter(0.2, 0.1, 2)\n    pi.set_epsilon(eps_2)\n    p_sa_2 = pi(s, a)\n    assert p_sa_2 < p_sa\n\n    pi.update(s, a)\n    pi.update(s, a)\n    p_sa_3 = pi(s, a)\n    print(eps_2.get_value())\n    assert p_sa_3 == p_sa\n\n\ndef test_boltzmann():\n    np.random.seed(88)\n    beta = Parameter(0.1)\n    pi = Boltzmann(beta)\n\n    Q = Table((10, 3))\n    Q.table = np.random.randn(10, 3)\n\n    pi.set_q(Q)\n\n    s = np.array([2])\n    a = np.array([1])\n\n    p_s = pi(s)\n    p_s_test = np.array([0.30676679, 0.36223227, 0.33100094])\n    assert np.allclose(p_s, p_s_test)\n\n    p_sa = pi(s, a)\n    p_sa_test = np.array([0.36223227])\n    assert np.allclose(p_sa, p_sa_test)\n\n    a = pi.draw_action(s)\n    a_test = 2\n    assert a.item() == a_test\n\n    beta_2 = LinearParameter(0.2, 0.1, 2)\n    pi.set_beta(beta_2)\n    p_sa_2 = pi(s, a)\n    assert p_sa_2 < p_sa\n\n    pi.update(s, a)\n    p_sa_3 = pi(s, a)\n    p_sa_3_test = np.array([0.33100094])\n    assert np.allclose(p_sa_3, p_sa_3_test)\n\n\ndef test_mellowmax():\n    np.random.seed(88)\n    omega = Parameter(3)\n    pi = Mellowmax(omega)\n\n    Q = Table((10, 3))\n    Q.table = np.random.randn(10, 3)\n\n    pi.set_q(Q)\n\n    s = np.array([2])\n    a = np.array([1])\n\n    p_s = pi(s)\n    p_s_test = np.array([0.08540336, 0.69215916, 0.22243748])\n    assert np.allclose(p_s, p_s_test)\n\n    p_sa = pi(s, a)\n    p_sa_test = np.array([0.69215916])\n    assert np.allclose(p_sa, p_sa_test)\n\n    a = pi.draw_action(s)\n    a_test = 2\n    assert a.item() == a_test\n\n    try:\n        beta = Parameter(0.1)\n        pi.set_beta(beta)\n    except RuntimeError:\n        pass\n    else:\n        assert False\n\n    try:\n        pi.update(s,a)\n    except RuntimeError:\n        pass\n    else:\n        assert False\n'"
tests/policy/test_torch_policy.py,4,"b""import torch\nimport torch.nn as nn\n\nimport numpy as np\n\nfrom mushroom_rl.policy.torch_policy import TorchPolicy, GaussianTorchPolicy\n\n\ndef abstract_method_tester(f, *args):\n    try:\n        f(*args)\n    except NotImplementedError:\n        pass\n    else:\n        assert False\n\n\nclass Network(nn.Module):\n    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n        super(Network, self).__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._h1 = nn.Linear(n_input, n_features)\n        self._h2 = nn.Linear(n_features, n_features)\n        self._h3 = nn.Linear(n_features, n_output)\n\n        nn.init.xavier_uniform_(self._h1.weight,\n                                gain=nn.init.calculate_gain('tanh'))\n        nn.init.xavier_uniform_(self._h2.weight,\n                                gain=nn.init.calculate_gain('tanh'))\n        nn.init.xavier_uniform_(self._h3.weight,\n                                gain=nn.init.calculate_gain('linear'))\n\n    def forward(self, state, **kwargs):\n        features1 = torch.tanh(self._h1(torch.squeeze(state, -1).float()))\n        features2 = torch.tanh(self._h2(features1))\n        a = self._h3(features2)\n\n        return a\n\n\ndef test_torch_policy():\n    tmp = TorchPolicy(False)\n    abstract_method_tester(tmp.draw_action_t, None)\n    abstract_method_tester(tmp.log_prob_t, None, None)\n    abstract_method_tester(tmp.entropy_t, None)\n    abstract_method_tester(tmp.distribution_t, None)\n    abstract_method_tester(tmp.set_weights, None)\n    abstract_method_tester(tmp.get_weights)\n    abstract_method_tester(tmp.parameters)\n    tmp.reset()\n    tmp.use_cuda\n\n\ndef test_gaussian_torch_policy():\n    np.random.seed(88)\n    torch.manual_seed(88)\n    pi = GaussianTorchPolicy(Network, (3,), (2,), n_features=50)\n\n    state = np.random.rand(3)\n    action = pi.draw_action(state)\n    action_test = np.array([-0.21276927,  0.27437747])\n    assert np.allclose(action, action_test)\n\n    p_sa = pi(state, action)\n    p_sa_test = 0.07710557966732147\n    assert np.allclose(p_sa, p_sa_test)\n\n    entropy = pi.entropy()\n    entropy_test = 2.837877\n    assert np.allclose(entropy, entropy_test)\n\n\n"""
tests/solvers/test_car_on_hill.py,0,"b'import numpy as np\n\nfrom mushroom_rl.environments.car_on_hill import CarOnHill\nfrom mushroom_rl.solvers.car_on_hill import solve_car_on_hill\n\n\ndef test_car_on_hill():\n    mdp = CarOnHill()\n    mdp._discrete_actions = np.array([-8., 8.])\n\n    states = np.array([[-.5, 0], [0., 0.], [.5, 0.]])\n    actions = np.array([[0], [1], [0]])\n    q = solve_car_on_hill(mdp, states, actions, .95)\n    q_test = np.array([0.5688000922764597, 0.48767497911552954,\n                       0.5688000922764597])\n\n    assert np.allclose(q, q_test)\n'"
tests/solvers/test_dynamic_programming.py,0,"b'import numpy as np\nfrom mushroom_rl.solvers.dynamic_programming import policy_iteration, value_iteration\n\n\ndef test_value_iteration():\n    p = np.array([[[1., 0., 0., 0.],\n                   [0.1, 0., 0.9, 0.],\n                   [1., 0., 0., 0.],\n                   [0.1, 0.9, 0., 0.]],\n                  [[0., 1., 0., 0.],\n                   [0., 0.1, 0., 0.9],\n                   [0.9, 0.1, 0., 0.],\n                   [0., 1., 0., 0.]],\n                  [[0.9, 0., 0.1, 0.],\n                   [0., 0., 1., 0.],\n                   [0., 0., 1., 0.],\n                   [0., 0., 0.1, 0.9]],\n                  [[0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.]]])\n    r = np.array([[[0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.]],\n                  [[0., 0., 0., 0.],\n                   [0., 0., 0., 1.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.]],\n                  [[0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 1.]],\n                  [[0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.]]])\n    gamma = .95\n    eps = 1e-1\n\n    q = value_iteration(p, r, gamma, eps)\n    test_q = np.array([0.93653921, 0.99439414, 0.99439414, 0.])\n\n    assert np.allclose(q, test_q)\n\n\ndef test_policy_iteration():\n    p = np.array([[[1., 0., 0., 0.],\n                   [0.1, 0., 0.9, 0.],\n                   [1., 0., 0., 0.],\n                   [0.1, 0.9, 0., 0.]],\n                  [[0., 1., 0., 0.],\n                   [0., 0.1, 0., 0.9],\n                   [0.9, 0.1, 0., 0.],\n                   [0., 1., 0., 0.]],\n                  [[0.9, 0., 0.1, 0.],\n                   [0., 0., 1., 0.],\n                   [0., 0., 1., 0.],\n                   [0., 0., 0.1, 0.9]],\n                  [[0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.]]])\n    r = np.array([[[0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.]],\n                  [[0., 0., 0., 0.],\n                   [0., 0., 0., 1.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.]],\n                  [[0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 1.]],\n                  [[0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.],\n                   [0., 0., 0., 0.]]])\n    gamma = .95\n\n    q, p = policy_iteration(p, r, gamma)\n    q_test = np.array([0.93953176, 0.99447514, 0.99447514, 0.])\n    p_test = np.array([1, 1, 3, 0])\n\n    assert np.allclose(q, q_test) and np.allclose(p, p_test)\n'"
tests/utils/test_callbacks.py,0,"b'from mushroom_rl.core import Core\nfrom mushroom_rl.environments import GridWorld\nfrom mushroom_rl.algorithms.value import SARSA\nfrom mushroom_rl.policy import EpsGreedy\nfrom mushroom_rl.utils.parameters import Parameter, ExponentialParameter\nfrom mushroom_rl.utils.callbacks import *\nimport numpy as np\n\n\ndef test_collect_dataset():\n    np.random.seed(88)\n    callback = CollectDataset()\n\n    mdp = GridWorld(4, 4, (2, 2))\n\n    eps = Parameter(0.1)\n    pi = EpsGreedy(eps)\n    alpha = Parameter(0.2)\n    agent = SARSA(mdp.info, pi, alpha)\n\n    core = Core(agent, mdp, callbacks_episode=[callback])\n\n    core.learn(n_steps=10, n_steps_per_fit=1, quiet=True)\n\n    dataset = callback.get()\n    assert len(dataset) == 10\n    core.learn(n_steps=5, n_steps_per_fit=1, quiet=True)\n    assert len(dataset) == 15\n\n    callback.clean()\n    dataset = callback.get()\n    assert len(dataset) == 0\n\n\ndef test_collect_Q():\n    np.random.seed(88)\n    mdp = GridWorld(3, 3, (2, 2))\n\n    eps = Parameter(0.1)\n    pi = EpsGreedy(eps)\n    alpha = Parameter(0.1)\n    agent = SARSA(mdp.info, pi, alpha)\n\n    callback_q = CollectQ(agent.Q)\n    callback_max_q = CollectMaxQ(agent.Q, np.array([2]))\n\n    core = Core(agent, mdp, callbacks_episode=[callback_q, callback_max_q])\n\n    core.learn(n_steps=1000, n_steps_per_fit=1, quiet=True)\n\n    V_test = np.array([2.4477574 , 0.02246188, 1.6210059 , 6.01867052])\n    V = callback_q.get()[-1]\n\n    assert np.allclose(V[0, :], V_test)\n\n    V_max = np.array([np.max(x[2, :], axis=-1) for x in callback_q.get()])\n    max_q = np.array(callback_max_q.get())\n\n    assert np.allclose(V_max, max_q)\n\n\ndef test_collect_parameter():\n    np.random.seed(88)\n    mdp = GridWorld(3, 3, (2, 2))\n\n    eps = ExponentialParameter(value=1, exp=.5,\n                               size=mdp.info.observation_space.size)\n    pi = EpsGreedy(eps)\n    alpha = Parameter(0.1)\n    agent = SARSA(mdp.info, pi, alpha)\n\n    callback_eps = CollectParameters(eps, 1)\n\n    core = Core(agent, mdp, callbacks_episode=[callback_eps])\n\n    core.learn(n_steps=10, n_steps_per_fit=1, quiet=True)\n\n    eps_test = np.array([1., 0.70710678, 0.70710678, 0.57735027, 0.57735027,\n                         0.57735027, 0.57735027, 0.57735027, 0.57735027, 0.57735027])\n    eps = callback_eps.get()\n\n    assert np.allclose(eps, eps_test)\n'"
tests/utils/test_dataset.py,0,"b'from mushroom_rl.core import Core\nfrom mushroom_rl.algorithms.value import SARSA\nfrom mushroom_rl.environments import GridWorld\nfrom mushroom_rl.utils.parameters import Parameter\nfrom mushroom_rl.policy import EpsGreedy\n\nfrom mushroom_rl.utils.dataset import *\n\n\ndef test_dataset_utils():\n    np.random.seed(88)\n\n    mdp = GridWorld(3, 3, (2,2))\n    epsilon = Parameter(value=0.)\n    alpha = Parameter(value=0.)\n    pi = EpsGreedy(epsilon=epsilon)\n\n    agent = SARSA(mdp.info, pi, alpha)\n    core = Core(agent, mdp)\n\n    dataset = core.evaluate(n_episodes=10)\n\n    J = compute_J(dataset, mdp.info.gamma)\n    J_test = np.array([1.16106307e-03, 2.78128389e-01, 1.66771817e+00, 3.09031544e-01,\n                       1.19725152e-01, 9.84770902e-01, 1.06111661e-02, 2.05891132e+00,\n                       2.28767925e+00, 4.23911583e-01])\n    assert np.allclose(J, J_test)\n\n    L = episodes_length(dataset)\n    L_test = np.array([87, 35, 18, 34, 43, 23, 66, 16, 15, 31])\n    assert np.array_equal(L, L_test)\n\n    dataset_ep = select_first_episodes(dataset, 3)\n    J = compute_J(dataset_ep, mdp.info.gamma)\n    assert np.allclose(J, J_test[:3])\n\n    L = episodes_length(dataset_ep)\n    assert np.allclose(L, L_test[:3])\n\n    samples = select_random_samples(dataset, 2)\n    s, a, r, ss, ab, last = parse_dataset(samples)\n    s_test = np.array([[6.], [1.]])\n    a_test = np.array([[0.], [1.]])\n    r_test = np.zeros(2)\n    ss_test = np.array([[3], [4]])\n    ab_test = np.zeros(2)\n    last_test = np.zeros(2)\n    assert np.array_equal(s, s_test)\n    assert np.array_equal(a, a_test)\n    assert np.array_equal(r, r_test)\n    assert np.array_equal(ss, ss_test)\n    assert np.array_equal(ab, ab_test)\n    assert np.array_equal(last, last_test)\n\n    index = np.sum(L_test[:2]) + L_test[2]//2\n    min_J, max_J, mean_J, n_episodes = compute_metrics(dataset[:index], mdp.info.gamma)\n    assert min_J == 0.0011610630703530948\n    assert max_J == 0.2781283894436937\n    assert mean_J == 0.1396447262570234\n    assert n_episodes == 2\n'"
tests/utils/test_folder.py,0,"b'from mushroom_rl.utils.folder import *\n\n\ndef test_folder_utils(tmpdir):\n    dir_path_1 = tmpdir / \'foo/bar\'\n    dir_path_2 = tmpdir / \'foo/foobar\'\n    mk_dir_recursive(str(dir_path_1))\n    mk_dir_recursive(str(dir_path_2))\n    filename = dir_path_1 / \'test.txt\'\n    filename.write(""content"")\n\n    assert len(dir_path_2.listdir()) == 0\n    assert len(dir_path_1.listdir()) == 1\n\n    symlink = tmpdir / \'foo/foofoo\'\n    force_symlink(str(dir_path_2), str(symlink))\n    force_symlink(str(dir_path_1), str(symlink))\n\n    filename_linked = symlink / \'test.txt\'\n    assert filename_linked.read() == ""content""\n\n\n\n\n\n'"
tests/utils/test_preprocessors.py,3,"b'import os\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom mushroom_rl.utils.parameters import Parameter\n\nfrom mushroom_rl.policy import EpsGreedy\n\nfrom mushroom_rl.algorithms.value import DQN\n\nfrom mushroom_rl.core import Core\n\nfrom mushroom_rl.approximators.parametric import TorchApproximator\nfrom torch import optim, nn\n\nfrom mushroom_rl.environments import Gym\nfrom mushroom_rl.utils.preprocessors import MinMaxPreprocessor\n\n\ndef test_normalizing_preprocessor():\n    np.random.seed(88)\n\n    class Network(nn.Module):\n        def __init__(self, input_shape, output_shape, **kwargs):\n            super().__init__()\n\n            n_input = input_shape[-1]\n            n_output = output_shape[0]\n\n            self._h1 = nn.Linear(n_input, n_output)\n\n            nn.init.xavier_uniform_(self._h1.weight,\n                                    gain=nn.init.calculate_gain(\'relu\'))\n\n        def forward(self, state, action=None):\n            q = F.relu(self._h1(torch.squeeze(state, 1).float()))\n            if action is None:\n                return q\n            else:\n                action = action.long()\n                q_acted = torch.squeeze(q.gather(1, action))\n                return q_acted\n\n    mdp = Gym(\'CartPole-v0\', horizon=500, gamma=.99)\n\n    # Policy\n    epsilon_random = Parameter(value=1.)\n    pi = EpsGreedy(epsilon=epsilon_random)\n\n    # Approximator\n    input_shape = mdp.info.observation_space.shape\n\n    approximator_params = dict(network=Network,\n                               optimizer={\'class\':  optim.Adam,\n                                          \'params\': {\'lr\': .001}},\n                               loss=F.smooth_l1_loss,\n                               input_shape=input_shape,\n                               output_shape=mdp.info.action_space.size,\n                               n_actions=mdp.info.action_space.n,\n                               n_features=2, use_cuda=False)\n\n    alg_params = dict(batch_size=5, n_approximators=1, initial_replay_size=10,\n                      max_replay_size=500, target_update_frequency=50)\n\n    agent = DQN(mdp.info, pi, TorchApproximator,\n                approximator_params=approximator_params, **alg_params)\n\n    norm_box = MinMaxPreprocessor(mdp_info=mdp.info,\n                                  clip_obs=5.0, alpha=0.001)\n\n    core = Core(agent, mdp, preprocessors=[norm_box])\n\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    # training correctly\n    assert (core._state.min() >= -norm_box.clip_obs\n            and core._state.max() <= norm_box.clip_obs)\n\n    # loading and setting data correctly\n    state_dict1 = norm_box.get_state()\n    norm_box.save_state(""./run_norm_state"")\n\n    core.learn(n_steps=100, n_steps_per_fit=1, quiet=True)\n\n    norm_box.load_state(""./run_norm_state"")\n    state_dict2 = norm_box.get_state()\n\n    os.remove(""./run_norm_state"")\n    assert ((state_dict1[""mean""] == state_dict2[""mean""]).all()\n            and (state_dict1[""var""] == state_dict2[""var""]).all()\n            and state_dict1[""count""] == state_dict2[""count""])\n'"
mushroom_rl/algorithms/actor_critic/__init__.py,0,"b""from .classic_actor_critic import StochasticAC, StochasticAC_AVG, COPDAC_Q\nfrom .deep_actor_critic import DeepAC, A2C, DDPG, TD3, SAC, TRPO, PPO\n\n__all__ = ['COPDAC_Q', 'StochasticAC', 'StochasticAC_AVG',\n           'DeepAC', 'A2C', 'DDPG', 'TD3', 'SAC', 'TRPO', 'PPO']\n"""
mushroom_rl/algorithms/policy_search/__init__.py,0,"b""from .policy_gradient import REINFORCE, GPOMDP, eNAC\nfrom .black_box_optimization import RWR, PGPE, REPS\n\n\n__all__ = ['REINFORCE', 'GPOMDP', 'eNAC', 'RWR', 'PGPE', 'REPS']\n"""
mushroom_rl/algorithms/value/__init__.py,0,"b""from .batch_td import *\nfrom .dqn import *\nfrom .td import *\n\n__all__ = ['FQI', 'DoubleFQI', 'LSPI', 'DQN', 'DoubleDQN',\n           'AveragedDQN', 'CategoricalDQN', 'QLearning', 'DoubleQLearning',\n           'WeightedQLearning', 'SpeedyQLearning', 'RLearning', 'RQLearning',\n           'SARSA', 'SARSALambda', 'SARSALambdaContinuous',\n           'ExpectedSARSA', 'TrueOnlineSARSALambda']\n"""
mushroom_rl/approximators/_implementations/__init__.py,0,b'\n'
mushroom_rl/approximators/_implementations/action_regressor.py,0,"b'import numpy as np\n\n\nclass ActionRegressor:\n    """"""\n    This class is used to approximate the Q-function with a different\n    approximator of the provided class for each action. It is often used in MDPs\n    with discrete actions and should not be used in MDPs with continuous\n    actions.\n\n    """"""\n    def __init__(self, approximator, n_actions, **params):\n        """"""\n        Constructor.\n\n        Args:\n            approximator (object): the model class to approximate the\n                Q-function of each action;\n            n_actions (int): number of different actions of the problem. It\n                determines the number of different regressors in the action\n                regressor;\n            **params (dict): parameters dictionary to create each regressor.\n\n        """"""\n        self.model = list()\n        self._n_actions = n_actions\n\n        for i in range(self._n_actions):\n            self.model.append(approximator(**params))\n\n    def fit(self, state, action, q, **fit_params):\n        """"""\n        Fit the model.\n\n        Args:\n            state (np.ndarray): states;\n            action (np.ndarray): actions;\n            q (np.ndarray): target q-values;\n            **fit_params (dict): other parameters used by the fit method\n                of each regressor.\n\n        """"""\n        for i in range(len(self.model)):\n            idxs = np.argwhere((action == i)[:, 0]).ravel()\n\n            if idxs.size:\n                self.model[i].fit(state[idxs, :], q[idxs], **fit_params)\n\n    def predict(self, *z, **predict_params):\n        """"""\n        Predict.\n\n        Args:\n            *z (list): a list containing states or states and actions depending\n                on whether the call requires to predict all q-values or only\n                one q-value corresponding to the provided action;\n            **predict_params (dict): other parameters used by the predict method\n                of each regressor.\n\n        Returns:\n            The predictions of the model.\n\n        """"""\n        assert len(z) == 1 or len(z) == 2\n\n        state = z[0]\n        if len(z) == 2:\n            action = z[1]\n            q = np.zeros(state.shape[0])\n            for i in range(self._n_actions):\n                idxs = np.argwhere((action == i)[:, 0]).ravel()\n                if idxs.size:\n                    q[idxs] = self.model[i].predict(state[idxs, :],\n                                                    **predict_params).squeeze()\n        else:\n            q = np.zeros((state.shape[0], self._n_actions))\n            for i in range(self._n_actions):\n                q[:, i] = self.model[i].predict(state,\n                                                **predict_params).squeeze()\n\n        return q\n\n    def reset(self):\n        """"""\n        Reset the model parameters.\n\n        """"""\n        try:\n            for m in self.model:\n                m.reset()\n        except AttributeError:\n            raise NotImplementedError(\'Attempt to reset weights of a\'\n                                      \' non-parametric regressor.\')\n\n    @property\n    def weights_size(self):\n        return self.model[0].weights_size * len(self.model)\n\n    def get_weights(self):\n        w = list()\n        for m in self.model:\n            w.append(m.get_weights())\n\n        return np.concatenate(w, axis=0)\n\n    def set_weights(self, w):\n        size = self.model[0].weights_size\n        for i, m in enumerate(self.model):\n            start = i * size\n            stop = start + size\n            m.set_weights(w[start:stop])\n\n    def diff(self, state, action):\n        if action is None:\n            diff = list()\n            for m in self.model:\n                diff.append(m.diff(state))\n\n            return diff\n        else:\n            diff = np.zeros(len(state) * len(self.model))\n            a = action[0]\n            s = len(state)\n            diff[s * a:s * (a + 1)] = self.model[a].diff(state)\n\n            return diff\n\n    def __len__(self):\n        return len(self.model[0])\n'"
mushroom_rl/approximators/_implementations/ensemble.py,0,"b'import numpy as np\nfrom sklearn.exceptions import NotFittedError\n\n\nclass Ensemble(object):\n    """"""\n    This class is used to create an ensemble of regressors.\n\n    """"""\n    def __init__(self, model, n_models, **params):\n        """"""\n        Constructor.\n\n        Args:\n            approximator (object): the model class to approximate the\n                Q-function.\n            n_models (int): number of regressors in the ensemble;\n            **params (dict): parameters dictionary to create each regressor.\n\n        """"""\n        self._model = list()\n\n        for _ in range(n_models):\n            self._model.append(model(**params))\n\n    def fit(self, *z, idx=None, **fit_params):\n        """"""\n        Fit the ``idx``-th model of the ensemble if ``idx`` is provided, every\n        model otherwise.\n\n        Args:\n            *z (list): a list containing the inputs to use to predict with each\n                regressor of the ensemble;\n            idx (int, None): index of the model to fit;\n            **fit_params (dict): other params.\n\n        """"""\n        if idx is None:\n            for i in range(len(self)):\n                self[i].fit(*z, **fit_params)\n        else:\n            self[idx].fit(*z, **fit_params)\n\n    def predict(self, *z, idx=None, prediction=\'mean\', compute_variance=False,\n                **predict_params):\n        """"""\n        Predict.\n\n        Args:\n            *z (list): a list containing the inputs to use to predict with each\n                regressor of the ensemble;\n            idx (int, None): index of the model to use for prediction;\n            prediction (str, \'mean\'): the type of prediction to make. It can\n                be a \'mean\' of the ensembles, or a \'sum\';\n            compute_variance (bool, False): whether to compute the variance\n                of the prediction or not;\n            **predict_params (dict): other parameters used by the predict method\n                the regressor.\n\n        Returns:\n            The predictions of the model.\n\n        """"""\n        if idx is None:\n            predictions = list()\n            for i in range(len(self._model)):\n                try:\n                    predictions.append(self[i].predict(*z, **predict_params))\n                except NotFittedError:\n                    pass\n\n            if len(predictions) == 0:\n                raise NotFittedError\n\n            if prediction == \'mean\':\n                results = np.mean(predictions, axis=0)\n            elif prediction == \'sum\':\n                results = np.sum(predictions, axis=0)\n            elif prediction == \'min\':\n                results = np.amin(predictions, axis=0)\n            else:\n                raise ValueError\n            if compute_variance:\n                results = [results, np.var(predictions, ddof=1, axis=0)]\n        else:\n            try:\n                results = self[idx].predict(*z, **predict_params)\n            except NotFittedError:\n                raise NotFittedError\n\n        return results\n\n    def reset(self):\n        """"""\n        Reset the model parameters.\n\n        """"""\n        try:\n            for m in self.model:\n                m.reset()\n        except AttributeError:\n            raise NotImplementedError(\'Attempt to reset weights of a\'\n                                      \' non-parametric regressor.\')\n\n    @property\n    def model(self):\n        """"""\n        Returns:\n            The list of the models in the ensemble.\n\n        """"""\n        return self._model\n\n    def __len__(self):\n        return len(self._model)\n\n    def __getitem__(self, idx):\n        return self._model[idx]\n'"
mushroom_rl/approximators/_implementations/generic_regressor.py,0,"b'class GenericRegressor:\n    """"""\n    This class is used to create a regressor that approximates a generic\n    function. An arbitrary number of inputs and outputs is supported.\n\n    """"""\n    def __init__(self, approximator, n_inputs, **params):\n        """"""\n        Constructor.\n\n        Args:\n            approximator (object): the model class to approximate the\n                a generic function;\n            n_inputs (int): number of inputs of the regressor;\n            **params (dict): parameters dictionary to the regressor;\n\n        """"""\n        self._n_inputs = n_inputs\n        self.model = approximator(**params)\n\n    def fit(self, *z, **fit_params):\n        """"""\n        Fit the model.\n\n        Args:\n            *z (list): list of inputs and targets;\n            **fit_params (dict): other parameters used by the fit method of the\n                regressor.\n\n        """"""\n        self.model.fit(*z, **fit_params)\n\n    def predict(self, *x, **predict_params):\n        """"""\n        Predict.\n\n        Args:\n            x (list): list of inputs;\n            **predict_params (dict): other parameters used by the predict method\n                the regressor.\n\n        Returns:\n            The predictions of the model.\n\n        """"""\n        return self.model.predict(*x, **predict_params)\n\n    def reset(self):\n        """"""\n        Reset the model parameters.\n\n        """"""\n        try:\n            self.model.reset()\n        except AttributeError:\n            raise NotImplementedError(\'Attempt to reset weights of a\'\n                                      \' non-parametric regressor.\')\n\n    @property\n    def weights_size(self):\n        return self.model.weights_size\n\n    def get_weights(self):\n        return self.model.get_weights()\n\n    def set_weights(self, w):\n        self.model.set_weights(w)\n\n    def diff(self, *x):\n        return self.model.diff(*x)\n\n    def __len__(self):\n        return len(self.model)\n'"
mushroom_rl/approximators/_implementations/q_regressor.py,0,"b'import numpy as np\n\n\nclass QRegressor:\n    """"""\n    This class is used to create a regressor that approximates the Q-function\n    using a multi-dimensional output where each output corresponds to the\n    Q-value of each action. This is used, for instance, by the ``ConvNet`` used\n    in examples/atari_dqn.\n\n    """"""\n    def __init__(self, approximator, **params):\n        """"""\n        Constructor.\n\n        Args:\n            approximator (object): the model class to approximate the\n                Q-function;\n            params (dict): parameters dictionary to the regressor.\n\n        """"""\n        self.model = approximator(**params)\n\n    def fit(self, state, action, q, **fit_params):\n        """"""\n        Fit the model.\n\n        Args:\n            state (np.ndarray): states;\n            action (np.ndarray): actions;\n            q (np.ndarray): target q-values;\n            **fit_params (dict): other parameters used by the fit method of the\n                regressor.\n\n        """"""\n        self.model.fit(state, action, q, **fit_params)\n\n    def predict(self, *z, **predict_params):\n        """"""\n        Predict.\n\n        Args:\n            *z (list): a list containing states or states and actions depending\n                on whether the call requires to predict all q-values or only\n                one q-value corresponding to the provided action;\n            **predict_params (dict): other parameters used by the predict method\n                of each regressor.\n\n        Returns:\n            The predictions of the model.\n\n        """"""\n        assert len(z) == 1 or len(z) == 2\n\n        state = z[0]\n        q = self.model.predict(state, **predict_params)\n\n        if len(z) == 2:\n            action = z[1].ravel()\n            if q.ndim == 1:\n                return q[action]\n            else:\n                return q[np.arange(q.shape[0]), action]\n        else:\n            return q\n\n    def reset(self):\n        """"""\n        Reset the model parameters.\n\n        """"""\n        try:\n            self.model.reset()\n        except AttributeError:\n            raise NotImplementedError(\'Attempt to reset weights of a\'\n                                      \' non-parametric regressor.\')\n\n    @property\n    def weights_size(self):\n        return self.model.weights_size\n\n    def get_weights(self):\n        return self.model.get_weights()\n\n    def set_weights(self, w):\n        self.model.set_weights(w)\n\n    def diff(self, state, action=None):\n        if action is None:\n            return self.model.diff(state)\n        else:\n            return self.model.diff(state, action).squeeze()\n\n    def __len__(self):\n        return len(self.model)\n'"
mushroom_rl/approximators/parametric/__init__.py,0,"b""from .linear import LinearApproximator\nfrom .torch_approximator import TorchApproximator\n\n__all__ = ['LinearApproximator', 'TorchApproximator']\n"""
mushroom_rl/approximators/parametric/linear.py,0,"b'import numpy as np\n\n\nclass LinearApproximator:\n    """"""\n    This class implements a linear approximator.\n\n    """"""\n    def __init__(self, weights=None, input_shape=None, output_shape=(1,),\n                 **kwargs):\n        """"""\n        Constructor.\n\n        Args:\n             weights (np.ndarray): array of weights to initialize the weights\n                of the approximator;\n             input_shape (np.ndarray, None): the shape of the input of the\n                model;\n             output_shape (np.ndarray, (1,)): the shape of the output of the\n                model;\n             **kwargs (dict): other params of the approximator.\n\n        """"""\n        assert len(input_shape) == 1 and len(output_shape) == 1\n\n        input_dim = input_shape[0]\n        output_dim = output_shape[0]\n\n        if weights is not None:\n            self._w = weights.reshape((output_dim, -1))\n        elif input_dim is not None:\n            self._w = np.zeros((output_dim, input_dim))\n        else:\n            raise ValueError(\'You should specify the initial parameter vector\'\n                             \' or the input dimension\')\n\n    def fit(self, x, y, **fit_params):\n        """"""\n        Fit the model.\n\n        Args:\n            x (np.ndarray): input;\n            y (np.ndarray): target;\n            **fit_params (dict): other parameters used by the fit method of the\n                regressor.\n\n        """"""\n        self._w = np.atleast_2d(np.linalg.pinv(x).dot(y).T)\n\n    def predict(self, x, **predict_params):\n        """"""\n        Predict.\n\n        Args:\n            x (np.ndarray): input;\n            **predict_params (dict): other parameters used by the predict method\n                the regressor.\n\n        Returns:\n            The predictions of the model.\n\n        """"""\n        prediction = np.ones((x.shape[0], self._w.shape[0]))\n        for i, x_i in enumerate(x):\n            prediction[i] = x_i.dot(self._w.T)\n\n        return prediction\n\n    @property\n    def weights_size(self):\n        """"""\n        Returns:\n            The size of the array of weights.\n\n        """"""\n        return self._w.size\n\n    def get_weights(self):\n        """"""\n        Getter.\n\n        Returns:\n            The set of weights of the approximator.\n\n        """"""\n        return self._w.flatten()\n\n    def set_weights(self, w):\n        """"""\n        Setter.\n\n        Args:\n            w (np.ndarray): the set of weights to set.\n\n        """"""\n        self._w = w.reshape(self._w.shape)\n\n    def diff(self, state, action=None):\n        """"""\n        Compute the derivative of the output w.r.t. ``state``, and ``action``\n        if provided.\n\n        Args:\n            state (np.ndarray): the state;\n            action (np.ndarray, None): the action.\n\n        Returns:\n            The derivative of the output w.r.t. ``state``, and ``action``\n            if provided.\n\n        """"""\n        if len(self._w.shape) == 1 or self._w.shape[0] == 1:\n            return state\n        else:\n            n_phi = self._w.shape[1]\n            n_outs = self._w.shape[0]\n\n            if action is None:\n                shape = (n_phi * n_outs, n_outs)\n                df = np.zeros(shape)\n                start = 0\n                for i in range(n_outs):\n                    stop = start + n_phi\n                    df[start:stop, i] = state\n                    start = stop\n            else:\n                shape = (n_phi * n_outs)\n                df = np.zeros(shape)\n                start = action[0] * n_phi\n                stop = start + n_phi\n                df[start:stop] = state\n\n            return df\n'"
mushroom_rl/approximators/parametric/torch_approximator.py,9,"b'import torch\nimport numpy as np\nfrom tqdm import trange, tqdm\n\nfrom mushroom_rl.utils.minibatches import minibatch_generator\nfrom mushroom_rl.utils.torch import get_weights, set_weights, zero_grad\n\n\nclass TorchApproximator:\n    """"""\n    Class to interface a pytorch model to the mushroom Regressor interface.\n    This class implements all is needed to use a generic pytorch model and train\n    it using a specified optimizer and objective function.\n    This class supports also minibatches.\n\n    """"""\n    def __init__(self, input_shape, output_shape, network, optimizer=None,\n                 loss=None, batch_size=0, n_fit_targets=1, use_cuda=False,\n                 reinitialize=False, dropout=False, quiet=True, **params):\n        """"""\n        Constructor.\n\n        Args:\n            input_shape (tuple): shape of the input of the network;\n            output_shape (tuple): shape of the output of the network;\n            network (torch.nn.Module): the network class to use;\n            optimizer (dict): the optimizer used for every fit step;\n            loss (torch.nn.functional): the loss function to optimize in the\n                fit method;\n            batch_size (int, 0): the size of each minibatch. If 0, the whole\n                dataset is fed to the optimizer at each epoch;\n            n_fit_targets (int, 1): the number of fit targets used by the fit\n                method of the network;\n            use_cuda (bool, False): if True, runs the network on the GPU;\n            reinitialize (bool, False): if True, the approximator is re\n            initialized at every fit call. To perform the initialization, the\n            weights_init method must be defined properly for the selected\n            model network.\n            dropout (bool, False): if True, dropout is applied only during\n                train;\n            quiet (bool, True): if False, shows two progress bars, one for\n                epochs and one for the minibatches;\n            params (dict): dictionary of parameters needed to construct the\n                network.\n\n        """"""\n        self._batch_size = batch_size\n        self._reinitialize = reinitialize\n        self._use_cuda = use_cuda\n        self._dropout = dropout\n        self._quiet = quiet\n        self._n_fit_targets = n_fit_targets\n\n        self.network = network(input_shape, output_shape, use_cuda=use_cuda,\n                               dropout=dropout, **params)\n\n        if self._use_cuda:\n            self.network.cuda()\n        if self._dropout:\n            self.network.eval()\n\n        if optimizer is not None:\n            self._optimizer = optimizer[\'class\'](self.network.parameters(),\n                                                 **optimizer[\'params\'])\n        self._loss = loss\n\n    def predict(self, *args, output_tensor=False, **kwargs):\n        """"""\n        Predict.\n\n        Args:\n            args (list): input;\n            output_tensor (bool, False): whether to return the output as tensor\n                or not;\n            **kwargs (dict): other parameters used by the predict method\n                the regressor.\n\n        Returns:\n            The predictions of the model.\n\n        """"""\n        if not self._use_cuda:\n            torch_args = [torch.from_numpy(x) if isinstance(x, np.ndarray) else x\n                          for x in args]\n            val = self.network.forward(*torch_args, **kwargs)\n\n            if output_tensor:\n                return val\n            elif isinstance(val, tuple):\n                val = tuple([x.detach().numpy() for x in val])\n            else:\n                val = val.detach().numpy()\n        else:\n            torch_args = [torch.from_numpy(x).cuda()\n                          if isinstance(x, np.ndarray) else x.cuda() for x in args]\n            val = self.network.forward(*torch_args,\n                                       **kwargs)\n\n            if output_tensor:\n                return val\n            elif isinstance(val, tuple):\n                val = tuple([x.detach().cpu().numpy() for x in val])\n            else:\n                val = val.detach().cpu().numpy()\n\n        return val\n\n    def fit(self, *args, n_epochs=None, weights=None, epsilon=None, patience=1,\n            validation_split=1., **kwargs):\n        """"""\n        Fit the model.\n\n        Args:\n            *args (list): input, where the last ``n_fit_targets`` elements\n                are considered as the target, while the others are considered\n                as input;\n            n_epochs (int, None): the number of training epochs;\n            weights (np.ndarray, None): the weights of each sample in the\n                computation of the loss;\n            epsilon (float, None): the coefficient used for early stopping;\n            patience (float, 1.): the number of epochs to wait until stop\n                the learning if not improving;\n            validation_split (float, 1.): the percentage of the dataset to use\n                as training set;\n            **kwargs (dict): other parameters used by the fit method of the\n                regressor.\n\n        """"""\n        if self._reinitialize:\n            self.network.weights_init()\n\n        if self._dropout:\n            self.network.train()\n\n        if epsilon is not None:\n            n_epochs = np.inf if n_epochs is None else n_epochs\n            check_loss = True\n        else:\n            n_epochs = 1 if n_epochs is None else n_epochs\n            check_loss = False\n\n        if weights is not None:\n            args += (weights,)\n            use_weights = True\n        else:\n            use_weights = False\n\n        if 0 < validation_split <= 1:\n            train_len = np.ceil(len(args[0]) * validation_split).astype(\n                np.int)\n            train_args = [a[:train_len] for a in args]\n            val_args = [a[train_len:] for a in args]\n        else:\n            raise ValueError\n\n        patience_count = 0\n        best_loss = np.inf\n        epochs_count = 0\n        if check_loss:\n            with tqdm(total=n_epochs if n_epochs < np.inf else None,\n                      dynamic_ncols=True, disable=self._quiet,\n                      leave=False) as t_epochs:\n                while patience_count < patience and epochs_count < n_epochs:\n                    mean_loss_current = self._fit_epoch(train_args, use_weights,\n                                                        kwargs)\n\n                    if len(val_args[0]):\n                        mean_val_loss_current = self._compute_batch_loss(\n                            val_args, use_weights, kwargs\n                        )\n\n                        loss = mean_val_loss_current.item()\n                    else:\n                        loss = mean_loss_current\n\n                    if not self._quiet:\n                        t_epochs.set_postfix(loss=loss)\n                        t_epochs.update(1)\n\n                    if best_loss - loss > epsilon:\n                        patience_count = 0\n                        best_loss = loss\n                    else:\n                        patience_count += 1\n\n                    epochs_count += 1\n        else:\n            with trange(n_epochs, disable=self._quiet) as t_epochs:\n                for _ in t_epochs:\n                    mean_loss_current = self._fit_epoch(train_args, use_weights,\n                                                        kwargs)\n\n                    if not self._quiet:\n                        t_epochs.set_postfix(loss=mean_loss_current)\n\n        if self._dropout:\n            self.network.eval()\n\n    def _fit_epoch(self, args, use_weights, kwargs):\n        if self._batch_size > 0:\n            batches = minibatch_generator(self._batch_size, *args)\n        else:\n            batches = [args]\n\n        loss_current = list()\n        for batch in batches:\n            loss_current.append(self._fit_batch(batch, use_weights, kwargs))\n\n        return np.mean(loss_current)\n\n    def _fit_batch(self, batch, use_weights, kwargs):\n        loss = self._compute_batch_loss(batch, use_weights, kwargs)\n\n        self._optimizer.zero_grad()\n        loss.backward()\n        self._optimizer.step()\n\n        return loss.item()\n\n    def _compute_batch_loss(self, batch, use_weights, kwargs):\n        if use_weights:\n            weights = torch.from_numpy(batch[-1]).type(torch.float)\n            if self._use_cuda:\n                weights = weights.cuda()\n            batch = batch[:-1]\n\n        if not self._use_cuda:\n            torch_args = [torch.from_numpy(x) for x in batch]\n        else:\n            torch_args = [torch.from_numpy(x).cuda() for x in batch]\n\n        x = torch_args[:-self._n_fit_targets]\n\n        y_hat = self.network(*x, **kwargs)\n\n        if isinstance(y_hat, tuple):\n            output_type = y_hat[0].dtype\n        else:\n            output_type = y_hat.dtype\n\n        y = [y_i.clone().detach().requires_grad_(False).type(output_type) for y_i\n             in torch_args[-self._n_fit_targets:]]\n\n        if self._use_cuda:\n            y = [y_i.cuda() for y_i in y]\n\n        if not use_weights:\n            loss = self._loss(y_hat, *y)\n        else:\n            loss = self._loss(y_hat, *y, reduction=\'none\')\n            loss @= weights\n            loss = loss / weights.sum()\n\n        return loss\n\n    def set_weights(self, weights):\n        """"""\n        Setter.\n\n        Args:\n            w (np.ndarray): the set of weights to set.\n\n        """"""\n        set_weights(self.network.parameters(), weights, self._use_cuda)\n\n    def get_weights(self):\n        """"""\n        Getter.\n\n        Returns:\n            The set of weights of the approximator.\n\n        """"""\n        return get_weights(self.network.parameters())\n\n    @property\n    def weights_size(self):\n        """"""\n        Returns:\n            The size of the array of weights.\n\n        """"""\n        return sum(p.numel() for p in self.network.parameters())\n\n    def diff(self, *args, **kwargs):\n        """"""\n        Compute the derivative of the output w.r.t. ``state``, and ``action``\n        if provided.\n\n        Args:\n            state (np.ndarray): the state;\n            action (np.ndarray, None): the action.\n\n        Returns:\n            The derivative of the output w.r.t. ``state``, and ``action``\n            if provided.\n\n        """"""\n        if not self._use_cuda:\n            torch_args = [torch.from_numpy(np.atleast_2d(x)) for x in args]\n        else:\n            torch_args = [torch.from_numpy(np.atleast_2d(x)).cuda()\n                          for x in args]\n\n        y_hat = self.network(*torch_args, **kwargs)\n        n_outs = 1 if len(y_hat.shape) == 0 else y_hat.shape[-1]\n        y_hat = y_hat.view(-1, n_outs)\n\n        gradients = list()\n        for i in range(y_hat.shape[1]):\n            zero_grad(self.network.parameters())\n            y_hat[:, i].backward(retain_graph=True)\n\n            gradient = list()\n            for p in self.network.parameters():\n                g = p.grad.data.detach().cpu().numpy()\n                gradient.append(g.flatten())\n\n            g = np.concatenate(gradient, 0)\n\n            gradients.append(g)\n\n        g = np.stack(gradients, -1)\n\n        return g\n\n    @property\n    def use_cuda(self):\n        return self._use_cuda\n'"
mushroom_rl/environments/generators/__init__.py,0,b'from .grid_world import *\nfrom .simple_chain import *\nfrom .taxi import *\n'
mushroom_rl/environments/generators/grid_world.py,0,"b'import numpy as np\n\nfrom mushroom_rl.environments.finite_mdp import FiniteMDP\n\n\ndef generate_grid_world(grid, prob, pos_rew, neg_rew, gamma=.9, horizon=100):\n    """"""\n    This Grid World generator requires a .txt file to specify the\n    shape of the grid world and the cells. There are five types of cells: \'S\' is\n    the starting position where the agent is; \'G\' is the goal state; \'.\' is a\n    normal cell; \'*\' is a hole, when the agent steps on a hole, it receives a\n    negative reward and the episode ends; \'#\' is a wall, when the agent is\n    supposed to step on a wall, it actually remains in its current state. The\n    initial states distribution is uniform among all the initial states\n    provided.\n\n    The grid is expected to be rectangular.\n\n    Args:\n        grid (str): the path of the file containing the grid structure;\n        prob (float): probability of success of an action;\n        pos_rew (float): reward obtained in goal states;\n        neg_rew (float): reward obtained in ""hole"" states;\n        gamma (float, .9): discount factor;\n        horizon (int, 100): the horizon.\n\n    Returns:\n        A FiniteMDP object built with the provided parameters.\n\n    """"""\n    grid_map, cell_list = parse_grid(grid)\n    p = compute_probabilities(grid_map, cell_list, prob)\n    r = compute_reward(grid_map, cell_list, pos_rew, neg_rew)\n    mu = compute_mu(grid_map, cell_list)\n\n    return FiniteMDP(p, r, mu, gamma, horizon)\n\n\ndef parse_grid(grid):\n    """"""\n    Parse the grid file:\n\n    Args:\n        grid (str): the path of the file containing the grid structure;\n\n    Returns:\n        A list containing the grid structure.\n\n    """"""\n    grid_map = list()\n    cell_list = list()\n    with open(grid, \'r\') as f:\n        m = f.read()\n\n        assert \'S\' in m and \'G\' in m\n\n        row = list()\n        row_idx = 0\n        col_idx = 0\n        for c in m:\n            if c in [\'#\', \'.\', \'S\', \'G\', \'*\']:\n                row.append(c)\n                if c in [\'.\', \'S\', \'G\', \'*\']:\n                    cell_list.append([row_idx, col_idx])\n                col_idx += 1\n            elif c == \'\\n\':\n                grid_map.append(row)\n                row = list()\n                row_idx += 1\n                col_idx = 0\n            else:\n                raise ValueError(\'Unknown marker.\')\n\n    return grid_map, cell_list\n\n\ndef compute_probabilities(grid_map, cell_list, prob):\n    """"""\n    Compute the transition probability matrix.\n\n    Args:\n        grid_map (list): list containing the grid structure;\n        cell_list (list): list of non-wall cells;\n        prob (float): probability of success of an action.\n\n    Returns:\n        The transition probability matrix;\n\n    """"""\n    g = np.array(grid_map)\n    c = np.array(cell_list)\n    n_states = len(cell_list)\n    p = np.zeros((n_states, 4, n_states))\n    directions = [[-1, 0], [1, 0], [0, -1], [0, 1]]\n\n    for i in range(len(c)):\n        state = c[i]\n\n        if g[tuple(state)] in [\'.\', \'S\']:\n            for a in range(len(directions)):\n                new_state = state + directions[a]\n                j = np.where((c == new_state).all(axis=1))[0]\n                if j.size > 0:\n                    assert j.size == 1\n\n                    p[i, a, i] = 1. - prob\n                    p[i, a, j] = prob\n                else:\n                    p[i, a, i] = 1.\n\n    return p\n\n\ndef compute_reward(grid_map, cell_list, pos_rew, neg_rew):\n    """"""\n    Compute the reward matrix.\n\n    Args:\n        grid_map (list): list containing the grid structure;\n        cell_list (list): list of non-wall cells;\n        pos_rew (float): reward obtained in goal states;\n        neg_rew (float): reward obtained in ""hole"" states;\n\n    Returns:\n        The reward matrix.\n\n    """"""\n    g = np.array(grid_map)\n    c = np.array(cell_list)\n    n_states = len(c)\n    r = np.zeros((n_states, 4, n_states))\n    directions = [[-1, 0], [1, 0], [0, -1], [0, 1]]\n\n    def give_reward(t, rew):\n        for x in np.argwhere(g == t):\n            j = np.where((c == x).all(axis=1))[0]\n\n            for a in range(len(directions)):\n                prev_state = x - directions[a]\n                if prev_state in c:\n                    i = np.where((c == prev_state).all(axis=1))[0]\n                    r[i, a, j] = rew\n\n    give_reward(\'G\', pos_rew)\n    give_reward(\'*\', neg_rew)\n\n    return r\n\n\ndef compute_mu(grid_map, cell_list):\n    """"""\n    Compute the initial states distribution.\n\n    Args:\n        grid_map (list): list containing the grid structure;\n        cell_list (list): list of non-wall cells.\n\n    Returns:\n        The initial states distribution.\n\n    """"""\n    g = np.array(grid_map)\n    c = np.array(cell_list)\n    n_states = len(c)\n    mu = np.zeros(n_states)\n    starts = np.argwhere(g == \'S\')\n\n    for s in starts:\n        i = np.where((c == s).all(axis=1))[0]\n        mu[i] = 1. / len(starts)\n\n    return mu\n'"
mushroom_rl/environments/generators/simple_chain.py,0,"b'import numpy as np\n\nfrom mushroom_rl.environments.finite_mdp import FiniteMDP\n\n\ndef generate_simple_chain(state_n, goal_states, prob, rew, mu=None, gamma=.9,\n                          horizon=100):\n    """"""\n    Simple chain generator.\n\n    Args:\n        state_n (int): number of states;\n        goal_states (list): list of goal states;\n        prob (float): probability of success of an action;\n        rew (float): reward obtained in goal states;\n        mu (np.ndarray): initial state probability distribution;\n        gamma (float, .9): discount factor;\n        horizon (int, 100): the horizon.\n\n    Returns:\n        A FiniteMDP object built with the provided parameters.\n\n    """"""\n    p = compute_probabilities(state_n, prob)\n    r = compute_reward(state_n, goal_states, rew)\n\n    assert mu is None or len(mu) == state_n\n\n    return FiniteMDP(p, r, mu, gamma, horizon)\n\n\ndef compute_probabilities(state_n, prob):\n    """"""\n    Compute the transition probability matrix.\n\n    Args:\n        state_n (int): number of states;\n        prob (float): probability of success of an action.\n\n    Returns:\n        The transition probability matrix;\n\n    """"""\n    p = np.zeros((state_n, 2, state_n))\n\n    for i in range(state_n):\n        if i == 0:\n            p[i, 1, i] = 1.\n        else:\n            p[i, 1, i] = 1. - prob\n            p[i, 1, i - 1] = prob\n\n        if i == state_n - 1:\n            p[i, 0, i] = 1.\n        else:\n            p[i, 0, i] = 1. - prob\n            p[i, 0, i + 1] = prob\n\n    return p\n\n\ndef compute_reward(state_n, goal_states, rew):\n    """"""\n    Compute the reward matrix.\n\n    Args:\n        state_n (int): number of states;\n        goal_states (list): list of goal states;\n        rew (float): reward obtained in goal states.\n\n    Returns:\n        The reward matrix.\n\n    """"""\n    r = np.zeros((state_n, 2, state_n))\n\n    for g in goal_states:\n        if g != 0:\n            r[g - 1, 0, g] = rew\n\n        if g != state_n - 1:\n            r[g + 1, 1, g] = rew\n\n    return r\n'"
mushroom_rl/environments/generators/taxi.py,0,"b'import numpy as np\nfrom sklearn.utils.extmath import cartesian\n\nfrom mushroom_rl.environments.finite_mdp import FiniteMDP\n\n\ndef generate_taxi(grid, prob=.9, rew=(0, 1, 3, 15), gamma=.99, horizon=np.inf):\n    """"""\n    This Taxi generator requires a .txt file to specify the shape of the grid\n    world and the cells. There are five types of cells: \'S\' is the starting\n    where the agent is; \'G\' is the goal state; \'.\' is a normal cell; \'F\' is a\n    passenger, when the agent steps on a hole, it picks up it.\n    \'#\' is a wall, when the agent is supposed to step on a wall, it actually\n    remains in its current state. The initial states distribution is uniform\n    among all the initial states provided. The episode terminates when the agent\n    reaches the goal state. The reward is always 0, except for the goal state\n    where it depends on the number of collected passengers. Each action has\n    a certain probability of success and, if it fails, the agent goes in a\n    perpendicular direction from the supposed one.\n\n    The grid is expected to be rectangular.\n\n    This problem is inspired from:\n    ""Bayesian Q-Learning"". Dearden R. et al.. 1998.\n\n    Args:\n        grid (str): the path of the file containing the grid structure;\n        prob (float, .9): probability of success of an action;\n        rew (tuple, (0, 1, 3, 15)): rewards obtained in goal states;\n        gamma (float, .99): discount factor;\n        horizon (int, np.inf): the horizon.\n\n    Returns:\n        A FiniteMDP object built with the provided parameters.\n\n    """"""\n    grid_map, cell_list, passenger_list = parse_grid(grid)\n\n    assert len(rew) == len(np.argwhere(np.array(grid_map) == \'F\')) + 1\n\n    p = compute_probabilities(grid_map, cell_list, passenger_list, prob)\n    r = compute_reward(grid_map, cell_list, passenger_list, rew)\n    mu = compute_mu(grid_map, cell_list, passenger_list)\n\n    return FiniteMDP(p, r, mu, gamma, horizon)\n\n\ndef parse_grid(grid):\n    """"""\n    Parse the grid file:\n\n    Args:\n        grid (str): the path of the file containing the grid structure.\n\n    Returns:\n        A list containing the grid structure.\n\n    """"""\n    grid_map = list()\n    cell_list = list()\n    passenger_list = list()\n    with open(grid, \'r\') as f:\n        m = f.read()\n\n        assert \'S\' in m and \'G\' in m\n\n        row = list()\n        row_idx = 0\n        col_idx = 0\n        for c in m:\n            if c in [\'#\', \'.\', \'S\', \'G\', \'F\']:\n                row.append(c)\n                if c in [\'.\', \'S\', \'G\', \'F\']:\n                    cell_list.append([row_idx, col_idx])\n                    if c == \'F\':\n                        passenger_list.append([row_idx, col_idx])\n                col_idx += 1\n            elif c == \'\\n\':\n                grid_map.append(row)\n                row = list()\n                row_idx += 1\n                col_idx = 0\n            else:\n                raise ValueError(\'Unknown marker.\')\n\n    return grid_map, cell_list, passenger_list\n\n\ndef compute_probabilities(grid_map, cell_list, passenger_list, prob):\n    """"""\n    Compute the transition probability matrix.\n\n    Args:\n        grid_map (list): list containing the grid structure;\n        cell_list (list): list of non-wall cells;\n        passenger_list (list): list of passenger cells;\n        prob (float): probability of success of an action.\n\n    Returns:\n        The transition probability matrix;\n\n    """"""\n    g = np.array(grid_map)\n    c = np.array(cell_list)\n    n_states = len(cell_list) * 2**len(passenger_list)\n    p = np.zeros((n_states, 4, n_states))\n    directions = [[-1, 0], [1, 0], [0, -1], [0, 1]]\n    passenger_states = cartesian([[0, 1]] * len(passenger_list))\n\n    for i in range(n_states):\n        idx = i // len(cell_list)\n        collected_passengers = np.array(\n            passenger_list)[np.argwhere(passenger_states[idx] == 1).ravel()]\n        state = c[i % len(cell_list)]\n\n        if g[tuple(state)] in [\'.\', \'S\', \'F\']:\n            if g[tuple(state)] in [\'F\']\\\n                    and state.tolist() not in collected_passengers.tolist():\n                continue\n            for a in range(len(directions)):\n                new_state = state + directions[a]\n\n                j = np.where((c == new_state).all(axis=1))[0]\n                if j.size > 0:\n                    assert j.size == 1\n\n                    if g[tuple(new_state)] == \'F\' and new_state.tolist()\\\n                            not in collected_passengers.tolist():\n                        current_passenger_state = np.zeros(len(passenger_list))\n                        current_passenger_idx = np.where(\n                            (new_state == passenger_list).all(axis=1))[0]\n                        current_passenger_state[current_passenger_idx] = 1\n                        new_passenger_state = passenger_states[\n                            idx] + current_passenger_state\n                        new_idx = np.where((\n                            passenger_states == new_passenger_state).all(\n                            axis=1))[0]\n\n                        j += len(cell_list) * new_idx\n                    else:\n                        j += len(cell_list) * idx\n                else:\n                    j = i\n\n                p[i, a, j] = prob\n\n                for d in [1 - np.abs(directions[a]),\n                          np.abs(directions[a]) - 1]:\n                    slip_state = state + d\n                    k = np.where((c == slip_state).all(axis=1))[0]\n                    if k.size > 0:\n                        assert k.size == 1\n\n                        if g[tuple(slip_state)] == \'F\' and slip_state.tolist()\\\n                                not in collected_passengers.tolist():\n                            current_passenger_state = np.zeros(\n                                len(passenger_list))\n                            current_passenger_idx = np.where(\n                                (slip_state == passenger_list).all(axis=1))[0]\n                            current_passenger_state[current_passenger_idx] = 1\n                            new_passenger_state = passenger_states[\n                                idx] + current_passenger_state\n                            new_idx = np.where((\n                                passenger_states == new_passenger_state).all(\n                                axis=1))[0]\n\n                            k += len(cell_list) * new_idx\n                        else:\n                            k += len(cell_list) * idx\n                    else:\n                        k = i\n\n                    p[i, a, k] += (1. - prob) * .5\n\n    return p\n\n\ndef compute_reward(grid_map, cell_list, passenger_list, rew):\n    """"""\n    Compute the reward matrix.\n\n    Args:\n        grid_map (list): list containing the grid structure;\n        cell_list (list): list of non-wall cells;\n        passenger_list (list): list of passenger cells;\n        rew (tuple): rewards obtained in goal states.\n\n    Returns:\n        The reward matrix.\n\n    """"""\n    g = np.array(grid_map)\n    c = np.array(cell_list)\n    n_states = len(cell_list) * 2**len(passenger_list)\n    r = np.zeros((n_states, 4, n_states))\n    directions = [[-1, 0], [1, 0], [0, -1], [0, 1]]\n    passenger_states = cartesian([[0, 1]] * len(passenger_list))\n\n    for goal in np.argwhere(g == \'G\'):\n        for a in range(len(directions)):\n            prev_state = goal - directions[a]\n            if prev_state in c:\n                for i in range(len(passenger_states)):\n                    i_idx = np.where((c == prev_state).all(axis=1))[0] + len(\n                        cell_list) * i\n                    j_idx = j = np.where((c == goal).all(axis=1))[0] + len(\n                        cell_list) * i\n\n                    r[i_idx, a, j_idx] = rew[np.sum(passenger_states[i])]\n\n    return r\n\n\ndef compute_mu(grid_map, cell_list, passenger_list):\n    """"""\n    Compute the initial states distribution.\n\n    Args:\n        grid_map (list): list containing the grid structure;\n        cell_list (list): list of non-wall cells;\n        passenger_list (list): list of passenger cells.\n\n    Returns:\n        The initial states distribution.\n\n    """"""\n    g = np.array(grid_map)\n    c = np.array(cell_list)\n    n_states = len(cell_list) * 2**len(passenger_list)\n    mu = np.zeros(n_states)\n    starts = np.argwhere(g == \'S\')\n\n    for s in starts:\n        i = np.where((c == s).all(axis=1))[0]\n        mu[i] = 1. / len(starts)\n\n    return mu\n'"
mushroom_rl/environments/mujoco_envs/__init__.py,0,"b'from .ball_in_a_cup import BallInACup\nfrom .humanoid_gait import HumanoidGait\n\n__all__ = [\'BallInACup\', ""HumanoidGait""]'"
mushroom_rl/environments/mujoco_envs/ball_in_a_cup.py,0,"b'from mushroom_rl.environments.mujoco import MuJoCo, ObservationType\nimport numpy as np\nfrom pathlib import Path\n\n\nclass BallInACup(MuJoCo):\n    """"""\n    Mujoco simulation of Ball In A Cup task, using Barret WAM robot.\n\n    """"""\n    def __init__(self):\n        """"""\n        Constructor.\n\n        """"""\n        xml_path = (Path(__file__).resolve().parent / ""data"" / ""ball_in_a_cup"" / ""model.xml"").as_posix()\n        action_spec = [""act/wam/base_yaw_joint"", ""act/wam/shoulder_pitch_joint"", ""act/wam/shoulder_yaw_joint"",\n                       ""act/wam/elbow_pitch_joint"", ""act/wam/wrist_yaw_joint"", ""act/wam/wrist_pitch_joint"",\n                       ""act/wam/palm_yaw_joint""]\n\n        observation_spec = [(""wam/base_yaw_joint"", ObservationType.JOINT_POS),\n                            (""wam/base_yaw_joint"", ObservationType.JOINT_VEL),\n                            (""wam/shoulder_pitch_joint"", ObservationType.JOINT_POS),\n                            (""wam/shoulder_pitch_joint"", ObservationType.JOINT_VEL),\n                            (""wam/shoulder_yaw_joint"", ObservationType.JOINT_POS),\n                            (""wam/shoulder_yaw_joint"", ObservationType.JOINT_VEL),\n                            (""wam/elbow_pitch_joint"", ObservationType.JOINT_POS),\n                            (""wam/elbow_pitch_joint"", ObservationType.JOINT_VEL),\n                            (""wam/wrist_yaw_joint"", ObservationType.JOINT_POS),\n                            (""wam/wrist_yaw_joint"", ObservationType.JOINT_VEL),\n                            (""wam/wrist_pitch_joint"", ObservationType.JOINT_POS),\n                            (""wam/wrist_pitch_joint"", ObservationType.JOINT_VEL),\n                            (""wam/palm_yaw_joint"", ObservationType.JOINT_POS),\n                            (""wam/palm_yaw_joint"", ObservationType.JOINT_VEL),\n                            (""ball"", ObservationType.BODY_POS),\n                            (""ball"", ObservationType.BODY_VEL)]\n\n        additional_data_spec = [(""ball_pos"", ""ball"", ObservationType.BODY_POS),\n                                (""goal_pos"", ""cup_goal_final"", ObservationType.SITE_POS)]\n\n        collision_groups = [(""ball"", [""ball_geom""]),\n                            (""robot"", [""cup_geom1"", ""cup_geom2"", ""wrist_palm_link_convex_geom"",\n                                       ""wrist_pitch_link_convex_decomposition_p1_geom"",\n                                       ""wrist_pitch_link_convex_decomposition_p2_geom"",\n                                       ""wrist_pitch_link_convex_decomposition_p3_geom"",\n                                       ""wrist_yaw_link_convex_decomposition_p1_geom"",\n                                       ""wrist_yaw_link_convex_decomposition_p2_geom"",\n                                       ""forearm_link_convex_decomposition_p1_geom"",\n                                       ""forearm_link_convex_decomposition_p2_geom""])]\n\n        super().__init__(xml_path, action_spec, observation_spec, 0.9999, 2000, n_substeps=4,\n                         additional_data_spec=additional_data_spec, collision_groups=collision_groups)\n\n        self.init_robot_pos = np.array([0.0, 0.58760536, 0.0, 1.36004913, 0.0, -0.32072943, -1.57])\n        self.p_gains = np.array([200, 300, 100, 100, 10, 10, 2.5])\n        self.d_gains = np.array([7, 15, 5, 2.5, 0.3, 0.3, 0.05])\n\n    def reward(self, state, action, next_state):\n        dist = self.read_data(""goal_pos"") - self.read_data(""ball_pos"")\n        return 1. if np.linalg.norm(dist) < 0.05 else 0.\n\n    def is_absorbing(self, state):\n        dist = self.read_data(""goal_pos"") - self.read_data(""ball_pos"")\n        return np.linalg.norm(dist) < 0.05 or self.check_collision(""ball"", ""robot"")\n\n    def setup(self):\n        # Copy the initial position after the reset\n        init_pos = self.sim.data.qpos.copy()\n        init_vel = np.zeros_like(init_pos)\n\n        # Reset the system and the set the intial robot position\n        self.sim.data.qpos[:] = init_pos\n        self.sim.data.qvel[:] = init_vel\n        self.sim.data.qpos[0:7] = self.init_robot_pos\n\n        # Do one simulation step to compute the new position of the goal_site\n        self.sim.step()\n\n        self.sim.data.qpos[:] = init_pos\n        self.sim.data.qvel[:] = init_vel\n        self.sim.data.qpos[0:7] = self.init_robot_pos\n        self.write_data(""ball_pos"", self.read_data(""goal_pos"") - np.array([0., 0., 0.329]))\n\n        # Stabilize the system around the initial position using a PD-Controller\n        for i in range(0, 500):\n            self.sim.data.qpos[7:] = 0.\n            self.sim.data.qvel[7:] = 0.\n            self.sim.data.qpos[7] = -0.2\n            cur_pos = self.sim.data.qpos[0:7].copy()\n            cur_vel = self.sim.data.qvel[0:7].copy()\n            trq = self.p_gains * (self.init_robot_pos - cur_pos) + self.d_gains * (\n                    np.zeros_like(self.init_robot_pos) - cur_vel)\n            self.sim.data.qfrc_applied[0:7] = trq\n            self.sim.step()\n\n        # Now simulate for more time-steps without resetting the position of the first link of the rope\n        for i in range(0, 500):\n            cur_pos = self.sim.data.qpos[0:7].copy()\n            cur_vel = self.sim.data.qvel[0:7].copy()\n            trq = self.p_gains * (self.init_robot_pos - cur_pos) + self.d_gains * (\n                    np.zeros_like(self.init_robot_pos) - cur_vel)\n            self.sim.data.qfrc_applied[0:7] = trq\n            self.sim.step()\n'"
mushroom_rl/features/_implementations/__init__.py,0,b''
mushroom_rl/features/_implementations/basis_features.py,0,"b'import numpy as np\n\nfrom .features_implementation import FeaturesImplementation\n\n\nclass BasisFeatures(FeaturesImplementation):\n    def __init__(self, basis):\n        self._basis = basis\n\n    def __call__(self, *args):\n        x = self._concatenate(args)\n\n        y = list()\n\n        x = np.atleast_2d(x)\n        for s in x:\n            out = np.empty(self.size)\n\n            for i, bf in enumerate(self._basis):\n                out[i] = bf(s)\n\n            y.append(out)\n\n        if len(y) == 1:\n            y = y[0]\n        else:\n            y = np.array(y)\n\n        return y\n\n    @property\n    def size(self):\n        return len(self._basis)\n'"
mushroom_rl/features/_implementations/features_implementation.py,0,"b'import numpy as np\n\n\nclass FeaturesImplementation(object):\n    def __call__(self, *x):\n        """"""\n        Evaluate the feature vector in the given raw input. If more than one\n        element is passed, the raw input is concatenated before computing the\n        features.\n\n        Args:\n            *x (list): the raw input.\n\n        Returns:\n            The features vector computed from the raw input.\n\n        """"""\n        pass\n\n    @staticmethod\n    def _concatenate(args):\n        if len(args) > 1:\n            x = np.concatenate(args, axis=-1)\n        else:\n            x = args[0]\n\n        return x\n\n    @property\n    def size(self):\n        """"""\n        Returns:\n             The number of elements in the features vector.\n\n        """"""\n        pass\n'"
mushroom_rl/features/_implementations/functional_features.py,0,"b'from .features_implementation import FeaturesImplementation\n\n\nclass FunctionalFeatures(FeaturesImplementation):\n    def __init__(self, n_outputs, function):\n        self._n_outputs = n_outputs\n        self._function = function if function is not None else self._identity\n\n    def __call__(self, *args):\n        x = self._concatenate(args)\n\n        return self._function(x)\n\n    def _identity(self, x):\n        return x\n\n    @property\n    def size(self):\n        return self._n_outputs\n'"
mushroom_rl/features/_implementations/pytorch_features.py,2,"b'import numpy as np\nimport torch\n\nfrom .features_implementation import FeaturesImplementation\n\n\nclass PyTorchFeatures(FeaturesImplementation):\n    def __init__(self, tensor_list, device=None):\n        self._phi = tensor_list\n        self._device = device\n\n    def __call__(self, *args):\n        x = self._concatenate(args)\n\n        x = torch.from_numpy(np.atleast_2d(x))\n\n        y_list = [self._phi[i].forward(x) for i in range(len(self._phi))]\n        y = torch.stack(y_list, dim=-1)\n\n        y = y.detach().numpy()\n\n        if y.shape[0] == 1:\n            return y[0]\n        else:\n            return y\n\n    @property\n    def size(self):\n        return len(self._phi)\n'"
mushroom_rl/features/_implementations/tiles_features.py,0,"b'import numpy as np\n\nfrom .features_implementation import FeaturesImplementation\n\n\nclass TilesFeatures(FeaturesImplementation):\n    def __init__(self, tiles):\n\n        if isinstance(tiles, list):\n            self._tiles = tiles\n        else:\n            self._tiles = [tiles]\n        self._size = 0\n\n        for tiling in self._tiles:\n            self._size += tiling.size\n\n    def __call__(self, *args):\n        x = self._concatenate(args)\n\n        y = list()\n\n        x = np.atleast_2d(x)\n        for s in x:\n            out = np.zeros(self._size)\n\n            offset = 0\n            for tiling in self._tiles:\n                index = tiling(s)\n\n                if index is not None:\n                    out[index + offset] = 1.\n\n                offset += tiling.size\n\n            y.append(out)\n\n        if len(y) == 1:\n            y = y[0]\n        else:\n            y = np.array(y)\n\n        return y\n\n    @property\n    def size(self):\n        return self._size\n'"
mushroom_rl/features/basis/__init__.py,0,"b""from .gaussian_rbf import GaussianRBF\nfrom .polynomial import PolynomialBasis\nfrom .fourier import FourierBasis\n\n__all__ = ['GaussianRBF', 'PolynomialBasis', 'FourierBasis']\n"""
mushroom_rl/features/basis/fourier.py,0,"b'import numpy as np\n\n\nclass FourierBasis:\n    r""""""\n    Class implementing Fourier basis functions. The value of the feature\n    is computed using the formula:\n\n    .. math::\n        \\sum \\cos{\\pi(X - m)/\\Delta c}\n\n    where X is the input, m is the vector of the minumum input values (for each\n    dimensions) , \\Delta is the vector of maximum\n\n    """"""\n    def __init__(self, low, delta, c, dimensions=None):\n        """"""\n        Constructor.\n\n        Args:\n            low (np.ndarray): vector of minimum values of the input variables;\n            delta (np.ndarray): vector of the maximum difference between two\n             values of the input variables, i.e. delta = high - low;\n            c (np.ndarray): vector of weights for the state variables;\n            dimensions (list, None): list of the dimensions of the input to be\n                considered by the feature.\n\n        """"""\n        self._low = low\n        self._delta = delta\n        self._dim = dimensions\n        self._c = c\n\n    def __call__(self, x):\n        if self._dim is not None:\n            x = x[self._dim]\n\n        s = (x - self._low) / self._delta\n\n        return np.cos(np.pi*s.dot(self._c))\n\n    def __str__(self):\n        return str(self._c)\n\n    @staticmethod\n    def generate(low, high, n, dimensions=None):\n        """"""\n        Factory method to build a set of fourier basis.\n\n        Args:\n            low (np.ndarray): vector of minimum values of the input variables;\n            high (np.ndarray): vector of maximum values of the input variables;\n            n (int): number of harmonics to consider for each state variable\n            dimensions (list, None): list of the dimensions of the input to be\n                considered by the features.\n\n        Returns:\n            The list of the generated fourier basis functions.\n\n        """"""\n        if dimensions is not None:\n            assert(len(low) == len(dimensions))\n\n        input_size = len(low)\n        delta = high - low\n        n_basis = (n + 1)**input_size\n        basis_list = list()\n\n        for index in range(n_basis):\n            c = np.zeros(input_size)\n            value = index\n\n            for i in range(input_size):\n                c[i] = value % (n + 1)\n                value = value // (n + 1)\n\n            basis_list.append(FourierBasis(low, delta, c, dimensions))\n\n        return basis_list\n'"
mushroom_rl/features/basis/gaussian_rbf.py,0,"b'import numpy as np\nfrom mushroom_rl.utils.features import uniform_grid\n\n\nclass GaussianRBF:\n    r""""""\n    Class implementing Gaussian radial basis functions. The value of the feature\n    is computed using the formula:\n    \n    .. math::\n        \\sum \\dfrac{(X_i - \\mu_i)^2}{\\sigma_i}\n\n    where X is the input, \\mu is the mean vector and \\sigma is the scale\n    parameter vector.\n\n    """"""\n    def __init__(self, mean, scale, dimensions=None):\n        """"""\n        Constructor.\n\n        Args:\n            mean (np.ndarray): the mean vector of the feature;\n            scale (np.ndarray): the scale vector of the feature;\n            dimensions (list, None): list of the dimensions of the input to be\n                considered by the feature. The number of dimensions must match\n                the dimensionality of ``mean`` and ``scale``.\n\n        """"""\n        self._mean = mean\n        self._scale = scale\n        self._dim = dimensions\n\n    def __call__(self, x):\n        if self._dim is not None:\n            x = x[self._dim]\n\n        return np.exp(-np.sum((x - self._mean)**2 / self._scale))\n\n    def __str__(self):\n        name = \'GaussianRBF \' + str(self._mean) + \' \' + str(self._scale)\n        if self._dim is not None:\n            name += \' \' + str(self._dim)\n        return name\n\n    @staticmethod\n    def generate(n_centers, low, high, dimensions=None):\n        r""""""\n        Factory method to build uniformly spaced gaussian radial basis functions\n        with a 25\\% overlap.\n\n        Args:\n            n_centers (list): list of the number of radial basis functions to be\n                used for each dimension.\n            low (np.ndarray): lowest value for each dimension;\n            high (np.ndarray): highest value for each dimension;\n            dimensions (list, None): list of the dimensions of the input to be\n                considered by the feature. The number of dimensions must match\n                the number of elements in ``n_centers`` and ``low``.\n\n        Returns:\n            The list of the generated radial basis functions.\n\n        """"""\n        n_features = len(low)\n        assert len(n_centers) == n_features\n        assert len(low) == len(high)\n        assert dimensions is None or n_features == len(dimensions)\n\n        grid, b = uniform_grid(n_centers, low, high)\n\n        basis = list()\n        for i in range(len(grid)):\n            v = grid[i, :]\n            bf = GaussianRBF(v, b, dimensions)\n            basis.append(bf)\n\n        return basis\n'"
mushroom_rl/features/basis/polynomial.py,0,"b'import numpy as np\n\n\nclass PolynomialBasis:\n    r""""""\n    Class implementing polynomial basis functions. The value of the feature\n    is computed using the formula:\n    \n    .. math::\n        \\prod X_i^{d_i}\n\n    where X is the input and d is the vector of the exponents of the polynomial.\n\n    """"""\n    def __init__(self, dimensions=None, degrees=None):\n        """"""\n        Constructor. If both parameters are None, the constant feature is built.\n\n        Args:\n            dimensions (list, None): list of the dimensions of the input to be\n                considered by the feature;\n            degrees (list, None): list of the degrees of each dimension to be\n                considered by the feature. It must match the number of elements\n                of ``dimensions``.\n\n        """"""\n        self._dim = dimensions\n        self._deg = degrees\n\n        assert (self._dim is None and self._deg is None) or (\n            len(self._dim) == len(self._deg))\n\n    def __call__(self, x):\n\n        if self._dim is None:\n            return 1\n\n        out = 1\n        for i, d in zip(self._dim, self._deg):\n            out *= x[i]**d\n\n        return out\n\n    def __str__(self):\n        if self._deg is None:\n            return \'1\'\n\n        name = \'\'\n        for i, d in zip(self._dim, self._deg):\n            name += \'x[\' + str(i) + \']\'\n            if d > 1:\n                name += \'^\' + str(d)\n        return name\n\n    @staticmethod\n    def _compute_exponents(order, n_variables):\n        """"""\n        Find the exponents of a multivariate polynomial expression of order\n        ``order`` and ``n_variables`` number of variables.\n\n        Args:\n            order (int): the maximum order of the polynomial;\n            n_variables (int): the number of elements of the input vector.\n\n        Yields:\n            The current exponent of the polynomial.\n\n        """"""\n        pattern = np.zeros(n_variables, dtype=np.int32)\n        for current_sum in range(1, order + 1):\n            pattern[0] = current_sum\n            yield pattern\n            while pattern[-1] < current_sum:\n                for i in range(2, n_variables + 1):\n                    if 0 < pattern[n_variables - i]:\n                        pattern[n_variables - i] -= 1\n                        if 2 < i:\n                            pattern[n_variables - i + 1] = 1 + pattern[-1]\n                            pattern[-1] = 0\n                        else:\n                            pattern[-1] += 1\n                        break\n                yield pattern\n            pattern[-1] = 0\n\n    @staticmethod\n    def generate(max_degree, input_size):\n        """"""\n        Factory method to build a polynomial of order ``max_degree`` based on\n        the first ``input_size`` dimensions of the input.\n\n        Args:\n            max_degree (int): maximum degree of the polynomial;\n            input_size (int): size of the input.\n\n        Returns:\n            The list of the generated polynomial basis functions.\n\n        """"""\n        assert (max_degree >= 0)\n        assert (input_size > 0)\n\n        basis_list = [PolynomialBasis()]\n\n        for e in PolynomialBasis._compute_exponents(max_degree, input_size):\n            dims = np.reshape(np.argwhere(e != 0), -1)\n            degs = e[e != 0]\n\n            basis_list.append(PolynomialBasis(dims, degs))\n\n        return basis_list\n'"
mushroom_rl/features/tensors/__init__.py,0,"b""from .gaussian_tensor import PyTorchGaussianRBF\n\n__all_ = ['PyTorchGaussianRBF']"""
mushroom_rl/features/tensors/gaussian_tensor.py,6,"b'import torch\nimport torch.nn as nn\nfrom mushroom_rl.utils.features import uniform_grid\n\n\nclass PyTorchGaussianRBF(nn.Module):\n    """"""\n    Pytorch module to implement a gaussian radial basis function.\n\n    """"""\n    def __init__(self, mu, scale, dim):\n        self._mu = torch.from_numpy(mu)\n        self._scale = torch.from_numpy(scale)\n        if dim is not None:\n            self._dim = torch.from_numpy(dim)\n        else:\n            self._dim = None\n\n    def forward(self, x):\n        if self._dim is not None:\n            x = torch.index_select(x, 1, self._dim)\n\n        delta = x - self._mu\n\n        return torch.exp(-torch.sum(delta**2 / self._scale, 1))\n\n    @staticmethod\n    def generate(n_centers, low, high,  dimensions=None):\n        """"""\n        Factory method that generates the list of dictionaries to build the\n        tensors representing a set of uniformly spaced Gaussian radial basis\n        functions with a 25% overlap.\n\n        Args:\n            n_centers (list): list of the number of radial basis functions to be\n                              used for each dimension;\n            low (np.ndarray): lowest value for each dimension;\n            high (np.ndarray): highest value for each dimension;\n            dimensions (list, None): list of the dimensions of the input to be\n                considered by the feature. The number of dimensions must match\n                the number of elements in ``n_centers`` and ``low``.\n\n        Returns:\n            The list of dictionaries as described above.\n\n        """"""\n        n_features = len(low)\n        assert len(n_centers) == n_features\n        assert len(low) == len(high)\n        assert dimensions is None or n_features == len(dimensions)\n\n        grid, scale = uniform_grid(n_centers, low, high)\n\n        tensor_list = list()\n        for i in range(len(grid)):\n            mu = grid[i, :]\n            tensor_list.append(PyTorchGaussianRBF(mu, scale, dimensions))\n\n        return tensor_list\n\n\n'"
mushroom_rl/features/tiles/__init__.py,0,"b""from .tiles import Tiles\n\n__all__ = ['Tiles']\n"""
mushroom_rl/features/tiles/tiles.py,0,"b'import numpy as np\n\n\nclass Tiles:\n    """"""\n    Class implementing rectangular tiling. For each point in the state space,\n    this class can be used to compute the index of the corresponding tile.\n\n    """"""\n    def __init__(self, x_range, n_tiles, state_components=None):\n        """"""\n        Constructor.\n\n        Args:\n            x_range (list): list of two-elements lists specifying the range of\n                each state variable;\n            n_tiles (list): list of the number of tiles to be used for each\n                dimension.\n            state_components (list, None): list of the dimensions of the input\n                to be considered by the tiling. The number of elements must\n                match the number of elements in ``x_range`` and ``n_tiles``.\n\n        """"""\n        if isinstance(x_range[0], list):\n            self._range = x_range\n        else:\n            self._range = [x_range]\n\n        if isinstance(n_tiles, list):\n            assert(len(n_tiles) == len(self._range))\n\n            self._n_tiles = n_tiles\n        else:\n            self._n_tiles = [n_tiles] * len(self._range)\n\n        self._state_components = state_components\n\n        if self._state_components is not None:\n            assert(len(self._state_components) == len(self._range))\n\n        self._size = 1\n\n        for s in self._n_tiles:\n            self._size *= s\n\n    def __call__(self, x):\n        if self._state_components is not None:\n            x = x[self._state_components]\n\n        multiplier = 1\n        tile_index = 0\n\n        for i, (r, N) in enumerate(zip(self._range, self._n_tiles)):\n            if r[0] <= x[i] < r[1]:\n                width = r[1] - r[0]\n                component_index = int(np.floor(N * (x[i] - r[0]) / width))\n                tile_index += component_index * multiplier\n                multiplier *= N\n            else:\n                tile_index = None\n                break\n\n        return tile_index\n\n    @staticmethod\n    def generate(n_tilings, n_tiles, low, high, uniform=False):\n        """"""\n        Factory method to build ``n_tilings`` tilings of ``n_tiles`` tiles with\n        a range between ``low`` and ``high`` for each dimension.\n\n        Args:\n            n_tilings (int): number of tilings;\n            n_tiles (list): number of tiles for each tilings for each dimension;\n            low (np.ndarray): lowest value for each dimension;\n            high (np.ndarray): highest value for each dimension.\n            uniform (bool, False): if True the displacement for each tiling will\n                                   be w/n_tilings, where w is the tile width.\n                                   Otherwise, the displacement will be\n                                   k*w/n_tilings, where k=2i+1, where i is the\n                                   dimension index.\n\n        Returns:\n            The list of the generated tiles.\n\n        """"""\n        assert len(n_tiles) == len(low) == len(high)\n\n        low = np.array(low, dtype=np.float)\n        high = np.array(high, dtype=np.float)\n\n        tilings = list()\n\n        shift = Tiles._compute_shift(uniform, len(low))\n        width = (high - low) / \\\n                (np.array(n_tiles) * n_tilings - shift*n_tilings + shift)\n        offset = width\n\n        for i in range(n_tilings):\n            x_min = low - (n_tilings - 1 - i) * offset * shift\n            x_max = high + i * offset\n            x_range = [[x, y] for x, y in zip(x_min, x_max)]\n            tilings.append(Tiles(x_range, n_tiles))\n\n        return tilings\n\n    @staticmethod\n    def _compute_shift(uniform, n_dims):\n        if uniform:\n            return 1\n        else:\n            shift = np.empty(n_dims)\n            for i in range(n_dims):\n                shift[i] = 2*i+1\n            return shift\n\n    @property\n    def size(self):\n        return self._size\n'"
mushroom_rl/utils/callbacks/__init__.py,0,"b""__extras__ = []\n\ntry:\n    from mushroom_rl.utils.callbacks.plot_dataset import PlotDataset\n    __extras__.append('PlotDataset')\nexcept ImportError:\n    pass\n\nfrom .callback import Callback\nfrom .collect_dataset import CollectDataset\nfrom .collect_max_q import CollectMaxQ\nfrom .collect_q import CollectQ\nfrom .collect_parameters import CollectParameters\n\n__all__ = ['Callback', 'CollectDataset', 'CollectQ', 'CollectMaxQ',\n           'CollectParameters'] + __extras__\n"""
mushroom_rl/utils/callbacks/callback.py,0,"b'class Callback(object):\n    """"""\n    Interface for all basic callbacks. Implements a list in which it is possible\n    to store data and methods to query and clean the content stored by the\n    callback.\n\n    """"""\n    def __init__(self):\n        """"""\n        Constructor.\n\n        """"""\n        self._data_list = list()\n\n    def __call__(self, dataset):\n        """"""\n        Add samples to the samples list.\n\n        Args:\n            dataset (list): the samples to collect.\n\n        """"""\n        raise NotImplementedError\n\n    def get(self):\n        """"""\n        Returns:\n             The current collected data as a list.\n\n        """"""\n        return self._data_list\n\n    def clean(self):\n        """"""\n        Delete the current stored data list\n\n        """"""\n        self._data_list = list()\n'"
mushroom_rl/utils/callbacks/collect_dataset.py,0,"b'from mushroom_rl.utils.callbacks.callback import Callback\n\n\nclass CollectDataset(Callback):\n    """"""\n    This callback can be used to collect samples during the learning of the\n    agent.\n\n    """"""\n    def __call__(self, dataset):\n        self._data_list += dataset\n'"
mushroom_rl/utils/callbacks/collect_max_q.py,0,"b'from mushroom_rl.utils.callbacks.callback import Callback\nimport numpy as np\n\n\nclass CollectMaxQ(Callback):\n    """"""\n    This callback can be used to collect the maximum action value in a given\n    state at each call.\n\n    """"""\n    def __init__(self, approximator, state):\n        """"""\n        Constructor.\n\n        Args:\n            approximator ([Table, EnsembleTable]): the approximator to use;\n            state (np.ndarray): the state to consider.\n\n        """"""\n        self._approximator = approximator\n        self._state = state\n\n        super().__init__()\n\n    def __call__(self, dataset):\n        q = self._approximator.predict(self._state)\n        max_q = np.max(q)\n\n        self._data_list.append(max_q)\n'"
mushroom_rl/utils/callbacks/collect_parameters.py,0,"b'from mushroom_rl.utils.callbacks.callback import Callback\nimport numpy as np\n\n\nclass CollectParameters(Callback):\n    """"""\n    This callback can be used to collect the values of a parameter\n    (e.g. learning rate) during a run of the agent.\n\n    """"""\n    def __init__(self, parameter, *idx):\n        """"""\n        Constructor.\n\n        Args:\n            parameter (Parameter): the parameter whose values have to be\n                collected;\n            *idx (list): index of the parameter when the ``parameter`` is\n                tabular.\n\n        """"""\n        self._parameter = parameter\n        self._idx = idx\n\n        super().__init__()\n\n    def __call__(self, dataset):\n        value = self._parameter.get_value(*self._idx)\n        if isinstance(value, np.ndarray):\n            value = np.array(value)\n        self._data_list.append(value)\n'"
mushroom_rl/utils/callbacks/collect_q.py,0,"b'from copy import deepcopy\n\nfrom mushroom_rl.utils.callbacks.callback import Callback\nfrom mushroom_rl.utils.table import EnsembleTable\n\n\nclass CollectQ(Callback):\n    """"""\n    This callback can be used to collect the action values in all states at the\n    current time step.\n\n    """"""\n    def __init__(self, approximator):\n        """"""\n        Constructor.\n\n        Args:\n            approximator ([Table, EnsembleTable]): the approximator to use to\n                predict the action values.\n\n        """"""\n        self._approximator = approximator\n\n        super().__init__()\n\n    def __call__(self, dataset):\n        if isinstance(self._approximator, EnsembleTable):\n            qs = list()\n            for m in self._approximator.model:\n                qs.append(m.table)\n            self._data_list.append(deepcopy(np.mean(qs, 0)))\n        else:\n            self._data_list.append(deepcopy(self._approximator.table))\n'"
mushroom_rl/utils/callbacks/plot_dataset.py,0,"b'import pickle\nimport numpy as np\n\nfrom mushroom_rl.utils.callbacks.collect_dataset import CollectDataset\nfrom mushroom_rl.utils.plots import DataBuffer, Window, Actions,\\\n    LenOfEpisodeTraining, Observations, RewardPerEpisode, RewardPerStep\nfrom mushroom_rl.utils.spaces import Box\nfrom mushroom_rl.utils.dataset import episodes_length, compute_J\n\n\nclass PlotDataset(CollectDataset):\n    """"""\n    This callback is used for plotting the values of the actions, observations,\n    reward per step, reward per episode, episode length only for the training.\n\n    """"""\n    def __init__(self, mdp_info, obs_normalized=False, window_size=1000,\n                 update_freq=10, show=True):\n\n        """"""\n        Constructor.\n\n        Args:\n            mdp_info (MDPInfo): information of the environment;\n            obs_normalized (bool, False): whether observation needs to be\n                normalized or not;\n            window_size (int, 1000): number of steps plotted in the windowed\n                plots. Only action, observation and reward per step plots are\n                affected. The other are always adding information;\n            update_freq (int, 10): Frequency(Hz) that window should be updated.\n                This update frequency is not accurate because the refresh\n                method of the window runs sequentially with the rest of the\n                script. So this update frequency is only relevant if the\n                frequency of refresh calls is too high, avoiding excessive\n                updates;\n            show (bool, True): whether to show the window or not.\n\n        """"""\n        super().__init__()\n\n        self.action_buffers_list = []\n        for i in range(mdp_info.action_space.shape[0]):\n            self.action_buffers_list.append(\n                DataBuffer(\'Action_\' + str(i), window_size))\n\n        self.observation_buffers_list = []\n        for i in range(mdp_info.observation_space.shape[0]):\n            self.observation_buffers_list.append(\n                DataBuffer(\'Observation_\' + str(i), window_size))\n\n        self.instant_reward_buffer = \\\n            DataBuffer(""Instant_reward"", window_size)\n\n        self.training_reward_buffer = DataBuffer(""Episode_reward"")\n\n        self.episodic_len_buffer_training = DataBuffer(""Episode_len"")\n\n        if isinstance(mdp_info.action_space, Box):\n            high_actions = mdp_info.action_space.high.tolist()\n            low_actions = mdp_info.action_space.low.tolist()\n        else:\n            high_actions = None\n            low_actions = None\n\n        actions_plot = Actions(self.action_buffers_list, maxs=high_actions,\n                               mins=low_actions)\n\n        dotted_limits = None\n        if isinstance(mdp_info.observation_space, Box):\n            high_mdp = mdp_info.observation_space.high.tolist()\n            low_mdp = mdp_info.observation_space.low.tolist()\n            if obs_normalized:\n                dotted_limits = []\n                for i in range(len(high_mdp)):\n                    if abs(high_mdp[i]) == np.inf:\n                        dotted_limits.append(True)\n                    else:\n                        dotted_limits.append(False)\n                        \n                    high_mdp[i] = 1\n                    low_mdp[i] = -1\n        else:\n            high_mdp = None\n            low_mdp = None\n\n        observation_plot = Observations(\n            self.observation_buffers_list, maxs=high_mdp, mins=low_mdp,\n            dotted_limits=dotted_limits\n        )\n\n        step_reward_plot = RewardPerStep(\n            self.instant_reward_buffer\n        )\n\n        training_reward_plot = RewardPerEpisode(\n            self.training_reward_buffer\n        )\n\n        episodic_len_training_plot = LenOfEpisodeTraining(\n            self.episodic_len_buffer_training\n        )\n\n        self.plot_window = Window(\n            plot_list=[training_reward_plot, episodic_len_training_plot,\n                       step_reward_plot, actions_plot, observation_plot],\n            title=""EnvironmentPlot"",\n            track_if_deactivated=[True, True, False, False, False],\n            update_freq=update_freq)\n\n        if show:\n            self.plot_window.show()\n\n    def __call__(self, dataset):\n        super().__call__(dataset)\n\n        for sample in dataset:\n            obs = sample[0]\n            action = sample[1]\n            reward = sample[2]\n\n            for i in range(len(action)):\n                self.action_buffers_list[i].update([action[i]])\n\n            for i in range(obs.size):\n                self.observation_buffers_list[i].update([obs[i]])\n\n            self.instant_reward_buffer.update([reward])\n\n        lengths_of_episodes = episodes_length(self._data_list)\n\n        start_index = 0\n        for length_of_episode in lengths_of_episodes:\n            sub_dataset = self._data_list[start_index:start_index+length_of_episode]\n\n            episodic_reward = compute_J(sub_dataset)\n            self.training_reward_buffer.update([episodic_reward[0]])\n            self.episodic_len_buffer_training.update([length_of_episode])\n\n            start_index = length_of_episode\n\n        self._data_list = self._data_list[start_index:]\n\n        self.plot_window.refresh()\n\n    def get_state(self):\n        """"""\n        Returns:\n             The dictionary of data in each data buffer in tree structure\n             associated with the plot name.\n\n        """"""\n        data = {plot.name: {buffer.name: buffer.get()}\n                for p_i, plot in enumerate(self.plot_window.plot_list)\n                for buffer in plot.data_buffers\n                if self.plot_window._track_if_deactivated[p_i]}\n\n        return data\n\n    def set_state(self, data):\n        """"""\n        Set the state of the DataBuffers to resume the plots.\n\n        Args:\n            data (dict): data of each plot and data buffer.\n\n        """"""\n        for plot_name, buffer_dict in data.items():\n            for plot in self.plot_window.plot_list:\n                if plot.name == plot_name:\n\n                    for buffer_name, buffer_data in buffer_dict.items():\n                        for buffer in plot.data_buffers:\n                            if buffer.name == buffer_name:\n                                buffer.set(buffer_data)\n\n    def save_state(self, path):\n        """"""\n        Save the data in the plots given a path.\n\n        Args:\n            path (str): path to save the data.\n\n        """"""\n        data = self.get_state()\n        with open(path, \'wb\') as f:\n            pickle.dump(data, f, protocol=3)\n\n    def load_state(self, path):\n        """"""\n        Load the data to the plots given a path.\n\n        Args:\n            path (str): path to load the data.\n\n        """"""\n        with open(path, \'rb\') as f:\n            data = pickle.load(f)\n            self.set_state(data)\n'"
mushroom_rl/utils/plots/__init__.py,0,"b""__all__ = []\n\ntry:\n    from .plot_item_buffer import PlotItemBuffer\n    __all__.append('PlotItemWBuffer')\n\n    from .databuffer import DataBuffer\n    __all__.append('DataBuffer')\n\n    from .window import Window\n    __all__.append('Window')\n\n    from .common_plots import Actions, LenOfEpisodeTraining, Observations,\\\n        RewardPerEpisode, RewardPerStep\n\n    __all__ += ['Actions', 'LenOfEpisodeTraining', 'Observations',\n                'RewardPerEpisode', 'RewardPerStep']\n\n    from ._implementations import common_buffers\n    __all__.append('common_buffers')\n\nexcept ImportError:\n    pass\n"""
mushroom_rl/utils/plots/common_plots.py,0,"b'from mushroom_rl.utils.plots import PlotItemBuffer, DataBuffer\nfrom mushroom_rl.utils.plots.plot_item_buffer import PlotItemBufferLimited\n\n\nclass RewardPerStep(PlotItemBuffer):\n    """"""\n    Class that represents a plot for the reward at every step.\n\n    """"""\n    def __init__(self, plot_buffer):\n        """"""\n        Constructor.\n\n        Args:\n            plot_buffer (DataBuffer): data buffer to be used.\n\n        """"""\n        title = ""Step_Reward""\n        curves_params = [dict(data_buffer=plot_buffer)]\n        super().__init__(title, curves_params)\n\n\nclass RewardPerEpisode(PlotItemBuffer):\n    """"""\n    Class that represents a plot for the accumulated reward per episode.\n\n    """"""\n    def __init__(self, plot_buffer):\n        """"""\n        Constructor.\n\n        Args:\n            plot_buffer (DataBuffer): data buffer to be used.\n\n        """"""\n        title = ""Episode_Reward""\n        curves_params = [dict(data_buffer=plot_buffer)]\n        super().__init__(title, curves_params)\n\n\nclass Actions(PlotItemBufferLimited):\n    """"""\n    Class that represents a plot for the actions.\n\n    """"""\n    def __init__(self, plot_buffers, maxs=None, mins=None):\n        """"""\n        Constructor.\n\n        Args:\n            plot_buffer (DataBuffer): data buffer to be used;\n            maxs(list, None): list of max values of each data buffer plotted.\n                If an element is None, no max line is drawn;\n            mins(list, None): list of min values of each data buffer plotted.\n                If an element is None, no min line is drawn.\n\n        """"""\n        title = ""Actions""\n        super().__init__(title, plot_buffers, maxs=maxs, mins=mins)\n\n\nclass Observations(PlotItemBufferLimited):\n    """"""\n    Class that represents a plot for the observations.\n\n    """"""\n    def __init__(self, plot_buffers, maxs=None, mins=None, dotted_limits=None):\n        """"""\n        Constructor.\n\n        Args:\n            plot_buffer (DataBuffer): data buffer to be used;\n            maxs(list, None): list of max values of each data buffer plotted.\n                If an element is None, no max line is drawn;\n            mins(list, None): list of min values of each data buffer plotted.\n                If an element is None, no min line is drawn.\n            dotted_limits (list, None): list of booleans. If True, the\n                corresponding limit is dotted; otherwise, it is printed as a\n                solid line.\n\n        """"""\n        title = ""Observations""\n        super().__init__(title, plot_buffers, maxs=maxs, mins=mins,\n                         dotted_limits=dotted_limits)\n\n\nclass LenOfEpisodeTraining(PlotItemBuffer):\n    """"""\n    Class that represents a plot for the length of the episode.\n\n    """"""\n    def __init__(self, plot_buffer):\n        """"""\n        Constructor.\n\n        Args:\n            plot_buffer (DataBuffer): data buffer to be used;\n\n        """"""\n        title = ""Len of Episode""\n        plot_params = [dict(data_buffer=plot_buffer)]\n        super().__init__(title, plot_params)\n'"
mushroom_rl/utils/plots/databuffer.py,0,"b'import pickle\nfrom collections import deque\n\n\nclass DataBuffer(object):\n    """"""\n    This class represents the data of a certain variable and how it should be\n    recorded.\n\n    """"""\n    def __init__(self, name, length=None):\n        """"""\n        Constructor.\n\n        Args:\n            name (str): name of the data buffer (each data buffer represents one\n                variable);\n            length (int, None): size of the data buffer queue. If length is\n                ``None``, the buffer is unbounded.\n\n        """"""\n        self._buffer = deque(maxlen=length)\n        self.name = name\n        self._tracking_enabled = True\n\n        assert isinstance(name, str), ""Name of DataBuffer needs to be a string""\n\n    @property\n    def size(self):\n        """"""\n        Returns:\n            The size of the queue.\n\n        """"""\n        return len(self._buffer)\n\n    def update(self, data):\n        """"""\n        Append values to buffer if tracking enabled.\n\n        Args:\n            data (list): list of values to append.\n\n        """"""\n        if self._tracking_enabled:\n            self._buffer.extend(data)\n\n    def get(self):\n        """"""\n        Getter.\n\n        Returns:\n            Buffer queue.\n\n        """"""\n        return self._buffer\n\n    def set(self, buffer):\n        """"""\n        Setter.\n\n        Args:\n            buffer (deque): the queue to be used as buffer.\n\n        """"""\n        self._buffer = buffer\n\n    def reset(self):\n        """"""\n        Remove all the values from buffer.\n\n        """"""\n        self._buffer.clear()\n\n    def enable_tracking(self, status):\n        """"""\n        Enable or disable tracking of data. If tracking is disabled, data is not\n        stored in the buffer.\n\n        Args:\n            status (bool): whether to enable (True) or disable (False) tracking.\n\n        """"""\n        self._tracking_enabled = status\n\n    def save(self, path):\n        """"""\n        Save the data buffer.\n\n        """"""\n        path = path + ""/{}"".format(self.name)\n        with open(path, ""wb"") as file:\n            pickle.dump(self, file)\n\n    @staticmethod\n    def load(path):\n        """"""\n        Load the data buffer.\n\n        Returns:\n            The loaded data buffer instance.\n\n        """"""\n        with open(path, ""rb"") as file:\n            loaded_instance = pickle.load(file)\n\n        return loaded_instance\n'"
mushroom_rl/utils/plots/plot_item_buffer.py,0,"b'import random\nfrom itertools import product\n\nimport numpy as np\nfrom PyQt5.QtGui import QPen\nfrom pyqtgraph import PlotItem, PlotDataItem, mkPen, mkColor, mkQApp\n\n\nclass PlotItemBuffer(PlotItem):\n    """"""\n    Class that represents a plot of any type of variable stored in a data\n    buffer.\n\n    """"""\n    def __init__(self, name, plot_item_data_params, *args, **kwargs):\n        """"""\n        Constructor.\n\n        Args:\n            name (str): name of plot. Also show in the title section of the\n                plot;\n            plot_item_data_params (list): dictionary of ``DataBuffer`` and\n                respective parameters;\n            *args (list): positional arguments to be passed to ``PlotItem``\n                class;\n            **kwargs (dict): dictionary to be passed to ``PlotItem`` class.\n\n        """"""\n        mkQApp()\n        self.data_buffers = list()\n        self._plot_drawing_params = list()\n        self._curves_names = []\n        self._refresh_activated = True\n\n        self.name = name\n\n        if len(plot_item_data_params) == 1 and (""pen"" not in plot_item_data_params[0]):\n            plot_item_data_params[0][""pen""] = mkPen(color=\'r\')\n\n        if len(plot_item_data_params) > 1:\n            for single_params in plot_item_data_params:\n                colors = self.get_n_colors(len(plot_item_data_params))\n                if ""pen"" not in single_params:\n                    color = colors[plot_item_data_params.index(single_params)]\n                    color = list(color)\n                    color.append(255)\n                    single_params[""pen""] = mkPen(color=mkColor(tuple(color)))\n                if ""name"" not in single_params:\n                    single_params[""name""] = single_params[""data_buffer""].name\n\n        for single_params in plot_item_data_params:\n            self.data_buffers.append(single_params[""data_buffer""])\n            self._curves_names.append(single_params[""data_buffer""].name)\n            single_params.pop(""data_buffer"", None)\n            self._plot_drawing_params.append(single_params)\n\n        kwargs[\'title\'] = self.name\n\n        super().__init__(*args, **kwargs)\n\n        if not isinstance(self.data_buffers, list):\n            self.data_buffers = [self.data_buffers]\n\n        self.plot_data_items_list = list()\n        if self._plot_drawing_params is not None:\n            for data_buffer, plot_drawing_param in zip(\n                    self.data_buffers, self._plot_drawing_params):\n                self.plot_data_items_list.append(\n                    PlotDataItem(data_buffer.get(), **plot_drawing_param)\n                )\n        else:\n            for data_buffer, plot_drawing_param in zip(\n                    self.data_buffers, self._plot_drawing_params):\n                self.plot_data_items_list.append(\n                    PlotDataItem(data_buffer.get(), **plot_drawing_param)\n                )\n\n        if self.len_curves > 1:\n            self.addLegend()\n\n    def refresh(self):\n        """"""\n        Refresh buffer only if activated in window.\n\n        """"""\n        if self._refresh_activated:\n            for curve, data_buffer in zip(\n                    self.plot_data_items_list, self.data_buffers):\n                curve.setData(data_buffer.get())\n\n    def draw(self, item):\n        """"""\n        Draw curve.\n\n        Args:\n            item (PlotDataItem): curve item associated with a DataBuffer to be\n                drawn.\n\n        """"""\n        if item not in self.listDataItems():\n            self.addItem(item)\n\n    def erase(self, item):\n        """"""\n        Erase curve in case it is drawn.\n\n        Args:\n             item (PlotDataItem): curve item associated with a data buffer to be\n                erased.\n\n        """"""\n        index_item = self.plot_data_items_list.index(item)\n\n        try:\n            self.legend.removeItem(self._curves_names[index_item])\n        except:\n            pass\n\n        if item in self.listDataItems():\n            self.removeItem(item)\n\n    def refresh_state(self, state):\n        """"""\n        Setter.\n\n        Args:\n            state (bool): whether to refresh state or not.\n\n        """"""\n        self._refresh_activated = state\n\n    @property\n    def curves_names(self):\n        """"""\n        Returns:\n             List of curves names.\n\n        """"""\n        return self._curves_names\n\n    @property\n    def len_curves(self):\n        """"""\n        Returns:\n            The number of curves.\n\n        """"""\n        return len(self.plot_data_items_list)\n\n    @staticmethod\n    def get_n_colors(number_of_colors, min_value=50, max_value=256):\n        """"""\n        Get n very distinct colors. The color vector is a 3D vector.\n        To calculate the different colors, it is considered that the volume of\n        RGB space is divided into n parts, and the coordinates of each center is\n        the value of the color.\n\n        Args:\n            number_of_colors (int): number of colors to get;\n            min_value (int, 50): minimum value of each component of the color;\n            max_value (int, 256): maximum value of each component of the color.\n\n        Returns:\n            List of RGB tuples that represent each color.\n\n        """"""\n        total_area = (max_value - min_value) ** 3\n        area_per_color = total_area / number_of_colors\n        side_of_area_per_color = int(area_per_color ** (1 / 3))\n        onedpoints = np.arange(min_value, max_value, side_of_area_per_color)\n\n        colors = random.sample(list(product(onedpoints, repeat=3)),\n                               number_of_colors)\n\n        return colors\n\n\nclass PlotItemBufferLimited(PlotItemBuffer):\n    """"""\n    This class represents the plots with limits on the variables.\n\n    """"""\n    def __init__(self, title, plot_buffers, maxs=None, mins=None,\n                 dotted_limits=None):\n        """"""\n        Contructor.\n\n        Args:\n            title (str): name of the plot. Also is show in the title section by\n                default;\n            plot_buffers (DataBuffer): List of DataBuffers for each curve;\n            maxs (list, None): list of maximums values to draw horizontal\n                lines;\n            mins (list, None): list of minimum values to draw horizontal\n                lines;\n            dotted_limits (list, None): list of booleans. If True, the\n                corresponding limit is dotted; otherwise, it is printed as a\n                solid line.\n\n        """"""\n        curves_params = []\n\n        error_msg = """"\n        if maxs is not None:\n            if len(maxs) != len(plot_buffers):\n                error_msg += ""maxs""\n        if mins is not None:\n            if len(mins) != len(plot_buffers):\n                error_msg += "" and mins""\n\n        if error_msg != """":\n            raise TypeError(\n                ""Size of {} parameter(s) doesnt correspond to the number of""\n                ""plot_buffers"".format(error_msg)\n            )\n\n        for i in plot_buffers:\n            curves_params.append(dict(data_buffer=i, name=i.name))\n\n        super().__init__(title, curves_params)\n        self._maxs_exist = False\n        self._mins_exist = False\n\n        if isinstance(maxs, list):\n            self._maxs_exist = bool(maxs) or bool(dotted_limits)\n        if isinstance(mins, list):\n            self._mins_exist = bool(mins) or bool(dotted_limits)\n\n        self._maxs_vals = maxs\n        self._mins_vals = mins\n\n        if dotted_limits is not None:\n            for i in range(len(dotted_limits)):\n                if dotted_limits[i]:\n                    self._maxs_vals[i] = 1\n                    self._mins_vals[i] = -1\n\n        self._max_line_items = []\n        self._min_line_items = []\n\n        if self._maxs_vals is not None:\n            for i in range(len(self._maxs_vals)):\n                if self._maxs_vals[i] == np.inf or self._maxs_vals[i] == -np.inf:\n                    self._maxs_vals[i] = None\n                self._max_line_items.append(None)\n        if self._mins_vals is not None:\n            for i in range(len(self._mins_vals)):\n                if self._mins_vals[i] == np.inf or self._mins_vals[i] == -np.inf:\n                    self._mins_vals[i] = None\n                self._min_line_items.append(None)\n\n        self._dotted_limits = dotted_limits\n\n    def draw(self, item):\n        super().draw(item)\n        i_item = self.plot_data_items_list.index(item)\n\n        pen = item.opts[\'pen\']\n\n        if self._dotted_limits is not None:\n            if self._dotted_limits[i_item]:\n                pen = QPen(pen)\n                pen.setDashPattern([6, 12, 6, 12])\n\n        if self._maxs_exist:\n            if self._maxs_vals[i_item] is not None:\n                self._max_line_items[i_item] = self.addLine(\n                    y=self._maxs_vals[i_item], pen=pen\n                )\n\n        if self._mins_exist:\n            if self._mins_vals[i_item] is not None:\n                self._min_line_items[i_item] = self.addLine(\n                    y=self._mins_vals[i_item], pen=pen\n                )\n\n    def erase(self, item):\n        super().erase(item)\n        i_item = self.plot_data_items_list.index(item)\n\n        if self._maxs_exist:\n            self.removeItem(self._max_line_items[i_item])\n\n        if self._mins_exist:\n            self.removeItem(self._min_line_items[i_item])\n'"
mushroom_rl/utils/plots/window.py,0,"b'import time\n\nfrom PyQt5.QtGui import QBrush, QColor\nfrom PyQt5.QtWidgets import QTreeWidgetItem\nfrom PyQt5 import QtCore\n\nimport pyqtgraph as pg\nfrom pyqtgraph.Qt import QtGui\n\n\nclass Window(QtGui.QSplitter):\n    """"""\n    This class is used creating windows for plotting.\n\n    """"""\n    WhiteBrush = QBrush(QColor(255, 255, 255))\n    GreenBrush = QBrush(QColor(0, 255, 0))\n\n    def __init__(self, plot_list, track_if_deactivated=False,\n                 title=""None"", size=(800, 600), fullscreen=False,\n                 update_freq=10):\n        """"""\n        Constructor.\n\n        Args:\n            plot_list (list): list of plots to add to the window;\n            track_if_deactivated (bool, False): flag that determines if the\n                respective data buffers should track the variable, when plot is\n                not active. Should be used when data buffers in the plot do not\n                need to be tracked;\n            title (str, ""None""): title of the plot;\n            size (tuple, (800, 600)): size of the window in pixels;\n            fullscreen (bool, False): fullscreen flag;\n            update_freq (int, 10): frequency(Hz) of update of the plots.\n\n        """"""\n        pg.mkQApp()\n\n        self._title = title\n        self._size = size\n        self._fullscreen = fullscreen\n        self._track_if_deactivated = track_if_deactivated\n\n        self.plot_list = plot_list\n\n        if not isinstance(self.plot_list, list):\n            self.plot_list = [self.plot_list]\n\n        super().__init__(QtCore.Qt.Horizontal)\n\n        self._activation_widget = QtGui.QTreeWidget()\n        self._activation_widget.setHeaderLabels([""Plots""])\n\n        self._activation_widget.itemClicked.connect(self.clicked)\n\n        self._dependencies = dict()\n        self._activation_items = dict()\n\n        listitem = QTreeWidgetItem(self._activation_widget, [""ALL""])\n        listitem.setBackground(0, Window.WhiteBrush)\n\n        for plot_instance in self.plot_list:\n            listitem_parent = QTreeWidgetItem(self._activation_widget, [plot_instance.name])\n            listitem_parent.setBackground(0, Window.WhiteBrush)\n\n            self._activation_items[plot_instance.name] = listitem_parent\n            self._dependencies[plot_instance.name] = [self, plot_instance]\n\n            for i in range(len(plot_instance.data_buffers)):\n                listitem = QTreeWidgetItem(listitem_parent, [plot_instance.data_buffers[i].name])\n                listitem.setBackground(0, Window.WhiteBrush)\n\n                self._activation_items[plot_instance.data_buffers[i].name] = listitem\n                self._dependencies[plot_instance.data_buffers[i].name] = [\n                    plot_instance, plot_instance.plot_data_items_list[i]\n                ]\n\n        self._GraphicsWindow = pg.GraphicsWindow(title=title)\n\n        self.addWidget(self._activation_widget)\n        self.addWidget(self._GraphicsWindow)\n\n        self.timecounter = time.perf_counter()\n        self.timeinterval = (1.0 / update_freq)\n\n        self.refresh()\n\n        for plot in self.plot_list:\n            self._deactivate_buffer_plots(plot)\n\n    def draw(self, item):\n        """"""\n        Draw ``PlotItem`` on the plot widget.\n\n        Args:\n            item (PlotItem): plot item to be drawn.\n\n        """"""\n        self._GraphicsWindow.addItem(item)\n        self._GraphicsWindow.nextRow()\n\n    def erase(self, item):\n        """"""\n        Remove ``PlotItem`` from the plot widget.\n\n        Args:\n            item (PlotItem): plot item to be removed.\n\n        """"""\n        self._GraphicsWindow.removeItem(item)\n\n    def refresh(self):\n        """"""\n        Refresh all plots if the refresh timer allows it.\n\n        """"""\n        if time.perf_counter() - self.timecounter > self.timeinterval:\n            self.timecounter = time.perf_counter()\n\n            for plot_instance in self.plot_list:\n                plot_instance.refresh()\n\n            QtGui.QGuiApplication.processEvents()\n\n    def activate(self, item):\n        """"""\n        Activate the plots and data buffers connected to the given item.\n\n        Args:\n             item (QTreeWidgetItem): ``QTreeWidgetItem`` that represents the\n             plots to be activated.\n\n        """"""\n        if not self.check_activated(item):\n            item.setBackground(0, Window.GreenBrush)\n            callback_func_params = self._dependencies[item.text(0)]\n            callback_func_params[0].draw(callback_func_params[1])\n            if isinstance(callback_func_params[0], Window):\n                self._activate_buffer_plots(callback_func_params[1])\n\n    def deactivate(self, item):\n        """"""\n        Deactivate the plots and DataBuffers connected to the given item\n\n        Args:\n             item (QTreeWidgetItem): ``QTreeWidgetItem`` that represents the\n             plots to be deactivated.\n\n        """"""\n        if self.check_activated(item):\n            item.setBackground(0, Window.WhiteBrush)\n            callback_func_params = self._dependencies[item.text(0)]\n            if isinstance(callback_func_params[0], Window):\n                for curve_name in callback_func_params[1].curves_names:\n                    self.deactivate(self._activation_items[curve_name])\n                self._deactivate_buffer_plots(callback_func_params[1])\n\n            callback_func_params[0].erase(callback_func_params[1])\n\n    def clicked(self, item):\n        """"""\n        Callback to be used when plot activation widget object is clicked.\n\n        Args:\n             item (QTreeWidgetItem): ``QTreeWidgetItem`` clicked.\n\n        """"""\n        if item.text(0) == ""ALL"":\n            if self.check_activated(item):\n                item.setBackground(0, Window.WhiteBrush)\n                for activation_item_key in self._activation_items:\n                    activation_item = self._activation_items[\n                        activation_item_key\n                    ]\n                    self.deactivate(activation_item)\n\n            else:\n                item.setBackground(0, Window.GreenBrush)\n                for activation_item_key in self._activation_items:\n                    activation_item = self._activation_items[\n                        activation_item_key\n                    ]\n                    self.activate(activation_item)\n\n        else:\n            if self.check_activated(item):\n                self.deactivate(item)\n            else:\n                self.activate(item)\n\n    def _deactivate_buffer_plots(self, plot):\n        plot.refresh_state(False)\n        if not self._track_if_deactivated[self.plot_list.index(plot)]:\n            for buffer in plot.data_buffers:\n                buffer.enable_tracking(False)\n\n    def _activate_buffer_plots(self, plot):\n        for buffer in plot.data_buffers:\n            plot.refresh_state(True)\n            buffer.enable_tracking(True)\n\n    @staticmethod\n    def check_activated(item):\n        """"""\n        Check if ``QTreeWidgetItem`` is activated.\n\n        Args:\n            item (QTreeWidgetItem): ``QTreeWidgetItem`` to be analysed.\n\n        Returns:\n            If activated returns True, else False.\n\n        """"""\n        return (item.background(0).color().getRgb()[0] == 0\n                and item.background(0).color().getRgb()[2] == 0)\n'"
tests/algorithms/helper/utils.py,6,"b'# Uncomment to run tests locally\n# import sys\n# import os\n# sys.path = [os.getcwd()] + sys.path\n\nimport torch\nimport numpy as np\nfrom sklearn.ensemble import ExtraTreesRegressor\nimport itertools\n\nimport mushroom_rl\nfrom mushroom_rl.environments.environment import MDPInfo\nfrom mushroom_rl.policy.td_policy import TDPolicy\nfrom mushroom_rl.policy.torch_policy import TorchPolicy\nfrom mushroom_rl.policy.policy import ParametricPolicy\nfrom mushroom_rl.algorithms.actor_critic.deep_actor_critic.sac import SACPolicy\nfrom mushroom_rl.utils.replay_memory import ReplayMemory, PrioritizedReplayMemory\nfrom mushroom_rl.approximators._implementations.ensemble import Ensemble\nfrom mushroom_rl.approximators._implementations.action_regressor import ActionRegressor\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.policy.noise_policy import OrnsteinUhlenbeckPolicy\nfrom mushroom_rl.features._implementations.tiles_features import TilesFeatures\nfrom mushroom_rl.utils.parameters import Parameter, AdaptiveParameter, LinearParameter\nfrom mushroom_rl.distributions.gaussian import GaussianDiagonalDistribution\nfrom mushroom_rl.utils.table import Table\nfrom mushroom_rl.utils.spaces import Discrete\nfrom mushroom_rl.features._implementations.functional_features import FunctionalFeatures\nfrom mushroom_rl.features._implementations.basis_features import BasisFeatures\n\nclass TestUtils:\n    \n    @classmethod\n    def assert_eq(cls, this, that):\n        """"""\n        Check and compare two objects for equality\n        """"""\n        if cls._check_type(this, that, list):\n            for a, b in zip(this, that): cls.assert_eq(a, b)\n        elif cls._check_type(this, that, dict):\n            for a, b in zip(this.values(), that.values()): cls.assert_eq(a, b)\n        elif cls._check_subtype(this, that, Ensemble):\n            assert len(this) == len(that)\n            for i in range(0, len(this)):\n                cls.assert_eq(this[i], that[i])\n        elif cls._check_type(this, that, Regressor):\n            if cls._check_type(this._impl, that._impl, ActionRegressor):\n                this = this.model\n                that = that.model\n            for i in range(0, len(this)):\n                if cls._check_type(this[i], that[i], list) or cls._check_type(this[i], that[i], Ensemble) or cls._check_type(this[i], that[i], ExtraTreesRegressor):\n                    cls.assert_eq(this[i], that[i])\n                else:\n                    assert cls.eq_weights(this[i], that[i])\n        elif  cls._check_subtype(this, that, TorchPolicy) or cls._check_type(this, that, SACPolicy) or cls._check_subtype(this, that, ParametricPolicy):\n            assert cls.eq_weights(this, that)\n        elif cls._check_subtype(this, that, TDPolicy):\n            cls.assert_eq(this.get_q(), that.get_q())\n        elif cls._check_type(this, that, torch.optim.Optimizer):\n            assert cls.eq_save_dict(this.state_dict(), that.state_dict())\n        elif cls._check_type(this, that, itertools.chain):\n            assert cls.eq_chain(this, that)\n        elif cls._check_type(this, that, MDPInfo):\n            assert cls.eq_mdp_info(this, that)\n        elif cls._check_type(this, that, ReplayMemory):\n            assert cls.eq_replay_memory(this, that)\n        elif cls._check_type(this, that, PrioritizedReplayMemory):\n            assert cls.eq_prioritized_replay_memory(this, that)\n        elif cls._check_type(this, that, OrnsteinUhlenbeckPolicy):\n            assert cls.eq_ornstein_uhlenbeck_policy(this, that)\n        elif cls._check_type(this, that, TilesFeatures):\n            assert cls.eq_tiles_features(this, that)\n        elif cls._check_type(this, that, Parameter):\n            assert cls.eq_parameter(this, that)\n        elif cls._check_type(this, that, AdaptiveParameter):\n            assert cls.eq_adaptive_parameter(this, that)\n        elif cls._check_type(this, that, LinearParameter):\n            assert cls.eq_linear_parameter(this, that)\n        elif cls._check_type(this, that, GaussianDiagonalDistribution):\n            assert cls.eq_gaussian_diagonal_dist(this, that)\n        elif cls._check_type(this, that, Table):\n            assert cls._eq_numpy(this.table, that.table)\n        elif cls._check_type(this, that, Discrete):\n            assert cls.eq_discrete(this, that)\n        elif cls._check_type(this, that, FunctionalFeatures):\n            assert cls._eq_functional_features(this, that)\n        elif cls._check_type(this, that, BasisFeatures):\n            assert cls._eq_basis_features(this, that)\n        elif cls._check_type(this, that, ExtraTreesRegressor):\n            pass # Equality of ExtraTreeRegressor is not tested\n        elif callable(this) and callable(that):\n            pass # Equality of Functions is not tested\n        elif cls._check_type(this, that, torch.nn.parameter.Parameter):\n            assert cls._eq_torch(this, that)\n        elif cls._check_type(this, that, np.ndarray):\n            assert cls._eq_numpy(this, that)\n        else:\n            assert this == that\n    \n    @classmethod\n    def eq_weights(cls, this, that):\n        """"""\n        Compare the weights of two objects for equality\n        """"""\n        return cls._eq_numpy(this.get_weights(), that.get_weights())\n    \n    @classmethod\n    def eq_box(cls, this, that):\n        """"""\n        Compare two Box objects for equality\n        """"""\n        return cls._eq_numpy(this.low, that.low) and cls._eq_numpy(this.high, that.high) and this.shape == that.shape\n    \n    @classmethod\n    def eq_discrete(cls, this, that):\n        """"""\n        Compare two Discrete objects for equality\n        """"""\n        return cls._eq_numpy(this.values, that.values) and this.n == that.n\n\n    @classmethod\n    def eq_chain(cls, this, that):\n        """"""\n        Compare two chain objects for equality\n        """"""\n        return list(this) == list(that)\n\n    @classmethod\n    def eq_save_dict(cls, this, that):\n        """"""\n        Compare two save_dict objects for equality\n        """"""\n        this_state, this_param_groups = this.values()\n        that_state, that_param_groups = that.values()\n        # params contains Tensor Ids which change after loading into a new optimizer instance ref: https://pytorch.org/docs/stable/_modules/torch/optim/optimizer.html\n        del this_param_groups[0][\'params\']\n        del that_param_groups[0][\'params\']\n        res = this_param_groups == that_param_groups\n        for t1, t2 in zip(this_state.values(), that_state.values()):\n            for v1, v2 in zip(t1.values(), t2.values()):\n                if isinstance(v1, torch.Tensor):\n                    res &= cls._eq_torch(v1, v2)\n                else:\n                    res &= v1 == v2\n        return res\n\n    @classmethod\n    def eq_mdp_info(cls, this, that):\n        """"""\n        Compare two mdp_info objects for equality\n        """"""\n        res = True\n        if isinstance(this.observation_space, mushroom_rl.utils.spaces.Box):\n            res &= cls.eq_box(this.observation_space, that.observation_space)\n        elif isinstance(this.observation_space, mushroom_rl.utils.spaces.Discrete):\n            res = cls.eq_discrete(this.observation_space, that.observation_space)\n        else:\n            raise TypeError(\'Type not supported\')\n\n        if isinstance(this.action_space, mushroom_rl.utils.spaces.Box):\n            res &= cls.eq_box(this.action_space, that.action_space)\n        elif isinstance(this.action_space, mushroom_rl.utils.spaces.Discrete):\n            res &= cls.eq_discrete(this.action_space, that.action_space)\n        else:\n            raise TypeError(\'Type not supported\')\n            \n        res &= this.gamma == that.gamma\n        res &= this.horizon == that.horizon\n        return res\n\n    @classmethod\n    def eq_ornstein_uhlenbeck_policy(cls, this, that):\n        """"""\n        Compare two OrnsteinUhlenbeckPolicy objects for equality\n        """"""\n        \n        res = cls.eq_weights(this, that)\n        res &= cls._eq_numpy(this._sigma, that._sigma)\n        res &= this._theta == that._theta\n        res &= this._dt == that._dt\n        res &= cls._eq_numpy(this._x0, that._x0)\n        res &= cls._eq_numpy(this._x_prev, that._x_prev)\n        return res\n\n    @classmethod\n    def eq_replay_memory(cls, this, that):\n        """"""\n        Compare two ReplayMemory objects for equality\n        """"""\n        \n        res = this._idx == that._idx\n        res &= this._full == that._full\n        for a, b in zip(this._states, that._states):\n            res &= cls._eq_numpy(a, b)\n        res &= this._actions == that._actions\n        res &= this._rewards == that._rewards\n        for a, b in zip(this._next_states, that._next_states):\n            res &= cls._eq_numpy(a, b)\n        res &= this._absorbing == that._absorbing\n        res &= this._last == that._last\n        return res\n\n    @classmethod\n    def eq_prioritized_replay_memory(cls, this, that):\n        """"""\n        Compare two PrioritizedReplayMemory objects for equality\n        """"""\n        \n        res = this._initial_size == that._initial_size\n        res &= this._max_size == that._max_size\n        res &= this._alpha == that._alpha\n        res &= cls.eq_linear_parameter(this._beta, that._beta)\n        res &= this._epsilon == that._epsilon\n        res &= cls.eq_sum_tree(this._tree, that._tree)\n        return res\n\n    @classmethod\n    def eq_sum_tree(cls, this, that):\n        """"""\n        Compare two SumTree objects for equality\n        """"""\n        \n        res = this._max_size == that._max_size\n        res &= cls._eq_numpy(this._tree, that._tree)\n        res &= len(this._data) == len(that._data)\n        for a, b in zip(this._data, that._data):\n            res &= cls._eq_listlike(a, b)\n        res &= this._idx == that._idx\n        res &= this._full == that._full\n        return res\n\n    @classmethod\n    def eq_tiles_features(cls, this, that):\n        """"""\n        Compare two TilesFeatures objects for equality\n        """"""\n        \n        res = this.size == that.size\n        for a, b in zip(this._tiles, that._tiles):\n            res &= cls.eq_tiles(a, b)\n        return res\n\n    @classmethod\n    def eq_tiles(cls, this, that):\n        """"""\n        Compare two Tiles objects for equality\n        """"""\n        \n        res = this.size == that.size\n        for a, b in zip(this._range, that._range):\n            res &= a == b\n        for a, b in zip(this._n_tiles, that._n_tiles):\n            res &= a == b\n        if this._state_components is not None and that._state_components is not None:\n            for a, b in zip(this._state_components, that._state_components):\n                res &= a == b\n        return res\n\n    @classmethod\n    def eq_parameter(cls, this, that):\n        """"""\n        Compare two Parameter objects for equality\n        """"""\n        \n        res = this._initial_value == that._initial_value\n        res &= this._min_value == that._min_value\n        res &= this._max_value == that._max_value\n        res &= cls._eq_numpy(this._n_updates.table, that._n_updates.table)\n        return res\n\n    @classmethod\n    def eq_linear_parameter(cls, this, that):\n        """"""\n        Compare two LinearParameter objects for equality\n        """"""\n        \n        res = cls.eq_parameter(this, that)\n        res &= this._coeff == that._coeff\n        return res\n\n    @classmethod\n    def eq_adaptive_parameter(cls, this, that):\n        """"""\n        Compare two AdaptiveParameter objects for equality\n        """"""\n        \n        res = cls._eq_numpy(this._eps, that._eps)\n        return res\n\n    @classmethod\n    def eq_gaussian_diagonal_dist(cls, this, that):\n        """"""\n        Compare two GaussianDiagonalDistribution objects for equality\n        """"""\n        \n        res = cls._eq_numpy(this.get_parameters(), that.get_parameters())\n        return res\n\n    @classmethod\n    def _eq_functional_features(cls, this, that):\n        """"""\n        Compare two FunctionalFeatures objects for equality\n        """"""\n        \n        res = this.size == that.size\n        return res\n\n    @classmethod\n    def _eq_basis_features(cls, this, that):\n        """"""\n        Compare two BasisFeatures objects for equality\n        """"""\n        \n        res = this.size == that.size\n        for a, b in zip(this._basis, that._basis):\n            res &= str(a) == str(b)\n        return res\n\n    @classmethod\n    def _eq_listlike(cls, this, that):\n        """"""\n        Compare the elements of two listlike objects for equality\n        """"""\n        \n        res = len(this) == len(that)\n        for a, b in zip(this, that):\n            if cls._check_type(a, b, np.ndarray):\n                res &= cls._eq_numpy(a, b)\n            elif cls._check_type(a, b, torch.nn.parameter.Parameter):\n                res &= cls._eq_torch(a, b)\n            else:\n                res &= a == b\n        return res\n\n    @staticmethod\n    def _check_type(this, that, check_type):\n        """"""\n        Check if two object have a specific type\n        """"""\n        return isinstance(this, check_type) and isinstance(that, check_type)\n\n    @staticmethod\n    def _check_subtype(this, that, check_type):\n        """"""\n        Check if two objects have the type of a subclass of a specific type\n        """"""\n        return issubclass(type(this), check_type) and issubclass(type(that), check_type) and type(this) == type(that)\n\n    @staticmethod\n    def _eq_numpy(this, that):\n        return np.array_equal(this, that)\n\n    @staticmethod\n    def _eq_torch(this, that):\n        return torch.equal(this, that)\n\n'"
tests/environments/mujoco_envs/__init__.py,0,b''
tests/environments/mujoco_envs/test_ball_in_a_cup.py,0,"b'try:\n    from mushroom_rl.environments.mujoco_envs import BallInACup\n    import numpy as np\n\n\n    def linear_movement(start, end, n_steps, i):\n        t = np.minimum(1., float(i) / float(n_steps))\n        return start + (end - start) * t\n\n\n    def test_ball_in_a_cup():\n        env = BallInACup()\n\n        des_pos = np.array([0.0, 0.58760536, 0.0, 1.36004913, 0.0, -0.32072943, -1.57])\n        p_gains = np.array([200, 300, 100, 100, 10, 10, 2.5])\n        d_gains = np.array([7, 15, 5, 2.5, 0.3, 0.3, 0.05])\n\n        obs = env.reset()\n        done = False\n        i = 0\n        while not done:\n            a = p_gains * (linear_movement(des_pos, np.zeros_like(des_pos), 500, i) -\n                           obs[0:14:2]) + d_gains * (np.zeros_like(des_pos) - obs[1:14:2])\n\n            # Check the observations\n            assert np.allclose(obs[0:14:2], env.sim.data.qpos[0:7])\n            assert np.allclose(obs[1:14:2], env.sim.data.qvel[0:7])\n            assert np.allclose(obs[14:17], env.sim.data.body_xpos[40])\n            assert np.allclose(obs[17:], env.sim.data.body_xvelp[40])\n            obs, reward, done, info = env.step(a)\n\n            i += 1\n\nexcept ImportError:\n    pass\n'"
tests/environments/mujoco_envs/test_humanoid_gait.py,0,"b'try:\n    import numpy as np\n\n    from mushroom_rl.environments.mujoco_envs import HumanoidGait\n    from mushroom_rl.environments.mujoco_envs.humanoid_gait import \\\n        VelocityProfile3D, RandomConstantVelocityProfile, ConstantVelocityProfile\n\n\n    def create_mdp(goal, gamma, horizon):\n        if goal == ""trajectory"":\n            mdp = HumanoidGait(gamma=gamma, horizon=horizon, n_intermediate_steps=10,\n                               goal_reward=""trajectory"",\n                               goal_reward_params=dict(use_error_terminate=True),\n                               use_muscles=True,\n                               obs_avg_window=1, act_avg_window=1)\n\n        elif goal == ""max_vel"":\n            mdp = HumanoidGait(gamma=gamma, horizon=horizon, n_intermediate_steps=10,\n                               goal_reward=""max_vel"",\n                               goal_reward_params=dict(traj_start=True),\n                               use_muscles=False,\n                               obs_avg_window=1, act_avg_window=1)\n\n        elif goal == ""vel_profile"":\n            velocity_profile = dict(profile_instance=VelocityProfile3D([\n                    RandomConstantVelocityProfile(min=0.5, max=2.0),\n                    ConstantVelocityProfile(0),\n                    ConstantVelocityProfile(0)]))\n\n            mdp = HumanoidGait(gamma=gamma, horizon=horizon, n_intermediate_steps=10,\n                               goal_reward=""vel_profile"",\n                               goal_reward_params=dict(traj_start=True,\n                                                       **velocity_profile),\n                               use_muscles=False,\n                               obs_avg_window=1, act_avg_window=1)\n        return mdp\n\n\n    def test_humanoid_gait():\n        np.random.seed(1)\n\n        # MDP\n        gamma = 0.99\n        horizon = 2000\n        for goal in [""trajectory"", ""vel_profile"", ""max_vel""]:\n            mdp = create_mdp(goal, gamma, horizon)\n\n            s = 0\n            done = True\n            mdp.reset()\n            while s < 100:\n                if done:\n                    mdp.reset()\n\n                random_action = np.random.uniform(low=mdp.info.action_space.low / 2.0,\n                                                  high=mdp.info.action_space.high / 2.0,\n                                                  size=mdp.info.action_space.shape[0])\n                obs, reward, done, _ = mdp.step(random_action)\n                s += 1\nexcept ImportError:\n    pass\n\n'"
docs/source/tutorials/code/advanced_experiment.py,0,"b""import numpy as np\n\nfrom mushroom_rl.algorithms.value import SARSALambdaContinuous\nfrom mushroom_rl.approximators.parametric import LinearApproximator\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.features import Features\nfrom mushroom_rl.features.tiles import Tiles\nfrom mushroom_rl.policy import EpsGreedy\nfrom mushroom_rl.utils.callbacks import CollectDataset\nfrom mushroom_rl.utils.parameters import Parameter\nfrom mushroom_rl.environments import Gym\n\n# MDP\nmdp = Gym(name='MountainCar-v0', horizon=np.inf, gamma=1.)\n\n# Policy\nepsilon = Parameter(value=0.)\npi = EpsGreedy(epsilon=epsilon)\n\n# Q-function approximator\nn_tilings = 10\ntilings = Tiles.generate(n_tilings, [10, 10],\n                         mdp.info.observation_space.low,\n                         mdp.info.observation_space.high)\nfeatures = Features(tilings=tilings)\n\napproximator_params = dict(input_shape=(features.size,),\n                           output_shape=(mdp.info.action_space.n,),\n                           n_actions=mdp.info.action_space.n)\n\n# Agent\nlearning_rate = Parameter(.1 / n_tilings)\n\nagent = SARSALambdaContinuous(mdp.info, pi, LinearApproximator,\n                              approximator_params=approximator_params,\n                              learning_rate=learning_rate,\n                              lambda_coeff=.9, features=features)\n\n# Algorithm\ncollect_dataset = CollectDataset()\ncallbacks = [collect_dataset]\ncore = Core(agent, mdp, callbacks_episode=callbacks)\n\n# Train\ncore.learn(n_episodes=100, n_steps_per_fit=1)\n\n# Evaluate\ncore.evaluate(n_episodes=1, render=True)\n"""
docs/source/tutorials/code/approximator.py,0,"b""import numpy as np\n\nfrom mushroom_rl.algorithms.value import SARSALambdaContinuous\nfrom mushroom_rl.approximators.parametric import LinearApproximator\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import *\nfrom mushroom_rl.features import Features\nfrom mushroom_rl.features.tiles import Tiles\nfrom mushroom_rl.policy import EpsGreedy\nfrom mushroom_rl.utils.callbacks import CollectDataset\nfrom mushroom_rl.utils.parameters import Parameter\n\n\n# MDP\nmdp = Gym(name='MountainCar-v0', horizon=np.inf, gamma=1.)\n\n# Policy\nepsilon = Parameter(value=0.)\npi = EpsGreedy(epsilon=epsilon)\n\n# Q-function approximator\nn_tilings = 10\ntilings = Tiles.generate(n_tilings, [10, 10],\n                         mdp.info.observation_space.low,\n                         mdp.info.observation_space.high)\nfeatures = Features(tilings=tilings)\n\n# Agent\nlearning_rate = Parameter(.1 / n_tilings)\napproximator_params = dict(input_shape=(features.size,),\n                           output_shape=(mdp.info.action_space.n,),\n                           n_actions=mdp.info.action_space.n)\nagent = SARSALambdaContinuous(mdp.info, pi, LinearApproximator,\n                              approximator_params=approximator_params,\n                              learning_rate=learning_rate,\n                              lambda_coeff= .9, features=features)\n\n# Algorithm\ncollect_dataset = CollectDataset()\ncallbacks = [collect_dataset]\ncore = Core(agent, mdp, callbacks_episode=callbacks)\n\n# Train\ncore.learn(n_episodes=100, n_steps_per_fit=1)\n\n# Evaluate\ncore.evaluate(n_episodes=1, render=True)\n"""
docs/source/tutorials/code/ddpg.py,6,"b""import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom mushroom_rl.algorithms.actor_critic import DDPG\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments.dm_control_env import DMControl\nfrom mushroom_rl.policy import OrnsteinUhlenbeckPolicy\nfrom mushroom_rl.utils.dataset import compute_J\n\n\nclass CriticNetwork(nn.Module):\n    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n        super().__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._h1 = nn.Linear(n_input, n_features)\n        self._h2 = nn.Linear(n_features, n_features)\n        self._h3 = nn.Linear(n_features, n_output)\n\n        nn.init.xavier_uniform_(self._h1.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h2.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h3.weight,\n                                gain=nn.init.calculate_gain('linear'))\n\n    def forward(self, state, action):\n        state_action = torch.cat((state.float(), action.float()), dim=1)\n        features1 = F.relu(self._h1(state_action))\n        features2 = F.relu(self._h2(features1))\n        q = self._h3(features2)\n\n        return torch.squeeze(q)\n\n\nclass ActorNetwork(nn.Module):\n    def __init__(self, input_shape, output_shape, n_features, **kwargs):\n        super(ActorNetwork, self).__init__()\n\n        n_input = input_shape[-1]\n        n_output = output_shape[0]\n\n        self._h1 = nn.Linear(n_input, n_features)\n        self._h2 = nn.Linear(n_features, n_features)\n        self._h3 = nn.Linear(n_features, n_output)\n\n        nn.init.xavier_uniform_(self._h1.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h2.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h3.weight,\n                                gain=nn.init.calculate_gain('linear'))\n\n    def forward(self, state):\n        features1 = F.relu(self._h1(torch.squeeze(state, 1).float()))\n        features2 = F.relu(self._h2(features1))\n        a = self._h3(features2)\n\n        return a\n\n\n# MDP\nhorizon = 500\ngamma = 0.99\ngamma_eval = 1.\nmdp = DMControl('walker', 'stand', horizon, gamma)\n\n# Policy\npolicy_class = OrnsteinUhlenbeckPolicy\npolicy_params = dict(sigma=np.ones(1) * .2, theta=.15, dt=1e-2)\n\n# Settings\ninitial_replay_size = 500\nmax_replay_size = 5000\nbatch_size = 200\nn_features = 80\ntau = .001\n\n# Approximator\nactor_input_shape = mdp.info.observation_space.shape\nactor_params = dict(network=ActorNetwork,\n                    n_features=n_features,\n                    input_shape=actor_input_shape,\n                    output_shape=mdp.info.action_space.shape)\n\nactor_optimizer = {'class': optim.Adam,\n                   'params': {'lr': 1e-5}}\n\ncritic_input_shape = (actor_input_shape[0] + mdp.info.action_space.shape[0],)\ncritic_params = dict(network=CriticNetwork,\n                     optimizer={'class': optim.Adam,\n                                'params': {'lr': 1e-3}},\n                     loss=F.mse_loss,\n                     n_features=n_features,\n                     input_shape=critic_input_shape,\n                     output_shape=(1,))\n\n# Agent\nagent = DDPG(mdp.info, policy_class, policy_params,\n             actor_params, actor_optimizer, critic_params,\n             batch_size, initial_replay_size, max_replay_size,\n             tau)\n\n# Algorithm\ncore = Core(agent, mdp)\n\n# Fill the replay memory with random samples\ncore.learn(n_steps=initial_replay_size, n_steps_per_fit=initial_replay_size)\n\n# RUN\nn_epochs = 40\nn_steps = 1000\nn_steps_test = 2000\n\ndataset = core.evaluate(n_steps=n_steps_test, render=False)\nJ = compute_J(dataset, gamma_eval)\nprint('Epoch: 0')\nprint('J: ', np.mean(J))\n\nfor n in range(n_epochs):\n    print('Epoch: ', n+1)\n    core.learn(n_steps=n_steps, n_steps_per_fit=1)\n    dataset = core.evaluate(n_steps=n_steps_test, render=False)\n    J = compute_J(dataset, gamma_eval)\n    print('J: ', np.mean(J))\n"""
docs/source/tutorials/code/dqn.py,4,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom mushroom_rl.algorithms.value import DQN\nfrom mushroom_rl.approximators.parametric import TorchApproximator\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import Atari\nfrom mushroom_rl.policy import EpsGreedy\nfrom mushroom_rl.utils.dataset import compute_metrics\nfrom mushroom_rl.utils.parameters import LinearParameter, Parameter\n\n\nclass Network(nn.Module):\n    n_features = 512\n\n    def __init__(self, input_shape, output_shape, **kwargs):\n        super().__init__()\n\n        n_input = input_shape[0]\n        n_output = output_shape[0]\n\n        self._h1 = nn.Conv2d(n_input, 32, kernel_size=8, stride=4)\n        self._h2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n        self._h3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n        self._h4 = nn.Linear(3136, self.n_features)\n        self._h5 = nn.Linear(self.n_features, n_output)\n\n        nn.init.xavier_uniform_(self._h1.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h2.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h3.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h4.weight,\n                                gain=nn.init.calculate_gain('relu'))\n        nn.init.xavier_uniform_(self._h5.weight,\n                                gain=nn.init.calculate_gain('linear'))\n\n    def forward(self, state, action=None):\n        h = F.relu(self._h1(state.float() / 255.))\n        h = F.relu(self._h2(h))\n        h = F.relu(self._h3(h))\n        h = F.relu(self._h4(h.view(-1, 3136)))\n        q = self._h5(h)\n\n        if action is None:\n            return q\n        else:\n            q_acted = torch.squeeze(q.gather(1, action.long()))\n\n            return q_acted\n\n\ndef print_epoch(epoch):\n    print('################################################################')\n    print('Epoch: ', epoch)\n    print('----------------------------------------------------------------')\n\n\ndef get_stats(dataset):\n    score = compute_metrics(dataset)\n    print(('min_reward: %f, max_reward: %f, mean_reward: %f,'\n          ' games_completed: %d' % score))\n\n    return score\n\n\nscores = list()\n\noptimizer = dict()\noptimizer['class'] = optim.Adam\noptimizer['params'] = dict(lr=.00025)\n\n# Settings\nwidth = 84\nheight = 84\nhistory_length = 4\ntrain_frequency = 4\nevaluation_frequency = 250000\ntarget_update_frequency = 10000\ninitial_replay_size = 50000\nmax_replay_size = 500000\ntest_samples = 125000\nmax_steps = 50000000\n\n# MDP\nmdp = Atari('BreakoutDeterministic-v4', width, height, ends_at_life=True,\n            history_length=history_length, max_no_op_actions=30)\n\n# Policy\nepsilon = LinearParameter(value=1.,\n                          threshold_value=.1,\n                          n=1000000)\nepsilon_test = Parameter(value=.05)\nepsilon_random = Parameter(value=1)\npi = EpsGreedy(epsilon=epsilon_random)\n\n# Approximator\ninput_shape = (history_length, height, width)\napproximator_params = dict(\n    network=Network,\n    input_shape=input_shape,\n    output_shape=(mdp.info.action_space.n,),\n    n_actions=mdp.info.action_space.n,\n    n_features=Network.n_features,\n    optimizer=optimizer,\n    loss=F.smooth_l1_loss\n)\n\napproximator = TorchApproximator\n\n# Agent\nalgorithm_params = dict(\n    batch_size=32,\n    target_update_frequency=target_update_frequency // train_frequency,\n    replay_memory=None,\n    initial_replay_size=initial_replay_size,\n    max_replay_size=max_replay_size\n)\n\nagent = DQN(mdp.info, pi, approximator,\n            approximator_params=approximator_params,\n            **algorithm_params)\n\n# Algorithm\ncore = Core(agent, mdp)\n\n# RUN\n\n# Fill replay memory with random dataset\nprint_epoch(0)\ncore.learn(n_steps=initial_replay_size,\n           n_steps_per_fit=initial_replay_size)\n\n# Evaluate initial policy\npi.set_epsilon(epsilon_test)\nmdp.set_episode_end(False)\ndataset = core.evaluate(n_steps=test_samples)\nscores.append(get_stats(dataset))\n\nfor n_epoch in range(1, max_steps // evaluation_frequency + 1):\n    print_epoch(n_epoch)\n    print('- Learning:')\n    # learning step\n    pi.set_epsilon(epsilon)\n    mdp.set_episode_end(True)\n    core.learn(n_steps=evaluation_frequency,\n               n_steps_per_fit=train_frequency)\n\n    print('- Evaluation:')\n    # evaluation step\n    pi.set_epsilon(epsilon_test)\n    mdp.set_episode_end(False)\n    dataset = core.evaluate(n_steps=test_samples)\n    scores.append(get_stats(dataset))\n"""
docs/source/tutorials/code/generic_regressor.py,0,"b""import numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.approximators.parametric import LinearApproximator\n\n\nx = np.arange(10).reshape(-1, 1)\n\nintercept = 10\nnoise = np.random.randn(10, 1) * 1\ny = 2 * x + intercept + noise\n\nphi = np.concatenate((np.ones(10).reshape(-1, 1), x), axis=1)\n\nregressor = Regressor(LinearApproximator,\n                      input_shape=(2,),\n                      output_shape=(1,))\n\nregressor.fit(phi, y)\n\nprint('Weights: ' + str(regressor.get_weights()))\nprint('Gradient: ' + str(regressor.diff(np.array([[5.]]))))\n\nplt.scatter(x, y)\nplt.plot(x, regressor.predict(phi))\nplt.show()\n"""
docs/source/tutorials/code/simple_experiment.py,0,"b'import numpy as np\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nfrom mushroom_rl.algorithms.value import FQI\nfrom mushroom_rl.core import Core\nfrom mushroom_rl.environments import CarOnHill\nfrom mushroom_rl.policy import EpsGreedy\nfrom mushroom_rl.utils.dataset import compute_J\nfrom mushroom_rl.utils.parameters import Parameter\n\nmdp = CarOnHill()\n\n# Policy\nepsilon = Parameter(value=1.)\npi = EpsGreedy(epsilon=epsilon)\n\n# Approximator\napproximator_params = dict(input_shape=mdp.info.observation_space.shape,\n                           n_actions=mdp.info.action_space.n,\n                           n_estimators=50,\n                           min_samples_split=5,\n                           min_samples_leaf=2)\napproximator = ExtraTreesRegressor\n\n# Agent\nagent = FQI(mdp.info, pi, approximator, n_iterations=20,\n            approximator_params=approximator_params)\n\ncore = Core(agent, mdp)\n\ncore.learn(n_episodes=1000, n_episodes_per_fit=1000)\n\npi.set_epsilon(Parameter(0.))\ninitial_state = np.array([[-.5, 0.]])\ndataset = core.evaluate(initial_states=initial_state)\n\nprint(compute_J(dataset, gamma=mdp.info.gamma))\n'"
mushroom_rl/algorithms/actor_critic/classic_actor_critic/__init__.py,0,"b""from .copdac_q import COPDAC_Q\nfrom .stochastic_ac import StochasticAC, StochasticAC_AVG\n\n__all__ = ['COPDAC_Q', 'StochasticAC', 'StochasticAC_AVG']"""
mushroom_rl/algorithms/actor_critic/classic_actor_critic/copdac_q.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.agent import Agent\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.approximators.parametric import LinearApproximator\n\n\nclass COPDAC_Q(Agent):\n    """"""\n    Compatible off-policy deterministic actor-critic algorithm.\n    ""Deterministic Policy Gradient Algorithms"".\n    Silver D. et al.. 2014.\n\n    """"""\n\n    def __init__(self, mdp_info, policy, mu, alpha_theta, alpha_omega, alpha_v,\n                 value_function_features=None, policy_features=None):\n        """"""\n        Constructor.\n\n        Args:\n            mu (Regressor): regressor that describe the deterministic policy to be\n                learned i.e., the deterministic mapping between state and action.\n            alpha_theta (Parameter): learning rate for policy update;\n            alpha_omega (Parameter): learning rate for the advantage function;\n            alpha_v (Parameter): learning rate for the value function;\n            value_function_features (Features, None): features used by the value\n                function approximator;\n            policy_features (Features, None): features used by the policy.\n\n        """"""\n        self._mu = mu\n        self._psi = value_function_features\n\n        self._alpha_theta = alpha_theta\n        self._alpha_omega = alpha_omega\n        self._alpha_v = alpha_v\n\n        if self._psi is not None:\n            input_shape = (self._psi.size,)\n        else:\n            input_shape = mdp_info.observation_space.shape\n\n        self._V = Regressor(LinearApproximator, input_shape=input_shape,\n                            output_shape=(1,))\n\n        self._A = Regressor(LinearApproximator,\n                            input_shape=(self._mu.weights_size,),\n                            output_shape=(1,))\n\n        self._add_save_attr(\n            _mu=\'pickle\',\n            _psi=\'pickle\',\n            _alpha_theta=\'pickle\',\n            _alpha_omega=\'pickle\',\n            _alpha_v=\'pickle\',\n            _V=\'pickle\',\n            _A=\'pickle\'\n        )\n\n        super().__init__(mdp_info, policy, policy_features)\n\n    def fit(self, dataset):\n        for step in dataset:\n            s, a, r, ss, absorbing, _ = step\n\n            s_phi = self.phi(s) if self.phi is not None else s\n            s_psi = self._psi(s) if self._psi is not None else s\n            ss_psi = self._psi(ss) if self._psi is not None else ss\n\n            q_next = self._V(ss_psi).item() if not absorbing else 0\n\n            grad_mu_s = np.atleast_2d(self._mu.diff(s_phi))\n            omega = self._A.get_weights()\n\n            delta = r + self.mdp_info.gamma * q_next - self._Q(s, a)\n            delta_theta = self._alpha_theta(s, a) * \\\n                          omega.dot(grad_mu_s.T).dot(grad_mu_s)\n            delta_omega = self._alpha_omega(s, a) * delta * self._nu(s, a)\n            delta_v = self._alpha_v(s, a) * delta * s_psi\n\n            theta_new = self._mu.get_weights() + delta_theta\n            self._mu.set_weights(theta_new)\n\n            omega_new = omega + delta_omega\n            self._A.set_weights(omega_new)\n\n            v_new = self._V.get_weights() + delta_v\n            self._V.set_weights(v_new)\n\n    def _Q(self, state, action):\n        state_psi = self._psi(state) if self._psi is not None else state\n\n        return self._V(state_psi).item() + self._A(self._nu(state,\n                                                            action)).item()\n\n    def _nu(self, state, action):\n        state_phi = self.phi(state) if self.phi is not None else state\n        grad_mu = np.atleast_2d(self._mu.diff(state_phi))\n        delta = action - self._mu(state_phi)\n\n        return delta.dot(grad_mu)\n'"
mushroom_rl/algorithms/actor_critic/classic_actor_critic/stochastic_ac.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.agent import Agent\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.approximators.parametric import LinearApproximator\n\n\nclass StochasticAC(Agent):\n    """"""\n    Stochastic Actor critic in the episodic setting as presented in:\n    ""Model-Free Reinforcement Learning with Continuous Action in Practice"".\n    Degris T. et al.. 2012.\n\n    """"""\n    def __init__(self, mdp_info, policy, alpha_theta, alpha_v, lambda_par=.9,\n                 value_function_features=None, policy_features=None):\n        """"""\n        Constructor.\n\n        Args:\n            alpha_theta (Parameter): learning rate for policy update;\n            alpha_v (Parameter): learning rate for the value function;\n            lambda_par (float, .9): trace decay parameter;\n            value_function_features (Features, None): features used by the\n                value function approximator;\n            policy_features (Features, None): features used by the policy.\n\n        """"""\n        self._psi = value_function_features\n\n        self._alpha_theta = alpha_theta\n        self._alpha_v = alpha_v\n\n        self._lambda = lambda_par\n\n        super().__init__(mdp_info, policy, policy_features)\n\n        if self._psi is not None:\n            input_shape = (self._psi.size,)\n        else:\n            input_shape = mdp_info.observation_space.shape\n\n        self._V = Regressor(LinearApproximator, input_shape=input_shape,\n                            output_shape=(1,))\n\n        self._e_v = np.zeros(self._V.weights_size)\n        self._e_theta = np.zeros(self.policy.weights_size)\n\n        self._add_save_attr(\n            _psi=\'pickle\',\n            _alpha_theta=\'pickle\',\n            _alpha_v=\'pickle\',\n            _lambda=\'numpy\',\n            _V=\'pickle\',\n            _e_v=\'numpy\',\n            _e_theta=\'numpy\'\n        )\n\n    def episode_start(self):\n        self._e_v = np.zeros(self._V.weights_size)\n        self._e_theta = np.zeros(self.policy.weights_size)\n\n        super().episode_start()\n\n    def fit(self, dataset):\n        for step in dataset:\n            s, a, r, ss, absorbing, _ = step\n\n            s_phi = self.phi(s) if self.phi is not None else s\n            s_psi = self._psi(s) if self._psi is not None else s\n            ss_psi = self._psi(ss) if self._psi is not None else ss\n\n            v_next = self._V(ss_psi) if not absorbing else 0\n\n            delta = self._compute_td_n_traces(a, r, v_next, s_psi, s_phi)\n\n            # Update value function\n            delta_v = self._alpha_v(s, a) * delta * self._e_v\n            v_new = self._V.get_weights() + delta_v\n            self._V.set_weights(v_new)\n\n            # Update policy\n            delta_theta = self._alpha_theta(s, a) * delta * self._e_theta\n            theta_new = self.policy.get_weights() + delta_theta\n            self.policy.set_weights(theta_new)\n\n    def _compute_td_n_traces(self, a, r, v_next, s_psi, s_phi):\n        # Compute TD error\n        delta = r + self.mdp_info.gamma * v_next - self._V(s_psi)\n\n        # Update traces\n        self._e_v = self.mdp_info.gamma * self._lambda * self._e_v + s_psi\n        self._e_theta = self.mdp_info.gamma * self._lambda * \\\n            self._e_theta + self.policy.diff_log(s_phi, a)\n\n        return delta\n\n\nclass StochasticAC_AVG(StochasticAC):\n    """"""\n    Stochastic Actor critic in the average reward setting as presented in:\n    ""Model-Free Reinforcement Learning with Continuous Action in Practice"".\n    Degris T. et al.. 2012.\n\n    """"""\n    def __init__(self, mdp_info, policy, alpha_theta, alpha_v, alpha_r,\n                 lambda_par=.9, value_function_features=None,\n                 policy_features=None):\n        """"""\n        Constructor.\n\n        Args:\n            alpha_r (Parameter): learning rate for the reward trace.\n\n        """"""\n        super().__init__(mdp_info, policy, alpha_theta, alpha_v, lambda_par,\n                         value_function_features, policy_features)\n\n        self._alpha_r = alpha_r\n        self._r_bar = 0\n\n        self._add_save_attr(_alpha_r=\'pickle\', _r_bar=\'numpy\')\n\n    def _compute_td_n_traces(self, a, r, v_next, s_psi, s_phi):\n        # Compute TD error\n        delta = r - self._r_bar + v_next - self._V(s_psi)\n\n        # Update traces\n        self._r_bar += self._alpha_r() * delta\n        self._e_v = self._lambda * self._e_v + s_psi\n        self._e_theta = self._lambda * self._e_theta + \\\n                        self.policy.diff_log(s_phi, a)\n\n        return delta\n'"
mushroom_rl/algorithms/actor_critic/deep_actor_critic/__init__.py,0,"b""from .deep_actor_critic import DeepAC\nfrom .a2c import A2C\nfrom .ddpg import DDPG\nfrom .td3 import TD3\nfrom .sac import SAC\nfrom .trpo import TRPO\nfrom .ppo import PPO\n\n__all__ = ['DeepAC', 'A2C', 'DDPG', 'TD3', 'SAC', 'TRPO', 'PPO']"""
mushroom_rl/algorithms/actor_critic/deep_actor_critic/a2c.py,2,"b'import torch\n\nfrom mushroom_rl.algorithms.actor_critic.deep_actor_critic import DeepAC\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.approximators.parametric import TorchApproximator\nfrom mushroom_rl.utils.value_functions import compute_advantage_montecarlo\nfrom mushroom_rl.utils.dataset import parse_dataset\nfrom mushroom_rl.utils.torch import to_float_tensor\n\nfrom copy import deepcopy\n\n\nclass A2C(DeepAC):\n    """"""\n    Advantage Actor Critic algorithm (A2C).\n    Synchronous version of the A3C algorithm.\n    ""Asynchronous Methods for Deep Reinforcement Learning"".\n    Mnih V. et. al.. 2016.\n\n    """"""\n\n    def __init__(self, mdp_info, policy, actor_optimizer, critic_params,\n                 ent_coeff, max_grad_norm=None, critic_fit_params=None):\n        """"""\n        Constructor.\n\n        Args:\n            policy (TorchPolicy): torch policy to be learned by the algorithm;\n            actor_optimizer (dict): parameters to specify the actor optimizer\n                algorithm;\n            critic_params (dict): parameters of the critic approximator to\n                build;\n            ent_coeff (float, 0): coefficient for the entropy penalty;\n            max_grad_norm (float, None): maximum norm for gradient clipping.\n                If None, no clipping will be performed, unless specified\n                otherwise in actor_optimizer;\n            critic_fit_params (dict, None): parameters of the fitting algorithm\n                of the critic approximator.\n\n        """"""\n        self._critic_fit_params = dict() if critic_fit_params is None else critic_fit_params\n\n        self._entropy_coeff = ent_coeff\n\n        self._V = Regressor(TorchApproximator, **critic_params)\n\n        if \'clipping\' not in actor_optimizer and max_grad_norm is not None:\n            actor_optimizer = deepcopy(actor_optimizer)\n            clipping_params = dict(max_norm=max_grad_norm, norm_type=2)\n            actor_optimizer[\'clipping\'] = dict(\n                method=torch.nn.utils.clip_grad_norm_, params=clipping_params)\n\n        self._add_save_attr(\n            _critic_fit_params=\'pickle\',\n            _entropy_coeff=\'numpy\',\n            _V=\'pickle\'\n        )\n\n        super().__init__(mdp_info, policy, actor_optimizer, policy.parameters())\n\n    def fit(self, dataset):\n            state, action, reward, next_state, absorbing, _ = parse_dataset(dataset)\n\n            v, adv = compute_advantage_montecarlo(self._V, state, next_state,\n                                                  reward, absorbing,\n                                                  self.mdp_info.gamma)\n            self._V.fit(state, v, **self._critic_fit_params)\n\n            loss = self._loss(state, action, adv)\n            self._optimize_actor_parameters(loss)\n\n    def _loss(self, state, action, adv):\n        use_cuda = self.policy.use_cuda\n\n        s = to_float_tensor(state, use_cuda)\n        a = to_float_tensor(action, use_cuda)\n\n        adv_t = to_float_tensor(adv, use_cuda)\n\n        gradient_loss = -torch.mean(self.policy.log_prob_t(s, a)*adv_t)\n        entropy_loss = -self.policy.entropy_t(s)\n\n        return gradient_loss + self._entropy_coeff * entropy_loss\n\n    def _post_load(self):\n        if self._optimizer is not None:\n            self._parameters = list(self.policy.parameters())\n'"
mushroom_rl/algorithms/actor_critic/deep_actor_critic/ddpg.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.actor_critic.deep_actor_critic import DeepAC\nfrom mushroom_rl.policy import Policy\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.approximators.parametric import TorchApproximator\nfrom mushroom_rl.utils.replay_memory import ReplayMemory\n\nfrom copy import deepcopy\n\n\nclass DDPG(DeepAC):\n    """"""\n    Deep Deterministic Policy Gradient algorithm.\n    ""Continuous Control with Deep Reinforcement Learning"".\n    Lillicrap T. P. et al.. 2016.\n\n    """"""\n    def __init__(self, mdp_info, policy_class, policy_params,\n                 actor_params, actor_optimizer, critic_params, batch_size,\n                 initial_replay_size, max_replay_size, tau, policy_delay=1,\n                 critic_fit_params=None):\n        """"""\n        Constructor.\n\n        Args:\n            policy_class (Policy): class of the policy;\n            policy_params (dict): parameters of the policy to build;\n            actor_params (dict): parameters of the actor approximator to\n                build;\n            actor_optimizer (dict): parameters to specify the actor optimizer\n                algorithm;\n            critic_params (dict): parameters of the critic approximator to\n                build;\n            batch_size (int): the number of samples in a batch;\n            initial_replay_size (int): the number of samples to collect before\n                starting the learning;\n            max_replay_size (int): the maximum number of samples in the replay\n                memory;\n            tau (float): value of coefficient for soft updates;\n            policy_delay (int, 1): the number of updates of the critic after\n                which an actor update is implemented;\n            critic_fit_params (dict, None): parameters of the fitting algorithm\n                of the critic approximator;\n\n        """"""\n        self._critic_fit_params = dict() if critic_fit_params is None else critic_fit_params\n\n        self._batch_size = batch_size\n        self._tau = tau\n        self._policy_delay = policy_delay\n        self._fit_count = 0\n\n        self._replay_memory = ReplayMemory(initial_replay_size, max_replay_size)\n\n        target_critic_params = deepcopy(critic_params)\n        self._critic_approximator = Regressor(TorchApproximator,\n                                              **critic_params)\n        self._target_critic_approximator = Regressor(TorchApproximator,\n                                                     **target_critic_params)\n\n        target_actor_params = deepcopy(actor_params)\n        self._actor_approximator = Regressor(TorchApproximator,\n                                             **actor_params)\n        self._target_actor_approximator = Regressor(TorchApproximator,\n                                                    **target_actor_params)\n\n        self._init_target(self._critic_approximator,\n                          self._target_critic_approximator)\n        self._init_target(self._actor_approximator,\n                          self._target_actor_approximator)\n\n        policy = policy_class(self._actor_approximator, **policy_params)\n\n        policy_parameters = self._actor_approximator.model.network.parameters()\n\n        self._add_save_attr(\n            _critic_fit_params=\'pickle\', \n            _batch_size=\'numpy\',\n            _tau=\'numpy\',\n            _policy_delay=\'numpy\',\n            _fit_count=\'numpy\',\n            _replay_memory=\'pickle\',\n            _critic_approximator=\'pickle\',\n            _target_critic_approximator=\'pickle\',\n            _actor_approximator=\'pickle\',\n            _target_actor_approximator=\'pickle\'\n        )\n\n        super().__init__(mdp_info, policy, actor_optimizer, policy_parameters)\n\n    def fit(self, dataset):\n        self._replay_memory.add(dataset)\n        if self._replay_memory.initialized:\n            state, action, reward, next_state, absorbing, _ =\\\n                self._replay_memory.get(self._batch_size)\n\n            q_next = self._next_q(next_state, absorbing)\n            q = reward + self.mdp_info.gamma * q_next\n\n            self._critic_approximator.fit(state, action, q,\n                                          **self._critic_fit_params)\n\n            if self._fit_count % self._policy_delay == 0:\n                loss = self._loss(state)\n                self._optimize_actor_parameters(loss)\n\n            self._update_target(self._critic_approximator,\n                                self._target_critic_approximator)\n            self._update_target(self._actor_approximator,\n                                self._target_actor_approximator)\n\n            self._fit_count += 1\n\n    def _loss(self, state):\n        action = self._actor_approximator(state, output_tensor=True)\n        q = self._critic_approximator(state, action, output_tensor=True)\n\n        return -q.mean()\n\n    def _next_q(self, next_state, absorbing):\n        """"""\n        Args:\n            next_state (np.ndarray): the states where next action has to be\n                evaluated;\n            absorbing (np.ndarray): the absorbing flag for the states in\n                ``next_state``.\n\n        Returns:\n            Action-values returned by the critic for ``next_state`` and the\n            action returned by the actor.\n\n        """"""\n        a = self._target_actor_approximator(next_state)\n\n        q = self._target_critic_approximator.predict(next_state, a)\n        q *= 1 - absorbing\n\n        return q\n\n    def _post_load(self):\n        if self._optimizer is not None:\n            self._parameters = list(self._actor_approximator.model.network.parameters())\n'"
mushroom_rl/algorithms/actor_critic/deep_actor_critic/deep_actor_critic.py,1,"b'from mushroom_rl.algorithms import Agent\n\n\nclass DeepAC(Agent):\n    """"""\n    Base class for algorithms that uses the reparametrization trick, such as\n    SAC, DDPG and TD3.\n\n    """"""\n\n    def __init__(self, mdp_info, policy, actor_optimizer, parameters):\n        """"""\n        Constructor.\n\n        Args:\n            actor_optimizer (dict): parameters to specify the actor optimizer\n                algorithm;\n            parameters: policy parameters to be optimized.\n        """"""\n        if actor_optimizer is not None:\n            self._optimizer = actor_optimizer[\'class\'](\n                parameters, **actor_optimizer[\'params\']\n            )\n\n            self._clipping = None\n            if parameters is not None and not isinstance(parameters, list):\n                parameters = list(parameters)\n            self._parameters = parameters\n\n            if \'clipping\' in actor_optimizer:\n                self._clipping = actor_optimizer[\'clipping\'][\'method\']\n                self._clipping_params = actor_optimizer[\'clipping\'][\'params\']\n        \n        self._add_save_attr(\n            _optimizer=\'pickle\',\n            _clipping=\'pickle\',\n            _clipping_params=\'pickle\'\n        )\n\n        super().__init__(mdp_info, policy)\n\n    def fit(self, dataset):\n        """"""\n        Fit step.\n\n        Args:\n            dataset (list): the dataset.\n\n        """"""\n        raise NotImplementedError(\'DeepAC is an abstract class\')\n\n    def _optimize_actor_parameters(self, loss):\n        """"""\n        Method used to update actor parameters to maximize a given loss.\n\n        Args:\n            loss (torch.tensor): the loss computed by the algorithm.\n\n        """"""\n        self._optimizer.zero_grad()\n        loss.backward()\n        self._clip_gradient()\n        self._optimizer.step()\n\n    def _clip_gradient(self):\n        if self._clipping:\n            self._clipping(self._parameters, **self._clipping_params)\n\n    @staticmethod\n    def _init_target(online, target):\n        for i in range(len(target)):\n            target[i].set_weights(online[i].get_weights())\n\n    def _update_target(self, online, target):\n        for i in range(len(target)):\n            weights = self._tau * online[i].get_weights()\n            weights += (1 - self._tau) * target[i].get_weights()\n            target[i].set_weights(weights)\n\n    def _post_load(self):\n        raise NotImplementedError(\'DeepAC is an abstract class. Subclasses need\'\n                                  \'to implement the `_post_load` method.\')\n'"
mushroom_rl/algorithms/actor_critic/deep_actor_critic/ppo.py,7,"b'import numpy as np\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn.functional as F\n\nfrom mushroom_rl.algorithms.agent import Agent\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.approximators.parametric import TorchApproximator\nfrom mushroom_rl.utils.torch import to_float_tensor\nfrom mushroom_rl.utils.minibatches import minibatch_generator\nfrom mushroom_rl.utils.dataset import parse_dataset, compute_J\nfrom mushroom_rl.utils.value_functions import compute_gae\n\n\nclass PPO(Agent):\n    """"""\n    Proximal Policy Optimization algorithm.\n    ""Proximal Policy Optimization Algorithms"".\n    Schulman J. et al.. 2017.\n\n    """"""\n    def __init__(self, mdp_info, policy, actor_optimizer, critic_params,\n                 n_epochs_policy, batch_size, eps_ppo, lam, quiet=True,\n                 critic_fit_params=None):\n        """"""\n        Constructor.\n\n        Args:\n            policy (TorchPolicy): torch policy to be learned by the algorithm\n            actor_optimizer (dict): parameters to specify the actor optimizer\n                algorithm;\n            critic_params (dict): parameters of the critic approximator to\n                build;\n            n_epochs_policy (int): number of policy updates for every dataset;\n            batch_size (int): size of minibatches for every optimization step\n            eps_ppo (float): value for probability ratio clipping;\n            lam float(float, 1.): lambda coefficient used by generalized\n                advantage estimation;\n            quiet (bool, True): if true, the algorithm will print debug\n                information;\n            critic_fit_params (dict, None): parameters of the fitting algorithm\n                of the critic approximator.\n\n        """"""\n        self._critic_fit_params = dict(n_epochs=10) if critic_fit_params is None else critic_fit_params\n\n        self._n_epochs_policy = n_epochs_policy\n        self._batch_size = batch_size\n        self._eps_ppo = eps_ppo\n\n        self._optimizer = actor_optimizer[\'class\'](policy.parameters(), **actor_optimizer[\'params\'])\n\n        self._lambda = lam\n\n        self._V = Regressor(TorchApproximator, **critic_params)\n\n        self._quiet = quiet\n        self._iter = 1\n\n        self._add_save_attr(\n            _critic_fit_params=\'pickle\', \n            _n_epochs_policy=\'numpy\',\n            _batch_size=\'numpy\',\n            _eps_ppo=\'numpy\',\n            _optimizer=\'pickle\',\n            _lambda=\'numpy\',\n            _V=\'pickle\',\n            _quiet=\'numpy\',\n            _iter=\'numpy\'\n        )\n\n        super().__init__(mdp_info, policy, None)\n\n    def fit(self, dataset):\n        if not self._quiet:\n            tqdm.write(\'Iteration \' + str(self._iter))\n\n        x, u, r, xn, absorbing, last = parse_dataset(dataset)\n        x = x.astype(np.float32)\n        u = u.astype(np.float32)\n        r = r.astype(np.float32)\n        xn = xn.astype(np.float32)\n\n        obs = to_float_tensor(x, self.policy.use_cuda)\n        act = to_float_tensor(u, self.policy.use_cuda)\n        v_target, np_adv = compute_gae(self._V, x, xn, r, absorbing, last, self.mdp_info.gamma, self._lambda)\n        np_adv = (np_adv - np.mean(np_adv)) / (np.std(np_adv) + 1e-8)\n        adv = to_float_tensor(np_adv, self.policy.use_cuda)\n\n        old_pol_dist = self.policy.distribution_t(obs)\n        old_log_p = old_pol_dist.log_prob(act)[:, None].detach()\n\n        self._V.fit(x, v_target, **self._critic_fit_params)\n\n        self._update_policy(obs, act, adv, old_log_p)\n\n        # Print fit information\n        self._print_fit_info(dataset, x, v_target, old_pol_dist)\n        self._iter += 1\n\n    def _update_policy(self, obs, act, adv, old_log_p):\n        for epoch in range(self._n_epochs_policy):\n            for obs_i, act_i, adv_i, old_log_p_i in minibatch_generator(\n                    self._batch_size, obs, act, adv, old_log_p):\n                self._optimizer.zero_grad()\n                prob_ratio = torch.exp(\n                    self.policy.log_prob_t(obs_i, act_i) - old_log_p_i\n                )\n                clipped_ratio = torch.clamp(prob_ratio, 1 - self._eps_ppo,\n                                            1 + self._eps_ppo)\n                loss = -torch.mean(torch.min(prob_ratio * adv_i,\n                                             clipped_ratio * adv_i))\n                loss.backward()\n                self._optimizer.step()\n\n    def _print_fit_info(self, dataset, x, v_target, old_pol_dist):\n        if not self._quiet:\n            logging_verr = []\n            torch_v_targets = torch.tensor(v_target, dtype=torch.float)\n            for idx in range(len(self._V)):\n                v_pred = torch.tensor(self._V(x, idx=idx), dtype=torch.float)\n                v_err = F.mse_loss(v_pred, torch_v_targets)\n                logging_verr.append(v_err.item())\n\n            logging_ent = self.policy.entropy(x)\n            new_pol_dist = self.policy.distribution(x)\n            logging_kl = torch.mean(torch.distributions.kl.kl_divergence(\n                new_pol_dist, old_pol_dist))\n            avg_rwd = np.mean(compute_J(dataset))\n            tqdm.write(""Iterations Results:\\n\\trewards {} vf_loss {}\\n\\tentropy {}  kl {}"".format(\n                avg_rwd, logging_verr, logging_ent, logging_kl))\n            tqdm.write(\n                \'--------------------------------------------------------------------------------------------------\')\n'"
mushroom_rl/algorithms/actor_critic/deep_actor_critic/sac.py,6,"b'import numpy as np\n\nimport torch\nimport torch.optim as optim\n\nfrom mushroom_rl.algorithms.actor_critic.deep_actor_critic import DeepAC\nfrom mushroom_rl.policy import Policy\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.approximators.parametric import TorchApproximator\nfrom mushroom_rl.utils.replay_memory import ReplayMemory\nfrom mushroom_rl.utils.torch import to_float_tensor\n\nfrom copy import deepcopy\nfrom itertools import chain\n\n\nclass SACPolicy(Policy):\n    """"""\n    Class used to implement the policy used by the Soft Actor-Critic\n    algorithm. The policy is a Gaussian policy squashed by a tanh.\n    This class implements the compute_action_and_log_prob and the\n    compute_action_and_log_prob_t methods, that are fundamental for\n    the internals calculations of the SAC algorithm.\n\n    """"""\n    def __init__(self, mu_approximator, sigma_approximator, min_a, max_a):\n        """"""\n        Constructor.\n\n        Args:\n            mu_approximator (Regressor): a regressor computing mean in given a\n                state;\n            sigma_approximator (Regressor): a regressor computing the variance\n                in given a state;\n            min_a (np.ndarray): a vector specifying the minimum action value\n                for each component;\n            max_a (np.ndarray): a vector specifying the maximum action value\n                for each component.\n\n        """"""\n        self._mu_approximator = mu_approximator\n        self._sigma_approximator = sigma_approximator\n\n        self._delta_a = to_float_tensor(.5 * (max_a - min_a), self.use_cuda)\n        self._central_a = to_float_tensor(.5 * (max_a + min_a), self.use_cuda)\n\n        use_cuda = self._mu_approximator.model.use_cuda\n\n        if use_cuda:\n            self._delta_a = self._delta_a.cuda()\n            self._central_a = self._central_a.cuda()\n\n    def __call__(self, state, action):\n        raise NotImplementedError\n\n    def draw_action(self, state):\n        return self.compute_action_and_log_prob_t(\n            state, compute_log_prob=False).detach().cpu().numpy()\n\n    def compute_action_and_log_prob(self, state):\n        """"""\n        Function that samples actions using the reparametrization trick and\n        the log probability for such actions.\n\n        Args:\n            state (np.ndarray): the state in which the action is sampled.\n\n        Returns:\n            The actions sampled and the log probability as numpy arrays.\n\n        """"""\n        a, log_prob = self.compute_action_and_log_prob_t(state)\n        return a.detach().cpu().numpy(), log_prob.detach().cpu().numpy()\n\n    def compute_action_and_log_prob_t(self, state, compute_log_prob=True):\n        """"""\n        Function that samples actions using the reparametrization trick and,\n        optionally, the log probability for such actions.\n\n        Args:\n            state (np.ndarray): the state in which the action is sampled;\n            compute_log_prob (bool, True): whether to compute the log\n            probability or not.\n\n        Returns:\n            The actions sampled and, optionally, the log probability as torch\n            tensors.\n\n        """"""\n        dist = self.distribution(state)\n        a_raw = dist.rsample()\n        a = torch.tanh(a_raw)\n        a_true = a * self._delta_a + self._central_a\n\n        if compute_log_prob:\n            log_prob = dist.log_prob(a_raw).sum(dim=1)\n            log_prob -= torch.log(1. - a.pow(2) + 1e-6).sum(dim=1)\n            return a_true, log_prob\n        else:\n            return a_true\n\n    def distribution(self, state):\n        """"""\n        Compute the policy distribution in the given states.\n\n        Args:\n            state (np.ndarray): the set of states where the distribution is\n                computed.\n\n        Returns:\n            The torch distribution for the provided states.\n\n        """"""\n        mu = self._mu_approximator.predict(state, output_tensor=True)\n        log_sigma = self._sigma_approximator.predict(state, output_tensor=True)\n        return torch.distributions.Normal(mu, log_sigma.exp())\n\n    def reset(self):\n        pass\n\n    def set_weights(self, weights):\n        """"""\n        Setter.\n\n        Args:\n            weights (np.ndarray): the vector of the new weights to be used by\n                the policy.\n\n        """"""\n        mu_weights = weights[:self._mu_approximator.weights_size]\n        sigma_weights = weights[self._mu_approximator.weights_size:]\n\n        self._mu_approximator.set_weights(mu_weights)\n        self._sigma_approximator.set_weights(sigma_weights)\n\n    def get_weights(self):\n        """"""\n        Getter.\n\n        Returns:\n             The current policy weights.\n\n        """"""\n        mu_weights = self._mu_approximator.get_weights()\n        sigma_weights = self._sigma_approximator.get_weights()\n\n        return np.concatenate([mu_weights, sigma_weights])\n\n    @property\n    def use_cuda(self):\n        """"""\n        True if the policy is using cuda_tensors.\n        """"""\n        return self._mu_approximator.model.use_cuda\n\n\nclass SAC(DeepAC):\n    """"""\n    Soft Actor-Critic algorithm.\n    ""Soft Actor-Critic Algorithms and Applications"".\n    Haarnoja T. et al.. 2019.\n\n    """"""\n    def __init__(self, mdp_info, actor_mu_params, actor_sigma_params,\n                 actor_optimizer, critic_params, batch_size,\n                 initial_replay_size, max_replay_size, warmup_transitions, tau,\n                 lr_alpha, target_entropy=None, critic_fit_params=None):\n        """"""\n        Constructor.\n\n        Args:\n            actor_mu_params (dict): parameters of the actor mean approximator\n                to build;\n            actor_sigma_params (dict): parameters of the actor sigm\n                approximator to build;\n            actor_optimizer (dict): parameters to specify the actor\n                optimizer algorithm;\n            critic_params (dict): parameters of the critic approximator to\n                build;\n            batch_size (int): the number of samples in a batch;\n            initial_replay_size (int): the number of samples to collect before\n                starting the learning;\n            max_replay_size (int): the maximum number of samples in the replay\n                memory;\n            warmup_transitions (int): number of samples to accumulate in the\n                replay memory to start the policy fitting;\n            tau (float): value of coefficient for soft updates;\n            lr_alpha (float): Learning rate for the entropy coefficient;\n            target_entropy (float, None): target entropy for the policy, if\n                None a default value is computed ;\n            critic_fit_params (dict, None): parameters of the fitting algorithm\n                of the critic approximator.\n\n        """"""\n        self._critic_fit_params = dict() if critic_fit_params is None else critic_fit_params\n\n        self._batch_size = batch_size\n        self._warmup_transitions = warmup_transitions\n        self._tau = tau\n\n        if target_entropy is None:\n            self._target_entropy = -np.prod(mdp_info.action_space.shape).astype(np.float32)\n        else:\n            self._target_entropy = target_entropy\n\n        self._replay_memory = ReplayMemory(initial_replay_size, max_replay_size)\n\n        if \'n_models\' in critic_params.keys():\n            assert critic_params[\'n_models\'] == 2\n        else:\n            critic_params[\'n_models\'] = 2\n\n        target_critic_params = deepcopy(critic_params)\n        self._critic_approximator = Regressor(TorchApproximator,\n                                              **critic_params)\n        self._target_critic_approximator = Regressor(TorchApproximator,\n                                                     **target_critic_params)\n\n        actor_mu_approximator = Regressor(TorchApproximator,\n                                          **actor_mu_params)\n        actor_sigma_approximator = Regressor(TorchApproximator,\n                                             **actor_sigma_params)\n\n        policy = SACPolicy(actor_mu_approximator,\n                           actor_sigma_approximator,\n                           mdp_info.action_space.low,\n                           mdp_info.action_space.high)\n\n        self._init_target(self._critic_approximator,\n                          self._target_critic_approximator)\n\n        self._log_alpha = torch.tensor(0., dtype=torch.float32)\n\n        if policy.use_cuda:\n            self._log_alpha = self._log_alpha.cuda().requires_grad_()\n        else:\n            self._log_alpha.requires_grad_()\n\n        self._alpha_optim = optim.Adam([self._log_alpha], lr=lr_alpha)\n\n        policy_parameters = chain(actor_mu_approximator.model.network.parameters(),\n                                  actor_sigma_approximator.model.network.parameters())\n\n        self._add_save_attr(\n            _critic_fit_params=\'pickle\',\n            _batch_size=\'numpy\',\n            _warmup_transitions=\'numpy\',\n            _tau=\'numpy\',\n            _target_entropy=\'numpy\',\n            _replay_memory=\'pickle\',\n            _critic_approximator=\'pickle\',\n            _target_critic_approximator=\'pickle\',\n            _log_alpha=\'pickle\',\n            _alpha_optim=\'pickle\'\n        )\n\n        super().__init__(mdp_info, policy, actor_optimizer, policy_parameters)\n\n    def fit(self, dataset):\n        self._replay_memory.add(dataset)\n        if self._replay_memory.initialized:\n            state, action, reward, next_state, absorbing, _ = \\\n                self._replay_memory.get(self._batch_size)\n\n            if self._replay_memory.size > self._warmup_transitions:\n                action_new, log_prob = self.policy.compute_action_and_log_prob_t(state)\n                loss = self._loss(state, action_new, log_prob)\n                self._optimize_actor_parameters(loss)\n                self._update_alpha(log_prob.detach())\n\n            q_next = self._next_q(next_state, absorbing)\n            q = reward + self.mdp_info.gamma * q_next\n\n            self._critic_approximator.fit(state, action, q,\n                                          **self._critic_fit_params)\n\n            self._update_target(self._critic_approximator,\n                                self._target_critic_approximator)\n\n    def _loss(self, state, action_new, log_prob):\n        q_0 = self._critic_approximator(state, action_new,\n                                        output_tensor=True, idx=0)\n        q_1 = self._critic_approximator(state, action_new,\n                                        output_tensor=True, idx=1)\n\n        q = torch.min(q_0, q_1)\n\n        return (self._alpha * log_prob - q).mean()\n\n    def _update_alpha(self, log_prob):\n        alpha_loss = - (self._log_alpha * (log_prob + self._target_entropy)).mean()\n        self._alpha_optim.zero_grad()\n        alpha_loss.backward()\n        self._alpha_optim.step()\n\n    def _next_q(self, next_state, absorbing):\n        """"""\n        Args:\n            next_state (np.ndarray): the states where next action has to be\n                evaluated;\n            absorbing (np.ndarray): the absorbing flag for the states in\n                ``next_state``.\n\n        Returns:\n            Action-values returned by the critic for ``next_state`` and the\n            action returned by the actor.\n\n        """"""\n        a, log_prob_next = self.policy.compute_action_and_log_prob(next_state)\n\n        q = self._target_critic_approximator.predict(\n            next_state, a, prediction=\'min\') - self._alpha_np * log_prob_next\n        q *= 1 - absorbing\n\n        return q\n\n    def _post_load(self):\n        if self._optimizer is not None:\n            self._parameters = list(\n                chain(self.policy._mu_approximator.model.network.parameters(),\n                      self.policy._sigma_approximator.model.network.parameters()\n                )\n            )\n\n    @property\n    def _alpha(self):\n        return self._log_alpha.exp()\n\n    @property\n    def _alpha_np(self):\n        return self._alpha.detach().cpu().numpy()\n'"
mushroom_rl/algorithms/actor_critic/deep_actor_critic/td3.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.actor_critic.deep_actor_critic import DDPG\nfrom mushroom_rl.policy import Policy\n\n\nclass TD3(DDPG):\n    """"""\n    Twin Delayed DDPG algorithm.\n    ""Addressing Function Approximation Error in Actor-Critic Methods"".\n    Fujimoto S. et al.. 2018.\n\n    """"""\n    def __init__(self, mdp_info, policy_class, policy_params, actor_params,\n                 actor_optimizer, critic_params, batch_size,\n                 initial_replay_size, max_replay_size, tau, policy_delay=2,\n                 noise_std=.2, noise_clip=.5, critic_fit_params=None):\n        """"""\n        Constructor.\n\n        Args:\n            policy_class (Policy): class of the policy;\n            policy_params (dict): parameters of the policy to build;\n            actor_params (dict): parameters of the actor approximator to\n                build;\n            actor_optimizer (dict): parameters to specify the actor\n                optimizer algorithm;\n            critic_params (dict): parameters of the critic approximator to\n                build;\n            batch_size (int): the number of samples in a batch;\n            initial_replay_size (int): the number of samples to collect before\n                starting the learning;\n            max_replay_size (int): the maximum number of samples in the replay\n                memory;\n            tau (float): value of coefficient for soft updates;\n            policy_delay (int, 2): the number of updates of the critic after\n                which an actor update is implemented;\n            noise_std (float, .2): standard deviation of the noise used for\n                policy smoothing;\n            noise_clip (float, .5): maximum absolute value for policy smoothing\n                noise;\n            critic_fit_params (dict, None): parameters of the fitting algorithm\n                of the critic approximator.\n\n        """"""\n        self._noise_std = noise_std\n        self._noise_clip = noise_clip\n\n        if \'n_models\' in critic_params.keys():\n            assert(critic_params[\'n_models\'] >= 2)\n        else:\n            critic_params[\'n_models\'] = 2\n\n        self._add_save_attr(\n            _noise_std=\'numpy\', \n            _noise_clip=\'numpy\'\n        )\n\n        super().__init__(mdp_info, policy_class, policy_params,  actor_params,\n                         actor_optimizer, critic_params, batch_size,\n                         initial_replay_size, max_replay_size, tau,\n                         policy_delay, critic_fit_params)\n\n    def _loss(self, state):\n        action = self._actor_approximator(state, output_tensor=True)\n        q = self._critic_approximator(state, action, idx=0, output_tensor=True)\n\n        return -q.mean()\n\n    def _next_q(self, next_state, absorbing):\n        """"""\n        Args:\n            next_state (np.ndarray): the states where next action has to be\n                evaluated;\n            absorbing (np.ndarray): the absorbing flag for the states in\n                ``next_state``.\n\n        Returns:\n            Action-values returned by the critic for ``next_state`` and the\n            action returned by the actor.\n\n        """"""\n        a = self._target_actor_approximator(next_state)\n\n        low = self.mdp_info.action_space.low\n        high = self.mdp_info.action_space.high\n        eps = np.random.normal(scale=self._noise_std, size=a.shape)\n        eps_clipped = np.clip(eps, -self._noise_clip, self._noise_clip)\n        a_smoothed = np.clip(a + eps_clipped, low, high)\n\n        q = self._target_critic_approximator.predict(next_state, a_smoothed,\n                                                     prediction=\'min\')\n        q *= 1 - absorbing\n\n        return q\n'"
mushroom_rl/algorithms/actor_critic/deep_actor_critic/trpo.py,14,"b'import numpy as np\nfrom tqdm import tqdm\n\nfrom copy import deepcopy\n\nimport torch\nimport torch.nn.functional as F\n\nfrom mushroom_rl.algorithms.agent import Agent\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.approximators.parametric import TorchApproximator\nfrom mushroom_rl.utils.torch import get_gradient, zero_grad, to_float_tensor\nfrom mushroom_rl.utils.dataset import parse_dataset, compute_J\nfrom mushroom_rl.utils.value_functions import compute_gae\n\n\nclass TRPO(Agent):\n    """"""\n    Trust Region Policy optimization algorithm.\n    ""Trust Region Policy Optimization"".\n    Schulman J. et al.. 2015.\n\n    """"""\n    def __init__(self, mdp_info, policy, critic_params,\n                 ent_coeff=0., max_kl=.001, lam=1.,\n                 n_epochs_line_search=10, n_epochs_cg=10,\n                 cg_damping=1e-2, cg_residual_tol=1e-10, quiet=True,\n                 critic_fit_params=None):\n        """"""\n        Constructor.\n\n        Args:\n            policy (TorchPolicy): torch policy to be learned by the algorithm\n            critic_params (dict): parameters of the critic approximator to\n                build;\n            ent_coeff (float, 0): coefficient for the entropy penalty;\n            max_kl (float, .001): maximum kl allowed for every policy\n                update;\n            lam float(float, 1.): lambda coefficient used by generalized\n                advantage estimation;\n            n_epochs_line_search (int, 10): maximum number of iterations\n                of the line search algorithm;\n            n_epochs_cg (int, 10): maximum number of iterations of the\n                conjugate gradient algorithm;\n            cg_damping (float, 1e-2): damping factor for the conjugate\n                gradient algorithm;\n            cg_residual_tol (float, 1e-10): conjugate gradient residual\n                tolerance;\n            quiet (bool, True): if true, the algorithm will print debug\n                information;\n            critic_fit_params (dict, None): parameters of the fitting algorithm\n                of the critic approximator.\n\n        """"""\n        self._critic_fit_params = dict(n_epochs=3) if critic_fit_params is None else critic_fit_params\n\n        self._n_epochs_line_search = n_epochs_line_search\n        self._n_epochs_cg = n_epochs_cg\n        self._cg_damping = cg_damping\n        self._cg_residual_tol = cg_residual_tol\n\n        self._max_kl = max_kl\n        self._ent_coeff = ent_coeff\n\n        self._lambda = lam\n\n        self._V = Regressor(TorchApproximator, **critic_params)\n\n        self._iter = 1\n        self._quiet = quiet\n\n        self._old_policy = None\n\n        self._add_save_attr(\n            _critic_fit_params=\'pickle\', \n            _n_epochs_line_search=\'numpy\',\n            _n_epochs_cg=\'numpy\',\n            _cg_damping=\'numpy\',\n            _cg_residual_tol=\'numpy\',\n            _max_kl=\'numpy\',\n            _ent_coeff=\'numpy\',\n            _lambda=\'numpy\',\n            _V=\'pickle\',\n            _old_policy=\'pickle\',\n            _iter=\'numpy\',\n            _quiet=\'numpy\'\n        )\n\n        super().__init__(mdp_info, policy, None)\n\n    def fit(self, dataset):\n        if not self._quiet:\n            tqdm.write(\'Iteration \' + str(self._iter))\n\n        state, action, reward, next_state, absorbing, last = parse_dataset(dataset)\n        x = state.astype(np.float32)\n        u = action.astype(np.float32)\n        r = reward.astype(np.float32)\n        xn = next_state.astype(np.float32)\n\n        obs = to_float_tensor(x, self.policy.use_cuda)\n        act = to_float_tensor(u, self.policy.use_cuda)\n        v_target, np_adv = compute_gae(self._V, x, xn, r, absorbing, last,\n                                       self.mdp_info.gamma, self._lambda)\n        np_adv = (np_adv - np.mean(np_adv)) / (np.std(np_adv) + 1e-8)\n        adv = to_float_tensor(np_adv, self.policy.use_cuda)\n\n        # Policy update\n        self._old_policy = deepcopy(self.policy)\n        old_pol_dist = self._old_policy.distribution_t(obs)\n        old_log_prob = self._old_policy.log_prob_t(obs, act).detach()\n\n        zero_grad(self.policy.parameters())\n        loss = self._compute_loss(obs, act, adv, old_log_prob)\n\n        prev_loss = loss.item()\n\n        # Compute Gradient\n        loss.backward()\n        g = get_gradient(self.policy.parameters())\n\n        # Compute direction through conjugate gradient\n        stepdir = self._conjugate_gradient(g, obs, old_pol_dist)\n\n        # Line search\n        self._line_search(obs, act, adv, old_log_prob, old_pol_dist, prev_loss, stepdir)\n\n        # VF update\n        self._V.fit(x, v_target, **self._critic_fit_params)\n\n        # Print fit information\n        self._print_fit_info(dataset, x, v_target, old_pol_dist)\n        self._iter += 1\n\n    def _fisher_vector_product(self, p, obs, old_pol_dist):\n        p_tensor = torch.from_numpy(p)\n        if self.policy.use_cuda:\n            p_tensor = p_tensor.cuda()\n\n        return self._fisher_vector_product_t(p_tensor, obs, old_pol_dist)\n\n    def _fisher_vector_product_t(self, p, obs, old_pol_dist):\n        kl = self._compute_kl(obs, old_pol_dist)\n        grads = torch.autograd.grad(kl, self.policy.parameters(), create_graph=True)\n        flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n\n        kl_v = torch.sum(flat_grad_kl * p)\n        grads_v = torch.autograd.grad(kl_v, self.policy.parameters(), create_graph=False)\n        flat_grad_grad_kl = torch.cat([grad.contiguous().view(-1) for grad in grads_v]).data\n\n        return flat_grad_grad_kl + p * self._cg_damping\n\n    def _conjugate_gradient(self, b, obs, old_pol_dist):\n        p = b.detach().cpu().numpy()\n        r = b.detach().cpu().numpy()\n        x = np.zeros_like(p)\n        r2 = r.dot(r)\n\n        for i in range(self._n_epochs_cg):\n            z = self._fisher_vector_product(p, obs, old_pol_dist).detach().cpu().numpy()\n            v = r2 / p.dot(z)\n            x += v * p\n            r -= v * z\n            r2_new = r.dot(r)\n            mu = r2_new / r2\n            p = r + mu * p\n\n            r2 = r2_new\n            if r2 < self._cg_residual_tol:\n                break\n        return x\n\n    def _line_search(self, obs, act, adv, old_log_prob, old_pol_dist, prev_loss, stepdir):\n        # Compute optimal step size\n        direction = self._fisher_vector_product(stepdir, obs, old_pol_dist).detach().cpu().numpy()\n        shs = .5 * stepdir.dot(direction)\n        lm = np.sqrt(shs / self._max_kl)\n        full_step = stepdir / lm\n        stepsize = 1.\n\n        # Save old policy parameters\n        theta_old = self.policy.get_weights()\n\n        # Perform Line search\n        violation = True\n\n        for _ in range(self._n_epochs_line_search):\n            theta_new = theta_old + full_step * stepsize\n            self.policy.set_weights(theta_new)\n\n            new_loss = self._compute_loss(obs, act, adv, old_log_prob)\n            kl = self._compute_kl(obs, old_pol_dist)\n            improve = new_loss - prev_loss\n            if kl <= self._max_kl * 1.5 or improve >= 0:\n                violation = False\n                break\n            stepsize *= .5\n\n        if violation:\n            self.policy.set_weights(theta_old)\n\n    def _compute_kl(self, obs, old_pol_dist):\n        new_pol_dist = self.policy.distribution_t(obs)\n        return torch.mean(torch.distributions.kl.kl_divergence(old_pol_dist, new_pol_dist))\n\n    def _compute_loss(self, obs, act, adv, old_log_prob):\n        ratio = torch.exp(self.policy.log_prob_t(obs, act) - old_log_prob)\n        J = torch.mean(ratio * adv)\n\n        return J + self._ent_coeff * self.policy.entropy_t(obs)\n\n    def _print_fit_info(self, dataset, x, v_target, old_pol_dist):\n        if not self._quiet:\n            logging_verr = []\n            torch_v_targets = torch.tensor(v_target, dtype=torch.float)\n            for idx in range(len(self._V)):\n                v_pred = torch.tensor(self._V(x, idx=idx), dtype=torch.float)\n                v_err = F.mse_loss(v_pred, torch_v_targets)\n                logging_verr.append(v_err.item())\n\n            logging_ent = self.policy.entropy(x)\n            new_pol_dist = self.policy.distribution(x)\n            logging_kl = torch.mean(\n                torch.distributions.kl.kl_divergence(old_pol_dist, new_pol_dist)\n            )\n            avg_rwd = np.mean(compute_J(dataset))\n            tqdm.write(""Iterations Results:\\n\\trewards {} vf_loss {}\\n\\tentropy {}  kl {}"".format(\n                avg_rwd, logging_verr, logging_ent, logging_kl))\n            tqdm.write(\n                \'--------------------------------------------------------------------------------------------------\')\n'"
mushroom_rl/algorithms/policy_search/black_box_optimization/__init__.py,0,"b""from .black_box_optimization import BlackBoxOptimization\nfrom .rwr import RWR\nfrom .reps import REPS\nfrom .pgpe import PGPE\n\n\n__all__ = ['RWR', 'PGPE', 'REPS']"""
mushroom_rl/algorithms/policy_search/black_box_optimization/black_box_optimization.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.agent import Agent\nfrom mushroom_rl.utils.dataset import compute_J\n\n\nclass BlackBoxOptimization(Agent):\n    """"""\n    Base class for black box optimization algorithms.\n    These algorithms work on a distribution of policy parameters and often they\n    do not rely on stochastic and differentiable policies.\n\n    """"""\n    def __init__(self, mdp_info, distribution, policy, features=None):\n        """"""\n        Constructor.\n\n        Args:\n            distribution (Distribution): the distribution of policy parameters;\n            policy (ParametricPolicy): the policy to use.\n\n        """"""\n        self.distribution = distribution\n        self._theta_list = list()\n\n        self._add_save_attr(distribution=\'pickle\', _theta_list=\'pickle\')\n\n        super().__init__(mdp_info, policy, features)\n\n    def episode_start(self):\n        theta = self.distribution.sample()\n        self._theta_list.append(theta)\n        self.policy.set_weights(theta)\n\n        super().episode_start()\n\n    def fit(self, dataset):\n        Jep = compute_J(dataset, self.mdp_info.gamma)\n\n        Jep = np.array(Jep)\n        theta = np.array(self._theta_list)\n\n        self._update(Jep, theta)\n\n        self._theta_list = list()\n\n    def stop(self):\n        self._theta_list = list()\n\n    def _update(self, Jep, theta):\n        """"""\n        Function that implements the update routine of distribution parameters.\n        Every black box algorithms should implement this function with the\n        proper update.\n\n        Args:\n            Jep (np.ndarray): a vector containing the J of the considered\n                trajectories;\n            theta (np.ndarray): a matrix of policy parameters of the considered\n                trajectories.\n\n        """"""\n        raise NotImplementedError(\'BlackBoxOptimization is an abstract class\')\n'"
mushroom_rl/algorithms/policy_search/black_box_optimization/pgpe.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.policy_search.black_box_optimization import BlackBoxOptimization\n\n\nclass PGPE(BlackBoxOptimization):\n    """"""\n    Policy Gradient with Parameter Exploration algorithm.\n    ""A Survey on Policy Search for Robotics"", Deisenroth M. P., Neumann G.,\n    Peters J.. 2013.\n\n    """"""\n    def __init__(self, mdp_info, distribution, policy, learning_rate,\n                 features=None):\n        """"""\n        Constructor.\n\n        Args:\n            learning_rate (Parameter): the learning rate for the gradient step.\n\n        """"""\n        self.learning_rate = learning_rate\n\n        self._add_save_attr(learning_rate=\'pickle\')\n\n        super().__init__(mdp_info, distribution, policy, features)\n\n    def _update(self, Jep, theta):\n        baseline_num_list = list()\n        baseline_den_list = list()\n        diff_log_dist_list = list()\n\n        # Compute derivatives of distribution and baseline components\n        for i in range(len(Jep)):\n            J_i = Jep[i]\n            theta_i = theta[i]\n\n            diff_log_dist = self.distribution.diff_log(theta_i)\n            diff_log_dist2 = diff_log_dist**2\n\n            diff_log_dist_list.append(diff_log_dist)\n            baseline_num_list.append(J_i * diff_log_dist2)\n            baseline_den_list.append(diff_log_dist2)\n\n        # Compute baseline\n        baseline = np.mean(baseline_num_list, axis=0) / \\\n            np.mean(baseline_den_list, axis=0)\n        baseline[np.logical_not(np.isfinite(baseline))] = 0.\n\n        # Compute gradient\n        grad_J_list = list()\n        for i in range(len(Jep)):\n            diff_log_dist = diff_log_dist_list[i]\n            J_i = Jep[i]\n\n            grad_J_list.append(diff_log_dist * (J_i - baseline))\n\n        grad_J = np.mean(grad_J_list, axis=0)\n\n        omega = self.distribution.get_parameters()\n        omega += self.learning_rate(grad_J) * grad_J\n        self.distribution.set_parameters(omega)\n'"
mushroom_rl/algorithms/policy_search/black_box_optimization/reps.py,0,"b'import numpy as np\n\nfrom scipy.optimize import minimize\n\nfrom mushroom_rl.algorithms.policy_search.black_box_optimization import BlackBoxOptimization\n\n\nclass REPS(BlackBoxOptimization):\n    """"""\n    Episodic Relative Entropy Policy Search algorithm.\n    ""A Survey on Policy Search for Robotics"", Deisenroth M. P., Neumann G.,\n    Peters J.. 2013.\n\n    """"""\n    def __init__(self, mdp_info, distribution, policy, eps, features=None):\n        """"""\n        Constructor.\n\n        Args:\n            eps (float): the maximum admissible value for the Kullback-Leibler\n                divergence between the new distribution and the\n                previous one at each update step.\n\n        """"""\n        self.eps = eps\n\n        self._add_save_attr(eps=\'numpy\')\n\n        super().__init__(mdp_info, distribution, policy, features)\n\n    def _update(self, Jep, theta):\n        eta_start = np.ones(1)\n\n        res = minimize(REPS._dual_function, eta_start,\n                       jac=REPS._dual_function_diff,\n                       bounds=((np.finfo(np.float32).eps, np.inf),),\n                       args=(self.eps, Jep, theta))\n\n        eta_opt = res.x.item()\n\n        Jep -= np.max(Jep)\n\n        d = np.exp(Jep / eta_opt)\n\n        self.distribution.mle(theta, d)\n\n    @staticmethod\n    def _dual_function(eta_array, *args):\n        eta = eta_array.item()\n        eps, Jep, theta = args\n\n        max_J = np.max(Jep)\n\n        r = Jep - max_J\n        sum1 = np.mean(np.exp(r / eta))\n\n        return eta * eps + eta * np.log(sum1) + max_J\n\n    @staticmethod\n    def _dual_function_diff(eta_array, *args):\n        eta = eta_array.item()\n        eps, Jep, theta = args\n\n        max_J = np.max(Jep)\n\n        r = Jep - max_J\n\n        sum1 = np.mean(np.exp(r / eta))\n        sum2 = np.mean(np.exp(r / eta) * r)\n\n        gradient = eps + np.log(sum1) - sum2 / (eta * sum1)\n\n        return np.array([gradient])\n'"
mushroom_rl/algorithms/policy_search/black_box_optimization/rwr.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.policy_search.black_box_optimization import BlackBoxOptimization\n\n\nclass RWR(BlackBoxOptimization):\n    """"""\n    Reward-Weighted Regression algorithm.\n    ""A Survey on Policy Search for Robotics"", Deisenroth M. P., Neumann G.,\n    Peters J.. 2013.\n\n    """"""\n    def __init__(self, mdp_info, distribution, policy, beta, features=None):\n        """"""\n        Constructor.\n\n        Args:\n            beta (float): the temperature for the exponential reward\n                transformation.\n\n        """"""\n        self.beta = beta\n\n        self._add_save_attr(beta=\'numpy\')\n\n        super().__init__(mdp_info, distribution, policy, features)\n\n    def _update(self, Jep, theta):\n        Jep -= np.max(Jep)\n\n        d = np.exp(self.beta * Jep)\n\n        self.distribution.mle(theta, d)\n'"
mushroom_rl/algorithms/policy_search/policy_gradient/__init__.py,0,"b""from .policy_gradient import PolicyGradient\nfrom .reinforce import REINFORCE\nfrom .gpomdp import GPOMDP\nfrom .enac import eNAC\n\n__all__ = ['REINFORCE', 'GPOMDP', 'eNAC']"""
mushroom_rl/algorithms/policy_search/policy_gradient/enac.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.policy_search.policy_gradient import PolicyGradient\n\n\nclass eNAC(PolicyGradient):\n    """"""\n    Episodic Natural Actor Critic algorithm.\n    ""A Survey on Policy Search for Robotics"", Deisenroth M. P., Neumann G.,\n    Peters J. 2013.\n\n    """"""\n    def __init__(self, mdp_info, policy, learning_rate, features=None,\n                 critic_features=None):\n        """"""\n        Constructor.\n\n        Args:\n            critic_features (Features, None): features used by the critic.\n\n        """"""\n        super().__init__(mdp_info, policy, learning_rate, features)\n        self.phi_c = critic_features\n\n        self.sum_grad_log = None\n        self.psi_ext = None\n        self.sum_grad_log_list = list()\n\n        self._add_save_attr(\n            phi_c=\'pickle\', \n            sum_grad_log=\'numpy\', \n            psi_ext=\'pickle\', \n            sum_grad_log_list=\'pickle\'\n        )\n\n    def _compute_gradient(self, J):\n        R = np.array(J)\n        PSI = np.array(self.sum_grad_log_list)\n\n        w_and_v = np.linalg.pinv(PSI).dot(R)\n        nat_grad = w_and_v[:self.policy.weights_size]\n\n        self.sum_grad_log_list = list()\n\n        return nat_grad,\n\n    def _step_update(self, x, u, r):\n        self.sum_grad_log += self.policy.diff_log(x, u)\n\n        if self.psi_ext is None:\n            if self.phi_c is None:\n                self.psi_ext = np.ones(1)\n            else:\n                self.psi_ext = self.phi_c(x)\n\n    def _episode_end_update(self):\n        psi = np.concatenate((self.sum_grad_log, self.psi_ext))\n        self.sum_grad_log_list.append(psi)\n\n    def _init_update(self):\n        self.psi_ext = None\n        self.sum_grad_log = np.zeros(self.policy.weights_size)\n'"
mushroom_rl/algorithms/policy_search/policy_gradient/gpomdp.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.policy_search.policy_gradient import PolicyGradient\n\n\nclass GPOMDP(PolicyGradient):\n    """"""\n    GPOMDP algorithm.\n    ""Infinite-Horizon Policy-Gradient Estimation"". Baxter J. and Bartlett P. L..\n    2001.\n\n    """"""\n    def __init__(self, mdp_info, policy, learning_rate, features=None):\n        super().__init__(mdp_info, policy, learning_rate, features)\n\n        self.sum_d_log_pi = None\n        self.list_sum_d_log_pi = list()\n        self.list_sum_d_log_pi_ep = list()\n\n        self.list_reward = list()\n        self.list_reward_ep = list()\n\n        self.baseline_num = list()\n        self.baseline_den = list()\n\n        self.step_count = 0\n\n        self._add_save_attr(\n            sum_d_log_pi=\'numpy\',\n            list_sum_d_log_pi=\'pickle\',\n            list_sum_d_log_pi_ep=\'pickle\',\n            list_reward=\'pickle\',\n            list_reward_ep=\'pickle\',\n            baseline_num=\'pickle\',\n            baseline_den=\'pickle\',\n            step_count=\'numpy\'\n        )\n\n        # Ignore divide by zero\n        np.seterr(divide=\'ignore\', invalid=\'ignore\')\n\n    def _compute_gradient(self, J):\n        gradient = np.zeros(self.policy.weights_size)\n\n        n_episodes = len(self.list_sum_d_log_pi_ep)\n\n        for i in range(n_episodes):\n            list_sum_d_log_pi = self.list_sum_d_log_pi_ep[i]\n            list_reward = self.list_reward_ep[i]\n\n            n_steps = len(list_sum_d_log_pi)\n\n            for t in range(n_steps):\n                step_grad = list_sum_d_log_pi[t]\n                step_reward = list_reward[t]\n                baseline = self.baseline_num[t] / self.baseline_den[t]\n                baseline[np.logical_not(np.isfinite(baseline))] = 0.\n                gradient += (step_reward - baseline) * step_grad\n\n        gradient /= n_episodes\n\n        self.list_reward_ep = list()\n        self.list_sum_d_log_pi_ep = list()\n\n        self.baseline_num = list()\n        self.baseline_den = list()\n\n        return gradient,\n\n    def _step_update(self, x, u, r):\n        discounted_reward = self.df * r\n        self.list_reward.append(discounted_reward)\n\n        d_log_pi = self.policy.diff_log(x, u)\n        self.sum_d_log_pi += d_log_pi\n\n        self.list_sum_d_log_pi.append(self.sum_d_log_pi)\n\n        squared_sum_d_log_pi = np.square(self.sum_d_log_pi)\n\n        if self.step_count < len(self.baseline_num):\n            self.baseline_num[\n                self.step_count] += discounted_reward * squared_sum_d_log_pi\n            self.baseline_den[self.step_count] += squared_sum_d_log_pi\n        else:\n            self.baseline_num.append(discounted_reward * squared_sum_d_log_pi)\n            self.baseline_den.append(squared_sum_d_log_pi)\n\n        self.step_count += 1\n\n    def _episode_end_update(self):\n        self.list_reward_ep.append(self.list_reward)\n        self.list_reward = list()\n\n        self.list_sum_d_log_pi_ep.append(self.list_sum_d_log_pi)\n        self.list_sum_d_log_pi = list()\n\n    def _init_update(self):\n        self.sum_d_log_pi = np.zeros(self.policy.weights_size)\n        self.list_sum_d_log_pi = list()\n        self.step_count = 0\n'"
mushroom_rl/algorithms/policy_search/policy_gradient/policy_gradient.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.agent import Agent\n\n\nclass PolicyGradient(Agent):\n    """"""\n    Abstract class to implement a generic Policy Search algorithm using the\n    gradient of the policy to update its parameters.\n    ""A survey on Policy Gradient algorithms for Robotics"". Deisenroth M. P. et\n    al.. 2011.\n\n    """"""\n    def __init__(self, mdp_info, policy, learning_rate, features):\n        """"""\n        Constructor.\n\n        Args:\n            learning_rate (float): the learning rate.\n\n        """"""\n        self.learning_rate = learning_rate\n        self.df = 1\n        self.J_episode = 0\n\n        self._add_save_attr(\n            learning_rate=\'pickle\',\n            df=\'numpy\',\n            J_episode=\'numpy\'\n        )\n\n        super().__init__(mdp_info, policy, features)\n\n    def fit(self, dataset):\n        J = list()\n        self.df = 1.\n        self.J_episode = 0.\n        self._init_update()\n        for sample in dataset:\n            x, u, r, xn, _, last = self._parse(sample)\n            self._step_update(x, u, r)\n            self.J_episode += self.df * r\n            self.df *= self.mdp_info.gamma\n\n            if last:\n                self._episode_end_update()\n                J.append(self.J_episode)\n                self.J_episode = 0.\n                self.df = 1.\n                self._init_update()\n\n        self._update_parameters(J)\n\n    def _update_parameters(self, J):\n        """"""\n        Update the parameters of the policy.\n\n        Args:\n             J (list): list of the cumulative discounted rewards for each\n                episode in the dataset.\n\n        """"""\n        res = self._compute_gradient(J)\n\n        theta = self.policy.get_weights()\n\n        if len(res) == 1:\n            grad = res[0]\n            delta = self.learning_rate(grad) * grad\n        else:\n            grad, nat_grad = res\n            delta = self.learning_rate(grad, nat_grad) * nat_grad\n\n        theta_new = theta + delta\n        self.policy.set_weights(theta_new)\n\n    def _init_update(self):\n        """"""\n        This function is called, when parsing the dataset, at the beginning\n        of each episode. The implementation is dependent on the algorithm (e.g.\n        REINFORCE resets some data structure).\n\n        """"""\n        raise NotImplementedError(\'PolicyGradient is an abstract class\')\n\n    def _step_update(self, x, u, r):\n        """"""\n        This function is called, when parsing the dataset, at each episode step.\n\n        Args:\n            x (np.ndarray): the state at the current step;\n            u (np.ndarray): the action at the current step;\n            r (np.ndarray): the reward at the current step.\n\n        """"""\n        raise NotImplementedError(\'PolicyGradient is an abstract class\')\n\n    def _episode_end_update(self):\n        """"""\n        This function is called, when parsing the dataset, at the beginning\n        of each episode. The implementation is dependent on the algorithm (e.g.\n        REINFORCE updates some data structures).\n\n        """"""\n        raise NotImplementedError(\'PolicyGradient is an abstract class\')\n\n    def _compute_gradient(self, J):\n        """"""\n        Return the gradient computed by the algorithm.\n\n        Args:\n             J (list): list of the cumulative discounted rewards for each\n                episode in the dataset.\n\n        """"""\n        raise NotImplementedError(\'PolicyGradient is an abstract class\')\n\n    def _parse(self, sample):\n        """"""\n        Utility to parse the sample.\n\n        Args:\n             sample (list): the current episode step.\n\n        Returns:\n            A tuple containing state, action, reward, next state, absorbing and\n            last flag. If provided, ``state`` is preprocessed with the features.\n\n        """"""\n        state = sample[0]\n        action = sample[1]\n        reward = sample[2]\n        next_state = sample[3]\n        absorbing = sample[4]\n        last = sample[5]\n\n        if self.phi is not None:\n            state = self.phi(state)\n\n        return state, action, reward, next_state, absorbing, last\n'"
mushroom_rl/algorithms/policy_search/policy_gradient/reinforce.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.policy_search.policy_gradient import PolicyGradient\n\n\nclass REINFORCE(PolicyGradient):\n    """"""\n    REINFORCE algorithm.\n    ""Simple Statistical Gradient-Following Algorithms for Connectionist\n    Reinforcement Learning"", Williams R. J.. 1992.\n\n    """"""\n    def __init__(self, mdp_info, policy, learning_rate, features=None):\n        super().__init__(mdp_info, policy, learning_rate, features)\n        self.sum_d_log_pi = None\n        self.list_sum_d_log_pi = list()\n        self.baseline_num = list()\n        self.baseline_den = list()\n\n        self._add_save_attr(\n            sum_d_log_pi=\'numpy\',\n            list_sum_d_log_pi=\'pickle\',\n            baseline_num=\'pickle\',\n            baseline_den=\'pickle\'\n        )\n\n        # Ignore divide by zero\n        np.seterr(divide=\'ignore\', invalid=\'ignore\')\n\n    def _compute_gradient(self, J):\n        baseline = np.mean(self.baseline_num, axis=0) / np.mean(\n            self.baseline_den, axis=0)\n        baseline[np.logical_not(np.isfinite(baseline))] = 0.\n        grad_J_episode = list()\n        for i, J_episode in enumerate(J):\n            sum_d_log_pi = self.list_sum_d_log_pi[i]\n            grad_J_episode.append(sum_d_log_pi * (J_episode - baseline))\n\n        grad_J = np.mean(grad_J_episode, axis=0)\n        self.list_sum_d_log_pi = list()\n        self.baseline_den = list()\n        self.baseline_num = list()\n\n        return grad_J,\n\n    def _step_update(self, x, u, r):\n        d_log_pi = self.policy.diff_log(x, u)\n        self.sum_d_log_pi += d_log_pi\n\n    def _episode_end_update(self):\n        self.list_sum_d_log_pi.append(self.sum_d_log_pi)\n        squared_sum_d_log_pi = np.square(self.sum_d_log_pi)\n        self.baseline_num.append(squared_sum_d_log_pi * self.J_episode)\n        self.baseline_den.append(squared_sum_d_log_pi)\n\n    def _init_update(self):\n        self.sum_d_log_pi = np.zeros(self.policy.weights_size)\n'"
mushroom_rl/algorithms/value/batch_td/__init__.py,0,"b""from .batch_td import BatchTD\nfrom .fqi import FQI, DoubleFQI\nfrom .lspi import LSPI\n\n__all__ = ['FQI', 'DoubleFQI', 'LSPI']"""
mushroom_rl/algorithms/value/batch_td/batch_td.py,0,"b'from mushroom_rl.algorithms.agent import Agent\nfrom mushroom_rl.approximators import Regressor\n\n\nclass BatchTD(Agent):\n    """"""\n    Abstract class to implement a generic Batch TD algorithm.\n\n    """"""\n    def __init__(self, mdp_info, policy, approximator, approximator_params=None,\n                 fit_params=None, features=None):\n        """"""\n        Constructor.\n\n        Args:\n            approximator (object): approximator used by the algorithm and the\n                policy.\n            approximator_params (dict, None): parameters of the approximator to\n                build;\n            fit_params (dict, None): parameters of the fitting algorithm of the\n                approximator;\n\n        """"""\n        self._approximator_params = dict() if approximator_params is None else\\\n            approximator_params\n        self._fit_params = dict() if fit_params is None else fit_params\n\n        self.approximator = Regressor(approximator,\n                                      **self._approximator_params)\n        policy.set_q(self.approximator)\n\n        self._add_save_attr(\n            _approximator_params=\'pickle\',\n            approximator=\'pickle\',\n            _fit_params=\'pickle\'\n        )\n\n        super().__init__(mdp_info, policy, features)\n'"
mushroom_rl/algorithms/value/batch_td/fqi.py,0,"b'import numpy as np\nfrom tqdm import trange\n\nfrom mushroom_rl.algorithms.value.batch_td import BatchTD\nfrom mushroom_rl.utils.dataset import parse_dataset\n\n\nclass FQI(BatchTD):\n    """"""\n    Fitted Q-Iteration algorithm.\n    ""Tree-Based Batch Mode Reinforcement Learning"", Ernst D. et al.. 2005.\n\n    """"""\n    def __init__(self, mdp_info, policy, approximator, n_iterations,\n                 approximator_params=None, fit_params=None, quiet=False,\n                 boosted=False):\n        """"""\n        Constructor.\n\n        Args:\n            n_iterations (int): number of iterations to perform for training;\n            quiet (bool, False): whether to show the progress bar or not;\n            boosted (bool, False): whether to use boosted FQI or not.\n\n        """"""\n        self._n_iterations = n_iterations\n        self._quiet = quiet\n\n        # ""Boosted Fitted Q-Iteration"". Tosatto S. et al.. 2017.\n        self._boosted = boosted\n        if self._boosted:\n            self._prediction = 0.\n            self._next_q = 0.\n            self._idx = 0\n            approximator_params[\'n_models\'] = n_iterations\n\n        self._add_save_attr(\n            _n_iterations=\'numpy\',\n            _quiet=\'numpy\',\n            _boosted=\'numpy\',\n            _prediction=\'numpy\',\n            _next_q=\'pickle\',\n            _idx=\'numpy\',\n            _target=\'pickle\'\n        )\n\n        super().__init__(mdp_info, policy, approximator, approximator_params,\n                         fit_params)\n\n        self._target = None\n\n    def fit(self, dataset):\n        """"""\n        Fit loop.\n\n        """"""\n        if self._boosted:\n            if self._target is None:\n                self._prediction = 0.\n                self._next_q = 0.\n                self._idx = 0\n            fit = self._fit_boosted\n        else:\n            fit = self._fit\n\n        for _ in trange(self._n_iterations, dynamic_ncols=True,\n                        disable=self._quiet, leave=False):\n            fit(dataset)\n\n    def _fit(self, x):\n        """"""\n        Single fit iteration.\n\n        Args:\n            x (list): the dataset.\n\n        """"""\n        state, action, reward, next_state, absorbing, _ = parse_dataset(x)\n        if self._target is None:\n            self._target = reward\n        else:\n            q = self.approximator.predict(next_state)\n            if np.any(absorbing):\n                q *= 1 - absorbing.reshape(-1, 1)\n\n            max_q = np.max(q, axis=1)\n            self._target = reward + self.mdp_info.gamma * max_q\n\n        self.approximator.fit(state, action, self._target, **self._fit_params)\n\n    def _fit_boosted(self, x):\n        """"""\n        Single fit iteration for boosted FQI.\n\n        Args:\n            x (list): the dataset.\n\n        """"""\n        state, action, reward, next_state, absorbing, _ = parse_dataset(x)\n        if self._target is None:\n            self._target = reward\n        else:\n            self._next_q += self.approximator.predict(next_state,\n                                                      idx=self._idx - 1)\n            if np.any(absorbing):\n                self._next_q *= 1 - absorbing.reshape(-1, 1)\n\n            max_q = np.max(self._next_q, axis=1)\n            self._target = reward + self.mdp_info.gamma * max_q\n\n        self._target -= self._prediction\n        self._prediction += self._target\n\n        self.approximator.fit(state, action, self._target, idx=self._idx,\n                              **self._fit_params)\n\n        self._idx += 1\n\n\nclass DoubleFQI(FQI):\n    """"""\n    Double Fitted Q-Iteration algorithm.\n    ""Estimating the Maximum Expected Value in Continuous Reinforcement Learning\n    Problems"". D\'Eramo C. et al.. 2017.\n\n    """"""\n    def __init__(self, mdp_info, policy, approximator, n_iterations,\n                 approximator_params=None, fit_params=None, quiet=False):\n        approximator_params[\'n_models\'] = 2\n\n        super().__init__(mdp_info, policy, approximator, n_iterations,\n                         approximator_params, fit_params, quiet)\n\n    def _fit(self, x):\n        state = list()\n        action = list()\n        reward = list()\n        next_state = list()\n        absorbing = list()\n\n        half = len(x) // 2\n        for i in range(2):\n            s, a, r, ss, ab, _ = parse_dataset(x[i * half:(i + 1) * half])\n            state.append(s)\n            action.append(a)\n            reward.append(r)\n            next_state.append(ss)\n            absorbing.append(ab)\n\n        if self._target is None:\n            self._target = reward\n        else:\n            for i in range(2):\n                q_i = self.approximator.predict(next_state[i], idx=i)\n\n                amax_q = np.expand_dims(np.argmax(q_i, axis=1), axis=1)\n                max_q = self.approximator.predict(next_state[i], amax_q,\n                                                  idx=1 - i)\n                if np.any(absorbing[i]):\n                    max_q *= 1 - absorbing[i]\n                self._target[i] = reward[i] + self.mdp_info.gamma * max_q\n\n        for i in range(2):\n            self.approximator.fit(state[i], action[i], self._target[i], idx=i,\n                                  **self._fit_params)\n'"
mushroom_rl/algorithms/value/batch_td/lspi.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.value.batch_td import BatchTD\nfrom mushroom_rl.approximators.parametric import LinearApproximator\nfrom mushroom_rl.features import get_action_features\nfrom mushroom_rl.utils.dataset import parse_dataset\n\n\nclass LSPI(BatchTD):\n    """"""\n    Least-Squares Policy Iteration algorithm.\n    ""Least-Squares Policy Iteration"". Lagoudakis M. G. and Parr R.. 2003.\n\n    """"""\n    def __init__(self, mdp_info, policy, approximator_params=None,\n                 epsilon=1e-2, fit_params=None, features=None):\n        """"""\n        Constructor.\n\n        Args:\n            epsilon (float, 1e-2): termination coefficient.\n\n        """"""\n        self._epsilon = epsilon\n\n        k = features.size * mdp_info.action_space.n\n        self._A = np.zeros((k, k))\n        self._b = np.zeros((k, 1))\n\n        self._add_save_attr(_epsilon=\'numpy\', _A=\'numpy\', _b=\'numpy\')\n\n        super().__init__(mdp_info, policy, LinearApproximator,\n                         approximator_params, fit_params, features)\n\n    def fit(self, dataset):\n        phi_state, action, reward, phi_next_state, absorbing, _ = parse_dataset(\n            dataset, self.phi)\n        phi_state_action = get_action_features(phi_state, action,\n                                               self.mdp_info.action_space.n)\n\n        norm = np.inf\n        while norm > self._epsilon:\n            q = self.approximator.predict(phi_next_state)\n            if np.any(absorbing):\n                q *= 1 - absorbing.reshape(-1, 1)\n\n            next_action = np.argmax(q, axis=1).reshape(-1, 1)\n            phi_next_state_next_action = get_action_features(\n                phi_next_state,\n                next_action,\n                self.mdp_info.action_space.n\n            )\n\n            tmp = phi_state_action - self.mdp_info.gamma *\\\n                phi_next_state_next_action\n            self._A += phi_state_action.T.dot(tmp)\n            self._b += (phi_state_action.T.dot(reward)).reshape(-1, 1)\n\n            old_w = self.approximator.get_weights()\n            if np.linalg.matrix_rank(self._A) == self._A.shape[1]:\n                w = np.linalg.solve(self._A, self._b).ravel()\n            else:\n                w = np.linalg.pinv(self._A).dot(self._b).ravel()\n            self.approximator.set_weights(w)\n\n            norm = np.linalg.norm(w - old_w)\n'"
mushroom_rl/algorithms/value/dqn/__init__.py,0,"b""from .dqn import DQN, DoubleDQN, AveragedDQN\nfrom .categorical_dqn import CategoricalDQN\n\n\n__all__ = ['DQN', 'DoubleDQN', 'AveragedDQN', 'CategoricalDQN']\n"""
mushroom_rl/algorithms/value/dqn/categorical_dqn.py,8,"b'from copy import deepcopy\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom mushroom_rl.algorithms.value.dqn import DQN\nfrom mushroom_rl.approximators.parametric.torch_approximator import *\n\n\nclass CategoricalNetwork(nn.Module):\n    def __init__(self, input_shape, output_shape, features_network, n_atoms,\n                 v_min, v_max, n_features, use_cuda, **kwargs):\n        super().__init__()\n\n        self._n_output = output_shape[0]\n        self._phi = features_network(input_shape, (n_features,),\n                                     n_features=n_features, **kwargs)\n        self._n_atoms = n_atoms\n        self._v_min = v_min\n        self._v_max = v_max\n\n        delta = (self._v_max - self._v_min) / (self._n_atoms - 1)\n        self._a_values = torch.arange(self._v_min, self._v_max + delta, delta)\n        if use_cuda:\n            self._a_values = self._a_values.cuda()\n\n        self._p = nn.ModuleList(\n            [nn.Linear(n_features, n_atoms) for _ in range(self._n_output)])\n\n        for i in range(self._n_output):\n            nn.init.xavier_uniform_(self._p[i].weight,\n                                    gain=nn.init.calculate_gain(\'linear\'))\n\n    def forward(self, state, action=None, get_distribution=False):\n        features = self._phi(state, action)\n\n        a_p = [F.softmax(self._p[i](features), -1) for i in range(self._n_output)]\n        a_p = torch.stack(a_p, dim=1)\n\n        if not get_distribution:\n            q = torch.empty(a_p.shape[:-1])\n            for i in range(a_p.shape[0]):\n                q[i] = a_p[i] @ self._a_values\n\n            if action is not None:\n                return torch.squeeze(q.gather(1, action))\n            else:\n                return q\n        else:\n            if action is not None:\n                action = torch.unsqueeze(\n                    action.long(), 2).repeat(1, 1, self._n_atoms)\n\n                return torch.squeeze(a_p.gather(1, action))\n            else:\n                return a_p\n\n\nclass CategoricalDQN(DQN):\n    """"""\n    Categorical DQN algorithm.\n    ""A Distributional Perspective on Reinforcement Learning"".\n    Bellemare M. et al.. 2017.\n\n    """"""\n    def __init__(self, mdp_info, policy, approximator_params, n_atoms, v_min,\n                 v_max, **params):\n        """"""\n        Constructor.\n\n        Args:\n            n_atoms (int): number of atoms;\n            v_min (float): minimum value of value-function;\n            v_max (float): maximum value of value-function.\n\n        """"""\n        features_network = approximator_params[\'network\']\n        params[\'approximator_params\'] = deepcopy(approximator_params)\n        params[\'approximator_params\'][\'network\'] = CategoricalNetwork\n        params[\'approximator_params\'][\'features_network\'] = features_network\n        params[\'approximator_params\'][\'n_atoms\'] = n_atoms\n        params[\'approximator_params\'][\'v_min\'] = v_min\n        params[\'approximator_params\'][\'v_max\'] = v_max\n\n        self._n_atoms = n_atoms\n        self._v_min = v_min\n        self._v_max = v_max\n        self._delta = (v_max - v_min) / (n_atoms - 1)\n        self._a_values = np.arange(v_min, v_max + self._delta, self._delta)\n\n        self._add_save_attr(\n            _n_atoms=\'numpy\',\n            _v_min=\'numpy\',\n            _v_max=\'numpy\',\n            _delta=\'numpy\',\n            _a_values=\'numpy\'\n        )\n\n        super().__init__(mdp_info, policy, TorchApproximator, **params)\n\n    def fit(self, dataset):\n        self._replay_memory.add(dataset)\n        if self._replay_memory.initialized:\n            state, action, reward, next_state, absorbing, _ =\\\n                self._replay_memory.get(self._batch_size)\n\n            if self._clip_reward:\n                reward = np.clip(reward, -1, 1)\n\n            q_next = self.target_approximator.predict(next_state)\n            a_max = np.argmax(q_next, 1)\n            gamma = self.mdp_info.gamma * (1 - absorbing)\n            p_next = self.target_approximator.predict(next_state, a_max,\n                                                      get_distribution=True)\n            gamma_z = gamma.reshape(-1, 1) * np.expand_dims(\n                self._a_values, 0).repeat(len(gamma), 0)\n            bell_a = (reward.reshape(-1, 1) + gamma_z).clip(self._v_min,\n                                                            self._v_max)\n\n            b = (bell_a - self._v_min) / self._delta\n            l = np.floor(b).astype(np.int)\n            u = np.ceil(b).astype(np.int)\n\n            m = np.zeros((self._batch_size, self._n_atoms))\n            for i in range(self._n_atoms):\n                l[:, i][(u[:, i] > 0) * (l[:, i] == u[:, i])] -= 1\n                u[:, i][(l[:, i] < (self._n_atoms - 1)) * (l[:, i] == u[:, i])] += 1\n\n                m[np.arange(len(m)), l[:, i]] += p_next[:, i] * (u[:, i] - b[:, i])\n                m[np.arange(len(m)), u[:, i]] += p_next[:, i] * (b[:, i] - l[:, i])\n\n            self.approximator.fit(state, action, m, get_distribution=True,\n                                  **self._fit_params)\n\n            self._n_updates += 1\n\n            if self._n_updates % self._target_update_frequency == 0:\n                self._update_target()\n'"
mushroom_rl/algorithms/value/dqn/dqn.py,0,"b'from copy import deepcopy\n\nimport numpy as np\n\nfrom mushroom_rl.algorithms.agent import Agent\nfrom mushroom_rl.approximators.parametric.torch_approximator import *\nfrom mushroom_rl.approximators.regressor import Ensemble, Regressor\nfrom mushroom_rl.utils.replay_memory import PrioritizedReplayMemory, ReplayMemory\n\n\nclass DQN(Agent):\n    """"""\n    Deep Q-Network algorithm.\n    ""Human-Level Control Through Deep Reinforcement Learning"".\n    Mnih V. et al.. 2015.\n\n    """"""\n    def __init__(self, mdp_info, policy, approximator, approximator_params,\n                 batch_size, target_update_frequency,\n                 replay_memory=None, initial_replay_size=500,\n                 max_replay_size=5000, fit_params=None, n_approximators=1,\n                 clip_reward=True):\n        """"""\n        Constructor.\n\n        Args:\n            approximator (object): the approximator to use to fit the\n               Q-function;\n            approximator_params (dict): parameters of the approximator to\n                build;\n            batch_size (int): the number of samples in a batch;\n            target_update_frequency (int): the number of samples collected\n                between each update of the target network;\n            replay_memory ([ReplayMemory, PrioritizedReplayMemory], None): the\n                object of the replay memory to use; if None, a default replay\n                memory is created;\n            initial_replay_size (int): the number of samples to collect before\n                starting the learning;\n            max_replay_size (int): the maximum number of samples in the replay\n                memory;\n            fit_params (dict, None): parameters of the fitting algorithm of the\n                approximator;\n            n_approximators (int, 1): the number of approximator to use in\n                ``AveragedDQN``;\n            clip_reward (bool, True): whether to clip the reward or not.\n\n        """"""\n        self._fit_params = dict() if fit_params is None else fit_params\n\n        self._batch_size = batch_size\n        self._n_approximators = n_approximators\n        self._clip_reward = clip_reward\n        self._target_update_frequency = target_update_frequency\n\n        if replay_memory is not None:\n            self._replay_memory = replay_memory\n            if isinstance(replay_memory, PrioritizedReplayMemory):\n                self._fit = self._fit_prioritized\n            else:\n                self._fit = self._fit_standard\n        else:\n            self._replay_memory = ReplayMemory(initial_replay_size,\n                                               max_replay_size)\n            self._fit = self._fit_standard\n\n        self._n_updates = 0\n\n        apprx_params_train = deepcopy(approximator_params)\n        apprx_params_target = deepcopy(approximator_params)\n        self.approximator = Regressor(approximator, **apprx_params_train)\n        self.target_approximator = Regressor(approximator,\n                                             n_models=self._n_approximators,\n                                             **apprx_params_target)\n        policy.set_q(self.approximator)\n\n        if self._n_approximators == 1:\n            self.target_approximator.set_weights(\n                self.approximator.get_weights())\n        else:\n            for i in range(self._n_approximators):\n                self.target_approximator[i].set_weights(\n                    self.approximator.get_weights())\n\n        self._add_save_attr(\n            _fit_params=\'pickle\',\n            _batch_size=\'numpy\',\n            _n_approximators=\'numpy\',\n            _clip_reward=\'numpy\',\n            _target_update_frequency=\'numpy\',\n            _replay_memory=\'pickle\',\n            _fit=\'pickle\',\n            _n_updates=\'numpy\',\n            approximator=\'pickle\',\n            target_approximator=\'pickle\'\n        )\n\n        super().__init__(mdp_info, policy)\n\n    def fit(self, dataset):\n        self._fit(dataset)\n\n        self._n_updates += 1\n        if self._n_updates % self._target_update_frequency == 0:\n            self._update_target()\n\n    def _fit_standard(self, dataset):\n        self._replay_memory.add(dataset)\n        if self._replay_memory.initialized:\n            state, action, reward, next_state, absorbing, _ = \\\n                self._replay_memory.get(self._batch_size)\n\n            if self._clip_reward:\n                reward = np.clip(reward, -1, 1)\n\n            q_next = self._next_q(next_state, absorbing)\n            q = reward + self.mdp_info.gamma * q_next\n\n            self.approximator.fit(state, action, q, **self._fit_params)\n\n    def _fit_prioritized(self, dataset):\n        self._replay_memory.add(\n            dataset, np.ones(len(dataset)) * self._replay_memory.max_priority)\n        if self._replay_memory.initialized:\n            state, action, reward, next_state, absorbing, _, idxs, is_weight = \\\n                self._replay_memory.get(self._batch_size)\n\n            if self._clip_reward:\n                reward = np.clip(reward, -1, 1)\n\n            q_next = self._next_q(next_state, absorbing)\n            q = reward + self.mdp_info.gamma * q_next\n            td_error = q - self.approximator.predict(state, action)\n\n            self._replay_memory.update(td_error, idxs)\n\n            self.approximator.fit(state, action, q, weights=is_weight,\n                                  **self._fit_params)\n\n    def _update_target(self):\n        """"""\n        Update the target network.\n\n        """"""\n        self.target_approximator.set_weights(\n            self.approximator.get_weights())\n\n    def _next_q(self, next_state, absorbing):\n        """"""\n        Args:\n            next_state (np.ndarray): the states where next action has to be\n                evaluated;\n            absorbing (np.ndarray): the absorbing flag for the states in\n                ``next_state``.\n\n        Returns:\n            Maximum action-value for each state in ``next_state``.\n\n        """"""\n        q = self.target_approximator.predict(next_state)\n        if np.any(absorbing):\n            q *= 1 - absorbing.reshape(-1, 1)\n\n        return np.max(q, axis=1)\n\n    def draw_action(self, state):\n        action = super(DQN, self).draw_action(np.array(state))\n\n        return action\n\n\nclass DoubleDQN(DQN):\n    """"""\n    Double DQN algorithm.\n    ""Deep Reinforcement Learning with Double Q-Learning"".\n    Hasselt H. V. et al.. 2016.\n\n    """"""\n    def _next_q(self, next_state, absorbing):\n        q = self.approximator.predict(next_state)\n        max_a = np.argmax(q, axis=1)\n\n        double_q = self.target_approximator.predict(next_state, max_a)\n        if np.any(absorbing):\n            double_q *= 1 - absorbing\n\n        return double_q\n\n\nclass AveragedDQN(DQN):\n    """"""\n    Averaged-DQN algorithm.\n    ""Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement\n    Learning"". Anschel O. et al.. 2017.\n\n    """"""\n    def __init__(self, mdp_info, policy, approximator, **params):\n        super().__init__(mdp_info, policy, approximator, **params)\n\n        self._n_fitted_target_models = 1\n\n        self._add_save_attr(_n_fitted_target_models=\'numpy\')\n\n        assert len(self.target_approximator) > 1\n\n    def _update_target(self):\n        idx = self._n_updates // self._target_update_frequency\\\n              % self._n_approximators\n        self.target_approximator[idx].set_weights(\n            self.approximator.get_weights())\n\n        if self._n_fitted_target_models < self._n_approximators:\n            self._n_fitted_target_models += 1\n\n    def _next_q(self, next_state, absorbing):\n        q = list()\n        for idx in range(self._n_fitted_target_models):\n            q.append(self.target_approximator.predict(next_state, idx=idx))\n        q = np.mean(q, axis=0)\n        if np.any(absorbing):\n            q *= 1 - absorbing.reshape(-1, 1)\n\n        return np.max(q, axis=1)\n'"
mushroom_rl/algorithms/value/td/__init__.py,0,"b""from .td import TD\nfrom .sarsa import SARSA\nfrom .sarsa_lambda import SARSALambda\nfrom .expected_sarsa import ExpectedSARSA\nfrom .q_learning import QLearning\nfrom .double_q_learning import DoubleQLearning\nfrom .speedy_q_learning import SpeedyQLearning\nfrom .r_learning import RLearning\nfrom .weighted_q_learning import WeightedQLearning\nfrom .rq_learning import RQLearning\nfrom .sarsa_lambda_continuous import SARSALambdaContinuous\nfrom .true_online_sarsa_lambda import TrueOnlineSARSALambda\n\n__all__ = ['SARSA', 'SARSALambda', 'ExpectedSARSA', 'QLearning',\n           'DoubleQLearning', 'SpeedyQLearning', 'RLearning',\n           'WeightedQLearning', 'RQLearning', 'SARSALambdaContinuous',\n           'TrueOnlineSARSALambda']\n"""
mushroom_rl/algorithms/value/td/double_q_learning.py,0,"b'import numpy as np\nfrom copy import deepcopy\n\nfrom mushroom_rl.algorithms.value.td import TD\nfrom mushroom_rl.utils.table import EnsembleTable\n\n\nclass DoubleQLearning(TD):\n    """"""\n    Double Q-Learning algorithm.\n    ""Double Q-Learning"". Hasselt H. V.. 2010.\n\n    """"""\n    def __init__(self, mdp_info, policy, learning_rate):\n        self.Q = EnsembleTable(2, mdp_info.size)\n\n        self._add_save_attr(Q=\'pickle\', alpha=\'pickle\')\n\n        super().__init__(mdp_info, policy, self.Q, learning_rate)\n\n        self.alpha = [deepcopy(self.alpha), deepcopy(self.alpha)]\n\n        assert len(self.Q) == 2, \'The regressor ensemble must\' \\\n                                 \' have exactly 2 models.\'\n\n    def _update(self, state, action, reward, next_state, absorbing):\n        approximator_idx = 0 if np.random.uniform() < .5 else 1\n\n        q_current = self.Q[approximator_idx][state, action]\n\n        if not absorbing:\n            q_ss = self.Q[approximator_idx][next_state, :]\n            max_q = np.max(q_ss)\n            a_n = np.array(\n                [np.random.choice(np.argwhere(q_ss == max_q).ravel())])\n            q_next = self.Q[1 - approximator_idx][next_state, a_n]\n        else:\n            q_next = 0.\n\n        q = q_current + self.alpha[approximator_idx](state, action) * (\n            reward + self.mdp_info.gamma * q_next - q_current)\n\n        self.Q[approximator_idx][state, action] = q\n'"
mushroom_rl/algorithms/value/td/expected_sarsa.py,0,"b'from mushroom_rl.algorithms.value.td import TD\nfrom mushroom_rl.utils.table import Table\n\n\nclass ExpectedSARSA(TD):\n    """"""\n    Expected SARSA algorithm.\n    ""A theoretical and empirical analysis of Expected Sarsa"". Seijen H. V. et\n    al.. 2009.\n\n    """"""\n    def __init__(self, mdp_info, policy, learning_rate):\n        self.Q = Table(mdp_info.size)\n        self._add_save_attr(Q=\'pickle\')\n\n        super().__init__(mdp_info, policy, self.Q, learning_rate)\n\n    def _update(self, state, action, reward, next_state, absorbing):\n        q_current = self.Q[state, action]\n\n        if not absorbing:\n            q_next = self.Q[next_state, :].dot(self.policy(next_state))\n        else:\n            q_next = 0.\n\n        self.Q[state, action] = q_current + self.alpha(state, action) * (\n            reward + self.mdp_info.gamma * q_next - q_current)\n'"
mushroom_rl/algorithms/value/td/q_learning.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.value.td import TD\nfrom mushroom_rl.utils.table import Table\n\n\nclass QLearning(TD):\n    """"""\n    Q-Learning algorithm.\n    ""Learning from Delayed Rewards"". Watkins C.J.C.H.. 1989.\n\n    """"""\n    def __init__(self, mdp_info, policy, learning_rate):\n        self.Q = Table(mdp_info.size)\n\n        self._add_save_attr(Q=\'pickle\')\n\n        super().__init__(mdp_info, policy, self.Q, learning_rate)\n\n    def _update(self, state, action, reward, next_state, absorbing):\n        q_current = self.Q[state, action]\n\n        q_next = np.max(self.Q[next_state, :]) if not absorbing else 0.\n\n        self.Q[state, action] = q_current + self.alpha(state, action) * (\n            reward + self.mdp_info.gamma * q_next - q_current)\n'"
mushroom_rl/algorithms/value/td/r_learning.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.value.td import TD\nfrom mushroom_rl.utils.table import Table\n\n\nclass RLearning(TD):\n    """"""\n    R-Learning algorithm.\n    ""A Reinforcement Learning Method for Maximizing Undiscounted Rewards"".\n    Schwartz A.. 1993.\n\n    """"""\n    def __init__(self, mdp_info, policy, learning_rate, beta):\n        """"""\n        Constructor.\n\n        Args:\n            beta (Parameter): beta coefficient.\n\n        """"""\n        self.Q = Table(mdp_info.size)\n        self._rho = 0.\n        self.beta = beta\n\n        self._add_save_attr(Q=\'pickle\', _rho=\'numpy\', beta=\'pickle\')\n\n        super().__init__(mdp_info, policy, self.Q, learning_rate)\n\n    def _update(self, state, action, reward, next_state, absorbing):\n        q_current = self.Q[state, action]\n        q_next = np.max(self.Q[next_state, :]) if not absorbing else 0.\n        delta = reward - self._rho + q_next - q_current\n        q_new = q_current + self.alpha(state, action) * delta\n\n        self.Q[state, action] = q_new\n\n        q_max = np.max(self.Q[state, :])\n        if q_new == q_max:\n            delta = reward + q_next - q_max - self._rho\n            self._rho += self.beta(state, action) * delta\n'"
mushroom_rl/algorithms/value/td/rq_learning.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.value.td import TD\nfrom mushroom_rl.utils.table import Table\n\n\nclass RQLearning(TD):\n    """"""\n    RQ-Learning algorithm.\n    ""Exploiting Structure and Uncertainty of Bellman Updates in Markov Decision\n    Processes"". Tateo D. et al.. 2017.\n\n    """"""\n    def __init__(self, mdp_info, policy, learning_rate, off_policy=False,\n                 beta=None, delta=None):\n        """"""\n        Constructor.\n\n        Args:\n            off_policy (bool, False): whether to use the off policy setting or\n                the online one;\n            beta (Parameter, None): beta coefficient;\n            delta (Parameter, None): delta coefficient.\n\n        """"""\n        self.off_policy = off_policy\n        if delta is not None and beta is None:\n            self.delta = delta\n            self.beta = None\n        elif delta is None and beta is not None:\n            self.delta = None\n            self.beta = beta\n        else:\n            raise ValueError(\'delta or beta parameters needed.\')\n\n        self.Q = Table(mdp_info.size)\n        self.Q_tilde = Table(mdp_info.size)\n        self.R_tilde = Table(mdp_info.size)\n\n        self._add_save_attr(\n            off_policy=\'pickle\',\n            delta=\'pickle\',\n            beta=\'pickle\',\n            Q=\'pickle\',\n            Q_tilde=\'pickle\',\n            R_tilde=\'pickle\'\n        )\n\n        super().__init__(mdp_info, policy, self.Q, learning_rate)\n\n    def _update(self, state, action, reward, next_state, absorbing):\n        alpha = self.alpha(state, action, target=reward)\n        self.R_tilde[state, action] += alpha * (reward - self.R_tilde[\n            state, action])\n\n        if not absorbing:\n            q_next = self._next_q(next_state)\n\n            if self.delta is not None:\n                beta = alpha * self.delta(state, action, target=q_next,\n                                          factor=alpha)\n            else:\n                beta = self.beta(state, action, target=q_next)\n\n            self.Q_tilde[state, action] += beta * (q_next - self.Q_tilde[\n                state, action])\n\n        self.Q[state, action] = self.R_tilde[\n            state, action] + self.mdp_info.gamma * self.Q_tilde[state, action]\n\n    def _next_q(self, next_state):\n        """"""\n        Args:\n            next_state (np.ndarray): the state where next action has to be\n                evaluated.\n\n        Returns:\n            The weighted estimator value in \'next_state\'.\n\n        """"""\n        if self.off_policy:\n            return np.max(self.Q[next_state, :])\n        else:\n            self.next_action = self.draw_action(next_state)\n\n            return self.Q[next_state, self.next_action]\n'"
mushroom_rl/algorithms/value/td/sarsa.py,0,"b'from mushroom_rl.algorithms.value.td import TD\nfrom mushroom_rl.utils.table import Table\n\n\nclass SARSA(TD):\n    """"""\n    SARSA algorithm.\n\n    """"""\n    def __init__(self, mdp_info, policy, learning_rate):\n        self.Q = Table(mdp_info.size)\n        self._add_save_attr(Q=\'pickle\')\n\n        super().__init__(mdp_info, policy, self.Q, learning_rate)\n\n    def _update(self, state, action, reward, next_state, absorbing):\n        q_current = self.Q[state, action]\n\n        self.next_action = self.draw_action(next_state)\n        q_next = self.Q[next_state, self.next_action] if not absorbing else 0.\n\n        self.Q[state, action] = q_current + self.alpha(state, action) * (\n            reward + self.mdp_info.gamma * q_next - q_current)\n'"
mushroom_rl/algorithms/value/td/sarsa_lambda.py,0,"b'from mushroom_rl.algorithms.value.td import TD\nfrom mushroom_rl.utils.eligibility_trace import EligibilityTrace\nfrom mushroom_rl.utils.table import Table\n\n\nclass SARSALambda(TD):\n    """"""\n    The SARSA(lambda) algorithm for finite MDPs.\n\n    """"""\n    def __init__(self, mdp_info, policy, learning_rate, lambda_coeff,\n                 trace=\'replacing\'):\n        """"""\n        Constructor.\n\n        Args:\n            lambda_coeff (float): eligibility trace coefficient;\n            trace (str, \'replacing\'): type of eligibility trace to use.\n\n        """"""\n        self.Q = Table(mdp_info.size)\n        self._lambda = lambda_coeff\n\n        self.e = EligibilityTrace(self.Q.shape, trace)\n        self._add_save_attr(\n            Q=\'pickle\',\n            _lambda=\'numpy\',\n            e=\'pickle\'\n        )\n\n        super().__init__(mdp_info, policy, self.Q, learning_rate)\n\n    def _update(self, state, action, reward, next_state, absorbing):\n        q_current = self.Q[state, action]\n\n        self.next_action = self.draw_action(next_state)\n        q_next = self.Q[next_state, self.next_action] if not absorbing else 0.\n\n        delta = reward + self.mdp_info.gamma * q_next - q_current\n        self.e.update(state, action)\n\n        self.Q.table += self.alpha(state, action) * delta * self.e.table\n        self.e.table *= self.mdp_info.gamma * self._lambda\n\n    def episode_start(self):\n        self.e.reset()\n\n        super().episode_start()\n'"
mushroom_rl/algorithms/value/td/sarsa_lambda_continuous.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.value.td import TD\nfrom mushroom_rl.approximators import Regressor\n\n\nclass SARSALambdaContinuous(TD):\n    """"""\n    Continuous version of SARSA(lambda) algorithm.\n\n    """"""\n    def __init__(self, mdp_info, policy, approximator, learning_rate,\n                 lambda_coeff, features, approximator_params=None):\n        """"""\n        Constructor.\n\n        Args:\n            lambda_coeff (float): eligibility trace coefficient.\n\n        """"""\n        self._approximator_params = dict() if approximator_params is None else \\\n            approximator_params\n\n        self.Q = Regressor(approximator, **self._approximator_params)\n        self.e = np.zeros(self.Q.weights_size)\n        self._lambda = lambda_coeff\n\n        self._add_save_attr(\n            _approximator_params=\'pickle\',\n            Q=\'pickle\',\n            _lambda=\'numpy\',\n            e=\'numpy\'\n        )\n\n        super().__init__(mdp_info, policy, self.Q, learning_rate, features)\n\n    def _update(self, state, action, reward, next_state, absorbing):\n        phi_state = self.phi(state)\n        q_current = self.Q.predict(phi_state, action)\n\n        alpha = self.alpha(state, action)\n\n        self.e = self.mdp_info.gamma * self._lambda * self.e + self.Q.diff(\n            phi_state, action)\n\n        self.next_action = self.draw_action(next_state)\n        phi_next_state = self.phi(next_state)\n        q_next = self.Q.predict(phi_next_state,\n                                self.next_action) if not absorbing else 0.\n\n        delta = reward + self.mdp_info.gamma * q_next - q_current\n\n        theta = self.Q.get_weights()\n        theta += alpha * delta * self.e\n        self.Q.set_weights(theta)\n\n    def episode_start(self):\n        self.e = np.zeros(self.Q.weights_size)\n\n        super().episode_start()\n'"
mushroom_rl/algorithms/value/td/speedy_q_learning.py,0,"b'import numpy as np\nfrom copy import deepcopy\n\nfrom mushroom_rl.algorithms.value.td import TD\nfrom mushroom_rl.utils.table import Table\n\n\nclass SpeedyQLearning(TD):\n    """"""\n    Speedy Q-Learning algorithm.\n    ""Speedy Q-Learning"". Ghavamzadeh et. al.. 2011.\n\n    """"""\n    def __init__(self, mdp_info, policy, learning_rate):\n        self.Q = Table(mdp_info.size)\n        self.old_q = deepcopy(self.Q)\n\n        self._add_save_attr(Q=\'pickle\', old_q=\'pickle\')\n\n        super().__init__(mdp_info, policy, self.Q, learning_rate)\n\n    def _update(self, state, action, reward, next_state, absorbing):\n        old_q = deepcopy(self.Q)\n\n        max_q_cur = np.max(self.Q[next_state, :]) if not absorbing else 0.\n        max_q_old = np.max(self.old_q[next_state, :]) if not absorbing else 0.\n\n        target_cur = reward + self.mdp_info.gamma * max_q_cur\n        target_old = reward + self.mdp_info.gamma * max_q_old\n\n        alpha = self.alpha(state, action)\n        q_cur = self.Q[state, action]\n        self.Q[state, action] = q_cur + alpha * (target_old - q_cur) + (\n            1. - alpha) * (target_cur - target_old)\n\n        self.old_q = old_q\n'"
mushroom_rl/algorithms/value/td/td.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.agent import Agent\n\n\nclass TD(Agent):\n    """"""\n    Implements functions to run TD algorithms.\n\n    """"""\n    def __init__(self, mdp_info, policy, approximator, learning_rate,\n                 features=None):\n        """"""\n        Constructor.\n\n        Args:\n            approximator (object): the approximator to use to fit the\n               Q-function;\n            learning_rate (Parameter): the learning rate.\n\n        """"""\n        self.alpha = learning_rate\n\n        policy.set_q(approximator)\n        self.approximator = approximator\n\n        self._add_save_attr(alpha=\'pickle\', approximator=\'pickle\')\n\n        super().__init__(mdp_info, policy, features)\n\n    def fit(self, dataset):\n        assert len(dataset) == 1\n\n        state, action, reward, next_state, absorbing = self._parse(dataset)\n        self._update(state, action, reward, next_state, absorbing)\n\n    @staticmethod\n    def _parse(dataset):\n        """"""\n        Utility to parse the dataset that is supposed to contain only a sample.\n\n        Args:\n            dataset (list): the current episode step.\n\n        Returns:\n            A tuple containing state, action, reward, next state, absorbing and\n            last flag.\n\n        """"""\n        sample = dataset[0]\n        state = sample[0]\n        action = sample[1]\n        reward = sample[2]\n        next_state = sample[3]\n        absorbing = sample[4]\n\n        return state, action, reward, next_state, absorbing\n\n    def _update(self, state, action, reward, next_state, absorbing):\n        """"""\n        Update the Q-table.\n\n        Args:\n            state (np.ndarray): state;\n            action (np.ndarray): action;\n            reward (np.ndarray): reward;\n            next_state (np.ndarray): next state;\n            absorbing (np.ndarray): absorbing flag.\n\n        """"""\n        pass\n'"
mushroom_rl/algorithms/value/td/true_online_sarsa_lambda.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.value.td import TD\nfrom mushroom_rl.approximators import Regressor\nfrom mushroom_rl.approximators.parametric import LinearApproximator\nfrom mushroom_rl.features import get_action_features\n\n\nclass TrueOnlineSARSALambda(TD):\n    """"""\n    True Online SARSA(lambda) with linear function approximation.\n    ""True Online TD(lambda)"". Seijen H. V. et al.. 2014.\n\n    """"""\n    def __init__(self, mdp_info, policy, learning_rate, lambda_coeff,\n                 features, approximator_params=None):\n        """"""\n        Constructor.\n\n        Args:\n            lambda_coeff (float): eligibility trace coefficient.\n\n        """"""\n        self._approximator_params = dict() if approximator_params is None else \\\n            approximator_params\n\n        self.Q = Regressor(LinearApproximator, **self._approximator_params)\n        self.e = np.zeros(self.Q.weights_size)\n        self._lambda = lambda_coeff\n        self._q_old = None\n\n        self._add_save_attr(\n            _approximator_params=\'pickle\',\n            Q=\'pickle\',\n            _q_old=\'pickle\',\n            _lambda=\'numpy\',\n            e=\'numpy\'\n        )\n\n        super().__init__(mdp_info, policy, self.Q, learning_rate, features)\n\n    def _update(self, state, action, reward, next_state, absorbing):\n        phi_state = self.phi(state)\n        phi_state_action = get_action_features(phi_state, action,\n                                               self.mdp_info.action_space.n)\n        q_current = self.Q.predict(phi_state, action)\n\n        if self._q_old is None:\n            self._q_old = q_current\n\n        alpha = self.alpha(state, action)\n\n        e_phi = self.e.dot(phi_state_action)\n        self.e = self.mdp_info.gamma * self._lambda * self.e + alpha * (\n            1. - self.mdp_info.gamma * self._lambda * e_phi) * phi_state_action\n\n        self.next_action = self.draw_action(next_state)\n        phi_next_state = self.phi(next_state)\n        q_next = self.Q.predict(phi_next_state,\n                                self.next_action) if not absorbing else 0.\n\n        delta = reward + self.mdp_info.gamma * q_next - self._q_old\n\n        theta = self.Q.get_weights()\n        theta += delta * self.e + alpha * (\n            self._q_old - q_current) * phi_state_action\n        self.Q.set_weights(theta)\n\n        self._q_old = q_next\n\n    def episode_start(self):\n        self._q_old = None\n        self.e = np.zeros(self.Q.weights_size)\n\n        super().episode_start()\n'"
mushroom_rl/algorithms/value/td/weighted_q_learning.py,0,"b'import numpy as np\n\nfrom mushroom_rl.algorithms.value.td import TD\nfrom mushroom_rl.utils.table import Table\n\n\nclass WeightedQLearning(TD):\n    """"""\n    Weighted Q-Learning algorithm.\n    ""Estimating the Maximum Expected Value through Gaussian Approximation"".\n    D\'Eramo C. et. al.. 2016.\n\n    """"""\n    def __init__(self, mdp_info, policy, learning_rate, sampling=True,\n                 precision=1000):\n        """"""\n        Constructor.\n\n        Args:\n            sampling (bool, True): use the approximated version to speed up\n                the computation;\n            precision (int, 1000): number of samples to use in the approximated\n                version.\n\n        """"""\n        self.Q = Table(mdp_info.size)\n        self._sampling = sampling\n        self._precision = precision\n\n        self._add_save_attr(\n            Q=\'pickle\',\n            _sampling=\'numpy\',\n            _precision=\'numpy\',\n            _n_updates=\'pickle\',\n            _sigma=\'pickle\',\n            _Q=\'pickle\',\n            _Q2=\'pickle\',\n            _weights_var=\'pickle\',\n            _w=\'numpy\'\n        )\n\n        super().__init__(mdp_info, policy, self.Q, learning_rate)\n\n        self._n_updates = Table(mdp_info.size)\n        self._sigma = Table(mdp_info.size, initial_value=1e10)\n        self._Q = Table(mdp_info.size)\n        self._Q2 = Table(mdp_info.size)\n        self._weights_var = Table(mdp_info.size)\n\n    def _update(self, state, action, reward, next_state, absorbing):\n        q_current = self.Q[state, action]\n        q_next = self._next_q(next_state) if not absorbing else 0.\n\n        target = reward + self.mdp_info.gamma * q_next\n\n        alpha = self.alpha(state, action)\n\n        self.Q[state, action] = q_current + alpha * (target - q_current)\n\n        self._n_updates[state, action] += 1\n\n        self._Q[state, action] += (\n            target - self._Q[state, action]) / self._n_updates[state, action]\n        self._Q2[state, action] += (target ** 2. - self._Q2[\n            state, action]) / self._n_updates[state, action]\n        self._weights_var[state, action] = (\n            1 - alpha) ** 2. * self._weights_var[state, action] + alpha ** 2.\n\n        if self._n_updates[state, action] > 1:\n            var = self._n_updates[state, action] * (\n                self._Q2[state, action] - self._Q[state, action] ** 2.) / (\n                self._n_updates[state, action] - 1.)\n            var_estimator = var * self._weights_var[state, action]\n            var_estimator = np.maximum(var_estimator, 1e-10)\n            self._sigma[state, action] = np.sqrt(var_estimator)\n\n    def _next_q(self, next_state):\n        """"""\n        Args:\n            next_state (np.ndarray): the state where next action has to be\n                evaluated.\n\n        Returns:\n            The weighted estimator value in ``next_state``.\n\n        """"""\n        means = self.Q[next_state, :]\n        sigmas = np.zeros(self.Q.shape[-1])\n\n        for a in range(sigmas.size):\n            sigmas[a] = self._sigma[next_state, np.array([a])]\n\n        if self._sampling:\n            samples = np.random.normal(np.repeat([means], self._precision, 0),\n                                       np.repeat([sigmas], self._precision, 0))\n            max_idx = np.argmax(samples, axis=1)\n            max_idx, max_count = np.unique(max_idx, return_counts=True)\n            count = np.zeros(means.size)\n            count[max_idx] = max_count\n\n            self._w = count / self._precision\n        else:\n            raise NotImplementedError\n\n        return np.dot(self._w, means)\n'"
mushroom_rl/environments/mujoco_envs/humanoid_gait/__init__.py,0,"b'from .humanoid_gait import HumanoidGait\nfrom .utils import convert_traj_quat_to_euler,convert_traj_euler_to_quat\nfrom .reward_goals.reward import NoGoalReward, MaxVelocityReward,\\\n    VelocityProfileReward, CompleteTrajectoryReward\nfrom .reward_goals.velocity_profile import *\nfrom .reward_goals.trajectory import HumanoidTrajectory\n'"
mushroom_rl/environments/mujoco_envs/humanoid_gait/humanoid_gait.py,0,"b'import mujoco_py\nfrom pathlib import Path\n\nfrom mushroom_rl.utils import spaces\nfrom mushroom_rl.environments.mujoco import MuJoCo, ObservationType\nfrom mushroom_rl.utils.running_stats import *\n\nfrom ._external_simulation import NoExternalSimulation, MuscleSimulation\nfrom .reward_goals import CompleteTrajectoryReward, VelocityProfileReward, \\\n     MaxVelocityReward, NoGoalReward, HumanoidTrajectory\nfrom mushroom_rl.environments.mujoco_envs.humanoid_gait.utils import quat_to_euler\n\n\nclass HumanoidGait(MuJoCo):\n    """"""\n    Mujoco simulation of a Humanoid Model, based on:\n    ""A deep reinforcement learning based approach towards generating human\n    walking behavior with a neuromuscular model"".\n    Anand, A., Zhao, G., Roth, H., and Seyfarth, A. (2019).\n\n    """"""\n    def __init__(self, gamma=0.99, horizon=2000, n_intermediate_steps=10,\n                 use_muscles=True, goal_reward=None, goal_reward_params=None,\n                 obs_avg_window=1, act_avg_window=1):\n        """"""\n        Constructor.\n\n        Args:\n            gamma (float, 0.99): discount factor for the environment;\n            horizon (int, 2000): horizon for the environment;\n            n_intermediate_steps (int, 10): number of steps to apply the same\n                action to the environment and wait for the next observation;\n            use_muscles (bool): if external muscle simulation should be used\n                for actions. If not apply torques directly to the joints;\n            goal_reward (string, None): type of trajectory used for training\n                Options available:\n                    \'trajectory\'  - Use trajectory in assets/GaitTrajectory.npz\n                                    as reference;\n                    \'vel_profile\' - Velocity goal for the center of mass of the\n                                    model to follow. The goal is given by a\n                                    VelocityProfile instance (or subclass).\n                                    And should be included in the\n                                    ``goal_reward_params``;\n                    \'max_vel\'     - Tries to achieve the maximum possible\n                                    velocity;\n                    None          - Follows no goal(just tries to survive);\n            goal_reward_params (dict, None): params needed for creation goal\n                reward;\n            obs_avg_window (int, 1): size of window used to average\n                observations;\n            act_avg_window (int, 1): size of window used to average actions.\n\n        """"""\n        self.use_muscles = use_muscles\n        self.goal_reward = goal_reward\n        self.act_avg_window = act_avg_window\n        self.obs_avg_window = obs_avg_window\n\n        model_path = Path(__file__).resolve().parent.parent / ""data"" / ""humanoid_gait"" / ""human7segment.xml""\n\n        action_spec = [""right_hip_frontal"", ""right_hip_sagittal"",\n                       ""right_knee"", ""right_ankle"", ""left_hip_frontal"",\n                       ""left_hip_sagittal"", ""left_knee"", ""left_ankle"",\n                       ]\n\n        observation_spec = [(""root"", ObservationType.JOINT_POS),\n                            (""right_hip_frontal"", ObservationType.JOINT_POS),\n                            (""right_hip_sagittal"", ObservationType.JOINT_POS),\n                            (""right_knee"", ObservationType.JOINT_POS),\n                            (""right_ankle"", ObservationType.JOINT_POS),\n                            (""left_hip_frontal"", ObservationType.JOINT_POS),\n                            (""left_hip_sagittal"", ObservationType.JOINT_POS),\n                            (""left_knee"", ObservationType.JOINT_POS),\n                            (""left_ankle"", ObservationType.JOINT_POS),\n\n                            (""root"", ObservationType.JOINT_VEL),\n                            (""right_hip_frontal"", ObservationType.JOINT_VEL),\n                            (""right_hip_sagittal"", ObservationType.JOINT_VEL),\n                            (""right_knee"", ObservationType.JOINT_VEL),\n                            (""right_ankle"", ObservationType.JOINT_VEL),\n                            (""left_hip_frontal"", ObservationType.JOINT_VEL),\n                            (""left_hip_sagittal"", ObservationType.JOINT_VEL),\n                            (""left_knee"", ObservationType.JOINT_VEL),\n                            (""left_ankle"", ObservationType.JOINT_VEL),\n                            ]\n\n        additional_data_spec = []\n\n        collision_groups = [(""floor"", [""floor""]),\n                            (""left_foot"", [""left_foot""]),\n                            (""right_foot"", [""right_foot""])\n                            ]\n\n        super().__init__(model_path.as_posix(), action_spec, observation_spec, gamma=gamma,\n                         horizon=horizon, n_substeps=1,\n                         n_intermediate_steps=n_intermediate_steps,\n                         additional_data_spec=additional_data_spec,\n                         collision_groups=collision_groups)\n\n        if use_muscles:\n            self.external_actuator = MuscleSimulation(self.sim)\n            self.info.action_space = spaces.Box(\n                *self.external_actuator.get_action_space())\n        else:\n            self.external_actuator = NoExternalSimulation()\n\n        low, high = self.info.action_space.low.copy(),\\\n                    self.info.action_space.high.copy()\n        self.norm_act_mean = (high + low) / 2.0\n        self.norm_act_delta = (high - low) / 2.0\n        self.info.action_space.low[:] = -1.0\n        self.info.action_space.high[:] = 1.0\n\n        if goal_reward_params is None:\n            goal_reward_params = dict()\n\n        if goal_reward == ""trajectory"":\n            control_dt = self.sim.model.opt.timestep * self.n_intermediate_steps\n            self.goal_reward = CompleteTrajectoryReward(self.sim, control_dt,\n                                                        **goal_reward_params)\n        elif goal_reward == ""vel_profile"":\n            self.goal_reward = VelocityProfileReward(self.sim, **goal_reward_params)\n        elif goal_reward == ""max_vel"":\n            self.goal_reward = MaxVelocityReward(self.sim, **goal_reward_params)\n        elif goal_reward is None:\n            self.goal_reward = NoGoalReward()\n        else:\n            raise NotImplementedError(""The specified goal reward has not been""\n                                      ""implemented: "", goal_reward)\n\n        if isinstance(self.goal_reward, HumanoidTrajectory):\n            self.reward_weights = dict(live_reward=0.10, goal_reward=0.40,\n                                       traj_vel_reward=0.50,\n                                       move_cost=0.10, fall_cost=0.00)\n        else:\n            self.reward_weights = dict(live_reward=0.10, goal_reward=0.90,\n                                       traj_vel_reward=0.00,\n                                       move_cost=0.10, fall_cost=0.00)\n\n        self.info.observation_space = spaces.Box(*self._get_observation_space())\n\n        self.mean_grf = RunningAveragedWindow(shape=(6,),\n                                              window_size=n_intermediate_steps)\n        self.mean_vel = RunningExpWeightedAverage(shape=(3,), alpha=0.005)\n        self.mean_obs = RunningAveragedWindow(\n            shape=self.info.observation_space.shape,\n            window_size=obs_avg_window\n        )\n        self.mean_act = RunningAveragedWindow(\n            shape=self.info.action_space.shape, window_size=act_avg_window)\n\n    def step(self, action):\n        action = ((action.copy() * self.norm_act_delta) + self.norm_act_mean)\n\n        state, reward, absorbing, info = super().step(action)\n\n        self.mean_obs.update_stats(state)\n        self.mean_vel.update_stats(self.sim.data.qvel[0:3])\n\n        avg_obs = self.mean_obs.mean\n        avg_obs[13:16] = self.mean_vel.mean\n        return avg_obs, reward, absorbing, info\n\n    def render(self):\n        if self.viewer is None:\n            self.viewer = mujoco_py.MjViewer(self.sim)\n            self.viewer._render_every_frame = True\n        self.viewer.render()\n\n    def setup(self):\n        self.goal_reward.reset_state()\n        start_obs = self._reset_model(qpos_noise=0.0, qvel_noise=0.0)\n        start_vel = (\n            self.sim.data.qvel[0:3] if (self.goal_reward is None or isinstance(\n                self.goal_reward, MaxVelocityReward)\n                                      ) else self.goal_reward.get_observation())\n\n        self.mean_vel.reset(start_vel)\n        self.mean_obs.reset(start_obs)\n        self.mean_act.reset()\n        self.external_actuator.reset()\n\n    def reward(self, state, action, next_state):\n        live_reward = 1.0\n\n        goal_reward = self.goal_reward(state, action, next_state)\n\n        traj_vel_reward = 0.0\n        if isinstance(self.goal_reward, HumanoidTrajectory):\n            traj_vel_reward = np.exp(-20.0 * np.square(\n                next_state[13] - next_state[33]))\n\n        move_cost = self.external_actuator.cost(\n            state, action / self.norm_act_delta, next_state)\n\n        fall_cost = 0.0\n        if self._has_fallen(next_state):\n            fall_cost = 1.0\n\n        total_reward = self.reward_weights[""live_reward""] * live_reward \\\n            + self.reward_weights[""goal_reward""] * goal_reward \\\n            + self.reward_weights[""traj_vel_reward""] * traj_vel_reward \\\n            - self.reward_weights[""move_cost""] * move_cost \\\n            - self.reward_weights[""fall_cost""] * fall_cost\n\n        return total_reward\n\n    def is_absorbing(self, state):\n        return (self._has_fallen(state)\n                or self.goal_reward.is_absorbing(state)\n                or self.external_actuator.is_absorbing(state)\n                )\n\n    def _get_observation_space(self):\n        sim_low, sim_high = (self.info.observation_space.low[2:],\n                             self.info.observation_space.high[2:])\n\n        grf_low, grf_high = (-np.ones((6,)) * np.inf,\n                             np.ones((6,)) * np.inf)\n\n        r_low, r_high = self.goal_reward.get_observation_space()\n\n        a_low, a_high = self.external_actuator.get_observation_space()\n\n        return (np.concatenate([sim_low, grf_low, r_low, a_low]),\n                np.concatenate([sim_high, grf_high, r_high, a_high]))\n\n    def _reset_model(self, qpos_noise=0.0, qvel_noise=0.0):\n        self._set_state(self.sim.data.qpos + np.random.uniform(\n            low=-qpos_noise, high=qpos_noise, size=self.sim.model.nq),\n            self.sim.data.qvel + np.random.uniform(low=-qvel_noise,\n                                                   high=qvel_noise,\n                                                   size=self.sim.model.nv)\n        )\n\n        return self._create_observation()\n\n    def _set_state(self, qpos, qvel):\n        old_state = self.sim.get_state()\n        new_state = mujoco_py.MjSimState(old_state.time, qpos, qvel,\n                                         old_state.act, old_state.udd_state)\n        self.sim.set_state(new_state)\n        self.sim.forward()\n\n    @staticmethod\n    def _has_fallen(state):\n        torso_euler = quat_to_euler(state[1:5])\n        return ((state[0] < 0.90) or (state[0] > 1.20)\n                or abs(torso_euler[0]) > np.pi / 12\n                or (torso_euler[1] < -np.pi / 12) or (torso_euler[1] > np.pi / 8)\n                or (torso_euler[2] < -np.pi / 4) or (torso_euler[2] > np.pi / 4)\n                )\n\n    def _create_observation(self):\n        """"""\n        Creates full vector of observations:\n\n        obs[0:13] -> qpos(from mujoco obs)\n        obs[0] -> torso z pos\n        obs[1:5] -> torso quaternion orientation\n        obs[5:13] -> leg joints angle\n\n        obs[13:27] -> qvel(from mujoco obs)\n        obs[13:16] -> torso linear velocity\n        obs[16:19] -> torso angular velocity\n        obs[19:27] -> leg joints angular velocity\n\n        obs[27:30] ->  ground force\n        obs[27:30] -> ground force on right foot(xyz)\n        obs[30:33] -> ground force on left foot(xyz)\n\n        obs[33:33+(len(goal_observation)] -> observations related\n                                             to the goal\n\n        obs[last_obs_id - len(ext_actuator_obs): last_obs_id]\n                -> observations related to the external actuator\n\n        """"""\n        obs = np.concatenate([super(HumanoidGait, self)._create_observation()[2:],\n                              self.mean_grf.mean / 1000.,\n                              self.goal_reward.get_observation(),\n                              self.external_actuator.get_observation()\n                              ]).flatten()\n        return obs\n\n    def _preprocess_action(self, action):\n        action = self.external_actuator.preprocess_action(action)\n        self.mean_act.update_stats(action)\n        return self.mean_act.mean\n\n    def _step_init(self, state, action):\n        self.external_actuator.initialize_internal_states(state, action)\n\n    def _compute_action(self, action):\n        action = self.external_actuator.external_stimulus_to_joint_torques(\n            action\n        )\n\n        return action\n\n    def _simulation_post_step(self):\n        grf = np.concatenate(\n            [self.get_collision_force(""floor"", ""right_foot"")[:3],\n             self.get_collision_force(""floor"", ""left_foot"")[:3]]\n        )\n\n        self.mean_grf.update_stats(grf)\n\n    def _step_finalize(self):\n        self.goal_reward.update_state()\n        self.external_actuator.update_state()\n\n    def _get_body_center_of_mass_pos(self, body_name):\n        return self.sim.data.subtree_com[\n            self.sim.model._body_name2id[body_name]]\n'"
mushroom_rl/environments/mujoco_envs/humanoid_gait/utils.py,0,"b'import numpy as np\nfrom mushroom_rl.utils.angles import euler_to_quat, quat_to_euler\n\n\ndef convert_traj_euler_to_quat(euler_traj, offset=0):\n    """"""\n    Convert humanoid trajectory from euler to quaternion.\n\n    Args:\n        euler_traj (np.ndarray): trajectory with euler angles;\n        offset (int, 0): number of observation to skip.\n\n    Returns:\n        The converted trajectory.\n\n    """"""\n    euler_traj = euler_traj.copy()\n    is_vect = len(euler_traj.shape) < 2\n\n    if is_vect:\n        quat_traj = np.zeros((euler_traj.shape[0] + 1, ))\n        quat_traj[:3-offset] = euler_traj[0:3-offset]\n        quat_traj[3-offset:7-offset] = euler_to_quat(euler_traj[\n                                                     3-offset:6-offset])\n        quat_traj[7-offset:] = euler_traj[6-offset:]\n    else:\n        quat_traj = np.zeros((euler_traj.shape[0] + 1, euler_traj.shape[1]))\n        quat_traj[:3-offset, :] = euler_traj[0:3-offset, :]\n        quat_traj[3-offset:7-offset, :] = euler_to_quat(euler_traj[\n                                                        3-offset:6-offset, :])\n        quat_traj[7-offset:, :] = euler_traj[6-offset:, :]\n\n    return quat_traj\n\n\ndef convert_traj_quat_to_euler(quat_traj, offset=0):\n    """"""\n    Convert humanoid trajectory from quaternion to euler.\n\n    Args:\n        quat_traj (np.ndarray): trajectory with quaternions;\n        offset (int, 0): number of observation to skip.\n\n    Returns:\n        The converted trajectory.\n\n    """"""\n    quat_traj = quat_traj.copy()\n    is_vect = len(quat_traj.shape) < 2\n\n    if is_vect:\n        euler_traj = np.zeros((quat_traj.shape[0] - 1, ))\n        euler_traj[:3-offset] = quat_traj[0:3-offset]\n        euler_traj[3-offset:6-offset] = quat_to_euler(quat_traj[\n                                                      3-offset:7-offset])\n        euler_traj[6-offset:] = quat_traj[7-offset:]\n    else:\n        euler_traj = np.zeros((quat_traj.shape[0] - 1, quat_traj.shape[1]))\n        euler_traj[:3-offset, :] = quat_traj[0:3-offset, :]\n        euler_traj[3-offset:6-offset, :] = quat_to_euler(quat_traj[\n                                                         3-offset:7-offset, :])\n        euler_traj[6-offset:, :] = quat_traj[7-offset:, :]\n\n    return euler_traj\n'"
mushroom_rl/environments/mujoco_envs/humanoid_gait/_external_simulation/__init__.py,0,"b'from .muscle_simulation import NoExternalSimulation, MuscleSimulation\n'"
mushroom_rl/environments/mujoco_envs/humanoid_gait/_external_simulation/human_muscle.py,0,"b'import numpy as np\nfrom .mtc_model import MuscleTendonComplex\n\n# timestep = 1e-3  # 2e-4 -> original value\n\n# general muscle parameters:\n\n# Series elastic element (SE) force-length relationship\neref = 0.04  # [lslack] tendon reference strain\n\n# excitation-contraction coupling\npreAct = 0.01  # 0.01     # [] preactivation\ntau = 0.01  # [s] delay time constant\n\n# contractile element (CE) force-length relationship\nw = 0.56  # [lopt] width\nc = 0.05  # []; remaining force at +/- width\n\n# CE force-velocity relationship\nN = 1.5  # Fmax] eccentric force enhancement\nK = 5.0  # [] shape factor\nstim = 0.0  # initial stimulation\nvce = 0.0\nfrcmtc = 0.0\nl_mtc = 0.0\n\n\ndef HAB(angHipFront, timestep):\n    """"""\n    HAB, hip abductor\n    :param angHipFront:\n    :return:\n    """"""\n    frcmax = 3000.0  # maximum isometric force [N]\n    lopt = 0.09  # optimum fiber length CE [m]\n    vmax = 12.0  # maximum contraction velocity [lopt/s]\n    lslack = 0.07  # tendon slack length [m]\n\n    rHAB = 0.06  # [m]   constant lever contribution\n    phirefHAB = 10 * np.pi / 180  # [rad] reference angle at which MTU length equals\n    rhoHAB = 0.7  # sum of lopt and lslack\n\n    r = np.array((rHAB,))\n    phiref = np.array((phirefHAB,))\n    phimaxref = np.array((0.0,))\n    rho = np.array((rhoHAB,))\n    dirAng = np.array((-1.0,))\n    offsetCorr = np.array((0,))\n    phiScale = np.array((0.0,))\n\n    act = preAct\n    lmtc = l_mtc\n    lce = lopt\n\n    paraMuscle = [frcmax, vmax, eref, lslack, lopt, tau, w, c, N, K]\n    stateMuscle = [stim, act, lmtc, lce, vce, frcmtc]\n    paraMusAttach = [r, phiref, phimaxref, rho, dirAng, phiScale]\n    musHAB = MuscleTendonComplex(paraMuscle=paraMuscle, stateMuscle=stateMuscle,\n                                 paraMusAttach=paraMusAttach,\n                                 offsetCorr=offsetCorr, timestep=timestep, nameMuscle=""HAB"",\n                                 angJoi=np.array((angHipFront,)))\n\n    return musHAB\n\n\ndef HAD(angHipFront, timestep):\n    """"""\n    HAD, hip adductor\n    :param angHipFront:\n    :return:\n    """"""\n    frcmax = 4500.0  # maximum isometric force [N]\n    lopt = 0.10  # optimum fiber length CE [m]\n    vmax = 12.0  # maximum contraction velocity [lopt/s]\n    lslack = 0.18  # tendon slack length [m]\n\n    rHAD = 0.03  # [m]   constant lever contribution\n    phirefHAD = 15 * np.pi / 180  # [rad] reference angle at which MTU length equals\n    rhoHAD = 1.0  # sum of lopt and lslack\n\n    r = np.array((rHAD,))\n    phiref = np.array((phirefHAD,))\n    phimaxref = np.array((0.0,))\n    rho = np.array((rhoHAD,))\n    dirAng = np.array((1.0,))\n    offsetCorr = np.array((0,))\n    phiScale = np.array((0.0,))\n\n    act = preAct\n    lmtc = l_mtc\n    lce = lopt\n\n    paraMuscle = [frcmax, vmax, eref, lslack, lopt, tau, w, c, N, K]\n    stateMuscle = [stim, act, lmtc, lce, vce, frcmtc]\n    paraMusAttach = [r, phiref, phimaxref, rho, dirAng, phiScale]\n    musHAD = MuscleTendonComplex(paraMuscle=paraMuscle, stateMuscle=stateMuscle,\n                                 paraMusAttach=paraMusAttach,\n                                 offsetCorr=offsetCorr, timestep=timestep, nameMuscle=""HAD"",\n                                 angJoi=np.array((angHipFront,)))\n\n    return musHAD\n\n\ndef GLU(angHip, timestep):\n    """"""\n    GLU, gluteus maximus\n    :param angHip: angle between trunk and thigh, rad, it is 180 deg in standing\n    :return:\n    """"""\n    frcmax = 1500.0  # maximum isometric force [N]\n    lopt = 0.11  # optimum fiber length CE [m]\n    vmax = 12.0  # maximum contraction velocity [lopt/s]\n    lslack = 0.13  # tendon slack length [m]\n    # level arm and reference angle\n    r = np.array((0.08,))  # [m]   constant lever contribution\n    phiref = np.array((120 * np.pi / 180,))  # [rad] reference angle at which MTU length equals\n    phimaxref = np.array((0.0, 0.0))\n    rho = np.array((0.5,))  # sum of lopt and lslack\n    dirAng = np.array((-1.0,))  # angle increase leads to MTC length decrease\n    offsetCorr = np.array((0,))  # no level arm correction\n    # typeMuscle = 1  # monoarticular\n    phiScale = np.array((0.0,))\n\n    act = preAct\n    lmtc = l_mtc  # will be computed in the initialization\n    lce = lopt  # will be computed in the initialization\n\n    paraMuscle = [frcmax, vmax, eref, lslack, lopt, tau, w, c, N, K]\n    stateMuscle = [stim, act, lmtc, lce, vce, frcmtc]\n    paraMusAttach = [r, phiref, phimaxref, rho, dirAng, phiScale]\n    musGLU = MuscleTendonComplex(paraMuscle=paraMuscle, stateMuscle=stateMuscle,\n                                 paraMusAttach=paraMusAttach,\n                                 offsetCorr=offsetCorr, timestep=timestep, nameMuscle=""GLU"",\n                                 angJoi=np.array((angHip,)))\n\n    return musGLU\n\n\ndef HFL(angHip, timestep):\n    """"""\n    HFL, hip flexor\n    :param angHip:\n    :return:\n    """"""\n    frcmax = 2000  # maximum isometric force [N]\n    lopt = 0.11  # optimum fiber length CE [m]\n    vmax = 12.0  # maximum contraction velocity [lopt/s]\n    lslack = 0.10  # tendon slack length [m]\n    # level arm and reference angle\n    r = np.array((0.08,))  # [m]   constant lever contribution\n    phiref = np.array((160 * np.pi / 180,))  # [rad] reference angle at which MTU length equals\n    phimaxref = np.array((0.0, 0.0))\n    rho = np.array((0.5,))  # sum of lopt and lslack\n    dirAng = np.array((1.0,))  # angle increase leads to MTC length increase\n    offsetCorr = np.array((0,))  # no level arm correction\n    # typeMuscle = 1  # monoarticular\n    phiScale = np.array((0.0,))\n\n    # act = preAct\n    act = 0.0\n    lmtc = l_mtc  # should be computed based on the joint angle and joint geometry\n    lce = lopt\n\n    paraMuscle = [frcmax, vmax, eref, lslack, lopt, tau, w, c, N, K]\n    stateMuscle = [stim, act, lmtc, lce, vce, frcmtc]\n    paraMusAttach = [r, phiref, phimaxref, rho, dirAng, phiScale]\n    musHFL = MuscleTendonComplex(paraMuscle=paraMuscle, stateMuscle=stateMuscle,\n                                 paraMusAttach=paraMusAttach,\n                                 offsetCorr=offsetCorr, timestep=timestep, nameMuscle=""HFL"",\n                                 angJoi=np.array((angHip,)))\n\n    return musHFL\n\n\ndef HAM(angHip, angKne, timestep):\n    """"""\n    HAM, hamstring\n    :param angHip:\n    :param angKne:\n    :return:\n    """"""\n    frcmax = 3000  # maximum isometric force [N]\n    lopt = 0.10  # optimum fiber length CE [m]\n    vmax = 12.0  # maximum contraction velocity [lopt/s]\n    lslack = 0.31  # tendon slack length [m]\n    # hamstring hip level arm and refernce angle\n    rHAMh = 0.08  # [m]   constant lever contribution\n    phirefHAMh = 150 * np.pi / 180  # [rad] reference angle at which MTU length equals\n    rhoHAMh = 0.5  # sum of lopt and lslack\n    # hamstring knee level arm and reference angle\n    rHAMk = 0.05  # [m]   constant lever contribution\n    phirefHAMk = 180 * np.pi / 180  # [rad] reference angle at which MTU length equals\n    rhoHAMk = 0.5  # sum of lopt and lslack\n\n    r = np.array((rHAMh, rHAMk))\n    phiref = np.array((phirefHAMh, phirefHAMk))\n    phimaxref = np.array((0.0, 0.0))\n    rho = np.array((rhoHAMh, rhoHAMk))\n    dirAng = np.array((-1.0, 1.0))\n    offsetCorr = np.array((0, 0))\n    # typeMuscle = 2\n    phiScale = np.array((0.0, 0.0))\n\n    act = preAct\n    lmtc = l_mtc\n    lce = lopt\n\n    paraMuscle = [frcmax, vmax, eref, lslack, lopt, tau, w, c, N, K]\n    stateMuscle = [stim, act, lmtc, lce, vce, frcmtc]\n    paraMusAttach = [r, phiref, phimaxref, rho, dirAng, phiScale]\n    musHAM = MuscleTendonComplex(paraMuscle=paraMuscle, stateMuscle=stateMuscle,\n                                 paraMusAttach=paraMusAttach,\n                                 offsetCorr=offsetCorr, timestep=timestep, nameMuscle=""HAM"",\n                                 angJoi=np.array((angHip, angKne)))\n\n    return musHAM\n\n\ndef REF(angHip, angKne, timestep):\n    """"""\n    REF, rectus femoris\n    :param angKne:\n    :param angHip:\n    :return:\n    """"""\n    frcmax = 1200  # maximum isometric force [N]\n    lopt = 0.08  # optimum fiber length CE [m]\n    vmax = 12.0  # maximum contraction velocity [lopt/s]\n    lslack = 0.35  # tendon slack length [m]\n    # REF group attachement (hip)\n    rREFh = 0.08  # [m]   constant lever contribution\n    phirefREFh = 170 * np.pi / 180  # [rad] reference angle at which MTU length equals\n    rhoREFh = 0.3  # sum of lopt and lslack\n    # REF group attachment (knee)\n    rREFkmax = 0.06  # [m]   maximum lever contribution\n    rREFkmin = 0.04  # [m]   minimum lever contribution\n    phimaxREFk = 165 * np.pi / 180  # [rad] angle of maximum lever contribution\n    phiminREFk = 45 * np.pi / 180  # [rad] angle of minimum lever contribution\n    phirefREFk = 125 * np.pi / 180  # [rad] reference angle at which MTU length equals\n    rhoREFk = 0.5  # sum of lopt and lslack\n    phiScaleREFk = np.arccos(rREFkmin / rREFkmax) / (phiminREFk - phimaxREFk)\n\n    r = np.array((rREFh, rREFkmax))\n    phiref = np.array((phirefREFh, phirefREFk))\n    phimaxref = np.array((0.0, phimaxREFk))\n    rho = np.array((rhoREFh, rhoREFk))\n    dirAng = np.array((1.0, -1.0))\n    offsetCorr = np.array((0, 1))\n    phiScale = np.array((0.0, phiScaleREFk))\n\n    act = preAct\n    lmtc = l_mtc\n    lce = lopt\n\n    paraMuscle = [frcmax, vmax, eref, lslack, lopt, tau, w, c, N, K]\n    stateMuscle = [stim, act, lmtc, lce, vce, frcmtc]\n    paraMusAttach = [r, phiref, phimaxref, rho, dirAng, phiScale]\n    musREF = MuscleTendonComplex(paraMuscle=paraMuscle, stateMuscle=stateMuscle,\n                                 paraMusAttach=paraMusAttach,\n                                 offsetCorr=offsetCorr, timestep=timestep, nameMuscle=""REF"",\n                                 angJoi=np.array((angHip, angKne)))\n\n    return musREF\n\n\ndef VAS(angKne, timestep):\n    """"""\n    VAS, vastus muscle\n    :param angKne:\n    :return:\n    """"""\n    frcmax = 6000  # maximum isometric force [N]\n    lopt = 0.08  # optimum fiber length CE [m]\n    vmax = 12.0  # maximum contraction velocity [lopt/s]\n    lslack = 0.23  # tendon slack length [m]\n    # VAS group attachment\n    rVASmax = 0.06  # [m]   maximum lever contribution\n    rVASmin = 0.04  # [m]   minimum lever contribution\n    phimaxVAS = 165 * np.pi / 180  # [rad] angle of maximum lever contribution\n    phiminVAS = 45 * np.pi / 180  # [rad] angle of minimum lever contribution\n    phirefVAS = 120 * np.pi / 180  # [rad] reference angle at which MTU length equals\n    rhoVAS = 0.6  # sum of lopt and lslack\n    phiScaleVAS = np.arccos(rVASmin / rVASmax) / (phiminVAS - phimaxVAS)\n\n    r = np.array((rVASmax,))\n    phiref = np.array((phirefVAS,))\n    phimaxref = np.array((phimaxVAS,))\n    rho = np.array((rhoVAS,))\n    dirAng = np.array((-1.0,))\n    offsetCorr = np.array((1,))\n    phiScale = np.array((phiScaleVAS,))\n\n    act = preAct\n    lmtc = l_mtc\n    lce = lopt\n\n    paraMuscle = [frcmax, vmax, eref, lslack, lopt, tau, w, c, N, K]\n    stateMuscle = [stim, act, lmtc, lce, vce, frcmtc]\n    paraMusAttach = [r, phiref, phimaxref, rho, dirAng, phiScale]\n    musVAS = MuscleTendonComplex(paraMuscle=paraMuscle, stateMuscle=stateMuscle,\n                                 paraMusAttach=paraMusAttach,\n                                 offsetCorr=offsetCorr, timestep=timestep, nameMuscle=""VAS"",\n                                 angJoi=np.array((angKne,)))\n\n    return musVAS\n\n\ndef BFSH(angKne, timestep):\n    """"""\n    BFSH, biceps femoris short head muscle\n    :param angKne:\n    :return:\n    """"""\n    frcmax = 350  # maximum isometric force [N]\n    lopt = 0.12  # optimum fiber length CE [m]\n    vmax = 12.0  # 6 # maximum contraction velocity [lopt/s]\n    lslack = 0.10  # tendon slack length [m]\n\n    # BFSH group attachment\n    rBFSH = 0.04  # [m]   constant lever contribution\n    phirefBFSH = 160 * np.pi / 180  # [rad] reference angle at which MTU length equals\n    rhoBFSH = 0.7  # sum of lopt and lslack\n\n    r = np.array((rBFSH,))\n    phiref = np.array((phirefBFSH,))\n    phimaxref = np.array((0.0,))\n    rho = np.array((rhoBFSH,))\n    dirAng = np.array((1.0,))\n    offsetCorr = np.array((0,))\n    phiScale = np.array((0.0,))\n\n    act = preAct\n    lmtc = l_mtc\n    lce = lopt\n\n    paraMuscle = [frcmax, vmax, eref, lslack, lopt, tau, w, c, N, K]\n    stateMuscle = [stim, act, lmtc, lce, vce, frcmtc]\n    paraMusAttach = [r, phiref, phimaxref, rho, dirAng, phiScale]\n    musBFSH = MuscleTendonComplex(paraMuscle=paraMuscle, stateMuscle=stateMuscle,\n                                  paraMusAttach=paraMusAttach,\n                                  offsetCorr=offsetCorr, timestep=timestep, nameMuscle=""BFSH"",\n                                  angJoi=np.array((angKne,)))\n\n    return musBFSH\n\n\ndef GAS(angKne, angAnk, timestep):\n    """"""\n    GAS, gastronemius msucle\n    :param angKne:\n    :param angAnk:\n    :return:\n    """"""\n    frcmax = 1500  # maximum isometric force [N]\n    lopt = 0.05  # optimum fiber length CE [m]\n    vmax = 12.0  # maximum contraction velocity [lopt/s]\n    lslack = 0.40  # tendon slack length [m]\n    # GAStrocnemius attachment (knee joint)\n    rGASkmax = 0.05  # [m]   maximum lever contribution\n    rGASkmin = 0.02  # [m]   minimum lever contribution\n    phimaxGASk = 140 * np.pi / 180  # [rad] angle of maximum lever contribution\n    phiminGASk = 45 * np.pi / 180  # [rad] angle of minimum lever contribution\n    phirefGASk = 165 * np.pi / 180  # [rad] reference angle at which MTU length equals\n    rhoGASk = 0.7  # sum of lopt and lslack\n    # rhoGASk     = 0.045           #       sum of lopt and lslack\n    phiScaleGASk = np.arccos(rGASkmin / rGASkmax) / (phiminGASk - phimaxGASk)\n    # GAStrocnemius attachment (ankle joint)\n    rGASamax = 0.06  # [m]   maximum lever contribution\n    rGASamin = 0.02  # [m]   minimum lever contribution\n    phimaxGASa = 100 * np.pi / 180  # [rad] angle of maximum lever contribution\n    phiminGASa = 180 * np.pi / 180  # [rad] angle of minimum lever contribution\n    phirefGASa = 80 * np.pi / 180  # [rad] reference angle at which MTU length equals\n    rhoGASa = 0.7  # sum of lopt and lslack\n    # rhoGASa     =        0.045    #       sum of lopt and lslack\n    phiScaleGASa = np.arccos(rGASamin / rGASamax) / (phiminGASa - phimaxGASa)\n\n    r = np.array((rGASkmax, rGASamax))\n    phiref = np.array((phirefGASk, phirefGASa))\n    phimaxref = np.array((phimaxGASk, phimaxGASa))\n    rho = np.array((rhoGASk, rhoGASa))\n    dirAng = np.array((1.0, -1.0))\n    offsetCorr = np.array((1, 1))\n    phiScale = np.array((phiScaleGASk, phiScaleGASa))\n\n    act = preAct\n    lmtc = l_mtc\n    lce = lopt\n\n    paraMuscle = [frcmax, vmax, eref, lslack, lopt, tau, w, c, N, K]\n    stateMuscle = [stim, act, lmtc, lce, vce, frcmtc]\n    paraMusAttach = [r, phiref, phimaxref, rho, dirAng, phiScale]\n    musGAS = MuscleTendonComplex(paraMuscle=paraMuscle, stateMuscle=stateMuscle,\n                                 paraMusAttach=paraMusAttach,\n                                 offsetCorr=offsetCorr, timestep=timestep, nameMuscle=""GAS"",\n                                 angJoi=np.array((angKne, angAnk)))\n\n    return musGAS\n\n\ndef SOL(angAnk, timestep):\n    """"""\n    SOL, soleus muscle\n    :param angAnk:\n    :return:\n    """"""\n    frcmax = 4000  # maximum isometric force [N]\n    lopt = 0.04  # optimum fiber length CE [m]\n    vmax = 6.0  # maximum contraction velocity [lopt/s]\n    lslack = 0.26  # tendon slack length [m]\n    # SOLeus attachment\n    rSOLmax = 0.06  # [m]   maximum lever contribution\n    rSOLmin = 0.02  # [m]   minimum lever contribution\n    phimaxSOL = 100 * np.pi / 180  # [rad] angle of maximum lever contribution\n    phiminSOL = 180 * np.pi / 180  # [rad] angle of minimum lever contribution\n    phirefSOL = 90 * np.pi / 180  # [rad] reference angle at which MTU length equals\n    rhoSOL = 0.5  # sum of lopt and lslack\n    phiScaleSOL = np.arccos(rSOLmin / rSOLmax) / (phiminSOL - phimaxSOL)\n\n    r = np.array((rSOLmax,))\n    phiref = np.array((phirefSOL,))\n    phimaxref = np.array((phimaxSOL,))\n    rho = np.array((rhoSOL,))\n    dirAng = np.array((-1.0,))\n    offsetCorr = np.array((1,))\n    phiScale = np.array((phiScaleSOL,))\n\n    act = preAct\n    lmtc = l_mtc\n    lce = lopt\n\n    paraMuscle = [frcmax, vmax, eref, lslack, lopt, tau, w, c, N, K]\n    stateMuscle = [stim, act, lmtc, lce, vce, frcmtc]\n    paraMusAttach = [r, phiref, phimaxref, rho, dirAng, phiScale]\n    musSOL = MuscleTendonComplex(paraMuscle=paraMuscle, stateMuscle=stateMuscle,\n                                 paraMusAttach=paraMusAttach,\n                                 offsetCorr=offsetCorr, timestep=timestep, nameMuscle=""SOL"",\n                                 angJoi=np.array((angAnk,)))\n\n    return musSOL\n\n\ndef TIA(angAnk, timestep):\n    """"""\n    TIA, tibialis anterior muscle\n    :param angAnk:\n    :return:\n    """"""\n    frcmax = 800  # maximum isometric force [N]\n    lopt = 0.06  # optimum fiber length CE [m]\n    vmax = 12.0  # maximum contraction velocity [lopt/s]\n    lslack = 0.24  # tendon slack length [m]\n    # Tibialis Anterior attachment\n    rTIAmax = 0.04  # [m]   maximum lever contribution\n    rTIAmin = 0.01  # [m]   minimum lever contribution\n    phimaxTIA = 80 * np.pi / 180  # [rad] angle of maximum lever contribution\n    phiminTIA = 180 * np.pi / 180  # [rad] angle of minimum lever contribution\n    phirefTIA = 110 * np.pi / 180  # [rad] reference angle at which MTU length equals\n    phiScaleTIA = np.arccos(rTIAmin / rTIAmax) / (phiminTIA - phimaxTIA)\n    rhoTIA = 0.7\n\n    r = np.array((rTIAmax,))\n    phiref = np.array((phirefTIA,))\n    phimaxref = np.array((phimaxTIA,))\n    rho = np.array((rhoTIA,))\n    dirAng = np.array((1.0,))\n    offsetCorr = np.array((1,))\n    phiScale = np.array((phiScaleTIA,))\n\n    act = preAct\n    lmtc = l_mtc\n    lce = lopt\n\n    paraMuscle = [frcmax, vmax, eref, lslack, lopt, tau, w, c, N, K]\n    stateMuscle = [stim, act, lmtc, lce, vce, frcmtc]\n    paraMusAttach = [r, phiref, phimaxref, rho, dirAng, phiScale]\n    musTIA = MuscleTendonComplex(paraMuscle=paraMuscle, stateMuscle=stateMuscle,\n                                 paraMusAttach=paraMusAttach,\n                                 offsetCorr=offsetCorr, timestep=timestep, nameMuscle=""TIA"",\n                                 angJoi=np.array((angAnk,)))\n\n    return musTIA\n'"
mushroom_rl/environments/mujoco_envs/humanoid_gait/_external_simulation/mtc_model.py,0,"b'import numpy as np\n\n""""""\nIt is created for using MTC in Mujoco. The dynamics in this model is not continuous. The integration error will be\naccumulated overtime. And the system might get unstable if the timestep is too large. It is recommended to set the\ntimestamp lower than 5e-4 to get decent results.\n\nThe model is created based on Song\'s and Geyer\'s 2015 paper:\nSong, S. and Geyer, H., 2015. A neural circuitry that emphasizes spinal feedback generates diverse behaviours of human\nlocomotion. The Journal of physiology, 593(16), pp.3493-3511.\n\nV0.1\nPassed basic tests. There\'re slightly difference compared to the simmechanics model.\n\nV0.2\n1. Verified with the simmechanics model. Difference in most of the cases can be ignored.\n2. Changed the integration method from forward Euler to trapezoid.\n3. Muscle force vce etc might vibrate/jitter if in some cases if the timestep is not low enough.\n   Need to improve this in the next version.\n   \n""""""\n\nclass MuscleTendonComplex:\n    def __init__(self, paraMuscle, stateMuscle, paraMusAttach, offsetCorr, timestep, nameMuscle, angJoi):\n        self.frcmax, self.vmax, self.eref, self.lslack, self.lopt, self.tau, self.w, self.c, self.N, self.K = paraMuscle\n        self.stim, self.act, self.lmtc, self.lce, self.vce, self.frcmtc = stateMuscle\n        self.timestep = timestep\n        self.nameMuscle = nameMuscle\n        self.angJoi = angJoi\n        self.offsetCorr = offsetCorr\n        self.r, self.phiref, self.phimaxref, self.rho, self.dirAng, self.phiScale = paraMusAttach\n        self.MR =  0.01\n        self.typeMuscle = self.angJoi.size\n        nJoi = self.typeMuscle\n        self.levelArm = np.zeros(nJoi)\n\n        tmpL = np.zeros(nJoi)\n        for i in range(0, nJoi):\n            if self.offsetCorr[i] == 0:\n                tmpL[i] = self.dirAng[i] * (self.angJoi[i] - self.phiref[i]) * self.r[i] * self.rho[i]\n                self.levelArm[i] = self.r[i]\n            elif self.offsetCorr[i] == 1:\n                tmp1 = np.sin((self.phiref[i] - self.phimaxref[i]) * self.phiScale[i])\n                tmp2 = np.sin((self.angJoi[i] - self.phimaxref[i]) * self.phiScale[i])\n                tmpL[i] = self.dirAng[i] * (tmp2 - tmp1) * self.r[i] * self.rho[i] / self.phiScale[i]\n                self.levelArm[i] = np.cos((self.angJoi[i] - self.phimaxref[i]) * self.phiScale[i]) * self.r[i]\n            else:\n                raise ValueError(\'Invalid muscle level arm offset correction type. \')\n        self.lmtc = self.lslack + self.lopt + np.sum(tmpL)\n\n        self.lce = self.lmtc - self.lslack\n        self.lse = self.lmtc - self.lce\n        # unitless parameters\n        self.Lse = self.lse / self.lslack\n        self.Lce = self.lce / self.lopt\n\n        self.actsubstep = (self.stim - self.act) * self.timestep / 2.0 / self.tau + self.act\n        self.lcesubstep = self.vce * self.timestep / 2.0 + self.lce\n\n        # test\n        self.lce_avg = self.lce\n        self.vce_avg = self.vce\n        self.frcmtc_avg = 0\n        self.act_avg = self.act\n        self.frame = 0\n        # self.Fse = 0.0\n        # self.Fbe = 0.0\n        # self.Fpe = 0.0\n        # self.Fce = 0.0\n\n\n    def stepUpdateState(self, angJoi):\n        """"""\n        Muscle Tendon Complex Dynamics\n        update muscle states based on the muscle dynamics\n        Muscle state stim has to be updated outside before this function is called\n        """"""\n        # update lmtc and level arm based on the geometry\n        self.angJoi = angJoi\n        nJoi = self.typeMuscle\n        tmpL = np.zeros(nJoi)\n        for i in range(0, nJoi):\n            if self.offsetCorr[i] == 0:\n                tmpL[i] = self.dirAng[i] * (self.angJoi[i] - self.phiref[i]) * self.r[i] * self.rho[i]\n                self.levelArm[i] = self.r[i]\n            elif self.offsetCorr[i] == 1:\n                tmp1 = np.sin((self.phiref[i] - self.phimaxref[i]) * self.phiScale[i])\n                tmp2 = np.sin((self.angJoi[i] - self.phimaxref[i]) * self.phiScale[i])\n                tmpL[i] = self.dirAng[i] * (tmp2 - tmp1) * self.r[i] * self.rho[i] / self.phiScale[i]\n                self.levelArm[i] = np.cos((self.angJoi[i] - self.phimaxref[i]) * self.phiScale[i]) * self.r[i]\n            else:\n                raise ValueError(\'Invalid muscle level arm offset correction type. \')\n        self.lmtc = self.lslack + self.lopt + np.sum(tmpL)\n\n        # update muscle activation\n        # integration, forward-Euler method\n        # self.act = (self.stim - self.act) * self.timestep / self.tau + self.act\n        # integration, trapezoidal method, 2-step\n        self.act = (self.stim - self.actsubstep) * self.timestep / 2.0 / self.tau + self.actsubstep\n        self.actsubstep = (self.stim - self.act) * self.timestep / 2.0 / self.tau + self.act\n\n        # update lce and lse based on the lmtc\n        # integration, forward-Euler method\n        # self.lce = self.vce * self.timestep + self.lce\n        # integration, trapezoidal method, 2-step\n        self.lce = self.vce * self.timestep / 2.0 + self.lcesubstep\n        self.lcesubstep = self.vce * self.timestep / 2.0 + self.lce\n\n        self.lse = self.lmtc - self.lce\n        self.Lse = self.lse / self.lslack\n        self.Lce = self.lce / self.lopt\n\n        # Serial Elastic element (tendon) force-length relationship\n        if self.Lse > 1.0:\n            Fse = np.power((self.Lse - 1.0) / self.eref, 2)\n        else:\n            Fse = 0.0\n\n        # Parallel Elasticity PE\n        if self.Lce > 1.0:\n            Fpe = np.power((self.Lce - 1.0) / self.w, 2)\n        else:\n            Fpe = 0.0\n\n        # update frcmtc\n        self.frcmtc = Fse * self.frcmax\n        #self.frcmtc =  np.clip(self.frcmtc, 0, self.frcmax)\n\n        # Buffer Elasticity BE\n        if (self.Lce - (1.0 - self.w)) < 0:\n            Fbe = np.power((self.Lce - (1.0 - self.w)) / (self.w / 2), 2)\n        else:\n            Fbe = 0.0\n\n        # Contractile Element force-length relationship\n        tmp = np.power(np.absolute(self.Lce - 1.0) / self.w, 3)\n        Fce = np.exp(tmp * np.log(self.c))\n\n        #Fv = (Fse + Fbe) / (Fpe + Fce * self.act)\n        if (Fpe + Fce * self.act) < 1e-10:  # avoid numerical error\n            if (Fse + Fbe) < 1e-10:\n                Fv = 1.0\n            else:\n                Fv = (Fse + Fbe) / 1e-10\n        else:\n            Fv = (Fse + Fbe) / (Fpe + Fce * self.act)\n\n        # Contractile Element inverse force-velocity relationship\n        if Fv <= 1.0:\n            # Concentric\n            v = (Fv - 1) / (Fv * self.K + 1.0)\n        elif Fv <= self.N:\n            # excentric\n            tmp = (Fv - self.N) / (self.N - 1.0)\n            v = (tmp + 1.0) / (1.0 - tmp * 7.56 * self.K)\n        else:\n            # excentric overshoot\n            v = ((Fv - self.N) * 0.01 + 1)\n\n        self.vce = v * self.lopt * self.vmax\n        v_frac = self.vce /  self.vmax\n        mr_scale =  self.act * np.absolute(self.frcmax*self.vmax) *self.timestep\n        if self.vce <= 1:\n            self.MR =  0.01 - 0.11*(v_frac) + 0.06*np.exp(-8*v_frac)\n        else:\n            self.MR =  0.23 - 0.16*np.exp(-8*v_frac) \n        self.MR *= mr_scale\n        self.frame += 1\n        self.lce_avg = (self.lce_avg*(self.frame - 1) +  self.lce) / self.frame\n        self.vce_avg = (self.vce_avg*(self.frame - 1) +  self.vce) / self.frame\n        self.frcmtc_avg = (self.frcmtc_avg*(self.frame - 1) +  self.frcmtc) / self.frame\n        self.act_avg = (self.act_avg*(self.frame - 1) +  self.act) / self.frame\n        #self.MR = np.exp(-self.MR)\n        # print(self.MR, np.exp(-self.MR))\n        # self.Fv = Fv\n        # self.Fse = Fse\n        # self.Fbe = Fbe\n        # self.Fpe = Fpe\n        # self.Fce = Fce\n\n    def reset_state(self):\n        self.frame = 0\n        self.lce_avg = 0\n        self.frcmtc_avg = 0\n        self.act_avg = 0\n        self.vce_avg = 0\n\n'"
mushroom_rl/environments/mujoco_envs/humanoid_gait/_external_simulation/muscle_simulation.py,0,"b'import numpy as np\nfrom .human_muscle import HAB, HAM, HAD, HFL, BFSH, GAS, GLU, REF, VAS, SOL, TIA\n\n\nclass NoExternalSimulation(object):\n    def get_action_space(self):\n        return np.array([]), np.array([])\n\n    def get_observation_space(self):\n        return np.array([]), np.array([])\n\n    def cost(self, state, action, next_state):\n        move_cost = np.sum(np.square(action))\n        return move_cost\n\n    def is_absorbing(self, state):\n        return False\n\n    def get_observation(self):\n        return np.array([])\n\n    def update_state(self):\n        pass\n\n    def reset(self):\n        pass\n\n    def preprocess_action(self, action):\n        return action\n\n    def initialize_internal_states(self, state, action):\n        pass\n\n    def external_stimulus_to_joint_torques(self, stimu):\n        return stimu\n\n\nclass MuscleSimulation(object):\n    def __init__(self, sim):\n        self.sim = sim\n        self.musc = self._create_muscles()  # container with all muscle objects {muscle name: muscle object}\n        self.nmusc = len(self.musc)\n\n        self._metcost = 0  # metabolic cost of all muscles for an action\n        self.reset()\n\n    def get_action_space(self):\n        high = np.ones(self.nmusc)\n        low = np.ones(self.nmusc) * 0.001\n        return low, high\n\n    def get_observation_space(self):\n        obs = self.get_observation()\n        high = np.inf * np.ones(len(obs))\n        low = -np.inf * np.ones(len(obs))\n        return low, high\n\n    def cost(self, state, action, next_state):\n        # returns cost value between [0, 1]\n        reward = 1 - (5 * self._metcost)\n        return reward\n\n    def is_absorbing(self, state):\n        return False\n\n    def update_state(self):\n        self._metcost = np.exp(-self._metcost / 50)\n\n    def get_observation(self):\n        musc_obs = np.zeros(self.nmusc * 4)\n        for m, musc in enumerate(self.musc.values()):\n            musc_obs[m + (0 * self.nmusc)] = musc.lce\n            musc_obs[m + (1 * self.nmusc)] = musc.vce\n            musc_obs[m + (2 * self.nmusc)] = musc.frcmtc / 1000.0\n            musc_obs[m + (3 * self.nmusc)] = musc.act\n        return musc_obs\n\n    def _create_muscles(self):\n        angHipFroR, angHipSagR, angKneR, angAnkR, \\\n        angHipFroL, angHipSagL, angKneL, angAnkL = self.sim.data.qpos[7:15]\n\n        angHipAbdR = -angHipFroR\n        angHipAbdL = angHipFroL\n        angHipSagR = angHipSagR + np.pi\n        angHipSagL = angHipSagL + np.pi\n        angKneR = np.pi - angKneR\n        angKneL = np.pi - angKneL\n        angAnkR = angAnkR + np.pi / 2.0\n        angAnkL = angAnkL + np.pi / 2.0\n\n        timestep = self.sim.model.opt.timestep\n        musc = {""HABR"":  HAB(angHipAbdR, timestep),\n                ""HADR"":  HAD(angHipAbdR, timestep),\n                ""GLUR"":  GLU(angHipSagR, timestep),\n                ""HFLR"":  HFL(angHipSagR, timestep),\n                ""HAMR"":  HAM(angHipSagR, angKneR, timestep),\n                ""REFR"":  REF(angHipSagR, angKneR, timestep),\n                ""BFSHR"": BFSH(angKneR, timestep),\n                ""VASR"":  VAS(angKneR, timestep),\n                ""GASR"":  GAS(angKneR, angAnkR, timestep),\n                ""SOLR"":  SOL(angAnkR, timestep),\n                ""TIAR"":  TIA(angAnkR, timestep),\n                ""HABL"":  HAB(angHipAbdL, timestep),\n                ""HADL"":  HAD(angHipAbdL, timestep),\n                ""GLUL"":  GLU(angHipSagL, timestep),\n                ""HFLL"":  HFL(angHipSagL, timestep),\n                ""HAML"":  HAM(angHipSagL, angKneL, timestep),\n                ""REFL"":  REF(angHipSagL, angKneL, timestep),\n                ""BFSHL"": BFSH(angKneL, timestep),\n                ""VASL"":  VAS(angKneL, timestep),\n                ""GASL"":  GAS(angKneL, angAnkL, timestep),\n                ""SOLL"":  SOL(angAnkL, timestep),\n                ""TIAL"":  TIA(angAnkL, timestep),\n                }\n        return musc\n\n    def reset(self):\n        self.musc = self._create_muscles()\n\n    def preprocess_action(self, stimu):\n        return np.clip(stimu, 0.001, 1.0)\n\n    def initialize_internal_states(self, state, stimu):\n        self._metcost = 0\n        for i, musc in enumerate(self.musc.values()):\n            musc.reset_state()\n\n        self.musc[""HABR""].stim = stimu[0]\n        self.musc[""HADR""].stim = stimu[1]\n        self.musc[""HFLR""].stim = stimu[2]\n        self.musc[""GLUR""].stim = stimu[3]\n        self.musc[""HAMR""].stim = stimu[4]\n        self.musc[""REFR""].stim = stimu[5]\n        self.musc[""VASR""].stim = stimu[6]\n        self.musc[""BFSHR""].stim = stimu[7]\n        self.musc[""GASR""].stim = stimu[8]\n        self.musc[""SOLR""].stim = stimu[9]\n        self.musc[""TIAR""].stim = stimu[10]\n        self.musc[""HABL""].stim = stimu[11]\n        self.musc[""HADL""].stim = stimu[12]\n        self.musc[""HFLL""].stim = stimu[13]\n        self.musc[""GLUL""].stim = stimu[14]\n        self.musc[""HAML""].stim = stimu[15]\n        self.musc[""REFL""].stim = stimu[16]\n        self.musc[""VASL""].stim = stimu[17]\n        self.musc[""BFSHL""].stim = stimu[18]\n        self.musc[""GASL""].stim = stimu[19]\n        self.musc[""SOLL""].stim = stimu[20]\n        self.musc[""TIAL""].stim = stimu[21]\n\n    def external_stimulus_to_joint_torques(self, stimu):\n        angHipFroR, angHipSagR, angKneR, angAnkR, \\\n        angHipFroL, angHipSagL, angKneL, angAnkL = self.sim.data.qpos[7:15]\n\n        angHipAbdR = -angHipFroR\n        angHipAbdL = angHipFroL\n        angHipSagR = angHipSagR + np.pi\n        angHipSagL = angHipSagL + np.pi\n        angKneR = np.pi - angKneR\n        angKneL = np.pi - angKneL\n        angAnkR = angAnkR + np.pi / 2.0\n        angAnkL = angAnkL + np.pi / 2.0\n\n        self.musc[""HABR""].stepUpdateState(np.array((angHipAbdR,)))\n        self.musc[""HADR""].stepUpdateState(np.array((angHipAbdR,)))\n        self.musc[""GLUR""].stepUpdateState(np.array((angHipSagR,)))\n        self.musc[""HFLR""].stepUpdateState(np.array((angHipSagR,)))\n        self.musc[""HAMR""].stepUpdateState(np.array((angHipSagR, angKneR)))\n        self.musc[""REFR""].stepUpdateState(np.array((angHipSagR, angKneR)))\n        self.musc[""BFSHR""].stepUpdateState(np.array((angKneR,)))\n        self.musc[""VASR""].stepUpdateState(np.array((angKneR,)))\n        self.musc[""GASR""].stepUpdateState(np.array((angKneR, angAnkR)))\n        self.musc[""SOLR""].stepUpdateState(np.array((angAnkR,)))\n        self.musc[""TIAR""].stepUpdateState(np.array((angAnkR,)))\n        self.musc[""HABL""].stepUpdateState(np.array((angHipAbdL,)))\n        self.musc[""HADL""].stepUpdateState(np.array((angHipAbdL,)))\n        self.musc[""GLUL""].stepUpdateState(np.array((angHipSagL,)))\n        self.musc[""HFLL""].stepUpdateState(np.array((angHipSagL,)))\n        self.musc[""HAML""].stepUpdateState(np.array((angHipSagL, angKneL)))\n        self.musc[""REFL""].stepUpdateState(np.array((angHipSagL, angKneL)))\n        self.musc[""BFSHL""].stepUpdateState(np.array((angKneL,)))\n        self.musc[""VASL""].stepUpdateState(np.array((angKneL,)))\n        self.musc[""GASL""].stepUpdateState(np.array((angKneL, angAnkL)))\n        self.musc[""SOLL""].stepUpdateState(np.array((angAnkL,)))\n        self.musc[""TIAL""].stepUpdateState(np.array((angAnkL,)))\n\n        self._metcost += np.sum([musc.MR for musc in self.musc.values()])\n\n        torHipAbdR = self.musc[""HABR""].frcmtc * self.musc[""HABR""].levelArm - self.musc[""HADR""].frcmtc * self.musc[""HADR""].levelArm\n\n        torHipExtR = self.musc[""GLUR""].frcmtc * self.musc[""GLUR""].levelArm - self.musc[""HFLR""].frcmtc * self.musc[""HFLR""].levelArm + \\\n                     self.musc[""HAMR""].frcmtc * self.musc[""HAMR""].levelArm[0] - self.musc[""REFR""].frcmtc * \\\n                     self.musc[""REFR""].levelArm[0]\n\n        torKneFleR = self.musc[""BFSHR""].frcmtc * self.musc[""BFSHR""].levelArm - self.musc[""VASR""].frcmtc * self.musc[""VASR""].levelArm + \\\n                     self.musc[""HAMR""].frcmtc * self.musc[""HAMR""].levelArm[1] - self.musc[""REFR""].frcmtc * \\\n                     self.musc[""REFR""].levelArm[1] + self.musc[""GASR""].frcmtc * self.musc[""GASR""].levelArm[0]\n\n        torAnkExtR = self.musc[""SOLR""].frcmtc * self.musc[""SOLR""].levelArm - self.musc[""TIAR""].frcmtc * self.musc[""TIAR""].levelArm + \\\n                     self.musc[""GASR""].frcmtc * self.musc[""GASR""].levelArm[1]\n\n        torHipAbdL = self.musc[""HABL""].frcmtc * self.musc[""HABL""].levelArm - self.musc[""HADL""].frcmtc * self.musc[""HADL""].levelArm\n\n        torHipExtL = self.musc[""GLUL""].frcmtc * self.musc[""GLUL""].levelArm - self.musc[""HFLL""].frcmtc * self.musc[""HFLL""].levelArm + \\\n                     self.musc[""HAML""].frcmtc * self.musc[""HAML""].levelArm[0] - self.musc[""REFL""].frcmtc * \\\n                     self.musc[""REFL""].levelArm[0]\n\n        torKneFleL = self.musc[""BFSHL""].frcmtc * self.musc[""BFSHL""].levelArm - self.musc[""VASL""].frcmtc * self.musc[""VASL""].levelArm + \\\n                     self.musc[""HAML""].frcmtc * self.musc[""HAML""].levelArm[1] - self.musc[""REFL""].frcmtc * \\\n                     self.musc[""REFL""].levelArm[1] +  self.musc[""GASL""].frcmtc * self.musc[""GASL""].levelArm[0]\n\n        torAnkExtL = self.musc[""SOLL""].frcmtc * self.musc[""SOLL""].levelArm - self.musc[""TIAL""].frcmtc * self.musc[""TIAL""].levelArm + \\\n                     self.musc[""GASL""].frcmtc * self.musc[""GASL""].levelArm[1]\n\n        tor = [-torHipAbdR, torHipExtR, torKneFleR, torAnkExtR,\n               torHipAbdL, torHipExtL, torKneFleL, torAnkExtL]\n\n        return np.squeeze(tor)'"
mushroom_rl/environments/mujoco_envs/humanoid_gait/reward_goals/__init__.py,0,"b'from .reward import NoGoalReward, MaxVelocityReward, \\\n    VelocityProfileReward, CompleteTrajectoryReward\n\nfrom .trajectory import HumanoidTrajectory\n\nfrom .velocity_profile import VelocityProfile, PeriodicVelocityProfile,\\\n    SinVelocityProfile, ConstantVelocityProfile, RandomConstantVelocityProfile,\\\n    SquareWaveVelocityProfile,  VelocityProfile3D\n'"
mushroom_rl/environments/mujoco_envs/humanoid_gait/reward_goals/reward.py,0,"b'from pathlib import Path\n\nimport numpy as np\nfrom collections import deque\n\nfrom mushroom_rl.environments.mujoco_envs.humanoid_gait.utils import convert_traj_quat_to_euler\nfrom .trajectory import HumanoidTrajectory\n\n\nclass GoalRewardInterface:\n    """"""\n    Interface to specify a reward function for the ``HumanoidGait`` environment.\n\n    """"""\n    def __call__(self, state, action, next_state):\n        """"""\n        Compute the reward.\n\n        Args:\n            state (np.ndarray): last state;\n            action (np.ndarray): applied action;\n            next_state (np.ndarray): current state.\n\n        Returs:\n            The reward for the current transition.\n\n        """"""\n        raise NotImplementedError\n\n    def get_observation_space(self):\n        """"""\n        Getter.\n\n        Returns:\n             The low and high arrays of the observation space\n\n        """"""\n        obs = self.get_observation()\n        return -np.inf * np.ones(len(obs)), np.inf * np.ones(len(obs))\n\n    def get_observation(self):\n        """"""\n        Getter.\n\n        Returns:\n             The current observation.\n\n        """"""\n        return np.array([])\n\n    def is_absorbing(self, state):\n        """"""\n        Getter.\n\n        Returns:\n            Whether the current state is absorbing or not.\n\n        """"""\n        return False\n\n    def update_state(self):\n        """"""\n        Update the state of the object after each transition.\n\n        """"""\n        pass\n\n    def reset_state(self):\n        """"""\n        Reset the state of the object.\n\n        """"""\n        pass\n\n\nclass NoGoalReward(GoalRewardInterface):\n    """"""\n    Implement a reward function that is always 0.\n\n    """"""\n    def __call__(self, state, action, next_state):\n        return 0\n\n\nclass MaxVelocityReward(GoalRewardInterface):\n    """"""\n    Implement a goal reward for achieving the maximum possible velocity.\n\n    """"""\n    def __init__(self, sim, traj_start=False, **kwargs):\n        """"""\n        Constructor.\n\n        Args:\n            sim (MjSim): Mujoco simulation object which is passed to\n                the Humanoid Trajectory as is used to set model to\n                trajectory corresponding initial state;\n            traj_start (bool, False): If model initial position should be set\n                from a valid trajectory state. If False starts from the\n                model.xml base position;\n            **kwargs (dict): additional parameters which can be passed to\n                trajectory when using ``traj_start``. ``traj_path`` should be\n                given to select a different trajectory. Rest of the arguments\n                are not important.\n\n        """"""\n        self.traj_start = traj_start\n\n        if traj_start:\n            if ""traj_path"" not in kwargs:\n                traj_path = Path(__file__).resolve().parent.parent.parent /\\\n                            ""data"" / ""humanoid_gait"" / ""gait_trajectory.npz""\n                kwargs[""traj_path""] = traj_path.as_posix()\n            self.trajectory = HumanoidTrajectory(sim, **kwargs)\n            self.reset_state()\n\n    def __call__(self, state, action, next_state):\n        return next_state[13]\n\n    def reset_state(self):\n        if self.traj_start:\n            self.trajectory.reset_trajectory()\n\n\nclass VelocityProfileReward(GoalRewardInterface):\n    """"""\n    Implement a goal reward for following a velocity profile.\n\n    """"""\n    def __init__(self, sim, profile_instance, traj_start=False, **kwargs):\n        """"""\n        Constructor.\n\n        Args:\n            sim (MjSim): Mujoco simulation object which is passed to\n                the Humanoid Trajectory as is used to set model to\n                trajectory corresponding initial state;\n            profile_instance (VelocityProfile): Velocity profile to\n                follow. See RewardGoals.velocity_profile.py;\n            traj_start (bool, False): If model initial position should be set\n                from a valid trajectory state. If False starts from the\n                model.xml base position;\n            **kwargs (dict): additional parameters which can be passed to\n                trajectory when using ``traj_start``. ``traj_path`` should be\n                given to select a diferent trajectory. Rest of the arguments\n                are not important.\n\n        """"""\n        self.profile = profile_instance\n        self.velocity_profile = deque(self.profile.values)\n\n        self.traj_start = traj_start\n        if traj_start:\n            if ""traj_path"" not in kwargs:\n                traj_path = Path(__file__).resolve().parent.parent.parent /\\\n                            ""data"" / ""humanoid_gait"" / ""gait_trajectory.npz""\n                kwargs[""traj_path""] = traj_path.as_posix()\n            self.trajectory = HumanoidTrajectory(sim, **kwargs)\n            self.reset_state()\n\n    def __call__(self, state, action, next_state):\n        return np.exp(-np.linalg.norm(next_state[13:16] - self.velocity_profile[0]))\n\n    def update_state(self):\n        self.velocity_profile.rotate(1)\n\n    def get_observation(self):\n        return self.velocity_profile[0]\n\n    def reset_state(self):\n        self.velocity_profile = deque(self.profile.reset())\n\n        if self.traj_start:\n            substep_no = np.argmin(\n                    np.linalg.norm(\n                            self.trajectory.velocity_profile\n                            - np.expand_dims(self.velocity_profile[0], 1),\n                            axis=0))\n            self.trajectory.reset_trajectory(substep_no=substep_no)\n\n    def plot_velocity_profile_example(self, horizon=1000, n_trajectories=10):\n        values = np.zeros((horizon * n_trajectories, 3))\n        i = 0\n        for t in range(n_trajectories):\n            self.reset_state()\n            for s in range(horizon):\n                self.update_state()\n                values[i, :] = self.get_observation()\n                i += 1\n\n        self.reset_state()\n        import matplotlib.pyplot as plt\n        plt.plot(values)\n        for line_pos in [i*horizon for i in range(n_trajectories)]:\n            plt.axvline(line_pos, c=""red"", alpha=0.3)\n        plt.legend([""axis x"", ""axis y"", ""axis z""])\n        plt.show()\n\n\nclass CompleteTrajectoryReward(GoalRewardInterface, HumanoidTrajectory):\n    """"""\n    Implements a goal reward for matching a kinematic trajectory.\n\n    """"""\n    def __init__(self, sim, control_dt=0.005, traj_path=None,\n                 traj_dt=0.0025, traj_speed_mult=1.0,\n                 use_error_terminate=False, **kwargs):\n        """"""\n        Constructor.\n\n        Args:\n            sim (MjSim): Mujoco simulation object which is passed to\n                the Humanoid Trajectory as is used to set model to\n                trajectory corresponding initial state;\n            control_dt (float, 0.005): frequency of the controller;\n            traj_path (string, None): path with the trajectory for the\n                model to follow. If None is passed, use default\n                trajectory;\n            traj_dt (float, 0.0025): time step of the trajectory file;\n            traj_speed_mult (float, 1.0): factor to speed up or slowdown the\n                trajectory velocity;\n            use_error_terminate (bool, False): If episode should be terminated\n                when the model deviates significantly from the reference\n                 trajectory.\n\n        """"""\n        if traj_path is None:\n            traj_path = Path(__file__).resolve().parent.parent.parent / ""data"" / ""humanoid_gait"" / ""gait_trajectory.npz""\n            traj_path = traj_path.as_posix()\n\n        super(CompleteTrajectoryReward, self).__init__(sim, traj_path, traj_dt,\n                                                       control_dt,\n                                                       traj_speed_mult)\n        self.error_terminate = use_error_terminate\n\n        self.error_threshold = 0.20\n        self.terminate_trajectory_flag = False\n\n        self.euler_traj = convert_traj_quat_to_euler(self.subtraj)\n\n        self.traj_data_range = np.clip(2 * np.std(self.euler_traj, axis=1), 0.15, np.inf)\n\n        self.joint_importance = np.where(self.traj_data_range < 0.15, 2 * self.traj_data_range, 1.0)\n        self.joint_importance[2 :14] *= 1.0\n        self.joint_importance[17:28] *= 0.1\n        self.joint_importance[28:34] *= 5.0\n        self.joint_importance = np.r_[self.joint_importance[2:14],\n                                      self.joint_importance[17:28],\n                                      self.joint_importance[28:34]]\n\n        self.traj_data_range = np.concatenate([self.traj_data_range[2:14],\n                                               self.traj_data_range[17:34]])\n\n    def __call__(self, state, action, next_state):\n        traj_reward_vec = self._calculate_each_comp_reward(state, action,\n                                                           next_state)\n\n        norm_traj_reward = np.sum(traj_reward_vec * self.joint_importance) / np.sum(\n                self.joint_importance)\n\n        if self.error_terminate and norm_traj_reward < (1 - self.error_threshold):\n            self.terminate_trajectory_flag = True\n\n        if self.error_terminate:\n            norm_traj_reward = 1 + (norm_traj_reward - 1) / self.error_threshold\n        return norm_traj_reward\n\n    def _calculate_each_comp_reward(self, state, action, next_state):\n        euler_state = convert_traj_quat_to_euler(next_state, offset=2)\n\n        foot_vec = np.append(\n            (self.sim.data.body_xpos[1] - self.sim.data.body_xpos[4]),\n            (self.sim.data.body_xpos[1] - self.sim.data.body_xpos[7])\n        )\n\n        current_state = np.concatenate([euler_state[0:12],\n                                        euler_state[15:26], foot_vec])\n\n        current_target = np.concatenate(\n            [self.euler_traj[2:14, self.subtraj_step_no],\n            self.euler_traj[17:34, self.subtraj_step_no]]\n        )\n\n        current_error_standard = (np.subtract(current_state, current_target) /\n                                  self.traj_data_range)\n\n        traj_reward_vec = np.exp(-np.square(current_error_standard))\n        return traj_reward_vec\n\n    def update_state(self):\n        self.subtraj_step_no += 1\n        if self.subtraj_step_no >= self.traj_length:\n            self.get_next_sub_trajectory()\n\n    def get_observation(self):\n        return self.velocity_profile[:, self.subtraj_step_no]\n\n    def is_absorbing(self, state):\n        return self.terminate_trajectory_flag\n\n    def reset_state(self):\n        self.reset_trajectory()\n        self.terminate_trajectory_flag = False\n'"
mushroom_rl/environments/mujoco_envs/humanoid_gait/reward_goals/trajectory.py,0,"b'import time\n\nimport matplotlib.pyplot as plt\nimport mujoco_py\nimport numpy as np\nfrom scipy import signal, interpolate\n\n\nclass Trajectory(object):\n    """"""\n    Builds a general trajectory from a numpy bin file(.npy), and automatically\n    synchronizes the trajectory timestep to the desired control timestep while\n    also allowing to change it\'s speed by the desired amount. When using\n    periodic trajectories it is also possible to pass split points which signal\n    the points where the trajectory repeats, and provides an utility to select\n    the desired cycle.\n\n    """"""\n    def __init__(self, traj_path, traj_dt=0.01, control_dt=0.01,\n                 traj_speed_mult=1.0):\n        """"""\n        Constructor.\n\n        Args:\n            traj_path (string): path with the trajectory for the\n                model to follow. Should be a numpy zipped file (.npz)\n                with a \'trajectory_data\' array and possibly a\n                \'split_points\' array inside. The \'trajectory_data\'\n                should be in the shape (joints x observations);\n            traj_dt (float, 0.01): time step of the trajectory file;\n            control_dt (float, 0.01): model control frequency (used to\n                synchronize trajectory with the control step);\n            traj_speed_mult (float, 1.0): factor to speed up or slowdown the\n                trajectory velocity.\n\n        """"""\n        trajectory_files = np.load(traj_path)\n        self.trajectory = trajectory_files[""trajectory_data""]\n\n        if ""split_points"" in trajectory_files.files:\n            self.split_points = trajectory_files[""split_points""]\n        else:\n            self.split_points = np.array([0, self.trajectory.shape[1]])\n\n        self.n_repeating_steps = len(self.split_points) - 1\n\n        self.traj_dt = traj_dt\n        self.control_dt = control_dt\n        self.traj_speed_multiplier = traj_speed_mult\n\n        if self.traj_dt != control_dt or traj_speed_mult != 1.0:\n            new_traj_sampling_factor = (1 / traj_speed_mult) * (\n                    self.traj_dt / control_dt)\n\n            self.trajectory = self._interpolate_trajectory(\n                self.trajectory, factor=new_traj_sampling_factor\n            )\n\n            self.split_points = np.round(\n                self.split_points * new_traj_sampling_factor).astype(np.int32)\n\n    def _interpolate_trajectory(self, traj, factor):\n        x = np.arange(traj.shape[1])\n        x_new = np.linspace(0, traj.shape[1] - 1, round(traj.shape[1] * factor),\n                            endpoint=True)\n        new_traj = interpolate.interp1d(x, traj, kind=""cubic"", axis=1)(x_new)\n        return new_traj\n\n    def _get_traj_gait_sub_steps(self, initial_walking_step,\n                                 number_of_walking_steps=1):\n        start_sim_step = self.split_points[initial_walking_step]\n        end_sim_step = self.split_points[\n            initial_walking_step + number_of_walking_steps\n        ]\n\n        sub_traj = self.trajectory[:, start_sim_step:end_sim_step].copy()\n        initial_x_pos = self.trajectory[0][start_sim_step]\n        sub_traj[0, :] -= initial_x_pos\n        return sub_traj\n\n\nclass HumanoidTrajectory(Trajectory):\n    """"""\n    Loads a trajectory to be used by the humanoid environment. The trajectory\n    file should be structured as:\n    trajectory[0:15] -> model\'s qpos;\n    trajectory[15:29] -> model\'s qvel;\n    trajectory[29:34] -> model\'s foot vector position\n    trajectory[34:36] -> model\'s ground force reaction over z\n\n    """"""\n    def __init__(self, sim, traj_path, traj_dt=0.0025,\n                 control_dt=0.005, traj_speed_mult=1.0,\n                 velocity_smooth_window=1001):\n        """"""\n        Constructor.\n\n        Args:\n            sim (MjSim): Mujoco simulation object which is passed to\n                the Humanoid Trajectory as is used to set model to\n                trajectory corresponding initial state;\n            traj_path (string): path with the trajectory for the\n                model to follow. Should be a numpy zipped file (.npz)\n                with a \'trajectory_data\' array and possibly a\n                \'split_points\' array inside. The \'trajectory_data\'\n                should be in the shape (joints x observations);\n            traj_dt (float, 0.0025): time step of the trajectory file;\n            control_dt (float, 0.005): Model control frequency(used to\n                synchronize trajectory with the control step)\n            traj_speed_mult (float, 1.0): factor to speed up or slowdown the\n                trajectory velocity;\n            velocity_smooth_window (int, 1001): size of window used to average\n                the torso velocity. It is used in order to get the average\n                travelling velocity(as walking velocity from humanoids\n                are sinusoidal).\n\n        """"""\n        super().__init__(traj_path, traj_dt, control_dt, traj_speed_mult)\n\n        self.sim = sim\n        self.trajectory[15:29] *= traj_speed_mult\n\n        self.complete_velocity_profile = self._smooth_vel_profile(\n                self.trajectory[15:18],  window_size=velocity_smooth_window)\n\n        self.subtraj_step_no = 0\n        self.x_dist = 0\n\n        self.subtraj = self.trajectory.copy()\n        self.velocity_profile = self.complete_velocity_profile.copy()\n        self.reset_trajectory()\n\n    @property\n    def traj_length(self):\n        return self.subtraj.shape[1]\n\n    def _get_traj_gait_sub_steps(self, initial_walking_step,\n                                 number_of_walking_steps=1):\n        start_sim_step = self.split_points[initial_walking_step]\n        end_sim_step = self.split_points[\n            initial_walking_step + number_of_walking_steps\n        ]\n\n        sub_traj = self.trajectory[:, start_sim_step:end_sim_step].copy()\n        initial_x_pos = self.trajectory[0][start_sim_step]\n        sub_traj[0, :] -= initial_x_pos\n\n        sub_vel_profile = self.complete_velocity_profile[\n                          :, start_sim_step:end_sim_step].copy()\n\n        return sub_traj, sub_vel_profile\n\n    def _smooth_vel_profile(self, vel, use_simple_mean=False, window_size=1001,\n                            polyorder=2):\n        if use_simple_mean:\n            filtered = np.tile(np.mean(vel, axis=1),\n                               reps=(self.trajectory.shape[1], 1)).T\n        else:\n            filtered = signal.savgol_filter(vel, window_length=window_size,\n                                            polyorder=polyorder, axis=1)\n        return filtered\n\n    def reset_trajectory(self, substep_no=None):\n        """"""\n        Resets the trajectory and the model. The trajectory can be forced\n        to start on the \'substep_no\' if desired, else it starts at\n        a random one.\n\n        Args:\n            substep_no (int, None): starting point of the trajectory.\n                If None, the trajectory starts from a random point.\n        """"""\n        self.x_dist = 0\n        if substep_no is None:\n            self.subtraj_step_no = int(np.random.rand() * (\n                    self.traj_length * 0.45))\n        else:\n            self.subtraj_step_no = substep_no\n\n        self.subtraj = self.trajectory.copy()\n        self.subtraj[0, :] -= self.subtraj[0, self.subtraj_step_no]\n\n        self.sim.data.qpos[0:15] = self.subtraj[0:15, self.subtraj_step_no]\n        self.sim.data.qvel[0:14] = self.subtraj[15:29, self.subtraj_step_no]\n\n    def get_next_sub_trajectory(self):\n        """"""\n        Get the next trajectory once the current one reaches it\'s end.\n\n        """"""\n        self.x_dist += self.subtraj[0][-1]\n        self.reset_trajectory()\n\n    def play_trajectory_demo(self, freq=200):\n        """"""\n        Plays a demo of the loaded trajectory by forcing the model\n        positions to the ones in the reference trajectory at every step\n\n        """"""\n        viewer = mujoco_py.MjViewer(self.sim)\n        viewer._render_every_frame = True\n        self.reset_trajectory()\n        while True:\n            if self.subtraj_step_no >= self.traj_length:\n                self.get_next_sub_trajectory()\n\n            self.sim.data.qpos[0:15] = np.r_[\n                self.x_dist + self.subtraj[0, self.subtraj_step_no],\n                self.subtraj[1:15, self.subtraj_step_no]\n            ]\n            self.sim.data.qvel[0:14] = self.subtraj[15:29, self.subtraj_step_no]\n            self.sim.forward()\n\n            self.subtraj_step_no += 1\n            time.sleep(1 / freq)\n            viewer.render()\n\n    def _plot_joint_trajectories(self, n_points=2000):\n        """"""\n        Plots the joint trajectories(qpos / qvel) in case the user wishes\n            to consult them.\n\n        """"""\n        fig, ax = plt.subplots(2, 8, figsize=(15 * 8, 15))\n        fig.suptitle(""Complete Trajectories Sample"", size=25)\n\n        for j in range(8):\n            ax[0, j].plot(self.subtraj[7 + j, 0:n_points])\n            ax[0, j].legend([""Joint {} pos"".format(j)])\n\n            ax[1, j].plot(self.subtraj[7 + j + 14, 0:n_points])\n            ax[1, j].plot(np.diff(\n                self.subtraj[7 + j, 0:n_points]) / self.control_dt)\n            ax[1, j].legend([""Joint {} vel"".format(j), ""derivate of pos""])\n        plt.show()\n'"
mushroom_rl/environments/mujoco_envs/humanoid_gait/reward_goals/velocity_profile.py,0,"b'import warnings\n\nimport numpy as np\nfrom scipy.signal import square\n\n\nclass VelocityProfile:\n    """"""\n    Interface that represents and handles the velocity profile of the center of\n    mass of the humanoid that must be matched at each timestep.\n\n    """"""\n    def __init__(self, velocity_profile_array, timestep):\n        """"""\n        Constructor.\n\n        Args:\n            velocity_profile_array (np.ndarray): velocity of the center at each\n                timestep;\n            timestep (float): time corresponding to each step of simulation.\n\n        """"""\n        self._velocity_profile_array = velocity_profile_array\n        self._timestep = timestep\n\n    @property\n    def values(self):\n        """"""\n        Returns:\n             The velocity profile.\n\n        """"""\n        return self._velocity_profile_array\n\n    @property\n    def timestep(self):\n        """"""\n        Returns:\n            The time corresponding to each step of simulation.\n\n        """"""\n        return self._timestep\n\n    @property\n    def size(self):\n        """"""\n        Returns:\n             The length of the velocity profile.\n\n        """"""\n        return self._velocity_profile_array.size\n\n    def reset(self):\n        """"""\n        Create a new velocity profile, if needed.\n\n        Returns:\n            The new velocity profile.\n\n        """"""\n        return self._velocity_profile_array\n\n\nclass PeriodicVelocityProfile(VelocityProfile):\n    """"""\n    Interface that represents a cyclic velocity profile.\n\n    """"""\n    def __init__(self, velocity_profile_array, period, timestep):\n        """"""\n        Constructor.\n\n        Args:\n            velocity_profile_array (np.ndarray): velocity of the center at each\n                timestep;\n            period (float): time corresponding to one cycle;\n            timestep (float): time corresponding to each step of simulation.\n\n        """"""\n        if 1 / timestep < 2 * (1 / period):\n            raise ValueError(""This timestep doesn\'t respect the Nyquist theorem""\n                             ""for this given period"")\n\n        sampling = period / timestep\n        rest = sampling - int(sampling)\n        if rest != 0:\n            warnings.warn(\n                \'Velocity Profile doesnt have a full period or a set of full\'\n                \'periods. There will be some desync due to sampling.\')\n\n        super().__init__(velocity_profile_array, timestep)\n\n\nclass SinVelocityProfile(PeriodicVelocityProfile):\n    """"""\n    Interface that represents velocity profile with a sine shape.\n\n    """"""\n    def __init__(self, amplitude, period, timestep, offset=0, phase=0):\n        """"""\n        Constructor.\n\n        Args:\n            amplitude (np.ndarray): amplitude of the sine wave;\n            period (float): time corresponding to one cycle;\n            timestep (float): time corresponding to each step of simulation;\n            offset (float, 0): increment of velocity to each velocity value;\n            phase (float, 0): angle in rads of the phase of the sine wave.\n\n        """"""\n        time_array = np.arange(0, period, timestep)\n        phase_array = 2 * np.pi * (time_array / period)\n        phase_array += phase\n        wave = amplitude * np.sin(phase_array) + offset\n        super(SinVelocityProfile, self).__init__(wave, period, timestep)\n\n\nclass ConstantVelocityProfile(VelocityProfile):\n    """"""\n    Interface that represents velocity profile with constant value.\n\n    """"""\n    def __init__(self, value):\n        """"""\n        Constructor.\n\n        Args:\n            value (float): constant value of the velocity profile.\n\n        """"""\n        super(ConstantVelocityProfile, self).__init__(np.array([value]), 0.0)\n\n\nclass RandomConstantVelocityProfile(ConstantVelocityProfile):\n    """"""\n    Interface that represents velocity profile with a constant value\n    per episode but random limited between two values between each episode.\n\n    """"""\n    def __init__(self, min, max):\n        """"""\n        Constructor.\n\n        Args:\n            min (float): minimum value of the velocity profile.\n            max (float): maximum value of the velocity profile.\n\n        """"""\n        self._max = max\n        self._min = min\n        super().__init__(self.get_random_val())\n\n    def reset(self):\n        self._velocity_profile_array[:] = self.get_random_val()\n        return super().reset()\n\n    def get_random_val(self):\n        return np.random.random() * (self._max - self._min) + self._min\n\n\nclass SquareWaveVelocityProfile(PeriodicVelocityProfile):\n    """"""\n    Interface that represents velocity profile with a square wave shape.\n\n    """"""\n    def __init__(self, amplitude, period, timestep, duty=0.5, offset=0,\n                 phase=0):\n        """"""\n        Constructor.\n\n        Args:\n            amplitude (np.ndarray): amplitude of the square wave;\n            period (float): time corresponding to one cycle;\n            timestep (float): time corresponding to each step of simulation;\n            duty (float, 0.5): value between 0 and 1 and determines the relative\n                time that the step transition occurs between the start and the\n                end of the cycle;\n            offset (float, 0): increment of velocity to each velocity value;\n            phase (float, 0): angle in rads of the phase of the sine wave.\n\n        """"""\n        time_array = np.arange(0, period, timestep)\n        phase_array = 2 * np.pi * (time_array / period)\n        phase_array += phase\n        wave = amplitude * square(phase_array, duty) + offset\n        super(SquareWaveVelocityProfile, self).__init__(wave, period, timestep)\n\n\nclass VelocityProfile3D:\n    """"""\n    Class that represents the ensemble of velocity profiles of the center\n    of mass of the Humanoid on 3 axis (X, Y, Z).\n\n    """"""\n    def __init__(self, velocity_profiles):\n        """"""\n        Constructor.\n\n        Args:\n            velocity_profiles (list): list of ``VelocityProfile`` instances.\n\n        """"""\n        self._profileslist = velocity_profiles\n\n        timestep = None\n        size = None\n\n        for i in range(len(self._profileslist)):\n            if not isinstance(self._profileslist[i], ConstantVelocityProfile):\n                if timestep is None:\n                    timestep = self._profileslist[i].timestep\n                else:\n                    if timestep != self._profileslist[i].timestep:\n                        raise ValueError(\'Values of timesteps differ in\'\n                                         \'velocity profiles\')\n\n                if size is None:\n                    size = self._profileslist[i].size\n                else:\n                    if size != self._profileslist[i].size:\n                        raise ValueError(\'Size of values buffer differ in\'\n                                         \'velocity profiles\')\n\n        if size == None:\n            size = 1\n\n        self._timestep = timestep\n        self._size = size\n\n    @property\n    def values(self):\n        values = []\n        for profile in self._profileslist:\n            if isinstance(profile, ConstantVelocityProfile):\n                vals = np.tile(profile.values, (self.size))\n            else:\n                vals = profile.values\n            values.append(vals)\n        return np.vstack(values).T\n\n    @property\n    def timestep(self):\n        return self._timestep\n\n    @property\n    def size(self):\n        return self._size\n\n    def reset(self):\n        for profile in self._profileslist:\n            profile.reset()\n        return self.values\n'"
