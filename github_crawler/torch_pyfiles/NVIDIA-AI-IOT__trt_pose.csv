file_path,api_count,code
setup.py,1,"b""from setuptools import setup, find_packages, Extension\nfrom torch.utils import cpp_extension\n\n# plugins_ext = Extension(\n#    name='trt_pose.plugins',\n#    sources=['trt_pose/find_peaks.cpp'],\n#    include_dirs=cpp_extension.include_paths(),\n#    language='c++'\n# )\n\nsetup(\n    name='trt_pose',\n    version='0.0.1',\n    description='Pose detection accelerated by NVIDIA TensorRT',\n    packages=find_packages(),\n    ext_package='trt_pose',\n    ext_modules=[cpp_extension.CppExtension('plugins', [\n        'trt_pose/parse/find_peaks.cpp',\n        'trt_pose/parse/paf_score_graph.cpp',\n        'trt_pose/parse/refine_peaks.cpp',\n        'trt_pose/parse/munkres.cpp',\n        'trt_pose/parse/connect_parts.cpp',\n        'trt_pose/plugins.cpp',\n        'trt_pose/train/generate_cmap.cpp',\n        'trt_pose/train/generate_paf.cpp',\n    ])],\n    cmdclass={'build_ext': cpp_extension.BuildExtension},\n    install_requires=[\n    ],\n)\n"""
trt_pose/__init__.py,0,b'import torch'
trt_pose/coco.py,22,"b'import torch\nimport torch.utils.data\nimport torch.nn\nimport os\nimport PIL.Image\nimport json\nimport tqdm\nimport trt_pose\nimport trt_pose.plugins\nimport glob\nimport torchvision.transforms.functional as FT\nimport numpy as np\nfrom trt_pose.parse_objects import ParseObjects\nimport pycocotools\nimport pycocotools.coco\nimport pycocotools.cocoeval\nimport torchvision\n\n\n        \ndef coco_category_to_topology(coco_category):\n    """"""Gets topology tensor from a COCO category\n    """"""\n    skeleton = coco_category[\'skeleton\']\n    K = len(skeleton)\n    topology = torch.zeros((K, 4)).int()\n    for k in range(K):\n        topology[k][0] = 2 * k\n        topology[k][1] = 2 * k + 1\n        topology[k][2] = skeleton[k][0] - 1\n        topology[k][3] = skeleton[k][1] - 1\n    return topology\n\n\ndef coco_category_to_parts(coco_category):\n    """"""Gets list of parts name from a COCO category\n    """"""\n    return coco_category[\'keypoints\']\n\n\ndef coco_annotations_to_tensors(coco_annotations,\n                                image_shape,\n                                parts,\n                                topology,\n                                max_count=100):\n    """"""Gets tensors corresponding to peak counts, peak coordinates, and peak to peak connections\n    """"""\n    annotations = coco_annotations\n    C = len(parts)\n    K = topology.shape[0]\n    M = max_count\n    IH = image_shape[0]\n    IW = image_shape[1]\n    counts = torch.zeros((C)).int()\n    peaks = torch.zeros((C, M, 2)).float()\n    visibles = torch.zeros((len(annotations), C)).int()\n    connections = -torch.ones((K, 2, M)).int()\n\n    for ann_idx, ann in enumerate(annotations):\n\n        kps = ann[\'keypoints\']\n\n        # add visible peaks\n        for c in range(C):\n\n            x = kps[c * 3]\n            y = kps[c * 3 + 1]\n            visible = kps[c * 3 + 2]\n\n            if visible:\n                peaks[c][counts[c]][0] = (float(y) + 0.5) / (IH + 1.0)\n                peaks[c][counts[c]][1] = (float(x) + 0.5) / (IW + 1.0)\n                counts[c] = counts[c] + 1\n                visibles[ann_idx][c] = 1\n\n        for k in range(K):\n            c_a = topology[k][2]\n            c_b = topology[k][3]\n            if visibles[ann_idx][c_a] and visibles[ann_idx][c_b]:\n                connections[k][0][counts[c_a] - 1] = counts[c_b] - 1\n                connections[k][1][counts[c_b] - 1] = counts[c_a] - 1\n\n    return counts, peaks, connections\n\n\ndef coco_annotations_to_mask_bbox(coco_annotations, image_shape):\n    mask = np.ones(image_shape, dtype=np.uint8)\n    for ann in coco_annotations:\n        if \'num_keypoints\' not in ann or ann[\'num_keypoints\'] == 0:\n            bbox = ann[\'bbox\']\n            x0 = round(bbox[0])\n            y0 = round(bbox[1])\n            x1 = round(x0 + bbox[2])\n            y1 = round(y0 + bbox[3])\n            mask[y0:y1, x0:x1] = 0\n    return mask\n            \n\ndef convert_dir_to_bmp(output_dir, input_dir):\n    files = glob.glob(os.path.join(input_dir, \'*.jpg\'))\n    for f in files:\n        new_path = os.path.join(\n            output_dir,\n            os.path.splitext(os.path.basename(f))[0] + \'.bmp\')\n        img = PIL.Image.open(f)\n        img.save(new_path)\n\n        \ndef get_quad(angle, translation, scale, aspect_ratio=1.0):\n    if aspect_ratio > 1.0:\n        # width > height =>\n        # increase height region\n        quad = np.array([\n            [0.0, 0.5 - 0.5 * aspect_ratio],\n            [0.0, 0.5 + 0.5 * aspect_ratio],\n            [1.0, 0.5 + 0.5 * aspect_ratio],\n            [1.0, 0.5 - 0.5 * aspect_ratio],\n            \n        ])\n    elif aspect_ratio < 1.0:\n        # width < height\n        quad = np.array([\n            [0.5 - 0.5 / aspect_ratio, 0.0],\n            [0.5 - 0.5 / aspect_ratio, 1.0],\n            [0.5 + 0.5 / aspect_ratio, 1.0],\n            [0.5 + 0.5 / aspect_ratio, 0.0],\n            \n        ])\n    else:\n        quad = np.array([\n            [0.0, 0.0],\n            [0.0, 1.0],\n            [1.0, 1.0],\n            [1.0, 0.0],\n        ])\n        \n    quad -= 0.5\n\n    R = np.array([\n        [np.cos(angle), -np.sin(angle)],\n        [np.sin(angle), np.cos(angle)]\n    ])\n\n    quad = np.dot(quad, R)\n\n    quad -= np.array(translation)\n    quad /= scale\n    quad += 0.5\n    \n    return quad\n\n\ndef transform_image(image, size, quad):\n    new_quad = np.zeros_like(quad)\n    new_quad[:, 0] = quad[:, 0] * image.size[0]\n    new_quad[:, 1] = quad[:, 1] * image.size[1]\n    \n    new_quad = (new_quad[0][0], new_quad[0][1],\n            new_quad[1][0], new_quad[1][1],\n            new_quad[2][0], new_quad[2][1],\n            new_quad[3][0], new_quad[3][1])\n    \n    return image.transform(size, PIL.Image.QUAD, new_quad)\n\n\ndef transform_points_xy(points, quad):\n    p00 = quad[0]\n    p01 = quad[1] - p00\n    p10 = quad[3] - p00\n    p01 /= np.sum(p01**2)\n    p10 /= np.sum(p10**2)\n    \n    A = np.array([\n        p10,\n        p01,\n    ]).transpose()\n    \n    return np.dot(points - p00, A)\n\n\ndef transform_peaks(counts, peaks, quad):\n    newpeaks = peaks.clone().numpy()\n    C = counts.shape[0]\n    for c in range(C):\n        count = int(counts[c])\n        newpeaks[c][0:count] = transform_points_xy(newpeaks[c][0:count][:, ::-1], quad)[:, ::-1]\n    return torch.from_numpy(newpeaks)\n\n\nclass CocoDataset(torch.utils.data.Dataset):\n    def __init__(self,\n                 images_dir,\n                 annotations_file,\n                 category_name,\n                 image_shape,\n                 target_shape,\n                 is_bmp=False,\n                 stdev=0.02,\n                 use_crowd=False,\n                 min_area=0.0,\n                 max_area=1.0,\n                 max_part_count=100,\n                 random_angle=(0.0, 0.0),\n                 random_scale=(1.0, 1.0),\n                 random_translate=(0.0, 0.0),\n                 transforms=None,\n                 keep_aspect_ratio=False):\n\n        self.keep_aspect_ratio = keep_aspect_ratio\n        self.transforms=transforms\n        self.is_bmp = is_bmp\n        self.images_dir = images_dir\n        self.image_shape = image_shape\n        self.target_shape = target_shape\n        self.stdev = stdev\n        self.random_angle = random_angle\n        self.random_scale = random_scale\n        self.random_translate = random_translate\n        \n        tensor_cache_file = annotations_file + \'.cache\'\n        \n        if tensor_cache_file is not None and os.path.exists(tensor_cache_file):\n            print(\'Cachefile found.  Loading from cache file...\')\n            cache = torch.load(tensor_cache_file)\n            self.counts = cache[\'counts\']\n            self.peaks = cache[\'peaks\']\n            self.connections = cache[\'connections\']\n            self.topology = cache[\'topology\']\n            self.parts = cache[\'parts\']\n            self.filenames = cache[\'filenames\']\n            self.samples = cache[\'samples\']\n            return\n            \n        with open(annotations_file, \'r\') as f:\n            data = json.load(f)\n\n        cat = [c for c in data[\'categories\'] if c[\'name\'] == category_name][0]\n        cat_id = cat[\'id\']\n\n        img_map = {}\n        for img in data[\'images\']:\n            img_map[img[\'id\']] = img\n\n        samples = {}\n        for ann in data[\'annotations\']:\n\n            # filter by category\n            if ann[\'category_id\'] != cat_id:\n                continue\n\n            # filter by crowd\n            if not use_crowd and ann[\'iscrowd\']:\n                continue\n\n            img_id = ann[\'image_id\']\n            img = img_map[img_id]\n            height = img[\'height\']\n            width = img[\'width\']\n            area = ann[\'area\']\n\n            # filter by object area\n            normalized_area = float(area) / float(height * width)\n            if normalized_area < min_area or normalized_area > max_area:\n                continue\n\n            # add metadata\n            if img_id not in samples:\n                sample = {}\n                sample[\'img\'] = img\n                sample[\'anns\'] = [ann]\n                samples[img_id] = sample\n            else:\n                samples[img_id][\'anns\'] += [ann]\n                \n        # generate tensors\n        self.topology = coco_category_to_topology(cat)\n        self.parts = coco_category_to_parts(cat)\n\n        N = len(samples)\n        C = len(self.parts)\n        K = self.topology.shape[0]\n        M = max_part_count\n\n        print(\'Generating intermediate tensors...\')\n        self.counts = torch.zeros((N, C), dtype=torch.int32)\n        self.peaks = torch.zeros((N, C, M, 2), dtype=torch.float32)\n        self.connections = torch.zeros((N, K, 2, M), dtype=torch.int32)\n        self.filenames = []\n        self.samples = []\n        \n        for i, sample in tqdm.tqdm(enumerate(samples.values())):\n            filename = sample[\'img\'][\'file_name\']\n            self.filenames.append(filename)\n            image_shape = (sample[\'img\'][\'height\'], sample[\'img\'][\'width\'])\n            counts_i, peaks_i, connections_i = coco_annotations_to_tensors(\n                sample[\'anns\'], image_shape, self.parts, self.topology)\n            self.counts[i] = counts_i\n            self.peaks[i] = peaks_i\n            self.connections[i] = connections_i\n            self.samples += [sample]\n\n        if tensor_cache_file is not None:\n            print(\'Saving to intermediate tensors to cache file...\')\n            torch.save({\n                \'counts\': self.counts,\n                \'peaks\': self.peaks,\n                \'connections\': self.connections,\n                \'topology\': self.topology,\n                \'parts\': self.parts,\n                \'filenames\': self.filenames,\n                \'samples\': self.samples\n            }, tensor_cache_file)\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n\n        if self.is_bmp:\n            filename = os.path.splitext(self.filenames[idx])[0] + \'.bmp\'\n        else:\n            filename = os.path.splitext(self.filenames[idx])[0] + \'.jpg\'\n\n        image = PIL.Image.open(os.path.join(self.images_dir, filename))\n        \n        im = self.samples[idx][\'img\']\n        \n        mask = coco_annotations_to_mask_bbox(self.samples[idx][\'anns\'], (im[\'height\'], im[\'width\']))\n        mask = PIL.Image.fromarray(mask)\n        \n        counts = self.counts[idx]\n        peaks = self.peaks[idx]\n        \n        # affine transformation\n        shiftx = float(torch.rand(1)) * (self.random_translate[1] - self.random_translate[0]) + self.random_translate[0]\n        shifty = float(torch.rand(1)) * (self.random_translate[1] - self.random_translate[0]) + self.random_translate[0]\n        scale = float(torch.rand(1)) * (self.random_scale[1] - self.random_scale[0]) + self.random_scale[0]\n        angle = float(torch.rand(1)) * (self.random_angle[1] - self.random_angle[0]) + self.random_angle[0]\n        \n        if self.keep_aspect_ratio:\n            ar = float(image.width) / float(image.height)\n            quad = get_quad(angle, (shiftx, shifty), scale, aspect_ratio=ar)\n        else:\n            quad = get_quad(angle, (shiftx, shifty), scale, aspect_ratio=1.0)\n        \n        image = transform_image(image, (self.image_shape[1], self.image_shape[0]), quad)\n        mask = transform_image(mask, (self.target_shape[1], self.target_shape[0]), quad)\n        peaks = transform_peaks(counts, peaks, quad)\n        \n        counts = counts[None, ...]\n        peaks = peaks[None, ...]\n\n        stdev = float(self.stdev * self.target_shape[0])\n\n        cmap = trt_pose.plugins.generate_cmap(counts, peaks,\n            self.target_shape[0], self.target_shape[1], stdev, int(stdev * 5))\n\n        paf = trt_pose.plugins.generate_paf(\n            self.connections[idx][None, ...], self.topology,\n            counts, peaks,\n            self.target_shape[0], self.target_shape[1], stdev)\n\n        image = image.convert(\'RGB\')\n        if self.transforms is not None:\n            image = self.transforms(image)\n            \n        return image, cmap[0], paf[0], torch.from_numpy(np.array(mask))[None, ...]\n\n    def get_part_type_counts(self):\n        return torch.sum(self.counts, dim=0)\n    \n    def get_paf_type_counts(self):\n        c = torch.sum(self.connections[:, :, 0, :] >= 0, dim=-1) # sum over parts\n        c = torch.sum(c, dim=0) # sum over batch\n        return c\n    \n    \nclass CocoHumanPoseEval(object):\n    \n    def __init__(self, images_dir, annotation_file, image_shape, keep_aspect_ratio=False):\n        \n        self.images_dir = images_dir\n        self.annotation_file = annotation_file\n        self.image_shape = tuple(image_shape)\n        self.keep_aspect_ratio = keep_aspect_ratio\n        \n        self.cocoGt = pycocotools.coco.COCO(\'annotations/person_keypoints_val2017.json\')\n        self.catIds = self.cocoGt.getCatIds(\'person\')\n        self.imgIds = self.cocoGt.getImgIds(catIds=self.catIds)\n        self.transform = torchvision.transforms.Compose([\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n        \n    def evaluate(self, model, topology):\n        self.parse_objects = ParseObjects(topology, cmap_threshold=0.1, link_threshold=0.1, cmap_window=5, line_integral_samples=7, max_num_parts=100, max_num_objects=100)\n        \n        results = []\n\n        for n, imgId in enumerate(self.imgIds[1:]):\n\n            # read image\n            img = self.cocoGt.imgs[imgId]\n            img_path = os.path.join(self.images_dir, img[\'file_name\'])\n\n            image = PIL.Image.open(img_path).convert(\'RGB\')#.resize(IMAGE_SHAPE)\n            \n            if self.keep_aspect_ratio:\n                ar = float(image.width) / float(image.height)\n            else:\n                ar = 1.0\n                \n            quad = get_quad(0.0, (0, 0), 1.0, aspect_ratio=ar)\n            image = transform_image(image, self.image_shape, quad)\n\n            data = self.transform(image).cuda()[None, ...]\n\n            cmap, paf = model(data)\n            cmap, paf = cmap.cpu(), paf.cpu()\n\n        #     object_counts, objects, peaks, int_peaks = postprocess(cmap, paf, cmap_threshold=0.05, link_threshold=0.01, window=5)\n        #     object_counts, objects, peaks = int(object_counts[0]), objects[0], peaks[0]\n\n            object_counts, objects, peaks = self.parse_objects(cmap, paf)\n            object_counts, objects, peaks = int(object_counts[0]), objects[0], peaks[0]\n\n            for i in range(object_counts):\n                object = objects[i]\n                score = 0.0\n                kps = [0]*(17*3)\n                x_mean = 0\n                y_mean = 0\n                cnt = 0\n                for j in range(17):\n                    k = object[j]\n                    if k >= 0:\n                        peak = peaks[j][k]\n                        if ar > 1.0: # w > h w/h\n                            x = peak[1]\n                            y = (peak[0] - 0.5) * ar + 0.5\n                        else:\n                            x = (peak[1] - 0.5) / ar + 0.5\n                            y = peak[0]\n\n                        x = round(float(img[\'width\'] * x))\n                        y = round(float(img[\'height\'] * y))\n\n                        score += 1.0\n                        kps[j * 3 + 0] = x\n                        kps[j * 3 + 1] = y\n                        kps[j * 3 + 2] = 2\n                        x_mean += x\n                        y_mean += y\n                        cnt += 1\n\n                ann = {\n                    \'image_id\': imgId,\n                    \'category_id\': 1,\n                    \'keypoints\': kps,\n                    \'score\': score / 17.0\n                }\n                results.append(ann)\n            if n % 100 == 0:\n                print(\'%d / %d\' % (n, len(self.imgIds)))\n\n\n        if len(results) == 0:\n            return\n        \n        with open(\'trt_pose_results.json\', \'w\') as f:\n            json.dump(results, f)\n            \n        cocoDt = self.cocoGt.loadRes(\'trt_pose_results.json\')\n        cocoEval = pycocotools.cocoeval.COCOeval(self.cocoGt, cocoDt, \'keypoints\')\n        cocoEval.params.imgIds = self.imgIds\n        cocoEval.params.catIds = [1]\n        cocoEval.evaluate()\n        cocoEval.accumulate()\n        cocoEval.summarize()'"
trt_pose/draw_objects.py,0,"b'import cv2\n\n\nclass DrawObjects(object):\n    \n    def __init__(self, topology):\n        self.topology = topology\n        \n    def __call__(self, image, object_counts, objects, normalized_peaks):\n        topology = self.topology\n        height = image.shape[0]\n        width = image.shape[1]\n        \n        K = topology.shape[0]\n        count = int(object_counts[0])\n        K = topology.shape[0]\n        for i in range(count):\n            color = (0, 255, 0)\n            obj = objects[0][i]\n            C = obj.shape[0]\n            for j in range(C):\n                k = int(obj[j])\n                if k >= 0:\n                    peak = normalized_peaks[0][j][k]\n                    x = round(float(peak[1]) * width)\n                    y = round(float(peak[0]) * height)\n                    cv2.circle(image, (x, y), 3, color, 2)\n\n            for k in range(K):\n                c_a = topology[k][2]\n                c_b = topology[k][3]\n                if obj[c_a] >= 0 and obj[c_b] >= 0:\n                    peak0 = normalized_peaks[0][c_a][obj[c_a]]\n                    peak1 = normalized_peaks[0][c_b][obj[c_b]]\n                    x0 = round(float(peak0[1]) * width)\n                    y0 = round(float(peak0[0]) * height)\n                    x1 = round(float(peak1[1]) * width)\n                    y1 = round(float(peak1[0]) * height)\n                    cv2.line(image, (x0, y0), (x1, y1), color, 2)'"
trt_pose/parse_objects.py,0,"b'import trt_pose.plugins\n\n\nclass ParseObjects(object):\n    \n    def __init__(self, topology, cmap_threshold=0.1, link_threshold=0.1, cmap_window=5, line_integral_samples=7, max_num_parts=100, max_num_objects=100):\n        self.topology = topology\n        self.cmap_threshold = cmap_threshold\n        self.link_threshold = link_threshold\n        self.cmap_window = cmap_window\n        self.line_integral_samples = line_integral_samples\n        self.max_num_parts = max_num_parts\n        self.max_num_objects = max_num_objects\n    \n    def __call__(self, cmap, paf):\n        \n        peak_counts, peaks = trt_pose.plugins.find_peaks(cmap, self.cmap_threshold, self.cmap_window, self.max_num_parts)\n        normalized_peaks = trt_pose.plugins.refine_peaks(peak_counts, peaks, cmap, self.cmap_window)\n        score_graph = trt_pose.plugins.paf_score_graph(paf, self.topology, peak_counts, normalized_peaks, self.line_integral_samples)\n        connections = trt_pose.plugins.assignment(score_graph, self.topology, peak_counts, self.link_threshold)\n        object_counts, objects =trt_pose.plugins.connect_parts(connections, self.topology, peak_counts, self.max_num_objects)\n        \n        return object_counts, objects, normalized_peaks'"
trt_pose/train.py,18,"b'import argparse\nimport subprocess\nimport torch\nimport torchvision\nimport os\nimport torch.optim\nimport tqdm\nimport apex.amp as amp\nimport time\nimport json\nimport pprint\nimport torch.nn.functional as F\nfrom .coco import CocoDataset, CocoHumanPoseEval\nfrom .models import MODELS\n\nOPTIMIZERS = {\n    \'SGD\': torch.optim.SGD,\n    \'Adam\': torch.optim.Adam\n}\n\nEPS = 1e-6\n\ndef set_lr(optimizer, lr):\n    for p in optimizer.param_groups:\n        p[\'lr\'] = lr\n        \n        \ndef save_checkpoint(model, directory, epoch):\n    if not os.path.exists(directory):\n        os.mkdir(directory)\n    filename = os.path.join(directory, \'epoch_%d.pth\' % epoch)\n    print(\'Saving checkpoint to %s\' % filename)\n    torch.save(model.state_dict(), filename)\n\n    \ndef write_log_entry(logfile, epoch, train_loss, test_loss):\n    with open(logfile, \'a+\') as f:\n        logline = \'%d, %f, %f\' % (epoch, train_loss, test_loss)\n        print(logline)\n        f.write(logline + \'\\n\')\n        \ndevice = torch.device(\'cuda\')\n\nif __name__ == \'__main__\':\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'config\')\n    args = parser.parse_args()\n    \n    print(\'Loading config %s\' % args.config)\n    with open(args.config, \'r\') as f:\n        config = json.load(f)\n        pprint.pprint(config)\n        \n    logfile_path = args.config + \'.log\'\n    \n    checkpoint_dir = args.config + \'.checkpoints\'\n    if not os.path.exists(checkpoint_dir):\n        print(\'Creating checkpoint directory % s\' % checkpoint_dir)\n        os.mkdir(checkpoint_dir)\n    \n        \n    # LOAD DATASETS\n    \n    train_dataset_kwargs = config[""train_dataset""]\n    train_dataset_kwargs[\'transforms\'] = torchvision.transforms.Compose([\n            torchvision.transforms.ColorJitter(**config[\'color_jitter\']),\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    \n    test_dataset_kwargs = config[""test_dataset""]\n    test_dataset_kwargs[\'transforms\'] = torchvision.transforms.Compose([\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n    \n    if \'evaluation\' in config:\n        evaluator = CocoHumanPoseEval(**config[\'evaluation\'])\n    \n    train_dataset = CocoDataset(**train_dataset_kwargs)\n    test_dataset = CocoDataset(**test_dataset_kwargs)\n    \n    part_type_counts = test_dataset.get_part_type_counts().float().cuda()\n    part_weight = 1.0 / part_type_counts\n    part_weight = part_weight / torch.sum(part_weight)\n    paf_type_counts = test_dataset.get_paf_type_counts().float().cuda()\n    paf_weight = 1.0 / paf_type_counts\n    paf_weight = paf_weight / torch.sum(paf_weight)\n    paf_weight /= 2.0\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        **config[""train_loader""]\n    )\n    \n    test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        **config[""test_loader""]\n    )\n    \n    model = MODELS[config[\'model\'][\'name\']](**config[\'model\'][\'kwargs\']).to(device)\n    \n    if ""initial_state_dict"" in config[\'model\']:\n        print(\'Loading initial weights from %s\' % config[\'model\'][\'initial_state_dict\'])\n        model.load_state_dict(torch.load(config[\'model\'][\'initial_state_dict\']))\n    \n    optimizer = OPTIMIZERS[config[\'optimizer\'][\'name\']](model.parameters(), **config[\'optimizer\'][\'kwargs\'])\n    model, optimizer = amp.initialize(model, optimizer, opt_level=""O1"")\n    \n    if \'mask_unlabeled\' in config and config[\'mask_unlabeled\']:\n        print(\'Masking unlabeled annotations\')\n        mask_unlabeled = True\n    else:\n        mask_unlabeled = False\n        \n    for epoch in range(config[""epochs""]):\n        \n        if str(epoch) in config[\'stdev_schedule\']:\n            stdev = config[\'stdev_schedule\'][str(epoch)]\n            print(\'Adjusting stdev to %f\' % stdev)\n            train_dataset.stdev = stdev\n            test_dataset.stdev = stdev\n            \n        if str(epoch) in config[\'lr_schedule\']:\n            new_lr = config[\'lr_schedule\'][str(epoch)]\n            print(\'Adjusting learning rate to %f\' % new_lr)\n            set_lr(optimizer, new_lr)\n        \n        if epoch % config[\'checkpoints\'][\'interval\'] == 0:\n            save_checkpoint(model, checkpoint_dir, epoch)\n        \n           \n        \n        train_loss = 0.0\n        model = model.train()\n        for image, cmap, paf, mask in tqdm.tqdm(iter(train_loader)):\n            image = image.to(device)\n            cmap = cmap.to(device)\n            paf = paf.to(device)\n            \n            if mask_unlabeled:\n                mask = mask.to(device).float()\n            else:\n                mask = torch.ones_like(mask).to(device).float()\n            \n            optimizer.zero_grad()\n            cmap_out, paf_out = model(image)\n            \n            cmap_mse = torch.mean(mask * (cmap_out - cmap)**2)\n            paf_mse = torch.mean(mask * (paf_out - paf)**2)\n            \n            loss = cmap_mse + paf_mse\n            \n            with amp.scale_loss(loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n#             loss.backward()\n            optimizer.step()\n            train_loss += float(loss)\n            \n        train_loss /= len(train_loader)\n        \n        test_loss = 0.0\n        model = model.eval()\n        for image, cmap, paf, mask in tqdm.tqdm(iter(test_loader)):\n      \n            with torch.no_grad():\n                image = image.to(device)\n                cmap = cmap.to(device)\n                paf = paf.to(device)\n                mask = mask.to(device).float()\n\n                if mask_unlabeled:\n                    mask = mask.to(device).float()\n                else:\n                    mask = torch.ones_like(mask).to(device).float()\n                \n                cmap_out, paf_out = model(image)\n                \n                cmap_mse = torch.mean(mask * (cmap_out - cmap)**2)\n                paf_mse = torch.mean(mask * (paf_out - paf)**2)\n\n                loss = cmap_mse + paf_mse\n\n                test_loss += float(loss)\n        test_loss /= len(test_loader)\n        \n        write_log_entry(logfile_path, epoch, train_loss, test_loss)\n        \n        \n        if \'evaluation\' in config:\n            evaluator.evaluate(model, train_dataset.topology)'"
tasks/human_pose/preprocess_coco_person.py,0,"b""import argparse\nimport json\nimport tqdm\n\n\ndef remove_link(cat, kp_a_name, kp_b_name):\n    skeleton = cat['skeleton']\n    keypoints = cat['keypoints']\n    new_skeleton = []\n    \n    for link in skeleton:\n        if (keypoints[link[0] - 1] == kp_a_name) and (keypoints[link[1] - 1] == kp_b_name):\n            continue # skip\n        if (keypoints[link[0] - 1] == kp_b_name) and (keypoints[link[1] - 1] == kp_a_name):\n            continue # skip\n        new_skeleton.append(link)\n    cat['skeleton'] = new_skeleton\n\n    \ndef add_link(cat, kp_a_name, kp_b_name):\n    keypoints = cat['keypoints']\n    cat['skeleton'].append([keypoints.index(kp_a_name) + 1, keypoints.index(kp_b_name) + 1])\n\n\ndef append_neck_keypoint(ann, cat):\n    keypoints = cat['keypoints']\n    kps = ann['keypoints']\n    l_idx = 3 * keypoints.index('left_shoulder')\n    r_idx = 3 * keypoints.index('right_shoulder')\n    x_neck = round((kps[l_idx] + kps[r_idx]) / 2.0)\n    y_neck = round((kps[l_idx + 1] + kps[r_idx + 1]) / 2.0)\n    \n    v_l = kps[l_idx + 2]\n    v_r = kps[r_idx + 2]\n    \n    if v_l == 0 or v_r == 0:\n        v_neck = 0\n    elif v_l == 1 or v_r == 1:\n        v_neck = 1\n    else:\n        v_neck = 2\n    \n    kps += [x_neck, y_neck, v_neck]\n    \ndef get_cat(data, cat_name):\n    return [c for c in data['categories'] if c['name'] == cat_name][0]\n\ndef get_anns(data, cat_id):\n    return [a for a in data['annotations'] if a['category_id'] == cat_id]\n\n\nif __name__ == '__main__':\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument('input_annotation_file', type=str, help='Path to COCO annotation file')\n    parser.add_argument('output_annotation_file', type=str, help='Path to COCO annotation file')\n    args = parser.parse_args()\n    \n    print('Loading...')\n    with open(args.input_annotation_file, 'r') as f:\n        data = json.load(f)\n    \n    print('Preprocessing...')\n    cat = get_cat(data, 'person')\n    cat_id = cat['id']\n    anns = get_anns(data, cat_id)\n\n    for a in anns:\n        append_neck_keypoint(a, cat)\n\n    cat['keypoints'].append('neck')\n    remove_link(cat, 'left_shoulder', 'right_shoulder')\n    remove_link(cat, 'left_shoulder', 'left_hip')\n    remove_link(cat, 'right_shoulder', 'right_hip')\n    add_link(cat, 'neck', 'nose')\n    add_link(cat, 'neck', 'left_shoulder')\n    add_link(cat, 'neck', 'right_shoulder')\n    add_link(cat, 'neck', 'left_hip')\n    add_link(cat, 'neck', 'right_hip')\n    \n    print('Saving...')\n    with open(args.output_annotation_file, 'w') as f:\n        json.dump(data, f)"""
trt_pose/models/__init__.py,0,"b""from .resnet import *\nfrom .densenet import *\nfrom .mnasnet import *\n# from .dla import *\n\nMODELS = {\n    'resnet18_baseline': resnet18_baseline,\n    'resnet34_baseline': resnet34_baseline,\n    'resnet50_baseline': resnet50_baseline,\n    'resnet101_baseline': resnet101_baseline,\n    'resnet152_baseline': resnet152_baseline,\n    'resnet18_baseline_att': resnet18_baseline_att,\n    'resnet34_baseline_att': resnet34_baseline_att,\n    'resnet50_baseline_att': resnet50_baseline_att,\n    'resnet101_baseline_att': resnet101_baseline_att,\n    'resnet152_baseline_att': resnet152_baseline_att,\n    'densenet121_baseline': densenet121_baseline,\n    'densenet169_baseline': densenet169_baseline,\n    'densenet201_baseline': densenet201_baseline,\n    'densenet161_baseline': densenet161_baseline,\n    'densenet121_baseline_att': densenet121_baseline_att,\n    'densenet169_baseline_att': densenet169_baseline_att,\n    'densenet201_baseline_att': densenet201_baseline_att,\n    'densenet161_baseline_att': densenet161_baseline_att,\n#     'dla34up_pose': dla34up_pose,\n#     'dla60up_pose': dla60up_pose,\n#     'dla102up_pose': dla102up_pose,\n#     'dla169up_pose': dla169up_pose,\n    'mnasnet0_5_baseline_att': mnasnet0_5_baseline_att,\n    'mnasnet0_75_baseline_att': mnasnet0_75_baseline_att,\n    'mnasnet1_0_baseline_att': mnasnet1_0_baseline_att,\n    'mnasnet1_3_baseline_att': mnasnet1_3_baseline_att,\n}\n"""
trt_pose/models/common.py,22,"b'import torch\n\n\nclass UpsampleCBR(torch.nn.Sequential):\n    def __init__(self, input_channels, output_channels, count=1, num_flat=0):\n        layers = []\n        for i in range(count):\n            if i == 0:\n                inch = input_channels\n            else:\n                inch = output_channels\n                \n            layers += [\n                torch.nn.ConvTranspose2d(inch, output_channels, kernel_size=4, stride=2, padding=1),\n                torch.nn.BatchNorm2d(output_channels),\n                torch.nn.ReLU()\n            ]\n            for i in range(num_flat):\n                layers += [\n                    torch.nn.Conv2d(output_channels, output_channels, kernel_size=3, stride=1, padding=1),\n                    torch.nn.BatchNorm2d(output_channels),\n                    torch.nn.ReLU()\n                ]\n            \n        super(UpsampleCBR, self).__init__(*layers)\n\n        \nclass SelectInput(torch.nn.Module):\n    \n    def __init__(self, index):\n        super(SelectInput, self).__init__()\n        self.index = index\n    \n    def forward(self, inputs):\n        return inputs[self.index]\n    \n    \nclass CmapPafHead(torch.nn.Module):\n    def __init__(self, input_channels, cmap_channels, paf_channels, upsample_channels=256, num_upsample=0, num_flat=0):\n        super(CmapPafHead, self).__init__()\n        if num_upsample > 0:\n            self.cmap_conv = torch.nn.Sequential(\n                UpsampleCBR(input_channels, upsample_channels, num_upsample, num_flat),\n                torch.nn.Conv2d(upsample_channels, cmap_channels, kernel_size=1, stride=1, padding=0)\n            )\n            self.paf_conv = torch.nn.Sequential(\n                UpsampleCBR(input_channels, upsample_channels, num_upsample, num_flat),\n                torch.nn.Conv2d(upsample_channels, paf_channels, kernel_size=1, stride=1, padding=0)\n            )\n        else:\n            self.cmap_conv = torch.nn.Conv2d(input_channels, cmap_channels, kernel_size=1, stride=1, padding=0)\n            self.paf_conv = torch.nn.Conv2d(input_channels, paf_channels, kernel_size=1, stride=1, padding=0)\n    \n    def forward(self, x):\n        return self.cmap_conv(x), self.paf_conv(x)\n    \n    \nclass CmapPafHeadAttention(torch.nn.Module):\n    def __init__(self, input_channels, cmap_channels, paf_channels, upsample_channels=256, num_upsample=0, num_flat=0):\n        super(CmapPafHeadAttention, self).__init__()\n        self.cmap_up = UpsampleCBR(input_channels, upsample_channels, num_upsample, num_flat)\n        self.paf_up = UpsampleCBR(input_channels, upsample_channels, num_upsample, num_flat)\n        self.cmap_att = torch.nn.Conv2d(upsample_channels, upsample_channels, kernel_size=3, stride=1, padding=1)\n        self.paf_att = torch.nn.Conv2d(upsample_channels, upsample_channels, kernel_size=3, stride=1, padding=1)\n            \n        self.cmap_conv = torch.nn.Conv2d(upsample_channels, cmap_channels, kernel_size=1, stride=1, padding=0)\n        self.paf_conv = torch.nn.Conv2d(upsample_channels, paf_channels, kernel_size=1, stride=1, padding=0)\n    \n    def forward(self, x):\n        xc = self.cmap_up(x)\n        ac = torch.sigmoid(self.cmap_att(xc))\n        \n        xp = self.paf_up(x)\n        ap = torch.tanh(self.paf_att(xp))\n        \n        return self.cmap_conv(xc * ac), self.paf_conv(xp * ap)\n    \n    \n'"
trt_pose/models/densenet.py,3,"b'import torch\nimport torchvision\nfrom .common import *\n\n\nclass DenseNetBackbone(torch.nn.Module):\n    \n    def __init__(self, densenet):\n        super(DenseNetBackbone, self).__init__()\n        self.densenet = densenet\n    \n    def forward(self, x):\n        x = self.densenet.features(x)\n        return x\n    \n    \ndef _densenet_pose(cmap_channels, paf_channels, upsample_channels, densenet, feature_channels, num_upsample, num_flat):\n    model = torch.nn.Sequential(\n        DenseNetBackbone(densenet),\n        CmapPafHead(feature_channels, cmap_channels, paf_channels, upsample_channels, num_upsample=num_upsample, num_flat=num_flat)\n    )\n    return model\n    \n    \ndef _densenet_pose_att(cmap_channels, paf_channels, upsample_channels, densenet, feature_channels, num_upsample, num_flat):\n    model = torch.nn.Sequential(\n        DenseNetBackbone(densenet),\n        CmapPafHeadAttention(feature_channels, cmap_channels, paf_channels, upsample_channels, num_upsample=num_upsample, num_flat=num_flat)\n    )\n    return model\n\n    \ndef densenet121_baseline(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    densenet = torchvision.models.densenet121(pretrained=pretrained)\n    return _densenet_pose(cmap_channels, paf_channels, upsample_channels, densenet, 1024, num_upsample, num_flat)\n\n\ndef densenet169_baseline(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    densenet = torchvision.models.densenet169(pretrained=pretrained)\n    return _densenet_pose(cmap_channels, paf_channels, upsample_channels, densenet, 1664, num_upsample, num_flat)\n\n\ndef densenet201_baseline(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    densenet = torchvision.models.densenet201(pretrained=pretrained)\n    return _densenet_pose(cmap_channels, paf_channels, upsample_channels, densenet, 1920, num_upsample, num_flat)\n\n\ndef densenet161_baseline(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    densenet = torchvision.models.densenet161(pretrained=pretrained)\n    return _densenet_pose(cmap_channels, paf_channels, upsample_channels, densenet, 2208, num_upsample, num_flat)\n\n\n    \ndef densenet121_baseline_att(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    densenet = torchvision.models.densenet121(pretrained=pretrained)\n    return _densenet_pose_att(cmap_channels, paf_channels, upsample_channels, densenet, 1024, num_upsample, num_flat)\n\n\ndef densenet169_baseline_att(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    densenet = torchvision.models.densenet169(pretrained=pretrained)\n    return _densenet_pose_att(cmap_channels, paf_channels, upsample_channels, densenet, 1664, num_upsample, num_flat)\n\n\ndef densenet201_baseline_att(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    densenet = torchvision.models.densenet201(pretrained=pretrained)\n    return _densenet_pose_att(cmap_channels, paf_channels, upsample_channels, densenet, 1920, num_upsample, num_flat)\n\n\ndef densenet161_baseline_att(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    densenet = torchvision.models.densenet161(pretrained=pretrained)\n    return _densenet_pose_att(cmap_channels, paf_channels, upsample_channels, densenet, 2208, num_upsample, num_flat)\n\n\n'"
trt_pose/models/dla.py,2,"b""import sys\nimport os\nimport torch\nsys.path.append(os.path.join(os.path.dirname(__file__), 'dla'))\nimport dla_up\n\n\nclass DlaWrapper(torch.nn.Module):\n    def __init__(self, dla_fn, cmap_channels, paf_channels):\n        super(DlaWrapper, self).__init__()\n        self.backbone = dla_fn(cmap_channels + paf_channels, pretrained_base='imagenet')\n        self.cmap_channels = cmap_channels\n        self.paf_channels = paf_channels\n        \n    def forward(self, x):\n        x = self.backbone(x)\n        cmap, paf = torch.split(x, [self.cmap_channels, self.paf_channels], dim=1)\n        return cmap, paf\n    \n    \ndef dla34up_pose(cmap_channels, paf_channels):\n    return DlaWrapper(dla_up.dla34up, cmap_channels, paf_channels)\n\ndef dla60up_pose(cmap_channels, paf_channels):\n    return DlaWrapper(dla_up.dla60up, cmap_channels, paf_channels)\n\ndef dla102up_pose(cmap_channels, paf_channels):\n    return DlaWrapper(dla_up.dla102up, cmap_channels, paf_channels)\n\ndef dla169up_pose(cmap_channels, paf_channels):\n    return DlaWrapper(dla_up.dla169up, cmap_channels, paf_channels)"""
trt_pose/models/mnasnet.py,3,"b'import torch\nimport torchvision\nfrom .common import *\n\n\nclass MnasnetBackbone(torch.nn.Module):\n    \n    def __init__(self, backbone):\n        super(MnasnetBackbone, self).__init__()\n        self.backbone = backbone\n    \n    def forward(self, x):\n        x = self.backbone.layers(x)\n        return x\n    \n    \ndef _mnasnet_pose(cmap_channels, paf_channels, upsample_channels, backbone, feature_channels, num_upsample, num_flat):\n    model = torch.nn.Sequential(\n        MnasnetBackbone(backbone),\n        CmapPafHead(feature_channels, cmap_channels, paf_channels, upsample_channels, num_upsample=num_upsample, num_flat=num_flat)\n    )\n    return model\n    \n    \ndef _mnasnet_pose_att(cmap_channels, paf_channels, upsample_channels, backbone, feature_channels, num_upsample, num_flat):\n    model = torch.nn.Sequential(\n        MnasnetBackbone(backbone),\n        CmapPafHeadAttention(feature_channels, cmap_channels, paf_channels, upsample_channels, num_upsample=num_upsample, num_flat=num_flat)\n    )\n    return model\n\n\ndef mnasnet0_5_baseline(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    backbone = torchvision.models.mnasnet0_5(pretrained=pretrained)\n    return _mnasnet_pose(cmap_channels, paf_channels, upsample_channels, backbone, 1280, num_upsample, num_flat)\n\n\ndef mnasnet0_75_baseline(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    backbone = torchvision.models.mnasnet0_75(pretrained=pretrained)\n    return _mnasnet_pose(cmap_channels, paf_channels, upsample_channels, backbone, 1280, num_upsample, num_flat)\n\n\ndef mnasnet1_0_baseline(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    backbone = torchvision.models.mnasnet1_0(pretrained=pretrained)\n    return _mnasnet_pose(cmap_channels, paf_channels, upsample_channels, backbone, 1280, num_upsample, num_flat)\n\n\ndef mnasnet1_3_baseline(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    backbone = torchvision.models.mnasnet1_3(pretrained=pretrained)\n    return _mnasnet_pose(cmap_channels, paf_channels, upsample_channels, backbone, 1280, num_upsample, num_flat)\n\n\n\ndef mnasnet0_5_baseline_att(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    backbone = torchvision.models.mnasnet0_5(pretrained=pretrained)\n    return _mnasnet_pose_att(cmap_channels, paf_channels, upsample_channels, backbone, 1280, num_upsample, num_flat)\n\n\ndef mnasnet0_75_baseline_att(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    backbone = torchvision.models.mnasnet0_75(pretrained=pretrained)\n    return _mnasnet_pose_att(cmap_channels, paf_channels, upsample_channels, backbone, 1280, num_upsample, num_flat)\n\n\ndef mnasnet1_0_baseline_att(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    backbone = torchvision.models.mnasnet1_0(pretrained=pretrained)\n    return _mnasnet_pose_att(cmap_channels, paf_channels, upsample_channels, backbone, 1280, num_upsample, num_flat)\n\n\ndef mnasnet1_3_baseline_att(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    backbone = torchvision.models.mnasnet1_3(pretrained=pretrained)\n    return _mnasnet_pose_att(cmap_channels, paf_channels, upsample_channels, backbone, 1280, num_upsample, num_flat)'"
trt_pose/models/resnet.py,4,"b'import torch\nimport torchvision\nfrom .common import *\n\n\nclass ResNetBackbone(torch.nn.Module):\n    \n    def __init__(self, resnet):\n        super(ResNetBackbone, self).__init__()\n        self.resnet = resnet\n    \n    def forward(self, x):\n        \n        x = self.resnet.conv1(x)\n        x = self.resnet.bn1(x)\n        x = self.resnet.relu(x)\n        x = self.resnet.maxpool(x)\n\n        x = self.resnet.layer1(x) # /4\n        x = self.resnet.layer2(x) # /8\n        x = self.resnet.layer3(x) # /16\n        x = self.resnet.layer4(x) # /32\n\n        return x\n\n\ndef _resnet_pose(cmap_channels, paf_channels, upsample_channels, resnet, feature_channels, num_upsample, num_flat):\n    model = torch.nn.Sequential(\n        ResNetBackbone(resnet),\n        CmapPafHead(feature_channels, cmap_channels, paf_channels, upsample_channels, num_upsample=num_upsample, num_flat=num_flat)\n    )\n    return model\n    \n    \ndef resnet18_baseline(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    resnet = torchvision.models.resnet18(pretrained=pretrained)\n    return _resnet_pose(cmap_channels, paf_channels, upsample_channels, resnet, 512, num_upsample, num_flat)\n\n\ndef resnet34_baseline(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    resnet = torchvision.models.resnet34(pretrained=pretrained)\n    return _resnet_pose(cmap_channels, paf_channels, upsample_channels, resnet, 512, num_upsample, num_flat)\n\n\ndef resnet50_baseline(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    resnet = torchvision.models.resnet50(pretrained=pretrained)\n    return _resnet_pose(cmap_channels, paf_channels, upsample_channels, resnet, 2048, num_upsample, num_flat)\n\n\ndef resnet101_baseline(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    resnet = torchvision.models.resnet101(pretrained=pretrained)\n    return _resnet_pose(cmap_channels, paf_channels, upsample_channels, resnet, 2048, num_upsample, num_flat)\n\n\ndef resnet152_baseline(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    resnet = torchvision.models.resnet152(pretrained=pretrained)\n    return _resnet_pose(cmap_channels, paf_channels, upsample_channels, resnet, 2048, num_upsample, num_flat)\n\n\ndef _resnet_pose(cmap_channels, paf_channels, upsample_channels, resnet, feature_channels, num_upsample, num_flat):\n    model = torch.nn.Sequential(\n        ResNetBackbone(resnet),\n        CmapPafHead(feature_channels, cmap_channels, paf_channels, upsample_channels, num_upsample=num_upsample, num_flat=num_flat)\n    )\n    return model\n  \n    \ndef _resnet_pose_att(cmap_channels, paf_channels, upsample_channels, resnet, feature_channels, num_upsample, num_flat):\n    model = torch.nn.Sequential(\n        ResNetBackbone(resnet),\n        CmapPafHeadAttention(feature_channels, cmap_channels, paf_channels, upsample_channels, num_upsample=num_upsample, num_flat=num_flat)\n    )\n    return model\n\n    \ndef resnet18_baseline_att(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    resnet = torchvision.models.resnet18(pretrained=pretrained)\n    return _resnet_pose_att(cmap_channels, paf_channels, upsample_channels, resnet, 512, num_upsample, num_flat)\n\n\ndef resnet34_baseline_att(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    resnet = torchvision.models.resnet34(pretrained=pretrained)\n    return _resnet_pose_att(cmap_channels, paf_channels, upsample_channels, resnet, 512, num_upsample, num_flat)\n\n\ndef resnet50_baseline_att(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    resnet = torchvision.models.resnet50(pretrained=pretrained)\n    return _resnet_pose_att(cmap_channels, paf_channels, upsample_channels, resnet, 2048, num_upsample, num_flat)\n\n\ndef resnet101_baseline_att(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    resnet = torchvision.models.resnet101(pretrained=pretrained)\n    return _resnet_pose_att(cmap_channels, paf_channels, upsample_channels, resnet, 2048, num_upsample, num_flat)\n\n\ndef resnet152_baseline_att(cmap_channels, paf_channels, upsample_channels=256, pretrained=True, num_upsample=3, num_flat=0):\n    resnet = torchvision.models.resnet152(pretrained=pretrained)\n    return _resnet_pose_att(cmap_channels, paf_channels, upsample_channels, resnet, 2048, num_upsample, num_flat)'"
trt_pose/utils/__init__.py,0,b''
trt_pose/utils/export_for_isaac.py,10,"b'\'\'\'\nThe MIT License (MIT)\nCopyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and\nassociated documentation files (the ""Software""), to deal in the Software without restriction,\nincluding without limitation the rights to use, copy, modify, merge, publish, distribute,\nsublicense, and/or sell copies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included\nin all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT\nNOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\nDAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\'\'\'\n\n""""""\nusage: export_for_isaac.py [-h] --input_checkpoint INPUT_CHECKPOINT\n                           [--input_model INPUT_MODEL]\n                           [--input_topology INPUT_TOPOLOGY]\n                           [--input_width INPUT_WIDTH]\n                           [--input_height INPUT_HEIGHT]\n                           [--output_model OUTPUT_MODEL]\n\nexample: ./export_for_isaac.py --input_checkpoint resnet18_baseline_att_224x224_A_epoch_249.pth\n\nNVIDIA AI IOT \'TensorRT Pose Estimation\' to Isaac \'OpenPose Inference\' model conversion tool.\nConverts the NVIDIA-AI-IOT/trt_pose neural network from the PyTorch into ONNX format compatible\nwith the Isaac OpenPose Inference codelet.\n\nPlease refer to Isaac Documentation and https://github.com/NVIDIA-AI-IOT/trt_pose documentation.\n\nRequired arguments:\n  --input_checkpoint INPUT_CHECKPOINT\n                        Input model weights (.pth)\n\nOptional arguments:\n  -h, --help            show this help message and exit\n  --input_model INPUT_MODEL\n                        Input model (trt_pose.models function)\n  --input_topology INPUT_TOPOLOGY\n                        Input topology (.json)\n  --input_width INPUT_WIDTH\n                        Input image width\n  --input_height INPUT_HEIGHT\n                        Input image width\n  --output_model OUTPUT_MODEL\n                        Output ONNX model (.onnx)\n""""""\n\nimport argparse, json, os, re\nimport torch                    # Please refer to: https://pytorch.org/get-started/locally/\nimport trt_pose.models\n\n\nclass InputReNormalization(torch.nn.Module):\n    """"""\n        This defines ""(input - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]"" custom operation\n        to conform to ""Unit"" normalized input RGB data.\n    """"""\n    def __init__(self):\n        super(InputReNormalization, self).__init__()\n        self.mean = torch.Tensor([0.485, 0.456, 0.406]).reshape((1,3,1,1)).cuda()\n        self.std = torch.Tensor([0.229, 0.224, 0.225]).reshape((1,3,1,1)).cuda()\n\n    def forward(self, x):\n        return (x - self.mean) / self.std\n\n\nclass HeatmapMaxpoolAndPermute(torch.nn.Module):\n    """"""\n        This defines MaxPool2d(kernel_size = 3, stride = 1) and permute([0,2,3,1]) custom operation\n        to conform to [part_affinity_fields, heatmap, maxpool_heatmap] output format.\n    """"""\n    def __init__(self):\n        super(HeatmapMaxpoolAndPermute, self).__init__()\n        self.maxpool = torch.nn.MaxPool2d(3, stride=1, padding=1)\n\n    def forward(self, x):\n        heatmap, part_affinity_fields = x\n        maxpool_heatmap = self.maxpool(heatmap)\n\n        part_affinity_fields = part_affinity_fields.permute([0,2,3,1])\n        heatmap = heatmap.permute([0,2,3,1])\n        maxpool_heatmap = maxpool_heatmap.permute([0,2,3,1])\n        return [part_affinity_fields, heatmap, maxpool_heatmap]\n\ndef main(args):\n    """"""\n    Loads PyTorch model from args.input_checkpoint, converts and saves it into args.output_model path\n\n    Arguments:\n    args: the parsed command line arguments\n    """"""\n\n\n    # Load model topology and define the model\n    if not args.input_topology.endswith("".json"") or not os.path.exists(args.input_topology):\n        raise SystemExit(""Input topology %s is not a valid (.json) file."" % args.input_topology)\n\n    with open(args.input_topology, \'r\') as f:\n        topology = json.load(f)\n\n    num_parts, num_links = len(topology[\'keypoints\']), len(topology[\'skeleton\'])\n    model = trt_pose.models.MODELS[args.input_model](num_parts, num_links * 2).cuda().eval()\n\n    # Load model weights\n    if not args.input_checkpoint.endswith("".pth""):\n        raise SystemExit(""Unsupported input model %s. Only (.pth) model weights are supported."" %\n                        args.input_checkpoint)\n\n    if not os.path.exists(args.input_checkpoint):\n        raise SystemExit(""Input model file %s doesn\'t exists."" % args.input_checkpoint)\n\n    model.load_state_dict(torch.load(args.input_checkpoint))\n\n    # Add InputReNormalization pre-processing and HeatmapMaxpoolAndPermute post-processing operations\n    converted_model = torch.nn.Sequential(InputReNormalization(), model, HeatmapMaxpoolAndPermute())\n\n    # Define input and output names for ONNX exported model.\n    input_names = [""input""]\n    output_names = [""part_affinity_fields"", ""heatmap"", ""maxpool_heatmap""]\n\n    # Export the model to ONNX.\n    dummy_input = torch.zeros((1, 3, args.input_height, args.input_width)).cuda()\n    torch.onnx.export(converted_model, dummy_input, args.output_model,\n                      input_names=input_names, output_names=output_names)\n    print(""Successfully completed convertion of %s to %s."" % (args.input_checkpoint, args.output_model))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description = ""NVIDIA AI IOT \'TensorRT Pose Estimation\' to ""\n        ""Isaac \'OpenPose Inference\' model conversion tool. Converts the NVIDIA-AI-IOT/trt_pose ""\n        ""neural network from the PyTorch into ONNX format compatible with the Isaac OpenPose ""\n        ""Inference codelet. Please refer to Isaac Documentation and ""\n        ""https://github.com/NVIDIA-AI-IOT/trt_pose documentation."", epilog = ""example:""\n        ""./export_for_isaac.py --input_checkpoint resnet18_baseline_att_224x224_A_epoch_249.pth"")\n\n    parser.add_argument(""--input_checkpoint"", help=""Input checkpoint file (.pth)"", required = True)\n    parser.add_argument(""--input_model"", help=""Input model (trt_pose.models function)"", required = False)\n    parser.add_argument(""--input_topology"", help=""Input topology (.json)"", default = ""human_pose.json"")\n    parser.add_argument(""--input_width"", help=""Input image width"", type = int, required = False)\n    parser.add_argument(""--input_height"", help=""Input image width"", type = int, required = False)\n    parser.add_argument(""--output_model"", help=""Output ONNX model (.onnx)"", required = False)\n    args = parser.parse_args()\n\n    if args.input_topology is None:\n        args.input_topology = ""human_pose.json""\n        print(""Input topology is not specified, using %s as a default."" % args.input_topology)\n\n    if args.input_model is None:\n        match = re.match(""^(\\w+)_\\d+x\\d+\\w+.pth"", args.input_checkpoint)\n        if match:\n            args.input_model, = match.groups()\n            print(""Input model is not specified, using %s as a default."" % args.input_model)\n        else:\n            raise SystemExit(""Input model is not specified and can not be inferenced from the ""\n                             ""name of the checkpoint %s. Please specify the model name ""\n                             ""(trt_pose.models function name). "" % args.input_checkpoint)\n\n    if args.input_width is None or args.input_height is None:\n        match = re.match(""^\\w+_(\\d+)x(\\d+)_\\w+.pth"", args.input_checkpoint)\n        if match:\n            args.input_height,args.input_width = map(int, match.groups())\n            print(""Input width/height are not specified, using %dx%d as a default."" %\n                  (args.input_height, args.input_width))\n        else:\n            raise SystemExit(""Input model height or width is not specified and can not be ""\n                            ""inferenced from the name of the checkpoint %s. Please specify the ""\n                            ""model height and width (for example 480x640)"" % args.input_checkpoint)\n\n\n\n    if args.output_model is None:\n        args.output_model = os.path.splitext(args.input_checkpoint)[0] + "".onnx""\n        print(""Output path is not specified, using %s as a default."" % args.output_model)\n\n    main(args)\n'"
