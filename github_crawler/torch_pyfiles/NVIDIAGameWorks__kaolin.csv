file_path,api_count,code
setup.py,5,"b'import os\nimport io\nimport logging\nfrom setuptools import setup, find_packages\nfrom pkg_resources import parse_version\nimport copy\n\nimport torch\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension, CppExtension\nimport torchvision\nimport numpy as np\n\n\ncwd = os.path.dirname(os.path.abspath(__file__))\nlogger = logging.getLogger()\nlogging.basicConfig(format=\'%(levelname)s - %(message)s\')\n\n\nif not torch.cuda.is_available():\n    # From: https://github.com/NVIDIA/apex/blob/b66ffc1d952d0b20d6706ada783ae5b23e4ee734/setup.py\n    # Extension builds after https://github.com/pytorch/pytorch/pull/23408 attempt to query torch.cuda.get_device_capability(),\n    # which will fail if you are compiling in an environment without visible GPUs (e.g. during an nvidia-docker build command).\n    logging.warning(\'\\nWarning: Torch did not find available GPUs on this system.\\n\'\n                    \'If your intention is to cross-compile, this is not an error.\\n\'\n                    \'By default, Kaolin will cross-compile for Pascal (compute capabilities 6.0, 6.1, 6.2),\\n\'\n                    \'Volta (compute capability 7.0), and Turing (compute capability 7.5).\\n\'\n                    \'If you wish to cross-compile for a single specific architecture,\\n\'\n                    \'export TORCH_CUDA_ARCH_LIST=""compute capability"" before running setup.py.\\n\')\n    if os.environ.get(""TORCH_CUDA_ARCH_LIST"", None) is None:\n        os.environ[""TORCH_CUDA_ARCH_LIST""] = ""6.0;6.1;6.2;7.0;7.5""\n\nPACKAGE_NAME = \'kaolin\'\nVERSION = \'0.1.0\'\nDESCRIPTION = \'Kaolin: A PyTorch library for accelerating 3D deep learning research\'\nURL = \'https://github.com/NVIDIAGameWorks/kaolin\'\nAUTHOR = \'NVIDIA\'\nLICENSE = \'Apache License 2.0\'\nDOWNLOAD_URL = \'\'\nLONG_DESCRIPTION = """"""\nKaolin is a PyTorch library aiming to accelerate 3D deep learning research. Kaolin provides efficient implementations\nof differentiable 3D modules for use in deep learning systems. With functionality to load and preprocess several popular\n3D datasets, and native functions to manipulate meshes, pointclouds, signed distance functions, and voxel grids, Kaolin\nmitigates the need to write wasteful boilerplate code. Kaolin packages together several differentiable graphics modules\nincluding rendering, lighting, shading, and view warping. Kaolin also supports an array of loss functions and evaluation\nmetrics for seamless evaluation and provides visualization functionality to render the 3D results. Importantly, we curate\na comprehensive model zoo comprising many state-of-the-art 3D deep learning architectures, to serve as a starting point\nfor future research endeavours.\n""""""\n\n\n\n# Check that PyTorch version installed meets minimum requirements\ntorch_ver = parse_version(torch.__version__)\nif torch_ver < parse_version(\'1.2.0\') or torch_ver >= parse_version(\'1.5.0\'):\n    logger.warning(f\'Kaolin is tested with PyTorch >=1.2.0, <1.5.0 Found version {torch.__version__} instead.\')\n\n# Check that torchvision version installed meets minimum requirements\ntorchvision_ver = parse_version(torchvision.__version__)\nif torchvision_ver < parse_version(\'0.4.0\') or torchvision_ver >= parse_version(\'0.6.0\'):\n    logger.warning(f\'Kaolin is tested with torchvision >=0.4.0, <0.6.0 Found version {torchvision.__version__} instead.\')\n\n# Get version number from version.py\nversion = {}\nwith open(""kaolin/version.py"") as fp:\n    exec(fp.read(), version)\n\n\ndef build_deps():\n    print(\'Building nv-usd...\')\n    os.system(\'./buildusd.sh\')\n\n\ndef read(*names, **kwargs):\n    with io.open(\n            os.path.join(os.path.dirname(__file__), *names),\n            encoding=kwargs.get(\'encoding\', \'utf8\')\n    ) as fp:\n        return fp.read()\n\n\ndef KaolinCUDAExtension(*args, **kwargs):\n    if not os.name == \'nt\':\n        FLAGS = [\'-Wno-deprecated-declarations\']\n        kwargs = copy.deepcopy(kwargs)\n        if \'extra_compile_args\' in kwargs:\n            kwargs[\'extra_compile_args\'] += FLAGS\n        else:\n            kwargs[\'extra_compile_args\'] = FLAGS\n\n    return CUDAExtension(*args, **kwargs)\n\n\nclass KaolinBuildExtension(BuildExtension):\n    def __init__(self, *args, **kwargs):\n        kwargs = copy.deepcopy(kwargs)\n        kwargs[\'use_ninja\'] = False  # ninja is interfering with compiling separate extensions in parallel\n        super().__init__(*args, **kwargs)\n\n    def build_extensions(self):\n        if not os.name == \'nt\':\n            FLAG_BLACKLIST = [\'-Wstrict-prototypes\']\n            FLAGS = [\'-Wno-deprecated-declarations\']\n            self.compiler.compiler_so = [x for x in self.compiler.compiler_so if x not in FLAG_BLACKLIST] + FLAGS  # Covers non-cuda\n\n        super().build_extensions()\n\n\ndef get_extensions():\n    use_cython = os.getenv(\'USE_CYTHON\')\n    ext = \'.pyx\' if use_cython else \'.cpp\'\n    cython_extensions = [\n        CppExtension(\n            \'kaolin.triangle_hash\',\n            sources=[\n                f\'kaolin/cython/triangle_hash{ext}\'\n            ],\n        ),\n        CppExtension(\n            \'kaolin.triangle_hash\',\n            sources=[\n                f\'kaolin/cython/triangle_hash{ext}\'\n            ],\n        ),\n        CppExtension(\n            \'kaolin.mise\',\n            sources=[\n                f\'kaolin/cython/mise{ext}\'\n            ],\n        ),\n        CppExtension(\n            \'kaolin.mcubes\',\n            sources=[\n                f\'kaolin/cython/mcubes{ext}\',\n                \'kaolin/cython/pywrapper.cpp\',\n                \'kaolin/cython/marchingcubes.cpp\'\n            ],\n            extra_compile_args=[\'-std=c++11\'],\n        ),\n        CppExtension(\n            \'kaolin.nnsearch\',\n            sources=[\n                f\'kaolin/cython/nnsearch{ext}\'\n            ],\n        ),\n    ]\n\n    # If building with readthedocs, don\'t compile CUDA extensions\n    if os.getenv(\'READTHEDOCS\') != \'True\':\n        # For every cuda extension, ensure that their is a corresponding function in\n        # docs/conf.py under `autodoc_mock_imports`.\n        cuda_extensions = [\n            KaolinCUDAExtension(\'kaolin.cuda.load_textures\', [\n                \'kaolin/cuda/load_textures_cuda.cpp\',\n                \'kaolin/cuda/load_textures_cuda_kernel.cu\',\n            ]),\n            KaolinCUDAExtension(\'kaolin.cuda.sided_distance\', [\n                \'kaolin/cuda/sided_distance.cpp\',\n                \'kaolin/cuda/sided_distance_cuda.cu\',\n            ]),\n            KaolinCUDAExtension(\'kaolin.cuda.furthest_point_sampling\', [\n                \'kaolin/cuda/furthest_point_sampling.cpp\',\n                \'kaolin/cuda/furthest_point_sampling_cuda.cu\',\n            ]),\n            KaolinCUDAExtension(\'kaolin.cuda.ball_query\', [\n                \'kaolin/cuda/ball_query.cpp\',\n                \'kaolin/cuda/ball_query_cuda.cu\',\n            ]),\n            KaolinCUDAExtension(\'kaolin.cuda.three_nn\', [\n                \'kaolin/cuda/three_nn.cpp\',\n                \'kaolin/cuda/three_nn_cuda.cu\',\n            ]),\n            KaolinCUDAExtension(\'kaolin.cuda.tri_distance\', [\n                \'kaolin/cuda/triangle_distance.cpp\',\n                \'kaolin/cuda/triangle_distance_cuda.cu\',\n            ]),\n            KaolinCUDAExtension(\'kaolin.cuda.mesh_intersection\', [\n                \'kaolin/cuda/mesh_intersection.cpp\',\n                \'kaolin/cuda/mesh_intersection_cuda.cu\',\n            ]),\n            KaolinCUDAExtension(\'kaolin.graphics.nmr.cuda.rasterize_cuda\', [\n                \'kaolin/graphics/nmr/cuda/rasterize_cuda.cpp\',\n                \'kaolin/graphics/nmr/cuda/rasterize_cuda_kernel.cu\',\n            ]),\n            KaolinCUDAExtension(\'kaolin.graphics.softras.soft_rasterize_cuda\', [\n                \'kaolin/graphics/softras/cuda/soft_rasterize_cuda.cpp\',\n                \'kaolin/graphics/softras/cuda/soft_rasterize_cuda_kernel.cu\',\n            ]),\n            KaolinCUDAExtension(\'kaolin.graphics.dib_renderer.cuda.rasterizer\', [\n                \'kaolin/graphics/dib_renderer/cuda/rasterizer.cpp\',\n                \'kaolin/graphics/dib_renderer/cuda/rasterizer_cuda.cu\',\n                \'kaolin/graphics/dib_renderer/cuda/rasterizer_cuda_back.cu\',\n            ]),\n        ]\n    else:\n        cuda_extensions = []\n\n    if use_cython:\n        from Cython.Build import cythonize\n        from Cython.Compiler import Options\n        compiler_directives = Options.get_directive_defaults()\n        compiler_directives[""emit_code_comments""] = False\n        cython_extensions = cythonize(cython_extensions, language=\'c++\',\n                                      compiler_directives=compiler_directives)\n\n    return cython_extensions + cuda_extensions\n\n\ncwd = os.path.dirname(os.path.abspath(__file__))\n\n\ndef get_requirements():\n    return [\n        \'matplotlib<3.0.0\',\n        \'scikit-image==0.16.2\',\n        \'trimesh>=3.0\',\n        \'scipy==1.4.1\',\n        \'tqdm==4.32.1\',\n        \'pptk==0.1.0\',\n        \'pillow<7.0.0\',\n    ]\n\n\nif __name__ == \'__main__\':\n    build_deps()\n    setup(\n        # Metadata\n        name=PACKAGE_NAME,\n        version=version[\'__version__\'],\n        author=AUTHOR,\n        description=DESCRIPTION,\n        url=URL,\n        long_description=LONG_DESCRIPTION,\n        license=LICENSE,\n        python_requires=\'~=3.6\',\n\n        # Package info\n        packages=find_packages(exclude=(\'docs\', \'test\', \'examples\')),\n        install_requires=get_requirements(),\n        zip_safe=True,\n        ext_modules=get_extensions(),\n        include_dirs=[np.get_include()],\n        cmdclass={\'build_ext\': KaolinBuildExtension}\n    )\n'"
docs/conf.py,1,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\nimport os\nimport sys\n\n\n# Get version number\nwith open(\'../kaolin/version.py\', \'r\') as f:\n    for row in f:\n        if \'__version__\' in row:\n            kal_version = row.split(""\'"")[-2]\n            break\n\n# sys.path.append(\'../\')\nsys.path.append(os.path.abspath(os.path.join(\n    os.path.dirname(__file__), \'..\', \'kaolin\')))\n# import sphinx_rtd_theme\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'kaolin\'\ncopyright = \'2019, NVIDIA Development Inc.\'\nauthor = \'NVIDIA\'\nversion = kal_version\n# The full version, including alpha/beta/rc tags\nrelease = kal_version\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.viewcode\',\n]\n\nnapoleon_use_ivar = True\n\n# Mock CUDA Imports\nautodoc_mock_imports = [\'kaolin.cuda.ball_query\',\n                        \'kaolin.cuda.load_textures\',\n                        \'kaolin.cuda.sided_distance\',\n                        \'kaolin.cuda.furthest_point_sampling\',\n                        \'kaolin.cuda.three_nn\',\n                        \'kaolin.cuda.tri_distance\',\n                        \'kaolin.cuda.mesh_intersection\',\n                        \'kaolin.graphics.nmr.cuda.rasterize_cuda\',\n                        \'kaolin.graphics.softras.soft_rasterize_cuda\',\n                        \'kaolin.graphics.dib_renderer.cuda.rasterizer\']\n\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n\n# The name of the Pygments (syntax highlighting) style to use\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing\ntodo_include_todos = True\n\n# Do not prepend module name to functions\nadd_module_names = False\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\n# html_theme = \'alabaster\'\nhtml_theme = \'sphinx_rtd_theme\'\nhtml_theme_path = [\'_themes\']\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\nhtml_css_files = [\n    \'copybutton.css\',\n]\nhtml_js_files = [\n    \'clipboard.min.js\',\n    \'copybutton.js\',\n]\n\n\n# -- Options for LaTeX output -------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\')\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\')\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Font packages\n    \'fontpkg\': \'\\\\usepackage{amsmath, amsfonts, amssymb, amsthm}\'\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LateX files. List of tuples\n# (source start file, target name, title,\n# author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'kaolin.tex\', u\'kaolin Documentation\',\n        [author], 1),\n]\n\n\n# -- Options for manual page output -------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'kaolin\', u\'kaolin Documentation\', \n        [author], 1)\n]\n\n\n# -- Options for Texinfo output -----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author, \n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'kaolin\', \'kaolin Documentation\', \n        author, \'kaolin\', \'NVIDIA 3D Deep Learning Library.\', \n        \'Miscellaneous\'),\n]\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/3\', None),\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy\', None),\n    \'torch\': (\'http://pytorch.org/docs/master\', None),\n}\n'"
kaolin/__init__.py,0,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .version import __version__\n\nfrom kaolin import conversions\nfrom kaolin import datasets\nfrom kaolin import graphics\nfrom kaolin import helpers\nfrom kaolin import mathutils\nfrom kaolin import rep\nfrom kaolin import vision\nfrom kaolin import visualize\nfrom kaolin import metrics\nfrom kaolin import models\nfrom kaolin import transforms\n\nfrom kaolin.engine import *\nfrom kaolin.mathutils import *\n'"
kaolin/helpers.py,15,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nSeveral helper functions, for internal use in Kaolin.\n""""""\nimport torch\nimport hashlib\nfrom pathlib import Path\nfrom typing import Callable\nimport numpy as np\n\n\ndef _composedecorator(*decs):\n    """"""Returns a composition of several decorators.\n\n    Source: https://stackoverflow.com/a/5409569\n\n    Usage::\n\n            @composedec(decorator1, decorator2)\n            def func_that_needs_decoration(args):\n                pass\n\n        is equavalent to::\n\n            @decorator1\n            @decorator2\n            def func_that_needs_decoration(args):\n                pass\n\n    """"""\n\n    def deco(f):\n        for dec in reversed(decs):\n            f = dec(f)\n        return f\n    return deco\n\n\ndef _normalize_zerosafe(matrix: torch.Tensor):\n    """"""Normalizes each row of a matrix in a \'division by zero\'-safe way.\n\n    Args:\n        matrix (torch.tensor): Matrix where each row contains a vector\n            to be normalized\n\n    """"""\n\n    assert matrix.dim() == 2, \'Need matrix to contain exactly 2 dimensions\'\n    magnitude = torch.sqrt(torch.sum(torch.pow(matrix, 2), dim=1))\n    valid_inds = magnitude > 0\n    matrix[valid_inds] = torch.div(\n        matrix[valid_inds], magnitude[valid_inds].unsqueeze(1))\n    return matrix\n\n\ndef _assert_tensor(inp):\n    """"""Asserts that the input is of type torch.Tensor. """"""\n    if not torch.is_tensor(inp):\n        raise TypeError(\'Expected input to be of type torch.Tensor.\'\n                        \' Got {0} instead\'.format(type(inp)))\n\n\ndef _assert_dim_gt(inp, tgt):\n    """"""Asserts that the number of dims in inp is greater than the\n    value sepecified in tgt. \n\n    Args:\n        inp (torch.Tensor): Input tensor, whose number of dimensions is\n            to be compared.\n        tgt (int): Value which the number of dims of inp should exceed.\n    """"""\n    if inp.dim() <= tgt:\n        raise ValueError(\'Expected input to contain more than {0} dims. \'\n                         \'Got {1} instead.\'.format(tgt, inp.dim()))\n\n\ndef _assert_dim_lt(inp, tgt):\n    """"""Asserts that the number of dims in inp is less than the\n    value sepecified in tgt. \n\n    Args:\n        inp (torch.Tensor): Input tensor, whose number of dimensions is\n            to be compared.\n        tgt (int): Value which the number of dims of inp should be less than.\n    """"""\n    if not inp.dim() >= tgt:\n        raise ValueError(\'Expected input to contain less than {0} dims. \'\n                         \'Got {1} instead.\'.format(tgt, inp.dim()))\n\n\ndef _assert_dim_ge(inp, tgt):\n    """"""Asserts that the number of dims in inp is greater than or equal to the\n    value sepecified in tgt. \n\n    Args:\n        inp (torch.Tensor): Input tensor, whose number of dimensions is\n            to be compared.\n        tgt (int): Value which the number of dims of inp should exceed.\n    """"""\n    if inp.dim() < tgt:\n        raise ValueError(\'Expected input to contain at least {0} dims. \'\n                         \'Got {1} instead.\'.format(tgt, inp.dim()))\n\n\ndef _assert_dim_le(inp, tgt):\n    """"""Asserts that the number of dims in inp is less than or equal to the\n    value sepecified in tgt. \n\n    Args:\n        inp (torch.Tensor): Input tensor, whose number of dimensions is\n            to be compared.\n        tgt (int): Value which the number of dims of inp should not exceed.\n    """"""\n    if inp.dim() > tgt:\n        raise ValueError(\'Expected input to contain at most {0} dims. \'\n                         \'Got {1} instead.\'.format(tgt, inp.dim()))\n\n\ndef _assert_dim_eq(inp, tgt):\n    """"""Asserts that the number of dims in inp is exactly equal to the\n    value sepecified in tgt. \n\n    Args:\n        inp (torch.Tensor): Input tensor, whose number of dimensions is\n            to be compared.\n        tgt (int): Value which the number of dims of inp should equal.\n    """"""\n    if inp.dim() != tgt:\n        raise ValueError(\'Expected input to contain exactly {0} dims. \'\n                         \'Got {1} instead.\'.format(tgt, inp.dim()))\n\n\ndef _assert_shape_eq(inp, tgt_shape, dim=None):\n    """"""Asserts that the shape of tensor `inp` is equal to the tuple `tgt_shape`\n    along dimension `dim`. If `dim` is None, shapes along all dimensions must\n    be equal.\n    """"""\n    if dim is None:\n        if inp.shape != tgt_shape:\n            raise ValueError(\'Size mismatch. Input and target have different \'\n                             \'shapes: {0} vs {1}.\'.format(inp.shape,\n                                                          tgt_shape))\n    else:\n        if inp.shape[dim] != tgt_shape[dim]:\n            raise ValueError(\'Size mismatch. Input and target have different \'\n                             \'shapes at dimension {2}: {0} vs {1}.\'.format(\n                                 inp.shape[dim], tgt_shape[dim], dim))\n\n\ndef _assert_gt(inp, val):\n    """"""Asserts that all elements in tensor `inp` are greater than value `val`.\n    """"""\n    if not (inp > val).all():\n        raise ValueError(\'Each element of input must be greater \'\n                         \'than {0}.\'.format(val))\n\n\ndef _get_hash(x):\n    """"""Generate a hash from a string, or dictionary.\n    """"""\n    if isinstance(x, dict):\n        x = tuple(sorted(pair for pair in x.items()))\n\n    return hashlib.md5(bytes(repr(x), \'utf-8\')).hexdigest()\n\n\nclass Cache(object):\n    """"""Caches the results of a function to disk.\n    If already cached, data is returned from disk, otherwise,\n    the function is executed. Output tensors are always on CPU device.\n\n        Args:\n            transforms (Iterable): List of transforms to compose.\n            cache_dir (str): Directory where objects will be cached. Default\n                             to \'cache\'.\n    """"""\n\n    def __init__(self, func: Callable, cache_dir: [str, Path], cache_key: str):\n        self.func = func\n        self.cache_dir = Path(cache_dir) / str(cache_key)\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n        self.cached_ids = [p.stem for p in self.cache_dir.glob(\'*\')]\n\n    def __call__(self, unique_id: str, *args, **kwargs):\n        """"""Execute self.func if not cached, otherwise, read data from disk.\n\n            Args:\n                unique_id (str): The unique id with which to name the cached file.\n                **kwargs: The arguments to be passed to self.func.\n\n            Returns:\n                dict of {str: torch.Tensor}: Dictionary of tensors.\n        """"""\n\n        fpath = self.cache_dir / f\'{unique_id}.p\'\n\n        if not fpath.exists():\n            output = self.func(*args, **kwargs)\n            self._write(output, fpath)\n            self.cached_ids.append(unique_id)\n        else:\n            output = self._read(fpath)\n\n        # Read file to move tensors to CPU.\n        return self._read(fpath)\n\n    def _write(self, x, fpath):\n        torch.save(x, fpath)\n\n    def _read(self, fpath):\n        return torch.load(fpath, map_location=\'cpu\')\n'"
kaolin/version.py,0,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n__version__ = \'0.1.0\'\n'"
tests/common.py,1,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n# Kornia components:\n# Copyright (C) 2017-2019, Arraiy, Inc., all rights reserved.\n# Copyright (C) 2019-    , Open Source Vision Foundation, all rights reserved.\n# Copyright (C) 2019-    , Kornia authors, all rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport pytest\n\n\n# From kornia\n# https://github.com/arraiyopensource/kornia/\ndef get_test_devices():\n    """"""Creates a list of strings indicating available devices to test on.\n    Checks for CUDA devices, primarily. Assumes CPU is always available.\n\n    Return:\n        list (str): list of device names\n\n    """"""\n\n    # Assumption: CPU is always available\n    devices = [\'cpu\']\n\n    if torch.cuda.is_available():\n        devices.append(\'cuda\')\n\n    return devices\n\n\n# Setup devices to run unit tests\nTEST_DEVICES = get_test_devices()\n\n\n@pytest.fixture()\ndef device_type(request):\n    typ = request.config.getoption(\'--typetest\')\n    return typ\n'"
tests/test_compute_adjacency_info.py,12,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport time\nimport numpy as np\nimport kaolin as kal\nfrom kaolin.rep import TriangleMesh\nfrom kaolin.rep import QuadMesh\nfrom kaolin.rep import Mesh\n\nmesh = TriangleMesh.from_obj((\'tests/model.obj\') )\nvertices = mesh.vertices\nfaces = mesh.faces\ndevice = vertices.device\n\nfaces_gpu = faces.cuda()\nvertices_gpu = vertices.cuda()\nedge2key, edges, vv, vv_count, ve, ve_count, vf, vf_count, ff, ff_count, ee, ee_count, \\\n    ef, ef_count = Mesh.compute_adjacency_info(vertices_gpu, faces_gpu)\n_edge2key, _edges, _vv, _vv_count, _ve, _ve_count, _vf, _vf_count,  _ff, _ff_count, _ee, \\\n    _ee_count, _ef, _ef_count = Mesh.old_compute_adjacency_info(vertices, faces)\n\nedges = edges.cpu()\nvv = vv.cpu()\nvv_count = vv_count.cpu()\nve = ve.cpu()\nve_count = ve_count.cpu()\nvf = vf.cpu()\nvf_count = vf_count.cpu()\nff = ff.cpu()\nff_count = ff_count.cpu()\nee = ee.cpu()\nee_count = ee_count.cpu()\nef = ef.cpu()\nef_count = ef_count.cpu()\n\nassert set(edge2key.keys()) == set(_edge2key.keys())\nassert set(edge2key.values()) == set(_edge2key.values())\n\n_key2key = {_key: edge2key[_edge] for _edge, _key in _edge2key.items()}\n_key2key_arr = torch.zeros(edges.shape[0], device=device, dtype=torch.long)\nfor _key, key in _key2key.items():\n    _key2key_arr[_key] = key\n_key2key[-1] = -1\nassert (edges[_key2key_arr] == _edges).all()\nassert (vv_count == _vv_count).all()\nassert (vv_count == torch.sum(vv != -1, dim=-1)).all()\nassert (torch.sort(vv, dim=1)[0] == torch.sort(_vv, dim=1)[0]).all()\nassert (ve_count == _ve_count).all()\nassert (ve_count == torch.sum(ve != -1, dim=-1)).all()\n\nfor i in range(vertices.shape[0]):\n    assert (torch.unique(ve[i, :ve_count[i]], sorted=True, dim=0) ==\n            torch.unique(_key2key_arr[_ve[i, :ve_count[i]]], sorted=True, dim=0)).all()\n\nassert (vf_count == _vf_count).all()\nassert (vf_count == torch.sum(vf != -1, dim=-1)).all()\nassert (torch.sort(vf, dim=1)[0] == torch.sort(_vf, dim=1)[0]).all()\n\nassert (ef_count[_key2key_arr] == _ef_count).all()\nassert (torch.sort(ef[_key2key_arr, :], dim=1)[0] == torch.sort(_ef, dim=1)[0]).all()\n\ntmp_ee = torch.full(_ee.shape, fill_value=-1, dtype=torch.long)\nfor i in range(_ee.shape[0]):\n    for j in range(_ee.shape[1]):\n        tmp_ee[i, j] = _key2key_arr[_ee[i, j]]\nassert (torch.sort(ee[_key2key_arr], dim=1)[0] == torch.sort(tmp_ee, dim=1)[0]).all()\n\nassert (ff_count == _ff_count).all()\nassert (torch.sort(ff, dim=1)[0] == torch.sort(_ff, dim=1)[0]).all()\n'"
tests/test_helpers.py,12,"b'import os\nimport shutil\n\nimport pytest\nimport torch\nimport numpy as np\n\nimport kaolin as kal\nfrom kaolin import helpers\n\n\nCACHE_DIR = \'tests/cache\'\n\n\n@pytest.fixture(autouse=True)\ndef cleanup():\n    """"""Cleanup after each test. """"""\n    yield\n    shutil.rmtree(CACHE_DIR)\n\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_cache_tensor(device):\n    tensor = torch.ones(5, device=device)\n\n    cache = helpers.Cache(func=lambda x: x, cache_dir=CACHE_DIR, cache_key=\'test\')\n    cache(\'tensor\', x=tensor)\n\n    # Make sure cache is created\n    assert os.path.exists(os.path.join(CACHE_DIR, \'test\', \'tensor.p\'))\n\n    # Confirm loaded tensor is correct and on CPU device\n    loaded = cache(\'tensor\')\n    assert torch.all(loaded.eq(tensor.cpu()))\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_cache_dict(device):\n    dictionary = {\n        \'a\': torch.ones(5, device=device),\n        \'b\': np.zeros(5),\n    }\n\n    cache = helpers.Cache(func=lambda x: x, cache_dir=CACHE_DIR, cache_key=\'test\')\n    cache(\'dictionary\', x=dictionary)\n\n    # Make sure cache is created\n    assert os.path.exists(os.path.join(CACHE_DIR, \'test\', \'dictionary.p\'))\n\n    # Confirm loaded dict is correct and on CPU device\n    loaded = cache(\'dictionary\')\n    assert torch.all(loaded[\'a\'].eq(dictionary[\'a\'].cpu()))\n    assert np.all(np.isclose(loaded[\'b\'], dictionary[\'b\']))\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_cache_mesh(device):\n    vertices = torch.ones(10, 3, device=device)\n    faces = torch.ones(20, 3, device=device, dtype=torch.long)\n    mesh = kal.rep.TriangleMesh.from_tensors(vertices, faces)\n\n    cache = helpers.Cache(func=lambda x: x, cache_dir=CACHE_DIR, cache_key=\'test\')\n    cache(\'mesh\', x=mesh)\n\n    # Make sure cache is created\n    assert os.path.exists(os.path.join(CACHE_DIR, \'test\', \'mesh.p\'))\n\n    # Confirm loaded mesh is correct and on CPU device\n    loaded = cache(\'mesh\')\n    assert torch.all(loaded.vertices.eq(vertices.cpu()))\n    assert torch.all(loaded.faces.eq(faces.cpu()))\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_cache_voxelgrid(device):\n    voxels = torch.ones(3, 3, 3, device=device)\n    voxelgrid = kal.rep.VoxelGrid(voxels)\n\n    cache = helpers.Cache(func=lambda x: x, cache_dir=CACHE_DIR, cache_key=\'test\')\n    cache(\'voxelgrid\', x=voxelgrid)\n\n    # Make sure cache is created\n    assert os.path.exists(os.path.join(CACHE_DIR, \'test\', \'voxelgrid.p\'))\n\n    # Confirm loaded voxelgrid is correct and on CPU device\n    loaded = cache(\'voxelgrid\')\n    assert torch.all(loaded.voxels.eq(voxels.cpu()))\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_cache_pointcloud(device):\n    points = torch.ones(10, 3, device=device)\n    pointcloud = kal.rep.PointCloud(points)\n\n    cache = helpers.Cache(func=lambda x: x, cache_dir=CACHE_DIR, cache_key=\'test\')\n    cache(\'pointcloud\', x=pointcloud)\n\n    # Make sure cache is created\n    assert os.path.exists(os.path.join(CACHE_DIR, \'test\', \'pointcloud.p\'))\n\n    # Confirm loaded pointcloud is correct and on CPU device\n    loaded = cache(\'pointcloud\')\n    assert torch.all(loaded.points.eq(points.cpu()))\n'"
tests/test_import.py,1,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport torch\n\nimport kaolin\n\n\n@pytest.mark.parametrize(\'batchsize, expected\', [(1,1), (2,2), (4,4), (8,8)])\ndef test_hello(batchsize, expected):\n\ta = torch.Tensor(batchsize, 20, 30)\n\tassert a.shape[0] == expected\n'"
examples/Classification/mesh_classification.py,7,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\n\nimport torch\n\nfrom tqdm import tqdm, trange\n\nimport kaolin as kal\nfrom kaolin.datasets import SHREC16\nfrom kaolin.models import MeshCNNClassifier\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--shrec-root"", type=str, required=True,\n                    help=""Path to root directory of SHREC16 data."")\nparser.add_argument(""--categories"", type=str, nargs=""+"", default=[""ants"", ""cat""],\n                    help=""List of categories to train on."")\nparser.add_argument(""--lr"", type=float, default=1e-3, help=""Learning rate"")\nparser.add_argument(""--device"", type=str, default=""cuda"",\n                    help=""Device to run training/evaluation on."")\nparser.add_argument(""--epochs"", type=int, default=1,\n                    help=""Number of epochs to train for."")\nargs = parser.parse_args()\n\n# Seed RNG, for repeatability.\ntorch.manual_seed(1234)\n\nmodel = MeshCNNClassifier(5, 2, [16, 32, 32], [1140, 780, 580], 100, 0, 750)\nmodel = model.to(args.device)\noptimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\nlossfn = torch.nn.functional.nll_loss\ndataset_train = SHREC16(\n    args.shrec_root, categories=args.categories, train=True\n)\ndataset_test = SHREC16(\n    args.shrec_root, categories=args.categories, train=False\n)\n\n\ndef prepare_mesh(mesh):\n    mesh.vertex_mask = torch.ones(mesh.vertices.shape[0]).to(mesh.vertices)\n    _, face_areas = kal.models.meshcnn.compute_face_normals_and_areas(mesh)\n    (\n        edge2key,\n        edges,\n        vv,\n        vv_count,\n        ve,\n        ve_count,\n        vf,\n        vf_count,\n        ff,\n        ff_count,\n        ee,\n        ee_count,\n        ef,\n        ef_count,\n    ) = kal.rep.Mesh.compute_adjacency_info(mesh.vertices, mesh.faces)\n\n    mesh.edges = edges\n    mesh.ve = ve\n    mesh.ve_count = ve_count\n    mesh.ee = ee\n    mesh.ee_count = ee_count\n\n    kal.models.meshcnn.build_gemm_representation(mesh, face_areas)\n    edge_points = kal.models.meshcnn.get_edge_points_vectorized(mesh)\n    kal.models.meshcnn.set_edge_lengths(mesh, edge_points)\n    kal.models.meshcnn.extract_meshcnn_features(mesh, edge_points)\n    mesh.edges_count = mesh.edges.shape[-2]\n    mesh.pool_count = 0\n\n    mesh.edges = mesh.edges.numpy()\n    mesh.faces = mesh.faces.numpy()\n    mesh.ve = list(mesh.ve.numpy())\n    mesh.ve = [\n        list([ve[j] for j in range(mesh.ve_count[i])]) for i, ve in enumerate(mesh.ve)\n    ]\n\n    return mesh\n\n\nmodel.train()\nfor e in trange(args.epochs):\n\n    # Shuffle the dataset_train\n    randperm = torch.randperm(len(dataset_train))\n\n    # Train\n    for idx in range(len(dataset_train)):\n\n        i = randperm[idx]\n        item = dataset_train[i]\n\n        mesh = prepare_mesh(item.data)\n\n        # Some SHREC meshes are ill-behaved (roughly 8% of the dataset_train). We ignore them.\n        x = None\n        try:\n            x = model(mesh.features.unsqueeze(0), [mesh])\n        except Exception:\n            pass\n            # print(""Skipping mesh:"", idx)\n\n        label = item.attributes[""label""]\n        label = torch.tensor([label], dtype=torch.long, device=args.device)\n        if x is not None:\n            loss = lossfn(x, label)\n            tqdm.write(f""Loss: {loss.item():.6}"")\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n# Test\ncorrect = 0\ntotal = 0\n\nmodel.eval()\n\nfor i in range(len(dataset_test)):\n\n    item = dataset_test[i]\n    mesh = prepare_mesh(item.data)\n    x = None\n    try:\n        x = model(mesh.features.unsqueeze(0), [mesh])\n    except Exception:\n        pass\n    label = item.attributes[""label""]\n    label = torch.tensor([label], dtype=torch.long, device=args.device)\n    if x is not None:\n        loss = lossfn(x, label)\n        pred = x.argmax()\n        if pred == label:\n            correct += 1\n        total += 1\nprint(""Accuracy:"", (correct * 100) / (total + 1e-6), ""(%)"")\n'"
examples/Classification/pointcloud_classification.py,9,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport time\n\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom kaolin.datasets import ModelNet\nfrom kaolin.models.PointNet import PointNetClassifier\nimport kaolin.transforms as tfs\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--modelnet-root\', type=str, help=\'Root directory of the ModelNet dataset.\')\nparser.add_argument(\'--categories\', type=str, nargs=\'+\',\n                    default=[\'chair\', \'sofa\'], help=\'list of object classes to use.\')\nparser.add_argument(\'--num-points\', type=int, default=1024, help=\'Number of points to sample from meshes.\')\nparser.add_argument(\'--epochs\', type=int, default=10, help=\'Number of train epochs.\')\nparser.add_argument(\'-lr\', \'--learning-rate\', type=float, default=1e-3, help=\'Learning rate.\')\nparser.add_argument(\'--batch-size\', type=int, default=12, help=\'Batch size.\')\nparser.add_argument(\'--viz-test\', action=\'store_true\', help=\'Visualize an output of a test sample\')\nparser.add_argument(\'--transforms-device\', type=str, default=\'cuda\', help=\'Device to use for data preprocessing.\')\nparser.add_argument(\'--workers\', type=int, default=4, help=\'number of workers used for each Dataloader\')\n\nargs = parser.parse_args()\n\n\ndef to_device(inp):\n    inp.to(args.transforms_device)\n    return inp\n\n\ntransform = tfs.Compose([\n    to_device,\n    tfs.TriangleMeshToPointCloud(num_samples=args.num_points),\n    tfs.NormalizePointCloud()\n])\n\nif args.transforms_device == \'cuda\':\n    num_workers = 0\n    pin_memory = False\nelse:\n    num_workers = args.workers\n    pin_memory = True\n\ntrain_loader = DataLoader(ModelNet(args.modelnet_root, categories=args.categories,\n                                   split=\'train\', transform=transform),\n                          batch_size=args.batch_size, shuffle=True,\n                          num_workers=num_workers, pin_memory=pin_memory)\n\nval_loader = DataLoader(ModelNet(args.modelnet_root, categories=args.categories,\n                                 split=\'test\', transform=transform),\n                        batch_size=args.batch_size,\n                        num_workers=num_workers, pin_memory=pin_memory)\n\nmodel = PointNetClassifier(num_classes=len(args.categories)).to(\'cuda\')\noptimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\ncriterion = torch.nn.CrossEntropyLoss()\nstart_time = time.time()\nfor e in range(args.epochs):\n\n    print(\'###################\')\n    print(\'Epoch:\', e)\n    print(\'###################\')\n\n    train_loss = 0.\n    train_accuracy = 0.\n    num_batches = 0\n\n    model.train()\n\n    for idx, (data, attributes) in enumerate(tqdm(train_loader)):\n        category = attributes[\'category\'].cuda()\n        pred = model(data.cuda())\n        loss = criterion(pred, category.view(-1))\n        train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        # Compute accuracy\n        pred_label = torch.argmax(pred, dim=1)\n        train_accuracy += torch.mean((pred_label == category.view(-1)).float()).detach().cpu().item()\n        num_batches += 1\n\n    print(\'Train loss:\', train_loss / num_batches)\n    print(\'Train accuracy:\', train_accuracy / num_batches)\n\n    val_loss = 0.\n    val_accuracy = 0.\n    num_batches = 0\n\n    model.eval()\n\n    with torch.no_grad():\n        for idx, (data, attributes) in enumerate(tqdm(val_loader)):\n            category = attributes[\'category\'].cuda()\n            pred = model(data.cuda())\n            loss = criterion(pred, category.view(-1))\n            val_loss += loss.item()\n\n            # Compute accuracy\n            pred_label = torch.argmax(pred, dim=1)\n            val_accuracy += torch.mean((pred_label == category.view(-1)).float()).cpu().item()\n            num_batches += 1\n\n    print(\'Val loss:\', val_loss / num_batches)\n    print(\'Val accuracy:\', val_accuracy / num_batches)\nend_time = time.time()\nprint(\'Training time: {}\'.format(end_time - start_time))\ntest_loader = DataLoader(ModelNet(args.modelnet_root, categories=args.categories,\n                                  split=\'test\', transform=transform),\n                         shuffle=True, batch_size=15, num_workers=num_workers, pin_memory=pin_memory)\n\ntest_batch = next(iter(test_loader))\nlabels = test_batch.attributes[\'category\'].cuda()\npreds = model(test_batch.data.cuda())\npred_labels = torch.max(preds, axis=1)[1]\n\nif args.viz_test:\n    from utils import visualize_batch\n    visualize_batch(test_batch.data, pred_labels, labels, args.categories)\n'"
examples/Classification/pointcloud_classification_engine.py,1,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\n\nfrom torch.utils.data import DataLoader\n\nimport kaolin.transforms as tfs\nfrom kaolin import ClassificationEngine\nfrom kaolin.datasets import ModelNet\nfrom kaolin.models.PointNet import PointNetClassifier\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--modelnet-root\', type=str, help=\'Root directory of the ModelNet dataset.\')\nparser.add_argument(\'--categories\', type=str, nargs=\'+\', default=[\'chair\', \'sofa\'], help=\'list of object classes to use.\')\nparser.add_argument(\'--num-points\', type=int, default=1024, help=\'Number of points to sample from meshes.\')\nparser.add_argument(\'--epochs\', type=int, default=10, help=\'Number of train epochs.\')\nparser.add_argument(\'-lr\', \'--learning-rate\', type=float, default=1e-3, help=\'Learning rate.\')\nparser.add_argument(\'--batch-size\', type=int, default=12, help=\'Batch size.\')\nparser.add_argument(\'--device\', type=str, default=\'cuda\', help=\'Device to use.\')\n\nargs = parser.parse_args()\n\nassert len(args.categories) >= 2, \'At least two categories must be specified.\'\n\ntransform = tfs.Compose([\n    tfs.TriangleMeshToPointCloud(num_samples=args.num_points),\n    tfs.NormalizePointCloud()\n])\n\ntrain_loader = DataLoader(ModelNet(args.modelnet_root, categories=args.categories,\n                                   split=\'train\', transform=transform),\n                          batch_size=args.batch_size, shuffle=True)\n\nval_loader = DataLoader(ModelNet(args.modelnet_root, categories=args.categories,\n                                 split=\'test\', transform=transform),\n                        batch_size=args.batch_size)\n\nmodel = PointNetClassifier(num_classes=len(args.categories))\nengine = ClassificationEngine(model, train_loader, val_loader)\nengine.fit()\n'"
examples/Classification/utils.py,0,"b""from mpl_toolkits.mplot3d import Axes3D     # unused import necessary to have access to 3d projection # noqa: F401\nimport matplotlib.pyplot as plt\n\n\ndef visualize_batch(pointclouds, pred_labels, labels, categories):\n    batch_size = len(pointclouds)\n    fig = plt.figure(figsize=(8, batch_size / 2))\n\n    ncols = 5\n    nrows = max(1, batch_size // 5)\n    for idx, pc in enumerate(pointclouds):\n        label = categories[labels[idx].item()]\n        pred = categories[pred_labels[idx]]\n        colour = 'g' if label == pred else 'r'\n        pc = pc.cpu().numpy()\n        ax = fig.add_subplot(nrows, ncols, idx + 1, projection='3d')\n        ax.scatter(pc[:, 0], pc[:, 1], pc[:, 2], c=colour, s=2)\n        ax.axis('off')\n        ax.set_title('GT: {0}\\nPred: {1}'.format(label, pred))\n\n    plt.show()\n"""
kaolin/conversions/__init__.py,0,b'from kaolin.conversions.meshconversions import *\nfrom kaolin.conversions.pointcloudconversions import *\nfrom kaolin.conversions.sdfconversions import *\nfrom kaolin.conversions.voxelgridconversions import *\n'
kaolin/conversions/meshconversions.py,19,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport numpy as np\nimport kaolin\n\nimport kaolin as kal\nfrom kaolin.metrics.point import directed_distance as directed_distance\n\n\ndef trianglemesh_to_pointcloud(mesh: kaolin.rep.Mesh, num_points: int):\n    r"""""" Converts  passed mesh to a pointcloud\n\n    Args:\n        mesh (kaolin.rep.Mesh): mesh to convert\n        num_points (int):number of points in converted point cloud\n\n    Returns:\n       (torch.Tensor): converted point cloud\n\n    Example:\n        >>> mesh = kal.TriangleMesh.from_obj(\'object.obj\')\n        >>> points = kal.conversions.trianglemesh_to_pointcloud(mesh, 10)\n        >>> points\n        tensor([[ 0.0524,  0.0039, -0.0111],\n                [-0.1995,  0.2999,  0.0408],\n                [-0.1921, -0.0268,  0.1811],\n                [ 0.1292,  0.0039,  0.2030],\n                [-0.1859,  0.1764,  0.0168],\n                [-0.1749,  0.1515, -0.0925],\n                [ 0.1990,  0.0039, -0.0083],\n                [ 0.2173, -0.1285, -0.2248],\n                [-0.1916, -0.2143,  0.2064],\n                [-0.1935,  0.2401,  0.1003]])\n        >>> points.shape\n        torch.Size([10, 3])\n    """"""\n\n    points, face_choices = mesh.sample(num_points)\n    return points, face_choices\n\n\ndef trianglemesh_to_voxelgrid(mesh: kaolin.rep.Mesh, resolution: int,\n                              normalize: bool = True, vertex_offset: float = 0.):\n    r"""""" Converts mesh to a voxel model of a given resolution\n\n    Args:\n        mesh (kaolin.rep.Mesh): mesh to convert\n        resolution (int): desired dresolution of generated voxel array\n        normalize (bool): Determines whether to normalize vertices to a\n            unit cube centered at the origin.\n        vertex_offset (float): Offset applied to all vertices after\n                               normalizing.\n\n    Returns:\n        voxels (torch.Tensor): voxel array of desired resolution\n\n    Example:\n        >>> mesh = kal.TriangleMesh.from_obj(\'model.obj\')\n        >>> voxel = kal.conversions.trianglemesh_to_voxelgrid(mesh, 32)\n        >>> voxel.shape\n\n    """"""\n    mesh = kal.rep.Mesh.from_tensors(mesh.vertices.clone(), mesh.faces.clone())\n    if normalize:\n        verts_max = mesh.vertices.max()\n        verts_min = mesh.vertices.min()\n        mesh.vertices = (mesh.vertices - verts_min) / (verts_max - verts_min) - 0.5\n\n    mesh.vertices = mesh.vertices + vertex_offset\n\n    points = mesh.vertices\n    smallest_side = (1. / resolution)**2\n\n    if mesh.faces.shape[-1] == 4:\n        tri_faces_1 = torch.cat((mesh.faces[:, :2], mesh.faces[:, 3:]), dim=1)\n        tri_faces_2 = torch.cat((mesh.faces[:, :1], mesh.faces[:, 2:]), dim=1)\n        faces = torch.cat((tri_faces_1, tri_faces_2))\n    else:\n        faces = mesh.faces.clone()\n\n    v1 = torch.index_select(mesh.vertices, 0, faces[:, 0])\n    v2 = torch.index_select(mesh.vertices, 0, faces[:, 1])\n    v3 = torch.index_select(mesh.vertices, 0, faces[:, 2])\n\n    while True:\n        side_1 = (torch.abs(v1 - v2)**2).sum(dim=1).unsqueeze(1)\n        side_2 = (torch.abs(v2 - v3)**2).sum(dim=1).unsqueeze(1)\n        side_3 = (torch.abs(v3 - v1)**2).sum(dim=1).unsqueeze(1)\n        sides = torch.cat((side_1, side_2, side_3), dim=1)\n        sides = sides.max(dim=1)[0]\n\n        keep = sides > smallest_side\n        if keep.sum() == 0:\n            break\n        v1 = v1[keep]\n        v2 = v2[keep]\n        v3 = v3[keep]\n        del(side_1, side_2, side_3, keep, sides)\n\n        v4 = (v1 + v3) / 2.\n        v5 = (v1 + v2) / 2.\n        v6 = (v2 + v3) / 2.\n\n        points = torch.cat((points, v4, v5, v6))\n\n        vertex_set = [v1, v2, v3, v4, v5, v6]\n        new_traingles = [[0, 3, 4], [4, 1, 5], [4, 3, 5], [3, 2, 5]]\n        new_verts = []\n\n        for i in range(4):\n            for j in range(3):\n                if i == 0:\n                    new_verts.append(vertex_set[new_traingles[i][j]])\n                else:\n                    new_verts[j] = torch.cat(\n                        (new_verts[j], vertex_set[new_traingles[i][j]]))\n        v1, v2, v3 = new_verts\n        del(v4, v5, v6, vertex_set, new_verts)\n\n    del(v1, v2, v3)\n\n    voxel = torch.zeros((resolution, resolution, resolution))\n    points = (points * (resolution - 1)).long()\n    points = torch.split(points.permute(1, 0), 1, dim=0)\n    points = [m.unsqueeze(0) for m in points]\n    voxel[points] = 1\n    return voxel\n\n\ndef trianglemesh_to_sdf(mesh: kaolin.rep.Mesh, num_points: int = 10000):\n    r"""""" Converts mesh to a SDF function\n\n    Args:\n        mesh (kaolin.rep.Mesh): mesh to convert.\n        num_points (int): number of points to sample on surface of the mesh.\n\n    Returns:\n        sdf: a signed distance function\n\n    Example:\n        >>> mesh = kal.TriangleMesh.from_obj(\'object.obj\')\n        >>> sdf = kal.conversions.trianglemesh_to_sdf(mesh)\n        >>> points = torch.rand(100,3)\n        >>> distances = sdf(points)\n    """"""\n    surface_points, _ = mesh.sample(num_points)\n\n    def eval_query(query):\n        distances = directed_distance(query, surface_points, mean=False)\n        occ_points = kal.rep.SDF.check_sign(mesh, query)\n        if torch.is_tensor(occ_points):\n            occ_points = occ_points.cpu().numpy()[0]\n        distances[np.where(occ_points)] *= -1\n        return distances\n\n    return eval_query\n'"
kaolin/conversions/pointcloudconversions.py,8,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Union\n\nimport torch\nimport numpy as np\n\nfrom kaolin.rep.PointCloud import PointCloud\nfrom kaolin.metrics.point import directed_distance\nfrom kaolin import helpers\nfrom kaolin.conversions.voxelgridconversions import voxelgrid_to_trianglemesh\nfrom kaolin.conversions.voxelgridconversions import voxelgrid_to_sdf\n\n\ndef pointcloud_to_voxelgrid(pts: Union[torch.Tensor, PointCloud, np.ndarray],\n                            voxres: int, voxsize: float):\n    r""""""Converts a pointcloud into a voxel grid.\n\n    Args:\n        - pts (torch.Tensor or PointCloud): Pointcloud\n            (shape: :math:`N \\times 3`, where :math:`N` is the number of points\n            in the pointcloud).\n        - voxres (int): Resolution of the voxel grid.\n        - voxsize (float): size of each voxel grid cell.\n\n    Returns:\n        (torch.Tensor): Voxel grid.\n    """"""\n\n    if isinstance(pts, PointCloud):\n        pts = pts.points\n    helpers._assert_tensor(pts)\n\n    # Create a voxel grid.\n    voxels = np.zeros((voxres, voxres, voxres), dtype=np.float32)\n    # Enumerate the coordinates of each grid cell\n    gridpts = np.where(voxels == 0)\n    gridpts = np.asarray([gridpts[0], gridpts[1], gridpts[2]]).T.astype(np.float32)\n    gridpts = torch.from_numpy(gridpts)\n    # Scale grid coordinates appropriately. We currently have coordinated\n    # denoting the corners of a voxel; modify so that we represent the center.\n    gridpts = voxsize * (gridpts - (voxres - 1) / 2)\n    # Get the distance of the closest point in the pointcloud to each grid\n    # point.\n    dists = directed_distance(gridpts.cuda().view(-1, 3).contiguous(),\n                              pts.cuda().view(-1, 3).contiguous(), mean=False)\n    dists = dists.view((voxres, voxres, voxres))\n    # If this distance is less than the size of a voxel, treat as occupied,\n    # else free.\n    on_voxels = np.where(dists.cpu().numpy() <= voxsize)\n    voxels[on_voxels] = 1\n\n    return voxels\n\n\ndef pointcloud_to_trianglemesh(points: torch.Tensor):\n    device = points.device\n    voxels = pointcloud_to_voxelgrid(points, 32, 0.1)\n    return voxelgrid_to_trianglemesh(torch.from_numpy(voxels).to(device))\n\n\ndef pointcloud_to_sdf(points: torch.Tensor, num_points=5000):\n    device = points.device\n    voxels = pointcloud_to_voxelgrid(points, 32, 0.1)\n    return voxelgrid_to_sdf(torch.from_numpy(voxels).to(device))\n'"
kaolin/conversions/sdfconversions.py,5,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n# Occupancy Networks\n#\n# Copyright 2019 Lars Mescheder, Michael Oechsle, Michael Niemeyer, Andreas Geiger, Sebastian Nowozin\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport torch\nimport numpy as np\n\nfrom kaolin.mise import MISE\nimport kaolin\nimport kaolin as kal\n\n\ndef sdf_to_voxelgrid(sdf: kaolin.rep.SDF, bbox_center: float = 0.,\n                     bbox_dim: float = 1., resolution: int = 32,\n                     upsampling_steps: int = 2):\n    r""""""Converts an SDF to a voxel grid.\n\n    Args:\n        sdf (kaolin.rep.SDF) : an object with a .eval_occ function that\n            indicates which of a set of passed points is inside the surface.\n        bbox_center (float): center of the surface\'s bounding box.\n        bbox_dim (float): largest dimension of the surface\'s bounding box.\n        resolution (int) : the initial resolution of the voxel, should be\n            large enough to properly define the surface.\n        upsampling_steps (int) : Number of times the initial resolution will\n            be doubled.\n            The returned resolution will be resolution * (2 ^ upsampling_steps)\n\n    Returns:\n        (torch.Tensor): a voxel grid\n\n    Example:\n        >>> sdf = kal.rep.SDF.sphere()\n        >>> voxel = kal.conversions.sdf_to_voxelgrid(sdf, bbox_dim = 2)\n    """"""\n\n    mesh_extractor = MISE(\n        resolution, upsampling_steps, .5)\n\n    points = mesh_extractor.query()\n    while points.shape[0] != 0:\n        # Query points\n        pointsf = torch.FloatTensor(points)\n        # Normalize to bounding box\n        pointsf = pointsf / (mesh_extractor.resolution - 1)\n        pointsf = bbox_dim * (pointsf + (bbox_center - 0.5))\n        values = sdf(pointsf) <= 0\n        values = values.data.cpu().numpy().astype(np.float64)\n        mesh_extractor.update(points, values)\n        points = mesh_extractor.query()\n\n    voxels = torch.FloatTensor(mesh_extractor.to_dense())\n\n    return voxels\n\n\ndef sdf_to_trianglemesh(sdf: kaolin.rep.SDF, bbox_center: float = 0.,\n                        bbox_dim: float = 1., resolution: int = 32,\n                        upsampling_steps: int = 2):\n    r"""""" Converts an SDF function to a mesh\n\n    Args:\n        sdf (kaolin.rep.SDF): an object with a .eval_occ function that\n            indicates which of a set of passed points is inside the surface.\n        bbox_center (float): center of the surface\'s bounding box.\n        bbox_dim (float): largest dimension of the surface\'s bounding box.\n        resolution (int) : the initial resolution of the voxel, should be large\n            enough to properly define the surface.\n        upsampling_steps (int) : Number of times the initial resolution will be\n            doubled.\n            The returned resolution will be resolution * (2 ^ upsampling_steps)\n\n    Returns:\n        (torch.Tensor): computed mesh preperties\n\n    Example:\n        >>> sdf = kal.rep.SDF.sphere()\n        >>> verts, faces = kal.conversion.sdf_to_trianglemesh(sdf, bbox_dim=2)\n        >>> mesh = kal.rep.TriangleMesh.from_tensors(verts, faces)\n\n    """"""\n    voxel = sdf_to_voxelgrid(sdf, bbox_center, bbox_dim,\n                             resolution, upsampling_steps)\n    verts, faces = kal.conversions.voxelgrid_to_trianglemesh(voxel)\n    return verts, faces\n\n\ndef sdf_to_pointcloud(sdf: kaolin.rep.SDF, bbox_center: float = 0.,\n                      bbox_dim: float = 1., resolution: int = 32,\n                      upsampling_steps: int = 2, num_points: int = 5000):\n    r""""""Converts an SDF fucntion to a point cloud.\n\n    Args:\n        sdf (kaolin.rep.SDF) : an object with a .eval_occ function that\n            indicates which of a set of passed points is inside the surface.\n        bbox_center (float): center of the surface\'s bounding box.\n        bbox_dim (float): largest dimension of the surface\'s bounding box.\n        resolution (int) : the initial resolution of the voxel, should be large\n            enough to properly define the surface.\n        upsampling_steps (int) : Number of times the initial resolution will be\n            doubled.\n            The returned resolution will be resolution * (2 ^ upsampling_steps)\n        num_points (int): number of points in computed point cloud.\n\n    Returns:\n        (torch.FloatTensor): computed point cloud\n\n    Example:\n        >>> sdf = kal.rep.SDF.sphere()\n        >>> points = kal.conversion.sdf_to_pointcloud(sdf, bbox_dim=2)\n\n    """"""\n    verts, faces = sdf_to_trianglemesh(sdf, bbox_center, bbox_dim,\n                                       resolution, upsampling_steps)\n    mesh = kal.rep.TriangleMesh.from_tensors(verts, faces)\n    return mesh.sample(num_points)[0]\n'"
kaolin/conversions/voxelgridconversions.py,73,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n# Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation\n#\n# Copyright (c) 2019 Edward Smith\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom typing import Optional, Union, List\n\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport kaolin as kal\nimport trimesh\nfrom scipy import ndimage\n\n# from kaolin.transforms import voxelfunc\nfrom kaolin.rep import VoxelGrid\nfrom kaolin import helpers\n\n\ndef downsample(voxel: Union[torch.Tensor, VoxelGrid], scale: List[int],\n               inplace: Optional[bool] = True):\n    r""""""Downsamples a voxelgrid, given a (down)scaling factor for each\n    dimension.\n\n    .. Note::\n        The voxel output is not thresholded.\n\n    Args:\n        voxel (torch.Tensor): Voxel grid to be downsampled (shape: must\n            be a tensor containing exactly 3 dimensions).\n        scale (list): List of tensors to scale each dimension down by\n            (length: 3).\n        inplace (bool, optional): Bool to make the operation in-place.\n\n    Returns:\n        (torch.Tensor): Downsampled voxelgrid.\n\n    Example:\n        >>> x = torch.ones([32, 32, 32])\n        >>> print (x.shape)\n        torch.Size([32, 32, 32])\n        >>> x = downsample(x, [2,2,2])\n        >>> print (x.shape)\n        torch.Size([16, 16, 16])\n    """"""\n    if isinstance(voxel, VoxelGrid):\n        voxel = voxel.voxels\n    voxel = confirm_def(voxel)\n\n    if not inplace:\n        voxel = voxel.clone()\n\n    # Verify that all elements of `scale` are greater than or equal to 1 and\n    # less than the voxel shape for the corresponding dimension.\n    scale_filter = [1, 1]\n    scale_factor = 1.\n    for i in range(3):\n        if scale[i] < 1:\n            raise ValueError(\'Downsample ratio must be at least 1 along every\'\n                             \' dimension.\')\n        if scale[i] >= voxel.shape[i]:\n            raise ValueError(\'Downsample ratio must be less than voxel shape\'\n                             \' along every dimension.\')\n        scale_filter.append(scale[i])\n        scale_factor *= scale[i]\n    conv_filter = torch.ones(scale_filter).to(voxel.device) / scale_factor\n\n    voxel = F.conv3d(voxel.unsqueeze(0).unsqueeze(\n        0), conv_filter, stride=scale, padding=0)\n    voxel = voxel.squeeze(0).squeeze(0)\n\n    return voxel\n\n\ndef upsample(voxel: torch.Tensor, dim: int):\n    r""""""Upsamples a voxel grid by a given scaling factor.\n\n    .. Note::\n        The voxel output is not thresholded.\n\n    Args:\n        voxel (torch.Tensor): Voxel grid to be upsampled (shape: must\n            be a 3D tensor)\n        dim (int): New dimensionality (number of voxels along each dimension\n            in the resulting voxel grid).\n\n    Returns:\n        torch.Tensor: Upsampled voxel grid.\n\n    Example:\n        >>> x = torch.ones([32, 32, 32])\n        >>> print (x.shape)\n        torch.Size([32, 32, 32])\n        >>> x = upsample(x, 64)\n        >>> print (x.shape)\n        torch.Size([64, 64, 64])\n    """"""\n    if isinstance(voxel, VoxelGrid):\n        voxel = voxel.voxels\n    voxel = confirm_def(voxel)\n\n    cur_shape = voxel.shape\n    assert (dim >= cur_shape[0]) and ((dim >= cur_shape[1]) and (\n        dim >= cur_shape[2])), \'All dim values must be larger then current dim\'\n\n    new_positions = []\n    old_positions = []\n\n    # Defining position correspondences\n    for i in range(3):\n        shape_params = [1, 1, 1]\n        shape_params[i] = dim\n        new_pos = np.arange(dim).reshape(shape_params)\n\n        for j in range(3):\n            if i == j:\n                continue\n            new_pos = np.repeat(new_pos, dim, axis=j)\n        new_pos = new_pos.reshape(-1)\n\n        ratio = float(cur_shape[i]) / float(dim)\n        old_pos = (new_pos * ratio).astype(int)\n\n        new_positions.append(new_pos)\n        old_positions.append(old_pos)\n\n    scaled_voxel = torch.FloatTensor(np.zeros([dim, dim, dim])).to(\n        voxel.device)\n    if voxel.is_cuda:\n        scaled_voxel = scaled_voxel.cuda()\n\n    scaled_voxel[tuple(new_positions)] = voxel[tuple(old_positions)]\n\n    return scaled_voxel\n\n\ndef fill(voxel: Union[torch.Tensor, VoxelGrid], thresh: float = .5):\n    r"""""" Fills the internal structures in a voxel grid. Used to fill holds\n    and \'solidify\' objects.\n\n    Args:\n        voxel (torch.Tensor): Voxel grid to be filled.\n        thresh (float): Threshold to use for binarization of the grid.\n\n    Returns:\n        torch.Tensor: filled voxel array\n    """"""\n\n    if isinstance(voxel, VoxelGrid):\n        voxel = voxel.voxels\n    voxel = confirm_def(voxel)\n    voxel = threshold(voxel, thresh)\n    voxel = voxel.clone()\n    on = ndimage.binary_fill_holes(voxel.data.cpu())\n    voxel[np.where(on)] = 1\n    return voxel\n\n\ndef extract_odms(voxel: Union[torch.Tensor, VoxelGrid]):\n    r""""""Extracts an orthographic depth map from a voxel grid.\n\n    Args:\n        voxel (torch.Tensor): Voxel grid from which odms are extracted.\n\n    Returns:\n        (torch.Tensor): 6 ODMs from the 6 primary viewing angles.\n\n    Example:\n        >>> voxel = torch.ones([128,128,128])\n        >>> voxel = extract_odms(voxel)\n        >>> voxel.shape\n        torch.Size([6, 128, 128])\n    """"""\n    if isinstance(voxel, VoxelGrid):\n        voxel = voxel.voxels\n    voxel = confirm_def(voxel)\n    cuda = voxel.is_cuda\n    voxel = extract_surface(voxel)\n    voxel = voxel.data.cpu().numpy()\n\n    dim = voxel.shape[-1]\n    a, b, c = np.where(voxel == 1)\n    big_list = [[[[dim, dim]\n                  for j in range(dim)] for i in range(dim)] for k in range(3)]\n    for i, j, k in zip(a, b, c):\n        big_list[0][i][j][0] = (min(dim - k - 1, big_list[0][i][j][0]))\n        big_list[0][i][j][1] = (min(k, big_list[0][i][j][1]))\n        big_list[1][i][k][0] = (min(dim - j - 1, big_list[1][i][k][0]))\n        big_list[1][i][k][1] = (min(j, big_list[1][i][k][1]))\n        big_list[2][j][k][0] = (min(dim - i - 1, big_list[2][j][k][0]))\n        big_list[2][j][k][1] = (min(i, big_list[2][j][k][1]))\n\n    odms = np.zeros((6, dim, dim))\n    big_list = np.array(big_list)\n    for k in range(6):\n        odms[k] = big_list[k // 2, :, :, k % 2]\n    odms = torch.FloatTensor(np.array(odms))\n\n    if cuda:\n        odms = odms.cuda()\n\n    return odms\n\n\ndef project_odms(odms: torch.Tensor,\n                 voxel: torch.Tensor = None, votes: int = 1):\n    r""""""Projects orthographic depth map onto a voxel array.\n\n    .. Note::\n        If no voxel grid is provided, we poject onto a completely filled grid.\n\n    Args:\n        odms (torch.Tensor): ODMs which are to be projected.\n        voxel (torch.Tensor): Voxel grid onto which ODMs are projected.\n\n    Returns:\n        (torch.Tensor): Updated voxel grid.\n\n    Example:\n        >>> odms = torch.rand([6,128,128])*128\n        >>> odms = voxel.int()\n        >>> voxel = project_odms(odms)\n        >>> voxel.shape\n        torch.Size([128, 128, 128])\n    """"""\n    cuda = odms.is_cuda\n    dim = odms.shape[-1]\n    subtractor = 1. / float(votes)\n\n    if voxel is None:\n        voxel = torch.ones((dim, dim, dim))\n    else:\n        for i in range(3):\n            assert (voxel.shape[i] == odms.shape[-1]\n                    ), \'Voxel and odm dimension size must be the same\'\n        if isinstance(voxel, VoxelGrid):\n            voxel = voxel.voxels\n        voxel = confirm_def(voxel)\n        voxel = threshold(voxel, .5)\n    voxel = voxel.data.cpu().numpy()\n    odms = odms.data.cpu().numpy()\n\n    for i in range(3):\n        odms[2 * i] = dim - odms[2 * i]\n\n    depths = np.where(odms <= dim)\n    for x, y, z in zip(*depths):\n        pos = int(odms[x, y, z])\n        if x == 0:\n            voxel[y, z, pos:dim] -= subtractor\n        elif x == 1:\n            voxel[y, z, 0:pos] -= subtractor\n        elif x == 2:\n            voxel[y, pos:dim, z] -= subtractor\n        elif x == 3:\n            voxel[y, 0:pos, z] -= subtractor\n        elif x == 4:\n            voxel[pos:dim, y, z] -= subtractor\n        else:\n            voxel[0:pos, y, z] -= subtractor\n\n    on = np.where(voxel > 0)\n    off = np.where(voxel <= 0)\n    voxel[on] = 1\n    voxel[off] = 0\n\n    voxel = confirm_def(voxel)\n    if cuda:\n        voxel = voxel.cuda()\n    return voxel\n\n\ndef confirm_def(voxgrid: torch.Tensor):\n    r"""""" Checks that the definition of the voxelgrid is correct.\n\n    Args:\n        voxgrid (torch.Tensor): Passed voxelgrid.\n\n    Return:\n        (torch.Tensor): Voxel grid as torch.Tensor.\n\n    """"""\n    if isinstance(voxgrid, np.ndarray):\n        voxgrid = torch.Tensor(voxgrid)\n    helpers._assert_tensor(voxgrid)\n    helpers._assert_dim_eq(voxgrid, 3)\n    assert ((voxgrid.max() <= 1.) and (voxgrid.min() >= 0.)\n            ), \'All values in passed voxel grid must be in range [0,1]\'\n    return voxgrid\n\n\ndef threshold(voxel: Union[torch.Tensor, VoxelGrid], thresh: float,\n              inplace: Optional[bool] = True):\n    r""""""Binarizes the voxel array using a specified threshold.\n\n    Args:\n        voxel (torch.Tensor): Voxel array to be binarized.\n        thresh (float): Threshold with which to binarize.\n        inplace (bool, optional): Bool to make the operation in-place.\n\n    Returns:\n        (torch.Tensor): Thresholded voxel array.\n\n    """"""\n    if isinstance(voxel, VoxelGrid):\n        voxel = voxel.voxels\n    helpers._assert_tensor(voxel)\n    if inplace:\n        voxel[:] = voxel > thresh\n    else:\n        voxel = (voxel > thresh).type(voxel.dtype)\n    return voxel\n\n\ndef extract_surface(voxel: Union[torch.Tensor, VoxelGrid], thresh: float = .5):\n    r""""""Removes any inernal structure(s) from a voxel array.\n\n    Args:\n        voxel (torch.Tensor): voxel array from which to extract surface\n        thresh (float): threshold with which to binarize\n\n    Returns:\n        torch.Tensor: surface voxel array\n    """"""\n\n    if isinstance(voxel, VoxelGrid):\n        voxel = voxel.voxels\n    voxel = confirm_def(voxel)\n    voxel = threshold(voxel, thresh)\n    off_positions = voxel == 0\n\n    conv_filter = torch.ones((1, 1, 3, 3, 3))\n    surface_voxel = torch.zeros(voxel.shape)\n    if voxel.is_cuda:\n        conv_filter = conv_filter.cuda()\n        surface_voxel = surface_voxel.cuda()\n\n    local_occupancy = F.conv3d(voxel.unsqueeze(\n        0).unsqueeze(0), conv_filter, padding=1)\n    local_occupancy = local_occupancy.squeeze(0).squeeze(0)\n    # only elements with exposed faces\n    surface_positions = (local_occupancy < 27) * (local_occupancy > 0)\n    surface_voxel[surface_positions] = 1\n    surface_voxel[off_positions] = 0\n\n    return surface_voxel\n\n\ndef voxelgrid_to_pointcloud(voxel: torch.Tensor, num_points: int,\n                            thresh: float = .5, mode: str = \'full\',\n                            normalize: bool = True):\n    r"""""" Converts  passed voxel to a pointcloud\n\n    Args:\n        voxel (torch.Tensor): voxel array\n        num_points (int): number of points in converted point cloud\n        thresh (float): threshold from which to make voxel binary\n        mode (str):\n            -\'full\': sample the whole voxel model\n            -\'surface\': sample only the surface voxels\n        normalize (bool): whether to scale the array to (-.5,.5)\n\n    Returns:\n       (torch.Tensor): converted pointcloud\n\n    Example:\n        >>> voxel = torch.ones([32,32,32])\n        >>> points = voxelgrid_to_pointcloud(voxel, 10)\n        >>> points\n        tensor([[0.5674, 0.8994, 0.8606],\n                [0.2669, 0.9445, 0.5501],\n                [0.2252, 0.9674, 0.8198],\n                [0.5127, 0.9347, 0.4470],\n                [0.7981, 0.1645, 0.5405],\n                [0.7384, 0.4255, 0.6084],\n                [0.9881, 0.3629, 0.2747],\n                [0.1690, 0.2880, 0.4849],\n                [0.8844, 0.3866, 0.0557],\n                [0.4829, 0.0413, 0.6700]])\n        >>> points.shape\n        torch.Size([10, 3])\n    """"""\n\n    assert (mode in [\'full\', \'surface\'])\n    voxel = confirm_def(voxel)\n    voxel = threshold(voxel, thresh=thresh)\n\n    if mode == \'surface\':\n        voxel = extract_surface(voxel)\n\n    voxel_positions = (voxel == 1).nonzero().float()\n\n    index_list = list(range(voxel_positions.shape[0]))\n    select_index = np.random.choice(index_list, size=num_points)\n    point_positions = voxel_positions[select_index]\n\n    point_displacement = torch.rand(\n        point_positions.shape).to(\n        point_positions.device)\n    point_positions += point_displacement\n\n    if normalize:\n        shape = torch.FloatTensor(\n            np.array(\n                voxel.shape)).to(\n            point_positions.device)\n        point_positions /= shape\n        point_positions = point_positions - .5\n\n    return point_positions\n\n\ndef voxelgrid_to_trianglemesh(voxel: torch.Tensor, thresh: int = .5,\n                              mode: str = \'marching_cubes\',\n                              normalize: bool = True):\n    r"""""" Converts  passed voxel to a mesh\n\n    Args:\n        voxel (torch.Tensor): voxel array\n        thresh (float): threshold from which to make voxel binary\n        mode (str):\n            -\'exact\': exect mesh conversion\n            -\'marching_cubes\': marching cubes is applied to passed voxel\n        normalize (bool): whether to scale the array to (-.5,.5)\n\n    Returns:\n        (torch.Tensor): computed mesh properties\n\n    Example:\n        >>> voxel = torch.ones([32,32,32])\n        >>> verts, faces = voxelgrid_to_trianglemesh(voxel)\n        >>> [verts.shape, faces.shape]\n        [torch.Size([6144, 3]), torch.Size([12284, 3])]\n\n    """"""\n    assert (mode in [\'exact\', \'marching_cubes\'])\n    voxel = confirm_def(voxel)\n    voxel = threshold(voxel, thresh=thresh)\n    voxel_np = np.array((voxel.cpu() > thresh)).astype(bool)\n    trimesh_voxel = trimesh.voxel.VoxelGrid(voxel_np)\n\n    if mode == \'exact\':\n        trimesh_voxel = trimesh_voxel.as_boxes()\n    elif mode == \'marching_cubes\':\n        trimesh_voxel = trimesh_voxel.marching_cubes\n\n    verts = torch.FloatTensor(trimesh_voxel.vertices)\n    faces = torch.LongTensor(trimesh_voxel.faces)\n    shape = torch.FloatTensor(np.array(voxel.shape))\n    if voxel.is_cuda:\n        verts = verts.cuda()\n        faces = faces.cuda()\n        shape = shape.cuda()\n    if normalize:\n\n        verts /= shape\n        verts = verts - .5\n\n    return verts, faces\n\n\ndef voxelgrid_to_quadmesh(voxel: torch.Tensor, thresh: str = .5,\n                          normalize: bool = True):\n    r"""""" Converts passed voxel to quad mesh\n\n    Args:\n        voxel (torch.Tensor): voxel array\n        thresh (float): threshold from which to make voxel binary\n        normalize (bool): whether to scale the array to (-.5,.5)\n\n    Returns:\n        (torch.Tensor): converted mesh properties\n\n    Example:\n        >>> voxel = torch.ones([32,32,32])\n        >>> verts, faces = voxelgrid_to_quadmesh(voxel)\n        >>> [verts.shape, faces.shape]\n        [torch.Size([6144, 3]), torch.Size([6142, 4])]\n\n    """"""\n    voxel = confirm_def(voxel)\n    voxel = threshold(voxel, thresh=thresh)\n    dim = voxel.shape[0]\n    new_voxel = np.zeros((dim + 2, dim + 2, dim + 2))\n    new_voxel[1:dim + 1, 1:dim + 1, 1:dim + 1] = voxel.cpu()\n\n    voxel = new_voxel\n\n    vert_dict = {}\n    verts = []\n    faces = []\n    curr_vert_num = 1\n    a, b, c = np.where(voxel == 1)\n    for i, j, k in zip(a, b, c):\n\n        # top\n        if voxel[i, j, k + 1] != 1:\n            vert_dict, verts, faces, curr_vert_num = _add_face(\n                0, vert_dict, verts, faces, [i, j, k], curr_vert_num)\n        # bottom\n        if voxel[i, j, k - 1] != 1:\n            vert_dict, verts, faces, curr_vert_num = _add_face(\n                1, vert_dict, verts, faces, [i, j, k], curr_vert_num)\n        # left\n        if voxel[i - 1, j, k] != 1:\n            vert_dict, verts, faces, curr_vert_num = _add_face(\n                2, vert_dict, verts, faces, [i, j, k], curr_vert_num)\n        # right\n        if voxel[i + 1, j, k] != 1:\n            vert_dict, verts, faces, curr_vert_num = _add_face(\n                3, vert_dict, verts, faces, [i, j, k], curr_vert_num)\n        # front\n        if voxel[i, j - 1, k] != 1:\n            vert_dict, verts, faces, curr_vert_num = _add_face(\n                4, vert_dict, verts, faces, [i, j, k], curr_vert_num)\n        # back\n        if voxel[i, j + 1, k] != 1:\n            vert_dict, verts, faces, curr_vert_num = _add_face(\n                5, vert_dict, verts, faces, [i, j, k], curr_vert_num)\n    verts = torch.FloatTensor(np.array(verts))\n    faces = torch.LongTensor(np.array(faces) - 1)\n    if normalize:\n        shape = torch.FloatTensor(np.array(voxel.shape))\n        verts /= shape\n        verts = verts - .5\n\n    return verts, faces\n\n\ndef _add_face(vert_set_index: int, vert_dict: dict, verts: torch.Tensor,\n              faces: torch.Tensor, location: list, curr_vert_num: int):\n    r"""""" Adds a face to the set of observed faces and verticies\n    """"""\n    top_verts = np.array([[0, 0, 1], [1, 0, 1], [1, 1, 1], [0, 1, 1]])\n    bottom_verts = np.array([[0, 0, 0], [0, 1, 0], [1, 1, 0], [1, 0, 0]])\n    left_verts = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 1, 0]])\n    right_verts = np.array([[1, 0, 0], [1, 1, 0], [1, 1, 1], [1, 0, 1]])\n    front_verts = np.array([[0, 0, 0], [1, 0, 0], [1, 0, 1], [0, 0, 1]])\n    back_verts = np.array([[0, 1, 0], [0, 1, 1], [1, 1, 1], [1, 1, 0]])\n    vert_sets = [top_verts, bottom_verts, left_verts,\n                 right_verts, front_verts, back_verts]\n\n    vert_set = vert_sets[vert_set_index]\n    face = np.array([0, 1, 2, 3]) + curr_vert_num\n    update = 4\n    for e, vs in enumerate(vert_set):\n        new_position = vs + np.array(location)\n        if str(new_position) in vert_dict:\n            index = vert_dict[str(new_position)]\n            face[e] = index\n            for idx in range(e + 1, 4):\n                face[idx] -= 1\n            update -= 1\n        else:\n            vert_dict[str(new_position)] = len(verts) + 1\n            verts.append(list(new_position))\n    faces.append(face)\n    curr_vert_num += update\n    return vert_dict, verts, faces, curr_vert_num\n\n\ndef voxelgrid_to_sdf(voxel: torch.Tensor, thresh: float = .5,\n                     normalize: bool = True):\n    r"""""" Converts passed voxel to a signed distance function\n\n    Args:\n        voxel (torch.Tensor): voxel array\n        thresh (float): threshold from which to make voxel binary\n        normalize (bool): whether to scale the array to (0,1)\n\n    Returns:\n        a signed distance function\n\n    Example:\n        >>> voxel = torch.ones([32,32,32])\n        >>> sdf = voxelgrid_to_sdf(voxel)\n        >>> distances = sdf(torch.rand(100,3))\n    """"""\n\n    voxel = confirm_def(voxel)\n    voxel = threshold(voxel, thresh=thresh)\n    on_points = (voxel > .5).nonzero().float()\n\n    if normalize:\n        on_points = on_points / float(voxel.shape[0])\n        on_points -= .5\n\n    distance_fn = kal.metrics.point.directed_distance\n\n    def eval_query(query):\n        distances = distance_fn(query, on_points, mean=False)\n        if normalize:\n            query = ((query + .5) * (voxel.shape[0] - 1))\n\n        query = np.floor(query.data.cpu().numpy())\n        query_positions = [query[:, 0], query[:, 1], query[:, 2]]\n        values = voxel[query_positions]\n        distances[values == 1] = 0\n\n        return distances\n\n    return eval_query\n'"
kaolin/cuda/__init__.py,0,b''
kaolin/datasets/__init__.py,0,"b'from .shapenet import *\nfrom .modelnet import *\nfrom .shrec import *\nfrom .scannet import *\n\n# nuscenes-devkit will import matplotlib trying for an x11 backend, workaround here\nimport matplotlib\nmatplotlib.use(\'Agg\')\n\ntry:\n    from .nusc import NuscDetection\nexcept ImportError as err:\n    import_err = err\n    import traceback\n    print(""Warning: unable to import datasets/nusc:\\n   %s"" % import_err)\n    print(""Warning: unable to import datasets/nusc:\\n   %s"" % traceback.print_exc())\n'"
kaolin/datasets/base.py,5,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import abstractmethod\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.multiprocessing import Pool\nfrom torch.utils.data import Dataset\n\nfrom kaolin import helpers\n\nfrom collections import namedtuple\n\n\ndef _preprocess_task(args):\n    torch.set_num_threads(1)\n    with torch.no_grad():\n        idx, get_data, get_attributes, cache_transform = args\n        name = get_attributes(idx)[\'name\']\n        data = get_data(idx)\n        cache_transform(name, *data)\n\n\nKaolinDatasetItem = namedtuple(\'KaolinDatasetItem\', [\'data\', \'attributes\'])\n\n\nclass KaolinDatasetMeta(type):\n    def __new__(metacls, cls_name, base_cls, class_dict):\n        if cls_name != ""KaolinDataset"":\n            class_dict[\'__doc__\'] += \\\n                """"""Additional args:\n        preprocessing_params (dict): parameters for the preprocessing:\n            - \'cache_dir\': path to the cached preprocessed data.\n            - \'num_workers\': number of process used in parallel for preprocessing (default: number of cores)\n        preprocessing_transform (Callable): Called on the outputs of _get_data over the indices\n            from 0 to len(self) during the construction of the dataset,\n            the preprocessed outputs are then cached to \'cache_dir\'.\n        transform (Callable): Called on the preprocessed data at __getitem__.\n\n        no_progress (bool): disable tqdm progress bar for preprocessing.""""""\n        return type.__new__(metacls, cls_name, base_cls, class_dict)\n\n\nclass KaolinDataset(Dataset, metaclass=KaolinDatasetMeta):\n    """"""\n    Abstract class for dataset with handling of multiprocess or cuda preprocessing.\n\n    A KaolinDataset children class will need the above implementation:\n       1) initialize:\n           Initialization function called at the beginning of the constructor.\n       2) _get_data:\n           Data getter that will be preprocessed => cached => transformed, take an index as input.\n       3) _get_attributes:\n           Attributes getter that will be preprocess / transform independent.\n       4) __len__:\n           Return the size of the dataset\n\n    When iterated, a KaolinDataset produces item in the format of KaolinDatasetItem.\n    KaolinDatasetItem is a named tuple with two properties: ""data"" and ""attributes"".\n    They can be accessed through ""item.data"" and ""item.attributes"", as well as by unpacking:\n    ""data, attributes = item"".\n\n    The preferred way to iterate is to use the ""for data, attributes in dataloader"" syntax.\n    """"""\n\n    def __init__(self, *args, preprocessing_transform=None, preprocessing_params: dict = None,\n                 transform=None, no_progress: bool = False, **kwargs):\n        """"""\n        Args:\n            positional and keyword arguments for initialize(*args, **kwargs) (see class and initialize documentation)\n            preprocessing_params (dict): parameters for the preprocessing:\n                - \'cache_dir\': path to the cached preprocessed data.\n                - \'num_workers\': number of process used in parallel for preprocessing (default: number of cores)\n            preprocessing_transform (Callable): Called on the outputs of _get_data over the indices\n                                                from 0 to len(self) during the construction of the dataset,\n                                                the preprocessed outputs are then cached to \'cache_dir\'.\n            transform (Callable): Called on the preprocessed data at __getitem__.\n            no_progress (bool): disable tqdm progress bar for preprocessing.\n        """"""\n        self.initialize(*args, **kwargs)\n        if preprocessing_transform is not None:\n            desc = \'Applying preprocessing\'\n            if preprocessing_params is None:\n                preprocessing_params = {}\n\n            cache_dir = preprocessing_params.get(\'cache_dir\')\n            assert cache_dir is not None, \'Cache directory is not given\'\n\n            self.cache_convert = helpers.Cache(\n                preprocessing_transform,\n                cache_dir=cache_dir,\n                cache_key=helpers._get_hash(repr(preprocessing_transform))\n            )\n\n            use_cuda = preprocessing_params.get(\'use_cuda\', False)\n\n            num_workers = preprocessing_params.get(\'num_workers\')\n            uncached = [idx for idx in range(len(self)) if self._get_attributes(idx)[\n                \'name\'] not in self.cache_convert.cached_ids]\n            if len(uncached) > 0:\n                if num_workers == 0:\n                    with torch.no_grad():\n                        for idx in tqdm(range(len(self)), desc=desc, disable=no_progress):\n                            name = self._get_attributes(idx)[\'name\']\n                            data = self._get_data(idx)\n                            self.cache_convert(name, *data)\n                else:\n                    p = Pool(num_workers)\n                    iterator = p.imap_unordered(\n                        _preprocess_task,\n                        [(idx, self._get_data, self._get_attributes, self.cache_convert)\n                            for idx in uncached])\n                    for i in tqdm(range(len(uncached)), desc=desc, disable=no_progress):\n                        next(iterator)\n        else:\n            self.cache_convert = None\n\n        self.transform = transform\n\n    def __getitem__(self, index):\n        """"""Returns the item at index idx. """"""\n        attributes = self._get_attributes(index)\n        data = (self._get_data(index) if self.cache_convert is None else\n                self.cache_convert(attributes[\'name\']))\n\n        if self.transform is not None:\n            data = self.transform(data)\n\n        return KaolinDatasetItem(data=data, attributes=attributes)\n\n    @abstractmethod\n    def initialize(self, *args, **kwargs):\n        pass\n\n    @abstractmethod\n    def _get_attributes(self, index):\n        pass\n\n    @abstractmethod\n    def _get_data(self, index):\n        pass\n\n    @abstractmethod\n    def __len__(self):\n        pass\n\n\nclass CombinationDataset(KaolinDataset):\n    """"""Dataset combining a list of datasets into a unified dataset object.\n    Useful when multiple output representations are needed from a common base representation\n    (Eg. when a mesh is to be served as both a pointcloud and a voxelgrid, etc.)\n    the output of _get_attributes will be a tuple of all the _get_attributes of the dataset list\n    the output of _get_data wiil be a tuple of all the \'data\' of the __getitem__ of the dataset list\n\n    Args:\n        datasets: list or tuple of KaolinDataset\n    """"""\n\n    def initialize(self, datasets):\n        self.len = len(datasets[0])\n        for i, d in enumerate(datasets):\n            assert len(d) == self.len, \\\n                f""All datasets must have the same length. Invalid length at index {i} (expected: {self.len}, got: {len(d)})""\n        self.datasets = datasets\n\n    def __len__(self):\n        return self.len\n\n    def _get_attributes(self, index):\n        return (d._get_attributes(index) for d in self.datasets)\n\n    def _get_data(self, index):\n        return (d[index].data for d in self.datasets)\n'"
kaolin/datasets/modelnet.py,1,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Iterable, Optional\n\nimport torch\nimport os\nfrom glob import glob\n\nfrom kaolin.rep.TriangleMesh import TriangleMesh\nfrom .base import KaolinDataset\n\n\nclass ModelNet(KaolinDataset):\n    r""""""Dataset class for the ModelNet dataset.\n\n    Args:\n        root (str): Path to the base directory of the ModelNet dataset.\n        split (str, optional): Split to load (\'train\' vs \'test\', default: \'train\').\n        categories (iterable, optional): List of categories to load (default: [\'chair\']).\n\n    Examples:\n        >>> dataset = ModelNet(root=\'data/ModelNet\')\n        >>> train_loader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=8)\n        >>> obj, label = next(iter(train_loader))\n    """"""\n\n    def initialize(self, root: str,\n                   split: Optional[str] = \'train\',\n                   categories: Optional[Iterable] = None):\n        """"""Initialize the dataset.\n\n        Args:\n            root (str): Path to the base directory of the ModelNet dataset.\n            split (str, optional): Split to load (\'train\' vs \'test\', default: \'train\').\n            categories (iterable, optional): List of categories to load (default: [\'chair\']).\n        """"""\n\n        assert split.lower() in [\'train\', \'test\']\n\n        if categories is None:\n            categories = [\'chair\']\n\n        self.root = root\n        self.categories = categories\n        self.names = []\n        self.filepaths = []\n        self.cat_idxs = []\n\n        if not os.path.exists(root):\n            raise ValueError(\'ModelNet was not found at ""{0}"".\'.format(root))\n\n        available_categories = [p for p in os.listdir(root) if os.path.isdir(os.path.join(root, p))]\n\n        for cat_idx, category in enumerate(categories):\n            assert category in available_categories, \'object class {0} not in list of available classes: {1}\'.format(\n                category, available_categories)\n\n            cat_paths = glob(os.path.join(root, category, split.lower(), \'*.off\'))\n\n            self.cat_idxs += [cat_idx] * len(cat_paths)\n            self.names += [os.path.splitext(os.path.basename(cp))[0] for cp in cat_paths]\n            self.filepaths += cat_paths\n\n    def __len__(self):\n        return len(self.names)\n\n    def _get_data(self, index):\n        data = TriangleMesh.from_off(self.filepaths[index])\n        return data\n\n    def _get_attributes(self, index):\n        category = torch.tensor(self.cat_idxs[index], dtype=torch.long)\n        return {\n            \'category\': category,\n        }\n'"
kaolin/datasets/nusc.py,2,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nfrom functools import reduce\n\nimport torch\nimport numpy as np\n\nfrom nuscenes.utils.geometry_utils import transform_matrix\nfrom nuscenes.utils.splits import create_splits_scenes\nfrom nuscenes.utils.data_classes import LidarPointCloud\nfrom nuscenes.nuscenes import NuScenes\nfrom nuscenes.utils.data_classes import Box\nfrom pyquaternion import Quaternion\n\nfrom .base import KaolinDataset\nfrom kaolin.rep import PointCloud\n\n\nclass NuscDetection(KaolinDataset):\n    r""""""nuScenes dataloader https://www.nuscenes.org/\n    Args:\n        nusc (NuScenes): nuScenes data parser\n        train (bool): If True, return the training set, otherwise the val set\n        nsweeps (int): number of lidar sweeps ( 1 <= nsweeps)\n        min_distance (float): minimum distance of lidar points to origin (in order to remove points on the ego car)\n    Returns:\n        .. code-block::\n\n           dict: {\n                attributes: {},\n                data: {pc: kaolin.rep.PointCloud, boxes: list}\n           }\n    Example:\n        >>> nusc = NuScenes(version=\'v1.0-mini\', dataroot=\'../data/nuscenes\')\n        >>> traindata = kal.datasets.NuscDetection(nusc, train=True, nsweeps=5)\n        >>> inst = traindata[0]\n        >>> inst[\'data\'][\'pc\'].points.shape\n        torch.Size([24630, 5])\n    """"""\n\n    def initialize(self, nusc: NuScenes, train: bool, nsweeps: int, min_distance: float = 2.2):\n        assert(nsweeps >= 1), f\'nsweeps {nsweeps} should be 1 or greater\'\n        self.nusc = nusc\n        self.train = train\n        self.nsweeps = nsweeps\n        self.min_distance = min_distance\n\n        self.scenes = self._get_scenes()\n        self.samples = self._get_samples()\n\n    def _get_scenes(self):\n        # filter by scene split\n        split = {\n            \'v1.0-trainval\': {True: \'train\', False: \'val\'},\n            \'v1.0-mini\': {True: \'mini_train\', False: \'mini_val\'},\n            \'lyft\': {True: \'lyft_train\', False: \'lyft_val\'},\n        }[self.nusc.version][self.train]\n\n        return create_splits_scenes()[split]\n\n    def _get_samples(self):\n        samples = list(self.nusc.sample)\n\n        # remove samples that aren\'t in this set\n        samples = [samp for samp in samples if\n                   self.nusc.get(\'scene\', samp[\'scene_token\'])[\'name\'] in self.scenes]\n\n        # sort by scene, timestamp\n        samples.sort(key=lambda x: (x[\'scene_token\'], x[\'timestamp\']))\n\n        return samples\n\n    def __len__(self):\n        """"""Returns the length of the dataset. """"""\n        return len(self.samples)\n\n    def _get_data(self, index):\n        sample = self.samples[index]\n\n        # point cloud\n        pc = get_lidar_data(self.nusc, sample, self.nsweeps,\n                            min_distance=self.min_distance)\n        pc = PointCloud(torch.from_numpy(pc.T))\n\n        # bounding boxes\n        boxes = get_boxes(self.nusc, sample)\n\n        return {\'pc\': pc, \'boxes\': boxes}\n\n    def _get_attributes(self, index):\n        return {}\n\n\ndef get_lidar_data(nusc, sample_rec, nsweeps, min_distance):\n    """"""Similar to LidarPointCloud.from_file_multisweep but returns in the ego car frame.""""""\n    # Init.\n    points = np.zeros((5, 0))\n\n    # Get reference pose and timestamp.\n    ref_sd_token = sample_rec[\'data\'][\'LIDAR_TOP\']\n    ref_sd_rec = nusc.get(\'sample_data\', ref_sd_token)\n    ref_pose_rec = nusc.get(\'ego_pose\', ref_sd_rec[\'ego_pose_token\'])\n    ref_cs_rec = nusc.get(\'calibrated_sensor\', ref_sd_rec[\'calibrated_sensor_token\'])\n    ref_time = 1e-6 * ref_sd_rec[\'timestamp\']\n\n    # Homogeneous transformation matrix from global to _current_ ego car frame.\n    car_from_global = transform_matrix(ref_pose_rec[\'translation\'], Quaternion(ref_pose_rec[\'rotation\']),\n                                       inverse=True)\n\n    # Aggregate current and previous sweeps.\n    sample_data_token = sample_rec[\'data\'][\'LIDAR_TOP\']\n    current_sd_rec = nusc.get(\'sample_data\', sample_data_token)\n    for _ in range(nsweeps):\n        # Load up the pointcloud and remove points close to the sensor.\n        current_pc = LidarPointCloud.from_file(os.path.join(nusc.dataroot, current_sd_rec[\'filename\']))\n        current_pc.remove_close(min_distance)\n\n        # Get past pose.\n        current_pose_rec = nusc.get(\'ego_pose\', current_sd_rec[\'ego_pose_token\'])\n        global_from_car = transform_matrix(current_pose_rec[\'translation\'],\n                                           Quaternion(current_pose_rec[\'rotation\']), inverse=False)\n\n        # Homogeneous transformation matrix from sensor coordinate frame to ego car frame.\n        current_cs_rec = nusc.get(\'calibrated_sensor\', current_sd_rec[\'calibrated_sensor_token\'])\n        car_from_current = transform_matrix(current_cs_rec[\'translation\'], Quaternion(current_cs_rec[\'rotation\']),\n                                            inverse=False)\n\n        # Fuse four transformation matrices into one and perform transform.\n        trans_matrix = reduce(np.dot, [car_from_global, global_from_car, car_from_current])\n        current_pc.transform(trans_matrix)\n\n        # Add time vector which can be used as a temporal feature.\n        time_lag = 1e-6 * current_sd_rec[\'timestamp\'] - ref_time\n        times = np.full((1, current_pc.nbr_points()), time_lag)\n\n        new_points = np.concatenate((current_pc.points, times), 0)\n        points = np.concatenate((points, new_points), 1)\n\n        # Abort if there are no previous sweeps.\n        if current_sd_rec[\'prev\'] == \'\':\n            break\n        else:\n            current_sd_rec = nusc.get(\'sample_data\', current_sd_rec[\'prev\'])\n\n    return points\n\n\ndef get_boxes(nusc, rec):\n    """"""Simplified version of nusc.get_boxes""""""\n    egopose = nusc.get(\'ego_pose\',\n                       nusc.get(\'sample_data\', rec[\'data\'][\'LIDAR_TOP\'])[\'ego_pose_token\'])\n    trans = -np.array(egopose[\'translation\'])\n    rot = Quaternion(egopose[\'rotation\']).inverse\n    boxes = []\n    for tok in rec[\'anns\']:\n        inst = nusc.get(\'sample_annotation\', tok)\n\n        box = Box(inst[\'translation\'], inst[\'size\'], Quaternion(inst[\'rotation\']),\n                  name=inst[\'category_name\'])\n        box.translate(trans)\n        box.rotate(rot)\n\n        boxes.append(box)\n\n    return boxes\n'"
kaolin/datasets/scannet.py,6,"b'from typing import Callable, Optional\n\nimport os\nfrom collections import OrderedDict\nimport torch.utils.data as data\nfrom torchvision import transforms\n\n\nclass ScanNet(data.Dataset):\n    r""""""ScanNet dataset http://www.scan-net.org/\n\n    Args:\n        root_dir (str): Path to the base directory of the dataset.\n        scene_file (str): Path to file containing a list of scenes\n            to be loaded.\n        transform (callable, optional): A function/transform that takes in a PIL\n            image and returns a transformed version of the image (default: None).\n        label_transform (callable, optional): A function/transform that takes\n            in the target and transforms it. (default: None).\n        loader (callable, optional): A function to load an image given its path.\n            By default, ``default_loader`` is used.\n        color_mean (list): A list of length 3, containing the R, G, B channelwise\n            mean.\n        color_std (list): A list of length 3, containing the R, G, B channelwise\n            standard deviation.\n        load_depth (bool): Whether or not to load depth images (architectures\n            that use depth information need depth to be loaded).\n        seg_classes (string): The palette of classes that the network should\n            learn.\n\n    """"""\n\n    def __init__(self, root_dir: str, scene_id: str,\n                 mode: Optional[str] = \'inference\',\n                 transform: Optional[Callable] = None,\n                 label_transform: Optional[Callable] = None,\n                 loader: Optional[Callable] = None,\n                 color_mean: Optional[list] = [0.,0.,0.],\n                 color_std: Optional[list] = [1.,1.,1.],\n                 load_depth: Optional[bool] = False,\n                 seg_classes: Optional[str] = \'nyu40\'):\n        self.root_dir = root_dir\n        self.scene_id = scene_id\n        self.mode = mode\n        self.transform = transform\n        self.label_transform = label_transform\n        self.loader = loader\n        self.length = 0\n        self.color_mean = color_mean\n        self.color_std = color_std\n        self.load_depth = load_depth\n        self.seg_classes = seg_classes\n        # color_encoding has to be initialized AFTER seg_classes\n        self.color_encoding = self.get_color_encoding()\n\n        if self.loader is None:\n            if self.load_depth is True:\n                self.loader = self.scannet_loader_depth\n            else:\n                self.loader = self.scannet_loader\n\n        # Get test data and labels filepaths\n        self.data, self.depth, self.labels = get_filenames_scannet(\n            self.root_dir, self.scene_id)\n        self.length += len(self.data)        \n\n    def __getitem__(self, index):\n        """""" Returns element at index in the dataset.\n\n        Args:\n            index (``int``): index of the item in the dataset\n\n        Returns:\n            A tuple of ``PIL.Image`` (image, label) where label is the ground-truth of the image\n\n        """"""\n\n        if self.load_depth is True:\n\n            data_path, depth_path, label_path = self.data[index], self.depth[index], self.labels[index]\n            rgbd, label = self.loader(data_path, depth_path, label_path, self.color_mean, self.color_std, \\\n                self.seg_classes)\n            return rgbd, label, data_path, depth_path, label_path\n\n        else:\n\n            data_path, label_path = self.data[index], self.labels[index]\n            img, label = self.loader(data_path, label_path, self.color_mean, self.color_std, self.seg_classes)\n\n            return img, label, data_path, label_path\n\n    def __len__(self):\n        """""" Returns the length of the dataset. """"""\n        return self.length\n\n    def get_color_encoding(self):\n        if self.seg_classes.lower() == \'nyu40\':\n            """"""Color palette for nyu40 labels """"""\n            return OrderedDict([\n                (\'unlabeled\', (0, 0, 0)),\n                (\'wall\', (174, 199, 232)),\n                (\'floor\', (152, 223, 138)),\n                (\'cabinet\', (31, 119, 180)),\n                (\'bed\', (255, 187, 120)),\n                (\'chair\', (188, 189, 34)),\n                (\'sofa\', (140, 86, 75)),\n                (\'table\', (255, 152, 150)),\n                (\'door\', (214, 39, 40)),\n                (\'window\', (197, 176, 213)),\n                (\'bookshelf\', (148, 103, 189)),\n                (\'picture\', (196, 156, 148)),\n                (\'counter\', (23, 190, 207)),\n                (\'blinds\', (178, 76, 76)),\n                (\'desk\', (247, 182, 210)),\n                (\'shelves\', (66, 188, 102)),\n                (\'curtain\', (219, 219, 141)),\n                (\'dresser\', (140, 57, 197)),\n                (\'pillow\', (202, 185, 52)),\n                (\'mirror\', (51, 176, 203)),\n                (\'floormat\', (200, 54, 131)),\n                (\'clothes\', (92, 193, 61)),\n                (\'ceiling\', (78, 71, 183)),\n                (\'books\', (172, 114, 82)),\n                (\'refrigerator\', (255, 127, 14)),\n                (\'television\', (91, 163, 138)),\n                (\'paper\', (153, 98, 156)),\n                (\'towel\', (140, 153, 101)),\n                (\'showercurtain\', (158, 218, 229)),\n                (\'box\', (100, 125, 154)),\n                (\'whiteboard\', (178, 127, 135)),\n                (\'person\', (120, 185, 128)),\n                (\'nightstand\', (146, 111, 194)),\n                (\'toilet\', (44, 160, 44)),\n                (\'sink\', (112, 128, 144)),\n                (\'lamp\', (96, 207, 209)),\n                (\'bathtub\', (227, 119, 194)),\n                (\'bag\', (213, 92, 176)),\n                (\'otherstructure\', (94, 106, 211)),\n                (\'otherfurniture\', (82, 84, 163)),\n                (\'otherprop\', (100, 85, 144)),\n            ])\n        elif self.seg_classes.lower() == \'scannet20\':\n            return OrderedDict([\n                (\'unlabeled\', (0, 0, 0)),\n                (\'wall\', (174, 199, 232)),\n                (\'floor\', (152, 223, 138)),\n                (\'cabinet\', (31, 119, 180)),\n                (\'bed\', (255, 187, 120)),\n                (\'chair\', (188, 189, 34)),\n                (\'sofa\', (140, 86, 75)),\n                (\'table\', (255, 152, 150)),\n                (\'door\', (214, 39, 40)),\n                (\'window\', (197, 176, 213)),\n                (\'bookshelf\', (148, 103, 189)),\n                (\'picture\', (196, 156, 148)),\n                (\'counter\', (23, 190, 207)),\n                (\'desk\', (247, 182, 210)),\n                (\'curtain\', (219, 219, 141)),\n                (\'refrigerator\', (255, 127, 14)),\n                (\'showercurtain\', (158, 218, 229)),\n                (\'toilet\', (44, 160, 44)),\n                (\'sink\', (112, 128, 144)),\n                (\'bathtub\', (227, 119, 194)),\n                (\'otherfurniture\', (82, 84, 163)),\n            ])\n\n    def get_filenames_scannet(base_dir: str, scene_id: str):\n        """"""Helper function that returns a list of scannet images and the\n        corresponding segmentation labels, given a base directory name\n        and a scene id.\n\n        Args:\n        base_dir (str): Path to the base directory containing ScanNet\n            data, in the directory structure specified in\n            https://github.com/angeladai/3DMV/tree/master/prepare_data\n        scene_id (str): ScanNet scene id\n\n        """"""\n\n        if not os.path.isdir(base_dir):\n            raise RuntimeError(\'\\\'{0}\\\' is not a directory.\'.format(base_dir))\n\n        color_images = []\n        depth_images = []\n        labels = []\n\n        # Explore the directory tree to get a list of all files\n        for path, _, files in os.walk(os.path.join(\n                                      base_dir, scene_id, \'color\')):\n            files = natsorted(files)\n            for file in files:\n                filename, _ = os.path.splitext(file)\n                depthfile = os.path.join(base_dir, scene_id, \'depth\',\n                    filename + \'.png\')\n                labelfile = os.path.join(base_dir, scene_id, \'label\',\n                    filename + \'.png\')\n                # Add this file to the list of train samples, only if its\n                # corresponding depth and label files exist.\n                if os.path.exists(depthfile) and os.path.exists(labelfile):\n                    color_images.append(os.path.join(base_dir, scene_id,\n                        \'color\', filename + \'.jpg\'))\n                    depth_images.append(depthfile)\n                    labels.append(labelfile)\n\n        # Assert that we have the same number of color, depth images as labels\n        assert (len(color_images) == len(depth_images) == len(labels))\n\n        return color_images, depth_images, labels\n\n    def get_files(self, folder: str, name_filter: Optional[str] = None,\n                  extension_filter: Optional[str] = None):\n        """"""Helper function that returns the list of files in a specified folder\n        with a specified extension.\n\n        Args:\n        folder (str): The path to a folder.\n        name_filter (str, optional): The returned files must contain\n            this substring in their filename (default: None, files are\n            not filtered).\n        extension_filter (str, optional): The desired file extension\n            (default: None; files are not filtered).\n\n        """"""\n        if not os.path.isdir(folder):\n            raise RuntimeError(""\\""{0}\\"" is not a folder."".format(folder))\n\n        # Filename filter: if not specified don\'t filter (condition always\n        # true); otherwise, use a lambda expression to filter out files that\n        # do not contain ""name_filter""\n        if name_filter is None:\n            # This looks hackish...there is probably a better way\n            name_cond = lambda filename: True\n        else:\n            name_cond = lambda filename: name_filter in filename\n\n        # Extension filter: if not specified don\'t filter (condition always\n        # true); otherwise, use a lambda expression to filter out files whose\n        # extension is not ""extension_filter""\n        if extension_filter is None:\n            # This looks hackish...there is probably a better way\n            ext_cond = lambda filename: True\n        else:\n            ext_cond = lambda filename: filename.endswith(extension_filter)\n\n        filtered_files = []\n\n        # Explore the directory tree to get files that contain ""name_filter""\n        # and with extension ""extension_filter""\n        for path, _, files in os.walk(folder):\n            files.sort()\n            for file in files:\n                if name_cond(file) and ext_cond(file):\n                    full_path = os.path.join(path, file)\n                    filtered_files.append(full_path)\n\n        return filtered_files\n\n    def scannet_loader(self, data_path: str, label_path: str,\n                       color_mean: Optional[list] = [0.,0.,0.],\n                       color_std: Optional[list] = [1.,1.,1.],\n                       seg_classes: str = \'nyu40\'):\n        """"""Loads a sample and label image given their path as PIL images\n        (nyu40 classes).\n\n        Args:\n        data_path (str): The filepath to the image.\n        label_path (str): The filepath to the ground-truth image.\n        color_mean (str): R, G, B channel-wise mean\n        color_std (str): R, G, B channel-wise stddev\n        seg_classes (str): Palette of classes to load labels for\n            (\'nyu40\' or \'scannet20\')\n\n        Returns the image and the label as PIL images.\n\n        """"""\n\n        # Load image.\n        data = np.array(imageio.imread(data_path))\n        # Reshape data from H x W x C to C x H x W.\n        data = np.moveaxis(data, 2, 0)\n        # Define normalizing transform.\n        normalize = transforms.Normalize(mean=color_mean, std=color_std)\n        # Convert image to float and map range from [0, 255] to [0.0, 1.0].\n        # Then normalize.\n        data = normalize(torch.Tensor(data.astype(np.float32) / 255.0))\n\n        # Load label.\n        if seg_classes.lower() == \'nyu40\':\n            label = np.array(imageio.imread(label_path)).astype(np.uint8)\n        elif seg_classes.lower() == \'scannet20\':\n            label = np.array(imageio.imread(label_path)).astype(np.uint8)\n            # Remap classes from \'nyu40\' to \'scannet20\'\n            label = self.nyu40_to_scannet20(label)\n\n        return data, label\n\n\n    def scannet_loader_depth(self, data_path: str, depth_path: str,\n                             label_path: str,\n                             color_mean: Optional[list] = [0.,0.,0.],\n                             color_std: Optional[list] = [1.,1.,1.],\n                             seg_classes: Optional[str] = \'nyu40\'):\n        """"""Loads a sample and label image given their path as PIL images\n        (nyu40 classes).\n\n        Args:\n        data_path (str): The filepath to the image.\n        depth_path (str): The filepath to the depth png.\n        label_path (str): The filepath to the ground-truth image.\n        color_mean (list): R, G, B channel-wise mean.\n        color_std (list): R, G, B channel-wise stddev.\n        seg_classes (str): Palette of classes to load labels for\n            (\'nyu40\' or \'scannet20\').\n\n        Returns:\n            (PIL.Image): the image\n            (PIL.Image): the label as PIL images.\n\n        """"""\n\n        # Load image\n        rgb = np.array(imageio.imread(data_path))\n        # Reshape rgb from H x W x C to C x H x W\n        rgb = np.moveaxis(rgb, 2, 0)\n        # Define normalizing transform\n        normalize = transforms.Normalize(mean=color_mean, std=color_std)\n        # Convert image to float and map range from [0, 255] to [0.0, 1.0].\n        # Then normalize.\n        rgb = normalize(torch.Tensor(rgb.astype(np.float32) / 255.0))\n\n        # Load depth\n        depth = torch.Tensor(np.array(imageio.imread(depth_path)).astype(\n            np.float32) / 1000.0)\n        depth = torch.unsqueeze(depth, 0)\n\n        # Concatenate rgb and depth\n        data = torch.cat((rgb, depth), 0)\n\n        # Load label\n        if seg_classes.lower() == \'nyu40\':\n            label = np.array(imageio.imread(label_path)).astype(np.uint8)\n        elif seg_classes.lower() == \'scannet20\':\n            label = np.array(imageio.imread(label_path)).astype(np.uint8)\n            # Remap classes from \'nyu40\' to \'scannet20\'\n            label = self.nyu40_to_scannet20(label)\n\n        return data, label\n\n\n    def nyu40_to_scannet20(self, label: str):\n        """"""Remap a label image from the \'nyu40\' class palette to the\n        \'scannet20\' class palette """"""\n\n        # Ignore indices 13, 15, 17, 18, 19, 20, 21, 22, 23, 25, 26. 27. 29.\n        # 30. 31. 32, 35. 37. 38, 40\n        # Because, these classes from \'nyu40\' are absent from \'scannet20\'.\n        # Our label files are in \'nyu40\' format, hence this \'hack\'.\n        # To see detailed class lists visit:\n        # http://kaldir.vc.in.tum.de/scannet_benchmark/labelids_all.txt\n        # (for \'nyu40\' labels), and\n        # http://kaldir.vc.in.tum.de/scannet_benchmark/labelids.txt\n        # (for \'scannet20\' labels).\n        # The remaining labels are then to be mapped onto a contiguous\n        # ordering in the range [0,20].\n\n        # The remapping array comprises tuples (src, tar), where \'src\'\n        # is the \'nyu40\' label, and \'tar\' is the corresponding target\n        # \'scannet20\' label.\n        remapping = [(0, 0), (13, 0), (15, 0), (17, 0), (18, 0), (19, 0),\n                     (20, 0), (21, 0), (22, 0), (23, 0), (25, 0), (26, 0),\n                     (27, 0), (29, 0), (30, 0), (31, 0), (32, 0), (35, 0),\n                     (37, 0), (38, 0), (40, 0), (14, 13), (16, 14), (24, 15),\n                     (28, 16), (33, 17), (34, 18), (36, 19), (39, 20)]\n        for src, tar in remapping:\n            label[np.where(label==src)] = tar\n        return label\n\n    def create_label_image(output, color_palette):\n        """"""Create a label image, given a network output (each pixel contains\n        # class index) and a color palette.\n\n        Args:\n        output (np.array, dtype = np.uint8): Output image. Height x Width.\n            Each pixel contains an integer, corresponding to the class label\n            for that pixel.\n        color_palette (OrderedDict): Contains (R, G, B) colors (uint8)\n            for each class.\n\n        """"""\n        \n        label_image = np.zeros((output.shape[0], output.shape[1], 3),\n                               dtype=np.uint8)\n        for idx, color in enumerate(color_palette):\n            label_image[output==idx] = color\n        return label_image\n'"
kaolin/datasets/shapenet.py,36,"b'# Copyright (c) 2019-2020, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nimport os\nfrom pathlib import Path\nimport torch\nimport torch.utils.data as data\nimport warnings\nimport urllib.request\nimport zipfile\nimport json\nimport re\nfrom collections import OrderedDict\nfrom glob import glob\nimport numpy as np\nimport random\n\nfrom tqdm import tqdm\nimport scipy.sparse\nimport tarfile\nfrom PIL import Image\n\nimport kaolin as kal\nfrom kaolin.rep.TriangleMesh import TriangleMesh\nfrom kaolin.rep.QuadMesh import QuadMesh\n\nfrom kaolin.transforms import pointcloudfunc as pcfunc\nfrom kaolin.transforms import meshfunc\nfrom kaolin.transforms import voxelfunc\nfrom kaolin.transforms import transforms as tfs\nfrom kaolin import helpers\nimport kaolin.conversions.meshconversions as mesh_cvt\n\nfrom .base import KaolinDataset\n\n# Synset to Label mapping (for ShapeNet core classes)\nsynset_to_label = {\'04379243\': \'table\', \'03211117\': \'monitor\', \'04401088\': \'phone\',\n                   \'04530566\': \'watercraft\', \'03001627\': \'chair\', \'03636649\': \'lamp\',\n                   \'03691459\': \'speaker\', \'02828884\': \'bench\', \'02691156\': \'plane\',\n                   \'02808440\': \'bathtub\', \'02871439\': \'bookcase\', \'02773838\': \'bag\',\n                   \'02801938\': \'basket\', \'02880940\': \'bowl\', \'02924116\': \'bus\',\n                   \'02933112\': \'cabinet\', \'02942699\': \'camera\', \'02958343\': \'car\',\n                   \'03207941\': \'dishwasher\', \'03337140\': \'file\', \'03624134\': \'knife\',\n                   \'03642806\': \'laptop\', \'03710193\': \'mailbox\', \'03761084\': \'microwave\',\n                   \'03928116\': \'piano\', \'03938244\': \'pillow\', \'03948459\': \'pistol\',\n                   \'04004475\': \'printer\', \'04099429\': \'rocket\', \'04256520\': \'sofa\',\n                   \'04554684\': \'washer\', \'04090263\': \'rifle\', \'02946921\': \'can\'}\n\n# Label to Synset mapping (for ShapeNet core classes)\nlabel_to_synset = {v: k for k, v in synset_to_label.items()}\n\n\nclass print_wrapper(object):\n    def __init__(self, text, logger=sys.stdout.write):\n        self.text = text\n        self.logger = logger\n\n    def __enter__(self):\n        self.logger(self.text)\n\n    def __exit__(self, *args):\n        self.logger(""\\t[done]\\n"")\n\n\ndef _convert_categories(categories):\n    assert categories is not None, \'List of categories cannot be empty!\'\n    if not (c in synset_to_label.keys() + label_to_synset.keys()\n            for c in categories):\n        warnings.warn(\'Some or all of the categories requested are not part of \\\n            ShapeNetCore. Data loading may fail if these categories are not avaliable.\')\n    synsets = [label_to_synset[c] if c in label_to_synset.keys()\n               else c for c in categories]\n    return synsets\n\n\nclass ShapeNet_Meshes(data.Dataset):\n    r""""""ShapeNet Dataset class for meshes.\n\n    Args:\n        root (str): Path to the root directory of the ShapeNet dataset.\n        categories (str): List of categories to load from ShapeNet. This list may\n                contain synset ids, class label names (for ShapeNetCore classes),\n                or a combination of both.\n        train (bool): If True, return the training set, otherwise the test set\n        split (float): fraction of the dataset to be used for training (>=0 and <=1)\n        no_progress (bool): if True, disables progress bar\n    Returns:\n        .. code-block::\n\n           dict: {\n               attributes: {name: str, path: str, synset: str, label: str},\n               data: {vertices: torch.Tensor, faces: torch.Tensor}\n           }\n    Example:\n        >>> meshes = ShapeNet_Meshes(root=\'../data/ShapeNet/\')\n        >>> obj = next(iter(meshes))\n        >>> obj[\'data\'][\'vertices\'].shape\n        torch.Size([2133, 3])\n        >>> obj[\'data\'][\'faces\'].shape\n        torch.Size([1910, 3])\n    """"""\n\n    def __init__(self, root: str, categories: list = [\'chair\'], train: bool = True,\n                 split: float = .7, no_progress: bool = False):\n        self.root = Path(root)\n        self.paths = []\n        self.synset_idxs = []\n        self.synsets = _convert_categories(categories)\n        self.labels = [synset_to_label[s] for s in self.synsets]\n\n        # loops through desired classes\n        for i in range(len(self.synsets)):\n            syn = self.synsets[i]\n            class_target = self.root / syn\n            if not class_target.exists():\n                raise ValueError(\'Class {0} ({1}) was not found at location {2}.\'.format(\n                    syn, self.labels[i], str(class_target)))\n\n            # find all objects in the class\n            models = sorted(class_target.glob(\'*\'))\n            stop = int(len(models) * split)\n            if train:\n                models = models[:stop]\n            else:\n                models = models[stop:]\n            self.paths += models\n            self.synset_idxs += [i] * len(models)\n\n        self.names = [p.name for p in self.paths]\n\n    def __len__(self):\n        """"""Returns the length of the dataset. """"""\n        return len(self.paths)\n\n    def __getitem__(self, index):\n        """"""Returns the item at index idx. """"""\n        data = dict()\n        attributes = dict()\n        synset_idx = self.synset_idxs[index]\n        obj_location = self.paths[index] / \'model.obj\'\n        mesh = TriangleMesh.from_obj(str(obj_location))\n\n        data[\'vertices\'] = mesh.vertices\n        data[\'faces\'] = mesh.faces\n        attributes[\'name\'] = self.names[index]\n        attributes[\'path\'] = obj_location\n        attributes[\'synset\'] = self.synsets[synset_idx]\n        attributes[\'label\'] = self.labels[synset_idx]\n        return {\'data\': data, \'attributes\': attributes}\n\n\nclass ShapeNet(KaolinDataset):\n    r""""""ShapeNetV1 Dataset class for meshes.\n\n    Args:\n        root (str): path to ShapeNet root directory\n        categories (list): List of categories to load from ShapeNet. This list may\n                           contain synset ids, class label names (for ShapeNetCore classes),\n                           or a combination of both.\n        train (bool): If True, return the training set, otherwise the test set\n        split (float): fraction of the dataset to be used for training (>=0 and <=1)\n    Returns:\n        .. code-block::\n\n           dict: {\n                attributes: {name: str, path: str, synset: str, label: str},\n                data: {vertices: torch.Tensor, faces: torch.Tensor}\n           }\n\n    Example:\n        >>> meshes = ShapeNet(root=\'../data/ShapeNet/\')\n        >>> obj = meshes[0]\n        >>> obj[\'data\'].vertices.shape\n        torch.Size([2133, 3])\n        >>> obj[\'data\'].faces.shape\n        torch.Size([1910, 3])\n    """"""\n\n    def initialize(self, root: str, categories: list, train: bool = True, split: float = .7):\n        """"""Initialize the dataset.\n\n        Args:\n            root (str): path to ShapeNet root directory\n            categories (list): List of categories to load from ShapeNet. This list may\n                               contain synset ids, class label names (for ShapeNetCore classes),\n                               or a combination of both.\n            train (bool): If True, return the training set, otherwise the test set\n            split (float): fraction of the dataset to be used for training (>=0 and <=1)""""""\n        self.root = Path(root)\n        self.paths = []\n        self.synset_idxs = []\n        self.synsets = _convert_categories(categories)\n        self.labels = [synset_to_label[s] for s in self.synsets]\n\n        # loops through desired classes\n        for i in range(len(self.synsets)):\n            syn = self.synsets[i]\n            class_target = self.root / syn\n            if not class_target.exists():\n                raise ValueError(\'Class {0} ({1}) was not found at location {2}.\'.format(\n                    syn, self.labels[i], str(class_target)))\n\n            # find all objects in the class\n            models = sorted(class_target.glob(\'*\'))\n            stop = int(len(models) * split)\n            if train:\n                models = models[:stop]\n            else:\n                models = models[stop:]\n            self.paths += models\n            self.synset_idxs += [i] * len(models)\n\n        self.names = [p.name for p in self.paths]\n\n    def __len__(self):\n        """"""Returns the length of the dataset. """"""\n        return len(self.paths)\n\n    def _get_data(self, index):\n        synset_idx = self.synset_idxs[index]\n        obj_location = self.paths[index] / \'model.obj\'\n        mesh = TriangleMesh.from_obj(str(obj_location))\n        return mesh\n\n    def _get_attributes(self, index):\n        synset_idx = self.synset_idxs[index]\n        attributes = {\n            \'name\': self.names[index],\n            \'path\': self.paths[index] / \'model.obj\',\n            \'synset\': self.synsets[synset_idx],\n            \'label\': self.labels[synset_idx]\n        }\n        return attributes\n\n\nclass ShapeNet_Images(data.Dataset):\n    r""""""ShapeNet Dataset class for images.\n\n    Arguments:\n        root (str): Path to the root directory of the ShapeNet dataset.\n        categories (str): List of categories to load from ShapeNet. This list may\n                contain synset ids, class label names (for ShapeNetCore classes),\n                or a combination of both.\n        train (bool): if true use the training set, else use the test set\n        split (float): amount of dataset that is training out of\n        views (int): number of viewpoints per object to load\n        transform (torchvision.transforms) : transformation to apply to images\n        no_progress (bool): if True, disables progress bar\n\n    Returns:\n        .. code-block::\n\n        dict: {\n            attributes: {name: str, path: str, synset: str, label: str},\n            data: {vertices: torch.Tensor, faces: torch.Tensor}\n            params: {\n                cam_mat: torch.Tensor,\n                cam_pos: torch.Tensor,\n                azi: float,\n                elevation: float,\n                distance: float\n            }\n        }\n\n    Example:\n        >>> from torch.utils.data import DataLoader\n        >>> images = ShapeNet_Images(root=\'../data/ShapeNetImages\')\n        >>> train_loader = DataLoader(images, batch_size=10, shuffle=True, num_workers=8)\n        >>> obj = next(iter(train_loader))\n        >>> image = obj[\'data\'][\'imgs\']\n        >>> image.shape\n        torch.Size([10, 4, 137, 137])\n    """"""\n\n    def __init__(self, root: str, categories: list = [\'chair\'], train: bool = True,\n                 split: float = .7, views: int = 24, transform=None):\n        self.root = Path(root)\n        self.synsets = _convert_categories(categories)\n        self.labels = [synset_to_label[s] for s in self.synsets]\n        self.transform = transform\n        self.views = views\n        self.names = []\n        self.synset_idx = []\n\n        # check if images exist\n        if not self.root.exists():\n            raise ValueError(\'ShapeNet images were not found at location {0}.\'.format(\n                str(self.root)))\n\n        # find all needed images\n        for i in range(len(self.synsets)):\n            syn = self.synsets[i]\n            class_target = self.root / syn\n            assert class_target.exists(), \\\n                ""ShapeNet class, {0}, is not found"".format(syn)\n\n            models = sorted(class_target.glob(\'*\'))\n            stop = int(len(models) * split)\n            if train:\n                models = models[:stop]\n            else:\n                models = models[stop:]\n            self.names += models\n\n            self.synset_idx += [i] * len(models)\n\n    def __len__(self):\n        """"""Returns the length of the dataset. """"""\n        return len(self.names)\n\n    def __getitem__(self, index):\n        """"""Returns the item at index idx. """"""\n        data = dict()\n        attributes = dict()\n        img_name = self.names[index]\n        view_num = random.randrange(0, self.views)\n        # load and process image\n        img = Image.open(str(img_name / f\'rendering/{view_num:02}.png\'))\n        # apply transformations\n        if self.transform is not None:\n            img = self.transform(img)\n        else:\n            img = torch.FloatTensor(np.array(img))\n            img = img.permute(2, 1, 0)\n            img = img / 255.\n        # load and process camera parameters\n        param_location = img_name / \'rendering/rendering_metadata.txt\'\n        azimuth, elevation, _, distance, _ = np.loadtxt(param_location)[view_num]\n        cam_params = kal.mathutils.geometry.transformations.compute_camera_params(\n            azimuth, elevation, distance)\n\n        data[\'images\'] = img\n        data[\'params\'] = dict()\n        data[\'params\'][\'cam_mat\'] = cam_params[0]\n        data[\'params\'][\'cam_pos\'] = cam_params[1]\n        data[\'params\'][\'azi\'] = azimuth\n        data[\'params\'][\'elevation\'] = elevation\n        data[\'params\'][\'distance\'] = distance\n        attributes[\'name\'] = img_name\n        attributes[\'synset\'] = self.synsets[self.synset_idx[index]]\n        attributes[\'label\'] = self.labels[self.synset_idx[index]]\n        return {\'data\': data, \'attributes\': attributes}\n\n\nclass ShapeNet_Voxels(data.Dataset):\n    r""""""ShapeNet Dataset class for voxels.\n\n    Args:\n        root (str): Path to the root directory of the ShapeNet dataset.\n        cache_dir (str): Path to save cached converted representations.\n        categories (str): List of categories to load from ShapeNet. This list may\n                contain synset ids, class label names (for ShapeNetCore classes),\n                or a combination of both.\n        train (bool): return the training set else the test set\n        split (float): amount of dataset that is training out of 1\n        resolutions (list): list of resolutions to be returned\n        no_progress (bool): if True, disables progress bar\n\n    Returns:\n        .. code-block::\n\n        dict: {\n            attributes: {name: str, synset: str, label: str},\n            data: {[res]: torch.Tensor}\n        }\n\n    Example:\n        >>> from torch.utils.data import DataLoader\n        >>> voxels = ShapeNet_Voxels(root=\'../data/ShapeNet/\', cache_dir=\'cache/\')\n        >>> train_loader = DataLoader(voxels, batch_size=10, shuffle=True, num_workers=8 )\n        >>> obj = next(iter(train_loader))\n        >>> obj[\'data\'][\'128\'].shape\n        torch.Size([10, 128, 128, 128])\n\n    """"""\n\n    def __init__(self, root: str, cache_dir: str, categories: list = [\'chair\'], train: bool = True,\n                 split: float = .7, resolutions=[128, 32], no_progress: bool = False):\n        self.root = Path(root)\n        self.cache_dir = Path(cache_dir) / \'voxels\'\n        self.cache_transforms = {}\n        self.params = {\n            \'resolutions\': resolutions,\n        }\n        mesh_dataset = ShapeNet_Meshes(root=root,\n                                       categories=categories,\n                                       train=train,\n                                       split=split,\n                                       no_progress=no_progress)\n        self.names = mesh_dataset.names\n        self.synset_idxs = mesh_dataset.synset_idxs\n        self.synsets = mesh_dataset.synsets\n        self.labels = mesh_dataset.labels\n\n        for res in self.params[\'resolutions\']:\n            self.cache_transforms[res] = tfs.CacheCompose([\n                tfs.TriangleMeshToVoxelGrid(res, normalize=False, vertex_offset=0.5),\n                tfs.FillVoxelGrid(thresh=0.5),\n                tfs.ExtractProjectOdmsFromVoxelGrid()\n            ], self.cache_dir)\n\n            desc = \'converting to voxels\'\n            for idx in tqdm(range(len(mesh_dataset)), desc=desc, disable=no_progress):\n                name = mesh_dataset.names[idx]\n                if name not in self.cache_transforms[res].cached_ids:\n                    sample = mesh_dataset[idx]\n                    mesh = TriangleMesh.from_tensors(sample[\'data\'][\'vertices\'],\n                                                     sample[\'data\'][\'faces\'])\n                    self.cache_transforms[res](name, mesh)\n\n    def __len__(self):\n        """"""Returns the length of the dataset. """"""\n        return len(self.names)\n\n    def __getitem__(self, index):\n        """"""Returns the item at index idx. """"""\n        data = dict()\n        attributes = dict()\n        name = self.names[index]\n        synset_idx = self.synset_idxs[index]\n\n        for res in self.params[\'resolutions\']:\n            data[str(res)] = self.cache_transforms[res](name)\n        attributes[\'name\'] = name\n        attributes[\'synset\'] = self.synsets[synset_idx]\n        attributes[\'label\'] = self.labels[synset_idx]\n        return {\'data\': data, \'attributes\': attributes}\n\n\nclass ShapeNet_Surface_Meshes(data.Dataset):\n    r""""""ShapeNet Dataset class for watertight meshes with only the surface preserved.\n\n    Arguments:\n        root (str): Path to the root directory of the ShapeNet dataset.\n        cache_dir (str): Path to save cached converted representations.\n        categories (str): List of categories to load from ShapeNet. This list may\n                contain synset ids, class label names (for ShapeNetCore classes),\n                or a combination of both.\n        train (bool): return the training set else the test set\n        split (float): amount of dataset that is training out of 1\n        resolution (int): resolution of voxel object to use when converting\n        smoothing_iteration (int): number of applications of laplacian smoothing\n        no_progress (bool): if True, disables progress bar\n\n    Returns:\n        .. code-block::\n\n        dict: {\n            attributes: {name: str, synset: str, label: str},\n            data: {vertices: torch.Tensor, faces: torch.Tensor}\n        }\n\n    Example:\n        >>> surface_meshes = ShapeNet_Surface_Meshes(root=\'../data/ShapeNet\', cache_dir=\'cache/\')\n        >>> obj = next(iter(surface_meshes))\n        >>> obj[\'data\'][\'vertices\'].shape\n        torch.Size([11617, 3])\n        >>> obj[\'data\'][\'faces\'].shape\n        torch.Size([23246, 3])\n\n    """"""\n\n    def __init__(self, root: str, cache_dir: str, categories: list = [\'chair\'], train: bool = True,\n                 split: float = .7, resolution: int = 100, smoothing_iterations: int = 3, mode=\'Tri\',\n                 no_progress: bool = False):\n        assert mode in [\'Tri\', \'Quad\']\n\n        self.root = Path(root)\n        self.cache_dir = Path(cache_dir) / \'surface_meshes\'\n        dataset_params = {\n            \'root\': root,\n            \'categories\': categories,\n            \'train\': train,\n            \'split\': split,\n            \'no_progress\': no_progress,\n        }\n        self.params = {\n            \'resolution\': resolution,\n            \'smoothing_iterations\': smoothing_iterations,\n            \'mode\': mode,\n        }\n\n        mesh_dataset = ShapeNet_Meshes(**dataset_params)\n        voxel_dataset = ShapeNet_Voxels(**dataset_params, cache_dir=cache_dir, resolutions=[resolution])\n        combined_dataset = ShapeNet_Combination([mesh_dataset, voxel_dataset])\n\n        self.names = combined_dataset.names\n        self.synset_idxs = combined_dataset.synset_idxs\n        self.synsets = combined_dataset.synsets\n        self.labels = combined_dataset.labels\n\n        if mode == \'Tri\':\n            mesh_conversion = tfs.VoxelGridToTriangleMesh(threshold=0.5,\n                                                          mode=\'marching_cubes\',\n                                                          normalize=False)\n        else:\n            mesh_conversion = tfs.VoxelGridToQuadMesh(threshold=0.5,\n                                                      normalize=False)\n\n        def convert(og_mesh, voxel):\n            transforms = tfs.Compose([mesh_conversion,\n                                      tfs.MeshLaplacianSmoothing(smoothing_iterations)])\n\n            new_mesh = transforms(voxel)\n            new_mesh.vertices = pcfunc.realign(new_mesh.vertices, og_mesh.vertices)\n            return {\'vertices\': new_mesh.vertices, \'faces\': new_mesh.faces}\n\n        self.cache_convert = helpers.Cache(convert, self.cache_dir,\n                                           cache_key=helpers._get_hash(self.params))\n\n        desc = \'converting to surface meshes\'\n        for idx in tqdm(range(len(combined_dataset)), desc=desc, disable=no_progress):\n            name = combined_dataset.names[idx]\n            if name not in self.cache_convert.cached_ids:\n                sample = combined_dataset[idx]\n                voxel = sample[\'data\'][str(resolution)]\n                og_mesh = TriangleMesh.from_tensors(sample[\'data\'][\'vertices\'],\n                                                    sample[\'data\'][\'faces\'])\n                self.cache_convert(name, og_mesh=og_mesh, voxel=voxel)\n\n    def __len__(self):\n        """"""Returns the length of the dataset. """"""\n        return len(self.names)\n\n    def __getitem__(self, index):\n        """"""Returns the item at index idx. """"""\n        data = dict()\n        attributes = dict()\n        name = self.names[index]\n        synset_idx = self.synset_idxs[index]\n\n        data = self.cache_convert(name)\n        mesh = TriangleMesh.from_tensors(data[\'vertices\'], data[\'faces\'])\n        data[\'adj\'] = mesh.compute_adjacency_matrix_sparse().coalesce()\n        attributes[\'name\'] = name\n        attributes[\'synset\'] = self.synsets[synset_idx]\n        attributes[\'label\'] = self.labels[synset_idx]\n        return {\'data\': data, \'attributes\': attributes}\n\n\nclass ShapeNet_Points(data.Dataset):\n    r""""""ShapeNet Dataset class for sampled point cloud from each object.\n\n    Args:\n        root (str): Path to the root directory of the ShapeNet dataset.\n        cache_dir (str): Path to save cached converted representations.\n        categories (str): List of categories to load from ShapeNet. This list may\n                contain synset ids, class label names (for ShapeNetCore classes),\n                or a combination of both.\n        train (bool): return the training set else the test set\n        split (float): amount of dataset that is training out of 1\n        num_points (int): number of point sampled on mesh\n        smoothing_iteration (int): number of application of laplacian smoothing\n        surface (bool): if only the surface of the original mesh should be used\n        resolution (int): resolution of voxel object to use when converting\n        normals (bool): should the normals of the points be saved\n        no_progress (bool): if True, disables progress bar\n\n    Returns:\n        .. code-block::\n\n        dict: {\n            attributes: {name: str, synset: str, label: str},\n            data: {points: torch.Tensor, normals: torch.Tensor}\n        }\n\n    Example:\n        >>> from torch.utils.data import DataLoader\n        >>> points = ShapeNet_Points(root=\'../data/ShapeNet\', cache_dir=\'cache/\')\n        >>> train_loader = DataLoader(points, batch_size=10, shuffle=True, num_workers=8)\n        >>> obj = next(iter(train_loader))\n        >>> obj[\'data\'][\'points\'].shape\n        torch.Size([10, 5000, 3])\n\n    """"""\n\n    def __init__(self, root: str, cache_dir: str, categories: list = [\'chair\'], train: bool = True,\n                 split: float = .7, num_points: int = 5000, smoothing_iterations=3,\n                 surface=True, resolution=100, normals=True, no_progress: bool = False):\n        self.root = Path(root)\n        self.cache_dir = Path(cache_dir) / \'points\'\n\n        dataset_params = {\n            \'root\': root,\n            \'categories\': categories,\n            \'train\': train,\n            \'split\': split,\n            \'no_progress\': no_progress,\n        }\n        self.params = {\n            \'num_points\': num_points,\n            \'smoothing_iterations\': smoothing_iterations,\n            \'surface\': surface,\n            \'resolution\': resolution,\n            \'normals\': normals,\n        }\n\n        if surface:\n            dataset = ShapeNet_Surface_Meshes(**dataset_params,\n                                              cache_dir=cache_dir,\n                                              resolution=resolution,\n                                              smoothing_iterations=smoothing_iterations)\n        else:\n            dataset = ShapeNet_Meshes(**dataset_params)\n\n        self.names = dataset.names\n        self.synset_idxs = dataset.synset_idxs\n        self.synsets = dataset.synsets\n        self.labels = dataset.labels\n\n        def convert(mesh):\n            points, face_choices = mesh_cvt.trianglemesh_to_pointcloud(mesh, num_points)\n            face_normals = mesh.compute_face_normals()\n            point_normals = face_normals[face_choices]\n            return {\'points\': points, \'normals\': point_normals}\n\n        self.cache_convert = helpers.Cache(convert, self.cache_dir,\n                                           cache_key=helpers._get_hash(self.params))\n\n        desc = \'converting to points\'\n        for idx in tqdm(range(len(dataset)), desc=desc, disable=no_progress):\n            name = dataset.names[idx]\n            if name not in self.cache_convert.cached_ids:\n                idx = dataset.names.index(name)\n                sample = dataset[idx]\n                mesh = TriangleMesh.from_tensors(sample[\'data\'][\'vertices\'],\n                                                 sample[\'data\'][\'faces\'])\n                self.cache_convert(name, mesh=mesh)\n\n    def __len__(self):\n        """"""Returns the length of the dataset. """"""\n        return len(self.names)\n\n    def __getitem__(self, index):\n        """"""Returns the item at index idx. """"""\n        data = dict()\n        attributes = dict()\n        name = self.names[index]\n        synset_idx = self.synset_idxs[index]\n\n        data = self.cache_convert(name)\n        attributes[\'name\'] = name\n        attributes[\'synset\'] = self.synsets[synset_idx]\n        attributes[\'label\'] = self.labels[synset_idx]\n        return {\'data\': data, \'attributes\': attributes}\n\n\nclass ShapeNet_SDF_Points(data.Dataset):\n    r""""""ShapeNet Dataset class for signed distance functions.\n\n    Args:\n        root (str): Path to the root directory of the ShapeNet dataset.\n        cache_dir (str): Path to save cached converted representations.\n        categories (str): List of categories to load from ShapeNet. This list may\n                contain synset ids, class label names (for ShapeNetCore classes),\n                or a combination of both.\n        train (bool): return the training set else the test set\n        split (float): amount of dataset that is training out of 1\n        resolution (int): resolution of voxel object to use when converting\n        num_points (int): number of sdf points sampled on mesh\n        occ (bool): should only occupancy values be returned instead of distances\n        smoothing_iteration (int): number of application of laplacian smoothing\n        sample_box (bool): whether to sample only from within mesh extents\n        no_progress (bool): if True, disables progress bar\n\n    Returns:\n        .. code-block::\n\n            dict: {\n                attributes: {name: str, synset: str, label: str},\n                data: {\n                    Union[\'occ_values\', \'sdf_distances\']: torch.Tensor,\n                    Union[\'occ_points, \'sdf_points\']: torch.Tensor}\n            }\n\n    Example:\n        >>> from torch.utils.data import DataLoader\n        >>> sdf_points = ShapeNet_SDF_Points(root=\'../data/ShapeNet\', cache_dir=\'cache/\')\n        >>> train_loader = DataLoader(sdf_points, batch_size=10, shuffle=True, num_workers=8)\n        >>> obj = next(iter(train_loader))\n        >>> obj[\'data\'][\'sdf_points\'].shape\n        torch.Size([10, 5000, 3])\n\n    """"""\n\n    def __init__(self, root: str, cache_dir: str, categories: list = [\'chair\'], train: bool = True,\n                 split: float = .7, resolution: int = 100, num_points: int = 5000, occ: bool = False,\n                 smoothing_iterations: int = 3, sample_box=True, no_progress: bool = False):\n        self.root = Path(root)\n        self.cache_dir = Path(cache_dir) / \'sdf_points\'\n\n        self.params = {\n            \'resolution\': resolution,\n            \'num_points\': num_points,\n            \'occ\': occ,\n            \'smoothing_iterations\': smoothing_iterations,\n            \'sample_box\': sample_box,\n        }\n\n        surface_mesh_dataset = ShapeNet_Surface_Meshes(root=root,\n                                                       cache_dir=cache_dir,\n                                                       categories=categories,\n                                                       train=train,\n                                                       split=split,\n                                                       resolution=resolution,\n                                                       smoothing_iterations=smoothing_iterations,\n                                                       no_progress=no_progress)\n\n        self.names = surface_mesh_dataset.names\n        self.synset_idxs = surface_mesh_dataset.synset_idxs\n        self.synsets = surface_mesh_dataset.synsets\n        self.labels = surface_mesh_dataset.labels\n\n        def convert(mesh):\n            sdf = mesh_cvt.trianglemesh_to_sdf(mesh, num_points)\n            bbox_true = torch.stack((mesh.vertices.min(dim=0)[0],\n                                     mesh.vertices.max(dim=0)[0]), dim=1).view(-1)\n            points = 1.05 * (torch.rand(self.params[\'num_points\'], 3).to(mesh.vertices.device) - .5)\n            distances = sdf(points)\n            return {\'points\': points, \'distances\': distances, \'bbox\': bbox_true}\n\n        self.cache_convert = helpers.Cache(convert, self.cache_dir,\n                                           cache_key=helpers._get_hash(self.params))\n\n        desc = \'converting to sdf points\'\n        for idx in tqdm(range(len(surface_mesh_dataset)), desc=desc, disable=no_progress):\n            name = surface_mesh_dataset.names[idx]\n            if name not in self.cache_convert.cached_ids:\n                idx = surface_mesh_dataset.names.index(name)\n                sample = surface_mesh_dataset[idx]\n                mesh = TriangleMesh.from_tensors(sample[\'data\'][\'vertices\'],\n                                                 sample[\'data\'][\'faces\'])\n\n                # Use cuda if available to speed up conversion\n                if torch.cuda.is_available():\n                    mesh.cuda()\n                self.cache_convert(name, mesh=mesh)\n\n    def __len__(self):\n        """"""Returns the length of the dataset. """"""\n        return len(self.names)\n\n    def __getitem__(self, index):\n        """"""Returns the item at index idx. """"""\n        data = dict()\n        attributes = dict()\n        name = self.names[index]\n        synset_idx = self.synset_idxs[index]\n\n        cached_data = self.cache_convert(name)\n        points = cached_data[\'points\']\n        distances = cached_data[\'distances\']\n\n        if self.params[\'sample_box\']:\n            bbox_values = kal.rep.bounding_points(points, cached_data[\'bbox\'])\n            points = points[bbox_values]\n            distances = distances[bbox_values]\n\n        selection = np.random.randint(points.shape[0], size=self.params[\'num_points\'])\n\n        if self.params[\'occ\']:\n            data[\'occ_values\'] = distances[selection] <= 0\n            data[\'occ_points\'] = points[selection]\n        else:\n            data[\'sdf_distances\'] = distances[selection]\n            data[\'sdf_points\'] = points[selection]\n\n        attributes[\'name\'] = self.names[index]\n        attributes[\'synset\'] = self.synsets[synset_idx]\n        attributes[\'label\'] = self.labels[synset_idx]\n        return {\'data\': data, \'attributes\': attributes}\n\n\nclass ShapeNet_Tags(data.Dataset):\n    r""""""ShapeNet Dataset class for tags.\n\n    Args:\n        dataset (kal.dataloader.shapenet.ShapeNet): One of the ShapeNet datasets\n        download (bool): If True will load taxonomy of objects if it is not loaded yet\n        transform (...) : transformation to apply to tags\n\n    Returns:\n        dict: Dictionary with key for the input tags encod and : \'tag_enc\':\n\n    Example:\n        >>> from torch.utils.data import DataLoader\n        >>> meshes = ShapeNet_Meshes(root=\'../data/ShapeNet/\', cache_dir=\'cache/\')\n        >>> tags = ShapeNet_Tags(meshes)\n        >>> train_loader = DataLoader(tags, batch_size=10, shuffle=True, num_workers=8 )\n        >>> obj = next(iter(train_loader))\n        >>> obj[\'data\'][\'tag_enc\'].shape\n        torch.Size([10, N])\n\n    """"""\n\n    def __init__(self, dataset, tag_aug=True):\n        self.root = dataset.root\n        self.paths = dataset.paths\n        self.synset_idxs = dataset.synset_idxs\n        self.synsets = dataset.synsets\n        self.names = dataset.names\n        self.tag_aug = tag_aug\n\n        # load taxonomy\n        self.get_taxonomy()\n        # get the mapping of sample indexes to file names\n        self.get_name_index_mapping()\n\n        self.all_tags = []\n        self.name_tag_map = {}\n        self.tag_name_map = OrderedDict()\n\n        # get labels for instances\n        for synset_index, synset in enumerate(self.synsets):\n            tag = self.get_tags_from_taxonomy(synset)\n            indexes = np.where(np.array(self.synset_idxs) == synset_index)[0]\n            instances = [self.names[ind] for ind in indexes]\n\n            for name in instances:\n                # get tags from labels\n                tag_list = self.get_tags_from_str(tag)\n                self.all_tags.extend(tag_list)\n\n                if name in self.name_to_index.keys():\n                    # populate the mappings\n                    self.update_name_tag_mappings(name, tag_list)\n                else:\n                    print(name)\n\n        # compute unique tags\n        self.all_tags = list(np.unique(self.all_tags))\n        # get indexes of tags for one-hot encoding\n        self.get_tag_indexes()\n\n    def get_tag_indexes(self):\n        self.tag_index = {}\n        for index, tag in enumerate(self.tag_name_map.keys()):\n            self.tag_index[tag] = index\n\n    def get_taxonomy(self):\n        r""""""Download the taxonomy from the web.""""""\n        taxonomy_location = os.path.join(self.root, \'taxonomy.json\')\n        if not os.path.exists(taxonomy_location):\n            with print_wrapper(""Downloading taxonomy ...""):\n                taxonomy_web_location = \'http://shapenet.cs.stanford.edu/shapenet/obj-zip/ShapeNetCore.v1/taxonomy.json\'\n                urllib.request.urlretrieve(taxonomy_web_location,\n                                           filename=taxonomy_location)\n\n    def update_name_tag_mappings(self, name, tag_list):\n        r""""""Create a mapping from the file name to its label and from each tag to a list of files with this tag.""""""\n        # populate tag to file name mapping\n        self.name_tag_map[name] = tag_list\n        # populate inverse mapping\n        for tag in tag_list:\n            if tag not in self.tag_name_map.keys():\n                self.tag_name_map[tag] = [name]\n            else:\n                self.tag_name_map[tag].append(name)\n\n    def get_name_index_mapping(self):\n        r""""""Calculate the mapping between file names and the sample indices""""""\n        self.name_to_index = {}\n        for index, name in enumerate(self.names):\n            self.name_to_index[name] = index\n\n    def get_tags_from_str(self, tags_str, inverse_order=True, forbidden_symbols=["" "", ""/"", ""-"", ""\\\\*""]):\n        r""""""Process the tag string and return a list of tags. ``Note``: The tags that contain forbidden_symbols are ignored.\n\n        Args:\n            tags_str (str): string with comma separated tags.\n            inverse_order (bool): reverse the order of tags\n\n        Returns:\n            list of tags.\n        """"""\n        output_list = [\n            tag.strip() for tag in tags_str.split(\',\')\n            if re.match("".*(:?{}).*"".format(""|"".join(forbidden_symbols)), tag.strip()) is None\n        ]\n        if inverse_order:\n            output_list = output_list[::-1]\n        return output_list\n\n    def get_category_paths(self, category=\'chair\'):\n        r""""""Get the list of SynSet IDs and the respective tags based on the taxonomy.\n\n        Args:\n            category (str): category of the object that needs to be retrieved.\n\n        Returns:\n            synsetIds (list): list of synsets\n            tags (list): list of tags for each synset\n        """"""\n        with open(os.path.join(self.root, \'taxonomy.json\'), \'r\') as json_f:\n            taxonomy = json.load(json_f)\n\n        synsetIds, children = [], []\n        parent_tags, tags = [], []\n\n        for c in taxonomy:\n            tag = c[\'name\']\n\n            matchObj = True\n            if category is not None:\n                matchObj = re.search(\n                    r\'(?<![a-zA-Z0-9])\' + category + \'(?![a-zA-Z0-9])\', tag,\n                    re.M | re.I)\n\n            if matchObj:\n                sid = c[\'synsetId\']\n                if sid not in synsetIds:\n                    synsetIds.append(sid)\n                    tags.append(tag)\n                for childId in c[\'children\']:\n                    if childId not in children:\n                        children.append(childId)\n                        parent_tags.append(tag)\n\n        while len(children) > 0:\n            new_children = []\n            new_parent_tags = []\n            for c in taxonomy:\n                sid = c[\'synsetId\']\n                if sid in children and sid not in synsetIds:\n                    synsetIds.append(sid)\n                    i = children.index(sid)\n                    tag = c[\'name\'] + \',\' + parent_tags[i]\n                    tags.append(tag)\n                    for childId in c[\'children\']:\n                        if childId not in new_children:\n                            new_children.append(childId)\n                            new_parent_tags.append(tag)\n\n            children = new_children\n            parent_tags = new_parent_tags\n\n        return synsetIds, tags\n\n    def get_tags_from_taxonomy(self, synset, verbose=False):\n        r""""""Load category based on the ShapeNet taxonomy.\n\n        Args:\n            category (str): catergory of the object that needs to be retrieved.\n            verbose (bool): If ``True`` - print some additional information.\n\n        Returns:\n            instances (list): list of object paths that contain objects from the requested category.\n            tags (list): list of tags for each instance.\n        """"""\n        category = synset_to_label[synset]\n        synsetIds, tags = self.get_category_paths(category=category)\n        sid = synsetIds.index(synset)\n        tag = tags[sid]\n\n        return tag\n\n    def rand_drop_tag(self, tag_list):\n        r""""""Drop some tags from the label randomly and return the orderring number of the most specific tag.""""""\n        if len(tag_list) == 1:\n            return tag_list, 0\n        else:\n            tags_to_keep = np.random.randint(1, len(tag_list) + 1)\n            res_tag_ind = np.random.choice(range(len(tag_list)),\n                                           tags_to_keep,\n                                           replace=False)\n            max_ind = max(res_tag_ind)\n            res_tag_list = [tag_list[el] for el in res_tag_ind]\n            return res_tag_list, max_ind\n\n    def tag_proc(self, tag_list):\n        """"""Get the embedding from the list of tags. By default this function does\n        one-hot encoding of the tags, but can be replaced by more complex encodings.\n\n        Args:\n            tag_list (list): List of textual tags that need to be encoded.\n        """"""\n        embed = np.zeros(len(self.tag_index), dtype=np.uint8)\n        for tag in tag_list:\n            embed[self.tag_index[tag]] = 1\n\n        return torch.from_numpy(embed)\n\n    def __len__(self):\n        """"""Returns the length of the dataset. """"""\n        return len(self.names)\n\n    def __getitem__(self, index):\n        """"""Returns the item at index idx. """"""\n        data = dict()\n        attributes = dict()\n        name = self.names[index]\n        synset_idx = self.synset_idxs[index]\n\n        full_tag = self.name_tag_map[self.names[index]]\n\n        # do tags augmentation\n        if self.tag_aug:\n            input_tags, last_tag_id = self.rand_drop_tag(full_tag)\n        else:\n            input_tags = full_tag\n            last_tag_id = len(full_tag) - 1\n\n        # tag encodings\n        data[\'tag_inp\'] = self.tag_proc(input_tags)\n        data[\'tag_full\'] = self.tag_proc(full_tag[:(last_tag_id + 1)])\n        data[\'tag_label\'] = self.tag_proc(full_tag)\n        # length of tags per\n        data[\'tag_inp_len\'] = torch.tensor(len(input_tags))\n        data[\'tag_full_len\'] = torch.tensor(len(full_tag[:(last_tag_id + 1)]))\n        data[\'tag_label_len\'] = torch.tensor(len(full_tag))\n\n        attributes[\'name\'] = name\n        attributes[\'synset\'] = self.synsets[synset_idx]\n        attributes[\'label\'] = self.labels[synset_idx]\n        return {\'data\': data, \'attributes\': attributes}\n\n\nclass ShapeNet_Combination(data.Dataset):\n    r""""""ShapeNet Dataset class for combinations of representations.\n\n    Arguments:\n        dataset (list): List of datasets to be combined\n        categories (str): List of categories to load from ShapeNet. This list may\n                contain synset ids, class label names (for ShapeNetCore classes),\n                or a combination of both.\n        root (str): Path to the root directory of the ShapeNet dataset.\n        train (bool): if true use the training set, else use the test set\n\n    Returns:\n        dict: Dictionary with keys indicated by passed datasets\n\n    Example:\n\n        >>> from torch.utils.data import DataLoader\n        >>> shapenet = ShapeNet_Meshes(root=\'../data/ShapeNet\', cache_dir=\'cache/\')\n        >>> voxels = ShapeNet_Voxels(root=\'../data/ShapeNet\', cache_dir=\'cache/\')\n        >>> images = ShapeNet_Images(root=\'../data/ShapeNet\', cache_dir=\'cache/\')\n        >>> points = ShapeNet_Points(root=\'../data/ShapeNet\', cache_dir=\'cache/\')\n        >>> dataset = ShapeNet_Combination([voxels, images, points])\n        >>> train_loader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=8)\n        >>> obj = next(iter(train_loader))\n        >>> for key in obj[\'data\']:\n        ...     print (key)\n        ...\n        params\n        128\n        32\n        imgs\n        cam_mat\n        cam_pos\n        azi\n        elevation\n        distance\n        points\n        normals\n    """"""\n\n    def __init__(self, datasets):\n        self.names = datasets[0].names\n        self.root = datasets[0].root\n        self.synset_idxs = datasets[0].synset_idxs\n        self.synsets = datasets[0].synsets\n        self.labels = datasets[0].labels\n        self.datasets = datasets\n\n    def __len__(self):\n        """"""Returns the length of the dataset. """"""\n        return len(self.names)\n\n    def __getitem__(self, index):\n        """"""Returns the item at index idx. """"""\n        obj = self.datasets[0][index]\n\n        for ds in self.datasets:\n            obj[\'data\'].update(ds[index][\'data\'])\n\n        return obj\n'"
kaolin/datasets/shrec.py,2,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Iterable\n\nimport os\nimport glob\n\nfrom ..rep import TriangleMesh\n\nfrom .base import KaolinDataset\n\n\nclass SHREC16(KaolinDataset):\n    r""""""Dataset class for SHREC16, used for the ""Large-scale 3D shape retrieval\n    from ShapeNet Core55"" contest at Eurographics 2016.\n\n    More details about the challenge and the dataset are available\n    `here <https://shapenet.cs.stanford.edu/shrec16/>`_.\n\n    Args:\n        root (str): Path to the root directory of the dataset.\n        categories (list): List of categories to load (each class is\n            specified as a string, and must be a valid `SHREC16`\n            category). If this argument is not specified, all categories\n            are loaded by default.\n        train (bool): If True, return the train split, else return the test\n            split (default: True).\n\n    Returns:\n        .. code-block::\n\n           dict: {\n                attributes: {path: str, category: str, label: int},\n                data: kaolin.rep.TriangleMesh\n           }\n\n        path: The filepath to the .obj file on disk.\n        category: A human-readable string describing the loaded sample.\n        label: An integer (in the range :math:`[0, \\text{len(categories)}]`) and can be used for training classifiers for example.\n        vertices: Vertices of the loaded mesh (:math:`(*, 3)`), where :math:`*` indicates a positive integer.\n        faces: Faces of the loaded mesh (:math:`(*, 3)`), where :math:`*` indicates a positive integer.\n\n    Example:\n        >>> dataset = SHREC16(root=\'/path/to/SHREC16/\', categories=[\'alien\', \'ants\'], train=False)\n        >>> sample = dataset[0]\n        >>> sample[""attributes""][""path""]\n        /path/to/SHREC16/alien/test/T411.obj\n        >>> sample[""attributes""][""category""]\n        alien\n        >>> sample[""attributes""][""label""]\n        0\n        >>> sample[""data""].vertices.shape\n        torch.Size([252, 3])\n        >>> sample[""data""].faces.shape\n        torch.Size([500, 3])\n\n    """"""\n    _VALID_CATEGORIES = [\n        ""alien"",\n        ""ants"",\n        ""armadillo"",\n        ""bird1"",\n        ""bird2"",\n        ""camel"",\n        ""cat"",\n        ""centaur"",\n        ""dinosaur"",\n        ""dino_ske"",\n        ""dog1"",\n        ""dog2"",\n        ""flamingo"",\n        ""glasses"",\n        ""gorilla"",\n        ""hand"",\n        ""horse"",\n        ""lamp"",\n        ""laptop"",\n        ""man"",\n        ""myScissor"",\n        ""octopus"",\n        ""pliers"",\n        ""rabbit"",\n        ""santa"",\n        ""shark"",\n        ""snake"",\n        ""spiders"",\n        ""two_balls"",\n        ""woman"",\n    ]\n\n    def initialize(\n        self,\n        root: str,\n        categories: Iterable = None,\n        train: bool = True,\n    ):\n\n        if not categories:\n            categories = SHREC16._VALID_CATEGORIES\n        for category in categories:\n            if category not in SHREC16._VALID_CATEGORIES:\n                raise ValueError(\n                    f""Specified category {category} is not valid. ""\n                    f""Valid categories are {SHREC16._VALID_CATEGORIES}""\n                )\n\n        self.root = root\n        self.categories_to_load = categories\n        self.train = train\n        self.num_samples = 0\n        self.paths = []\n        self.category_names = []\n        self.labels = []\n        for i, cl in enumerate(self.categories_to_load):\n            clsdir = os.path.join(root, cl, ""train"" if self.train else ""test"")\n            cur = glob.glob(clsdir + ""/*.obj"")\n\n            self.paths = self.paths + cur\n            self.category_names += [cl] * len(cur)\n            self.labels += [i] * len(cur)\n            self.num_samples += len(cur)\n            if len(cur) == 0:\n                raise RuntimeWarning(\n                    ""No .obj files could be read "" f""for category \'{cl}\'. Skipping...""\n                )\n\n    def __len__(self):\n        """"""Returns the length of the dataset. """"""\n        return self.num_samples\n\n    def _get_data(self, idx):\n        obj_location = self.paths[idx]\n        mesh = TriangleMesh.from_obj(obj_location)\n        return mesh\n\n    def _get_attributes(self, idx):\n        attributes = {\n            ""path"": self.paths[idx],\n            ""category"": self.category_names[idx],\n            ""label"": self.labels[idx],\n        }\n        return attributes\n'"
kaolin/datasets/usdfile.py,9,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging as log\nimport torch\nfrom torch.utils import data\nfrom pathlib import Path\nfrom pxr import Usd, UsdGeom, UsdLux, Sdf, Gf, Vt\n\nfrom kaolin import helpers\n\n\ndef get_mesh_attributes(usd_mesh):\n    face_count = torch.tensor(usd_mesh.GetFaceVertexCountsAttr().Get())[0]\n    vertices = torch.tensor(usd_mesh.GetPointsAttr().Get())\n    faces = torch.tensor(usd_mesh.GetFaceVertexIndicesAttr().Get())\n    faces = faces.view(-1, face_count)\n    return {\'vertices\': vertices, \'faces\': faces}\n\n\nclass USDMeshes(data.Dataset):\n    """"""Import mesh objects from a USD file.\n\n        Args:\n            usd_filepath (str): Path to usd file (*.usd, *.usda)\n\n        Returns:\n            dict: {\n                \'attributes\': {\'name\': str}\n                \'data\': {\'vertices\': torch.Tensor, \'faces\': torch.Tensor}\n            }\n\n    Example:\n            >>> usd_dataset = USDMeshes(usd_filepath=\'Kitchen_set.usd\',\n            >>>                         cache_dir=\'./datasets/USDMeshes/\')\n            >>> obj = next(iter(usd_dataset))\n            >>> obj[\'data\'][\'vertices\'].shape\n            torch.Size([114, 3])\n            >>> obj[\'data\'][\'faces\'].shape\n            torch.Size([448, 3])\n    """"""\n    def __init__(self, usd_filepath: str, cache_dir: str = \'../data/USDMeshes\'):\n        usd_filepath = Path(usd_filepath)\n        assert usd_filepath.suffix in [\'.usd\', \'.usda\']\n        assert usd_filepath.exists(), f\'USD file at {usd_filepath} was not found.\'\n\n        self.cache = helpers.Cache(get_mesh_attributes, cache_dir,\n                                   cache_key=helpers._get_hash(usd_filepath))\n        self.names = self.cache.cached_ids\n\n        stage = Usd.Stage.Open(str(usd_filepath))\n        mesh_prims = [x for x in stage.Traverse() if UsdGeom.Mesh(x)]\n        uncached_mesh_prims = filter(lambda x: x.GetName() not in self.names, mesh_prims)\n        for mesh_prim in uncached_mesh_prims:\n            name = mesh_prim.GetName()\n            mesh = UsdGeom.Mesh(mesh_prim)\n            face_counts = torch.tensor(mesh.GetFaceVertexCountsAttr().Get())\n            if not torch.allclose(face_counts, face_counts[0]):\n                log.warn(f\'Skipping mesh {name}, not all faces have the same \'\n                             \'number of vertices.\')\n            else:\n                self.cache(name, usd_mesh=mesh)\n\n    def __len__(self):\n        return len(self.names)\n\n    def __getitem__(self, index):\n        attributes = {\'name\': self.names[index]}\n        data = self.cache(self.names[index])\n        return {\'attributes\': attributes, \'data\': data}\n'"
kaolin/engine/__init__.py,0,b'from .engine import Engine\nfrom .classification import ClassificationEngine\n'
kaolin/engine/classification.py,13,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Dict, Optional, Union\n\nimport torch\n\nfrom .engine import Engine\n\n\nclass ClassificationEngine(Engine):\n    r""""""An ``Engine`` that runs the training/validation/inference process for\n    a classification task.\n\n    Example:\n        >>> engine = ClassificationEngine(model, train_loader, val_loader)\n\n    """"""\n\n    def __init__(self, model: torch.nn.Module,\n                 train_loader: torch.utils.data.DataLoader,\n                 val_loader: torch.utils.data.DataLoader,\n                 optimizer: Optional[torch.optim.Optimizer] = None,\n                 hyperparams: Optional[Dict] = {},\n                 engineparams: Optional[Dict] = {},\n                 criterion: Optional[torch.nn.Module] = None,\n                 device: Optional[Union[torch.device, str]] = None\n                 ):\n        # Model to be trained.\n        self.model = model\n        # Train dataloader.\n        self.train_loader = train_loader\n        # Validataion dataloader.\n        self.val_loader = val_loader\n        # Hyperaparameters (learning rate, momentum, etc.).\n        self.hyperparams = self.initialize_hyperparams(hyperparams)\n        # Engine parameters (number of epochs to train for, etc.)\n        self.engineparams = self.initialize_engineparams(engineparams)\n        # Optimizer.\n        if optimizer is None:\n            self.optimizer = self.initialize_optimizer()\n        else:\n            self.optimizer = optimizer\n        # Loss function to use\n        if criterion is None:\n            self.criterion = torch.nn.CrossEntropyLoss()\n        else:\n            self.criterion = criterion\n        # Device\n        if device is None:\n            self.device = \'cpu\'\n        else:\n            self.device = device\n\n        # Cast model to device.\n        self.model = self.model.to(self.device)\n\n        """"""\n        Engine stats.\n        """"""\n\n        self.current_epoch = 0\n        # Train loss/accuracy for the current epoch.\n        self.train_loss_cur_epoch = 0.\n        self.train_accuracy_cur_epoch = 0.\n        # Validation loss/accuracy for the current epoch.\n        self.val_loss_cur_epoch = 0.\n        self.val_accuracy_cur_epoch = 0.\n        # Number of minibatches thus far, in the current epoch.\n        # This variable is reused across train/val phases.\n        self.batch_idx = 0\n        # Train loss/accuracy history.\n        self.train_loss = []\n        self.train_accuracy = []\n        # Validation loss/accuracy history.\n        self.val_loss = []\n        self.val_accuracy = []\n\n    def initialize_hyperparams(self, hyperparams: Dict):\n        r""""""Initializes hyperparameters for the training process. Uses defaults\n        wherever user specifications are unavailable.\n\n        Args:\n            hyperparams (dict): User-specified hyperparameters (\'lr\', \'beta1\',\n                \'beta2\' for Adam).\n\n        """"""\n        paramdict = {}\n\n        if \'lr\' in hyperparams:\n            paramdict[\'lr\'] = hyperparams[\'lr\']\n        else:\n            paramdict[\'lr\'] = 1e-3\n        if \'beta1\' in hyperparams:\n            paramdict[\'beta1\'] = hyperparams[\'beta1\']\n        else:\n            paramdict[\'beta1\'] = 0.9\n        if \'beta2\' in hyperparams:\n            paramdict[\'beta2\'] = hyperparams[\'beta2\']\n        else:\n            paramdict[\'beta2\'] = 0.999\n\n        return paramdict\n\n    def initialize_engineparams(self, engineparams: Dict):\n        r""""""Initializes engine parameters. Uses defaults wherever user\n        specified values are unavilable.\n\n        Args:\n            engineparams (dict): User-specified engine parameters (\'epochs\',\n                \'validate-every\', \'print-every\', \'save\', \'savedir\').\n                \'epochs\': number of epochs to train for.\n                \'validate-every\': number of epochs after which validation\n                    occurs.\n                \'print-every\': number of iterations (batches) after which to\n                    keep printing progress to stdout.\n                \'save\': whether or not to save trained models.\n                \'savedir\': directory to save trained models to.\n\n        """"""\n        paramdict = {}\n\n        if \'epochs\' in engineparams:\n            paramdict[\'epochs\'] = engineparams[\'epochs\']\n        else:\n            paramdict[\'epochs\'] = 10\n        if \'validate-every\' in engineparams:\n            paramdict[\'validate-every\'] =\\\n                engineparams[\'validate-every\']\n        else:\n            paramdict[\'validate-every\'] = 2\n        if \'print-every\' in engineparams:\n            paramdict[\'print-every\'] = engineparams[\'print-every\']\n        else:\n            paramdict[\'print-every\'] = 10\n        if \'save\' in engineparams:\n            paramdict[\'save\'] = engineparams[\'save\']\n        else:\n            paramdict[\'save\'] = False\n        # Currently, we do not set a default \'savedir\'.\n        if \'savedir\' in engineparams:\n            paramdict[\'savedir\'] = engineparams[\'savedir\']\n\n        return paramdict\n\n    def initialize_optimizer(self):\n        r""""""Initializes the optimizer. By default, uses the Adam optimizer with\n        the specified hyperparameter for learning-rates, beta1, and beta2.\n\n        """"""\n        return torch.optim.Adam(self.model.parameters(),\n                                lr=self.hyperparams[\'lr\'],\n                                betas=(self.hyperparams[\'beta1\'],\n                                       self.hyperparams[\'beta2\']))\n\n    def train_batch(self, batch):\n        r""""""Train for one mini-batch.\n\n        Args:\n            batch (tuple): One mini-batch of training data.\n\n        """"""\n        data, attr = batch\n        data = data.to(self.device)\n        label = attr[\'category\'].to(self.device)\n        pred = self.model(data)\n        loss = self.criterion(pred, label.view(-1))\n        self.train_loss_cur_epoch += loss.item()\n        predlabel = torch.argmax(pred, dim=1)\n        accuracy = torch.mean((predlabel == label.view(-1)).float())\n        self.train_accuracy_cur_epoch += accuracy.detach().cpu().item()\n        self.batch_idx += 1\n        return loss, accuracy\n\n    def validate_batch(self, batch):\n        r""""""Validate for one mini-batch.\n\n        Args:\n            batch (tuple): One mini-batch of validateion data.\n\n        """"""\n        data, attr = batch\n        data = data.to(self.device)\n        label = attr[\'category\'].to(self.device)\n        pred = self.model(data)\n        loss = self.criterion(pred, label.view(-1))\n        self.val_loss_cur_epoch += loss.item()\n        predlabel = torch.argmax(pred, dim=1)\n        accuracy = torch.mean((predlabel == label.view(-1)).float())\n        self.val_accuracy_cur_epoch += accuracy.detach().cpu().item()\n        self.batch_idx += 1\n        return loss, accuracy\n\n    def optimize_batch(self, loss):\n        r""""""Optimize model parameters for one mini-batch.\n\n        Args:\n            loss: training loss computed over the mini-batch.\n        """"""\n        loss.backward()\n        self.optimizer.step()\n        self.optimizer.zero_grad()\n\n    def fit(self):\n        r""""""Train the model using the specified engine parameters.\n        """"""\n\n        for epoch in range(self.engineparams[\'epochs\']):\n\n            # Train phase\n            \n            self.train_loss_cur_epoch = 0.\n            self.train_accuracy_cur_epoch = 0.\n            self.batch_idx = 0\n\n            self.model.train()\n\n            for idx, batch in enumerate(self.train_loader):\n                loss, accuracy = self.train_batch(batch)\n                self.optimize_batch(loss)\n                self.print_train_stats()\n            self.train_accuracy.append(self.train_accuracy_cur_epoch\\\n                / self.batch_idx)\n\n            # Validation phase\n\n            self.val_loss_cur_epoch = 0.\n            self.val_accuracy_cur_epoch = 0.\n            self.batch_idx = 0\n\n            self.model.eval()\n\n            with torch.no_grad():\n                for idx, batch in enumerate(self.val_loader):\n                    loss, accuracy = self.validate_batch(batch)\n                    self.print_validation_stats()\n                self.val_accuracy.append(self.val_accuracy_cur_epoch\\\n                    / self.batch_idx)\n\n            self.current_epoch += 1\n\n    def print_train_stats(self):\n        r""""""Print current stats.\n        """"""\n        print(\'Epoch: {0}, Train loss: {1}, Train accuracy: {2}\'.format(\n            self.current_epoch, self.train_loss_cur_epoch / self.batch_idx,\n            self.train_accuracy_cur_epoch / self.batch_idx))\n\n    def print_validation_stats(self):\n        r""""""Print current stats.\n        """"""\n        print(\'Epoch: {0}, Val loss: {1}, Val accuracy: {2}\'.format(\n            self.current_epoch, self.val_loss_cur_epoch / self.batch_idx,\n            self.val_accuracy_cur_epoch / self.batch_idx))\n'"
kaolin/engine/engine.py,0,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import abstractmethod\n\nimport torch\n\n\nclass Engine(object):\n    r""""""An ``Engine`` that runs the training/validation/inference process\n    with the parameters specified.\n\n    """"""\n\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def fit(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def predict(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def train_batch(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def validate_batch(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def optimize(self):\n        raise NotImplementedError\n'"
kaolin/graphics/DIBRenderer.py,0,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n# DIB-R\n#\n# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom .dib_renderer.renderer import Renderer as DIBRenderer\n'"
kaolin/graphics/DifferentiableRenderer.py,1,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import abstractmethod\n\nimport torch\n\n\nclass DifferentiableRenderer(torch.nn.Module):\n    r"""""" Base class for differentiable renderers. All major components\n    of the graphics processing pipeline have been instantiated as\n    abstract methods.\n\n    A differentiable renderer takes in vertex geometry, faces (usually\n    triangles), and optionally texture (and any other information\n    deemed relevant), and implements the following main steps.\n\n        * Lighting (ambient and optionally, directional lighting, i.e.,\n          diffuse/Lambertian reflection, and optionally specular reflection)\n        * (Vertex) Shading (Gouraud/Phong/etc.)\n        * Geometric transformation (usually, this is needed to view the\n          object to be rendered from a specific viewing angle, etc.)\n          (the geometric transformations are usually applied AFTER lighting,\n          as it is easier to perform lighting in the object frame, rather\n          than at the scene level)\n        * Projection (perspective/orthographic/weak-orthographic(affine))\n          from 3D (camera) coordinates to 2D (image) coordinates.\n        * Rasterization to convert the primitives (usually triangles) to\n          pixels, and determine texture (when available).\n\n    """"""\n\n    @abstractmethod\n    def __init__(self):\n        super(DifferentiableRenderer, self).__init__()\n\n    @abstractmethod\n    def forward(self, vertices, faces, textures=None):\n        # Generally, this method will call the other\n        # (already implemented) methods.\n        # self.lighting()\n        # self.shading()\n        # self.transform_to_camera_frame()\n        # self.project_to_image()\n        # self.rasterize()\n        raise NotImplementedError\n\n    @abstractmethod\n    def lighting(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def shading(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def transform_to_camera_frame(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def project_to_image(self):\n        raise NotImplementedError\n\n    @abstractmethod\n    def rasterize(self):\n        raise NotImplementedError\n'"
kaolin/graphics/Lighting.py,54,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# \n# \n# Soft Rasterizer (SoftRas)\n# \n# Copyright (c) 2017 Hiroharu Kato\n# Copyright (c) 2018 Nikos Kolotouros\n# Copyright (c) 2019 Shichen Liu\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport torch\nimport torch.nn.functional as F\n\n\ndef compute_ambient_light(\n        face_vertices: torch.Tensor,\n        textures: torch.Tensor,\n        ambient_intensity: float = 1.,\n        ambient_color: torch.Tensor = None):\n    r""""""Computes ambient lighting to a mesh, given faces and face textures.\n\n    Args:\n        face_vertices (torch.Tensor): A tensor containing a list of (per-face)\n            vertices of the mesh (shape: `B` :math:`\\times` `num_faces`\n            :math:`\\times 9`). Here, :math:`B` is the batchsize, `num_faces`\n            is the number of faces in the mesh, and since each face is assumed\n            to be a triangle, it has 3 vertices, and hence 9 coordinates in\n            total.\n        textures (torch.Tensor): TODO: Add docstring\n        ambient_intensity (float): Intensity of ambient light (in the range\n            :math:`\\left[0, 1\\right]`). If the values provided are outside\n            this range, we clip them so that they fall in range.\n        ambient_color (torch.Tensor): Color of the ambient light (R, G, B)\n            (shape: :math:`3`)\n\n    Returns:\n        light (torch.Tensor): A light tensor, which can be elementwise\n            multiplied with the textures, to obtain the mesh with lighting\n            applied (shape: `B` :math:`\\times` `num_faces` :math:`\\times\n            1 \\times 1 \\times 1 \\times 3`)\n\n    """"""\n\n    if not torch.is_tensor(face_vertices):\n        raise TypeError(\'Expected input face_vertices to be of type \'\n                        \'torch.Tensor. Got {0} instead.\'.format(\n                            type(face_vertices)))\n    if not torch.is_tensor(textures):\n        raise TypeError(\'Expected input textures to be of type \'\n                        \'torch.Tensor. Got {0} instead.\'.format(\n                            type(textures)))\n    if not isinstance(ambient_intensity, float) and not isinstance(\n            ambient_intensity, int):\n        raise TypeError(\'Expected input ambient_intensity to be of \'\n                        \'type float. Got {0} instead.\'.format(\n                            type(ambient_intensity)))\n    if ambient_color is None:\n        ambient_color = torch.ones(3, dtype=face_vertices.dtype,\n                                   device=face_vertices.device)\n    if not torch.is_tensor(ambient_color):\n        raise TypeError(\'Expected input ambient_color to be of type \'\n                        \'torch.Tensor. Got {0} instead.\'.format(\n                            type(ambient_color)))\n\n    # if face_vertices.dim() != 3 or face_vertices.shape[-1] != 9:\n    #     raise ValueError(\'Input face_vertices must have 3 dimensions \'\n    #                      \'and be of shape (..., ..., 9). Got {0} dimensions \'\n    #                      \'and shape {1} instead.\'.format(face_vertices.dim(),\n    #                                                      face_vertices.shape))\n    # TODO: check texture dims\n    if ambient_color.dim() != 1 or ambient_color.shape != (3,):\n        raise ValueError(\'Input ambient_color must have 1 dimension \'\n                         \'and be of shape 3. Got {0} dimensions and shape {1} \'\n                         \'instead.\'.format(ambient_color.dim(), ambient_color.shape))\n\n    # Clip ambient_intensity to be in the range [0, 1]\n    if ambient_intensity < 0:\n        ambient_intensity = 0.\n    if ambient_intensity > 1:\n        ambient_intensity = 1.\n\n    batchsize = face_vertices.shape[0]\n    num_faces = face_vertices.shape[1]\n    device = face_vertices.device\n\n    if ambient_color.dim() == 1:\n        ambient_color = ambient_color[None, :].to(device)\n\n    light = torch.zeros(batchsize, num_faces, 3).to(device)\n\n    # If the intensity of the ambient light is 0, do not\n    # bother computing lighting.\n    if ambient_intensity == 0:\n        return light\n\n    # Ambient lighting is constant everywhere, and is given as\n    # I = I_a * K_a\n    # where,\n    # I: Intensity at a vertex\n    # I_a: Intensity of the ambient light\n    # K_a: Ambient reflectance of the vertex (3 channels, R, G, B)\n    light += ambient_intensity * ambient_color[:, None, :]\n\n    return light[:, :, None, :]\n\n\ndef apply_ambient_light(\n        face_vertices: torch.Tensor,\n        textures: torch.Tensor,\n        ambient_intensity: float = 1.,\n        ambient_color: torch.Tensor = torch.ones(3)):\n    r""""""Computes and applies ambient lighting to a mesh, given faces and\n    face textures.\n\n    Args:\n        face_vertices (torch.Tensor): A tensor containing a list of (per-face)\n            vertices of the mesh (shape: `B` :math:`\\times` `num_faces`\n            :math:`\\times 9`). Here, :math:`B` is the batchsize, `num_faces`\n            is the number of faces in the mesh, and since each face is assumed\n            to be a triangle, it has 3 vertices, and hence 9 coordinates in\n            total.\n        textures (torch.Tensor): TODO: Add docstring\n        ambient_intensity (float): Intensity of ambient light (in the range\n            :math:`\\left[0, 1\\right]`). If the values provided are outside\n            this range, we clip them so that they fall in range.\n        ambient_color (torch.Tensor): Color of the ambient light (R, G, B)\n            (shape: :math:`3`)\n\n    Returns:\n        textures (torch.Tensor): Updated textures, with ambient lighting\n            applied (shape: same as input `textures`) #TODO: Update docstring\n\n    """"""\n\n    light = compute_ambient_light(face_vertices, textures, ambient_intensity,\n                                  ambient_color)\n    return textures * light\n\n\ndef compute_directional_light(\n        face_vertices: torch.Tensor,\n        textures: torch.Tensor,\n        directional_intensity: float = 1.,\n        directional_color: torch.Tensor = None,\n        direction: torch.Tensor = None):\n    r""""""Computes directional lighting to a mesh, given faces and face textures.\n\n    Args:\n        face_vertices (torch.Tensor): A tensor containing a list of (per-face)\n            vertices of the mesh (shape: `B` :math:`\\times` `num_faces`\n            :math:`\\times 9`). Here, :math:`B` is the batchsize, `num_faces`\n            is the number of faces in the mesh, and since each face is assumed\n            to be a triangle, it has 3 vertices, and hence 9 coordinates in\n            total.\n        textures (torch.Tensor): TODO: Add docstring\n        directional_intensity (float): Intensity of directional light (in the\n            range :math:`\\left[0, 1\\right]`). If the values provided are\n            outside this range, we clip them so that they fall in range.\n        directional_color (torch.Tensor): Color of the directional light\n            (R, G, B) (shape: :math:`3`).\n        direction (torch.Tensor): Direction of light from the light source.\n            (default: :math:`\\left( 0, 1, 0 \\right)^T`)\n\n    Returns:\n        light (torch.Tensor): A light tensor, which can be elementwise\n            multiplied with the textures, to obtain the mesh with lighting\n            applied (shape: `B` :math:`\\times` `num_faces` :math:`\\times\n            1 \\times 1 \\times 1 \\times 3`)\n\n    """"""\n\n    if not torch.is_tensor(face_vertices):\n        raise TypeError(\'Expected input face_vertices to be of type \'\n                        \'torch.Tensor. Got {0} instead.\'.format(\n                            type(face_vertices)))\n    if not torch.is_tensor(textures):\n        raise TypeError(\'Expected input textures to be of type \'\n                        \'torch.Tensor. Got {0} instead.\'.format(\n                            type(textures)))\n    if not isinstance(directional_intensity, float) and not isinstance(\n            directional_intensity, int):\n        raise TypeError(\'Expected input directional_intensity to be of \'\n                        \'type float. Got {0} instead.\'.format(\n                            type(directional_intensity)))\n    if directional_color is None:\n        directional_color = torch.ones(3, dtype=face_vertices.dtype,\n                                       device=face_vertices.device)\n    if not torch.is_tensor(directional_color):\n        raise TypeError(\'Expected input directional_color to be of type \'\n                        \'torch.Tensor. Got {0} instead.\'.format(\n                            type(directional_color)))\n    if direction is None:\n        direction = torch.tensor([0., 1., 0.], dtype=face_vertices.dtype,\n                                 device=face_vertices.device)\n    if not torch.is_tensor(direction):\n        raise TypeError(\'Expected input direction to be of type \'\n                        \'torch.Tensor. Got {0} instead.\'.format(type(direction)))\n\n    # if face_vertices.dim() != 3 or face_vertices.shape[-1] != 9:\n    #     raise ValueError(\'Input face_vertices must have 3 dimensions \'\n    #                      \'and be of shape (..., ..., 9). Got {0} dimensions \'\n    #                      \'and shape {1} instead.\'.format(face_vertices.dim(),\n    #                                                      face_vertices.shape))\n    # TODO: check texture dims\n    if directional_color.dim() != 1 or directional_color.shape != (3,):\n        raise ValueError(\'Input directional_color must have 1 dimension \'\n                         \'and be of shape 3. Got {0} dimensions and shape {1} \'\n                         \'instead.\'.format(directional_color.dim(),\n                                           directional_color.shape))\n    if direction.dim() != 1 or direction.shape != (3,):\n        raise ValueError(\'Input direction must have 1 dimension and be \'\n                         \'of shape 3. Got {0} dimensions and shape {1} \'\n                         \'instead.\'.format(direction.dim(), direction.shape))\n\n    batchsize = face_vertices.shape[0]\n    num_faces = face_vertices.shape[1]\n    device = face_vertices.device\n\n    if directional_color.dim() == 1:\n        directional_color = directional_color[None, :].to(device)\n    if direction.dim() == 1:\n        direction = direction[None, :].to(device)\n\n    # Clip directional intensity to be in the range [0, 1]\n    if directional_intensity < 0:\n        directional_intensity = 0.\n    if directional_intensity > 1:\n        directional_intensity = 1.\n\n    # Initialize light to zeros\n    light = torch.zeros(batchsize, num_faces, 3).to(device)\n\n    # If the intensity of the directional light is 0, do not\n    # bother computing lighting.\n    if directional_intensity == 0:\n        return light\n\n    # Compute face normals.\n    v10 = face_vertices[:, :, 0] - face_vertices[:, :, 1]\n    v12 = face_vertices[:, :, 2] - face_vertices[:, :, 1]\n    normals = F.normalize(torch.cross(v12, v10), p=2, dim=2, eps=1e-6)\n    # Reshape, to get back the batchsize dimension.\n    normals = normals.reshape(batchsize, num_faces, 3)\n\n    # Get direction to 3 dimensions, if not already there.\n    if direction.dim() == 2:\n        direction = direction[:, None, :]\n\n    cos = F.relu(torch.sum(normals * direction, dim=2))\n\n    light += directional_intensity * (directional_color[:, None, :]\n                                      * cos[:, :, None])\n\n    return light[:, :, None, :]\n\n\ndef apply_directional_light(\n        face_vertices: torch.Tensor,\n        textures: torch.Tensor,\n        directional_intensity: float = 1.,\n        directional_color: torch.Tensor = torch.ones(3),\n        direction: torch.Tensor = torch.FloatTensor([0, 1, 0])):\n    r""""""Computes and applies directional lighting to a mesh, given faces\n    and face textures.\n\n    Args:\n        face_vertices (torch.Tensor): A tensor containing a list of (per-face)\n            vertices of the mesh (shape: `B` :math:`\\times` `num_faces`\n            :math:`\\times 9`). Here, :math:`B` is the batchsize, `num_faces`\n            is the number of faces in the mesh, and since each face is assumed\n            to be a triangle, it has 3 vertices, and hence 9 coordinates in\n            total.\n        textures (torch.Tensor): TODO: Add docstring\n        directional_intensity (float): Intensity of directional light (in the\n            range :math:`\\left[0, 1\\right]`). If the values provided are\n            outside this range, we clip them so that they fall in range.\n        directional_color (torch.Tensor): Color of the directional light\n            (R, G, B) (shape: :math:`3`).\n        direction (torch.Tensor): Direction of light from the light source.\n            (default: :math:`\\left( 0, 1, 0 \\right)^T`)\n\n    Returns:\n        light (torch.Tensor): A light tensor, which can be elementwise\n            multiplied with the textures, to obtain the mesh with lighting\n            applied (shape: `B` :math:`\\times` `num_faces` :math:`\\times\n            1 \\times 1 \\times 1 \\times 3`)\n\n    """"""\n\n    light = compute_directional_light(face_vertices, textures,\n                                      directional_intensity, directional_color, direction)\n    return textures * light\n'"
kaolin/graphics/NeuralMeshRenderer.py,25,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n# A PyTorch implementation of Neural 3D Mesh Renderer\n#\n# Copyright (c) 2017 Hiroharu Kato\n# Copyright (c) 2018 Nikos Kolotouros\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import division\n\nimport math\nimport torch\nimport torch.nn as nn\nimport numpy\n\nfrom .nmr import util\nfrom .nmr import rasterizer\n\n\nclass NeuralMeshRenderer(nn.Module):\n    def __init__(self,\n                 image_size=256,\n                 anti_aliasing=True,\n                 background_color=None,\n                 fill_back=True,\n                 camera_mode=\'projection\',\n                 K=None,\n                 R=None,\n                 t=None,\n                 dist_coeffs=None,\n                 orig_size=1024,\n                 perspective=True,\n                 viewing_angle=30,\n                 camera_direction=None,\n                 near=0.1,\n                 far=100,\n                 light_intensity_ambient=0.5,\n                 light_intensity_directional=0.5,\n                 light_color_ambient=None,\n                 light_color_directional=None,\n                 light_direction=None):\n        """"""Initialize the NeuralMeshRenderer.\n\n        NOTE: NeuralMeshRenderer works only in GPU mode!\n\n        Args:\n            image_size (int): Size of the (square) image to be rendered.\n            anti_aliasing (bool): Whether or not to perform anti-aliasing\n                (default: True)\n            background_color (torch.Tensor): Background color of rendered image\n                (size: math:`3`, default: :math:`\\left[0, 0, 0\\right]`)\n            fill_back (bool): Whether or not to fill color to the back\n                side of each triangle as well (sometimes helps, when\n                the triangles in the mesh are not properly oriented.)\n                (default: True)\n            camera_mode (str): Choose from among `projection`, `look`, and\n                `look_at`. In the `projection` mode, the camera is at the\n                origin, and its optical axis is aligned with the positive\n                Z-axis. In the `look_at` mode, the object (not the camera)\n                is placed at the origin. The camera ""looks at"" the object\n                from a predefined ""eye"" location, which is computed from\n                the `viewing_angle` (another input to this function). In\n                the `look` mode, only the direction in which the camera\n                needs to look is specified. It does not necessarily look\n                towards the origin, as it allows the specification of a\n                custom ""upwards"" direction (default: \'projection\').\n            K (torch.Tensor): Camera intrinsics matrix. Note that, unlike\n                standard notation, K here is a 4 x 4 matrix (with the last\n                row and last column drawn from the 4 x 4 identity matrix)\n                (default: None)\n            R (torch.Tensor): Rotation matrix (again, 4 x 4, as opposed\n                to the usual 3 x 3 convention).\n            t (torch.Tensor): Translation vector (3 x 1). Note that the\n                (negative of the) tranlation is applied before rotation,\n                to be consistent with the projective geometry convention\n                of transforming a 3D point X by doing\n                torch.matmul(R.transpose(), X - t) (default: None)\n            viewing_angle (float): Angle at which the object is to be viewed\n                (assumed to be in degrees!) (default: 30.)\n            camera_direction (float): Direction in which the camera is facing\n                (used only in the `look` and `look_at` modes) (default:\n                :math:`[0, 0, 1]`)\n            near (float): Near clipping plane (for depth values) (default: 0.1)\n            far (float): Far clipping plane (for depth values) (default: 100)\n            light_intensity_ambient (float): Intensity of ambient light (in the\n                range :math:`\\left[ 0, 1 \\right]`) (default: 0.5).\n            light_intensity_directional (float): Intensity of directional light\n                (in the range :math:`\\left[ 0, 1 \\right]`) (default: 0.5).\n            light_color_ambient (torch.Tensor): Color of ambient light\n                (default: :math:`\\left[ 1, 1, 1 \\right]`)\n            light_color_directional (torch.Tensor): Color of directional light\n                (default: :math:`\\left[ 1, 1, 1 \\right]`)\n            light_direction (torch.Tensor): Light direction, for directional\n                light (default: :math:`\\left[ 0, 1, 0 \\right]`)\n        """"""\n        super(NeuralMeshRenderer, self).__init__()\n\n        # default arguments\n        if background_color is None:\n            background_color = [0, 0, 0]\n\n        if camera_direction is None:\n            camera_direction = [0, 0, 1]\n\n        if light_color_ambient is None:\n            light_color_ambient = [1, 1, 1]\n\n        if light_color_directional is None:\n            light_color_directional = [1, 1, 1]\n\n        if light_direction is None:\n            light_direction = [0, 1, 0]\n\n        # rendering\n        self.image_size = image_size\n        self.anti_aliasing = anti_aliasing\n        self.background_color = background_color\n        self.fill_back = fill_back\n\n        # camera\n        self.camera_mode = camera_mode\n        if self.camera_mode == \'projection\':\n            self.K = K\n            self.R = R\n            self.t = t\n            self.dist_coeffs = dist_coeffs\n\n            if isinstance(self.K, numpy.ndarray):\n                self.K = torch.cuda.FloatTensor(self.K)\n\n            else:\n                self.K = self.K.cuda()\n\n            if isinstance(self.R, numpy.ndarray):\n                self.R = torch.cuda.FloatTensor(self.R)\n\n            else:\n                self.R = self.R.cuda()\n\n            if isinstance(self.t, numpy.ndarray):\n                self.t = torch.cuda.FloatTensor(self.t)\n\n            else:\n                self.t = self.t.cuda()\n\n            if dist_coeffs is None:\n                self.dist_coeffs = torch.zeros(1, 5, device=\'cuda\')\n\n            else:\n                self.dist_coeffs = self.dist_coeffs.cuda()\n\n            self.orig_size = orig_size\n        elif self.camera_mode in [\'look\', \'look_at\']:\n            self.perspective = perspective\n            self.viewing_angle = viewing_angle\n            self.eye = [\n                0, 0, -(1. / math.tan(math.radians(self.viewing_angle)) + 1)]\n            self.camera_direction = camera_direction\n        else:\n            raise ValueError(\n                \'Camera mode has to be one of projection, look or look_at\')\n\n        self.near = near\n        self.far = far\n\n        # light\n        self.light_intensity_ambient = light_intensity_ambient\n        self.light_intensity_directional = light_intensity_directional\n        self.light_color_ambient = light_color_ambient\n        self.light_color_directional = light_color_directional\n        self.light_direction = light_direction\n\n        # rasterization\n        self.rasterizer_eps = 1e-3\n\n    def forward(self, vertices, faces, textures=None, mode=None, K=None, R=None, t=None, dist_coeffs=None, orig_size=None):\n        \'\'\'\n        Implementation of forward rendering method\n        The old API is preserved for back-compatibility with the Chainer implementation\n        \'\'\'\n\n        if mode is None:\n            return self.render(vertices, faces, textures, K, R, t, dist_coeffs, orig_size)\n        elif mode is \'rgb\':\n            return self.render_rgb(vertices, faces, textures, K, R, t, dist_coeffs, orig_size)\n        elif mode == \'silhouettes\':\n            return self.render_silhouettes(vertices, faces, K, R, t, dist_coeffs, orig_size)\n        elif mode == \'depth\':\n            return self.render_depth(vertices, faces, K, R, t, dist_coeffs, orig_size)\n        else:\n            raise ValueError(\n                ""mode should be one of None, \'silhouettes\' or \'depth\'"")\n\n    def render_silhouettes(self, vertices, faces, K=None, R=None, t=None, dist_coeffs=None, orig_size=None):\n\n        # fill back\n        if self.fill_back:\n            faces = torch.cat(\n                (faces, faces[:, :, list(reversed(range(faces.shape[-1])))]), dim=1)\n\n        # viewpoint transformation\n        if self.camera_mode == \'look_at\':\n            vertices = util.look_at(vertices, self.eye)\n            # perspective transformation\n            if self.perspective:\n                vertices = util.perspective(vertices, angle=self.viewing_angle)\n        elif self.camera_mode == \'look\':\n            vertices = util.look(vertices, self.eye, self.camera_direction)\n            # perspective transformation\n            if self.perspective:\n                vertices = util.perspective(vertices, angle=self.viewing_angle)\n        elif self.camera_mode == \'projection\':\n            if K is None:\n                K = self.K\n            if R is None:\n                R = self.R\n            if t is None:\n                t = self.t\n            if dist_coeffs is None:\n                dist_coeffs = self.dist_coeffs\n            if orig_size is None:\n                orig_size = self.orig_size\n            vertices = util.projection(\n                vertices, K, R, t, dist_coeffs, orig_size)\n\n        # rasterization\n        faces = util.vertices_to_faces(vertices, faces)\n        images = rasterizer.rasterize_silhouettes(\n            faces, self.image_size, self.anti_aliasing)\n        return images\n\n    def render_depth(self, vertices, faces, K=None, R=None, t=None, dist_coeffs=None, orig_size=None):\n\n        # fill back\n        if self.fill_back:\n            faces = torch.cat(\n                (faces, faces[:, :, list(reversed(range(faces.shape[-1])))]), dim=1).detach()\n\n        # viewpoint transformation\n        if self.camera_mode == \'look_at\':\n            vertices = util.look_at(vertices, self.eye)\n            # perspective transformation\n            if self.perspective:\n                vertices = util.perspective(vertices, angle=self.viewing_angle)\n        elif self.camera_mode == \'look\':\n            vertices = util.look(vertices, self.eye, self.camera_direction)\n            # perspective transformation\n            if self.perspective:\n                vertices = util.perspective(vertices, angle=self.viewing_angle)\n        elif self.camera_mode == \'projection\':\n            if K is None:\n                K = self.K\n            if R is None:\n                R = self.R\n            if t is None:\n                t = self.t\n            if dist_coeffs is None:\n                dist_coeffs = self.dist_coeffs\n            if orig_size is None:\n                orig_size = self.orig_size\n            vertices = util.projection(\n                vertices, K, R, t, dist_coeffs, orig_size)\n\n        # rasterization\n        faces = util.vertices_to_faces(vertices, faces)\n        images = rasterizer.rasterize_depth(\n            faces, self.image_size, self.anti_aliasing)\n        return images\n\n    def render_rgb(self, vertices, faces, textures, K=None, R=None, t=None, dist_coeffs=None, orig_size=None):\n        # fill back\n        if self.fill_back:\n            faces = torch.cat(\n                (faces, faces[:, :, list(reversed(range(faces.shape[-1])))]), dim=1).detach()\n            textures = torch.cat(\n                (textures, textures.permute((0, 1, 4, 3, 2, 5))), dim=1)\n\n        # lighting\n        faces_lighting = util.vertices_to_faces(vertices, faces)\n        textures = util.lighting(\n            faces_lighting,\n            textures,\n            self.light_intensity_ambient,\n            self.light_intensity_directional,\n            self.light_color_ambient,\n            self.light_color_directional,\n            self.light_direction)\n\n        # viewpoint transformation\n        if self.camera_mode == \'look_at\':\n            vertices = util.look_at(vertices, self.eye)\n            # perspective transformation\n            if self.perspective:\n                vertices = util.perspective(vertices, angle=self.viewing_angle)\n        elif self.camera_mode == \'look\':\n            vertices = util.look(vertices, self.eye, self.camera_direction)\n            # perspective transformation\n            if self.perspective:\n                vertices = util.perspective(vertices, angle=self.viewing_angle)\n        elif self.camera_mode == \'projection\':\n            if K is None:\n                K = self.K\n            if R is None:\n                R = self.R\n            if t is None:\n                t = self.t\n            if dist_coeffs is None:\n                dist_coeffs = self.dist_coeffs\n            if orig_size is None:\n                orig_size = self.orig_size\n            vertices = util.projection(\n                vertices, K, R, t, dist_coeffs, orig_size)\n\n        # rasterization\n        faces = util.vertices_to_faces(vertices, faces)\n        images = rasterizer.rasterize(\n            faces, textures, self.image_size, self.anti_aliasing, self.near, self.far, self.rasterizer_eps,\n            self.background_color)\n        return images\n\n    def render(self, vertices, faces, textures, K=None, R=None, t=None, dist_coeffs=None, orig_size=None):\n        """"""Renders the RGB, depth, and alpha channels.\n\n        Args:\n            vertices (torch.Tensor): Vertices of the mesh (shape: :math:`B\n                \\times V \\times 3`), where :math:`B` is the batchsize,\n                and :math:`V` is the number of vertices in the mesh.\n            faces (torch.Tensor): Faces of the mesh (shape: :math:`B \\times\n                F \\times 3`), where :math:`B` is the batchsize, and :math:`F`\n                is the number of faces in the mesh.\n            textures (torch.Tensor): Mesh texture (shape: :math:`B \\times F\n                \\times 4 \\times 4 \\times 4 \\times 3`)\n            K (torch.Tensor): Camera intrinsics (default: None) (shape:\n                :math:`B \\times 4 \\times 4` or :math:`4 \\times 4`)\n            R (torch.Tensor): Rotation matrix (default: None) (shape:\n                :math:`B \\times 4 \\times 4` or :math:`4 \\times 4`)\n            t (torch.Tensor): Translation vector (default: None)\n                (shape: :math:`B \\times 3` or :math:`3`)\n        """"""\n        # fill back\n        if self.fill_back:\n            faces = torch.cat(\n                (faces, faces[:, :, list(reversed(range(faces.shape[-1])))]), dim=1).detach()\n            textures = torch.cat(\n                (textures, textures.permute((0, 1, 4, 3, 2, 5))), dim=1)\n\n        # lighting\n        faces_lighting = util.vertices_to_faces(vertices, faces)\n        textures = util.lighting(\n            faces_lighting,\n            textures,\n            self.light_intensity_ambient,\n            self.light_intensity_directional,\n            self.light_color_ambient,\n            self.light_color_directional,\n            self.light_direction)\n\n        # viewpoint transformation\n        if self.camera_mode == \'look_at\':\n            vertices = util.look_at(vertices, self.eye)\n            # perspective transformation\n            if self.perspective:\n                vertices = util.perspective(vertices, angle=self.viewing_angle)\n        elif self.camera_mode == \'look\':\n            vertices = util.look(vertices, self.eye, self.camera_direction)\n            # perspective transformation\n            if self.perspective:\n                vertices = util.perspective(vertices, angle=self.viewing_angle)\n        elif self.camera_mode == \'projection\':\n            if K is None:\n                K = self.K\n            if R is None:\n                R = self.R\n            if t is None:\n                t = self.t\n            if dist_coeffs is None:\n                dist_coeffs = self.dist_coeffs\n            if orig_size is None:\n                orig_size = self.orig_size\n            vertices = util.projection(\n                vertices, K, R, t, dist_coeffs, orig_size)\n\n        # rasterization\n        faces = util.vertices_to_faces(vertices, faces)\n        out = rasterizer.rasterize_rgbad(\n            faces, textures, self.image_size, self.anti_aliasing, self.near, self.far, self.rasterizer_eps,\n            self.background_color)\n        return out[\'rgb\'], out[\'depth\'], out[\'alpha\']\n'"
kaolin/graphics/SoftRenderer.py,118,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# \n# \n# Soft Rasterizer (SoftRas)\n# \n# Copyright (c) 2017 Hiroharu Kato\n# Copyright (c) 2018 Nikos Kolotouros\n# Copyright (c) 2019 Shichen Liu\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport math\nfrom typing import Optional, Union\n\nimport torch\n\nfrom .DifferentiableRenderer import DifferentiableRenderer\nfrom .Lighting import compute_ambient_light\nfrom .Lighting import compute_directional_light\nfrom .softras.soft_rasterize import soft_rasterize\n\n\nclass SoftRenderer(DifferentiableRenderer):\n    r""""""A class implementing the \\emph{Soft Renderer}\n        from the following ICCV 2019 paper:\n            Soft Rasterizer: A differentiable renderer for image-based 3D reasoning\n            Shichen Liu, Tianye Li, Weikai Chen, and Hao Li\n            Link: https://arxiv.org/abs/1904.01786\n\n    .. note::\n        If you use this code, please cite the original paper in addition to Kaolin.\n\n        .. code-block::\n\n            @article{liu2019softras,\n              title={Soft Rasterizer: A Differentiable Renderer for Image-based 3D Reasoning},\n              author={Liu, Shichen and Li, Tianye and Chen, Weikai and Li, Hao},\n              journal={The IEEE International Conference on Computer Vision (ICCV)},\n              month = {Oct},\n              year={2019}\n            }\n\n    """"""\n\n    def __init__(\n            self,\n            image_size: int = 256,\n            anti_aliasing: bool = True,\n            bg_color: torch.Tensor = None,\n            fill_back: bool = True,\n            camera_mode: str = \'look_at\',\n            K: torch.Tensor = None,\n            rmat: torch.Tensor = None,\n            tvec: torch.Tensor = None,\n            perspective_distort: bool = True,\n            sigma_val: float = 1e-5,\n            dist_func: str = \'euclidean\',\n            dist_eps: float = 1e-4,\n            gamma_val: float = 1e-4,\n            aggr_func_rgb: str = \'softmax\',\n            aggr_func_alpha: str = \'prod\',\n            texture_type: str = \'surface\',\n            viewing_angle: float = 30.,\n            viewing_scale: float = 1.0, \n            eye: torch.Tensor = None,\n            camera_direction: torch.Tensor = None,\n            near: float = 1,\n            far: float = 100,\n            light_mode: str = \'surface\',\n            light_intensity_ambient: float = 0.5,\n            light_intensity_directional: float = 0.5,\n            light_color_ambient: torch.Tensor = None,\n            light_color_directional: torch.Tensor = None,\n            light_direction: torch.Tensor = None,\n            device: str = \'cpu\'):\n        r""""""Initalize the SoftRenderer object.\n\n        NOTE: SoftRenderer works only in GPU mode!\n\n        Args:\n            image_size (int): Size of the (square) image to be rendered (default: 256).\n            anti_aliasing (bool): Whether or not to perform anti-aliasing\n                (default: True)\n            bg_color (torch.Tensor): Background color of rendered image\n                (size: math:`3`, default: :math:`\\left[0, 0, 0\\right]`)\n            fill_back (bool): Whether or not to fill color to the back\n                side of each triangle as well (sometimes helps, when\n                the triangles in the mesh are not properly oriented.)\n                (default: True)\n            camera_mode (str): Choose from among `projection`, `look`, and\n                `look_at`. In the `projection` mode, the camera is at the\n                origin, and its optical axis is aligned with the positive\n                Z-axis. In the `look_at` mode, the object (not the camera)\n                is placed at the origin. The camera ""looks at"" the object\n                from a predefined ""eye"" location, which is computed from\n                the `viewing_angle` (another input to this function). In\n                the `look` mode, only the direction in which the camera\n                needs to look is specified. It does not necessarily look\n                towards the origin, as it allows the specification of a\n                custom ""upwards"" direction (default: \'look_at\').\n            K (torch.Tensor): Camera intrinsics matrix. Note that, unlike\n                standard notation, K here is a 4 x 4 matrix (with the last\n                row and last column drawn from the 4 x 4 identity matrix)\n                (default: None)\n            rmat (torch.Tensor): Rotation matrix (again, 4 x 4, as opposed\n                to the usual 3 x 3 convention) (default: None).\n            tvec (torch.Tensor): Translation vector (3 x 1). Note that the\n                (negative of the) tranlation is applied before rotation,\n                to be consistent with the projective geometry convention\n                of transforming a 3D point X by doing\n                torch.matmul(R.transpose(), X - t) (default: None).\n            perspective_distort (bool): Whether or not to perform perspective\n                distortion (to simulate field-of-view based distortion effects)\n                (default: True).\n            sigma_val (float): Sharpness of the probability distribution of each triangle\n                (Refer to the paper for more details) (default: 1e-5).\n            dist_func (str): Distance function to use (default: \'euclidean\')\n            dist_eps (float): A tiny positive number, for numerically stable distance\n                computation (default: 1e-4).\n            gamma_val (float): Sharpness parameter for the aggregation function (Refer to\n                the paper for more details) (default: 1e-4).\n            aggr_func_rgb (str): Aggregation function to use for the RGB channels\n                (default: \'softmax\').\n            aggr_func_alpha (str): Aggregation function to use for the alpha channel\n                (default: \'prod\').\n            texture_type (str): Type of textures to use (\'surface\' vs \'vertex\')\n                (default: \'surface\').\n            viewing_angle (float): Angle at which the object is to be viewed\n                (assumed to be in degrees!) (default: 30.)\n            viewing_scale (float): Scale at which the object is to be viewed\n                (default: 1).\n            eye (torch.Tensor): Location of the ""eye"" (of the camera). Used in the `look`\n                and `look_at` modes (default: computed using `viewing_angle`).\n            camera_direction (torch.Tensor): Direction in which the camera is facing\n                (used only in the `look` and `look_at` modes) (default:\n                :math:`[0, 0, 1]`)\n            near (float): Near clipping plane (for depth values) (default: 1).\n            far (float): Far clipping plane (for depth values) (default: 100).\n            light_mode (str): Mode of lighting up the mesh (choices: \'surface\', \'vertex\')\n                (default: \'surface\').\n            light_intensity_ambient (float): Intensity of ambient light (in the\n                range :math:`\\left[ 0, 1 \\right]`) (default: 0.5).\n            light_intensity_directional (float): Intensity of directional light\n                (in the range :math:`\\left[ 0, 1 \\right]`) (default: 0.5).\n            light_color_ambient (torch.Tensor): Color of ambient light\n                (default: :math:`\\left[ 1, 1, 1 \\right]`)\n            light_color_directional (torch.Tensor): Color of directional light\n                (default: :math:`\\left[ 1, 1, 1 \\right]`)\n            light_direction (torch.Tensor): Light direction, for directional\n                light (default: :math:`\\left[ 0, 1, 0 \\right]`)\n            device (torch.Tensor): Device on which all tensors are stored.\n                NOTE: Although the default device is set to \'cpu\', at the moment,\n                rendering will work only if the device is CUDA enabled.\n                Eg. \'cuda:0\'.\n\n        """"""\n\n        super(SoftRenderer, self).__init__()\n\n        # Size of the image to be generated.\n        self.image_size = image_size\n        # Whether or not to enable anti-aliasing\n        # If enabled, we render an image that is twice as large as the required\n        # size, and then downsample it.\n        self.anti_aliasing = anti_aliasing\n        # Device on which tensors of the class reside. At present, this function\n        # only works when the device is CUDA enabled, such as a GPU.\n        self.device = device\n        # Background color of the rendered image.\n        if bg_color is None:\n            self.bg_color = torch.zeros(3, device=device)\n        else:\n            self.bg_color = bg_color\n        # Whether or not to fill in color to the back faces of each triangle.\n        # Usually helps, especially when some of the triangles in the mesh\n        # have improper orientation specifications.\n        self.fill_back = fill_back\n\n        # camera_mode specifies how the scene is to be set up.\n        self.camera_mode = camera_mode\n        # Camera direction specifies the optical axis of the camera.\n        if camera_direction is None:\n            self.camera_direction = torch.tensor([0, 0, 1.], device=device)\n        else:\n            self.camera_direction = camera_direction.to(device)\n        # If the mode is \'projection\', use the input camera intrinsics and\n        # extrinsics.\n        if self.camera_mode == \'projection\':\n            if K is None:\n                self.K = torch.eye(3).unsqueeze(0).to(device)\n            else:\n                self.K = K.to(device)\n            if rmat is none:\n                self.rmat = torch.eye(3).unsqueeze(0).to(device)\n            else:\n                self.rmat = rmat\n            if tvec is none:\n                self.tvec = torch.zeros(1, 3)\n                # Translate sufficiently along negative Z, to ensure most points have positive depths.\n                self.tvec[0, 2] = -5\n                self.tvec = self.tvec.to(device)\n            else:\n                self.tvec = tvec.to(device)\n        # If the mode is \'look\' or \'look_at\', use the viewing angle to determine\n        # perspective distortion and camera position and orientation.\n        elif self.camera_mode in [\'look\', \'look_at\']:\n            # Whether or not to perform perspective distortion.\n            self.perspective_distort = perspective_distort\n            self.viewing_angle = viewing_angle\n            # Set the position of the eye\n            if eye is None:\n                self.eye = torch.tensor([0, 0, -(1. / math.tan(math.radians(self.viewing_angle)) + 1)], device=device)\n            else:\n                self.eye = eye.to(device)\n            # Direction in which the camera\'s optical axis is facing\n            self.camera_direction = torch.tensor([0, 0, 1.], device=self.device)\n\n        # Near and far clipping planes.\n        self.near = near\n        self.far = far\n\n        # Render quality parameters. Refer to the SoftRas paper for more details.\n        self.sigma_val = sigma_val\n        self.dist_func = dist_func\n        self.dist_eps = dist_eps\n        self.gamma_val = gamma_val\n        self.aggr_func_rgb = aggr_func_rgb\n        self.aggr_func_alpha = aggr_func_alpha\n        self.texture_type = texture_type\n\n        # Ambient and directional lighting parameters.\n        self.light_intensity_ambient = light_intensity_ambient\n        self.light_intensity_directional = light_intensity_directional\n        if light_color_ambient is None:\n            self.light_color_ambient = torch.ones(3, device=device)\n        else:\n            self.light_color_ambient = light_color_ambient.to(device)\n        if light_color_directional is None:\n            self.light_color_directional = torch.ones(3, device=device)\n        else:\n            self.light_color_directional = light_color_directional.to(device)\n        if light_direction is None:\n            self.light_direction = torch.tensor([0, 1., 0], device=device)\n        else:\n            self.light_direction = light_direction.to(device)\n\n        self.rasterizer_eps = 1e-3\n\n    def forward(self, vertices: torch.Tensor, faces: torch.Tensor, textures: Optional[torch.Tensor] = None,\n                mode: Optional[torch.Tensor] = None, K: Optional[torch.Tensor] = None,\n                rmat: Optional[torch.Tensor] = None, tvec: Optional[torch.Tensor] = None):\n\n        return self.render(vertices, faces, textures, mode, K, rmat, tvec)\n\n    def render(self, vertices: torch.Tensor, faces: torch.Tensor, textures: Optional[torch.Tensor] = None,\n               mode: Optional[torch.Tensor] = None, K: Optional[torch.Tensor] = None,\n               rmat=None, tvec=None):\n        r""""""Renders the RGB, depth, and alpha channels.\n\n        Args:\n            vertices (torch.Tensor): Vertices of the mesh (shape: :math:`B\n                \\times V \\times 3`), where :math:`B` is the batchsize,\n                and :math:`V` is the number of vertices in the mesh.\n            faces (torch.Tensor): Faces of the mesh (shape: :math:`B \\times\n                F \\times 3`), where :math:`B` is the batchsize, and :math:`F`\n                is the number of faces in the mesh.\n            textures (torch.Tensor): Mesh texture (shape: :math:`B \\times F\n                \\times 4 \\times 4 \\times 4 \\times 3`)\n            mode (str): Renderer mode (choices: \'rgb\', \'silhouette\',\n                \'depth\', None) (default: None). If the mode is None, the rgb,\n                depth, and alpha channels are all rendered. In the rgb mode,\n                only the rgb image channels are rendered. In the silhouette\n                mode, only a silhouette image is rendered. In the depth mode,\n                only a depth image is rendered.\n            K (torch.Tensor): Camera intrinsics (default: None) (shape:\n                :math:`B \\times 4 \\times 4` or :math:`4 \\times 4`) (default: None).\n            rmat (torch.Tensor): Rotation matrix (default: None) (shape:\n                :math:`B \\times 4 \\times 4` or :math:`4 \\times 4`)\n            tvec (torch.Tensor): Translation vector (default: None)\n                (shape: :math:`B \\times 3` or :math:`3`)\n\n        Returns:\n            (torch.Tensor): rendered RGB image channels\n            (torch.Tensor): rendered depth channel\n            (torch.Tensor): rendered alpha channel\n\n            Each of the channels is of shape\n            `self.image_size` x `self.image_size`.\n\n        """"""\n\n        # Lighting (not needed when we are rendering only depth/silhouette\n        # images)\n        if mode not in [\'depth\', \'silhouette\']:\n            textures = self.lighting(vertices, faces, textures)\n\n        # Transform vertices to the camera frame\n        vertices = self.transform_to_camera_frame(vertices)\n\n        # Project the vertices from the camera coordinate frame to the image.\n        vertices = self.project_to_image(vertices)\n\n        # Rasterization\n        images = self.rasterize(vertices, faces, textures)\n\n        return images\n\n    def lighting(self, vertices: torch.Tensor, faces: torch.Tensor, textures: torch.Tensor):\n        r""""""Applies ambient and directional lighting to the mesh.\n\n        Args:\n            vertices (torch.Tensor): Vertices of the mesh (shape: :math:`B\n                \\times V \\times 3`), where :math:`B` is the batchsize,\n                and :math:`V` is the number of vertices in the mesh.\n            faces (torch.Tensor): Faces of the mesh (shape: :math:`B \\times\n                F \\times 3`), where :math:`B` is the batchsize, and :math:`F`\n                is the number of faces in the mesh.\n            textures (torch.Tensor): Mesh texture (shape: :math:`B \\times F\n                \\times 4 \\times 4 \\times 4 \\times 3`)\n\n        """"""\n\n        faces_lighting = self.vertices_to_faces(vertices, faces)\n        ambient_lighting = compute_ambient_light(faces_lighting, textures,\n                                                 self.light_intensity_ambient,\n                                                 self.light_color_ambient)\n        directional_lighting = compute_directional_light(faces_lighting, textures,\n                                                         self.light_intensity_directional,\n                                                         self.light_color_directional)\n\n        return (ambient_lighting + directional_lighting) * textures\n\n    def shading(self):\n        r""""""Does nothing. Placeholder for shading functionality. """"""\n        raise NotImplementedError\n\n    def transform_to_camera_frame(self, vertices: torch.Tensor):\n        r""""""Transforms the mesh vertices to the camera frame, based on the\n        camera mode to be used.\n\n        Args:\n            vertices (torch.Tensor): Mesh vertices (shape: :math:`B \\times\n                V \\times 3`), where `B` is the batchsize, and `V` is the\n                number of mesh vertices.\n\n        Returns:\n            vertices (torch.Tensor): Transformed vertices into the camera\n                coordinate frame (shape: :math:`B \\times V \\times 3`).\n\n        """"""\n        if self.camera_mode == \'look_at\':\n            vertices = self.look_at(vertices, self.eye)\n\n        elif self.camera_mode == \'look\':\n            vertices = self.look(vertices, self.eye, self.camera_direction)\n\n        elif self.camera_mode == \'projection\':\n            if K is None:\n                K = self.K\n            if rmat is None:\n                rmat = self.rmat\n            if tvec is None:\n                tvec = self.tvec\n\n        return vertices\n\n    def project_to_image(self, vertices: torch.Tensor):\n        r""""""Projects the mesh vertices from the camera coordinate frame down\n        to the image.\n\n        Args:\n            vertices (torch.Tensor): Mesh vertices (shape: :math:`B \\times\n                V \\times 3`), where `B` is the batchsize, and `V` is the\n                number of mesh vertices.\n\n        Returns:\n            vertices (torch.Tensor): Projected image coordinates (u, v) for\n                each vertex, with an appended depth channel. (shape:\n                :math:`B \\times V \\times 3`), where :math:`B` is the\n                batchsize and :math:`V` is the number of vertices.\n\n        """"""\n\n        # TODO: Replace all of these by perspective_projection. Use different\n        # rmat, tvec combinations, based on the mode, but use a consistent\n        # projection function across all modes. Helps avoid redundancy.\n        if self.camera_mode == \'look_at\':\n            vertices = self.perspective_distortion(vertices,\n                                                   angle=self.viewing_angle)\n\n        elif self.camera_mode == \'look\':\n            vertices = self.perspective_distortion(vertices,\n                                                   angle=self.viewing_angle)\n\n        elif self.camera_mode == \'projection\':\n            vertices = self.perspective_projection(vertices, K, rmat, tvec)\n\n        return vertices\n\n    def rasterize(self, vertices: torch.Tensor, faces: torch.Tensor, textures: torch.Tensor):\n        r""""""Performs rasterization, i.e., conversion of triangles to pixels.\n\n        Args:\n            vertices (torch.Tensor): Vertices of the mesh (shape: :math:`B\n                \\times V \\times 3`), where :math:`B` is the batchsize,\n                and :math:`V` is the number of vertices in the mesh.\n            faces (torch.Tensor): Faces of the mesh (shape: :math:`B \\times\n                F \\times 3`), where :math:`B` is the batchsize, and :math:`F`\n                is the number of faces in the mesh.\n            textures (torch.Tensor): Mesh texture (shape: :math:`B \\times F\n                \\times 2 \\times 3`)\n\n        """"""\n\n        face_vertices = self.vertices_to_faces(vertices, faces)\n        face_textures = textures\n        if self.texture_type == ""vertex"":\n            face_textures = self.textures_to_faces(textures, faces)\n\n        image_size = self.image_size * (2 if self.anti_aliasing else 1)\n        out = soft_rasterize(face_vertices,\n                             face_textures,\n                             image_size,\n                             self.bg_color,\n                             self.near,\n                             self.far,\n                             self.fill_back,\n                             self.rasterizer_eps,\n                             self.sigma_val,\n                             self.dist_func,\n                             self.dist_eps,\n                             self.gamma_val,\n                             self.aggr_func_rgb,\n                             self.aggr_func_alpha,\n                             self.texture_type)\n        if self.anti_aliasing:\n            out = torch.nn.functional.avg_pool2d(out, kernel_size=2, stride=2)\n\n        return out\n\n    def look_at(self, vertices: torch.Tensor, eye: torch.Tensor,\n                at: Optional[torch.Tensor] = None, up: Optional[torch.Tensor] = None):\n        r""""""Camera ""looks at"" an object whose center is at the tensor represented\n        by ""at"". And ""up"" is the upwards direction.\n\n        Args:\n            vertices (torch.Tensor): Vertices of the mesh (shape: :math:`B\n                \\times V \\times 3`), where :math:`B` is the batchsize,\n                and :math:`V` is the number of vertices in the mesh.\n            eye (torch.Tensor): Location of the eye (camera) (shape: :math:`3`).\n            at (torch.Tensor): Location of the object to look at (shape: :math:`3`)\n                (default: :math:`[0., 0., 0.]`).\n            up (torch.Tensor): ""Up"" direction for the camera (shape: :math:`3`)\n                (default: :math:`[0., 1., 0.]`).\n\n        Returns:\n            vertices (torch.Tensor): Input vertices transformed to the camera coordinate\n                frame (shape: :math:`B \\times V \\times 3`), where :math:`B` is the batchsize,\n                and :math:`V` is the number of vertices in the mesh.\n\n        """"""\n\n        import torch.nn.functional as F\n\n        device = vertices.device\n\n        eye = eye.to(device)\n        if at is None:\n            at = torch.tensor([0., 0., 0.], device=device)\n        else:\n            at = at.to(device)\n        if up is None:\n            up = torch.tensor([0., 1., 0.], device=device)\n        else:\n            up = up.to(device)\n\n        batchsize = vertices.shape[0]\n\n        if eye.dim() == 1:\n            eye = eye[None, :].repeat(batchsize, 1)\n        if at.dim() == 1:\n            at = at[None, :].repeat(batchsize, 1)\n        if up.dim() == 1:\n            up = up[None, :].repeat(batchsize, 1)\n\n        # Create new axes\n        # eps is chosen as 1e-5 because that\'s what the authors use\n        # in their (Chainer) implementation\n        z_axis = F.normalize(at - eye, eps=1e-5)\n        x_axis = F.normalize(torch.cross(up, z_axis), eps=1e-5)\n        y_axis = F.normalize(torch.cross(z_axis, x_axis), eps=1e-5)\n\n        # Create rotation matrices\n        R = torch.cat((x_axis[:, None, :], y_axis[:, None, :],\n                       z_axis[:, None, :]), dim=1)\n\n        # Apply\n        # [B, V, 3] -> [B, V, 3] -> [B, V, 3]\n        if vertices.shape != eye.shape:\n            eye = eye[:, None, :]\n        vertices = vertices - eye\n        vertices = torch.matmul(vertices, R.transpose(1, 2))\n\n        return vertices\n\n    def look(self, vertices: torch.Tensor, eye: torch.Tensor, direction: Optional[torch.Tensor] = None,\n             up: Optional[torch.Tensor] = None):\n        r""""""Apply the ""look"" transformation to the vertices.\n\n        Args:\n            vertices (torch.Tensor): Vertices of the mesh (shape: :math:`B\n                \\times V \\times 3`), where :math:`B` is the batchsize,\n                and :math:`V` is the number of vertices in the mesh.\n            eye (torch.Tensor): Location of the eye (camera) (shape: :math:`3`).\n            direction (torch.Tensor): Direction along which the eye looks at (shape: :math:`3`)\n                (default: :math:`[0., 0., 0.]`).\n            up (torch.Tensor): ""Up"" direction for the camera (shape: :math:`3`)\n                (default: :math:`[0., 1., 0.]`).\n\n        Returns:\n            vertices (torch.Tensor): Input vertices transformed to the camera coordinate\n                frame (shape: :math:`B \\times V \\times 3`), where :math:`B` is the batchsize,\n                and :math:`V` is the number of vertices in the mesh.\n        """"""\n\n        import torch.nn.functional as F\n\n        device = vertices.device\n        if direction is None:\n            direction = torch.tensor([0, 1., 0], device=self.device)\n        else:\n            direction = direction.to(device)\n        if up is None:\n            up = torch.tensor([0, 1., 0], device=device)\n        else:\n            up = up.to(device)\n\n        if eye.dim() == 1:\n            eye = eye[None, :]\n        if direction.dim() == 1:\n            direction = direction[None, :]\n        if up.dim() == 1:\n            up = up[None, :]\n\n        # Create new axes\n        z_axis = F.normalize(direction, eps=1e-5)\n        x_axis = F.normalize(torch.cross(up, z_axis), eps=1e-5)\n        y_axis = F.normalize(torch.cross(z_axis, x_axis), eps=1e-5)\n\n        # Create rotation matrix (B x 3 x 3)\n        R = torch.cat((x_axis[:, None, :], y_axis[:, None, :],\n                       z_axis[:, None, :]), dim=1)\n\n        # Apply\n        if vertices.shape != eye.shape:\n            eye = eye[:, None, :]\n        vertices = vertices - eye\n        vertices = torch.matmul(vertices, R.transpose(1, 2))\n\n        return vertices\n\n    def perspective_distortion(self, vertices: torch.Tensor, angle: Optional[float] = 30.):\n        r""""""Compute perspective distortion from a given viewing angle.\n\n        Args:\n            vertices (torch.Tensor): Input vertices of the mesh (shape: :math:`B \\times V \\times 3`),\n                where :math:`B` is the batchsize, and :math:`V` is the number of vertices in the mesh.\n            angle (float): Angle to use for perspective distortion (default: 30 degrees).\n\n        Returns:\n            vertices (torch.Tensor): Input vertices transformed to the camera coordinate\n                frame (shape: :math:`B \\times V \\times 3`), where :math:`B` is the batchsize,\n                and :math:`V` is the number of vertices in the mesh.\n        """"""\n        device = vertices.device\n        angle = torch.tensor([angle / 180 * math.pi], device=self.device)\n        width = torch.tan(angle)\n        width = width[:, None]\n        z = vertices[..., 2]\n        xy = vertices[..., :2] / (z * width).unsqueeze(-1)\n        return torch.cat((xy, z.unsqueeze(-1)), dim=2)\n\n    def vertices_to_faces(self, vertices: torch.Tensor, faces: torch.Tensor):\n        r""""""Expand vertices in the layout defined by faces.\n\n        Args:\n            vertices (torch.Tensor): Mesh vertices (shape: math:`B \\times V \\times 3`).\n            faces (torch.Tensor): Mesh faces (shape: math:`B \\times F \\times 3`)\n\n        Returns:\n            (torch.Tensor): Input vertices transformed to the camera coordinate\n                frame (shape: :math:`B \\times V \\times 3`), where :math:`B` is the batchsize,\n                and :math:`V` is the number of vertices in the mesh.\n        """"""\n        B = vertices.shape[0]\n        V = vertices.shape[1]\n        device = vertices.device\n        faces = faces + (torch.arange(B, dtype=torch.long, device=device) * V)[:, None, None]\n        vertices = vertices.reshape(B * V, 3)\n        return vertices[faces]\n\n    def textures_to_faces(self, textures, faces):\n        r""""""Expand textures in the layout defined by faces.\n\n        Args:\n            textures (torch.Tensor): shape: math:`B \\times F \\times 2 \\times 3`\n            faces (torch.Tensor): shape: math:`B \\times F \\times 3`\n\n        Returns:\n            (torch.Tensor): Input textures transformed to the camera coordinate\n                frame (shape: :math:`B \\times F \\times 2 \\times 3`), where :math:`B`\n                is the batchsize, and :math:`F` is the number of faces in the mesh.\n        """"""\n        B = textures.shape[0]\n        F = textures.shape[1]\n        device = textures.device\n        faces = faces + (torch.arange(B, device=device) * F)[:, None, None]\n        textures = textures.reshape(B * F, 2, 3)\n        return textures[faces]\n\n    def set_eye_from_angles(self, distance: Union[int, float, torch.Tensor],\n                            elevation: Union[int, float, torch.Tensor],\n                            azimuth: Union[int, float, torch.Tensor],\n                            degrees: bool = True):\n        r""""""Compute the position of the camera given azimuth and elevation angles,\n        and the distance of the camera from the origin (i.e., spherical coordinates).\n\n        Args:\n            distance (float, or torch.Tensor): Distance of the camera from the origin.\n            azimuth (float, or torch.Tensor): Azimuth angle.\n            elevation (float, or torch.Tensor): Elevation angle.\n            degrees (optional, bool): Whether the azimuth and elevation angles are specified\n                in degrees (if True) or radians (if False) (default: True).\n        """"""\n        if isinstance(distance, float) or isinstance(distance, int):\n            if degrees:\n                elevation = (math.pi / 180.) * elevation\n                azimuth = (math.pi / 180.) * azimuth\n            if not torch.is_tensor(distance):\n                distance = torch.tensor([distance], dtype=torch.float32)\n            if not torch.is_tensor(elevation):\n                elevation = torch.tensor([elevation], dtype=torch.float32)\n            if not torch.is_tensor(azimuth):\n                azimuth = torch.tensor([azimuth], dtype=torch.float32)\n            self.eye = torch.tensor([\n                distance * torch.cos(elevation) * torch.sin(azimuth),\n                distance * torch.sin(elevation),\n                -distance * torch.cos(elevation) * torch.cos(azimuth),\n            ], device=self.device)\n'"
kaolin/graphics/__init__.py,0,b'from .Lighting import *\nfrom .nmr import *\nfrom .softras import *\n\nfrom .DifferentiableRenderer import DifferentiableRenderer\nfrom .NeuralMeshRenderer import NeuralMeshRenderer\nfrom .SoftRenderer import SoftRenderer\nfrom .DIBRenderer import DIBRenderer\n'
kaolin/mathutils/__init__.py,0,b'from kaolin.mathutils.common import *\nfrom kaolin.mathutils.geometry import *\n'
kaolin/mathutils/common.py,11,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n# Kornia components:\n# Copyright (C) 2017-2019, Arraiy, Inc., all rights reserved.\n# Copyright (C) 2019-    , Open Source Vision Foundation, all rights reserved.\n# Copyright (C) 2019-    , Kornia authors, all rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport torch\n\n\n# Borrowed from kornia\n# https://github.com/arraiyopensource/kornia\n\n# Pytorch does not redefine np.pi, hence\npi = torch.tensor(3.14159265358979323846)\n\n\n# Borrowed from kornia.\n# https://github.com/kornia/kornia/blob/master/kornia/geometry/conversions.py\ndef rad2deg(tensor):\n    r""""""Converts a tensor of angles from radians to degrees\n\n    Args:\n        tensor (torch.Tensor): Input tensor (no shape restrictions).\n\n    Returns:\n        torch.Tensor: Tensor of same shape as input.\n\n    Example:\n        >>> deg = kaolin.pi * kaolin.rad2deg(torch.rand(1, 2, 3))\n\n    """"""\n\n    if not torch.is_tensor(tensor):\n        raise TypeError(\'Expected torch.Tensor. Got {} instead.\'.format(\n            type(tensor)))\n\n    return 180. * tensor / pi.to(tensor.device).type(tensor.dtype)\n\n\n# Borrowed from kornia.\n# https://github.com/kornia/kornia/blob/master/kornia/geometry/conversions.py\ndef deg2rad(tensor):\n    r""""""Converts angles from degrees to radians\n\n    Args:\n        tensor (torch.Tensor): Input tensor (no shape restrictions).\n\n    Returns:\n        torch.Tensor: Tensor of same shape as input.\n\n    Example:\n        >>> rad = kaolin.deg2rad(360. * torch.rand(1, 3, 3))\n\n    """"""\n\n    if not torch.is_tensor(tensor):\n        raise TypeError(\'Expected torch.Tensor. Got {} instead.\'.format(\n            type(tensor)))\n\n    return tensor * pi.to(tensor.device).type(tensor.dtype) / 180.\n'"
kaolin/metrics/__init__.py,0,b'from .voxel import *\nfrom .point import *\nfrom .mesh import *\n'
kaolin/metrics/mesh.py,41,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n# GEOMetrics\n#\n# Copyright (c) 2019 Edward Smith\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport torch\nimport kaolin as kal\nfrom kaolin.rep import Mesh\nimport kaolin.cuda.tri_distance as td\nimport numpy as np\n\n\n\nclass TriangleDistanceFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, points, verts_1, verts_2, verts_3):\n        batchsize, n, _ = points.size()\n        points = points.contiguous()\n        verts_1 = verts_1.contiguous()\n        verts_2 = verts_2.contiguous()\n        verts_3 = verts_3.contiguous()\n\n        dist1 = torch.zeros(batchsize, n)\n        idx1 = torch.zeros(batchsize, n, dtype=torch.int)\n        type1 = torch.zeros(batchsize, n, dtype=torch.int)\n\n        dist1 = dist1.cuda()\n        idx1 = idx1.cuda()\n        type1 = type1.cuda()\n        td.forward_cuda(points, verts_1, verts_2, verts_3, dist1, idx1, type1)\n        ctx.save_for_backward(idx1)\n\n        return dist1[0].detach(), idx1[0].detach().long(), type1[0].long()\n\n\nclass TriangleDistance(torch.nn.Module):\n    def forward(self, points, verts_1, verts_2, verts_3):\n        points = points.view(1, -1, 3)\n        verts_1 = verts_1.view(1, -1, 3)\n        verts_2 = verts_2.view(1, -1, 3)\n        verts_3 = verts_3.view(1, -1, 3)\n\n        return TriangleDistanceFunction.apply(points, verts_1, verts_2, verts_3)\n\n\ndef chamfer_distance(mesh1: Mesh, mesh2: Mesh,\n                     w1: float = 1., w2: float = 1., num_points=3000):\n    r"""""" computes the chamfer distance bewteen two meshes by sampling the two surfaces\n\n    Args:\n        mesh1 (Mesh): first mesh\n        mesh2 (Mesh): second mesh\n        w1 (float): weighting of forward direction\n        w2 (float): weighting of backward direction\n        num_points (int): number of points to sample on each mesh\n\n    Returns:\n        chamfer_distance (torch.Tensor): chamfer distance\n\n    Example:\n        >>> mesh1 = TriangleMesh.from_obj(file1)\n        >>> mesh2 = TriangleMesh.from_obj(file2)\n        >>> distance = chamfer_distance(mesh1, mesh2, 500)\n\n    """"""\n\n    set1 = mesh1.sample(num_points)[0]\n    set2 = mesh2.sample(num_points)[0]\n    return kal.metrics.point.chamfer_distance(set1, set2, w1, w2)\n\n\ndef edge_length(mesh: Mesh):\n    r""""""Returns the average length of a face in a mesh\n\n    Args:\n        mesh (Mesh): mesh over which to calcuale edge length\n\n    Returns:\n        edge_length (torch.Tensor): averge lenght of mesh edge\n\n    Example:\n        >>> mesh  = TriangleMesh.from_obj(file)\n        >>> length = edge_length(mesh)\n\n    """"""\n\n    p1 = torch.index_select(mesh.vertices, 0, mesh.faces[:, 0])\n    p2 = torch.index_select(mesh.vertices, 0, mesh.faces[:, 1])\n    p3 = torch.index_select(mesh.vertices, 0, mesh.faces[:, 2])\n    # get edge lentgh\n    e1 = p2 - p1\n    e2 = p3 - p1\n    e3 = p2 - p3\n\n    el1 = ((torch.sum(e1**2, 1))).mean()\n    el2 = ((torch.sum(e2**2, 1))).mean()\n    el3 = ((torch.sum(e3**2, 1))).mean()\n\n    edge_length = (el1 + el2 + el3) / 6.\n    return edge_length\n\n\ndef laplacian_loss(mesh1: Mesh, mesh2: Mesh):\n    r""""""Returns the change in laplacian over two meshes\n\n    Args:\n        mesh1 (Mesh): first mesh\n        mesh2: (Mesh): second mesh\n\n\n    Returns:\n        lap_loss (torch.Tensor):  laplacian change over the mesh\n\n    Example:\n        >>> mesh1 = TriangleMesh.from_obj(file)\n        >>> mesh2 = TriangleMesh.from_obj(file)\n        >>> mesh2.vertices = mesh2.vertices * 1.05\n        >>> lap = laplacian_loss(mesh1, mesh2)\n\n    """"""\n\n    lap1 = mesh1.compute_laplacian()\n    lap2 = mesh2.compute_laplacian()\n    lap_loss = torch.mean(torch.sum((lap1 - lap2)**2, 1))\n    return lap_loss\n\n\ndef point_to_surface(points: torch.Tensor, mesh: Mesh):\n    r""""""Computes the minimum distances from a set of points to a mesh\n\n    Args:\n        points (torch.Tensor): set of points\n        mesh (Mesh): mesh to calculate distance\n\n    Returns:\n        distance: mean distance between points and surface\n\n    Example:\n        >>> mesh = TriangleMesh.from_obj(file)\n        >>> points = torch.rand(1000,3)\n        >>> loss = point_to_surface(points, mesh)\n\n    """"""\n\n    # extract triangle defs from mesh\n    v1 = torch.index_select(mesh.vertices.clone(), 0, mesh.faces[:, 0])\n    v2 = torch.index_select(mesh.vertices.clone(), 0, mesh.faces[:, 1])\n    v3 = torch.index_select(mesh.vertices.clone(), 0, mesh.faces[:, 2])\n\n    # if quad mesh the separate the triangles\n    if mesh.faces.shape[-1] == 4:\n        v4 = torch.index_select(mesh.vertices.clone(), 0, mesh.faces[:, 3])\n        temp1 = v1.clone()\n        temp2 = v2.clone()\n        temp3 = v3.clone()\n        v1 = torch.cat((v1, v1), dim=0)\n        v2 = torch.cat((v2, v4), dim=0)\n        v3 = torch.cat((v3, v3), dim=0)\n\n    if points.is_cuda:\n\n        tri_minimum_dist = TriangleDistance()\n        # pass to cuda\n        distance, indx, dist_type = tri_minimum_dist(points, v1, v2, v3)\n        indx = indx.data.cpu().numpy()\n        dist_type = torch.LongTensor(dist_type.data.cpu().numpy())\n        # reconpute distances to define gradient\n        grad_dist = _recompute_point_to_surface(\n            [v1, v2, v3], points, indx, dist_type)\n        # sanity check\n        # print(distance.mean(), grad_dist)\n    else:\n        grad_dist = _point_to_surface_cpu(v1, v2, v3, points)\n\n    return grad_dist\n\n\ndef _recompute_point_to_surface(verts, p, indecies, dist_type):\n    # recompute surface based the calcualted correct assignments of points and triangles\n    # and the type of distacne, type 1 to 3 idicates which edge to calcualte to,\n    # type 4 indicates the distance is from a point on the triangle not an edge\n    v1, v2, v3 = verts\n    v1 = v1[indecies]\n    v2 = v2[indecies]\n    v3 = v3[indecies]\n\n    type_1 = (dist_type == 0)\n    type_2 = (dist_type == 1)\n    type_3 = (dist_type == 2)\n    type_4 = (dist_type == 3)\n\n    v21 = v2 - v1\n    v32 = v3 - v2\n    v13 = v1 - v3\n\n    p1 = p - v1\n    p2 = p - v2\n    p3 = p - v3\n\n    dists = []\n    dists.append(_compute_edge_dist(v21[type_1], p1[type_1]).view(-1))\n    dists.append(_compute_edge_dist(v32[type_2], p2[type_2]).view(-1))\n    dists.append(_compute_edge_dist(v13[type_3], p3[type_3]).view(-1))\n\n    if len(np.where(type_4)[0]) > 0:\n        nor = torch.cross(v21[type_4], v13[type_4])\n        dists.append(_compute_planar_dist(nor, p1[type_4]))\n\n    distances = torch.cat(dists)\n\n    return torch.mean((distances))\n\n\ndef _compute_edge_dist(v, p):\n    # batched distance between an edge and a point\n    if v.shape[0] == 0:\n        return v\n    dots = _compute_dot(v, p)\n    dots_div = _compute_dot(v, v,)\n    dots = torch.clamp(dots / dots_div, 0.0, 1.0).view(-1, 1)\n    dots = (v * dots) - p\n    dots = _compute_dot(dots, dots)\n    return dots.view(-1, 1)\n\n\ndef _compute_dot(p1, p2):\n    # batched dot product\n    return torch.bmm(p1.view(p1.shape[0], 1, 3),\n                     p2.view(p2.shape[0], 3, 1)).view(-1)\n\n\ndef _compute_planar_dist(nor, p):\n    # batched distance between a point and a tiangle\n    if nor.shape[0] == 0:\n        return nor\n    dot = _compute_dot(nor, p)\n    dot_div = _compute_dot(nor, nor)\n    return dot * dot / dot_div\n\n\ndef _point_to_surface_cpu(v1, v2, v3, points):\n\n    faces_len = v1.shape[0]\n    v21 = v2 - v1\n    v32 = v3 - v2\n    v13 = v1 - v3\n    nor = torch.cross(v21, v13)\n\n    # make more of them, one set for each sampled point\n    v1 = v1.view(\n        1, -1, 3).expand(points.shape[0], v1.shape[0], 3).contiguous().view(-1, 3)\n    v2 = v2.view(\n        1, -1, 3).expand(points.shape[0], v2.shape[0], 3).contiguous().view(-1, 3)\n    v3 = v3.view(\n        1, -1, 3).expand(points.shape[0], v3.shape[0], 3).contiguous().view(-1, 3)\n    v21 = v21.view(\n        1, -1, 3).expand(points.shape[0], v21.shape[0], 3).contiguous().view(-1, 3)\n    v32 = v32.view(\n        1, -1, 3).expand(points.shape[0], v32.shape[0], 3).contiguous().view(-1, 3)\n    v13 = v13.view(\n        1, -1, 3).expand(points.shape[0], v13.shape[0], 3).contiguous().view(-1, 3)\n    nor = nor.view(\n        1, -1, 3).expand(points.shape[0], nor.shape[0], 3).contiguous().view(-1, 3)\n    p = points.view(-1, 1,\n                    3).expand(points.shape[0], faces_len, 3).contiguous().view(-1, 3)\n\n    p1 = p - v1\n    p2 = p - v2\n    p3 = p - v3\n\n    del(v1)\n    del(v2)\n    del(v3)\n\n    sign1 = _compute_sign(v21, nor, p1)\n    sign2 = _compute_sign(v32, nor, p2)\n    sign3 = _compute_sign(v13, nor, p3)\n\n    outside_triangle = torch.le(torch.abs(sign1 + sign2 + sign3), 2)\n    inside_triangle = torch.gt(torch.abs(sign1 + sign2 + sign3), 2)\n    distances = torch.FloatTensor(np.zeros(sign1.shape))\n\n    del (sign1)\n    del (sign2)\n    del (sign3)\n\n    outside_triangle = np.where(outside_triangle)\n    inside_triangle = np.where(inside_triangle)\n\n    try:\n        dotter1 = _compute_dotter(v21[outside_triangle], p1[outside_triangle])\n        dotter2 = _compute_dotter(v32[outside_triangle], p2[outside_triangle])\n        dotter3 = _compute_dotter(v13[outside_triangle], p3[outside_triangle])\n        dots = torch.cat((dotter1, dotter2, dotter3), dim=1)\n        edge_distance = torch.min(dots, dim=1)[0]\n    except BaseException:\n        edge_distance = 0\n\n    distances[outside_triangle] = edge_distance\n    try:\n        face_distance = _compute_planar_dist(\n            nor[inside_triangle], p1[inside_triangle])\n    except BaseException:\n        face_distance = 0\n    distances[inside_triangle] = face_distance\n\n    distances = distances.view(points.shape[0], faces_len)\n\n    min_distaces = torch.min(distances, dim=1)[0]\n\n    return torch.mean(min_distaces)\n\n\ndef _compute_sign(v, nor, p):\n    sign = torch.cross(v, nor)\n    sign = _compute_dot(sign, p)\n    sign = sign.sign()\n    return sign\n\n\ndef _compute_dotter(v, p):\n    if v.shape[0] == 0:\n        return v\n    dotter = _compute_dot(v, p)\n    dotter_div = _compute_dot(v, v,)\n    dotter = torch.clamp(dotter / dotter_div, 0.0, 1.0).view(-1, 1)\n    dotter = (v * dotter) - p\n    dotter = _compute_dot(dotter, dotter)\n    return dotter.view(-1, 1)\n'"
kaolin/metrics/point.py,42,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nfrom kaolin.nnsearch import nnsearch\nimport kaolin.cuda.sided_distance as sd\nfrom scipy.spatial import cKDTree as Tree\nimport numpy as np\n\n\nclass SidedDistanceFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, S1, S2):\n\n        batchsize, n, _ = S1.size()\n        S1 = S1.contiguous()\n        S2 = S2.contiguous()\n        dist1 = torch.zeros(batchsize, n)\n        idx1 = torch.zeros(batchsize, n, dtype=torch.int)\n        dist1 = dist1.cuda()\n        idx1 = idx1.cuda()\n        try:\n            sd.forward(S1, S2, dist1, idx1)\n        except BaseException:\n            sd.forward_cuda(S1, S2, dist1, idx1)\n\n        return idx1.long()\n\n\nclass SidedDistance(torch.nn.Module):\n    r""""""For every point in set1 find the indecies of the closest point in set2\n\n    Args:\n            set1 (torch.cuda.Tensor) : set of pointclouds of shape B x N x 3\n            set2 (torch.cuda.Tensor) : set of pointclouds of shape B x M x 3\n\n    Returns:\n            torch.cuda.Tensor: indecies of the closest points in set2\n\n    Example:\n            >>> A = torch.rand(2,300,3)\n            >>> B = torch.rand(2,200,3)\n            >>> sided_minimum_dist = SidedDistance()\n            >>> indices = sided_minimum_dist(A,B)\n            >>> indices.shape\n            torch.Size([2, 300])\n\n\n    """"""\n\n    def forward(self, S1: torch.Tensor, S2: torch.Tensor):\n        assert len(S1.shape) == 3\n        assert len(S2.shape) == 3\n        return SidedDistanceFunction.apply(S1, S2).detach()\n\n\ndef chamfer_distance(S1: torch.Tensor, S2: torch.Tensor,\n                     w1: float = 1., w2: float = 1.):\n    r""""""Computes the chamfer distance between two point clouds\n\n    Args:\n            S1 (torch.Tensor): point cloud\n            S2 (torch.Tensor): point cloud\n            w1: (float): weighting of forward direction\n            w2: (float): weighting of backward direction\n\n    Returns:\n            torch.Tensor: chamfer distance between two point clouds S1 and S2\n\n    Example:\n            >>> A = torch.rand(300,3)\n            >>> B = torch.rand(200,3)\n            >>> >>> chamfer_distance(A,B)\n            tensor(0.1868)\n\n    """"""\n\n    assert (S1.dim() == S2.dim()), \'S1 and S2 must have the same dimesionality\'\n    assert (S1.dim() == 2), \'the dimensions of the input must be 2 \'\n\n    dist_to_S2 = directed_distance(S1, S2)\n    dist_to_S1 = directed_distance(S2, S1)\n\n    distance = w1 * dist_to_S2 + w2 * dist_to_S1\n\n    return distance\n\n\ndef directed_distance(S1: torch.Tensor, S2: torch.Tensor, mean: bool = True):\n    r""""""Computes the average distance from point cloud S1 to point cloud S2\n\n    Args:\n            S1 (torch.Tensor): point cloud\n            S2 (torch.Tensor): point cloud\n            mean (bool): if the distances should be reduced to the average\n\n    Returns:\n            torch.Tensor: ditance from point cloud S1 to point cloud S2\n\n    Args:\n\n    Example:\n            >>> A = torch.rand(300,3)\n            >>> B = torch.rand(200,3)\n            >>> >>> directed_distance(A,B)\n            tensor(0.1868)\n\n    """"""\n\n    if S1.is_cuda and S2.is_cuda:\n        sided_minimum_dist = SidedDistance()\n        closest_index_in_S2 = sided_minimum_dist(\n            S1.unsqueeze(0), S2.unsqueeze(0))[0]\n        closest_S2 = torch.index_select(S2, 0, closest_index_in_S2)\n\n    else:\n        from time import time\n        closest_index_in_S2 = nnsearch(S1, S2)\n        closest_S2 = S2[closest_index_in_S2]\n\n    dist_to_S2 = (((S1 - closest_S2)**2).sum(dim=-1))\n    if mean:\n        dist_to_S2 = dist_to_S2.mean()\n\n    return dist_to_S2\n\n\ndef iou(points1: torch.Tensor, points2: torch.Tensor, thresh=.5):\n    r"""""" Computes the intersection over union values for two sets of points\n\n    Args:\n            points1 (torch.Tensor): first points\n            points2 (torch.Tensor): second points\n    Returns:\n            iou (torch.Tensor) : IoU scores for the two sets of points\n\n    Examples:\n            >>> points1 = torch.rand( 1000)\n            >>> points2 = torch.rand( 1000)\n            >>> loss = iou(points1, points2)\n            tensor(0.3400)\n\n    """"""\n    points1[points1 <= thresh] = 0\n    points1[points1 > thresh] = 1\n\n    points2[points2 <= thresh] = 0\n    points2[points2 > thresh] = 1\n\n    points1 = points1.view(-1).byte()\n    points2 = points2.view(-1).byte()\n\n    assert points1.shape == points2.shape, \'points1 and points2 must have the same shape\'\n\n    iou = torch.sum(torch.mul(points1, points2).float()) / \\\n        torch.sum((points1 + points2).clamp(min=0, max=1).float())\n\n    return iou\n\n\ndef f_score(gt_points: torch.Tensor, pred_points: torch.Tensor,\n            radius: float = 0.01, extend=False):\n    r"""""" Computes the f-score of two sets of points, with a hit defined by two point existing withing a defined radius of each other\n\n    Args:\n            gt_points (torch.Tensor): ground truth points\n            pred_points (torch.Tensor): predicted points points\n            radius (float): radisu from a point to define a hit\n            extend (bool): if the alternate f-score definition should be applied\n\n    Returns:\n            (float): computed f-score\n\n    Example:\n            >>> points1 = torch.rand(1000)\n            >>> points2 = torch.rand(1000)\n            >>> loss = f_score(points1, points2)\n            >>> loss\n            tensor(0.0070)\n\n    """"""\n\n    pred_distances = torch.sqrt(directed_distance(\n        gt_points, pred_points, mean=False))\n    gt_distances = torch.sqrt(directed_distance(\n        pred_points, gt_points, mean=False))\n\n    if extend:\n        fp = (gt_distances > radius).float().sum()\n        tp = (gt_distances <= radius).float().sum()\n        precision = tp / (tp + fp)\n        tp = (pred_distances <= radius).float().sum()\n        fn = (pred_distances > radius).float().sum()\n        recall = tp / (tp + fn)\n\n    else:\n        fn = torch.sum(pred_distances > radius)\n        fp = torch.sum(gt_distances > radius).float()\n        tp = torch.sum(gt_distances <= radius).float()\n\n        precision = tp / (tp + fp)\n        recall = tp / (tp + fn)\n\n    f_score = 2 * (precision * recall) / (precision + recall + 1e-8)\n    return f_score\n'"
kaolin/metrics/voxel.py,7,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport kaolin as kal\n\n\ndef iou(pred, gt, thresh=.5, reduction=\'mean\'):\n    r"""""" Computes IoU across two voxel grids\n\n    Arguments:\n            pred (torch.Tensor): predicted (binary) voxel grid\n            gt (torch.Tensor): ground-truth (binary) voxel grid\n            thresh (float): value to threshold the prediction with\n\n    Returns:\n            iou (torch.Tensor): the intersection over union value\n\n    Example:\n            >>> pred = torch.rand(32, 32, 32)\n            >>> gt = torch.rand(32, 32, 32) *2. // 1\n            >>> loss = iou(pred, gt)\n            >>> loss\n            tensor(0.3338)\n    """"""\n    pred = pred.clone()\n    pred[pred <= thresh] = 0\n    pred[pred > thresh] = 1\n\n    pred = pred.view(-1).byte()\n    gt = gt.view(-1).byte()\n    assert pred.shape == gt.shape, \'pred and gt must have the same shape\'\n\n    iou = torch.sum(torch.mul(pred, gt).float()) / \\\n        torch.sum((pred + gt).clamp(min=0, max=1).float())\n\n    return iou\n'"
kaolin/models/DIBREncoder.py,2,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass DIBREncoder(nn.Module):\n    r""""""Encoder architecture used for single-image based mesh prediction in\n    the Neurips 2019 paper ""Learning to Predict 3D Objects with an\n    Interpolation-based Differentiable Renderer""\n    \n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @inproceedings{chen2019dibrender,\n                title={Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer},\n                author={Wenzheng Chen and Jun Gao and Huan Ling and Edward Smith and Jaakko Lehtinen and Alec Jacobson and Sanja Fidler},\n                booktitle={Advances In Neural Information Processing Systems},\n                year={2019}\n            }\n            \n    """"""\n\n    def __init__(self, N_CHANNELS, N_KERNELS, \\\n                 BATCH_SIZE, IMG_DIM, VERTS):\n        super(Encoder, self).__init__()\n        \n        block1 = self.convblock(N_CHANNELS, 32, N_KERNELS, stride=2, pad=2)\n        block2 = self.convblock(32, 64, N_KERNELS, stride=2, pad=2)\n        block3 = self.convblock(64, 128, N_KERNELS, stride=2, pad=2)\n        block4 = self.convblock(128, 128, N_KERNELS, stride=2, pad=2)\n        \n        linear1 = self.linearblock(10368, 1024)\n        linear2 = self.linearblock(1024, 1024)\n        self.linear3 = nn.Linear(1024, 1024)\n        \n        linear4 = self.linearblock(1024, 1024)\n        linear5 = self.linearblock(1024, 2048)\n        self.linear6 = nn.Linear(2048, VERTS*6)\n       \n        #################################################\n        all_blocks = block1 + block2 + block3 + block4\n        self.encoder1 = nn.Sequential(*all_blocks)\n        \n        all_blocks = linear1 + linear2\n        self.encoder2 = nn.Sequential(*all_blocks)\n        \n        all_blocks = linear4 + linear5\n        self.decoder = nn.Sequential(*all_blocks)\n        \n        # Initialize with Xavier Glorot\n        for m in self.modules():\n            if isinstance(m, nn.ConvTranspose2d) \\\n            or isinstance(m, nn.Linear) \\\n            or isinstance(object, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.normal_(m.weight, mean=0, std=0.001)\n\n        # Free some memory\n        del all_blocks, block1, block2, block3, \\\n        linear1, linear2, linear4, linear5, \\\n    \n    def convblock(self, indim, outdim, ker, stride, pad):\n        block2 = [\n            nn.Conv2d(indim, outdim, ker, stride, pad),\n            nn.BatchNorm2d(outdim),\n            nn.ReLU()\n        ]\n        return block2\n    \n    def linearblock(self, indim, outdim):\n        block2 = [\n            nn.Linear(indim, outdim),\n            nn.BatchNorm1d(outdim),\n            nn.ReLU()\n        ]\n        return block2\n        \n    def forward(self, x):\n        \n        for layer in self.encoder1:\n            x = layer(x)\n        \n        bnum = x.shape[0] \n        x = x.view(bnum, -1) \n        for layer in self.encoder2:\n            x = layer(x)\n        x = self.linear3(x)\n        \n       \n        for layer in self.decoder:\n            x = layer(x)\n        x = self.linear6(x).view(x.shape[0], -1,6)\n        verts = x[:,:,:3]\n        colors = x[:,:,3:]\n        return verts, colors\n'"
kaolin/models/GEOMetrics.py,8,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch \nfrom torch import nn \n\n\nclass VoxelDecoder(nn.Module):\n    r""""""\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n\n        .. code-block::\n\n            @InProceedings{smith19a,\n                title = {{GEOM}etrics: Exploiting Geometric Structure for Graph-Encoded Objects},\n                author = {Smith, Edward and Fujimoto, Scott and Romero, Adriana and Meger, David},\n                booktitle = {Proceedings of the 36th International Conference on Machine Learning},\n                pages = {5866--5876},\n                year = {2019},\n                volume = {97},\n                series = {Proceedings of Machine Learning Research},\n                publisher = {PMLR},\n            }\n\n    """"""\n    def __init__(self, latent_length): \n        super(VoxelDecoder, self).__init__()\n        self.fully = torch.nn.Sequential(\n            torch.nn.Linear(latent_length, 512)\n        )\n\n        self.model = torch.nn.Sequential(\n            torch.nn.ConvTranspose3d(64, 64, 4, stride=2, padding=(1, 1, 1)), \n            nn.BatchNorm3d(64),\n            nn.ELU(inplace=True),\n\n            torch.nn.ConvTranspose3d(64, 64, 4, stride=2, padding=(1, 1, 1)), \n            nn.BatchNorm3d(64),\n            nn.ELU(inplace=True),\n\n            torch.nn.ConvTranspose3d(64, 32, 4, stride=2, padding=(1, 1, 1)), \n            nn.BatchNorm3d(32),\n            nn.ELU(inplace=True),\n\n            torch.nn.ConvTranspose3d(32, 8, 4, stride=2, padding=(1, 1, 1)), \n            nn.BatchNorm3d(8),\n            nn.ELU(inplace=True),\n\n            nn.Conv3d(8, 1, (3, 3, 3), stride=1, padding=(1, 1, 1))\n        )\n\n    def forward(self, latent):\n        decode = self.fully(latent).view(-1, 64, 2, 2, 2)\n        decode = self.model(decode).reshape(-1, 32, 32, 32)\n        voxels = torch.sigmoid(decode)\n        return voxels\n'"
kaolin/models/GraphResNet.py,1,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom torch import nn \nimport torch.nn.functional as F\n\nfrom .SimpleGCN import SimpleGCN\n\n\nclass GraphResNet(nn.Module):\n    r""""""An enhanced version of the MeshEncoder; used residual connections\n    across graph convolution layers.\n\n    """"""\n\n    def __init__(self, input_features, hidden=192, output_features=3):\n        super(GraphResNet, self).__init__()\n        self.gc1 = SimpleGCN(input_features, hidden)\n        self.gc2 = SimpleGCN(hidden, hidden)\n        self.gc3 = SimpleGCN(hidden, hidden)\n        self.gc4 = SimpleGCN(hidden, hidden)\n        self.gc5 = SimpleGCN(hidden, hidden)\n        self.gc6 = SimpleGCN(hidden, hidden)\n        self.gc7 = SimpleGCN(hidden, hidden)\n        self.gc8 = SimpleGCN(hidden, hidden)\n        self.gc9 = SimpleGCN(hidden, hidden)\n        self.gc10 = SimpleGCN(hidden, hidden)\n        self.gc11 = SimpleGCN(hidden, hidden)\n        self.gc12 = SimpleGCN(hidden, hidden)\n        self.gc13 = SimpleGCN(hidden, hidden)\n        self.gc14 = SimpleGCN(hidden, output_features)\n        self.hidden = hidden\n\n    def forward(self, features, adj):\n\n        x = (F.relu(self.gc1(features, adj)))\n        x = (F.relu(self.gc2(x, adj)))\n        features = features[..., :self.hidden] + x\n        features /= 2.\n        # 2\n        x = (F.relu(self.gc3(features, adj)))\n        x = (F.relu(self.gc4(x, adj)))\n        features = features + x\n        features /= 2.\n        # 3\n        x = (F.relu(self.gc5(features, adj)))\n        x = (F.relu(self.gc6(x, adj)))\n        features = features + x\n        features /= 2.\n\n        # 4\n        x = (F.relu(self.gc7(features, adj)))\n        x = (F.relu(self.gc8(x, adj)))\n        features = features + x\n        features /= 2.\n\n        # 5\n        x = (F.relu(self.gc9(features, adj)))\n        x = (F.relu(self.gc10(x, adj)))\n        features = features + x\n        features /= 2.\n\n        # 6\n        x = (F.relu(self.gc11(features, adj)))\n        x = (F.relu(self.gc12(x, adj)))\n        features = features + x\n        features /= 2.\n\n        # 7\n        x = (F.relu(self.gc13(features, adj)))\n\n        features = features + x\n        features /= 2.\n\n        coords = (self.gc14(features, adj))\n        return coords, features\n'"
kaolin/models/Image2MeshReconstructionBaseline.py,2,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ImageToMeshReconstructionBaseline(nn.Module):\n    r""""""A simple mesh reconstruction architecture from images. This serves\n    as a baseline for mesh reconstruction systems.\n    \n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @InProceedings{smith19a,\n                title = {{GEOM}etrics: Exploiting Geometric Structure for Graph-Encoded Objects},\n                author = {Smith, Edward and Fujimoto, Scott and Romero, Adriana and Meger, David},\n                booktitle = {Proceedings of the 36th International Conference on Machine Learning},\n                pages = {5866--5876},\n                year = {2019},\n                volume = {97},\n                series = {Proceedings of Machine Learning Research},\n                publisher = {PMLR},\n            }\n    """"""\n\n    def __init__(self, N_CHANNELS, N_KERNELS, \\\n                 BATCH_SIZE, IMG_DIM, VERTS):\n        super(ImageToMeshReconstruction, self).__init__()\n        \n        block1 = self.convblock(N_CHANNELS, 32, N_KERNELS, stride=2, pad=2)\n        block2 = self.convblock(32, 64, N_KERNELS, stride=2, pad=2)\n        block3 = self.convblock(64, 128, N_KERNELS, stride=2, pad=2)\n        block4 = self.convblock(128, 128, N_KERNELS, stride=2, pad=2)\n        \n        linear1 = self.linearblock(10368, 1024)\n        linear2 = self.linearblock(1024, 1024)\n        self.linear3 = nn.Linear(1024, 1024)\n        \n        linear4 = self.linearblock(1024, 1024)\n        linear5 = self.linearblock(1024, 2048)\n        self.linear6 = nn.Linear(2048, VERTS*3)\n       \n        #################################################\n        all_blocks = block1 + block2 + block3 + block4\n        self.encoder1 = nn.Sequential(*all_blocks)\n        \n        all_blocks = linear1 + linear2\n        self.encoder2 = nn.Sequential(*all_blocks)\n        \n        all_blocks = linear4 + linear5\n        self.decoder = nn.Sequential(*all_blocks)\n        \n        # Initialize with Xavier Glorot\n        for m in self.modules():\n            if isinstance(m, nn.ConvTranspose2d) \\\n            or isinstance(m, nn.Linear) \\\n            or isinstance(object, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.normal_(m.weight, mean=0, std=0.001)\n\n        # Free some memory\n        del all_blocks, block1, block2, block3, \\\n        linear1, linear2, linear4, linear5, \\\n       \n    def convblock(self, indim, outdim, ker, stride, pad):\n        block2 = [\n            nn.Conv2d(indim, outdim, ker, stride, pad),\n            nn.BatchNorm2d(outdim),\n            nn.ReLU()\n        ]\n        return block2\n    \n    def linearblock(self, indim, outdim):\n        block2 = [\n            nn.Linear(indim, outdim),\n            nn.BatchNorm1d(outdim),\n            nn.ReLU()\n        ]\n        return block2\n        \n    def forward(self, x):\n        \n        for layer in self.encoder1:\n            x = layer(x)\n        \n        bnum = x.shape[0] \n        x = x.view(bnum, -1) \n        for layer in self.encoder2:\n            x = layer(x)\n        x = self.linear3(x)\n       \n        for layer in self.decoder:\n            x = layer(x)\n        x = self.linear6(x)\n       \n        return x.view(x.shape[0], -1,3)\n'"
kaolin/models/MeshEncoder.py,3,"b'import torch \nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom .SimpleGCN import SimpleGCN\n\nclass MeshEncoder(nn.Module):\n    def __init__(self, latent_length):\n        super(MeshEncoder, self).__init__()\n        self.h1 = SimpleGCN(3, 60)\n        self.h21 = SimpleGCN(60, 60)\n        self.h22 = SimpleGCN(60, 60)\n        self.h23 = SimpleGCN(60, 60)\n        self.h24 = SimpleGCN(60, 120)\n        self.h3 = SimpleGCN(120, 120)\n        self.h4 = SimpleGCN(120, 120)\n        self.h41 = SimpleGCN(120, 150)\n        self.h5 = SimpleGCN(150, 200)\n        self.h6 = SimpleGCN(200, 210)\n        self.h7 = SimpleGCN(210, 250)\n        self.h8 = SimpleGCN(250, 300)\n        self.h81 = SimpleGCN(300, 300)\n        self.h9 = SimpleGCN(300, 300)\n        self.h10 = SimpleGCN(300, 300)\n        self.h11 = SimpleGCN(300, 300)\n        self.reduce = SimpleGCN(300, latent_length) \n\n    def resnet(self, features, res):\n        temp = features[:, :res.shape[1]]\n        temp = temp + res\n        features = torch.cat((temp, features[:, res.shape[1]:]), dim=1)\n        return features, features\n\n    def forward(self, positions, adj):\n        res = positions\n        features = F.elu(self.h1(positions, adj))\n        features = F.elu(self.h21(features, adj))\n        features = F.elu(self.h22(features, adj))\n        features = F.elu(self.h23(features, adj))\n        features = F.elu(self.h24(features, adj))\n        features = F.elu(self.h3(features, adj))\n        features = F.elu(self.h4(features, adj))\n        features = F.elu(self.h41(features, adj))\n        features = F.elu(self.h5(features, adj))\n        features = F.elu(self.h6(features, adj))\n        features = F.elu(self.h7(features, adj))\n        features = F.elu(self.h8(features, adj))\n        features = F.elu(self.h81(features, adj))\n        features = F.elu(self.h9(features, adj))\n        features = F.elu(self.h10(features, adj))\n        features = F.elu(self.h11(features, adj))\n\n        latent = F.elu(self.reduce(features , adj))  \n        latent = (torch.max(latent, dim=0)[0])      \n        return latent\n'"
kaolin/models/OccupancyNetwork.py,10,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n# Copyright 2019 Lars Mescheder, Michael Oechsle, Michael Niemeyer,\n# Andreas Geiger, Sebastian Nowozin\n\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation the\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or\n# sell copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport math\n\nimport torch \nfrom torch import nn \nfrom torch.nn.parameter import Parameter\nimport torch.nn.functional as F\nfrom torchvision import models\nimport torch.distributions as dist\n\nimport torch\nfrom torch.nn import Parameter\n\nclass Resnet18(nn.Module):\n    r\'\'\' ResNet-18 encoder network for image input.\n    Args:\n        c_dim (int): output dimension of the latent embedding\n        normalize (bool): whether the input images should be normalized\n        use_linear (bool): whether a final linear layer should be used\n    \'\'\'\n\n    def __init__(self, c_dim, normalize=True, use_linear=True):\n        super().__init__()\n        self.normalize = normalize\n        self.use_linear = use_linear\n        self.features = models.resnet18(pretrained=True)\n        self.features.fc = nn.Sequential()\n        if use_linear:\n            self.fc = nn.Linear(512, c_dim)\n        elif c_dim == 512:\n            self.fc = nn.Sequential()\n        else:\n            raise ValueError(\'c_dim must be 512 if use_linear is False\')\n\n    def forward(self, x):\n        if self.normalize:\n            x = normalize_imagenet(x)\n        net = self.features(x)\n        out = self.fc(net)\n        return out\n\ndef normalize_imagenet(x):\n    \'\'\' Normalize input images according to ImageNet standards.\n    Args:\n        x (tensor): input images\n    \'\'\'\n    x = x.clone()\n    x[:, 0] = (x[:, 0] - 0.485) / 0.229\n    x[:, 1] = (x[:, 1] - 0.456) / 0.224\n    x[:, 2] = (x[:, 2] - 0.406) / 0.225\n    return x\n\n\nclass DecoderCBatchNorm(nn.Module):\n    \'\'\' Decoder with conditional batch normalization (CBN) class.\n    Args:\n        dim (int): input dimension\n        z_dim (int): dimension of latent code z\n        c_dim (int): dimension of latent conditioned code c\n        hidden_size (int): hidden size of Decoder network\n        leaky (bool): whether to use leaky ReLUs\n        legacy (bool): whether to use the legacy structure\n    \'\'\'\n\n    def __init__(self, dim=3, z_dim=128, c_dim=128,\n                 hidden_size=256, leaky=False, legacy=False):\n        super().__init__()\n        self.z_dim = z_dim\n        if not z_dim == 0:\n            self.fc_z = nn.Linear(z_dim, hidden_size)\n\n        self.fc_p = nn.Conv1d(dim, hidden_size, 1)\n        self.block0 = CResnetBlockConv1d(c_dim, hidden_size, legacy=legacy)\n        self.block1 = CResnetBlockConv1d(c_dim, hidden_size, legacy=legacy)\n        self.block2 = CResnetBlockConv1d(c_dim, hidden_size, legacy=legacy)\n        self.block3 = CResnetBlockConv1d(c_dim, hidden_size, legacy=legacy)\n        self.block4 = CResnetBlockConv1d(c_dim, hidden_size, legacy=legacy)\n\n        if not legacy:\n            self.bn = CBatchNorm1d(c_dim, hidden_size)\n        else:\n            self.bn = CBatchNorm1d_legacy(c_dim, hidden_size)\n\n        self.fc_out = nn.Conv1d(hidden_size, 1, 1)\n\n        if not leaky:\n            self.actvn = F.relu\n        else:\n            self.actvn = lambda x: F.leaky_relu(x, 0.2)\n\n    def forward(self, p, z, c, **kwargs):\n        p = p.transpose(1, 2)\n        batch_size, D, T = p.size()\n        net = self.fc_p(p)\n\n        if self.z_dim != 0:\n            net_z = self.fc_z(z).unsqueeze(2)\n            net = net + net_z\n\n        net = self.block0(net, c)\n        net = self.block1(net, c)\n        net = self.block2(net, c)\n        net = self.block3(net, c)\n        net = self.block4(net, c)\n\n        out = self.fc_out(self.actvn(self.bn(net, c)))\n        out = out.squeeze(1)\n\n        return out\n\n\ndef get_prior_z(device):\n    \'\'\' Returns prior distribution for latent code z.\n    Args:\n        cfg (dict): imported yaml config\n        device (device): pytorch device\n    \'\'\'\n    z_dim = 0\n    p0_z = dist.Normal(\n        torch.zeros(z_dim, device = device),\n        torch.ones(z_dim, device = device)\n    )\n\n    return p0_z\n\n\nclass CBatchNorm1d(nn.Module):\n    \'\'\' Conditional batch normalization layer class.\n    Args:\n        c_dim (int): dimension of latent conditioned code c\n        f_dim (int): feature dimension\n        norm_method (str): normalization method\n    \'\'\'\n\n    def __init__(self, c_dim, f_dim, norm_method=\'batch_norm\'):\n        super().__init__()\n        self.c_dim = c_dim\n        self.f_dim = f_dim\n        self.norm_method = norm_method\n        # Submodules\n        self.conv_gamma = nn.Conv1d(c_dim, f_dim, 1)\n        self.conv_beta = nn.Conv1d(c_dim, f_dim, 1)\n        if norm_method == \'batch_norm\':\n            self.bn = nn.BatchNorm1d(f_dim, affine=False)\n        elif norm_method == \'instance_norm\':\n            self.bn = nn.InstanceNorm1d(f_dim, affine=False)\n        elif norm_method == \'group_norm\':\n            self.bn = nn.GroupNorm1d(f_dim, affine=False)\n        else:\n            raise ValueError(\'Invalid normalization method!\')\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.zeros_(self.conv_gamma.weight)\n        nn.init.zeros_(self.conv_beta.weight)\n        nn.init.ones_(self.conv_gamma.bias)\n        nn.init.zeros_(self.conv_beta.bias)\n\n    def forward(self, x, c):\n        assert(x.size(0) == c.size(0))\n        assert(c.size(1) == self.c_dim)\n\n        # c is assumed to be of size batch_size x c_dim x T\n        if len(c.size()) == 2:\n            c = c.unsqueeze(2)\n\n        # Affine mapping\n        gamma = self.conv_gamma(c)\n        beta = self.conv_beta(c)\n\n        # Batchnorm\n        net = self.bn(x)\n        out = gamma * net + beta\n\n        return out\n\n\nclass CResnetBlockConv1d(nn.Module):\n    \'\'\' Conditional batch normalization-based Resnet block class.\n    Args:\n        c_dim (int): dimension of latend conditioned code c\n        size_in (int): input dimension\n        size_out (int): output dimension\n        size_h (int): hidden dimension\n        norm_method (str): normalization method\n        legacy (bool): whether to use legacy blocks \n    \'\'\'\n\n    def __init__(self, c_dim, size_in, size_h=None, size_out=None,\n                 norm_method=\'batch_norm\', legacy=False):\n        super().__init__()\n        # Attributes\n        if size_h is None:\n            size_h = size_in\n        if size_out is None:\n            size_out = size_in\n\n        self.size_in = size_in\n        self.size_h = size_h\n        self.size_out = size_out\n        # Submodules\n        if not legacy:\n            self.bn_0 = CBatchNorm1d(\n                c_dim, size_in, norm_method=norm_method)\n            self.bn_1 = CBatchNorm1d(\n                c_dim, size_h, norm_method=norm_method)\n        else:\n            self.bn_0 = CBatchNorm1d_legacy(\n                c_dim, size_in, norm_method=norm_method)\n            self.bn_1 = CBatchNorm1d_legacy(\n                c_dim, size_h, norm_method=norm_method)\n\n        self.fc_0 = nn.Conv1d(size_in, size_h, 1)\n        self.fc_1 = nn.Conv1d(size_h, size_out, 1)\n        self.actvn = nn.ReLU()\n\n        if size_in == size_out:\n            self.shortcut = None\n        else:\n            self.shortcut = nn.Conv1d(size_in, size_out, 1, bias=False)\n        # Initialization\n        nn.init.zeros_(self.fc_1.weight)\n\n    def forward(self, x, c):\n        net = self.fc_0(self.actvn(self.bn_0(x, c)))\n        dx = self.fc_1(self.actvn(self.bn_1(net, c)))\n\n        if self.shortcut is not None:\n            x_s = self.shortcut(x)\n        else:\n            x_s = x\n\n        return x_s + dx\n\n\nclass OccupancyNetwork(nn.Module):\n    \'\'\' Occupancy Network class.\n    Args:\n        decoder (nn.Module): decoder network\n        encoder (nn.Module): encoder network\n        p0_z (dist): prior distribution for latent code z\n        device (device): torch device\n\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @inproceedings{Occupancy Networks,\n                title = {Occupancy Networks: Learning 3D Reconstruction in Function Space},\n                author = {Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael and Nowozin, Sebastian and Geiger, Andreas},\n                booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},\n                year = {2019}\n            }\n    \'\'\'\n\n    def __init__(self, device):\n        super().__init__()\n        self.device = device\n        self.decoder = DecoderCBatchNorm(dim=3, z_dim=0, c_dim=256,\n            hidden_size=256).to(self.device)\n        self.encoder = Resnet18(256, normalize=True, use_linear=True).to(self.device)\n\n        self.p0_z = get_prior_z(self.device)\n\n    def forward(self, p, inputs, sample=True, **kwargs):\n        \'\'\' Performs a forward pass through the network.\n        Args:\n            p (tensor): sampled points\n            inputs (tensor): conditioning input\n            sample (bool): whether to sample for z\n        \'\'\'\n        batch_size = p.size(0)\n        c = self.encode_inputs(inputs)\n        z = self.get_z_from_prior((batch_size,), sample=sample)\n        p_r = self.decode(p, z, c, **kwargs)\n        return p_r\n\n    def compute_elbo(self, p, occ, inputs, **kwargs):\n        \'\'\' Computes the expectation lower bound.\n        Args:\n            p (tensor): sampled points\n            occ (tensor): occupancy values for p\n            inputs (tensor): conditioning input\n        \'\'\'\n        c = self.encode_inputs(inputs)\n        q_z = self.infer_z(p, occ, c, **kwargs)\n        z = q_z.rsample()\n        p_r = self.decode(p, z, c, **kwargs)\n\n        rec_error = -p_r.log_prob(occ).sum(dim=-1)\n        kl = dist.kl_divergence(q_z, self.p0_z).sum(dim=-1)\n        elbo = -rec_error - kl\n\n        return elbo, rec_error, kl\n\n    def encode_inputs(self, inputs):\n        \'\'\' Encodes the input.\n        Args:\n            input (tensor): the input\n        \'\'\'\n        c = self.encoder(inputs)\n       \n\n        return c\n\n    def decode(self, p, z, c, **kwargs):\n        \'\'\' Returns occupancy probabilities for the sampled points.\n        Args:\n            p (tensor): points\n            z (tensor): latent code z\n            c (tensor): latent conditioned code c\n        \'\'\'\n\n        logits = self.decoder(p, z, c, **kwargs)\n        p_r = dist.Bernoulli(logits=logits)\n        return p_r\n\n    def infer_z(self, p, occ, c, **kwargs):\n        \'\'\' Infers z.\n        Args:\n            p (tensor): points tensor\n            occ (tensor): occupancy values for occ\n            c (tensor): latent conditioned code c\n        \'\'\'\n        \n        batch_size = p.size(0)\n        mean_z = torch.empty(batch_size, 0).to(self.device)\n        logstd_z = torch.empty(batch_size, 0).to(self.device)\n\n        q_z = dist.Normal(mean_z, torch.exp(logstd_z))\n\n        return q_z\n\n    def get_z_from_prior(self, size=torch.Size([]), sample=True):\n        \'\'\' Returns z from prior distribution.\n        Args:\n            size (Size): size of z\n            sample (bool): whether to sample\n        \'\'\'\n        if sample:\n            z = self.p0_z.sample(size).to(self.device)\n        else:\n            z = self.p0_z.mean.to(self.device)\n            z = z.expand(*size, *z.size())\n\n        return z\n        '"
kaolin/models/Pixel2Mesh.py,11,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\n\nimport torch \nfrom torch import nn \nfrom torch.nn.parameter import Parameter\nimport torch.nn.functional as F\n\nimport torch\nfrom torch.nn import Parameter\n\n\nclass VGG(nn.Module):\n    r""""""\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @InProceedings{Simonyan15,\n              author       = ""Karen Simonyan and Andrew Zisserman"",\n              title        = ""Very Deep Convolutional Networks for Large-Scale Image Recognition"",\n              booktitle    = ""International Conference on Learning Representations"",\n              year         = ""2015"",\n            }\n    """"""\n    \n    def __init__(self, channels = 4):\n        \n        super(VGG, self).__init__()\n\n        self.layer0_1 = nn.Conv2d(channels, 16, 3, stride = 1, padding = 1)\n        self.layer0_2 = nn.Conv2d(16, 16, 3, stride = 1, padding = 1)\n        \n        self.layer1_1 = nn.Conv2d(16, 32, 3, stride = 2, padding = 1) \n        self.layer1_2 = nn.Conv2d(32, 32, 3, stride = 1, padding = 1)\n        self.layer1_3 = nn.Conv2d(32, 32, 3, stride = 1, padding = 1)\n        \n        self.layer2_1 = nn.Conv2d(32, 64, 3, stride = 2, padding = 1) \n        self.layer2_2 = nn.Conv2d(64, 64, 3, stride = 1, padding = 1)\n        self.layer2_3 = nn.Conv2d(64, 64, 3, stride = 1, padding = 1)\n        \n        self.layer3_1 = nn.Conv2d(64, 128, 3, stride = 2, padding = 1) \n        self.layer3_2 = nn.Conv2d(128, 128, 3, stride = 1, padding = 1)\n        self.layer3_3 = nn.Conv2d(128, 128, 3, stride = 1, padding = 1)\n        \n        self.layer4_1 = nn.Conv2d(128, 256, 5, stride = 2, padding = 2) \n        self.layer4_2 = nn.Conv2d(256, 256, 3, stride = 1, padding = 1)\n        self.layer4_3 = nn.Conv2d(256, 256, 3, stride = 1, padding = 1)\n        \n        self.layer5_1 = nn.Conv2d(256, 512, 5, stride = 2, padding = 2) \n        self.layer5_2 = nn.Conv2d(512, 512, 3, stride = 1, padding = 1)\n        self.layer5_3 = nn.Conv2d(512, 512, 3, stride = 1, padding = 1)\n        self.layer5_4 = nn.Conv2d(512, 512, 3, stride = 1, padding = 1)\n        \n    def forward(self, img):\n        \n        img = F.relu(self.layer0_1(img))\n        img = F.relu(self.layer0_2(img))\n        \n        img = F.relu(self.layer1_1(img))\n        img = F.relu(self.layer1_2(img))\n        img = F.relu(self.layer1_3(img))\n        \n        img = F.relu(self.layer2_1(img))\n        img = F.relu(self.layer2_2(img))\n        img = F.relu(self.layer2_3(img))\n        A = torch.squeeze(img) \n        \n        img = F.relu(self.layer3_1(img))\n        img = F.relu(self.layer3_2(img))\n        img = F.relu(self.layer3_3(img))\n        B = torch.squeeze(img) \n        \n        img = F.relu(self.layer4_1(img))\n        img = F.relu(self.layer4_2(img))\n        img = F.relu(self.layer4_3(img))\n        C = torch.squeeze(img) \n        \n        img = F.relu(self.layer5_1(img))\n        img = F.relu(self.layer5_2(img))\n        img = F.relu(self.layer5_3(img))\n        img = F.relu(self.layer5_4(img))\n        D = torch.squeeze(img) \n        \n        return [A, B, C, D]\n\n\nclass G_Res_Net(nn.Module):\n    r""""""Pixel2Mesh architecture.\n\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @inProceedings{wang2018pixel2mesh,\n              title={Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images},\n              author={Nanyang Wang and Yinda Zhang and Zhuwen Li and Yanwei Fu and Wei Liu and Yu-Gang Jiang},\n              booktitle={ECCV},\n              year={2018}\n            }\n    """"""\n\n    def __init__(self, input_features, hidden = 128, output_features = 3):\n        super(G_Res_Net, self).__init__()\n        self.gc1 = GCN(input_features, hidden)\n        self.gc2 = GCN(hidden, hidden)\n        self.gc3 = GCN(hidden , hidden)\n        self.gc4 = GCN(hidden, hidden)\n        self.gc5 = GCN(hidden , hidden)\n        self.gc6 = GCN(hidden, hidden)\n        self.gc7 = GCN(hidden , hidden)\n        self.gc8 = GCN(hidden, hidden)\n        self.gc9 = GCN(hidden , hidden)\n        self.gc10 = GCN(hidden, hidden)\n        self.gc11 = GCN(hidden , hidden)\n        self.gc12 = GCN(hidden, hidden)\n        self.gc13 = GCN(hidden , hidden)\n        self.gc14 = GCN(hidden,  output_features)\n        self.hidden = hidden\n\n    def forward(self, features, adj):\n        features = features.unsqueeze(0)\n\n        x = (F.relu(self.gc1(features, adj)))\n        x = (F.relu(self.gc2(x, adj)))\n        features = features[..., :self.hidden] + x\n        features /= 2.\n        # 2\n        x = (F.relu(self.gc3(features, adj)))\n        x = (F.relu(self.gc4(x, adj)))\n        features = features + x\n        features /= 2.\n        # 3\n        x = (F.relu(self.gc5(features, adj)))\n        x = (F.relu(self.gc6(x, adj)))\n        features = features + x\n        features /= 2.\n\n        # 4\n        x = (F.relu(self.gc7(features, adj)))\n        x = (F.relu(self.gc8(x, adj)))\n        features = features + x\n        features /= 2.\n\n        # 5\n        x = (F.relu(self.gc9(features, adj)))\n        x = (F.relu(self.gc10(x, adj)))\n        features = features + x\n        features /= 2.\n\n        # 6\n        x = (F.relu(self.gc11(features, adj)))\n        x = (F.relu(self.gc12(x, adj)))\n        features = features + x\n        features /= 2.\n\n        # 7\n        x = (F.relu(self.gc13(features, adj)))\n\n        features = features + x\n        features /= 2.\n\n        coords = (self.gc14(features, adj))\n        return coords.squeeze(0),features.squeeze(0)\n\n\nclass GCN(nn.Module):\n    """"""\n    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @article{kipf2016semi,\n              title={Semi-Supervised Classification with Graph Convolutional Networks},\n              author={Kipf, Thomas N and Welling, Max},\n              journal={arXiv preprint arXiv:1609.02907},\n              year={2016}\n            }\n    """"""\n    def __init__(self, in_features, out_features):\n        super(GCN, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.Tensor(in_features, out_features))\n        self.bias = Parameter(torch.Tensor(out_features))\n      \n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = .6 / math.sqrt((self.weight.size(1) + self.weight.size(0)))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-.1, .1)\n\n    def forward(self, input, adj):\n\n        support = torch.bmm(input, self.weight.unsqueeze(0).expand(input.shape[0], -1, -1))\n\n        output = torch.bmm(adj.unsqueeze(0).expand(input.shape[0], -1, -1), support)\n\n        output = output + self.bias\n        return output\n\n    def __repr__(self):\n        return self.__class__.__name__ + \' (\' \\\n               + str(self.in_features) + \' -> \' \\\n               + str(self.out_features) + \')\'\n'"
kaolin/models/PointNet.py,11,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Iterable\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass PointNetFeatureExtractor(nn.Module):\n    r""""""PointNet feature extractor (extracts either global or local, i.e.,\n    per-point features).\n\n    Based on the original PointNet paper:.\n\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n\n        .. code-block::\n\n            @article{qi2016pointnet,\n              title={PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},\n              author={Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},\n              journal={arXiv preprint arXiv:1612.00593},\n              year={2016}\n            }\n\n    Args:\n        in_channels (int): Number of channels in the input pointcloud\n            (default: 3, for X, Y, Z coordinates respectively).\n        feat_size (int): Size of the global feature vector\n            (default: 1024)\n        layer_dims (Iterable[int]): Sizes of fully connected layers\n            to be used in the feature extractor (excluding the input and\n            the output layer sizes). Note: the number of\n            layers in the feature extractor is implicitly parsed from\n            this variable.\n        global_feat (bool): Extract global features (i.e., one feature\n            for the entire pointcloud) if set to True. If set to False,\n            extract per-point (local) features (default: True).\n        activation (function): Nonlinearity to be used as activation\n                    function after each batchnorm (default: F.relu)\n        batchnorm (bool): Whether or not to use batchnorm layers\n            (default: True)\n        transposed_input (bool): Whether the input\'s second and third dimension\n            is already transposed. If so, a transpose operation can be avoided,\n            improving performance.\n            See documentation for the forward method for more details.\n\n    For example, to specify a PointNet feature extractor with 4 linear\n    layers (sizes 6 -> 10, 10 -> 40, 40 -> 500, 500 -> 1024), with\n    3 input channels in the pointcloud and a global feature vector of size\n    1024, see the example below.\n\n    Example:\n\n        >>> pointnet = PointNetFeatureExtractor(in_channels=3, feat_size=1024,\n                                           layer_dims=[10, 20, 40, 500])\n        >>> x = torch.rand(2, 3, 30)\n        >>> y = pointnet(x)\n        print(y.shape)\n\n    """"""\n\n    def __init__(self,\n                 in_channels: int = 3,\n                 feat_size: int = 1024,\n                 layer_dims: Iterable[int] = [64, 128],\n                 global_feat: bool = True,\n                 activation=F.relu,\n                 batchnorm: bool = True,\n                 transposed_input: bool = False):\n        super(PointNetFeatureExtractor, self).__init__()\n\n        if not isinstance(in_channels, int):\n            raise TypeError(\'Argument in_channels expected to be of type int. \'\n                            \'Got {0} instead.\'.format(type(in_channels)))\n        if not isinstance(feat_size, int):\n            raise TypeError(\'Argument feat_size expected to be of type int. \'\n                            \'Got {0} instead.\'.format(type(feat_size)))\n        if not hasattr(layer_dims, \'__iter__\'):\n            raise TypeError(\'Argument layer_dims is not iterable.\')\n        for idx, layer_dim in enumerate(layer_dims):\n            if not isinstance(layer_dim, int):\n                raise TypeError(\'Elements of layer_dims must be of type int. \'\n                                \'Found type {0} at index {1}.\'.format(\n                                    type(layer_dim), idx))\n        if not isinstance(global_feat, bool):\n            raise TypeError(\'Argument global_feat expected to be of type \'\n                            \'bool. Got {0} instead.\'.format(\n                                type(global_feat)))\n\n        # Store feat_size as a class attribute\n        self.feat_size = feat_size\n\n        # Store activation as a class attribute\n        self.activation = activation\n\n        # Store global_feat as a class attribute\n        self.global_feat = global_feat\n\n        # Add in_channels to the head of layer_dims (the first layer\n        # has number of channels equal to `in_channels`). Also, add\n        # feat_size to the tail of layer_dims.\n        if not isinstance(layer_dims, list):\n            layer_dims = list(layer_dims)\n        layer_dims.insert(0, in_channels)\n        layer_dims.append(feat_size)\n\n        self.conv_layers = nn.ModuleList()\n        if batchnorm:\n            self.bn_layers = nn.ModuleList()\n        for idx in range(len(layer_dims) - 1):\n            self.conv_layers.append(nn.Conv1d(layer_dims[idx],\n                                              layer_dims[idx + 1], 1))\n            if batchnorm:\n                self.bn_layers.append(nn.BatchNorm1d(layer_dims[idx + 1]))\n\n        # Store whether or not to use batchnorm as a class attribute\n        self.batchnorm = batchnorm\n\n        self.transposed_input = transposed_input\n\n    def forward(self, x: torch.Tensor):\n        r""""""Forward pass through the PointNet feature extractor.\n\n        Args:\n            x (torch.Tensor): Tensor representing a pointcloud\n                (shape: :math:`B \\times N \\times D`, where :math:`B`\n                is the batchsize, :math:`N` is the number of points\n                in the pointcloud, and :math:`D` is the dimensionality\n                of each point in the pointcloud).\n                If self.transposed_input is True, then the shape is\n                :math:`B \\times D \\times N`.\n\n        """"""\n        if not self.transposed_input:\n            x = x.transpose(1, 2)\n\n        # Number of points\n        num_points = x.shape[2]\n\n        # By default, initialize local features (per-point features)\n        # to None.\n        local_features = None\n\n        # Apply a sequence of conv-batchnorm-nonlinearity operations\n\n        # For the first layer, store the features, as these will be\n        # used to compute local features (if specified).\n        if self.batchnorm:\n            x = self.activation(self.bn_layers[0](self.conv_layers[0](x)))\n        else:\n            x = self.activation(self.conv_layers[0](x))\n        if self.global_feat is False:\n            local_features = x\n\n        # Pass through the remaining layers (until the penultimate layer).\n        for idx in range(1, len(self.conv_layers) - 1):\n            if self.batchnorm:\n                x = self.activation(self.bn_layers[idx](\n                    self.conv_layers[idx](x)))\n            else:\n                x = self.activation(self.conv_layers[idx](x))\n\n        # For the last layer, do not apply nonlinearity.\n        if self.batchnorm:\n            x = self.bn_layers[-1](self.conv_layers[-1](x))\n        else:\n            x = self.conv_layers[-1](x)\n\n        # Max pooling.\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, self.feat_size)\n\n        # If extracting global features, return at this point.\n        if self.global_feat:\n            return x\n\n        # If extracting local features, compute local features by\n        # concatenating global features, and per-point features\n        x = x.view(-1, self.feat_size, 1).repeat(1, 1, num_points)\n        return torch.cat((x, local_features), dim=1)\n\n\nclass PointNetClassifier(nn.Module):\n    r""""""PointNet classifier. Uses the PointNet feature extractor, and\n    adds classification layers on top.\n\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n\n        .. code-block::\n\n            @article{qi2016pointnet,\n              title={PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},\n              author={Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},\n              journal={arXiv preprint arXiv:1612.00593},\n              year={2016}\n            }\n\n    Args:\n        in_channels (int): Number of channels in the input pointcloud\n            (default: 3, for X, Y, Z coordinates respectively).\n        feat_size (int): Size of the global feature vector\n            (default: 1024)\n        num_classes (int): Number of classes (for the classification\n            task) (default: 2).\n        dropout (float): Dropout ratio to use (default: 0.). Note: If\n            the ratio is set to 0., we altogether skip using a dropout\n            layer.\n        layer_dims (Iterable[int]): Sizes of fully connected layers\n            to be used in the feature extractor (excluding the input and\n            the output layer sizes). Note: the number of\n            layers in the feature extractor is implicitly parsed from\n            this variable.\n        activation (function): Nonlinearity to be used as activation\n            function after each batchnorm (default: F.relu)\n        batchnorm (bool): Whether or not to use batchnorm layers\n            (default: True)\n        transposed_input (bool): Whether the input\'s second and third dimension\n            is already transposed. If so, a transpose operation can be avoided,\n            improving performance.\n            See documentation of PointNetFeatureExtractor for more details.\n\n    Example:\n\n        >>> pointnet = PointNetClassifier(in_channels=6, feat_size=1024, \\\n                                      feat_layer_dims=[32, 64, 256], \\\n                                      classifier_layer_dims=[500, 200, 100])\n        >>> x = torch.rand(5, 6, 30)\n        >>> y = pointnet(x)\n        >>> print(y.shape)\n\n    """"""\n\n    def __init__(self,\n                 in_channels: int = 3,\n                 feat_size: int = 1024,\n                 num_classes: int = 2,\n                 dropout: float = 0.,\n                 classifier_layer_dims: Iterable[int] = [512, 256],\n                 feat_layer_dims: Iterable[int] = [64, 128],\n                 activation=F.relu,\n                 batchnorm: bool = True,\n                 transposed_input: bool = False):\n\n        super(PointNetClassifier, self).__init__()\n\n        if not isinstance(num_classes, int):\n            raise TypeError(\'Argument num_classes must be of type int. \'\n                            \'Got {0} instead.\'.format(type(num_classes)))\n        if not isinstance(dropout, float):\n            raise TypeError(\'Argument dropout must be of type float. \'\n                            \'Got {0} instead.\'.format(type(dropout)))\n        if dropout < 0 or dropout > 1:\n            raise ValueError(\'Dropout ratio must always be in the range\'\n                             \'[0, 1]. Got {0} instead.\'.format(dropout))\n        if not hasattr(classifier_layer_dims, \'__iter__\'):\n            raise TypeError(\'Argument classifier_layer_dims is not iterable.\')\n        for idx, layer_dim in enumerate(classifier_layer_dims):\n            if not isinstance(layer_dim, int):\n                raise TypeError(\'Expected classifier_layer_dims to contain \'\n                                \'int. Found type {0} at index {1}.\'.format(\n                                    type(layer_dim), idx))\n\n        # Add feat_size to the head of classifier_layer_dims (the output of\n        # the PointNet feature extractor has number of elements equal to\n        # has number of channels equal to `in_channels`).\n        if not isinstance(classifier_layer_dims, list):\n            classifier_layer_dims = list(classifier_layer_dims)\n        classifier_layer_dims.insert(0, feat_size)\n\n        # Note that `global_feat` MUST be set to True, for global\n        # classification tasks.\n        self.feature_extractor = PointNetFeatureExtractor(\n            in_channels=in_channels, feat_size=feat_size,\n            layer_dims=feat_layer_dims, global_feat=True,\n            activation=activation, batchnorm=batchnorm,\n            transposed_input=transposed_input\n        )\n\n        self.linear_layers = nn.ModuleList()\n        if batchnorm:\n            self.bn_layers = nn.ModuleList()\n        for idx in range(len(classifier_layer_dims) - 1):\n            self.linear_layers.append(nn.Linear(classifier_layer_dims[idx],\n                                                classifier_layer_dims[idx + 1]))\n            if batchnorm:\n                self.bn_layers.append(nn.BatchNorm1d(\n                    classifier_layer_dims[idx + 1]))\n\n        self.last_linear_layer = nn.Linear(classifier_layer_dims[-1],\n                                           num_classes)\n\n        # Store activation as a class attribute\n        self.activation = activation\n\n        # Dropout layer (if dropout ratio is in the interval (0, 1]).\n        if dropout > 0:\n            self.dropout = nn.Dropout(p=dropout)\n\n        else:\n            self.dropout = None\n\n        # Store whether or not to use batchnorm as a class attribute\n        self.batchnorm = batchnorm\n\n        self.transposed_input = transposed_input\n\n    def forward(self, x):\n        r""""""Forward pass through the PointNet classifier.\n\n        Args:\n            x (torch.Tensor): Tensor representing a pointcloud\n                (shape: :math:`B \\times N \\times D`, where :math:`B`\n                is the batchsize, :math:`N` is the number of points\n                in the pointcloud, and :math:`D` is the dimensionality\n                of each point in the pointcloud).\n                If self.transposed_input is True, then the shape is\n                :math:`B \\times D \\times N`.\n\n        """"""\n        x = self.feature_extractor(x)\n        for idx in range(len(self.linear_layers) - 1):\n            if self.batchnorm:\n                x = self.activation(self.bn_layers[idx](\n                    self.linear_layers[idx](x)))\n            else:\n                x = self.activation(self.linear_layers[idx](x))\n        # For penultimate linear layer, apply dropout before batchnorm\n        if self.dropout:\n            if self.batchnorm:\n                x = self.activation(self.bn_layers[-1](self.dropout(\n                    self.linear_layers[-1](x))))\n            else:\n                x = self.activation(self.dropout(self.linear_layers[-1](x)))\n        else:\n            if self.batchnorm:\n                x = self.activation(self.bn_layers[-1](\n                    self.linear_layers[-1](x)))\n            else:\n                x = self.activation(self.linear_layers[-1](x))\n        # TODO: Use dropout before batchnorm of penultimate linear layer\n        x = self.last_linear_layer(x)\n        # return F.log_softmax(x, dim=1)\n        return x\n\n\nclass PointNetSegmenter(nn.Module):\n    r""""""PointNet segmenter. Uses the PointNet feature extractor, and\n    adds per-point segmentation layers on top.\n\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n\n        .. code-block::\n\n            @article{qi2016pointnet,\n              title={PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation},\n              author={Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},\n              journal={arXiv preprint arXiv:1612.00593},\n              year={2016}\n            }\n\n    Args:\n        in_channels (int): Number of channels in the input pointcloud\n            (default: 3, for X, Y, Z coordinates respectively).\n        feat_size (int): Size of the global feature vector\n            (default: 1024)\n        num_classes (int): Number of classes (for the segmentation\n            task) (default: 2).\n        dropout (float): Dropout ratio to use (default: 0.). Note: If\n            the ratio is set to 0., we altogether skip using a dropout\n            layer.\n        layer_dims (Iterable[int]): Sizes of fully connected layers\n            to be used in the feature extractor (excluding the input and\n            the output layer sizes). Note: the number of\n            layers in the feature extractor is implicitly parsed from\n            this variable.\n        activation (function): Nonlinearity to be used as activation\n            function after each batchnorm (default: F.relu)\n        batchnorm (bool): Whether or not to use batchnorm layers\n            (default: True)\n        transposed_input (bool): Whether the input\'s second and third dimension\n            is already transposed. If so, a transpose operation can be avoided,\n            improving performance.\n            See documentation of PointNetFeatureExtractor for more details.\n\n    Example:\n\n        >>> pointnet = PointNetSegmenter(in_channels=6, feat_size=1024, \\\n                                         feat_layer_dims=[32, 64, 256], \\\n                                         classifier_layer_dims=[500, 200, 100])\n        >>> x = torch.rand(5, 6, 30)\n        >>> y = pointnet(x)\n        >>> print(y.shape)\n\n    """"""\n\n    def __init__(self,\n                 in_channels: int = 3,\n                 feat_size: int = 1024,\n                 num_classes: int = 2,\n                 dropout: float = 0.,\n                 classifier_layer_dims: Iterable[int] = [512, 256],\n                 feat_layer_dims: Iterable[int] = [64, 128],\n                 activation=F.relu,\n                 batchnorm: bool = True,\n                 transposed_input: bool = False):\n        super(PointNetSegmenter, self).__init__()\n\n        if not isinstance(num_classes, int):\n            raise TypeError(\'Argument num_classes must be of type int. \'\n                            \'Got {0} instead.\'.format(type(num_classes)))\n        if not isinstance(dropout, float):\n            raise TypeError(\'Argument dropout must be of type float. \'\n                            \'Got {0} instead.\'.format(type(dropout)))\n        if not hasattr(classifier_layer_dims, \'__iter__\'):\n            raise TypeError(\'Argument classifier_layer_dims is not iterable.\')\n        for idx, layer_dim in enumerate(classifier_layer_dims):\n            if not isinstance(layer_dim, int):\n                raise TypeError(\'Expected classifier_layer_dims to contain \'\n                                \'int. Found type {0} at index {1}.\'.format(\n                                    type(layer_dim), idx))\n\n        # Add feat_size to the head of classifier_layer_dims (the output of\n        # the PointNet feature extractor has number of elements equal to\n        # has number of channels equal to `in_channels`).\n        if not isinstance(classifier_layer_dims, list):\n            classifier_layer_dims = list(classifier_layer_dims)\n        classifier_layer_dims.insert(0, feat_size)\n\n        # Note that `global_feat` MUST be set to False, for\n        # segmentation tasks.\n        self.feature_extractor = PointNetFeatureExtractor(\n            in_channels=in_channels, feat_size=feat_size,\n            layer_dims=feat_layer_dims, global_feat=False,\n            activation=activation, batchnorm=batchnorm,\n            transposed_input=transposed_input\n        )\n\n        # Compute the dimensionality of local features\n        # Local feature size = (global feature size) + (feature size\n        #       from the output of the first layer of feature extractor)\n        # Note: In self.feature_extractor, we manually append in_channels\n        # to the head of feat_layer_dims. So, we use index 1 of\n        # feat_layer_dims in the below line, to compute local_feat_size,\n        # and not index 0.\n        self.local_feat_size = feat_size + feat_layer_dims[1]\n\n        self.conv_layers = nn.ModuleList()\n        if batchnorm:\n            self.bn_layers = nn.ModuleList()\n        # First classifier layer\n        self.conv_layers.append(nn.Conv1d(self.local_feat_size,\n                                          classifier_layer_dims[0], 1))\n        if batchnorm:\n            self.bn_layers.append(nn.BatchNorm1d(classifier_layer_dims[0]))\n        for idx in range(len(classifier_layer_dims) - 1):\n            self.conv_layers.append(nn.Conv1d(classifier_layer_dims[idx],\n                                              classifier_layer_dims[idx + 1], 1))\n            if batchnorm:\n                self.bn_layers.append(nn.BatchNorm1d(\n                    classifier_layer_dims[idx + 1]))\n\n        self.last_conv_layer = nn.Conv1d(classifier_layer_dims[-1],\n                                         num_classes, 1)\n\n        # Store activation as a class attribute\n        self.activation = activation\n\n        # Store the number of classes as an attribute\n        self.num_classes = num_classes\n\n        # Store whether or not to use batchnorm as a class attribute\n        self.batchnorm = batchnorm\n\n        self.transposed_input = transposed_input\n\n    def forward(self, x):\n        r""""""Forward pass through the PointNet segmentation model.\n\n        Args:\n            x (torch.Tensor): Tensor representing a pointcloud\n                shape: :math:`B \\times N \\times D`, where :math:`B`\n                is the batchsize, :math:`N` is the number of points\n                in the pointcloud, and :math:`D` is the dimensionality\n                of each point in the pointcloud.\n                If self.transposed_input is True, then the shape is\n                :math:`B \\times D \\times N`.\n\n        """"""\n        batchsize = x.shape[0]\n        num_points = x.shape[2] if self.transposed_input else x.shape[1]\n        x = self.feature_extractor(x)\n        for idx in range(len(self.conv_layers)):\n            if self.batchnorm:\n                x = self.activation(self.bn_layers[idx](\n                    self.conv_layers[idx](x)))\n            else:\n                x = self.activation(self.conv_layers[idx](x))\n        x = self.last_conv_layer(x)\n        x = x.transpose(2, 1).contiguous()\n        # x = F.log_softmax(x.view(-1, self.num_classes), dim=-1)\n        print(""x.shape = {}"".format(x.shape))\n        return x.view(batchsize, num_points, self.num_classes)\n'"
kaolin/models/PointNet2.py,64,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n# pointnet2\n#\n# Copyright (c) 2017, Geometric Computation Group of Stanford University\n#\n# The MIT License (MIT)\n#\n# Copyright (c) 2017 Charles R. Qi\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .PointNet import PointNetFeatureExtractor\n\nimport kaolin.cuda as ext\nimport kaolin.cuda.ball_query\nimport kaolin.cuda.furthest_point_sampling\nimport kaolin.cuda.three_nn\n\n\nclass FurthestPointSampling(torch.autograd.Function):\n    r""""""\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @article{qi2017pointnet2,\n                title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},\n                author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},\n                year = {2017},\n                journal={arXiv preprint arXiv:1706.02413},\n            }\n    """"""\n\n    @staticmethod\n    def forward(ctx, xyz, num_points_out):\n        """"""Uses iterative furthest point sampling to select a set of num_points_out features that have the largest minimum distance.\n\n        Args:\n            xyz (torch.Tensor): (B, N, 3) tensor where N > num_points_out\n            num_points_out (int32): number of features in the sampled set\n\n        Returns:\n            (torch.Tensor): (B, num_points_out) tensor containing the set\n        """"""\n        return ext.furthest_point_sampling.furthest_point_sampling(xyz, num_points_out)\n\n    @staticmethod\n    def backward(xyz, a=None):\n        return None, None\n\n\nfurthest_point_sampling = FurthestPointSampling.apply\n\n\nclass FPSGatherByIndex(torch.autograd.Function):\n    r""""""\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @article{qi2017pointnet2,\n                title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},\n                author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},\n                year = {2017},\n                journal={arXiv preprint arXiv:1706.02413},\n            }\n    """"""\n    @staticmethod\n    def forward(ctx, features, idx):\n        """"""TODO: documentation (and the ones below)\n        Args:\n            features (torch.Tensor): (B, C, N) tensor\n\n            idx (torch.Tensor): (B, npoint) tensor of the features to gather\n\n        Returns:\n            (torch.Tensor): (B, C, npoint) tensor\n        """"""\n\n        _, C, N = features.size()\n\n        ctx.for_backwards = (idx, C, N)\n\n        return ext.furthest_point_sampling.gather_by_index(features, idx)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        idx, C, N = ctx.for_backwards\n\n        grad_features = ext.furthest_point_sampling.gather_by_index_grad(\n            grad_out.contiguous(), idx, N)\n        return grad_features, None\n\n\nfps_gather_by_index = FPSGatherByIndex.apply\n\n\nclass ThreeNN(torch.autograd.Function):\n    r""""""\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @article{qi2017pointnet2,\n                title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},\n                author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},\n                year = {2017},\n                journal={arXiv preprint arXiv:1706.02413},\n            }\n    """"""\n    @staticmethod\n    def forward(ctx, unknown, known):\n        # type: (Any, torch.Tensor, torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]\n        r""""""\n            Find the three nearest neighbors of unknown in known\n        Parameters\n        ----------\n        unknown : torch.Tensor\n            (B, n, 3) tensor of known features\n        known : torch.Tensor\n            (B, m, 3) tensor of unknown features\n\n        Returns\n        -------\n        dist : torch.Tensor\n            (B, n, 3) l2 distance to the three nearest neighbors\n        idx : torch.Tensor\n            (B, n, 3) index of 3 nearest neighbors\n        """"""\n        dist2, idx = ext.three_nn.three_nn(unknown, known)\n\n        return torch.sqrt(dist2), idx\n\n    @staticmethod\n    def backward(ctx, a=None, b=None):\n        return None, None\n\n\nthree_nn = ThreeNN.apply\n\n\nclass ThreeInterpolate(torch.autograd.Function):\n    r""""""\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @article{qi2017pointnet2,\n                title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},\n                author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},\n                year = {2017},\n                journal={arXiv preprint arXiv:1706.02413},\n            }\n    """"""\n    @staticmethod\n    def forward(ctx, features, idx, weight):\n        # type(Any, torch.Tensor, torch.Tensor, torch.Tensor) -> Torch.Tensor\n        r""""""\n            Performs weight linear interpolation on 3 features\n        Parameters\n        ----------\n        features : torch.Tensor\n            (B, c, m) Features descriptors to be interpolated from\n        idx : torch.Tensor\n            (B, n, 3) three nearest neighbors of the target features in features\n        weight : torch.Tensor\n            (B, n, 3) weights\n\n        Returns\n        -------\n        torch.Tensor\n            (B, c, n) tensor of the interpolated features\n        """"""\n        B, c, m = features.size()\n        n = idx.size(1)\n\n        ctx.three_interpolate_for_backward = (idx, weight, m)\n\n        return ext.three_nn.three_interpolate(features, idx, weight)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        # type: (Any, torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n        r""""""\n        Parameters\n        ----------\n        grad_out : torch.Tensor\n            (B, c, n) tensor with gradients of ouputs\n\n        Returns\n        -------\n        grad_features : torch.Tensor\n            (B, c, m) tensor with gradients of features\n\n        None\n\n        None\n        """"""\n        idx, weight, m = ctx.three_interpolate_for_backward\n\n        grad_features = ext.three_nn.three_interpolate_grad(\n            grad_out.contiguous(), idx, weight, m\n        )\n\n        return grad_features, None, None\n\n\nthree_interpolate = ThreeInterpolate.apply\n\n\nclass GroupGatherByIndex(torch.autograd.Function):\n    r""""""\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @article{qi2017pointnet2,\n                title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},\n                author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},\n                year = {2017},\n                journal={arXiv preprint arXiv:1706.02413},\n            }\n    """"""\n    @staticmethod\n    def forward(ctx, features, idx):\n        # type: (Any, torch.Tensor, torch.Tensor) -> torch.Tensor\n        r""""""\n\n        Parameters\n        ----------\n        features : torch.Tensor\n            (B, C, N) tensor of features to group\n        idx : torch.Tensor\n            (B, npoint, nsample) tensor containing the indicies of features to group with\n\n        Returns\n        -------\n        torch.Tensor\n            (B, C, npoint, nsample) tensor\n        """"""\n        B, nfeatures, nsample = idx.size()\n        _, C, N = features.size()\n\n        ctx.for_backwards = (idx, N)\n\n        return ext.ball_query.gather_by_index(features, idx)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        # type: (Any, torch.tensor) -> Tuple[torch.Tensor, torch.Tensor]\n        r""""""\n\n        Parameters\n        ----------\n        grad_out : torch.Tensor\n            (B, C, npoint, nsample) tensor of the gradients of the output from forward\n\n        Returns\n        -------\n        torch.Tensor\n            (B, C, N) gradient of the features\n        None\n        """"""\n        idx, N = ctx.for_backwards\n\n        grad_features = ext.ball_query.gather_by_index_grad(\n            grad_out.contiguous(), idx, N)\n\n        return grad_features, None\n\n\ngroup_gather_by_index = GroupGatherByIndex.apply\n\n\nclass BallQuery(torch.autograd.Function):\n    r""""""\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @article{qi2017pointnet2,\n                title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},\n                author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},\n                year = {2017},\n                journal={arXiv preprint arXiv:1706.02413},\n            }\n    """"""\n\n    @staticmethod\n    def forward(ctx, radius, nsample, xyz, new_xyz, use_random=False):\n        # type: (Any, float, int, torch.Tensor, torch.Tensor) -> torch.Tensor\n        r""""""\n        TODO: documentation\n\n        Parameters\n        ----------\n        radius : float\n            radius of the balls\n        nsample : int\n            maximum number of features in the balls\n        xyz : torch.Tensor\n            (B, N, 3) xyz coordinates of the features\n        new_xyz : torch.Tensor\n            (B, npoint, 3) centers of the ball query\n\n        Returns\n        -------\n        torch.Tensor\n            (B, npoint, nsample) tensor with the indicies of the features that form the query balls\n        """"""\n        if use_random:\n            return ext.ball_query.ball_random_query(\n                torch.randint(int(1e9), ()).item(), new_xyz, xyz, radius,\n                nsample)\n\n        return ext.ball_query.ball_query(new_xyz, xyz, radius, nsample)\n\n    @staticmethod\n    def backward(ctx, a=None):\n        return None, None, None, None\n\n\nball_query = BallQuery.apply\n\n# TODO: improvement: experiment with random sampling instead of current approach.\n\n\ndef separate_xyz_and_features(points):\n    """"""Break up a point cloud into position vectors (first 3 dimensions) and feature vectors.\n\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @article{qi2017pointnet2,\n                title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},\n                author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},\n                year = {2017},\n                journal={arXiv preprint arXiv:1706.02413},\n            }\n\n    Args:\n        points (torch.Tensor): shape = (batch_size, num_points, 3 + num_features)\n            The point cloud to separate.\n\n    Returns:\n        xyz (torch.Tensor): shape = (batch_size, num_points, 3)\n            The position vectors of the points.\n        features (torch.Tensor|None): shape = (batch_size, num_features, num_points)\n            The feature vectors of the points.\n            If there are no feature vectors, features will be None.\n    """"""\n    assert (len(points.shape) == 3 and points.shape[2] >= 3), (\n        \'Expected shape of points to be (batch_size, num_points, 3 + num_features), got {}\'\n        .format(points.shape))\n\n    xyz = points[:, :, 0:3].contiguous()\n    features = (points[:, :, 3:].transpose(1, 2).contiguous()\n                if points.shape[2] > 3 else None)\n\n    return xyz, features\n\n\nclass PointNet2GroupingLayer(nn.Module):\n    """"""\n    TODO: documentation: if radius is None, then group everything\n\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @article{qi2017pointnet2,\n                title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},\n                author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},\n                year = {2017},\n                journal={arXiv preprint arXiv:1706.02413},\n            }\n    """"""\n\n    def __init__(self, radius, num_samples, use_xyz_feature=True, use_random_ball_query=False):\n        super(PointNet2GroupingLayer, self).__init__()\n        self.radius = radius\n        self.num_samples = num_samples\n        self.use_xyz_feature = use_xyz_feature\n        self.use_random_ball_query = use_random_ball_query\n\n    def forward(self, xyz, new_xyz, features=None):\n        r""""""\n        Parameters\n        ----------\n        xyz : torch.Tensor\n            xyz coordinates of the features (B, N, 3)\n        new_xyz : torch.Tensor\n            centriods (B, npoint, 3)\n        features : torch.Tensor\n            Descriptors of the features (B, C, N)\n\n        Returns\n        -------\n        new_features : torch.Tensor\n            (B, 3 + C, npoint, nsample) tensor\n        """"""\n\n        if self.radius is None:\n            grouped_xyz = xyz.transpose(1, 2)\n            if features is not None:\n                grouped_features = features\n                if self.use_xyz_feature:\n                    new_features = torch.cat(\n                        [grouped_xyz, grouped_features], dim=1\n                    )  # (B, 3 + C, 1, N)\n                else:\n                    new_features = grouped_features\n            else:\n                new_features = grouped_xyz\n\n            return new_features\n\n        else:\n            idx = ball_query(self.radius, self.num_samples, xyz,\n                             new_xyz, self.use_random_ball_query)\n            xyz_trans = xyz.transpose(1, 2).contiguous()\n            grouped_xyz = group_gather_by_index(\n                xyz_trans, idx)  # (B, 3, npoint, nsample)\n            grouped_xyz -= new_xyz.transpose(1, 2).unsqueeze(-1)\n\n            if features is not None:\n                grouped_features = group_gather_by_index(features, idx)\n                if self.use_xyz_feature:\n                    new_features = torch.cat(\n                        [grouped_xyz, grouped_features], dim=1\n                    )  # (B, C + 3, npoint, nsample)\n                else:\n                    new_features = grouped_features\n            else:\n                assert self.use_xyz_feature, ""Must have at least one feature or set use_xyz_feature = True""\n                new_features = grouped_xyz\n\n            return new_features.transpose(1, 2).contiguous()\n\n\nclass PointNet2SetAbstraction(nn.Module):\n    """"""A single set-abstraction layer for the PointNet++ architecture.\n    Supports multi-scale grouping (MSG).\n\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @article{qi2017pointnet2,\n                title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},\n                author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},\n                year = {2017},\n                journal={arXiv preprint arXiv:1706.02413},\n            }\n\n    Args:\n        num_points_out (int|None): The number of output points.\n            If None, group all points together.\n\n        pointnet_in_features (int): The number of features to input into pointnet.\n            Note: if use_xyz_feature is true, this value will be increased by 3.\n\n        pointnet_layer_dims_list (List[List[int]]): The pointnet MLP dimensions list for each scale.\n            Note: the first (input) dimension SHOULD NOT be included in each list,\n            while the last (output) dimension SHOULD be included in each list.\n\n        radii_list (List[float]|None): The grouping radius for each scale.\n            If num_points_out is None, this value is ignored.\n\n        num_samples_list (List[int]|None): The number of samples in each ball query for each scale.\n            If num_points_out is None, this value is ignored.\n\n        batchnorm (bool): Whether or not to use batch normalization.\n\n        use_xyz_feature (bool): Whether or not to use the coordinates of the\n            points as feature.\n\n        use_random_ball_query (bool): Whether or not to use random sampling when\n            there are too many points per ball.\n    """"""\n\n    def __init__(self,\n                 num_points_out,\n                 pointnet_in_features,\n                 pointnet_layer_dims_list,\n                 radii_list=None,\n                 num_samples_list=None,\n                 batchnorm=True,\n                 use_xyz_feature=True,\n                 use_random_ball_query=False):\n\n        super(PointNet2SetAbstraction, self).__init__()\n\n        # TODO: Testing: test if the model works with each of the parameters\n\n        if num_points_out is None:\n            radii_list = [None]\n            num_samples_list = [None]\n        else:\n            assert isinstance(radii_list, list) and isinstance(\n                num_samples_list, list), \'radii_list and num_samples_list must be lists\'\n\n        assert (len(radii_list) == len(num_samples_list) == len(pointnet_layer_dims_list)), (\n            \'Dimension of radii_list ({}), num_samples_list ({}), pointnet_layer_dims_list ({}) must match\'\n            .format(len(radii_list), len(num_samples_list), len(pointnet_layer_dims_list)))\n\n        self.num_points_out = num_points_out\n        self.pointnet_layer_dims_list = pointnet_layer_dims_list\n        self.sub_modules = nn.ModuleList()\n        self.layers = []\n        self.pointnet_in_channels = pointnet_in_features + \\\n            (3 if use_xyz_feature else 0)\n\n        num_scales = len(radii_list)\n        for i in range(num_scales):\n            radius = radii_list[i]\n            num_samples = num_samples_list[i]\n            pointnet_layer_dims = pointnet_layer_dims_list[i]\n\n            assert isinstance(pointnet_layer_dims, list), \'Each pointnet_layer_dims must be a list, got {} instead\'.format(\n                pointnet_layer_dims)\n            assert len(\n                pointnet_layer_dims) > 0, \'Each pointnet_layer_dims must have at least one element\'\n\n            grouper = PointNet2GroupingLayer(\n                radius, num_samples, use_xyz_feature=use_xyz_feature,\n                use_random_ball_query=use_random_ball_query)\n\n            # TODO: refactor: add dropout parameters\n            pointnet = PointNetFeatureExtractor(\n                in_channels=self.pointnet_in_channels,\n                feat_size=pointnet_layer_dims[-1],\n                layer_dims=pointnet_layer_dims[:-1],\n                global_feat=True,\n                batchnorm=batchnorm,\n                transposed_input=True\n            )\n\n            # Register sub-modules\n            self.sub_modules.append(grouper)\n            self.sub_modules.append(pointnet)\n\n            self.layers.append((grouper, pointnet, num_samples))\n\n    def forward(self, xyz, features=None):\n        """"""\n        Args:\n            xyz (torch.Tensor): shape = (batch_size, num_points_in, 3)\n                The 3D coordinates of each point.\n\n            features (torch.Tensor|None): shape = (batch_size, num_features, num_points_in)\n                The features of each point.\n\n        Returns:\n            new_xyz (torch.Tensor|None): shape = (batch_size, num_points_out, 3)\n                The new coordinates of the grouped points.\n                If self.num_points_out is None, new_xyz will be None.\n\n            new_features (torch.Tensor): shape = (batch_size, out_num_features, num_points_out)\n                The features of each output point.\n                If self.num_points_out is None, new_features will have shape:\n                (batch_size, num_features_out)\n        """"""\n        batch_size = xyz.shape[0]\n\n        new_xyz = None\n        if self.num_points_out is not None:\n            # TODO: implement: this is flipped here for some reason\n            new_xyz_idx = furthest_point_sampling(xyz, self.num_points_out)\n            new_xyz = fps_gather_by_index(\n                xyz.transpose(1, 2).contiguous(), new_xyz_idx)\n            new_xyz = new_xyz.transpose(1, 2).contiguous()\n\n        new_features_list = []\n        for grouper, pointnet, num_samples in self.layers:\n            new_features = grouper(xyz, new_xyz, features)\n            # shape = (batch_size, num_points_out, self.pointnet_in_channels, num_samples)\n            # if num_points_out is None:\n            # shape = (batch_size, self.pointnet_in_channels, num_samples)\n\n            if self.num_points_out is not None:\n                new_features = new_features.view(-1,\n                                                 self.pointnet_in_channels, num_samples)\n\n            new_features = pointnet(new_features)\n            # shape = (batch_size * num_points_out, feat_size)\n            # if num_points_out is None:\n            # shape = (batch_size, feat_size)\n\n            # TODO: Optimization: avoid this packing and unpacking step by refactoring and generalizing pointnet\n            if self.num_points_out is not None:\n                new_features = new_features.view(\n                    batch_size, self.num_points_out, -1).transpose(1, 2)\n                # shape = (batch_size, feat_size, num_points_out)\n\n            new_features_list.append(new_features)\n\n        new_features = torch.cat(new_features_list, dim=1)\n        # shape = (batch_size, num_features_out, num_points_out)\n        # if num_points_out is None:\n        # shape = (batch_size, num_features_out)\n\n        return new_xyz, new_features\n\n    def get_num_features_out(self):\n        return sum([lst[-1] for lst in self.pointnet_layer_dims_list])\n\n\nclass PointNet2FeaturePropagator(nn.Module):\n    """"""A single feature-propagation layer for the PointNet++ architecture.\n\n    Used for segmentation.\n\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @article{qi2017pointnet2,\n                title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},\n                author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},\n                year = {2017},\n                journal={arXiv preprint arXiv:1706.02413},\n            }\n\n    Args:\n        num_features (int): The number of features in the current layer.\n            Note: this is the number of output features of the corresponding\n            set abstraction layer.\n\n        num_features_prev (int): The number of features from the previous\n            feature propagation layer (corresponding to the next layer during\n            feature extraction).\n            Note: this is the number of output features of the previous feature\n            propagation layer (or the number of output features of the final set\n            abstraction layer, if this is the very first feature propagation\n            layer)\n\n        layer_dims (List[int]): Sizes of the MLP layer.\n            Note: the first (input) dimension SHOULD NOT be included in the list,\n            while the last (output) dimension SHOULD be included in the list.\n\n        batchnorm (bool): Whether or not to use batch normalization.\n    """"""\n\n    def __init__(self, num_features, num_features_prev, layer_dims, batchnorm=True):\n        super(PointNet2FeaturePropagator, self).__init__()\n\n        self.layer_dims = layer_dims\n\n        unit_pointnets = []\n        in_features = num_features + num_features_prev\n        for out_features in layer_dims:\n            unit_pointnets.append(\n                nn.Conv1d(in_features, out_features, 1))\n\n            if batchnorm:\n                unit_pointnets.append(nn.BatchNorm1d(out_features))\n\n            unit_pointnets.append(nn.ReLU())\n            in_features = out_features\n\n        self.unit_pointnet = nn.Sequential(*unit_pointnets)\n\n    def forward(self, xyz, xyz_prev, features=None, features_prev=None):\n        """"""\n        Args:\n            xyz (torch.Tensor): shape = (batch_size, num_points, 3)\n                The 3D coordinates of each point at current layer,\n                computed during feature extraction (i.e. set abstraction).\n\n            xyz_prev (torch.Tensor|None): shape = (batch_size, num_points_prev, 3)\n                The 3D coordinates of each point from the previous feature\n                propagation layer (corresponding to the next layer during\n                feature extraction).\n                This value can be None (i.e. for the very first propagator layer).\n\n            features (torch.Tensor|None): shape = (batch_size, num_features, num_points)\n                The features of each point at current layer,\n                computed during feature extraction (i.e. set abstraction).\n\n            features_prev (torch.Tensor|None): shape = (batch_size, num_features_prev, num_points_prev)\n                The features of each point from the previous feature\n                propagation layer (corresponding to the next layer during\n                feature extraction).\n\n        Returns:\n            (torch.Tensor): shape = (batch_size, num_features_out, num_points)\n        """"""\n        num_points = xyz.shape[1]\n        if xyz_prev is None:  # Very first feature propagation layer\n            new_features = features_prev.expand(\n                *(features.shape + [num_points]))\n\n        else:\n            dist, idx = three_nn(xyz, xyz_prev)\n            # shape = (batch_size, num_points, 3), (batch_size, num_points, 3)\n            inverse_dist = 1.0 / (dist + 1e-8)\n            total_inverse_dist = torch.sum(inverse_dist, dim=2, keepdim=True)\n            weights = inverse_dist / total_inverse_dist\n            new_features = three_interpolate(features_prev, idx, weights)\n            # shape = (batch_size, num_features_prev, num_points)\n\n        if features is not None:\n            new_features = torch.cat([new_features, features], dim=1)\n\n        return self.unit_pointnet(new_features)\n\n    def get_num_features_out(self):\n        return self.layer_dims[-1]\n\n\nclass PointNet2Classifier(nn.Module):\n    r""""""PointNet++ classification network.\n\n    Based on the original PointNet++ paper.\n\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @article{qi2017pointnet2,\n                title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},\n                author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},\n                year = {2017},\n                journal={arXiv preprint arXiv:1706.02413},\n            }\n\n    Args:\n        in_features (int): Number of features (not including xyz coordinates) in\n            the input point cloud (default: 0).\n        num_classes (int): Number of classes (for the classification\n            task) (default: 2).\n        batchnorm (bool): Whether or not to use batch normalization.\n            (default: True)\n        use_xyz_feature (bool): Whether or not to use the coordinates of the\n            points as feature.\n        use_random_ball_query (bool): Whether or not to use random sampling when\n            there are too many points per ball.\n\n    TODO: Documentation: add example\n\n    """"""\n\n    # TODO: Implement: ssg\n\n    def __init__(self,\n                 in_features=0,\n                 num_classes=2,\n                 batchnorm=True,\n                 use_xyz_feature=True,\n                 use_random_ball_query=False):\n\n        super(PointNet2Classifier, self).__init__()\n\n        self.set_abstractions = nn.ModuleList()\n\n        self.set_abstractions.append(\n            PointNet2SetAbstraction(\n                num_points_out=512,\n                pointnet_in_features=in_features,\n                pointnet_layer_dims_list=[\n                    [32, 32, 64],\n                    [64, 64, 128],\n                    [64, 96, 128],\n                ],\n                radii_list=[0.1, 0.2, 0.4],\n                num_samples_list=[16, 32, 128],\n                batchnorm=batchnorm,\n                use_xyz_feature=use_xyz_feature,\n                use_random_ball_query=use_random_ball_query\n            )\n        )\n\n        self.set_abstractions.append(\n            PointNet2SetAbstraction(\n                num_points_out=128,\n                pointnet_in_features=self.set_abstractions[-1].get_num_features_out(\n                ),\n                pointnet_layer_dims_list=[\n                    [64, 64, 128],\n                    [128, 128, 256],\n                    [128, 128, 256],\n                ],\n                radii_list=[0.2, 0.4, 0.8],\n                num_samples_list=[32, 64, 128],\n                batchnorm=batchnorm,\n                use_xyz_feature=use_xyz_feature,\n                use_random_ball_query=use_random_ball_query\n            )\n        )\n\n        self.set_abstractions.append(\n            PointNet2SetAbstraction(\n                num_points_out=None,\n                pointnet_in_features=self.set_abstractions[-1].get_num_features_out(\n                ),\n                pointnet_layer_dims_list=[\n                    [256, 512, 1024],\n                ],\n                batchnorm=batchnorm,\n                use_xyz_feature=use_xyz_feature,\n                use_random_ball_query=use_random_ball_query\n            )\n        )\n\n        final_layer_modules = [\n            module for module in [\n                nn.Linear(\n                    self.set_abstractions[-1].get_num_features_out(), 512),\n                nn.BatchNorm1d(512) if batchnorm else None,\n                nn.ReLU(),\n                nn.Dropout(0.5),\n                nn.Linear(512, 256),\n                nn.BatchNorm1d(256) if batchnorm else None,\n                nn.ReLU(),\n                nn.Dropout(0.5),\n                nn.Linear(256, num_classes)\n            ] if module is not None\n        ]\n        self.final_layers = nn.Sequential(*final_layer_modules)\n\n    def forward(self, points):\n        """"""\n        Args:\n            points (torch.Tensor): shape = (batch_size, num_points, 3 + in_features)\n                The points to classify.\n\n        Returns:\n            (torch.Tensor): shape = (batch_size, num_classes)\n                The score of the inputs being in each class.\n                Note: no softmax or logsoftmax will be applied.\n        """"""\n        xyz, features = separate_xyz_and_features(points)\n\n        for module in self.set_abstractions:\n            xyz, features = module(xyz, features)\n\n        return self.final_layers(features)\n\n\nclass PointNet2Segmenter(nn.Module):\n    """"""PointNet++ classification network.\n\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @article{qi2017pointnet2,\n                title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},\n                author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},\n                year = {2017},\n                journal={arXiv preprint arXiv:1706.02413},\n            }\n\n    Args:\n        in_features (int): Number of features (not including xyz coordinates) in\n            the input point cloud (default: 0).\n        num_classes (int): Number of classes (for the classification\n            task) (default: 2).\n        batchnorm (bool): Whether or not to use batch normalization.\n            (default: True)\n        use_xyz_feature (bool): Whether or not to use the coordinates of the\n            points as feature.\n        use_random_ball_query (bool): Whether or not to use random sampling when\n            there are too many points per ball.\n\n    TODO: Documentation: add example\n\n    """"""\n\n    def __init__(self,\n                 in_features=0,\n                 num_classes=2,\n                 batchnorm=True,\n                 use_xyz_feature=True,\n                 use_random_ball_query=False):\n\n        super(PointNet2Segmenter, self).__init__()\n\n        self.set_abstractions = nn.ModuleList()\n\n        self.set_abstractions.append(\n            PointNet2SetAbstraction(\n                num_points_out=1024,\n                pointnet_in_features=in_features,\n                pointnet_layer_dims_list=[\n                    [16, 16, 32],\n                    [32, 32, 64],\n                ],\n                radii_list=[0.05, 0.1],\n                num_samples_list=[16, 32],\n                batchnorm=batchnorm,\n                use_xyz_feature=use_xyz_feature,\n                use_random_ball_query=use_random_ball_query\n            )\n        )\n\n        self.set_abstractions.append(\n            PointNet2SetAbstraction(\n                num_points_out=256,\n                pointnet_in_features=self.set_abstractions[-1].get_num_features_out(\n                ),\n                pointnet_layer_dims_list=[\n                    [64, 64, 128],\n                    [64, 96, 128],\n                ],\n                radii_list=[0.1, 0.2],\n                num_samples_list=[16, 32],\n                batchnorm=batchnorm,\n                use_xyz_feature=use_xyz_feature,\n                use_random_ball_query=use_random_ball_query\n            )\n        )\n\n        self.set_abstractions.append(\n            PointNet2SetAbstraction(\n                num_points_out=64,\n                pointnet_in_features=self.set_abstractions[-1].get_num_features_out(\n                ),\n                pointnet_layer_dims_list=[\n                    [128, 196, 256],\n                    [128, 196, 256],\n                ],\n                radii_list=[0.2, 0.4],\n                num_samples_list=[16, 32],\n                batchnorm=batchnorm,\n                use_xyz_feature=use_xyz_feature,\n                use_random_ball_query=use_random_ball_query\n            )\n        )\n\n        self.set_abstractions.append(\n            PointNet2SetAbstraction(\n                num_points_out=16,\n                pointnet_in_features=self.set_abstractions[-1].get_num_features_out(\n                ),\n                pointnet_layer_dims_list=[\n                    [256, 256, 512],\n                    [256, 384, 512],\n                ],\n                radii_list=[0.4, 0.8],\n                num_samples_list=[16, 32],\n                batchnorm=batchnorm,\n                use_xyz_feature=use_xyz_feature,\n                use_random_ball_query=use_random_ball_query\n            )\n        )\n\n        self.feature_propagators = nn.ModuleList()\n\n        # TODO: implement: this is different from the original paper.\n\n        self.feature_propagators.append(\n            PointNet2FeaturePropagator(\n                num_features=self.set_abstractions[-2].get_num_features_out(),\n                num_features_prev=self.set_abstractions[-1].get_num_features_out(),\n                layer_dims=[512, 512],\n                batchnorm=batchnorm,\n            )\n        )\n\n        self.feature_propagators.append(\n            PointNet2FeaturePropagator(\n                num_features=self.set_abstractions[-3].get_num_features_out(),\n                num_features_prev=self.feature_propagators[-1].get_num_features_out(\n                ),\n                layer_dims=[512, 512],\n                batchnorm=batchnorm,\n            )\n        )\n\n        self.feature_propagators.append(\n            PointNet2FeaturePropagator(\n                num_features=self.set_abstractions[-4].get_num_features_out(),\n                num_features_prev=self.feature_propagators[-1].get_num_features_out(\n                ),\n                layer_dims=[256, 256],\n                batchnorm=batchnorm,\n            )\n        )\n\n        self.feature_propagators.append(\n            PointNet2FeaturePropagator(\n                num_features=in_features,\n                num_features_prev=self.feature_propagators[-1].get_num_features_out(\n                ),\n                layer_dims=[128, 128],\n                batchnorm=batchnorm,\n            )\n        )\n\n        final_layer_modules = [\n            module for module in [\n                nn.Conv1d(\n                    self.feature_propagators[-1].get_num_features_out(), 128, 1),\n                nn.BatchNorm1d(128) if batchnorm else None,\n                nn.ReLU(),\n                nn.Dropout(0.5),\n                nn.Conv1d(128, num_classes, 1)\n            ] if module is not None\n        ]\n        self.final_layers = nn.Sequential(*final_layer_modules)\n\n    def forward(self, points):\n        """"""\n        Args:\n            points (torch.Tensor): shape = (batch_size, num_points, 3 + in_features)\n                The points to perform segmentation on.\n\n        Returns:\n            (torch.Tensor): shape = (batch_size, num_points, num_classes)\n                The score of each point being in each class.\n                Note: no softmax or logsoftmax will be applied.\n        """"""\n        xyz, features = separate_xyz_and_features(points)\n\n        xyz_list, features_list = [xyz], [features]\n\n        for module in self.set_abstractions:\n            xyz, features = module(xyz, features)\n            xyz_list.append(xyz)\n            features_list.append(features)\n\n        target_index = -2\n        for module in self.feature_propagators:\n            features_list[target_index] = module(\n                xyz_list[target_index],\n                xyz_list[target_index + 1],\n                features_list[target_index],\n                features_list[target_index + 1])\n\n            target_index -= 1\n\n        return (self.final_layers(features_list[0])\n                .transpose(1, 2)\n                .contiguous())\n'"
kaolin/models/SimpleGCN.py,9,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\n\nimport torch \nfrom torch import nn \nfrom torch.nn.parameter import Parameter\n\n\nclass SimpleGCN(nn.Module):\n    r""""""A simple graph convolution layer, similar to the one defined in\n    Kipf et al. https://arxiv.org/abs/1609.02907\n\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n\n        .. code-block::\n\n            @article{kipf2016semi,\n              title={Semi-Supervised Classification with Graph Convolutional Networks},\n              author={Kipf, Thomas N and Welling, Max},\n              journal={arXiv preprint arXiv:1609.02907},\n              year={2016}\n            }\n\n    """"""\n\n    def __init__(self, in_features, out_features, bias=True):\n        super(SimpleGCN, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight1 = Parameter(torch.Tensor(in_features, out_features))\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_features))\n        else:\n            self.register_parameter(\'bias\', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 6. / math.sqrt((self.weight1.size(1) + self.weight1.size(0)))\n        stdv *= .6\n        self.weight1.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-.1, .1)\n\n    def forward(self, input, adj):\n        support = torch.mm(input, self.weight1)\n        side_len = max(support.shape[1] // 3, 2)\n        if adj.type() == \'torch.cuda.sparse.FloatTensor\': \n            norm = torch.sparse.mm(adj, torch.ones((support.shape[0], 1)).cuda())\n            normalized_support = support[:, :side_len] / norm\n            side_1 = torch.sparse.mm(adj, normalized_support)\n        else: \n            side_1 = torch.mm(adj, support[:, :side_len])\n\n        side_2 = support[:, side_len:]\n        output = torch.cat((side_1, side_2), dim=1)\n\n        if self.bias is not None:\n            output = output + self.bias\n        return output\n\n    def __repr__(self):\n        return self.__class__.__name__ + \' (\' \\\n            + str(self.in_features) + \' -> \' \\\n            + str(self.out_features) + \')\'\n'"
kaolin/models/VGG18.py,0,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom torch import nn \n\n\nclass VGG18(nn.Module):\n    r""""""\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n\n        .. code-block::\n\n            @InProceedings{Simonyan15,\n              author       = ""Karen Simonyan and Andrew Zisserman"",\n              title        = ""Very Deep Convolutional Networks for Large-Scale Image Recognition"",\n              booktitle    = ""International Conference on Learning Representations"",\n              year         = ""2015"",\n            }\n    """"""\n\n    def __init__(self):\n        super(VGG18, self).__init__()\n\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(4, 16, kernel_size=3, padding=1), \n            nn.BatchNorm2d(16), \n            nn.ReLU(inplace=True))\n\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n            nn.BatchNorm2d(16), \n            nn.ReLU(inplace=True))\n\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=3, padding=1, stride=2),\n            nn.BatchNorm2d(32), \n            nn.ReLU(inplace=True))\n\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32), \n            nn.ReLU(inplace=True))  \n\n        self.layer5 = nn.Sequential(\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32), \n            nn.ReLU(inplace=True))\n\n        self.layer6 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=2),\n            nn.BatchNorm2d(64), \n            nn.ReLU(inplace=True))\n\n        self.layer7 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64), \n            nn.ReLU(inplace=True))\n\n        self.layer8 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64), \n            nn.ReLU(inplace=True))\n\n        self.layer9 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=2),\n            nn.BatchNorm2d(128),    \n            nn.ReLU(inplace=True))\n\n        self.layer10 = nn.Sequential(\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),    \n            nn.ReLU(inplace=True))\n\n        self.layer11 = nn.Sequential(\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),    \n            nn.ReLU(inplace=True))\n\n        self.layer12 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=2),\n            nn.BatchNorm2d(256),    \n            nn.ReLU(inplace=True))\n\n        self.layer13 = nn.Sequential(\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True))\n\n        self.layer14 = nn.Sequential(\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True))\n\n        self.layer15 = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=2),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True))\n\n        self.layer16 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True))\n\n        self.layer17 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True))\n        self.layer18 = nn.Sequential(\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True))\n\n    def forward(self, tensor):\n        x = self.layer1(tensor)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        x = self.layer6(x)\n        x = self.layer7(x)\n        x = self.layer8(x)\n        A = x \n        x = self.layer9(x) \n        x = self.layer10(x)\n        x = self.layer11(x)\n        B = x \n        x = self.layer12(x)\n        x = self.layer13(x)\n        x = self.layer14(x)\n        C = x\n        x = self.layer15(x)\n        x = self.layer16(x)\n        x = self.layer17(x)\n        D = self.layer18(x)\n\n        return [A, B, C, D]\n'"
kaolin/models/Voxel3DIWGAN.py,32,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Voxel3DIWGenerator(nn.Module):\n    """"""TODO: Add docstring.\n\n    https://arxiv.org/abs/1707.09557\n\n    Input shape: B x 200\n    Output shape: B x 32 x 32 x 32\n\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @article{DBLP:journals/corr/SmithM17,\n              author    = {Edward J. Smith and\n                           David Meger},\n              title     = {Improved Adversarial Systems for 3D Object Generation and Reconstruction},\n              journal   = {CoRR},\n              volume    = {abs/1707.09557},\n              year      = {2017},\n              url       = {http://arxiv.org/abs/1707.09557},\n              archivePrefix = {arXiv},\n              eprint    = {1707.09557},\n              timestamp = {Mon, 13 Aug 2018 16:46:50 +0200},\n              biburl    = {https://dblp.org/rec/bib/journals/corr/SmithM17},\n              bibsource = {dblp computer science bibliography, https://dblp.org}\n            }\n    """"""\n\n    def __init__(self):\n\n        super(Voxel3DIWGenerator, self).__init__()\n\n        self.linear = nn.Linear(200, 256 * 2 * 2 * 2)\n        self.post_linear = torch.nn.Sequential(\n            torch.nn.BatchNorm3d(256),\n            torch.nn.ReLU()\n        )\n\n        self.layer1 = torch.nn.Sequential(\n            torch.nn.ConvTranspose3d(256, 256, kernel_size=4, stride=2,\n                                     padding=(1, 1, 1)),\n            torch.nn.BatchNorm3d(256),\n            torch.nn.ReLU()\n        )\n        self.layer2 = torch.nn.Sequential(\n            torch.nn.ConvTranspose3d(256, 128, kernel_size=4, stride=2,\n                                     padding=(1, 1, 1)),\n            torch.nn.BatchNorm3d(128),\n            torch.nn.ReLU()\n        )\n        self.layer3 = torch.nn.Sequential(\n            torch.nn.ConvTranspose3d(128, 64, kernel_size=4, stride=2,\n                                     padding=(1, 1, 1)),\n            torch.nn.BatchNorm3d(64),\n            torch.nn.ReLU()\n        )\n        self.layer4 = torch.nn.Sequential(\n            torch.nn.ConvTranspose3d(64, 1, kernel_size=4, stride=2,\n                                     padding=(1, 1, 1))\n        )\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = x.view(-1, 256, 2, 2, 2)\n        x = self.post_linear(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = x.squeeze(1)\n        x = torch.tanh(x[:, :32, :32, :32])\n        return x\n\n\nclass Voxel3DIWDiscriminator(nn.Module):\n    """"""TODO: Add docstring.\n\n    https://arxiv.org/abs/1707.09557\n\n    Input shape: B x 32 x 32 x 32\n    Output shape: B x 1\n    """"""\n\n    def __init__(self):\n\n        super(Voxel3DIWDiscriminator, self).__init__()\n\n        self.layer1 = torch.nn.Sequential(\n            torch.nn.Conv3d(1, 32, kernel_size=4, stride=2),\n            torch.nn.LeakyReLU(.2)\n        )\n        self.layer2 = torch.nn.Sequential(\n            torch.nn.Conv3d(32, 64, kernel_size=4, stride=2),\n            torch.nn.LeakyReLU(.2)\n        )\n        self.layer3 = torch.nn.Sequential(\n            torch.nn.Conv3d(64, 128, kernel_size=4, stride=2),\n            torch.nn.LeakyReLU(.2)\n        )\n        self.layer4 = torch.nn.Sequential(\n            torch.nn.Conv3d(128, 256, kernel_size=2, stride=2),\n            torch.nn.LeakyReLU(.2)\n        )\n        self.layer5 = nn.Linear(256, 1)\n\n    def forward(self, x):\n        x = x.view(-1, 1, 32, 32, 32)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = x.view(x.shape[0], -1)\n        x = self.layer5(x)\n        return x\n'"
kaolin/models/VoxelGAN.py,36,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Generator(nn.Module):\n    """"""TODO: Add docstring.\n\n    https://arxiv.org/abs/1610.07584\n\n    Input shape: B x 200 (B -> batchsize, 200 -> latent code size)\n    Output shape: B x 1 x 32 x 32 x 32\n\n\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @inproceedings{3dgan,\n              title={Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling},\n              author={Wu, Jiajun and Zhang, Chengkai and Xue, Tianfan and Freeman, William T and Tenenbaum, Joshua B},\n              booktitle={Advances in Neural Information Processing Systems},\n              pages={82--90},\n              year={2016}\n            }\n\n            \n    """"""\n\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.layer1 = torch.nn.Sequential(\n            torch.nn.ConvTranspose3d(200, 512, 4, 2, 0),\n            torch.nn.BatchNorm3d(512),\n            torch.nn.ReLU()\n        )\n        self.layer2 = torch.nn.Sequential(\n            torch.nn.ConvTranspose3d(512, 256, 4, 2, 1),\n            torch.nn.BatchNorm3d(256),\n            torch.nn.ReLU()\n        )\n        self.layer3 = torch.nn.Sequential(\n            torch.nn.ConvTranspose3d(256, 128, 4, 2, 1),\n            torch.nn.BatchNorm3d(128),\n            torch.nn.ReLU()\n        )\n        self.layer4 = torch.nn.Sequential(\n            torch.nn.ConvTranspose3d(128, 1, 4, 2, 1),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = x.view(-1, 200, 1, 1, 1)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n\nclass Discriminator(nn.Module):\n    """"""TODO: Add docstring.\n    \n    https://arxiv.org/abs/1610.07584\n\n    Input shape: B x 1 x 32 x 32 x 32\n    Output shape: B x 1 x 1 x 1 x 1\n    """"""\n\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.layer1 = torch.nn.Sequential(\n            torch.nn.Conv3d(1, 64, 4, 2, 1),\n            torch.nn.BatchNorm3d(64),\n            torch.nn.LeakyReLU()\n        )\n        self.layer2 = torch.nn.Sequential(\n            torch.nn.Conv3d(64, 128, 4, 2, 1),\n            torch.nn.BatchNorm3d(128),\n            torch.nn.LeakyReLU()\n        )\n        self.layer3 = torch.nn.Sequential(\n            torch.nn.Conv3d(128, 256, 4, 2, 1),\n            torch.nn.BatchNorm3d(256),\n            torch.nn.LeakyReLU()\n        )\n        self.layer4 = torch.nn.Sequential(\n            torch.nn.Conv3d(256, 512, 4, 2, 1),\n            torch.nn.BatchNorm3d(512),\n            torch.nn.LeakyReLU()\n        )\n        self.layer5 = torch.nn.Sequential(\n            torch.nn.Conv3d(512, 1, 2, 2, 0),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = x.view(-1, 1, 32, 32, 32)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        return x\n'"
kaolin/models/VoxelSuperresODM.py,3,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SuperresNetwork(nn.Module):\n    """"""TODO: Add docstring.\n\n    https://arxiv.org/abs/1802.09987\n\n    Input shape: B x 128 x 128 x 128\n    Output shape: B x (high//low * 128) x (high//low * 128) x (high//low * 128)\n\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @incollection{ODM,\n                title = {Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation},\n                author = {Smith, Edward and Fujimoto, Scott and Meger, David},\n                booktitle = {Advances in Neural Information Processing Systems 31},\n                editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},\n                pages = {6479--6489},\n                year = {2018},\n                publisher = {Curran Associates, Inc.},\n                url = {http://papers.nips.cc/paper/7883-multi-view-silhouette-and-depth-decomposition-for-high-resolution-3d-object-representation.pdf}\n            }\n    """"""\n\n    def __init__(self, high, low):\n        super(SuperresNetwork, self).__init__()\n        self.ratio = high // low\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(6, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128))\n        self.inner_convs_1 = nn.ModuleList([\n            nn.Conv2d(128, 128, kernel_size=3, padding=1) for i in range(16)])\n        self.inner_bns_1 = nn.ModuleList(\n            [nn.BatchNorm2d(128) for i in range(16)])\n        self.inner_convs_2 = nn.ModuleList([\n            nn.Conv2d(128, 128, kernel_size=3, padding=1) for i in range(16)])\n        self.inner_bns_2 = nn.ModuleList([\n            nn.BatchNorm2d(128) for i in range(16)])\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n        )\n        sub_list = [nn.Conv2d(128, 128, kernel_size=3, padding=1),\n                    nn.PixelShuffle(2)]\n        i = 0\n        for i in range(int(math.log(self.ratio, 2)) - 1):\n            sub_list.append(nn.Conv2d(32, 128, kernel_size=3, padding=1))\n            sub_list.append(nn.PixelShuffle(2))\n        self.sub_list = nn.ModuleList(sub_list)\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(32, 6, kernel_size=1, padding=0),\n        )\n\n    def forward(self, x):\n        x = self.layer1(x)\n        temp = x.clone()\n        for i in range(16):\n            recall = self.inner_convs_1[i](x.clone())\n            recall = self.inner_bns_1[i](recall)\n            recall = F.relu(recall)\n            recall = self.inner_convs_2[i](recall)\n            recall = self.inner_bns_2[i](recall)\n            recall = recall + temp\n            temp = recall.clone()\n        recall = self.layer2(recall)\n        x = x + recall\n        for i in range(int(math.log(self.ratio, 2))):\n            x = self.sub_list[2 * i](x)\n            x = self.sub_list[2 * i + 1](x)\n        x = self.layer3(x)\n        x = torch.sigmoid(x)\n        return x\n'"
kaolin/models/VoxelSuperresSimple.py,2,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass EncoderDecoder(nn.Module):\n    """"""A simple encoder-decoder style voxel superresolution network""""""\n\n    def __init__(self):\n        super(EncoderDecoder, self).__init__()\n        self.conv1 = nn.Conv3d(1, 16, 3, stride=2, padding=1)\n        self.bn1 = nn.BatchNorm3d(16)\n        self.conv2 = nn.Conv3d(16, 32, 3, stride=2, padding=1)\n        self.bn2 = nn.BatchNorm3d(32)\n        self.deconv3 = nn.ConvTranspose3d(32, 16, 3, stride=2, padding=1)\n        self.bn3 = nn.BatchNorm3d(16)\n        self.deconv4 = nn.ConvTranspose3d(16, 8, 3, stride=2, padding=0)\n        self.deconv5 = nn.ConvTranspose3d(8, 1, 3, stride=2, padding=0)\n\n    def forward(self, x):\n        # Encoder\n        x = (F.relu(self.bn1(self.conv1(x))))\n        x = (F.relu(self.bn2(self.conv2(x))))\n        # Decoder\n        x = F.relu(self.bn3(self.deconv3(x)))\n        x = F.relu(self.deconv4(x))\n        # Superres layer\n        return self.deconv5(x)\n'"
kaolin/models/__init__.py,0,"b'# PointNet\nimport kaolin.models.PointNet\n# from .PointNet import PointNetFeatureExtractor\n# from .PointNet import PointNetClassifier\n# from .PointNet import PointNetSegmenter\n\n# PointNet2, i.e., PointNet++\nimport kaolin.models.PointNet2\n# from .PointNet2 import PointNet2SetAbstraction\n# from .PointNet2 import PointNet2FeaturePropagator\n# from .PointNet2 import PointNet2Classifier\n# from .PointNet2 import PointNet2Segmenter\n\n# VoxelGAN\nimport kaolin.models.VoxelGAN\n\n# Voxel3DIWGAN\nimport kaolin.models.Voxel3DIWGAN\n\n# VoxelSuperresSimple\nimport kaolin.models.VoxelSuperresSimple\n\n# VoxelSuperresODM\nimport kaolin.models.VoxelSuperresODM\n\n# DGCNN\nimport kaolin.models.dgcnn\n\n# DIBREncoder\nimport kaolin.models.DIBREncoder\n\n# GEOMetrics\nimport kaolin.models.GEOMetrics\n\n# Image2MeshReconstructionBaseline\nimport kaolin.models.Image2MeshReconstructionBaseline\n\n# MeshCNN\nfrom .meshcnn import MeshCNNClassifier\n\n# MeshEncoder\nimport kaolin.models.MeshEncoder\n\n# OccupancyNetwork\nimport kaolin.models.OccupancyNetwork\n\n# Pixel2Mesh\nimport kaolin.models.Pixel2Mesh\n\n# SimpleGCN\nimport kaolin.models.SimpleGCN\n\n# VGG18\nimport kaolin.models.VGG18\n'"
kaolin/models/dgcnn.py,13,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n# pytorch_DGCNN\n#\n# Copyright (c) 2018 Muhan Zhang\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport os\nimport sys\nimport copy\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef knn(x, k):\n    inner = -2 * torch.matmul(x.transpose(2, 1), x)\n    xx = torch.sum(x**2, dim=1, keepdim=True)\n    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n\n    idx = pairwise_distance.topk(k=k, dim=-1)[1]  # (batch_size, num_points, k)\n    return idx\n\n\nclass DGCNN(nn.Module):\n    """"""Implementation of the DGCNNfor pointcloud classificaiton.\n    \n    Args:\n        input_dim (int): number of features per point. Default: ``3`` (xyz point coordinates)\n        conv_dims (list): list of output feature dimensions of the convolutional layers. Default: ``[64,64,128,256]`` (as preoposed in original implementation).\n        emb_dims (int): dimensionality of the intermediate embedding.\n        fc_dims (list): list of output feature dimensions of the fully connected layers. Default: ``[512, 256]`` (as preoposed in original implementation).\n        output_channels (int): number of output channels. Default: ``64``.\n        dropout (float): dropout probability (applied to fully connected layers only). Default: ``0.5``.\n        k (int): number of nearest neighbors.\n        use_cuda (bool): if ``True`` will move the model to GPU\n\n    .. note::\n\n        If you use this code, please cite the original paper in addition to Kaolin.\n        \n        .. code-block::\n\n            @article{dgcnn,\n                title={Dynamic Graph CNN for Learning on Point Clouds},\n                author={Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E. and Bronstein, Michael M. and Solomon, Justin M.},\n                journal={ACM Transactions on Graphics (TOG)},\n                year={2019}\n            }\n\n    """"""\n    def __init__(\n            self,\n            input_dim=3,\n            conv_dims=[64, 64, 128, 256],\n            emb_dims=1024,  # dimension of embeddings\n            fc_dims=[512, 256],\n            output_channels=64,\n            dropout=0.5,  # dropout probability\n            k=20,  # number of nearest neighbors\n            use_cuda=True,  # use CUDA or not\n    ):\n        super(DGCNN, self).__init__()\n        self.k = k\n        self.use_cuda = use_cuda\n\n        emb_input_dim = sum(conv_dims)\n        self.conv_dims = [input_dim] + conv_dims\n\n        for it in range(len(self.conv_dims) - 1):\n            # self.conv_layers.append(\n            self.__setattr__(f\'conv_layers_{it}\',\n                self.get_layer(\n                    nn.Sequential(\n                        nn.Conv2d(self.conv_dims[it] * 2,\n                                  self.conv_dims[it + 1],\n                                  kernel_size=1,\n                                  bias=False),\n                        nn.BatchNorm2d(self.conv_dims[it + 1]),\n                        nn.LeakyReLU(negative_slope=0.2))))\n        \n        # create intermediate embedding\n        self.embedding_layer = self.get_layer(\n            nn.Sequential(\n                nn.Conv1d(emb_input_dim, emb_dims, kernel_size=1, bias=False),\n                nn.BatchNorm1d(emb_dims), nn.LeakyReLU(negative_slope=0.2)))\n\n        # fully connected layers\n        self.fc_dims = [emb_dims * 2] + fc_dims\n        for it in range(len(self.fc_dims) - 1):\n            self.__setattr__(f\'fc_layers_{it}\',\n                self.get_layer(\n                    nn.Sequential(\n                        nn.Linear(self.fc_dims[it], self.fc_dims[it + 1], bias=False),\n                        nn.BatchNorm1d(self.fc_dims[it + 1]),\n                        nn.LeakyReLU(negative_slope=0.2),\n                        nn.Dropout(p=dropout))))\n\n        # final output projection\n        self.final_layer = self.get_layer(\n            nn.Linear(self.fc_dims[-1], output_channels))\n\n    def get_layer(self, layer):\n        """"""Convert the layer to cuda if needed.\n\n        Args:\n            layer: torch.nn layer\n        """"""\n        if self.use_cuda:\n            return layer.cuda()\n        else:\n            return layer\n\n    def get_graph_feature(self, x, k=20, idx=None):\n        """"""Compute Graph feature.\n\n        Args:\n            x (torch.tensor): torch tensor\n            k (int): number of nearest neighbors\n        """"""\n        batch_size = x.size(0)\n        num_points = x.size(2)\n        x = x.view(batch_size, -1, num_points)\n        if idx is None:\n            idx = knn(x, k=k)  # (batch_size, num_points, k)\n        if self.use_cuda:\n            device = torch.device(\'cuda\')\n        else:\n            device = torch.device(\'cpu\')\n\n        idx_base = torch.arange(0, batch_size, device=device).view(\n            -1, 1, 1) * num_points\n        idx = idx + idx_base\n        idx = idx.view(-1)\n\n        _, num_dims, _ = x.size()\n\n        x = x.transpose(2, 1).contiguous(\n        )  # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n        feature = x.view(batch_size * num_points, -1)[idx, :]\n        feature = feature.view(batch_size, num_points, k, num_dims)\n        x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n\n        feature = torch.cat((feature - x, x), dim=3).permute(0, 3, 1, 2)\n\n        return feature\n\n    def forward(self, x):\n        """"""Forward pass of the DGCNN model.\n\n        Args:\n            x (torch.tensor): input to the network in the format: [B, N_feat, N_points]\n        """"""\n        batch_size = x.size(0)\n        x_list = []\n\n        # convolutional layers\n        for it in range(len(self.conv_dims) - 1):\n            x = self.get_graph_feature(x, k=self.k)\n            x = self.__getattr__(f\'conv_layers_{it}\')(x)\n            x = x.max(dim=-1, keepdim=False)[0]\n            x_list.append(x)\n\n        # embedding layer\n        x = self.embedding_layer(torch.cat(x_list, dim=1))\n\n        # prepare for FC layer input\n        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)\n        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)\n        x = torch.cat((x1, x2), 1)\n\n        # fully connected layers\n        for it in range(len(self.fc_dims) - 1):\n            x = self.__getattr__(f\'fc_layers_{it}\')(x)\n\n        # final layer\n        x = self.final_layer(x)\n\n        return x\n'"
kaolin/models/meshcnn.py,95,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n# MIT License\n\n# Copyright (c) 2019 Rana Hanocka\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport math\nfrom heapq import heappop, heapify\nfrom threading import Thread\nfrom typing import Iterable, Optional\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n__all__ = [\n    ""MeshCNNClassifier"",\n    ""compute_face_normals_and_areas"",\n    ""extract_meshcnn_features"",\n]\n\n\ndef compute_face_normals_for_mesh(mesh):\n    r""""""Compute face normals for an input kaolin.rep.TriangleMesh object.\n\n    Args:\n        mesh (kaolin.rep.TriangleMesh): A triangle mesh object.\n\n    Returns:\n        face_normals (torch.Tensor): Tensor containing face normals for\n            each triangle in the mesh (shape: :math:`(M, 3)`), where :math:`N`\n            is the number of faces (triangles) in the mesh.\n    """"""\n    face_normals = torch.cross(\n        mesh.vertices[mesh.faces[:, 1]] - mesh.vertices[mesh.faces[:, 0]],\n        mesh.vertices[mesh.faces[:, 2]] - mesh.vertices[mesh.faces[:, 1]],\n    )\n    face_normals = face_normals / face_normals.norm(p=2, dim=-1)[..., None]\n    return face_normals\n\n\ndef compute_face_normals_and_areas(mesh):\n    r""""""Compute face normals and areas for an input kaolin.rep.TriangleMesh object.\n\n    Args:\n       mesh (kaolin.rep.TriangleMesh): A triangle mesh object.\n\n    Returns:\n        face_normals (torch.Tensor): Tensor containing face normals for\n            each triangle in the mesh (shape: :math:`(M, 3)`), where :math:`M`\n            is the number of faces (triangles) in the mesh.\n        face_areas (torch.Tensor): Tensor containing areas for each triangle\n            in the mesh (shape: :math:`(M, 1)`), where :math:`M` is the number\n            of faces (triangles) in the mesh.\n    """"""\n    face_normals = torch.cross(\n        mesh.vertices[mesh.faces[:, 1]] - mesh.vertices[mesh.faces[:, 0]],\n        mesh.vertices[mesh.faces[:, 2]] - mesh.vertices[mesh.faces[:, 1]],\n    )\n    face_normal_lengths = face_normals.norm(p=2, dim=-1)\n    face_normals = face_normals / face_normal_lengths[..., None]\n    # Recall: area of a triangle defined by vectors a and b is 0.5 * norm(cross(a, b))\n    face_areas = 0.5 * face_normal_lengths\n    return face_normals, face_areas\n\n\ndef is_two_manifold(mesh):\n    """"""Returns whether the current mesh is 2-manifold. Assumes that adjacency info\n    for the mesh is enabled.\n\n    Args:\n        mesh (kaolin.rep.TriangleMesh): A triangle mesh object (assumes adjacency\n            info is enabled).\n    """"""\n    return (mesh.ef.shape[-1] == 2) and (mesh.ef.min() >= 0)\n\n\ndef build_gemm_representation(mesh, face_areas):\n    r""""""Build a GeMM-suitable representation for the current mesh.\n\n    The GeMM representation contains the following attributes:\n        gemm_edges: tensor of four 1-ring neighbours per edge (E, 4)\n        sides: tensor of indices (in the range [0, 3]) indicating the index of an edge\n            in the gemm_edges entry of the 4 neighbouring edges.\n        Eg. edge i => gemm_edges[gemm_edges[i], sides[i]] = [i, i, i, i]\n\n    Args:\n        mesh (kaolin.rep.TriangleMesh): A triangle mesh that is 2-manifold.\n        face_areas (torch.Tensor): Areas of each triangle in the mesh\n            (shape: :math:`(F)`, where :math:`F` is the number of faces).\n\n    """"""\n    # Retain first four neighbours for each edge (Needed esp if using newer\n    # adjacency computation code).\n    mesh.gemm_edges = mesh.ee[..., :4]\n    # Compute the ""sides"" tensor\n    mesh.sides = torch.zeros_like(mesh.gemm_edges)\n\n    # TODO: Vectorize this!\n    for i in range(mesh.gemm_edges.shape[-2]):\n        for j in range(mesh.gemm_edges.shape[-1]):\n            nbr = mesh.gemm_edges[i, j]\n            ind = torch.nonzero(mesh.gemm_edges[nbr] == i)\n            mesh.sides[i, j] = ind\n\n    # Average area of all faces neighbouring an edge (normalized by the overall area of\n    # the mesh). Weirdly, MeshCNN, computes averages by dividing by 3 (as opposed to\n    # dividing by 2), and hence, we adopt their convention.\n    mesh.edge_areas = face_areas[mesh.ef].sum(dim=-1) / (3 * face_areas.sum())\n\n\ndef get_edge_points_vectorized(mesh):\n    r""""""Get the edge points (a, b, c, d, e) as defined in Fig. 4 of the MeshCNN\n    paper: https://arxiv.org/pdf/1809.05910.pdf.\n\n    Args:\n        mesh (kaolin.rep.TriangleMesh): A triangle mesh object.\n\n    Returns:\n        (torch.Tensor): Tensor containing ""edge points"" of the mesh, as per\n            MeshCNN convention (shape: :math:`(E, 4)`), where :math:`E` is the\n            total number of edges in the mesh.\n    """"""\n\n    a = mesh.edges\n    b = mesh.edges[mesh.gemm_edges[:, 0]]\n    c = mesh.edges[mesh.gemm_edges[:, 1]]\n    d = mesh.edges[mesh.gemm_edges[:, 2]]\n    e = mesh.edges[mesh.gemm_edges[:, 3]]\n\n    v1 = torch.zeros(a.shape[0]).bool().to(a.device)\n    v2 = torch.zeros_like(v1)\n    v3 = torch.zeros_like(v1)\n\n    a_in_b = (a[:, 1] == b[:, 0]) + (a[:, 1] == b[:, 1])\n    not_a_in_b = ~a_in_b\n    a_in_b = a_in_b.long()\n    not_a_in_b = not_a_in_b.long()\n    b_in_c = ((b[:, 1] == c[:, 0]) + (b[:, 1] == c[:, 1])).long()\n    d_in_e = ((d[:, 1] == e[:, 0]) + (d[:, 1] == e[:, 1])).long()\n\n    arange = torch.arange(mesh.edges.shape[0]).to(a.device)\n\n    return torch.stack(\n        (\n            a[arange, a_in_b],\n            a[arange, not_a_in_b],\n            b[arange, b_in_c],\n            d[arange, d_in_e],\n        ),\n        dim=-1,\n    )\n\n\ndef set_edge_lengths(mesh, edge_points):\n    r""""""Set edge lengths for each of the edge points. \n\n    Args:\n       mesh (kaolin.rep.TriangleMesh): A triangle mesh object.\n       edge_points (torch.Tensor): Tensor containing ""edge points"" of the mesh,\n            as per MeshCNN convention (shape: :math:`(E, 4)`), where :math:`E`\n            is the total number of edges in the mesh.\n    """"""\n    mesh.edge_lengths = (\n        mesh.vertices[edge_points[:, 0]] - mesh.vertices[edge_points[:, 1]]\n    ).norm(p=2, dim=1)\n\n\ndef compute_normals_from_gemm(mesh, edge_points, side, eps=1e-1):\n    r""""""Compute vertex normals from the GeMM representation.\n\n    Args:\n        mesh (kaolin.rep.TriangleMesh): A triangle mesh object.\n        edge_points (torch.Tensor): Tensor containing ""edge points"" of the mesh,\n            as per MeshCNN convention (shape: :math:`(E, 4)`), where :math:`E`\n            is the total number of edges in the mesh.\n        side (int): Side of the edge used in computing normals (0, 1, 2, or 3\n            following MeshCNN convention).\n        eps (float): A small number, for numerical stability (default: 1e-1,\n            following MeshCNN implementation).\n\n    Returns:\n        (torch.Tensor): Face normals for each vertex on the chosen side of the\n            edge (shape: :math:`(E, 2)`, where :math:`E` is the number of edges\n            in the mesh).\n    """"""\n    a = (\n        mesh.vertices[edge_points[:, side // 2 + 2]]\n        - mesh.vertices[edge_points[:, side // 2]]\n    )\n    b = (\n        mesh.vertices[edge_points[:, 1 - side // 2]]\n        - mesh.vertices[edge_points[:, side // 2 + 2]]\n    )\n    normals = torch.cross(a, b)\n    return normals / (normals.norm(p=2, dim=-1)[:, None] + eps)\n\n\ndef compute_dihedral_angles(mesh, edge_points):\n    r""""""Compute dihedral angle features for each edge. \n\n    Args:\n        mesh (kaolin.rep.TriangleMesh): A triangle mesh object.\n        edge_points (torch.Tensor): Tensor containing ""edge points"" of the mesh,\n            as per MeshCNN convention (shape: :math:`(E, 4)`), where :math:`E`\n            is the total number of edges in the mesh.\n\n    Returns:\n        (torch.Tensor): Dihedral angle features for each edge in the mesh\n            (shape: :math:`(E, 4)`, where :math:`E` is the number of edges\n            in the mesh).\n    """"""\n    a = compute_normals_from_gemm(mesh, edge_points, 0)\n    b = compute_normals_from_gemm(mesh, edge_points, 3)\n    dot = (a * b).sum(dim=-1).clamp(-1, 1)\n    return math.pi - torch.acos(dot)\n\n\ndef compute_opposite_angles(mesh, edge_points, side, eps=1e-1):\n    r""""""Compute opposite angle features for each edge.\n\n    Args:\n        mesh (kaolin.rep.TriangleMesh): A triangle mesh object.\n        edge_points (torch.Tensor): Tensor containing ""edge points"" of the mesh,\n            as per MeshCNN convention (shape: :math:`(E, 4)`), where :math:`E`\n            is the total number of edges in the mesh.\n        side (int): Side of the edge used in computing normals (0, 1, 2, or 3\n            following MeshCNN convention).\n        eps (float): A small number, for numerical stability (default: 1e-1,\n            following MeshCNN implementation).\n\n    Returns:\n        (torch.Tensor): Opposite angle features on the chosen side of the\n            edge (shape: :math:`(E, 2)`, where :math:`E` is the number of edges\n            in the mesh).\n    """"""\n    a = (\n        mesh.vertices[edge_points[:, side // 2]]\n        - mesh.vertices[edge_points[:, side // 2 + 2]]\n    )\n    b = (\n        mesh.vertices[edge_points[:, 1 - side // 2]]\n        - mesh.vertices[edge_points[:, side // 2 + 2]]\n    )\n    a = a / (a.norm(p=2, dim=-1)[:, None] + eps)\n    b = b / (b.norm(p=2, dim=-1)[:, None] + eps)\n    dot = (a * b).sum(dim=-1).clamp(-1, 1)\n    return torch.acos(dot)\n\n\ndef compute_symmetric_opposite_angles(mesh, edge_points):\n    r""""""Compute symmetric opposite angle features for each edge.\n\n    Args:\n        mesh (kaolin.rep.TriangleMesh): A triangle mesh object.\n        edge_points (torch.Tensor): Tensor containing ""edge points"" of the mesh,\n            as per MeshCNN convention (shape: :math:`(E, 4)`), where :math:`E`\n            is the total number of edges in the mesh.\n\n    Returns:\n        (torch.Tensor): Symmetric opposite angle features for each edge in the mesh\n            (shape: :math:`(E, 4)`, where :math:`E` is the number of edges\n            in the mesh).\n    """"""\n    a = compute_opposite_angles(mesh, edge_points, 0)\n    b = compute_opposite_angles(mesh, edge_points, 3)\n    angles = torch.stack((a, b), dim=0)\n    val, _ = torch.sort(angles, dim=0)\n    return val\n\n\ndef compute_edgelength_ratios(mesh, edge_points, side, eps=1e-1):\n    r""""""Compute edge-length ratio features for each edge.\n\n    Args:\n        mesh (kaolin.rep.TriangleMesh): A triangle mesh object.\n        edge_points (torch.Tensor): Tensor containing ""edge points"" of the mesh,\n            as per MeshCNN convention (shape: :math:`(E, 4)`), where :math:`E`\n            is the total number of edges in the mesh.\n        side (int): Side of the edge used in computing normals (0, 1, 2, or 3\n            following MeshCNN convention).\n        eps (float): A small number, for numerical stability (default: 1e-1,\n            following MeshCNN implementation).\n\n    Returns:\n        (torch.Tensor): Edge-length ratio features on the chosen side of the\n            edge (shape: :math:`(E, 2)`, where :math:`E` is the number of edges\n            in the mesh).\n    """"""\n    edge_lengths = (\n        mesh.vertices[edge_points[:, side // 2]]\n        - mesh.vertices[edge_points[:, 1 - side // 2]]\n    ).norm(p=2, dim=-1)\n    o = mesh.vertices[edge_points[:, side // 2 + 2]]\n    a = mesh.vertices[edge_points[:, side // 2]]\n    b = mesh.vertices[edge_points[:, 1 - side // 2]]\n    ab = b - a\n    projection_length = (ab * (o - a)).sum(dim=-1) / (ab.norm(p=2, dim=-1) + eps)\n    closest_point = a + (projection_length / edge_lengths)[:, None] * ab\n    d = (o - closest_point).norm(p=2, dim=-1)\n    return d / edge_lengths\n\n\ndef compute_symmetric_edgelength_ratios(mesh, edge_points):\n    r""""""Compute symmetric edge-length ratio features for each edge.\n\n    Args:\n        mesh (kaolin.rep.TriangleMesh): A triangle mesh object.\n        edge_points (torch.Tensor): Tensor containing ""edge points"" of the mesh,\n            as per MeshCNN convention (shape: :math:`(E, 4)`), where :math:`E`\n            is the total number of edges in the mesh.\n\n    Returns:\n        (torch.Tensor): Symmetric edge-length ratio features for each edge in the mesh\n            (shape: :math:`(E, 4)`, where :math:`E` is the number of edges\n            in the mesh).\n    """"""\n    ratios_a = compute_edgelength_ratios(mesh, edge_points, 0)\n    ratios_b = compute_edgelength_ratios(mesh, edge_points, 3)\n    ratios = torch.stack((ratios_a, ratios_b), dim=0)\n    val, _ = torch.sort(ratios, dim=0)\n    return val\n\n\ndef extract_meshcnn_features(mesh, edge_points):\n    r""""""Extract the various features used by MeshCNN.\n\n    Args:\n        mesh (kaolin.rep.TriangleMesh): Input (2-manifold) triangle mesh.\n        edge_points (torch.Tensor): Computed edge points from the input\n            triangle mesh (following MeshCNN convention).\n    """"""\n    dihedral_angles = compute_dihedral_angles(mesh, edge_points).unsqueeze(0)\n    symmetric_opposite_angles = compute_symmetric_opposite_angles(mesh, edge_points)\n    symmetric_edgelength_ratios = compute_symmetric_edgelength_ratios(mesh, edge_points)\n    mesh.features = torch.cat(\n        (dihedral_angles, symmetric_opposite_angles, symmetric_edgelength_ratios), dim=0\n    )\n\n\nclass MeshCNNConv(torch.nn.Module):\n    r""""""Implements the MeshCNN convolution operator. Recall that convolution is performed on the 1-ring\n    neighbours of each (non-manifold) edge in the mesh.\n\n    Args:\n        in_channels (int): number of channels (features) in the input.\n        out_channels (int): number of channels (features) in the output.\n        kernel_size (int): kernel size of the filter.\n        bias (bool, Optional): whether or not to use a bias term (default: True).\n\n    .. note::\n\n    If you use this code, please cite the original paper in addition to Kaolin.\n\n    .. code-block::\n\n        @article{meshcnn,\n          title={MeshCNN: A Network with an Edge},\n          author={Hanocka, Rana and Hertz, Amir and Fish, Noa and Giryes, Raja and Fleishman, Shachar and Cohen-Or, Daniel},\n          journal={ACM Transactions on Graphics (TOG)},\n          volume={38},\n          number={4},\n          pages = {90:1--90:12},\n          year={2019},\n          publisher={ACM}\n        }\n    """"""\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: int,\n        bias: Optional[bool] = True,\n    ):\n        super(MeshCNNConv, self).__init__()\n        self.conv = torch.nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=(1, kernel_size),\n            bias=bias,\n        )\n        self.kernel_size = kernel_size\n\n    def __call__(self, edge_features: torch.Tensor, meshes: Iterable):\n        r""""""Calls forward when invoked.\n\n        Args:\n            edge_features (torch.Tensor): input features of the mesh (shape: :math:`(B, F, E)`), where :math:`B` is\n                the batchsize, :math:`F` is the number of features per edge, and :math:`E` is the number of edges\n                in the input mesh.\n            meshes (list[kaolin.rep.TriangleMesh]): list of TriangleMesh objects. Length of the list must be equal\n                to the batchsize :math:`B` of `edge_features`.\n        """"""\n        return self.forward(edge_features, meshes)\n\n    def forward(self, x, meshes):\n        r""""""Implements forward pass of the MeshCNN convolution operator.\n\n        Args:\n            x (torch.Tensor): input features of the mesh (shape: :math:`(B, F, E)`), where :math:`B` is\n                the batchsize, :math:`F` is the number of features per edge, and :math:`E` is the number of edges\n                in the input mesh.\n            meshes (list[kaolin.rep.TriangleMesh]): list of TriangleMesh objects. Length of the list must be equal\n                to the batchsize :math:`B` of `x`.\n        """"""\n        x = x.squeeze(-1)\n        G = torch.cat([self.pad_gemm(i, x.shape[2], x.device) for i in meshes], 0)\n        # MeshCNN ""trick"": Build a ""neighbourhood map"" and apply 2D convolution.\n        G = self.create_gemm(x, G)\n        return self.conv(G)\n\n    def flatten_gemm_inds(self, Gi: torch.Tensor):\n        r""""""Flattens the indices of the gemm representation.\n        """"""\n        B, NE, NN = Gi.shape\n        NE += 1\n        batch_n = torch.floor(\n            torch.arange(B * NE, device=Gi.device, dtype=torch.float) / NE\n        ).view(B, NE)\n        add_fac = batch_n * NE\n        add_fac = add_fac.view(B, NE, 1)\n        add_fac = add_fac.repeat(1, 1, NN)\n        Gi = Gi.float() + add_fac[:, 1:, :]\n        return Gi\n\n    def create_gemm(self, x: torch.Tensor, Gi: torch.Tensor):\n        r""""""Gathers edge features (x) from within the 1-ring neighbours (Gi) and applies symmetric pooling for order\n        invariance. Returns a ""neighbourhood map"" that we can use 2D convolution on.\n\n        Args:\n            x (torch.Tensor):\n            Gi (torch.Tensor):\n        """"""\n        Gishape = Gi.shape\n        # Zero-pad the first row of every sample in the batch.\n        # TODO: Can replace by torch.nn.functional.pad()\n        padding = torch.zeros(\n            (x.shape[0], x.shape[1], 1),\n            requires_grad=True,\n            device=x.device,\n            dtype=x.dtype,\n        )\n        x = torch.cat((padding, x), dim=2)\n        Gi = Gi + 1\n\n        # Flatten indices\n        Gi_flat = self.flatten_gemm_inds(Gi)\n        Gi_flat = Gi_flat.view(-1).long()\n\n        outdims = x.shape\n        x = x.permute(0, 2, 1).contiguous()\n        x = x.view(outdims[0] * outdims[2], outdims[1])\n\n        f = torch.index_select(x, dim=0, index=Gi_flat)\n        f = f.view(Gishape[0], Gishape[1], Gishape[2], -1)\n        f = f.permute(0, 3, 1, 2)\n\n        # Perform ""symmetrization"" (ops defined in paper) for the convolution to be ""equivariant"".\n        x1 = f[:, :, :, 1] + f[:, :, :, 3]\n        x2 = f[:, :, :, 2] + f[:, :, :, 4]\n        x3 = (f[:, :, :, 1] - f[:, :, :, 3]).abs()\n        x4 = (f[:, :, :, 2] - f[:, :, :, 4]).abs()\n        return torch.stack((f[:, :, :, 0], x1, x2, x3, x4), dim=3)\n\n    def pad_gemm(self, mesh, desired_size: int, device: torch.device):\n        r""""""Extracts the 1-ring neighbours (four per edge), adds the edge itself to the list, and pads to\n        `desired_size`.\n\n        Args:\n            mesh (kaolin.rep.TriangleMesh): Mesh to convolve over.\n            desired_size (int): Desired size to pad to.\n\n        """"""\n        # padded_gemm = torch.tensor(mesh.gemm_edges, device=device, dtype=torch.float, requires_grad=True)\n        padded_gemm = mesh.gemm_edges.clone().float()\n        padded_gemm.requires_grad = True\n        # TODO: Revisit when batching is implemented, to update `mesh.edges.shape[1]`.\n        num_edges = mesh.edges.shape[-2]\n        padded_gemm = torch.cat(\n            (\n                torch.arange(num_edges, device=device, dtype=torch.float).unsqueeze(-1),\n                padded_gemm,\n            ),\n            dim=1,\n        )\n        padded_gemm = F.pad(\n            padded_gemm, (0, 0, 0, desired_size - num_edges), ""constant"", 0\n        )\n        return padded_gemm.unsqueeze(0)\n\n\nclass MeshCNNUnpool(torch.nn.Module):\n    r""""""Implements the MeshCNN unpooling operator.\n\n    Args:\n        unroll_target (int): number of target edges to unroll to.\n\n    .. note::\n\n    If you use this code, please cite the original paper in addition to Kaolin.\n\n    .. code-block::\n\n        @article{meshcnn,\n          title={MeshCNN: A Network with an Edge},\n          author={Hanocka, Rana and Hertz, Amir and Fish, Noa and Giryes, Raja and Fleishman, Shachar and Cohen-Or, Daniel},\n          journal={ACM Transactions on Graphics (TOG)},\n          volume={38},\n          number={4},\n          pages = {90:1--90:12},\n          year={2019},\n          publisher={ACM}\n        }\n    """"""\n\n    def __init__(self, unroll_target: int):\n        super(MeshCNNUnpool, self).__init__()\n        self.unroll_target = unroll_target\n\n    def __call__(self, features: torch.Tensor, meshes):\n        return self.forward(features, meshes)\n\n    def pad_groups(self, group: torch.Tensor, unroll_start: int):\n        start, end = group.shape\n        padding_rows = unroll_start - start\n        padding_cols = self.unroll_target - end\n        if padding_rows != 0 or padding_cols != 0:\n            padding = torch.nn.ConstantPad2d((0, padding_cols, 0, padding_rows), 0)\n            group = padding(group)\n        return group\n\n    def pad_occurrences(self, occurrences: torch.Tensor):\n        padding = self.unroll_target - occurrences.shape[0]\n        if padding != 0:\n            padding = torch.nn.ConstantPad1d((0, padding), 1)\n            occurrences = padding(occurrences)\n        return occurrences\n\n    def forward(self, features: torch.Tensor, meshes):\n        r""""""Implements forward pass of the MeshCNN convolution operator.\n\n        Args:\n            x (torch.Tensor): input features of the mesh (shape: :math:`(B, F, E)`), where :math:`B` is\n                the batchsize, :math:`F` is the number of features per edge, and :math:`E` is the number of edges\n                in the input mesh.\n            meshes (list[kaolin.rep.TriangleMesh]): list of TriangleMesh objects. Length of the list must be equal\n                to the batchsize :math:`B` of `x`.\n\n        Returns:\n            (torch.Tensor): output features, at the target unpooled size\n                (shape: :math:`(B, F, \\text{self.unroll_target})`)\n        """"""\n        B, F, E = features.shape\n        groups = [self.pad_groups(mesh.get_groups(), E) for mesh in meshes]\n        unroll_mat = torch.cat(groups, dim=0).view(B, E, -1)\n        occurrences = [self.pad_occurrences(mesh.get_occurrences()) for mesh in meshes]\n        occurrences = torch.cat(occurrences, dim=0).view(B, 1, -1)\n        occurrences = occurrences.expand(unroll_mat.shape)\n        unroll_mat = unroll_mat / occurrences\n        urnoll_mat = unroll_mat.to(features)\n        for mesh in meshes:\n            mesh.unroll_gemm()\n        return torch.matmul(features, unroll_mat)\n\n\nclass MeshUnion:\n    r""""""Implements the MeshCNN ""union"" operator.\n\n    Args:\n        num_edges (int): number of edges to attach (i.e., perform ""union"").\n        device (torch.device): device on which tensors reside.\n\n    .. note::\n\n    If you use this code, please cite the original paper in addition to Kaolin.\n\n    .. code-block::\n\n        @article{meshcnn,\n          title={MeshCNN: A Network with an Edge},\n          author={Hanocka, Rana and Hertz, Amir and Fish, Noa and Giryes, Raja and Fleishman, Shachar and Cohen-Or, Daniel},\n          journal={ACM Transactions on Graphics (TOG)},\n          volume={38},\n          number={4},\n          pages = {90:1--90:12},\n          year={2019},\n          publisher={ACM}\n        }\n    """"""\n\n    def __init__(self, num_edges: int, device: torch.device):\n        self.groups = torch.eye(num_edges, device=device)\n        self.rebuild_features = self.rebuild_features_average\n\n    def union(self, source, target):\n        self.groups[target, :] = self.groups[target, :] + self.groups[source, :]\n\n    def remove_group(self, index):\n        return\n\n    def get_group(self, edge_key):\n        return self.groups[edge_key, :]\n\n    def get_occurrences(self):\n        return torch.sum(self.groups, 0)\n\n    def get_groups(self, mask):\n        self.groups = self.groups.clamp(0, 1)\n        return self.groups[mask, :]\n\n    def rebuild_features_average(self, features, mask, target_edges):\n        self.prepare_groups(features, mask)\n        faces_edges = torch.matmul(features.squeeze(-1), self.groups)\n        occurrences = torch.sum(self.groups, 0).expand(faces_edges.shape)\n        faces_edges = faces_edges / occurrences\n        padding = target_edges - faces_edges.shape[1]\n        if padding > 0:\n            padding = torch.nn.ConstantPad2d((0, padding, 0, 0), 0)\n            faces_edges = padding(faces_edges)\n        return faces_edges\n\n    def prepare_groups(self, features, mask):\n        mask = torch.from_numpy(mask)\n        self.groups = self.groups[mask, :].clamp(0, 1).transpose_(1, 0)\n        padding = features.shape[1] - self.groups.shape[0]\n        if padding > 0:\n            padding = torch.nn.ConstantPad2d((0, 0, 0, padding), 0)\n            self.groups = padding(self.groups)\n\n\nclass MeshPool(torch.nn.Module):\n    r""""""Implements the MeshCNN pooling operator.\n\n    Args:\n        target (int): number of target edges to pool to.\n        multi_thread (bool): Optionally run multi-threaded (default: False).\n\n    .. note::\n\n    If you use this code, please cite the original paper in addition to Kaolin.\n\n    .. code-block::\n\n        @article{meshcnn,\n          title={MeshCNN: A Network with an Edge},\n          author={Hanocka, Rana and Hertz, Amir and Fish, Noa and Giryes, Raja and Fleishman, Shachar and Cohen-Or, Daniel},\n          journal={ACM Transactions on Graphics (TOG)},\n          volume={38},\n          number={4},\n          pages = {90:1--90:12},\n          year={2019},\n          publisher={ACM}\n        }\n    """"""\n\n    def __init__(self, target, multi_thread=False):\n        super(MeshPool, self).__init__()\n        self.__out_target = target\n        self.__multi_thread = multi_thread\n        self.__fe = None\n        self.__updated_fe = None\n        self.__meshes = None\n        self.__merge_edges = [-1, -1]\n\n    def __call__(self, fe, meshes):\n        return self.forward(fe, meshes)\n\n    def forward(self, fe, meshes):\n        r""""""Pool edges from the mesh and update features.\n\n        Args:\n            fe (torch.Tensor): Face-edge neighbourhood tensor.\n            meshes (Iterable[kaolin.rep.TriangleMesh]): List of meshes to pool.\n\n        Returns:\n            out_features (torch.Tensor): Updated mesh features.\n\n        """"""\n\n        self.__updated_fe = [[] for _ in range(len(meshes))]\n        pool_threads = []\n        self.__fe = fe\n        self.__meshes = meshes\n        # iterate over batch\n        for mesh_index in range(len(meshes)):\n            if self.__multi_thread:\n                pool_threads.append(Thread(target=self.__pool_main, args=(mesh_index,)))\n                pool_threads[-1].start()\n            else:\n                self.__pool_main(mesh_index)\n        if self.__multi_thread:\n            for mesh_index in range(len(meshes)):\n                pool_threads[mesh_index].join()\n        out_features = torch.cat(self.__updated_fe).view(\n            len(meshes), -1, self.__out_target\n        )\n        return out_features\n\n    def __pool_main(self, mesh_index):\n        mesh = self.__meshes[mesh_index]\n        queue = self.__build_queue(\n            self.__fe[mesh_index, :, : mesh.edges_count], mesh.edges_count\n        )\n        # recycle = []\n        # last_queue_len = len(queue)\n        last_count = mesh.edges_count + 1\n        mask = np.ones(mesh.edges_count, dtype=np.bool)\n        edge_groups = MeshUnion(mesh.edges_count, self.__fe.device)\n        while mesh.edges_count > self.__out_target:\n            value, edge_id = heappop(queue)\n            edge_id = int(edge_id)\n            if mask[edge_id]:\n                self.__pool_edge(mesh, edge_id, mask, edge_groups)\n        self.clean(mesh, mask, edge_groups)\n        fe = edge_groups.rebuild_features(\n            self.__fe[mesh_index], mask, self.__out_target\n        )\n        self.__updated_fe[mesh_index] = fe\n\n    def __pool_edge(self, mesh, edge_id, mask, edge_groups):\n        if self.has_boundaries(mesh, edge_id):\n            return False\n        elif (\n            self.__clean_side(mesh, edge_id, mask, edge_groups, 0)\n            and self.__clean_side(mesh, edge_id, mask, edge_groups, 2)\n            and self.__is_one_ring_valid(mesh, edge_id)\n        ):\n            self.__merge_edges[0] = self.__pool_side(\n                mesh, edge_id, mask, edge_groups, 0\n            )\n            self.__merge_edges[1] = self.__pool_side(\n                mesh, edge_id, mask, edge_groups, 2\n            )\n            self.merge_vertices(mesh, edge_id)\n            mask[edge_id] = False\n            MeshPool.__remove_group(mesh, edge_groups, edge_id)\n            mesh.edges_count -= 1\n            return True\n        else:\n            return False\n\n    def merge_vertices(self, mesh, edgeidx):\n        self.remove_edge(mesh, edgeidx)\n        edge = mesh.edges[edgeidx]\n        v_a = mesh.vertices[edge[0]]\n        v_b = mesh.vertices[edge[1]]\n        v_a.__iadd__(v_b)\n        v_a.__itruediv__(2)\n        mesh.vertex_mask[edge[1]] = False\n        mask = mesh.edges == edge[1]\n        mesh.ve[edge[0]].extend(mesh.ve[edge[1]])\n        mesh.edges[mask] = edge[0]\n\n    def __clean_side(self, mesh, edge_id, mask, edge_groups, side):\n        if mesh.edges_count <= self.__out_target:\n            return False\n        invalid_edges = MeshPool.__get_invalids(mesh, edge_id, edge_groups, side)\n        while len(invalid_edges) != 0 and mesh.edges_count > self.__out_target:\n            self.__remove_triplete(mesh, mask, edge_groups, invalid_edges)\n            if mesh.edges_count <= self.__out_target:\n                return False\n            if self.has_boundaries(mesh, edge_id):\n                return False\n            invalid_edges = self.__get_invalids(mesh, edge_id, edge_groups, side)\n        return True\n\n    def clean(self, mesh, edges_mask, groups):\n        edges_mask = edges_mask.astype(bool)\n        torch_mask = torch.from_numpy(edges_mask.copy())\n        mesh.gemm_edges = mesh.gemm_edges[edges_mask]\n        mesh.edges = mesh.edges[edges_mask]\n        mesh.sides = mesh.sides[edges_mask]\n        new_ve = []\n        edges_mask = np.concatenate([edges_mask, [False]])\n        new_indices = np.zeros(edges_mask.shape[0], dtype=np.int32)\n        new_indices[-1] = -1\n        new_indices[edges_mask] = np.arange(0, np.ma.where(edges_mask)[0].shape[0])\n        mesh.gemm_edges[:, :] = (\n            torch.from_numpy(new_indices[mesh.gemm_edges[:, :]])\n            .to(mesh.vertices.device)\n            .long()\n        )\n        for v_index, ve in enumerate(mesh.ve):\n            update_ve = []\n            # if mesh.v_mask[v_index]:\n            for e in ve:\n                update_ve.append(new_indices[e])\n            new_ve.append(update_ve)\n        mesh.ve = new_ve\n        mesh.pool_count += 1\n\n    @staticmethod\n    def has_boundaries(mesh, edge_id):\n        for edge in mesh.gemm_edges[edge_id]:\n            if edge == -1 or -1 in mesh.gemm_edges[edge]:\n                return True\n        return False\n\n    @staticmethod\n    def __is_one_ring_valid(mesh, edge_id):\n        v_a = set(mesh.edges[mesh.ve[mesh.edges[edge_id, 0]]].reshape(-1))\n        v_b = set(mesh.edges[mesh.ve[mesh.edges[edge_id, 1]]].reshape(-1))\n        shared = v_a & v_b - set(mesh.edges[edge_id])\n        return len(shared) == 2\n\n    def __pool_side(self, mesh, edge_id, mask, edge_groups, side):\n        info = MeshPool.__get_face_info(mesh, edge_id, side)\n        key_a, key_b, side_a, side_b, _, other_side_b, _, other_keys_b = info\n        self.__redirect_edges(\n            mesh,\n            key_a,\n            side_a - side_a % 2,\n            other_keys_b[0],\n            mesh.sides[key_b, other_side_b],\n        )\n        self.__redirect_edges(\n            mesh,\n            key_a,\n            side_a - side_a % 2 + 1,\n            other_keys_b[1],\n            mesh.sides[key_b, other_side_b + 1],\n        )\n        MeshPool.__union_groups(mesh, edge_groups, key_b, key_a)\n        MeshPool.__union_groups(mesh, edge_groups, edge_id, key_a)\n        mask[key_b] = False\n        MeshPool.__remove_group(mesh, edge_groups, key_b)\n        self.remove_edge(mesh, key_b)\n        mesh.edges_count -= 1\n        return key_a\n\n    def remove_edge(self, mesh, edgeidx):\n        vs = mesh.edges[edgeidx]\n        for v in vs:\n            mesh.ve[v].remove(edgeidx)\n\n    @staticmethod\n    def __get_invalids(mesh, edge_id, edge_groups, side):\n        info = MeshPool.__get_face_info(mesh, edge_id, side)\n        (\n            key_a,\n            key_b,\n            side_a,\n            side_b,\n            other_side_a,\n            other_side_b,\n            other_keys_a,\n            other_keys_b,\n        ) = info\n        shared_items = MeshPool.__get_shared_items(other_keys_a, other_keys_b)\n        if len(shared_items) == 0:\n            return []\n        else:\n            assert len(shared_items) == 2\n            middle_edge = other_keys_a[shared_items[0]]\n            update_key_a = other_keys_a[1 - shared_items[0]]\n            update_key_b = other_keys_b[1 - shared_items[1]]\n            update_side_a = mesh.sides[key_a, other_side_a + 1 - shared_items[0]]\n            update_side_b = mesh.sides[key_b, other_side_b + 1 - shared_items[1]]\n            MeshPool.__redirect_edges(mesh, edge_id, side, update_key_a, update_side_a)\n            MeshPool.__redirect_edges(\n                mesh, edge_id, side + 1, update_key_b, update_side_b\n            )\n            MeshPool.__redirect_edges(\n                mesh,\n                update_key_a,\n                MeshPool.__get_other_side(update_side_a),\n                update_key_b,\n                MeshPool.__get_other_side(update_side_b),\n            )\n            MeshPool.__union_groups(mesh, edge_groups, key_a, edge_id)\n            MeshPool.__union_groups(mesh, edge_groups, key_b, edge_id)\n            MeshPool.__union_groups(mesh, edge_groups, key_a, update_key_a)\n            MeshPool.__union_groups(mesh, edge_groups, middle_edge, update_key_a)\n            MeshPool.__union_groups(mesh, edge_groups, key_b, update_key_b)\n            MeshPool.__union_groups(mesh, edge_groups, middle_edge, update_key_b)\n            return [key_a, key_b, middle_edge]\n\n    @staticmethod\n    def __redirect_edges(mesh, edge_a_key, side_a, edge_b_key, side_b):\n        mesh.gemm_edges[edge_a_key, side_a] = edge_b_key\n        mesh.gemm_edges[edge_b_key, side_b] = edge_a_key\n        mesh.sides[edge_a_key, side_a] = side_b\n        mesh.sides[edge_b_key, side_b] = side_a\n\n    @staticmethod\n    def __get_shared_items(list_a, list_b):\n        shared_items = []\n        for i in range(len(list_a)):\n            for j in range(len(list_b)):\n                if list_a[i] == list_b[j]:\n                    shared_items.extend([i, j])\n        return shared_items\n\n    @staticmethod\n    def __get_other_side(side):\n        return side + 1 - 2 * (side % 2)\n\n    @staticmethod\n    def __get_face_info(mesh, edge_id, side):\n        key_a = mesh.gemm_edges[edge_id, side]\n        key_b = mesh.gemm_edges[edge_id, side + 1]\n        side_a = mesh.sides[edge_id, side]\n        side_b = mesh.sides[edge_id, side + 1]\n        other_side_a = (side_a - (side_a % 2) + 2) % 4\n        other_side_b = (side_b - (side_b % 2) + 2) % 4\n        other_keys_a = [\n            mesh.gemm_edges[key_a, other_side_a],\n            mesh.gemm_edges[key_a, other_side_a + 1],\n        ]\n        other_keys_b = [\n            mesh.gemm_edges[key_b, other_side_b],\n            mesh.gemm_edges[key_b, other_side_b + 1],\n        ]\n        return (\n            key_a,\n            key_b,\n            side_a,\n            side_b,\n            other_side_a,\n            other_side_b,\n            other_keys_a,\n            other_keys_b,\n        )\n\n    @staticmethod\n    def __remove_triplete(mesh, mask, edge_groups, invalid_edges):\n        vertex = set(mesh.edges[invalid_edges[0]])\n        for edge_key in invalid_edges:\n            vertex &= set(mesh.edges[edge_key])\n            mask[edge_key] = False\n            MeshPool.__remove_group(mesh, edge_groups, edge_key)\n        mesh.edges_count -= 3\n        vertex = list(vertex)\n        assert len(vertex) == 1\n        mesh.vertex_mask[vertex[0]] = False\n        # mesh.remove_vertex(vertex[0])\n\n    def __build_queue(self, features, edges_count):\n        # delete edges with smallest norm\n        squared_magnitude = torch.sum(features * features, 0)\n        if squared_magnitude.shape[-1] != 1:\n            squared_magnitude = squared_magnitude.unsqueeze(-1)\n        edge_ids = torch.arange(\n            edges_count, device=squared_magnitude.device, dtype=torch.float32\n        ).unsqueeze(-1)\n        heap = torch.cat((squared_magnitude, edge_ids), dim=-1).tolist()\n        heapify(heap)\n        return heap\n\n    @staticmethod\n    def __union_groups(mesh, edge_groups, source, target):\n        edge_groups.union(source, target)\n        # mesh.union_groups(source, target)\n\n    @staticmethod\n    def __remove_group(mesh, edge_groups, index):\n        edge_groups.remove_group(index)\n        # mesh.remove_group(index)\n\n\nclass MResConv(torch.nn.Module):\n    r""""""Implements a residual block of MeshCNNConv layers.\n\n    Args:\n        in_channels (int): number of channels (features) in the input.\n        out_channels (int): number of channels (features) in the output.\n        skip (Optional, int): number of skip connected layers to add (default: 1).\n        kernel_size (Optional, int): kernel size of the (2D) conv filter. (default: 5).\n\n    .. note::\n\n    If you use this code, please cite the original paper in addition to Kaolin.\n\n    .. code-block::\n\n        @article{meshcnn,\n          title={MeshCNN: A Network with an Edge},\n          author={Hanocka, Rana and Hertz, Amir and Fish, Noa and Giryes, Raja and Fleishman, Shachar and Cohen-Or, Daniel},\n          journal={ACM Transactions on Graphics (TOG)},\n          volume={38},\n          number={4},\n          pages = {90:1--90:12},\n          year={2019},\n          publisher={ACM}\n        }\n    """"""\n\n    def __init__(self, in_channels, out_channels, skip=1, kernel_size=5):\n        super(MResConv, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.skip = 1\n        self.conv0 = MeshCNNConv(\n            self.in_channels, self.out_channels, kernel_size=kernel_size, bias=False\n        )\n        for i in range(self.skip):\n            setattr(self, f""bn{i + 1}"", torch.nn.BatchNorm2d(self.out_channels))\n            setattr(\n                self,\n                f""conv{i + 1}"",\n                MeshCNNConv(\n                    self.out_channels,\n                    self.out_channels,\n                    kernel_size=kernel_size,\n                    bias=False,\n                ),\n            )\n\n    def forward(self, x, mesh):\n        x = self.conv0(x, mesh)\n        x1 = x\n        for i in range(self.skip):\n            x = getattr(self, f""bn{i + 1}"")(F.relu(x))\n            x = getattr(self, f""conv{i + 1}"")(x, mesh)\n        x = x + x1\n        return F.relu(x)\n\n\nclass MeshCNNClassifier(torch.nn.Module):\n    r""""""Implements a MeshCNN classifier.\n\n    Args:\n        in_channels (int): number of channels (features) in the input.\n        out_channels (int): number of channels (features) in the output (usually equal\n            to the number of classes).\n        conv_layer_sizes (Iterable): List of sizes of residual MeshCNNConv blocks to\n            be used.\n        pool_sizes (Iterable): Target number of edges in the mesh after each pooling\n            step.\n        fc_size (int): Number of neurons in the penultimate fully-connected layer.\n        num_res_blocks (int): Number of residual blocks to use in the classifier.\n        num_edges_in (int): Number of edges in the input mesh.\n\n    .. note::\n\n    If you use this code, please cite the original paper in addition to Kaolin.\n\n    .. code-block::\n\n        @article{meshcnn,\n          title={MeshCNN: A Network with an Edge},\n          author={Hanocka, Rana and Hertz, Amir and Fish, Noa and Giryes, Raja and Fleishman, Shachar and Cohen-Or, Daniel},\n          journal={ACM Transactions on Graphics (TOG)},\n          volume={38},\n          number={4},\n          pages = {90:1--90:12},\n          year={2019},\n          publisher={ACM}\n        }\n    """"""\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        conv_layer_sizes,\n        pool_sizes,\n        fc_size,\n        num_res_blocks,\n        num_edges_in,\n    ):\n        super(MeshCNNClassifier, self).__init__()\n        self.layer_sizes = [in_channels] + conv_layer_sizes\n        self.edge_sizes = [num_edges_in] + pool_sizes\n\n        for i, size in enumerate(self.layer_sizes[:-1]):\n            setattr(\n                self,\n                f""conv{i}"",\n                MResConv(size, self.layer_sizes[i + 1], num_res_blocks),\n            )\n            setattr(self, f""pool{i}"", MeshPool(self.edge_sizes[i + 1]))\n\n        self.global_pooling = torch.nn.AvgPool1d(self.edge_sizes[-1])\n        self.fc1 = torch.nn.Linear(self.layer_sizes[-1], fc_size)\n        self.fc2 = torch.nn.Linear(fc_size, out_channels)\n\n    def forward(self, x, mesh):\n        for i in range(len(self.layer_sizes) - 1):\n            x = F.relu(getattr(self, f""conv{i}"")(x, mesh))\n            x = getattr(self, f""pool{i}"")(x, mesh)\n        x = self.global_pooling(x)\n        x = x.view(-1, self.layer_sizes[-1])\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n'"
kaolin/rep/Mesh.py,135,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional\n\nfrom abc import abstractmethod\nimport os\nfrom PIL import Image\n\nimport torch\nimport numpy as np\n\nfrom kaolin.helpers import _assert_tensor\nfrom kaolin.helpers import _composedecorator\n\nimport kaolin.cuda.load_textures as load_textures_cuda\nimport kaolin as kal\n\n\nclass Mesh():\n    """""" Abstract class to represent 3D polygon meshes. """"""\n\n    def __init__(self, vertices: torch.Tensor, faces: torch.Tensor,\n                 uvs: torch.Tensor, face_textures: torch.Tensor,\n                 textures: torch.Tensor, edges: torch.Tensor, edge2key: dict, vv: torch.Tensor,\n                 vv_count: torch.Tensor, vf: torch.Tensor, vf_count: torch.Tensor,\n                 ve: torch.Tensor, ve_count: torch.Tensor, ff: torch.Tensor,\n                 ff_count: torch.Tensor, ef: torch.Tensor, ef_count: torch.Tensor,\n                 ee: torch.Tensor, ee_count: torch.Tensor):\n\n        # Vertices of the mesh\n        self.vertices = vertices\n        # Faces of the mesh\n        self.faces = faces\n        # uv coordinates of each vertex\n        self.uvs = uvs\n        # uv indecies for each face\n        self.face_textures = face_textures\n        # texture for each face\n        self.textures = textures\n        # Edges of the mesh\n        self.edges = edges\n        # Dictionary that maps an edge (tuple) to an edge idx\n        self.edge2key = edge2key\n        # Vertex-Vertex neighborhood tensor (for each vertex, contains\n        # indices of the vertices neighboring it)\n        self.vv = vv\n        # Number of vertices neighbouring each vertex\n        self.vv_count = vv_count\n        # Vertex-Face neighborhood tensor\n        self.vf = vf\n        # Number of faces neighbouring each vertex\n        self.vf_count = vf_count\n        # Vertex-Edge neighborhood tensor\n        self.ve = ve\n        # Number of edges neighboring each vertex\n        self.ve_count = ve_count\n        # Face-Face neighborhood tensor\n        self.ff = ff\n        # Number of faces neighbouring each face\n        self.ff_count = ff_count\n        # Edge-Face neighbourhood tensor\n        self.ef = ef\n        # Number of edges neighbouring each face\n        self.ef_count = ef_count\n        # Edge-Edge neighbourhood tensor\n        self.ee = ee\n        # Number of edges neighbouring each edge\n        self.ee_count = ee_count\n        # adjacency matrix for verts\n        self.adj = None\n\n        # Initialize device on which tensors reside.\n        self.device = self.vertices.device\n\n    @classmethod\n    def from_tensors(cls, vertices: torch.Tensor, faces: torch.Tensor,\n                     uvs: torch.Tensor = None,\n                     face_textures: torch.Tensor = None,\n                     textures: torch.Tensor = None, enable_adjacency=False):\n        r""""""Returns mesh with supplied tensor information.\n\n        Args:\n            vertices (torch.Tensor): mesh vertices.\n            faces (torch.Tensor): mesh faces.\n            uvs (torch.Tensor): uv coordinates for the vertices in mesh.\n            face_textures (torch.Tensor): uv number for each face\'s vertices.\n            textures (torch.Tensor):  texture info for each face.\n            enable_adjacency (torch.Tensor): adjacency information is computed\n        """"""\n        vertices = vertices.clone()\n        faces = faces.clone()\n        if enable_adjacency:\n            edge2key, edges, vv, vv_count, ve, ve_count, vf, vf_count, ff, \\\n                ff_count, ee, ee_count, ef, ef_count = \\\n                cls.compute_adjacency_info(vertices, faces)\n            return cls(vertices, faces, uvs, face_textures, textures, edges,\n                       edge2key, vv, vv_count, vf, vf_count, ve, ve_count,\n                       ff, ff_count, ef, ef_count, ee, ee_count)\n        else:\n            return cls(vertices, faces, uvs, face_textures, textures,\n                       None, None, None, None, None, None, None, None,\n                       None, None, None, None, None, None)\n\n    @_composedecorator(classmethod, abstractmethod)\n    def from_obj(self, filename: str, with_vt: bool = False,\n                 enable_adjacency: bool = False, texture_res=4):\n        r""""""Loads object in .obj wavefront format.\n\n        Args:\n            filename (str) : location of file.\n            with_vt (bool): objects loaded with textures specified by vertex\n                textures.\n            enable_adjacency (bool): adjacency information is computed.\n            texture_res (int): resolution of loaded face colors.\n\n        Note: the with_vt parameter requires cuda.\n\n        Example:\n            >>> mesh = Mesh.from_obj(\'model.obj\')\n            >>> mesh.vertices.shape\n            torch.Size([482, 3])\n            >>> mesh.faces.shape\n            torch.Size([960, 3])\n\n        """"""\n        # run through obj file and extract obj info\n        vertices = []\n        faces = []\n        face_textures = []\n        uvs = []\n        with open(filename, \'r\') as mesh:\n            for line in mesh:\n                data = line.split()\n                if len(data) == 0:\n                    continue\n                if data[0] == \'v\':\n                    vertices.append(data[1:])\n                elif data[0] == \'vt\':\n                    uvs.append(data[1:3])\n                elif data[0] == \'f\':\n                    if \'//\' in data[1]:\n                        data = [da.split(\'//\') for da in data]\n                        faces.append([int(d[0]) for d in data[1:]])\n                        face_textures.append([int(d[1]) for d in data[1:]])\n                    elif \'/\' in data[1]:\n                        data = [da.split(\'/\') for da in data]\n                        faces.append([int(d[0]) for d in data[1:]])\n                        face_textures.append([int(d[1]) for d in data[1:]])\n                    else:\n                        faces.append([int(d) for d in data[1:]])\n                        continue\n        vertices = torch.FloatTensor([float(el) for sublist in vertices for el in sublist]).view(-1, 3)\n        faces = torch.LongTensor(faces) - 1\n\n        # compute texture info\n        textures = None\n        if with_vt:\n            with open(filename, \'r\') as f:\n                textures = None\n                for line in f:\n                    if line.startswith(\'mtllib\'):\n                        filename_mtl = os.path.join(\n                            os.path.dirname(filename), line.split()[1])\n                        textures = self.load_textures(\n                            filename, filename_mtl, texture_res)\n\n                f.close()\n\n        if len(uvs) > 0:\n            uvs = torch.FloatTensor([float(el) for sublist in uvs for el in sublist]).view(-1, 2)\n        else:\n            uvs = None\n        if len(face_textures) > 0:\n            face_textures = torch.LongTensor(face_textures) - 1\n        else:\n            face_textures = None\n\n        if enable_adjacency:\n            edge2key, edges, vv, vv_count, ve, ve_count, vf, vf_count, ff, ff_count, \\\n                ee, ee_count, ef, ef_count = self.compute_adjacency_info(\n                    vertices, faces)\n        else:\n            edge2key, edges, vv, vv_count, ve, ve_count, vf, vf_count, ff, \\\n                ff_count, ee, ee_count, ef, ef_count = None, None, None, \\\n                None, None, None, None, None, None, None, None, None, None, \\\n                None\n\n        output = self(vertices, faces, uvs, face_textures, textures, edges,\n                    edge2key, vv, vv_count, vf, vf_count, ve, ve_count, ff, ff_count,\n                    ef, ef_count, ee, ee_count)\n        return output\n\n    @classmethod\n    def from_off(self, filename: str,\n                 enable_adjacency: Optional[bool] = False):\n        r""""""Loads a mesh from a .off file.\n\n        Args:\n            filename (str): Path to the .off file.\n            enable_adjacency (str): Whether or not to compute adjacency info.\n\n        Returns:\n            (kaolin.rep.Mesh): Mesh object.\n\n        """"""\n        vertices = []\n        faces = []\n        num_vertices = 0\n        num_faces = 0\n        num_edges = 0\n        # Flag to store the number of vertices, faces, and edges that have\n        # been read.\n        read_vertices = 0\n        read_faces = 0\n        read_edgs = 0\n        # Flag to indicate whether or not metadata (number of vertices,\n        # number of faces, (optionally) number of edges) has been read.\n        # For .off files, metadata is the first valid line of each file\n        # (neglecting the ""OFF"" header).\n        metadata_read = False\n        with open(filename, \'r\') as infile:\n            for line in infile.readlines():\n                # Ignore comments\n                if line.startswith(\'#\'):\n                    continue\n                if line.startswith(\'OFF\'):\n                    continue\n                data = line.strip().split()\n                data = [da for da in data if len(da) > 0]\n                # Ignore blank lines\n                if len(data) == 0:\n                    continue\n                if metadata_read is False:\n                    num_vertices = int(data[0])\n                    num_faces = int(data[1])\n                    if len(data) == 3:\n                        num_edges = int(data[2])\n                    metadata_read = True\n                    continue\n                if read_vertices < num_vertices:\n                    vertices.append([float(d) for d in data])\n                    read_vertices += 1\n                    continue\n                if read_faces < num_faces:\n                    numedges = int(data[0])\n                    faces.append([int(d) for d in data[1:1+numedges]])\n                    read_faces += 1\n                    continue\n                if read_edges < num_edges:\n                    edges.append([int(d) for d in data[1:]])\n                    read_edges += 1\n                    continue\n        vertices = torch.FloatTensor(np.array(vertices, dtype=np.float32))\n        faces = torch.LongTensor(np.array(faces, dtype=np.int64))\n\n        if enable_adjacency:\n            edge2key, edges, vv, vv_count, ve, ve_count, vf, vf_count, ff, ff_count, \\\n                ee, ee_count, ef, ef_count = self.compute_adjacency_info(\n                    vertices, faces)\n        else:\n            edge2key, edges, vv, vv_count, ve, ve_count, vf, vf_count, ff, \\\n                ff_count, ee, ee_count, ef, ef_count = None, None, None, \\\n                None, None, None, None, None, None, None, None, None, None, \\\n                None\n\n        return self(vertices, faces, None, None, None, edges,\n                    edge2key, vv, vv_count, vf, vf_count, ve, ve_count, ff, ff_count,\n                    ef, ef_count, ee, ee_count)\n\n    @staticmethod\n    def _cuda_helper(tensor):\n        if tensor is not None:\n            return tensor.cuda()\n\n    @staticmethod\n    def _cpu_helper(tensor):\n        if tensor is not None:\n            return tensor.cpu()\n\n    @staticmethod\n    def _to_helper(tensor, device):\n        if tensor is not None:\n            return tensor.to(device)\n\n    def cuda(self):\n        r""""""""Maps all tensors of the current class to CUDA. """"""\n\n        self.vertices = self._cuda_helper(self.vertices)\n\n        self.faces = self._cuda_helper(self.faces)\n        self.uvs = self._cuda_helper(self.uvs)\n        self.face_textures = self._cuda_helper(self.face_textures)\n        self.textures = self._cuda_helper(self.textures)\n        self.edges = self._cuda_helper(self.edges)\n        self.vv = self._cuda_helper(self.vv)\n        self.vv_count = self._cuda_helper(self.vv_count)\n        self.vf = self._cuda_helper(self.vf)\n        self.vf_count = self._cuda_helper(self.vf_count)\n        self.ve = self._cuda_helper(self.ve)\n        self.ve_count = self._cuda_helper(self.ve_count)\n        self.ff = self._cuda_helper(self.ff)\n        self.ff_count = self._cuda_helper(self.ff_count)\n        self.ef = self._cuda_helper(self.ef)\n        self.ef_count = self._cuda_helper(self.ef_count)\n        self.ee = self._cuda_helper(self.ee)\n        self.ee_count = self._cuda_helper(self.ee_count)\n\n        self.device = self.vertices.device\n\n    def cpu(self):\n        r""""""""Maps all tensors of the current class to CPU. """"""\n\n        self.vertices = self._cpu_helper(self.vertices)\n\n        self.faces = self._cpu_helper(self.faces)\n        self.uvs = self._cpu_helper(self.uvs)\n        self.face_textures = self._cpu_helper(self.face_textures)\n        self.textures = self._cpu_helper(self.textures)\n        self.edges = self._cpu_helper(self.edges)\n        self.vv = self._cpu_helper(self.vv)\n        self.vv_count = self._cpu_helper(self.vv_count)\n        self.vf = self._cpu_helper(self.vf)\n        self.vf_count = self._cpu_helper(self.vf_count)\n        self.ve = self._cpu_helper(self.ve)\n        self.ve_count = self._cpu_helper(self.ve_count)\n        self.ff = self._cpu_helper(self.ff)\n        self.ff_count = self._cpu_helper(self.ff_count)\n        self.ef = self._cpu_helper(self.ef)\n        self.ef_count = self._cpu_helper(self.ef_count)\n        self.ee = self._cpu_helper(self.ee)\n        self.ee_count = self._cpu_helper(self.ee_count)\n\n        self.device = self.vertices.device\n\n    def to(self, device):\n        r""""""Maps all tensors of the current class to the specified device. """"""\n\n        self.vertices = self._to_helper(self.vertices, device)\n\n        self.faces = self._to_helper(self.faces, device)\n        self.uvs = self._to_helper(self.uvs, device)\n        self.face_textures = self._to_helper(self.face_textures, device)\n        self.textures = self._to_helper(self.textures, device)\n        self.edges = self._to_helper(self.edges, device)\n        self.vv = self._to_helper(self.vv, device)\n        self.vv_count = self._to_helper(self.vv_count, device)\n        self.vf = self._to_helper(self.vf, device)\n        self.vf_count = self._to_helper(self.vf_count, device)\n        self.ve = self._to_helper(self.ve, device)\n        self.ve_count = self._to_helper(self.ve_count, device)\n        self.ff = self._to_helper(self.ff, device)\n        self.ff_count = self._to_helper(self.ff_count, device)\n        self.ef = self._to_helper(self.ef, device)\n        self.ef_count = self._to_helper(self.ef_count, device)\n        self.ee = self._to_helper(self.ee, device)\n        self.ee_count = self._to_helper(self.ee_count, device)\n\n        self.device = self.vertices.device\n\n\n    @staticmethod\n    def load_mtl(filename_mtl: str):\n        r"""""" Returns all colours and texture files found in an mtl files.\n\n        Args:\n                filename_mtl (str) : mtl file name\n\n        """"""\n        texture_filenames = {}\n        colors = {}\n        material_name = \'\'\n        with open(filename_mtl) as f:\n            for line in f.readlines():\n                if len(line.split()) != 0:\n                    if line.split()[0] == \'newmtl\':\n                        material_name = line.split()[1]\n                    if line.split()[0] == \'map_Kd\':\n                        texture_filenames[material_name] = line.split()[1]\n                    if line.split()[0] == \'Kd\':\n                        colors[material_name] = np.array(\n                            list(map(float, line.split()[1:4])))\n        return colors, texture_filenames\n\n    @classmethod\n    def load_textures(self, filename_obj: str, filename_mtl: str,\n                      texture_res: int):\n        r"""""" Returns texture for a given obj file, where texture is\n        defined using vertex texture uvs.\n\n        Args:\n            filename_obj (str) : obj file name\n            filename_mtl (str) : mtl file name\n            texture_res  (int) : texture resolution for each face\n\n\n        Returns:\n           textures (torch.Tensor) : texture values for each face\n\n        """"""\n        assert torch.cuda.is_available()\n        vertices = []\n        with open(filename_obj) as f:\n            lines = f.readlines()\n        for line in lines:\n            if len(line.split()) == 0:\n                continue\n            if line.split()[0] == \'vt\':\n                vertices.append([float(v) for v in line.split()[1:3]])\n        vertices = np.vstack(vertices).astype(np.float32)\n\n        # load faces for textures\n        faces = []\n        material_names = []\n        material_name = \'\'\n        for line in lines:\n            if len(line.split()) == 0:\n                continue\n            if line.split()[0] == \'f\':\n                vs = line.split()[1:]\n                nv = len(vs)\n                if \'/\' in vs[0] and \'//\' not in vs[0]:\n                    v0 = int(vs[0].split(\'/\')[1])\n                else:\n                    v0 = 0\n                for i in range(nv - 2):\n                    if \'/\' in vs[i + 1] and \'//\' not in vs[i + 1]:\n                        v1 = int(vs[i + 1].split(\'/\')[1])\n                    else:\n                        v1 = 0\n                    if \'/\' in vs[i + 2] and \'//\' not in vs[i + 2]:\n                        v2 = int(vs[i + 2].split(\'/\')[1])\n                    else:\n                        v2 = 0\n                    faces.append((v0, v1, v2))\n                    material_names.append(material_name)\n            if line.split()[0] == \'usemtl\':\n                material_name = line.split()[1]\n        faces = np.vstack(faces).astype(np.int32) - 1\n        faces = vertices[faces]\n        faces = torch.from_numpy(faces).cuda()\n        faces[1 < faces] = faces[1 < faces] % 1\n\n        colors, texture_filenames = self.load_mtl(filename_mtl)\n        textures = torch.ones(\n            faces.shape[0], texture_res**2, 3, dtype=torch.float32)\n        textures = textures.cuda()\n\n        for material_name, color in list(colors.items()):\n            color = torch.from_numpy(color).cuda()\n            for i, material_name_f in enumerate(material_names):\n                if material_name == material_name_f:\n                    textures[i, :, :] = color[None, :]\n\n        for material_name, filename_texture in list(texture_filenames.items()):\n            filename_texture = os.path.join(\n                os.path.dirname(filename_obj), filename_texture)\n            image = np.array(Image.open(filename_texture)\n                             ).astype(np.float32) / 255.\n\n            # texture image may have one channel (grey color)\n            if len(image.shape) == 2:\n                image = np.stack((image,) * 3, -1)\n            # or has extral alpha channel shoule ignore for now\n            if image.shape[2] == 4:\n                image = image[:, :, :3]\n\n            # pytorch does not support negative slicing for the moment\n            image = image[::-1, :, :]\n            image = torch.from_numpy(image.copy()).cuda()\n            is_update = (np.array(material_names)\n                         == material_name).astype(np.int32)\n            is_update = torch.from_numpy(is_update).cuda()\n            textures = load_textures_cuda.load_textures(\n                image, faces, textures, is_update)\n        return textures\n\n    @staticmethod\n    def get_edges_from_face(f: torch.Tensor):\n        """"""Returns a list of edges forming the current face.\n\n        Args:\n            f: Face (quadruplet of indices into \'vertices\').\n            vertices (torch.Tensor): Vertices (3D points).\n\n        Returns:\n            edge_inds (list): List of tuples (a, b) for each edge (a, b) in\n                faces.\n        """"""\n        _assert_tensor(f)\n        n = f.numel()\n        edges = []\n        for i in range(n):\n            if f[i] < f[(i + 1) % n]:\n                edges.append((f[i].item(), f[(i + 1) % n].item()))\n            else:\n                edges.append((f[(i + 1) % n].item(), f[i].item()))\n        return edges\n\n    @staticmethod\n    def get_edge_order(a: int, b: int):\n        """""" Returns (a, b) or (b, a), depending on which is smaller.\n        (Smaller element first, for unique keys)\n\n        Args:\n            a (int): Index of first vertex in edge.\n            b (int): Index of second vertex in edge.\n\n        """"""\n        return (a, b) if a < b else (b, a)\n\n    @staticmethod\n    def has_common_vertex(e1: torch.Tensor, e2: torch.Tensor):\n        r""""""Returns True if the vertices e1, e2 share a common vertex,\n        False otherwise.\n\n        Args:\n            e1 (torch.Tensor): First edge (shape: :math:`2`).\n            e2 (torch.Tensor): Second edge (shape: :math: `2`).\n\n        Returns:\n            (bool): Whether or not e1 and e2 share a common vertex.\n\n        """"""\n        return (e1[0] in e2) or (e1[1] in e2)\n\n    @staticmethod\n    def get_common_vertex(e1: torch.Tensor, e2: torch.Tensor):\n        r""""""Returns the common vertex in edges e1 and e2 (if any).\n\n        Args:\n            e1 (torch.Tensor): First edge (shape: :math:`2`).\n            e2 (torch.Tensor): Second edge (shape: :math:`2`).\n\n        Returns:\n            common_vertex (torch.LongTensor): Index of common vertex\n                    (shape: :math:`1`).\n            first_nbr (torch.LongTensor): Index of one neighbouring\n                    vertex of the common vertex (shape: :math:`1`).\n            second_nbr (torch.LongTensor): Index of the other neighbouring\n                    vertex of the common vertex (shape: :math:`1`).\n\n        """"""\n        if e1[0] == e2[0]:\n            return e1[0], e1[1], e2[1]\n        if e1[0] == e2[1]:\n            return e1[0], e1[1], e2[0]\n        if e1[1] == e2[0]:\n            return e1[1], e1[0], e2[1]\n        if e1[1] == e2[1]:\n            return e1[1], e1[0], e2[0]\n        return None, None, None\n\n    @staticmethod\n    def list_of_lists_to_matrix(\n            list_of_lists: list, sublist_lengths: torch.Tensor, matrix: torch.Tensor):\n        r""""""Takes a list of lists (each sub-list of variable size), and maps it\n        to a matrix. Decorated by numba, for efficiency sake.\n\n        Args:\n            list_of_lists (list): A list containing \'sub-\'lists (Note: the sub-list\n                    cannont contain lists; needs to contain numbers).\n            sublist_lengths (torch.Tensor): Array containing lengths of each sublist.\n            matrix (torch.Tensor): Matrix in which to `mould` the list\n                    (Note: the matrix must contain as many columns as required to\n                    encapsulate the largest sub-list of `list_of_lists`).\n\n        """"""\n        for i in range(matrix.shape[0]):\n            l = sublist_lengths[i]\n            if l > 0:\n                matrix[i, 0:l] = list_of_lists[i]\n        return matrix\n\n    @staticmethod\n    def compute_adjacency_info(vertices: torch.Tensor, faces: torch.Tensor):\n        """"""Build data structures to help speed up connectivity queries. Assumes\n        a homogeneous mesh, i.e., each face has the same number of vertices.\n\n        The outputs have the following format: AA, AA_count\n\n        AA_count: ``[count_0, ..., count_n]``\n\n        with AA:\n\n        .. code-block::\n\n            [[aa_{0,0}, ..., aa_{0,count_0} (, -1, ..., -1)],\n            [aa_{1,0}, ..., aa_{1,count_1} (, -1, ..., -1)],\n                        ...\n            [aa_{n,0}, ..., aa_{n,count_n} (, -1, ..., -1)]]\n        """"""\n\n        device = vertices.device\n        facesize = faces.shape[1]\n        nb_vertices = vertices.shape[0]\n        nb_faces = faces.shape[0]\n        edges = torch.cat([faces[:,i:i+2] for i in range(facesize - 1)] +\n                          [faces[:,[-1,0]]], dim=0)\n        # Sort the vertex of edges in increasing order\n        edges = torch.sort(edges, dim=1)[0]\n        # id of corresponding face in edges\n        face_ids = torch.arange(nb_faces, device=device, dtype=torch.long).repeat(facesize)\n        # remove multiple occurences and sort by the first vertex\n        # the edge key / id is fixed from now as the first axis position\n        # edges_ids will give the key of the edges on the original vector\n        edges, edges_ids = torch.unique(edges, sorted=True, return_inverse=True, dim=0)\n        nb_edges = edges.shape[0]\n\n        # EDGE2EDGES\n        _edges_ids = edges_ids.reshape(facesize, nb_faces)\n        edges2edges = torch.cat([\n            torch.stack([_edges_ids[1:], _edges_ids[:-1]], dim=-1).reshape(-1, 2),\n            torch.stack([_edges_ids[-1:], _edges_ids[:1]], dim=-1).reshape(-1, 2)\n        ], dim=0)\n\n        double_edges2edges = torch.cat([edges2edges, torch.flip(edges2edges, dims=(1,))], dim=0)\n        double_edges2edges = torch.cat(\n            [double_edges2edges, torch.arange(double_edges2edges.shape[0], device=device, dtype=torch.long).reshape(-1, 1)], dim=1)\n        double_edges2edges = torch.unique(double_edges2edges, sorted=True, dim=0)[:,:2]\n        idx_first = torch.where(\n            torch.nn.functional.pad(double_edges2edges[1:,0] != double_edges2edges[:-1,0],\n                                    (1, 0), value=1))[0]\n        nb_edges_per_edge = idx_first[1:] - idx_first[:-1]\n        offsets = torch.zeros(double_edges2edges.shape[0], device=device, dtype=torch.long)\n        offsets[idx_first[1:]] = nb_edges_per_edge\n        sub_idx = (torch.arange(double_edges2edges.shape[0], device=device,dtype=torch.long) -\n                   torch.cumsum(offsets, dim=0))\n        nb_edges_per_edge = torch.cat([nb_edges_per_edge,\n                                       double_edges2edges.shape[0] - idx_first[-1:]],\n                                      dim=0)\n        max_sub_idx = torch.max(nb_edges_per_edge)\n        ee = torch.full((nb_edges, max_sub_idx), device=device, dtype=torch.long, fill_value=-1)\n        ee[double_edges2edges[:,0], sub_idx] = double_edges2edges[:,1]\n\n        # EDGE2FACE\n        sorted_edges_ids, order_edges_ids = torch.sort(edges_ids)\n        sorted_faces_ids = face_ids[order_edges_ids]\n        # indices of first occurences of each key\n        idx_first = torch.where(\n            torch.nn.functional.pad(sorted_edges_ids[1:] != sorted_edges_ids[:-1],\n                                    (1,0), value=1))[0]\n        nb_faces_per_edge = idx_first[1:] - idx_first[:-1]\n        # compute sub_idx (2nd axis indices to store the faces)\n        offsets = torch.zeros(sorted_edges_ids.shape[0], device=device, dtype=torch.long)\n        offsets[idx_first[1:]] = nb_faces_per_edge\n        sub_idx = (torch.arange(sorted_edges_ids.shape[0], device=device, dtype=torch.long) -\n                   torch.cumsum(offsets, dim=0))\n        # TODO(cfujitsang): potential way to compute sub_idx differently\n        #                   to test with bigger model\n        #sub_idx = torch.ones(sorted_edges_ids.shape[0], device=device, dtype=torch.long)\n        #sub_idx[0] = 0\n        #sub_idx[idx_first[1:]] = 1 - nb_faces_per_edge\n        #sub_idx = torch.cumsum(sub_idx, dim=0)\n        nb_faces_per_edge = torch.cat([nb_faces_per_edge,\n                                       sorted_edges_ids.shape[0] - idx_first[-1:]],\n                                      dim=0)\n        max_sub_idx = torch.max(nb_faces_per_edge)\n        ef = torch.full((nb_edges, max_sub_idx), device=device, dtype=torch.long, fill_value=-1)\n        ef[sorted_edges_ids, sub_idx] = sorted_faces_ids\n        # FACE2FACES\n        nb_faces_per_face = torch.stack([nb_faces_per_edge[edges_ids[i*nb_faces:(i+1)*nb_faces]]\n                                         for i in range(facesize)], dim=1).sum(dim=1) - facesize\n        ff = torch.cat([ef[edges_ids[i*nb_faces:(i+1)*nb_faces]] for i in range(facesize)], dim=1)\n        # remove self occurences\n        ff[ff == torch.arange(nb_faces, device=device, dtype=torch.long).view(-1,1)] = -1\n        ff = torch.sort(ff, dim=-1, descending=True)[0]\n        to_del = (ff[:,1:] == ff[:,:-1]) & (ff[:,1:] != -1)\n        ff[:,1:][to_del] = -1\n        nb_faces_per_face = nb_faces_per_face - torch.sum(to_del, dim=1)\n        max_sub_idx = torch.max(nb_faces_per_face)\n        ff = torch.sort(ff, dim=-1, descending=True)[0][:,:max_sub_idx]\n\n        # VERTEX2VERTICES and VERTEX2EDGES\n        npy_edges = edges.cpu().numpy()\n        edge2key = {tuple(npy_edges[i]): i for i in range(nb_edges)}\n        #_edges and double_edges 2nd axis correspond to the triplet:\n        # [left vertex, right vertex, edge key]\n        _edges = torch.cat([edges, torch.arange(nb_edges, device=device).view(-1, 1)],\n                           dim=1)\n        double_edges = torch.cat([_edges, _edges[:,[1,0,2]]], dim=0)\n        double_edges = torch.unique(double_edges, sorted=True, dim=0)\n        # TODO(cfujitsang): potential improvment, to test with bigger model:\n        #double_edges0, order_double_edges = torch.sort(double_edges[0])\n        nb_double_edges = double_edges.shape[0]\n        # indices of first occurences of each key\n        idx_first = torch.where(\n            torch.nn.functional.pad(double_edges[1:,0] != double_edges[:-1,0],\n                                    (1,0), value=1))[0]\n        nb_edges_per_vertex = idx_first[1:] - idx_first[:-1]\n        # compute sub_idx (2nd axis indices to store the edges)\n        offsets = torch.zeros(nb_double_edges, device=device, dtype=torch.long)\n        offsets[idx_first[1:]] = nb_edges_per_vertex\n        sub_idx = (torch.arange(nb_double_edges, device=device, dtype=torch.long) -\n                   torch.cumsum(offsets, dim=0))\n        nb_edges_per_vertex = torch.cat([nb_edges_per_vertex,\n                                         nb_double_edges - idx_first[-1:]], dim=0)\n        max_sub_idx = torch.max(nb_edges_per_vertex)\n        vv = torch.full((nb_vertices, max_sub_idx), device=device, dtype=torch.long, fill_value=-1)\n        vv[double_edges[:,0], sub_idx] = double_edges[:,1]\n        ve = torch.full((nb_vertices, max_sub_idx), device=device, dtype=torch.long, fill_value=-1)\n        ve[double_edges[:,0], sub_idx] = double_edges[:,2]\n\n        # VERTEX2FACES\n        vertex_ordered, order_vertex = torch.sort(faces.view(-1))\n        face_ids_in_vertex_order = order_vertex / facesize\n        # indices of first occurences of each id\n        idx_first = torch.where(\n            torch.nn.functional.pad(vertex_ordered[1:] != vertex_ordered[:-1], (1,0), value=1))[0]\n        nb_faces_per_vertex = idx_first[1:] - idx_first[:-1]\n        # compute sub_idx (2nd axis indices to store the faces)\n        offsets = torch.zeros(vertex_ordered.shape[0], device=device, dtype=torch.long)\n        offsets[idx_first[1:]] = nb_faces_per_vertex\n        sub_idx = (torch.arange(vertex_ordered.shape[0], device=device, dtype=torch.long) -\n                   torch.cumsum(offsets, dim=0))\n        # TODO(cfujitsang): it seems that nb_faces_per_vertex == nb_edges_per_vertex ?\n        nb_faces_per_vertex = torch.cat([nb_faces_per_vertex,\n                                         vertex_ordered.shape[0] - idx_first[-1:]], dim=0)\n        max_sub_idx = torch.max(nb_faces_per_vertex)\n        vf = torch.full((nb_vertices, max_sub_idx), device=device, dtype=torch.long, fill_value=-1)\n        vf[vertex_ordered, sub_idx] = face_ids_in_vertex_order\n\n        return edge2key, edges, vv, nb_edges_per_vertex, ve, nb_edges_per_vertex, vf, \\\n            nb_faces_per_vertex, ff, nb_faces_per_face, ee, nb_edges_per_edge, ef, nb_faces_per_edge\n\n\n    @staticmethod\n    def old_compute_adjacency_info(vertices: torch.Tensor, faces: torch.Tensor):\n        """"""Build data structures to help speed up connectivity queries. Assumes\n        a homogeneous mesh, i.e., each face has the same number of vertices.\n\n        """"""\n        \n        device = vertices.device\n\n        facesize = faces.shape[1]\n\n        # Dictionary to hash each edge\n        edge2key = dict()\n        # List of edges\n        edges = []\n        # List of neighboring vertices to each vertex\n        vertex_vertex_nbd = [set() for _ in vertices]\n        # List of neighboring edges to each vertex\n        vertex_edge_nbd = [set() for _ in vertices]\n        # List of neighboring faces to each vertex\n        vertex_face_nbd = [set() for _ in vertices]\n        # List of neighboring edges to each edge\n        edge_edge_nbd = []\n        # List of neighboring faces to each edge\n        edge_face_nbd = []\n        # List of neighboring faces to each face\n        face_face_nbd = [set() for _ in faces]\n        # Counter for edges\n        num_edges = 0\n\n        for fid, f in enumerate(faces):\n\n            # Get a list of edges in the current face\n            face_edges = Mesh.get_edges_from_face(f)\n            # Run a pass through the edges, and add any new\n            # edges found, to the list of edges. Also, initialize\n            # corresponding neighborhood info.\n            for idx, edge in enumerate(face_edges):\n                if edge not in edge2key:\n                    edge2key[edge] = num_edges\n                    edges.append(list(edge))\n                    edge_edge_nbd.append([])\n                    edge_face_nbd.append([fid])\n                    vertex_edge_nbd[edge[0]].add(num_edges)\n                    vertex_edge_nbd[edge[1]].add(num_edges)\n                    num_edges += 1\n            # Now, run another pass through the edges, this time to\n            # compute adjacency info.\n            for idx, edge in enumerate(face_edges):\n                k = edge2key[edge]\n                for j in range(1, facesize):\n                    q = edge2key[face_edges[(idx + j) % facesize]]\n                    common_vtx, first_nbr, second_nbr = Mesh.get_common_vertex(\n                        edges[k], edges[q])\n                    edge_edge_nbd[k].append(q)\n                    if common_vtx:\n                        vertex_vertex_nbd[common_vtx].add(first_nbr)\n                        vertex_vertex_nbd[common_vtx].add(second_nbr)\n                        vertex_vertex_nbd[first_nbr].add(common_vtx)\n                        vertex_vertex_nbd[second_nbr].add(common_vtx)\n\n                # q = edge2key[face_edges[(idx+1)%facesize]]\n                # r = edge2key[face_edges[(idx+2)%facesize]]\n                # s = edge2key[face_edges[(idx+3)%facesize]]\n                # if Mesh.has_common_vertex(edges[k], edges[q]):\n                #     edge_edge_nbd[k].append(q)\n                # if Mesh.has_common_vertex(edges[k], edges[r]):\n                #     edge_edge_nbd[k].append(r)\n                # if Mesh.has_common_vertex(edges[k], edges[s]):\n                #     edge_edge_nbd[k].append(s)\n                if fid not in edge_face_nbd[k]:\n                    edge_face_nbd[k].append(fid)\n                vertex_edge_nbd[edge[0]].add(k)\n                vertex_edge_nbd[edge[1]].add(k)\n                vertex_face_nbd[edge[0]].add(fid)\n                vertex_face_nbd[edge[1]].add(fid)\n        # Compute face-face adjacency info\n        for fid, f in enumerate(faces):\n            face_edges = Mesh.get_edges_from_face(f)\n            for idx, edge in enumerate(face_edges):\n                k = edge2key[edge]\n                for nbr in edge_face_nbd[k]:\n                    if nbr == fid:\n                        continue\n                    face_face_nbd[fid].add(nbr)\n\n        # Helper variables\n        N = vertices.shape[0]\n        M = len(edges)\n        P = faces.shape[0]\n\n        # Convert sets to lists in vertex_edge_nbd, vertex_face_nbd, and\n        # face_face_nbd\n        vertex_vertex_nbd = [torch.Tensor(list(l)).long().to(device)\n                             for l in vertex_vertex_nbd]\n        vertex_edge_nbd = [torch.Tensor(list(l)).long().to(device)\n                           for l in vertex_edge_nbd]\n        vertex_face_nbd = [torch.Tensor(list(l)).long().to(device)\n                           for l in vertex_face_nbd]\n        face_face_nbd = [torch.Tensor(list(l)).long().to(device)\n                         for l in face_face_nbd]\n        edge_edge_nbd = [torch.Tensor(l).long().to(device)\n                         for l in edge_edge_nbd]\n        edge_face_nbd = [torch.Tensor(l).long().to(device)\n                         for l in edge_face_nbd]\n\n        # Map vertex_vertex_nbd to a matrix\n        vv_count = torch.Tensor([len(l) for l in vertex_vertex_nbd]).long()\n        vv_max = max(vv_count)\n        vv = -torch.ones((N, vv_max)).long().to(device)\n        vv = Mesh.list_of_lists_to_matrix(vertex_vertex_nbd, vv_count, vv)\n\n        # Map vertex_edge_nbd to a matrix\n        ve_count = torch.Tensor([len(l) for l in vertex_edge_nbd]).long()\n        ve_max = max(ve_count)\n        ve = -torch.ones((N, ve_max)).long().to(device)\n        ve = Mesh.list_of_lists_to_matrix(vertex_edge_nbd, ve_count, ve)\n\n        # Map vertex_face_nbd to a matrix\n        vf_count = torch.Tensor([len(l) for l in vertex_face_nbd]).long()\n        vf_max = max(vf_count)\n        vf = -torch.ones((N, vf_max)).long().to(device)\n        vf = Mesh.list_of_lists_to_matrix(vertex_face_nbd, vf_count, vf)\n\n        # Map edge_edge_nbd to a matrix\n        ee_count = torch.Tensor([len(l) for l in edge_edge_nbd]).long()\n        ee_max = max(ee_count)\n        ee = -torch.ones((M, ee_max)).long().to(device)\n        ee = Mesh.list_of_lists_to_matrix(edge_edge_nbd, ee_count, ee)\n\n        # Map edge_face_nbd to a matrix\n        ef_count = torch.Tensor([len(l) for l in edge_face_nbd]).long()\n        ef_max = max(ef_count)\n        ef = -torch.ones((M, ef_max)).long().to(device)\n        ef = Mesh.list_of_lists_to_matrix(edge_face_nbd, ef_count, ef)\n\n        # Map face_face_nbd to a matrix\n        ff_count = torch.Tensor([len(l) for l in face_face_nbd]).long()\n        ff_max = max(ff_count)\n        ff = -torch.ones((P, ff_max)).long().to(device)\n        ff = Mesh.list_of_lists_to_matrix(face_face_nbd, ff_count, ff)\n\n        # Convert to numpy arrays\n        edges = torch.Tensor(edges).long().to(device)\n\n        return edge2key, edges, vv, vv_count, ve, ve_count, vf, vf_count, \\\n            ff, ff_count, ee, ee_count, ef, ef_count\n\n    def laplacian_smoothing(self, iterations: int = 1):\n        r"""""" Applies laplacian smoothing to the mesh.\n\n            Args:\n                iterations (int) : number of iterations to run the algorithm for.\n\n            Example:\n                >>> mesh = Mesh.from_obj(\'model.obj\')\n                >>> mesh.compute_laplacian().abs().mean()\n                tensor(0.0010)\n                >>> mesh.laplacian_smoothing(iterations=3)\n                >>> mesh.compute_laplacian().abs().mean()\n                tensor(9.9956e-05)\n    """"""\n\n        adj_sparse = self.compute_adjacency_matrix_sparse()\n\n        neighbor_num = torch.sparse.sum(\n            adj_sparse, dim=1).to_dense().view(-1, 1)\n\n        for _ in range(iterations):\n            neighbor_sum = torch.sparse.mm(adj_sparse, self.vertices)\n            self.vertices = neighbor_sum / neighbor_num\n\n    def compute_laplacian(self):\n        r""""""Calcualtes the laplcaian of the graph, meaning the average\n                difference between a vertex and its neighbors.\n\n            Returns:\n                (FloatTensor) : laplacian of the mesh.\n\n            Example:\n                >>> mesh = Mesh.from_obj(\'model.obj\')\n                >>> lap = mesh.compute_laplacian()\n\n        """"""\n\n        adj_sparse = self.compute_adjacency_matrix_sparse()\n\n        neighbor_sum = torch.sparse.mm(\n            adj_sparse, self.vertices) - self.vertices\n        neighbor_num = torch.sparse.sum(\n            adj_sparse, dim=1).to_dense().view(-1, 1) - 1\n        neighbor_num[neighbor_num == 0] = 1\n        neighbor_num = (1. / neighbor_num).view(-1, 1)\n\n        neighbor_sum = neighbor_sum * neighbor_num\n        lap = self.vertices - neighbor_sum\n        return lap\n\n    def show(self):\n        r"""""" Visuailizes the mesh.\n\n            Example:\n                >>> mesh = Mesh.from_obj(\'model.obj\')\n                >>> mesh.show()\n\n        """"""\n\n        kal.visualize.show_mesh(self)\n\n    def save_tensors(self, filename: (str)):\n        r""""""Saves the tensor information of the mesh in a numpy .npz format.\n\n        Args:\n            filename: the file name to save the file under\n\n        Example:\n            >>> mesh = Mesh.from_obj(\'model.obj\')\n            >>> mesh.save_tensors()\n\n        """"""\n        np.savez(filename, vertices=self.vertices.data.cpu().numpy(),\n                 faces=self.faces.data.cpu().numpy())\n\n    @staticmethod\n    def normalize_zerosafe(matrix: torch.Tensor):\n        """"""Normalizes each row of a matrix in a \'division by zero\'-safe way.\n\n        Args:\n            matrix (torch.tensor): Matrix where each row contains a vector\n                to be normalized.\n\n        """"""\n\n        assert matrix.dim() == 2, \'Need matrix to contain exactly 2 dimensions\'\n        magnitude = torch.sqrt(torch.sum(torch.pow(matrix, 2), dim=1))\n        valid_inds = magnitude > 0\n        matrix[valid_inds] = torch.div(matrix[valid_inds], magnitude[\n                                       valid_inds].unsqueeze(1))\n        return matrix\n\n    def sample(self, num_points):\n        raise NotImplementedError\n\n    def compute_vertex_normals(self):\n        raise NotImplementedError\n\n    def compute_edge_lengths(self):\n        raise NotImplementedError\n\n    def compute_face_areas(self):\n        raise NotImplementedError\n\n    def compute_interior_angles_per_edge(self):\n        raise NotImplementedError\n\n    def compute_dihedral_angles_per_edge(self):\n        raise NotImplementedError\n\n    def __getstate__(self):\n        outputs = {\'vertices\': self.vertices,\n                   \'faces\': self.faces}\n        if self.uvs is not None:\n            outputs[\'uvs\'] = self.uvs\n        if self.face_textures is not None:\n            outputs[\'face_textures\'] = self.face_textures\n        if self.textures is not None:\n            outputs[\'textures\'] = self.textures\n        return outputs\n\n    def __setstate__(self, args):\n        self.vertices = args[\'vertices\']\n        self.faces = args[\'faces\']\n        if \'uvs\' in args:\n            self.uvs = args[\'uvs\']\n        if \'face_textures\' in args:\n            self.face_textures = args[\'face_textures\']\n        if \'textures\' in args:\n            self.textures = args[\'textures\']\n'"
kaolin/rep/PointCloud.py,15,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional\n\nimport numpy as np\nimport torch\n\nfrom kaolin import helpers\nimport kaolin as kal\n\n\nclass PointCloud(object):\n    r""""""Base class to hold pointcloud objects. """"""\n\n    def __init__(self, points: Optional[torch.Tensor] = None,\n                 normals: torch.Tensor = None, device: Optional[str] = \'cpu\',\n                 copy: Optional[bool] = False):\n        r""""""Initialize a PointCloud object, given a tensor of points, and\n        optionally, a tensor representing poincloud normals.\n\n        Args:\n            pts (torch.Tensor): Points that make up the pointcloud (shape:\n                :math:`... \\times N \\times D`), where :math:`N` denotes the\n                number of points in the cloud, and :math:`D` denotes the\n                dimensionality of each point.\n            normals (torch.Tensor): Normals for each point in the cloud\n                (shape: :math:`N \\times D`, where `D` = 2 or `D` = 3).\n                That is, normals can only be provided for 2D or 3D pointclouds.\n            device (str, Optional): Device to store the pointcloud object on\n                (default: \'cpu\'). Must be a valid `torch.device` type.\n            copy (bool, Optional): Whether or not to create a deep copy of the\n                Tensor(s) used to initialze class members.\n\n        """"""\n        if points is None:\n            self.points = None\n        else:\n            helpers._assert_tensor(points)\n            helpers._assert_dim_ge(points, 2)\n            self.points = points.clone() if copy else points\n            self.points = self.points.to(device)\n        if normals is None:\n            self.normals = None\n        else:\n            helpers._assert_tensor(normals)\n            if points.dim() == 2:\n                helpers._assert_shape_eq(normals, (points.shape[-2], 3))\n            self.normals = normals.clone() if copy else normals\n            self.normals = self.normals.to(device)\n\n    def show(self):\n        kal.visualize.show_pointcloud(self.points)\n\n\ndef bounding_points(points: torch.Tensor, bbox: list, padding: float = .05):\n    r""""""Returns the indices of a set of points which lies within a supplied\n    bounding box.\n\n    Args:\n        point (torch.Tensor) : Input pointcloud\n        bbox (list) : bouding box values (min_x, max_x, min_y, max_y,\n            min_z, max_z)\n        padding (float) : padding to add to bounding box\n\n    Returns:\n        (list): list of indices which lie within supplied bounding box\n\n    Example:\n        >>> points = torch.rand(1000)\n        >>> subset_idx = bounding_points(points, [.1, .9, .1, .9, .1, .9])\n        >>> subset = points[subset_idx]\n    """"""\n\n    x_vals = points[:, 0] >= (bbox[0] - padding)\n    x_vals = x_vals & (points[:, 0] <= (bbox[1] + padding))\n\n    y_vals = points[:, 1] >= (bbox[2] - padding)\n    y_vals = y_vals & (points[:, 1] <= (bbox[3] + padding))\n\n    z_vals = points[:, 2] >= (bbox[4] - padding)\n    z_vals = z_vals & (points[:, 2] <= (bbox[5] + padding))\n\n    sample_box = x_vals & (y_vals & z_vals)\n\n    return sample_box\n\n\ndef random_input_dropout(points: torch.Tensor,\n                         max_dropout_rate: float = 0.95):\n    """"""Returns a copy of the given cloud with points randomly removed\n    according to max_dropout_rate.\n\n    For each batch, first select a dropout_rate from the uniform distribution\n    [0, max_dropout_rate], then  remove (i.e. set to an existing point)\n    with a probability equal to the dropout rate.\n\n    Based on the technique described in PointNet++.\n\n    Args:\n        points (torch.Tensor or np.ndarray): Input pointcloud\n            shape = (batch_size, num_points, num_dim) or (num_points, num_dim)\n        max_dropout_rate (float): See method description above.\n\n    """"""\n\n    if isinstance(points, np.ndarray):\n        points = torch.from_numpy(points)\n\n    if not torch.is_tensor(points):\n        raise TypeError(\'Expected type torch.Tensor. Got {} instead.\'.format(\n            type(points)))\n\n    assert points.dim() == 2 or points.dim(\n    ) == 3, \'Point cloud must contain exactly 2 or 3 dimensions.\'\n\n    batched = False\n    if points.dim() == 3:\n        batched = True\n\n    else:\n        points = points.unsqueeze(0)\n\n    batch_size = points.shape[0]\n    dropout_rates = torch.FloatTensor(\n        batch_size, 1).uniform_(0, max_dropout_rate)\n\n    r = torch.rand(*points.shape[:2])\n    points = points.clone()\n    points[r < dropout_rates, :] = points[0, 0, :]\n\n    if not batched:\n        points = points.squeeze(0)\n\n    return points\n'"
kaolin/rep/QuadMesh.py,49,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import abstractmethod\n\nimport torch\n\nfrom kaolin.helpers import _composedecorator\nfrom kaolin.rep.Mesh import Mesh\nimport numpy as np\n\n\nclass QuadMesh(Mesh):\n    """""" Abstract class to represent 3D Quad meshes. """"""\n\n    def __init__(self, vertices: torch.Tensor, faces: torch.Tensor,\n                 uvs: torch.Tensor, face_textures: torch.Tensor,\n                 textures: torch.Tensor, edges: torch.Tensor, edge2key: dict, vv: torch.Tensor,\n                 vv_count: torch.Tensor, vf: torch.Tensor, vf_count: torch.Tensor,\n                 ve: torch.Tensor, ve_count: torch.Tensor, ff: torch.Tensor,\n                 ff_count: torch.Tensor, ef: torch.Tensor, ef_count: torch.Tensor,\n                 ee: torch.Tensor, ee_count: torch.Tensor):\n\n        # Vertices of the mesh\n        self.vertices = vertices\n        # Faces of the mesh\n        self.faces = faces\n        # uv coordinates of each vertex\n        self.uvs = uvs\n        # uv indecies for each face\n        self.face_textures = face_textures\n        # texture for each face\n        self.textures = textures\n        # Edges of the mesh\n        self.edges = edges\n        # Dictionary that maps an edge (tuple) to an edge idx\n        self.edge2key = edge2key\n        # Vertex-Vertex neighborhood tensor (for each vertex, contains\n        # indices of the vertices neighboring it)\n        self.vv = vv\n        # Number of vertices neighbouring each vertex\n        self.vv_count = vv_count\n        # Vertex-Face neighborhood tensor\n        self.vf = vf\n        # Number of faces neighbouring each vertex\n        self.vf_count = vf_count\n        # Vertex-Edge neighborhood tensor\n        self.ve = ve\n        # Number of edges neighboring each vertex\n        self.ve_count = ve_count\n        # Face-Face neighborhood tensor\n        self.ff = ff\n        # Number of faces neighbouring each face\n        self.ff_count = ff_count\n        # Edge-Face neighbourhood tensor\n        self.ef = ef\n        # Number of edges neighbouring each face\n        self.ef_count = ef_count\n        # Edge-Edge neighbourhood tensor\n        self.ee = ee\n        # Number of edges neighbouring each edge\n        self.ee_count = ee_count\n        # adjacency matrix for verts\n        self.adj = None\n\n        # Initialize device on which tensors reside.\n        self.device = self.vertices.device\n\n    def save_mesh(self, filename):\n        r"""""" Save a mesh to a wavefront .obj file format\n\n            Args:\n                filename (str) : target filename\n                verts (FloatTensor) : vertices of the mesh\n                faces (LongTensor) : list of vertex indexes for each face\n\n            Example:\n                >>> verts, faces = load_obj(\'object.obj\')\n                >>> verts = verts * 20\n                >>> save_mesh(\'larger_object.obj\', verts, faces)\n        """"""\n\n        with open(filename, \'w\') as f:\n\n            # write verts\n            for vert in self.vertices:\n                f.write(\'v %f %f %f\\n\' % tuple(vert))\n            # write faces\n            for face in self.faces:\n                f.write(\'f %d %d %d %d\\n\' % tuple(face + 1))\n\n    def sample(self, num_samples: int):\n        r""""""Uniformly samples the surface of a mesh.\n\n            Args:\n                num_samples (int): number of points to sample.\n\n            Returns:\n                points (torch.Tensor): uniformly sampled points\n                face_choices (torch.Tensor): the face indexes which each point corresponds to.\n\n            Example:\n                >>> points, chosen_faces = mesh.sample(10)\n                >>> points\n                tensor([[ 0.0293,  0.2179,  0.2168],\n                        [ 0.2003, -0.3367,  0.2187],\n                        [ 0.2152, -0.0943,  0.1907],\n                        [-0.1852,  0.1686, -0.0522],\n                        [-0.2167,  0.3171,  0.0737],\n                        [ 0.2219, -0.0289,  0.1531],\n                        [ 0.2217, -0.0115,  0.1247],\n                        [-0.1400,  0.0364, -0.1618],\n                        [ 0.0658, -0.0310, -0.2198],\n                        [ 0.1926, -0.1867, -0.2153]])\n                >>> chosen_faces\n                tensor([ 953,   38,    6, 3480,  563,  393,  395, 3309,  373,  271])\n\n        """"""\n\n        if self.vertices.is_cuda:\n            dist_uni = torch.distributions.Uniform(\n                torch.tensor([0.0]).cuda(), torch.tensor([1.0]).cuda())\n        else:\n            dist_uni = torch.distributions.Uniform(\n                torch.tensor([0.0]), torch.tensor([1.0]))\n        tri_faces_1 = torch.cat((self.faces[:, :2], self.faces[:, 3:]), dim=1)\n        tri_faces_2 = torch.cat((self.faces[:, :1], self.faces[:, 2:]), dim=1)\n        tri_faces = torch.cat((tri_faces_1, tri_faces_2))\n\n        # calculate area of each face\n        x1, x2, x3 = torch.split(torch.index_select(\n            self.vertices, 0, tri_faces[:, 0]) - torch.index_select(self.vertices, 0, tri_faces[:, 1]), 1, dim=1)\n        y1, y2, y3 = torch.split(torch.index_select(\n            self.vertices, 0, tri_faces[:, 1]) - torch.index_select(self.vertices, 0, tri_faces[:, 2]), 1, dim=1)\n        a = (x2 * y3 - x3 * y2)**2\n        b = (x3 * y1 - x1 * y3)**2\n        c = (x1 * y2 - x2 * y1)**2\n        Areas = torch.sqrt(a + b + c) / 2\n        # percentage of each face w.r.t. full surface area\n        Areas = Areas / torch.sum(Areas)\n\n        # define descrete distribution w.r.t. face area ratios caluclated\n        cat_dist = torch.distributions.Categorical(Areas.view(-1))\n        face_choices = cat_dist.sample([num_samples])\n\n        # from each face sample a point\n        select_faces = tri_faces[face_choices]\n        xs = torch.index_select(self.vertices, 0, select_faces[:, 0])\n        ys = torch.index_select(self.vertices, 0, select_faces[:, 1])\n        zs = torch.index_select(self.vertices, 0, select_faces[:, 2])\n        u = torch.sqrt(dist_uni.sample([num_samples]))\n        v = dist_uni.sample([num_samples])\n        points = (1 - u) * xs + (u * (1 - v)) * ys + u * v * zs\n\n        # redefining face choices to match quad faces\n        face_choices[face_choices >= self.faces.shape[\n            0]] -= self.faces.shape[0]\n\n        return points, face_choices\n\n    @classmethod\n    def compute_vertex_normals(self):\n        raise NotImplementedError\n\n    def compute_edge_lengths(self):\n        raise NotImplementedError\n\n    def compute_face_areas(self):\n        raise NotImplementedError\n\n    def compute_interior_angles_per_edge(self):\n        raise NotImplementedError\n\n    def compute_dihedral_angles_per_edge(self):\n        raise NotImplementedError\n\n    def load_tensors(filename: (str), enable_adjacency: bool = False):\n        r""""""Loads the tensor information of the mesh from a saved numpy array.\n\n        Args:\n            filename: the file name to load the file from.\n\n        Example:\n            >>> mesh = QuadMesh.load_tensors(\'mesh.npy\')\n\n        """"""\n        data = np.load(filename)\n\n        vertices = torch.FloatTensor(data[\'vertices\'])\n        faces = torch.LongTensor(data[\'faces\'].astype(int))\n\n        return QuadMesh.from_tensors(vertices, faces)\n\n    def compute_adjacency_matrix_full(self):\n        r"""""" calcualtes a binary adjacency matrix for a mesh\n\n            Returns:\n                (torch.Tensor) : binary adjacency matrix\n\n            Example:\n                >>> mesh = QuadMesh.from_obj(\'model.obj\')\n                >>> adj_info = mesh.compute_adjacency_matrix_full()\n                >>> neighborhood_sum = torch.mm( adj_info, mesh.vertices)\n        """"""\n\n        adj = torch.zeros((self.vertices.shape[0], self.vertices.shape[0])).to(\n            self.vertices.device)\n        v1 = self.faces[:, 0]\n        v2 = self.faces[:, 1]\n        v3 = self.faces[:, 2]\n        v4 = self.faces[:, 2]\n\n        adj[(v1, v1)] = 1\n        adj[(v2, v2)] = 1\n        adj[(v3, v3)] = 1\n        adj[(v4, v4)] = 1\n\n        adj[(v1, v2)] = 1\n        adj[(v2, v1)] = 1\n        adj[(v1, v4)] = 1\n        adj[(v4, v1)] = 1\n\n        adj[(v3, v2)] = 1\n        adj[(v2, v3)] = 1\n        adj[(v3, v4)] = 1\n        adj[(v4, v3)] = 1\n\n        return adj\n\n    def compute_adjacency_matrix_sparse(self):\n        r"""""" Calcualtes a sparse adjacency matrix for a mess\n\n            Returns:\n                (torch.sparse.Tensor) : sparse adjacency matrix\n\n            Example:\n                >>> mesh = Mesh.from_obj(\'model.obj\')\n                >>> adj_info = mesh.compute_adjacency_matrix_sparse()\n                >>> neighborhood_sum = torch.sparse.mm(adj_info, mesh.vertices)\n\n        """"""\n\n        if self.adj is None:\n\n            v1 = self.faces[:, 0].view(-1, 1)\n            v2 = self.faces[:, 1].view(-1, 1)\n            v3 = self.faces[:, 2].view(-1, 1)\n            v4 = self.faces[:, 2].view(-1, 1)\n\n            vert_len = self.vertices.shape[0]\n            identity_indices = torch.arange(vert_len).view(-1, 1).to(v1.device)\n            identity = torch.cat(\n                (identity_indices, identity_indices), dim=1).to(v1.device)\n            identity = torch.cat((identity, identity))\n\n            i_1 = torch.cat((v1, v2), dim=1)\n            i_2 = torch.cat((v1, v4), dim=1)\n\n            i_3 = torch.cat((v2, v1), dim=1)\n            i_4 = torch.cat((v2, v3), dim=1)\n\n            i_5 = torch.cat((v3, v2), dim=1)\n            i_6 = torch.cat((v3, v4), dim=1)\n\n            i_7 = torch.cat((v4, v3), dim=1)\n            i_8 = torch.cat((v4, v1), dim=1)\n\n            indices = torch.cat(\n                (identity, i_1, i_2, i_3, i_4, i_5, i_6, i_7, i_8), dim=0).t()\n            values = torch.ones(indices.shape[1]).to(indices.device) * .5\n            self.adj = torch.sparse.FloatTensor(\n                indices, values, torch.Size([vert_len, vert_len]))\n        return self.adj.clone()\n'"
kaolin/rep/SDF.py,19,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n# Occupancy Networks\n#\n# Copyright 2019 Lars Mescheder, Michael Oechsle, Michael Niemeyer, Andreas Geiger, Sebastian Nowozin\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy \n# of this software and associated documentation files (the ""Software""), to deal \n# in the Software without restriction, including without limitation the rights \n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell \n# copies of the Software, and to permit persons to whom the Software is \n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all \n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE \n# SOFTWARE.\n\nimport numpy as np\nimport torch\n\nimport kaolin as kal\nfrom kaolin.triangle_hash import TriangleHash as _TriangleHash\nimport kaolin.cuda.mesh_intersection as mint\n\n\nclass MeshIntersectionFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, points: torch.Tensor, verts_1: torch.Tensor,\n                verts_2: torch.Tensor, verts_3: torch.Tensor):\n        batchsize, n, _ = points.size()\n        points = points.contiguous()\n        verts_1 = verts_1.contiguous()\n        verts_2 = verts_2.contiguous()\n        verts_3 = verts_3.contiguous()\n\n        ints = torch.zeros(batchsize, n)\n        ints = ints.cuda()\n\n        mint.forward_cuda(points, verts_1, verts_2, verts_3, ints)\n        ctx.save_for_backward(ints)\n\n        return ints\n\n    @staticmethod\n    def backward(ctx, graddist1: torch.Tensor, graddist2: torch.Tensor):\n        ints = ctx.saved_tensors\n        gradxyz1 = torch.zeros(ints.size())\n        return gradxyz1\n\n\nclass MeshIntersection(torch.nn.Module):\n    def forward(self, points: torch.Tensor, verts_1: torch.Tensor,\n                verts_2: torch.Tensor, verts_3: torch.Tensor):\n        return MeshIntersectionFunction.apply(points, verts_1, verts_2,\n                                              verts_3)\n\n\ndef check_sign_fast(mesh, points):\n    intersector = MeshIntersection()\n    v1 = torch.index_select(mesh.vertices, 0, mesh.faces[:, 0]).view(1, -1, 3)\n    v2 = torch.index_select(mesh.vertices, 0, mesh.faces[:, 1]).view(1, -1, 3)\n    v3 = torch.index_select(mesh.vertices, 0, mesh.faces[:, 2]).view(1, -1, 3)\n    contains = intersector(points.view(1, -1, 3), v1, v2, v3)\n    contains = contains % 2 == 1\n    return contains\n\n\ndef check_sign(mesh, points, hash_resolution=512):\n    r"""""" Checks if a set of points is contained within a mesh\n\n    Args:\n        mesh (kal.rep.Mesh): mesh to check against\n        points (torch.Tensor): points to check\n        hash_resolution: resolution used to check the points sign\n\n    Returns:\n        bool value for every point inciating if point is inside object\n\n    Example:\n\n\n    """"""\n    assert mesh.device == points.device\n\n    if mesh.device.type == \'cuda\':\n        return check_sign_fast(mesh, points)\n    else:\n        intersector = _MeshIntersector(mesh, hash_resolution)\n        contains = intersector.query(points.data.cpu().numpy())\n        return contains\n\n\ndef _length(points):\n    return torch.sqrt(((points**2).sum(dim=1)))\n\n\ndef sphere(r=.5):\n\n    def eval_sdf(points):\n        return _length(points) - r\n    return eval_sdf\n\n\ndef box(h=.2, w=.4, l=.5):\n    def eval_sdf(points):\n        d = torch.abs(points)\n        d[:, 0] -= h\n        d[:, 1] -= w\n        d[:, 2] -= l\n        positive_len = _length(torch.max(d, torch.zeros(d.shape).to(d.device)))\n\n        negative_res = torch.max(d[:, 1], d[:, 2])\n        negative_res = torch.max(d[:, 0], negative_res)\n        negative_res = torch.min(negative_res, torch.zeros(\n            negative_res.shape).to(d.device))\n        positive_len = positive_len + negative_res\n\n        return positive_len\n    return eval_sdf\n\n\nclass _MeshIntersector:\n    r""""""Class to determine if a point in space lies within our outside a mesh.\n    """"""\n\n    def __init__(self, mesh, resolution=512):\n        triangles = mesh.vertices.data.cpu().numpy(\n        )[mesh.faces.data.cpu().numpy()].astype(np.float64)\n        n_tri = triangles.shape[0]\n\n        self.resolution = resolution\n        self.bbox_min = triangles.reshape(3 * n_tri, 3).min(axis=0)\n        self.bbox_max = triangles.reshape(3 * n_tri, 3).max(axis=0)\n        # Tranlate and scale it to [0.5, self.resolution - 0.5]^3\n        self.scale = (resolution - 1) / (self.bbox_max - self.bbox_min)\n        self.translate = 0.5 - self.scale * self.bbox_min\n\n        self._triangles = triangles = self.rescale(triangles)\n\n        triangles2d = triangles[:, :, :2]\n        self._tri_intersector2d = _TriangleIntersector2d(\n            triangles2d, resolution)\n\n    def query(self, points):\n        # Rescale points\n        points = self.rescale(points)\n\n        # placeholder result with no hits we\'ll fill in later\n        contains = np.zeros(len(points), dtype=np.bool)\n\n        # cull points outside of the axis aligned bounding box\n        # this avoids running ray tests unless points are close\n        inside_aabb = np.all(\n            (0 <= points) & (points <= self.resolution), axis=1)\n        if not inside_aabb.any():\n            return contains\n\n        # Only consider points inside bounding box\n        mask = inside_aabb\n        points = points[mask]\n\n        # Compute intersection depth and check order\n        points_indices, tri_indices = self._tri_intersector2d.query(\n            points[:, :2])\n\n        triangles_intersect = self._triangles[tri_indices]\n        points_intersect = points[points_indices]\n\n        depth_intersect, abs_n_2 = self.compute_intersection_depth(\n            points_intersect, triangles_intersect)\n\n        # Count number of intersections in both directions\n        smaller_depth = depth_intersect >= points_intersect[:, 2] * abs_n_2\n        bigger_depth = depth_intersect < points_intersect[:, 2] * abs_n_2\n        points_indices_0 = points_indices[smaller_depth]\n        points_indices_1 = points_indices[bigger_depth]\n\n        nintersect0 = np.bincount(points_indices_0, minlength=points.shape[0])\n        nintersect1 = np.bincount(points_indices_1, minlength=points.shape[0])\n\n        # Check if point contained in mesh\n        contains1 = (np.mod(nintersect0, 2) == 1)\n        contains2 = (np.mod(nintersect1, 2) == 1)\n        # if (contains1 != contains2).any():\n        #     print(\'Warning: contains1 != contains2 for some points.\')\n        contains[mask] = (contains1 & contains2)\n        return contains\n\n    def compute_intersection_depth(self, points, triangles):\n        t1 = triangles[:, 0, :]\n        t2 = triangles[:, 1, :]\n        t3 = triangles[:, 2, :]\n\n        v1 = t3 - t1\n        v2 = t2 - t1\n        # v1 = v1 / np.linalg.norm(v1, axis=-1, keepdims=True)\n        # v2 = v2 / np.linalg.norm(v2, axis=-1, keepdims=True)\n\n        normals = np.cross(v1, v2)\n        alpha = np.sum(normals[:, :2] * (t1[:, :2] - points[:, :2]), axis=1)\n\n        n_2 = normals[:, 2]\n        t1_2 = t1[:, 2]\n        s_n_2 = np.sign(n_2)\n        abs_n_2 = np.abs(n_2)\n\n        mask = (abs_n_2 != 0)\n\n        depth_intersect = np.full(points.shape[0], np.nan)\n        depth_intersect[mask] = \\\n            t1_2[mask] * abs_n_2[mask] + alpha[mask] * s_n_2[mask]\n\n        # Test the depth:\n        # TODO: remove and put into tests\n        # points_new = np.concatenate([points[:, :2], depth_intersect[:, None]], axis=1)\n        # alpha = (normals * t1).sum(-1)\n        # mask = (depth_intersect == depth_intersect)\n        # assert(np.allclose((points_new[mask] * normals[mask]).sum(-1),\n        #                    alpha[mask]))\n        return depth_intersect, abs_n_2\n\n    def rescale(self, array):\n        array = self.scale * array + self.translate\n        return array\n\n\nclass _TriangleIntersector2d:\n    def __init__(self, triangles, resolution=128):\n        self.triangles = triangles\n        self.tri_hash = _TriangleHash(triangles, resolution)\n\n    def query(self, points):\n        point_indices, tri_indices = self.tri_hash.query(points)\n        point_indices = np.array(point_indices, dtype=np.int64)\n        tri_indices = np.array(tri_indices, dtype=np.int64)\n        points = points[point_indices]\n        triangles = self.triangles[tri_indices]\n        mask = self.check_triangles(points, triangles)\n        point_indices = point_indices[mask]\n        tri_indices = tri_indices[mask]\n        return point_indices, tri_indices\n\n    def check_triangles(self, points, triangles):\n        contains = np.zeros(points.shape[0], dtype=np.bool)\n        A = triangles[:, :2] - triangles[:, 2:]\n        A = A.transpose([0, 2, 1])\n        y = points - triangles[:, 2]\n\n        detA = A[:, 0, 0] * A[:, 1, 1] - A[:, 0, 1] * A[:, 1, 0]\n\n        mask = (np.abs(detA) != 0.)\n        A = A[mask]\n        y = y[mask]\n        detA = detA[mask]\n\n        s_detA = np.sign(detA)\n        abs_detA = np.abs(detA)\n\n        u = (A[:, 1, 1] * y[:, 0] - A[:, 0, 1] * y[:, 1]) * s_detA\n        v = (-A[:, 1, 0] * y[:, 0] + A[:, 0, 0] * y[:, 1]) * s_detA\n\n        sum_uv = u + v\n        contains[mask] = (\n            (0 < u) & (u < abs_detA) & (0 < v) & (v < abs_detA)\n            & (0 < sum_uv) & (sum_uv < abs_detA)\n        )\n        return contains\n'"
kaolin/rep/TriangleMesh.py,73,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import abstractmethod\n\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\n\n\nfrom kaolin.helpers import _composedecorator\nfrom kaolin.rep.Mesh import Mesh\n\n\nclass TriangleMesh(Mesh):\n    """""" Abstract class to represent 3D Trianlge meshes. """"""\n\n    def __init__(self, vertices: torch.Tensor, faces: torch.Tensor,\n                 uvs: torch.Tensor, face_textures: torch.Tensor,\n                 textures: torch.Tensor, edges: torch.Tensor, edge2key: dict,\n                 vv: torch.Tensor, vv_count: torch.Tensor,\n                 vf: torch.Tensor, vf_count: torch.Tensor,\n                 ve: torch.Tensor, ve_count: torch.Tensor,\n                 ff: torch.Tensor, ff_count: torch.Tensor,\n                 ef: torch.Tensor, ef_count: torch.Tensor,\n                 ee: torch.Tensor, ee_count: torch.Tensor):\n\n        # Vertices of the mesh\n        self.vertices = vertices\n        # Faces of the mesh\n        self.faces = faces\n        # uv coordinates of each vertex\n        self.uvs = uvs\n        # uv indecies for each face\n        self.face_textures = face_textures\n        # texture for each face\n        self.textures = textures\n        # Edges of the mesh\n        self.edges = edges\n        # Dictionary that maps an edge (tuple) to an edge idx\n        self.edge2key = edge2key\n        # Vertex-Vertex neighborhood tensor (for each vertex, contains\n        # indices of the vertices neighboring it)\n        self.vv = vv\n        # Number of vertices neighbouring each vertex\n        self.vv_count = vv_count\n        # Vertex-Face neighborhood tensor\n        self.vf = vf\n        # Number of faces neighbouring each vertex\n        self.vf_count = vf_count\n        # Vertex-Edge neighborhood tensor\n        self.ve = ve\n        # Number of edges neighboring each vertex\n        self.ve_count = ve_count\n        # Face-Face neighborhood tensor\n        self.ff = ff\n        # Number of faces neighbouring each face\n        self.ff_count = ff_count\n        # Edge-Face neighbourhood tensor\n        self.ef = ef\n        # Number of edges neighbouring each face\n        self.ef_count = ef_count\n        # Edge-Edge neighbourhood tensor\n        self.ee = ee\n        # Number of edges neighbouring each edge\n        self.ee_count = ee_count\n        # adjacency matrix for verts\n        self.adj = None\n\n        # Initialize device on which tensors reside.\n        self.device = self.vertices.device\n\n    @staticmethod\n    def normalize_zerosafe(matrix):\n        """"""Normalizes each row of a matrix in a \'division by zero\'-safe way.\n\n        Args:\n            matrix (torch.tensor): Matrix where each row contains a vector\n                to be normalized\n\n        """"""\n\n        assert matrix.dim() == 2, \'Need matrix to contain exactly 2 dimensions\'\n        magnitude = torch.sqrt(torch.sum(torch.pow(matrix, 2), dim=1))\n        valid_inds = magnitude > 0\n        matrix[valid_inds] = torch.div(matrix[valid_inds], magnitude[\n                                       valid_inds].unsqueeze(1))\n        return matrix\n\n    def compute_vertex_normals(self):\n        """"""Compute vertex normals for each mesh vertex. """"""\n\n        # Let each face ordering be denoted a, b, c, d. For consistent order,\n        # we vectorize operations, so that a (for example) denotes the first\n        # vertex of each face in the mesh.\n        a = torch.index_select(self.vertices, dim=0,\n                               index=self.faces[:, 0].flatten())\n        b = torch.index_select(self.vertices, dim=0,\n                               index=self.faces[:, 1].flatten())\n        c = torch.index_select(self.vertices, dim=0,\n                               index=self.faces[:, 2].flatten())\n\n        # Compute vertex normals.\n        # Eg. Normals for vertices \'a\' are given by (b-a) x (c - a)\n        vn_a = TriangleMesh.normalize_zerosafe(\n            torch.cross(b - a, c - a, dim=1))\n        vn_b = TriangleMesh.normalize_zerosafe(\n            torch.cross(c - b, a - b, dim=1))\n        vn_c = TriangleMesh.normalize_zerosafe(\n            torch.cross(a - c, b - c, dim=1))\n\n        # Using the above, we have duplicate vertex normals (since a vertex is\n        # usually a part of more than one face). We only select the first face\n        # each vertex is a \'neighbor\' to, to avoid confusion.\n        face_inds = self.vf[:, 0]\n\n        # Now that we know which face each vertex belongs to, we need to find\n        # the index of the vertex in that selected face. (i.e., is the\n        # selected vertex the \'a\', the \'b\', the \'c\', or the \'d\' vertex of the\n        # face?).\n        vertex_inds = torch.arange(self.vertices.shape[0]).unsqueeze(\n            1).to(self.vertices.device)\n        # Mask that specifies which index of each face to look at, for the\n        # vertex we wish to find.\n        mask_abc = self.faces[face_inds] == vertex_inds.repeat(1, 3)\n        mask_abc = mask_abc.cuda()\n\n        # Array to hold vertex normals\n        vn = torch.zeros_like(self.vertices)\n\n        inds = torch.nonzero(mask_abc[:, 0])\n        inds = torch.cat((inds, torch.zeros_like(inds)), dim=1)\n        vn[inds] = vn_a[face_inds[inds]]\n        inds = torch.nonzero(mask_abc[:, 1])\n        inds = torch.cat((inds, 1 * torch.ones_like(inds)), dim=1)\n        vn[inds] = vn_b[face_inds[inds]]\n        inds = torch.nonzero(mask_abc[:, 2])\n        inds = torch.cat((inds, 2 * torch.ones_like(inds)), dim=1)\n        vn[inds] = vn_c[face_inds[inds]]\n\n        return vn\n\n    def compute_face_normals(self):\n        r""""""Compute normals for each face in the mesh. """"""\n\n        # Let each face be denoted (a, b, c). We vectorize operations, so,\n        # we take `a` to mean the ""first vertex of every face"", and so on.\n        a = torch.index_select(self.vertices, dim=0,\n                               index=self.faces[:, 0].flatten())\n        b = torch.index_select(self.vertices, dim=0,\n                               index=self.faces[:, 1].flatten())\n        c = torch.index_select(self.vertices, dim=0,\n                               index=self.faces[:, 2].flatten())\n\n        # Compute vertex normals (for each face). Note the the same vertex\n        # can have different normals for each face.\n        # Eg. Normals for vertices \'a\' are given by (b-a) x (c - a)\n        vn_a = TriangleMesh.normalize_zerosafe(\n            torch.cross(b - a, c - a, dim=1))\n        vn_b = TriangleMesh.normalize_zerosafe(\n            torch.cross(c - b, a - b, dim=1))\n        vn_c = TriangleMesh.normalize_zerosafe(\n            torch.cross(a - c, b - c, dim=1))\n        # Add and normalize the normals (for a more robust estimate)\n        face_normals = vn_a + vn_b + vn_c\n        face_normals_norm = face_normals.norm(dim=1)\n        face_normals = face_normals / torch.where(face_normals_norm > 0,\n            face_normals_norm, torch.ones_like(face_normals_norm)).view(-1, 1)\n        return face_normals\n\n    def compute_edge_lengths(self):\n        """"""Compute edge lengths for each edge of the mesh. """"""\n\n        self.edges = self.edges.to(self.vertices.device)\n        # Let each edge be denoted (a, b). We perform a vectorized select\n        # and then compute the magnitude of the vector b - a.\n        a = torch.index_select(self.vertices, dim=0,\n                               index=self.edges[:, 0].flatten())\n        b = torch.index_select(self.vertices, dim=0,\n                               index=self.edges[:, 1].flatten())\n        return (b - a).norm(dim=1)\n\n    def compute_face_areas(self):\n        raise NotImplementedError\n\n    def compute_interior_angles_per_edge(self):\n        raise NotImplementedError\n\n    def compute_dihedral_angles_per_edge(self):\n        raise NotImplementedError\n\n    def save_mesh(self, filename: str):\n        r"""""" Save a mesh to a wavefront .obj file format\n\n        Args:\n            filename (str) : target filename\n\n        """"""\n\n        with open(filename, \'w\') as f:\n\n            # write vertices\n            for vert in self.vertices:\n                f.write(\'v %f %f %f\\n\' % tuple(vert))\n            # write faces\n            for face in self.faces:\n                f.write(\'f %d %d %d\\n\' % tuple(face + 1))\n\n    def sample(self, num_samples: int, eps: float = 1e-10):\n        r"""""" Uniformly samples the surface of a mesh.\n\n            Args:\n                num_samples (int): number of points to sample\n                eps (float): a small number to prevent division by zero\n                             for small surface areas.\n\n            Returns:\n                (torch.Tensor, torch.Tensor) uniformly sampled points and\n                    the face idexes which each point corresponds to.\n\n            Example:\n                >>> points, chosen_faces = mesh.sample(10)\n                >>> points\n                tensor([[ 0.0293,  0.2179,  0.2168],\n                        [ 0.2003, -0.3367,  0.2187],\n                        [ 0.2152, -0.0943,  0.1907],\n                        [-0.1852,  0.1686, -0.0522],\n                        [-0.2167,  0.3171,  0.0737],\n                        [ 0.2219, -0.0289,  0.1531],\n                        [ 0.2217, -0.0115,  0.1247],\n                        [-0.1400,  0.0364, -0.1618],\n                        [ 0.0658, -0.0310, -0.2198],\n                        [ 0.1926, -0.1867, -0.2153]])\n                >>> chosen_faces\n                tensor([ 953,  38,  6, 3480,  563,  393,  395, 3309, 373, 271])\n        """"""\n\n        if self.vertices.is_cuda:\n            dist_uni = torch.distributions.Uniform(\n                torch.tensor([0.0]).cuda(), torch.tensor([1.0]).cuda())\n        else:\n            dist_uni = torch.distributions.Uniform(\n                torch.tensor([0.0]), torch.tensor([1.0]))\n\n        # calculate area of each face\n        x1, x2, x3 = torch.split(torch.index_select(\n            self.vertices, 0, self.faces[:, 0]) - torch.index_select(\n            self.vertices, 0, self.faces[:, 1]), 1, dim=1)\n        y1, y2, y3 = torch.split(torch.index_select(\n            self.vertices, 0, self.faces[:, 1]) - torch.index_select(\n            self.vertices, 0, self.faces[:, 2]), 1, dim=1)\n        a = (x2 * y3 - x3 * y2)**2\n        b = (x3 * y1 - x1 * y3)**2\n        c = (x1 * y2 - x2 * y1)**2\n        Areas = torch.sqrt(a + b + c) / 2\n        # percentage of each face w.r.t. full surface area\n        Areas = Areas / (torch.sum(Areas) + eps)\n\n        # define descrete distribution w.r.t. face area ratios caluclated\n        cat_dist = torch.distributions.Categorical(Areas.view(-1))\n        face_choices = cat_dist.sample([num_samples])\n\n        # from each face sample a point\n        select_faces = self.faces[face_choices]\n        v0 = torch.index_select(self.vertices, 0, select_faces[:, 0])\n        v1 = torch.index_select(self.vertices, 0, select_faces[:, 1])\n        v2 = torch.index_select(self.vertices, 0, select_faces[:, 2])\n        u = torch.sqrt(dist_uni.sample([num_samples]))\n        v = dist_uni.sample([num_samples])\n        points = (1 - u) * v0 + (u * (1 - v)) * v1 + u * v * v2\n\n        return points, face_choices\n\n    def compute_adjacency_matrix_full(self):\n        r""""""Calcualtes a binary adjacency matrix for a mesh.\n\n            Returns:\n                (torch.Tensor) : binary adjacency matrix\n\n            Example:\n                >>> mesh = TriangleMesh.from_obj(\'model.obj\')\n                >>> adj_info = mesh.compute_adjacency_matrix_full()\n                >>> neighborhood_sum = torch.mm( adj_info, mesh.vertices)\n        """"""\n\n        adj = torch.zeros((self.vertices.shape[0], self.vertices.shape[0])).to(\n            self.vertices.device)\n        v1 = self.faces[:, 0]\n        v2 = self.faces[:, 1]\n        v3 = self.faces[:, 2]\n        adj[(v1, v1)] = 1\n        adj[(v2, v2)] = 1\n        adj[(v3, v3)] = 1\n        adj[(v1, v2)] = 1\n        adj[(v2, v1)] = 1\n        adj[(v1, v3)] = 1\n        adj[(v3, v1)] = 1\n        adj[(v2, v3)] = 1\n        adj[(v2, v3)] = 1\n\n        return adj\n\n    def load_tensors(filename: (str), enable_adjacency: bool = False):\n        r""""""Loads the tensor information of the mesh from a saved numpy array.\n\n        Args:\n            filename: Path of the file to load the file from.\n\n        Example:\n            >>> mesh = TriangleMesh.load_tensors(\'mesh.npy\')\n\n        """"""\n        data = np.load(filename)\n\n        vertices = torch.FloatTensor(data[\'vertices\'])\n        faces = torch.LongTensor(data[\'faces\'].astype(int))\n\n        return TriangleMesh.from_tensors(vertices, faces)\n\n    def compute_adjacency_matrix_sparse(self):\n        r"""""" Calcualtes a sparse adjacency matrix for a mess\n\n            Returns:\n                (torch.sparse.Tensor) : sparse adjacency matrix\n\n            Example:\n                >>> mesh = Mesh.from_obj(\'model.obj\')\n                >>> adj_info = mesh.compute_adjacency_matrix_sparse()\n                >>> neighborhood_sum = torch.sparse.mm(adj_info, mesh.vertices)\n\n        """"""\n\n        if self.adj is None:\n\n            v1 = self.faces[:, 0].view(-1, 1)\n            v2 = self.faces[:, 1].view(-1, 1)\n            v3 = self.faces[:, 2].view(-1, 1)\n\n            vert_len = self.vertices.shape[0]\n            identity_indices = torch.arange(vert_len).view(-1, 1).to(v1.device)\n            identity = torch.cat(\n                (identity_indices, identity_indices), dim=1).to(v1.device)\n            identity = torch.cat((identity, identity))\n\n            i_1 = torch.cat((v1, v2), dim=1)\n            i_2 = torch.cat((v1, v3), dim=1)\n\n            i_3 = torch.cat((v2, v1), dim=1)\n            i_4 = torch.cat((v2, v3), dim=1)\n\n            i_5 = torch.cat((v3, v2), dim=1)\n            i_6 = torch.cat((v3, v1), dim=1)\n            indices = torch.cat(\n                (identity, i_1, i_2, i_3, i_4, i_5, i_6), dim=0).t()\n            values = torch.ones(indices.shape[1]).to(indices.device) * .5\n            self.adj = torch.sparse.FloatTensor(\n                indices, values, torch.Size([vert_len, vert_len]))\n        return self.adj.clone()\n'"
kaolin/rep/VoxelGrid.py,2,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# \n# Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation components\n#\n# Copyright (c) 2019 Edward Smith\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom typing import Optional\n\nimport torch\n\nfrom kaolin import helpers\n\n\nclass VoxelGrid(object):\n    """"""Base class to hold (regular) voxel grids. """"""\n\n    def __init__(self, voxels: Optional[torch.Tensor] = None,\n                 copy: Optional[bool] = False):\n        r""""""Initialize a voxel grid, given a tensor of voxel `features`.\n\n        Args:\n            voxels (torch.Tensor, optional): Tensor containing voxel features\n                (shape: Any shape that has >= 3 dims).\n            copy (bool, optional): Whether or not to create a deep copy of the\n                Tensor(s) used to initialize class member(s).\n\n        Note:\n            By default, the created VoxelGrid object stores a reference to the\n            input `voxels` tensor. To create a deep copy of the voxels, set the\n            `copy` argument to `True`.\n\n        """"""\n        super(VoxelGrid, self).__init__()\n        if voxels is None:\n            self.voxels = None\n        else:\n            helpers._assert_tensor(voxels)\n            helpers._assert_dim_ge(voxels, 3)\n            self.voxels = voxels.clone() if copy else voxels\n'"
kaolin/rep/__init__.py,0,b'from .Mesh import *\nfrom .TriangleMesh import *\nfrom .QuadMesh import *\nfrom .PointCloud import *\nfrom .VoxelGrid import *\nfrom .SDF import *\n'
kaolin/testing/__init__.py,2,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n# Kornia components:\n# Copyright (C) 2017-2019, Arraiy, Inc., all rights reserved.\n# Copyright (C) 2019-    , Open Source Vision Foundation, all rights reserved.\n# Copyright (C) 2019-    , Kornia authors, all rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nTesting specific utils\n""""""\n\nimport torch\n\n# Borrowed from kornia\n# https://github.com/arraiyopensource/kornia\n# https://github.com/kornia/kornia/blob/master/kornia/testing/__init__.py\ndef tensor_to_gradcheck_var(tensor, dtype=torch.float64, requires_grad=True):\n    """"""Makes input tensors gradcheck-compatible (i.e., float64, and\n       requires_grad = True).\n    """"""\n\n    assert torch.is_tensor(tensor), type(tensor)\n    return tensor.requires_grad_(requires_grad).type(dtype)\n'"
kaolin/transforms/__init__.py,0,b'from .transforms import *\n'
kaolin/transforms/meshfunc.py,28,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Iterable, List, Optional, Type, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom scipy import ndimage\n\nfrom kaolin.rep import Mesh, TriangleMesh, QuadMesh\nfrom kaolin import helpers\n\n\n# Tiny eps\nEPS = 1e-6\n\n\ndef sample_triangle_mesh(vertices: torch.Tensor, faces: torch.Tensor,\n                         num_samples: int, eps: float = 1e-10):\n    r"""""" Uniformly samples the surface of a mesh.\n\n    Args:\n        vertices (torch.Tensor): Vertices of the mesh (shape:\n            :math:`N \\times 3`, where :math:`N` is the number of vertices)\n        faces (torch.LongTensor): Faces of the mesh (shape: :math:`F \\times 3`,\n            where :math:`F` is the number of faces).\n        num_samples (int): Number of points to sample\n        eps (float): A small number to prevent division by zero\n                     for small surface areas.\n\n    Returns:\n        (torch.Tensor): Uniformly sampled points from the triangle mesh.\n\n    Example:\n        >>> points = sample_triangle_mesh(vertices, faces, 10)\n        >>> points\n        tensor([[ 0.0293,  0.2179,  0.2168],\n                [ 0.2003, -0.3367,  0.2187],\n                [ 0.2152, -0.0943,  0.1907],\n                [-0.1852,  0.1686, -0.0522],\n                [-0.2167,  0.3171,  0.0737],\n                [ 0.2219, -0.0289,  0.1531],\n                [ 0.2217, -0.0115,  0.1247],\n                [-0.1400,  0.0364, -0.1618],\n                [ 0.0658, -0.0310, -0.2198],\n                [ 0.1926, -0.1867, -0.2153]])\n    """"""\n\n    helpers._assert_tensor(vertices)\n    helpers._assert_tensor(faces)\n    helpers._assert_dim_ge(vertices, 2)\n    helpers._assert_dim_ge(faces, 2)\n    # We want the last dimension of vertices to be of shape 3.\n    helpers._assert_shape_eq(vertices, (-1, 3), dim=-1)\n\n    dist_uni = torch.distributions.Uniform(torch.zeros((1,), device=vertices.device),\n                                           1.)\n\n    # calculate area of each face\n    x1, x2, x3 = torch.split(torch.index_select(\n        vertices, 0, faces[:, 0]) - torch.index_select(\n        vertices, 0, faces[:, 1]), 1, dim=1)\n    y1, y2, y3 = torch.split(torch.index_select(\n        vertices, 0, faces[:, 1]) - torch.index_select(\n        vertices, 0, faces[:, 2]), 1, dim=1)\n    a = (x2 * y3 - x3 * y2) ** 2\n    b = (x3 * y1 - x1 * y3) ** 2\n    c = (x1 * y2 - x2 * y1) ** 2\n    Areas = torch.sqrt(a + b + c) / 2\n    # percentage of each face w.r.t. full surface area\n    Areas = Areas / (torch.sum(Areas) + eps)\n\n    # define descrete distribution w.r.t. face area ratios caluclated\n    cat_dist = torch.distributions.Categorical(Areas.view(-1))\n    face_choices = cat_dist.sample([num_samples])\n\n    # from each face sample a point\n    select_faces = faces[face_choices]\n    xs = torch.index_select(vertices, 0, select_faces[:, 0])\n    ys = torch.index_select(vertices, 0, select_faces[:, 1])\n    zs = torch.index_select(vertices, 0, select_faces[:, 2])\n    u = torch.sqrt(dist_uni.sample([num_samples]))\n    v = dist_uni.sample([num_samples])\n    points = (1 - u) * xs + (u * (1 - v)) * ys + u * v * zs\n\n    return points\n\n\ndef normalize(mesh: Type[Mesh], inplace: Optional[bool] = True):\n    r""""""Normalize a mesh such that it is centered at the orgin and has\n    unit standard deviation.\n\n    Args:\n        mesh (Mesh): Mesh to be normalized.\n        inplace (bool, optional): Bool to make this operation in-place.\n\n    Returns:\n        (Mesh): Normalized mesh.\n\n    """"""\n    if not isinstance(mesh, Mesh):\n        raise TypeError(\'Input mesh must be of type Mesh. \'\n            \'Got {0} instead.\'.format(type(mesh)))\n    if not inplace:\n        mesh = mesh.clone()\n\n    mesh.vertices = (mesh.vertices - mesh.vertices.mean(-2).unsqueeze(-2))\\\n        / (mesh.vertices.std(-2).unsqueeze(-2) + EPS)\n\n    return mesh\n\n\ndef scale(mesh: Type[Mesh], scf: Union[float, Iterable],\n          inplace: Optional[bool] = True):\n    r""""""Scale a mesh given a specified scaling factor. A scalar scaling factor\n    can be provided, in which case it is applied isotropically to all dims.\n    Optionally, a list/tuple of anisotropic scale factors can be provided per\n    dimension.\n\n    Args:\n        mesh (Mesh): Mesh to be scaled.\n        scf (float or iterable): Scaling factor per dimension. If only a single\n            scaling factor is provided (or a list of size 1 is provided), it is\n            isotropically applied to all dimensions. Else, a list/tuple of 3\n            scaling factors is expected, which are applied to the X, Y, and Z\n            directions respectively.\n        inplace (bool, optional): Bool to make this operation in-place.\n\n    Returns:\n        (Mesh): Scaled mesh.\n\n    """"""\n    if not isinstance(mesh, Mesh):\n        raise TypeError(\'Input mesh must be of type Mesh. \'\n            \'Got {0} instead.\'.format(type(mesh)))\n    if not inplace:\n        mesh = mesh.clone()\n\n    _scf = []\n    if isinstance(scf, float) or isinstance(scf, int):\n        _scf = [scf, scf, scf]\n    elif isinstance(scf, list) or isinstance(scf, tuple):\n        if len(scf) == 1:\n            _scf = [scf[0], scf[0], scf[0]]\n        elif len(scf) == 3:\n            _scf = [scf[0], scf[1], scf[2]]\n        else:\n            raise ValueError(\'Exactly 1 or 3 values required for input scf.\'\n                \'Got {0} instead.\'.format(len(scf)))\n    else:\n        raise TypeError(\'Input scf must be of type int, float, list, or tuple.\'\n            \' Got {0} instead.\'.format(type(scf)))\n\n    _scf = torch.Tensor(_scf).to(mesh.vertices.device).view(1, 3)\n    mesh.vertices = _scf * mesh.vertices\n\n    return mesh\n\n\ndef translate(mesh: Type[Mesh], trans: Union[torch.Tensor, Iterable],\n              inplace: Optional[bool] = True):\n    r""""""Translate a mesh given a (3D) translation vector.\n\n    Args:\n        mesh (Mesh): Mesh to be normalized.\n        trans (torch.Tensor or iterable): Translation vector (shape:\n            torch.Tensor or iterable must have exactly 3 elements).\n        inplace (bool, optional): Bool to make this operation in-place.\n\n    Returns:\n        (Mesh): Translated mesh.\n\n    """"""\n    if not isinstance(mesh, Mesh):\n        raise TypeError(\'Input mesh must be of type Mesh. \'\n            \'Got {0} instead.\'.format(type(mesh)))\n    if not inplace:\n        mesh = mesh.clone()\n    if torch.is_tensor(trans):\n        if trans.numel() != 3:\n            raise ValueError(\'Input trans must contain exactly 3 elements. \'\n                \'Got {0} instead.\'.format(trans.numel()))\n        trans = trans.view(1, 3)\n    elif isinstance(trans, list) or isinstance(trans, tuple):\n        if len(trans) != 3:\n            raise ValueError(\'Exactly 1 or 3 values required for input trans.\'\n                \'Got {0} instead.\'.format(len(trans)))\n        trans = torch.Tensor([trans[0], trans[1], trans[2]]).to(\n            mesh.vertices.device).view(1, 3)\n\n    mesh.vertices = mesh.vertices + trans\n    return mesh\n\n\ndef rotate(mesh: Type[Mesh], rotmat: torch.Tensor,\n           inplace: Optional[bool] = True):\n    r""""""Rotate a mesh given a 3 x 3 rotation matrix.\n\n    Args:\n        mesh (Mesh): Mesh to be rotated.\n        rotmat (torch.Tensor): Rotation matrix (shape: :math:`3 \\times 3`).\n        inplace (bool, optional): Bool to make this operation in-place.\n\n    Returns:\n        (Mesh): Rotatted mesh.\n    """"""\n    if not isinstance(mesh, Mesh):\n        raise TypeError(\'Input mesh must be of type Mesh. \'\n            \'Got {0} instead.\'.format(type(mesh)))\n    if not inplace:\n        mesh = mesh.clone()\n\n    helpers._assert_tensor(rotmat)\n    helpers._assert_shape_eq(rotmat, (3, 3))\n\n    mesh.vertices = torch.matmul(rotmat, mesh.vertices.t()).t()\n\n    return mesh\n\n\nif __name__ == \'__main__\':\n\n    device = \'cpu\'\n    mesh = TriangleMesh.from_obj(\'tests/model.obj\')\n\n    # # Test sample_triangle_mesh\n    # pts = sample_triangle_mesh(mesh.vertices.to(device),\n    #     mesh.faces.to(device), 10)\n    # print(pts)\n\n    # # Test normalize\n    # mesh = normalize(mesh)\n\n    # # Test scale\n    # print(mesh.vertices[:10])\n    # mesh = scale(mesh, [2, 1, 2])\n    # print(mesh.vertices[:10])\n\n    # # Test translate\n    # print(mesh.vertices[:10])\n    # mesh = translate(mesh, torch.Tensor([2, 2, 2]))\n    # print(mesh.vertices[:10])\n\n    # # Test rotate\n    # print(mesh.vertices[:10])\n    # rmat = 2 * torch.eye(3)\n    # mesh = rotate(mesh, rmat)\n    # print(mesh.vertices[:10])\n'"
kaolin/transforms/pointcloudfunc.py,27,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional, Union\n\nimport numpy as np\nimport torch\n\nfrom kaolin.rep import PointCloud\nfrom kaolin import helpers\n\n\n# Tiny eps\nEPS = 1e-6\n\n\ndef scale(cloud: Union[torch.Tensor, PointCloud],\n          scf: Union[float, int, torch.Tensor],\n          inplace: Optional[bool] = True):\n    """"""Scales the input pointcloud by a scaling factor.\n\n    Args:\n        cloud (torch.Tensor or kaolin.rep.PointCloud): pointcloud (ndims >= 2).\n        scf (float, int, or torch.Tensor): scaling factor (scalar, or tensor).\n            All elements of scf must be positive.\n        inplace (bool, optional): Bool to make the transform in-place.\n\n    Returns:\n        (torch.Tensor): scaled pointcloud of the same shape as input.\n\n    Shape:\n        - cloud: :math:`(B x N x D)` (or) :math:`(N x D)`, where :math:`(B)`\n            is the batchsize, :math:`(N)` is the number of points per cloud,\n            and :math:`(D)` is the dimensionality of each cloud.\n        - scf: :math:`(1)` or :math:`(B)`.\n\n    Example:\n        >>> points = torch.rand(1000,3)\n        >>> points2 = scale(points, torch.FloatTensor([3]))\n\n    """"""\n\n    if isinstance(cloud, np.ndarray):\n        cloud = torch.from_numpy(cloud)\n\n    if isinstance(scf, np.ndarray):\n        scf = torch.from_numpy(scf)\n\n    if isinstance(cloud, PointCloud):\n        cloud = cloud.points\n\n    if isinstance(scf, int) or isinstance(scf, float):\n        scf = torch.Tensor([scf]).to(cloud.device)\n\n    helpers._assert_tensor(cloud)\n    helpers._assert_tensor(scf)\n    helpers._assert_dim_ge(cloud, 2)\n    helpers._assert_gt(scf, 0.)\n\n    if not inplace:\n        cloud = cloud.clone()\n\n    return scf * cloud\n\n\ndef rotate(cloud: Union[torch.Tensor, PointCloud], rotmat: torch.Tensor,\n           inplace: Optional[bool] = True):\n    """"""Rotates the the input pointcloud by a rotation matrix.\n\n    Args:\n        cloud (Tensor or np.array): pointcloud (ndims = 2 or 3)\n        rotmat (Tensor or np.array): rotation matrix (3 x 3, 1 per cloud).\n        inplace (bool, optional): Bool to make the transform in-place.\n\n    Returns:\n        cloud_rot (Tensor): rotated pointcloud of the same shape as input\n\n    Shape:\n        - cloud: :math:`(B x N x 3)` (or) :math:`(N x 3)`, where :math:`(B)`\n            is the batchsize, :math:`(N)` is the number of points per cloud,\n            and :math:`(3)` is the dimensionality of each cloud.\n        - rotmat: :math:`(3, 3)` or :math:`(B, 3, 3)`.\n\n    Example:\n        >>> points = torch.rand(1000,3)\n        >>> r_mat = torch.rand(3,3)\n        >>> points2 = rotate(points, r_mat)\n\n    """"""\n    if isinstance(cloud, np.ndarray):\n        cloud = torch.from_numpy(cloud)\n    if isinstance(cloud, PointCloud):\n        cloud = cloud.points\n    if isinstance(rotmat, np.ndarray):\n        rotmat = torch.from_numpy(rotmat)\n\n    helpers._assert_tensor(cloud)\n    helpers._assert_tensor(rotmat)\n    helpers._assert_dim_ge(cloud, 2)\n    helpers._assert_dim_ge(rotmat, 2)\n    # Rotation matrix must have last two dimensions of shape 3.\n    helpers._assert_shape_eq(rotmat, (3, 3), dim=-1)\n    helpers._assert_shape_eq(rotmat, (3, 3), dim=-2)\n\n    if not inplace:\n        cloud = cloud.clone()\n\n    if rotmat.dim() == 2 and cloud.dim() == 2:\n        cloud = torch.mm(rotmat, cloud.transpose(0, 1)).transpose(0, 1)\n    else:\n        if rotmat.dim() == 2:\n            rotmat = rotmat.expand(cloud.shape[0], 3, 3)\n        cloud = torch.bmm(rotmat, cloud.transpose(1, 2)).transpose(1, 2)\n\n    return cloud\n\n\ndef realign(src: Union[torch.Tensor, PointCloud],\n            tgt: Union[torch.Tensor, PointCloud],\n            inplace: Optional[bool] = True):\n    r"""""" Aligns a pointcloud `src` to be in the same (axis-aligned) bounding\n    box as that of pointcloud `tgt`.\n\n    Args:\n        src (torch.Tensor or PointCloud) : Source pointcloud to be transformed\n            (shape: :math:`\\cdots \\times N \\times D`, where :math:`N` is the\n            number of points in the pointcloud, and :math:`D` is the\n            dimensionality of each point in the cloud).\n        tgt (torch.Tensor or PointCloud) : Target pointcloud to which `src`is\n            to be transformed (The `src` cloud is transformed to the\n            axis-aligned bounding box that the target cloud maps to). This\n            cloud must have the same number of dimensions :math:`D` as in the\n            source cloud. (shape: :math:`\\cdots \\times \\cdots \\times D`).\n        inplace (bool, optional): Bool to make the transform in-place.\n\n    Returns:\n        (torch.Tensor): Pointcloud `src` realigned to fit in the (axis-aligned)\n            bounding box of the `tgt` cloud.\n\n    Example:\n        >>> tgt = torch.rand(1000)\n        >>> src = (tgt * 100) + 3\n        >>> src_realigned = realign(src, tgt)\n\n    """"""\n    if isinstance(src, PointCloud):\n        src = src.points\n    if isinstance(tgt, PointCloud):\n        tgt = tgt.points\n    helpers._assert_tensor(src)\n    helpers._assert_tensor(tgt)\n    helpers._assert_dim_ge(src, 2)\n    helpers._assert_dim_ge(tgt, 2)\n    helpers._assert_shape_eq(src, tgt.shape, dim=-1)\n\n    if not inplace:\n        src = src.clone()\n\n    # Compute the relative scaling factor and scale the src cloud.\n    src_min, _ = src.min(-2, keepdim=True)\n    src_max, _ = src.max(-2, keepdim=True)\n    tgt_min, _ = tgt.min(-2, keepdim=True)\n    tgt_max, _ = tgt.max(-2, keepdim=True)\n\n    src = ((src - src_min) / (src_max - src_min + EPS)) * (tgt_max - tgt_min) + tgt_min\n    return src\n\n\ndef normalize(cloud: Union[torch.Tensor, PointCloud],\n              inplace: Optional[bool] = True):\n    r""""""Returns a normalized pointcloud with zero-mean and unit standard\n    deviation. For batched clouds, each cloud is independently normalized.\n\n    Args:\n        cloud (torch.Tensor or PointCloud): Input pointcloud to be normalized\n            (shape: :math:`B \\times \\cdots \\times N \\times D`, where :math:`B`\n            is the batchsize (optional), :math:`N` is the number of points in\n            the cloud, and :math:`D` is the dimensionality of the cloud.\n        inplace (bool, optional): Bool to make the transform in-place.\n\n    Returns:\n        (torch.Tensor or PointCloud): The normalized pointcloud.\n\n    """"""\n\n    if isinstance(cloud, np.ndarray):\n        cloud = torch.from_numpy(cloud)\n\n    helpers._assert_tensor(cloud)\n    helpers._assert_dim_ge(cloud, 2)\n    if not inplace:\n        cloud = cloud.clone()\n\n    cloud = (cloud - cloud.mean(-2).unsqueeze(-2))\\\n        / (cloud.std(-2).unsqueeze(-2) + EPS)\n\n    return cloud\n'"
kaolin/transforms/transforms.py,71,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Iterable, List, Optional, Type, Union, Callable\nfrom pathlib import Path\nimport hashlib\n\nimport scipy\nimport numpy as np\nimport torch\n\nfrom kaolin.rep.PointCloud import PointCloud\nfrom kaolin.rep.VoxelGrid import VoxelGrid\nfrom kaolin.rep.Mesh import Mesh\nfrom kaolin.rep.TriangleMesh import TriangleMesh\nfrom kaolin.rep.QuadMesh import QuadMesh\n\nimport kaolin.conversions as cvt\n\n# from kaolin.conversion import mesh as cvt_mesh\n# from kaolin.conversion import SDF as cvt_SDF\n# from kaolin.conversion import voxel as cvt_voxel\nfrom kaolin.transforms import pointcloudfunc as pcfunc\nfrom kaolin.transforms import meshfunc\nfrom kaolin.transforms import voxelfunc\n\n\ndef _get_repr(obj):\n    # TODO: Improve hashing of tensors such that shape matters\n    if isinstance(obj, np.ndarray):\n        return hashlib.sha1(obj).hexdigest()\n\n    if isinstance(obj, torch.Tensor):\n        return hashlib.sha1(obj.cpu().numpy()).hexdigest()\n\n    return repr(obj)\n\n\nclass Transform(object):\n    """"""Base class for all Kaolin transforms.\n\n    This class generates __repr__ string automatically. Given that all attributes\n    have valid __repr__, it generates a string with the following format:\n    .. code-block::\n       MyClass(a=1, b=2, ...)\n\n    The attributes are sorted alphabetically to ensure determinism.\n\n    To exclude certain attributes from appearing in the __repr__, define a list\n    ""__ignored_params__"" in the subclass (not the instance). Example:\n    .. code-block::\n       class MyClass(Transform):\n           __ignored_params__ = [\'a\', \'b\']\n\n    NOTE:\n    - Since the __repr__ generation depends on the __repr__ of attributes,\n      objects that have incorrect (such as arbitrary objects that output\n      their memory address by default) or indeterministic (such as\n      dictionaries) __repr__ should not be assigned as attribute.\n      If unavoidable, override __repr__ manually.\n    """"""\n\n    def __repr__(self):\n        ignored = set(getattr(self, \'__ignored_params__\', []))\n        names = [\n            k for k in self.__dict__.keys()\n            if k not in ignored\n        ]\n        names.sort()\n        params = \', \'.join([\n            \'{}={}\'.format(name, _get_repr(self.__dict__[name]))\n            for name in names\n        ])\n        return \'{}({})\'.format(self.__class__.__name__, params)\n\n\nclass Compose(Transform):\n    """"""Composes (chains) multiple transforms sequentially. Identical to\n    `torchvision.transforms.Compose`.\n\n    Args:\n        tforms (list): List of transforms to compose.\n\n    TODO: Example.\n\n    """"""\n\n    def __init__(self, transforms: Iterable):\n        self.transforms = transforms\n\n    def __call__(self, value: torch.Tensor):\n        for t in self.transforms:\n            value = t(value)\n        return value\n\n\nclass CacheCompose(Transform):\n    """"""Caches the results of the provided compose pipeline to disk.\n    If the pipeline is already cached, data is returned from disk,\n    otherwise, data is converted following the provided transforms.\n\n    Args:\n        transforms (Iterable): List of transforms to compose.\n        cache_dir (str): Directory where objects will be cached. Default\n                            to \'cache\'.\n    """"""\n\n    __ignored_params__ = [\'cache_dir\', \'cached_ids\']\n\n    def __init__(self, transforms: Iterable, cache_dir: str = \'cache\'):\n        self.compose = Compose(transforms)\n        self.cache_dir = Path(cache_dir) / self.get_hash()\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n        self.cached_ids = [p.stem for p in self.cache_dir.glob(\'*\')]\n\n    def __call__(self, object_id: str, inp: Union[torch.Tensor, Mesh] = None):\n        """"""Transform input. If transformed input was cached, is is read from disk\n\n            Args:\n                inp (torch.Tensor or Mesh): input tensor or Mesh object to be transformed,\n                name (str): object name used to write and read from disk.\n\n            Returns:\n                Union[torch.Tensor, Mesh]: Tensor or Mesh object.\n        """"""\n\n        fpath = self.cache_dir / \'{0}.npz\'.format(object_id)\n\n        if not fpath.exists():\n            assert inp is not None\n            transformed = self.compose(inp)\n            self._write(transformed, fpath)\n            self.cached_ids.append(object_id)\n        else:\n            transformed = self._read(fpath)\n\n        return transformed\n\n    def _write(self, x, fpath):\n        if isinstance(x, Mesh):\n            np.savez(fpath, vertices=x.vertices.data.cpu().numpy(),\n                     faces=x.faces.data.cpu().numpy())\n        elif isinstance(x, VoxelGrid):\n            # Save voxel grid as sparse matrix for quick loading\n            res = x.voxels.size(0)\n            sparse_voxel = scipy.sparse.csc_matrix(x.voxels.reshape(res, -1).cpu().numpy())\n            # np.savez_compressed(fpath, sparse=sparse_voxel)\n            scipy.sparse.save_npz(fpath, sparse_voxel)\n        else:\n            np.savez(fpath, x.data.cpu().numpy())\n\n    def _read(self, fpath):\n        data = np.load(fpath)\n        if \'vertices\' in data and \'faces\' in data:\n            verts = torch.from_numpy(data[\'vertices\'])\n            faces = torch.from_numpy(data[\'faces\'])\n            if data[\'faces\'].shape[-1] == 4:\n                data = QuadMesh.from_tensors(verts, faces)\n            else:\n                data = TriangleMesh.from_tensors(verts, faces)\n        elif \'format\' in data:\n            matrix_format = data[\'format\'].item()\n            sparse = scipy.sparse.csc_matrix((data[\'data\'], data[\'indices\'], data[\'indptr\']), data[\'shape\'])\n            data = torch.from_numpy(sparse.todense())\n            res = data.size(0)\n            data = data.reshape(res, res, res)\n        else:\n            data = torch.from_numpy(data[\'arr_0\'])\n\n        return data\n\n    def get_hash(self):\n        return hashlib.md5(bytes(repr(self.compose), \'utf-8\')).hexdigest()\n\n\nclass NumpyToTensor(Transform):\n    """"""Converts a `np.ndarray` object to a `torch.Tensor` object. """"""\n\n    def __call__(self, arr: np.ndarray):\n        """"""\n        Args:\n            arr (np.ndarray): Numpy array to be converted to Tensor.\n\n        Returns:\n            (torch.Tensor): Converted array\n        """"""\n        return torch.from_numpy(arr)\n\n\nclass ScalePointCloud(Transform):\n    """"""Scale a pointcloud with a fixed scaling factor.\n    Given a scale factor `scf`, this transform will scale each point in the\n    pointcloud, i.e.,\n    ``cloud = scf * cloud``\n\n    Args:\n        scf (int or float or torch.Tensor): Scale factor by which input clouds\n            are to be scaled (Note: if passing in a torch.Tensor type, only\n            one-element tensors are allowed).\n        inplace (bool, optional): Whether or not the transformation should be\n            in-place (default: True).\n\n    TODO: Example.\n\n    """"""\n\n    def __init__(self, scf: Union[int, float, torch.Tensor],\n                 inplace: Optional[bool] = True):\n        self.scf = scf\n        self.inplace = inplace\n\n    def __call__(self, cloud: Union[torch.Tensor, PointCloud]):\n        """"""\n        Args:\n            cloud (torch.Tensor or PointCloud): Pointcloud to be scaled.\n\n        Returns:\n            (torch.Tensor or PointCloud): Scaled pointcloud.\n        """"""\n        return pcfunc.scale(cloud, scf=self.scf, inplace=self.inplace)\n\n\nclass RotatePointCloud(Transform):\n    r""""""Rotate a pointcloud with a given rotation matrix.\n    Given a :math:`3 \\times 3` rotation matrix, this transform will rotate each\n    point in the cloud by the rotation matrix specified.\n\n    Args:\n        rotmat (torch.Tensor): Rotation matrix that specifies the rotation to\n            be applied to the pointcloud (shape: :math:`3 \\times 3`).\n        inplace (bool, optional): Bool to make this operation in-place.\n\n    TODO: Example.\n\n    """"""\n\n    def __init__(self, rotmat: torch.Tensor, inplace: Optional[bool] = True):\n        self.rotmat = rotmat\n        self.inplace = inplace\n\n    def __call__(self, cloud: Union[torch.Tensor, PointCloud]):\n        """"""\n        Args:\n            cloud (torch.Tensor or PointCloud): Input pointcloud to be rotated.\n\n        Returns:\n            (torch.Tensor or PointCloud): Rotated pointcloud.\n        """"""\n        return pcfunc.rotate(cloud, rotmat=self.rotmat, inplace=self.inplace)\n\n\nclass RealignPointCloud(Transform):\n    r""""""Re-align a `src` pointcloud such that it fits in an axis-aligned\n    bounding box whose size matches the `tgt` pointcloud.\n\n    Args:\n        tgt (torch.Tensor or PointCloud): Target pointcloud, to whose\n            dimensions the source pointcloud must be aligned.\n        inplace (bool, optional): Bool to make this operation in-place.\n\n    TODO: Example.\n\n    """"""\n\n    def __init__(self, tgt: Union[torch.Tensor, PointCloud],\n                 inplace: Optional[bool] = True):\n        self.tgt = tgt\n        self.inplace = inplace\n\n    def __call__(self, src: Union[torch.Tensor, PointCloud]):\n        """"""\n        Args:\n            src (torch.Tensor or PointCloud): Source pointcloud, which needs\n                to be aligned to the target pointcloud.\n\n        Returns:\n            (torch.Tensor or PointCloud): Source pointcloud aligned to match\n                the axis-aligned bounding box of the target pointcloud `tgt`.\n        """"""\n        return pcfunc.realign(src, self.tgt, inplace=self.inplace)\n\n\nclass NormalizePointCloud(Transform):\n    r""""""Normalize a pointcloud such that it is centered at the orgin and has\n    unit standard deviation.\n\n    Args:\n        inplace (bool, optional): Bool to make this operation in-place.\n\n    TODO: Example.\n\n    """"""\n\n    def __init__(self, inplace: Optional[bool] = True):\n        self.inplace = inplace\n\n    def __call__(self, cloud: Union[torch.Tensor, PointCloud]):\n        r""""""\n        Args:\n            src (torch.Tensor or PointCloud): Pointcloud to be normalized\n                (shape: :math:`B \\times \\cdots \\times N \\times D`, where\n                :math:`B` is the batchsize (optional), :math:`N` is the\n                number of points in the cloud, and :math:`D` is the\n                dimensionality of the cloud.\n        """"""\n        return pcfunc.normalize(cloud, inplace=self.inplace)\n\n\nclass DownsampleVoxelGrid(Transform):\n    r""""""Downsamples a voxelgrid, given a (down)scaling factor for each\n    dimension.\n\n    .. Note::\n        The voxel output is not thresholded.\n\n    Args:\n        scale (list): List of tensors to scale each dimension down by\n            (length: 3).\n        inplace (bool, optional): Bool to make the operation in-place.\n\n    TODO: Example.\n\n    """"""\n\n    def __init__(self, scale: List[int], inplace=True):\n        self.scale = scale\n        self.inplace = inplace\n\n    def __call__(self, voxgrid: Union[torch.Tensor, VoxelGrid]):\n        """"""\n        Args:\n            voxgrid (torch.Tensor or VoxelGrid): Voxel grid to be downsampled\n                (shape: must be a tensor containing exactly 3 dimensions).\n\n        Returns:\n            (torch.Tensor): Downsampled voxel grid.\n        """"""\n        return cvt.downsample(voxgrid, scale=self.scale,\n                              inplace=self.inplace)\n\n\nclass UpsampleVoxelGrid(Transform):\n    r""""""Upsamples a voxelgrid, given a target dimensionality (this target\n    dimensionality is homogeneously applied to all three axes).\n\n    .. Note::\n        The output voxels are not thresholded to contain values in the range\n        [0, 1].\n\n    Args:\n        dim (int): New dimensionality (number of voxels along each dimension\n            in the resulting voxel grid).\n\n    TODO: Example.\n\n    """"""\n\n    def __init__(self, dim: int):\n        self.dim = dim\n\n    def __call__(self, voxgrid: Union[torch.Tensor, VoxelGrid]):\n        """"""\n        Args:\n            voxgrid (torch.Tensor or VoxelGrid): Voxel grid to be upsampled\n                (shape: must be a tensor containing exactly 3 dimensions).\n\n        Returns:\n            (torch.Tensor): Upsampled voxel grid.\n        """"""\n        return cvt.upsample(voxgrid, dim=self.dim)\n\n\nclass ThresholdVoxelGrid(Transform):\n    r""""""Binarizes the voxel array using a specified threshold.\n\n    Args:\n        thresh (float): Threshold with which to binarize.\n        inplace (bool, optional): Bool to make the operation in-place.\n\n    """"""\n\n    def __init__(self, thresh: float, inplace: Optional[bool] = True):\n        self.thresh = thresh\n        self.inplace = inplace\n\n    def __call__(self, voxgrid: Union[torch.Tensor, VoxelGrid]):\n        """"""\n        Args:\n            voxel (torch.Tensor): Voxel array to be binarized.\n\n        Returns:\n            (torch.Tensor): Thresholded voxel array.\n        """"""\n        return cvt.threshold(voxgrid, thresh=self.thresh,\n                             inplace=self.inplace)\n\n\nclass FillVoxelGrid(Transform):\n    r""""""Fills the internal structures in a voxel grid. Used to fill holds\n    and \'solidify\' objects.\n\n    Args:\n        thresh (float): Threshold to use for binarization of the grid.\n\n    """"""\n\n    def __init__(self, thresh: float):\n        self.thresh = thresh\n\n    def __call__(self, voxgrid: Union[torch.Tensor, VoxelGrid]):\n        """"""\n        Args:\n            voxel (torch.Tensor or VoxelGrid): Voxel grid to be filled.\n\n        Returns:\n            (torch.Tensor): Filled-in voxel grid.\n        """"""\n        return cvt.fill(voxgrid, thresh=self.thresh)\n\n\nclass ExtractSurfaceVoxels(Transform):\n    r""""""Removes any inernal structure(s) from a voxel array.\n\n    Args:\n        thresh (float): threshold with which to binarize\n    """"""\n\n    def __init__(self, thresh: float):\n        self.thresh = thresh\n\n    def __call__(self, voxgrid: Union[torch.Tensor, VoxelGrid]):\n        """"""\n        Args:\n            voxel (torch.Tensor): Voxel grid from which to extract surface.\n\n        Returns:\n            (torch.Tensor): Voxel grid with the internals removed (i.e.,\n                containing only voxels that reside on the surface of the\n                object).\n        """"""\n        return cvt.extract_surface(voxgrid, self.thresh)\n\n\nclass ExtractOdmsFromVoxelGrid(Transform):\n    r""""""Extracts a set of orthographic depth maps from a voxel grid.\n    """"""\n\n    def __call__(self, voxgrid: Union[torch.Tensor, VoxelGrid]):\n        """"""\n        Args:\n            voxel (torch.Tensor or VoxelGrid): Voxel grid from which ODMs are\n                extracted.\n\n        Returns:\n            (torch.Tensor): 6 ODMs from the 6 primary viewing angles.\n        """"""\n        return cvt.extract_odms(voxgrid)\n\n\nclass ExtractProjectOdmsFromVoxelGrid(Transform):\n    r""""""Extracts a set of orthographic depth maps (odms) from a voxel grid and\n        then projects the odms onto a voxel grid.\n    """"""\n\n    def __call__(self, voxel: Union[torch.Tensor, VoxelGrid]):\n        """"""\n        Args:\n            voxel (torch.Tensor or VoxelGrid): Voxel grid from which ODMs are\n                extracted.\n\n        Returns:\n            (torch.Tensor): Voxel grid.\n        """"""\n        odms = cvt.extract_odms(voxel)\n        return VoxelGrid(cvt.project_odms(odms))\n\n\nclass SampleTriangleMesh(Transform):\n    r""""""Sample points uniformly over the surface of a triangle mesh.\n\n    Args:\n        num_samples (int): Number of points to sample from the mesh.\n        eps (float, optional): A small number to prevent division by zero\n                     for small surface areas.\n    """"""\n\n    def __init__(self, num_samples: int, eps: Optional[float] = 1e-10):\n        self.num_samples = num_samples\n        self.eps = eps\n\n    def __call__(self, mesh: TriangleMesh):\n        """"""\n        Args:\n            mesh (TriangleMesh): A triangle mesh object.\n\n        Returns:\n            (torch.Tensor): Uniformly sampled points over the surface of the\n                input mesh.\n        """"""\n        if not isinstance(mesh, TriangleMesh):\n            raise TypeError(\'Input mesh must be of type TriangleMesh. \'\n                            \'Got {0} instead\'.format(type(mesh)))\n        return meshfunc.sample_triangle_mesh(mesh.vertices, mesh.faces,\n                                             self.num_samples, eps=self.eps)\n\n\nclass NormalizeMesh(Transform):\n    r""""""Normalize a mesh such that it is centered at the orgin and has\n    unit standard deviation.\n\n    Args:\n        inplace (bool, optional): Bool to make this operation in-place.\n\n    TODO: Example.\n\n    """"""\n\n    def __init__(self, inplace: Optional[bool] = True):\n        self.inplace = inplace\n\n    def __call__(self, mesh: Type[Mesh]):\n        r""""""\n        Args:\n            mesh (Mesh): Mesh to be normalized.\n\n        Returns:\n            (Mesh): Normalized mesh (centered at origin,\n                unit variance along all dimensions)\n        """"""\n        return meshfunc.normalize(mesh, inplace=self.inplace)\n\n\nclass ScaleMesh(Transform):\n    r""""""Scale a mesh given a specified scaling factor. A scalar scaling factor\n    can be provided, in which case it is applied isotropically to all dims.\n    Optionally, a list/tuple of anisotropic scale factors can be provided per\n    dimension.\n\n    Args:\n        scf (float or iterable): Scaling factor per dimension. If only a single\n            scaling factor is provided (or a list of size 1 is provided), it is\n            isotropically applied to all dimensions. Else, a list/tuple of 3\n            scaling factors is expected, which are applied to the X, Y, and Z\n            directions respectively.\n        inplace (bool, optional): Bool to make this operation in-place.\n\n    """"""\n\n    def __init__(self, scf: Union[float, int, Iterable],\n                 inplace: Optional[bool] = True):\n        self.scf = scf\n        self.inplace = inplace\n\n    def __call__(self, mesh: Type[Mesh]):\n        """"""\n        Args:\n            mesh (Mesh): Mesh to be scaled.\n\n        Returns:\n            (Mesh): Scaled mesh.\n        """"""\n        return meshfunc.scale(mesh, scf=self.scf, inplace=self.inplace)\n\n\nclass TranslateMesh(Transform):\n    r""""""Translate a mesh given a (3D) translation vector.\n\n    Args:\n        trans (torch.Tensor or iterable): Translation vector (shape:\n            torch.Tensor or iterable must have exactly 3 elements).\n        inplace (bool, optional): Bool to make this operation in-place.\n    """"""\n\n    def __init__(self, trans: Union[torch.Tensor, Iterable],\n                 inplace: Optional[bool] = True):\n        self.trans = trans\n        self.inplace = inplace\n\n    def __call__(self, mesh: Type[Mesh]):\n        """"""\n        Args:\n            mesh (Mesh): Mesh to be translated.\n\n        Returns:\n            (Mesh): Translated mesh.\n        """"""\n        return meshfunc.translate(mesh, trans=self.trans, inplace=self.inplace)\n\n\nclass RotateMesh(Transform):\n    r""""""Rotate a mesh given a 3 x 3 rotation matrix.\n\n    Args:\n        rotmat (torch.Tensor): Rotation matrix (shape: :math:`3 \\times 3`).\n        inplace (bool, optional): Bool to make this operation in-place.\n    """"""\n\n    def __init__(self, rotmat: torch.Tensor, inplace: Optional[bool] = True):\n        self.rotmat = rotmat\n        self.inplace = inplace\n\n    def __call__(self, mesh: Type[Mesh]):\n        """"""\n        Args:\n            mesh (Mesh): Mesh to be rotated.\n\n        Returns:\n            (Mesh): Rotated mesh.\n        """"""\n        return meshfunc.rotate(mesh, rotmat=self.rotmat, inplace=self.inplace)\n\n\nclass TriangleMeshToPointCloud(Transform):\n    r""""""Converts a triange mesh to a pointcloud with a specified number of\n    points. Uniformly samples points over the surface of the mesh.\n\n    Args:\n        num_samples (int): Number of points to sample from the mesh.\n        eps (float, optional): A small number to prevent division by zero\n                     for small surface areas.\n    """"""\n\n    def __init__(self, num_samples: int, eps: Optional[float] = 1e-10):\n        self.num_samples = num_samples\n        self.eps = eps\n\n    def __call__(self, mesh: TriangleMesh):\n        """"""\n        Args:\n            mesh (TriangleMesh): A triangle mesh object.\n\n        Returns:\n            (torch.Tensor): Uniformly sampled points over the surface of the\n                input mesh.\n        """"""\n        if not isinstance(mesh, TriangleMesh):\n            raise TypeError(\'Input mesh must be of type TriangleMesh. \'\n                            \'Got {0} instead\'.format(type(mesh)))\n        return meshfunc.sample_triangle_mesh(mesh.vertices, mesh.faces,\n                                             self.num_samples, eps=self.eps)\n\n\nclass TriangleMeshToVoxelGrid(Transform):\n    r""""""Converts a triangle mesh to a voxel grid with a specified reolution.\n    The resolution of the voxel grid is assumed to be homogeneous along all\n    three dimensions (X, Y, Z axes).\n\n    Args:\n        resolution (int): Desired resolution of generated voxel grid.\n        normalize (bool): Determines whether to normalize vertices to a\n            unit cube centered at the origin.\n        vertex_offset (float): Offset applied to all vertices after\n                               normalizing.\n\n    """"""\n\n    def __init__(self, resolution: int,\n                 normalize: bool = True,\n                 vertex_offset: float = 0.):\n        self.resolution = resolution\n        self.normalize = normalize\n        self.vertex_offset = vertex_offset\n\n    def __call__(self, mesh: TriangleMesh):\n        """"""\n        Args:\n            mesh (kaolin.rep.TriangleMesh): Triangle mesh to convert to a\n                voxel grid.\n\n        Returns:\n            voxgrid (kaolin.rep.VoxelGrid): Converted voxel grid.\n\n        """"""\n        voxels = cvt.trianglemesh_to_voxelgrid(mesh, self.resolution,\n                                               normalize=self.normalize,\n                                               vertex_offset=self.vertex_offset)\n        return voxels\n\n\nclass TriangleMeshToSDF(Transform):\n    r""""""Converts a triangle mesh to a non-parameteric (point-based) signed\n    distance function (SDF).\n\n    Args:\n        num_samples (int): Number of points to sample on the surface of the\n            triangle mesh.\n        noise (float): Fraction of distance from the surface from which the\n            SDF is sampled (Eg. a value of 0.05 samples points that are at\n            a 5% fraction outside/inside the surface).\n\n    """"""\n\n    def __init__(self, num_samples: int = 10000, noise: float = 0.05):\n        self.num_samples = num_samples\n        self.noise = 1 + noise\n\n    def __call__(self, mesh: TriangleMesh):\n        """"""\n        Args:\n            mesh (kaolin.rep.TriangleMesh): Triangle mesh to convert to a\n                signed distance function.\n\n        Returns:\n            (torch.Tensor): A signed distance function.\n        """"""\n        sdf = cvt.trianglemesh_to_sdf(mesh, self.num_samples)\n        return sdf(self.noise * (torch.rand(self.num_samples, 3).to(mesh.device) - .5))\n\n\nclass MeshLaplacianSmoothing(Transform):\n    r"""""" Applies laplacian smoothing to the mesh.\n\n        Args:\n            iterations (int) : number of iterations to run the algorithm for.\n    """"""\n\n    def __init__(self, iterations: int):\n        self.iterations = iterations\n\n    def __call__(self, mesh: Type[Mesh]):\n        """"""\n        Args:\n            mesh (Mesh): Mesh to be smoothed.\n\n        Returns:\n            (Mesh): Rotated mesh.\n        """"""\n        mesh.laplacian_smoothing(self.iterations)\n        return mesh\n\n\nclass RealignMesh(Transform):\n    r"""""" Aligns the vertices to be in the same (axis-aligned) bounding\n    box as that of `target` vertices or point cloud.\n\n    Args:\n        target (torch.Tensor or PointCloud) : Target pointcloud to which `src`is\n            to be transformed (The `src` cloud is transformed to the\n            axis-aligned bounding box that the target cloud maps to). This\n            cloud must have the same number of dimensions :math:`D` as in the\n            source cloud. (shape: :math:`\\cdots \\times \\cdots \\times D`).\n\n    Returns:\n        (torch.Tensor): Pointcloud `src` realigned to fit in the (axis-aligned)\n            bounding box of the `tgt` cloud.\n\n    """"""\n\n    def __init__(self, target: Union[torch.Tensor, PointCloud]):\n        self.target = target\n\n    def __call__(self, mesh: Type[Mesh]):\n        """"""\n        Args:\n            mesh (Mesh): Mesh to be realigned.\n\n        Returns:\n            (Mesh): Realigned mesh.\n        """"""\n        mesh.vertices = pcfunc.realign(mesh.vertices, self.target)\n        return mesh\n\n\nclass SDFToTriangleMesh(Transform):\n    r"""""" Converts an SDF function to a mesh\n\n    Args:\n        bbox_center (float): Center of the surface\'s bounding box.\n        bbox_dim (float): Largest dimension of the surface\'s bounding box.\n        resolution (int) : The initial resolution of the voxel, should be large enough to\n            properly define the surface.\n        upsampling_steps (int) : Number of times the initial resolution will be doubled.\n            The returned resolution will be resolution * (2 ^ upsampling_steps).\n    """"""\n\n    def __init__(self, bbox_center: float, bbox_dim: float, resolution: int, upsampling_steps: int):\n        self.bbox_center = bbox_center\n        self.bbox_dim = bbox_dim\n        self.resolution = resolution\n        self.upsampling_steps = upsampling_steps\n\n    def __call__(self, sdf: Callable):\n        """"""\n        Args:\n            sdf (Callable): An object with a .eval_occ function which indicates\n                       which of a set of passed points is inside the surface.\n\n        Returns:\n            (TriangleMesh): Computed triangle mesh.\n        """"""\n        verts, faces = cvt.sdf_to_trianglemesh(sdf, self.bbox_center, self.bbox_dim,\n                                               self.resolution, self.upsampling_steps)\n        return TriangleMesh.from_tensors(vertices=verts, faces=faces)\n\n\nclass SDFToPointCloud(Transform):\n    r"""""" Converts an SDF fucntion to a point cloud\n\n    Args:\n        bbox_center (float): Center of the surface\'s bounding box.\n        bbox_dim (float): Largest dimension of the surface\'s bounding box.\n        resolution (int) : The initial resolution of the voxel, should be large enough to\n            properly define the surface.\n        upsampling_steps (int) : Number of times the initial resolution will be doubled.\n            The returned resolution will be resolution * (2 ^ upsampling_steps).\n        num_points (int): Number of points in computed point cloud.\n    """"""\n\n    def __init__(self, bbox_center: float, bbox_dim: float, resolution: int,\n                 upsampling_steps: int, num_points: int):\n        self.bbox_center = bbox_center\n        self.bbox_dim = bbox_dim\n        self.resolution = resolution\n        self.upsampling_steps = upsampling_steps\n        self.num_points = num_points\n\n    def __call__(self, sdf: Callable):\n        """"""\n        Args:\n            sdf (Callable): An object with a .eval_occ fucntion which indicates\n                       which of a set of passed points is inside the surface.\n\n        Returns:\n            (torch.FloatTensor): Computed point cloud.\n        """"""\n        return cvt.sdf_to_pointcloud(sdf, self.bbox_center, self.bbox_dim, self.resolution,\n                                     self.upsampling_steps, self.num_points)\n\n\nclass SDFToVoxelGrid(Transform):\n    r"""""" Converts an SDF function to a to a voxel grid\n\n    Args:\n        bbox_center (float): Center of the surface\'s bounding box.\n        bbox_dim (float): Largest dimension of the surface\'s bounding box.\n        resolution (int) : The initial resolution of the voxel, should be large enough to\n            properly define the surface.\n        upsampling_steps (int) : Number of times the initial resolution will be doubled.\n            The returned resolution will be resolution * (2 ^ upsampling_steps).\n    """"""\n\n    def __init__(self, bbox_center: float, bbox_dim: float, resolution: int,\n                 upsampling_steps: int):\n        self.bbox_center = bbox_center\n        self.bbox_dim = bbox_dim\n        self.resolution = resolution\n        self.upsampling_steps = upsampling_steps\n\n    def __call__(self, sdf: Callable):\n        """"""\n        Args:\n            sdf (Callable): An object with a .eval_occ fucntion which indicates\n                       which of a set of passed points is inside the surface.\n\n        Returns:\n            (torch.FloatTensor): Computed point cloud.\n        """"""\n        return cvt.sdf_to_voxelgrid(sdf, self.bbox_center, self.bbox_dim, self.resolution,\n                                    self.upsampling_steps)\n\n\nclass VoxelGridToTriangleMesh(Transform):\n    r"""""" Converts passed voxel to a mesh\n\n    Args:\n        thresh (float): threshold from which to make voxel binary\n        mode (str):\n            -\'exact\': exect mesh conversion\n            -\'marching_cubes\': marching cubes is applied to passed voxel\n        normalize (bool): whether to scale the array to (-.5,.5)\n    """"""\n\n    def __init__(self, threshold, mode, normalize):\n        self.thresh = threshold\n        self.mode = mode\n        self.normalize = normalize\n\n    def __call__(self, voxel: Type[VoxelGrid]):\n        """"""\n        Args:\n            voxel (torch.Tensor): Voxel grid.\n\n        Returns:\n            (TriangleMesh): Converted triangle mesh.\n        """"""\n        verts, faces = cvt.voxelgrid_to_trianglemesh(voxel, self.thresh, self.mode, self.normalize)\n        return TriangleMesh.from_tensors(vertices=verts, faces=faces)\n\n\nclass VoxelGridToQuadMesh(Transform):\n    r"""""" Converts passed voxel to quad mesh\n\n    Args:\n        threshold (float): Threshold from which to make voxel binary.\n        normalize (bool): Whether to scale the array to (-.5,.5).\n    """"""\n\n    def __init__(self, threshold: float, normalize: bool):\n        self.thresh = threshold\n        self.normalize = normalize\n\n    def __call__(self, voxel: Type[VoxelGrid]):\n        """"""\n        Args:\n            voxel (torch.Tensor): Voxel grid.\n\n        Returns:\n            (QuadMesh): Converted triangle mesh.\n        """"""\n        verts, faces = cvt.voxelgrid_to_quadmesh(voxel, self.thresh, self.normalize)\n        return QuadMesh.from_tensors(vertices=verts, faces=faces)\n\n\nclass VoxelGridToPointCloud(Transform):\n    r"""""" Converts  passed voxel to a pointcloud\n\n    Args:\n        num_points (int): Number of points in converted point cloud.\n        thresh (float): Threshold from which to make voxel binary.\n        mode (str):\n            -\'full\': Sample the whole voxel model.\n            -\'surface\': Sample only the surface voxels.\n        normalize (bool): Whether to scale the array to (-.5,.5).\n    """"""\n\n    def __init__(self, num_points: int, threshold: float, mode: str, normalize: bool):\n        self.num_points\n        self.thresh = threshold\n        self.mode\n        self.normalize = normalize\n\n    def __call__(self, voxel: Type[VoxelGrid]):\n        """"""\n        Args:\n            voxel (torch.Tensor): Voxel grid.\n\n        Returns:\n            (torch.Tensor): Converted point cloud.\n        """"""\n        return cvt.voxelgrid_to_pointcloud(voxel, self.num_points, self.thresh, self.mode, self.normalize)\n\n\nclass VoxelGridToSDF(Transform):\n    r"""""" Converts passed voxel to a signed distance fucntion.\n\n    Args:\n        voxel (torch.Tensor): Voxel grid\n        thresh (float): threshold from which to make voxel binary\n        normalize (bool): whether to scale the array to (0,1)\n\n    Returns:\n        a signed distance fucntion\n    """"""\n\n    def __init__(self, threshold: float, normalize: bool):\n        self.thresh = threshold\n        self.normalize = normalize\n\n    def __call__(self, voxel: Type[VoxelGrid]):\n        """"""\n        Args:\n            voxel (torch.Tensor): Voxel grid.\n\n        Returns:\n            (SDF): A signed distance function.\n        """"""\n        return cvt.voxelgrid_to_sdf(voxel, self.thresh, self.normalize)\n'"
kaolin/transforms/voxelfunc.py,47,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n# Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation components\n#\n# Copyright (c) 2019 Edward Smith\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom typing import Optional, Union, List\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom scipy import ndimage\n\nfrom kaolin.rep import VoxelGrid\nfrom kaolin.conversions.voxelgridconversions import confirm_def\nfrom kaolin.conversions.voxelgridconversions import threshold\nfrom kaolin.conversions.voxelgridconversions import extract_surface\nfrom kaolin import helpers\n\n\n# Tiny eps\nEPS = 1e-6\n\n\n# def downsample(voxel: Union[torch.Tensor, VoxelGrid], scale: List[int],\n#                inplace: Optional[bool] = True):\n#     r""""""Downsamples a voxelgrid, given a (down)scaling factor for each\n#     dimension.\n\n#     .. Note::\n#         The voxel output is not thresholded.\n\n#     Args:\n#         voxel (torch.Tensor): Voxel grid to be downsampled (shape: must\n#             be a tensor containing exactly 3 dimensions).\n#         scale (list): List of tensors to scale each dimension down by\n#             (length: 3).\n#         inplace (bool, optional): Bool to make the operation in-place.\n\n#     Returns:\n#         (torch.Tensor): Downsampled voxelgrid.\n\n#     Example:\n#         >>> x = torch.ones([32, 32, 32])\n#         >>> print (x.shape)\n#         torch.Size([32, 32, 32])\n#         >>> x = downsample(x, [2,2,2])\n#         >>> print (x.shape)\n#         torch.Size([16, 16, 16])\n#     """"""\n#     if isinstance(voxel, VoxelGrid):\n#         voxel = voxel.voxels\n#     voxel = confirm_def(voxel)\n\n#     if not inplace:\n#         voxel = voxel.clone()\n\n#     # Verify that all elements of `scale` are greater than or equal to 1 and\n#     # less than the voxel shape for the corresponding dimension.\n#     scale_filter = [1, 1]\n#     scale_factor = 1.\n#     for i in range(3):\n#         if scale[i] < 1:\n#             raise ValueError(\'Downsample ratio must be at least 1 along every\'\n#                 \' dimension.\')\n#         if scale[i] >= voxel.shape[i]:\n#             raise ValueError(\'Downsample ratio must be less than voxel shape\'\n#                 \' along every dimension.\')\n#         scale_filter.append(scale[i])\n#         scale_factor *= scale[i]\n#     conv_filter = torch.ones(scale_filter).to(voxel.device) / scale_factor\n\n#     voxel = F.conv3d(voxel.unsqueeze(0).unsqueeze(\n#         0), conv_filter, stride=scale, padding=0)\n#     voxel = voxel.squeeze(0).squeeze(0)\n\n#     return voxel\n\n\n# def upsample(voxel: torch.Tensor, dim: int):\n#     r""""""Upsamples a voxel grid by a given scaling factor.\n\n#     .. Note::\n#         The voxel output is not thresholded.\n\n#     Args:\n#         voxel (torch.Tensor): Voxel grid to be upsampled (shape: must\n#             be a 3D tensor)\n#         dim (int): New dimensionality (number of voxels along each dimension\n#             in the resulting voxel grid).\n\n#     Returns:\n#         torch.Tensor: Upsampled voxel grid.\n\n#     Example:\n#         >>> x = torch.ones([32, 32, 32])\n#         >>> print (x.shape)\n#         torch.Size([32, 32, 32])\n#         >>> x = upsample(x, 64)\n#         >>> print (x.shape)\n#         torch.Size([64, 64, 64])\n#     """"""\n#     if isinstance(voxel, VoxelGrid):\n#         voxel = voxel.voxels\n#     voxel = confirm_def(voxel)\n\n#     cur_shape = voxel.shape\n#     assert (dim >= cur_shape[0]) and ((dim >= cur_shape[1]) and (\n#         dim >= cur_shape[2])), \'All dim values must be larger then current dim\'\n\n#     new_positions = []\n#     old_positions = []\n\n#     # Defining position correspondences\n#     for i in range(3):\n#         shape_params = [1, 1, 1]\n#         shape_params[i] = dim\n#         new_pos = np.arange(dim).reshape(shape_params)\n\n#         for j in range(3):\n#             if i == j:\n#                 continue\n#             new_pos = np.repeat(new_pos, dim, axis=j)\n#         new_pos = new_pos.reshape(-1)\n\n#         ratio = float(cur_shape[i]) / float(dim)\n#         old_pos = (new_pos * ratio).astype(int)\n\n#         new_positions.append(new_pos)\n#         old_positions.append(old_pos)\n\n#     scaled_voxel = torch.FloatTensor(np.zeros([dim, dim, dim])).to(\n#         voxel.device)\n#     if voxel.is_cuda:\n#         scaled_voxel = scaled_voxel.cuda()\n\n#     scaled_voxel[tuple(new_positions)] = voxel[tuple(old_positions)]\n\n#     return scaled_voxel\n\n\n# def threshold(voxel: Union[torch.Tensor, VoxelGrid], thresh: float,\n#               inplace: Optional[bool] = True):\n#     r""""""Binarizes the voxel array using a specified threshold.\n\n#     Args:\n#         voxel (torch.Tensor): Voxel array to be binarized.\n#         thresh (float): Threshold with which to binarize.\n#         inplace (bool, optional): Bool to make the operation in-place.\n\n#     Returns:\n#         (torch.Tensor): Thresholded voxel array.\n\n#     """"""\n#     if isinstance(voxel, VoxelGrid):\n#         voxel = voxel.voxels\n#     if not inplace:\n#         voxel = voxel.clone()\n#     helpers._assert_tensor(voxel)\n#     voxel[voxel <= thresh] = 0\n#     voxel[voxel > thresh] = 1\n#     return voxel\n\n\n# def fill(voxel: Union[torch.Tensor, VoxelGrid], thresh: float = .5):\n#     r"""""" Fills the internal structures in a voxel grid. Used to fill holds\n#     and \'solidify\' objects.\n\n#     Args:\n#         voxel (torch.Tensor): Voxel grid to be filled.\n#         thresh (float): Threshold to use for binarization of the grid.\n\n#     Returns:\n#         torch.Tensor: filled voxel array\n#     """"""\n\n#     if isinstance(voxel, VoxelGrid):\n#         voxel = voxel.voxels\n#     voxel = confirm_def(voxel)\n#     voxel = threshold(voxel, thresh)\n#     voxel = voxel.clone()\n#     on = ndimage.binary_fill_holes(voxel.data.cpu())\n#     voxel[np.where(on)] = 1\n#     return voxel\n\n\n# def extract_surface(voxel: Union[torch.Tensor, VoxelGrid], thresh: float = .5):\n#     r""""""Removes any inernal structure(s) from a voxel array.\n\n#     Args:\n#         voxel (torch.Tensor): voxel array from which to extract surface\n#         thresh (float): threshold with which to binarize\n\n#     Returns:\n#         torch.Tensor: surface voxel array\n#     """"""\n\n#     if isinstance(voxel, VoxelGrid):\n#         voxel = voxel.voxels\n#     voxel = confirm_def(voxel)\n#     voxel = threshold(voxel, thresh)\n#     off_positions = voxel == 0\n\n#     conv_filter = torch.ones((1, 1, 3, 3, 3))\n#     surface_voxel = torch.zeros(voxel.shape)\n#     if voxel.is_cuda:\n#         conv_filter = conv_filter.cuda()\n#         surface_voxel = surface_voxel.cuda()\n\n#     local_occupancy = F.conv3d(voxel.unsqueeze(\n#         0).unsqueeze(0), conv_filter, padding=1)\n#     local_occupancy = local_occupancy.squeeze(0).squeeze(0)\n#     # only elements with exposed faces\n#     surface_positions = (local_occupancy < 27) * (local_occupancy > 0)\n#     surface_voxel[surface_positions] = 1\n#     surface_voxel[off_positions] = 0\n\n#     return surface_voxel\n\n\n# def extract_odms(voxel: Union[torch.Tensor, VoxelGrid]):\n#     r""""""Extracts an orthographic depth map from a voxel grid.\n\n#     Args:\n#         voxel (torch.Tensor): Voxel grid from which odms are extracted.\n\n#     Returns:\n#         (torch.Tensor): 6 ODMs from the 6 primary viewing angles.\n\n#     Example:\n#         >>> voxel = torch.ones([128,128,128])\n#         >>> voxel = extract_odms(voxel)\n#         >>> voxel.shape\n#         torch.Size([6, 128, 128])\n#     """"""\n#     if isinstance(voxel, VoxelGrid):\n#         voxel = voxel.voxels\n#     voxel = confirm_def(voxel)\n#     cuda = voxel.is_cuda\n#     voxel = extract_surface(voxel)\n#     voxel = voxel.data.cpu().numpy()\n\n#     dim = voxel.shape[-1]\n#     a, b, c = np.where(voxel == 1)\n#     big_list = [[[[dim, dim]\n#                   for j in range(dim)] for i in range(dim)] for k in range(3)]\n#     for i, j, k in zip(a, b, c):\n#         big_list[0][i][j][0] = (min(dim - k - 1, big_list[0][i][j][0]))\n#         big_list[0][i][j][1] = (min(k, big_list[0][i][j][1]))\n#         big_list[1][i][k][0] = (min(dim - j - 1, big_list[1][i][k][0]))\n#         big_list[1][i][k][1] = (min(j, big_list[1][i][k][1]))\n#         big_list[2][j][k][0] = (min(dim - i - 1, big_list[2][j][k][0]))\n#         big_list[2][j][k][1] = (min(i, big_list[2][j][k][1]))\n\n#     odms = np.zeros((6, dim, dim))\n#     big_list = np.array(big_list)\n#     for k in range(6):\n#         odms[k] = big_list[k // 2, :, :, k % 2]\n#     odms = torch.FloatTensor(np.array(odms))\n\n#     if cuda:\n#         odms = odms.cuda()\n\n#     return odms\n\n\n# def project_odms(odms: torch.Tensor,\n#                  voxel: torch.Tensor = None, votes: int = 1):\n#     r""""""Projects orthographic depth map onto a voxel array.\n\n#     .. Note::\n#         If no voxel grid is provided, we poject onto a completely filled grid.\n\n#     Args:\n#         odms (torch.Tensor): ODMs which are to be projected.\n#         voxel (torch.Tensor): Voxel grid onto which ODMs are projected.\n\n#     Returns:\n#         (torch.Tensor): Updated voxel grid.\n\n#     Example:\n#         >>> odms = torch.rand([6,128,128])*128\n#         >>> odms = voxel.int()\n#         >>> voxel = kal.rep.voxel.project_odms(odms)\n#         >>> voxel.shape\n#         torch.Size([128, 128, 128])\n#     """"""\n#     cuda = odms.is_cuda\n#     dim = odms.shape[-1]\n#     subtractor = 1. / float(votes)\n\n#     if voxel is None:\n#         voxel = torch.ones((dim, dim, dim))\n#     else:\n#         for i in range(3):\n#             assert (voxel.shape[i] == odms.shape[-1]\n#                     ), \'Voxel and odm dimension size must be the same\'\n#         if isinstance(voxel, VoxelGrid):\n#             voxel = voxel.voxels\n#         voxel = confirm_def(voxel)\n#         voxel = threshold(voxel, .5)\n#     voxel = voxel.data.cpu().numpy()\n#     odms = odms.data.cpu().numpy()\n\n#     for i in range(3):\n#         odms[2 * i] = dim - odms[2 * i]\n\n#     depths = np.where(odms <= dim)\n#     for x, y, z in zip(*depths):\n#         pos = int(odms[x, y, z])\n#         if x == 0:\n#             voxel[y, z, pos:dim] -= subtractor\n#         elif x == 1:\n#             voxel[y, z, 0:pos] -= subtractor\n#         elif x == 2:\n#             voxel[y, pos:dim, z] -= subtractor\n#         elif x == 3:\n#             voxel[y, 0:pos, z] -= subtractor\n#         elif x == 4:\n#             voxel[pos:dim, y, z] -= subtractor\n#         else:\n#             voxel[0:pos, y, z] -= subtractor\n\n#     on = np.where(voxel > 0)\n#     off = np.where(voxel <= 0)\n#     voxel[on] = 1\n#     voxel[off] = 0\n\n#     voxel = confirm_def(voxel)\n#     if cuda:\n#         voxel = voxel.cuda()\n#     return voxel\n\n\ndef max_connected(voxel: Union[torch.Tensor, VoxelGrid], thresh: float = .5):\n    r""""""Removes unconnecred voxels.\n\n    .. Note::\n        Largest maximum connected voxel is maintained.\n\n    Args:\n        voxel = voxel array\n        thresh (float): threshold with which to binarize\n\n    Returns:\n        torch.Torch: updated voxel array\n\n    Example:\n        >>> voxel = torch.rand \n    """"""\n    \n    voxel = voxel.clone()\n    voxel = threshold(voxel, thresh)\n    max_component = np.zeros(voxel.shape, dtype=bool)\n    for startx in range(voxel.shape[0]):\n        for starty in range(voxel.shape[1]):\n            for startz in range(voxel.shape[2]):\n                if not voxel[startx,starty,startz]:\n                    continue\n                # start a new component\n                component = np.zeros(voxel.shape, dtype=bool)\n                stack = [[startx,starty,startz]]\n                component[startx,starty,startz] = True\n                voxel[startx,starty,startz] = False\n                while len(stack) > 0:\n                    x,y,z = stack.pop()\n                    for i in range(x-1, x+1 + 1):\n                        for j in range(y-1, y+1 + 1):\n                            for k in range(z-1, z+1 + 1):\n                                if (i-x)**2+(j-y)**2+(k-z)**2 > 1:\n                                    continue\n                                if _voxel_exist(voxel, i,j,k):\n                                    voxel[i,j,k] = False\n                                    component[i,j,k] = True\n                                    stack.append([i,j,k])\n                if component.sum() > max_component.sum():\n                    max_component = component\n    \n    return torch.FloatTensor(max_component).to(voxel.device)\n\n\ndef _voxel_exist(voxels, x,y,z):\n    if x < 0 or y < 0 or z < 0 or x >= voxels.shape[0]\\\n        or y >= voxels.shape[1] or z >= voxels.shape[2]:\n        return False\n    else :\n        return voxels[x,y,z] == 1 \n\n\nif __name__ == \'__main__\':\n\n    device = \'cpu\'\n\n    # # Test downsample\n    # x = torch.ones([32, 32, 32]).to(device)\n    # x_ = downsample(x, [2, 2, 2])\n    # print(x.shape)\n    # print(x_.shape)\n\n    # # Test upsample\n    # x = torch.ones([32, 32, 32]).to(device)\n    # x_ = upsample(x, 64)\n    # print(x.shape)\n    # print(x_.shape)\n\n    # # Test threshold\n    # x = torch.rand([2, 2, 2]).to(device)\n    # x_ = threshold(x, 0.4)\n'"
kaolin/vision/__init__.py,0,b'from .geometry import *\n'
kaolin/vision/geometry.py,49,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n# Kornia components:\n# Copyright (C) 2017-2019, Arraiy, Inc., all rights reserved.\n# Copyright (C) 2019-    , Open Source Vision Foundation, all rights reserved.\n# Copyright (C) 2019-    , Kornia authors, all rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n""""""\nProjective geometry utility functions\n""""""\n\nimport torch\n\nfrom kaolin.mathutils import *\n\n# Borrows from Kornia.\n# https://github.com/kornia/kornia/blob/master/kornia/geometry/camera/perspective.py\ndef project_points(pts: torch.Tensor, intrinsics: torch.Tensor,\n                   extrinsics: torch.Tensor = None):\n    r""""""Projects a set of 3D points onto a 2D image, given the\n    camera parameters (intrinsics and extrinsics).\n\n    Args:\n        pts (torch.Tensor): 3D points to be projected onto the image\n            (shape: :math:`\\cdots \\times N \\times 3` or :math:`\\cdots\n            \\times N \\times 4`).\n        intrinsics (torch.Tensor): camera intrinsic matrix/matrices\n            (shape: :math:`B \\times 4 \\times 4` or :math:`4 \\times 4`).\n        extrinsics (torch.Tensor): camera extrinsic matrix/matrices\n            (shape: :math:`B \\times 4 \\times 4` or :math:`4 \\times 4`).\n\n    Note:\n        If pts is not of dim 2, then it is treated as a minibatch. For\n        each point set in the minibatch (of size :math:`B`), one can\n        apply the same pair of intrinsics or extrinsics (size :math:`4\n        \\times 4`), or choose a different intrinsic-extrinsic pair. In\n        the latter case, the passed intrinsics/extrinsics must be of\n        shape (:math:`B \\times 4 \\times 4`).\n\n    Returns:\n        (torch.Tensor): pixel coordinates of the input 3D points.\n\n    Examples:\n        >>> pts = torch.rand(5, 3)\n        tensor([[0.6411, 0.4996, 0.7689],\n                [0.2288, 0.9391, 0.2062],\n                [0.4991, 0.4673, 0.6192],\n                [0.0397, 0.3477, 0.4895],\n                [0.9219, 0.4121, 0.8046]])\n        >>> intrinsics = torch.FloatTensor([[720, 0, 120, 0],\n                                            [0, 720, 90, 0],\n                                            [0, 0, 1, 0],\n                                            [0, 0, 0, 1]])\n        >>> img_pts = kal.vision.project_points(pts, intrinsics)\n        tensor([[ 720.3091,  557.8361],\n                [ 919.0596, 3369.7185],\n                [ 700.4019,  633.3636],\n                [ 178.3637,  601.4078],\n                [ 945.0151,  458.7479]])\n        >>> img_pts.shape\n        torch.Size([5, 2])\n\n        >>> # `project_points()` also takes in batched inputs\n        >>> pts = torch.rand(10, 5, 3)\n        >>> # Applies the same intrinsics to all samples in the batch\n        >>> img_pts = kal.vision.project_points(pts, intrinsics)\n        torch.Size([10, 5, 2])\n\n        >>> # Optionally, can use a per-sample intrinsic, for each\n        >>> # example in the minibatch.\n        >>> intrinsics_a = intrinsics.repeat(5, 1, 1)\n        >>> intrinsics_b = torch.eye(4).repeat(5, 1, 1)\n        >>> # Use `intrinsics_a` for the first 5 samples and\n        >>> # `intrinsics_b` for the last 5\n        >>> intrinsics = torch.cat((intrinsics_a, intrinsics_b), dim=0)\n        >>> img_pts = kal.vision.project_points(pts, intrinsics)\n        >>> img_pts.shape\n        torch.Size([10, 5, 2])\n\n        >>> # Can also use a per sample extrinsics matrix\n        >>> pts = torch.rand(10, 5, 3)\n        >>> extrinsics = torch.eye(4).repeat(10, 1, 1)\n        >>> img_pts = kal.vision.project_points(pts, intrinsics, extrinsics)\n\n    """"""\n    if not torch.is_tensor(pts):\n        raise TypeError(\'Expected input pts to be of type torch.Tensor. \'\n                        \'Got {0} instead.\'.format(type(pts)))\n    if not torch.is_tensor(intrinsics):\n        raise TypeError(\'Expected input intrinsics to be of type \'\n                        \'torch.Tensor. Got {0} instead\'.format(type(intrinsics)))\n    if extrinsics is not None:\n        if not torch.is_tensor(extrinsics):\n            raise TypeError(\'Expected input extrinsics to be of type \'\n                            \'torch.Tensor. Got{0} instead.\'.format(type(extrinsics)))\n\n    if pts.dim() < 2:\n        raise ValueError(\'Expected input pts to have at least 2 dims. \'\n                         \'Got only {0}\'.format(pts.dim()))\n    if pts.shape[-1] not in [3, 4]:\n        raise ValueError(\'Last dim of input pts must be of shape \'\n                         \'3 or 4. Got {0} instead.\'.format(pts.shape[-1]))\n\n    # Infer the batchsize of pts (assume it is 1, to begin with)\n    batchsize = 1\n\n    if pts.dim() > 2:\n        batchsize = pts.shape[0]\n    # if pts.dim() == 2:\n    #     pts = pts.unsqueeze(0)\n\n    # If extrinsics is None, set to identity\n    if extrinsics is None:\n        extrinsics = torch.eye(4).to(pts.device)\n\n    if intrinsics.shape[-2:] != (4, 4):\n        raise ValueError(\'Expected intrinsics to be of shape (4, 4). \'\n                         \'Got {0} instead.\'.format(intrinsics.shape))\n    if extrinsics.shape[-2:] != (4, 4):\n        raise ValueError(\'Expected extrinsics to be of shape (4, 4). \'\n                         \'Got {0} instead.\'.format(extrinsics.shape))\n\n    if intrinsics.dim() > 2:\n        if intrinsics.shape[0] != batchsize and intrinsics.shape[0] != 1:\n            raise ValueError(\'Dimension 0 of intrinsics must be either \'\n                             \'equal to 1, or equal to the batch size of input pts. \'\n                             \'Got {0} instead.\'.format(intrinsics.shape[0]))\n    if extrinsics.dim() > 2:\n        if extrinsics.shape[0] != batchsize and extrinsics.shape[0] != 1:\n            raise ValueError(\'Dimension 0 of extrinsics must be either \'\n                             \'equal to 1, or equal to the batch size of input pts. \'\n                             \'Got {0} instead.\'.format(extrinsics.shape[0]))\n        if intrinsics.shape[0] != extrinsics.shape[0]:\n            raise ValueError(\'Inputs intrinsics and extrinsics must \'\n                             \'have same shape at dim 0. Got {0} and {1}.\'.format(\n                                 intrinsics.shape[0], extrinsics.shape[0]))\n\n    # Determine whether or not to homogenize pts\n    if pts.shape[-1] == 3:\n        pts = homogenize_points(pts)\n\n    # Perform projection\n    pts = transform3d(pts, torch.matmul(intrinsics, extrinsics))\n    x = pts[..., 0]\n    y = pts[..., 1]\n    z = pts[..., 2]\n    u = x / torch.where(z == 0, torch.ones_like(z), z)\n    v = y / torch.where(z == 0, torch.ones_like(z), z)\n\n    return torch.stack([u, v], dim=-1)\n\n\n# Borrows from Kornia.\n# https://github.com/kornia/kornia/blob/master/kornia/geometry/camera/perspective.py\ndef unproject_points(pts: torch.Tensor, depth: torch.Tensor,\n                     intrinsics: torch.Tensor):\n    r""""""Unprojects (back-projects) a set of points from a 2D image\n    to 3D camera coordinates, given depths and the intrinsics.\n\n    Args:\n        pts (torch.Tensor): 2D points to be \'un\'projected to 3D.\n            (shape: :math:`\\cdots \\times N \\times 2` or\n            :math:`\\cdots \\times 3`).\n        depth (torch.Tensor): Depth for each point in pts (shape:\n            :math:`\\cdots \\times N \\times 1`).\n        intrinsics (torch.Tensor): Camera intrinsics (shape: :math:`\n            \\cdots \\times 4 \\times 4`).\n\n    Returns:\n        (torch.Tensor): Camera coordinates of the input points.\n            (shape: :math:`\\cdots \\times 3`)\n\n    Examples:\n        >>> img_pts = torch.rand(5, 2)\n        tensor([[0.6591, 0.8643],\n                [0.4913, 0.8048],\n                [0.2129, 0.2338],\n                [0.9604, 0.2347],\n                [0.5779, 0.9745]])\n        >>> depths = torch.rand(5) + 1.\n        tensor([1.4135, 1.0138, 1.6001, 1.6868, 1.0867])\n        >>> intrinsics = torch.FloatTensor([[720, 0, 120, 0],\n                                            [0, 720, 90, 0],\n                                            [0, 0, 1, 0],\n                                            [0, 0, 0, 1]])\n        >>> cam_pts = kal.vision.unproject_points(img_pts, depths, intrinsics)\n        tensor([[-0.2343, -0.1750,  1.4135],\n                [-0.1683, -0.1256,  1.0138],\n                [-0.2662, -0.1995,  1.6001],\n                [-0.2789, -0.2103,  1.6868],\n                [-0.1802, -0.1344,  1.0867]])\n        >>> cam_pts.shape\n        torch.Size([5, 3])\n\n        >>> # Also works for batched inputs\n        >>> img_pts = torch.rand(10, 5, 2)\n        >>> depths = torch.rand(10, 5) + 1.\n        >>> cam_pts = kal.vision.unproject_points(img_pts, depths, intrinsics)\n        >>> cam_pts.shape\n        torch.Size([10, 5, 3])\n\n        >>> # Just like for `project_points()`, can use a per-sample intrinsics\n        >>> # matrix.\n        >>> intrinsics_a = intrinsics.repeat(5, 1, 1)\n        >>> intrinsics_b = torch.eye(4).repeat(5, 1, 1)\n        >>> # Use `intrinsics_a` for the first 5 samples and\n        >>> # `intrinsics_b` for the last 5\n        >>> intrinsics = torch.cat((intrinsics_a, intrinsics_b), dim=0)\n        >>> cam_pts = kal.vision.project_points(img_pts, depths, intrinsics)\n        >>> cam_pts.shape\n        torch.Size([10, 5, 3])\n\n    """"""\n    if not torch.is_tensor(pts):\n        raise TypeError(\'Expected input pts to be of type torch.Tensor. \'\n                        \'Got {0} instead.\'.format(type(pts)))\n    if not torch.is_tensor(depth):\n        raise TypeError(\'Expected input depth to be of type torch.Tensor. \'\n                        \'Got {0} instead.\'.format(type(depth)))\n    if not torch.is_tensor(intrinsics):\n        raise TypeError(\'Expected input intrinsics to be of type \'\n                        \'torch.Tensor. Got {0} instead\'.format(\n                            type(intrinsics)))\n\n    if pts.dim() < 2:\n        raise ValueError(\'Expected input pts to have at least 2 dims. \'\n                         \'Got only {0}\'.format(pts.dim()))\n    if pts.shape[-1] not in [2, 3]:\n        raise ValueError(\'Last dim of input pts must be of shape \'\n                         \'2 or 3. Got {0} instead.\'.format(pts.shape[-1]))\n\n    if depth.shape[-1] != 1:\n        if depth.dim() == pts.dim() - 1:\n            # If the dim of depth differs from the dim of pts by just 1,\n            # try appending an additional dimension to make it work.\n            # Else, raise a ValueError.\n            depth = depth.unsqueeze(-1)\n        else:\n            raise ValueError(\'Input depth must have shape 1 in the last \'\n                             \'dimension. Got {0} instead.\'.format(depth.shape[-1]))\n    if depth.shape[:-1] != pts.shape[:-1]:\n        raise ValueError(\'Inputs pts and depth must have matching shapes \'\n                         \'except at the last dimension. Got {0} and {1} respectively.\'\n                         \'\'.format(pts.shape, depth.shape))\n\n    # Homogenize pts if needed\n    if pts.shape[-1] == 2:\n        # If pts is 2D, homogenize twice (as we need to\n        # apply a 4 x 4 intrinsics inverse matrix)\n        pts = homogenize_points(pts)\n        pts = homogenize_points(pts)\n    elif pts.shape[-1] == 3:\n        pts = homogenize_points(pts)\n\n    return transform3d(pts, intrinsics.inverse()) * depth\n'"
kaolin/visualize/__init__.py,0,b'from .vis import *\n'
kaolin/visualize/vis.py,6,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Union\n\nimport torch\nimport trimesh\nimport numpy as np\nimport kaolin\nimport kaolin as kal\nimport pptk\n\n\ndef show(inp: Union[kaolin.rep.Mesh, kaolin.rep.PointCloud,\n         kaolin.rep.VoxelGrid], options: dict = {}, mode=\'points\'):\n    r""""""Visualizer class, for the representations defined in kaolin.rep.\n\n    Args:\n        inp (kaolin.rep.Mesh or kaolin.rep.PointCloud or kaolin.rep.VoxelGrid):\n            A kaolin.rep object to visualize\n        options (dict): Visualization options\n\n    """"""\n    if isinstance(inp, kaolin.rep.Mesh):\n        colors = [.7, .2, .2]\n        if \'colors\' in options:\n            colors = options[\'colors\']\n        show_mesh(inp, colors)\n    elif isinstance(inp, kaolin.rep.PointCloud):\n        colors = [.7, .2, .2]\n        if \'colors\' in options:\n            colors = options[\'colors\']\n        show_pointcloud(inp, colors)\n    elif isinstance(inp, kaolin.rep.VoxelGrid):\n        thresh = 0.5\n        mode = \'exact\'\n        colors = [.7, .2, .2]\n        if \'thres\' in options:\n            thresh = options[\'thresh\']\n        if mode in options:\n            mode = options[\'mode\']\n        if \'colors\' in options:\n            colors = options[\'colors\']\n        show_voxelgrid(inp, thresh, mode, colors)\n    elif mode == \'voxels\':\n        thresh = 0.5\n        mode = \'exact\'\n        colors = [.7, .2, .2]\n        if \'thres\' in options:\n            thresh = options[\'thresh\']\n        if mode in options:\n            mode = options[\'mode\']\n        if \'colors\' in options:\n            colors = options[\'colors\']\n        show_voxelgrid(inp, thresh, mode, colors)\n    elif mode == \'points\':\n        colors = [.7, .2, .2]\n        if \'colors\' in options:\n            colors = options[\'colors\']\n        show_pointcloud(inp, colors)\n\n\ndef show_mesh(input_mesh: kaolin.rep.Mesh, colors: list = [.7, .2, .2]):\n    r"""""" Visualizer for meshes\n\n    Args:\n            verts (torch.Tensor): vertices of mesh to be visualized\n            faces (torch.Tensor): faces of mesh to be visualized\n            colors (list): rbg colour values for rendered mesh\n    """"""\n\n    mesh = trimesh.Trimesh(vertices=input_mesh.vertices.data.cpu().numpy(),\n                           faces=input_mesh.faces.data.cpu().numpy())\n    mesh.visual.vertex_colors = colors\n    mesh.show()\n\n\ndef show_sdf(sdf: kaolin.rep.SDF, mode=\'mesh\', bbox_center: float = 0.,\n             bbox_dim: float = 1., num_points: int = 100000,\n             colors=[.7, .2, .2]):\n    r"""""" Visualizer for voxel array\n\n    Args:\n        sdf (kaolin.rep.SDF): sdf class object.\n        mode (str): visualization mode, can render as a mesh, a pointcloud,\n                or a colourful sdf pointcloud.\n        colors (list): RGB colour values for rendered array.\n    """"""\n    assert mode in [\'mesh\', \'pointcloud\', \'sdf\']\n\n    if mode == \'mesh\':\n        verts, faces = kal.conversion.SDF.to_mesh(sdf, bbox_center, bbox_dim)\n        mesh = trimesh.Trimesh(vertices=verts.data.cpu().numpy(),\n                               faces=faces.data.cpu().numpy())\n        mesh.visual.vertex_colors = colors\n        mesh.show()\n\n    elif mode == \'pointcloud\':\n        points = torch.rand(num_points, 3)\n        points = bbox_dim * (points + (bbox_center - .5))\n        distances = sdf(points)\n        points = points[distances <= 0]\n        kal.visualize.show_point(points)\n\n    elif mode == \'sdf\':\n        points = torch.rand(num_points, 3)\n        points = bbox_dim * (points + (bbox_center - .5))\n        distances = sdf(points)\n        v = pptk.viewer(points.data.cpu().numpy())\n        v.attributes(distances.data.cpu().numpy())\n        input()\n        v.close()\n\n\ndef show_pointcloud(points, colors=[.7, .2, .2]):\n    r""""""Visualizer for point clouds.\n\n    Args:\n        points (torch.Tensor): point cloud to be visualized\n        colors (list): rbg colour values for rendered array\n\n    """"""\n    point_colours = np.zeros(points.shape)\n    point_colours[:] = colors\n\n    v = pptk.viewer(points.data.cpu().numpy())\n    v.attributes(point_colours)\n    input()\n    v.close()\n\n\ndef show_voxelgrid(voxel, thresh=.5, mode=\'exact\', colors=[.7, .2, .2]):\n    r"""""" Visualizer for voxel array\n\n    Args:\n            voxel (torch.Tensor): voxel array to be visualized\n            threshold (float): threshold for turning on voxel\n            mode (str): mode for visualizing, either the exact model, or converted to mesh using marching cubes\n            colors (list): rbg colour values for rendered array\n    """"""\n    assert (mode in [\'exact\', \'marching_cubes\'])\n    voxel = kal.conversions.voxelgridconversions.confirm_def(voxel)\n    voxel = kal.conversions.voxelgridconversions.threshold(voxel, thresh=thresh)\n\n    verts, faces = kal.conversions.voxelgrid_to_trianglemesh(\n        voxel.cpu(), thresh=.5, mode=mode)\n    mesh = trimesh.Trimesh(vertices=verts,\n                           faces=faces)\n    mesh.visual.vertex_colors = colors\n    mesh.show()\n'"
kaolin/visualize/vis_usd.py,12,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nimport math\nimport torch\nimport logging\nimport numpy as np\nfrom pathlib import Path\nimport os\nfrom typing import Dict, Iterable, List, Tuple, Union\n\nfrom pxr import Usd, UsdGeom, UsdLux, Sdf, Gf, Vt\n\nfrom kaolin.rep.Mesh import Mesh\nfrom kaolin.rep import TriangleMesh\nfrom kaolin.rep import QuadMesh\nfrom kaolin.rep import PointCloud\nfrom kaolin.rep import VoxelGrid\n\n\ndef is_mesh(obj):\n    if isinstance(obj, dict) and {\'vertices\', \'faces\'} <= set(obj):\n        return len(obj[\'faces\'][0]) in [3, 4]\n    return isinstance(obj, TriangleMesh)\n\n\nclass VisUsd:\n    r""""""Class to visualize using USD (Universal Scene Description).\n    """"""\n\n    STAGE_SIZE = 100\n\n    def __init__(self):\n        self.stage = None\n\n    def set_stage(self, filepath: str = \'./visualizations/visualize.usda\', up_axis=\'Z\'):\n        r""""""Setup stage with basic structure and primitives.\n\n        Args:\n            filepath (str): Path to save visualizations to. Must end in either "".usd"" or "".usda"".\n            up_axis (str): Specify up-axis, choose from [""Y"", ""Z""].\n        """"""\n        filepath = Path(filepath)\n        if not filepath.parent.exists():\n            filepath.parent.mkdir()\n        self.stage = Usd.Stage.CreateNew(str(filepath))\n\n        self.up_axis = up_axis.upper()\n        self.up_axis_index = {\'Y\': 1, \'Z\': 2}[self.up_axis]\n        assert (self.up_axis in [\'Y\', \'Z\'])\n        UsdGeom.SetStageUpAxis(self.stage, self.up_axis)\n\n        self._setup_primitives()\n\n        self.save()\n\n    def _setup_primitives(self):\n        r"""""" Setup basic primitives useful for rendering meshes, point clouds and voxels. """"""\n        UsdGeom.Xform.Define(self.stage, \'/Root\')\n        root = self.stage.GetPrimAtPath(\'/Root\')\n        self.stage.SetDefaultPrim(root)\n        UsdGeom.Xform.Define(self.stage, \'/Root/Visualizer\')\n        instancer = UsdGeom.PointInstancer.Define(self.stage, \'/PointInstancer\')\n\n        prim_paths = []\n\n        sphere_proto = UsdGeom.Sphere.Define(self.stage, instancer.GetPath().AppendChild(\'Sphere\'))\n        cube_proto = UsdGeom.Cube.Define(self.stage, instancer.GetPath().AppendChild(\'Cube\'))\n        prim_paths = [sphere_proto.GetPath(), cube_proto.GetPath()]\n\n        instancer.CreatePrototypesRel().SetTargets(prim_paths)\n        self.instancer = instancer\n        self.save()\n\n    def visualize(self, object_3d: Union[Dict[str, torch.Tensor], Mesh, PointCloud, VoxelGrid],\n                  object_path: str = \'/Root/Visualizer/object\',\n                  fit_to_stage: bool = True, meet_ground: bool = True, center_on_stage: bool = True,\n                  translation: Tuple[float, float, float] = (0., 0., 0.)):\n        r"""""" Create USD file with object_3d representation.\n\n            Args:\n                object_3d (dict or Mesh or PointCloud or VoxelGrid): The object to visualize.\n                object_path (str): The object\'s path in the USD. This argument is only applicable to meshes.\n                fit_to_stage (bool): Whether to resize the objec to fit within a 100x100x100 stage.\n                meet_ground (bool): Whether to translate the object to be resting on a ground place at 0.\n                translation (Tuple[float, float, float]): Translation of object. Applied after meet_ground.\n        """"""\n\n        assert self.stage is not None\n\n        params = {\n            \'object_path\': object_path,\n            \'translation\': translation,\n            \'fit_to_stage\': fit_to_stage,\n            \'meet_ground\': meet_ground,\n            \'center_on_stage\': center_on_stage,\n        }\n\n        if is_mesh(object_3d):\n            self._visualize_mesh(object_3d, **params)\n        elif isinstance(object_3d, PointCloud):\n            self._visualize_points(object_3d, **params)\n        elif isinstance(object_3d, VoxelGrid):\n            self._visualize_voxels(object_3d, **params)\n        else:\n            raise ValueError(f\'Object of type {type(object_3d)} is not supported.\')\n\n    def _visualize_voxels(self, voxels: VoxelGrid, translation: Tuple[float, float, float] = (0., 0., 0.),\n                          **kwargs):\n        r"""""" Visualize voxels in USD.\n\n        Args:\n            points (torch.Tensor): Array of points of size (num_points, 3)\n\n        """"""\n\n        points = torch.nonzero(voxels.voxels.detach()).float()\n        points, scale, _ = self._fit_to_stage(points, **kwargs)\n        points, _ = self._set_points_bottom(points, scale)\n\n        points[:, 0] += translation[0]\n        points[:, 1] += translation[1]\n        points[:, 2] += translation[2]\n\n        indices = [1] * points.shape[0]\n        points = points.cpu().numpy().astype(float)\n        positions = points.tolist()\n        orientations = [Gf.Quath(1.0, 0.0, 0.0, 0.0)] * points.shape[0]\n        scales = [Gf.Vec3f(float(scale))] * points.shape[0]\n\n        self.instancer.GetProtoIndicesAttr().Set(indices)\n        self.instancer.GetPositionsAttr().Set(positions)\n        self.instancer.GetOrientationsAttr().Set(orientations)\n        self.instancer.GetScalesAttr().Set(scales)\n\n        self.save()\n\n    def _visualize_points(self, pointcloud: PointCloud, translation: Tuple[float, float, float] = (0., 0., 0.),\n                          **kwargs):\n        r"""""" Visualize points in USD.\n        """"""\n\n        points, _, _ = self._fit_to_stage(pointcloud.points.detach())\n\n        points[:, 0] += translation[0]\n        points[:, 1] += translation[1]\n        points[:, 2] += translation[2]\n\n        indices = [0] * points.shape[0]\n        points = points.cpu().numpy().astype(float)\n        positions = points.tolist()\n        orientations = [Gf.Quath(1.0, 0.0, 0.0, 0.0)] * points.shape[0]\n        scales = [Gf.Vec3f(1)] * points.shape[0]\n\n        self.instancer.GetProtoIndicesAttr().Set(indices)\n        self.instancer.GetPositionsAttr().Set(positions)\n        self.instancer.GetOrientationsAttr().Set(orientations)\n        self.instancer.GetScalesAttr().Set(scales)\n\n        self.save()\n\n    def _visualize_mesh(self, mesh: Union[Dict[str, torch.Tensor], Mesh],\n                        object_path: str,\n                        translation: Tuple[float, float, float] = (0., 0., 0.), **kwargs):\n        r"""""" Visualize mesh in USD.\n        """"""\n\n        if isinstance(mesh, Mesh):\n            vertices, faces = mesh.vertices.detach(), mesh.faces.detach()\n        else:\n            vertices, faces = mesh[\'vertices\'], mesh[\'faces\']\n\n        usd_mesh = UsdGeom.Mesh.Define(self.stage, object_path)\n\n        num_faces = faces.size(0)\n        is_tri = (faces.size(1) == 3)\n        face_vertex_counts = [faces.size(1)] * num_faces\n\n        vertices, _, _ = self._fit_to_stage(vertices, **kwargs)\n\n        vertices = vertices.detach().cpu().numpy().astype(float)\n        points = vertices.tolist()\n        faces = faces.detach().cpu().view(-1).numpy().astype(int)\n\n        usd_mesh.GetFaceVertexCountsAttr().Set(face_vertex_counts)\n        usd_mesh.GetPointsAttr().Set(Vt.Vec3fArray(points))\n        usd_mesh.GetFaceVertexIndicesAttr().Set(faces)\n        if is_tri:\n            usd_mesh.GetPrim().GetAttribute(\'subdivisionScheme\').Set(\'none\')\n        UsdGeom.XformCommonAPI(usd_mesh.GetPrim()).SetTranslate(translation)\n\n        self.save()\n\n    def _fit_to_stage(self, points: torch.Tensor, center_on_stage: bool = True,\n                      meet_ground: bool = True, fit_to_stage: bool = True, **kwargs):\n        r"""""" Scale and translate (in the Y axis) to fit to\n        the specified stage size and keep the object above the floor.\n\n        Args:\n            points (torch.Tensor): Tensor of points of size (N, 3).\n\n        Returns:\n            Tuple[torch.Tensor, float, torch.Tensor]: Tuple containing scaled and\n                translated tensor, scale value, and translation tensor.\n        """"""\n        scale = 1.\n        translation = [0., 0., 0.]\n        if center_on_stage:\n            # center at 0, 0, 0\n            points, translation = self._set_points_center(points)\n\n        if fit_to_stage:\n            # scale points to fit within STAGE_SIZE\n            points, scale = self._fit_points(points, self.STAGE_SIZE)\n\n        if meet_ground:\n            # set bottom as 0.0 on up_axis\n            points, up_translation = self._set_points_bottom(points, **kwargs)\n            translation[self.up_axis_index] += up_translation\n        return points, scale, translation\n\n    def _fit_points(self, points, fit_size):\n        r""""""Scale points to fit in cube of size fit_size. """"""\n        longest_side = max(torch.abs(torch.max(points, 0)[0] - torch.min(points, 0)[0]))\n        scale = fit_size / longest_side\n        points *= scale\n        return points, scale\n\n    def _set_points_bottom(self, points: np.ndarray, bottom: float = 0.0, **kwargs):\n        r""""""Move points to be above bottom value on axis""""""\n        y_translation = torch.min(points[:, self.up_axis_index])\n        points[:, self.up_axis_index] -= y_translation - bottom\n        return points, y_translation\n\n    def _set_points_center(self, points, center_point: List[float] = [0., 0., 0.]):\n        r""""""Set center of points to match center_point.""""""\n        center_point = torch.tensor(center_point, device=points.device)\n        extents = torch.max(points, 0)[0] - torch.min(points, 0)[0]\n        curr_center = torch.max(points, 0)[0] - extents / 2.0\n        translation = center_point - curr_center\n        points += translation\n        return points, translation\n\n    def save(self):\n        r"""""" Save stage. """"""\n        self.stage.Save()\n'"
packman/bootstrap/install_package.py,0,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\r\n\r\n# Licensed under the Apache License, Version 2.0 (the ""License"");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an ""AS IS"" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\nimport logging\r\nimport zipfile\r\nimport tempfile\r\nimport sys\r\nimport shutil\r\n\r\n__author__ = ""hfannar""\r\nlogging.basicConfig(level=logging.WARNING, format=""%(message)s"")\r\nlogger = logging.getLogger(""install_package"")\r\n\r\n\r\nclass TemporaryDirectory:\r\n    def __init__(self):\r\n        self.path = None\r\n\r\n    def __enter__(self):\r\n        self.path = tempfile.mkdtemp()\r\n        return self.path\r\n\r\n    def __exit__(self, type, value, traceback):\r\n        # Remove temporary data created\r\n        shutil.rmtree(self.path)\r\n\r\n\r\ndef install_package(package_src_path, package_dst_path):\r\n    with zipfile.ZipFile(\r\n        package_src_path, allowZip64=True\r\n    ) as zip_file, TemporaryDirectory() as temp_dir:\r\n        zip_file.extractall(temp_dir)\r\n        # Recursively copy (temp_dir will be automatically cleaned up on exit)\r\n        try:\r\n            # Recursive copy is needed because both package name and version folder could be missing in\r\n            # target directory:\r\n            shutil.copytree(temp_dir, package_dst_path)\r\n        except OSError as exc:\r\n            logger.warning(\r\n                ""Directory %s already present, packaged installation aborted"" % package_dst_path\r\n            )\r\n        else:\r\n            logger.info(""Package successfully installed to %s"" % package_dst_path)\r\n\r\n\r\ninstall_package(sys.argv[1], sys.argv[2])\r\n'"
tests/conversions/test_meshconversions.py,1,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport torch\nimport sys\n\nimport kaolin as kal\nfrom kaolin.rep import TriangleMesh\n\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_to_trianglemesh_to_pointcloud(device):\n\tmesh = TriangleMesh.from_obj(\'tests/model.obj\')\n\tif device == \'cuda\':\n\t\tmesh.cuda()\n\t\n\tpoints, _ = kal.conversions.trianglemesh_to_pointcloud(mesh, 10)\n\tassert (set(points.shape) == set([10, 3]))\n\tpoints, _ = kal.conversions.trianglemesh_to_pointcloud(mesh, 10000)\n\tassert (set(points.shape) == set([10000, 3]))\n\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_trianglemesh_to_voxelgrid(device):\n\tmesh = TriangleMesh.from_obj(\'tests/model.obj\')\n\tif device == \'cuda\':\n\t\tmesh.cuda()\n\tvoxels = kal.conversions.trianglemesh_to_voxelgrid(mesh, 32,\n\t\tnormalize =\'unit\')\n\tassert (set(voxels.shape) == set([32, 32, 32]))\n\tvoxels = kal.conversions.trianglemesh_to_voxelgrid(mesh, 64,\n\t\tnormalize =\'unit\')\n\tassert (set(voxels.shape) == set([64, 64, 64]))\n\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_trianglemesh_to_sdf(device):\n\tmesh = TriangleMesh.from_obj(\'tests/model.obj\')\n\tif device == \'cuda\':\n\t\tmesh.cuda()\n\tprint(mesh.device)\n\tsdf = kal.conversions.trianglemesh_to_sdf(mesh)\n\tdistances = sdf(torch.rand(100,3).to(device) -.5)\n\tassert (set(distances.shape) == set([100]))\n\tassert ((distances >0).sum()) > 0\n\tassert ((distances <0).sum()) > 0\n'"
tests/conversions/test_pointcloudconversions.py,0,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport torch\nimport sys\n\nimport kaolin as kal\nfrom kaolin.rep import TriangleMesh\n\n\ndef test_pointcloud_to_voxelgrid(device=\'cpu\'): \n    mesh = TriangleMesh.from_obj(\'tests/model.obj\')\n    if device == \'cuda\':\n        mesh.cuda()\n    pts, _ = kal.conversions.trianglemesh_to_pointcloud(mesh, 1000)\n\n    voxels = kal.conversions.pointcloud_to_voxelgrid(pts, 32, 0.1)\n    assert(voxels.shape == (32, 32, 32))\n\n\ndef test_pointcloud_to_trianglemesh(device=\'cpu\'):\n    mesh = TriangleMesh.from_obj(\'tests/model.obj\')\n    if device == \'cuda\':\n        mesh.cuda()\n    pts, _ = kal.conversions.trianglemesh_to_pointcloud(mesh, 1000)\n    mesh_ = kal.conversions.pointcloud_to_trianglemesh(pts)\n\n\ndef test_pointcloud_to_sdf(device=\'cpu\'):\n    mesh = TriangleMesh.from_obj(\'tests/model.obj\')\n    if device == \'cuda\':\n        mesh.cuda()\n    pts, _ = kal.conversions.trianglemesh_to_pointcloud(mesh, 1000)\n    sdf_ = kal.conversions.pointcloud_to_trianglemesh(pts)    \n'"
tests/conversions/test_sdfconversions.py,6,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport torch\nimport sys\n\nimport kaolin as kal\n\n\ndef test_sdf_to_pointcloud():\n\t\n\tsdf = kal.rep.SDF.sphere()\n\tpoints = kal.conversions.sdf_to_pointcloud(sdf, bbox_center=0.,\n\t\tresolution=10, bbox_dim=1,  num_points = 10000)\n\t\n\tassert (set(points.shape) == set([10000, 3]))\n\tassert kal.rep.SDF._length(points).mean() <=.6\n\tassert kal.rep.SDF._length(points).mean() >=.4\n\n\tpoints = kal.conversions.sdf_to_pointcloud(sdf, bbox_center=0.,\n\t\tresolution=32, bbox_dim=1,  num_points = 10000)\n\t\n\tassert (set(points.shape) == set([10000, 3]))\n\tassert kal.rep.SDF._length(points).mean() <=.55\n\tassert kal.rep.SDF._length(points).mean() >=.45\n\n\tsdf = kal.rep.SDF.box(h =.2, w = .4, l = .5)\n\tpoints = kal.conversions.sdf_to_pointcloud(sdf, bbox_center=0.,\n\t\tresolution=10, bbox_dim=1,  num_points = 10000)\n\t\n\tassert (torch.abs(points[:,0])>.22).sum() == 0\n\tassert (torch.abs(points[:,1])>.44).sum() == 0\n\tassert (torch.abs(points[:,2])>.55).sum() == 0\n\n\ndef test_sdf_to_trianglemesh():\n\n\tsdf = kal.rep.SDF.sphere()\n\tverts, faces = kal.conversions.sdf_to_trianglemesh(sdf, bbox_center=0.,\n\t\tresolution=10, bbox_dim=1)\n\n\tassert kal.rep.SDF._length(verts).mean() <=.6\n\tassert kal.rep.SDF._length(verts).mean() >=.4\n\n\tverts, faces = kal.conversions.sdf_to_trianglemesh(sdf, bbox_center=0.,\n\t\tresolution=32, bbox_dim=1)\n\t\n\tassert kal.rep.SDF._length(verts).mean() <=.55\n\tassert kal.rep.SDF._length(verts).mean() >=.45\n\n\tsdf = kal.rep.SDF.box(h =.2, w = .4, l = .5)\n\tverts, faces = kal.conversions.sdf_to_trianglemesh(sdf, bbox_center=0.,\n\t\tresolution=10, bbox_dim=1)\n\t\n\tassert (torch.abs(verts[:,0])>.22).sum() == 0\n\tassert (torch.abs(verts[:,1])>.44).sum() == 0\n\tassert (torch.abs(verts[:,2])>.55).sum() == 0\n\n\ndef test_sdf_to_voxelgrid():\n\n\tsdf = kal.rep.SDF.sphere()\n\tvoxels = kal.conversions.sdf_to_voxelgrid(sdf, bbox_center=0.,\n\t\tresolution=10, bbox_dim=1)\n\n\tassert (voxels).sum() >= 1\n\tassert (voxels).sum() <= (41**3) /2\n\n\tvoxels = kal.conversions.sdf_to_voxelgrid(sdf, bbox_center=0.,\n\t\tresolution=32, bbox_dim=1)\n\n\tassert (voxels).sum() >= 1\n\tassert (voxels).sum() <= (129**3) /2\n\n'"
tests/conversions/test_voxelgridconversions.py,10,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport torch\nimport sys\n\nimport kaolin as kal\n\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_voxelgrid_to_pointcloud(device):\n\tvoxel = torch.ones([32,32,32]).to(device)\n\tpoints = kal.conversions.voxelgrid_to_pointcloud(voxel, 10)\n\tassert (set(points.shape) == set([10, 3]))\n\tassert points.max() <= .5\n\tassert points.min() >= -.5\n\n\tpoints = kal.conversions.voxelgrid_to_pointcloud(voxel, 100000)\n\tassert (set(points.shape) == set([100000, 3]))\n\tassert points.max() <= .5\n\tassert points.min() >= -.5\n\n\tvoxel = torch.rand([64,64,64]).to(device)\n\tpoints = kal.conversions.voxelgrid_to_pointcloud(voxel, 10000)\n\tassert (set(points.shape) == set([10000, 3]))\n\tassert points.max() <= .5\n\tassert points.min() >= -.5\n\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_voxelgrid_to_trianglemesh(device):\n\n\tvoxel = torch.rand([32,32,32]).to(device)\n\tverts, faces = kal.conversions.voxelgrid_to_trianglemesh(voxel,\n\t\tmode=\'marching_cubes\')\n\tassert verts.shape[0] > 0\n\tassert faces.shape[0] > 0\n\n\tvoxel = torch.rand([64,64,64]).to(device)\n\tverts, faces = kal.conversions.voxelgrid_to_trianglemesh(voxel,\n\t\tmode=\'exact\')\n\tassert verts.shape[0] > 0\n\tassert faces.shape[0] > 0\n\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_voxelgrid_to_quadmesh(device):\n\n\tvoxel = torch.rand([32,32,32]).to(device)\n\tverts, faces = kal.conversions.voxelgrid_to_quadmesh(voxel,\n\t\tthresh=.1)\n\tassert verts.shape[0] > 0\n\tassert faces.shape[0] > 0\n\n\tvoxel = torch.ones([64,64,64]).to(device)\n\tverts, faces = kal.conversions.voxelgrid_to_quadmesh(voxel,\n\t\tthresh= .1)\n\tassert verts.shape[0] > 0\n\tassert faces.shape[0] > 0\n\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_voxelgrid_to_sdf(device):\n\n\tvoxel = torch.rand([10,10,10]).to(device)\n\tsdf = kal.conversions.voxelgrid_to_sdf(voxel, thresh= .5)\n\tpoints = torch.rand((200,3)).to(device) - .5\n\tdistances = sdf(points)\n\tassert set(distances.shape) == set([200])\n\tassert distances.max() <= 1\n\tassert distances.min() >=-1\n\n\tvoxel = torch.ones([10,10,10]).to(device)\n\tsdf = kal.conversions.voxelgrid_to_sdf(voxel, thresh= .5)\n\tpoints = torch.rand((200,3)).to(device) - .5\n\tdistances = sdf(points)\n\tassert set(distances.shape) == set([200])\n\tassert distances.sum() == 0\n'"
tests/datasets/test_ModelNet.py,0,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport pytest\nimport shutil\nimport torch\n\nimport kaolin as kal\nimport kaolin.transforms.transforms as tfs\n\n\nMODELNET_ROOT = \'/data/ModelNet10/\'\nCACHE_DIR = \'tests/datasets/cache\'\n\n\n# Tests below can only be run if a ModelNet dataset is available\nREASON = \'ModelNet not found at default location: {}\'.format(MODELNET_ROOT)\n\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\n@pytest.mark.skipif(not os.path.exists(MODELNET_ROOT), reason=REASON)\ndef test_ModelNet(device):\n    models = kal.datasets.ModelNet(root=MODELNET_ROOT, categories=[\'bathtub\'], split=\'test\')\n\n    assert len(models) == 50\n    for data, attributes in models:\n        assert attributes[\'category\'].item() == 0\n        assert isinstance(data, kal.rep.Mesh)\n'"
tests/datasets/test_ShapeNet.py,1,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport torch\nimport os\nimport shutil\nfrom pathlib import Path\n\nimport kaolin as kal\nfrom torch.utils.data import DataLoader\nfrom kaolin.datasets import shapenet\n\n\nSHAPENET_ROOT = \'/data/ShapeNet/\'\nSHAPENET_RENDERING_ROOT = \'/data/ShapeNetRendering\'\nCACHE_DIR = \'tests/datasets/cache\'\n\n\n# Tests below can only be run if a ShapeNet dataset is available\nSHAPENET_NOT_FOUND = \'ShapeNet not found at default location: {}\'.format(SHAPENET_ROOT)\nSHAPENET_RENDERING_NOT_FOUND = \'ShapeNetRendering not found at default location: {}\'.format(\n    SHAPENET_RENDERING_ROOT)\n\n\n@pytest.mark.skipif(not Path(SHAPENET_ROOT).exists(), reason=SHAPENET_NOT_FOUND)\ndef test_Meshes():\n    meshes1 = shapenet.ShapeNet_Meshes(root=SHAPENET_ROOT,\n                                       categories=[\'can\'], train=True, split=.7)\n    assert len(meshes1) > 0\n    for mesh in meshes1:\n        assert Path(mesh[\'attributes\'][\'path\']).is_file()\n        assert mesh[\'data\'][\'vertices\'].shape[0] > 0\n\n    meshes2 = shapenet.ShapeNet_Meshes(root=SHAPENET_ROOT,\n                                       categories=[\'can\', \'bowl\'], train=True, split=.7)\n    assert len(meshes2) > len(meshes1)\n\n\n@pytest.mark.skipif(not Path(SHAPENET_ROOT).exists(), reason=SHAPENET_NOT_FOUND)\ndef test_Voxels():\n    voxels = shapenet.ShapeNet_Voxels(root=SHAPENET_ROOT, cache_dir=CACHE_DIR,\n                                      categories=[\'can\'], train=True, split=.7, resolutions=[32])\n    assert len(voxels) == 75\n    assert voxels.cache_dir.exists()\n    assert len(list(voxels.cache_dir.rglob(\'*.npz\'))) == 75\n    for obj in voxels:\n        # assert os.path.isfile(obj[\'32_name\'])\n        assert (set(obj[\'data\'][\'32\'].shape) == set([32, 32, 32]))\n\n    voxels = shapenet.ShapeNet_Voxels(root=SHAPENET_ROOT, cache_dir=CACHE_DIR,\n                                      categories=[\'can\'], train=False, split=.7, resolutions=[32])\n    assert len(voxels) == 33\n\n    shutil.rmtree(\'tests/datasets/cache/voxels\')\n\n\n@pytest.mark.parametrize(\'categories\', [[\'chair\'], [\'plane\', \'bench\', \'cabinet\', \'car\', \'chair\',\n                                                    \'monitor\', \'lamp\', \'speaker\', \'rifle\',\n                                                    \'sofa\', \'table\', \'phone\', \'watercraft\']])\n@pytest.mark.skipif(not Path(SHAPENET_RENDERING_ROOT).exists(),\n                    reason=SHAPENET_RENDERING_NOT_FOUND)\ndef test_Images(categories):\n    images = shapenet.ShapeNet_Images(root=SHAPENET_RENDERING_ROOT,\n                                      categories=categories, views=24, train=True, split=.7)\n    if categories == [\'chair\']:\n        assert len(images) == 4744\n    else:\n        assert len(images) == 30644\n    for obj in images:\n        assert list(obj[\'data\'][\'images\'].shape) == [4, 137, 137]\n        assert (Path(obj[\'attributes\'][\'name\']) / \'rendering/00.png\').is_file()\n        assert list(obj[\'data\'][\'params\'][\'cam_mat\'].shape) == [3, 3]\n        assert list(obj[\'data\'][\'params\'][\'cam_pos\'].shape) == [3]\n\n\n@pytest.mark.skipif(not Path(SHAPENET_ROOT).exists(), reason=SHAPENET_NOT_FOUND)\ndef test_Surface_Meshes():\n    surface_meshes = shapenet.ShapeNet_Surface_Meshes(root=SHAPENET_ROOT, cache_dir=CACHE_DIR,\n                                                      categories=[\'can\'], train=True, split=.1,\n                                                      resolution=100, smoothing_iterations=3,\n                                                      mode=\'Tri\')\n    assert len(surface_meshes) == 10\n    assert surface_meshes.cache_dir.exists()\n    assert len(list(surface_meshes.cache_dir.rglob(\'*.p\'))) == 10\n    for smesh in surface_meshes:\n        assert smesh[\'data\'][\'vertices\'].shape[0] > 0\n        assert smesh[\'data\'][\'faces\'].shape[1] == 3\n\n    shutil.rmtree(\'tests/datasets/cache/surface_meshes\')\n\n    surface_meshes = shapenet.ShapeNet_Surface_Meshes(root=SHAPENET_ROOT, cache_dir=CACHE_DIR,\n                                                      categories=[\'can\'], train=True, split=.1,\n                                                      resolution=100, smoothing_iterations=3,\n                                                      mode=\'Quad\')\n    assert len(surface_meshes) == 10\n    assert surface_meshes.cache_dir.exists()\n    assert len(list(surface_meshes.cache_dir.rglob(\'*.p\'))) == 10\n    for smesh in surface_meshes:\n        assert smesh[\'data\'][\'vertices\'].shape[0] > 0\n        assert smesh[\'data\'][\'faces\'].shape[1] == 4\n    shutil.rmtree(\'tests/datasets/cache/voxels\')\n    shutil.rmtree(\'tests/datasets/cache/surface_meshes\')\n\n\n@pytest.mark.skipif(not Path(SHAPENET_ROOT).exists(), reason=SHAPENET_NOT_FOUND)\ndef test_Points():\n    points = shapenet.ShapeNet_Points(root=SHAPENET_ROOT, cache_dir=CACHE_DIR,\n                                      categories=[\'can\'], train=True, split=.1,\n                                      resolution=100, smoothing_iterations=3, num_points=5000,\n                                      surface=False, normals=False)\n\n    assert len(points) == 10\n    assert points.cache_dir.exists()\n    assert len(list(points.cache_dir.rglob(\'*.p\'))) == 10\n    for obj in points:\n        assert set(obj[\'data\'][\'points\'].shape) == set([5000, 3])\n        assert set(obj[\'data\'][\'normals\'].shape) == set([5000, 3])\n\n    shutil.rmtree(\'tests/datasets/cache/points\')\n\n    points = shapenet.ShapeNet_Points(root=SHAPENET_ROOT, cache_dir=CACHE_DIR,\n                                      categories=[\'can\'], train=True, split=.1,\n                                      resolution=100, smoothing_iterations=3, num_points=5000,\n                                      surface=True, normals=True)\n\n    assert len(points) == 10\n    assert points.cache_dir.exists()\n    assert len(list(points.cache_dir.rglob(\'*.p\'))) == 10\n    for obj in points:\n        assert set(obj[\'data\'][\'points\'].shape) == set([5000, 3])\n        assert set(obj[\'data\'][\'normals\'].shape) == set([5000, 3])\n\n    shutil.rmtree(\'tests/datasets/cache/points\')\n    shutil.rmtree(\'tests/datasets/cache/voxels\')\n    shutil.rmtree(\'tests/datasets/cache/surface_meshes\')\n\n\n@pytest.mark.skipif(not Path(SHAPENET_ROOT).exists(), reason=SHAPENET_NOT_FOUND)\ndef test_SDF_Points():\n    sdf_points = shapenet.ShapeNet_SDF_Points(root=SHAPENET_ROOT, cache_dir=CACHE_DIR,\n                                              categories=[\'can\'], train=True, split=.1,\n                                              resolution=100, smoothing_iterations=3,\n                                              num_points=5000, occ=False, sample_box=True)\n\n    assert len(sdf_points) == 10\n    assert sdf_points.cache_dir.exists()\n    assert len(list(sdf_points.cache_dir.rglob(\'*.p\'))) == 10\n    for obj in sdf_points:\n        assert set(obj[\'data\'][\'sdf_points\'].shape) == set([5000, 3])\n        assert set(obj[\'data\'][\'sdf_distances\'].shape) == set([5000])\n\n    shutil.rmtree(\'tests/datasets/cache/sdf_points\')\n\n    sdf_points = shapenet.ShapeNet_SDF_Points(root=SHAPENET_ROOT, cache_dir=CACHE_DIR,\n                                              categories=[\'can\'], train=True, split=.1,\n                                              resolution=100, smoothing_iterations=3,\n                                              num_points=5000, occ=True, sample_box=True)\n\n    assert len(sdf_points) == 10\n    assert sdf_points.cache_dir.exists()\n    assert len(list(sdf_points.cache_dir.rglob(\'*.p\'))) == 10\n    for obj in sdf_points:\n        assert set(obj[\'data\'][\'occ_points\'].shape) == set([5000, 3])\n        assert set(obj[\'data\'][\'occ_values\'].shape) == set([5000])\n\n    shutil.rmtree(\'tests/datasets/cache/sdf_points\')\n    shutil.rmtree(\'tests/datasets/cache/voxels\')\n    shutil.rmtree(\'tests/datasets/cache/surface_meshes\')\n\n\n@pytest.mark.skipif(not Path(SHAPENET_ROOT).exists(), reason=SHAPENET_NOT_FOUND)\ndef test_Combination():\n    dataset_params = {\n        \'root\': SHAPENET_ROOT,\n        \'categories\': [\'can\'],\n        \'train\': True,\n        \'split\': .8,\n    }\n    # images = shapenet.ShapeNet_Images(root=SHAPENET_ROOT, cache_dir=CACHE_DIR,\n    #                                   categories=[\'bowl\'], views=1, train=True, split=.8)\n    meshes = shapenet.ShapeNet_Meshes(**dataset_params)\n    voxels = shapenet.ShapeNet_Voxels(**dataset_params, cache_dir=CACHE_DIR, resolutions=[32])\n    sdf_points = shapenet.ShapeNet_SDF_Points(**dataset_params, cache_dir=CACHE_DIR,\n                                              smoothing_iterations=3, num_points=500, occ=False,\n                                              sample_box=True)\n\n    points = shapenet.ShapeNet_Points(**dataset_params, cache_dir=CACHE_DIR, resolution=100,\n                                      smoothing_iterations=3, num_points=500,\n                                      surface=False, normals=True)\n\n    dataset = shapenet.ShapeNet_Combination([voxels, sdf_points, points])\n\n    for obj in dataset:\n        obj_data = obj[\'data\']\n        assert set(obj[\'data\'][\'sdf_points\'].shape) == set([500, 3])\n        assert set(obj[\'data\'][\'sdf_distances\'].shape) == set([500])\n        assert set(obj[\'data\'][\'32\'].shape) == set([32, 32, 32])\n        assert set(obj[\'data\'][\'points\'].shape) == set([500, 3])\n        assert set(obj[\'data\'][\'normals\'].shape) == set([500, 3])\n\n    train_loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=8)\n    for batch in train_loader:\n        assert set(batch[\'data\'][\'sdf_points\'].shape) == set([2, 500, 3])\n        assert set(batch[\'data\'][\'sdf_distances\'].shape) == set([2, 500])\n        assert set(batch[\'data\'][\'32\'].shape) == set([2, 32, 32, 32])\n        assert set(batch[\'data\'][\'points\'].shape) == set([2, 500, 3])\n        assert set(batch[\'data\'][\'normals\'].shape) == set([2, 500, 3])\n\n    shutil.rmtree(\'tests/datasets/cache/sdf_points\')\n    shutil.rmtree(\'tests/datasets/cache/points\')\n    shutil.rmtree(\'tests/datasets/cache/voxels\')\n    shutil.rmtree(\'tests/datasets/cache/surface_meshes\')\n'"
tests/datasets/test_nusc.py,0,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport pytest\nimport shutil\nimport torch\nfrom nuscenes.nuscenes import NuScenes\n\nimport kaolin as kal\nimport kaolin.transforms.transforms as tfs\nfrom kaolin.rep import PointCloud\n\n\nNUSCENES_ROOT = \'/data/nuscenes_mini/\'\n\n\n# Tests below can only be run if a ModelNet dataset is available\nREASON = \'Nuscenes mini not found at default location: {}\'.format(NUSCENES_ROOT)\n\n\n@pytest.mark.skipif(not os.path.exists(NUSCENES_ROOT), reason=REASON)\ndef test_nusc():\n    nusc = NuScenes(version=\'v1.0-mini\',\n                    dataroot=NUSCENES_ROOT,\n                    verbose=False)\n\n    traindata = kal.datasets.NuscDetection(nusc, train=True, nsweeps=5)\n    traindata_large = kal.datasets.NuscDetection(nusc, train=True, nsweeps=10)\n    valdata = kal.datasets.NuscDetection(nusc, train=False, nsweeps=5)\n\n    assert len(traindata) == 323\n    assert len(valdata) == 81\n\n    inst = traindata[10]\n    assert isinstance(inst.data[\'pc\'], PointCloud)\n\n    # check dimension of point cloud for 5 lidar sweeps\n    N, D = inst.data[\'pc\'].points.shape\n    assert N == 129427\n    assert D == 5\n\n    # check dimension of point cloud for 10 lidar sweeps\n    N, D = traindata_large[10].data[\'pc\'].points.shape\n    assert N == 258188\n    assert D == 5\n'"
tests/datasets/test_shrec.py,0,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nimport pytest\n\nimport kaolin as kal\n\nSHREC16_ROOT = ""/data/SHREC16/""\nCACHE_DIR = ""tests/datasets/cache""\n\n# Tests below can only be run is a ShapeNet dataset is available\nREASON = ""SHREC16 not found at default location: {}"".format(SHREC16_ROOT)\n\n\n@pytest.mark.parametrize(""device"", [""cpu"", ""cuda""])\n@pytest.mark.skipif(not os.path.exists(SHREC16_ROOT), reason=REASON)\ndef test_SHREC16(device):\n    models = kal.datasets.SHREC16(\n        root=SHREC16_ROOT, categories=[""ants""], train=False\n    )\n    assert len(models) == 4\n'"
tests/datasets/test_usdfile.py,0,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\nimport shutil\nfrom pathlib import Path\n\nfrom kaolin.datasets.usdfile import USDMeshes\n\n\ndef test_usd_meshes():\n    fpath = \'./tests/model.usd\'\n    cache_dir = \'./tests/datasets_eval/USDMeshes/\'\n    usd_dataset = USDMeshes(usd_filepath=fpath, cache_dir=cache_dir)\n    assert len(usd_dataset) == 1\n\n    # test caching\n    assert len(list(Path(cache_dir).glob(\'**/*.p\'))) == 1\n    shutil.rmtree(\'tests/datasets_eval/USDMeshes\')\n\n# Tests below must be run with KitchenSet dataset\n\n# def test_usd_meshes():\n#     fpath = \'data/Kitchen_set/Kitchen_set.usd\'\n#     cache_dir = \'./tests/datasets_eval/USDMeshes/\'\n#     usd_dataset = USDMeshes(usd_filepath=fpath, cache_dir=cache_dir)\n#     assert len(usd_dataset) == 740\n\n#     # test caching\n#     assert len(list(Path(cache_dir).glob(\'**/*.npz\'))) == 740\n'"
tests/graphics/test_softras.py,3,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\nimport torch\nimport kaolin\n\n\n@pytest.mark.skipif(""not torch.cuda.is_available()"", reason=""softras needs GPU"")\ndef test_softras(device=\'cuda:0\'):\n\n    renderer = kaolin.graphics.SoftRenderer(camera_mode=""look_at"", device=device)\n    filename_input = ""tests/graphics/banana.obj""\n\n    mesh = kaolin.rep.TriangleMesh.from_obj(filename_input)\n\n    vertices = mesh.vertices\n    faces = mesh.faces\n    vertices = vertices[None, :, :].cuda()  \n    faces = faces[None, :, :].cuda()\n    vertices_max = vertices.max()\n    vertices_min = vertices.min()\n    vertices_middle = (vertices_max + vertices_min) / 2.\n    vertices = vertices - vertices_middle\n    coef = 5\n    vertices = vertices * coef\n    textures = torch.ones(1, faces.shape[1], 2, 3, dtype=torch.float32).cuda()\n    renderer.set_eye_from_angles(2., 30., 0.)\n\n    rgba = renderer.forward(vertices, faces, textures)\n    assert rgba.shape == (1, 4, 256, 256)\n\n    # Compare correctness with a reference softras output\n    target = torch.load(""tests/graphics/softras_reference_render.pt"").to(device)\n    avgmse = (rgba - target).abs().mean()\n    assert avgmse <= 0.0002\n'"
tests/mathutils/test_common.py,5,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n# Kornia components:\n# Copyright (C) 2017-2019, Arraiy, Inc., all rights reserved.\n# Copyright (C) 2019-    , Open Source Vision Foundation, all rights reserved.\n# Copyright (C) 2019-    , Kornia authors, all rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\nimport kaolin as kal\nfrom kaolin.mathutils import pi\nfrom kaolin.testing import tensor_to_gradcheck_var\n\n# from tests.common import device_type as device\n\n\ndef test_pi():\n\tassert_allclose(pi, 3.14159265)\n\n\n@pytest.mark.parametrize(\'shape\', [(1,2,3), (2,1,4), (1,1,1)])\ndef test_rad2deg(shape, device=\'cpu\'):\n\tx_rad = pi * torch.rand(shape).to(device)\n\tx_deg = kal.mathutils.rad2deg(x_rad)\n\tx_deg_to_rad = kal.mathutils.deg2rad(x_deg)\n\tassert_allclose(x_rad, x_deg_to_rad, atol=1e-8, rtol=1e-5)\n\n\tassert gradcheck(kal.mathutils.rad2deg, (tensor_to_gradcheck_var(x_rad)), \n\t\traise_exception=True)\n\n\n@pytest.mark.parametrize(\'shape\', [(2,3,1), (1,3,2), (4,2,18)])\ndef test_deg2rad(shape, device=\'cpu\'):\n\tx_deg = 180. * torch.rand(shape)\n\tx_deg = x_deg.to(torch.device(device))\n\tx_rad = kal.mathutils.deg2rad(x_deg)\n\tx_rad_to_deg = kal.mathutils.rad2deg(x_rad)\n\tassert_allclose(x_deg, x_rad_to_deg, atol=1e-8, rtol=1e-5)\n\n\tassert gradcheck(kal.mathutils.deg2rad, (tensor_to_gradcheck_var(x_deg)), \n\t\traise_exception=True)\n'"
tests/mathutils/test_transformations.py,14,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n# Kornia components:\n# Copyright (C) 2017-2019, Arraiy, Inc., all rights reserved.\n# Copyright (C) 2019-    , Open Source Vision Foundation, all rights reserved.\n# Copyright (C) 2019-    , Kornia authors, all rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch.testing import assert_allclose\n\nimport kaolin as kal\nfrom kaolin.mathutils import pi\nfrom kaolin.testing import tensor_to_gradcheck_var\n\n\nclass TestRotx:\n\n    def test_smoke(self):\n        theta = torch.Tensor([0.])\n        assert kal.mathutils.rotx(theta).shape[0] == 1\n        assert_allclose(kal.mathutils.rotx(theta), kal.mathutils.rotx(-theta))\n\n    def test_casual_rotation(self):\n        theta = pi * torch.rand(10)\n        rx = kal.mathutils.rotx(theta)\n        rx_transpose = rx.transpose(1, 2)\n        rx_inv = kal.mathutils.rotx(-theta)\n        assert_allclose(rx_transpose, rx_inv)\n\n    def test_small_rotation(self):\n        theta = 1e-4 * torch.rand(10)\n        rx = kal.mathutils.rotx(theta)\n        rx_transpose = rx.transpose(1, 2)\n        rx_inv = kal.mathutils.rotx(-theta)\n        assert_allclose(rx_transpose, rx_inv)\n\n    # def test_gradcheck(self):\n    #     theta = tensor_to_gradcheck_var(pi * torch.rand(10))\n    #     assert gradcheck(kal.mathutils.rotx, theta, raise_exception=True)\n\n\nclass TestRoty:\n\n    def test_smoke(self):\n        theta = torch.Tensor([0.])\n        assert kal.mathutils.roty(theta).shape[0] == 1\n        assert_allclose(kal.mathutils.roty(theta), kal.mathutils.roty(-theta))\n\n    def test_casual_rotation(self):\n        theta = pi * torch.rand(10)\n        ry = kal.mathutils.roty(theta)\n        ry_transpose = ry.transpose(1, 2)\n        ry_inv = kal.mathutils.roty(-theta)\n        assert_allclose(ry_transpose, ry_inv)\n\n    def test_small_rotation(self):\n        theta = 1e-4 * torch.rand(10)\n        ry = kal.mathutils.roty(theta)\n        ry_transpose = ry.transpose(1, 2)\n        ry_inv = kal.mathutils.roty(-theta)\n        assert_allclose(ry_transpose, ry_inv)\n\n\nclass TestRotz:\n\n    def test_smoke(self):\n        theta = torch.Tensor([0.])\n        assert kal.mathutils.rotz(theta).shape[0] == 1\n        assert_allclose(kal.mathutils.rotz(theta), kal.mathutils.rotz(-theta))\n\n    def test_casual_rotation(self):\n        theta = pi * torch.rand(10)\n        rz = kal.mathutils.rotz(theta)\n        rz_transpose = rz.transpose(1, 2)\n        rz_inv = kal.mathutils.rotz(-theta)\n        assert_allclose(rz_transpose, rz_inv)\n\n    def test_small_rotation(self):\n        theta = 1e-4 * torch.rand(10)\n        rz = kal.mathutils.rotz(theta)\n        rz_transpose = rz.transpose(1, 2)\n        rz_inv = kal.mathutils.rotz(-theta)\n        assert_allclose(rz_transpose, rz_inv)\n\n\ndef test_homogenize_unhomogenize():\n    pts = torch.randn(20, 3)\n    pts_homo = kal.mathutils.homogenize_points(pts)\n    pts_homo_unhomo = kal.mathutils.unhomogenize_points(pts_homo)\n    assert_allclose(pts, pts_homo_unhomo)\n\n\ndef test_homogenize_unhomogenize_batch():\n    pts = torch.randn(10, 20, 3)\n    pts_homo = kal.mathutils.homogenize_points(pts)\n    pts_homo_unhomo = kal.mathutils.unhomogenize_points(pts_homo)\n    assert_allclose(pts, pts_homo_unhomo)\n'"
tests/metrics/test_mesh_metrics.py,3,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport torch\nimport sys\n\nimport kaolin as kal\nfrom kaolin.rep import TriangleMesh\n\n\ndef test_chamfer_distance(device = \'cpu\'):\n\tmesh1 = TriangleMesh.from_obj(\'tests/model.obj\') \n\tmesh2 = TriangleMesh.from_obj(\'tests/model.obj\') \n\tif device == \'cuda\': \n\t\tmesh1.cuda()\n\t\tmesh2.cuda()\n\n\tmesh2.vertices = mesh2.vertices * 1.5\n\tdistance = kal.metrics.mesh.chamfer_distance(mesh1, mesh2, num_points = 100)\n\tdistance = kal.metrics.mesh.chamfer_distance(mesh1, mesh2, num_points = 200)\n\tassert kal.metrics.mesh.chamfer_distance(mesh1, mesh1, num_points = 500) <= 0.1\n\ndef test_edge_length(device = \'cpu\'):\n\tmesh = TriangleMesh.from_obj(\'tests/model.obj\') \n\tif device == \'cuda\': \n\t\tmesh.cuda()\n\tlength1 = kal.metrics.mesh.edge_length(mesh)\n\tmesh.vertices = mesh.vertices * 2 \n\tlength2 = kal.metrics.mesh.edge_length(mesh)\n\tassert (length1 < length2)\n\tmesh.vertices = mesh.vertices * 0\n\tassert kal.metrics.mesh.edge_length(mesh) == 0\n\t\ndef test_laplacian_loss(device = \'cpu\'):\n\tmesh1 = TriangleMesh.from_obj(\'tests/model.obj\') \n\tmesh2 = TriangleMesh.from_obj(\'tests/model.obj\') \n\tif device == \'cuda\': \n\t\tmesh1.cuda()\n\t\tmesh2.cuda() \n\tmesh2.vertices = mesh2.vertices *1.5\n\tassert kal.metrics.mesh.laplacian_loss(mesh1, mesh2) > 0 \n\tassert kal.metrics.mesh.laplacian_loss(mesh1, mesh1) == 0 \n\t\n\n\ndef test_point_to_surface(device = \'cpu\'):\n\ttorch.manual_seed(1)\n\ttorch.cuda.manual_seed(1)\n\tmesh = TriangleMesh.from_obj(\'tests/model.obj\') \n\tpoints = torch.rand(500,3) -.5\n\tif device == \'cuda\': \n\t\tmesh.cuda()\n\t\tpoints = points.cuda()\n\t\n\tdistance = kal.metrics.mesh.point_to_surface(points, mesh)\n\t\n\tassert (distance > 1).sum() == 0 \n\tassert ( distance <= 0 ).sum() == 0 \n\tassert (distance.sum() <= .2) \n\ndef test_chamfer_distance_cpu(): \n\ttest_chamfer_distance(""cuda"")\ndef test_edge_length_cpu(): \n\ttest_edge_length(""cuda"")\ndef test_laplacian_loss_cpu(): \n\ttest_laplacian_loss(""cuda"")\ndef test_point_to_surface_cpu(): \n\ttest_point_to_surface(""cuda"")\n'"
tests/metrics/test_point_metrics.py,6,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport torch\nimport sys\n\nimport kaolin as kal\nfrom kaolin.metrics.point import chamfer_distance\n\ndef test_chamfer_distance(device = \'cpu\'): \t\n\n\tA = torch.rand(300,3).to(device)\n\tB = torch.rand(200,3).to(device)\n\tdistance = kal.metrics.point.chamfer_distance(A,B)\t\n\tassert distance >= 0 \n\tassert distance <= 2.\n\n\t\n\tB = A.clone()\n\tdistance = kal.metrics.point.chamfer_distance(A,B)\t \n\tassert distance == 0\n\n\ndef test_directed_distance(device = \'cpu\'): \t\t\n\tA = torch.rand(300,3).to(device)\n\tB = torch.rand(200,3).to(device)\n\tdistance = kal.metrics.point.directed_distance(A,B, mean=True)\t\n\n\tassert distance >= 0 \n\tassert distance <= .5\n\n\tdistances = kal.metrics.point.directed_distance(A,B, mean=False)\t\n\tassert ((distances <0).sum() == 1) == 0 \n\tassert ((distances >1).sum() == 1) == 0\n\tassert (set(distances.shape)  == set([300]))\n\n\n\tB = A.clone()\n\tdistance = kal.metrics.point.directed_distance(A,B, mean=True)\t \n\tassert distance == 0\n\n\tdistances = kal.metrics.point.directed_distance(A,B, mean=False)\t \n\tassert distances.sum() == 0\n\t\n\n\ndef test_iou(device = \'cpu\'): \n\tpoints = torch.rand(300,3).to(device)\n\tpoints1 = points *2. //1 \n\tpoints2 = points *1.5 //1\n\n\tiou1 = kal.metrics.point.iou(points1,points2)\t\n\tassert iou1 >= 0 \n\tassert iou1 <= 1 \n\n\tpoints3 = points*1.1 //1\n\n\tiou2 = kal.metrics.point.iou(points1,points3)\t\n\tassert iou2 >= 0 \n\tassert iou2 <= 1 \n\tassert iou1 >= iou2\n\n\ndef test_f_score(device = \'cpu\'): \n\n\tpoints1 = torch.rand(1,3).to(device) \n\tpoints2 = points1.clone()\n\n\tf = kal.metrics.point.f_score(points1,points2, radius = 0.01)\t\n\tassert (f == 1) \n\n\tpoints2 = points1 * 1.01\n\n\tf1 = kal.metrics.point.f_score(points1,points2, radius = 0.01)\t\n\t\n\tpoints2 = points2 * 1.011\n\n\tf2 = kal.metrics.point.f_score(points1,points2, radius = 0.01)\n\tassert (f1>= f2)\n\n\tf3 = kal.metrics.point.f_score(points1,points2, radius = 0.015)\n\n\tassert (f3>= f2)\n\ndef test_chamfer_distance_gpu(): \n\ttest_chamfer_distance(""cuda"")\ndef test_directed_distance_gpu(): \n\ttest_directed_distance(""cuda"")\ndef test_iou_gpu(): \n\ttest_iou(""cuda"")\ndef test_f_score_gpu(): \n\ttest_f_score(""cuda"")'"
tests/metrics/test_voxel_metrics.py,7,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport torch\nimport sys\n\nimport kaolin as kal\nfrom kaolin.metrics.voxel import iou\n\ndef test_iou(device = \'cpu\'): \t\n\t\n\t\n\tA = torch.rand(2,32,32,32).to(device)\n\tB = torch.ones(2,32,32,32).to(device)\n\tdistance = iou(A,B)\t \n\tassert (distance >=0 and distance <=1.)\n\n\tA = torch.ones(2,32,32,32).to(device)\n\tdistance = iou(A,B)\t \n\tassert (distance==1)\n\n\n\tA = torch.zeros(2,32,32,32).to(device)\n\tB = torch.ones(2,32,32,32).to(device)\n\tdistance = iou(A,B)\t \n\tassert (distance ==0)\n\n\tA = torch.zeros(2,32,32,32).to(device)\n\tB = torch.zeros(2,32,32,32).to(device)\n\tdistance = iou(A,B)\t \n\tassert (distance != distance) # should be NaN\n\ndef test_iou_gpu(): \n\ttest_iou(""cuda"")'"
tests/models/test_PointNet.py,4,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\nimport torch\nimport kaolin as kal\nfrom kaolin import helpers\n\n\ndef test_smoke(device=\'cpu\'):\n    model = kal.models.PointNet.PointNetClassifier(\n        in_channels=3,\n        feat_size=1024,\n        num_classes=13,\n        batchnorm=True,\n    )\n    x = torch.randn(2, 128, 3)\n    out = model(x)\n    helpers._assert_shape_eq(out, (2, 13))\n\n    model = kal.models.PointNet.PointNetClassifier(\n        in_channels=3,\n        feat_size=1024,\n        num_classes=13,\n        batchnorm=True,\n        transposed_input=True,\n    )\n    x = torch.randn(2, 3, 128)\n    out = model(x)\n    helpers._assert_shape_eq(out, (2, 13))\n\n    model = kal.models.PointNet.PointNetSegmenter(\n        in_channels=3,\n        feat_size=1024,\n        num_classes=13,\n        batchnorm=True,\n    )\n    x = torch.randn(2, 128, 3)\n    out = model(x)\n    helpers._assert_shape_eq(out, (2, 128, 13))\n\n    model = kal.models.PointNet.PointNetSegmenter(\n        in_channels=3,\n        feat_size=1024,\n        num_classes=13,\n        batchnorm=True,\n        transposed_input=True,\n    )\n    x = torch.randn(2, 3, 128)\n    out = model(x)\n    helpers._assert_shape_eq(out, (2, 128, 13))\n'"
tests/models/test_PointNet2.py,2,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\nimport torch\nimport kaolin as kal\nfrom kaolin import helpers\n\n\ndef test_smoke():\n    model = kal.models.PointNet2.PointNet2Classifier(\n        in_features=4,\n        num_classes=13,\n        batchnorm=True,\n    ).cuda()\n    x = torch.randn(2, 128, 7).cuda()\n    out = model(x)\n    helpers._assert_shape_eq(out, (2, 13))\n\n    model = kal.models.PointNet2.PointNet2Segmenter(\n        in_features=4,\n        num_classes=13,\n        batchnorm=True,\n    ).cuda()\n    x = torch.randn(2, 128, 7).cuda()\n    out = model(x)\n    helpers._assert_shape_eq(out, (2, 128, 13))\n'"
tests/models/test_Voxel3DIWGAN.py,1,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport torch\n\nimport kaolin as kal\nfrom kaolin import helpers\n\n\ndef test_smoke(device=\'cpu\'):\n    g = kal.models.Voxel3DIWGAN.Voxel3DIWGenerator()\n    d = kal.models.Voxel3DIWGAN.Voxel3DIWDiscriminator()\n    g = g.to(device)\n    d = d.to(device)\n    x = torch.randn(4, 200)\n    x_ = g(x)\n    x_ = d(x_)\n    helpers._assert_shape_eq(x, x_.shape, dim=0)\n'"
tests/models/test_VoxelGAN.py,1,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport torch\n\nimport kaolin as kal\nfrom kaolin import helpers\n\n\ndef test_smoke(device=\'cpu\'):\n    g = kal.models.VoxelGAN.Generator()\n    d = kal.models.VoxelGAN.Discriminator()\n    g = g.to(device)\n    d = d.to(device)\n    x = torch.randn(4, 200)\n    x_ = g(x)\n    x_ = d(x_)\n    helpers._assert_shape_eq(x, x_.shape, dim=0)\n'"
tests/models/test_VoxelSuperresODM.py,1,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport torch\n\nimport kaolin as kal\nfrom kaolin import helpers\n\n\ndef test_smoke(device=\'cpu\'):\n    net = kal.models.VoxelSuperresODM.SuperresNetwork(30, 15)\n    x = torch.randn(1, 6, 128, 128)\n    x_ = net(x)\n    helpers._assert_shape_eq(x, x_.shape, dim=0)\n'"
tests/models/test_VoxelSuperresSimple.py,1,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport torch\n\nimport kaolin as kal\nfrom kaolin import helpers\n\n\ndef test_smoke(device=\'cpu\'):\n    net = kal.models.VoxelSuperresSimple.EncoderDecoder()\n    x = torch.randn(1, 1, 32, 32, 32)\n    x_ = net(x)\n    helpers._assert_shape_eq(x, x_.shape, dim=0)\n'"
tests/rep/test_SDF_representation.py,6,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport torch\nimport sys\n\nimport kaolin as kal\nfrom kaolin.rep import TriangleMesh\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_check_sign(device): \n    mesh = TriangleMesh.from_obj((\'tests/model.obj\') )\n    if device == \'cuda\': \n        mesh.cuda()\n    points = torch.rand((1000,3), device=device) -.5\n    signs = kal.rep.SDF.check_sign(mesh, points)\n    assert (signs == True).sum() > 0 \n    assert (signs == False).sum() > 0 \n\n    points = (torch.rand((1000,3), device=device) -.5) * .001\n    signs = kal.rep.SDF.check_sign(mesh, points)\n    assert (signs == False).sum() == 0 \n\n\n    points = torch.rand((1000,3), device=device) +10\n    signs = kal.rep.SDF.check_sign(mesh, points)\n    assert (signs == True).sum() == 0 \n\n# def test_check_sign_gpu(): \n#     test_check_sign(""cuda"")\n\n\n# def test_check_sign_fast(device=\'cuda\'):\n#     mesh = TriangleMesh.from_obj(\'tests/model.obj\')\n#     mesh.to(device)\n#     points = torch.rand(1000, 3).to(device) - .5\n#     signs = kal.rep.SDF.check_sign_fast(mesh, points)\n#     assert (signs == True).float().sum() > 0\n#     assert (signs == False).sum() > 0\n\n\n# if __name__ == \'__main__\':\n\n# \tmesh = TriangleMesh.from_obj(\'tests/model.obj\')\n# \tmesh.cuda()\n# \tpoints = torch.rand(1000, 3).cuda() - .5\n# \tsign_fast = kal.rep.SDF.check_sign_fast(mesh, points)\n\n# \tmesh = TriangleMesh.from_obj(\'tests/model.obj\')\n# \tmesh.cuda()\n# \tsign = kal.rep.SDF.check_sign(mesh, points)\n# \timport numpy as np\n# \tsign = torch.from_numpy(np.asarray(sign)).cuda()\n    \n# \tprint((sign == sign_fast).float().sum())\n'"
tests/rep/test_mesh_representation.py,3,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport torch\nimport sys\nimport os\n\nimport kaolin as kal\nfrom kaolin.rep import TriangleMesh\nfrom kaolin.rep import QuadMesh\n\n\ndef test_load_obj(device = \'cpu\'): \n\tmesh = TriangleMesh.from_obj((\'tests/model.obj\') )\n\tif device == \'cuda\': \n\t\tmesh.cuda()\n\tassert mesh.vertices.shape[0] > 0 \n\tassert mesh.vertices.shape[1] == 3 \n\tassert mesh.faces.shape[0] > 0 \n\tassert mesh.faces.shape[1] == 3 \n\n\tmesh = TriangleMesh.from_obj(\'tests/model.obj\', with_vt=True, texture_res = 4) \n\tif device == \'cuda\': \n\t\tmesh.cuda()\n\tassert mesh.textures.shape[0] > 0 \n\n\tmesh = TriangleMesh.from_obj(\'tests/model.obj\', with_vt=True, texture_res = 4, enable_adjacency = True)\n\tassert mesh.vv.shape[0] > 0 \n\tassert mesh.edges.shape[0] > 0\n\tassert mesh.vv_count.shape[0] > 0\n\tassert mesh.ve.shape[0] > 0\n\tassert mesh.ve_count.shape[0] > 0\n\tassert mesh.ff.shape[0] > 0\n\tassert mesh.ff_count.shape[0] > 0\n\tassert mesh.ef.shape[0] > 0\n\tassert mesh.ef_count.shape[0] > 0\n\tassert mesh.ee.shape[0] > 0\n\tassert mesh.ee_count.shape[0] > 0\n\tif device == \'cuda\': \n\t\tmesh.cuda()\n\ndef test_from_tensors(device=\'cpu\'): \n\tmesh = TriangleMesh.from_obj(\'tests/model.obj\', with_vt=True, texture_res = 4)\n\tif device == \'cuda\': \n\t\tmesh.cuda()\n\n\tverts = mesh.vertices.clone()\n\tfaces = mesh.faces.clone() \n\tuvs = mesh.uvs.clone()\n\tface_textures = mesh.face_textures.clone()\n\ttextures = mesh.textures.clone()\n\t\n\tmesh = TriangleMesh.from_tensors(verts, faces, uvs = uvs, face_textures = face_textures,\n\t\t\t textures=textures)\n\n\t\ndef test_sample_mesh( device = \'cpu\'):\n\tmesh = TriangleMesh.from_obj(\'tests/model.obj\')\n\tif device == \'cuda\': \n\t\tmesh.cuda() \n\t\n\tpoints, choices = mesh.sample(100)\n\tassert (set(points.shape) == set([100, 3])) \n\tpoints, choices = mesh.sample(10000)\n\tassert (set(points.shape) == set([10000, 3])) \n\ndef test_laplacian_smoothing(device = \'cpu\'): \n\tmesh = TriangleMesh.from_obj((\'tests/model.obj\'))\n\tif device == \'cuda\': \n\t\tmesh.cuda()\n\tv1 = mesh.vertices.clone()\n\tmesh.laplacian_smoothing(iterations = 3)\n\tv2 = mesh.vertices.clone()\n\tassert (torch.abs(v1 - v2)).sum() >0 \n\ndef test_compute_laplacian(device = \'cpu\'): \n\tmesh = TriangleMesh.from_obj((\'tests/model.obj\'))\n\tif device == \'cuda\': \n\t\tmesh.cuda()\n\tlap = mesh.compute_laplacian()\n\tassert ((lap**2).sum(dim=1) > .1).sum() == 0  # asserting laplacian of sphere is small\n\ndef test_load_and_save_Tensors(device = \'cpu\'): \n\tmesh1 = TriangleMesh.from_obj((\'tests/model.obj\'))\n\tif device == \'cuda\': \n\t\tmesh1.cuda()\n\tmesh1.save_tensors(\'copy.npz\')\n\tassert os.path.isfile(\'copy.npz\')\n\tmesh2 = TriangleMesh.load_tensors (\'copy.npz\')\n\tif device == \'cuda\': \n\t\tmesh2.cuda()\n\tassert (torch.abs(mesh1.vertices - mesh2.vertices)).sum() ==0\n\tassert (torch.abs(mesh1.faces - mesh2.faces)).sum() ==0\n\tos.remove(""copy.npz"")\n\ndef test_adj_computations(device = \'cpu\'): \n\tmesh = TriangleMesh.from_obj((\'tests/model.obj\'))\n\tif device == \'cuda\': \n\t\tmesh.cuda()\n\n\tadj_full = mesh.compute_adjacency_matrix_full()\n\tadj_sparse = mesh.compute_adjacency_matrix_sparse().coalesce()\n\n\tassert adj_full.shape[0] == mesh.vertices.shape[0]\n\tassert ((adj_full-adj_sparse.to_dense()) != 0).sum() == 0 \n\n\n\ndef test_load_obj_gpu(): \n\ttest_load_obj(""cuda"")\ndef test_from_tensors_gpu(): \n\ttest_from_tensors(""cuda"")\ndef test_sample_mesh_gpu(): \n\ttest_sample_mesh(""cuda"")\ndef test_laplacian_smoothing_gpu(): \n\ttest_laplacian_smoothing(""cuda"")\ndef test_compute_laplacian_gpu(): \n\ttest_compute_laplacian(""cuda"")\ndef test_load_and_save_Tensors_gpu(): \n\ttest_load_and_save_Tensors(""cuda"")\ndef test_adj_computations_gpu(): \n\ttest_adj_computations(""cuda"")'"
tests/rep/test_point_representation.py,14,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport torch\nimport sys\n\nimport kaolin as kal\n\n\ndef test_pointcloud_creation(device=\'cpu\'):\n\tpts = torch.ones(100, 3)\n\tnormals = torch.ones(100, 3)\n\tpcd = kal.rep.PointCloud(points=pts)\n\tpcd = kal.rep.PointCloud(points=pts, normals=normals)\n\n\n# def test_points(device = \'cpu\'): \n# \tpoints1 = torch.ones((100,3)).to(device)\n# \tpoints2 = kal.rep.point.scale(points1, torch.FloatTensor([.5]).to(device))\n# \tassert (points1 - 2*points2).sum() == 0 \n# \tpoints1 = torch.rand((100,3)).to(device)\n# \tpoints2 = kal.rep.point.scale(points1, torch.FloatTensor([2]).to(device))\n# \tassert (2*points1 - points2).sum() == 0 \n\n# def test_rotate(device = \'cpu\'): \n# \tpoints1 = torch.rand((100,3)).to(device)\n# \trot_mat = [[1,.3,.4], [-.1,.2,.5], [-.2,-.3,-.4]]\n# \tpoints2 = kal.rep.point.rotate(points1, torch.FloatTensor(rot_mat).to(device))\n# \tassert (points1 - 2*points2).sum() > 0 \n\n# def test_re_align(device = \'cpu\'): \n\n# \tpoints1 = torch.rand((100,3)).to(device)\n# \tpoints2 = points1 *3\n# \tpoints2 = kal.rep.point.re_align(points1, points2)\n# \tassert (points1 - points2).sum()  < .001 \n\n# \tpoints1 = torch.rand((1000,3)).to(device)\n# \tpoints2 = 5*torch.rand((1000,3)).to(device) - .3\n# \tpoints2 = kal.rep.point.re_align(points1, points2)\n\n# \tassert torch.abs(torch.max(points1) - torch.max(points2)) < .00001\n# \tassert torch.abs(torch.min(points1) - torch.min(points2)) < .00001\n\n# def test_bounding_points(device = \'cpu\'): \n\n# \tpoints = torch.rand((10000,3)).to(device)\n# \tbbox = [.75, .25, .75, .25, .75, .25]\n# \tnew_points_idx = kal.rep.point.bounding_points(points, bbox, padding = 0)\n# \tnew_points = points[new_points_idx]\n# \tassert (new_points > .75 ).sum() == 0 \n# \tassert (new_points < .25 ).sum() == 0 \n\n# def test_points_cpu(): \n# \ttest_points(""cuda"")\n# def test_rotate_cpu(): \n# \ttest_rotate(""cuda"")\n# def test_re_align_cpu(): \n# \ttest_re_align(""cuda"")\n# def test_bounding_points_cpu(): \n\t# test_bounding_points(""cuda"")\n'"
tests/rep/test_voxel_representation.py,21,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\nimport torch\n\nimport kaolin as kal\n\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_voxelgrid(device):\n    voxels = torch.ones([32, 32, 32]).to(device)\n    kal.rep.VoxelGrid(voxels=voxels, copy=False)\n\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_scale_down(device):\n    voxel = torch.ones([32, 32, 32]).to(device)\n    down = kal.conversions.voxelgridconversions.downsample(voxel, [2, 2, 2])\n    assert (set(down.shape) == set([16, 16, 16]))\n    down = kal.conversions.voxelgridconversions.downsample(voxel, [3, 3, 3])\n    assert (set(down.shape) == set([10, 10, 10]))\n    down = kal.conversions.voxelgridconversions.downsample(voxel, [3, 2, 1])\n    assert (set(down.shape) == set([10, 16, 32]))\n\n\n    voxel = torch.zeros([128, 128, 128]).to(device)\n    down = kal.conversions.voxelgridconversions.downsample(voxel, [2, 2, 2])\n    assert (set(down.shape) == set([64, 64, 64]))\n    down = kal.conversions.voxelgridconversions.downsample(voxel, [3, 3, 3])\n    assert (set(down.shape) == set([42, 42, 42]))\n    down = kal.conversions.voxelgridconversions.downsample(voxel, [3, 2, 1])\n    assert (set(down.shape) == set([42, 64, 128]))\n\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_scale_up(device):\n    voxel = torch.ones([32, 32, 32]).to(device)\n    down = kal.conversions.voxelgridconversions.upsample(voxel, 64)\n    assert (set(down.shape) == set([64, 64, 64]))\n    down = kal.conversions.voxelgridconversions.upsample(voxel, 33)\n    assert (set(down.shape) == set([33, 33, 33]))\n    down = kal.conversions.voxelgridconversions.upsample(voxel, 256)\n    assert (set(down.shape) == set([256, 256, 256]))\n\n\n    voxel = torch.zeros([128, 128, 128]).to(device)\n    down = kal.conversions.voxelgridconversions.upsample(voxel, 128)\n    assert (set(down.shape) == set([128, 128, 128]))\n\n    down = kal.conversions.voxelgridconversions.upsample(voxel, 150)\n    assert (set(down.shape) == set([150, 150, 150]))\n\n    down = kal.conversions.voxelgridconversions.upsample(voxel, 256 )\n    assert (set(down.shape) == set([256, 256, 256]))\n\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_threshold(device):\n    voxel = torch.ones([32, 32, 32]).to(device)\n    binary = kal.conversions.voxelgridconversions.threshold(voxel, .5)\n    assert binary.sum() == 32*32*32\n    assert (set(binary.shape) == set([32, 32, 32]))\n\n    voxel = torch.ones([32, 32, 32]).to(device) * .3\n    binary = kal.conversions.voxelgridconversions.threshold(voxel, .5)\n    assert binary.sum() == 0\n    assert (set(binary.shape) == set([32, 32, 32]))\n\n    voxel = torch.ones([64, 64, 64]).to(device) * .7\n    binary = kal.conversions.voxelgridconversions.threshold(voxel, .5)\n    assert binary.sum() == 64*64*64\n    assert (set(binary.shape) == set([64, 64, 64]))\n\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_fill(device):\n    voxel = torch.ones([32, 32, 32]).to(device)\n    voxel[16, 16, 16] = 0 \n    filled_voxel = kal.conversions.voxelgridconversions.fill(voxel)\n    assert voxel.sum() < filled_voxel.sum()\n    assert filled_voxel.sum() == 32*32*32\n\n    voxel = torch.ones([32, 32, 32]).to(device)\n    filled_voxel = kal.conversions.voxelgridconversions.fill(voxel)\n    assert voxel.sum()  == filled_voxel.sum()\n    assert filled_voxel.sum() == 32*32*32\n\n    voxel = torch.zeros([64, 64, 64]).to(device)\n    filled_voxel = kal.conversions.voxelgridconversions.fill(voxel)\n    assert voxel.sum()  == filled_voxel.sum()\n    assert filled_voxel.sum() == 0\n\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_extract_surface(device):\n    voxel = torch.ones([32, 32, 32]).to(device)\n    surface_voxel = kal.conversions.voxelgridconversions.extract_surface(voxel)\n    assert voxel.sum() > surface_voxel.sum()\n    assert surface_voxel.sum() == 2*(32*32 + 32*30 + 30*30)\n    assert kal.conversions.voxelgridconversions.extract_surface(surface_voxel).sum() == surface_voxel.sum()\n\n    voxel = torch.zeros([32, 32, 32]).to(device)\n    surface_voxel = kal.conversions.voxelgridconversions.extract_surface(voxel)\n    assert voxel.sum() == surface_voxel.sum()\n    assert surface_voxel.sum() == 0\n    assert kal.conversions.voxelgridconversions.extract_surface(surface_voxel).sum() == surface_voxel.sum()\t\n\n    voxel = torch.ones([64, 64, 64]).to(device)\n    surface_voxel = kal.conversions.voxelgridconversions.extract_surface(voxel)\n    assert voxel.sum() > surface_voxel.sum()\n    assert surface_voxel.sum() == 2*(64*64 + 64*62 + 62*62)\n    assert kal.conversions.voxelgridconversions.extract_surface(surface_voxel).sum() == surface_voxel.sum()\n\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_extract_odms(device):\n    voxel = torch.rand([32, 32, 32]).to(device)\n    voxel = kal.conversions.voxelgridconversions.threshold(voxel, .9)\n    odms = kal.conversions.voxelgridconversions.extract_odms(voxel)\n    assert (set(odms.shape) == set([6, 32, 32]))\n    assert (odms.max() == 32)\n    assert (odms.min() == 0)\n\n    voxel = torch.ones([32, 32, 32]).to(device)\n    voxel = kal.conversions.voxelgridconversions.threshold(voxel, .5)\n    odms = kal.conversions.voxelgridconversions.extract_odms(voxel)\n    assert (set(odms.shape) == set([6, 32, 32]))\n    assert (odms.max() == 0)\n\n    voxel = torch.zeros([128, 128, 128]).to(device)\n    voxel = kal.conversions.voxelgridconversions.threshold(voxel, .5)\n    odms = kal.conversions.voxelgridconversions.extract_odms(voxel)\n    assert (set(odms.shape) == set([6, 128, 128]))\n    assert (odms.max() == 128)\n\n\n@pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\ndef test_project_odms(device): \n\n    odms = torch.rand([32, 32, 32]).to(device) *32\n    odms = odms.int()\n    voxel = kal.conversions.voxelgridconversions.project_odms(odms)\n    assert (set(voxel.shape) == set([32, 32, 32]))\n\n    odms = torch.rand([128, 128, 128]).to(device) *128\n    odms = odms.int()\n    voxel = kal.conversions.voxelgridconversions.project_odms(odms)\n    assert (set(voxel.shape) == set([128, 128, 128]))\n\n    voxel = torch.ones([32, 32, 32]).to(device)\n    voxel = kal.conversions.voxelgridconversions.threshold(voxel, .9)\n    odms = kal.conversions.voxelgridconversions.extract_odms(voxel)\n    new_voxel = kal.conversions.voxelgridconversions.project_odms(odms)\n    assert (set(new_voxel.shape) == set([32, 32, 32]))\n    assert (torch.abs(voxel - new_voxel).sum() == 0)\n'"
tests/transforms/test_pointcloudfunc.py,3,"b""import pytest\n\nimport torch\nfrom torch.testing import assert_allclose\n\nimport kaolin as kal\n\ndef test_realign(device='cpu'):\n    src = torch.randn(4, 3).to(device)\n    tgt = torch.arange(4).expand(3, 4).t().to(device, dtype=torch.float)\n    src_ = kal.transforms.pointcloudfunc.realign(src, tgt)\n"""
tests/transforms/test_transforms.py,15,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pytest\n\nimport numpy as np\nimport torch\nfrom torch.testing import assert_allclose\n\nimport kaolin as kal\nfrom kaolin import helpers\nfrom kaolin.rep import TriangleMesh\n\n\ndef test_numpy_to_tensor(device=\'cpu\'):\n    nptotensor = kal.transforms.NumpyToTensor()\n    arr = np.zeros(3)\n    ten = nptotensor(arr)\n    assert torch.is_tensor(ten)\n\n\ndef test_scale_pointcloud(device=\'cpu\'):\n    twice = kal.transforms.ScalePointCloud(2)\n    halve = kal.transforms.ScalePointCloud(0.5)\n    pc = torch.ones(4, 3)\n    pc_ = halve(twice(pc))\n    assert_allclose(pc_, torch.ones(4, 3))\n\n\ndef test_rotate_pointcloud(device=\'cpu\'):\n    pc = torch.ones(4, 3)\n    rmat = 2 * torch.eye(3)\n    rmatinv = rmat.inverse()\n    rotate1 = kal.transforms.RotatePointCloud(rmat)\n    rotate2 = kal.transforms.RotatePointCloud(rmatinv)\n    iden = kal.transforms.Compose([rotate1, rotate2])\n    assert_allclose(iden(pc), torch.ones(4, 3))\n\n\ndef test_realign_pointcloud(device=\'cpu\'):\n    torch.manual_seed(1234)\n    src = 10 * torch.rand(4, 3).to(device)\n    tgt = 10 * torch.rand(4, 3).to(device)\n    realign = kal.transforms.RealignPointCloud(tgt)\n    src_ = realign(src)\n    # After transformation, src_.mean(-2) should be equal to tgt.mean(-2)\n    assert_allclose(src_.mean(-2), tgt.mean(-2), atol=1e-2, rtol=1e1)\n    # Similarly, the stddevs should be equal along dim -2.\n    assert_allclose(src_.std(-2), tgt.std(-2), atol=1e-2, rtol=1e1)\n\n\ndef test_normalize_pointcloud(device=\'cpu\'):\n    src = torch.rand(4, 3).to(device)\n    normalize = kal.transforms.NormalizePointCloud()\n    src = normalize(src)\n    assert_allclose(src.mean(-2), torch.zeros_like(src.mean(-2)))\n    assert_allclose(src.std(-2), torch.ones_like(src.std(-2)))\n\n\ndef test_downsample_voxelgrid(device=\'cpu\'):\n    voxel = torch.ones([32, 32, 32]).to(device)\n    down = kal.transforms.DownsampleVoxelGrid([2, 2, 2], inplace=False)\n    helpers._assert_shape_eq(down(voxel), (16, 16, 16))\n    down = kal.transforms.DownsampleVoxelGrid([3, 3, 3], inplace=False)\n    helpers._assert_shape_eq(down(voxel), (10, 10, 10))\n    down = kal.transforms.DownsampleVoxelGrid([3, 2, 1], inplace=False)\n    helpers._assert_shape_eq(down(voxel), (10, 16, 32))\n\n\ndef test_upsample_voxelgrid(device=\'cpu\'):\n    voxel = torch.ones([32, 32, 32]).to(device)\n    up = kal.transforms.UpsampleVoxelGrid(64)\n    helpers._assert_shape_eq(up(voxel), (64, 64, 64))\n    up = kal.transforms.UpsampleVoxelGrid(33)\n    helpers._assert_shape_eq(up(voxel), (33, 33, 33))\n\n\ndef test_triangle_mesh_to_pointcloud(device=\'cpu\'):\n    mesh = TriangleMesh.from_obj(\'tests/model.obj\')\n    mesh.to(device)\n    mesh2cloud = kal.transforms.TriangleMeshToPointCloud(10000)\n    pts = mesh2cloud(mesh)\n    helpers._assert_shape_eq(pts, (10000, 3))\n\n\ndef test_triangle_mesh_to_voxelgrid(device=\'cpu\'):\n    mesh = TriangleMesh.from_obj(\'tests/model.obj\')\n    mesh.to(device)\n    mesh2voxel = kal.transforms.TriangleMeshToVoxelGrid(32)\n    helpers._assert_shape_eq(mesh2voxel(mesh), (32, 32, 32))\n\n\ndef test_triangle_mesh_to_sdf(device=\'cpu\'):\n    mesh = TriangleMesh.from_obj(\'tests/model.obj\')\n    mesh.to(device)\n    mesh2sdf = kal.transforms.TriangleMeshToSDF(100)\n    helpers._assert_shape_eq(mesh2sdf(mesh), (100,), dim=-1)\n'"
tests/vision/test_geometry.py,13,"b'""""""\nUnittests for projective geometry utility functions\n""""""\n\n# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n# Kornia components:\n# Copyright (C) 2017-2019, Arraiy, Inc., all rights reserved.\n# Copyright (C) 2019-    , Open Source Vision Foundation, all rights reserved.\n# Copyright (C) 2019-    , Kornia authors, all rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nfrom torch.testing import assert_allclose\n\nimport kaolin as kal\nfrom kaolin.vision import *\n\n\ndef test_project_unproject():\n    pts = torch.rand(20, 3)\n    # intrinsics = torch.rand(4, 4)\n    intrinsics = torch.FloatTensor([[720, 0, 120, 0], \n                                    [0, 720, 90, 0], \n                                    [0, 0, 1, 0], \n                                    [0, 0, 0, 1]])\n    img_pts = project_points(pts, intrinsics)\n    # depths = torch.rand(pts.shape[0]) + 1.\n    depths = pts[..., 2]\n    cam_pts = unproject_points(img_pts, depths, intrinsics)\n    assert_allclose(pts, cam_pts, atol=1e-3, rtol=1e-3)\n\n\ndef test_unproject_project():\n    img_pts = torch.rand(20, 2)\n    depths = torch.rand(20) + 1.\n    intrinsics = torch.FloatTensor([[720, 0, 120, 0], \n                                    [0, 720, 90, 0], \n                                    [0, 0, 1, 0], \n                                    [0, 0, 0, 1]])\n    cam_pts = unproject_points(img_pts, depths, intrinsics)\n    img_pts_reproj = project_points(cam_pts, intrinsics)\n    assert_allclose(img_pts, img_pts_reproj, atol=1e-3, rtol=1e-3)\n\n\ndef test_project_unproject_batch():\n    pts = torch.rand(10, 20, 3)\n    intrinsics = torch.FloatTensor([[720, 0, 120, 0], \n                                    [0, 720, 90, 0], \n                                    [0, 0, 1, 0], \n                                    [0, 0, 0, 1]])\n    intrinsics = intrinsics.repeat(10, 1, 1)\n    img_pts = project_points(pts, intrinsics)\n    depths = pts[..., 2]\n    cam_pts = unproject_points(img_pts, depths, intrinsics)\n    assert_allclose(pts, cam_pts, atol=1e-3, rtol=1e-3)\n\n\ndef test_unproject_project_batch():\n    img_pts = torch.rand(10, 20, 2)\n    depths = torch.rand(10, 20) + 1.\n    intrinsics = torch.FloatTensor([[720, 0, 120, 0], \n                                    [0, 720, 90, 0], \n                                    [0, 0, 1, 0], \n                                    [0, 0, 0, 1]])\n    intrinsics = intrinsics.repeat(10, 1, 1)\n    cam_pts = unproject_points(img_pts, depths, intrinsics)\n    img_pts_reproj = project_points(cam_pts, intrinsics)\n    assert_allclose(img_pts, img_pts_reproj, atol=1e-3, rtol=1e-3)\n'"
tests/visualize/test_vis_usd.py,2,"b""import os\nimport pytest\nimport torch\nfrom pathlib import Path\n\nfrom kaolin.rep import TriangleMesh, VoxelGrid, PointCloud\nfrom kaolin.conversions.meshconversions import trianglemesh_to_pointcloud, trianglemesh_to_voxelgrid\nfrom kaolin.visualize.vis_usd import VisUsd\n\nroot = Path('tests/visualize/results')\nroot.mkdir(exist_ok=True)\nmesh = TriangleMesh.from_obj('tests/model.obj')\nvoxels = VoxelGrid(trianglemesh_to_voxelgrid(mesh, 32))\npc = PointCloud(trianglemesh_to_pointcloud(mesh, 500)[0])\n\nvis = VisUsd()\n\n@pytest.mark.parametrize('object_3d', [mesh, voxels, pc])\n@pytest.mark.parametrize('device', ['cpu', 'cuda'])\n@pytest.mark.parametrize('meet_ground', [True, False])\n@pytest.mark.parametrize('center_on_stage', [True, False])\n@pytest.mark.parametrize('fit_to_stage', [True, False])\ndef test_vis(object_3d, device, meet_ground, center_on_stage, fit_to_stage):\n    if device == 'cuda':\n        if isinstance(object_3d, TriangleMesh):\n            object_3d.cuda()\n        elif isinstance(object_3d, PointCloud):\n            object_3d.points = object_3d.points.to(torch.device(device))\n        elif isinstance(object_3d, VoxelGrid):\n            object_3d.voxels = object_3d.voxels.to(torch.device(device))\n\n    vis.set_stage(filepath=str(root / f'{type(object_3d).__name__}_{device}.usda'))\n    vis.visualize(object_3d, meet_ground=meet_ground, center_on_stage=center_on_stage,\n                  fit_to_stage=fit_to_stage)\n"""
docs/_themes/sphinx_rtd_theme/__init__.py,0,"b'""""""\nSphinx Read the Docs theme.\n\nFrom https://github.com/ryan-roemer/sphinx-bootstrap-theme.\n""""""\n\nfrom os import path\n\nimport sphinx\n\n\n__version__ = \'0.4.3.dev0\'\n__version_full__ = __version__\n\n\ndef get_html_theme_path():\n    """"""Return list of HTML theme paths.""""""\n    cur_dir = path.abspath(path.dirname(path.dirname(__file__)))\n    return cur_dir\n\ndef scb_static_path(app):\n    static_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \'static\'))\n    app.config.html_static_path.append(static_path)\n\nclipboard_js_url = ""https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js""\n\n\n# See http://www.sphinx-doc.org/en/stable/theming.html#distribute-your-theme-as-a-python-package\ndef setup(app):\n    app.add_html_theme(\'sphinx_rtd_theme\', path.abspath(path.dirname(__file__)))\n    app.connect(\'builder-inited\', scb_static_path)\n    app.add_stylesheet(\'static/copybutton.css\')\n    app.add_javascript(\'static/clipboard.min.js\')\n    app.add_javascript(\'static/copybutton.js\')\n\n    if sphinx.version_info >= (1, 8, 0):\n        # Add Sphinx message catalog for newer versions of Sphinx\n        # See http://www.sphinx-doc.org/en/master/extdev/appapi.html#sphinx.application.Sphinx.add_message_catalog\n        rtd_locale_path = path.join(path.abspath(path.dirname(__file__)), \'locale\')\n        app.add_message_catalog(\'sphinx\', rtd_locale_path)\n'"
examples/GANs/3D-IWGAN/architectures.py,2,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nNetwork architecture definitions\n""""""\n\nimport torch\nimport torch.nn as nn\n\n\nclass Generator(nn.Module):\n    """"""A simple encoder-decoder style voxel superresolution network""""""\n\n\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.linear = nn.Linear(200, 256 * 2 * 2 * 2)\n        self.post_linear = nn.Sequential(\n            nn.BatchNorm3d(256),\n            nn.ReLU()\n        )\n\n        self.layer1 = nn.Sequential(\n            nn.ConvTranspose3d(256, 256, kernel_size=4, stride=2, padding=(1, 1, 1)),\n            nn.BatchNorm3d(256),\n            nn.ReLU()\n        )\n        self.layer2 = nn.Sequential(\n            nn.ConvTranspose3d(256, 128, kernel_size=4, stride=2, padding=(1, 1, 1)),\n            nn.BatchNorm3d(128),\n            nn.ReLU()\n        )\n        self.layer3 = nn.Sequential(\n            nn.ConvTranspose3d(128, 64, kernel_size=4, stride=2, padding=(1, 1, 1)),\n            nn.BatchNorm3d(64),\n            nn.ReLU()\n        )\n        self.layer4 = nn.Sequential(\n            nn.ConvTranspose3d(64, 1, kernel_size=4, stride=2, padding=(1, 1, 1))\n        )\n\n        # initialize weights\n        for m in self.modules():\n            if (isinstance(m, nn.ConvTranspose3d)\n                    or isinstance(m, nn.Linear)):\n                nn.init.normal_(m.weight, std=0.02)\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = x.view(-1, 256, 2, 2, 2)\n        x = self.post_linear(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = x.squeeze(1)\n\n        x = torch.tanh(x[:, :32, :32, :32])\n        return x\n\nclass Discriminator(nn.Module):\n    """"""A simple encoder-decoder style voxel superresolution network""""""\n\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.layer1 = nn.Sequential(\n            nn.Conv3d(1, 32, kernel_size=4, stride=2),\n            nn.LeakyReLU(.2)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv3d(32, 64, kernel_size=4, stride=2),\n            nn.LeakyReLU(.2)\n        )\n        self.layer3 = nn.Sequential(\n            nn.Conv3d(64, 128, kernel_size=4, stride=2),\n            nn.LeakyReLU(.2)\n        )\n        self.layer4 = nn.Sequential(\n            nn.Conv3d(128, 256, kernel_size=2, stride=2),\n            nn.LeakyReLU(.2)\n        )\n        self.layer5 = nn.Linear(256, 1)\n\n        # initialize weights\n        for m in self.modules():\n            if (isinstance(m, nn.Conv3d)\n                    or isinstance(m, nn.Linear)):\n                nn.init.normal_(m.weight, std=0.02)\n\n    def forward(self, x):\n        x = x.view(-1, 1, 32, 32, 32)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = x.view(x.shape[0], -1)\n        x = self.layer5(x)\n        return x\n'"
examples/GANs/3D-IWGAN/eval.py,4,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nimport argparse\nimport torch\n\nfrom architectures import Generator\nimport kaolin as kal \n\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--logdir\', type=str, default=\'log\', help=\'Directory to log data to.\')\nparser.add_argument(\'--expid\', type=str, default=\'3D_IWGAN\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'--device\', type=str, default=\'cuda\', help=\'Device to use.\')\nparser.add_argument(\'--batchsize\', type=int, default=50, help=\'Batch size.\')\nargs = parser.parse_args()\n\nlogdir = os.path.join(args.logdir, args.expid)\n\ngen = Generator().to(args.device)\ngen.load_state_dict(torch.load(os.path.join(logdir, \'gen.pth\')))\ngen.eval()\n\nz = torch.normal(torch.zeros(args.batchsize, 200), torch.ones(args.batchsize, 200)).to(args.device)\n\nfake_voxels = gen(z)\n\nfor i, model in enumerate(fake_voxels): \n    print(\'Rendering model {}\'.format(i))\n    model = model[:-2, :-2, :-2]\n    model = kal.transforms.voxelfunc.max_connected(model, .7)\n    verts, faces = kal.conversions.voxelgrid_to_quadmesh(model)\n    mesh = kal.rep.QuadMesh.from_tensors(verts, faces)\n    mesh.laplacian_smoothing(iterations=3)\n    mesh.show()\n'"
examples/GANs/3D-IWGAN/train.py,14,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport json\nimport os\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n\nfrom utils import calculate_gradient_penalty\nfrom architectures import Generator, Discriminator\n\nimport kaolin as kal \n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--modelnet-root\', type=str, help=\'Root directory of the ModelNet dataset.\')\nparser.add_argument(\'--cache-dir\', type=str, default=None, help=\'Path to write intermediate representation to.\')\nparser.add_argument(\'--expid\', type=str, default=\'3D_IWGAN\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'--device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'--categories\', type=str, nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'--epochs\', type=int, default=50000, help=\'Number of train epochs.\')\nparser.add_argument(\'--batchsize\', type=int, default=50, help=\'Batch size.\')\nparser.add_argument(\'--print-every\', type=int, default=2, help=\'Print frequency (batches).\')\nparser.add_argument(\'--logdir\', type=str, default=\'log\', help=\'Directory to log data to.\')\nparser.add_argument(\'--resume\', action=\'store_true\', help=\'Resume training from last checkpoint.\')\n\nargs = parser.parse_args()\n\n\n# Setup Dataloader\ntrain_set = kal.datasets.modelnet.ModelNetVoxels(basedir=args.modelnet_root, cache_dir=args.cache_dir,\n                                                 categories=args.categories, resolutions=[30])\ndataloader_train = DataLoader(train_set, batch_size=args.batchsize, shuffle=True, num_workers=8)\n\n\n# Setup Models\ngen = Generator().to(args.device)\ndis = Discriminator().to(args.device)\n\n\noptim_g = optim.Adam(gen.parameters(), lr=.0001, betas=(0.5, 0.9))\noptim_d = optim.Adam(dis.parameters(), lr=.0001, betas=(0.5, 0.9))\n\n# Create log directory, if it doesn\'t already exist\nlogdir = os.path.join(args.logdir, args.expid)\nif not os.path.isdir(logdir):\n    os.makedirs(logdir)\n    print(\'Created dir:\', logdir)\n\n# Log all commandline args\nwith open(os.path.join(logdir, \'args.txt\'), \'w\') as f:\n    json.dump(args.__dict__, f, indent=2)\n\n\nclass Engine(object):\n    """"""Engine that runs training and inference.\n    Args\n        - cur_epoch (int): Current epoch.\n        - print_every (int): How frequently (# batches) to print loss.\n        - validate_every (int): How frequently (# epochs) to run validation.\n    """"""\n\n    def __init__(self, print_every=1, resume=False):\n        self.cur_epoch = 0\n        self.train_loss = []\n        self.val_loss = []\n        self.bestval = 0\n        self.print_every = print_every\n\n        if resume:\n            self.load()\n\n    def train(self):\n        loss_epoch = 0.\n        num_batches = 0\n        train_dis = True\n        gen.train()\n        dis.train()\n\n        # Train loop\n        for i, sample in enumerate(tqdm(dataloader_train), 0):\n            voxels = sample[\'data\'][\'30\']\n            optim_g.zero_grad(), gen.zero_grad()\n            optim_d.zero_grad(), dis.zero_grad()\n\n            # data creation\n            real_voxels = torch.zeros(voxels.shape[0], 32, 32, 32).to(args.device)\n            real_voxels[:, 1:-1, 1:-1, 1:-1] = voxels.to(args.device)\n\n\n            z = torch.normal(torch.zeros(voxels.shape[0], 200), torch.ones(voxels.shape[0], 200)).to(args.device)\n\n            fake_voxels = gen(z)\n            d_on_fake = torch.mean(dis(fake_voxels))\n            d_on_real = torch.mean(dis(real_voxels))\n            gp_loss = 10 * calculate_gradient_penalty(dis, real_voxels.data, fake_voxels.data)\n            d_loss = -d_on_real + d_on_fake + gp_loss\n\n            if i % 5 == 0: \n                g_loss = -d_on_fake\n                g_loss.backward()\n                optim_g.step()\n            else: \n                d_loss.backward()\n                optim_d.step()\n\n            # logging\n            num_batches += 1\n            if i % args.print_every == 0:\n                message = f\'[TRAIN] Epoch {self.cur_epoch:03d}, Batch {i:03d}: gen: {float(g_loss.item()):2.3f}\'\n                message += f\' dis = {float(d_loss.item()):2.3f}, gp = {float(gp_loss.item()):2.3f}\'\n                tqdm.write(message)\n\n        self.train_loss.append(loss_epoch)\n        self.cur_epoch += 1\n\n    def load(self):\n        gen.load_state_dict(torch.load(os.path.join(logdir, \'gen.pth\')))\n        dis.load_state_dict(torch.load(os.path.join(logdir, \'dis.pth\')))\n        optim_g.load_state_dict(torch.load(os.path.join(logdir, \'optim_g.pth\')))\n        optim_d.load_state_dict(torch.load(os.path.join(logdir, \'optim_d.pth\')))\n        # Read data corresponding to the loaded model\n        with open(os.path.join(logdir, \'recent.log\'), \'r\') as f:\n            run_data = json.load(f)\n        self.cur_epoch = run_data[\'epoch\']\n\n\n    def save(self):\n        # Create a dictionary of all data to save\n        log_table = {\n            \'epoch\': self.cur_epoch\n        }\n\n        # Save the recent model/optimizer states\n        torch.save(gen.state_dict(), os.path.join(logdir, \'gen.pth\'))\n        torch.save(dis.state_dict(), os.path.join(logdir, \'dis.pth\'))\n        torch.save(optim_g.state_dict(), os.path.join(logdir, \'optim_g.pth\'))\n        torch.save(optim_d.state_dict(), os.path.join(logdir, \'optim_d.pth\'))\n        # Log other data corresponding to the recent model\n        with open(os.path.join(logdir, \'recent.log\'), \'w\') as f:\n            f.write(json.dumps(log_table))\n\n        tqdm.write(\'====== Saved recent model ======>\')\n\n\ntrainer = Engine(print_every=args.print_every, resume=args.resume)\n\nfor epoch in range(args.epochs): \n    trainer.train()\n    if epoch % 5 == 4: \n        trainer.save()\n'"
examples/GANs/3D-IWGAN/utils.py,5,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch \n\ntorch.manual_seed(1)\ntorch.cuda.manual_seed(1)\n\n\ndef calculate_gradient_penalty(D, real_samples, fake_samples):\n    """"""Calculates the gradient penalty loss""""""\n    # Random weight for interpolation between real and fake samples\n    eta = torch.rand((real_samples.size(0), 1, 1, 1), device=real_samples.device)\n    # Get random interpolation between real and fake samples\n    interpolates = (eta * real_samples + ((1 - eta) * fake_samples)).requires_grad_(True)\n    # calculate probability of interpolated examples\n    d_interpolates = D(interpolates)\n    # Get gradient w.r.t. interpolates\n    fake = torch.ones_like(d_interpolates, device=real_samples.device, requires_grad=False)\n    gradients = torch.autograd.grad(outputs=d_interpolates,\n                                    inputs=interpolates,\n                                    grad_outputs=fake,\n                                    create_graph=True,\n                                    retain_graph=True)[0]\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty\n'"
examples/GANs/3DGAN/architectures.py,35,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport torch\nimport torch.nn as nn\n\n\nclass Generator(nn.Module):\n    """"""A simple encoder-decoder style voxel superresolution network""""""\n\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.layer1 = torch.nn.Sequential(\n            torch.nn.ConvTranspose3d(200, 512, 4, 2, 0),\n            torch.nn.BatchNorm3d(512),\n            torch.nn.ReLU()\n        )\n        self.layer2 = torch.nn.Sequential(\n            torch.nn.ConvTranspose3d(512, 256, 4, 2, 1),\n            torch.nn.BatchNorm3d(256),\n            torch.nn.ReLU()\n        )\n        self.layer3 = torch.nn.Sequential(\n            torch.nn.ConvTranspose3d(256, 128, 4, 2, 1),\n            torch.nn.BatchNorm3d(128),\n            torch.nn.ReLU()\n        )\n        self.layer4 = torch.nn.Sequential(\n            torch.nn.ConvTranspose3d(128, 1, 4, 2, 1),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = x.view(-1, 200, 1, 1, 1)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        return x\n\nclass Discriminator(nn.Module):\n    """"""A simple encoder-decoder style voxel superresolution network""""""\n\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.layer1 = torch.nn.Sequential(\n            torch.nn.Conv3d(1, 64, 4, 2, 1),\n            torch.nn.BatchNorm3d(64),\n            torch.nn.LeakyReLU()\n        )\n        self.layer2 = torch.nn.Sequential(\n            torch.nn.Conv3d(64, 128, 4, 2, 1),\n            torch.nn.BatchNorm3d(128),\n            torch.nn.LeakyReLU()\n        )\n        self.layer3 = torch.nn.Sequential(\n            torch.nn.Conv3d(128, 256, 4, 2, 1),\n            torch.nn.BatchNorm3d(256),\n            torch.nn.LeakyReLU()\n        )\n        self.layer4 = torch.nn.Sequential(\n            torch.nn.Conv3d(256, 512, 4, 2, 1),\n            torch.nn.BatchNorm3d(512),\n            torch.nn.LeakyReLU()\n        )\n        self.layer5 = torch.nn.Sequential(\n            torch.nn.Conv3d(512, 1, 2, 2, 0),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = x.view(-1, 1, 32, 32, 32)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        return x\n'"
examples/GANs/3DGAN/eval.py,2,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nimport argparse\nimport torch\n\nfrom architectures import Generator\nimport kaolin as kal\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--logdir\', type=str, default=\'log\', help=\'Directory to log data to.\')\nparser.add_argument(\'--expid\', type=str, default=\'GAN\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'--device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'--batchsize\', type=int, default=50, help=\'Batch size.\')\nargs = parser.parse_args()\n\nlogdir = os.path.join(args.logdir, args.expid)\n\ngen = Generator().to(args.device)\ngen.load_state_dict(torch.load(os.path.join(logdir, \'gen.pth\')))\ngen.eval()\n\nz = torch.normal(torch.zeros(args.batchsize, 200), torch.ones(args.batchsize, 200) * .33).to(args.device)\n\nfake_voxels = gen(z)[:, 0]\nfor i, model in enumerate(fake_voxels): \n    model = model[:-2, :-2, :-2]\n    model = kal.transforms.voxelfunc.max_connected(model, .5)\n    verts, faces = kal.conversions.voxelgrid_to_quadmesh(model)\n    mesh = kal.rep.QuadMesh.from_tensors(verts, faces)\n    mesh.laplacian_smoothing(iterations=3)\n    mesh.show()\n'"
examples/GANs/3DGAN/train.py,17,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport argparse\nimport json\nimport os\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nimport kaolin as kal\n\nfrom architectures import Generator, Discriminator\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--modelnet-root\', type=str, help=\'Root directory of the ModelNet dataset.\')\nparser.add_argument(\'--cache-dir\', type=str, default=None, help=\'Path to write intermediate representation to.\')\nparser.add_argument(\'--expid\', type=str, default=\'GAN\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'--device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'--categories\', type=str, nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'--epochs\', type=int, default=50000, help=\'Number of train epochs.\')\nparser.add_argument(\'--batchsize\', type=int, default=50, help=\'Batch size.\')\nparser.add_argument(\'--print-every\', type=int, default=2, help=\'Print frequency (batches).\')\nparser.add_argument(\'--logdir\', type=str, default=\'log\', help=\'Directory to log data to.\')\nparser.add_argument(\'--resume\', action=\'store_true\', help=\'Resume training from last checkpoint.\')\n\nargs = parser.parse_args()\n\n\n# Setup Dataloader\ntrain_set = kal.datasets.ModelNetVoxels(basedir=args.modelnet_root, cache_dir=args.cache_dir,\n                                        categories=args.categories, resolutions=[30])\ndataloader_train = DataLoader(train_set, batch_size=args.batchsize, shuffle=True, num_workers=8)\n\n\n# Setup Models\ngen = Generator().to(args.device)\ndis = Discriminator().to(args.device)\n\n\noptim_g = optim.Adam(gen.parameters(), lr=.0025, betas=(0.5, 0.999))\noptim_d = optim.Adam(dis.parameters(), lr=.00005, betas=(0.5, 0.999))\n\n# Create log directory, if it doesn\'t already exist\nlogdir = os.path.join(args.logdir, args.expid)\nif not os.path.isdir(logdir):\n    os.makedirs(logdir)\n    print(\'Created dir:\', logdir)\n\n# Log all commandline args\nwith open(os.path.join(logdir, \'args.txt\'), \'w\') as f:\n    json.dump(args.__dict__, f, indent=2)\n\n\nclass Engine(object):\n    """"""Engine that runs training and inference.\n    Args\n        - cur_epoch (int): Current epoch.\n        - print_every (int): How frequently (# batches) to print loss.\n        - validate_every (int): How frequently (# epochs) to run validation.\n    """"""\n\n    def __init__(self, print_every=1, resume=False):\n        self.cur_epoch = 0\n        self.train_loss = []\n        self.val_loss = []\n        self.bestval = 0\n        self.print_every = print_every\n\n        if resume:\n            self.load()\n\n    def train(self):\n        loss_epoch = 0.\n        num_batches = 0\n        train_dis = True\n        gen.train()\n        dis.train()\n        # Train loop\n        for i, sample in enumerate(tqdm(dataloader_train), 0):\n            data = sample[\'data\']\n            optim_g.zero_grad(), gen.zero_grad()\n            optim_d.zero_grad(), dis.zero_grad()\n\n            # data creation\n            real_voxels = torch.zeros(data[\'30\'].shape[0], 32, 32, 32).to(args.device)\n            real_voxels[:, 1:-1, 1:-1, 1:-1] = data[\'30\'].to(args.device)\n\n            # train discriminator\n            z = torch.normal(torch.zeros(data[\'30\'].shape[0], 200), \n                             torch.ones(data[\'30\'].shape[0], 200) * .33).to(args.device)\n\n            fake_voxels = gen(z)\n            d_on_fake = dis(fake_voxels)\n            d_on_real = dis(real_voxels)\n\n            d_loss = -torch.mean(torch.log(d_on_real) + torch.log(1. - d_on_fake))\n\n            d_accuracy = ((d_on_real >= .5).float().mean() + (d_on_fake < .5).float().mean()) / 2. \n            g_accuracy = (d_on_fake > .5).float().mean()\n            train_dis = d_accuracy < .8\n\n            if train_dis:\n                dis.zero_grad()\n                d_loss.backward()\n                optim_d.step()\n\n            # train generator\n            z = torch.normal(torch.zeros(data[\'30\'].shape[0], 200),\n                             torch.ones(data[\'30\'].shape[0], 200) * .33).to(args.device)\n            fake_voxels = gen(z)\n            d_on_fake = dis(fake_voxels)\n            g_loss = -torch.mean(torch.log(d_on_fake))\n\n            dis.zero_grad()\n            gen.zero_grad()\n            g_loss.backward()\n            optim_g.step()\n\n            # logging\n            num_batches += 1\n            if i % self.print_every == 0:\n                tqdm.write(f\'[TRAIN] Epoch {self.cur_epoch:03d}, Batch {i:03d}: \'\n                           f\'gen: {float(g_accuracy.item()):2.3f}, dis = {float(d_accuracy.item()):2.3f}\')\n\n        self.train_loss.append(loss_epoch)\n        self.cur_epoch += 1\n\n    def load(self):\n        gen.load_state_dict(torch.load(os.path.join(logdir, \'gen.pth\')))\n        dis.load_state_dict(torch.load(os.path.join(logdir, \'dis.pth\')))\n        optim_g.load_state_dict(torch.load(os.path.join(logdir, \'optim_g.pth\')))\n        optim_d.load_state_dict(torch.load(os.path.join(logdir, \'optim_d.pth\')))\n        # Read data corresponding to the loaded model\n        with open(os.path.join(logdir, \'recent.log\'), \'r\') as f:\n            run_data = json.load(f)\n        self.cur_epoch = run_data[\'epoch\']\n\n    def save(self):\n        # Create a dictionary of all data to save\n        log_table = {\n            \'epoch\': self.cur_epoch\n        }\n\n        # Save the recent model/optimizer states\n        torch.save(gen.state_dict(), os.path.join(logdir, \'gen.pth\'))\n        torch.save(dis.state_dict(), os.path.join(logdir, \'dis.pth\'))\n        torch.save(optim_g.state_dict(), os.path.join(logdir, \'optim_g.pth\'))\n        torch.save(optim_d.state_dict(), os.path.join(logdir, \'optim_d.pth\'))\n        # Log other data corresponding to the recent model\n        with open(os.path.join(logdir, \'recent.log\'), \'w\') as f:\n            f.write(json.dumps(log_table))\n\n        tqdm.write(\'====== Saved recent model ======>\')\n\ntrainer = Engine(print_every=args.print_every, resume=args.resume)\n\nfor epoch in range(args.epochs): \n    trainer.train()\n    if epoch % 10 == 9: \n        trainer.save()\n'"
examples/ImageRecon/Dib/architectures.py,2,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nNetwork architecture definitions\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n\n\nclass Encoder(nn.Module):\n\n\tdef __init__(self, N_CHANNELS, N_KERNELS, \\\n\t\t\t\t BATCH_SIZE, IMG_DIM, VERTS):\n\t\tsuper(Encoder, self).__init__()\n\t\t\n\t\tblock1 = self.convblock(N_CHANNELS, 32, N_KERNELS, stride=2, pad=2)\n\t\tblock2 = self.convblock(32, 64, N_KERNELS, stride=2, pad=2)\n\t\tblock3 = self.convblock(64, 128, N_KERNELS, stride=2, pad=2)\n\t\tblock4 = self.convblock(128, 128, N_KERNELS, stride=2, pad=2)\n\t\t\n\t\tlinear1 = self.linearblock(10368, 1024)\n\t\tlinear2 = self.linearblock(1024, 1024)\n\t\tself.linear3 = nn.Linear(1024, 1024)\n\t\t\n\t\tlinear4 = self.linearblock(1024, 1024)\n\t\tlinear5 = self.linearblock(1024, 2048)\n\t\tself.linear6 = nn.Linear(2048, VERTS*6)\n\t   \n\t\t\n\t\t#################################################\n\t\tall_blocks = block1 + block2 + block3 + block4\n\t\tself.encoder1 = nn.Sequential(*all_blocks)\n\t\t\n\t\tall_blocks = linear1 + linear2\n\t\tself.encoder2 = nn.Sequential(*all_blocks)\n\t\t\n\t\tall_blocks = linear4 + linear5\n\t\tself.decoder = nn.Sequential(*all_blocks)\n\t\t\n\t   \n\t\t\n\t\t# Initialize with Xavier Glorot\n\t\tfor m in self.modules():\n\t\t\tif isinstance(m, nn.ConvTranspose2d) \\\n\t\t\tor isinstance(m, nn.Linear) \\\n\t\t\tor isinstance(object, nn.Conv2d):\n\t\t\t\tnn.init.xavier_uniform_(m.weight)\n\t\t\t\tnn.init.normal_(m.weight, mean=0, std=0.001)\n\n\t\t# Free some memory\n\t\tdel all_blocks, block1, block2, block3, \\\n\t\tlinear1, linear2, linear4, linear5, \\\n\t   \n\n\t   \n\t\t\n\t\n\tdef convblock(self, indim, outdim, ker, stride, pad):\n\t\tblock2 = [\n\t\t\tnn.Conv2d(indim, outdim, ker, stride, pad),\n\t\t\tnn.BatchNorm2d(outdim),\n\t\t\tnn.ReLU()\n\t\t]\n\t\treturn block2\n\t\n\tdef linearblock(self, indim, outdim):\n\t\tblock2 = [\n\t\t\tnn.Linear(indim, outdim),\n\t\t\tnn.BatchNorm1d(outdim),\n\t\t\tnn.ReLU()\n\t\t]\n\t\treturn block2\n\t\t\n\tdef forward(self, x):\n\t\t\n\t\tfor layer in self.encoder1:\n\t\t\tx = layer(x)\n\t\t\n\t\tbnum = x.shape[0] \n\t\tx = x.view(bnum, -1) \n\t\tfor layer in self.encoder2:\n\t\t\tx = layer(x)\n\t\tx = self.linear3(x)\n\t\t\n\t   \n\t\tfor layer in self.decoder:\n\t\t\tx = layer(x)\n\t\tx = self.linear6(x).view(x.shape[0], -1,6)\n\t\tverts = x[:,:,:3]\n\t\tcolors = x[:,:,3:]\n\t\treturn verts, colors\n\n\n'"
examples/ImageRecon/Dib/eval.py,4,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport os\nimport torch\nimport sys\nfrom tqdm import tqdm\nfrom PIL import Image\nimport numpy as np\n\nfrom torch.utils.data import DataLoader\nfrom graphics.render.base import Render as Dib_Renderer\nfrom graphics.utils.utils_perspective import  perspectiveprojectionnp\n\nfrom utils import preprocess, collate_fn, normalize_adj\nfrom architectures import Encoder\nimport kaolin as kal \n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-expid\', type=str, default=\'Direct\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'-categories\', type=str,nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'-vis\', action=\'store_true\', help=\'Visualize each model while evaluating\')\nparser.add_argument(\'-batchsize\', type=int, default=16, help=\'Batch size.\')\nparser.add_argument(\'-f_score\', action=\'store_true\', help=\'compute F-score\')\nargs = parser.parse_args()\n\n\n# Data\npoints_set_valid = kal.dataloader.ShapeNet.Points(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = False, split = .7, num_points=5000 )\nimages_set_valid = kal.dataloader.ShapeNet.Images(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = False,  split = .7, views=1, transform= preprocess )\nmeshes_set_valid = kal.dataloader.ShapeNet.Meshes(root =\'../../datasets/\', categories =args.categories , \\\n\tdownload = True, train = False,  split = .7)\n\nvalid_set = kal.dataloader.ShapeNet.Combination([points_set_valid, images_set_valid, meshes_set_valid], root=\'../../datasets/\')\ndataloader_val = DataLoader(valid_set, batch_size=args.batchsize, shuffle=False, collate_fn = collate_fn,\n\tnum_workers=8)\n\n# Model\nmesh = kal.rep.TriangleMesh.from_obj(\'386.obj\', enable_adjacency= True)\nmesh.cuda()\nnormalize_adj(mesh)\n    \n\ninitial_verts = mesh.vertices.clone()\ncamera_fov_y = 49.13434207744484 * np.pi/ 180.0 \ncam_proj = perspectiveprojectionnp(camera_fov_y, 1.0 )\ncam_proj =  torch.FloatTensor(cam_proj).cuda()\nmodel = Encoder(4, 5, args.batchsize, 137, mesh.vertices.shape[0] ).cuda()\nrenderer = Dib_Renderer(137, 137, mode = \'VertexColor\')\n\nmodel.load_state_dict(torch.load(\'log/{0}/best.pth\'.format(args.expid)))\n\nloss_epoch = 0.\nf_epoch = 0.\nnum_batches = 0\nnum_items = 0\nloss_fn = kal.metrics.point.chamfer_distance\n\nmodel.eval()\nwith torch.no_grad():\n\tfor data in tqdm(dataloader_val): \n\t\t# data creation\n\t\ttgt_points = data[\'points\'].cuda()\n\t\tinp_images = data[\'imgs\'].cuda()\n\t\timage_gt = inp_images.permute(0,2,3,1)[:,:,:,:3]\n\t\talhpa_gt = inp_images.permute(0,2,3,1)[:,:,:,3:]\n\t\tcam_mat = data[\'cam_mat\'].cuda()\n\t\tcam_pos = data[\'cam_pos\'].cuda()\n\t\tgt_verts = data[\'verts\']\n\t\tgt_faces = data[\'faces\']\n\n\t\t# inference \n\t\tdelta_verts = model(inp_images)\n\n\t\t# set viewing parameters \n\t\trenderer.camera_params = [cam_mat, cam_pos, cam_proj]\n\t\n\t\t# predict mesh properties\n\t\tdelta_verts, colours = model(inp_images)\n\t\tpred_verts = initial_verts + delta_verts\n\t\n\t\t# render image\n\t\timage_pred, _, _ = renderer.forward(points=[(pred_verts*.57 ), mesh.faces], colors=[colours])\n\t\t\n\t\t# mesh loss\n\t\t\n\t\tfor verts, tgt, inp_img, pred_img, gt_v, gt_f in zip(pred_verts, tgt_points, inp_images, image_pred, gt_verts, gt_faces): \t\n\t\t\tmesh.vertices = verts\n\t\t\tpred_points, _ = mesh.sample(3000)\t\n\t\t\tloss_epoch += 3000 * loss_fn(pred_points, tgt).item() / float(args.batchsize)\t\n\n\t\t\tif args.f_score: \n\t\t\t\n\t\t\t\tf_score = kal.metrics.point.f_score(tgt, pred_points, extend = False)\n\t\t\t\tf_epoch += (f_score  / float(args.batchsize)).item()\n\n\t\t\tif args.vis: \n\t\t\t\ttgt_mesh = meshes_set_valid[num_items]\n\t\t\t\ttgt_mesh = kal.rep.TriangleMesh.from_tensors(gt_v, gt_f)\n\n\t\t\t\tprint (\'Displaying input image\')\n\t\t\t\timg = inp_img.data.cpu().numpy().transpose((1, 2, 0))\n\t\t\t\timg = (img*255.).astype(np.uint8)\n\t\t\t\tImage.fromarray(img).show()\n\t\t\t\tinput()\n\t\t\t\tprint (\'Displaying predicted image\')\n\t\t\t\timg = pred_img.data.cpu().numpy()\n\t\t\t\timg = (img*255.).astype(np.uint8)\n\t\t\t\tImage.fromarray(img).show()\n\t\t\t\tinput()\n\n\t\t\t\tprint (\'Rendering Target Mesh\')\n\t\t\t\tkal.visualize.show_mesh(tgt_mesh)\n\t\t\t\tprint (\'Rendering Predicted Mesh\')\n\t\t\t\tmesh.show()\n\t\t\t\tprint(\'----------------------\')\n\t\t\t\tnum_items += 1\n\n\n\t\t\n\t\tnum_batches += 1.\n\nout_loss = loss_epoch / float(num_batches)\nprint (\'Loss over validation set is {0}\'.format(out_loss))\nif args.f_score: \n\tout_f = f_epoch / float(num_batches)\n\tprint (\'F-score over validation set is {0}\'.format(out_f))\n'"
examples/ImageRecon/Dib/train.py,10,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport json\nimport numpy as np\nimport os\nimport torch\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport sys\nfrom tqdm import tqdm\n\nfrom utils import preprocess, loss_lap, collate_fn, normalize_adj, loss_flat\nfrom graphics.render.base import Render as Dib_Renderer\nfrom graphics.utils.utils_perspective import  perspectiveprojectionnp\nfrom architectures import Encoder\n\nimport kaolin as kal \n\n\n""""""\nCommandline arguments\n""""""\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-expid\', type=str, default=\'Direct\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'-categories\', type=str,nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'-epochs\', type=int, default=500, help=\'Number of train epochs.\')\nparser.add_argument(\'-batchsize\', type=int, default=16, help=\'Batch size.\')\nparser.add_argument(\'-lr\', type=float, default=1e-4, help=\'Learning rate.\')\nparser.add_argument(\'-val-every\', type=int, default=5, help=\'Validation frequency (epochs).\')\nparser.add_argument(\'-print-every\', type=int, default=20, help=\'Print frequency (batches).\')\nparser.add_argument(\'-logdir\', type=str, default=\'log\', help=\'Directory to log data to.\')\nparser.add_argument(\'-save-model\', action=\'store_true\', help=\'Saves the model and a snapshot \\\n\tof the optimizer state.\')\nargs = parser.parse_args()\n\n\n\n""""""\nDataset\n""""""\nsdf_set = kal.dataloader.ShapeNet.SDF_Points(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = True, split = .7, num_points=3000 )\npoint_set = kal.dataloader.ShapeNet.Points(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = True, split = .7, num_points=3000 )\nimages_set = kal.dataloader.ShapeNet.Images(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = True,  split = .7, views=23, transform= preprocess )\ntrain_set = kal.dataloader.ShapeNet.Combination([sdf_set, images_set, point_set], root=\'../../kaolin/datasets/\')\n\ndataloader_train = DataLoader(train_set, batch_size=args.batchsize, shuffle=True, num_workers=8)\n\n\n\n\nimages_set_valid = kal.dataloader.ShapeNet.Images(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = False,  split = .7, views=1, transform= preprocess )\ndataloader_val = DataLoader(images_set_valid, batch_size=args.batchsize, shuffle=False, \n\tnum_workers=8)\n\n\n\n""""""\nModel settings \n""""""\nmesh = kal.rep.TriangleMesh.from_obj(\'386.obj\', enable_adjacency= True)\nmesh.cuda()\nnormalize_adj(mesh)\n    \n\ninitial_verts = mesh.vertices.clone()\ncamera_fov_y = 49.13434207744484 * np.pi/ 180.0 \ncam_proj = perspectiveprojectionnp(camera_fov_y, 1.0 )\ncam_proj =  torch.FloatTensor(cam_proj).cuda()\n\nmodel = Encoder(4, 5, args.batchsize, 137, mesh.vertices.shape[0] ).cuda()\n\n\noptimizer = optim.Adam(model.parameters(), lr=args.lr)\nrenderer = Dib_Renderer(137, 137, mode = \'VertexColor\')\n\n\n\n# Create log directory, if it doesn\'t already exist\nargs.logdir = os.path.join(args.logdir, args.expid)\nif not os.path.isdir(args.logdir):\n\tos.makedirs(args.logdir)\n\tprint(\'Created dir:\', args.logdir)\n\n# Log all commandline args\nwith open(os.path.join(args.logdir, \'args.txt\'), \'w\') as f:\n\tjson.dump(args.__dict__, f, indent=2)\n\nclass Engine(object):\n\n\n\tdef __init__(self,  cur_epoch=0, print_every=1, validate_every=1):\n\t\tself.cur_epoch = cur_epoch\n\t\tself.train_loss = []\n\t\tself.val_loss = []\n\t\tself.bestval = 1000.\n\n\tdef train(self):\n\t\tloss_epoch = 0.\n\t\tnum_batches = 0\n\n\t\tmodel.train()\n\t\t# Train loop\n\t\tfor i, data in enumerate(tqdm(dataloader_train), 0):\n\t\t\toptimizer.zero_grad()\n\t\t\t\n\t\t\t# data creation\n\t\t\ttgt_points = data[\'points\'].cuda()\n\t\t\tinp_images = data[\'imgs\'].cuda()\n\t\t\timage_gt = inp_images.permute(0,2,3,1)[:,:,:,:3]\n\t\t\talhpa_gt = inp_images.permute(0,2,3,1)[:,:,:,3:]\n\t\t\tcam_mat = data[\'cam_mat\'].cuda()\n\t\t\tcam_pos = data[\'cam_pos\'].cuda()\n\n\t\t\t# set viewing parameters \n\t\t\trenderer.camera_params = [cam_mat, cam_pos, cam_proj]\n\t\t\n\t\t\t# predict mesh properties\n\t\t\tdelta_verts, colours = model(inp_images)\n\t\t\tpred_verts = initial_verts + delta_verts\n\t\t\n\t\t\t# render image\n\t\t\timage_pred, alpha_pred, face_norms = renderer.forward(points=[(pred_verts*.57), mesh.faces], colors=[colours])\n\t\t\t\n\t\t\t# colour loss\n\t\t\timg_loss = ((image_pred - image_gt)**2).mean()\n\n\t\t\t# alpha loss \n\t\t\talpha_loss = ((alpha_pred - alhpa_gt)**2).mean()\n\n\t\t\t# mesh loss\n\t\t\tlap_loss = 0.\n\t\t\tflat_loss = 0.\n\t\t\tfor verts, tgt, norms in zip(pred_verts, tgt_points, face_norms): \t\n\t\t\t\tlap_loss += .1*loss_lap(mesh) / float(args.batchsize)\n\t\t\t\tflat_loss += .0001 *loss_flat(mesh, norms) / float(args.batchsize)\n\n\n\n\t\t\tloss =  img_loss + alpha_loss + lap_loss + flat_loss \n\t\t\tloss.backward()\n\t\t\tloss_epoch += float(loss.item())\n\n\t\t\t# logging\n\t\t\tnum_batches += 1\n\t\t\tif i % args.print_every == 0:\n\t\t\t\tmessage = f\'[TRAIN] Epoch {self.cur_epoch:03d}, Batch {i:03d}:, Img: {(img_loss.item()):4.3f}, \'\n\t\t\t\tmessage = message + f\' Alpha: {(alpha_loss.item()):3.3f}\'\n\t\t\t\tmessage = message + f\' Flat: {(flat_loss.item()):3.3f}, Lap: {(lap_loss.item()):3.3f} \'\n\t\t\t\t\n\t\t\t\ttqdm.write(message)\n\t\t\toptimizer.step()\n\t\t\n\t\t\n\t\tloss_epoch = loss_epoch / num_batches\n\t\tself.train_loss.append(loss_epoch)\n\t\tself.cur_epoch += 1\n\n\t\t\n\t\t\n\tdef validate(self):\n\t\tmodel.eval()\n\t\twith torch.no_grad():\t\n\t\t\tnum_batches = 0\n\t\t\tloss_epoch = 0.\n\n\t\t\t# Validation loop\n\t\t\tfor i, data in enumerate(tqdm(dataloader_val), 0):\n\n\t\t\t\t# data creation\n\t\t\t\tinp_images = data[\'imgs\'].cuda()\n\t\t\t\timage_gt = inp_images.permute(0,2,3,1)\n\t\t\t\tcam_mat = data[\'cam_mat\'].cuda()\n\t\t\t\tcam_pos = data[\'cam_pos\'].cuda()\n\n\n\t\t\t\t# set viewing parameters \n\t\t\t\trenderer.camera_params = [cam_mat, cam_pos, cam_proj]\n\t\t\t\n\t\t\t\t# predict mesh properties\n\t\t\t\tdelta_verts, colours = model(inp_images)\n\t\t\t\tpred_verts = initial_verts + delta_verts\n\t\t\t\n\t\t\t\t# render image\n\t\t\t\timage_pred, alpha_pred, _ = renderer.forward(points=[(pred_verts*.57 ), mesh.faces], colors=[colours])\n\t\t\t\t\n\t\t\t\tfull_pred = torch.cat((image_pred, alpha_pred), dim = -1)\n\t\n\t\t\t\t# colour loss\n\t\t\t\timg_loss = ((full_pred - image_gt)**2).mean()\n\t\t\t\tloss_epoch += float(img_loss.item())\n\n\t\t\t\t# logging\n\t\t\t\tnum_batches += 1\n\t\t\t\tif i % args.print_every == 0:\n\t\t\t\t\tout_loss = loss_epoch / float(num_batches)\n\t\t\t\t\tmessage = f\'[VAL] Epoch {self.cur_epoch:03d}, Batch {i:03d}:, loss: {(out_loss):4.3f}\'\n\t\t\t\t\ttqdm.write(message)\n\t\t\t\t\t\t\n\t\t\tout_loss = loss_epoch / float(num_batches)\n\t\t\ttqdm.write(f\'[VAL Total] Epoch {self.cur_epoch:03d}, Batch {i:03d}: loss: {out_loss:4.5f}\')\n\n\t\t\tself.val_loss.append(out_loss)\n\n\tdef save(self):\n\n\t\tsave_best = False\n\t\tif self.val_loss[-1] <= self.bestval:\n\t\t\tself.bestval = self.val_loss[-1]\n\t\t\tsave_best = True\n\t\t\n\t\t# Create a dictionary of all data to save\n\t\tlog_table = {\n\t\t\t\'epoch\': self.cur_epoch,\n\t\t\t\'bestval\': self.bestval,\n\t\t\t\'train_loss\': self.train_loss,\n\t\t\t\'val_loss\': self.val_loss\n\t\t}\n\n\t\t# Save the recent model/optimizer states\n\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, \'recent.pth\'))\n\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, \'recent_optim.pth\'))\n\t\t# Log other data corresponding to the recent model\n\t\twith open(os.path.join(args.logdir, \'recent.log\'), \'w\') as f:\n\t\t\tf.write(json.dumps(log_table))\n\n\t\ttqdm.write(\'====== Saved recent model ======>\')\n\t\t\n\t\tif save_best:\n\t\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, \'best.pth\'))\n\t\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, \'best_optim.pth\'))\n\t\t\ttqdm.write(\'====== Overwrote best model ======>\')\n\t\t\t\n\t\ntrainer = Engine()\n\nfor epoch in range(args.epochs): \n\ttrainer.train()\n\tif epoch %4 == 0:\n\t\ttrainer.validate()\n\t\ttrainer.save()'"
examples/ImageRecon/Dib/utils.py,7,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch \nimport numpy as np\nfrom torchvision import transforms\nfrom torchvision.transforms import Normalize as norm \n\nimport kaolin as kal\n\n\n\npreprocess = transforms.Compose([\n   transforms.CenterCrop(137),\n   transforms.ToTensor()\n])\n\n\ndef loss_lap(mesh): \n\n\tnew_lap = torch.matmul(mesh.adj, mesh.vertices)\n\tloss = 0.01 * torch.mean((new_lap - mesh.vertices) ** 2) * mesh.vertices.shape[0] * 3\n\treturn loss \n\ndef loss_flat(mesh, norms): \n\tloss  = 0.\n\tfor i in range(3): \n\n\t\tnorm1 = norms\n\t\tnorm2 = norms[mesh.ff[:, i]]\n\t\tcos = torch.sum(norm1 * norm2, dim=1)\n\t\tloss += torch.mean((cos - 1) ** 2) \n\tloss *= (mesh.faces.shape[0]/2.)\n\treturn loss\n\ndef collate_fn(data): \n\tnew_data = {}\n\tfor k in data[0].keys():\n\t\t\n\t\tif k in [\'points\',\'norms\', \'imgs\', \'cam_mat\', \'cam_pos\', \'sdf_points\']:\n\t\t\tnew_info = tuple(d[k] for d in data)\n\t\t\tnew_info = torch.stack(new_info, 0)\n\t\telif k in [\'adj\']: \n\t\t\t\n\t\t\tadj_values = tuple(d[k].coalesce().values() for d in data)\n\t\t\tadj_indices = tuple(d[k].coalesce().indices() for d in data)\n\t\t\tnew_data[\'adj_values\'] = adj_values\n\t\t\tnew_data[\'adj_indices\'] = adj_indices\n\t\t\t\n\t\telse: \n\t\t\tnew_info = tuple(d[k] for d in data)\n\n\t\tnew_data[k] = new_info\n\treturn new_data\n\ndef normalize_adj(mesh): \n\tadj = mesh.compute_adjacency_matrix_full()\n\teye = torch.FloatTensor(np.eye(adj.shape[0])).to(adj.device)\n\tadj = adj - eye\n\tnei_count = torch.sum(adj, dim=1)\n\tadj /= nei_count\n\tmesh.adj = adj \n\n'"
examples/ImageRecon/GEOmetrics/eval.py,6,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport argparse\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n\nfrom PIL import Image\n\nimport kaolin as kal \nfrom kaolin.datasets import shapenet\nfrom kaolin.models.VGG18 import VGG18 as Encoder\nfrom kaolin.models.GraphResNet import GraphResNet\n\nfrom utils import preprocess, pooling, get_pooling_index\nfrom utils import setup_meshes, split_meshes, reset_meshes\nfrom utils import collate_fn\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--shapenet-root\', type=str, help=\'Root directory of the ShapeNet dataset.\')\nparser.add_argument(\'--shapenet-images-root\', type=str, help=\'Root directory of the ShapeNet Rendering dataset.\')\nparser.add_argument(\'--cache-dir\', type=str, default=\'cache\', help=\'Path to write intermediate representation to.\')\nparser.add_argument(\'--expid\', type=str, default=\'GEOmetrics_1\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'--device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'--categories\', type=str, nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'--no-vis\', action=\'store_true\', help=\'Turn off visualization of each model while evaluating\')\nparser.add_argument(\'--f-score\', action=\'store_true\', help=\'compute F-score\')\nparser.add_argument(\'--batch-size\', type=int, default=1, help=\'Batch size.\')\nparser.add_argument(\'--logdir\', type=str, default=\'log\', help=\'Directory where log data was saved to.\')\nargs = parser.parse_args()\n\n\n# Data\npoints_set_valid = shapenet.ShapeNet_Points(root=args.shapenet_root, cache_dir=args.cache_dir, categories=args.categories,\n                                            train=False, split=.7, num_points=5000)\nimages_set_valid = shapenet.ShapeNet_Images(root=args.shapenet_images_root, categories=args.categories,\n                                            train=False, split=.7, views=1, transform=preprocess)\nmeshes_set_valid = shapenet.ShapeNet_Meshes(root=args.shapenet_root, categories=args.categories,\n                                            train=False, split=.7)\nvalid_set = shapenet.ShapeNet_Combination([points_set_valid, images_set_valid, meshes_set_valid])\n\ndataloader_val = DataLoader(valid_set, batch_size=args.batch_size, collate_fn=collate_fn, shuffle=False, \n                            num_workers=8)\n\n# Model\nmeshes = setup_meshes(filename=\'meshes/386.obj\', device=args.device)\n\nencoders = [Encoder().to(args.device) for i in range(3)]\nmesh_update_kernels = [963, 1091, 1091] \nmesh_updates = [GraphResNet(mesh_update_kernels[i], hidden=128, output_features=3).to(args.device) for i in range(3)]\n\nlogdir = os.path.join(args.logdir, args.expid)\n\n# Load saved weights\ncheckpoint = torch.load(os.path.join(logdir, \'best.ckpt\'))\nfor i, e in enumerate(encoders):\n    e.load_state_dict(checkpoint[\'encoders\'][i])\n    e.eval()\nfor i, m in enumerate(mesh_updates):\n    m.load_state_dict(checkpoint[\'mesh_updates\'][i])\n    m.eval()\n\nencoding_dims = [56, 28, 14, 7]\n\nloss_epoch = 0.\nf_epoch = 0.\nnum_batches = 0\nnum_items = 0\n\nwith torch.no_grad():\n    for sample in tqdm(valid_set):\n        data = sample[\'data\']\n        # data creation\n        tgt_points = data[\'points\'].to(args.device)\n        inp_images = data[\'images\'].to(args.device).unsqueeze(0)\n        cam_mat = data[\'params\'][\'cam_mat\'].to(args.device)\n        cam_pos = data[\'params\'][\'cam_pos\'].to(args.device)\n        tgt_verts = data[\'vertices\'].to(args.device)\n        tgt_faces = data[\'faces\'].to(args.device)\n\n        # Inference\n        img_features = [e(inp_images) for e in encoders]\n\n        reset_meshes(meshes)\n        # Layer_1\n        pool_indices = get_pooling_index(meshes[\'init\'][0].vertices, cam_mat, cam_pos, encoding_dims)\n        projected_image_features = pooling(img_features[0], pool_indices, 0)\n        full_vert_features = torch.cat((meshes[\'init\'][0].vertices, projected_image_features), dim=1)\n\n        delta, future_features = mesh_updates[0](full_vert_features, meshes[\'adjs\'][0])\n        meshes[\'update\'][0].vertices = (meshes[\'init\'][0].vertices + delta.clone())\n        future_features = split_meshes(meshes, future_features, 0)\t\t\t\n\n        # Layer_2\n        pool_indices = get_pooling_index(meshes[\'init\'][1].vertices, cam_mat, cam_pos, encoding_dims)\n        projected_image_features = pooling(img_features[1], pool_indices, 0)\n        full_vert_features = torch.cat((meshes[\'init\'][1].vertices, projected_image_features, future_features), dim=1)\n\n        delta, future_features = mesh_updates[1](full_vert_features, meshes[\'adjs\'][1])\n        meshes[\'update\'][1].vertices = (meshes[\'init\'][1].vertices + delta.clone())\n        future_features = split_meshes(meshes, future_features, 1)\t\n\n        # Layer_3\n        pool_indices = get_pooling_index(meshes[\'init\'][2].vertices, cam_mat, cam_pos, encoding_dims)\n        projected_image_features = pooling(img_features[2], pool_indices, 0)\n        full_vert_features = torch.cat((meshes[\'init\'][2].vertices, projected_image_features, future_features), dim=1)\n\n        delta, future_features = mesh_updates[2](full_vert_features, meshes[\'adjs\'][2])\n        meshes[\'update\'][2].vertices = (meshes[\'init\'][2].vertices + delta.clone())\n\n        pred_points, _ = meshes[\'update\'][2].sample(5000)\n\n        loss = 3000 * kal.metrics.point.chamfer_distance(pred_points, tgt_points)\n\n        if not args.no_vis: \n            tgt_mesh = kal.rep.TriangleMesh.from_tensors(tgt_verts, tgt_faces)\n\n            print(\'Displaying input image\')\n            img = inp_images[0].data.cpu().numpy().transpose((1, 2, 0))\n            img = (img * 255.).astype(np.uint8)\n            Image.fromarray(img).show()\n            print(\'Rendering Target Mesh\')\n            kal.visualize.show_mesh(tgt_mesh)\n            print(\'Rendering Predicted Mesh\')\n            kal.visualize.show_mesh(meshes[\'update\'][2])\n            print(\'----------------------\')\n            num_items += 1\n\n        if args.f_score:\n            # Compute f score\n            f_score = kal.metrics.point.f_score(tgt_points, pred_points, extend=False)\n            f_epoch += f_score.item()\n\n        loss_epoch += loss.item()\n\n        num_batches += 1.\n\nout_loss = loss_epoch / num_batches\nprint(f\'Loss over validation set is {out_loss}\')\nif args.f_score: \n    out_f = f_epoch / num_batches\n    print(f\'F-score over validation set is {out_f}\')\n'"
examples/ImageRecon/GEOmetrics/eval_auto_encoder.py,2,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport argparse\nimport torch\nfrom tqdm import tqdm\n\nimport kaolin as kal\nfrom kaolin.models.GEOMetrics import VoxelDecoder\nfrom kaolin.models.MeshEncoder import MeshEncoder\nfrom kaolin.datasets import shapenet\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--shapenet-root\', type=str, help=\'Root directory of the ShapeNet dataset.\')\nparser.add_argument(\'--cache-dir\', type=str, default=\'cache\', help=\'Path to write intermediate representation to.\')\nparser.add_argument(\'--expid\', type=str, default=\'GEOmetrics_1\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'--device\', type=str, default=\'cuda\', help=\'Device to use.\')\nparser.add_argument(\'--categories\', type=str, nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use.\')\nparser.add_argument(\'--no-vis\', action=\'store_true\', help=\'Disable visualization of each model.\')\nargs = parser.parse_args()\n\n\n# Data\nmesh_set = shapenet.ShapeNet_Surface_Meshes(root=args.shapenet_root, cache_dir=args.cache_dir, categories=args.categories,\n                                            resolution=32, train=False, split=.7, mode=\'Tri\')\nvoxel_set = shapenet.ShapeNet_Voxels(root=args.shapenet_root, cache_dir=args.cache_dir, categories=args.categories,\n                                     train=False, resolutions=[32], split=.7)\nvalid_set = shapenet.ShapeNet_Combination([mesh_set, voxel_set])\n\n\nencoder = MeshEncoder(30).to(args.device)\ndecoder = VoxelDecoder(30).to(args.device)\n\nlogdir = f\'log/{args.expid}/AutoEncoder\'\ncheckpoint = torch.load(os.path.join(logdir, \'best.ckpt\'))\nencoder.load_state_dict(checkpoint[\'encoder\'])\ndecoder.load_state_dict(checkpoint[\'decoder\'])\n\nloss_epoch = 0.\nnum_batches = 0\nnum_items = 0\n\nencoder.eval(), decoder.eval()\nwith torch.no_grad():\n    for sample in tqdm(valid_set):\n        data = sample[\'data\']\n\n        tgt_voxels = data[\'32\'].to(args.device)\n        inp_verts = data[\'vertices\'].to(args.device)\n        inp_faces = data[\'faces\'].to(args.device)\n        inp_adj = data[\'adj\'].to(args.device)\n\n        # Inference\n        latent_encoding = encoder(inp_verts, inp_adj).unsqueeze(0)\n        pred_voxels = decoder(latent_encoding)[0]\n\n        # Losses\n        iou = kal.metrics.voxel.iou(pred_voxels.contiguous(), tgt_voxels.contiguous())\n\n        if not args.no_vis: \n            tgt_mesh = kal.rep.TriangleMesh.from_tensors(inp_verts, inp_faces)\n            print(\'Rendering Input Mesh\')\n            tgt_mesh.show()\n            print(\'Rendering Target Voxels\')\n            kal.visualize.show_voxelgrid(tgt_voxels, mode=\'exact\', thresh=.5)\n            print(\'Rendering Predicted Voxels\')\n            kal.visualize.show_voxelgrid(pred_voxels, mode=\'exact\', thresh=.5)\n            print(\'----------------------\')\n            num_items += 1\n\n        loss_epoch += iou.item()\n        num_batches += 1.\n\nout_loss = loss_epoch / float(num_batches)\nprint(f\'IoU over validation set is {out_loss}\')\n'"
examples/ImageRecon/GEOmetrics/train.py,15,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport json\nimport os\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nimport kaolin as kal\nfrom kaolin.datasets import shapenet\nfrom kaolin.models.VGG18 import VGG18 as Encoder\nfrom kaolin.models.GraphResNet import GraphResNet\nfrom kaolin.models.MeshEncoder import MeshEncoder\n\nfrom utils import preprocess, pooling, get_pooling_index\nfrom utils import setup_meshes, split_meshes, reset_meshes\nfrom utils import loss_surf, loss_edge, loss_lap , collate_fn\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--shapenet-root\', type=str, help=\'Root directory of the ShapeNet dataset.\')\nparser.add_argument(\'--shapenet-images-root\', type=str, help=\'Root directory of the ShapeNet Rendering dataset.\')\nparser.add_argument(\'--cache-dir\', type=str, default=\'cache\', help=\'Path to write intermediate representation to.\')\nparser.add_argument(\'--expid\', type=str, default=\'GEOmetrics_1\', help=\'Unique experiment identifier. If using latent-loss, \'\n                    \'must be the same expid used during auto-encoder training.\')\nparser.add_argument(\'--device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'--categories\', type=str, nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'--epochs\', type=int, default=600, help=\'Number of train epochs.\')\nparser.add_argument(\'-lr\', \'--learning-rate\', type=float, default=1e-4, help=\'Learning rate.\')\nparser.add_argument(\'--batch-size\', type=int, default=5, help=\'batch size\')\nparser.add_argument(\'--val-every\', type=int, default=5, help=\'Validation frequency (epochs).\')\nparser.add_argument(\'--print-every\', type=int, default=20, help=\'Print frequency (batches).\')\nparser.add_argument(\'--latent-loss\', action=\'store_true\', help=\'indicates latent loss should be used\')\nparser.add_argument(\'--logdir\', type=str, default=\'log\', help=\'Directory to log data to.\')\nparser.add_argument(\'--resume\', choices=[\'best\', \'recent\'], default=None,\n                    help=\'Choose which weights to resume training from (None to start from random initialization.)\')\nargs = parser.parse_args()\n\n\n# Setup Dataset\npoints_set = shapenet.ShapeNet_Points(root=args.shapenet_root, cache_dir=args.cache_dir, categories=args.categories,\n                                      train=True, split=.7, num_points=3000)\nimages_set = shapenet.ShapeNet_Images(root=args.shapenet_images_root, categories=args.categories,\n                                      train=True, split=.7, views=23, transform=preprocess)\nif args.latent_loss:\n    mesh_set = shapenet.ShapeNet_Surface_Meshes(root=args.shapenet_root, cache_dir=args.cache_dir, categories=args.categories,\n                                                resolution=100, train=True, split=.7, mode=\'Tri\')\n    train_set = shapenet.ShapeNet_Combination([points_set, images_set, mesh_set])\n    dataloader_train = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn,\n                                  num_workers=8)\nelse: \n    train_set = shapenet.ShapeNet_Combination([points_set, images_set])\n    dataloader_train = DataLoader(train_set, batch_size=args.batch_size, shuffle=True,\n                                  num_workers=8)\n\n\npoints_set_valid = shapenet.ShapeNet_Points(root=args.shapenet_root, cache_dir=args.cache_dir, categories=args.categories,\n                                            train=False, split=.7, num_points=10000)\nimages_set_valid = shapenet.ShapeNet_Images(root=args.shapenet_images_root, categories=args.categories,\n                                            train=False, split=.7, views=1, transform=preprocess)\nvalid_set = shapenet.ShapeNet_Combination([points_set_valid, images_set_valid])\n\ndataloader_val = DataLoader(valid_set, batch_size=args.batch_size, shuffle=False, num_workers=8)\n\n\n# Setup models\nmeshes = setup_meshes(filename=\'meshes/386.obj\', device=args.device)\n\nencoders = [Encoder().to(args.device) for i in range(3)]\nmesh_update_kernels = [963, 1091, 1091] \nmesh_updates = [GraphResNet(mesh_update_kernels[i], hidden=128, output_features=3).to(args.device) for i in range(3)]\nif args.latent_loss:\n    mesh_encoder = MeshEncoder(30).to(args.device)\n    mesh_encoder.load_state_dict(torch.load(os.path.join(args.logdir, args.expid, \'AutoEncoder/best_encoder.pth\')))\n\nparameters = []\n\nfor i in range(3):\n    parameters += list(encoders[i].parameters()) \n    parameters += list(mesh_updates[i].parameters())\noptimizer = optim.Adam(parameters, lr=args.learning_rate)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[200], gamma=0.1)\n\nencoding_dims = [56, 28, 14, 7]\n\n\n# Create log directory, if it doesn\'t already exist\nlogdir = os.path.join(args.logdir, args.expid)\nif not os.path.isdir(logdir):\n    os.makedirs(logdir)\n    print(\'Created dir:\', logdir)\n\n\n# Log all commandline args\nwith open(os.path.join(logdir, \'args.txt\'), \'w\') as f:\n    json.dump(args.__dict__, f, indent=2)\n\n\n# Loss weights\nweights = {\n    \'surface\': 6000,\n    \'edge\': 180,\n    \'laplace\': 1500,\n    \'latent\': 0.2,\n}\n# Other model parameters\nANGLE_THRESHOLD = 130   # angle at which a face is split at the end of each module\n\nclass Engine(object):\n    """"""Engine that runs training and inference.\n    Args\n        - print_every (int): How frequently (# batches) to print loss.\n        - resume_name (str): Prefix of weights from which to resume training. If None,\n            no weights are loaded.\n    """"""\n\n    def __init__(self, print_every, resume_name=None):\n        self.cur_epoch = 0\n        self.train_loss = {}\n        self.val_score = {}\n        self.bestval = 0\n        self.print_every = print_every\n\n        if resume_name:\n            self.load(resume_name)\n\n    def train(self):\n        loss_epoch = 0.\n        num_batches = 0\n\n        [e.train() for e in encoders], [m.train() for m in mesh_updates]\n\n        # Train loop\n        for i, sample in enumerate(tqdm(dataloader_train), 0):\n            data = sample[\'data\']\n            optimizer.zero_grad()\n\n            # Data Creation\n            tgt_points = data[\'points\'].to(args.device)\n            inp_images = data[\'images\'].to(args.device)\n            cam_mat = data[\'params\'][\'cam_mat\'].to(args.device)\n            cam_pos = data[\'params\'][\'cam_pos\'].to(args.device)\n            if (tgt_points.shape[0] != args.batch_size) and (inp_images.shape[0] != args.batch_size) \\\n                    and (cam_mat.shape[0] != args.batch_size) and (cam_pos.shape[0] != args.batch_size): \n                continue\n            surf_loss, edge_loss, lap_loss, latent_loss, loss, f_score = 0, 0, 0, 0, 0, 0\n\n            # Inference\n            img_features = [e(inp_images) for e in encoders]\n            for bn in range(args.batch_size):\n                reset_meshes(meshes)\n\n                # Layer_1\n                pool_indices = get_pooling_index(meshes[\'init\'][0].vertices, cam_mat[bn], cam_pos[bn], encoding_dims)\n                projected_image_features = pooling(img_features[0], pool_indices, bn)\n                full_vert_features = torch.cat((meshes[\'init\'][0].vertices, projected_image_features), dim=1)\n\n                delta, future_features = mesh_updates[0](full_vert_features, meshes[\'adjs\'][0])\n                meshes[\'update\'][0].vertices = (meshes[\'init\'][0].vertices + delta.clone())\n                future_features = split_meshes(meshes, future_features, 0, angle=ANGLE_THRESHOLD)\t\t\t\n\n                # Layer_2\n                pool_indices = get_pooling_index(meshes[\'init\'][1].vertices, cam_mat[bn], cam_pos[bn], encoding_dims)\n                projected_image_features = pooling(img_features[1], pool_indices, bn)\n                full_vert_features = torch.cat((meshes[\'init\'][1].vertices, projected_image_features, future_features), dim=1)\n\n                delta, future_features = mesh_updates[1](full_vert_features, meshes[\'adjs\'][1])\n                meshes[\'update\'][1].vertices = (meshes[\'init\'][1].vertices + delta.clone())\n                future_features = split_meshes(meshes, future_features, 1, angle=ANGLE_THRESHOLD)\t\n\n                # Layer_3\n                pool_indices = get_pooling_index(meshes[\'init\'][2].vertices, cam_mat[bn], cam_pos[bn], encoding_dims)\n                projected_image_features = pooling(img_features[2], pool_indices, bn)\n                full_vert_features = torch.cat((meshes[\'init\'][2].vertices, projected_image_features, future_features), dim=1)\n                delta, future_features = mesh_updates[2](full_vert_features, meshes[\'adjs\'][2])\n                meshes[\'update\'][2].vertices = (meshes[\'init\'][2].vertices + delta.clone())\n\n                if args.latent_loss:\n                    inds = data[\'adj\'][\'indices\'][bn]\n                    vals = data[\'adj\'][\'values\'][bn]\n                    gt_verts = data[\'vertices\'][bn].to(args.device)\n                    vert_len = gt_verts.shape[0]\n                    gt_adj = torch.sparse.FloatTensor(inds, vals, torch.Size([vert_len, vert_len])).to(args.device)\n\n                    predicted_latent = mesh_encoder(meshes[\'update\'][2].vertices, meshes[\'adjs\'][2])  \n                    gt_latent = mesh_encoder(gt_verts, gt_adj)  \n                    latent_loss += weights[\'latent\'] * torch.mean(torch.abs(predicted_latent - gt_latent)) / args.batch_size\n\n\n                # Losses\n                surf_loss += weights[\'surface\'] * loss_surf(meshes, tgt_points[bn]) / args.batch_size\n                edge_loss += weights[\'edge\'] * loss_edge(meshes) / args.batch_size\n                lap_loss += weights[\'laplace\'] * loss_lap(meshes) / args.batch_size\n\n                # F-Score\n                f_score += kal.metrics.point.f_score(.57 * tgt_points[bn], .57 * meshes[\'update\'][2].sample(2466)[0],\n                                                     extend=False) / args.batch_size\n\n\n                loss = surf_loss + edge_loss + lap_loss\n                if args.latent_loss: \n                    loss += latent_loss\n            loss.backward()\n            loss_epoch += float(loss.item())\n\n            # logging\n            num_batches += 1\n            if i % args.print_every == 0:\n                message = f\'[TRAIN]\\tEpoch {self.cur_epoch:03d}, Batch {i:03d} | Total Loss: {loss.item():4.3f} \'\n                message += f\'Surf: {(surf_loss.item()):3.3f}, Lap: {(lap_loss.item()):3.3f}, \'\n                message += f\'Edge: {(edge_loss.item()):3.3f}\'\n                if args.latent_loss: \n                    message = message + f\', Latent: {(latent_loss.item()):3.3f}\'\n                message = message + f\', F-score: {(f_score.item()):3.3f}\'\n                tqdm.write(message)\n\n            optimizer.step()\n\n        loss_epoch = loss_epoch / num_batches\n        self.train_loss[self.cur_epoch] = loss_epoch\n\n    def validate(self):\n        [e.eval() for e in encoders], [m.eval() for m in mesh_updates]\n        with torch.no_grad():\t\n            num_batches = 0\n            loss_epoch = 0.\n            f_score = 0 \n            # Validation loop\n            for i, sample in enumerate(tqdm(dataloader_val), 0):\n                data = sample[\'data\']\n                optimizer.zero_grad()\n\n                # Data Creation\n                tgt_points = data[\'points\'].to(args.device)\n                inp_images = data[\'images\'].to(args.device)\n                cam_mat = data[\'params\'][\'cam_mat\'].to(args.device)\n                cam_pos = data[\'params\'][\'cam_pos\'].to(args.device)\n                if (tgt_points.shape[0] != args.batch_size) and (inp_images.shape[0] != args.batch_size)  \\\n                        and (cam_mat.shape[0] != args.batch_size) and (cam_pos.shape[0] != args.batch_size): \n                    continue\n                surf_loss = 0\n\n                # Inference\n                img_features = [e(inp_images) for e in encoders]\n                for bn in range(args.batch_size):\n                    reset_meshes(meshes)\n\n                    # Layer_1\n                    pool_indices = get_pooling_index(meshes[\'init\'][0].vertices, cam_mat[bn], cam_pos[bn], encoding_dims)\n                    projected_image_features = pooling(img_features[0], pool_indices, bn)\n                    full_vert_features = torch.cat((meshes[\'init\'][0].vertices, projected_image_features), dim=1)\n\n                    delta, future_features = mesh_updates[0](full_vert_features, meshes[\'adjs\'][0])\n                    meshes[\'update\'][0].vertices = (meshes[\'init\'][0].vertices + delta.clone())\n                    future_features = split_meshes(meshes, future_features, 0, angle=ANGLE_THRESHOLD)\t\t\t\n\n                    # Layer_2\n                    pool_indices = get_pooling_index(meshes[\'init\'][1].vertices, cam_mat[bn], cam_pos[bn], encoding_dims)\n                    projected_image_features = pooling(img_features[1], pool_indices, bn)\n                    full_vert_features = torch.cat((meshes[\'init\'][1].vertices, projected_image_features, future_features), dim=1)\n\n                    delta, future_features = mesh_updates[1](full_vert_features, meshes[\'adjs\'][1])\n                    meshes[\'update\'][1].vertices = (meshes[\'init\'][1].vertices + delta.clone())\n                    future_features = split_meshes(meshes, future_features, 1, angle=ANGLE_THRESHOLD)\t\n\n                    # Layer_3\n                    pool_indices = get_pooling_index(meshes[\'init\'][2].vertices, cam_mat[bn], cam_pos[bn], encoding_dims)\n                    projected_image_features = pooling(img_features[2], pool_indices, bn)\n                    full_vert_features = torch.cat((meshes[\'init\'][2].vertices, projected_image_features, future_features), dim=1)\n\n                    delta, future_features = mesh_updates[2](full_vert_features, meshes[\'adjs\'][2])\n                    meshes[\'update\'][2].vertices = (meshes[\'init\'][2].vertices + delta.clone())\n                    pred_points, _ = meshes[\'update\'][2].sample(10000)\n\n                    # Losses\n                    surf_loss = weights[\'surface\'] * kal.metrics.point.chamfer_distance(pred_points, tgt_points[bn])\n\n                    # F-Score\n                    f_score += (kal.metrics.point.f_score(.57 * meshes[\'update\'][2].sample(2466)[0], .57 * tgt_points[bn],\n                                                          extend=False).item() / args.batch_size)\n\n                    loss_epoch += surf_loss.item() / args.batch_size\n\n                # logging\n                num_batches += 1\n                if i % args.print_every == 0:\n                    out_loss = loss_epoch / num_batches\n                    out_f_score = f_score / num_batches\n                    tqdm.write(f\'[VAL]\\tEpoch {self.cur_epoch:03d}, Batch {i:03d}: F-Score: {out_f_score:3.3f}\')\n\n            out_loss = loss_epoch / num_batches\n            out_f_score = f_score / num_batches\n            tqdm.write(f\'[VAL Total] Epoch {self.cur_epoch:03d}, Batch {i:03d}: F-Score: {out_f_score:3.3f}\')\n\n            self.val_score[self.cur_epoch] = out_f_score\n\n    def step(self):\n        self.cur_epoch += 1\n\n    def load(self, resume_name):\n        checkpoint = torch.load(os.path.join(logdir, f\'{resume_name}.ckpt\'))\n        for i, e in enumerate(encoders):\n            e.load_state_dict(checkpoint[\'encoders\'][i])\n        for i, m in enumerate(mesh_updates):\n            m.load_state_dict(checkpoint[\'mesh_updates\'][i])\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        scheduler.load_state_dict(checkpoint[\'scheduler\'])\n\n        # Read data corresponding to the loaded model\n        with open(os.path.join(logdir, f\'{resume_name}.log\'), \'r\') as f:\n            log = json.load(f)\n        self.cur_epoch = log[\'epoch\']\n        self.bestval = log[\'bestval\']\n        self.train_loss = log[\'train_loss\']\n        self.val_score = log[\'val_score\']\n\n        # step to next epoch\n        self.step()\n\n    def save(self):\n        # Save the recent model/optimizer states\n        self._save_checkpoint(\'recent\')\n\n        # Save the current model if it outperforms previous ones\n        if self.val_score.get(self.cur_epoch, 0) > self.bestval:\n            self.bestval = self.val_score[self.cur_epoch]\n            self._save_checkpoint(\'best\')\n\n    def _save_checkpoint(self, name):\n        # Save Checkpoint\n        checkpoint = {\n            \'encoders\': [e.state_dict() for e in encoders],\n            \'mesh_updates\': [m.state_dict() for m in mesh_updates],\n            \'optimizer\': optimizer.state_dict(),\n            \'scheduler\': scheduler.state_dict(),\n        }\n        torch.save(checkpoint, os.path.join(logdir, f\'{name}.ckpt\'))\n\n        # Log other data corresponding to the recent model\n        log_table = {\n            \'epoch\': self.cur_epoch,\n            \'bestval\': self.bestval,\n            \'train_loss\': self.train_loss,\n            \'val_score\': self.val_score,\n        } \n        with open(os.path.join(logdir, f\'{name}.log\'), \'w\') as f:\n            f.write(json.dumps(log_table, separators=(\',\', \':\'), indent=4))\n        tqdm.write(f\'====== Saved {name} checkpoint ======>\')\n\n\ntrainer = Engine(print_every=args.print_every, resume_name=args.resume)\n\nfor epoch in range(trainer.cur_epoch, args.epochs): \n    trainer.train()\n    if epoch % args.val_every == 0:\n        trainer.validate()\n    trainer.save()\n    trainer.step()\n    scheduler.step()\n'"
examples/ImageRecon/GEOmetrics/train_auto_encoder.py,9,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport json\nimport os\nimport torch\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport random\n\nimport kaolin as kal \nfrom kaolin.datasets import shapenet\nfrom kaolin.models.GEOMetrics import VoxelDecoder\nfrom kaolin.models.MeshEncoder import MeshEncoder\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--shapenet-root\', type=str, help=\'Root directory of the ShapeNet dataset.\')\nparser.add_argument(\'--cache-dir\', type=str, default=\'cache\', help=\'Path to write intermediate representation to.\')\nparser.add_argument(\'--expid\', type=str, default=\'GEOmetrics_1\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'--device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'--categories\', type=str, nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'--epochs\', type=int, default=100, help=\'Number of train epochs.\')\nparser.add_argument(\'-lr\', \'--learning-rate\', type=float, default=1e-4, help=\'Learning rate.\')\nparser.add_argument(\'--batch-size\', type=int, default=25, help=\'batch size\')\nparser.add_argument(\'--val-every\', type=int, default=5, help=\'Validation frequency (epochs).\')\nparser.add_argument(\'--print-every\', type=int, default=20, help=\'Print frequency (batches).\')\nparser.add_argument(\'--logdir\', type=str, default=\'log\', help=\'Directory to log data to.\')\nparser.add_argument(\'--resume\', choices=[\'best\', \'recent\'], default=None,\n                    help=\'Choose which weights to resume training from (none to start from random initialization.)\')\nargs = parser.parse_args()\n\n\n# Setup Datasets - Training\nmesh_set = shapenet.ShapeNet_Surface_Meshes(root=args.shapenet_root, cache_dir=args.cache_dir, categories=args.categories,\n                                            train=True, split=.7, mode=\'Tri\', resolution=32)\nvoxel_set = shapenet.ShapeNet_Voxels(root=args.shapenet_root, cache_dir=args.cache_dir, categories=args.categories,\n                                     train=True, split=.7, resolutions=[32])\n\ntrain_set = shapenet.ShapeNet_Combination([mesh_set, voxel_set])\n\n\n# Setup Datasets - Validation\nmesh_set = shapenet.ShapeNet_Surface_Meshes(root=args.shapenet_root, cache_dir=args.cache_dir, categories=args.categories,\n                                            train=False, split=.7, mode=\'Tri\', resolution=32)\nvoxel_set = shapenet.ShapeNet_Voxels(root=args.shapenet_root, cache_dir=args.cache_dir, categories=args.categories,\n                                     train=False, split=.7, resolutions=[32])\nvalid_set = shapenet.ShapeNet_Combination([mesh_set, voxel_set])\n\n\n# Setup Models\nencoder = MeshEncoder(30).to(args.device)\ndecoder = VoxelDecoder(30).to(args.device)\n\nparameters = list(encoder.parameters()) + list(decoder.parameters()) \noptimizer = optim.Adam(parameters, lr=args.learning_rate)\n\nloss_fn = torch.nn.MSELoss()\n\n\n# Create log directory, if it doesn\'t already exist\nlogdir = os.path.join(args.logdir, args.expid, \'AutoEncoder\')\nif not os.path.isdir(logdir):\n    os.makedirs(logdir)\n    print(\'Created dir:\', logdir)\n\n# Log all commandline args\nwith open(os.path.join(logdir, \'args.txt\'), \'w\') as f:\n    json.dump(args.__dict__, f, indent=2)\n\n\nclass Engine(object):\n    """"""Engine that runs training and inference.\n    Args\n        - print_every (int): How frequently (# batches) to print loss.\n        - resume_name (str, optional): Prefix of weights from which to resume training.\n            If None, no weights are loaded.\n    """"""\n\n    def __init__(self, print_every=1, resume_name=None):\n        self.cur_epoch = 0\n        self.train_loss = {}\n        self.val_score = {}\n        self.bestval = 0.\n        self.print_every = print_every\n\n        if resume_name:\n            self.load(resume_name)\n\n    def train(self):\n        loss_epoch = 0.\n        num_batches = 0\n        encoder.train(), decoder.train()\n\n        # Train loop\n        for i in tqdm(range(len(train_set) // args.batch_size)): \n            tgt_voxels = []\n            latent_encodings = []\n\n            # Can\'t use a dataloader due to the adj matrix\n            for j in range(args.batch_size):\n                optimizer.zero_grad()\n\n                # Data Creation\n                selection = random.randint(0, len(train_set) - 1)\n                tgt_voxels.append(train_set[selection][\'data\'][\'32\'].to(args.device).unsqueeze(0))\n                inp_verts = train_set[selection][\'data\'][\'vertices\'].to(args.device)\n                inp_adj = train_set[selection][\'data\'][\'adj\'].to(args.device)\n\n                # Inverence\n                latent_encodings.append(encoder(inp_verts, inp_adj).unsqueeze(0))\n\n            tgt_voxels = torch.cat(tgt_voxels)\n            latent_encodings = torch.cat(latent_encodings)\n            pred_voxels = decoder(latent_encodings)\n\n            # Loss\n            loss = loss_fn(pred_voxels, tgt_voxels)\n            loss.backward()\n            loss_epoch += float(loss.item())\n\n            # Logging\n            iou = kal.metrics.voxel.iou(pred_voxels.contiguous(), tgt_voxels)\n            num_batches += 1\n            if i % args.print_every == 0:\n                tqdm.write(f\'[TRAIN] Epoch {self.cur_epoch:03d}, Batch {i:03d}: Loss: {loss.item():.5f} \'\n                           f\'IoU: {iou:.4f}\')\n            optimizer.step()\n        loss_epoch = loss_epoch / num_batches\n        self.train_loss[self.cur_epoch] = loss_epoch\n\n    def validate(self):\n        encoder.eval(), decoder.eval()\n        with torch.no_grad():\t\n            num_batches = 0\n            iou_epoch = 0.\n\n            # Validation loop\n            for i in tqdm(range(len(valid_set) // args.batch_size)): \n                tgt_voxels = []\n                latent_encodings = []\n                for j in range(args.batch_size):\n                    optimizer.zero_grad()\n\n                    # Data Creation\n                    tgt_voxels.append(valid_set[i * args.batch_size + j][\'data\'][\'32\'].to(args.device).unsqueeze(0))\n                    inp_verts = valid_set[i * args.batch_size + j][\'data\'][\'vertices\'].to(args.device)\n                    inp_adj = valid_set[i * args.batch_size + j][\'data\'][\'adj\'].to(args.device)\n\n                    # Inference\n                    latent_encodings.append(encoder(inp_verts, inp_adj).unsqueeze(0))\n\n                tgt_voxels = torch.cat(tgt_voxels)\n                latent_encodings = torch.cat(latent_encodings)\n                pred_voxels = decoder(latent_encodings)\n\n                # Loss\n                iou = kal.metrics.voxel.iou(pred_voxels.contiguous(), tgt_voxels)\n                iou_epoch += iou\n\n                # logging\n                num_batches += 1\n                if i % args.print_every == 0:\n                    out_iou = iou_epoch.item() / float(num_batches)\n                    tqdm.write(f\'[VAL] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou:.4f}\')\n\n            out_iou = iou_epoch.item() / float(num_batches)\n            tqdm.write(f\'[VAL Total] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou:.4f}\')\n            self.val_score[self.cur_epoch] = out_iou\n\n    def step(self):\n        self.cur_epoch += 1\n\n    def load(self, resume_name):\n        checkpoint = torch.load(os.path.join(logdir, f\'{resume_name}.ckpt\'))\n        encoder.load_state_dict(checkpoint[\'encoder\'])\n        decoder.load_state_dict(checkpoint[\'decoder\'])\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n\n        # Read data corresponding to the loaded model\n        with open(os.path.join(logdir, f\'{resume_name}.log\'), \'r\') as f:\n            run_data = json.load(f)\n        self.cur_epoch = run_data[\'epoch\']\n        self.bestval = run_data[\'bestval\']\n        self.train_loss = run_data[\'train_loss\']\n        self.val_score = run_data[\'val_score\']\n\n        # step to next epoch\n        self.step()\n\n    def save(self):\n        self._save_checkpoint(\'recent\')\n\n        if self.val_score.get(self.cur_epoch, 0) > self.bestval:\n            self.bestval = self.val_score.get(self.cur_epoch, 0)\n            self._save_checkpoint(\'best\')\n\n    def _save_checkpoint(self, name):\n        # Save Checkpoint\n        checkpoint = {\n            \'encoder\': encoder.state_dict(),\n            \'decoder\': decoder.state_dict(),\n            \'optimizer\': optimizer.state_dict(),\n        }\n        torch.save(checkpoint, os.path.join(logdir, f\'{name}.ckpt\'))\n\n        # Log other data corresponding to the recent model\n        log_table = {\n            \'epoch\': self.cur_epoch,\n            \'bestval\': self.bestval,\n            \'train_loss\': self.train_loss,\n            \'val_score\': self.val_score,\n        }\n        with open(os.path.join(logdir, f\'{name}.log\'), \'w\') as f:\n            f.write(json.dumps(log_table, separators=(\',\', \':\'), indent=4))\n        tqdm.write(f\'====== Saved {name} checkpoint ======>\')\n\ntrainer = Engine(print_every=args.print_every, resume_name=args.resume)\n\nfor epoch in range(trainer.cur_epoch, args.epochs): \n    trainer.train()\n    if epoch % args.val_every == 0: \n        trainer.validate()\n    trainer.save()\n    trainer.step()\n'"
examples/ImageRecon/GEOmetrics/utils.py,80,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch \nfrom torchvision import transforms\nimport kaolin as kal\nfrom collections import defaultdict\nimport numpy as np\nfrom torch._six import container_abcs\n\nfrom kaolin.rep import TriangleMesh\n\n\npreprocess = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor()\n])\n\n\ndef collate_fn(batch):\n    elem = batch[0]\n    elem_module = type(batch[0]).__module__\n\n    if isinstance(elem, torch.Tensor):\n        if elem.is_sparse:\n            return {\n                \'values\': tuple(b.coalesce().values() for b in batch),\n                \'indices\': tuple(b.coalesce().indices() for b in batch),\n            }\n        elif all([list(d.size()) == list(batch[0].size()) for d in batch]):\n            return torch.stack(batch, 0)\n        else:\n            return batch\n    elif elem_module == \'numpy\':\n        return torch.tensor(np.stack(batch))\n    elif isinstance(elem, container_abcs.Mapping):\n        return {key: collate_fn([d[key] for d in batch]) for key in elem}\n    return batch\n\n\ndef get_pooling_index(positions, cam_mat, cam_pos, dims):\n    device = positions.device\n    # project points into 2D\n    positions = positions * .57  # accounting for recalling in 3Dr2n\n    positions = positions - cam_pos \n    positions = torch.mm(positions, cam_mat.permute(1, 0))\n    positions_xs = positions[:, 1] / positions[:, 2]\n    positions_ys = -positions[:, 0] / positions[:, 2] \n\n    # do bilinear interpolation over pixel coordinates\n    data_meta = defaultdict(list)\n\n    for dim in dims:\n        focal_length = 250. / 224. * dim\n        xs = positions_xs * focal_length + dim / 2.\n        ys = positions_ys * focal_length + dim / 2.\n\n        cur_xs = torch.clamp(xs , 0, dim - 1)\n        cur_ys = torch.clamp(ys , 0, dim - 1)\n\n        x1s, y1s, x2s, y2s = torch.floor(cur_xs), torch.floor(cur_ys), torch.ceil(cur_xs), torch.ceil(cur_ys)\n\n        A = x2s - cur_xs\n        B = cur_xs - x1s\n        G = y2s - cur_ys\n        H = cur_ys - y1s\n\n        y1s = y1s + torch.arange(positions.shape[0]).float().to(device) * dim \n        y2s = y2s + torch.arange(positions.shape[0]).float().to(device) * dim \n\n        data_meta[\'A\'].append(A.float().unsqueeze(0))\n        data_meta[\'B\'].append(B.float().unsqueeze(0))\n        data_meta[\'G\'].append(G.float().unsqueeze(0))\n        data_meta[\'H\'].append(H.float().unsqueeze(0))\n        data_meta[\'x1s\'].append(x1s.long().unsqueeze(0))\n        data_meta[\'x2s\'].append(x2s.long().unsqueeze(0))\n        data_meta[\'y1s\'].append(y1s.long().unsqueeze(0))\n        data_meta[\'y2s\'].append(y2s.long().unsqueeze(0))\n\n    for key in data_meta:\n        data_meta[key] = torch.cat(data_meta[key], dim=0)\n    return data_meta\n\n\ndef pooling(blocks, pooling_indices, index):\n    full_features = None \n    for i_block, block in enumerate(blocks):\n        block = block[index]\n        A = pooling_indices[\'A\'][i_block]\n        B = pooling_indices[\'B\'][i_block]\n        G = pooling_indices[\'G\'][i_block]\n        H = pooling_indices[\'H\'][i_block]\n\n        x1s = pooling_indices[\'x1s\'][i_block]\n        x2s = pooling_indices[\'x2s\'][i_block]\n        y1s = pooling_indices[\'y1s\'][i_block]\n        y2s = pooling_indices[\'y2s\'][i_block]\n\n\n        C = torch.index_select(block, 1, x1s).view(block.shape[0], -1)\n        C = torch.index_select(C, 1, y1s)\n        D = torch.index_select(block, 1, x1s).view(block.shape[0], -1)\n        D = torch.index_select(D, 1, y2s)\n        E = torch.index_select(block, 1, x2s).view(block.shape[0], -1)\n        E = torch.index_select(E, 1, y1s)\n        F = torch.index_select(block, 1, x2s).view(block.shape[0], -1)\n        F = torch.index_select(F, 1, y2s)\n\n        features = ((A * C * G) + (H * D * A) + (G * E * B) + (B * F * H)).permute(1, 0)\n\n        full_features = features if full_features is None else torch.cat((full_features, features), dim=1)\n\n    return full_features\n\n\nnorm_distance = kal.metrics.point.SidedDistance()\n\ndef chamfer_normal(pred_mesh, gt_points, gt_norms):\n    # find closest gt points\n    gt_indices = norm_distance(pred_mesh.vertices.unsqueeze(0), gt_points.unsqueeze(0))[0]\n    # select norms from closest points and expand to match edges lengths\n\n    gt_norm_selections = gt_norms[gt_indices]\n    new_dimensions = (gt_norm_selections.shape[0], pred_mesh.ve.shape[1], 3)\n    vertex_norms = gt_norm_selections.view(-1, 1, 3).expand(new_dimensions)\n\n    # get all neighbour positions\n    neighbor_indices = pred_mesh.vv.clone()\n    empty_indices = (neighbor_indices < 0)\n    neighbor_indices[empty_indices] = 0 \n    empty_indices = ((empty_indices - 1) * -1).float().unsqueeze(-1)\n    neighbor_indices = neighbor_indices.view(-1)\n    vertex_neighbors = pred_mesh.vertices[neighbor_indices].view(new_dimensions)\n\n    # mask both tensors\n    vertex_norms = vertex_norms * empty_indices\n    vertex_norms = vertex_norms.contiguous().view(-1, 3)\n    vertex_neighbors = vertex_neighbors * empty_indices \n    vertex_neighbors = vertex_neighbors.contiguous().view(-1, 3)\n\n    # calculate normal loss, divide by number of unmasked elements to get mean \n    normal_loss = (torch.abs(torch.sum(vertex_norms * vertex_neighbors, dim=1))) \n    normal_loss = normal_loss.sum() / float(empty_indices.sum())\n    return normal_loss\n\n\ndef setup_meshes(filename=\'meshes/156.obj\', device=\'cuda\'): \n    mesh_1 = kal.rep.TriangleMesh.from_obj(filename, enable_adjacency=True)\n    mesh_1.to(device)\n    adj_1 = mesh_1.compute_adjacency_matrix_full().clone()\n    adj_1 = normalize_adj(adj_1)\n    mesh_1_i = kal.rep.TriangleMesh.from_tensors(mesh_1.vertices.clone(), mesh_1.faces.clone())\n    face_list_1 = calc_face_list(mesh_1)\n\n    initial_meshes = [mesh_1_i]\n    updated_meshes = [mesh_1]\n    adjs = [adj_1]\n\n    face_lists = [face_list_1]\n\n    mesh_info = {\'init\': initial_meshes, \'update\': updated_meshes , \'adjs\': adjs,\n                 \'face_lists\': face_lists}\n\n    return mesh_info\n\n\ndef calc_face_list(mesh): \n    face_list = np.zeros((len(mesh.faces), 3, 3))\n    for e, f1 in enumerate(mesh.faces):\n        for ee, index in enumerate(mesh.ff[e]):\n            f2 = mesh.faces[index]\n            f1_position = -1\n            f2_position = -1\n            if f1[0] in f2 and f1[1] in f2: \n                f1_position = 0\n            elif f1[1] in f2 and f1[2] in f2: \n                f1_position = 1\n            elif f1[0] in f2 and f1[2] in f2: \n                f1_position = 2\n            if f1_position >= 0 : \n                if f2[0] in f1 and f2[1] in f1: \n                    face_list[e][f1_position] = [index, 0, e]\n                elif f2[1] in f1 and f2[2] in f1: \n                    face_list[e][f1_position] = [index, 1, e]\n                elif f2[0] in f1 and f2[2] in f1: \n                    face_list[e][f1_position] = [index, 2, e]\n\n    return torch.LongTensor(face_list).to(mesh.faces.device)\n\n\ndef compute_splitting_faces(meshes, index, angle=50, show=False): \n    eps = .00001\n\n    # extract vertex coordinated for each vertex in face \n    faces = meshes[\'face_archive\'][index]\n    verts = meshes[\'update\'][index].vertices\n    face_list = meshes[\'face_lists\'][index]\n    p1 = torch.index_select(verts, 0, faces[:, 1])\n    p2 = torch.index_select(verts, 0, faces[:, 0])\n    p3 = torch.index_select(verts, 0, faces[:, 2])\n\n    # cauculate normals of each face \n    e1 = p2 - p1\n    e2 = p3 - p1\n    face_normals = torch.cross(e1, e2)\n    qn = torch.norm(face_normals, p=2, dim=1).detach().view(-1, 1)\n    face_normals = face_normals.div(qn.expand_as(face_normals))\n    main_face_normals = torch.index_select(face_normals, 0, face_list[:, 0, 2])\n\n    # cauculate the curvature with the 3 nighbor faces \n    # 1\n    face_1_normals = torch.index_select(face_normals, 0, face_list[:, 0, 0])\n    curvature_proxi_rad = torch.sum(main_face_normals * face_1_normals, dim=1).clamp(-1.0 + eps, 1.0 - eps).acos()\n    curvature_proxi_1 = (curvature_proxi_rad).view(-1, 1)\n    # 2\n    face_2_normals = torch.index_select(face_normals, 0, face_list[:, 1, 0])\n    curvature_proxi_rad = torch.sum(main_face_normals * face_2_normals, dim=1).clamp(-1.0 + eps, 1.0 - eps).acos()\n    curvature_proxi_2 = (curvature_proxi_rad).view(-1, 1)\n    # 3\n    face_3_normals = torch.index_select(face_normals, 0, face_list[:, 2, 0])\n    curvature_proxi_rad = torch.sum(main_face_normals * face_3_normals, dim=1).clamp(-1.0 + eps, 1.0 - eps).acos()\n    curvature_proxi_3 = (curvature_proxi_rad).view(-1, 1)\n\n    # get average over neighbors \n    curvature_proxi_full = torch.cat((curvature_proxi_1, curvature_proxi_2, curvature_proxi_3), dim=1)\n    curvature_proxi = torch.mean(curvature_proxi_full, dim=1)\n\n    # select faces with high curvature and return their index\n    splitting_faces = np.where(curvature_proxi.cpu() * 180 / np.pi > angle)[0]\n\n    if splitting_faces.shape[0] < 3:\n        splitting_faces = curvature_proxi.topk(3, sorted=False)[1] \n    else:\n        splitting_faces = torch.LongTensor(splitting_faces).to(faces.device)\n    return splitting_faces\n\n\ndef split_meshes(meshes, features, index, angle=70): \n    # compute faces to split\n    faces_to_split = compute_splitting_faces(meshes, index, angle, show=(index == 1))\n    # split mesh with selected faces\n    new_verts, new_faces, new_face_archive, new_face_list, new_features = split_info(meshes, faces_to_split, features, index)\n    new_mesh = TriangleMesh.from_tensors(new_verts, new_faces)\n    new_mesh_i = TriangleMesh.from_tensors(new_verts, new_faces)\n    # compute new adj matrix\n    new_adj = new_mesh.compute_adjacency_matrix_full().clone()\n    new_adj = normalize_adj(new_adj)\n    # update the meshes dictionary\n    meshes[\'init\'].append(new_mesh)\n    meshes[\'update\'].append(new_mesh_i)\n    meshes[\'adjs\'].append(new_adj)\n    meshes[\'face_lists\'].append(new_face_list)\n    meshes[\'face_archive\'].append(new_face_archive)\n\n    return new_features\n\n\ndef normalize_adj(mx):\n    rowsum = mx.sum(dim=1).view(-1)\n    r_inv = 1. / rowsum\n    r_inv[r_inv != r_inv] = 0.\n    r_mat_inv = torch.eye(r_inv.shape[0]).to(mx.device) * r_inv\n    mx = torch.mm(r_mat_inv, mx)\n    return mx\n\n\ndef reset_meshes(meshes): \n    meshes[\'face_archive\'] = [meshes[\'init\'][0].faces.clone()]\n    meshes[\'init\'] = meshes[\'init\'][:1]\n    meshes[\'update\'] = meshes[\'update\'][:1]\n    meshes[\'adjs\'] = meshes[\'adjs\'][:1]\n    meshes[\'face_lists\'] = meshes[\'face_lists\'][:1]\n\n\ndef split_features(split_mx, features): \n    features = features.permute(1, 0)\n    new_features = torch.mm(features, split_mx)\n    features = torch.cat((features, new_features), dim=1).permute(1, 0)\n    return features\n\n\ndef loss_surf(meshes, tgt_points):\t\n    loss = kal.metrics.point.chamfer_distance(tgt_points, meshes[\'update\'][0].sample(3000)[0])\n    loss += kal.metrics.point.chamfer_distance(tgt_points, meshes[\'update\'][1].sample(3000)[0])\n    loss += kal.metrics.point.chamfer_distance(tgt_points, meshes[\'update\'][2].sample(3000)[0])\n    return loss\n\n\ndef loss_surf2(meshes, tgt_points):\t\n    loss = nvl.metrics.mesh.point_to_surface(tgt_points, meshes[\'update\'][0])\n    loss += nvl.metrics.point.directed_distance((meshes[\'update\'][0].sample(3000)[0]), tgt_points)\n    loss += nvl.metrics.mesh.point_to_surface(tgt_points, meshes[\'update\'][1])\n    loss += nvl.metrics.point.directed_distance((meshes[\'update\'][1].sample(3000)[0]), tgt_points)\n    loss += nvl.metrics.mesh.point_to_surface(tgt_points, meshes[\'update\'][2])\n    loss += nvl.metrics.point.directed_distance((meshes[\'update\'][2].sample(3000)[0]), tgt_points)\n    return loss\n\n\ndef loss_edge(meshes):\t\n    loss = kal.metrics.mesh.edge_length(meshes[\'update\'][0])\n    loss += kal.metrics.mesh.edge_length(meshes[\'update\'][1])\n    loss += kal.metrics.mesh.edge_length(meshes[\'update\'][2])\n    return loss\n\n\ndef loss_lap(meshes): \n    loss = .3 * kal.metrics.mesh.laplacian_loss(meshes[\'init\'][0], meshes[\'update\'][0])\n    loss += kal.metrics.mesh.laplacian_loss(meshes[\'init\'][1], meshes[\'update\'][1])\n    loss += kal.metrics.mesh.laplacian_loss(meshes[\'init\'][2], meshes[\'update\'][2])\n\n    loss += torch.sum((meshes[\'init\'][1].vertices - meshes[\'update\'][1].vertices) ** 2, 1).mean() * .0666\n    loss += torch.sum((meshes[\'init\'][2].vertices - meshes[\'update\'][2].vertices) ** 2, 1).mean() * .0666\n\n    return loss \n\n\ndef split_info(meshes, split_faces, features, index):\n    device = meshes[\'init\'][0].vertices.device\n    faces_verts = meshes[\'face_archive\'][index].clone()     # vertex info of all faces made\n    face_list = meshes[\'face_lists\'][index].clone()\n\n\n    splitting_face_list_values = torch.index_select(face_list, 0, split_faces)     # neighbor info of faces to be split \n    splitting_face_list_len = splitting_face_list_values.shape[0]\n\n    counter = torch.zeros((face_list.shape[0])).to(device)\n    counter[split_faces] = 1\n    unsplitting_faces_list_indices = np.where(counter.cpu() == 0)[0]\n    unsplitting_face_list_values = torch.index_select(face_list, 0, torch.LongTensor(unsplitting_faces_list_indices).to(device))  # neighbor info of faces not split \n\n    splitting_faces_indices = splitting_face_list_values[:, 0, 2]   # indices of faces being split in faces_verts\n    unsplitting_faces_indices = unsplitting_face_list_values[:, 0, 2]   # indices of faces not being split from face_verts \n\n    # indices of new faces being made in, in the updated faces_verts array \n    new_faces_indices_1 = torch.arange(splitting_face_list_len).to(device).view(-1, 1) + faces_verts.shape[0]\n    new_faces_indices_2 = new_faces_indices_1 + splitting_face_list_len\n    new_faces_indices_3 = new_faces_indices_2 + splitting_face_list_len\n    splitting_new_faces_indices = torch.cat((new_faces_indices_1, new_faces_indices_2, new_faces_indices_3), dim=1)\n    unsplitting_new_faces_indices = torch.cat((unsplitting_faces_indices.view(-1, 1),\n                                               unsplitting_faces_indices.view(-1, 1),\n                                               unsplitting_faces_indices.view(-1, 1)), dim=1)\n\n    # saving where each face will be held in the updated face_verts array, saved in this manner for quick selection\n    new_positions = torch.zeros((faces_verts.shape[0], 3)).to(device).long()\n    new_positions[splitting_faces_indices] = splitting_new_faces_indices\n    new_positions[unsplitting_faces_indices] = unsplitting_new_faces_indices\n\n    # adding unsplitting triangles to new face_list \n    # get location of 3 neighbors \n    unsplitting_connecting_face_1 = new_positions[unsplitting_face_list_values[:, 0, 0],\n                                                  unsplitting_face_list_values[:, 0, 1]].view(-1, 1, 1)\n    unsplitting_connecting_face_2 = new_positions[unsplitting_face_list_values[:, 1, 0],\n                                                  unsplitting_face_list_values[:, 1, 1]].view(-1, 1, 1)\n    unsplitting_connecting_face_3 = new_positions[unsplitting_face_list_values[:, 2, 0],\n                                                  unsplitting_face_list_values[:, 2, 1]].view(-1, 1, 1)\t\n    # get the neighbours index in updated face_verts array \n    unsplitting_connecting_side_1 = unsplitting_face_list_values[:, 0, 1].view(-1, 1, 1)\n    unsplitting_connecting_side_2 = unsplitting_face_list_values[:, 1, 1].view(-1, 1, 1)\n    unsplitting_connecting_side_3 = unsplitting_face_list_values[:, 2, 1].view(-1, 1, 1)\n    # make new face_list \n    unsplitting_face_number = unsplitting_faces_indices.view(-1, 1, 1)\n    new_unsplitting_face_list_1 = torch.cat((unsplitting_connecting_face_1,\n                                             unsplitting_connecting_side_1, unsplitting_face_number), dim=2)\n    new_unsplitting_face_list_2 = torch.cat((unsplitting_connecting_face_2, unsplitting_connecting_side_2,\n                                             unsplitting_face_number), dim=2)\n    new_unsplitting_face_list_3 = torch.cat((unsplitting_connecting_face_3, unsplitting_connecting_side_3,\n                                             unsplitting_face_number), dim=2)\n    new_unsplitting_face_list = torch.cat((new_unsplitting_face_list_1, new_unsplitting_face_list_2,\n                                           new_unsplitting_face_list_3), dim=1)\n\n    # adding splitting triangles to new face_list \n    # new triangle 1\n    # get location of 3 neighbors\n    splitting_connecting_face_1_1 = new_positions[splitting_face_list_values[:, 0, 0],\n                                                  splitting_face_list_values[:, 0, 1]].view(-1, 1, 1)   # one old face is its neigboors\n    splitting_connecting_face_1_2 = new_faces_indices_2.view(-1, 1, 1)  # 2 new faces are its neighbor \n    splitting_connecting_face_1_3 = new_faces_indices_3.view(-1, 1, 1)\n    # get the neighbours index in updated face_verts array \n    splitting_connecting_side_1_1 = splitting_face_list_values[:, 0, 1].view(-1, 1, 1)    # get old face\'s index in face_verts \n    splitting_connecting_side_1_2 = torch.zeros(splitting_face_list_len).view(-1, 1, 1).long().to(device)   # use new faces\' known indices \n    splitting_connecting_side_1_3 = torch.zeros(splitting_face_list_len).view(-1, 1, 1).long().to(device)\n    # make new face_list \n    splitting_face_number_1 = new_faces_indices_1.view(-1, 1, 1)\n    new_splitting_face_list_1_1 = torch.cat((splitting_connecting_face_1_1,\n                                             splitting_connecting_side_1_1,\n                                             splitting_face_number_1), dim=2) \n    new_splitting_face_list_1_2 = torch.cat((splitting_connecting_face_1_2,\n                                             splitting_connecting_side_1_2,\n                                             splitting_face_number_1), dim=2)\n    new_splitting_face_list_1_3 = torch.cat((splitting_connecting_face_1_3,\n                                             splitting_connecting_side_1_3,\n                                             splitting_face_number_1), dim=2)\n    new_splitting_face_list_1 = torch.cat((new_splitting_face_list_1_1,\n                                           new_splitting_face_list_1_2,\n                                           new_splitting_face_list_1_3), dim=1)\n\n    # new triangle 2\n    splitting_connecting_face_2_1 = new_faces_indices_1.view(-1, 1, 1)\n    splitting_connecting_face_2_2 = new_positions[splitting_face_list_values[:, 1, 0],\n                                                  splitting_face_list_values[:, 1, 1]].view(-1, 1, 1)\n    splitting_connecting_face_2_3 = new_faces_indices_3.view(-1, 1, 1)\n\n    splitting_connecting_side_2_1 = torch.ones(splitting_face_list_len).view(-1, 1, 1).long().to(device)\n    splitting_connecting_side_2_2 = splitting_face_list_values[:, 1, 1].view(-1, 1, 1)\n    splitting_connecting_side_2_3 = torch.ones(splitting_face_list_len).view(-1, 1, 1).long().to(device)\n\n    splitting_face_number_2 = new_faces_indices_2.view(-1, 1, 1)\n    new_splitting_face_list_2_1 = torch.cat((splitting_connecting_face_2_1,\n                                             splitting_connecting_side_2_1, splitting_face_number_2), dim=2)\n    new_splitting_face_list_2_2 = torch.cat((splitting_connecting_face_2_2,\n                                             splitting_connecting_side_2_2, splitting_face_number_2), dim=2)\n    new_splitting_face_list_2_3 = torch.cat((splitting_connecting_face_2_3,\n                                             splitting_connecting_side_2_3, splitting_face_number_2), dim=2)\n    new_splitting_face_list_2 = torch.cat((new_splitting_face_list_2_1, new_splitting_face_list_2_2,\n                                           new_splitting_face_list_2_3), dim=1)\n\n\n    # new triangle 3\n    splitting_connecting_face_3_1 = new_faces_indices_1.view(-1, 1, 1)\n    splitting_connecting_face_3_2 = new_faces_indices_2.view(-1, 1, 1)\n    splitting_connecting_face_3_3 = new_positions[splitting_face_list_values[:, 2, 0], splitting_face_list_values[:, 2, 1]].view(-1, 1, 1)\n\n    splitting_connecting_side_3_1 = torch.ones(splitting_face_list_len).view(-1, 1, 1).long().to(device) * 2\n    splitting_connecting_side_3_2 = torch.ones(splitting_face_list_len).view(-1, 1, 1).long().to(device) * 2\n    splitting_connecting_side_3_3 = splitting_face_list_values[:, 2, 1].view(-1, 1, 1)\n\n    splitting_face_number_3 = new_faces_indices_3.view(-1, 1, 1)\n    new_splitting_face_list_3_1 = torch.cat((splitting_connecting_face_3_1, splitting_connecting_side_3_1, splitting_face_number_3), dim=2)\n    new_splitting_face_list_3_2 = torch.cat((splitting_connecting_face_3_2, splitting_connecting_side_3_2, splitting_face_number_3), dim=2)\n    new_splitting_face_list_3_3 = torch.cat((splitting_connecting_face_3_3, splitting_connecting_side_3_3, splitting_face_number_3), dim=2)\n    new_splitting_face_list_3 = torch.cat((new_splitting_face_list_3_1, new_splitting_face_list_3_2, new_splitting_face_list_3_3), dim=1)\n\n    # complete new face_list is made \n\n    new_splitting_face_list = torch.cat((new_unsplitting_face_list, new_splitting_face_list_1, new_splitting_face_list_2, new_splitting_face_list_3))\n    split_faces = faces_verts[splitting_faces_indices]\n\n    # now to make the new vertex \n    vertex_count = meshes[\'update\'][index].vertices.shape[0]\n    new_len = split_faces.shape[0] + vertex_count\n\n    # select the vertices of the faces to be split \n    x_f = split_faces[:, 0]\n    y_f = split_faces[:, 1]\n    z_f = split_faces[:, 2]\n    verts_and_features = torch.cat((meshes[\'update\'][index].vertices, features), dim=1) \n    x_v = verts_and_features[x_f] \n    y_v = verts_and_features[y_f]\n    z_v = verts_and_features[z_f]\n    # average the features and position \n    v1 = x_v / 3 + y_v / 3 + z_v / 3 \n    verts_and_features = torch.cat((verts_and_features, v1))\n    verts = verts_and_features[:, :3]\n\n    features = verts_and_features[:, 3:]\n    v1_inds = (vertex_count + torch.arange(split_faces.shape[0])).to(device).view(-1, 1)\n    x_f = x_f.view(-1, 1)\n    y_f = y_f.view(-1, 1)\n    z_f = z_f.view(-1, 1)\n    # define verts of new faces \n    new_face_1 = torch.cat((x_f, y_f, v1_inds), dim=1)\n    new_face_2 = torch.cat((v1_inds, y_f, z_f), dim=1)\n    new_face_3 = torch.cat((x_f, v1_inds, z_f), dim=1)\n    # name new face_verts array by appending then to old in order previously defined \n    face_archive = torch.cat((faces_verts, new_face_1, new_face_2, new_face_3))\n\n    faces = face_archive[new_splitting_face_list[:, 0, 2]]\n\n    return verts, faces, face_archive, new_splitting_face_list, features\n'"
examples/ImageRecon/Image_Mesh_Recon_Direct/architectures.py,2,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nNetwork architecture definitions\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n\n\nclass Encoder(nn.Module):\n\n\tdef __init__(self, N_CHANNELS, N_KERNELS, \\\n\t\t\t\t BATCH_SIZE, IMG_DIM, VERTS):\n\t\tsuper(Encoder, self).__init__()\n\t\t\n\t\tblock1 = self.convblock(N_CHANNELS, 32, N_KERNELS, stride=2, pad=2)\n\t\tblock2 = self.convblock(32, 64, N_KERNELS, stride=2, pad=2)\n\t\tblock3 = self.convblock(64, 128, N_KERNELS, stride=2, pad=2)\n\t\tblock4 = self.convblock(128, 128, N_KERNELS, stride=2, pad=2)\n\t\t\n\t\tlinear1 = self.linearblock(10368, 1024)\n\t\tlinear2 = self.linearblock(1024, 1024)\n\t\tself.linear3 = nn.Linear(1024, 1024)\n\t\t\n\t\tlinear4 = self.linearblock(1024, 1024)\n\t\tlinear5 = self.linearblock(1024, 2048)\n\t\tself.linear6 = nn.Linear(2048, VERTS*3)\n\t   \n\t\t\n\t\t#################################################\n\t\tall_blocks = block1 + block2 + block3 + block4\n\t\tself.encoder1 = nn.Sequential(*all_blocks)\n\t\t\n\t\tall_blocks = linear1 + linear2\n\t\tself.encoder2 = nn.Sequential(*all_blocks)\n\t\t\n\t\tall_blocks = linear4 + linear5\n\t\tself.decoder = nn.Sequential(*all_blocks)\n\t\t\n\t   \n\t\t\n\t\t# Initialize with Xavier Glorot\n\t\tfor m in self.modules():\n\t\t\tif isinstance(m, nn.ConvTranspose2d) \\\n\t\t\tor isinstance(m, nn.Linear) \\\n\t\t\tor isinstance(object, nn.Conv2d):\n\t\t\t\tnn.init.xavier_uniform_(m.weight)\n\t\t\t\tnn.init.normal_(m.weight, mean=0, std=0.001)\n\n\t\t# Free some memory\n\t\tdel all_blocks, block1, block2, block3, \\\n\t\tlinear1, linear2, linear4, linear5, \\\n\t   \n\n\t   \n\t\t\n\t\n\tdef convblock(self, indim, outdim, ker, stride, pad):\n\t\tblock2 = [\n\t\t\tnn.Conv2d(indim, outdim, ker, stride, pad),\n\t\t\tnn.BatchNorm2d(outdim),\n\t\t\tnn.ReLU()\n\t\t]\n\t\treturn block2\n\t\n\tdef linearblock(self, indim, outdim):\n\t\tblock2 = [\n\t\t\tnn.Linear(indim, outdim),\n\t\t\tnn.BatchNorm1d(outdim),\n\t\t\tnn.ReLU()\n\t\t]\n\t\treturn block2\n\t\t\n\tdef forward(self, x):\n\t\t\n\t\tfor layer in self.encoder1:\n\t\t\tx = layer(x)\n\t\t\n\t\tbnum = x.shape[0] \n\t\tx = x.view(bnum, -1) \n\t\tfor layer in self.encoder2:\n\t\t\tx = layer(x)\n\t\tx = self.linear3(x)\n\t\t\n\t   \n\t\tfor layer in self.decoder:\n\t\t\tx = layer(x)\n\t\tx = self.linear6(x)\n\t   \n\t\treturn x.view(x.shape[0], -1,3)\n\n\n'"
examples/ImageRecon/Image_Mesh_Recon_Direct/eval.py,3,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport os\nimport torch\nimport sys\nfrom tqdm import tqdm\nfrom PIL import Image\nimport numpy as np\n\nfrom torch.utils.data import DataLoader\n\nfrom utils import preprocess\nfrom architectures import Encoder\nimport kaolin as kal \n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-expid\', type=str, default=\'Direct\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'-device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'-categories\', type=str,nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'-vis\', action=\'store_true\', help=\'Visualize each model while evaluating\')\nparser.add_argument(\'-batchsize\', type=int, default=16, help=\'Batch size.\')\nparser.add_argument(\'-f_score\', action=\'store_true\', help=\'compute F-score\')\nargs = parser.parse_args()\n\n\n# Data\npoints_set_valid = kal.dataloader.ShapeNet.Points(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = False, split = .7, num_points=5000 )\nimages_set_valid = kal.dataloader.ShapeNet.Images(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = False,  split = .7, views=1, transform= preprocess )\nmeshes_set_valid = kal.dataloader.ShapeNet.Meshes(root =\'../../datasets/\', categories =args.categories , \\\n\tdownload = True, train = False,  split = .7)\n\nvalid_set = kal.dataloader.ShapeNet.Combination([points_set_valid, images_set_valid], root=\'../../datasets/\')\ndataloader_val = DataLoader(valid_set, batch_size=args.batchsize, shuffle=False, \n\tnum_workers=8)\n# Model\nmesh = kal.rep.TriangleMesh.from_obj(\'386.obj\')\nif args.device == ""cuda"": \n\tmesh.cuda()\ninitial_verts = mesh.vertices.clone()\n\n\nmodel = Encoder(4, 5, args.batchsize, 137, mesh.vertices.shape[0] ).to(args.device)\n# Load saved weights\nmodel.load_state_dict(torch.load(\'log/{0}/best.pth\'.format(args.expid)))\n\nloss_epoch = 0.\nf_epoch = 0.\nnum_batches = 0\nnum_items = 0\nloss_fn = kal.metrics.point.chamfer_distance\n\nmodel.eval()\nwith torch.no_grad():\n\tfor data in tqdm(dataloader_val): \n\t\t# data creation\n\t\ttgt_points = data[\'points\'].to(args.device)\n\t\tinp_images = data[\'imgs\'].to(args.device)\n\n\t\t# inference \n\t\tdelta_verts = model(inp_images)\n\n\t\t# eval \n\t\t\n\t\tloss = 0. \n\t\tfor deltas, tgt, img in zip(delta_verts, tgt_points, inp_images): \t\n\t\t\tmesh.vertices = deltas + initial_verts\n\t\t\tpred_points, _ = mesh.sample(3000)\n\t\t\tloss += 3000* loss_fn(pred_points, tgt) / float(args.batchsize)\n\n\t\t\tif args.f_score: \n\t\t\t\n\t\t\t\tf_score = kal.metrics.point.f_score(tgt, pred_points, extend = False)\n\t\t\t\tf_epoch += (f_score  / float(args.batchsize)).item()\n\n\t\t\tif args.vis: \n\t\t\t\ttgt_mesh = meshes_set_valid[num_items]\n\t\t\t\ttgt_verts = tgt_mesh[\'verts\']\n\t\t\t\ttgt_faces = tgt_mesh[\'faces\']\n\t\t\t\ttgt_mesh = kal.rep.TriangleMesh.from_tensors(tgt_verts, tgt_faces)\n\n\t\t\t\tprint (\'Displaying input image\')\n\t\t\t\timg = img.data.cpu().numpy().transpose((1, 2, 0))\n\t\t\t\timg = (img*255.).astype(np.uint8)\n\t\t\t\tImage.fromarray(img).show()\n\t\t\t\tprint (\'Rendering Target Mesh\')\n\t\t\t\tkal.visualize.show_mesh(tgt_mesh)\n\t\t\t\tprint (\'Rendering Predicted Mesh\')\n\t\t\t\tkal.visualize.show_mesh(mesh)\n\t\t\t\tprint(\'----------------------\')\n\t\t\t\tnum_items += 1\n\n\n\t\tloss_epoch += loss.item()\n\t\t\n\t\t\n\t\tnum_batches += 1.\n\nout_loss = loss_epoch / float(num_batches)\nprint (\'Loss over validation set is {0}\'.format(out_loss))\nif args.f_score: \n\tout_f = f_epoch / float(num_batches)\n\tprint (\'F-score over validation set is {0}\'.format(out_f))\n'"
examples/ImageRecon/Image_Mesh_Recon_Direct/train.py,8,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport json\nimport numpy as np\nimport os\nimport torch\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport sys\nfrom tqdm import tqdm\n\nfrom utils import preprocess, loss_lap\nfrom architectures import Encoder\n\nimport kaolin as kal \n""""""\nCommandline arguments\n""""""\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-expid\', type=str, default=\'Direct\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'-device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'-categories\', type=str,nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'-epochs\', type=int, default=50, help=\'Number of train epochs.\')\nparser.add_argument(\'-batchsize\', type=int, default=16, help=\'Batch size.\')\nparser.add_argument(\'-lr\', type=float, default=1e-4, help=\'Learning rate.\')\nparser.add_argument(\'-val-every\', type=int, default=5, help=\'Validation frequency (epochs).\')\nparser.add_argument(\'-print-every\', type=int, default=20, help=\'Print frequency (batches).\')\nparser.add_argument(\'-logdir\', type=str, default=\'log\', help=\'Directory to log data to.\')\nparser.add_argument(\'-save-model\', action=\'store_true\', help=\'Saves the model and a snapshot \\\n\tof the optimizer state.\')\nargs = parser.parse_args()\n\n\n\n""""""\nDataset\n""""""\npoints_set = kal.dataloader.ShapeNet.Points(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = True, split = .7, num_points=3000 )\nimages_set = kal.dataloader.ShapeNet.Images(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = True,  split = .7, views=23, transform= preprocess )\ntrain_set = kal.dataloader.ShapeNet.Combination([points_set, images_set], root=\'../../datasets/\')\n\ndataloader_train = DataLoader(train_set, batch_size=args.batchsize, shuffle=True, \n\tnum_workers=8)\n\n\n\npoints_set_valid = kal.dataloader.ShapeNet.Points(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = False, split = .7, num_points=5000 )\nimages_set_valid = kal.dataloader.ShapeNet.Images(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = False,  split = .7, views=1, transform= preprocess )\nvalid_set = kal.dataloader.ShapeNet.Combination([points_set_valid, images_set_valid], root=\'../../datasets/\')\n\ndataloader_val = DataLoader(valid_set, batch_size=args.batchsize, shuffle=False, \n\tnum_workers=8)\n\n\n\n""""""\nModel settings \n""""""\nmesh = kal.rep.TriangleMesh.from_obj(\'386.obj\')\nif args.device == ""cuda"": \n\tmesh.cuda()\ninitial_verts = mesh.vertices.clone()\n\n\nmodel = Encoder(4, 5, args.batchsize, 137, mesh.vertices.shape[0] ).to(args.device)\n\nloss_fn = kal.metrics.point.chamfer_distance\nloss_edge = kal.metrics.mesh.edge_length\n\noptimizer = optim.Adam(model.parameters(), lr=args.lr)\n\n\n\n\n# Create log directory, if it doesn\'t already exist\nargs.logdir = os.path.join(args.logdir, args.expid)\nif not os.path.isdir(args.logdir):\n\tos.makedirs(args.logdir)\n\tprint(\'Created dir:\', args.logdir)\n\n# Log all commandline args\nwith open(os.path.join(args.logdir, \'args.txt\'), \'w\') as f:\n\tjson.dump(args.__dict__, f, indent=2)\n \n\nclass Engine(object):\n\t""""""Engine that runs training and inference.\n\tArgs\n\t\t- cur_epoch (int): Current epoch.\n\t\t- print_every (int): How frequently (# batches) to print loss.\n\t\t- validate_every (int): How frequently (# epochs) to run validation.\n\t\t\n\t""""""\n\n\tdef __init__(self,  cur_epoch=0, print_every=1, validate_every=1):\n\t\tself.cur_epoch = cur_epoch\n\t\tself.train_loss = []\n\t\tself.val_loss = []\n\t\tself.bestval = 1000.\n\n\tdef train(self):\n\t\tloss_epoch = 0.\n\t\tnum_batches = 0\n\n\t\tmodel.train()\n\t\t# Train loop\n\t\tfor i, data in enumerate(tqdm(dataloader_train), 0):\n\t\t\toptimizer.zero_grad()\n\t\t\t\n\t\t\t# data creation\n\t\t\ttgt_points = data[\'points\'].to(args.device)\n\t\t\tinp_images = data[\'imgs\'].to(args.device)\n\n\t\t\t# inference \n\t\t\tdelta_verts = model(inp_images)\n\n\t\t\t# losses \n\t\t\tsurf_loss = 0. \n\t\t\tedge_loss = 0.\n\t\t\tlap_loss = 0.\n\t\t\tfor deltas, tgt in zip(delta_verts, tgt_points): \t\n\t\t\t\tmesh.vertices = deltas + initial_verts\n\t\t\t\tpred_points, _ = mesh.sample(3000)\n\t\t\t\tsurf_loss += 3000 * loss_fn(pred_points, tgt) / float(args.batchsize)\n\t\t\t\tedge_loss += 300 * loss_edge(mesh) / float(args.batchsize)\n\t\t\t\tlap_loss += 150 * loss_lap(mesh, deltas)\n\n\n\t\t\tloss = surf_loss + edge_loss + lap_loss\n\t\t\tloss.backward()\n\t\t\tloss_epoch += float(surf_loss.item())\n\n\t\t\t# logging\n\t\t\tnum_batches += 1\n\t\t\tif i % args.print_every == 0:\n\t\t\t\ttqdm.write(f\'[TRAIN] Epoch {self.cur_epoch:03d}, Batch {i:03d}: Loss: {float(surf_loss.item()):3.3f}, Edge: {float(edge_loss.item()):3.3f}, lap: {float(lap_loss.item()):3.3f}\')\n\t\t\toptimizer.step()\n\t\t\n\t\t\n\t\tloss_epoch = loss_epoch / num_batches\n\t\tself.train_loss.append(loss_epoch)\n\t\tself.cur_epoch += 1\n\n\t\t\n\t\t\n\tdef validate(self):\n\t\tmodel.eval()\n\t\twith torch.no_grad():\t\n\t\t\tnum_batches = 0\n\t\t\tloss_epoch = 0.\n\n\t\t\t# Validation loop\n\t\t\tfor i, data in enumerate(tqdm(dataloader_val), 0):\n\n\t\t\t\t# data creation\n\t\t\t\ttgt_points = data[\'points\'].to(args.device)\n\t\t\t\tinp_images = data[\'imgs\'].to(args.device)\n\n\t\t\t\t# inference \n\t\t\t\tdelta_verts = model(inp_images)\n\n\t\t\t\t# losses \n\t\t\t\tloss = 0. \n\t\t\t\tfor deltas, tgt in zip(delta_verts, tgt_points): \t\n\t\t\t\t\tmesh.vertices = deltas + initial_verts\n\t\t\t\t\tpred_points, _ = mesh.sample(3000)\n\n\t\t\t\t\tloss += 3000 * loss_fn(pred_points, tgt) / float(args.batchsize)\n\t\t\t\tloss_epoch += loss.item()\n\n\t\t\t\t# logging\n\t\t\t\tnum_batches += 1\n\t\t\t\tif i % args.print_every == 0:\n\t\t\t\t\t\tout_loss = loss_epoch / float(num_batches)\n\t\t\t\t\t\ttqdm.write(f\'[VAL] Epoch {self.cur_epoch:03d}, Batch {i:03d}: loss: {out_loss}\')\n\t\t\t\t\t\t\n\t\t\tout_loss = loss_epoch / float(num_batches)\n\t\t\ttqdm.write(f\'[VAL Total] Epoch {self.cur_epoch:03d}, Batch {i:03d}: loss: {out_loss}\')\n\n\t\t\tself.val_loss.append(out_loss)\n\n\tdef save(self):\n\n\t\tsave_best = False\n\t\tif self.val_loss[-1] <= self.bestval:\n\t\t\tself.bestval = self.val_loss[-1]\n\t\t\tsave_best = True\n\t\t\n\t\t# Create a dictionary of all data to save\n\t\tlog_table = {\n\t\t\t\'epoch\': self.cur_epoch,\n\t\t\t\'bestval\': self.bestval,\n\t\t\t\'train_loss\': self.train_loss,\n\t\t\t\'val_loss\': self.val_loss\n\t\t}\n\n\t\t# Save the recent model/optimizer states\n\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, \'recent.pth\'))\n\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, \'recent_optim.pth\'))\n\t\t# Log other data corresponding to the recent model\n\t\twith open(os.path.join(args.logdir, \'recent.log\'), \'w\') as f:\n\t\t\tf.write(json.dumps(log_table))\n\n\t\ttqdm.write(\'====== Saved recent model ======>\')\n\t\t\n\t\tif save_best:\n\t\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, \'best.pth\'))\n\t\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, \'best_optim.pth\'))\n\t\t\ttqdm.write(\'====== Overwrote best model ======>\')\n\t\t\t\n\t\ntrainer = Engine()\n\nfor epoch in range(args.epochs): \n\ttrainer.train()\n\tif epoch %4 == 0:\n\t\ttrainer.validate()\n\t\ttrainer.save()'"
examples/ImageRecon/Image_Mesh_Recon_Direct/utils.py,1,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch \nfrom torchvision import transforms\nfrom torchvision.transforms import Normalize as norm \n\nimport kaolin as kal\n\n\n\npreprocess = transforms.Compose([\n   transforms.CenterCrop(137),\n   transforms.ToTensor()\n])\n\n\ndef loss_lap(mesh1, deltas ): \n\tmesh2 = kal.rep.TriangleMesh.from_tensors(mesh1.vertices - deltas, mesh1.faces)\n\tloss =   kal.metrics.mesh.laplacian_loss(mesh1, mesh2)\n\tloss += torch.sum((deltas)**2, 1).mean() * .0666 \n\treturn loss '"
examples/ImageRecon/OccNet/architectures.py,10,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\n\nimport torch \nfrom torch import nn \nfrom torch.nn.parameter import Parameter\nimport torch.nn.functional as F\nfrom torchvision import models\nimport torch.distributions as dist\n\nimport torch\nfrom torch.nn import Parameter\n\nclass Resnet18(nn.Module):\n    r\'\'\' ResNet-18 encoder network for image input.\n    Args:\n        c_dim (int): output dimension of the latent embedding\n        normalize (bool): whether the input images should be normalized\n        use_linear (bool): whether a final linear layer should be used\n    \'\'\'\n\n    def __init__(self, c_dim, normalize=True, use_linear=True):\n        super().__init__()\n        self.normalize = normalize\n        self.use_linear = use_linear\n        self.features = models.resnet18(pretrained=True)\n        self.features.fc = nn.Sequential()\n        if use_linear:\n            self.fc = nn.Linear(512, c_dim)\n        elif c_dim == 512:\n            self.fc = nn.Sequential()\n        else:\n            raise ValueError(\'c_dim must be 512 if use_linear is False\')\n\n    def forward(self, x):\n        if self.normalize:\n            x = normalize_imagenet(x)\n        net = self.features(x)\n        out = self.fc(net)\n        return out\n\ndef normalize_imagenet(x):\n    \'\'\' Normalize input images according to ImageNet standards.\n    Args:\n        x (tensor): input images\n    \'\'\'\n    x = x.clone()\n    x[:, 0] = (x[:, 0] - 0.485) / 0.229\n    x[:, 1] = (x[:, 1] - 0.456) / 0.224\n    x[:, 2] = (x[:, 2] - 0.406) / 0.225\n    return x\n\n\nclass DecoderCBatchNorm(nn.Module):\n    \'\'\' Decoder with conditional batch normalization (CBN) class.\n    Args:\n        dim (int): input dimension\n        z_dim (int): dimension of latent code z\n        c_dim (int): dimension of latent conditioned code c\n        hidden_size (int): hidden size of Decoder network\n        leaky (bool): whether to use leaky ReLUs\n        legacy (bool): whether to use the legacy structure\n    \'\'\'\n\n    def __init__(self, dim=3, z_dim=128, c_dim=128,\n                 hidden_size=256, leaky=False, legacy=False):\n        super().__init__()\n        self.z_dim = z_dim\n        if not z_dim == 0:\n            self.fc_z = nn.Linear(z_dim, hidden_size)\n\n        self.fc_p = nn.Conv1d(dim, hidden_size, 1)\n        self.block0 = CResnetBlockConv1d(c_dim, hidden_size, legacy=legacy)\n        self.block1 = CResnetBlockConv1d(c_dim, hidden_size, legacy=legacy)\n        self.block2 = CResnetBlockConv1d(c_dim, hidden_size, legacy=legacy)\n        self.block3 = CResnetBlockConv1d(c_dim, hidden_size, legacy=legacy)\n        self.block4 = CResnetBlockConv1d(c_dim, hidden_size, legacy=legacy)\n\n        if not legacy:\n            self.bn = CBatchNorm1d(c_dim, hidden_size)\n        else:\n            self.bn = CBatchNorm1d_legacy(c_dim, hidden_size)\n\n        self.fc_out = nn.Conv1d(hidden_size, 1, 1)\n\n        if not leaky:\n            self.actvn = F.relu\n        else:\n            self.actvn = lambda x: F.leaky_relu(x, 0.2)\n\n    def forward(self, p, z, c, **kwargs):\n        p = p.transpose(1, 2)\n        batch_size, D, T = p.size()\n        net = self.fc_p(p)\n\n        if self.z_dim != 0:\n            net_z = self.fc_z(z).unsqueeze(2)\n            net = net + net_z\n\n        net = self.block0(net, c)\n        net = self.block1(net, c)\n        net = self.block2(net, c)\n        net = self.block3(net, c)\n        net = self.block4(net, c)\n\n        out = self.fc_out(self.actvn(self.bn(net, c)))\n        out = out.squeeze(1)\n\n        return out\n\n\ndef get_prior_z(device):\n    \'\'\' Returns prior distribution for latent code z.\n    Args:\n        cfg (dict): imported yaml config\n        device (device): pytorch device\n    \'\'\'\n    z_dim = 0\n    p0_z = dist.Normal(\n        torch.zeros(z_dim, device = device),\n        torch.ones(z_dim, device = device)\n    )\n\n    return p0_z\n\nclass CBatchNorm1d(nn.Module):\n    \'\'\' Conditional batch normalization layer class.\n    Args:\n        c_dim (int): dimension of latent conditioned code c\n        f_dim (int): feature dimension\n        norm_method (str): normalization method\n    \'\'\'\n\n    def __init__(self, c_dim, f_dim, norm_method=\'batch_norm\'):\n        super().__init__()\n        self.c_dim = c_dim\n        self.f_dim = f_dim\n        self.norm_method = norm_method\n        # Submodules\n        self.conv_gamma = nn.Conv1d(c_dim, f_dim, 1)\n        self.conv_beta = nn.Conv1d(c_dim, f_dim, 1)\n        if norm_method == \'batch_norm\':\n            self.bn = nn.BatchNorm1d(f_dim, affine=False)\n        elif norm_method == \'instance_norm\':\n            self.bn = nn.InstanceNorm1d(f_dim, affine=False)\n        elif norm_method == \'group_norm\':\n            self.bn = nn.GroupNorm1d(f_dim, affine=False)\n        else:\n            raise ValueError(\'Invalid normalization method!\')\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.zeros_(self.conv_gamma.weight)\n        nn.init.zeros_(self.conv_beta.weight)\n        nn.init.ones_(self.conv_gamma.bias)\n        nn.init.zeros_(self.conv_beta.bias)\n\n    def forward(self, x, c):\n        assert(x.size(0) == c.size(0))\n        assert(c.size(1) == self.c_dim)\n\n        # c is assumed to be of size batch_size x c_dim x T\n        if len(c.size()) == 2:\n            c = c.unsqueeze(2)\n\n        # Affine mapping\n        gamma = self.conv_gamma(c)\n        beta = self.conv_beta(c)\n\n        # Batchnorm\n        net = self.bn(x)\n        out = gamma * net + beta\n\n        return out\n\n\nclass CResnetBlockConv1d(nn.Module):\n    \'\'\' Conditional batch normalization-based Resnet block class.\n    Args:\n        c_dim (int): dimension of latend conditioned code c\n        size_in (int): input dimension\n        size_out (int): output dimension\n        size_h (int): hidden dimension\n        norm_method (str): normalization method\n        legacy (bool): whether to use legacy blocks \n    \'\'\'\n\n    def __init__(self, c_dim, size_in, size_h=None, size_out=None,\n                 norm_method=\'batch_norm\', legacy=False):\n        super().__init__()\n        # Attributes\n        if size_h is None:\n            size_h = size_in\n        if size_out is None:\n            size_out = size_in\n\n        self.size_in = size_in\n        self.size_h = size_h\n        self.size_out = size_out\n        # Submodules\n        if not legacy:\n            self.bn_0 = CBatchNorm1d(\n                c_dim, size_in, norm_method=norm_method)\n            self.bn_1 = CBatchNorm1d(\n                c_dim, size_h, norm_method=norm_method)\n        else:\n            self.bn_0 = CBatchNorm1d_legacy(\n                c_dim, size_in, norm_method=norm_method)\n            self.bn_1 = CBatchNorm1d_legacy(\n                c_dim, size_h, norm_method=norm_method)\n\n        self.fc_0 = nn.Conv1d(size_in, size_h, 1)\n        self.fc_1 = nn.Conv1d(size_h, size_out, 1)\n        self.actvn = nn.ReLU()\n\n        if size_in == size_out:\n            self.shortcut = None\n        else:\n            self.shortcut = nn.Conv1d(size_in, size_out, 1, bias=False)\n        # Initialization\n        nn.init.zeros_(self.fc_1.weight)\n\n    def forward(self, x, c):\n        net = self.fc_0(self.actvn(self.bn_0(x, c)))\n        dx = self.fc_1(self.actvn(self.bn_1(net, c)))\n\n        if self.shortcut is not None:\n            x_s = self.shortcut(x)\n        else:\n            x_s = x\n\n        return x_s + dx\n\n\n\nclass OccupancyNetwork(nn.Module):\n    \'\'\' Occupancy Network class.\n    Args:\n        decoder (nn.Module): decoder network\n        encoder (nn.Module): encoder network\n        p0_z (dist): prior distribution for latent code z\n        device (device): torch device\n    \'\'\'\n\n    def __init__(self, device):\n        super().__init__()\n        self.device = device\n        self.decoder = DecoderCBatchNorm(dim=3, z_dim=0, c_dim=256,\n            hidden_size=256).to(self.device)\n        self.encoder = Resnet18(256, normalize=True, use_linear=True).to(self.device)\n\n        self.p0_z = get_prior_z(self.device)\n\n    def forward(self, p, inputs, sample=True, **kwargs):\n        \'\'\' Performs a forward pass through the network.\n        Args:\n            p (tensor): sampled points\n            inputs (tensor): conditioning input\n            sample (bool): whether to sample for z\n        \'\'\'\n        batch_size = p.size(0)\n        c = self.encode_inputs(inputs)\n        z = self.get_z_from_prior((batch_size,), sample=sample)\n        p_r = self.decode(p, z, c, **kwargs)\n        return p_r\n\n    def compute_elbo(self, p, occ, inputs, **kwargs):\n        \'\'\' Computes the expectation lower bound.\n        Args:\n            p (tensor): sampled points\n            occ (tensor): occupancy values for p\n            inputs (tensor): conditioning input\n        \'\'\'\n        c = self.encode_inputs(inputs)\n        q_z = self.infer_z(p, occ, c, **kwargs)\n        z = q_z.rsample()\n        p_r = self.decode(p, z, c, **kwargs)\n\n        rec_error = -p_r.log_prob(occ).sum(dim=-1)\n        kl = dist.kl_divergence(q_z, self.p0_z).sum(dim=-1)\n        elbo = -rec_error - kl\n\n        return elbo, rec_error, kl\n\n    def encode_inputs(self, inputs):\n        \'\'\' Encodes the input.\n        Args:\n            input (tensor): the input\n        \'\'\'\n        c = self.encoder(inputs)\n       \n\n        return c\n\n    def decode(self, p, z, c, **kwargs):\n        \'\'\' Returns occupancy probabilities for the sampled points.\n        Args:\n            p (tensor): points\n            z (tensor): latent code z\n            c (tensor): latent conditioned code c\n        \'\'\'\n\n        logits = self.decoder(p, z, c, **kwargs)\n        p_r = dist.Bernoulli(logits=logits)\n        return p_r\n\n    def infer_z(self, p, occ, c, **kwargs):\n        \'\'\' Infers z.\n        Args:\n            p (tensor): points tensor\n            occ (tensor): occupancy values for occ\n            c (tensor): latent conditioned code c\n        \'\'\'\n        \n        batch_size = p.size(0)\n        mean_z = torch.empty(batch_size, 0).to(self.device)\n        logstd_z = torch.empty(batch_size, 0).to(self.device)\n\n        q_z = dist.Normal(mean_z, torch.exp(logstd_z))\n\n        return q_z\n\n    def get_z_from_prior(self, size=torch.Size([]), sample=True):\n        \'\'\' Returns z from prior distribution.\n        Args:\n            size (Size): size of z\n            sample (bool): whether to sample\n        \'\'\'\n        if sample:\n            z = self.p0_z.sample(size).to(self.device)\n        else:\n            z = self.p0_z.mean.to(self.device)\n            z = z.expand(*size, *z.size())\n\n        return z\n\n    '"
examples/ImageRecon/OccNet/eval.py,7,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport json\nimport numpy as np\nimport os\nimport torch\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport sys\nfrom tqdm import tqdm\n\nfrom utils import occ_function, collate_fn, extract_mesh\nfrom architectures import OccupancyNetwork\nfrom PIL import Image\nimport kaolin as kal \n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-expid\', type=str, default=\'Direct\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'-device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'-categories\', type=str,nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'-vis\', action=\'store_true\', help=\'Visualize each model while evaluating\')\nparser.add_argument(\'-f_score\', action=\'store_true\', help=\'compute F-score\')\nparser.add_argument(\'-batch_size\', type=int, default=3, help=\'Batch size.\')\n\nargs = parser.parse_args()\n\n\n# Data\nimages_set = kal.dataloader.ShapeNet.Images(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = False,  split = .7, views=1)\npoints_set_valid = kal.dataloader.ShapeNet.Points(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = False, split = .7, num_points=5000 )\nsdf_set = kal.dataloader.ShapeNet.SDF_Points(root= \'../../datasets/\', categories=args.categories, \\\n\tdownload=True, train = False, split = .7, num_points = 100000, occ = True)\ndata_set_mesh = kal.dataloader.ShapeNet.Meshes(root= \'../../datasets/\', \\\n\tcategories=args.categories, download=True, train = False, split = .7)\n\n\nvalid_set = kal.dataloader.ShapeNet.Combination([sdf_set, images_set, data_set_mesh, points_set_valid], root=\'../../datasets/\')\n\ndataloader_val = DataLoader(valid_set, batch_size=5, shuffle=False, num_workers=8, collate_fn=collate_fn)\n\n\n# Model\nmodel = OccupancyNetwork(args.device)\n# Load saved weights\n\nmodel.encoder.load_state_dict(torch.load(\'log/{}/best_encoder.pth\'.format(args.expid)))\n\nmodel.decoder.load_state_dict(torch.load(\'log/{}/best_decoder.pth\'.format(args.expid)))\n\niou_epoch = 0.\nf_epoch = 0.\nnum_batches = 0\nnum_items = 0\n\n\n\nwith torch.no_grad():\n\tmodel.encoder.eval()\n\tmodel.decoder.eval() \n\tfor data in tqdm(dataloader_val):\n\t\timgs = data[\'imgs\'][:,:3].to(args.device)\n\t\tsdf_points = data[\'occ_points\'].to(args.device)\n\t\tsurface_points = data[\'points\'].to(args.device)\n\t\tgt_occ = data[\'occ_values\'].to(args.device)\n\n\n\t\t\n\t\tencoding = model.encode_inputs(imgs)\n\t\tpred_occ = model.decode(sdf_points, torch.zeros(args.batch_size, 0), encoding ).logits\n\n\t\ti = 0 \n\t\tfor sdf_point, gt_oc, pred_oc, gt_surf, code in zip(sdf_points, gt_occ, pred_occ, surface_points, encoding):\n\t\t\t#### compute iou ####\n\t\t\tiou_epoch += float((kal.metrics.point.iou(gt_oc, pred_oc, thresh=.2) / \\\n\t\t\t\tfloat(gt_occ.shape[0])).item())\n\t\t\t\n\t\t\tif args.f_score or args.vis:\n\t\t\t\t\n\t\t\t\t# extract mesh from sdf \n\t\t\t\tsdf = kal.rep.SDF(occ_function(model, code))\n\t\t\t\tvoxelization = kal.conversion.SDF.to_voxel(sdf)\n\t\t\t\tverts, faces = extract_mesh( voxelization, model)\\\n\t\t\t\t# algin new mesh to positions and scale of predicted occupancy\n\t\t\t\tocc_points = sdf_point[pred_oc >= .2]\n\t\t\t\tverts = kal.rep.point.re_align(occ_points, verts.clone())\n\t\t\t\tmesh = kal.rep.TriangleMesh.from_tensors(verts, faces)\n\t\t\t\tif verts.shape[0] == 0: # if mesh is empty count as 0 f-score\n\t\t\t\t\tcontinue \n\n\t\t\t\tif args.vis: \n\n\t\t\t\t\ttgt_verts = data[\'verts\'][i]\n\t\t\t\t\ttgt_faces = data[\'faces\'][i]\n\t\t\t\t\ttgt_mesh = kal.rep.TriangleMesh.from_tensors(tgt_verts, tgt_faces)\n\n\t\t\t\t\tprint (\'Displaying input image\')\n\t\t\t\t\timg = imgs[i].data.cpu().numpy().transpose((2, 1, 0)) * 255\n\t\t\t\t\timg = (img).astype(np.uint8)\n\t\t\t\t\tImage.fromarray(img).show()\n\t\t\t\t\tprint (\'Rendering Target Mesh\')\n\t\t\t\t\tkal.visualize.show_mesh(tgt_mesh)\n\t\t\t\t\tprint (\'Rendering Predicted Mesh\')\n\t\t\t\t\tmesh.show()\n\t\t\t\t\tprint(\'----------------------\')\n\t\t\t\t\tnum_items += 1\n\n\t\t\t\tif args.f_score:\n\t\t\t\t\t#### compute f score #### \n\t\t\t\t\tpred_surf,_ = mesh.sample(5000)\n\t\t\t\t\tf_score = kal.metrics.point.f_score(gt_surf, pred_surf, extend = False)\n\t\t\t\t\tf_epoch += (f_score  / float(gt_occ.shape[0])).item()\n\t\t\ti+= 1\t\t\n\n\t\t\n\t\tnum_batches += 1.\n\nout_iou = iou_epoch / float(num_batches)\nprint (\'IoU over validation set is {0}\'.format(out_iou))\nif args.f_score: \n\tout_f = f_epoch / float(num_batches)\n\tprint (\'F-score over validation set is {0}\'.format(out_f))\n\n\n\n \n\n\n\t\n\n\t'"
examples/ImageRecon/OccNet/train.py,14,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport json\nimport numpy as np\nimport os\nimport torch\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport sys\nfrom utils import preprocess\nfrom tqdm import tqdm\nimport torch.nn.functional as F\n\nimport torch.distributions as dist\nfrom architectures import OccupancyNetwork\n\nimport kaolin as kal \n""""""\nCommandline arguments\n""""""\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-expid\', type=str, default=\'Direct\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'-device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'-categories\', type=str,nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'-epochs\', type=int, default=500, help=\'Number of train epochs.\')\nparser.add_argument(\'-lr\', type=float, default=1e-4, help=\'Learning rate.\')\nparser.add_argument(\'-val-every\', type=int, default=5, help=\'Validation frequency (epochs).\')\nparser.add_argument(\'-batch_size\', type=int, default=64, help=\'batch size\')\nparser.add_argument(\'-print-every\', type=int, default=3, help=\'Print frequency (batches).\')\nparser.add_argument(\'-logdir\', type=str, default=\'log\', help=\'Directory to log data to.\')\nparser.add_argument(\'-save-model\', action=\'store_true\', help=\'Saves the model and a snapshot \\\n\tof the optimizer state.\')\nargs = parser.parse_args()\n\n\n\n""""""\nDataset\n""""""\nsdf_set = kal.dataloader.ShapeNet.SDF_Points(root= \'../../datasets/\', categories=args.categories, \\\n\tdownload=True, train = True, split = .7, num_points = 2024, occ = True)\nimages_set = kal.dataloader.ShapeNet.Images(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = True,  split = .7, views=23, transform= preprocess)\ntrain_set = kal.dataloader.ShapeNet.Combination([sdf_set, images_set], root=\'../../datasets/\')\ndataloader_train = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=8)\n\n\n\nsdf_set = kal.dataloader.ShapeNet.SDF_Points(root= \'../../datasets/\', categories=args.categories, \\\n\tdownload=True, train = False, split = .95, num_points = 100000, occ = True)\nimages_set = kal.dataloader.ShapeNet.Images(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = False,  split = .7, views=1, transform= preprocess)\nvalid_set = kal.dataloader.ShapeNet.Combination([sdf_set, images_set], root=\'../../datasets/\')\n\ndataloader_val = DataLoader(valid_set, batch_size=5, shuffle=False, num_workers=8)\n\n\n""""""\nModel settings \n""""""\n\n\nmodel = OccupancyNetwork(args.device)\nparameters = list(model.encoder.parameters()) +  list(model.decoder.parameters())\noptimizer = optim.Adam(parameters, lr=args.lr)\n\n\n\n""""""\nInitial settings\n""""""\n\n\n\n# Create log directory, if it doesn\'t already exist\nargs.logdir = os.path.join(args.logdir, args.expid)\nif not os.path.isdir(args.logdir):\n\tos.makedirs(args.logdir)\n\tprint(\'Created dir:\', args.logdir)\n\n# Log all commandline args\nwith open(os.path.join(args.logdir, \'args.txt\'), \'w\') as f:\n\tjson.dump(args.__dict__, f, indent=2)\n \n\nclass Engine(object):\n\t""""""Engine that runs training and inference.\n\tArgs\n\t\t- cur_epoch (int): Current epoch.\n\t\t- print_every (int): How frequently (# batches) to print loss.\n\t\t- validate_every (int): How frequently (# epochs) to run validation.\n\t\t\n\t""""""\n\n\tdef __init__(self,  cur_epoch=0, print_every=1, validate_every=1):\n\t\tself.cur_epoch = cur_epoch\n\t\tself.train_loss = []\n\t\tself.val_loss = []\n\t\tself.bestval =0\n\n\tdef train(self):\n\t\tloss_epoch = 0.\n\t\tnum_batches = 0\n\t\tmodel.encoder.train()\n\t\tmodel.decoder.train()\n\n\t\t# Train loop\n\t\tfor i, data in enumerate(tqdm(dataloader_train), 0):\n\t\t\toptimizer.zero_grad()\n\t\t\t\n\t\t\t###############################\n\t\t\t####### data creation #########\n\t\t\t###############################\n\t\t\timgs = data[\'imgs\'][:,:3].to(args.device)\n\t\t\tpoints = data[\'occ_points\'].to(args.device)\n\t\t\tgt_occ = data[\'occ_values\'].to(args.device)\n\n\t\t\t###############################\n\t\t\t########## inference ##########\n\t\t\t###############################\n\t\t\tencoding = model.encode_inputs(imgs)\n\t\t\tpred_occ = model.decode(points, torch.zeros(args.batch_size, 0), encoding ).logits\n\n\t\t\t###############################\n\t\t\t########## losses #############\n\t\t\t###############################\n\t\t\tloss = F.binary_cross_entropy_with_logits(pred_occ, gt_occ).mean()\t\n\t\t\tloss.backward()\n\t\t\tloss_epoch += float(loss.item())\n\n\t\t\t\n\t\t\tnum_batches += 1\n\t\t\tif i % args.print_every == 0:\n\t\t\t\tmessage = f\'[TRAIN] Epoch {self.cur_epoch:03d}, Batch {i:03d}:, Loss: {(loss.item()):4.3f}\'\n\t\t\t\ttqdm.write(message)\n\t\t\toptimizer.step()\n\t\t\n\t\t\n\t\tloss_epoch = loss_epoch / num_batches\n\t\tself.train_loss.append(loss_epoch)\n\t\tself.cur_epoch += 1\n\n\t\n\n\tdef validate_iou(self):\n\t\tmodel.encoder.eval()\n\t\tmodel.decoder.eval()\n\t\twith torch.no_grad():\t\n\t\t\tnum_batches = 0\n\t\t\tiou_epoch = 0.\n\n\t\t\t# Validation loop\n\t\t\tfor i, data in enumerate(tqdm(dataloader_val), 0):\n\t\t\t\toptimizer.zero_grad()\n\t\t\t\t\n\t\t\t\t###############################\n\t\t\t\t####### data creation #########\n\t\t\t\t###############################\n\t\t\t\timgs = data[\'imgs\'][:,:3].to(args.device)\n\t\t\t\tpoints = data[\'occ_points\'].to(args.device)\n\t\t\t\tgt_occ = data[\'occ_values\'].to(args.device)\n\n\t\t\t\t###############################\n\t\t\t\t########## inference ##########\n\t\t\t\t###############################\n\t\t\t\tencoding = model.encode_inputs(imgs)\n\t\t\t\tpred_occ = model.decode(points, torch.zeros(args.batch_size, 0), encoding ).logits\n\n\t\t\t\t###############################\n\t\t\t\t########## losses #############\n\t\t\t\t###############################\n\t\t\t\n\t\t\t\tfor pt1, pt2 in zip(gt_occ, pred_occ):\n\t\t\t\t\tiou_epoch += float((kal.metrics.point.iou(pt1, pt2, thresh=.2) / float(gt_occ.shape[0])).item())\n\t\t\t\t\n\n\t\t\t\tnum_batches += 1\n\t\t\t\tif i % args.print_every == 0:\n\t\t\t\t\tout_loss = iou_epoch / float(num_batches)\n\t\t\t\t\ttqdm.write(f\'[VAL IoU] Epoch {self.cur_epoch:03d}, Batch {i:03d}: iou: {out_loss:3.3f}\')\n\t\t\t\t\t\n\t\t\tout_loss = iou_epoch / float(num_batches)\n\t\t\ttqdm.write(f\'[VAL IoU Total] Epoch {self.cur_epoch:03d}, Batch {i:03d}: iou: {out_loss:3.3f}\')\n\n\t\t\tself.val_loss.append(out_loss)\n\n\tdef save(self):\n\n\t\tsave_best = False\n\t\tif self.val_loss[-1] >= self.bestval:\n\t\t\tself.bestval = self.val_loss[-1]\n\t\t\tsave_best = True\n\t\t\n\t\t# Create a dictionary of all data to save\n\t\tlog_table = {\n\t\t\t\'epoch\': self.cur_epoch,\n\t\t\t\'bestval\': self.bestval,\n\t\t\t\'train_loss\': self.train_loss,\n\t\t\t\'val_loss\': self.val_loss,\n\t\t\t\'train_metrics\': [\'Chamfer\'],\n\t\t\t\'val_metrics\': [\'Chamfer\'],\n\t\t}\n\n\t\t# Save the recent model/optimizer states\n\t\t\n\t\ttorch.save(model.encoder.state_dict(), os.path.join(args.logdir, \'encoder.pth\'))\n\t\ttorch.save(model.decoder.state_dict(), os.path.join(args.logdir, \'decoder.pth\'))\n\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, \'recent_optim.pth\'))\n\t\t# Log other data corresponding to the recent model\n\t\twith open(os.path.join(args.logdir, \'recent.log\'), \'w\') as f:\n\t\t\tf.write(json.dumps(log_table))\n\n\t\ttqdm.write(\'====== Saved recent model ======>\')\n\t\t\n\t\tif save_best:\n\t\t\ttorch.save(model.encoder.state_dict(), os.path.join(args.logdir, \'best_encoder.pth\'))\n\t\t\ttorch.save(model.decoder.state_dict(), os.path.join(args.logdir, \'best_decoder.pth\'))\n\t\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, \'best_optim.pth\'))\n\t\t\ttqdm.write(\'====== Overwrote best model ======>\')\n\t\t\t\n\t\ntrainer = Engine()\n\nfor epoch in range(args.epochs): \n\ttrainer.train()\n\tif epoch % 4 == 0: \n\t\ttrainer.validate_iou()\n\t\ttrainer.save()\n\t\t\n\n\n'"
examples/ImageRecon/OccNet/utils.py,5,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch \nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\nimport kaolin as kal\nimport trimesh\nimport time \nfrom kaolin import mcubes\n\n\n\n\npreprocess = transforms.Compose([\n   transforms.CenterCrop(224),\n   transforms.ToTensor()\n])\n\n\ndef get_prior_z(cfg, device, **kwargs):\n    \'\'\' Returns prior distribution for latent code z.\n    Args:\n        cfg (dict): imported yaml config\n        device (device): pytorch device\n    \'\'\'\n    z_dim = 0\n    p0_z = dist.Normal(\n        torch.zeros(z_dim, device=device),\n        torch.ones(z_dim, device=device)\n    )\n\n    return p0_z\n\n\ndef occ_function(model,code):\n\tz = torch.zeros(1, 0)\n\n\tdef eval_query(query):\n\t\tpred_occ = model.decode(query.unsqueeze(0), z, code.unsqueeze(0) ).logits[0]\n         # values less then .2 are sent to 1 and above(occupied ) are set to 0 -> part fo surface \n\t\tvalues = pred_occ < .2\n\t\t\n\n\t\treturn values\n\n\t\t\n\treturn eval_query \n\n\ndef collate_fn(data): \n\tnew_data = {}\n\tfor k in data[0].keys():\n\t\t\n\t\tif k in [\'occ_points\',\'occ_values\', \'imgs\', \'points\']:\n\t\t\tnew_info = tuple(d[k] for d in data)\n\t\t\tnew_info = torch.stack(new_info, 0)\n\t\telse: \n\t\t\tnew_info = tuple(d[k] for d in data)\n\n\t\tnew_data[k] = new_info\n\treturn new_data\n\n\ndef extract_mesh(occ_hat, model, c=None, stats_dict=dict()):\n    \n    n_x, n_y, n_z = occ_hat.shape\n    box_size = 1 + .05\n    threshold = .2\n    # Make sure that mesh is watertight\n    t0 = time.time()\n    occ_hat_padded = np.pad(\n        occ_hat, 1, \'constant\', constant_values=-1e6)\n    vertices, triangles = mcubes.marching_cubes(\n        occ_hat_padded, threshold)\n    return torch.FloatTensor(vertices.astype(float)).cuda(), torch.LongTensor(triangles.astype(int)).cuda()\n\n'"
examples/ImageRecon/Pixel2Mesh/architectures.py,11,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\n\nimport torch \nfrom torch import nn \nfrom torch.nn.parameter import Parameter\nimport torch.nn.functional as F\n\nimport torch\nfrom torch.nn import Parameter\n\n\n\n# from torch_geometric.nn import GCNConv\n\nclass VGG(nn.Module):\n    \n    def __init__(self, channels = 4):\n        \n        super(VGG, self).__init__()\n\n        self.layer0_1 = nn.Conv2d(channels, 16, 3, stride = 1, padding = 1)\n        self.layer0_2 = nn.Conv2d(16, 16, 3, stride = 1, padding = 1)\n        \n        self.layer1_1 = nn.Conv2d(16, 32, 3, stride = 2, padding = 1) \n        self.layer1_2 = nn.Conv2d(32, 32, 3, stride = 1, padding = 1)\n        self.layer1_3 = nn.Conv2d(32, 32, 3, stride = 1, padding = 1)\n        \n        self.layer2_1 = nn.Conv2d(32, 64, 3, stride = 2, padding = 1) \n        self.layer2_2 = nn.Conv2d(64, 64, 3, stride = 1, padding = 1)\n        self.layer2_3 = nn.Conv2d(64, 64, 3, stride = 1, padding = 1)\n        \n        self.layer3_1 = nn.Conv2d(64, 128, 3, stride = 2, padding = 1) \n        self.layer3_2 = nn.Conv2d(128, 128, 3, stride = 1, padding = 1)\n        self.layer3_3 = nn.Conv2d(128, 128, 3, stride = 1, padding = 1)\n        \n        self.layer4_1 = nn.Conv2d(128, 256, 5, stride = 2, padding = 2) \n        self.layer4_2 = nn.Conv2d(256, 256, 3, stride = 1, padding = 1)\n        self.layer4_3 = nn.Conv2d(256, 256, 3, stride = 1, padding = 1)\n        \n        self.layer5_1 = nn.Conv2d(256, 512, 5, stride = 2, padding = 2) \n        self.layer5_2 = nn.Conv2d(512, 512, 3, stride = 1, padding = 1)\n        self.layer5_3 = nn.Conv2d(512, 512, 3, stride = 1, padding = 1)\n        self.layer5_4 = nn.Conv2d(512, 512, 3, stride = 1, padding = 1)\n        \n    def forward(self, img):\n        \n        img = F.relu(self.layer0_1(img))\n        img = F.relu(self.layer0_2(img))\n        \n        img = F.relu(self.layer1_1(img))\n        img = F.relu(self.layer1_2(img))\n        img = F.relu(self.layer1_3(img))\n        \n        img = F.relu(self.layer2_1(img))\n        img = F.relu(self.layer2_2(img))\n        img = F.relu(self.layer2_3(img))\n        A = torch.squeeze(img) \n        \n        img = F.relu(self.layer3_1(img))\n        img = F.relu(self.layer3_2(img))\n        img = F.relu(self.layer3_3(img))\n        B = torch.squeeze(img) \n        \n        img = F.relu(self.layer4_1(img))\n        img = F.relu(self.layer4_2(img))\n        img = F.relu(self.layer4_3(img))\n        C = torch.squeeze(img) \n        \n        img = F.relu(self.layer5_1(img))\n        img = F.relu(self.layer5_2(img))\n        img = F.relu(self.layer5_3(img))\n        img = F.relu(self.layer5_4(img))\n        D = torch.squeeze(img) \n        \n        return [A, B, C, D]\n\n\n\n\n\nclass G_Res_Net(nn.Module):\n    def __init__(self, input_features, hidden = 128, output_features = 3):\n        super(G_Res_Net, self).__init__()\n        self.gc1 = GCN(input_features, hidden)\n        self.gc2 = GCN(hidden, hidden)\n        self.gc3 = GCN(hidden , hidden)\n        self.gc4 = GCN(hidden, hidden)\n        self.gc5 = GCN(hidden , hidden)\n        self.gc6 = GCN(hidden, hidden)\n        self.gc7 = GCN(hidden , hidden)\n        self.gc8 = GCN(hidden, hidden)\n        self.gc9 = GCN(hidden , hidden)\n        self.gc10 = GCN(hidden, hidden)\n        self.gc11 = GCN(hidden , hidden)\n        self.gc12 = GCN(hidden, hidden)\n        self.gc13 = GCN(hidden , hidden)\n        self.gc14 = GCN(hidden,  output_features)\n        self.hidden = hidden\n    def forward(self, features, adj):\n        features = features.unsqueeze(0)\n\n        x = (F.relu(self.gc1(features, adj)))\n        x = (F.relu(self.gc2(x, adj)))\n        features = features[..., :self.hidden] + x\n        features /= 2.\n        # 2\n        x = (F.relu(self.gc3(features, adj)))\n        x = (F.relu(self.gc4(x, adj)))\n        features = features + x\n        features /= 2.\n        # 3\n        x = (F.relu(self.gc5(features, adj)))\n        x = (F.relu(self.gc6(x, adj)))\n        features = features + x\n        features /= 2.\n\n        # 4\n        x = (F.relu(self.gc7(features, adj)))\n        x = (F.relu(self.gc8(x, adj)))\n        features = features + x\n        features /= 2.\n\n        # 5\n        x = (F.relu(self.gc9(features, adj)))\n        x = (F.relu(self.gc10(x, adj)))\n        features = features + x\n        features /= 2.\n\n        # 6\n        x = (F.relu(self.gc11(features, adj)))\n        x = (F.relu(self.gc12(x, adj)))\n        features = features + x\n        features /= 2.\n\n        # 7\n        x = (F.relu(self.gc13(features, adj)))\n\n        features = features + x\n        features /= 2.\n\n        coords = (self.gc14(features, adj))\n        return coords.squeeze(0),features.squeeze(0)\n\nclass GCN(nn.Module):\n    """"""\n    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n    """"""\n    def __init__(self, in_features, out_features):\n        super(GCN, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.Tensor(in_features, out_features))\n        self.bias = Parameter(torch.Tensor(out_features))\n      \n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = .6 / math.sqrt((self.weight.size(1) + self.weight.size(0)))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-.1, .1)\n\n    def forward(self, input, adj):\n\n        support = torch.bmm(input, self.weight.unsqueeze(0).expand(input.shape[0], -1, -1))\n\n        output = torch.bmm(adj.unsqueeze(0).expand(input.shape[0], -1, -1), support)\n\n        \n        output = output + self.bias\n        return output\n\n    def __repr__(self):\n        return self.__class__.__name__ + \' (\' \\\n               + str(self.in_features) + \' -> \' \\\n               + str(self.out_features) + \')\'\n\n\n\n\n\n\n\n\n'"
examples/ImageRecon/Pixel2Mesh/eval.py,7,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport os\nimport torch\nimport sys\nfrom tqdm import tqdm\nfrom PIL import Image\nimport numpy as np\n\nfrom torch.utils.data import DataLoader\n\nfrom utils import preprocess, pooling, get_pooling_index\nfrom utils import setup_meshes, split\nfrom architectures import VGG as Encoder, G_Res_Net\nimport kaolin as kal \n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-expid\', type=str, default=\'Direct\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'-device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'-categories\', type=str,nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'-vis\', action=\'store_true\', help=\'Visualize each model while evaluating\')\nparser.add_argument(\'-batchsize\', type=int, default=1, help=\'Batch size.\')\nparser.add_argument(\'-f_score\', action=\'store_true\', help=\'compute F-score\')\nargs = parser.parse_args()\n\n\n# Data\npoints_set_valid = kal.dataloader.ShapeNet.Points(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = False, split = .7, num_points=5000 )\nimages_set_valid = kal.dataloader.ShapeNet.Images(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = False,  split = .7, views=1, transform= preprocess )\nmeshes_set_valid = kal.dataloader.ShapeNet.Meshes(root =\'../../datasets/\', categories =args.categories , \\\n\tdownload = True, train = False,  split = .7)\nvalid_set = kal.dataloader.ShapeNet.Combination([points_set_valid, images_set_valid, meshes_set_valid], root=\'../../datasets/\')\n\n\n\n# Model\nmeshes = setup_meshes(filename=\'meshes/156.obj\', device =args.device)\n\nencoder = Encoder().to(args.device)\nmesh_update_kernels = [963, 1091, 1091] \nmesh_updates = [G_Res_Net(mesh_update_kernels[i], hidden = 128, output_features = 3).to(args.device) for i in range(3)]\n\n# Load saved weights\nencoder.load_state_dict(torch.load(\'log/{0}/best_encoder.pth\'.format(args.expid)))\nfor i,m in enumerate(mesh_updates):\n\tm.load_state_dict(torch.load(\'log/{}/best_mesh_update_{}.pth\'.format(args.expid, i)))\nencoding_dims = [56, 28, 14, 7]\n\nloss_epoch = 0.\nf_epoch = 0.\nnum_batches = 0\nnum_items = 0\nloss_fn = kal.metrics.point.chamfer_distance\n\nencoder.eval(), [m.eval() for m in mesh_updates]\nwith torch.no_grad():\n\tfor data in tqdm(valid_set): \n\t\t# data creation\n\t\ttgt_points = data[\'points\'].to(args.device)\n\t\tinp_images = data[\'imgs\'].to(args.device).unsqueeze(0)\n\t\tcam_mat = data[\'cam_mat\'].to(args.device)\n\t\tcam_pos = data[\'cam_pos\'].to(args.device)\n\n\t\t\n\t\t###############################\n\t\t########## inference ##########\n\t\t###############################\n\t\timg_features = encoder(inp_images)\n\t\t\n\t\t##### layer_1 ##### \n\t\tpool_indices = get_pooling_index(meshes[\'init\'][0].vertices, cam_mat, cam_pos, encoding_dims)\n\t\tprojected_image_features = pooling(img_features, pool_indices)\n\t\tfull_vert_features = torch.cat((meshes[\'init\'][0].vertices, projected_image_features), dim = 1)\n\t\t\n\n\t\tpred_verts, future_features = mesh_updates[0](full_vert_features, meshes[\'adjs\'][0])\n\t\tmeshes[\'update\'][0].vertices = pred_verts.clone()\n\n\t\t##### layer_2 ##### \n\t\tfuture_features = split(meshes, future_features, 0)\n\t\tpool_indices = get_pooling_index(meshes[\'init\'][1].vertices, cam_mat, cam_pos, encoding_dims)\n\t\tprojected_image_features = pooling(img_features, pool_indices)\n\t\tfull_vert_features = torch.cat((meshes[\'init\'][1].vertices, projected_image_features, future_features), dim = 1)\n\t\t\n\t\tpred_verts, future_features = mesh_updates[1](full_vert_features, meshes[\'adjs\'][1])\n\t\tmeshes[\'update\'][1].vertices = pred_verts.clone()\n\n\t\t##### layer_3 ##### \n\t\tfuture_features = split(meshes, future_features, 1)\n\t\tpool_indices = get_pooling_index(meshes[\'init\'][2].vertices, cam_mat, cam_pos, encoding_dims)\n\t\tprojected_image_features = pooling(img_features, pool_indices)\n\t\tfull_vert_features = torch.cat((meshes[\'init\'][2].vertices, projected_image_features, future_features), dim = 1)\n\n\t\tpred_verts, future_features = mesh_updates[2](full_vert_features, meshes[\'adjs\'][2])\n\t\tmeshes[\'update\'][2].vertices = pred_verts.clone()\n\t\t\n\n\t\t \t\n\t\tmeshes[\'update\'][2].vertices = pred_verts.clone()\n\t\tpred_points, _ = meshes[\'update\'][2].sample(5000)\n\t\n\t\tloss = 3000 * kal.metrics.point.chamfer_distance(pred_points, tgt_points)\n\n\t\tif args.vis: \n\t\t\ttgt_mesh = meshes_set_valid[num_items]\n\t\t\ttgt_verts = tgt_mesh[\'verts\']\n\t\t\ttgt_faces = tgt_mesh[\'faces\']\n\t\t\ttgt_mesh = kal.rep.TriangleMesh.from_tensors(tgt_verts, tgt_faces)\n\n\t\t\tprint (\'Displaying input image\')\n\t\t\timg = inp_images[0].data.cpu().numpy().transpose((1, 2, 0))\n\t\t\timg = (img*255.).astype(np.uint8)\n\t\t\tImage.fromarray(img).show()\n\t\t\tprint (\'Rendering Target Mesh\')\n\t\t\tkal.visualize.show_mesh(tgt_mesh)\n\t\t\tprint (\'Rendering Predicted Mesh\')\n\t\t\tkal.visualize.show_mesh(meshes[\'update\'][2])\n\t\t\tprint(\'----------------------\')\n\t\t\tnum_items += 1\n\n\t\tif args.f_score: \n\t\t\t#### compute f score #### \n\t\t\tf_score = kal.metrics.point.f_score(tgt_points, pred_points, extend = False)\n\t\t\tf_epoch += (f_score  / float(args.batchsize)).item()\n\n\t\tloss_epoch += loss.item()\n\t\t\n\t\t\n\t\tnum_batches += 1.\n\nout_loss = loss_epoch / float(num_batches)\nprint (\'Loss over validation set is {0}\'.format(out_loss))\nif args.f_score: \n\tout_f = f_epoch / float(num_batches)\n\tprint (\'F-score over validation set is {0}\'.format(out_f))\n'"
examples/ImageRecon/Pixel2Mesh/train.py,16,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport json\nimport numpy as np\nimport os\nimport torch\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport sys\nfrom tqdm import tqdm\n\nfrom utils import preprocess, pooling, get_pooling_index\nfrom utils import setup_meshes, split\nfrom utils import loss_surf, loss_edge, loss_lap, loss_norm\nfrom architectures import VGG as Encoder, G_Res_Net\n\nimport kaolin as kal \n""""""\nCommandline arguments\n""""""\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-expid\', type=str, default=\'Direct\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'-device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'-categories\', type=str,nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'-epochs\', type=int, default=50, help=\'Number of train epochs.\')\nparser.add_argument(\'-lr\', type=float, default=1e-4, help=\'Learning rate.\')\nparser.add_argument(\'-val-every\', type=int, default=5, help=\'Validation frequency (epochs).\')\nparser.add_argument(\'-print-every\', type=int, default=20, help=\'Print frequency (batches).\')\nparser.add_argument(\'-logdir\', type=str, default=\'log\', help=\'Directory to log data to.\')\nparser.add_argument(\'-save-model\', action=\'store_true\', help=\'Saves the model and a snapshot \\\n\tof the optimizer state.\')\nargs = parser.parse_args()\n\n\n\n""""""\nDataset\n""""""\n\npoints_set = kal.dataloader.ShapeNet.Points(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = True, split = .7, num_points=2466 )\nimages_set = kal.dataloader.ShapeNet.Images(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = True,  split = .7, views=23, transform= preprocess )\ntrain_set = kal.dataloader.ShapeNet.Combination([points_set, images_set], root=\'../../datasets/\')\n\ndataloader_train = DataLoader(train_set, batch_size=1, shuffle=True, \n\tnum_workers=8)\n\n\npoints_set_valid = kal.dataloader.ShapeNet.Points(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = False, split = .7, num_points=10000 )\nimages_set_valid = kal.dataloader.ShapeNet.Images(root =\'../../datasets/\',categories =args.categories , \\\n\tdownload = True, train = False,  split = .7, views=1, transform= preprocess )\nvalid_set = kal.dataloader.ShapeNet.Combination([points_set_valid, images_set_valid], root=\'../../datasets/\')\n\ndataloader_val = DataLoader(valid_set, batch_size=1, shuffle=False, \n\tnum_workers=8)\n\n\n\n""""""\nModel settings \n""""""\nmeshes = setup_meshes(filename=\'meshes/156.obj\', device= args.device)\n\n\n\nencoder = Encoder().to(args.device)\nmesh_update_kernels = [963, 1091, 1091] \nmesh_updates = [G_Res_Net(mesh_update_kernels[i], hidden = 128, output_features = 3).to(args.device) for i in range(3)]\n\nparameters = list(encoder.parameters()) \nfor i in range(3): \n\tparameters += list(mesh_updates[i].parameters())\noptimizer = optim.Adam(parameters, lr=args.lr)\n\nencoding_dims = [56, 28, 14, 7]\n\n\n\n""""""\nInitial settings\n""""""\n\n\n\n# Create log directory, if it doesn\'t already exist\nargs.logdir = os.path.join(args.logdir, args.expid)\nif not os.path.isdir(args.logdir):\n\tos.makedirs(args.logdir)\n\tprint(\'Created dir:\', args.logdir)\n\n# Log all commandline args\nwith open(os.path.join(args.logdir, \'args.txt\'), \'w\') as f:\n\tjson.dump(args.__dict__, f, indent=2)\n \n\nclass Engine(object):\n\t""""""Engine that runs training and inference.\n\tArgs\n\t\t- cur_epoch (int): Current epoch.\n\t\t- print_every (int): How frequently (# batches) to print loss.\n\t\t- validate_every (int): How frequently (# epochs) to run validation.\n\t\t\n\t""""""\n\n\tdef __init__(self,  cur_epoch=0, print_every=1, validate_every=1):\n\t\tself.cur_epoch = cur_epoch\n\t\tself.train_loss = []\n\t\tself.val_loss = []\n\t\tself.bestval = 1000.\n\n\tdef train(self):\n\t\tloss_epoch = 0.\n\t\tnum_batches = 0\n\t\tencoder.train(), [m.train() for m in mesh_updates]\n\n\t\t# Train loop\n\t\tfor i, data in enumerate(tqdm(dataloader_train), 0):\n\t\t\toptimizer.zero_grad()\n\t\t\t\n\t\t\t###############################\n\t\t\t####### data creation #########\n\t\t\t###############################\n\t\t\ttgt_points = data[\'points\'].to(args.device)[0]\n\t\t\ttgt_norms = data[\'normals\'].to(args.device)[0]\n\t\t\tinp_images = data[\'imgs\'].to(args.device)\n\t\t\tcam_mat = data[\'cam_mat\'].to(args.device)[0]\n\t\t\tcam_pos = data[\'cam_pos\'].to(args.device)[0]\n\n\t\t\t###############################\n\t\t\t########## inference ##########\n\t\t\t###############################\n\t\t\timg_features = encoder(inp_images)\n\n\t\t\t\n\t\t\t##### layer_1 ##### \n\t\t\tpool_indices = get_pooling_index(meshes[\'init\'][0].vertices, cam_mat, cam_pos, encoding_dims)\n\t\t\tprojected_image_features = pooling(img_features, pool_indices)\n\t\t\tfull_vert_features = torch.cat((meshes[\'init\'][0].vertices, projected_image_features), dim = 1)\n\t\t\t\n\n\t\t\tpred_verts, future_features = mesh_updates[0](full_vert_features, meshes[\'adjs\'][0])\n\t\t\tmeshes[\'update\'][0].vertices = pred_verts.clone()\n\n\t\t\t##### layer_2 ##### \n\t\t\tfuture_features = split(meshes, future_features, 0)\n\t\t\tpool_indices = get_pooling_index(meshes[\'init\'][1].vertices, cam_mat, cam_pos, encoding_dims)\n\t\t\tprojected_image_features = pooling(img_features, pool_indices)\n\t\t\tfull_vert_features = torch.cat((meshes[\'init\'][1].vertices, projected_image_features, future_features), dim = 1)\n\t\t\t\n\t\t\tpred_verts, future_features = mesh_updates[1](full_vert_features, meshes[\'adjs\'][1])\n\t\t\tmeshes[\'update\'][1].vertices = pred_verts.clone()\n\n\t\t\t##### layer_3 ##### \n\t\t\tfuture_features = split(meshes, future_features, 1)\n\t\t\tpool_indices = get_pooling_index(meshes[\'init\'][2].vertices, cam_mat, cam_pos, encoding_dims)\n\t\t\tprojected_image_features = pooling(img_features, pool_indices)\n\t\t\tfull_vert_features = torch.cat((meshes[\'init\'][2].vertices, projected_image_features, future_features), dim = 1)\n\n\t\t\tpred_verts, future_features = mesh_updates[2](full_vert_features, meshes[\'adjs\'][2])\n\t\t\tmeshes[\'update\'][2].vertices = pred_verts.clone()\n\n\t\n\t\t\n\t\t\t###############################\n\t\t\t########## losses #############\n\t\t\t###############################\n\t\t\tsurf_loss = 3000 * loss_surf(meshes, tgt_points)\n\t\t\tedge_loss = 300  * loss_edge(meshes) \n\t\t\tlap_loss  = 1500 * loss_lap(meshes)\n\t\t\tnorm_loss = .5\t * loss_norm(meshes, tgt_points, tgt_norms)\n\t\t\tloss = surf_loss + edge_loss + lap_loss + norm_loss\n\t\t\tloss.backward()\n\t\t\tloss_epoch += float(surf_loss.item())\n\n\t\t\t# logging\n\t\t\tnum_batches += 1\n\t\t\tif i % args.print_every == 0:\n\t\t\t\tf_loss = kal.metrics.point.f_score(meshes[\'update\'][2].sample(2466)[0],tgt_points,  extend=False) \n\t\t\t\tmessage = f\'[TRAIN] Epoch {self.cur_epoch:03d}, Batch {i:03d}:, Loss: {(surf_loss.item()):4.3f}, \'\n\t\t\t\tmessage = message + f\'Lap: {(lap_loss.item()):3.3f}, Edge: {(edge_loss.item()):3.3f}, Norm: {(norm_loss.item()):3.3f}\'\n\t\t\t\tmessage = message + f\' F: {(f_loss.item()):3.3f}\'\n\t\t\t\ttqdm.write(message)\n\t\t\toptimizer.step()\n\t\t\n\t\t\n\t\tloss_epoch = loss_epoch / num_batches\n\t\tself.train_loss.append(loss_epoch)\n\t\tself.cur_epoch += 1\n\n\t\t\n\t\t\n\tdef validate(self):\n\t\tencoder.eval(), [m.eval() for m in mesh_updates]\n\t\twith torch.no_grad():\t\n\t\t\tnum_batches = 0\n\t\t\tloss_epoch = 0.\n\t\t\tf_loss = 0.\n\n\t\t\t# Validation loop\n\t\t\tfor i, data in enumerate(tqdm(dataloader_val), 0):\n\t\t\t\toptimizer.zero_grad()\n\t\t\t\t\n\t\t\t\t# data creation\n\t\t\t\ttgt_points = data[\'points\'].to(args.device)[0]\n\t\t\t\tinp_images = data[\'imgs\'].to(args.device)\n\t\t\t\tcam_mat = data[\'cam_mat\'].to(args.device)[0]\n\t\t\t\tcam_pos = data[\'cam_pos\'].to(args.device)[0]\n\n\t\t\t\t\n\t\t\t\t###############################\n\t\t\t\t########## inference ##########\n\t\t\t\t###############################\n\t\t\t\timg_features = encoder(inp_images)\n\t\t\t\t\n\t\t\t\t##### layer_1 ##### \n\t\t\t\tpool_indices = get_pooling_index(meshes[\'init\'][0].vertices, cam_mat, cam_pos, encoding_dims)\n\t\t\t\tprojected_image_features = pooling(img_features, pool_indices)\n\t\t\t\tfull_vert_features = torch.cat((meshes[\'init\'][0].vertices, projected_image_features), dim = 1)\n\t\t\t\t\n\n\t\t\t\tpred_verts, future_features = mesh_updates[0](full_vert_features, meshes[\'adjs\'][0])\n\t\t\t\tmeshes[\'update\'][0].vertices = pred_verts.clone()\n\n\t\t\t\t##### layer_2 ##### \n\t\t\t\tfuture_features = split(meshes, future_features, 0)\n\t\t\t\tpool_indices = get_pooling_index(meshes[\'init\'][1].vertices, cam_mat, cam_pos, encoding_dims)\n\t\t\t\tprojected_image_features = pooling(img_features, pool_indices)\n\t\t\t\tfull_vert_features = torch.cat((meshes[\'init\'][1].vertices, projected_image_features, future_features), dim = 1)\n\t\t\t\t\n\t\t\t\tpred_verts, future_features = mesh_updates[1](full_vert_features, meshes[\'adjs\'][1])\n\t\t\t\tmeshes[\'update\'][1].vertices = pred_verts.clone()\n\n\t\t\t\t##### layer_3 ##### \n\t\t\t\tfuture_features = split(meshes, future_features, 1)\n\t\t\t\tpool_indices = get_pooling_index(meshes[\'init\'][2].vertices, cam_mat, cam_pos, encoding_dims)\n\t\t\t\tprojected_image_features = pooling(img_features, pool_indices)\n\t\t\t\tfull_vert_features = torch.cat((meshes[\'init\'][2].vertices, projected_image_features, future_features), dim = 1)\n\n\t\t\t\tpred_verts, future_features = mesh_updates[2](full_vert_features, meshes[\'adjs\'][2])\n\t\t\t\tmeshes[\'update\'][2].vertices = pred_verts.clone()\n\t\t\t\t\n\t\t\t\tf_loss += kal.metrics.point.f_score(meshes[\'update\'][2].sample(2466)[0],tgt_points,  extend=False)\n\n\t\t\t\n\t\t\t\t###############################\n\t\t\t\t########## losses #############\n\t\t\t\t###############################\n\t\t\t\tsurf_loss = 3000 * kal.metrics.point.chamfer_distance(pred_verts.clone(), tgt_points)\n\t\t\t\tloss_epoch += surf_loss.item()\n\n\t\t\t\t# logging\n\t\t\t\tnum_batches += 1\n\t\t\t\tif i % args.print_every == 0:\n\t\t\t\t\t\tout_loss = loss_epoch / float(num_batches)\n\t\t\t\t\t\tf_out_loss = f_loss / float(num_batches)\n\t\t\t\t\t\ttqdm.write(f\'[VAL] Epoch {self.cur_epoch:03d}, Batch {i:03d}: loss: {out_loss:3.3f}, F: {(f_out_loss.item()):3.3f}\')\n\t\t\t\t\t\t\n\t\t\tout_loss = loss_epoch / float(num_batches)\n\t\t\tf_out_loss = f_loss / float(num_batches)\n\t\t\ttqdm.write(f\'[VAL Total] Epoch {self.cur_epoch:03d}, Batch {i:03d}: loss: {out_loss:3.3f}, F: {(f_out_loss.item()):3.3f}\')\n\n\t\t\tself.val_loss.append(out_loss)\n\n\tdef save(self):\n\n\t\tsave_best = False\n\t\tif self.val_loss[-1] <= self.bestval:\n\t\t\tself.bestval = self.val_loss[-1]\n\t\t\tsave_best = True\n\t\t\n\t\t# Create a dictionary of all data to save\n\t\tlog_table = {\n\t\t\t\'epoch\': self.cur_epoch,\n\t\t\t\'bestval\': self.bestval,\n\t\t\t\'train_loss\': self.train_loss,\n\t\t\t\'val_loss\': self.val_loss\n\t\t}\n\n\t\t# Save the recent model/optimizer states\n\t\ttorch.save(encoder.state_dict(), os.path.join(args.logdir, \'encoder.pth\'))\n\t\tfor i,m in enumerate(mesh_updates):\n\t\t\ttorch.save(m.state_dict(), os.path.join(args.logdir, \'mesh_update_{}.pth\'.format(i)))\n\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, \'recent_optim.pth\'))\n\t\t# Log other data corresponding to the recent model\n\t\twith open(os.path.join(args.logdir, \'recent.log\'), \'w\') as f:\n\t\t\tf.write(json.dumps(log_table))\n\n\t\ttqdm.write(\'====== Saved recent model ======>\')\n\t\t\n\t\tif save_best:\n\t\t\ttorch.save(encoder.state_dict(), os.path.join(args.logdir, \'best_encoder.pth\'))\n\t\t\tfor i,m in enumerate(mesh_updates):\n\t\t\t\ttorch.save(m.state_dict(), os.path.join(args.logdir, \'best_mesh_update_{}.pth\'.format(i)))\n\t\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, \'best_optim.pth\'))\n\t\t\ttqdm.write(\'====== Overwrote best model ======>\')\n\t\t\t\n\t\ntrainer = Engine()\n\nfor epoch in range(args.epochs): \n\ttrainer.train()\n\tif epoch % 4 == 0: \n\t\ttrainer.validate()\n\t\ttrainer.save()\n\t\t\n\t\t'"
examples/ImageRecon/Pixel2Mesh/utils.py,26,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch \nfrom torchvision import transforms\nfrom torchvision.transforms import Normalize as norm \nimport trimesh\nfrom sklearn.preprocessing import normalize\nimport kaolin as kal\nfrom PIL import Image\nfrom collections import defaultdict\nimport numpy as np\nfrom kaolin.rep import TriangleMesh\nimport kaolin as kal\n\n\n\n\npreprocess = transforms.Compose([\n   transforms.Resize(224),\n   transforms.ToTensor()\n])\n\n\n\ndef get_pooling_index( positions, cam_mat, cam_pos, dims):\n\n\t#project points into 2D\n\tpositions = positions * .57 # accounting for recaling in 3Dr2n\n\tpositions = positions - cam_pos \n\tpositions = torch.mm(positions,cam_mat.permute(1,0))\n\tpositions_xs =  positions[:, 1] / positions[:, 2]\n\tpositions_ys = -positions[:, 0] / positions[:, 2] \n\n\t# do bilinear interpolation over pixel coordiantes\n\tdata_meta = defaultdict(list)\n\n\n\tfor dim in dims:\n\t\tfocal_length = 250./224. * dim\n\t\txs = positions_xs * focal_length + dim/2.\n\t\tys = positions_ys * focal_length + dim/2.\n\n\t\tcur_xs = torch.clamp(xs , 0, dim - 1)\n\t\tcur_ys = torch.clamp(ys , 0, dim - 1)\n\n\n\t\t# img = np.zeros((dim,dim))\n\t\t# for x,y in zip (cur_xs, cur_ys): \n\t\t# \timg[x.int(), y.int()] = 255\n\t\t# from PIL import Image\n\t\t# Image.fromarray(img).show()\n\t\t\n\t\t\n\t\tx1s, y1s, x2s, y2s = torch.floor(cur_xs), torch.floor(cur_ys), torch.ceil(cur_xs), torch.ceil(cur_ys)\n\n\t\tA = x2s - cur_xs\n\t\tB = cur_xs - x1s\n\t\tG = y2s - cur_ys\n\t\tH = cur_ys - y1s\n\n\t\ty1s = y1s + torch.arange(positions.shape[0]).float().to(positions.device)*dim \n\t\ty2s = y2s + torch.arange(positions.shape[0]).float().to(positions.device)*dim \n\n\t\tdata_meta[\'A\'].append(A.float().unsqueeze(0))\n\t\tdata_meta[\'B\'].append(B.float().unsqueeze(0))\n\t\tdata_meta[\'G\'].append(G.float().unsqueeze(0))\n\t\tdata_meta[\'H\'].append(H.float().unsqueeze(0))\n\t\tdata_meta[\'x1s\'].append(x1s.long().unsqueeze(0))\n\t\tdata_meta[\'x2s\'].append(x2s.long().unsqueeze(0))\n\t\tdata_meta[\'y1s\'].append(y1s.long().unsqueeze(0))\n\t\tdata_meta[\'y2s\'].append(y2s.long().unsqueeze(0))\n\n\tfor key in data_meta:\n\t\tdata_meta[key] = torch.cat(data_meta[key], dim=0)\n\treturn data_meta\n\n\n\n\n\ndef pooling(blocks, pooling_indices):\n\n\tfull_features = None \n\tfor i_block, block in enumerate(blocks):\n\t   \n\t\tA = pooling_indices[\'A\'][i_block]\n\t\tB = pooling_indices[\'B\'][i_block]\n\t\tG = pooling_indices[\'G\'][i_block]\n\t\tH = pooling_indices[\'H\'][i_block]\n\n\t\tx1s = pooling_indices[\'x1s\'][i_block]\n\t\tx2s = pooling_indices[\'x2s\'][i_block]\n\t\ty1s = pooling_indices[\'y1s\'][i_block]\n\t\ty2s = pooling_indices[\'y2s\'][i_block]\n\n\n\t\tC =torch.index_select(block, 1, x1s).view(block.shape[0], -1 )\n\t\tC = torch.index_select(C, 1, y1s)\n\t\tD =torch.index_select(block, 1, x1s).view(block.shape[0], -1 )\n\t\tD = torch.index_select(D, 1, y2s)\n\t\tE =torch.index_select(block, 1, x2s).view(block.shape[0], -1 )\n\t\tE = torch.index_select(E, 1, y1s)\n\t\tF =torch.index_select(block, 1, x2s).view(block.shape[0], -1 )\n\t\tF = torch.index_select(F, 1, y2s)\n\n\n\t\tfeatures = (A*C*G + H*D*A + G*E*B + B*F*H).permute(1,0)\n\n\t\tif full_features is None: full_features = features\n\t\telse: full_features = torch.cat((full_features, features), dim = 1)\n \n\treturn full_features\n\nnorm_distance = kal.metrics.point.SidedDistance()\n\ndef chamfer_normal(pred_mesh, gt_points,gt_norms): \n\n\t# find closest gt points\n\tgt_indices = norm_distance(pred_mesh.vertices.unsqueeze(0), gt_points.unsqueeze(0))[0]\n\t# select norms from closest points and exand to match edges lengths\n\n\tgt_norm_selections = gt_norms[gt_indices]\n\tnew_dimensions = (gt_norm_selections.shape[0],pred_mesh.ve.shape[1], 3 )\n\tvertex_norms = gt_norm_selections.view(-1,1,3).expand(new_dimensions)\n\n\n\t\n\t# get all nieghbor positions\n\tneighbor_indecies = pred_mesh.vv.clone()\n\tempty_indecies = (neighbor_indecies >=0)\n\tother_indecies = (neighbor_indecies <0)\n\tneighbor_indecies[other_indecies] = 0 \n\tempty_indecies = (empty_indecies).float().unsqueeze(-1)\n\tneighbor_indecies = neighbor_indecies.view(-1)\n\tvertex_neighbors  = pred_mesh.vertices[neighbor_indecies].view(new_dimensions)\n\n\t# mask both tensors\n\tvertex_norms = vertex_norms * empty_indecies\n\tvertex_norms = vertex_norms.contiguous().view(-1,3)\n\tvertex_neighbors = vertex_neighbors * empty_indecies \n\tvertex_neighbors = vertex_neighbors.contiguous().view(-1,3)\n\n\t# calculate normal loss, devide by number of unmasked elements to get mean \n\tnormal_loss = (torch.abs(torch.sum(vertex_norms * vertex_neighbors, dim = 1))) \n\tnormal_loss = normal_loss.sum() / float(empty_indecies.sum())\n\treturn normal_loss\n\n\n\n\ndef setup_meshes(filename=\'meshes/156.obj\', device=""cuda"" ): \n\t\n\n\tmesh_1 = kal.rep.TriangleMesh.from_obj(filename, enable_adjacency=True)\n\tif device == \'cuda\': \n\t\tmesh_1.cuda()\n\tadj_1 = mesh_1.compute_adjacency_matrix_full().clone()\n\tadj_1 = normalize_adj(adj_1)\n\tmesh_1_i = kal.rep.TriangleMesh.from_tensors(mesh_1.vertices.clone(), mesh_1.faces.clone())\n\n\tmesh_2, split_mx_1 = split_mesh(mesh_1)\n\tadj_2 = mesh_2.compute_adjacency_matrix_full().clone()\n\tadj_2 = normalize_adj(adj_2)\n\tmesh_2_i = kal.rep.TriangleMesh.from_tensors(mesh_2.vertices.clone(), mesh_2.faces.clone())\n\n\tmesh_3, split_mx_2 = split_mesh(mesh_2)\n\tadj_3 = mesh_3.compute_adjacency_matrix_full().clone()\n\tadj_3 = normalize_adj(adj_3)\n\tmesh_3_i = kal.rep.TriangleMesh.from_tensors(mesh_3.vertices.clone(), mesh_3.faces.clone())\n\n\tinitial_meshes = [mesh_1_i, mesh_2_i, mesh_3_i]\n\tupdated_meshes = [mesh_1, mesh_2, mesh_3]\n\n\tadjs = [adj_1, adj_2, adj_3]\n\tsplit_mxs = [split_mx_1, split_mx_2]\n\tmesh_info = {\'init\':initial_meshes, \'update\':updated_meshes , \'adjs\': adjs, \'split_mxs\': split_mxs}\n\n\treturn mesh_info\n\n\n\ndef normalize_adj(mx):\n\trowsum = mx.sum(dim =1).view(-1)\n\tr_inv = 1./rowsum\n\tr_inv[r_inv!= r_inv] = 0.\n\tr_mat_inv = torch.eye(r_inv.shape[0]).to(mx.device)*r_inv\n\tmx = torch.mm(r_mat_inv,mx)\n\treturn mx\n\t\n\n\ndef split(meshes, features, index):\n\tmeshes[\'init\'][index+1].vertices = split_features(meshes[\'split_mxs\'][index], meshes[\'update\'][index].vertices)\n\tnew_features = split_features(meshes[\'split_mxs\'][index],features)\n\treturn new_features\n\n\ndef split_mesh(mesh):\n\tfaces = mesh.faces.clone()\n\ttracker = dict()\n\tvertex_count = mesh.vertices.shape[0]\n\tconstant_vertex_count = vertex_count\n\tcolumns = np.zeros((vertex_count, 0))\n\tnew_faces = []\n\t\n\n\n\tfor face in faces: \n\t\tx,y,z = face.int()\n\t\tnew_verts = []\n\t\tedges = [[x,y], [y,z], [z, x]]\n\n\t\tfor a,b in edges:\n\n\t\t\tkey = [a,b] \n\t\t\tkey.sort()\n\t\t\tkey = str(key)\n\t\t\tif key in tracker: \n\t\t\t\tnew_verts.append(tracker[key])\n\t\t\telse: \n\t\t\t\tnew_verts.append(vertex_count) \n\t\t\t\tcolumn = np.zeros((constant_vertex_count, 1))\n\t\t\t\tcolumn[a] = .5\n\t\t\t\tcolumn[b] = .5\n\t\t\t\tcolumns = np.concatenate((columns, column), axis = 1)\n\t\t\t\ttracker[key] = vertex_count  \n\t\t\t\tvertex_count += 1 \n\n\t\tv1,v2,v3 = new_verts\n\t\tnew_faces.append([x,v1,v3])\n\t\tnew_faces.append([v1,y,v2])\n\t\tnew_faces.append([v2,z,v3])\n\t\tnew_faces.append([v1,v2,v3])\n\n\tsplit_mx = torch.FloatTensor(columns).to(face.device)\n\n\tnew_faces = torch.LongTensor(new_faces).to(face.device)\n\n\tnew_verts =  split_features(split_mx, mesh.vertices)\n\tupdated_mesh = TriangleMesh.from_tensors(new_verts, new_faces, enable_adjacency=True)\n\n\treturn updated_mesh, split_mx\n\ndef split_features(split_mx, features): \n\tfeatures = features.permute(1,0)\n\tnew_features = torch.mm(features, split_mx)\n\tfeatures = torch.cat((features, new_features), dim= 1 ).permute(1,0)\n\treturn features\n\ndef loss_surf(meshes, tgt_points):\t\n\tloss =  kal.metrics.point.chamfer_distance(meshes[\'update\'][0].vertices, tgt_points, w1=1., w2 =0.55) \n\tloss += kal.metrics.point.chamfer_distance(meshes[\'update\'][1].vertices, tgt_points, w1=1., w2 =0.55) \n\tloss += kal.metrics.point.chamfer_distance(meshes[\'update\'][2].vertices, tgt_points, w1=1., w2 =0.55) \n\treturn loss\n\ndef loss_edge(meshes):\t\n\tloss =  kal.metrics.mesh.edge_length(meshes[\'update\'][0])\n\tloss += kal.metrics.mesh.edge_length(meshes[\'update\'][1])\n\tloss += kal.metrics.mesh.edge_length(meshes[\'update\'][2])\n\treturn loss\n\ndef loss_lap(meshes): \n\tloss =  .1* kal.metrics.mesh.laplacian_loss(meshes[\'init\'][0],meshes[\'update\'][0])\n\tloss += kal.metrics.mesh.laplacian_loss(meshes[\'init\'][1],meshes[\'update\'][1])\n\tloss += kal.metrics.mesh.laplacian_loss(meshes[\'init\'][2],meshes[\'update\'][2])\n\n\tloss += torch.sum((meshes[\'init\'][0].vertices-meshes[\'update\'][0].vertices)**2, 1).mean() * .0666 * .1\n\tloss += torch.sum((meshes[\'init\'][1].vertices-meshes[\'update\'][1].vertices)**2, 1).mean() * .0666\n\tloss += torch.sum((meshes[\'init\'][2].vertices-meshes[\'update\'][2].vertices)**2, 1).mean() * .0666\n\t\n\treturn loss \n\ndef loss_norm(meshes, tgt_points, tgt_norms): \n\tloss =  chamfer_normal(meshes[\'update\'][0], tgt_points, tgt_norms)\n\tloss += chamfer_normal(meshes[\'update\'][1], tgt_points, tgt_norms)\n\tloss += chamfer_normal(meshes[\'update\'][2], tgt_points, tgt_norms)\n\treturn loss \n\n\n'"
examples/SuperResolution/ODM-ModelNet/architectures.py,3,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nNetwork architecture definitions\n""""""\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n\n\n\nclass upscale(nn.Module):\n\tdef __init__(self, high, low):\n\t\tsuper(upscale, self).__init__()\n\t\tself.ratio = high // low \n\t\tself.layer1 = nn.Sequential(\n\t\t\tnn.Conv2d(6, 128, kernel_size=3, padding=1),\n\t\t\tnn.BatchNorm2d(128))\n\n\t\tself.inner_convs_1 = nn.ModuleList([nn.Conv2d(128, 128, kernel_size=3, padding=1) for i in range(16)])\n\t\tself.inner_bns_1 = nn.ModuleList([nn.BatchNorm2d(128) for i in range(16)])\n\t\tself.inner_convs_2 = nn.ModuleList([nn.Conv2d(128, 128, kernel_size=3, padding=1) for i in range(16)])\n\t\tself.inner_bns_2 = nn.ModuleList([nn.BatchNorm2d(128) for i in range(16)])\n\t\t\n\t\tself.layer2 = nn.Sequential(\n\t\t\tnn.Conv2d(128, 128, kernel_size=3, padding=1),\n\t\t\tnn.BatchNorm2d(128),\n\t\t)\n\n\t\tsub_list = [nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.PixelShuffle(2)]\n\t\ti = 0 \n\t\tfor i in range(int(math.log(self.ratio,2))-1):\n\t\t\tsub_list.append(nn.Conv2d(32, 128,kernel_size=3, padding=1))\n\t\t\tsub_list.append(nn.PixelShuffle(2))\n\t\tself.sub_list = nn.ModuleList(sub_list)\n\n\t\tself.layer3 = nn.Sequential(\n\t\t\tnn.Conv2d( 32, 6, kernel_size=1, padding=0),\n\t\t\t\n\t\t)\n\t\t\n\n\tdef forward(self, x):\n\t\tx = self.layer1(x)\n\t\ttemp = x.clone()\n\t\tfor i in range(16): \n\t\t\trecall = self.inner_convs_1[i](x.clone())\n\t\t\trecall = self.inner_bns_1[i](recall)\n\t\t\trecall = F.relu(recall)\n\t\t\trecall = self.inner_convs_2[i](recall)\n\t\t\trecall = self.inner_bns_2[i](recall)\n\t\t\trecall = recall + temp \n\t\t\ttemp = recall.clone()\n\t\trecall = self.layer2(recall)\n\t\tx = x + recall \n\t\n\t\tfor i in range(int(math.log(self.ratio,2))):\n\t\t\tx = self.sub_list[2*i](x)\n\t\t\tx = self.sub_list[2*i + 1](x)\n\t\t\n\t\tx = self.layer3(x)\n\t\tx = torch.sigmoid(x)\n\t\treturn x\n'"
examples/SuperResolution/ODM-ModelNet/dataloaders.py,3,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nDataset classes\n""""""\n\nimport numpy as np\nimport os\nimport torch\nfrom tqdm import tqdm \n\nimport torch.utils.data as data\nimport scipy.io as sio\n\nimport kaolin as kal\n\n\n\nclass ModelNet_ODMS(object):\n\tr""""""\n\tDataloader for downloading and reading from ModelNet \n\n\tNote: \n\t\tMade to be passed to the torch dataloader \n\n\tArgs: \n\t\troot (str): location the dataset should be downloaded to /loaded from \n\t\ttrain (bool): if True loads training set, else loads test \n\t\tdownload (bool): downloads the dataset if not found in root \n\t\tobject_class (str): object class to be loaded, if \'all\' then all are loaded \n\t\tsingle_view (bool): if true only on roation is used, if not all 12 views are loaded \n\n\tAttributes: \n\t\tlist: list of all voxel locations to be loaded from \n\n\tExamples:\n\t\t>>> data_set = ModelNet(root =\'./datasets/\')\n\t\t>>> train_loader = DataLoader(data_set, batch_size=10, n=True, num_workers=8) \n\n\t""""""\n\n\tdef __init__(self, root=\'../datasets/\', train=True, download=True, compute=True, categories=[\'chair\'], single_view=True, voxels = True):\n\t\tvoxel_set = kal.dataloader.ModelNet( root, train=train, download=download, categories=categories, single_view=single_view)\n\t\todm_location = root + \'/ModelNet/ODMs/\'\n\t\tif voxels: \n\t\t\tself.load_voxels = True \n\t\t\tself.voxel_names = []\n\n\t\tself.names = []\n\t\tif not os.path.exists(odm_location) and compute:\n\t\t\t\tprint (\'ModelNet ODMS were not found at {0}, and compute is set to False\'.format(odm_location))\n\t\t\t\n\t\tfor n in tqdm(voxel_set.names): \n\t\t\n\t\t\texample_location = odm_location + n.split(\'volumetric_data\')[-1]\n\t\t\texample_length = len(example_location.split(\'/\')[-1])\n\t\t\texample_folder = example_location[:-example_length]\n\t\t\tif not os.path.exists(example_folder):\n\t\t\t\tif compute:  \n\t\t\t\t\tos.makedirs(example_folder)\n\t\t\t\telse: \n\t\t\t\t\tprint (\'ModelNet ODMS were not found at {0}, and compute is set to False\'.format(example_location))\n\t\t\tif not os.path.exists(example_location):\n\t\t\t\tvoxel = sio.loadmat(n)[\'instance\']\n\t\t\t\todms = kal.rep.voxel.extract_odms(voxel)\n\t\t\t\todms = np.array(odms)\n\t\t\t\tsio.savemat(example_location, {\'odm\': odms})\n\n\t\t\t\t\t\t\t\n\t\t\tself.names.append(example_location)\n\t\t\tif self.load_voxels: \n\t\t\t\tself.voxel_names.append(n)\n\n\n\t\t\t\t\t\t\n\tdef __len__(self):\n\t\t"""""" \n\t\tReturns:\n\t\t\tnumber of odms lists in active dataloader\n\n\t\t""""""\n\t\treturn len(self.names)\n\n\tdef __getitem__(self, item):\n\t\t""""""Gets a single example of a ModelNet voxel model \n\t\tArgs: \n\t\t\titem (int): index of required model \n\n\t\treturn: \n\t\t\tdictionary which contains a odm data\n\n\t\t""""""\n\t\tdata = {}\n\t\todm_path = self.names[item]\n\t\todms = sio.loadmat(odm_path)[\'odm\']\n\t\tdata[\'odms\'] = torch.FloatTensor(odms.astype(float))\n\t\tif self.load_voxels: \n\t\t\tvoxels = sio.loadmat(self.voxel_names[item])[\'instance\']\n\t\t\tdata[\'voxels\'] = torch.FloatTensor(voxels.astype(float))\n\t\treturn data\n\n'"
examples/SuperResolution/ODM-ModelNet/eval_Direct.py,5,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport os\nimport torch\nimport sys\nfrom tqdm import tqdm\n\nfrom torch.utils.data import DataLoader\n\nfrom architectures import upscale\nfrom utils import down_sample, up_sample\nfrom dataloaders import ModelNet_ODMS\nimport kaolin as kal \n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-expid\', type=str, default=\'Direct\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'-device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'-vis\', action=\'store_true\', help=\'Visualize each model while evaluating\')\nparser.add_argument(\'-categories\', type=str,nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'-batchsize\', type=int, default=16, help=\'Batch size.\')\nargs = parser.parse_args()\n\n\n\n# Data\nvalid_set = ModelNet_ODMS(root =\'../../datasets/\',categories = args.categories, \\\n\tdownload = True, train = False)\ndataloader_val = DataLoader(valid_set, batch_size=args.batchsize, shuffle=False, \\\n\tnum_workers=8)\n\n# Model\nmodel = upscale(30,15)\nmodel = model.to(args.device)\n# Load saved weights\nmodel.load_state_dict(torch.load(\'log/{0}/best.pth\'.format(args.expid)))\n\n\niou_epoch = 0.\niou_NN_epoch = 0.\nnum_batches = 0\n\nmodel.eval()\nwith torch.no_grad():\n\tfor data in tqdm(dataloader_val): \n\t\t\n\t\ttgt_odms = data[\'odms\'].to(args.device)\n\t\ttgt_voxels = data[\'voxels\'].to(args.device)\n\t\tinp_voxels = down_sample(tgt_voxels)\n\t\tinp_odms = []\n\t\tfor voxel in inp_voxels: \n\t\t\tinp_odms.append(kal.rep.voxel.extract_odms(voxel).unsqueeze(0)) \n\t\tinp_odms = torch.cat(inp_odms)\n\n\t\tpred_odms = model(inp_odms)*30\n\n\t\tNN_pred = up_sample(inp_voxels)\n\t\tiou_NN = kal.metrics.voxel.iou(NN_pred.contiguous(), tgt_voxels)\n\t\tiou_NN_epoch += iou_NN\n\n\t\tpred_odms = pred_odms.int()\n\t\tpred_voxels = []\n\t\tfor odms, NN_voxel in zip(pred_odms, NN_pred): \n\t\t\tpred_voxels.append(kal.rep.voxel.project_odms(odms, voxel= NN_voxel, votes = 2).unsqueeze(0))\n\t\tpred_voxels = torch.cat(pred_voxels)\n\t\tiou = kal.metrics.voxel.iou(pred_voxels.contiguous(), tgt_voxels)\n\t\tiou_epoch += iou\n\n\t\t\n\n\t\tnum_batches += 1\n\t\tif args.vis: \n\t\t\tfor i in range(inp_voxels.shape[0]):\t\n\t\t\t\tprint (\'Rendering low resolution input\')\n\t\t\t\tkal.visualize.show_voxel(inp_voxels[i], mode = \'exact\', thresh = .5)\n\t\t\t\tprint (\'Rendering high resolution target\')\n\t\t\t\tkal.visualize.show_voxel(tgt_voxels[i], mode = \'exact\', thresh = .5)\n\t\t\t\tprint (\'Rendering high resolution prediction\')\n\t\t\t\tkal.visualize.show_voxel(pred_voxels[i], mode = \'exact\', thresh = .5)\n\t\t\t\tprint(\'----------------------\')\niou_NN_epoch = iou_NN_epoch.item() / float(num_batches)\nprint (\'IoU for Nearest Neighbor baseline over validation set is {0}\'.format(iou_NN_epoch))\nout_iou = iou_epoch.item() / float(num_batches)\nprint (\'IoU over validation set is {0}\'.format(out_iou))'"
examples/SuperResolution/ODM-ModelNet/eval_MVD.py,6,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport os\nimport torch\nimport sys\nfrom tqdm import tqdm \n\nfrom torch.utils.data import DataLoader\n\nfrom architectures import upscale\nfrom utils import down_sample\nfrom dataloaders import ModelNet_ODMS\nfrom utils import down_sample, up_sample, upsample_omd, to_occpumancy_map\nimport kaolin as kal \n\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-expid\', type=str, default=\'MVD\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'-device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'-vis\', action=\'store_true\', help=\'Visualize each model while evaluating\')\nparser.add_argument(\'-categories\', type=str,nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'-batchsize\', type=int, default=16, help=\'Batch size.\')\nargs = parser.parse_args()\n\n\n\n\n# Data\nvalid_set = ModelNet_ODMS(root =\'../../datasets/\',categories = [\'chair\'], \\\n\tdownload = True, train = False)\ndataloader_val = DataLoader(valid_set, batch_size=args.batchsize, shuffle=False, \\\n\tnum_workers=8)\n\n# Model\nmodel_res = upscale(30,15)\nmodel_res = model_res.to(args.device)\nmodel_occ = upscale(30,15)\nmodel_occ = model_occ.to(args.device)\n# Load saved weights\nmodel_res.load_state_dict(torch.load(\'log/{0}/resbest.pth\'.format(args.expid)))\nmodel_occ.load_state_dict(torch.load(\'log/{0}/occbest.pth\'.format(args.expid)))\n\n\niou_epoch = 0.\niou_NN_epoch = 0.\nnum_batches = 0\n\n\nmodel_res.eval()\nmodel_occ.eval()\nwith torch.no_grad():\n\tfor data in tqdm(dataloader_val): \n\t\t\n\t\ttgt_odms = data[\'odms\'].to(args.device)\n\t\ttgt_voxels = data[\'voxels\'].to(args.device)\n\t\tinp_voxels = down_sample(tgt_voxels)\n\t\tinp_odms = []\n\t\tfor voxel in inp_voxels: \n\t\t\tinp_odms.append(kal.rep.voxel.extract_odms(voxel).unsqueeze(0)) \n\t\tinp_odms = torch.cat(inp_odms)\n\t\t\n\t\t# inference res\n\t\tinitial_odms = upsample_omd(inp_odms)*2\n\t\tdistance = 30 - initial_odms\n\t\tpred_odms_update = model_res(inp_odms)\n\t\tpred_odms_update = pred_odms_update * distance\n\t\tpred_odms_res = initial_odms + pred_odms_update\n\n\t\t# inference occ\n\t\tpred_odms_occ = model_occ(inp_odms)\n\n\t\t# combine\n\t\tpred_odms_res = pred_odms_res.int()\n\t\tones = pred_odms_occ > .5\n\t\tzeros = pred_odms_occ <= .5\n\t\tpred_odms_occ[ones] =  pred_odms_occ.shape[-1]\n\t\tpred_odms_occ[zeros] = 0  \n\n\t\tNN_pred = up_sample(inp_voxels)\n\t\tiou_NN = kal.metrics.voxel.iou(NN_pred.contiguous(), tgt_voxels)\n\t\tiou_NN_epoch += iou_NN\n\t\t\n\t\tpred_voxels = []\n\t\tfor i in range(inp_voxels.shape[0]):\t\n\t\t\tvoxel = NN_pred[i]\t\t\t\n\t\t\tvoxel = kal.rep.voxel.project_odms(pred_odms_res[i], voxel = voxel, votes = 2)\n\t\t\tvoxel = kal.rep.voxel.project_odms(pred_odms_occ[i], voxel = voxel, votes = 2)\n\t\t\tvoxel = voxel.unsqueeze(0)\n\t\t\tpred_voxels.append(voxel)\n\t\tpred_voxels = torch.cat(pred_voxels)\n\t\tiou = kal.metrics.voxel.iou(pred_voxels.contiguous(), tgt_voxels)\n\t\tiou_epoch += iou\n\n\t\t\n\t\tif args.vis: \n\t\t\tfor i in range(inp_voxels.shape[0]):\t\n\t\t\t\n\t\t\t\tprint (\'Rendering low resolution input\')\n\t\t\t\tkal.visualize.show_voxel(inp_voxels[i], mode = \'exact\', thresh = .5)\n\t\t\t\tprint (\'Rendering high resolution target\')\n\t\t\t\tkal.visualize.show_voxel(tgt_voxels[i], mode = \'exact\', thresh = .5)\n\t\t\t\tprint (\'Rendering high resolution prediction\')\n\t\t\t\tkal.visualize.show_voxel(pred_voxels[i], mode = \'exact\', thresh = .5)\n\t\t\t\tprint(\'----------------------\')\n\t\tnum_batches += 1  \niou_NN_epoch = iou_NN_epoch.item() / float(num_batches)\nprint (\'IoU for Nearest Neighbor baseline over validation set is {0}\'.format(iou_NN_epoch))\nout_iou = iou_epoch.item() / float(num_batches)\nprint (\'IoU over validation set is {0}\'.format(out_iou))'"
examples/SuperResolution/ODM-ModelNet/train_Direct.py,13,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport json\nimport numpy as np\nimport os\nimport sys\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\n\nfrom architectures import upscale\nfrom dataloaders import ModelNet_ODMS\nfrom utils import down_sample, up_sample\nimport kaolin as kal \n""""""\nCommandline arguments\n""""""\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-expid\', type=str, default=\'Direct\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'-device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'-categories\', type=str,nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'-epochs\', type=int, default=30, help=\'Number of train epochs.\')\nparser.add_argument(\'-batchsize\', type=int, default=16, help=\'Batch size.\')\nparser.add_argument(\'-lr\', type=float, default=1e-3, help=\'Learning rate.\')\nparser.add_argument(\'-val-every\', type=int, default=5, help=\'Validation frequency (epochs).\')\nparser.add_argument(\'-print-every\', type=int, default=20, help=\'Print frequency (batches).\')\nparser.add_argument(\'-logdir\', type=str, default=\'log\', help=\'Directory to log data to.\')\nparser.add_argument(\'-save-model\', action=\'store_true\', help=\'Saves the model and a snapshot \\\n\tof the optimizer state.\')\nargs = parser.parse_args()\n\n\n\n\n""""""\nDataset\n""""""\ntrain_set = ModelNet_ODMS(root =\'../../datasets/\',categories = args.categories,  \\\n\tdownload = True)\ndataloader_train = DataLoader(train_set, batch_size=args.batchsize, shuffle=True, \n\tnum_workers=8)\n\nvalid_set = ModelNet_ODMS(root =\'../../datasets/\',categories = args.categories, \\\n\tdownload = True, train = False)\ndataloader_val = DataLoader(valid_set, batch_size=args.batchsize, shuffle=False, \\\n\tnum_workers=8)\n\n\n""""""\nModel settings \n""""""\nmodel = upscale(30, 15 ).to(args.device)\n\nloss_fn = torch.nn.MSELoss()\n\noptimizer = optim.Adam(model.parameters(), lr=args.lr)\n\n\n# Create log directory, if it doesn\'t already exist\nargs.logdir = os.path.join(args.logdir, args.expid)\nif not os.path.isdir(args.logdir):\n\tos.makedirs(args.logdir)\n\tprint(\'Created dir:\', args.logdir)\n\n# Log all commandline args\nwith open(os.path.join(args.logdir, \'args.txt\'), \'w\') as f:\n\tjson.dump(args.__dict__, f, indent=2)\n \n\n\n\n\n\nclass Engine(object):\n\t""""""Engine that runs training and inference.\n\tArgs\n\t\t- cur_epoch (int): Current epoch.\n\t\t- print_every (int): How frequently (# batches) to print loss.\n\t\t- validate_every (int): How frequently (# epochs) to run validation.\n\t\t\n\t""""""\n\n\tdef __init__(self,  cur_epoch=0, print_every=1, validate_every=1):\n\t\tself.cur_epoch = cur_epoch\n\t\tself.train_loss = []\n\t\tself.val_loss = []\n\t\tself.bestval = 0\n\n\tdef train(self):\n\t\tloss_epoch = 0.\n\t\tnum_batches = 0\n\t\tdiff = 0 \n\t\tmodel.train()\n\t\t# Train loop\n\t\tfor i, data in enumerate(tqdm(dataloader_train), 0):\n\t\t\toptimizer.zero_grad()\n\t\t\t\n\t\t\t# data creation\n\t\t\ttgt_odms = data[\'odms\'].to(args.device)\n\t\t\ttgt_voxels = data[\'voxels\'].to(args.device)\n\t\t\tinp_voxels = down_sample(tgt_voxels)\n\t\t\tinp_odms = []\n\t\t\tfor voxel in inp_voxels: \n\t\t\t\tinp_odms.append(kal.rep.voxel.extract_odms(voxel).unsqueeze(0)) \n\t\t\tinp_odms = torch.cat(inp_odms)\n\t\t\t\n\t\t\t# inference \n\t\t\tpred_odms = model(inp_odms)*30\n\n\t\t\t# losses \n\t\t\tloss = loss_fn(pred_odms, tgt_odms)\n\t\t\tloss.backward()\n\t\t\tloss_epoch += float(loss.item())\n\n\t\t\t# logging\n\t\t\tnum_batches += 1\n\t\t\tif i % args.print_every == 0:\n\t\t\t\ttqdm.write(f\'[TRAIN] Epoch {self.cur_epoch:03d}, Batch {i:03d}: Loss: {float(loss.item())}\')\n\t\t\t\t\n\t\t\toptimizer.step()\n\t\t\n\t\t\n\t\tloss_epoch = loss_epoch / num_batches\n\t\tself.train_loss.append(loss_epoch)\n\t\tself.cur_epoch += 1\n\n\t\t\n\t\t\n\tdef validate(self):\n\t\tmodel.eval()\n\t\twith torch.no_grad():\t\n\t\t\tiou_epoch = 0.\n\t\t\tiou_NN_epoch = 0.\n\t\t\tnum_batches = 0\n\t\t\tloss_epoch = 0.\n\n\t\t\t# Validation loop\n\t\t\tfor i, data in enumerate(tqdm(dataloader_val), 0):\n\n\t\t\t\t# data creation\n\t\t\t\ttgt_odms = data[\'odms\'].to(args.device)\n\t\t\t\ttgt_voxels = data[\'voxels\'].to(args.device)\n\t\t\t\tinp_voxels = down_sample(tgt_voxels)\n\t\t\t\tinp_odms = []\n\t\t\t\tfor voxel in inp_voxels: \n\t\t\t\t\tinp_odms.append(kal.rep.voxel.extract_odms(voxel).unsqueeze(0)) \n\t\t\t\tinp_odms = torch.cat(inp_odms)\n\t\t\t\t\n\t\t\t\t# inference \n\t\t\t\tpred_odms = model(inp_odms)*30\n\n\t\t\t\t# losses \n\t\t\t\tloss = loss_fn(pred_odms, tgt_odms)\n\t\t\t\tloss_epoch += float(loss.item())\n\n\t\t\t\tNN_pred = up_sample(inp_voxels)\n\t\t\t\tiou_NN = kal.metrics.voxel.iou(NN_pred.contiguous(), tgt_voxels)\n\t\t\t\tiou_NN_epoch += iou_NN\n\n\t\t\t\tpred_odms = pred_odms.int()\n\t\t\t\tpred_voxels = []\n\t\t\t\tfor odms, voxel_NN in zip(pred_odms,NN_pred): \n\t\t\t\t\tpred_voxels.append(kal.rep.voxel.project_odms(odms, voxel = voxel_NN, votes = 2).unsqueeze(0))\n\t\t\t\tpred_voxels = torch.cat(pred_voxels)\n\t\t\t\tiou = kal.metrics.voxel.iou(pred_voxels.contiguous(), tgt_voxels)\n\t\t\t\tiou_epoch += iou\n\t\t\t\t\n\n\t\t\t\t\n\n\t\t\t\t# logging\n\t\t\t\tnum_batches += 1\n\t\t\t\tif i % args.print_every == 0:\n\t\t\t\t\t\tout_iou = iou_epoch.item() / float(num_batches)\n\t\t\t\t\t\tout_iou_NN = iou_NN_epoch.item() / float(num_batches)\n\t\t\t\t\t\ttqdm.write(f\'[VAL] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou}, Iou Base: {out_iou_NN}\')\n\t\t\t\t\t\t\n\t\t\tout_iou = iou_epoch.item() / float(num_batches)\n\t\t\tout_iou_NN = iou_NN_epoch.item() / float(num_batches)\n\t\t\ttqdm.write(f\'[VAL Total] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou}, Iou Base: {out_iou_NN}\')\n\n\t\t\tloss_epoch = loss_epoch / num_batches\n\t\t\tself.val_loss.append(out_iou)\n\n\tdef save(self):\n\n\t\tsave_best = False\n\t\tif self.val_loss[-1] >= self.bestval:\n\t\t\tself.bestval = self.val_loss[-1]\n\t\t\tsave_best = True\n\t\t\n\t\t# Create a dictionary of all data to save\n\t\tlog_table = {\n\t\t\t\'epoch\': self.cur_epoch,\n\t\t\t\'bestval\': np.min(np.asarray(self.val_loss)),\n\t\t\t\'train_loss\': self.train_loss,\n\t\t\t\'val_loss\': self.val_loss,\n\t\t\t\'train_metrics\': [\'NLLLoss\', \'iou\'],\n\t\t\t\'val_metrics\': [\'NLLLoss\', \'iou\', \'iou_NN\'],\n\t\t}\n\n\t\t# Save the recent model/optimizer states\n\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, \'recent.pth\'))\n\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, \'recent_optim.pth\'))\n\t\t# Log other data corresponding to the recent model\n\t\twith open(os.path.join(args.logdir, \'recent.log\'), \'w\') as f:\n\t\t\tf.write(json.dumps(log_table))\n\n\t\ttqdm.write(\'====== Saved recent model ======>\')\n\t\t\n\t\tif save_best:\n\t\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, \'best.pth\'))\n\t\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, \'best_optim.pth\'))\n\t\t\t# Log other data corresponding to the recent model\n\t\t\twith open(os.path.join(args.logdir, \'best.log\'), \'w\') as f:\n\t\t\t\tf.write(json.dumps(log_table))\n\t\t\ttqdm.write(\'====== Overwrote best model ======>\')\n\t\t\t\n\t\ntrainer = Engine()\n\nfor epoch in range(args.epochs): \n\ttrainer.train()\n\ttrainer.validate()\n\ttrainer.save()'"
examples/SuperResolution/ODM-ModelNet/train_MVD.py,22,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport json\nimport numpy as np\nimport os\nimport sys\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\n\nfrom architectures import upscale\nfrom dataloaders import ModelNet_ODMS\nfrom utils import down_sample, up_sample, upsample_omd, to_occpumancy_map\nimport kaolin as kal \n""""""\nCommandline arguments\n""""""\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-expid\', type=str, default=\'MVD\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'-device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'-categories\', type=str,nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'-epochs\', type=int, default=30, help=\'Number of train epochs.\')\nparser.add_argument(\'-batchsize\', type=int, default=16, help=\'Batch size.\')\nparser.add_argument(\'-lr\', type=float, default=1e-3, help=\'Learning rate.\')\nparser.add_argument(\'-val-every\', type=int, default=5, help=\'Validation frequency (epochs).\')\nparser.add_argument(\'-print-every\', type=int, default=20, help=\'Print frequency (batches).\')\nparser.add_argument(\'-logdir\', type=str, default=\'log\', help=\'Directory to log data to.\')\nparser.add_argument(\'-save-model\', action=\'store_true\', help=\'Saves the model and a snapshot \\\n\tof the optimizer state.\')\nargs = parser.parse_args()\n\n\n\n""""""\nDataset\n""""""\ntrain_set = ModelNet_ODMS(root =\'../../datasets/\',categories = args.categories, \\\n\tdownload = True)\ndataloader_train = DataLoader(train_set, batch_size=args.batchsize, shuffle=True, \n\tnum_workers=8)\n\nvalid_set = ModelNet_ODMS(root =\'../../datasets/\',categories = args.categories, \\\n\tdownload = True, train = False)\ndataloader_val = DataLoader(valid_set, batch_size=args.batchsize, shuffle=False, \\\n\tnum_workers=8)\n\n\n""""""\nModel settings \n""""""\nmodel = upscale(30, 15 ).to(args.device)\n\nloss_fn = torch.nn.MSELoss()\n\noptimizer = optim.Adam(model.parameters(), lr=args.lr)\n\n\n# Create log directory, if it doesn\'t already exist\nargs.logdir = os.path.join(args.logdir, args.expid)\nif not os.path.isdir(args.logdir):\n\tos.makedirs(args.logdir)\n\tprint(\'Created dir:\', args.logdir)\n\n# Log all commandline args\nwith open(os.path.join(args.logdir, \'args.txt\'), \'w\') as f:\n\tjson.dump(args.__dict__, f, indent=2)\n \n\n\n\n\n\nclass Engine_Residual(object):\n\t""""""Engine that runs training and inference.\n\tArgs\n\t\t- cur_epoch (int): Current epoch.\n\t\t- print_every (int): How frequently (# batches) to print loss.\n\t\t- validate_every (int): How frequently (# epochs) to run validation.\n\t\t\n\t""""""\n\n\tdef __init__(self,  cur_epoch=0, print_every=1, validate_every=1):\n\t\tself.cur_epoch = cur_epoch\n\t\tself.train_loss = []\n\t\tself.val_loss = []\n\t\tself.bestval = 0\n\n\tdef train(self):\n\t\tloss_epoch = 0.\n\t\tnum_batches = 0\n\t\tdiff = 0 \n\t\tmodel.train()\n\t\t# Train loop\n\t\tfor i, data in enumerate(tqdm(dataloader_train), 0):\n\t\t\toptimizer.zero_grad()\n\t\t\t\n\t\t\t# data creation\n\t\t\ttgt_odms = data[\'odms\'].to(args.device)\n\t\t\ttgt_voxels = data[\'voxels\'].to(args.device)\n\t\t\tinp_voxels = down_sample(tgt_voxels)\n\t\t\tinp_odms = []\n\t\t\tfor voxel in inp_voxels: \n\t\t\t\tinp_odms.append(kal.rep.voxel.extract_odms(voxel).unsqueeze(0)) \n\t\t\tinp_odms = torch.cat(inp_odms)\n\t\t\t\n\t\t\t# inference \n\t\t\tinitial_odms = upsample_omd(inp_odms)*2\n\t\t\tdistance = 30 - initial_odms\n\t\t\tpred_odms_update = model(inp_odms)\n\t\t\tpred_odms_update = pred_odms_update * distance\n\t\t\tpred_odms = initial_odms + pred_odms_update\n\t\t\t\n\t\t\tpred_odms = initial_odms + pred_odms_update\n\n\t\t\t# losses \n\t\t\tloss = loss_fn(pred_odms, tgt_odms)\n\t\t\tloss.backward()\n\t\t\tloss_epoch += float(loss.item())\n\n\t\t\t# logging\n\t\t\tnum_batches += 1\n\t\t\tif i % args.print_every == 0:\n\t\t\t\ttqdm.write(f\'[TRAIN] Epoch {self.cur_epoch:03d}, Batch {i:03d}: Loss: {float(loss.item())}\')\n\t\t\t\t\n\t\t\toptimizer.step()\n\t\t\n\t\t\n\t\tloss_epoch = loss_epoch / num_batches\n\t\tself.train_loss.append(loss_epoch)\n\t\tself.cur_epoch += 1\n\n\t\t\n\t\t\n\tdef validate(self):\n\t\tmodel.eval()\n\t\twith torch.no_grad():\t\n\t\t\tiou_epoch = 0.\n\t\t\tiou_NN_epoch = 0.\n\t\t\tnum_batches = 0\n\t\t\tloss_epoch = 0.\n\n\t\t\t# Validation loop\n\t\t\tfor i, data in enumerate(tqdm(dataloader_val), 0):\n\n\t\t\t\t# data creation\n\t\t\t\ttgt_odms = data[\'odms\'].to(args.device)\n\t\t\t\ttgt_voxels = data[\'voxels\'].to(args.device)\n\t\t\t\tinp_voxels = down_sample(tgt_voxels)\n\t\t\t\tinp_odms = []\n\t\t\t\tfor voxel in inp_voxels: \n\t\t\t\t\tinp_odms.append(kal.rep.voxel.extract_odms(voxel).unsqueeze(0)) \n\t\t\t\tinp_odms = torch.cat(inp_odms)\n\t\t\t\t\n\t\t\t\t# inference \n\t\t\t\tinitial_odms = upsample_omd(inp_odms)*2\n\t\t\t\tdistance = 30 - initial_odms\n\t\t\t\tpred_odms_update = model(inp_odms)\n\t\t\t\tpred_odms_update = pred_odms_update * distance\n\t\t\t\tpred_odms = initial_odms + pred_odms_update\n\n\t\t\t\t# losses \n\t\t\t\tloss = loss_fn(pred_odms, tgt_odms)\n\t\t\t\tloss_epoch += float(loss.item())\n\n\t\t\t\tNN_pred = up_sample(inp_voxels)\n\t\t\t\tiou_NN = kal.metrics.voxel.iou(NN_pred.contiguous(), tgt_voxels)\n\t\t\t\tiou_NN_epoch += iou_NN\n\n\t\t\t\tpred_odms = pred_odms.int()\n\t\t\t\tpred_voxels = []\n\t\t\t\tfor odms, voxel_NN in zip(pred_odms, NN_pred): \n\t\t\t\t\tpred_voxels.append(kal.rep.voxel.project_odms(odms,voxel =voxel_NN, votes = 2).unsqueeze(0))\n\t\t\t\tpred_voxels = torch.cat(pred_voxels)\n\t\t\t\tiou = kal.metrics.voxel.iou(pred_voxels.contiguous(), tgt_voxels)\n\t\t\t\tiou_epoch += iou\n\t\t\t\t\n\n\t\t\t\t# logging\n\t\t\t\tnum_batches += 1\n\t\t\t\tif i % args.print_every == 0:\n\t\t\t\t\t\tout_iou = iou_epoch.item() / float(num_batches)\n\t\t\t\t\t\tout_iou_NN = iou_NN_epoch.item() / float(num_batches)\n\t\t\t\t\t\ttqdm.write(f\'[VAL] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou}, Iou Base: {out_iou_NN}\')\n\t\t\t\t\t\t\n\t\t\tout_iou = iou_epoch.item() / float(num_batches)\n\t\t\tout_iou_NN = iou_NN_epoch.item() / float(num_batches)\n\t\t\ttqdm.write(f\'[VAL Total] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou}, Iou Base: {out_iou_NN}\')\n\n\t\t\tloss_epoch = loss_epoch / num_batches\n\t\t\tself.val_loss.append(out_iou)\n\n\tdef save(self):\n\n\t\tsave_best = False\n\t\tif self.val_loss[-1] >= self.bestval:\n\t\t\tself.bestval = self.val_loss[-1]\n\t\t\tsave_best = True\n\t\t\n\t\t\n\t\t# Create a dictionary of all data to save\n\t\tlog_table = {\n\t\t\t\'epoch\': self.cur_epoch,\n\t\t\t\'bestval\': np.min(np.asarray(self.val_loss)),\n\t\t\t\'train_loss\': self.train_loss,\n\t\t\t\'val_loss\': self.val_loss,\n\t\t\t\'train_metrics\': [\'NLLLoss\', \'iou\'],\n\t\t\t\'val_metrics\': [\'NLLLoss\', \'iou\', \'iou_NN\'],\n\t\t}\n\n\t\t# Save the recent model/optimizer states\n\t\todm_type = \'res\'\n\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, odm_type + \'recent.pth\'))\n\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, odm_type + \'recent_optim.pth\'))\n\t\t# Log other data corresponding to the recent model\n\t\twith open(os.path.join(args.logdir, odm_type + \'recent.log\'), \'w\') as f:\n\t\t\tf.write(json.dumps(log_table))\n\n\t\ttqdm.write(\'====== Saved recent model ======>\')\n\t\t\n\t\tif save_best:\n\t\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, odm_type + \'best.pth\'))\n\t\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, odm_type + \'best_optim.pth\'))\n\t\t\t# Log other data corresponding to the recent model\n\t\t\twith open(os.path.join(args.logdir, odm_type + \'best.log\'), \'w\') as f:\n\t\t\t\tf.write(json.dumps(log_table))\n\t\t\ttqdm.write(\'====== Overwrote best model ======>\')\n\n\ntrainer = Engine_Residual()\nfor epoch in range(args.epochs): \n\ttrainer.train()\n\ttrainer.validate()\n\ttrainer.save()\n\n\n\nmodel = upscale(30, 15 ).to(args.device)\nloss_fn = torch.nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=args.lr)\n\nclass Engine_Occ(object):\n\t""""""Engine that runs training and inference.\n\tArgs\n\t\t- cur_epoch (int): Current epoch.\n\t\t- print_every (int): How frequently (# batches) to print loss.\n\t\t- validate_every (int): How frequently (# epochs) to run validation.\n\t\t\n\t""""""\n\n\tdef __init__(self,  cur_epoch=0, print_every=1, validate_every=1):\n\t\tself.cur_epoch = cur_epoch\n\t\tself.train_loss = []\n\t\tself.val_loss = []\n\t\tself.bestval = 0\n\n\tdef train(self):\n\t\tloss_epoch = 0.\n\t\tnum_batches = 0\n\t\tdiff = 0 \n\n\t\t# Train loop\n\t\tfor i, data in enumerate(tqdm(dataloader_train), 0):\n\t\t\toptimizer.zero_grad()\n\t\t\t\n\t\t\t# data creation\n\t\t\ttgt_odms = data[\'odms\'].to(args.device)\n\t\t\ttgt_voxels = data[\'voxels\'].to(args.device)\n\t\t\tinp_voxels = down_sample(tgt_voxels)\n\n\t\t\tinp_odms = []\n\t\t\tfor voxel in inp_voxels: \n\t\t\t\tinp_odms.append(kal.rep.voxel.extract_odms(voxel).unsqueeze(0)) \n\t\t\tinp_odms = torch.cat(inp_odms)\n\n\t\t\ttgt_odms_occ = to_occpumancy_map(tgt_odms)\n\t\t\tdiff += tgt_odms_occ.sum()\n\t\t\t\n\t\t\t# inference \n\t\t\tpred_odms = model(inp_odms)\n\n\t\t\t# losses \n\t\t\tloss = loss_fn(pred_odms, tgt_odms_occ)\n\t\t\t\n\t\t\tloss.backward()\n\t\t\tloss_epoch += float(loss.item())\n\n\t\t\t# logging\n\t\t\tnum_batches += 1\n\t\t\tif i % args.print_every == 0:\n\t\t\t\ttqdm.write(f\'[TRAIN] Epoch {self.cur_epoch:03d}, Batch {i:03d}: Loss: {float(loss.item())}\')\n\t\t\t\t\n\t\t\toptimizer.step()\n\t\t\n\t\t\n\t\tloss_epoch = loss_epoch / num_batches\n\t\tself.train_loss.append(loss_epoch)\n\t\tself.cur_epoch += 1\n\n\t\t\n\t\t\n\tdef validate(self):\n\t\tmodel.eval()\n\t\twith torch.no_grad():\t\n\t\t\tiou_epoch = 0.\n\t\t\tiou_NN_epoch = 0.\n\t\t\tnum_batches = 0\n\t\t\tloss_epoch = 0.\n\n\t\t\t# Validation loop\n\t\t\tfor i, data in enumerate(tqdm(dataloader_val), 0):\n\n\t\t\t\t# data creation\n\t\t\t\ttgt_odms = data[\'odms\'].to(args.device)\n\t\t\t\ttgt_voxels = data[\'voxels\'].to(args.device)\n\t\t\t\tinp_voxels = down_sample(tgt_voxels)\n\n\t\t\t\tinp_odms = []\n\t\t\t\tfor voxel in inp_voxels: \n\t\t\t\t\tinp_odms.append(kal.rep.voxel.extract_odms(voxel).unsqueeze(0)) \n\t\t\t\tinp_odms = torch.cat(inp_odms)\n\n\t\t\t\ttgt_odms_occ = to_occpumancy_map(tgt_odms)\n\t\t\t\t\n\t\t\t\t# inference \n\t\t\t\tpred_odms = model(inp_odms)\n\n\t\t\t\t# losses \n\t\t\t\tloss = loss_fn(pred_odms, tgt_odms_occ)\n\t\t\t\tloss_epoch += float(loss.item())\n\n\t\t\t\tones = pred_odms > .5\n\t\t\t\tzeros = pred_odms <= .5\n\t\t\t\tpred_odms[ones] =  pred_odms.shape[-1]\n\t\t\t\tpred_odms[zeros] = 0 \n\n\t\t\t\tNN_pred = up_sample(inp_voxels)\n\t\t\t\tiou_NN = kal.metrics.voxel.iou(NN_pred.contiguous(), tgt_voxels)\n\t\t\t\tiou_NN_epoch += iou_NN\n\t\t\t\t\n\t\t\t\tpred_voxels = []\n\t\t\t\tfor odms, voxel_NN in zip(pred_odms, NN_pred): \n\t\t\t\t\tpred_voxels.append(kal.rep.voxel.project_odms(odms, voxel = voxel_NN, votes = 2).unsqueeze(0))\n\t\t\t\tpred_voxels = torch.cat(pred_voxels)\n\t\t\t\tiou = kal.metrics.voxel.iou(pred_voxels.contiguous(), tgt_voxels)\n\t\t\t\tiou_epoch += iou\n\t\t\t\t\n\n\t\t\t\t# logging\n\t\t\t\tnum_batches += 1\n\t\t\t\tif i % args.print_every == 0:\n\t\t\t\t\t\tout_iou = iou_epoch.item() / float(num_batches)\n\t\t\t\t\t\tout_iou_NN = iou_NN_epoch.item() / float(num_batches)\n\t\t\t\t\t\ttqdm.write(f\'[VAL] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou}, Iou Base: {out_iou_NN}\')\n\t\t\t\t\t\t\n\t\t\tout_iou = iou_epoch.item() / float(num_batches)\n\t\t\tout_iou_NN = iou_NN_epoch.item() / float(num_batches)\n\t\t\ttqdm.write(f\'[VAL Total] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou}, Iou Base: {out_iou_NN}\')\n\n\t\t\tloss_epoch = loss_epoch / num_batches\n\t\t\tself.val_loss.append(out_iou)\n\n\tdef save(self):\n\n\t\tsave_best = False\n\t\tif self.val_loss[-1] >= self.bestval:\n\t\t\tself.bestval = self.val_loss[-1]\n\t\t\tsave_best = True\n\t\t\n\t\t# Create a dictionary of all data to save\n\t\tlog_table = {\n\t\t\t\'epoch\': self.cur_epoch,\n\t\t\t\'bestval\': np.min(np.asarray(self.val_loss)),\n\t\t\t\'train_loss\': self.train_loss,\n\t\t\t\'val_loss\': self.val_loss,\n\t\t\t\'train_metrics\': [\'NLLLoss\', \'iou\'],\n\t\t\t\'val_metrics\': [\'NLLLoss\', \'iou\', \'iou_NN\'],\n\t\t}\n\n\t\todm_type = \'occ\'\n\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, odm_type + \'recent.pth\'))\n\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, odm_type + \'recent_optim.pth\'))\n\t\t# Log other data corresponding to the recent model\n\t\twith open(os.path.join(args.logdir, odm_type + \'recent.log\'), \'w\') as f:\n\t\t\tf.write(json.dumps(log_table))\n\n\t\ttqdm.write(\'====== Saved recent model ======>\')\n\t\t\n\t\tif save_best:\n\t\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, odm_type + \'best.pth\'))\n\t\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, odm_type + \'best_optim.pth\'))\n\t\t\t# Log other data corresponding to the recent model\n\t\t\twith open(os.path.join(args.logdir, odm_type + \'best.log\'), \'w\') as f:\n\t\t\t\tf.write(json.dumps(log_table))\n\t\t\ttqdm.write(\'====== Overwrote best model ======>\')\n\n\n\ntrainer = Engine_Occ()\nfor epoch in range(args.epochs): \n\ttrainer.train()\n\ttrainer.validate()\n\ttrainer.save()'"
examples/SuperResolution/ODM-ModelNet/utils.py,3,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kaolin as kal \nimport torch\n\ndef down_sample(tgt): \n\tinp = []\n\tfor t in tgt : \n\t\tlow_res_inp = kal.rep.voxel.scale_down(t, scale = [2, 2, 2])\n\t\tlow_res_inp = kal.rep.voxel.threshold(low_res_inp, .1)\n\t\tinp.append(low_res_inp.unsqueeze(0))\n\tinp = torch.cat(inp, dim = 0 )\n\treturn inp\n\ndef up_sample(inp): \n\tNN_pred = []\n\tfor voxel in inp: \n\t\tNN_pred.append(kal.rep.voxel.scale_up(voxel, dim = 30))\n\tNN_pred = torch.stack(NN_pred)\n\treturn NN_pred\n\ndef to_occpumancy_map(inp, threshold = None):\n\tif threshold is None: \n\t\tthreshold = inp.shape[-1]\n\tzeros = inp< threshold\n\tones = inp >= threshold\n\tinp = inp.clone()\n\tinp[ones] = 1 \n\tinp[zeros] = 0 \n\treturn inp\n\n\ndef upsample_omd(inp): \n\tscaling = torch.nn.Upsample(scale_factor=2, mode=\'nearest\')\n\tinp = scaling(inp)\n\treturn inp\n\n'"
examples/SuperResolution/ODM-ShapeNet/architectures.py,3,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n\n\n\nclass upscale(nn.Module):\n\tdef __init__(self, high, low):\n\t\tsuper(upscale, self).__init__()\n\t\tself.ratio = high // low \n\t\tself.layer1 = nn.Sequential(\n\t\t\tnn.Conv2d(6, 128, kernel_size=3, padding=1),\n\t\t\tnn.BatchNorm2d(128))\n\n\t\tself.inner_convs_1 = nn.ModuleList([nn.Conv2d(128, 128, kernel_size=3, padding=1) for i in range(16)])\n\t\tself.inner_bns_1 = nn.ModuleList([nn.BatchNorm2d(128) for i in range(16)])\n\t\tself.inner_convs_2 = nn.ModuleList([nn.Conv2d(128, 128, kernel_size=3, padding=1) for i in range(16)])\n\t\tself.inner_bns_2 = nn.ModuleList([nn.BatchNorm2d(128) for i in range(16)])\n\t\t\n\t\tself.layer2 = nn.Sequential(\n\t\t\tnn.Conv2d(128, 128, kernel_size=3, padding=1),\n\t\t\tnn.BatchNorm2d(128),\n\t\t)\n\n\t\tsub_list = [nn.Conv2d(128, 128, kernel_size=3, padding=1), nn.PixelShuffle(2)]\n\t\ti = 0 \n\t\tfor i in range(int(math.log(self.ratio,2))-1):\n\t\t\tsub_list.append(nn.Conv2d(32, 128,kernel_size=3, padding=1))\n\t\t\tsub_list.append(nn.PixelShuffle(2))\n\t\tself.sub_list = nn.ModuleList(sub_list)\n\n\t\tself.layer3 = nn.Sequential(\n\t\t\tnn.Conv2d( 32, 6, kernel_size=1, padding=0),\n\t\t\t\n\t\t)\n\t\t\n\n\tdef forward(self, x):\n\t\tx = self.layer1(x)\n\t\ttemp = x.clone()\n\t\tfor i in range(16): \n\t\t\trecall = self.inner_convs_1[i](x.clone())\n\t\t\trecall = self.inner_bns_1[i](recall)\n\t\t\trecall = F.relu(recall)\n\t\t\trecall = self.inner_convs_2[i](recall)\n\t\t\trecall = self.inner_bns_2[i](recall)\n\t\t\trecall = recall + temp \n\t\t\ttemp = recall.clone()\n\t\trecall = self.layer2(recall)\n\t\tx = x + recall \n\t\n\t\tfor i in range(int(math.log(self.ratio,2))):\n\t\t\tx = self.sub_list[2*i](x)\n\t\t\tx = self.sub_list[2*i + 1](x)\n\t\t\n\t\tx = self.layer3(x)\n\t\tx = torch.sigmoid(x)\n\t\treturn x\n'"
examples/SuperResolution/ODM-ShapeNet/dataloaders.py,3,"b'""""""\nDataset classes\n""""""\n\nimport numpy as np\nimport os\nimport torch\nfrom tqdm import tqdm \n\nimport torch.utils.data as data\nimport scipy.io as sio\nimport scipy.sparse\n\nimport kaolin as kal\n\n\n\nclass ShapeNet_ODMS(object):\n\tr""""""\n\tDataloader for downloading and reading from ModelNet \n\n\tNote: \n\t\tMade to be passed to the torch dataloader \n\n\tArgs: \n\t\troot (str): location the dataset should be downloaded to /loaded from \n\t\ttrain (bool): if True loads training set, else loads test \n\t\tdownload (bool): downloads the dataset if not found in root \n\t\tobject_class (str): object class to be loaded, if \'all\' then all are loaded \n\t\tsingle_view (bool): if true only on roation is used, if not all 12 views are loaded \n\n\tAttributes: \n\t\tlist: list of all voxel locations to be loaded from \n\n\tExamples:\n\t\t>>> data_set = ModelNet(root =\'./datasets/\')\n\t\t>>> train_loader = DataLoader(data_set, batch_size=10, n=True, num_workers=8) \n\n\t""""""\n\n\tdef __init__(self, root=\'../datasets/\', train=True, download=True, compute=True, high=128, low=32, categories=[\'chair\'], single_view=True, voxels = True, split = .7):\n\t\tself.high = high\n\t\tself.low = low\n\t\tvoxel_set = kal.dataloader.ShapeNet.Voxels( root, train=train, download=download, categories=categories, resolutions=[high,low], split = split)\n\t\todm_location = root + \'/ShapeNet/ODMs/\'\n\t\tself.load_voxels = voxels\n\t\tif self.load_voxels :  \n\t\t\tself.voxel_names = {}\n\n\t\tself.names = {}\n\t\tif not os.path.exists(odm_location) and compute:\n\t\t\t\tprint (\'ShapeNet ODMS were not found at {0}, and compute is set to False\'.format(odm_location))\n\t\n\t\tfor res in [high, low]:\n\t\t\tself.names[res] = []\n\t\t\tif voxels: \n\t\t\t\tself.voxel_names[res] = []\n\t\t\tprint (""Computing ODMs from Shapenet Dataset classes {0} in resolution {1}"".format(categories, res))\n\t\t\t\n\t\t\tfor n in tqdm(voxel_set.names[res]): \n\t\t\t\texample_location = odm_location + n.split(\'voxel\')[-1][:-4] + \'.mat\'\n\t\t\t\texample_length = len(example_location.split(\'/\')[-1])\n\t\t\t\texample_folder = example_location[:-example_length]\n\t\t\t\tif not os.path.exists(example_folder):\n\t\t\t\t\tif compute:  \n\t\t\t\t\t\tos.makedirs(example_folder)\n\t\t\t\t\telse: \n\t\t\t\t\t\tprint (\'ModelNet ODMS were not found at {0}, and compute is set to False\'.format(example_location))\n\t\t\t\tif not os.path.exists(example_location):\n\t\t\t\t\tvoxel = scipy.sparse.load_npz(n)\n\t\t\t\t\tvoxel = np.array((voxel.todense()))\n\t\t\t\t\tvoxel_res = voxel.shape[0]\n\t\t\t\t\tvoxel = voxel.reshape((voxel_res, voxel_res, voxel_res))\n\t\t\t\t\todms = kal.rep.voxel.extract_odms(voxel)\n\t\t\t\t\todms = np.array(odms)\n\t\t\t\t\tsio.savemat(example_location, {\'odm\': odms})\n\n\t\t\t\tself.names[res].append(example_location)\n\t\t\t\tif self.load_voxels: \n\t\t\t\t\tself.voxel_names[res].append(n)\n\n\n\t\t\t\t\t\t\n\tdef __len__(self):\n\t\t"""""" \n\t\tReturns:\n\t\t\tnumber of odms lists in active dataloader\n\n\t\t""""""\n\t\treturn len(self.names[self.high])\n\n\tdef __getitem__(self, item):\n\t\t""""""Gets a single example of a ModelNet voxel model \n\t\tArgs: \n\t\t\titem (int): index of required model \n\n\t\treturn: \n\t\t\tdictionary which contains a odm data\n\n\t\t""""""\n\t\tdata = {}\n\t\tfor res in [self.high, self.low]:\n\t\t\todm_path = self.names[res][item]\n\t\t\todms = sio.loadmat(odm_path)[\'odm\']\n\t\t\tdata[\'odms_{0}\'.format(res)] = torch.FloatTensor(odms.astype(float))\n\t\t\tif self.load_voxels: \n\t\t\t\tvoxel = scipy.sparse.load_npz(self.voxel_names[res][item])\n\t\t\t\tvoxel = np.array((voxel.todense()))\n\t\t\t\tvoxel_res = voxel.shape[0]\n\t\t\t\tvoxel = voxel.reshape((voxel_res, voxel_res, voxel_res))\n\t\t\t\tdata[\'voxels_{0}\'.format(res)] = torch.FloatTensor(voxel.astype(float))\n\t\treturn data\n\n\n'"
examples/SuperResolution/ODM-ShapeNet/eval_Direct.py,4,"b""import argparse\nimport os\nimport torch\nimport sys\nfrom tqdm import tqdm\n\nfrom torch.utils.data import DataLoader\n\nfrom architectures import upscale\nfrom utils import down_sample, up_sample\nfrom dataloaders import ShapeNet_ODMS\nimport kaolin as kal \n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('-expid', type=str, default='Direct', help='Unique experiment identifier.')\nparser.add_argument('-device', type=str, default='cuda', help='Device to use')\nparser.add_argument('-categories', type=str,nargs='+', default=['chair'], help='list of object classes to use')\nparser.add_argument('-vis', action='store_true', help='Visualize each model while evaluating')\nparser.add_argument('-batchsize', type=int, default=16, help='Batch size.')\nargs = parser.parse_args()\n\n\n\n# Data\nvalid_set = ShapeNet_ODMS(root ='../../datasets/',categories = args.categories, \\\n\tdownload = True, train = False, high = 128, low = 32, split=.97, voxels = True)\ndataloader_val = DataLoader(valid_set, batch_size=args.batchsize, shuffle=False, \\\n\tnum_workers=8)\n\n\n# Model\nmodel = upscale(128,32)\nmodel = model.to(args.device)\n# Load saved weights\nmodel.load_state_dict(torch.load('log/{0}/best.pth'.format(args.expid)))\n\niou_epoch = 0.\niou_NN_epoch = 0.\nnum_batches = 0\n\nmodel.eval()\nwith torch.no_grad():\n\tfor data in tqdm(dataloader_val): \n\t\t\n\t\ttgt_odms = data['odms_128'].to(args.device)\n\t\ttgt_voxels = data['voxels_128'].to(args.device)\n\t\tinp_odms = data['odms_32'].to(args.device)\n\t\tinp_voxels = data['voxels_32'].to(args.device)\n\n\t\t# inference \n\t\tpred_odms = model(inp_odms)*128\n\n\t\tNN_pred = up_sample(inp_voxels)\n\t\tiou_NN = kal.metrics.voxel.iou(NN_pred.contiguous(), tgt_voxels)\n\t\tiou_NN_epoch += iou_NN\n\n\t\tpred_odms = pred_odms.int()\n\t\tpred_voxels = []\n\t\tfor odms, NN_odms in zip(pred_odms, NN_pred): \n\t\t\tpred_voxels.append(kal.rep.voxel.project_odms(odms, voxel = NN_odms, votes = 2).unsqueeze(0))\n\t\tpred_voxels = torch.cat(pred_voxels)\n\n\t\tiou = kal.metrics.voxel.iou(pred_voxels.contiguous(), tgt_voxels)\n\t\tiou_epoch += iou\n\t\t\n\n\t\t\n\t\t\n\t\tif args.vis: \n\t\t\tfor i in range(inp_voxels.shape[0]):\t\n\t\t\t\tprint ('Rendering low resolution input')\n\t\t\t\tkal.visualize.show_voxel(inp_voxels[i], mode = 'exact', thresh = .5)\n\t\t\t\tprint ('Rendering high resolution target')\n\t\t\t\tkal.visualize.show_voxel(tgt_voxels[i], mode = 'exact', thresh = .5)\n\t\t\t\tprint ('Rendering high resolution prediction')\n\t\t\t\tkal.visualize.show_voxel(pred_voxels[i], mode = 'exact', thresh = .5)\n\t\t\t\tprint('----------------------')\n\t\tnum_batches += 1 \n\nout_iou_NN = iou_NN_epoch.item() / float(num_batches)\nprint ('Nearest Neighbor Baseline IoU over validation set is {0}'.format(out_iou_NN))\nout_iou = iou_epoch.item() / float(num_batches)\nprint ('IoU over validation set is {0}'.format(out_iou))"""
examples/SuperResolution/ODM-ShapeNet/eval_MVD.py,5,"b""import argparse\nimport os\nimport torch\nimport sys\nfrom tqdm import tqdm \nfrom PIL import Image \n\nfrom torch.utils.data import DataLoader\n\nfrom architectures import upscale\nfrom utils import down_sample\nfrom dataloaders import ShapeNet_ODMS\nfrom utils import down_sample, up_sample, upsample_omd, to_occpumancy_map\nimport kaolin as kal \n\nparser = argparse.ArgumentParser()\nparser.add_argument('-expid', type=str, default='MVD', help='Unique experiment identifier.')\nparser.add_argument('-device', type=str, default='cuda', help='Device to use')\nparser.add_argument('-categories', type=str,nargs='+', default=['chair'], help='list of object classes to use')\nparser.add_argument('-vis', action='store_true', help='Visualize each model while evaluating')\nparser.add_argument('-batchsize', type=int, default=16, help='Batch size.')\nargs = parser.parse_args()\n\n# Data\nvalid_set = ShapeNet_ODMS(root ='../..//datasets',categories = args.categories, \\\n\tdownload = True, train = False, high = 128, low = 32, split=.97, voxels = True)\ndataloader_val = DataLoader(valid_set, batch_size=args.batchsize, shuffle=False, \\\n\tnum_workers=8)\n\n\n# Model\nmodel_res = upscale(128,32)\nmodel_res = model_res.to(args.device)\nmodel_occ = upscale(128,32)\nmodel_occ = model_occ.to(args.device)\n# Load saved weights\nmodel_res.load_state_dict(torch.load('log/{0}/resbest.pth'.format(args.expid)))\nmodel_occ.load_state_dict(torch.load('log/{0}/occbest.pth'.format(args.expid)))\n\n\niou_epoch = 0.\niou_NN_epoch = 0.\nnum_batches = 0\n\n\nmodel_res.eval()\nmodel_occ.eval()\nwith torch.no_grad():\n\tfor data in tqdm(dataloader_val): \n\t\t\n\t\ttgt_odms = data['odms_128'].to(args.device)\n\t\ttgt_voxels = data['voxels_128'].to(args.device)\n\t\ttgt_odms_occ = to_occpumancy_map(tgt_odms)\n\t\tinp_odms = data['odms_32'].to(args.device)\n\t\tinp_voxels = data['voxels_32'].to(args.device)\n\t\t\n\t\t# inference res\n\t\tinitial_odms = upsample_omd(inp_odms)*4\n\t\tdistance = 128 - initial_odms\n\t\tpred_odms_update = model_res(inp_odms)\n\t\tpred_odms_update = pred_odms_update * distance\n\t\tpred_odms_res = initial_odms + pred_odms_update\n\n\t\t# inference occ\n\t\tpred_odms_occ = model_occ(inp_odms)\n\n\t\t# combine\n\t\tpred_odms_res = pred_odms_res.int()\n\t\tones = pred_odms_occ > .5\n\t\tzeros = pred_odms_occ <= .5\n\t\tpred_odms_occ[ones] =  pred_odms_occ.shape[-1]\n\t\tpred_odms_occ[zeros] = 0  \n\n\t\tNN_pred = up_sample(inp_voxels)\n\t\tiou_NN = kal.metrics.voxel.iou(NN_pred.contiguous(), tgt_voxels)\n\t\tiou_NN_epoch += iou_NN\n\t\t\n\t\tpred_voxels = []\n\t\tfor i in range(inp_voxels.shape[0]):\t\n\t\t\tvoxel = NN_pred[i]\n\t\t\tvoxel = kal.rep.voxel.project_odms(pred_odms_res[i], voxel = voxel, votes = 2)\n\t\t\tvoxel = kal.rep.voxel.project_odms(pred_odms_occ[i], voxel = voxel, votes = 2)\n\t\t\tvoxel = voxel.unsqueeze(0)\n\t\t\tpred_voxels.append(voxel)\n\t\tpred_voxels = torch.cat(pred_voxels)\n\t\tiou = kal.metrics.voxel.iou(pred_voxels.contiguous(), tgt_voxels)\n\t\tiou_epoch += iou\n\n\t\t\n\t\tif args.vis: \n\t\t\tfor i in range(inp_voxels.shape[0]):\t\n\t\t\t\n\t\t\t\tprint ('Rendering low resolution input')\n\t\t\t\tkal.visualize.show_voxel(inp_voxels[i], mode = 'exact', thresh = .5)\n\t\t\t\tprint ('Rendering high resolution target')\n\t\t\t\tkal.visualize.show_voxel(tgt_voxels[i], mode = 'exact', thresh = .5)\n\t\t\t\tprint ('Rendering high resolution prediction')\n\t\t\t\tkal.visualize.show_voxel(pred_voxels[i], mode = 'exact', thresh = .5)\n\t\t\t\tprint('----------------------')\n\t\tnum_batches += 1  \niou_NN_epoch = iou_NN_epoch.item() / float(num_batches)\nprint ('IoU for Nearest Neighbor baseline over validation set is {0}'.format(iou_NN_epoch))\t\t\nout_iou = iou_epoch.item() / float(num_batches)\nprint ('IoU over validation set is {0}'.format(out_iou))"""
examples/SuperResolution/ODM-ShapeNet/train_Direct.py,11,"b'import argparse\nimport json\nimport numpy as np\nimport os\nimport sys\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\n\nfrom architectures import upscale\nfrom dataloaders import ShapeNet_ODMS\nfrom utils import down_sample, up_sample\nimport kaolin as kal \n""""""\nCommandline arguments\n""""""\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-expid\', type=str, default=\'Direct\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'-device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'-categories\', type=str,nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'-epochs\', type=int, default=30, help=\'Number of train epochs.\')\nparser.add_argument(\'-batchsize\', type=int, default=16, help=\'Batch size.\')\nparser.add_argument(\'-lr\', type=float, default=1e-3, help=\'Learning rate.\')\nparser.add_argument(\'-val-every\', type=int, default=5, help=\'Validation frequency (epochs).\')\nparser.add_argument(\'-print-every\', type=int, default=20, help=\'Print frequency (batches).\')\nparser.add_argument(\'-logdir\', type=str, default=\'log\', help=\'Directory to log data to.\')\nparser.add_argument(\'-save-model\', action=\'store_true\', help=\'Saves the model and a snapshot \\\n\tof the optimizer state.\')\nargs = parser.parse_args()\n\n\n\n\n""""""\nDataset\n""""""\ntrain_set = ShapeNet_ODMS(root =\'../../datasets/\',categories = args.categories,  \\\n\tdownload = True, high = 128, low = 32, split=.97, voxels = False)\ndataloader_train = DataLoader(train_set, batch_size=args.batchsize, shuffle=True, \\\n\tnum_workers=8)\n\nvalid_set = ShapeNet_ODMS(root =\'../../datasets/\',categories = args.categories, \\\n\tdownload = True, train = False, high = 128, low = 32, split=.97, voxels = True)\ndataloader_val = DataLoader(valid_set, batch_size=args.batchsize, shuffle=False, \\\n\tnum_workers=8)\n\n\n""""""\nModel settings \n""""""\nmodel = upscale(128, 32 ).to(args.device)\n\nloss_fn = torch.nn.MSELoss()\n\noptimizer = optim.Adam(model.parameters(), lr=args.lr)\n\n\n# Create log directory, if it doesn\'t already exist\nargs.logdir = os.path.join(args.logdir, args.expid)\nif not os.path.isdir(args.logdir):\n\tos.makedirs(args.logdir)\n\tprint(\'Created dir:\', args.logdir)\n\n# Log all commandline args\nwith open(os.path.join(args.logdir, \'args.txt\'), \'w\') as f:\n\tjson.dump(args.__dict__, f, indent=2)\n \n\n\n\n\n\nclass Engine(object):\n\t""""""Engine that runs training and inference.\n\tArgs\n\t\t- cur_epoch (int): Current epoch.\n\t\t- print_every (int): How frequently (# batches) to print loss.\n\t\t- validate_every (int): How frequently (# epochs) to run validation.\n\t\t\n\t""""""\n\n\tdef __init__(self,  cur_epoch=0, print_every=1, validate_every=1):\n\t\tself.cur_epoch = cur_epoch\n\t\tself.train_loss = []\n\t\tself.val_loss = []\n\t\tself.bestval = 0\n\n\tdef train(self):\n\t\tloss_epoch = 0.\n\t\tnum_batches = 0\n\t\tdiff = 0 \n\t\tmodel.train()\n\t\t# Train loop\n\t\tfor i, data in enumerate(tqdm(dataloader_train), 0):\n\t\t\toptimizer.zero_grad()\n\t\t\t\n\t\t\t# data creation\n\t\t\ttgt_odms = data[\'odms_128\'].to(args.device)\n\t\t\tinp_odms = data[\'odms_32\'].to(args.device)\n\t\t\t\n\t\t\t# inference \n\t\t\tpred_odms = model(inp_odms)*128\n\n\t\t\t# losses \n\t\t\tloss = loss_fn(pred_odms, tgt_odms)\n\t\t\tloss.backward()\n\t\t\tloss_epoch += float(loss.item())\n\n\t\t\t# logging\n\t\t\tnum_batches += 1\n\t\t\tif i % args.print_every == 0:\n\t\t\t\ttqdm.write(f\'[TRAIN] Epoch {self.cur_epoch:03d}, Batch {i:03d}: Loss: {float(loss.item())}\')\n\t\t\t\t\n\t\t\toptimizer.step()\n\t\t\n\t\t\n\t\tloss_epoch = loss_epoch / num_batches\n\t\tself.train_loss.append(loss_epoch)\n\t\tself.cur_epoch += 1\n\n\t\t\n\t\t\n\tdef validate(self):\n\t\tmodel.eval()\n\t\twith torch.no_grad():\t\n\t\t\tiou_epoch = 0.\n\t\t\tiou_NN_epoch = 0.\n\t\t\tnum_batches = 0\n\t\t\tloss_epoch = 0.\n\n\t\t\t# Validation loop\n\t\t\tfor i, data in enumerate(tqdm(dataloader_val), 0):\n\n\t\t\t\t# data creation\n\t\t\t\t# data creation\n\t\t\t\ttgt_odms = data[\'odms_128\'].to(args.device)\n\t\t\t\ttgt_voxels = data[\'voxels_128\'].to(args.device)\n\t\t\t\tinp_odms = data[\'odms_32\'].to(args.device)\n\t\t\t\tinp_voxels = data[\'voxels_32\'].to(args.device)\n\n\t\t\t\t# inference \n\t\t\t\tpred_odms = model(inp_odms)*128\n\n\t\t\t\t# losses \n\t\t\t\tloss = loss_fn(pred_odms, tgt_odms)\n\t\t\t\tloss_epoch += float(loss.item())\n\n\t\t\t\tNN_pred = up_sample(inp_voxels)\n\t\t\t\tiou_NN = kal.metrics.voxel.iou(NN_pred.contiguous(), tgt_voxels)\n\t\t\t\tiou_NN_epoch += iou_NN\n\n\t\t\t\tpred_odms = pred_odms.int()\n\t\t\t\tpred_voxels = []\n\t\t\t\tfor odms, voxel_NN in zip(pred_odms, NN_pred): \n\t\t\t\t\tpred_voxels.append(kal.rep.voxel.project_odms(odms, voxel= voxel_NN, votes = 2).unsqueeze(0))\n\t\t\t\tpred_voxels = torch.cat(pred_voxels)\n\t\t\t\tiou = kal.metrics.voxel.iou(pred_voxels.contiguous(), tgt_voxels)\n\t\t\t\tiou_epoch += iou\n\n\t\t\t\t# logging\n\t\t\t\tnum_batches += 1\n\t\t\t\tif i % args.print_every == 0:\n\t\t\t\t\t\tout_iou = iou_epoch.item() / float(num_batches)\n\t\t\t\t\t\tout_iou_NN = iou_NN_epoch.item() / float(num_batches)\n\t\t\t\t\t\ttqdm.write(f\'[VAL] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou}, Iou Base: {out_iou_NN}\')\n\t\t\t\t\t\t\n\t\t\tout_iou = iou_epoch.item() / float(num_batches)\n\t\t\tout_iou_NN = iou_NN_epoch.item() / float(num_batches)\n\t\t\ttqdm.write(f\'[VAL Total] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou}, Iou Base: {out_iou_NN}\')\n\n\t\t\tloss_epoch = loss_epoch / num_batches\n\t\t\tself.val_loss.append(out_iou)\n\n\tdef save(self):\n\n\t\tsave_best = False\n\t\tif self.val_loss[-1] >= self.bestval:\n\t\t\tself.bestval = self.val_loss[-1]\n\t\t\tsave_best = True\n\t\t\n\t\t# Create a dictionary of all data to save\n\t\tlog_table = {\n\t\t\t\'epoch\': self.cur_epoch,\n\t\t\t\'bestval\': np.min(np.asarray(self.val_loss)),\n\t\t\t\'train_loss\': self.train_loss,\n\t\t\t\'val_loss\': self.val_loss,\n\t\t\t\'train_metrics\': [\'NLLLoss\', \'iou\'],\n\t\t\t\'val_metrics\': [\'NLLLoss\', \'iou\', \'iou_NN\'],\n\t\t}\n\n\t\t# Save the recent model/optimizer states\n\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, \'recent.pth\'))\n\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, \'recent_optim.pth\'))\n\t\t# Log other data corresponding to the recent model\n\t\twith open(os.path.join(args.logdir, \'recent.log\'), \'w\') as f:\n\t\t\tf.write(json.dumps(log_table))\n\n\t\ttqdm.write(\'====== Saved recent model ======>\')\n\t\t\n\t\tif save_best:\n\t\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, \'best.pth\'))\n\t\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, \'best_optim.pth\'))\n\t\t\t# Log other data corresponding to the recent model\n\t\t\twith open(os.path.join(args.logdir, \'best.log\'), \'w\') as f:\n\t\t\t\tf.write(json.dumps(log_table))\n\t\t\ttqdm.write(\'====== Overwrote best model ======>\')\n\t\t\t\n\t\ntrainer = Engine()\n\nfor i, epoch in enumerate(range(args.epochs)): \n\ttrainer.train()\n\tif i % 4 == 0: \n\t\ttrainer.validate()\n\t\ttrainer.save()'"
examples/SuperResolution/ODM-ShapeNet/train_MVD.py,18,"b'import argparse\nimport json\nimport numpy as np\nimport os\nimport sys\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\n\nfrom architectures import upscale\nfrom dataloaders import ShapeNet_ODMS\nfrom utils import down_sample, up_sample, upsample_omd, to_occpumancy_map\nimport kaolin as kal \n""""""\nCommandline arguments\n""""""\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-expid\', type=str, default=\'MVD\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'-device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'-categories\', type=str,nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'-epochs\', type=int, default=30, help=\'Number of train epochs.\')\nparser.add_argument(\'-batchsize\', type=int, default=16, help=\'Batch size.\')\nparser.add_argument(\'-lr\', type=float, default=1e-4, help=\'Learning rate.\')\nparser.add_argument(\'-val-every\', type=int, default=5, help=\'Validation frequency (epochs).\')\nparser.add_argument(\'-print-every\', type=int, default=20, help=\'Print frequency (batches).\')\nparser.add_argument(\'-logdir\', type=str, default=\'log\', help=\'Directory to log data to.\')\nparser.add_argument(\'-save-model\', action=\'store_true\', help=\'Saves the model and a snapshot \\\n\tof the optimizer state.\')\nargs = parser.parse_args()\n\n\n\n\n""""""\nDataset\n""""""\ntrain_set = ShapeNet_ODMS(root =\'../../datasets/\',categories = args.categories,  \\\n\tdownload = True, high = 128, low = 32, split=.97, voxels = True)\ndataloader_train = DataLoader(train_set, batch_size=args.batchsize, shuffle=True, \\\n\tnum_workers=8)\n\nvalid_set = ShapeNet_ODMS(root =\'../../datasets/\',categories = args.categories, \\\n\tdownload = True, train = False, high = 128, low = 32, split=.97, voxels = True)\ndataloader_val = DataLoader(valid_set, batch_size=args.batchsize, shuffle=False, \\\n\tnum_workers=8)\n\n\n""""""\nModel settings \n""""""\nmodel = upscale(128, 32 ).to(args.device)\n\nloss_fn = torch.nn.MSELoss()\n\noptimizer = optim.Adam(model.parameters(), lr=args.lr)\n\n\n# Create log directory, if it doesn\'t already exist\nargs.logdir = os.path.join(args.logdir, args.expid)\nif not os.path.isdir(args.logdir):\n\tos.makedirs(args.logdir)\n\tprint(\'Created dir:\', args.logdir)\n\n# Log all commandline args\nwith open(os.path.join(args.logdir, \'args.txt\'), \'w\') as f:\n\tjson.dump(args.__dict__, f, indent=2)\n \n\n\n\n\n\nclass Engine_Residual(object):\n\t""""""Engine that runs training and inference.\n\tArgs\n\t\t- cur_epoch (int): Current epoch.\n\t\t- print_every (int): How frequently (# batches) to print loss.\n\t\t- validate_every (int): How frequently (# epochs) to run validation.\n\t\t\n\t""""""\n\n\tdef __init__(self,  cur_epoch=0, print_every=1, validate_every=1):\n\t\tself.cur_epoch = cur_epoch\n\t\tself.train_loss = []\n\t\tself.val_loss = []\n\t\tself.bestval = 0\n\n\tdef train(self):\n\t\tloss_epoch = 0.\n\t\tnum_batches = 0\n\t\tdiff = 0 \n\t\tmodel.train()\n\t\t# Train loop\n\t\tfor i, data in enumerate(tqdm(dataloader_train), 0):\n\t\t\toptimizer.zero_grad()\n\t\t\t\n\t\t\t# data creation\n\t\t\t\n\t\t\t\n\n\t\t\ttgt_odms = data[\'odms_128\'].to(args.device)\n\t\t\tinp_odms = data[\'odms_32\'].to(args.device)\n\t\t\t\n\t\t\t# inference \n\t\t\tinitial_odms = upsample_omd(inp_odms)*4\n\t\t\tdistance = 128 - initial_odms\n\t\t\tpred_odms_update = model(inp_odms)\n\t\t\tpred_odms_update = pred_odms_update * distance\n\t\t\tpred_odms = initial_odms + pred_odms_update\n\n\t\t\t# losses \n\t\t\tloss = loss_fn(pred_odms, tgt_odms)\n\t\t\tloss.backward()\n\t\t\tloss_epoch += float(loss.item())\n\n\t\t\t# logging\n\t\t\tnum_batches += 1\n\t\t\tif i % args.print_every == 0:\n\t\t\t\ttqdm.write(f\'[TRAIN] Epoch {self.cur_epoch:03d}, Batch {i:03d}: Loss: {float(loss.item())}\')\n\t\t\t\t\n\t\t\toptimizer.step()\n\t\t\n\t\t\n\t\tloss_epoch = loss_epoch / num_batches\n\t\tself.train_loss.append(loss_epoch)\n\t\tself.cur_epoch += 1\n\n\t\t\n\t\t\n\tdef validate(self):\n\t\tmodel.eval()\n\t\twith torch.no_grad():\t\n\t\t\tiou_epoch = 0.\n\t\t\tiou_NN_epoch = 0.\n\t\t\tnum_batches = 0\n\t\t\tloss_epoch = 0.\n\n\t\t\t# Validation loop\n\t\t\tfor i, data in enumerate(tqdm(dataloader_val), 0):\n\n\t\t\t\t# data creation\n\t\t\t\ttgt_odms = data[\'odms_128\'].to(args.device)\n\t\t\t\ttgt_voxels = data[\'voxels_128\'].to(args.device)\n\t\t\t\tinp_odms = data[\'odms_32\'].to(args.device)\n\t\t\t\tinp_voxels = data[\'voxels_32\'].to(args.device)\n\t\t\t\t\n\t\t\t\t# inference \n\t\t\t\tinitial_odms = upsample_omd(inp_odms)*4\n\t\t\t\tdistance = 128 - initial_odms\n\t\t\t\tpred_odms_update = model(inp_odms)\n\t\t\t\tpred_odms_update = pred_odms_update * distance\n\t\t\t\tpred_odms = initial_odms + pred_odms_update\n\t\t\t\t\n\t\t\t\t# losses \n\t\t\t\tloss = loss_fn(pred_odms, tgt_odms)\n\t\t\t\tloss_epoch += float(loss.item())\n\n\t\t\t\t\n\t\t\t\tNN_pred = up_sample(inp_voxels)\n\t\t\t\tiou_NN = kal.metrics.voxel.iou(NN_pred.contiguous(), tgt_voxels)\n\t\t\t\tiou_NN_epoch += iou_NN\n\n\n\t\t\t\tpred_voxels = []\n\t\t\t\tpred_odms = pred_odms.int()\n\t\t\t\tfor odms, voxel_NN in zip(pred_odms,NN_pred): \n\t\t\t\t\tpred_voxels.append(kal.rep.voxel.project_odms(odms,voxel_NN , votes = 2).unsqueeze(0))\n\t\t\t\tpred_voxels = torch.cat(pred_voxels)\n\t\t\t\tiou = kal.metrics.voxel.iou(pred_voxels.contiguous(), tgt_voxels)\n\t\t\t\tiou_epoch += iou\n\n\t\t\t\t# logging\n\t\t\t\tnum_batches += 1\n\t\t\t\tif i % args.print_every == 0:\n\t\t\t\t\t\tout_iou = iou_epoch.item() / float(num_batches)\n\t\t\t\t\t\tout_iou_NN = iou_NN_epoch.item() / float(num_batches)\n\t\t\t\t\t\ttqdm.write(f\'[VAL] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou}, Iou Base: {out_iou_NN}\')\n\t\t\t\t\t\t\n\t\t\tout_iou = iou_epoch.item() / float(num_batches)\n\t\t\tout_iou_NN = iou_NN_epoch.item() / float(num_batches)\n\t\t\ttqdm.write(f\'[VAL Total] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou}, Iou Base: {out_iou_NN}\')\n\n\t\t\tloss_epoch = loss_epoch / num_batches\n\t\t\tself.val_loss.append(out_iou)\n\n\tdef save(self):\n\n\t\tsave_best = False\n\t\tif self.val_loss[-1] >= self.bestval:\n\t\t\tself.bestval = self.val_loss[-1]\n\t\t\tsave_best = True\n\t\t\n\t\t\n\t\t# Create a dictionary of all data to save\n\t\tlog_table = {\n\t\t\t\'epoch\': self.cur_epoch,\n\t\t\t\'bestval\': np.min(np.asarray(self.val_loss)),\n\t\t\t\'train_loss\': self.train_loss,\n\t\t\t\'val_loss\': self.val_loss,\n\t\t\t\'train_metrics\': [\'NLLLoss\', \'iou\'],\n\t\t\t\'val_metrics\': [\'NLLLoss\', \'iou\', \'iou_NN\'],\n\t\t}\n\n\t\t# Save the recent model/optimizer states\n\t\todm_type = \'res\'\n\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, odm_type + \'recent.pth\'))\n\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, odm_type + \'recent_optim.pth\'))\n\t\t# Log other data corresponding to the recent model\n\t\twith open(os.path.join(args.logdir, odm_type + \'recent.log\'), \'w\') as f:\n\t\t\tf.write(json.dumps(log_table))\n\n\t\ttqdm.write(\'====== Saved recent model ======>\')\n\t\t\n\t\tif save_best:\n\t\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, odm_type + \'best.pth\'))\n\t\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, odm_type + \'best_optim.pth\'))\n\t\t\t# Log other data corresponding to the recent model\n\t\t\twith open(os.path.join(args.logdir, odm_type + \'best.log\'), \'w\') as f:\n\t\t\t\tf.write(json.dumps(log_table))\n\t\t\ttqdm.write(\'====== Overwrote best model ======>\')\n\n\ntrainer = Engine_Residual()\nfor i, epoch in enumerate(range(args.epochs)): \n\ttrainer.train()\n\tif i % 4 == 0: \n\t\ttrainer.validate()\n\t\ttrainer.save()\n\n\nmodel = upscale(128, 32 ).to(args.device)\nloss_fn =  torch.nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=args.lr)\n\nclass Engine_Occ(object):\n\t""""""Engine that runs training and inference.\n\tArgs\n\t\t- cur_epoch (int): Current epoch.\n\t\t- print_every (int): How frequently (# batches) to print loss.\n\t\t- validate_every (int): How frequently (# epochs) to run validation.\n\t\t\n\t""""""\n\n\tdef __init__(self,  cur_epoch=0, print_every=1, validate_every=1):\n\t\tself.cur_epoch = cur_epoch\n\t\tself.train_loss = []\n\t\tself.val_loss = []\n\t\tself.bestval = 0\n\n\tdef train(self):\n\t\tloss_epoch = 0.\n\t\tnum_batches = 0\n\t\tdiff = 0 \n\n\t\t# Train loop\n\t\tfor i, data in enumerate(tqdm(dataloader_train), 0):\n\t\t\toptimizer.zero_grad()\n\t\t\t\n\t\t\t# data creation\n\t\t\ttgt_odms = data[\'odms_128\'].to(args.device)\n\t\t\tinp_odms = data[\'odms_32\'].to(args.device)\n\t\t\ttgt_odms_occ = to_occpumancy_map(tgt_odms)\n\t\t\t\n\t\t\tdiff += tgt_odms_occ.mean()\n\t\t\t\n\t\t\t# inference \n\t\t\tpred_odms = model(inp_odms)\n\n\t\t\t# losses \n\t\t\tloss = loss_fn(pred_odms, tgt_odms_occ)\n\t\t\t\n\t\t\tloss.backward()\n\t\t\tloss_epoch += float(loss.item())\n\n\t\t\t# logging\n\t\t\tnum_batches += 1\n\t\t\tif i % args.print_every == 0:\n\t\t\t\ttqdm.write(f\'[TRAIN] Epoch {self.cur_epoch:03d}, Batch {i:03d}: Loss: {float(loss.item())}\')\n\t\t\t\t\n\t\t\toptimizer.step()\n\t\t# print ( diff/ float(num_batches))\n\t\t\n\t\tloss_epoch = loss_epoch / num_batches\n\t\tself.train_loss.append(loss_epoch)\n\t\tself.cur_epoch += 1\n\n\t\t\n\t\t\n\tdef validate(self):\n\t\tmodel.eval()\n\t\twith torch.no_grad():\t\n\t\t\tiou_epoch = 0.\n\t\t\tiou_NN_epoch = 0.\n\t\t\tnum_batches = 0\n\t\t\tloss_epoch = 0.\n\n\t\t\t# Validation loop\n\t\t\tfor i, data in enumerate(tqdm(dataloader_val), 0):\n\n\t\t\t\t# data creation\n\t\t\t\ttgt_odms = data[\'odms_128\'].to(args.device)\n\t\t\t\ttgt_voxels = data[\'voxels_128\'].to(args.device)\n\t\t\t\tinp_odms = data[\'odms_32\'].to(args.device)\n\t\t\t\tinp_voxels = data[\'voxels_32\'].to(args.device)\n\t\t\t\ttgt_odms_occ = to_occpumancy_map(tgt_odms)\n\t\t\t\t\n\t\t\t\t# inference \n\t\t\t\tpred_odms = model(inp_odms)\n\n\t\t\t\t# losses \n\t\t\t\tloss = loss_fn(pred_odms, tgt_odms_occ)\n\n\t\t\t\tloss_epoch += float(loss.item())\n\n\t\t\t\tones = pred_odms > .3\n\t\t\t\tzeros = pred_odms <= .7\n\t\t\t\tpred_odms[ones] =  pred_odms.shape[-1]\n\t\t\t\tpred_odms[zeros] = 0 \n\t\t\t\t\n\n\t\t\t\tNN_pred = up_sample(inp_voxels)\n\t\t\t\tiou_NN = kal.metrics.voxel.iou(NN_pred.contiguous(), tgt_voxels)\n\t\t\t\tiou_NN_epoch += iou_NN\n\n\n\t\t\t\tpred_voxels = []\n\t\t\t\tfor odms, voxel_NN in zip(pred_odms,NN_pred): \n\t\t\t\t\tpred_voxels.append(kal.rep.voxel.project_odms(odms, voxel_NN, votes = 2).unsqueeze(0))\n\t\t\t\tpred_voxels = torch.cat(pred_voxels)\n\t\t\t\tiou = kal.metrics.voxel.iou(pred_voxels.contiguous(), tgt_voxels)\n\t\t\t\tiou_epoch += iou\n\t\t\t\t\n\n\t\t\t\t\n\t\t\t\t# logging\n\t\t\t\tnum_batches += 1\n\t\t\t\tif i % args.print_every == 0:\n\t\t\t\t\t\tout_iou = iou_epoch.item() / float(num_batches)\n\t\t\t\t\t\tout_iou_NN = iou_NN_epoch.item() / float(num_batches)\n\t\t\t\t\t\ttqdm.write(f\'[VAL] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou}, Iou Base: {out_iou_NN}\')\n\t\t\t\t\t\t\n\t\t\tout_iou = iou_epoch.item() / float(num_batches)\n\t\t\tout_iou_NN = iou_NN_epoch.item() / float(num_batches)\n\t\t\ttqdm.write(f\'[VAL Total] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou}, Iou Base: {out_iou_NN}\')\n\n\t\t\tloss_epoch = loss_epoch / num_batches\n\t\t\tself.val_loss.append(out_iou)\n\n\tdef save(self):\n\n\t\tsave_best = False\n\t\tif self.val_loss[-1] >= self.bestval:\n\t\t\tself.bestval = self.val_loss[-1]\n\t\t\tsave_best = True\n\t\t\n\t\t# Create a dictionary of all data to save\n\t\tlog_table = {\n\t\t\t\'epoch\': self.cur_epoch,\n\t\t\t\'bestval\': np.min(np.asarray(self.val_loss)),\n\t\t\t\'train_loss\': self.train_loss,\n\t\t\t\'val_loss\': self.val_loss,\n\t\t\t\'train_metrics\': [\'NLLLoss\', \'iou\'],\n\t\t\t\'val_metrics\': [\'NLLLoss\', \'iou\', \'iou_NN\'],\n\t\t}\n\n\t\todm_type = \'occ\'\n\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, odm_type + \'recent.pth\'))\n\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, odm_type + \'recent_optim.pth\'))\n\t\t# Log other data corresponding to the recent model\n\t\twith open(os.path.join(args.logdir, odm_type + \'recent.log\'), \'w\') as f:\n\t\t\tf.write(json.dumps(log_table))\n\n\t\ttqdm.write(\'====== Saved recent model ======>\')\n\t\t\n\t\tif save_best:\n\t\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, odm_type + \'best.pth\'))\n\t\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, odm_type + \'best_optim.pth\'))\n\t\t\t# Log other data corresponding to the recent model\n\t\t\twith open(os.path.join(args.logdir, odm_type + \'best.log\'), \'w\') as f:\n\t\t\t\tf.write(json.dumps(log_table))\n\t\t\ttqdm.write(\'====== Overwrote best model ======>\')\n\n\n\ntrainer = Engine_Occ()\nfor i, epoch in enumerate(range(args.epochs)): \n\ttrainer.train()\n\tif i % 4 == 0: \n\t\ttrainer.validate()\n\t\ttrainer.save()\n'"
examples/SuperResolution/ODM-ShapeNet/utils.py,4,"b""import kaolin as kal \nimport torch\n\ndef down_sample(tgt): \n\tinp = []\n\tfor t in tgt : \n\t\tlow_res_inp = kal.rep.voxel.scale_down(t, scale = [2, 2, 2])\n\t\tlow_res_inp = kal.rep.voxel.threshold(low_res_inp, .1)\n\t\tinp.append(low_res_inp.unsqueeze(0))\n\tinp = torch.cat(inp, dim = 0 )\n\treturn inp\n\ndef up_sample(inp): \n\tscaling = torch.nn.Upsample(scale_factor=4, mode='nearest')\n\tNN_pred = []\n\tfor voxel in inp: \n\t\tNN_pred.append(scaling(voxel.unsqueeze(0).unsqueeze(0)).squeeze(1))\n\tNN_pred = torch.stack(NN_pred).squeeze(1)\n\treturn NN_pred\n\ndef to_occpumancy_map(inp, threshold = None):\n\tif threshold is None: \n\t\tthreshold = inp.shape[-1]\n\tzeros = inp< threshold\n\tones = inp >= threshold\n\tinp = inp.clone()\n\tinp[ones] = 1 \n\tinp[zeros] = 0 \n\treturn inp\n\n\ndef upsample_omd(inp): \n\tscaling = torch.nn.Upsample(scale_factor=4, mode='nearest')\n\tinp = scaling(inp)\n\treturn inp\n\n"""
examples/SuperResolution/voxel-ModelNet/architectures.py,3,"b'""""""\nNetwork architecture definitions\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass EncoderDecoder(nn.Module):\n\t""""""A simple encoder-decoder style voxel superresolution network""""""\n\n\n\tdef __init__(self):\n\t\tsuper(EncoderDecoder, self).__init__()\n\n\t\tself.conv1 = nn.Conv3d(1, 16, 3, stride = 2, padding=1)\n\t\tself.bn1 = nn.BatchNorm3d(16)\n\t\tself.conv2 = nn.Conv3d(16, 32, 3, stride = 2, padding=1)\n\t\tself.bn2 = nn.BatchNorm3d(32)\n\t\tself.deconv3 = nn.ConvTranspose3d(32, 16, 3, stride = 2,  padding=1)\n\t\tself.bn3 = nn.BatchNorm3d(16)\n\t\tself.deconv4 = nn.ConvTranspose3d(16, 8, 3, stride = 2 ,padding=0)\n\t\tself.deconv5 = nn.ConvTranspose3d(8, 1, 3,stride = 2, padding=0)\n\n\t\n\t\t\n\n\n\tdef forward(self, x):\n\t\t\n\n\t\t# Encoder\n\t\tx = (F.relu(self.bn1(self.conv1(x))))\n\t\tx = (F.relu(self.bn2(self.conv2(x))))\n\t\t# Decoder\n\t\tx = F.relu(self.bn3(self.deconv3(x)))\n\t\tx = F.relu(self.deconv4(x))\n\t\n\t\treturn F.sigmoid((self.deconv5(x)))[:,0, :30,:30,:30]\n\n\n\nclass EncoderDecoderForNLL(nn.Module):\n\t""""""A simple encoder-decoder style voxel superresolution network, intended for \n\tuse with the NLL loss. (The major change here is in the shape of each voxel \n\tgrid batch. It is now B x 2 x N x N x N, where B is the batchsize, 2 denotes the \n\toccupancy classes (occupied vs unoccupied), and N is the number of voxels along \n\teach dimension.)\n\t""""""\n\n\tdef __init__(self):\n\t\tsuper(EncoderDecoderForNLL, self).__init__()\n\n\t\tself.conv1 = nn.Conv3d(1, 16, 3, stride = 2, padding=1)\n\t\tself.bn1 = nn.BatchNorm3d(16)\n\t\tself.conv2 = nn.Conv3d(16, 32, 3, stride = 2, padding=1)\n\t\tself.bn2 = nn.BatchNorm3d(32)\n\t\tself.deconv3 = nn.ConvTranspose3d(32, 16, 3, stride = 2,  padding=1)\n\t\tself.bn3 = nn.BatchNorm3d(16)\n\t\tself.deconv4 = nn.ConvTranspose3d(16, 8, 3, stride = 2 ,padding=0)\n\t\tself.deconv5 = nn.ConvTranspose3d(8, 2, 3,stride = 2, padding=0)\n\n\t\n\t\tself.log_softmax = nn.LogSoftmax(dim=1)\n\n\n\tdef forward(self, x):\n\t\t\n\n\t\t# Encoder\n\t\tx = (F.relu(self.bn1(self.conv1(x))))\n\t\tx = (F.relu(self.bn2(self.conv2(x))))\n\t\t# Decoder\n\t\tx = F.relu(self.bn3(self.deconv3(x)))\n\t\tx = F.relu(self.deconv4(x))\n\t\n\t\treturn torch.exp(self.log_softmax(self.deconv5(x)))[:,:, :30,:30,:30]\n\n\n\n'"
examples/SuperResolution/voxel-ModelNet/dataloaders.py,8,"b'""""""\nDataset classes\n""""""\n\nimport numpy as np\nimport os\nimport torch\nimport torch.utils.data as data\n\n\nclass Ellipsoids(data.Dataset):\n\t""""""Dataset class for synthetic ellipsoids.\n\n\tArgs:\n\t\t- datapath_input (str): path to the dir containing the input voxel grids\n\t\t- datapath_target (str): path to the dir containing the target voxel grids\n\t\t- mode (str, choice: [\'train\', \'val\']): train vs val mode\n\t\t- num_train (int, default=5000): number of training samples\n\t\t- num_val (int, default=2000): number of val samples\n\t""""""\n\n\tdef __init__(self, datapath_input, datapath_target, mode=\'train\', \\\n\t\tnum_train=5000, num_val=2000):\n\t\tassert mode in [\'train\', \'val\'], \'Invalide mode specified. Must be train or val.\'\n\t\tself.datapath_input = datapath_input\n\t\tself.datapath_target = datapath_target\n\t\tself.mode = mode\n\t\tself.num_train = num_train\n\t\tself.num_val = num_val\n\n\n\tdef __len__(self):\n\t\t""""""Returns the length of the dataset. """"""\n\t\tif self.mode == \'train\':\n\t\t\treturn self.num_train\n\t\telif self.mode == \'val\':\n\t\t\treturn self.num_val\n\n\n\tdef __getitem__(self, idx):\n\t\t""""""Returns the element at index \'idx\'. """"""\n\t\tif self.mode == \'val\':\n\t\t\tidx += self.num_train\n\t\tfilepath_input = os.path.join(self.datapath_input, str(idx).zfill(5) + \'.npy\')\n\t\tfilepath_target = os.path.join(self.datapath_target, str(idx).zfill(5) + \'.npy\')\n\t\tinp = torch.from_numpy(np.load(filepath_input)).unsqueeze(0)\n\t\ttgt = torch.from_numpy(np.load(filepath_target)).unsqueeze(0)\n\t\treturn inp, tgt\n\n\nclass EllipsoidsForNLL(data.Dataset):\n\t""""""Dataset class for synthetic ellipsoids.\n\n\tArgs:\n\t\t- datapath_input (str): path to the dir containing the input voxel grids\n\t\t- datapath_target (str): path to the dir containing the target voxel grids\n\t\t- mode (str, choice: [\'train\', \'val\']): train vs val mode\n\t\t- num_train (int, default=5000): number of training samples\n\t\t- num_val (int, default=2000): number of val samples\n\t""""""\n\n\tdef __init__(self, datapath_input, datapath_target, mode=\'train\', \\\n\t\tnum_train=5000, num_val=2000):\n\t\tassert mode in [\'train\', \'val\'], \'Invalide mode specified. Must be train or val.\'\n\t\tself.datapath_input = datapath_input\n\t\tself.datapath_target = datapath_target\n\t\tself.mode = mode\n\t\tself.num_train = num_train\n\t\tself.num_val = num_val\n\n\n\tdef __len__(self):\n\t\t""""""Returns the length of the dataset. """"""\n\t\tif self.mode == \'train\':\n\t\t\treturn self.num_train\n\t\telif self.mode == \'val\':\n\t\t\treturn self.num_val\n\n\n\tdef __getitem__(self, idx):\n\t\t""""""Returns the element at index \'idx\'. """"""\n\t\tif self.mode == \'val\':\n\t\t\tidx += self.num_train\n\t\tfilepath_input = os.path.join(self.datapath_input, str(idx).zfill(5) + \'.npy\')\n\t\tfilepath_target = os.path.join(self.datapath_target, str(idx).zfill(5) + \'.npy\')\n\t\tinp_ = torch.from_numpy(np.load(filepath_input)).unsqueeze(0)\n\t\tinp = torch.zeros(2, inp_.shape[1], inp_.shape[2], inp_.shape[3])\n\t\tcond = (inp_[0] == 1).float()\n\t\tinp[1] = cond * inp_ + (1 - cond) * inp_\n\n\t\ttgt = torch.from_numpy(np.load(filepath_target)).long()\n\t\t# This is not needed for the target, when using NLLLoss.\n\t\ttgt_ = torch.from_numpy(np.load(filepath_target)).unsqueeze(0)\n\t\t# tgt = torch.zeros(2, tgt_.shape[1], tgt_.shape[2], tgt_.shape[3])\n\t\t# cond = (tgt_[0] == 1).float()\n\t\t# tgt[1] = cond * tgt_ + (1 - cond) * tgt_\n\t\t\n\t\treturn inp, tgt\n'"
examples/SuperResolution/voxel-ModelNet/datautils.py,0,"b'""""""\nUtils to generate data to test toy 3d super-resolution\n""""""\n\nimport argparse\nimport numpy as np\nimport os\nfrom tqdm import tqdm, trange\n\nfrom genutils import rotx, roty, rotz, points2voxels\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-expid\', type=str, default=\'def\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'-numsamples\', type=int, default=5, help=\'Number of samples to generate.\')\nparser.add_argument(\'-numpts\', type=int, default=100, help=\'Number of points in each sample.\')\nparser.add_argument(\'-debug\', action=\'store_true\', help=\'Debug mode (for visualization).\')\nparser.add_argument(\'-savedir\', type=str, default=\'data\', \\\n\thelp=\'Directory to store generated samples.\')\nparser.add_argument(\'-randomseed\', type=int, default=12, help=\'Seed random number generator.\')\nparser.add_argument(\'-rad-min\', type=float, default=0.1, \\\n\thelp=\'Minimum radius of the generated ellipsoid.\')\nparser.add_argument(\'-rad-max\', type=float, default=1., \\\n\thelp=\'Maximum radius of the generated ellipsoid.\')\nparser.add_argument(\'-res-in\', type=int, default=16, help=\'Resolution of input voxel grid.\')\nparser.add_argument(\'-res-out\', type=int, default=32, help=\'Resolution of output voxel grid.\')\nparser.add_argument(\'-voxsize\', type=float, default=0.2, help=\'Size of each voxel grid cell.\')\nparser.add_argument(\'-rot-min\', type=float, default=-1.5, help=\'Rotation range minimum.\')\nparser.add_argument(\'-rot-max\', type=float, default=1.5, help=\'Rotation range maximum.\')\nparser.add_argument(\'-trans-min\', type=float, default=-0.5, help=\'Translation range minimum.\')\nparser.add_argument(\'-trans-max\', type=float, default=0.5, help=\'Translation range maximum.\')\n\n\ndef sample_on_ellipsoid(numpts, radius=(1.,1.,1.), rot=(0.,0.,0.), trans=(0.,0.,0.)):\n\t""""""Sample points uniformly randomly on the surface of an ellipsoid.\n\n\tArgs:\n\t\t- numpts (int): Number of points to sample\n\t\t- radius (tuple, float): radius of each axis (X, Y, Z) of the ellipsoid\n\t\t- rot (np.array): rotation parameters (ZYX Euler angles)\n\t\t- trans (np.array): translation vector\n\t""""""\n\n\tassert isinstance(numpts, int), \'Integer required\'\n\n\tphi = np.random.uniform(0, np.pi, numpts)\n\ttheta = np.random.uniform(0, 2*np.pi, numpts)\n\tx = np.expand_dims(radius[0] * np.cos(theta) * np.sin(phi), axis=1)\n\ty = np.expand_dims(radius[1] * np.sin(theta) * np.sin(phi), axis=1)\n\tz = np.expand_dims(radius[2] * np.cos(phi), axis=1)\n\tpts = np.concatenate([x, y, z], axis=1)\n\n\t# NOTE: Scaling is not really required (redundant as we already perturb the radius)\n\t# Rotate and translate the ellipsoid\n\trotmat = rotz(rot[0]).dot(roty(rot[1]).dot(rotx(rot[0])))\n\tpts = rotmat.dot(pts.T).T + np.reshape(np.asarray(trans), (1,3))\n\n\t# Retain only the number of points that are required\n\tidx = np.round(np.linspace(1, pts.shape[0]-1, numpts)).astype(int)\n\tpts = pts[idx,:]\n\n\treturn pts\n\n\ndef generate_ellipsoid_voxels(radius, rot, trans, res_in=16, res_out=32, voxsize=1.):\n\t""""""Generate a voxel grid containing an ellipsoid. One low-res grid, corresponding to \n\tthe input, and it\'s respective hi-res version.\n\n\tParams:\n\t\t- radius (tuple, float): radius of each axis (X, Y, Z) of the ellipsoid\n\t\t- rot (np.array): rotation parameters (ZYX Euler angles)\n\t\t- trans (np.array): translation vector\n\t\t- res_in (int): resolution of the input voxel grid\n\t\t- res_out (int): resolution of the output voxel grid\n\t\t- voxsize (float): size of each voxel grid cell\n\t""""""\n\n\tpts = sample_on_ellipsoid(5000, radius=radius, rot=rot, trans=trans)\n\tin_voxels = points2voxels(pts, res_in, voxsize)\n\tout_voxels = points2voxels(pts, res_out, voxsize)\n\treturn pts, in_voxels, out_voxels\n\n\nif __name__ == \'__main__\':\n\n\t# Parse commandline args\n\targs = parser.parse_args()\n\n\t# Seed random number generator (for repeatability)\n\tnp.random.seed(args.randomseed)\n\n\t# Debug mode (visualize)\n\tif args.debug:\n\t\t\n\t\t# Generate points on ellipsoid\n\t\t# pts = sample_on_ellipsoid(args.numpts, radius=(1.,2.,1.), rot=(0.86,0.,0.), \\\n\t\t# \ttrans=(1.0,0.5,0.))\n\t\tpts, in_voxels, out_voxels = generate_ellipsoid_voxels(radius=(1.,2.,1.), \\\n\t\t\trot=(0.86,0.,0.), trans=(0.2,0.3,0), voxsize=0.2)\n\t\t\n\t\timport matplotlib\n\t\tfrom matplotlib import pyplot as plt\n\t\tfrom mpl_toolkits.mplot3d import axes3d\n\t\tfig, ax = plt.subplots(1, 1, subplot_kw={\'projection\': \'3d\', \'aspect\': \'equal\'})\n\t\tax.scatter(pts[:,0], pts[:,1], pts[:,2], c=\'r\')\n\t\tplt.show()\n\n\t\timport sys\n\t\tsys.exit(0)\n\n\t# Create dir to save samples, if it does not already exist\n\targs.savedir = os.path.join(args.savedir, args.expid)\n\tsavedir_pts = os.path.join(args.savedir, \'pts_\' + str(args.numpts).zfill(4))\n\tsavedir_vox_in = os.path.join(args.savedir, \'vox_\' + str(args.res_in).zfill(4))\n\tsavedir_vox_out = os.path.join(args.savedir, \'vox_\' + str(args.res_out).zfill(4))\n\tif not os.path.isdir(args.savedir):\n\t\tos.makedirs(args.savedir)\n\t\tprint(\'Created dir:\', args.savedir)\n\t\tos.makedirs(savedir_pts)\n\t\tos.makedirs(savedir_vox_in)\n\t\tos.makedirs(savedir_vox_out)\n\t\tprint(\'Created subdirs.\')\n\n\t# Generate samples\n\tfor i in trange(args.numsamples):\n\t\t# Sample a radius\n\t\trad = (np.random.uniform(args.rad_min, args.rad_max), \\\n\t\t\tnp.random.uniform(args.rad_min, args.rad_max), \\\n\t\t\tnp.random.uniform(args.rad_min, args.rad_max))\n\t\trot = (np.random.uniform(args.rot_min, args.rot_max), \\\n\t\t\tnp.random.uniform(args.rot_min, args.rot_max), \\\n\t\t\tnp.random.uniform(args.rot_min, args.rot_max))\n\t\ttrans = (np.random.uniform(args.trans_min, args.trans_max), \\\n\t\t\tnp.random.uniform(args.trans_min, args.trans_max), \\\n\t\t\tnp.random.uniform(args.trans_min, args.trans_max))\n\t\t\n\t\t# Generate ellipsoid data sample\n\t\tpts, in_voxels, out_voxels = generate_ellipsoid_voxels(radius=rad, rot=rot, \\\n\t\t\ttrans=trans, res_in=args.res_in, res_out=args.res_out, voxsize=args.voxsize)\n\t\t# Save sample\n\t\tcurfile = str(i).zfill(5)\n\t\tnp.save(os.path.join(savedir_pts, curfile), pts)\n\t\tnp.save(os.path.join(savedir_vox_in, curfile), in_voxels)\n\t\tnp.save(os.path.join(savedir_vox_out, curfile), out_voxels)\n'"
examples/SuperResolution/voxel-ModelNet/eval_MSE.py,3,"b""import argparse\nimport os\nimport torch\nimport sys\nfrom tqdm import tqdm \n\nfrom torch.utils.data import DataLoader\nfrom utils import down_sample, up_sample\n\nfrom architectures import EncoderDecoder\nimport kaolin as kal \n\nparser = argparse.ArgumentParser()\nparser.add_argument('-expid', type=str, default='MSE', help='Unique experiment identifier.')\nparser.add_argument('-device', type=str, default='cuda', help='Device to use')\nparser.add_argument('-categories', type=str,nargs='+', default=['chair'], help='list of object classes to use')\nparser.add_argument('-vis', action='store_true', help='Visualize each model while evaluating')\nparser.add_argument('-batchsize', type=int, default=16, help='Batch size.')\nargs = parser.parse_args()\n\n# Data\nvalid_set = kal.dataloader.ModelNet(root ='../../datasets/', categories = args.categories, download = True, train = False)\ndataloader_val = DataLoader(valid_set, batch_size=args.batchsize, shuffle=False, \\\n\tnum_workers=8)\n\n# Model\nmodel = EncoderDecoder()\nmodel = model.to(args.device)\n# Load saved weights\nmodel.load_state_dict(torch.load('log/{0}/best.pth'.format(args.expid)))\n\niou_epoch = 0.\niou_NN_epoch = 0.\nnum_batches = 0\n\nmodel.eval()\nwith torch.no_grad():\n\tfor data in tqdm(dataloader_val): \n\t\ttgt = data['data'].to(args.device)\n\t\tinp = down_sample(tgt)\n\n\t\t# inference \n\t\tpred = model(inp)\n\n\t\t# losses\n\t\tiou = kal.metrics.voxel.iou(pred.contiguous(), tgt)\n\t\tiou_epoch += iou\n\t\t\n\t\tNN_pred = up_sample(inp)\n\t\tiou_NN = kal.metrics.voxel.iou(NN_pred.contiguous(), tgt)\n\t\tiou_NN_epoch += iou_NN\n\n\t\tnum_batches += 1.\n\n\t\tif args.vis: \n\t\t\tfor i in range(pred.shape[0]):\n\t\t\t\tprint ('Rendering low resolution input')\n\t\t\t\tkal.visualize.show_voxel(inp[i,0], mode = 'exact', thresh = .5)\n\t\t\t\tprint ('Rendering high resolution target')\n\t\t\t\tkal.visualize.show_voxel(tgt[i], mode = 'exact', thresh = .5)\n\t\t\t\tprint ('Rendering high resolution prediction')\n\t\t\t\tkal.visualize.show_voxel(pred[i], mode = 'exact', thresh = .5)\n\t\t\t\tprint('----------------------')\n\nout_iou_NN = iou_NN_epoch.item() / float(num_batches)\nprint ('Nearest Neighbor Baseline IoU over validation set is {0}'.format(out_iou_NN))\nout_iou = iou_epoch.item() / float(num_batches)\nprint ('IoU over validation set is {0}'.format(out_iou))"""
examples/SuperResolution/voxel-ModelNet/eval_NLLL.py,3,"b""import argparse\nimport os\nimport torch\nimport sys\nfrom tqdm import tqdm \n\nfrom torch.utils.data import DataLoader\nfrom utils import down_sample, up_sample\n\nfrom architectures import EncoderDecoderForNLL\nimport kaolin as kal \n\nparser = argparse.ArgumentParser()\nparser.add_argument('-expid', type=str, default='NLLL', help='Unique experiment identifier.')\nparser.add_argument('-device', type=str, default='cuda', help='Device to use')\nparser.add_argument('-categories', type=str,nargs='+', default=['chair'], help='list of object classes to use')\nparser.add_argument('-vis', action='store_true', help='Visualize each model while evaluating')\nparser.add_argument('-batchsize', type=int, default=16, help='Batch size.')\nargs = parser.parse_args()\n\n# Data\nvalid_set = kal.dataloader.ModelNet(root ='../../datasets/', categories = args.categories, download = True, train = False)\ndataloader_val = DataLoader(valid_set, batch_size=args.batchsize, shuffle=False, \\\n\tnum_workers=8)\n\n# Model\nmodel = EncoderDecoderForNLL()\nmodel = model.to(args.device)\n# Load saved weights\nmodel.load_state_dict(torch.load('log/{0}/best.pth'.format(args.expid)))\n\n\niou_epoch = 0.\niou_NN_epoch = 0.\nnum_batches = 0\n\nmodel.eval()\nwith torch.no_grad():\n\tfor data in tqdm(dataloader_val): \n\t\ttgt = data['data'].to(args.device)\n\t\tinp = down_sample(tgt)\n\n\t\t# inference \n\t\tpred = model(inp)\n\n\t\tiou = kal.metrics.voxel.iou(pred[:,1,:,:].contiguous(), tgt)\n\t\tiou_epoch += iou\n\t\t\n\t\tNN_pred = up_sample(inp)\n\t\tiou_NN = kal.metrics.voxel.iou(NN_pred.contiguous(), tgt)\n\t\tiou_NN_epoch += iou_NN\n\n\t\tif args.vis: \n\t\t\tfor i in range(pred.shape[0]):\n\t\t\t\tprint ('Rendering low resolution input')\n\t\t\t\tkal.visualize.show_voxel(inp[i,0], mode = 'exact', thresh = .5)\n\t\t\t\tprint ('Rendering high resolution target')\n\t\t\t\tkal.visualize.show_voxel(tgt[i], mode = 'exact', thresh = .5)\n\t\t\t\tprint ('Rendering high resolution prediction')\n\t\t\t\tkal.visualize.show_voxel(pred[i,1], mode = 'exact', thresh = .5)\n\t\t\t\tprint('----------------------')\n\t\tnum_batches += 1. \n\nout_iou_NN = iou_NN_epoch.item() / float(num_batches)\nprint ('Nearest Neighbor Baseline IoU over validation set is {0}'.format(out_iou_NN))\nout_iou = iou_epoch.item() / float(num_batches)\nprint ('IoU over validation set is {0}'.format(out_iou))"""
examples/SuperResolution/voxel-ModelNet/train_MSE.py,9,"b'import argparse\nimport json\nimport numpy as np\nimport os\nimport torch\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport sys\nfrom tqdm import tqdm\n\n\nfrom utils import down_sample, up_sample\nfrom architectures import EncoderDecoder\n\nimport kaolin as kal \n""""""\nCommandline arguments\n""""""\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-expid\', type=str, default=\'MSE\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'-device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'-categories\', type=str,nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'-epochs\', type=int, default=30, help=\'Number of train epochs.\')\nparser.add_argument(\'-batchsize\', type=int, default=16, help=\'Batch size.\')\nparser.add_argument(\'-lr\', type=float, default=1e-3, help=\'Learning rate.\')\nparser.add_argument(\'-val-every\', type=int, default=5, help=\'Validation frequency (epochs).\')\nparser.add_argument(\'-print-every\', type=int, default=100, help=\'Print frequency (batches).\')\nparser.add_argument(\'-logdir\', type=str, default=\'log\', help=\'Directory to log data to.\')\nparser.add_argument(\'-save-model\', action=\'store_true\', help=\'Saves the model and a snapshot \\\n\tof the optimizer state.\')\nargs = parser.parse_args()\n\n\n\n\n""""""\nDataset\n""""""\ntrain_set = kal.dataloader.ModelNet(root =\'../../datasets/\',categories = args.categories, download = True)\ndataloader_train = DataLoader(train_set, batch_size=args.batchsize, shuffle=True, \n\tnum_workers=8)\n\nvalid_set = kal.dataloader.ModelNet(root =\'../../datasets/\',categories = args.categories, download = True, train = False)\ndataloader_val = DataLoader(valid_set, batch_size=args.batchsize, shuffle=False, \\\n\tnum_workers=8)\n\n""""""\nModel settings \n""""""\nmodel = EncoderDecoder().to(args.device)\n\nloss_fn = torch.nn.MSELoss()\n\noptimizer = optim.Adam(model.parameters(), lr=args.lr)\n\n\n# Create log directory, if it doesn\'t already exist\nargs.logdir = os.path.join(args.logdir, args.expid)\nif not os.path.isdir(args.logdir):\n\tos.makedirs(args.logdir)\n\tprint(\'Created dir:\', args.logdir)\n\n# Log all commandline args\nwith open(os.path.join(args.logdir, \'args.txt\'), \'w\') as f:\n\tjson.dump(args.__dict__, f, indent=2)\n \n\nclass Engine(object):\n\t""""""Engine that runs training and inference.\n\tArgs\n\t\t- cur_epoch (int): Current epoch.\n\t\t- print_every (int): How frequently (# batches) to print loss.\n\t\t- validate_every (int): How frequently (# epochs) to run validation.\n\t\t\n\t""""""\n\n\tdef __init__(self,  cur_epoch=0, print_every=1, validate_every=1):\n\t\tself.cur_epoch = cur_epoch\n\t\tself.train_loss = []\n\t\tself.val_loss = []\n\t\tself.bestval = 0\n\n\tdef train(self):\n\t\tloss_epoch = 0.\n\t\tnum_batches = 0\n\t\tmodel.train()\n\t\t# Train loop\n\t\tfor i, data in enumerate(tqdm(dataloader_train), 0):\n\t\t\toptimizer.zero_grad()\n\t\t\t\n\t\t\t# data creation\n\t\t\ttgt = data[\'data\'].to(args.device)\n\t\t\t\n\t\t\tinp = down_sample(tgt)\n\n\t\t\t# inference \n\t\t\tpred = model(inp)\n\n\t\t\t# losses \n\t\t\tloss = loss_fn(pred, tgt)\n\t\t\tloss.backward()\n\t\t\tloss_epoch += float(loss.item())\n\t\t\tiou = kal.metrics.voxel.iou(pred.contiguous(), tgt)\n\n\t\t\t# logging\n\t\t\tnum_batches += 1\n\t\t\tif i % args.print_every == 0:\n\t\t\t\ttqdm.write(f\'[TRAIN] Epoch {self.cur_epoch:03d}, Batch {i:03d}: Loss: {float(loss.item())}\')\n\t\t\t\ttqdm.write(\'Metric iou: {0}\'.format(iou))\n\t\t\toptimizer.step()\n\t\t\n\t\t\n\t\tloss_epoch = loss_epoch / num_batches\n\t\tself.train_loss.append(loss_epoch)\n\t\tself.cur_epoch += 1\n\n\t\t\n\t\t\n\tdef validate(self):\n\t\tmodel.eval()\n\t\twith torch.no_grad():\t\n\t\t\tiou_epoch = 0.\n\t\t\tiou_NN_epoch = 0.\n\t\t\tnum_batches = 0\n\t\t\tloss_epoch = 0.\n\n\t\t\t# Validation loop\n\t\t\tfor i, data in enumerate(tqdm(dataloader_val), 0):\n\n\t\t\t\t# data creation\n\t\t\t\ttgt = data[\'data\'].to(args.device)\n\t\t\t\tinp = down_sample(tgt)\n\n\t\t\t\t# inference \n\t\t\t\tpred = model(inp)\n\t\t\n\t\t\t\t# losses\n\t\t\t\tloss = loss_fn(pred, tgt)\n\t\t\t\tloss_epoch += float(loss.item())\n\n\t\t\t\tiou = kal.metrics.voxel.iou(pred.contiguous(), tgt)\n\t\t\t\tiou_epoch += iou\n\t\t\t\t\n\t\t\t\tNN_pred = up_sample(inp)\n\t\t\t\tiou_NN = kal.metrics.voxel.iou(NN_pred.contiguous(), tgt)\n\t\t\t\tiou_NN_epoch += iou_NN\n\n\t\t\t\t# logging\n\t\t\t\tnum_batches += 1\n\t\t\t\tif i % args.print_every == 0:\n\t\t\t\t\t\tout_iou = iou_epoch.item() / float(num_batches)\n\t\t\t\t\t\tout_iou_NN = iou_NN_epoch.item() / float(num_batches)\n\t\t\t\t\t\ttqdm.write(f\'[VAL] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou}, Iou Base: {out_iou_NN}\')\n\t\t\t\t\t\t\n\t\t\tout_iou = iou_epoch.item() / float(num_batches)\n\t\t\tout_iou_NN = iou_NN_epoch.item() / float(num_batches)\n\t\t\ttqdm.write(f\'[VAL Total] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou}, Iou Base: {out_iou_NN}\')\n\n\t\t\tloss_epoch = loss_epoch / num_batches\n\t\t\tself.val_loss.append(out_iou)\n\n\tdef save(self):\n\n\t\tsave_best = False\n\t\tif self.val_loss[-1] >= self.bestval:\n\t\t\tself.bestval = self.val_loss[-1]\n\t\t\tsave_best = True\n\t\t\n\t\t# Create a dictionary of all data to save\n\t\tlog_table = {\n\t\t\t\'epoch\': self.cur_epoch,\n\t\t\t\'bestval\': np.min(np.asarray(self.val_loss)),\n\t\t\t\'train_loss\': self.train_loss,\n\t\t\t\'val_loss\': self.val_loss,\n\t\t\t\'train_metrics\': [\'NLLLoss\', \'iou\'],\n\t\t\t\'val_metrics\': [\'NLLLoss\', \'iou\', \'iou_NN\'],\n\t\t}\n\n\t\t# Save the recent model/optimizer states\n\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, \'recent.pth\'))\n\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, \'recent_optim.pth\'))\n\t\t# Log other data corresponding to the recent model\n\t\twith open(os.path.join(args.logdir, \'recent.log\'), \'w\') as f:\n\t\t\tf.write(json.dumps(log_table))\n\n\t\ttqdm.write(\'====== Saved recent model ======>\')\n\t\t\n\t\tif save_best:\n\t\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, \'best.pth\'))\n\t\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, \'best_optim.pth\'))\n\t\t\t# Log other data corresponding to the recent model\n\t\t\twith open(os.path.join(args.logdir, \'best.log\'), \'w\') as f:\n\t\t\t\tf.write(json.dumps(log_table))\n\t\t\ttqdm.write(\'====== Overwrote best model ======>\')\n\t\t\t\n\t\ntrainer = Engine()\n\nfor epoch in range(args.epochs): \n\ttrainer.train()\n\ttrainer.validate()\n\ttrainer.save()'"
examples/SuperResolution/voxel-ModelNet/train_NLLL.py,10,"b'import argparse\nimport json\nimport numpy as np\nimport os\nimport torch\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport sys\nfrom tqdm import tqdm\n\nfrom utils import down_sample, up_sample\nfrom architectures import EncoderDecoderForNLL\n\nimport kaolin as kal \n""""""\nCommandline arguments\n""""""\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-expid\', type=str, default=\'NLLL\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'-device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'-categories\', type=str,nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use\')\nparser.add_argument(\'-epochs\', type=int, default=30, help=\'Number of train epochs.\')\nparser.add_argument(\'-batchsize\', type=int, default=16, help=\'Batch size.\')\nparser.add_argument(\'-lr\', type=float, default=1e-3, help=\'Learning rate.\')\nparser.add_argument(\'-val-every\', type=int, default=5, help=\'Validation frequency (epochs).\')\nparser.add_argument(\'-print-every\', type=int, default=100, help=\'Print frequency (batches).\')\nparser.add_argument(\'-logdir\', type=str, default=\'log\', help=\'Directory to log data to.\')\nparser.add_argument(\'-save-model\', action=\'store_true\', help=\'Saves the model and a snapshot \\\n\tof the optimizer state.\')\nargs = parser.parse_args()\n\n\n\n\n""""""\nDataset\n""""""\ntrain_set = kal.dataloader.ModelNet(root =\'../../datasets/\',categories =args.categories,\\\n download = True, train = True)\ndataloader_train = DataLoader(train_set, batch_size=args.batchsize, shuffle=True, \n\tnum_workers=8)\n\nvalid_set = kal.dataloader.ModelNet(root =\'../../datasets/\',categories =args.categories, \\\n download = True, train = False)\ndataloader_val = DataLoader(valid_set, batch_size=args.batchsize, shuffle=False, \\\n\tnum_workers=8)\n\n""""""\nModel settings \n""""""\nmodel = EncoderDecoderForNLL().to(args.device)\n\nclass_weights = torch.from_numpy(np.asarray([0.0586, 0.9414])).float().to(args.device)\nloss_fn = torch.nn.NLLLoss(weight=class_weights)\n\n\noptimizer = optim.Adam(model.parameters(), lr=args.lr)\n\n\n# Create log directory, if it doesn\'t already exist\nargs.logdir = os.path.join(args.logdir, args.expid)\nif not os.path.isdir(args.logdir):\n\tos.makedirs(args.logdir)\n\tprint(\'Created dir:\', args.logdir)\n\n# Log all commandline args\nwith open(os.path.join(args.logdir, \'args.txt\'), \'w\') as f:\n\tjson.dump(args.__dict__, f, indent=2)\n\n\nclass Engine(object):\n\t""""""Engine that runs training and inference.\n\tArgs\n\t\t- cur_epoch (int): Current epoch.\n\t\t- print_every (int): How frequently (# batches) to print loss.\n\t\t- validate_every (int): How frequently (# epochs) to run validation.\n\t\t\n\t""""""\n\n\tdef __init__(self,  cur_epoch=0, print_every=1, validate_every=1):\n\t\tself.cur_epoch = cur_epoch\n\t\tself.train_loss = []\n\t\tself.val_loss = []\n\t\tself.bestval = 0\n\n\tdef train(self):\n\t\tloss_epoch = 0.\n\t\tnum_batches = 0\n\t\tdiff = 0 \n\t\tmodel.train()\n\t\t# Train loop\n\t\tfor i, data in enumerate(tqdm(dataloader_train), 0):\n\t\t\toptimizer.zero_grad()\n\t\t\t\n\t\t\t# data creation\n\t\t\ttgt = data[\'data\'].to(args.device)\n\t\t\t\n\t\t\tinp = down_sample(tgt)\n\n\t\t\t# inference \n\t\t\tpred = model(inp)\n\n\t\t\t# losses \n\t\t\tloss = loss_fn(pred, tgt.long())\n\t\t\tloss.backward()\n\t\t\tloss_epoch += float(loss.item())\n\t\t\tiou = kal.metrics.voxel.iou(pred[:,1,:,:].contiguous(), tgt)\n\n\t\t\t# logging\n\t\t\tnum_batches += 1\n\t\t\tif i % args.print_every == 0:\n\t\t\t\ttqdm.write(f\'[TRAIN] Epoch {self.cur_epoch:03d}, Batch {i:03d}: Loss: {float(loss.item())}\')\n\t\t\t\ttqdm.write(\'Metric iou: {0}\'.format(iou))\n\t\t\toptimizer.step()\n\t\t\n\t\t\n\t\tloss_epoch = loss_epoch / num_batches\n\t\tself.train_loss.append(loss_epoch)\n\t\tself.cur_epoch += 1\n\n\t\t\n\t\t\n\tdef validate(self):\n\t\tmodel.eval()\n\t\twith torch.no_grad():\t\n\t\t\tiou_epoch = 0.\n\t\t\tiou_NN_epoch = 0.\n\t\t\tnum_batches = 0\n\t\t\tloss_epoch = 0.\n\n\t\t\t# Validation loop\n\t\t\tfor i, data in enumerate(tqdm(dataloader_val), 0):\n\n\t\t\t\t# data creation\n\t\t\t\ttgt = data[\'data\'].to(args.device)\n\t\t\t\tinp = down_sample(tgt)\n\n\t\t\t\t# inference \n\t\t\t\tpred = model(inp)\n\t\t\n\t\t\t\t# losses\n\t\t\t\tloss = loss_fn(pred, tgt.long())\n\t\t\t\tloss_epoch += float(loss.item())\n\n\t\t\t\tiou = kal.metrics.voxel.iou(pred[:,1,:,:].contiguous(), tgt)\n\t\t\t\tiou_epoch += iou\n\t\t\t\t\n\t\t\t\tNN_pred = up_sample(inp)\n\t\t\t\tiou_NN = kal.metrics.voxel.iou(NN_pred.contiguous(), tgt)\n\t\t\t\tiou_NN_epoch += iou_NN\n\n\t\t\t\t# logging\n\t\t\t\tnum_batches += 1\n\t\t\t\tif i % args.print_every == 0:\n\t\t\t\t\t\tout_iou = iou_epoch.item() / float(num_batches)\n\t\t\t\t\t\tout_iou_NN = iou_NN_epoch.item() / float(num_batches)\n\t\t\t\t\t\ttqdm.write(f\'[VAL] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou}, Iou Base: {out_iou_NN}\')\n\t\t\t\t\t\t\n\t\t\tout_iou = iou_epoch.item() / float(num_batches)\n\t\t\tout_iou_NN = iou_NN_epoch.item() / float(num_batches)\n\t\t\ttqdm.write(f\'[VAL Total] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou}, Iou Base: {out_iou_NN}\')\n\n\t\t\tloss_epoch = loss_epoch / num_batches\n\t\t\tself.val_loss.append(out_iou)\n\n\tdef save(self):\n\n\t\tsave_best = False\n\t\tif self.val_loss[-1] >= self.bestval:\n\t\t\tself.bestval = self.val_loss[-1]\n\t\t\tsave_best = True\n\t\t\n\t\t# Create a dictionary of all data to save\n\t\tlog_table = {\n\t\t\t\'epoch\': self.cur_epoch,\n\t\t\t\'bestval\': np.min(np.asarray(self.val_loss)),\n\t\t\t\'train_loss\': self.train_loss,\n\t\t\t\'val_loss\': self.val_loss,\n\t\t\t\'train_metrics\': [\'NLLLoss\', \'iou\'],\n\t\t\t\'val_metrics\': [\'NLLLoss\', \'iou\', \'iou_NN\'],\n\t\t}\n\n\t\t# Save the recent model/optimizer states\n\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, \'recent.pth\'))\n\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, \'recent_optim.pth\'))\n\t\t# Log other data corresponding to the recent model\n\t\twith open(os.path.join(args.logdir, \'recent.log\'), \'w\') as f:\n\t\t\tf.write(json.dumps(log_table))\n\n\t\ttqdm.write(\'====== Saved recent model ======>\')\n\t\t\n\t\tif save_best:\n\t\t\ttorch.save(model.state_dict(), os.path.join(args.logdir, \'best.pth\'))\n\t\t\ttorch.save(optimizer.state_dict(), os.path.join(args.logdir, \'best_optim.pth\'))\n\t\t\t# Log other data corresponding to the recent model\n\t\t\twith open(os.path.join(args.logdir, \'best.log\'), \'w\') as f:\n\t\t\t\tf.write(json.dumps(log_table))\n\t\t\ttqdm.write(\'====== Overwrote best model ======>\')\n\t\t\t\n\t\ntrainer = Engine()\n\nfor epoch in range(args.epochs): \n\ttrainer.train()\n\ttrainer.validate()\n\ttrainer.save()'"
examples/SuperResolution/voxel-ModelNet/utils.py,2,"b'import torch\n\nimport kaolin as kal \n\ndef down_sample(tgt): \n\tinp = []\n\tfor t in tgt : \n\t\tlow_res_inp = kal.rep.voxel.scale_down(t, scale = [2, 2, 2])\n\t\tlow_res_inp = kal.rep.voxel.threshold(low_res_inp, .1)\n\t\tinp.append(low_res_inp.unsqueeze(0))\n\tinp = torch.cat(inp, dim = 0 )\n\tinp = inp.unsqueeze(1)\n\treturn inp\n\ndef up_sample(inp): \n\n\tinp = inp[:,0]\n\tNN_pred = []\n\tfor voxel in inp: \n\t\tNN_pred.append(kal.rep.voxel.scale_up(voxel, dim = 30))\n\tNN_pred = torch.stack(NN_pred)\n\treturn NN_pred'"
examples/SuperResolution/voxel-ShapeNet/architectures.py,4,"b'""""""\nNetwork architecture definitions\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass EncoderDecoder_32_128(nn.Module):\n    def __init__(self):\n        super(EncoderDecoder_32_128, self).__init__()\n\n        self.encoder = nn.Sequential(\n            nn.Conv3d(1, 16, 3, stride=2, padding=0),\n            nn.BatchNorm3d(16),\n            nn.ReLU(),\n            nn.Conv3d(16, 32, 3, stride=2, padding=1),\n            nn.BatchNorm3d(32),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose3d(32, 16, 3, stride=2, padding=0),\n            nn.BatchNorm3d(16),\n            nn.ReLU(),\n            nn.ConvTranspose3d(16, 8, 3, stride=2, padding=0),\n            nn.BatchNorm3d(8),\n            nn.ReLU(),\n            nn.ConvTranspose3d(8, 4, 3, stride=2, padding=0),\n            nn.BatchNorm3d(4),\n            nn.ReLU(),\n            nn.ConvTranspose3d(4, 1, 3, stride=2, padding=0),\n        )\n\n    def forward(self, x):\n\n        x = self.encoder(x)\n        x = self.decoder(x)\n\n        return torch.sigmoid(x)[:, 0, :128, :128, :128]\n\n\nclass EncoderDecoderForNLL_32_128(nn.Module):\n    def __init__(self):\n        super(EncoderDecoderForNLL_32_128, self).__init__()\n\n        self.encoder = nn.Sequential(\n            nn.Conv3d(1, 16, 3, stride=2, padding=1),\n            nn.BatchNorm3d(16),\n            nn.ReLU(),\n            nn.Conv3d(16, 32, 3, stride=2, padding=1),\n            nn.BatchNorm3d(32),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose3d(32, 16, 3, stride=2, padding=0),\n            nn.BatchNorm3d(16),\n            nn.ReLU(),\n            nn.ConvTranspose3d(16, 8, 3, stride=2, padding=0),\n            nn.BatchNorm3d(8),\n            nn.ReLU(),\n            nn.ConvTranspose3d(8, 4, 3, stride=2, padding=0),\n            nn.BatchNorm3d(4),\n            nn.ReLU(),\n            nn.ConvTranspose3d(4, 2, 3, stride=2, padding=0),\n        )\n        self.log_softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)[:, :, :128, :128, :128]\n\n        return torch.exp(self.log_softmax(x))\n'"
examples/SuperResolution/voxel-ShapeNet/eval.py,4,"b""import argparse\nimport os\nimport torch\nimport sys\nfrom tqdm import tqdm\n\nfrom torch.utils.data import DataLoader\n\nfrom architectures import EncoderDecoder_32_128, EncoderDecoderForNLL_32_128\nfrom utils import up_sample\nimport kaolin as kal\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--shapenet-root', type=str, required=True, help='Root directory of the ShapeNet dataset.')\nparser.add_argument('--cache-dir', type=str, default='cache', help='Directory where intermediate representations will be stored.')\nparser.add_argument('--expid', type=str, default='superres', help='Unique experiment identifier.')\nparser.add_argument('--device', type=str, default='cuda', help='Device to use.')\nparser.add_argument('--categories', type=str, nargs='+', default=['chair'], help='list of object classes to use.')\nparser.add_argument('--no-vis', action='store_true', help='Disable visualization of each model while evaluating.')\nparser.add_argument('--batchsize', type=int, default=16, help='Batch size.')\nargs = parser.parse_args()\n\n\ndevice = torch.device(args.device)\n\n# Dataset Setup\nvalid_set = kal.datasets.ShapeNet_Voxels(root=args.shapenet_root, cache_dir=args.cache_dir,\n                                         categories=args.categories, train=False, resolutions=[128, 32],\n                                         split=.97)\ndataloader_val = DataLoader(valid_set, batch_size=args.batchsize, shuffle=False, num_workers=8)\n\n\n# Model\nmodel = EncoderDecoderForNLL_32_128()\nmodel = model.to(device)\n\n# Load saved weights\nmodel.load_state_dict(torch.load(f'log/{args.expid}/best.pth'))\n\niou_epoch = 0.\niou_NN_epoch = 0.\nnum_batches = 0\n\n\nmodel.eval()\nwith torch.no_grad():\n    for sample in tqdm(dataloader_val):\n        data = sample['data']\n        tgt = data['128'].to(device)\n        inp = data['32'].to(device)\n\n        # inference\n        pred = model(inp.unsqueeze(1))[:, 1, :, :]\n\n        iou = kal.metrics.voxel.iou(pred.contiguous(), tgt)\n        iou_epoch += iou\n\n        NN_pred = up_sample(inp)\n        iou_NN = kal.metrics.voxel.iou(NN_pred.contiguous(), tgt)\n        iou_NN_epoch += iou_NN\n\n        if not args.no_vis:\n            for i in range(inp.shape[0]):\n                print('Rendering low resolution input')\n                kal.visualize.show_voxelgrid(inp[i], mode='exact', thresh=.5)\n                print('Rendering high resolution target')\n                kal.visualize.show_voxelgrid(tgt[i], mode='exact', thresh=.5)\n                print('Rendering high resolution prediction')\n                kal.visualize.show_voxelgrid(pred[i], mode='exact', thresh=.5)\n                print('----------------------')\n        num_batches += 1.\nout_iou_NN = iou_NN_epoch / float(num_batches)\nprint('Nearest Neighbor Baseline IoU over validation set is {0}'.format(out_iou_NN))\nout_iou = iou_epoch.item() / float(num_batches)\nprint('IoU over validation set is {0}'.format(out_iou))\n"""
examples/SuperResolution/voxel-ShapeNet/train.py,13,"b'import argparse\nimport json\nimport numpy as np\nimport os\nimport torch\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport sys\nfrom tqdm import tqdm\n\nfrom utils import up_sample\nfrom architectures import EncoderDecoder_32_128, EncoderDecoderForNLL_32_128\n\nimport kaolin as kal\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--shapenet-root\', type=str, required=True, help=\'Root directory of the ShapeNet dataset.\')\nparser.add_argument(\'--cache-dir\', type=str, default=\'cache\', help=\'Directory where intermediate representations will be stored.\')\nparser.add_argument(\'--expid\', type=str, default=\'superres\', help=\'Unique experiment identifier.\')\nparser.add_argument(\'--device\', type=str, default=\'cuda\', help=\'Device to use\')\nparser.add_argument(\'--categories\', type=str, nargs=\'+\', default=[\'chair\'], help=\'list of object classes to use.\')\nparser.add_argument(\'--epochs\', type=int, default=30, help=\'Number of train epochs.\')\nparser.add_argument(\'--batchsize\', type=int, default=16, help=\'Batch size.\')\nparser.add_argument(\'-lr\', \'--learning-rate\', type=float, default=1e-3, help=\'Learning rate.\')\nparser.add_argument(\'--val-every\', type=int, default=5, help=\'Validation frequency (epochs).\')\nparser.add_argument(\'--print-every\', type=int, default=20, help=\'Print frequency (batches).\')\nparser.add_argument(\'--logdir\', type=str, default=\'log\', help=\'Directory to log data to.\')\nparser.add_argument(\'--resume\', choices=[\'none\', \'best\', \'recent\'], default=\'none\',\n                    help=\'Choose which weights to resume training from (none to start from random initialization.)\')\nargs = parser.parse_args()\n\n\ndevice = torch.device(args.device)\n\n# Dataset Setup\ntrain_set = kal.datasets.ShapeNet_Voxels(root=args.shapenet_root, cache_dir=args.cache_dir,\n                                         categories=args.categories, train=True, resolutions=[128, 32],\n                                         split=.97)\ndataloader_train = DataLoader(train_set, batch_size=args.batchsize, shuffle=True, num_workers=8)\n\nvalid_set = kal.datasets.ShapeNet_Voxels(root=args.shapenet_root, cache_dir=args.cache_dir,\n                                         categories=args.categories, train=False, resolutions=[128, 32],\n                                         split=.97)\ndataloader_val = DataLoader(valid_set, batch_size=args.batchsize, shuffle=False, num_workers=8)\n\n\n# Model settings\nmodel = EncoderDecoderForNLL_32_128().to(args.device)\nclass_weights = torch.tensor([0.0283, 1 - 0.0283], dtype=torch.float, device=device)\nloss_fn = torch.nn.NLLLoss(weight=class_weights)\n\noptimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n\n\n# Create log directory, if it doesn\'t already exist\nlogdir = os.path.join(args.logdir, args.expid)\nif not os.path.isdir(logdir):\n    os.makedirs(logdir)\n    print(\'Created dir:\', logdir)\n\n# Log all commandline args\nwith open(os.path.join(logdir, \'args.txt\'), \'w\') as f:\n    json.dump(args.__dict__, f, indent=2)\n\n\nclass Engine(object):\n    """"""Engine that runs training and inference.\n    Args\n        print_every (int): How frequently (# batches) to print loss.\n        resume_name (str): Prefix of weights from which to resume training. If \'none\',\n            no weights are loaded.\n    """"""\n\n    def __init__(self, print_every=1, resume_name=\'none\'):\n        self.cur_epoch = 0\n        self.train_loss = []\n        self.val_loss = []\n        self.bestval = 0\n        self.print_every = print_every\n\n        if resume_name != \'none\':\n            self.load(resume_name)\n\n    def train(self):\n        loss_epoch = 0.\n        num_batches = 0\n        diff = 0\n        model.train()\n        # Train loop\n        for i, sample in enumerate(tqdm(dataloader_train), 0):\n            data = sample[\'data\']\n            optimizer.zero_grad()\n\n            tgt = data[\'128\'].to(device)\n            inp = data[\'32\'].to(device)\n\n            # inference\n            pred = model(inp.unsqueeze(1))\n\n            # losses\n            tgt = tgt.long()\n            loss = loss_fn(pred, tgt)\n            loss.backward()\n            loss_epoch += float(loss.item())\n            pred = pred[:, 1, :, :]\n            iou = kal.metrics.voxel.iou(pred.contiguous(), tgt)\n\n            # logging\n            num_batches += 1\n            if i % args.print_every == 0:\n                tqdm.write(f\'[TRAIN] Epoch {self.cur_epoch:03d}, Batch {i:03d}: Loss: {float(loss.item())}\')\n                tqdm.write(\'Metric iou: {0}\'.format(iou))\n            optimizer.step()\n\n        loss_epoch = loss_epoch / num_batches\n        self.train_loss.append(loss_epoch)\n        self.cur_epoch += 1\n\n    def validate(self):\n        model.eval()\n        with torch.no_grad():\n            iou_epoch = 0.\n            iou_NN_epoch = 0.\n            num_batches = 0\n            loss_epoch = 0.\n\n            # Validation loop\n            for i, sample in enumerate(tqdm(dataloader_val), 0):\n                data = sample[\'data\']\n                # data creation\n                tgt = data[\'128\'].to(device)\n                inp = data[\'32\'].to(device)\n\n                # inference\n                pred = model(inp.unsqueeze(1))\n\n                # losses\n                tgt = tgt.long()\n                loss = loss_fn(pred, tgt)\n                loss_epoch += float(loss.item())\n\n                pred = pred[:, 1, :, :]\n                iou = kal.metrics.voxel.iou(pred.contiguous(), tgt)\n                iou_epoch += iou\n\n                NN_pred = up_sample(inp)\n                iou_NN = kal.metrics.voxel.iou(NN_pred.contiguous(), tgt)\n                iou_NN_epoch += iou_NN\n\n                # logging\n                num_batches += 1\n                if i % args.print_every == 0:\n                    out_iou = iou_epoch.item() / float(num_batches)\n                    out_iou_NN = iou_NN_epoch.item() / float(num_batches)\n                    tqdm.write(f\'[VAL] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou}, Iou Base: {out_iou_NN}\')\n\n            out_iou = iou_epoch.item() / float(num_batches)\n            out_iou_NN = iou_NN_epoch.item() / float(num_batches)\n            tqdm.write(f\'[VAL Total] Epoch {self.cur_epoch:03d}, Batch {i:03d}: IoU: {out_iou}, Iou Base: {out_iou_NN}\')\n\n            loss_epoch = loss_epoch / num_batches\n            self.val_loss.append(out_iou)\n\n    def load(self, resume_name):\n        model_path = os.path.join(logdir, f\'{resume_name}.pth\')\n        optim_path = os.path.join(logdir, f\'{resume_name}_optim.pth\')\n        assert os.path.exists(model_path), f\'Model weights not found at ""{model_path}""\'\n        assert os.path.exists(optim_path), f\'Optim weights not found at ""{optim_path}""\'\n        model.load_state_dict(torch.load(model_path))\n        optimizer.load_state_dict(torch.load(optim_path))\n\n        # Read data corresponding to the loaded model\n        with open(os.path.join(logdir, f\'{resume_name}.log\'), \'r\') as f:\n            run_data = json.load(f)\n        self.cur_epoch = run_data[\'epoch\']\n\n        tqdm.write(f\'====== Loaded {resume_name} model ======>\')\n\n    def save(self):\n        save_best = False\n        if self.val_loss[-1] >= self.bestval:\n            self.bestval = self.val_loss[-1]\n            save_best = True\n\n        # Create a dictionary of all data to save\n        log_table = {\n            \'epoch\': self.cur_epoch,\n            \'bestval\': np.min(np.asarray(self.val_loss)),\n            \'train_loss\': self.train_loss,\n            \'val_loss\': self.val_loss,\n            \'train_metrics\': [\'NLLLoss\', \'iou\'],\n            \'val_metrics\': [\'NLLLoss\', \'iou\', \'iou_NN\'],\n        }\n\n        # Save the recent model/optimizer states\n        torch.save(model.state_dict(), os.path.join(logdir, \'recent.pth\'))\n        torch.save(optimizer.state_dict(), os.path.join(logdir, \'recent_optim.pth\'))\n        # Log other data corresponding to the recent model\n        with open(os.path.join(logdir, \'recent.log\'), \'w\') as f:\n            f.write(json.dumps(log_table))\n\n        tqdm.write(\'====== Saved recent model ======>\')\n\n        if save_best:\n            torch.save(model.state_dict(), os.path.join(logdir, \'best.pth\'))\n            torch.save(optimizer.state_dict(), os.path.join(logdir, \'best_optim.pth\'))\n            # Log other data corresponding to the recent model\n            with open(os.path.join(logdir, \'best.log\'), \'w\') as f:\n                f.write(json.dumps(log_table))\n            tqdm.write(\'====== Overwrote best model ======>\')\n\n\ntrainer = Engine(print_every=args.print_every, resume_name=args.resume)\n\nfor epoch in range(args.epochs):\n    trainer.train()\n    trainer.validate()\n    trainer.save()\n'"
examples/SuperResolution/voxel-ShapeNet/utils.py,1,b'import torch\n\nimport kaolin.transforms as tfs\n\n\ndef up_sample(inp):\n    NN_pred = []\n    upsample_128 = tfs.UpsampleVoxelGrid(dim=128)\n    for voxel in inp:\n        NN_pred.append(upsample_128(voxel))\n    NN_pred = torch.stack(NN_pred)\n    return NN_pred\n'
examples/renderers/DIB-R/example.py,4,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom PIL import Image\nfrom kaolin.graphics import DIBRenderer as Renderer\nfrom kaolin.graphics.dib_renderer.utils.sphericalcoord import get_spherical_coords_x\nfrom kaolin.rep import TriangleMesh\nimport argparse\nimport imageio\nimport numpy as np\nimport os\nimport torch\nimport tqdm\n\nROOT_DIR = os.path.abspath(os.path.dirname(__file__))\n\n###########################\n# Settings\n###########################\n\nCAMERA_DISTANCE = 2\nCAMERA_ELEVATION = 30\nMESH_SIZE = 5\nHEIGHT = 256\nWIDTH = 256\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(description=\'Kaolin DIB-R Example\')\n\n    parser.add_argument(\'--mesh\', type=str, default=os.path.join(ROOT_DIR, \'banana.obj\'),\n                        help=\'Path to the mesh OBJ file\')\n    parser.add_argument(\'--use_texture\', action=\'store_true\',\n                        help=\'Whether to render a textured mesh\')\n    parser.add_argument(\'--texture\', type=str, default=os.path.join(ROOT_DIR, \'texture.png\'),\n                        help=\'Specifies path to the texture to be used\')\n    parser.add_argument(\'--output_path\', type=str, default=os.path.join(ROOT_DIR, \'results\'),\n                        help=\'Path to the output directory\')\n\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_arguments()\n\n    ###########################\n    # Load mesh\n    ###########################\n\n    mesh = TriangleMesh.from_obj(args.mesh)\n    vertices = mesh.vertices.cuda()\n    faces = mesh.faces.int().cuda()\n\n    # Expand such that batch size = 1\n\n    vertices = vertices.unsqueeze(0)\n\n    ###########################\n    # Normalize mesh position\n    ###########################\n\n    vertices_max = vertices.max()\n    vertices_min = vertices.min()\n    vertices_middle = (vertices_max + vertices_min) / 2.\n    vertices = (vertices - vertices_middle) * MESH_SIZE\n\n    ###########################\n    # Generate vertex color\n    ###########################\n\n    if not args.use_texture:\n        vert_min = torch.min(vertices, dim=1, keepdims=True)[0]\n        vert_max = torch.max(vertices, dim=1, keepdims=True)[0]\n        colors = (vertices - vert_min) / (vert_max - vert_min)\n\n    ###########################\n    # Generate texture mapping\n    ###########################\n\n    if args.use_texture:\n        uv = get_spherical_coords_x(vertices[0].cpu().numpy())\n        uv = torch.from_numpy(uv).cuda()\n\n        # Expand such that batch size = 1\n        uv = uv.unsqueeze(0)\n\n    ###########################\n    # Load texture\n    ###########################\n\n    if args.use_texture:\n        # Load image as numpy array\n        texture = np.array(Image.open(args.texture))\n\n        # Convert numpy array to PyTorch tensor\n        texture = torch.from_numpy(texture).cuda()\n\n        # Convert from [0, 255] to [0, 1]\n        texture = texture.float() / 255.0\n\n        # Convert to NxCxHxW layout\n        texture = texture.permute(2, 0, 1).unsqueeze(0)\n\n    ###########################\n    # Render\n    ###########################\n\n    if args.use_texture:\n        renderer_mode = \'Lambertian\'\n\n    else:\n        renderer_mode = \'VertexColor\'\n\n    renderer = Renderer(HEIGHT, WIDTH, mode=renderer_mode)\n\n    loop = tqdm.tqdm(list(range(0, 360, 4)))\n    loop.set_description(\'Drawing\')\n\n    os.makedirs(args.output_path, exist_ok=True)\n    writer = imageio.get_writer(os.path.join(args.output_path, \'example.gif\'), mode=\'I\')\n    for azimuth in loop:\n        renderer.set_look_at_parameters([90 - azimuth],\n                                        [CAMERA_ELEVATION],\n                                        [CAMERA_DISTANCE])\n\n        if args.use_texture:\n            predictions, _, _ = renderer(points=[vertices, faces.long()],\n                                         uv_bxpx2=uv,\n                                         texture_bx3xthxtw=texture)\n\n        else:\n            predictions, _, _ = renderer(points=[vertices, faces.long()],\n                                         colors_bxpx3=colors)\n\n        image = predictions.detach().cpu().numpy()[0]\n        writer.append_data((image * 255).astype(np.uint8))\n\n    writer.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/renderers/NMR/example1.py,2,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Neural Mesh Renderer\n\n# MIT License\n\n# Copyright (c) 2017 Hiroharu Kato\n# Copyright (c) 2018 Nikos Kolotouros\n# A PyTorch implementation of Neural 3D Mesh Renderer (https://github.com/hiroharu-kato/neural_renderer)\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport argparse\nimport os\nimport numpy as np\nimport torch\nimport tqdm\nimport imageio\n\nfrom kaolin.graphics import NeuralMeshRenderer as Renderer\nfrom kaolin.graphics.nmr.util import get_points_from_angles\nfrom kaolin.rep import TriangleMesh\nfrom util import normalize_vertices\n\nROOT_DIR = os.path.abspath(os.path.dirname(__file__))\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(description=\'NMR Example 1: Render mesh\')\n\n    parser.add_argument(\'--mesh\', type=str, default=os.path.join(ROOT_DIR, \'rocket.obj\'),\n                        help=\'Path to the mesh OBJ file\')\n    parser.add_argument(\'--output_path\', type=str, default=os.path.join(ROOT_DIR, \'results\'),\n                        help=\'Path to the output directory\')\n    parser.add_argument(\'--camera_distance\', type=float, default=2.732,\n                        help=\'Distance from camera to object center\')\n    parser.add_argument(\'--elevation\', type=float, default=30,\n                        help=\'Camera elevation\')\n    parser.add_argument(\'--texture_size\', type=int, default=2,\n                        help=\'Dimension of texture\')\n\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_arguments()\n\n    ###########################\n    # Load mesh\n    ###########################\n\n    mesh = TriangleMesh.from_obj(args.mesh)\n    mesh.cuda()\n    # Normalize into unit cube, and expand such that batch size = 1\n    vertices = normalize_vertices(mesh.vertices).unsqueeze(0)\n    faces = mesh.faces.unsqueeze(0)\n\n    ###########################\n    # Generate texture (NMR format)\n    ###########################\n\n    textures = torch.ones(\n        1, faces.shape[1], args.texture_size, args.texture_size, args.texture_size,\n        3, dtype=torch.float32,\n        device=\'cuda\'\n    )\n\n    ###########################\n    # Render\n    ###########################\n\n    renderer = Renderer(camera_mode=\'look_at\')\n\n    loop = tqdm.tqdm(range(0, 360, 4))\n    loop.set_description(\'Drawing\')\n\n    os.makedirs(args.output_path, exist_ok=True)\n    writer = imageio.get_writer(os.path.join(\n        args.output_path, \'example1.gif\'), mode=\'I\')\n    for azimuth in loop:\n        renderer.eye = get_points_from_angles(\n            args.camera_distance, args.elevation, azimuth)\n\n        images, _, _ = renderer(vertices, faces, textures)\n\n        image = images.detach()[0].permute(1, 2, 0).cpu().numpy()  # [image_size, image_size, RGB]\n        writer.append_data((255 * image).astype(np.uint8))\n\n    writer.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/renderers/NMR/example2.py,6,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Neural Mesh Renderer\n\n# MIT License\n\n# Copyright (c) 2017 Hiroharu Kato\n# Copyright (c) 2018 Nikos Kolotouros\n# A PyTorch implementation of Neural 3D Mesh Renderer (https://github.com/hiroharu-kato/neural_renderer)\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import division\n\nfrom kaolin.graphics import NeuralMeshRenderer as Renderer\nfrom kaolin.graphics.nmr.util import get_points_from_angles\nfrom kaolin.rep import TriangleMesh\nfrom skimage.io import imread\nfrom util import normalize_vertices\nimport argparse\nimport imageio\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport tqdm\n\nROOT_DIR = os.path.abspath(os.path.dirname(__file__))\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(\n        description=\'NMR Example 2: Optimize vertices\')\n\n    parser.add_argument(\'--mesh\', type=str, default=os.path.join(ROOT_DIR, \'rocket.obj\'),\n                        help=\'Path to the mesh OBJ file\')\n    parser.add_argument(\'--image\', type=str, default=os.path.join(ROOT_DIR, \'example2_ref.png\'),\n                        help=\'Path to the target image file to optimize to\')\n    parser.add_argument(\'--output_path\', type=str, default=os.path.join(ROOT_DIR, \'results\'),\n                        help=\'Path to the output directory\')\n    parser.add_argument(\'--epochs\', type=int, default=300,\n                        help=\'Number of epochs to optimize\')\n    parser.add_argument(\'--camera_distance\', type=float, default=2.732,\n                        help=\'Distance from camera to object center\')\n    parser.add_argument(\'--elevation\', type=float, default=0,\n                        help=\'Camera elevation\')\n    parser.add_argument(\'--azimuth\', type=float, default=90,\n                        help=\'Camera azimuth\')\n    parser.add_argument(\'--texture_size\', type=int, default=2,\n                        help=\'Dimension of texture\')\n\n    return parser.parse_args()\n\n\nclass Model(nn.Module):\n\n    def __init__(self, mesh_path, image_path, args):\n        super(Model, self).__init__()\n\n        self.args = args\n\n        ###########################\n        # Load mesh\n        ###########################\n\n        mesh = TriangleMesh.from_obj(mesh_path)\n        mesh.cuda()\n        # Normalize into unit cube, and expand such that batch size = 1\n        vertices = normalize_vertices(mesh.vertices).unsqueeze(0)\n        faces = mesh.faces.unsqueeze(0)\n\n        self.vertices = nn.Parameter(vertices)\n        self.register_buffer(\'faces\', faces)\n\n        ###########################\n        # Generate texture (NMR format)\n        ###########################\n\n        textures = torch.ones(\n            1, self.faces.shape[1], self.args.texture_size, self.args.texture_size, self.args.texture_size,\n            3, dtype=torch.float32,\n            device=\'cuda\'\n        )\n        self.register_buffer(\'textures\', textures)\n\n        ###########################\n        # Load target image\n        ###########################\n\n        image_ref = torch.from_numpy(imread(image_path).astype(\n            np.float32).mean(-1) / 255.)[None, ::]\n        self.register_buffer(\'image_ref\', image_ref)\n\n        ###########################\n        # Setup renderer\n        ###########################\n\n        renderer = Renderer(camera_mode=\'look_at\')\n        self.renderer = renderer\n\n    def forward(self):\n        ###########################\n        # Render\n        ###########################\n\n        self.renderer.eye = get_points_from_angles(\n            self.args.camera_distance, self.args.elevation, self.args.azimuth)\n        image = self.renderer(self.vertices, self.faces, mode=\'silhouettes\')\n        loss = torch.sum((image - self.image_ref[None, :, :]) ** 2)\n\n        return loss\n\n\ndef main():\n    args = parse_arguments()\n\n    ###########################\n    # Setup model\n    ###########################\n\n    model = Model(args.mesh, args.image, args)\n    model.cuda()\n\n    ###########################\n    # Optimize\n    ###########################\n\n    loop = tqdm.tqdm(range(args.epochs))\n    loop.set_description(\'Optimizing\')\n\n    optimizer = torch.optim.Adam(\n        [p for p in model.parameters() if p.requires_grad])\n\n    os.makedirs(args.output_path, exist_ok=True)\n    writer = imageio.get_writer(os.path.join(\n        args.output_path, \'example2_optimization.gif\'), mode=\'I\')\n    for i in loop:\n        optimizer.zero_grad()\n\n        loss = model()\n\n        loss.backward()\n        optimizer.step()\n\n        images, _, _ = model.renderer(\n            model.vertices, model.faces, model.textures)\n\n        image = images.detach()[0].permute(1, 2, 0).cpu().numpy()\n        writer.append_data((255 * image).astype(np.uint8))\n\n    writer.close()\n\n    ###########################\n    # Render optimized mesh\n    ###########################\n\n    loop = tqdm.tqdm(range(0, 360, 4))\n    loop.set_description(\'Drawing\')\n\n    os.makedirs(args.output_path, exist_ok=True)\n    writer = imageio.get_writer(os.path.join(\n        args.output_path, \'example2_mesh.gif\'), mode=\'I\')\n    for azimuth in loop:\n        model.renderer.eye = get_points_from_angles(\n            args.camera_distance, args.elevation, azimuth)\n\n        images, _, _ = model.renderer(\n            model.vertices, model.faces, model.textures)\n\n        image = images.detach()[0].permute(1, 2, 0).cpu().numpy()\n        writer.append_data((255 * image).astype(np.uint8))\n\n    writer.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/renderers/NMR/example3.py,9,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Neural Mesh Renderer\n\n# MIT License\n\n# Copyright (c) 2017 Hiroharu Kato\n# Copyright (c) 2018 Nikos Kolotouros\n# A PyTorch implementation of Neural 3D Mesh Renderer (https://github.com/hiroharu-kato/neural_renderer)\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import division\n\nfrom kaolin.graphics import NeuralMeshRenderer as Renderer\nfrom kaolin.graphics.nmr.util import get_points_from_angles\nfrom kaolin.rep import TriangleMesh\nfrom skimage.io import imread\nfrom util import normalize_vertices\nimport argparse\nimport imageio\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport tqdm\n\nROOT_DIR = os.path.abspath(os.path.dirname(__file__))\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(\n        description=\'NMR Example 3: Optimize texture\')\n\n    parser.add_argument(\'--mesh\', type=str, default=os.path.join(ROOT_DIR, \'rocket.obj\'),\n                        help=\'Path to the mesh OBJ file\')\n    parser.add_argument(\'--image\', type=str, default=os.path.join(ROOT_DIR, \'example3_ref.png\'),\n                        help=\'Path to the target image file to optimize to\')\n    parser.add_argument(\'--output_path\', type=str, default=os.path.join(ROOT_DIR, \'results\'),\n                        help=\'Path to the output directory\')\n    parser.add_argument(\'--epochs\', type=int, default=300,\n                        help=\'Number of epochs to optimize\')\n    parser.add_argument(\'--camera_distance\', type=float, default=2.732,\n                        help=\'Distance from camera to object center\')\n    parser.add_argument(\'--elevation\', type=float, default=0,\n                        help=\'Camera elevation\')\n    parser.add_argument(\'--texture_size\', type=int, default=4,\n                        help=\'Dimension of texture\')\n\n    return parser.parse_args()\n\n\nclass Model(nn.Module):\n\n    def __init__(self, mesh_path, image_path, args):\n        super(Model, self).__init__()\n\n        self.args = args\n\n        ###########################\n        # Load mesh\n        ###########################\n\n        mesh = TriangleMesh.from_obj(mesh_path)\n        mesh.cuda()\n        # Normalize into unit cube, and expand such that batch size = 1\n        vertices = normalize_vertices(mesh.vertices).unsqueeze(0)\n        faces = mesh.faces.unsqueeze(0)\n\n        self.register_buffer(\'vertices\', vertices)\n        self.register_buffer(\'faces\', faces)\n\n        ###########################\n        # Initialize texture (NMR format)\n        ###########################\n\n        textures = torch.zeros(\n            1, self.faces.shape[1], self.args.texture_size, self.args.texture_size, self.args.texture_size,\n            3, dtype=torch.float32,\n            device=\'cuda\'\n        )\n        self.textures = nn.Parameter(textures)\n\n        ###########################\n        # Load target image\n        ###########################\n\n        image_ref = torch.from_numpy(imread(image_path).astype(\n            \'float32\') / 255.).permute(2, 0, 1)[:3, ...][None, ::]\n        self.register_buffer(\'image_ref\', image_ref)\n\n        ###########################\n        # Setup renderer\n        ###########################\n\n        renderer = Renderer(camera_mode=\'look_at\')\n        # renderer.perspective = False\n        renderer.light_intensity_directional = 0.0\n        renderer.light_intensity_ambient = 1.0\n        self.renderer = renderer\n\n    def forward(self):\n        ###########################\n        # Render\n        ###########################\n\n        self.renderer.eye = get_points_from_angles(\n            self.args.camera_distance, self.args.elevation,\n            np.random.uniform(0, 360)\n        )\n        image, _, _ = self.renderer(\n            self.vertices,\n            self.faces,\n            torch.tanh(self.textures)\n        )\n        loss = torch.sum((image - self.image_ref) ** 2)\n\n        return loss\n\n\ndef main():\n    args = parse_arguments()\n\n    ###########################\n    # Setup model\n    ###########################\n\n    model = Model(args.mesh, args.image, args)\n    model.cuda()\n\n    ###########################\n    # Optimize\n    ###########################\n\n    loop = tqdm.tqdm(range(args.epochs))\n    loop.set_description(\'Optimizing\')\n\n    optimizer = torch.optim.Adam(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=0.1, betas=(0.5, 0.999)\n    )\n\n    azimuth = 0.0\n\n    os.makedirs(args.output_path, exist_ok=True)\n    writer = imageio.get_writer(os.path.join(\n        args.output_path, \'example3_optimization.gif\'), mode=\'I\')\n    for i in loop:\n        optimizer.zero_grad()\n\n        loss = model()\n\n        loss.backward()\n        optimizer.step()\n\n        model.renderer.eye = get_points_from_angles(\n            args.camera_distance, args.elevation, azimuth)\n\n        images, _, _ = model.renderer(\n            model.vertices,\n            model.faces,\n            torch.tanh(model.textures)\n        )\n\n        image = images.detach()[0].permute(1, 2, 0).cpu().numpy()\n        writer.append_data((255 * image).astype(np.uint8))\n\n        azimuth = (azimuth + 4) % 360\n\n    writer.close()\n\n    ###########################\n    # Render optimized mesh\n    ###########################\n\n    loop = tqdm.tqdm(range(0, 360, 4))\n    loop.set_description(\'Drawing\')\n\n    os.makedirs(args.output_path, exist_ok=True)\n    writer = imageio.get_writer(os.path.join(\n        args.output_path, \'example3_mesh.gif\'), mode=\'I\')\n    for azimuth in loop:\n        model.renderer.eye = get_points_from_angles(\n            args.camera_distance, args.elevation, azimuth)\n\n        images, _, _ = model.renderer(\n            model.vertices,\n            model.faces,\n            torch.tanh(model.textures)\n        )\n\n        image = images.detach()[0].permute(1, 2, 0).cpu().numpy()\n        writer.append_data((255 * image).astype(np.uint8))\n\n    writer.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/renderers/NMR/example4.py,7,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Neural Mesh Renderer\n\n# MIT License\n\n# Copyright (c) 2017 Hiroharu Kato\n# Copyright (c) 2018 Nikos Kolotouros\n# A PyTorch implementation of Neural 3D Mesh Renderer (https://github.com/hiroharu-kato/neural_renderer)\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import division\n\nfrom kaolin.graphics import NeuralMeshRenderer as Renderer\nfrom kaolin.graphics.nmr.util import get_points_from_angles\nfrom kaolin.rep import TriangleMesh\nfrom skimage.io import imread\nfrom util import normalize_vertices\nimport argparse\nimport imageio\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport tqdm\n\nROOT_DIR = os.path.abspath(os.path.dirname(__file__))\n\n###########################\n# Settings\n###########################\n\nINITIAL_CAMERA_POS = [6, 10, -14]\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(\n        description=\'NMR Example 4: Optimize camera\')\n\n    parser.add_argument(\'--mesh\', type=str, default=os.path.join(ROOT_DIR, \'rocket.obj\'),\n                        help=\'Path to the mesh OBJ file\')\n    parser.add_argument(\'--image\', type=str, default=os.path.join(ROOT_DIR, \'example4_ref.png\'),\n                        help=\'Path to the target image file to optimize to\')\n    parser.add_argument(\'--output_path\', type=str, default=os.path.join(ROOT_DIR, \'results\'),\n                        help=\'Path to the output directory\')\n    parser.add_argument(\'--epochs\', type=int, default=300,\n                        help=\'Number of epochs to optimize\')\n    parser.add_argument(\'--texture_size\', type=int, default=2,\n                        help=\'Dimension of texture\')\n\n    return parser.parse_args()\n\n\nclass Model(nn.Module):\n\n    def __init__(self, mesh_path, image_path, args):\n        super(Model, self).__init__()\n\n        self.args = args\n\n        ###########################\n        # Load mesh\n        ###########################\n\n        mesh = TriangleMesh.from_obj(mesh_path)\n        mesh.cuda()\n        # Normalize into unit cube, and expand such that batch size = 1\n        vertices = normalize_vertices(mesh.vertices).unsqueeze(0)\n        faces = mesh.faces.unsqueeze(0)\n\n        self.register_buffer(\'vertices\', vertices)\n        self.register_buffer(\'faces\', faces)\n\n        ###########################\n        # Initialize texture (NMR format)\n        ###########################\n\n        textures = torch.ones(\n            1, self.faces.shape[1], self.args.texture_size, self.args.texture_size, self.args.texture_size,\n            3, dtype=torch.float32,\n            device=\'cuda\'\n        )\n        self.register_buffer(\'textures\', textures)\n\n        ###########################\n        # Load target image\n        ###########################\n\n        image_ref = torch.from_numpy(\n            (imread(image_path)[:, :, :3].max(-1) > 0.1).astype(np.float32)\n        )[None, ::]\n        self.register_buffer(\'image_ref\', image_ref)\n\n        from skimage.io import imsave\n        imsave(image_path + \'.test.png\', image_ref.numpy().transpose((1, 2, 0)))\n\n        ###########################\n        # Initialize camera position\n        ###########################\n\n        self.camera_position = nn.Parameter(\n            torch.from_numpy(np.array(INITIAL_CAMERA_POS, dtype=np.float32)))\n\n        ###########################\n        # Setup renderer\n        ###########################\n\n        renderer = Renderer(camera_mode=\'look_at\')\n        renderer.eye = self.camera_position\n        self.renderer = renderer\n\n    def forward(self):\n        ###########################\n        # Render\n        ###########################\n\n        image = self.renderer(self.vertices, self.faces, mode=\'silhouettes\')\n        loss = torch.sum((image - self.image_ref[None, :, :]) ** 2)\n\n        return loss\n\n\ndef main():\n    args = parse_arguments()\n\n    ###########################\n    # Setup model\n    ###########################\n\n    model = Model(args.mesh, args.image, args)\n    model.cuda()\n\n    ###########################\n    # Optimize\n    ###########################\n\n    loop = tqdm.tqdm(range(args.epochs))\n    loop.set_description(\'Optimizing\')\n\n    optimizer = torch.optim.Adam(\n        [p for p in model.parameters() if p.requires_grad],\n        lr=0.1\n    )\n\n    os.makedirs(args.output_path, exist_ok=True)\n    writer = imageio.get_writer(os.path.join(\n        args.output_path, \'example4.gif\'), mode=\'I\')\n    for i in loop:\n        optimizer.zero_grad()\n\n        loss = model()\n\n        loss.backward()\n        optimizer.step()\n\n        images, _, _ = model.renderer(\n            model.vertices,\n            model.faces,\n            model.textures\n        )\n\n        image = images.detach()[0].permute(1, 2, 0).cpu().numpy()\n        writer.append_data((255 * image).astype(np.uint8))\n\n    writer.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/renderers/NMR/setup.py,1,"b'from setuptools import setup, find_packages\nimport unittest\n\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nCUDA_FLAGS = []\nINSTALL_REQUIREMENTS = []\n\ndef test_all():\n    test_loader = unittest.TestLoader()\n    test_suite = test_loader.discover(\'tests\', pattern=\'test_*.py\')\n    return test_suite\n\next_modules=[\n    CUDAExtension(\'neural_renderer.cuda.load_textures\', [\n        \'neural_renderer/cuda/load_textures_cuda.cpp\',\n        \'neural_renderer/cuda/load_textures_cuda_kernel.cu\',\n        ]),\n    CUDAExtension(\'neural_renderer.cuda.rasterize\', [\n        \'neural_renderer/cuda/rasterize_cuda.cpp\',\n        \'neural_renderer/cuda/rasterize_cuda_kernel.cu\',\n        ]),\n    CUDAExtension(\'neural_renderer.cuda.create_texture_image\', [\n        \'neural_renderer/cuda/create_texture_image_cuda.cpp\',\n        \'neural_renderer/cuda/create_texture_image_cuda_kernel.cu\',\n        ]),\n    ]\n\nsetup(\n    description=\'PyTorch implementation of ""A 3D mesh renderer for neural networks""\',\n    author=\'Nikolaos Kolotouros\',\n    author_email=\'nkolot@seas.upenn.edu\',\n    license=\'MIT License\',\n    version=\'1.1.3\',\n    name=\'neural_renderer_pytorch\',\n    test_suite=\'setup.test_all\',\n    packages=[\'neural_renderer\', \'neural_renderer.cuda\'],\n    install_requires=INSTALL_REQUIREMENTS,\n    ext_modules=ext_modules,\n    cmdclass = {\'build_ext\': BuildExtension}\n)\n'"
examples/renderers/NMR/util.py,1,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Neural Mesh Renderer\n\n# MIT License\n\n# Copyright (c) 2017 Hiroharu Kato\n# Copyright (c) 2018 Nikos Kolotouros\n# A PyTorch implementation of Neural 3D Mesh Renderer (https://github.com/hiroharu-kato/neural_renderer)\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import division\nimport math\n\nimport torch\n\n\ndef normalize_vertices(vertices):\n    """"""\n    Normalize mesh vertices into a unit cube centered at zero.\n    """"""\n    vertices = vertices - vertices.min(0)[0][None, :]\n    vertices /= torch.abs(vertices).max()\n    vertices *= 2\n    vertices -= vertices.max(0)[0][None, :] / 2\n    return vertices\n'"
examples/renderers/softras/softras_simple_render.py,4,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n# Soft Rasterizer (SoftRas)\n#\n# Copyright (c) 2017 Hiroharu Kato\n# Copyright (c) 2018 Nikos Kolotouros\n# Copyright (c) 2019 Shichen Liu\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport argparse\nimport os\n\nimport imageio\nimport numpy as np\nimport torch\nfrom tqdm import trange\n\nimport kaolin\n\n# Example script that uses SoftRas to render an image, given a mesh input\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--stride"", type=int, default=6,\n                        help=""Rotation (in degrees) between successive render azimuth angles."")\n    parser.add_argument(""--no-viz"", action=""store_true"",\n                        help=""Skip visualization steps."")\n    args = parser.parse_args()\n\n    # Initialize the soft rasterizer.\n    renderer = kaolin.graphics.SoftRenderer(camera_mode=""look_at"", device=""cuda:0"")\n\n    # Camera settings.\n    camera_distance = 2.  # Distance of the camera from the origin (i.e., center of the object).\n    elevation = 30.       # Angle of elevation\n\n    # Directory in which sample data is located.\n    DATA_DIR = os.path.join(os.path.dirname(__file__), "".."", ""sampledata"")\n\n    # Read in the input mesh.\n    mesh = kaolin.rep.TriangleMesh.from_obj(os.path.join(DATA_DIR, ""banana.obj""))\n\n    # Output filename (to write out a rendered .gif to).\n    outfile = ""softras_render.gif""\n\n    # Extract the vertices, faces, and texture the mesh (currently color with white).\n    vertices = mesh.vertices\n    faces = mesh.faces\n    vertices = vertices[None, :, :].cuda()\n    faces = faces[None, :, :].cuda()\n    # Initialize all faces to yellow (to color the banana)!\n    textures = torch.cat(\n        (\n            torch.ones(1, faces.shape[1], 2, 1, dtype=torch.float32, device=""cuda:0""),\n            torch.ones(1, faces.shape[1], 2, 1, dtype=torch.float32, device=""cuda:0""),\n            torch.zeros(1, faces.shape[1], 2, 1, dtype=torch.float32, device=""cuda:0""),\n        ),\n        dim=-1,\n    )\n\n    # Translate the mesh such that its centered at the origin.\n    vertices_max = vertices.max()\n    vertices_min = vertices.min()\n    vertices_middle = (vertices_max + vertices_min) / 2.\n    vertices = vertices - vertices_middle\n    # Scale the vertices slightly (so that they occupy a sizeable image area).\n    # Skip if using models other than the banana.obj file.\n    coef = 5\n    vertices = vertices * coef\n\n    # Loop over a set of azimuth angles, and render the image.\n    print(""Rendering using softras..."")\n    if not args.no_viz:\n        writer = imageio.get_writer(outfile, mode=""I"")\n    for azimuth in trange(0, 360, args.stride):\n        renderer.set_eye_from_angles(camera_distance, elevation, azimuth)\n        # Render an image.\n        rgba = renderer.forward(vertices, faces, textures)\n        if not args.no_viz:\n            img = rgba[0].permute(1, 2, 0).detach().cpu().numpy()\n            writer.append_data((255 * img).astype(np.uint8))\n    if not args.no_viz:\n        writer.close()\n'"
examples/renderers/softras/softras_texture_optimization.py,7,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n# Soft Rasterizer (SoftRas)\n#\n# Copyright (c) 2017 Hiroharu Kato\n# Copyright (c) 2018 Nikos Kolotouros\n# Copyright (c) 2019 Shichen Liu\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport argparse\nimport os\n\nimport imageio\nimport numpy as np\nimport torch\nfrom tqdm import tqdm, trange\n\nimport kaolin\n\n# Example script that uses SoftRas to optimize the texture for a given mesh.\n\n\nclass Model(torch.nn.Module):\n    """"""Wrap textures into an nn.Module, for optimization. """"""\n\n    def __init__(self, textures):\n        super(Model, self).__init__()\n        self.textures = torch.nn.Parameter(textures)\n\n    def forward(self):\n        return torch.sigmoid(self.textures)\n\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--iters"", type=int, default=20,\n                        help=""Number of iterations to run optimization for."")\n    parser.add_argument(""--no-viz"", action=""store_true"",\n                        help=""Skip visualization steps."")\n    args = parser.parse_args()\n\n    # Initialize the soft rasterizer.\n    renderer = kaolin.graphics.SoftRenderer(camera_mode=""look_at"", device=""cuda:0"")\n\n    # Camera settings.\n    camera_distance = 2.  # Distance of the camera from the origin (i.e., center of the object)\n    elevation = 30.       # Angle of elevation\n    azimuth = 0.          # Azimuth angle\n\n    # Directory in which sample data is located.\n    DATA_DIR = os.path.join(os.path.dirname(__file__), "".."", ""sampledata"")\n\n    # Read in the input mesh. TODO: Add filepath as argument.\n    mesh = kaolin.rep.TriangleMesh.from_obj(os.path.join(DATA_DIR, ""banana.obj""))\n\n    # Output filename to write out a rendered .gif to, showing the progress of optimization.\n    progressfile = ""texture_optimization_progress.gif""\n    # Output filename to write out a rendered .gif file to, rendering the optimized mesh.\n    outfile = ""texture_optimization_output.gif""\n\n    # Extract the vertices, faces, and texture the mesh (currently color with white).\n    vertices = mesh.vertices\n    faces = mesh.faces\n    vertices = vertices[None, :, :].cuda()\n    faces = faces[None, :, :].cuda()\n    textures = torch.ones(1, faces.shape[1], 2, 3, dtype=torch.float32, device=""cuda:0"")\n\n    # Translate the mesh such that its centered at the origin.\n    vertices_max = vertices.max()\n    vertices_min = vertices.min()\n    vertices_middle = (vertices_max + vertices_min) / 2.\n    vertices = vertices - vertices_middle\n    # Scale the vertices slightly (so that they occupy a sizeable image area).\n    # Skip if using models other than the banana.obj file.\n    coef = 5\n    vertices = vertices * coef\n\n    img_target = torch.from_numpy(\n        imageio.imread(os.path.join(DATA_DIR, ""banana.png"")).astype(np.float32) / 255\n    ).cuda()\n    img_target = img_target[None, ...].permute(0, 3, 1, 2)\n\n    # Create a \'model\' (an nn.Module) that wraps around the vertices, making it \'optimizable\'.\n    # TODO: Replace with a torch optimizer that takes vertices as a \'params\' argument.\n    # Deform the vertices slightly.\n    model = Model(textures).cuda()\n    # renderer.transform.set_eyes_from_angles(camera_distance, elevation, azimuth)\n    optimizer = torch.optim.Adam(model.parameters(), 1., betas=(0.5, 0.99))\n    renderer.set_eye_from_angles(camera_distance, elevation, azimuth)\n    mseloss = torch.nn.MSELoss()\n\n    # Perform texture optimization.\n    if not args.no_viz:\n        writer = imageio.get_writer(progressfile, mode=""I"")\n    for i in trange(args.iters):\n        optimizer.zero_grad()\n        textures = model()\n        rgba = renderer(vertices, faces, textures)\n        loss = mseloss(rgba, img_target)\n        loss.backward()\n        optimizer.step()\n        if i % 5 == 0:\n            # TODO: Add functionality to write to gif output file.\n            tqdm.write(f""Loss: {loss.item():.5}"")\n            if not args.no_viz:\n                img = rgba[0].permute(1, 2, 0).detach().cpu().numpy()\n                writer.append_data((255 * img).astype(np.uint8))\n    if not args.no_viz:\n        writer.close()\n\n        # Write optimized mesh to output file.\n        writer = imageio.get_writer(outfile, mode=""I"")\n        for azimuth in trange(0, 360, 6):\n            renderer.set_eye_from_angles(camera_distance, elevation, azimuth)\n            rgba = renderer.forward(vertices, faces, model())\n            img = rgba[0].permute(1, 2, 0).detach().cpu().numpy()\n            writer.append_data((255 * img).astype(np.uint8))\n        writer.close()\n'"
examples/renderers/softras/softras_vertex_optimization.py,9,"b'# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n#\n# Soft Rasterizer (SoftRas)\n#\n# Copyright (c) 2017 Hiroharu Kato\n# Copyright (c) 2018 Nikos Kolotouros\n# Copyright (c) 2019 Shichen Liu\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport argparse\nimport os\n\nimport imageio\nimport numpy as np\nimport torch\nfrom tqdm import tqdm, trange\n\nimport kaolin\n\n# Example script that uses SoftRas to deform a sphere mesh to aproximate\n# the image of a banana.\n\n\nclass Model(torch.nn.Module):\n    """"""Wrap vertices into an nn.Module, for optimization. """"""\n\n    def __init__(self, vertices):\n        super(Model, self).__init__()\n        self.update = torch.nn.Parameter(torch.rand(vertices.shape) * 0.001)\n        self.verts = vertices\n\n    def forward(self):\n        return self.update + self.verts\n\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--iters"", type=int, default=2000,\n                        help=""Number of iterations to run optimization for."")\n    parser.add_argument(""--no-viz"", action=""store_true"",\n                        help=""Skip visualization steps."")\n    args = parser.parse_args()\n\n    # Initialize the soft rasterizer.\n    renderer = kaolin.graphics.SoftRenderer(camera_mode=""look_at"", device=""cuda:0"")\n\n    # Camera settings.\n    camera_distance = 2.  # Distance of the camera from the origin (i.e., center of the object)\n    elevation = 30.       # Angle of elevation\n    azimuth = 0.          # Azimuth angle\n\n    # Directory in which sample data is located.\n    DATA_DIR = os.path.join(os.path.dirname(__file__), "".."", ""sampledata"")\n\n    # Read in the input mesh. TODO: Add filepath as argument.\n    mesh = kaolin.rep.TriangleMesh.from_obj(os.path.join(DATA_DIR, ""sphere.obj""))\n\n    # Output filename to write out a rendered .gif to, showing the progress of optimization.\n    progressfile = ""vertex_optimization_progress.gif""\n    # Output filename to write out a rendered .gif file to, rendering the optimized mesh.\n    outfile = ""vertex_optimization_output.gif""\n\n    # Extract the vertices, faces, and texture the mesh (currently color with white).\n    vertices = mesh.vertices\n    faces = mesh.faces\n    vertices = vertices[None, :, :].cuda()\n    faces = faces[None, :, :].cuda()\n    # Initialize all faces to yellow (to color the banana)!\n    textures = torch.cat(\n        (\n            torch.ones(1, faces.shape[1], 2, 1, dtype=torch.float32, device=""cuda:0""),\n            torch.ones(1, faces.shape[1], 2, 1, dtype=torch.float32, device=""cuda:0""),\n            torch.zeros(1, faces.shape[1], 2, 1, dtype=torch.float32, device=""cuda:0""),\n        ),\n        dim=-1,\n    )\n\n    # # TODO: Normalize vertices (Use kaolin functionality to do this)\n    # vertices = vertices - 0.5 * (vertices.max() - vertices.min())\n\n    img_target = torch.from_numpy(\n        imageio.imread(os.path.join(DATA_DIR, ""banana.png"")).astype(np.float32) / 255\n    ).cuda()\n    img_target = img_target[None, ...].permute(0, 3, 1, 2)\n\n    # Create a \'model\' (an nn.Module) that wraps around the vertices, making it \'optimizable\'.\n    # TODO: Replace with a torch optimizer that takes vertices as a \'params\' argument.\n    # Deform the vertices slightly.\n    model = Model(vertices.clone()).cuda()\n    renderer.set_eye_from_angles(camera_distance, elevation, azimuth)\n    optimizer = torch.optim.Adam(model.parameters(), 0.01, betas=(0.5, 0.99))\n    mseloss = torch.nn.MSELoss()\n\n    # Perform vertex optimization.\n    if not args.no_viz:\n        writer = imageio.get_writer(progressfile, mode=""I"")\n    for i in trange(args.iters):\n        optimizer.zero_grad()\n        new_vertices = model()\n        rgba = renderer(new_vertices, faces, textures)\n        loss = mseloss(rgba, img_target)\n        loss.backward()\n        optimizer.step()\n        if i % 20 == 0:\n            # TODO: Add functionality to write to gif output file.\n            tqdm.write(f""Loss: {loss.item():.5}"")\n            if not args.no_viz:\n                img = rgba[0].permute(1, 2, 0).detach().cpu().numpy()\n                writer.append_data((255 * img).astype(np.uint8))\n    if not args.no_viz:\n        writer.close()\n\n        # Write optimized mesh to output file.\n        writer = imageio.get_writer(outfile, mode=""I"")\n        for azimuth in trange(0, 360, 6):\n            renderer.set_eye_from_angles(camera_distance, elevation, azimuth)\n            rgba = renderer.forward(model(), faces, textures)\n            img = rgba[0].permute(1, 2, 0).detach().cpu().numpy()\n            writer.append_data((255 * img).astype(np.uint8))\n        writer.close()\n'"
examples/renderers/test/opt_textures.py,30,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nDemo deform.\nDeform template mesh based on input silhouettes and camera pose\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom skimage.io import imread, imsave\nimport os\nimport tqdm\nimport numpy as np\nimport imageio\nimport argparse\n\nimport neural_renderer as nr\nimport soft_renderer as sr\nimport kaolin as kal\nfrom kaolin.rep import TriangleMesh \nimport graphics \nfrom graphics.render.base import Render as Dib_Renderer\nfrom graphics.utils.utils_sphericalcoord import get_spherical_coords_x\nfrom graphics.utils.utils_perspective import lookatnp, perspectiveprojectionnp\n\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata_dir = os.path.join(current_dir, \'data\')\noutput_directory = os.path.join(data_dir, \'results\')\n\noutput_directory_nmr = os.path.join(output_directory, \'nmr\')\nos.makedirs(output_directory_nmr, exist_ok=True)\n\noutput_directory_sr = os.path.join(output_directory, \'sr\')\nos.makedirs(output_directory_sr, exist_ok=True)\n\noutput_directory_dib = os.path.join(output_directory, \'Dib\')\nos.makedirs(output_directory_dib, exist_ok=True)\n\nclass Model(nn.Module):\n\n\tdef __init__(self, textures):\n\t\tsuper(Model, self).__init__()\n\t\tself.textures = nn.Parameter(textures)\n\tdef forward(self):\n\t\treturn torch.sigmoid(self.textures) \n\n\n\n\ndef main():\n\n\n\t###########################\n\t# camera settings\n\t###########################\n\tcamera_distance = 2.732\n\televation = 0\n\tazimuth = 0\n  \n\t###########################\n\t# load object\n\t###########################\n\tfilename_input = os.path.join(data_dir, \'banana.obj\')\n\tfilename_ref = os.path.join(data_dir, \'example3_ref.png\')\n\t\n\tmesh = TriangleMesh.from_obj(filename_input)\n\tvertices = mesh.vertices\n\tfaces = mesh.faces.int()\n\tuvs = mesh.uvs\n\tface_textures = mesh.face_textures\n\t\t\n\t\n\tpmax = vertices.max()\n\tpmin = vertices.min()\n\tpmiddle = (pmax + pmin) / 2\n\tvertices = vertices - pmiddle    \n\tcoef = 8\n\tvertices = vertices * coef\n\n\t\n\tvertices = vertices[None, :, :].cuda()  \n\tfaces = faces[None, :, :].cuda() \n\tuvs = uvs[None, :, :].cuda()\n\tface_textures[None, :, :].cuda()\n\n\timage_gt = torch.from_numpy(imread(filename_ref).astype(\'float32\') / 255.).permute(2,0,1)[None, ::].cuda()\n\n\n\t##########################\n\t#NMR \n\t##########################\n\ttextures = torch.rand(1, faces.shape[1], 2, 2, 2, 3, dtype=torch.float32)\n\tmodel = Model( textures ).cuda()\n\trenderer = nr.Renderer(camera_mode=\'look_at\')\n\trenderer.eye = nr.get_points_from_angles(camera_distance, elevation, azimuth)\n\trenderer.perspective = False\n\trenderer.light_intensity_directional = 0.0\n\trenderer.light_intensity_ambient = 1.0\n\toptimizer = torch.optim.Adam(model.parameters(), 0.01, betas=(0.5, 0.99))\n\n\n\tloop = tqdm.tqdm(range(2000))\n\tloop.set_description(\'Optimizing NMR\')\n\twriter = imageio.get_writer(os.path.join(output_directory_nmr, \'texture.gif\'), mode=\'I\')\n\tfor i in loop:\n\t\toptimizer.zero_grad()\n\t\tnew_texture = model() \n\t\timage_pred  ,_, _= renderer(vertices, faces, new_texture)\n\t\tloss = torch.sum((image_pred - image_gt)**2)\n\n\t\tloss.backward()\n\t\toptimizer.step()\n\t\tloop.set_description(\'Loss: %.4f\'%(loss.item()))\n\t\tif i % 20 == 0:\n\t\t\timage = image_pred.detach().cpu().numpy()[0].transpose((1, 2, 0))\n\t\t\twriter.append_data((255*image).astype(np.uint8))\n\n\n   \n\n\t##########################\n\t#Soft Rasterizer \n\t##########################\n\ttextures = torch.rand(1, faces.shape[1], 2, 3, dtype=torch.float32)\n\tmodel = Model( textures ).cuda()\n\trenderer = sr.SoftRenderer(image_size=256, sigma_val=3e-5, \n\t\t\t\t\t\t\t   camera_mode=\'look_at\', perspective = False,\n\t\t\t\t\t\t\t   light_intensity_directionals = 0.0,light_intensity_ambient = 1.0  )\n\trenderer.transform.set_eyes_from_angles(camera_distance, elevation, azimuth)\n\toptimizer = torch.optim.Adam(model.parameters(), 0.01, betas=(0.5, 0.99))\n\tloop = tqdm.tqdm(range(2000))\n\tloop.set_description(\'Optimizing SR\')\n\twriter = imageio.get_writer(os.path.join(output_directory_sr, \'texture.gif\'), mode=\'I\')\n\tfor i in loop:\n\t\toptimizer.zero_grad()\n\t\tnew_texture = model() \n\t\tmesh = sr.Mesh(vertices, faces, new_texture)\n\t\timage_pred = renderer.render_mesh(mesh)\n\n\t\tloss = torch.sum(((image_pred[:,:3] - image_gt[None, :, :]))**2)\n\t\tloss.backward()\n\t\toptimizer.step()\n\t\tloop.set_description(\'Loss: %.4f\'%(loss.item()))\n\t\tif i % 20 == 0:\n\t\t\timage = image_pred.detach().cpu().numpy()[0].transpose((1, 2, 0))\n\n\t\t\twriter.append_data((255*image).astype(np.uint8))\n\n\n\n\t###########################\n\t# Dib-Renderer - Vertex Colours\n\t###########################\n\ttextures = torch.rand(1, vertices.shape[1], 3).cuda()\n\tmodel = Model(textures).cuda()\n\trenderer = Dib_Renderer(256, 256, mode = \'VertexColor\')\n\trenderer.set_look_at_parameters([90-azimuth], [elevation], [camera_distance])\n\toptimizer = torch.optim.Adam(model.parameters(), 0.01, betas=(0.5, 0.99))\n\tloop = tqdm.tqdm(range(2000))\n\tloop.set_description(\'Optimizing VertexColor\')\n\twriter = imageio.get_writer(os.path.join(output_directory_dib, \'texture_VertexColor.gif\'), mode=\'I\')\n\tfor i in loop:\n\t\toptimizer.zero_grad()\n\t\tnew_texture = model()         \n\n\t\timage_pred, alpha, _ = renderer.forward(points=[vertices, faces[0].long()], colors=[new_texture])\n\t\timage_pred = torch.cat((image_pred, alpha), dim = 3)\n\t\timage_pred = image_pred.permute(0,3,1,2)\n\t\tloss = torch.sum(((image_pred[:,:3] - image_gt[None, :, :]))**2)\n\t\tloss.backward()\n\t\toptimizer.step()\n\t\tloop.set_description(\'Loss: %.4f\'%(loss.item()))\n\t\tif i % 20 == 0:\n\t\t\timage = image_pred.detach().cpu().numpy()[0].transpose((1, 2, 0))\n\n\t\t\twriter.append_data((255*image).astype(np.uint8))\n\n\t###########################\n\t# Dib-Renderer - Lambertian\n\t###########################\n\ttextures = torch.rand(1, 3, 256, 256).cuda()\n\tmodel = Model(textures).cuda()\n\trenderer = Dib_Renderer(256, 256, mode = \'Lambertian\')\n\trenderer.set_look_at_parameters([90-azimuth], [elevation], [camera_distance])\n\toptimizer = torch.optim.Adam(model.parameters(), 0.01, betas=(0.5, 0.99))\n\tloop = tqdm.tqdm(range(2000))\n\tloop.set_description(\'Optimizing Lambertian\')\n\twriter = imageio.get_writer(os.path.join(output_directory_dib, \'texture_Lambertian.gif\'), mode=\'I\')\n\tfor i in loop:\n\t\toptimizer.zero_grad()\n\t\tnew_texture = model()         \n\n\t\timage_pred, alpha, _ = renderer.forward(points=[vertices, faces[0].long()], \\\n\t\t\tcolors=[uvs, face_textures.long(), new_texture])\n\t\timage_pred = torch.cat((image_pred, alpha), dim = 3)\n\t\timage_pred = image_pred.permute(0,3,1,2)\n\t\tloss = torch.sum(((image_pred[:,:3] - image_gt[None, :, :]))**2)\n\t\tloss.backward()\n\t\toptimizer.step()\n\t\tloop.set_description(\'Loss: %.4f\'%(loss.item()))\n\t\tif i % 20 == 0:\n\t\t\timage = image_pred.detach().cpu().numpy()[0].transpose((1, 2, 0))\n\n\t\t\twriter.append_data((255*image).astype(np.uint8))\n\n\t###########################\n\t# Dib-Renderer - Phong\n\t# ###########################\n\ttextures = torch.rand(1, 3, 256, 256).cuda()\n\tmodel = Model(textures).cuda()\n\trenderer = Dib_Renderer(256, 256, mode = \'Phong\')\n\trenderer.set_look_at_parameters([90-azimuth], [elevation], [camera_distance])\n\toptimizer = torch.optim.Adam(model.parameters(), 0.01, betas=(0.5, 0.99))\n\tloop = tqdm.tqdm(range(2000))\n\tloop.set_description(\'Optimizing Phong\')\n\n\n\t### Lighting info ###\n\tmaterial = np.array([[0.3, 0.3, 0.3], \n\t\t\t\t\t\t [1.0, 1.0, 1.0],\n\t\t\t\t\t\t [0.4, 0.4, 0.4]], dtype=np.float32).reshape(-1, 3, 3)\n\tmaterial = torch.from_numpy(material).repeat(1, 1, 1).cuda()\n\t\n\tshininess = np.array([100], dtype=np.float32).reshape(-1, 1)\n\tshininess = torch.from_numpy(shininess).repeat(1, 1).cuda()\n\n\tlightdirect = 2 * np.random.rand(1, 3).astype(np.float32) - 1\n\tlightdirect[:, 2] += 2\n\tlightdirect = torch.from_numpy(lightdirect).cuda()\n\n\n\n\twriter = imageio.get_writer(os.path.join(output_directory_dib, \'texture_Phong.gif\'), mode=\'I\')\n\tfor i in loop:\n\t\toptimizer.zero_grad()\n\t\tnew_texture = model()         \n\n\t\timage_pred, alpha, _ = renderer.forward(points=[vertices, faces[0].long()], \\\n\t\t\tcolors=[uvs, face_textures.long(), new_texture],\\\n\t\t\t\t\t\t\t\t\t\t\t  light= lightdirect, \\\n\t\t\t\t\t\t\t\t\t\t\t  material=material, \\\n\t\t\t\t\t\t\t\t\t\t\t  shininess=shininess)\n\t\timage_pred = torch.cat((image_pred, alpha), dim = 3)\n\t\timage_pred = image_pred.permute(0,3,1,2)\n\n\n\t\tloss = torch.sum(((image_pred[:,:3] - image_gt[None, :, :]))**2)\n\t\tloss.backward()\n\t\toptimizer.step()\n\t\tloop.set_description(\'Loss: %.4f\'%(loss.item()))\n\t\tif i % 20 == 0:\n\t\t\timage = image_pred.detach().cpu().numpy()[0].transpose((1, 2, 0))\n\n\t\t\twriter.append_data((255*image).astype(np.uint8))\n\n\n\t###########################\n\t# Dib-Renderer - SphericalHarmonics\n\t###########################\n\ttextures = torch.rand(1, 3, 256, 256).cuda()\n\tmodel = Model(textures).cuda()\n\trenderer = Dib_Renderer(256, 256, mode = \'SphericalHarmonics\')\n\trenderer.set_look_at_parameters([90-azimuth], [elevation], [camera_distance])\n\toptimizer = torch.optim.Adam(model.parameters(), 0.01, betas=(0.5, 0.99))\n\t\n\n\tlightparam = np.random.rand(1, 9).astype(np.float32)\n\tlightparam[:, 0] += 4\n\tlightparam = torch.from_numpy(lightparam).cuda()\n\t\n\n\tloop = tqdm.tqdm(range(2000))\n\tloop.set_description(\'Optimizing SH\')\n\twriter = imageio.get_writer(os.path.join(output_directory_dib, \'texture_SH.gif\'), mode=\'I\')\n\tfor i in loop:\n\t\toptimizer.zero_grad()\n\t\tnew_texture = model()         \n\n\t\timage_pred, alpha, _ = renderer.forward(points=[vertices, faces[0].long()], \\\n\t\t\tcolors=[uvs, face_textures.long(), new_texture],\\\n\t\t\tlight=lightparam)\n\t\timage_pred = torch.cat((image_pred, alpha), dim = 3)\n\t\timage_pred = image_pred.permute(0,3,1,2)\n\t\tloss = torch.sum(( (image_pred[:,:3] - image_gt[None, :, :]))**2)\n\t\tloss.backward()\n\t\toptimizer.step()\n\t\tloop.set_description(\'Loss: %.4f\'%(loss.item()))\n\t\tif i % 20 == 0:\n\t\t\timage = image_pred.detach().cpu().numpy()[0].transpose((1, 2, 0))\n\n\t\t\twriter.append_data((255*image).astype(np.uint8))\n\n\n \n\nif __name__ == \'__main__\':\n\tmain()'"
examples/renderers/test/opt_vertex.py,31,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nDemo deform.\nDeform template mesh based on input silhouettes and camera pose\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom skimage.io import imread, imsave\nimport os\nimport tqdm\nimport numpy as np\nimport imageio\nimport argparse\n\nimport neural_renderer as nr\nimport soft_renderer as sr\nimport kaolin as kal\nfrom kaolin.rep import TriangleMesh \nimport graphics \nfrom graphics.render.base import Render as Dib_Renderer\nfrom graphics.utils.utils_sphericalcoord import get_spherical_coords_x\nfrom graphics.utils.utils_perspective import lookatnp, perspectiveprojectionnp\n\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata_dir = os.path.join(current_dir, \'data\')\noutput_directory = os.path.join(data_dir, \'results\')\n\n\noutput_directory_nmr = os.path.join(output_directory, \'nmr\')\nos.makedirs(output_directory_nmr, exist_ok=True)\n\noutput_directory_sr = os.path.join(output_directory, \'sr\')\nos.makedirs(output_directory_sr, exist_ok=True)\n\noutput_directory_dib = os.path.join(output_directory, \'Dib\')\nos.makedirs(output_directory_dib, exist_ok=True)\n\nclass Model(nn.Module):\n\n    def __init__(self, vertices):\n        super(Model, self).__init__()\n        self.update = nn.Parameter(torch.rand(vertices.shape)*.001)\n        self.verts = vertices\n    def forward(self):\n        return self.update + self.verts\n\n\n\n\ndef main():\n    ###########################\n    # camera settings\n    ###########################\n    camera_distance = 2.732\n    elevation = 0\n    azimuth = 0 \n  \n    ###########################\n    # load object\n    ###########################\n    filename_input = os.path.join(data_dir, \'banana.obj\')\n    filename_ref = os.path.join(data_dir, \'example2_ref.png\')\n    image_gt = torch.from_numpy(imread(filename_ref).astype(np.float32).mean(-1) / 255.)[None, ::].cuda()\n\n\n    mesh = TriangleMesh.from_obj(filename_input)\n    vertices = mesh.vertices\n    faces = mesh.faces.int()\n    uvs = torch.FloatTensor(get_spherical_coords_x(vertices.data.numpy())) \n    face_textures = (faces).clone()\n\n\n    pmax = vertices.max()\n    pmin = vertices.min()\n    pmiddle = (pmax + pmin) / 2\n    vertices = vertices - pmiddle    \n    coef = 10\n    vertices = vertices * coef\n\n    \n    vertices = vertices[None, :, :].cuda()  \n    faces = faces[None, :, :].cuda() \n    uvs = uvs[None, :, :].cuda()\n    face_textures[None, :, :].cuda()\n\n    ##########################\n    # normalize verts\n    ##########################\n    vertices_max = vertices.max()\n    vertices_min = vertices.min()\n    vertices_middle = (vertices_max + vertices_min)/2.\n    vertices = vertices - vertices_middle\n    \n    # coef = 5\n    # vertices = vertices * coef\n\n\n\n    ###########################\n    # NMR \n    ###########################\n    textures = torch.ones(1, faces.shape[1], 2,2,2, 3, dtype=torch.float32).cuda()\n    model = Model(vertices.clone()).cuda()\n    renderer = nr.Renderer(camera_mode=\'look_at\')\n    renderer.eye = nr.get_points_from_angles(camera_distance, elevation, azimuth)\n    optimizer = torch.optim.Adam(model.parameters(), 0.001, betas=(0.5, 0.99))\n\n\n    loop = tqdm.tqdm(range(2000))\n    loop.set_description(\'Optimizing NMR\')\n    writer = imageio.get_writer(os.path.join(output_directory_nmr, \'deform.gif\'), mode=\'I\')\n    for i in loop:\n        optimizer.zero_grad()\n        new_vertices = model() \n        image_pred  ,_, _= renderer(new_vertices, faces, textures)\n        loss = torch.sum((image_pred - image_gt[None, :, :])**2)\n\n        loss.backward()\n        optimizer.step()\n        loop.set_description(\'Loss: %.4f\'%(loss.item()))\n        if i % 20 == 0:\n            image = image_pred.detach().cpu().numpy()[0].transpose((1, 2, 0))\n            other_image = image_gt.detach().cpu().numpy().transpose((1, 2, 0))\n           \n            pass_image = image + other_image\n            writer.append_data((128*pass_image).astype(np.uint8))\n\n\n   \n\n    ###########################\n    # Soft Rasterizer \n    ###########################\n    textures = torch.ones(1, faces.shape[1], 2, 3, dtype=torch.float32).cuda()\n    model = Model(vertices.clone()).cuda()\n    mesh = sr.Mesh(vertices, faces, textures)\n    renderer = sr.SoftRenderer(image_size=256, sigma_val=3e-5, aggr_func_rgb=\'hard\', \n                               camera_mode=\'look_at\')\n    renderer.transform.set_eyes_from_angles(camera_distance, elevation, azimuth)\n    optimizer = torch.optim.Adam(model.parameters(), 0.001, betas=(0.5, 0.99))\n\n\n    loop = tqdm.tqdm(range(2000))\n    loop.set_description(\'Optimizing SR\')\n    writer = imageio.get_writer(os.path.join(output_directory_sr, \'deform.gif\'), mode=\'I\')\n    for i in loop:\n        optimizer.zero_grad()\n        new_vertices = model() \n        new_mesh = sr.Mesh(new_vertices, faces, textures)\n        image_pred = renderer.render_mesh(new_mesh)\n        loss = torch.sum((image_pred - image_gt[None, :, :])**2)\n        loss.backward()\n        optimizer.step()\n        loop.set_description(\'Loss: %.4f\'%(loss.item()))\n        if i % 20 == 0:\n            image = image_pred.detach().cpu().numpy()[0].transpose((1, 2, 0))\n            other_image = image_gt.detach().cpu().numpy().transpose((1, 2, 0))\n           \n            pass_image = image + other_image\n            writer.append_data((128*pass_image).astype(np.uint8))\n\n    \n\n    ################################\n    # Dib-Renderer - Vertex Colours\n    ################################\n    model = Model(vertices.clone()).cuda()\n    textures = torch.ones(1, vertices.shape[1], 3).cuda() \n    renderer = Dib_Renderer(256, 256, mode = \'VertexColor\')\n    renderer.set_look_at_parameters([90-azimuth], [elevation], [camera_distance])\n    optimizer = torch.optim.Adam(model.parameters(), 0.001, betas=(0.5, 0.99))\n\n\n    loop = tqdm.tqdm(range(2000))\n    loop.set_description(\'Optimizing Dib_Renderer VertexColor\')\n    writer = imageio.get_writer(os.path.join(output_directory_dib, \'deform_VertexColor.gif\'), mode=\'I\')\n    for i in loop:\n        optimizer.zero_grad()\n        new_vertices = model() \n        image_pred, alpha, _ = renderer.forward(points=[new_vertices, faces[0].long()], colors=[textures])\n\n        image_pred = torch.cat((image_pred, alpha), dim = 3)\n        image_pred = image_pred.permute(0,3,1,2)\n        \n        loss = torch.sum((image_pred - image_gt[None, :, :])**2) \n     \n        loss.backward()\n        optimizer.step()\n       \n        loop.set_description(\'Loss: %.4f\'%(loss.item()))\n\n        if i % 20 == 0:\n            image = image_pred.detach().cpu().numpy()[0].transpose((1, 2, 0))\n            other_image = image_gt.detach().cpu().numpy().transpose((1, 2, 0))\n           \n            pass_image = image + other_image\n            writer.append_data((127*pass_image).astype(np.uint8))\n    \n    ################################\n    # Dib-Renderer - Lambertian\n    ################################\n    model = Model(vertices.clone()).cuda()\n    textures = torch.ones(1, 3, 256, 256).cuda()\n    renderer = Dib_Renderer(256, 256, mode = \'Lambertian\')\n    renderer.set_look_at_parameters([90-azimuth], [elevation], [camera_distance])\n    optimizer = torch.optim.Adam(model.parameters(), 0.001, betas=(0.5, 0.99))\n\n\n    loop = tqdm.tqdm(range(2000))\n    loop.set_description(\'Optimizing Dib_Renderer Lambertian\')\n    writer = imageio.get_writer(os.path.join(output_directory_dib, \'deform_Lambertian.gif\'), mode=\'I\')\n    for i in loop:\n        optimizer.zero_grad()\n        new_vertices = model() \n        image_pred, alpha, _ = renderer.forward(points=[new_vertices, faces[0].long()], colors=[uvs, face_textures.long(), textures])\n        image_pred = torch.cat((image_pred, alpha), dim = 3)\n        image_pred = image_pred.permute(0,3,1,2)\n\n        loss = torch.sum((image_pred - image_gt[None, :, :])**2) \n     \n        loss.backward()\n        optimizer.step()\n       \n        loop.set_description(\'Loss: %.4f\'%(loss.item()))\n\n        if i % 20 == 0:\n            image = image_pred.detach().cpu().numpy()[0].transpose((1, 2, 0))\n            other_image = image_gt.detach().cpu().numpy().transpose((1, 2, 0))\n           \n            pass_image = image + other_image\n            writer.append_data((127*pass_image).astype(np.uint8))\n\n\n    ################################\n    # Dib-Renderer - Phong\n    ################################\n    model = Model(vertices.clone()).cuda()\n    textures = torch.ones(1, 3, 256, 256).cuda() \n    renderer = Dib_Renderer(256, 256, mode = \'Phong\')\n    renderer.set_look_at_parameters([90-azimuth], [elevation], [camera_distance])\n    optimizer = torch.optim.Adam(model.parameters(), 0.001, betas=(0.5, 0.99))\n\n    ### Lighting info ###\n    material = np.array([[0.1, 0.1, 0.1], \n                         [1.0, 1.0, 1.0],\n                         [0.4, 0.4, 0.4]], dtype=np.float32).reshape(-1, 3, 3)\n    material = torch.from_numpy(material).repeat(1, 1, 1).cuda()\n    \n    shininess = np.array([100], dtype=np.float32).reshape(-1, 1)\n    shininess = torch.from_numpy(shininess).repeat(1, 1).cuda()\n\n    lightdirect = 2 * np.random.rand(1, 3).astype(np.float32) - 1\n    lightdirect[:, 2] += 2\n    lightdirect = torch.from_numpy(lightdirect).cuda()\n\n    loop = tqdm.tqdm(range(2000))\n    loop.set_description(\'Optimizing Dib_Renderer Phong\')\n    writer = imageio.get_writer(os.path.join(output_directory_dib, \'deform_Phong.gif\'), mode=\'I\')\n    for i in loop:\n        optimizer.zero_grad()\n        new_vertices = model() \n        image_pred, alpha, _ = renderer.forward(points=[new_vertices, faces[0].long()], \\\n                                              colors=[uvs, face_textures.long(), textures],\\\n                                              light= lightdirect, \\\n                                              material=material, \\\n                                              shininess=shininess)\n        image_pred = torch.cat((image_pred, alpha), dim = 3)\n        image_pred = image_pred.permute(0,3,1,2)\n\n        loss = torch.sum((image_pred - image_gt[None, :, :])**2) \n     \n        loss.backward()\n        optimizer.step()\n       \n        loop.set_description(\'Loss: %.4f\'%(loss.item()))\n\n        if i % 20 == 0:\n            image = image_pred.detach().cpu().numpy()[0].transpose((1, 2, 0))\n            other_image = image_gt.detach().cpu().numpy().transpose((1, 2, 0))\n           \n            pass_image = image + other_image\n            writer.append_data((127*pass_image).astype(np.uint8))\n\n\n    ################################\n    # Dib-Renderer - SphericalHarmonics\n    ################################\n    model = Model(vertices.clone()).cuda()\n    textures = torch.ones(1, 3, 256, 256).cuda()\n    renderer = Dib_Renderer(256, 256, mode = \'SphericalHarmonics\')\n    renderer.set_look_at_parameters([90-azimuth], [elevation], [camera_distance])\n    optimizer = torch.optim.Adam(model.parameters(), 0.001, betas=(0.5, 0.99))\n\n    lightparam = np.random.rand(1, 9).astype(np.float32)\n    lightparam[:, 0] += 2\n    lightparam = torch.from_numpy(lightparam).cuda()\n    \n\n    loop = tqdm.tqdm(range(2000))\n    loop.set_description(\'Optimizing Dib_Renderer SH\')\n    writer = imageio.get_writer(os.path.join(output_directory_dib, \'deform_SH.gif\'), mode=\'I\')\n    for i in loop:\n        optimizer.zero_grad()\n        new_vertices = model() \n        image_pred, alpha, _ = renderer.forward(points=[new_vertices, faces[0].long()],\\\n                colors=[uvs, face_textures.long(), textures], light =lightparam)\n        image_pred = torch.cat((image_pred, alpha), dim = 3)\n\n        image_pred = image_pred.permute(0,3,1,2)\n\n        loss = torch.sum((image_pred - image_gt[None, :, :])**2) \n     \n        loss.backward()\n        optimizer.step()\n       \n        loop.set_description(\'Loss: %.4f\'%(loss.item()))\n\n        if i % 20 == 0:\n            image = image_pred.detach().cpu().numpy()[0].transpose((1, 2, 0))\n            other_image = image_gt.detach().cpu().numpy().transpose((1, 2, 0))\n           \n            pass_image = image + other_image\n            writer.append_data((127*pass_image).astype(np.uint8))\n\n\n\n\nif __name__ == \'__main__\':\n    main()'"
examples/renderers/test/simple_render.py,11,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""\nExample 1. Drawing a teapot from multiple viewpoints.\n""""""\nimport os\nimport math \n\nimport torch\nimport numpy as np\nimport tqdm\nimport imageio\nfrom PIL import Image\n\nimport neural_renderer as nr\nimport soft_renderer as sr\nimport graphics \nfrom graphics.render.base import Render as Dib_Renderer\nimport kaolin as kal\nfrom kaolin.rep import TriangleMesh \nfrom graphics.utils.utils_sphericalcoord import get_spherical_coords_x\nfrom graphics.utils.utils_perspective import lookatnp, perspectiveprojectionnp\n\n\n\ndef obj_centened_camera_pos(dist, azimuth_deg, elevation_deg):\n    phi = float(elevation_deg) / 180.0 * math.pi\n    theta = float(azimuth_deg) / 180.0 * math.pi\n    x = (dist * math.cos(theta) * math.cos(phi))\n    y = (dist * math.sin(theta) * math.cos(phi))\n    z = (dist * math.sin(phi))\n    return (x, y, z)\n\n\n\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata_dir = os.path.join(current_dir, \'data\')\noutput_directory = os.path.join(data_dir, \'results\')\\\n\noutput_directory_nmr = os.path.join(output_directory, \'nmr\')\nos.makedirs(output_directory_nmr, exist_ok=True)\n\noutput_directory_sr = os.path.join(output_directory, \'sr\')\nos.makedirs(output_directory_sr, exist_ok=True)\n\noutput_directory_dib = os.path.join(output_directory, \'Dib\')\nos.makedirs(output_directory_dib, exist_ok=True)\n\ndef main():\n    filename_input = os.path.join(data_dir, \'banana.obj\')\n    filename_output = os.path.join(output_directory, \'example1.gif\')\n    \n    ###########################\n    # camera settings\n    ###########################\n    camera_distance = 2\n    elevation = 30\n    \n    ###########################\n    # load object\n    ###########################\n    mesh = TriangleMesh.from_obj(filename_input)\n    vertices = mesh.vertices\n    faces = mesh.faces.int()\n    uvs = torch.FloatTensor(get_spherical_coords_x(vertices.data.numpy())) \n    face_textures = (faces).clone()\n    \n    vertices = vertices[None, :, :].cuda()  \n    faces = faces[None, :, :].cuda() \n    uvs = uvs[None, :, :].cuda()\n    face_textures[None, :, :].cuda()\n\n    ###########################\n    # normalize verts\n    ###########################\n    vertices_max = vertices.max()\n    vertices_min = vertices.min()\n    vertices_middle = (vertices_max + vertices_min)/2.\n    vertices = vertices - vertices_middle\n    \n    coef = 5\n    vertices = vertices * coef\n\n    ###########################\n    # NMR \n    ###########################\n    textures = torch.ones(1, faces.shape[1], 2, 2, 2, 3, dtype=torch.float32).cuda()\n    renderer = nr.Renderer(camera_mode=\'look_at\')\n    loop = tqdm.tqdm(list(range(0, 360, 4)))\n    loop.set_description(\'Drawing NMR\')\n    writer = imageio.get_writer(os.path.join(output_directory_nmr, \'rotation.gif\'), mode=\'I\')\n    for num, azimuth in enumerate(loop):\n        renderer.eye =  nr.get_points_from_angles(camera_distance, elevation, azimuth)\n        images, _, _ = renderer(vertices, faces, textures)  \n        image = images.detach().cpu().numpy()[0].transpose((1, 2, 0)) \n        writer.append_data((255*image).astype(np.uint8))\n    writer.close()\n\n\n    \n\n    ###########################\n    # Soft Rasterizer \n    ###########################\n    textures = torch.ones(1, faces.shape[1], 2, 3, dtype=torch.float32).cuda()\n    mesh = sr.Mesh(vertices, faces, textures)\n    renderer = sr.SoftRenderer(camera_mode=\'look_at\')\n    loop = tqdm.tqdm(list(range(0, 360, 4)))\n    loop.set_description(\'Drawing SR\')\n    writer = imageio.get_writer(os.path.join(output_directory_sr, \'rotation.gif\'), mode=\'I\')\n    for azimuth in loop:\n        mesh.reset_()\n        renderer.transform.set_eyes_from_angles(camera_distance, elevation, azimuth)\n        images = renderer.render_mesh(mesh)\n        image = images.detach().cpu().numpy()[0].transpose((1, 2, 0))\n        writer.append_data((255*image).astype(np.uint8))\n    writer.close()\n\n\n\n    ################################\n    # Dib-Renderer - Vertex Colours\n    ################################ \n    renderer = Dib_Renderer(256, 256, mode = \'VertexColor\')\n    textures = torch.ones(1, vertices.shape[1], 3).cuda()\n    loop = tqdm.tqdm(list(range(0, 360, 4)))\n    loop.set_description(\'Drawing Dib_Renderer VertexColor\')\n    writer = imageio.get_writer(os.path.join(output_directory_dib, \'rotation_VertexColor.gif\'), mode=\'I\')\n    for azimuth in loop:\n        renderer.set_look_at_parameters([90-azimuth], [elevation], [camera_distance])\n        predictions, _, _ = renderer.forward(points=[vertices, faces[0].long()], colors=[textures])\n        image = predictions.detach().cpu().numpy()[0]\n        writer.append_data((image*255).astype(np.uint8))\n    writer.close()\n \n\n\n\n    ################################\n    # Dib-Renderer - Lambertian\n    ################################  \n    renderer = Dib_Renderer(256, 256, mode = \'Lambertian\')\n    textures = torch.ones(1, 3, 256, 256).cuda()\n    loop = tqdm.tqdm(list(range(0, 360, 4)))\n    loop.set_description(\'Drawing Dib_Renderer Lambertian\')\n    writer = imageio.get_writer(os.path.join(output_directory_dib, \'rotation_Lambertian.gif\'), mode=\'I\')\n    for azimuth in loop:\n        renderer.set_look_at_parameters([90-azimuth], [elevation], [camera_distance])\n        predictions, _, _ = renderer.forward(points=[vertices, faces[0].long()], \\\n                                              colors=[uvs, face_textures.long(), textures])\n        image = predictions.detach().cpu().numpy()[0]\n        writer.append_data((image*255).astype(np.uint8))\n    writer.close()\n\n\n    ################################\n    # Dib-Renderer - Phong\n    ################################\n    renderer = Dib_Renderer(256, 256, mode = \'Phong\')\n    textures = torch.ones(1, 3, 256, 256).cuda()\n\n    ### Lighting info ###\n    material = np.array([[0.1, 0.1, 0.1], \n                         [1.0, 1.0, 1.0],\n                         [0.4, 0.4, 0.4]], dtype=np.float32).reshape(-1, 3, 3)\n    material = torch.from_numpy(material).repeat(1, 1, 1).cuda()\n    \n    shininess = np.array([100], dtype=np.float32).reshape(-1, 1)\n    shininess = torch.from_numpy(shininess).repeat(1, 1).cuda()\n\n    lightdirect = 2 * np.random.rand(1, 3).astype(np.float32) - 1\n    lightdirect[:, 2] += 2\n    lightdirect = torch.from_numpy(lightdirect).cuda()\n    \n   \n    loop = tqdm.tqdm(list(range(0, 360, 4)))\n    loop.set_description(\'Drawing Dib_Renderer Phong\')\n    writer = imageio.get_writer(os.path.join(output_directory_dib, \'rotation_Phong.gif\'), mode=\'I\')\n    for azimuth in loop:\n        renderer.set_look_at_parameters([90-azimuth], [elevation], [camera_distance])\n        predictions, _, _ = renderer.forward(points=[vertices, faces[0].long()], \\\n                                              colors=[uvs, face_textures.long(), textures],\\\n                                              light= lightdirect, \\\n                                              material=material, \\\n                                              shininess=shininess )\n        image = predictions.detach().cpu().numpy()[0]\n        writer.append_data((image*255).astype(np.uint8))\n    writer.close()\n \n   \n    ################################\n    # Dib-Renderer - SH\n    ################################\n    renderer = Dib_Renderer(256, 256, mode = \'SphericalHarmonics\')\n    textures = torch.ones(1, 3, 256, 256).cuda()\n    \n    ### Lighting info ###\n    lightparam = np.random.rand(1, 9).astype(np.float32)\n    lightparam[:, 0] += 2\n    lightparam = torch.from_numpy(lightparam).cuda()\n    \n   \n    loop = tqdm.tqdm(list(range(0, 360, 4)))\n    loop.set_description(\'Drawing Dib_Renderer SH\')\n    writer = imageio.get_writer(os.path.join(output_directory_dib, \'rotation_SH.gif\'), mode=\'I\')\n    for azimuth in loop:\n        renderer.set_look_at_parameters([90-azimuth], [elevation], [camera_distance])\n        predictions, _, _ = renderer.forward(points=[vertices, faces[0].long()], \\\n                                              colors=[uvs, face_textures.long(), textures],\\\n                                              light=lightparam)\n        image = predictions.detach().cpu().numpy()[0]\n        writer.append_data((image*255).astype(np.uint8))\n    writer.close()\n\n  \n\n\n\n\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/renderers/test/test_dibr.py,1,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport graphics\nfrom graphics.utils.utils_perspective import lookatnp, perspectiveprojectionnp\nfrom graphics.utils.utils_sphericalcoord import get_spherical_coords_x\nfrom graphics.render.base import Render as Dib_Renderer\nimport os\nimport sys\nimport math\n\nimport torch\nimport numpy as np\nimport tqdm\nimport imageio\n# from PIL import Image\n\nimport kaolin as kal\nfrom kaolin.rep import TriangleMesh\n\nsys.path.append(str(os.path.join(os.path.dirname(os.path.abspath(__file__)), \'../DIB-R\')))\n\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata_dir = os.path.join(current_dir, \'data\')\noutput_directory = os.path.join(data_dir, \'results\')\n\noutput_directory_dib = os.path.join(output_directory, \'dib\')\nos.makedirs(output_directory_dib, exist_ok=True)\n\n\ndef main():\n    filename_input = os.path.join(data_dir, \'banana.obj\')\n    filename_output = os.path.join(output_directory, \'example1.gif\')\n\n    ###########################\n    # camera settings\n    ###########################\n    camera_distance = 2\n    elevation = 30\n\n    ###########################\n    # load object\n    ###########################\n    mesh = TriangleMesh.from_obj(filename_input)\n    vertices = mesh.vertices\n    faces = mesh.faces.int()\n    face_textures = (faces).clone()\n\n    vertices = vertices[None, :, :].cuda()\n    faces = faces[None, :, :].cuda()\n    face_textures[None, :, :].cuda()\n\n    ###########################\n    # normalize verts\n    ###########################\n    vertices_max = vertices.max()\n    vertices_min = vertices.min()\n    vertices_middle = (vertices_max + vertices_min) / 2.\n    vertices = vertices - vertices_middle\n\n    coef = 5\n    vertices = vertices * coef\n\n    ###########################\n    # DIB-Renderer\n    ###########################\n    renderer = Dib_Renderer(256, 256, mode=\'VertexColor\')\n    textures = torch.ones(1, vertices.shape[1], 3).cuda()\n    loop = tqdm.tqdm(list(range(0, 360, 4)))\n    loop.set_description(\'Drawing Dib_Renderer VertexColor\')\n    writer = imageio.get_writer(os.path.join(output_directory_dib, \'rotation_VertexColor.gif\'), mode=\'I\')\n    for azimuth in loop:\n        renderer.set_look_at_parameters([90 - azimuth], [elevation], [camera_distance])\n        predictions, _, _ = renderer.forward(points=[vertices, faces[0].long()], colors=[textures])\n        image = predictions.detach().cpu().numpy()[0]\n        writer.append_data((image * 255).astype(np.uint8))\n    writer.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/renderers/test/test_nmr.py,1,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport math \n\nimport torch\nimport numpy as np\nimport tqdm\nimport imageio\n# from PIL import Image\n\nimport kaolin as kal\nfrom kaolin.rep import TriangleMesh\nimport neural_renderer as nr\n\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata_dir = os.path.join(current_dir, \'data\')\noutput_directory = os.path.join(data_dir, \'results\')\n\noutput_directory_nmr = os.path.join(output_directory, \'nmr\')\nos.makedirs(output_directory_nmr, exist_ok=True)\n\ndef main():\n    filename_input = os.path.join(data_dir, \'banana.obj\')\n    filename_output = os.path.join(output_directory, \'example1.gif\')\n    \n    ###########################\n    # camera settings\n    ###########################\n    camera_distance = 2\n    elevation = 30\n    \n    ###########################\n    # load object\n    ###########################\n    mesh = TriangleMesh.from_obj(filename_input)\n    vertices = mesh.vertices\n    faces = mesh.faces.int()\n    face_textures = (faces).clone()\n    \n    vertices = vertices[None, :, :].cuda()  \n    faces = faces[None, :, :].cuda()\n    face_textures[None, :, :].cuda()\n\n    ###########################\n    # normalize verts\n    ###########################\n    vertices_max = vertices.max()\n    vertices_min = vertices.min()\n    vertices_middle = (vertices_max + vertices_min)/2.\n    vertices = vertices - vertices_middle\n    \n    coef = 5\n    vertices = vertices * coef\n\n    ###########################\n    # NMR \n    ###########################\n    textures = torch.ones(1, faces.shape[1], 2, 2, 2, 3, dtype=torch.float32).cuda()\n    renderer = nr.Renderer(camera_mode=\'look_at\')\n    # loop = tqdm.tqdm(list(range(0, 360, 4)))\n    # loop.set_description(\'Drawing NMR\')\n    # writer = imageio.get_writer(os.path.join(output_directory_nmr, \'rotation.gif\'), mode=\'I\')\n    renderer.eye =  nr.get_points_from_angles(camera_distance, elevation, 0)\n    images, _, _ = renderer(vertices, faces, textures)\n    # for num, azimuth in enumerate(loop):\n    #     renderer.eye =  nr.get_points_from_angles(camera_distance, elevation, azimuth)\n    #     images, _, _ = renderer(vertices, faces, textures)\n    #     image = images.detach().cpu().numpy()[0].transpose((1, 2, 0)) \n    #     writer.append_data((255*image).astype(np.uint8))\n    # writer.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/renderers/test/test_softras.py,1,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport math \n\nimport torch\nimport numpy as np\nimport tqdm\nimport imageio\n# from PIL import Image\n\nimport kaolin as kal\nfrom kaolin.rep import TriangleMesh\nimport soft_renderer as sr\n\n\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\ndata_dir = os.path.join(current_dir, \'data\')\noutput_directory = os.path.join(data_dir, \'results\')\n\noutput_directory_sr = os.path.join(output_directory, \'sr\')\nos.makedirs(output_directory_sr, exist_ok=True)\n\ndef main():\n    filename_input = os.path.join(data_dir, \'banana.obj\')\n    filename_output = os.path.join(output_directory, \'example1.gif\')\n    \n    ###########################\n    # camera settings\n    ###########################\n    camera_distance = 2\n    elevation = 30\n    \n    ###########################\n    # load object\n    ###########################\n    mesh = TriangleMesh.from_obj(filename_input)\n    vertices = mesh.vertices\n    faces = mesh.faces.int()\n    face_textures = (faces).clone()\n    \n    vertices = vertices[None, :, :].cuda()  \n    faces = faces[None, :, :].cuda()\n    face_textures[None, :, :].cuda()\n\n    ###########################\n    # normalize verts\n    ###########################\n    vertices_max = vertices.max()\n    vertices_min = vertices.min()\n    vertices_middle = (vertices_max + vertices_min)/2.\n    vertices = vertices - vertices_middle\n    \n    coef = 5\n    vertices = vertices * coef\n\n    ###########################\n    # Soft Rasterizer \n    ###########################\n    textures = torch.ones(1, faces.shape[1], 2, 3, dtype=torch.float32).cuda()\n    mesh = sr.Mesh(vertices, faces, textures)\n    renderer = sr.SoftRenderer(camera_mode=\'look_at\')\n    loop = tqdm.tqdm(list(range(0, 360, 4)))\n    loop.set_description(\'Drawing SR\')\n    writer = imageio.get_writer(os.path.join(output_directory_sr, \'rotation.gif\'), mode=\'I\')\n    for azimuth in loop:\n        mesh.reset_()\n        renderer.transform.set_eyes_from_angles(camera_distance, elevation, azimuth)\n        images = renderer.render_mesh(mesh)\n        image = images.detach().cpu().numpy()[0].transpose((1, 2, 0))\n        writer.append_data((255*image).astype(np.uint8))\n    writer.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
kaolin/graphics/dib_renderer/__init__.py,0,b''
kaolin/graphics/nmr/__init__.py,0,b'from .rasterizer import *\n'
kaolin/graphics/nmr/rasterizer.py,35,"b'# MIT License\n\n# Copyright (c) 2017 Hiroharu Kato\n# Copyright (c) 2018 Nikos Kolotouros\n# A PyTorch implementation of Neural 3D Mesh Renderer (https://github.com/hiroharu-kato/neural_renderer)\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Function\n\nfrom .cuda import rasterize_cuda\n\nDEFAULT_IMAGE_SIZE = 256\nDEFAULT_ANTI_ALIASING = True\nDEFAULT_NEAR = 0.1\nDEFAULT_FAR = 100\nDEFAULT_EPS = 1e-4\nDEFAULT_BACKGROUND_COLOR = (0, 0, 0)\n\n\nclass RasterizeFunction(Function):\n    \'\'\'\n    Definition of differentiable rasterize operation\n    Some parts of the code are implemented in CUDA\n    Currently implemented only for cuda Tensors\n    \'\'\'\n    @staticmethod\n    def forward(ctx, faces, textures, image_size, near, far, eps, background_color,\n                return_rgb=False, return_alpha=False, return_depth=False):\n        \'\'\'\n        Forward pass\n        \'\'\'\n        ctx.image_size = image_size\n        ctx.near = near\n        ctx.far = far\n        ctx.eps = eps\n        ctx.background_color = background_color\n        ctx.return_rgb = return_rgb\n        ctx.return_alpha = return_alpha\n        ctx.return_depth = return_depth\n\n        faces = faces.clone()\n\n        ctx.device = faces.device\n        ctx.batch_size, ctx.num_faces = faces.shape[:2]\n\n        if ctx.return_rgb:\n            textures = textures.contiguous()\n            ctx.texture_size = textures.shape[2]\n        else:\n            # initializing with dummy values\n            textures = torch.cuda.FloatTensor(1).fill_(0)\n            ctx.texture_size = None\n\n        face_index_map = torch.cuda.IntTensor(\n            ctx.batch_size, ctx.image_size, ctx.image_size).fill_(-1)\n        weight_map = torch.cuda.FloatTensor(\n            ctx.batch_size, ctx.image_size, ctx.image_size, 3).fill_(0.0)\n        depth_map = torch.cuda.FloatTensor(\n            ctx.batch_size, ctx.image_size, ctx.image_size).fill_(ctx.far)\n\n        if ctx.return_rgb:\n            rgb_map = torch.cuda.FloatTensor(\n                ctx.batch_size, ctx.image_size, ctx.image_size, 3).fill_(0)\n            sampling_index_map = torch.cuda.IntTensor(\n                ctx.batch_size, ctx.image_size, ctx.image_size, 8).fill_(0)\n            sampling_weight_map = torch.cuda.FloatTensor(\n                ctx.batch_size, ctx.image_size, ctx.image_size, 8).fill_(0)\n        else:\n            rgb_map = torch.cuda.FloatTensor(1).fill_(0)\n            sampling_index_map = torch.cuda.FloatTensor(1).fill_(0)\n            sampling_weight_map = torch.cuda.FloatTensor(1).fill_(0)\n        if ctx.return_alpha:\n            alpha_map = torch.cuda.FloatTensor(\n                ctx.batch_size, ctx.image_size, ctx.image_size).fill_(0)\n        else:\n            alpha_map = torch.cuda.FloatTensor(1).fill_(0)\n        if ctx.return_depth:\n            face_inv_map = torch.cuda.FloatTensor(\n                ctx.batch_size, ctx.image_size, ctx.image_size, 3, 3).fill_(0)\n        else:\n            face_inv_map = torch.cuda.FloatTensor(1).fill_(0)\n\n        face_index_map, weight_map, depth_map, face_inv_map =\\\n            RasterizeFunction.forward_face_index_map(ctx, faces, face_index_map,\n                                                     weight_map, depth_map,\n                                                     face_inv_map)\n\n        rgb_map, sampling_index_map, sampling_weight_map =\\\n            RasterizeFunction.forward_texture_sampling(ctx, faces, textures,\n                                                       face_index_map, weight_map,\n                                                       depth_map, rgb_map,\n                                                       sampling_index_map,\n                                                       sampling_weight_map)\n\n        rgb_map = RasterizeFunction.forward_background(\n            ctx, face_index_map, rgb_map)\n        alpha_map = RasterizeFunction.forward_alpha_map(\n            ctx, alpha_map, face_index_map)\n\n        ctx.save_for_backward(faces, textures, face_index_map, weight_map,\n                              depth_map, rgb_map, alpha_map, face_inv_map,\n                              sampling_index_map, sampling_weight_map)\n\n        rgb_r, alpha_r, depth_r = torch.tensor(\n            []), torch.tensor([]), torch.tensor([])\n        if ctx.return_rgb:\n            rgb_r = rgb_map\n        if ctx.return_alpha:\n            alpha_r = alpha_map.clone()\n        if ctx.return_depth:\n            depth_r = depth_map.clone()\n        return rgb_r, alpha_r, depth_r\n\n    @staticmethod\n    def backward(ctx, grad_rgb_map, grad_alpha_map, grad_depth_map):\n        \'\'\'\n        Backward pass\n        \'\'\'\n        faces, textures, face_index_map, weight_map,\\\n            depth_map, rgb_map, alpha_map, face_inv_map,\\\n            sampling_index_map, sampling_weight_map = \\\n            ctx.saved_tensors\n        # initialize output buffers\n        # no need for explicit allocation of cuda.FloatTensor because\n        # zeros_like does it automatically\n        grad_faces = torch.zeros_like(faces, dtype=torch.float32)\n        if ctx.return_rgb:\n            grad_textures = torch.zeros_like(textures, dtype=torch.float32)\n        else:\n            grad_textures = torch.cuda.FloatTensor(1).fill_(0.0)\n\n        # get grad_outputs\n        if ctx.return_rgb:\n            if grad_rgb_map is not None:\n                grad_rgb_map = grad_rgb_map.contiguous()\n            else:\n                grad_rgb_map = torch.zeros_like(rgb_map)\n        else:\n            grad_rgb_map = torch.cuda.FloatTensor(1).fill_(0.0)\n        if ctx.return_alpha:\n            if grad_alpha_map is not None:\n                grad_alpha_map = grad_alpha_map.contiguous()\n            else:\n                grad_alpha_map = torch.zeros_like(alpha_map)\n        else:\n            grad_alpha_map = torch.cuda.FloatTensor(1).fill_(0.0)\n        if ctx.return_depth:\n            if grad_depth_map is not None:\n                grad_depth_map = grad_depth_map.contiguous()\n            else:\n                grad_depth_map = torch.zeros_like(ctx.depth_map)\n        else:\n            grad_depth_map = torch.cuda.FloatTensor(1).fill_(0.0)\n\n        # backward pass\n        grad_faces = RasterizeFunction.backward_pixel_map(\n            ctx, faces, face_index_map, rgb_map,\n            alpha_map, grad_rgb_map, grad_alpha_map,\n            grad_faces)\n        grad_textures = RasterizeFunction.backward_textures(\n            ctx, face_index_map, sampling_weight_map,\n            sampling_index_map, grad_rgb_map, grad_textures)\n        grad_faces = RasterizeFunction.backward_depth_map(\n            ctx, faces, depth_map, face_index_map,\n            face_inv_map, weight_map, grad_depth_map,\n            grad_faces)\n\n        if not textures.requires_grad:\n            grad_textures = None\n\n        return grad_faces, grad_textures, None, None, None, None, None, None, None, None\n\n    @staticmethod\n    def forward_face_index_map(ctx, faces, face_index_map, weight_map,\n                               depth_map, face_inv_map):\n        faces_inv = torch.zeros_like(faces)\n        return rasterize_cuda.forward_face_index_map(faces, face_index_map, weight_map,\n                                                     depth_map, face_inv_map, faces_inv,\n                                                     ctx.image_size, ctx.near, ctx.far,\n                                                     ctx.return_rgb, ctx.return_alpha,\n                                                     ctx.return_depth)\n\n    @staticmethod\n    def forward_texture_sampling(ctx, faces, textures, face_index_map,\n                                 weight_map, depth_map, rgb_map,\n                                 sampling_index_map, sampling_weight_map):\n        if not ctx.return_rgb:\n            return rgb_map, sampling_index_map, sampling_weight_map\n        else:\n            return rasterize_cuda.forward_texture_sampling(faces, textures, face_index_map,\n                                                           weight_map, depth_map, rgb_map,\n                                                           sampling_index_map, sampling_weight_map,\n                                                           ctx.image_size, ctx.eps)\n\n    @staticmethod\n    def forward_alpha_map(ctx, alpha_map, face_index_map):\n        if ctx.return_alpha:\n            alpha_map[face_index_map >= 0] = 1\n        return alpha_map\n\n    @staticmethod\n    def forward_background(ctx, face_index_map, rgb_map):\n        if ctx.return_rgb:\n            background_color = torch.cuda.FloatTensor(ctx.background_color)\n            mask = (face_index_map >= 0).float()[:, :, :, None]\n            if background_color.ndimension() == 1:\n                rgb_map = rgb_map * mask + \\\n                    (1 - mask) * background_color[None, None, None, :]\n            elif background_color.ndimension() == 2:\n                rgb_map = rgb_map * mask + \\\n                    (1 - mask) * background_color[:, None, None, :]\n        return rgb_map\n\n    @staticmethod\n    def backward_pixel_map(ctx, faces, face_index_map, rgb_map,\n                           alpha_map, grad_rgb_map, grad_alpha_map, grad_faces):\n        if (not ctx.return_rgb) and (not ctx.return_alpha):\n            return grad_faces\n        else:\n            return rasterize_cuda.backward_pixel_map(faces, face_index_map, rgb_map,\n                                                     alpha_map, grad_rgb_map, grad_alpha_map,\n                                                     grad_faces, ctx.image_size, ctx.eps, ctx.return_rgb,\n                                                     ctx.return_alpha)\n\n    @staticmethod\n    def backward_textures(ctx, face_index_map, sampling_weight_map,\n                          sampling_index_map, grad_rgb_map, grad_textures):\n        if not ctx.return_rgb:\n            return grad_textures\n        else:\n            return rasterize_cuda.backward_textures(face_index_map, sampling_weight_map,\n                                                    sampling_index_map, grad_rgb_map,\n                                                    grad_textures, ctx.num_faces)\n\n    @staticmethod\n    def backward_depth_map(ctx, faces, depth_map, face_index_map,\n                           face_inv_map, weight_map, grad_depth_map, grad_faces):\n        if not ctx.return_depth:\n            return grad_faces\n        else:\n            return rasterize_cuda.backward_depth_map(faces, depth_map, face_index_map,\n                                                     face_inv_map, weight_map,\n                                                     grad_depth_map, grad_faces, ctx.image_size)\n\n\nclass Rasterize(nn.Module):\n    \'\'\'\n    Wrapper around the autograd function RasterizeFunction\n    Currently implemented only for cuda Tensors\n    \'\'\'\n\n    def __init__(self, image_size, near, far, eps, background_color,\n                 return_rgb=False, return_alpha=False, return_depth=False):\n        super(Rasterize, self).__init__()\n        self.image_size = image_size\n        self.image_size = image_size\n        self.near = near\n        self.far = far\n        self.eps = eps\n        self.background_color = background_color\n        self.return_rgb = return_rgb\n        self.return_alpha = return_alpha\n        self.return_depth = return_depth\n\n    def forward(self, faces, textures):\n        if faces.device == ""cpu"" or (\n                textures is not None and textures.device == ""cpu""):\n            raise TypeError(\'Rasterize module supports only cuda Tensors\')\n        return RasterizeFunction.apply(faces, textures, self.image_size, self.near, self.far,\n                                       self.eps, self.background_color,\n                                       self.return_rgb, self.return_alpha, self.return_depth)\n\n\ndef rasterize_rgbad(\n        faces,\n        textures=None,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n        background_color=DEFAULT_BACKGROUND_COLOR,\n        return_rgb=True,\n        return_alpha=True,\n        return_depth=True,\n):\n    """"""\n    Generate RGB, alpha channel, and depth images from faces and textures (for RGB).\n\n    Args:\n        faces (torch.Tensor): Faces. The shape is [batch size, number of faces, 3 (vertices), 3 (XYZ)].\n        textures (torch.Tensor): Textures.\n            The shape is [batch size, number of faces, texture size, texture size, texture size, 3 (RGB)].\n        image_size (int): Width and height of rendered images.\n        anti_aliasing (bool): do anti-aliasing by super-sampling.\n        near (float): nearest z-coordinate to draw.\n        far (float): farthest z-coordinate to draw.\n        eps (float): small epsilon for approximated differentiation.\n        background_color (tuple): background color of RGB images.\n        return_rgb (bool): generate RGB images or not.\n        return_alpha (bool): generate alpha channels or not.\n        return_depth (bool): generate depth images or not.\n\n    Returns:\n        dict:\n            {\n                \'rgb\': RGB images. The shape is [batch size, 3, image_size, image_size].\n                \'alpha\': Alpha channels. The shape is [batch size, image_size, image_size].\n                \'depth\': Depth images. The shape is [batch size, image_size, image_size].\n            }\n\n    """"""\n    if textures is None:\n        inputs = [faces, None]\n    else:\n        inputs = [faces, textures]\n\n    if anti_aliasing:\n        # 2x super-sampling\n        rgb, alpha, depth = Rasterize(\n            image_size * 2, near, far, eps, background_color, return_rgb, return_alpha, return_depth)(*inputs)\n    else:\n        rgb, alpha, depth = Rasterize(\n            image_size, near, far, eps, background_color, return_rgb, return_alpha, return_depth)(*inputs)\n\n    # transpose & vertical flip\n    if return_rgb:\n        rgb = rgb.permute((0, 3, 1, 2))\n        # pytorch does not support negative slicing for the moment\n        # may need to look at this again because it seems to be very slow\n        # rgb = rgb[:, :, ::-1, :]\n        rgb = rgb[:, :, list(reversed(range(rgb.shape[2]))), :]\n    if return_alpha:\n        # alpha = alpha[:, ::-1, :]\n        alpha = alpha[:, list(reversed(range(alpha.shape[1]))), :]\n    if return_depth:\n        # depth = depth[:, ::-1, :]\n        depth = depth[:, list(reversed(range(depth.shape[1]))), :]\n\n    if anti_aliasing:\n        # 0.5x down-sampling\n        if return_rgb:\n            rgb = F.avg_pool2d(rgb, kernel_size=(2, 2))\n        if return_alpha:\n            alpha = F.avg_pool2d(\n                alpha[:, None, :, :], kernel_size=(2, 2))[:, 0]\n        if return_depth:\n            depth = F.avg_pool2d(\n                depth[:, None, :, :], kernel_size=(2, 2))[:, 0]\n\n    ret = {\n        \'rgb\': rgb if return_rgb else None,\n        \'alpha\': alpha if return_alpha else None,\n        \'depth\': depth if return_depth else None,\n    }\n\n    return ret\n\n\ndef rasterize(\n        faces,\n        textures,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n        background_color=DEFAULT_BACKGROUND_COLOR,\n):\n    """"""\n    Generate RGB images from faces and textures.\n\n    Args:\n        faces: see `rasterize_rgbad`.\n        textures: see `rasterize_rgbad`.\n        image_size: see `rasterize_rgbad`.\n        anti_aliasing: see `rasterize_rgbad`.\n        near: see `rasterize_rgbad`.\n        far: see `rasterize_rgbad`.\n        eps: see `rasterize_rgbad`.\n        background_color: see `rasterize_rgbad`.\n\n    Returns:\n        ~torch.Tensor: RGB images. The shape is [batch size, 3, image_size, image_size].\n\n    """"""\n    return rasterize_rgbad(\n        faces, textures, image_size, anti_aliasing, near, far, eps, background_color, True, False, False)[\'rgb\']\n\n\ndef rasterize_silhouettes(\n        faces,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n):\n    """"""\n    Generate alpha channels from faces.\n\n    Args:\n        faces: see `rasterize_rgbad`.\n        image_size: see `rasterize_rgbad`.\n        anti_aliasing: see `rasterize_rgbad`.\n        near: see `rasterize_rgbad`.\n        far: see `rasterize_rgbad`.\n        eps: see `rasterize_rgbad`.\n\n    Returns:\n        ~torch.Tensor: Alpha channels. The shape is [batch size, image_size, image_size].\n\n    """"""\n    return rasterize_rgbad(faces, None, image_size, anti_aliasing,\n                           near, far, eps, None, False, True, False)[\'alpha\']\n\n\ndef rasterize_depth(\n        faces,\n        image_size=DEFAULT_IMAGE_SIZE,\n        anti_aliasing=DEFAULT_ANTI_ALIASING,\n        near=DEFAULT_NEAR,\n        far=DEFAULT_FAR,\n        eps=DEFAULT_EPS,\n):\n    """"""\n    Generate depth images from faces.\n\n    Args:\n        faces: see `rasterize_rgbad`.\n        image_size: see `rasterize_rgbad`.\n        anti_aliasing: see `rasterize_rgbad`.\n        near: see `rasterize_rgbad`.\n        far: see `rasterize_rgbad`.\n        eps: see `rasterize_rgbad`.\n\n    Returns:\n        ~torch.Tensor: Depth images. The shape is [batch size, image_size, image_size].\n\n    """"""\n    return rasterize_rgbad(faces, None, image_size, anti_aliasing,\n                           near, far, eps, None, False, False, True)[\'depth\']\n'"
kaolin/graphics/nmr/util.py,50,"b'# MIT License\n\n# Copyright (c) 2017 Hiroharu Kato\n# Copyright (c) 2018 Nikos Kolotouros\n# A PyTorch implementation of Neural 3D Mesh Renderer (https://github.com/hiroharu-kato/neural_renderer)\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import division\n\nimport numpy as np\nimport torch.nn.functional as F\nimport math\nimport torch\n\n\ndef get_points_from_angles(distance, elevation, azimuth, degrees=True):\n    if isinstance(distance, float) or isinstance(distance, int):\n        if degrees:\n            elevation = math.radians(elevation)\n            azimuth = math.radians(azimuth)\n        return (\n            distance * math.cos(elevation) * math.sin(azimuth),\n            distance * math.sin(elevation),\n            -distance * math.cos(elevation) * math.cos(azimuth))\n    else:\n        if degrees:\n            elevation = math.pi / 180. * elevation\n            azimuth = math.pi / 180. * azimuth\n    #\n        return torch.stack([\n            distance * torch.cos(elevation) * torch.sin(azimuth),\n            distance * torch.sin(elevation),\n            -distance * torch.cos(elevation) * torch.cos(azimuth)\n        ]).transpose(1, 0)\n\n\ndef lighting(faces, textures, intensity_ambient=0.5, intensity_directional=0.5,\n             color_ambient=(1, 1, 1), color_directional=(1, 1, 1), direction=(0, 1, 0)):\n\n    bs, nf = faces.shape[:2]\n    device = faces.device\n\n    # arguments\n    # make sure to convert all inputs to float tensors\n    if isinstance(color_ambient, tuple) or isinstance(color_ambient, list):\n        color_ambient = torch.tensor(\n            color_ambient, dtype=torch.float32, device=device)\n    elif isinstance(color_ambient, np.ndarray):\n        color_ambient = torch.from_numpy(color_ambient).float().to(device)\n    if isinstance(color_directional, tuple) or isinstance(color_directional, list):\n        color_directional = torch.tensor(\n            color_directional, dtype=torch.float32, device=device)\n    elif isinstance(color_directional, np.ndarray):\n        color_directional = torch.from_numpy(\n            color_directional).float().to(device)\n    if isinstance(direction, tuple) or isinstance(direction, list):\n        direction = torch.tensor(direction, dtype=torch.float32, device=device)\n    elif isinstance(direction, np.ndarray):\n        direction = torch.from_numpy(direction).float().to(device)\n    if color_ambient.ndimension() == 1:\n        color_ambient = color_ambient[None, :]\n    if color_directional.ndimension() == 1:\n        color_directional = color_directional[None, :]\n    if direction.ndimension() == 1:\n        direction = direction[None, :]\n\n    # create light\n    light = torch.zeros(bs, nf, 3, dtype=torch.float32).to(device)\n\n    # ambient light\n    if intensity_ambient != 0:\n        light += intensity_ambient * color_ambient[:, None, :]\n\n    # directional light\n    if intensity_directional != 0:\n        faces = faces.reshape((bs * nf, 3, 3))\n        v10 = faces[:, 0] - faces[:, 1]\n        v12 = faces[:, 2] - faces[:, 1]\n        # pytorch normalize divides by max(norm, eps) instead of (norm+eps) in chainer\n        normals = F.normalize(torch.cross(v10, v12), eps=1e-5)\n        normals = normals.reshape((bs, nf, 3))\n\n        if direction.ndimension() == 2:\n            direction = direction[:, None, :]\n        cos = F.relu(torch.sum(normals * direction, dim=2))\n        # may have to verify that the next line is correct\n        light += intensity_directional * \\\n            (color_directional[:, None, :] * cos[:, :, None])\n\n    # apply\n    light = light[:, :, None, None, None, :]\n    textures *= light\n    return textures\n\n\ndef look(vertices, eye, direction=[0, 1, 0], up=None):\n    """"""\n    ""Look"" transformation of vertices.\n    """"""\n    if (vertices.ndimension() != 3):\n        raise ValueError(\'vertices Tensor should have 3 dimensions\')\n\n    device = vertices.device\n\n    if isinstance(direction, list) or isinstance(direction, tuple):\n        direction = torch.tensor(direction, dtype=torch.float32, device=device)\n    elif isinstance(direction, np.ndarray):\n        direction = torch.from_numpy(direction).to(device)\n    elif torch.is_tensor(direction):\n        direction = direction.to(device)\n\n    if isinstance(eye, list) or isinstance(eye, tuple):\n        eye = torch.tensor(eye, dtype=torch.float32, device=device)\n    elif isinstance(eye, np.ndarray):\n        eye = torch.from_numpy(eye).to(device)\n    elif torch.is_tensor(eye):\n        eye = eye.to(device)\n\n    if up is None:\n        up = torch.cuda.FloatTensor([0, 1, 0])\n    if eye.ndimension() == 1:\n        eye = eye[None, :]\n    if direction.ndimension() == 1:\n        direction = direction[None, :]\n    if up.ndimension() == 1:\n        up = up[None, :]\n\n    # create new axes\n    z_axis = F.normalize(direction, eps=1e-5)\n    x_axis = F.normalize(torch.cross(up, z_axis), eps=1e-5)\n    y_axis = F.normalize(torch.cross(z_axis, x_axis), eps=1e-5)\n\n    # create rotation matrix: [bs, 3, 3]\n    r = torch.cat((x_axis[:, None, :], y_axis[:, None, :],\n                   z_axis[:, None, :]), dim=1)\n\n    # apply\n    # [bs, nv, 3] -> [bs, nv, 3] -> [bs, nv, 3]\n    if vertices.shape != eye.shape:\n        eye = eye[:, None, :]\n    vertices = vertices - eye\n    vertices = torch.matmul(vertices, r.transpose(1, 2))\n\n    return vertices\n\n\ndef look_at(vertices, eye, at=[0, 0, 0], up=[0, 1, 0]):\n    """"""\n    ""Look at"" transformation of vertices.\n    """"""\n    if (vertices.ndimension() != 3):\n        raise ValueError(\'vertices Tensor should have 3 dimensions\')\n\n    device = vertices.device\n\n    # if list or tuple convert to numpy array\n    if isinstance(at, list) or isinstance(at, tuple):\n        at = torch.tensor(at, dtype=torch.float32, device=device)\n    # if numpy array convert to tensor\n    elif isinstance(at, np.ndarray):\n        at = torch.from_numpy(at).to(device)\n    elif torch.is_tensor(at):\n        at.to(device)\n\n    if isinstance(up, list) or isinstance(up, tuple):\n        up = torch.tensor(up, dtype=torch.float32, device=device)\n    elif isinstance(up, np.ndarray):\n        up = torch.from_numpy(up).to(device)\n    elif torch.is_tensor(up):\n        up.to(device)\n\n    if isinstance(eye, list) or isinstance(eye, tuple):\n        eye = torch.tensor(eye, dtype=torch.float32, device=device)\n    elif isinstance(eye, np.ndarray):\n        eye = torch.from_numpy(eye).to(device)\n    elif torch.is_tensor(eye):\n        eye = eye.to(device)\n\n    batch_size = vertices.shape[0]\n    if eye.ndimension() == 1:\n        eye = eye[None, :].repeat(batch_size, 1)\n    if at.ndimension() == 1:\n        at = at[None, :].repeat(batch_size, 1)\n    if up.ndimension() == 1:\n        up = up[None, :].repeat(batch_size, 1)\n\n    # create new axes\n    # eps is chosen as 0.5 to match the chainer version\n    z_axis = F.normalize(at - eye, eps=1e-5)\n    x_axis = F.normalize(torch.cross(up, z_axis), eps=1e-5)\n    y_axis = F.normalize(torch.cross(z_axis, x_axis), eps=1e-5)\n\n    # create rotation matrix: [bs, 3, 3]\n    r = torch.cat((x_axis[:, None, :], y_axis[:, None, :],\n                   z_axis[:, None, :]), dim=1)\n\n    # apply\n    # [bs, nv, 3] -> [bs, nv, 3] -> [bs, nv, 3]\n    if vertices.shape != eye.shape:\n        eye = eye[:, None, :]\n    vertices = vertices - eye\n    vertices = torch.matmul(vertices, r.transpose(1, 2))\n\n    return vertices\n\n\ndef perspective(vertices, angle=30.):\n    \'\'\'\n    Compute perspective distortion from a given angle\n    \'\'\'\n    if (vertices.ndimension() != 3):\n        raise ValueError(\'vertices Tensor should have 3 dimensions\')\n    device = vertices.device\n    angle = torch.tensor(angle / 180 * math.pi,\n                         dtype=torch.float32, device=device)\n    angle = angle[None]\n    width = torch.tan(angle)\n    width = width[:, None]\n    z = vertices[:, :, 2]\n    x = vertices[:, :, 0] / z / width\n    y = vertices[:, :, 1] / z / width\n    vertices = torch.stack((x, y, z), dim=2)\n    return vertices\n\n\ndef projection(vertices, K, R, t, dist_coeffs, orig_size, eps=1e-9):\n    \'\'\'\n    Calculate projective transformation of vertices given a projection matrix\n    Input parameters:\n    K: batch_size * 3 * 3 intrinsic camera matrix\n    R, t: batch_size * 3 * 3, batch_size * 1 * 3 extrinsic calibration parameters\n    dist_coeffs: vector of distortion coefficients\n    orig_size: original size of image captured by the camera\n    Returns: For each point [X,Y,Z] in world coordinates [u,v,z] where u,v are the coordinates of the projection in\n    pixels and z is the depth\n    \'\'\'\n\n    # instead of P*x we compute x\'*P\'\n    vertices = torch.matmul(vertices, R.transpose(2, 1)) + t\n    x, y, z = vertices[:, :, 0], vertices[:, :, 1], vertices[:, :, 2]\n    x_ = x / (z + eps)\n    y_ = y / (z + eps)\n\n    # Get distortion coefficients from vector\n    k1 = dist_coeffs[:, None, 0]\n    k2 = dist_coeffs[:, None, 1]\n    p1 = dist_coeffs[:, None, 2]\n    p2 = dist_coeffs[:, None, 3]\n    k3 = dist_coeffs[:, None, 4]\n\n    # we use x_ for x\' and x__ for x\'\' etc.\n    r = torch.sqrt(x_ ** 2 + y_ ** 2)\n    x__ = x_ * (1 + k1 * (r**2) + k2 * (r**4) + k3 * (r**6)) + \\\n        2 * p1 * x_ * y_ + p2 * (r**2 + 2 * x_**2)\n    y__ = y_ * (1 + k1 * (r**2) + k2 * (r**4) + k3 * (r**6)) + \\\n        p1 * (r**2 + 2 * y_**2) + 2 * p2 * x_ * y_\n    vertices = torch.stack([x__, y__, torch.ones_like(z)], dim=-1)\n    vertices = torch.matmul(vertices, K.transpose(1, 2))\n    u, v = vertices[:, :, 0], vertices[:, :, 1]\n    v = orig_size - v\n    # map u,v from [0, img_size] to [-1, 1] to use by the renderer\n    u = 2 * (u - orig_size / 2.) / orig_size\n    v = 2 * (v - orig_size / 2.) / orig_size\n    vertices = torch.stack([u, v, z], dim=-1)\n    return vertices\n\n\ndef vertices_to_faces(vertices, faces):\n    """"""\n    :param vertices: [batch size, number of vertices, 3]\n    :param faces: [batch size, number of faces, 3)\n    :return: [batch size, number of faces, 3, 3]\n    """"""\n    assert (vertices.ndimension() == 3)\n    assert (faces.ndimension() == 3)\n    assert (vertices.shape[0] == faces.shape[0])\n    assert (vertices.shape[2] == 3)\n    assert (faces.shape[2] == 3)\n\n    faces = faces.long()\n\n    bs, nv = vertices.shape[:2]\n    bs, nf = faces.shape[:2]\n    device = vertices.device\n    faces = faces + \\\n        (torch.arange(bs, dtype=torch.long, device=device) * nv)[:, None, None]\n    vertices = vertices.reshape((bs * nv, 3))\n    return vertices[faces]\n'"
kaolin/graphics/softras/__init__.py,0,b''
kaolin/graphics/softras/soft_rasterize.py,6,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# \n# \n# Soft Rasterizer (SoftRas)\n# \n# Copyright (c) 2017 Hiroharu Kato\n# Copyright (c) 2018 Nikos Kolotouros\n# Copyright (c) 2019 Shichen Liu\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport torch\nfrom torch.autograd import Function\nimport numpy as np\n\nfrom . import soft_rasterize_cuda\n\n\nclass SoftRasterizeFunction(Function):\n\n    @staticmethod\n    def forward(ctx,\n                face_vertices,\n                textures,\n                image_size=256, \n                background_color=None,\n                near=1,\n                far=100, \n                fill_back=True,\n                eps=1e-3,\n                sigma_val=1e-5,\n                dist_func=\'euclidean\',\n                dist_eps=1e-4,\n                gamma_val=1e-4,\n                aggr_func_rgb=\'softmax\',\n                aggr_func_alpha=\'prod\',\n                texture_type=\'surface\'):\n\n        # face_vertices: [nb, nf, 9]\n        # textures: [nb, nf, 9]\n\n        func_dist_map = {\'hard\': 0, \'barycentric\': 1, \'euclidean\': 2}\n        func_rgb_map = {\'hard\': 0, \'softmax\': 1}\n        func_alpha_map = {\'hard\': 0, \'sum\': 1, \'prod\': 2}\n        func_map_sample = {\'surface\': 0, \'vertex\': 1}\n\n        ctx.image_size = image_size\n        if background_color is None:\n            ctx.background_color = [0, 0, 0]\n        else:\n            ctx.background_color = background_color\n        ctx.near = near\n        ctx.far = far\n        ctx.eps = eps\n        ctx.sigma_val = sigma_val\n        ctx.gamma_val = gamma_val\n        ctx.func_dist_type = func_dist_map[dist_func]\n        ctx.dist_eps = np.log(1. / dist_eps - 1.)\n        ctx.func_rgb_type = func_rgb_map[aggr_func_rgb]\n        ctx.func_alpha_type = func_alpha_map[aggr_func_alpha]\n        ctx.texture_type = func_map_sample[texture_type]\n        ctx.fill_back = fill_back\n\n        face_vertices = face_vertices.clone()\n        textures = textures.clone()\n\n        ctx.device = face_vertices.device\n        ctx.batch_size, ctx.num_faces = face_vertices.shape[:2]\n\n        faces_info = torch.zeros(ctx.batch_size, ctx.num_faces, 9 * 3, device=ctx.device)\n        aggrs_info = torch.zeros(ctx.batch_size, 2, ctx.image_size, ctx.image_size, device=ctx.device)\n\n        soft_colors = torch.FloatTensor(ctx.batch_size, 4, ctx.image_size, ctx.image_size).fill_(1.0).to(device=ctx.device) \n        soft_colors[:, 0, :, :] *= background_color[0]\n        soft_colors[:, 1, :, :] *= background_color[1]\n        soft_colors[:, 2, :, :] *= background_color[2]\n\n        faces_info, aggrs_info, soft_colors = \\\n            soft_rasterize_cuda.forward(face_vertices, textures,\n                                        faces_info, aggrs_info,\n                                        soft_colors,\n                                        image_size, near, far, eps,\n                                        sigma_val, ctx.func_dist_type, ctx.dist_eps,\n                                        gamma_val, ctx.func_rgb_type, ctx.func_alpha_type,\n                                        ctx.texture_type, fill_back)\n\n        ctx.save_for_backward(face_vertices, textures, soft_colors, faces_info, aggrs_info)\n        return soft_colors\n\n    @staticmethod\n    def backward(ctx, grad_soft_colors):\n\n        face_vertices, textures, soft_colors, faces_info, aggrs_info = ctx.saved_tensors\n        image_size = ctx.image_size\n        background_color = ctx.background_color\n        near = ctx.near\n        far = ctx.far\n        eps = ctx.eps\n        sigma_val = ctx.sigma_val\n        dist_eps = ctx.dist_eps\n        gamma_val = ctx.gamma_val\n        func_dist_type = ctx.func_dist_type\n        func_rgb_type = ctx.func_rgb_type\n        func_alpha_type = ctx.func_alpha_type\n        texture_type = ctx.texture_type\n        fill_back = ctx.fill_back\n\n        grad_faces = torch.zeros_like(face_vertices, dtype=torch.float32).to(ctx.device).contiguous()\n        grad_textures = torch.zeros_like(textures, dtype=torch.float32).to(ctx.device).contiguous()\n        grad_soft_colors = grad_soft_colors.contiguous()\n\n        grad_faces, grad_textures = \\\n            soft_rasterize_cuda.backward(face_vertices, textures, soft_colors, \n                                         faces_info, aggrs_info,\n                                         grad_faces, grad_textures, grad_soft_colors, \n                                         image_size, near, far, eps,\n                                         sigma_val, func_dist_type, dist_eps,\n                                         gamma_val, func_rgb_type, func_alpha_type,\n                                         texture_type, fill_back)\n\n        return grad_faces, grad_textures, None, None, None, None, None, None, None, None, None, None, None, None, None\n\n\ndef soft_rasterize(face_vertices,\n                   textures,\n                   image_size=256, \n                   background_color=None,\n                   near=1,\n                   far=100, \n                   fill_back=True,\n                   eps=1e-3,\n                   sigma_val=1e-5,\n                   dist_func=\'euclidean\',\n                   dist_eps=1e-4,\n                   gamma_val=1e-4,\n                   aggr_func_rgb=\'softmax\',\n                   aggr_func_alpha=\'prod\',\n                   texture_type=\'surface\'):\n    if face_vertices.device == ""cpu"":\n        raise TypeError(\'Rasterize module supports only cuda Tensors\')\n\n    if background_color is None:\n        background_color = [0, 0, 0]\n\n    return SoftRasterizeFunction.apply(face_vertices, textures, image_size, \n                                       background_color, near, far,\n                                       fill_back, eps,\n                                       sigma_val, dist_func, dist_eps,\n                                       gamma_val, aggr_func_rgb, aggr_func_alpha, \n                                       texture_type)\n'"
kaolin/mathutils/geometry/__init__.py,0,b'from kaolin.mathutils.geometry.transformations import *\nfrom kaolin.mathutils.geometry.lie import *\n'
kaolin/mathutils/geometry/lie.py,23,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n\n__all__ = [\n    \'SO3Exp\'\n]\n\n\ndef SO3_hat(omega):\n    r""""""Takes in exponential coordinates (:math:`\\omega` :math:`B x 3`) and converts to\n    tangent vector representations (:math:`[\\omega]_{\\times}` :math:`B x 3 x 3`).\n\n    Args:\n        omega (torch.Tensor): Exponential coordinates\n\n    Returns:\n        omega_cross (torch.Tensor): Tangent vectors\n\n    Shape:\n        input: :math:`(B, 3)` where :math:`B` is the batchsize.\n        output: :math:`(B, 3, 3)` where :math:`B` is the batchsize.\n\n    """"""\n\n    # Note: Since this function is usually called from another layer,\n    # it is assumed that assertion checks are performed before calling.\n\n    # Batchsize\n    B = omega.shape[0]\n    # Tensor type\n    dtype = omega.dtype\n    # Device ID\n    device = omega.device\n\n    omega_cross = torch.zeros((B, 3, 3)).type(dtype).to(device)\n    omega_cross[:, 2, 1] = omega[:, 0].clone().type(dtype).to(device)\n    omega_cross[:, 1, 2] = -1 * omega[:, 0].clone().type(dtype).to(device)\n    omega_cross[:, 0, 2] = omega[:, 1].clone().type(dtype).to(device)\n    omega_cross[:, 2, 0] = -1 * omega[:, 1].clone().type(dtype).to(device)\n    omega_cross[:, 1, 0] = omega[:, 2].clone().type(dtype).to(device)\n    omega_cross[:, 0, 1] = -1 * omega[:, 2].clone().type(dtype).to(device)\n\n    return omega_cross\n\n    # """"""\n    # Another way to do it, but using torch.cross\n    # """"""\n    # # Canonical axes\n    # e1 = torch.Tensor([[1, 0, 0]]).type(dtype).to(device)\n    # e1 = e1.expand((B, 3))\n    # e2 = torch.Tensor([[0, 1, 0]]).type(dtype).to(device)\n    # e2 = e2.expand((B, 3))\n    # e3 = torch.Tensor([[0, 0, 1]]).type(dtype).to(device)\n    # e3 = e3.expand((B, 3))\n\n    # # Construct omega_cross, by concatenating the cross products\n    # # of each element of the batch with the canonical axes\n    # c1 = torch.cross(omega, e1, dim=1).unsqueeze(2)\n    # c2 = torch.cross(omega, e2, dim=1).unsqueeze(2)\n    # c3 = torch.cross(omega, e3, dim=1).unsqueeze(2)\n\n    # e1.requires_grad = True\n    # e2.requires_grad = True\n    # e3.requires_grad = True\n\n    # return torch.cat([c1, c2, c3], dim=2)\n\n\nclass SO3Exp(nn.Module):\n    r""""""Exponential map for SO(3), i.e., for 3 x 3 rotation matrices.\n\n    Map a batch of so(3) vectors (axis-angle) to 3D rotation matrices.\n\n    Args:\n        x (torch.Tensor): Input so(3) exponential coordinates\n        eps (float, optional): Threshold to determine which angles are deemed \'small\'\n\n    Returns:\n        x (torch.Tensor): Output SO(3) rotation matrices\n\n    Shape:\n        input: :math:`(B, 3)` where :math:`B` is the batchsize.\n        output: :math:`(B, 3, 3)` where :math:`B` is the batchsize.\n\n    Example:\n        >>> omega = torch.Tensor([[1, 2, 3], [4, 5, 6]])\n        >>> so3exp = SO3Exp()\n        >>> print(R)\n        >>> print(torch.bmm(R, R.transpose(1,2))) # Should be nearly identity matrices\n\n    """"""\n\n    def __init__(self, eps=1e-8):\n        super(SO3Exp, self).__init__()\n        self.SO3_hat = SO3_hat\n        self.eps = eps\n\n    def forward(self, x):\n\n        if not torch.is_tensor(x):\n            raise TypeError(\'Expected torch.Tensor. Got {} instead.\'.format(\n                type(omega)))\n\n        assert x.dim() == 2, \'Expected 2-D tensor. Got {}-D.\'.format(x.dim())\n        assert x.shape[1] == 3, \'Dim 1 of tensor x must be of shape 3. Got {} instead\'.format(\n            x.shape[1])\n\n        # Compute omega_cross and omega_cross_squared\n        omega = x\n        omega_cross = SO3_hat(x)\n        omega_cross_sq = torch.bmm(omega_cross, omega_cross)\n\n        # Get angles of rotation (theta)\n        theta = torch.norm(omega, dim=1)\n\n        # Get a mask (of shape B; 0 for all angles deemed \'small\', 1 otherwise)\n        mask = theta > self.eps\n        mask = mask.type(x.dtype).to(x.device)\n\n        # Compute the coefficients in the Rodrigues formula\n        sin_theta_by_theta = mask * (torch.sin(theta) / theta)\n        one_minus_cos_theta_by_theta_sq = mask * \\\n            ((1 - torch.cos(theta)) / (theta**2))\n\n        # A = (sin(theta) / theta) * omega_cross\n        # For small angles, A = omega_cross\n        A = (sin_theta_by_theta.view(-1, 1, 1) * omega_cross) + \\\n            ((1 - mask).view(-1, 1, 1) * omega_cross)\n        # B = ((1-cos(theta))/(theta**2)) * omega_cross**2\n        # For small angles, B = 0 (we do not need to add the term in, as the mask\n        # already zeros out all small angle terms)\n        B = one_minus_cos_theta_by_theta_sq.view(-1, 1, 1) * omega_cross_sq\n        # I (identity matrix, tiled B times)\n        I = torch.eye(3).expand_as(omega_cross)\n\n        # Compute the exponential map of x\n        x = I + A + B\n\n        return x\n'"
kaolin/mathutils/geometry/transformations.py,65,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n# Kornia components:\n# Copyright (C) 2017-2019, Arraiy, Inc., all rights reserved.\n# Copyright (C) 2019-    , Open Source Vision Foundation, all rights reserved.\n# Copyright (C) 2019-    , Kornia authors, all rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport torch\n\n# Borrows from kornia\n# https://github.com/arraiyopensource/kornia\n\ndef rotx(theta, enc=\'rad\'):\n    r""""""Returns a 3D rotation matrix about the X-axis\n\n    Returns the 3 x 3 rotation matrix :math:`R` that rotates a 3D point by an angle\n    theta about the X-axis of the canonical Cartesian axes :math:`\\left[ e_1 e_2 e_3 \\right]`.\n\n    Note:\n\n    .. math::\n        e_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, e_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, e_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\n\n    Generated matrix:\n\n    .. math::\n\n        R = \\begin{bmatrix}\n                1 & 0 & 0 \\\\\n                0 & cos(\\theta) & -sin(\\theta) \\\\\n                0 & sin(\\theta) & cos(\\theta)\n            \\end{bmatrix}\n\n    Args:\n        theta (Tensor or np.array): degree of rotation (assumes radians\n            by default)\n        enc (str, choices=[\'rad\', \'deg\']): whether the angle is specified in\n            degrees (\'deg\') or radians (\'rad\'). Default: \'rad\'.\n\n    Returns:\n        Tensor: one 3 x 3 rotation matrix, for each input entry in theta\n\n    Shape:\n        - Input: :math:`(B)` (or) :math:`(B, 1)` (:math:`B` is the batchsize)\n        - Output: :math:`(B, 3, 3)`\n\n    Examples:\n        >>> # Create a random batch of angles of rotation\n        >>> theta = torch.randn(10, 1)\n        >>> # Get a 10 x 3 x 3 rotation matrix, one 3 x 3 matrix for each element\n        >>> rx = kaolin.mathutils.rotx(theta)\n        >>> # Alternatively, use rotations specified in degrees\n        >>> theta = 180 * torch.randn(10, 1)\n        >>> rx = kaolin.mathutils.rotx(theta, enc=\'deg\')\n\n    """"""\n\n    if isinstance(theta, np.ndarray) or isinstance(\n            theta, float) or isinstance(theta, int):\n        theta = torch.from_numpy(theta)\n\n    # Used an f-string here, so can maybe support only Python 3.6+\n    if not torch.is_tensor(theta):\n        raise TypeError(\'Expected type torch.Tensor for argument theta. \\\n            Got {} instead\'.format(type(theta)))\n\n    # Check that the passed tensor is 1D (or 1D-like)\n    if theta.dim() != 1:\n        assert theta.dim() == 2, \'Invalid shape. Exceeds two dimensions.\'\n        if theta.dim() == 2:\n            assert (theta.shape[0] == 1 or theta.shape[\n                    1] == 1), \'Must be 1D-like.\'\n        theta = theta.view(theta.numel())\n\n    # Raise a NotImplementedError if the input encoding is something other\n    # than \'rad\'\n    if enc != \'rad\':\n        raise NotImplementedError\n\n    # Compute the rotation matrix\n    c = torch.cos(theta)\n    s = torch.sin(theta)\n    rx = torch.zeros(theta.numel(), 3, 3)\n    rx[:, 0, 0] = torch.ones(theta.numel())\n    rx[:, 1, 1] = rx[:, 2, 2] = c\n    rx[:, 1, 2] = -s\n    rx[:, 2, 1] = s\n\n    return rx\n\n\ndef roty(theta, enc=\'rad\'):\n    r""""""Returns a 3D rotation matrix about the Y-axis\n\n    Returns the 3 x 3 rotation matrix that rotates a 3D point by an angle\n    theta about the Y-axis of the canonical Cartesian axes [e1 e2 e3].\n\n    Note:\n\n    .. math::\n        e_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, e_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, e_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\n\n    Generated matrix:\n\n    .. math::\n        R = \\begin{bmatrix}\n                cos(\\theta) & 0 & sin(\\theta) \\\\\n                0 & 1 & 0 \\\\\n                -sin(\\theta) & 0 & cos(\\theta)\n            \\end{bmatrix}\n\n    Args:\n        theta (Tensor or np.array): degree of rotation (assumes radians\n            by default)\n        enc (str, choices=[\'rad\', \'deg\']): whether the angle is specified in\n            degrees (\'deg\') or radians (\'rad\'). Default: \'rad\'.\n\n    Returns:\n        Tensor: one 3 x 3 rotation matrix, for each input entry in theta\n\n    Shape:\n        - Input: :math:`(B)` (or) :math:`(B, 1)` (:math:`B` is the batchsize)\n        - Output: :math:`(B, 3, 3)`\n\n    Examples:\n        >>> # Create a random batch of angles of rotation\n        >>> theta = torch.randn(10, 1)\n        >>> # Get a 10 x 3 x 3 rotation matrix, one 3 x 3 matrix for each element\n        >>> ry = kaolin.mathutils.roty(theta)\n        >>> # Alternatively, use rotations specified in degrees\n        >>> theta = 180 * torch.randn(10, 1)\n        >>> ry = kaolin.mathutils.roty(theta, enc=\'deg\')\n\n    """"""\n\n    if isinstance(theta, np.ndarray) or isinstance(\n            theta, float) or isinstance(theta, int):\n        theta = torch.from_numpy(theta)\n\n    # Used an f-string here, so can maybe support only Python 3.6+\n    if not torch.is_tensor(theta):\n        raise TypeError(\'Expected type torch.Tensor for argument theta. \\\n            Got {} instead\'.format(type(theta)))\n\n    # Check that the passed tensor is 1D (or 1D-like)\n    if theta.dim() != 1:\n        assert theta.dim() == 2, \'Invalid shape. Exceeds two dimensions.\'\n        if theta.dim() == 2:\n            assert (theta.shape[0] == 1 or theta.shape[\n                    1] == 1), \'Must be 1D-like.\'\n        theta = theta.view(theta.numel())\n\n    # Raise a NotImplementedError if the input encoding is something other\n    # than \'rad\'\n    if enc != \'rad\':\n        raise NotImplementedError\n\n    # Compute the rotation matrix\n    c = torch.cos(theta)\n    s = torch.sin(theta)\n    ry = torch.zeros(theta.numel(), 3, 3)\n    ry[:, 1, 1] = torch.ones(theta.numel())\n    ry[:, 0, 0] = ry[:, 2, 2] = c\n    ry[:, 2, 0] = -s\n    ry[:, 0, 2] = s\n\n    return ry\n\n\ndef rotz(theta, enc=\'rad\'):\n    r""""""Returns a 3D rotation matrix about the Z-axis\n\n    Returns the 3 x 3 rotation matrix that rotates a 3D point by an angle\n    theta about the Z-axis of the canonical Cartesian axes [e1 e2 e3].\n\n    Note:\n\n    .. math::\n        e_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, e_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, e_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\n\n    Generated matrix:\n\n    .. math::\n        R = \\begin{bmatrix}\n                cos(\\theta) & -sin(\\theta) & 0  \\\\\n                sin(\\theta) & cos(\\theta) & 0 \\\\\n                0 & 0 & 1\n            \\end{bmatrix}\n\n    Args:\n        theta (Tensor or np.array): degree of rotation (assumes radians\n            by default)\n        enc (str, choices=[\'rad\', \'deg\']): whether the angle is specified in\n            degrees (\'deg\') or radians (\'rad\'). Default: \'rad\'.\n\n    Returns:\n        Tensor: one 3 x 3 rotation matrix, for each input entry in theta\n\n    Shape:\n        - Input: :math:`(B)` (or) :math:`(B, 1)` (:math:`B` is the batchsize)\n        - Output: :math:`(B, 3, 3)`\n\n    Examples:\n        >>> # Create a random batch of angles of rotation\n        >>> theta = torch.randn(10, 1)\n        >>> # Get a 10 x 3 x 3 rotation matrix, one 3 x 3 matrix for each element\n        >>> rz = kaolin.mathutils.rotz(theta)\n        >>> # Alternatively, use rotations specified in degrees\n        >>> theta = 180 * torch.randn(10, 1)\n        >>> rz = kaolin.mathutils.rotz(theta, enc=\'deg\')\n\n    """"""\n\n    if isinstance(theta, np.ndarray) or isinstance(\n            theta, float) or isinstance(theta, int):\n        theta = torch.from_numpy(theta)\n\n    # Used an f-string here, so can maybe support only Python 3.6+\n    if not torch.is_tensor(theta):\n        raise TypeError(\'Expected type torch.Tensor for argument theta. \\\n            Got {} instead\'.format(type(theta)))\n\n    # Check that the passed tensor is 1D (or 1D-like)\n    if theta.dim() != 1:\n        assert theta.dim() == 2, \'Invalid shape. Exceeds two dimensions.\'\n        if theta.dim() == 2:\n            assert (theta.shape[0] == 1 or theta.shape[\n                    1] == 1), \'Must be 1D-like.\'\n        theta = theta.view(theta.numel())\n\n    # Raise a NotImplementedError if the input encoding is something other\n    # than \'rad\'\n    if enc != \'rad\':\n        raise NotImplementedError\n\n    # Compute the rotation matrix\n    c = torch.cos(theta)\n    s = torch.sin(theta)\n    rz = torch.zeros(theta.numel(), 3, 3)\n    rz[:, 2, 2] = torch.ones(theta.numel())\n    rz[:, 0, 0] = rz[:, 1, 1] = c\n    rz[:, 0, 1] = -s\n    rz[:, 1, 0] = s\n\n    return rz\n\n\n# Borrows from Kornia.\n# https://github.com/kornia/kornia/blob/master/kornia/geometry/conversions.py\ndef homogenize_points(pts: torch.Tensor):\n    r""""""Converts a set of points to homogeneous coordinates.\n\n    Args:\n        pts (torch.Tensor): Tensor containing points to be homogenized.\n\n    Returns:\n        (torch.Tensor): Homogeneous coordinates for pts.\n\n    Shape:\n        pts: :math:`\\cdots \\times 2` or :math:`\\cdots \\times 3`\n\n    Example:\n        >>> pts = torch.randn(2, 5, 3)\n        tensor([[[ 0.0897, -0.1876,  0.1637],\n                 [-0.1026, -0.4994,  0.8622],\n                 [-1.2909,  0.2678, -1.8021],\n                 [-0.2500,  0.3505,  0.9121],\n                 [ 0.0580,  1.4497, -0.7224]],\n\n                [[ 0.8102, -0.2467,  0.1951],\n                 [ 0.4059, -1.9658,  0.1850],\n                 [ 1.5487, -0.8154, -0.5592],\n                 [ 0.2269, -0.4137,  0.7187],\n                 [-1.1810, -2.3412, -0.4925]]])\n\n        >>> homo_pts = homogenize_points(pts)\n        tensor([[[ 0.0897, -0.1876,  0.1637,  1.0000],\n                 [-0.1026, -0.4994,  0.8622,  1.0000],\n                 [-1.2909,  0.2678, -1.8021,  1.0000],\n                 [-0.2500,  0.3505,  0.9121,  1.0000],\n                 [ 0.0580,  1.4497, -0.7224,  1.0000]],\n\n                [[ 0.8102, -0.2467,  0.1951,  1.0000],\n                 [ 0.4059, -1.9658,  0.1850,  1.0000],\n                 [ 1.5487, -0.8154, -0.5592,  1.0000],\n                 [ 0.2269, -0.4137,  0.7187,  1.0000],\n                 [-1.1810, -2.3412, -0.4925,  1.0000]]])\n\n        >>> homo_pts.shape\n        torch.Size([2, 5, 4])\n\n    """"""\n\n    if not torch.is_tensor(pts):\n        raise TypeError(\'Expected input of type torch.Tensor. \'\n                        \'Got {0} instead.\'.format(type(pts)))\n    if pts.dim() < 2:\n        raise ValueError(\'Input tensors must have at least 2 dims. \'\n                         \'Got {0} instead.\'.format(pts.dim()))\n\n    pad = torch.nn.ConstantPad1d((0, 1), 1.)\n    return pad(pts)\n\n\n# Borrows from Kornia.\n# https://github.com/kornia/kornia/blob/master/kornia/geometry/conversions.py\ndef unhomogenize_points(pts: torch.Tensor):\n    r""""""Convert a set of points from homogeneous coordinates\n    (i.e., projective space) to Euclidean space.\n\n    Usually, for each point :math:`(x, y, z, w)` for the 3D case,\n    `unhomogenize_points` returns :math:`\\left(\\frac{x}{w}, \\frac{y}{w},\n    \\frac{z}{w} \\right)`. For the special case where :math:`w` is zero,\n    `unhomogenize_points` returns :math:`(x, y, z)`, following OpenCV\'s\n    convention.\n\n    Args:\n        pts (torch.Tensor): Tensor containing points to be unhomogenized.\n\n    Shape:\n        pts: :math:`\\cdots \\times 3` or :math:`\\cdots \\times 4` (usually).\n\n    Returns:\n        (torch.Tensor): Unhomogenized coordinates for `pts`.\n\n    Examples:\n        >>> homo_pts = torch.randn(2, 5, 4)\n        tensor([[[ 0.0897, -0.1876,  0.1637,  1.0000],\n                 [-0.1026, -0.4994,  0.8622,  1.0000],\n                 [-1.2909,  0.2678, -1.8021,  1.0000],\n                 [-0.2500,  0.3505,  0.9121,  1.0000],\n                 [ 0.0580,  1.4497, -0.7224,  1.0000]],\n\n                [[ 0.8102, -0.2467,  0.1951,  1.0000],\n                 [ 0.4059, -1.9658,  0.1850,  1.0000],\n                 [ 1.5487, -0.8154, -0.5592,  1.0000],\n                 [ 0.2269, -0.4137,  0.7187,  1.0000],\n                 [-1.1810, -2.3412, -0.4925,  1.0000]]])\n\n        >>> unhomo_pts = kal.math.unhomogenize_points(homo_pts)\n        tensor([[[ 0.0897, -0.1876,  0.1637],\n                 [-0.1026, -0.4994,  0.8622],\n                 [-1.2909,  0.2678, -1.8021],\n                 [-0.2500,  0.3505,  0.9121],\n                 [ 0.0580,  1.4497, -0.7224]],\n\n                [[ 0.8102, -0.2467,  0.1951],\n                 [ 0.4059, -1.9658,  0.1850],\n                 [ 1.5487, -0.8154, -0.5592],\n                 [ 0.2269, -0.4137,  0.7187],\n                 [-1.1810, -2.3412, -0.4925]]])\n\n        >>> unhomo_pts = kal.math.unhomogenize_points(unhomo_pts)\n        tensor([[[  0.5482,  -1.1463],\n                 [ -0.1190,  -0.5792],\n                 [  0.7163,  -0.1486],\n                 [ -0.2741,   0.3843],\n                 [ -0.0803,  -2.0066]],\n\n                [[  4.1518,  -1.2645],\n                 [  2.1938, -10.6255],\n                 [ -2.7696,   1.4582],\n                 [  0.3157,  -0.5756],\n                 [  2.3977,   4.7533]]])\n\n    """"""\n\n    if not torch.is_tensor(pts):\n        raise TypeError(\'Expected input of type torch.Tensor. \'\n                        \'Got {0} instead.\'.format(type(pts)))\n    if pts.dim() < 2:\n        raise ValueError(\'Input tensors must have at least 2 dims. \'\n                         \'Got {0} instead.\'.format(pts.dim()))\n\n    # Get points with the last coordinate (scale) as 0 (points at inf).\n    w = pts[..., -1:]\n    # Determine the scale factor each point needs to be multiplied by.\n    # For points at infinity, use a scale factor 1.\n    eps = 1e-6\n    scale = torch.where(torch.abs(w) > eps, 1. / w, torch.ones_like(w))\n\n    return scale * pts[..., :-1]\n\n\n# Borrows from Kornia.\n# https://github.com/kornia/kornia/blob/master/kornia/geometry/linalg.py\ndef transform3d(pts: torch.Tensor, tform: torch.Tensor) -> torch.Tensor:\n    r""""""Transform a set of points `pts` using a general 3D transform\n    `tform`.\n\n    Args:\n        pts (torch.Tensor): Points to be transformed (shape: :math:`\\cdots\n            \\times 4`)\n        tform (torch.Tensor): A 3D projective transformation matrix.\n            (shape: :math:`4 \\times 4`)\n\n    Returns\n        (torch.Tensor): Transformed points.\n\n    """"""\n\n    if not torch.is_tensor(pts):\n        raise TypeError(\'Expected input pts to be of type torch.Tensor. \'\n                        \'Got {0} instead.\'.format(type(pts)))\n    if not torch.is_tensor(tform):\n        raise TypeError(\'Expected input tform to be of type torch.Tensor. \'\n                        \'Got {0} instead.\'.format(type(tform)))\n\n    if pts.dim() < 2:\n        raise ValueError(\'Input pts must have at least 2 dimensions. \'\n                         \'Got only {0}.\'.format(pts.dim()))\n    if pts.shape[-1] != 4:\n        raise ValueError(\'Input pts must have shape 4 in its last dimension. \'\n                         \'Got {0} instead.\'.format(pts.shape[-1]))\n    if tform.dim() < 2 or tform.shape[-1] != 4 or tform.shape[-2] != 4:\n        raise ValueError(\'Input tform must have at least 2 dimensions \'\n                         \'and the last two dims must be of shape 4. Got {0} dimensions and \'\n                         \'shape {1}.\'.format(tform.dim(), tform.shape))\n\n    # Unsqueezing at dim -3 (to handle arbitrary batchsizes)\n    # For a 2D tensor, unsqueeze(-3) is equivalent to unsqueeze(0)\n    # tform is ordered as (B, 4, 4)\n    # pts is ordered as (B, N, 3), where B: batchsize, N: num points\n    pts_tformed_homo = torch.matmul(tform.unsqueeze(-3), pts.unsqueeze(-1))\n    pts_tformed = unhomogenize_points(pts_tformed_homo.squeeze(-1))\n\n    return pts_tformed[..., :3]\n\n\n# Borrows from Kornia.\n# https://github.com/kornia/kornia/blob/master/kornia/geometry/linalg.py\ndef invert_rigid_transform_3d(tform: torch.Tensor):\n    r""""""Invert a 3D rigid body (SE(3)) transform.\n\n    Args:\n        tform (torch.Tensor): SE(3) transformation matrix (shape:\n            :math:`\\cdots \\times 4 \\times 4`)\n\n    Returns:\n        inv_tform (torch.Tensor): Inverse transformation matrix (shape:\n            :math:`\\cdots \\times 4 \\times 4`)\n    """"""\n\n    if not torch.is_tensor(tform):\n        raise TypeError(\'Expected input tform to be of type torch.Tensor. \'\n                        \'Got {0} instead.\'.format(type(tform)))\n    if tform.shape[-2, :] != (4, 4):\n        raise ValueError(\'Input tform must be of shape (..., 4, 4). \'\n                         \'Got {0} instead.\'.format(tform.shape))\n\n    # Unpack translation and rotation components\n    rot = tform[..., :3, :3]\n    trans = tform[..., :3, :3]\n\n    # Compute the inverse\n    inv_rot = torch.transpose(rot, -1, -2)\n    inv_trans = torch.matmul(-inv_rot, trans)\n\n    # Pack the inverse rotation and translation components\n    inv_trans = torch.zeros_like(tform)\n    inv_trans[..., :3, :3] = inv_rot\n    inv_trans[..., :3, 3] = inv_trans\n    inv_trans[..., -1, -1] = 1.\n\n    return inv_trans\n\n\n# Borrows from Kornia.\n# https://github.com/kornia/kornia/blob/master/kornia/geometry/linalg.py\ndef compose_transforms_3d(tforms):\n    r""""""Compose (concatenate) a series of 3D transformation matrices.\n\n    Args:\n        tforms (tuple, list): Iterable containing the transforms\n            to be composed (Each transform must be of type torch.Tensor)\n            (shape: :math:`\\cdots \\times 4 \\times 4`).\n\n    Returns:\n        cat (torch.Tensor): Concatenated transform. (shape:\n            :math:`\\cdots \\times 4 \\times 4`)\n\n    """"""\n\n    if len(tforms) == 1:\n        raise ValueError(\'Expected at least 2 transforms to compose. \'\n                         \'Got only 1.\')\n    cat = None\n    for idx, tform in enumerate(tforms):\n        if not torch.is_tensor(tform):\n            raise TypeError(\'Expected elements of tforms to be of type \'\n                            \'torch.Tensor. Got {0} at index {1}\'.format(\n                                type(tform), idx))\n        if tform.shape[-2, :] != (4, 4):\n            raise TypeError(\'Expected elements of tforms to be of shape \'\n                            \'(..., 4, 4). Got {0} at index {1}\'.format(\n                                tform.shape, idx))\n        if idx == 0:\n            cat = tform\n        else:\n            cat = torch.matmul(cat, tform)\n\n    return cat\n\n\ndef compute_camera_params(azimuth: float, elevation: float, distance: float):\n\n    theta = np.deg2rad(azimuth)\n    phi = np.deg2rad(elevation)\n\n    camY = distance * np.sin(phi)\n    temp = distance * np.cos(phi)\n    camX = temp * np.cos(theta)\n    camZ = temp * np.sin(theta)\n    cam_pos = np.array([camX, camY, camZ])\n\n    axisZ = cam_pos.copy()\n    axisY = np.array([0, 1, 0])\n    axisX = np.cross(axisY, axisZ)\n    axisY = np.cross(axisZ, axisX)\n\n    cam_mat = np.array([axisX, axisY, axisZ])\n    l2 = np.atleast_1d(np.linalg.norm(cam_mat, 2, 1))\n    l2[l2 == 0] = 1\n    cam_mat = cam_mat / np.expand_dims(l2, 1)\n\n    return torch.FloatTensor(cam_mat), torch.FloatTensor(cam_pos)\n'"
kaolin/graphics/dib_renderer/cuda/__init__.py,0,b''
kaolin/graphics/dib_renderer/rasterizer/__init__.py,0,b'from .rasterizer import *\n'
kaolin/graphics/dib_renderer/rasterizer/rasterizer.py,33,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport torch\nimport torch.nn\nimport torch.autograd\nfrom torch.autograd import Function\n\nfrom ..cuda import rasterizer as cuda_rasterizer\n\nimport numpy as np\nimport datetime\n\n\n@torch.jit.script\ndef prepare_tfpoints(tfpoints3d_bxfx9, tfpoints2d_bxfx6,\n                     multiplier: float, batch_size: int, num_faces: int, expand: float):\n    # avoid numeric error\n    tfpoints2dmul_bxfx6 = multiplier * tfpoints2d_bxfx6\n\n    # bbox\n    tfpoints2d_bxfx3x2 = tfpoints2dmul_bxfx6.view(batch_size, num_faces, 3, 2)\n    tfpoints_min = torch.min(tfpoints2d_bxfx3x2, dim=2)[0]\n    tfpoints_max = torch.max(tfpoints2d_bxfx3x2, dim=2)[0]\n    tfpointsbbox_bxfx4 = torch.cat((tfpoints_min, tfpoints_max), dim=2)\n\n    # bbox2\n    tfpoints_min = tfpoints_min - expand * multiplier\n    tfpoints_max = tfpoints_max + expand * multiplier\n    tfpointsbbox2_bxfx4 = torch.cat((tfpoints_min, tfpoints_max), dim=2)\n\n    # depth\n    _tfpoints3d_bxfx9 = tfpoints3d_bxfx9.permute(2, 0, 1)\n    tfpointsdep_bxfx1 = (_tfpoints3d_bxfx9[2, :, :] +\n                         _tfpoints3d_bxfx9[5, :, :] +\n                         _tfpoints3d_bxfx9[8, :, :]).unsqueeze(-1) / 3.0\n\n    return tfpoints2dmul_bxfx6, tfpointsbbox_bxfx4, tfpointsbbox2_bxfx4, tfpointsdep_bxfx1\n\n\nclass LinearRasterizer(Function):\n    @staticmethod\n    def forward(ctx,\n                width,\n                height,\n                tfpoints3d_bxfx9,\n                tfpoints2d_bxfx6,\n                tfnormalz_bxfx1,\n                vertex_attr_bxfx3d,\n                expand=None,\n                knum=None,\n                multiplier=None,\n                delta=None,\n                debug=False):\n\n        if expand is None:\n            expand = 0.02\n        if knum is None:\n            knum = 30\n        if multiplier is None:\n            multiplier = 1000\n        if delta is None:\n            delta = 7000\n\n        batch_size = tfpoints3d_bxfx9.shape[0]\n        num_faces = tfpoints3d_bxfx9.shape[1]\n\n        num_vertex_attr = vertex_attr_bxfx3d.shape[2] / 3\n        assert num_vertex_attr == int(\n            num_vertex_attr), \\\n            \'vertex_attr_bxfx3d has shape {} which is not a multiple of 3\' \\\n            .format(vertex_attr_bxfx3d.shape[2])\n\n        num_vertex_attr = int(num_vertex_attr)\n\n        ###################################################\n        start = datetime.datetime.now()\n\n        tfpoints2dmul_bxfx6, tfpointsbbox_bxfx4, tfpointsbbox2_bxfx4, tfpointsdep_bxfx1 = \\\n            prepare_tfpoints(tfpoints3d_bxfx9, tfpoints2d_bxfx6,\n                             multiplier, batch_size, num_faces, expand)\n\n        device = tfpoints2dmul_bxfx6.device\n\n        # output\n        tfimidxs_bxhxwx1 = torch.zeros(\n            batch_size, height, width, 1, dtype=torch.float32, device=device)\n        # set depth as very far\n        tfimdeps_bxhxwx1 = torch.full(\n            (batch_size, height, width, 1), fill_value=-1000., dtype=torch.float32, device=device)\n        tfimweis_bxhxwx3 = torch.zeros(\n            batch_size, height, width, 3, dtype=torch.float32, device=device)\n        tfims_bxhxwxd = torch.zeros(\n            batch_size, height, width, num_vertex_attr, dtype=torch.float32, device=device)\n        tfimprob_bxhxwx1 = torch.zeros(\n            batch_size, height, width, 1, dtype=torch.float32, device=device)\n\n        # intermidiate varibales\n        tfprobface = torch.zeros(\n            batch_size, height, width, knum, dtype=torch.float32, device=device)\n        tfprobcase = torch.zeros(\n            batch_size, height, width, knum, dtype=torch.float32, device=device)\n        tfprobdis = torch.zeros(batch_size, height, width,\n                                knum, dtype=torch.float32, device=device)\n        tfprobdep = torch.zeros(batch_size, height, width,\n                                knum, dtype=torch.float32, device=device)\n        tfprobacc = torch.zeros(batch_size, height, width,\n                                knum, dtype=torch.float32, device=device)\n\n        # face direction\n        tfpointsdirect_bxfx1 = tfnormalz_bxfx1.contiguous()\n        cuda_rasterizer.forward(tfpoints3d_bxfx9,\n                                tfpoints2dmul_bxfx6,\n                                tfpointsdirect_bxfx1,\n                                tfpointsbbox_bxfx4,\n                                tfpointsbbox2_bxfx4,\n                                tfpointsdep_bxfx1,\n                                vertex_attr_bxfx3d,\n                                tfimidxs_bxhxwx1,\n                                tfimdeps_bxhxwx1,\n                                tfimweis_bxhxwx3,\n                                tfprobface,\n                                tfprobcase,\n                                tfprobdis,\n                                tfprobdep,\n                                tfprobacc,\n                                tfims_bxhxwxd,\n                                tfimprob_bxhxwx1,\n                                multiplier,\n                                delta)\n\n        end = datetime.datetime.now()\n        ###################################################\n\n        debug_im = torch.zeros(batch_size, height, width, 3,\n                               dtype=torch.float32, device=device)\n\n        ctx.save_for_backward(tfims_bxhxwxd, tfimprob_bxhxwx1,\n                              tfimidxs_bxhxwx1, tfimweis_bxhxwx3,\n                              tfpoints2dmul_bxfx6, vertex_attr_bxfx3d,\n                              tfprobface, tfprobcase, tfprobdis, tfprobdep, tfprobacc,\n                              debug_im)\n\n        ctx.multiplier = multiplier\n        ctx.delta = delta\n        ctx.debug = debug\n\n        tfims_bxhxwxd.requires_grad = True\n        tfimprob_bxhxwx1.requires_grad = True\n\n        return tfims_bxhxwxd, tfimprob_bxhxwx1\n\n    @staticmethod\n    def backward(ctx, dldI_bxhxwxd, dldp_bxhxwx1):\n        tfims_bxhxwxd, tfimprob_bxhxwx1, \\\n            tfimidxs_bxhxwx1, tfimweis_bxhxwx3, \\\n            tfpoints2dmul_bxfx6, tfcolors_bxfx3d, \\\n            tfprobface, tfprobcase, tfprobdis, tfprobdep, tfprobacc, \\\n            debug_im = ctx.saved_variables\n\n        multiplier = ctx.multiplier\n        delta = ctx.delta\n        debug = ctx.debug\n        # avoid numeric error\n        # multiplier = 1000\n        # tfpoints2d_bxfx6 *= multiplier\n\n        dldp2 = torch.zeros_like(tfpoints2dmul_bxfx6)\n        dldp2_prob = torch.zeros_like(tfpoints2dmul_bxfx6)\n        dldc = torch.zeros_like(tfcolors_bxfx3d)\n        cuda_rasterizer.backward(dldI_bxhxwxd.contiguous(),\n                                 dldp_bxhxwx1.contiguous(),\n                                 tfims_bxhxwxd, tfimprob_bxhxwx1,\n                                 tfimidxs_bxhxwx1, tfimweis_bxhxwx3,\n                                 tfprobface, tfprobcase, tfprobdis, tfprobdep, tfprobacc,\n                                 tfpoints2dmul_bxfx6, tfcolors_bxfx3d,\n                                 dldp2, dldc, dldp2_prob,\n                                 debug_im, multiplier, delta)\n        if debug:\n            print(dldc[dldc > 0.1])\n            print(dldc[dldc > 0.1].shape)\n            print(dldp2[dldp2 > 0.1])\n            print(dldp2[dldp2 > 0.1].shape)\n            print(dldp2_prob[dldp2_prob > 0.1])\n            print(dldp2_prob[dldp2_prob > 0.1].shape)\n\n        return \\\n            None, \\\n            None, \\\n            None, \\\n            dldp2 + dldp2_prob, \\\n            None, \\\n            dldc, \\\n            None, \\\n            None, \\\n            None, \\\n            None, \\\n            None, \\\n            None\n\n\nlinear_rasterizer = LinearRasterizer.apply\n'"
kaolin/graphics/dib_renderer/renderer/__init__.py,0,b'from .base import *\nfrom .phongrender import *\nfrom .shrender import *\nfrom .texrender import *\nfrom .vcrender import *\n'
kaolin/graphics/dib_renderer/renderer/base.py,4,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\nfrom __future__ import division\n\nfrom ....mathutils.geometry.transformations import compute_camera_params\nfrom ..utils import perspectiveprojectionnp\nfrom .phongrender import PhongRender\nfrom .shrender import SHRender\nfrom .texrender import TexRender as Lambertian\nfrom .vcrender import VCRender\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n\nrenderers = {\'VertexColor\': VCRender, \'Lambertian\': Lambertian, \'SphericalHarmonics\': SHRender, \'Phong\': PhongRender}\n\n\nclass Renderer(nn.Module):\n\n    def __init__(self, height, width, mode=\'VertexColor\', camera_center=None,\n                 camera_up=None, camera_fov_y=None):\n        super(Renderer, self).__init__()\n        assert mode in renderers, ""Passed mode {0} must in in list of accepted modes: {1}"".format(mode, renderers)\n        self.mode = mode\n        self.renderer = renderers[mode](height, width)\n        if camera_center is None:\n            self.camera_center = np.array([0, 0, 0], dtype=np.float32)\n        if camera_up is None:\n            self.camera_up = np.array([0, 1, 0], dtype=np.float32)\n        if camera_fov_y is None:\n            self.camera_fov_y = 49.13434207744484 * np.pi / 180.0\n        self.camera_params = None\n\n    def forward(self, points, *args, **kwargs):\n\n        if self.camera_params is None:\n            print(\'Camera parameters have not been set, default perspective parameters of distance = 1, elevation = 30, azimuth = 0 are being used\')\n            self.set_look_at_parameters([0], [30], [1])\n\n        assert self.camera_params[0].shape[0] == points[0].shape[0], ""Set camera parameters batch size must equal batch size of passed points""\n\n        return self.renderer(points, self.camera_params, *args, **kwargs)\n\n    def set_look_at_parameters(self, azimuth, elevation, distance):\n\n        camera_projection_mtx = perspectiveprojectionnp(self.camera_fov_y, 1.0)\n        camera_projection_mtx = torch.FloatTensor(camera_projection_mtx).cuda()\n\n        camera_view_mtx = []\n        camera_view_shift = []\n        for a, e, d in zip(azimuth, elevation, distance):\n            mat, pos = compute_camera_params(a, e, d)\n            camera_view_mtx.append(mat)\n            camera_view_shift.append(pos)\n        camera_view_mtx = torch.stack(camera_view_mtx).cuda()\n        camera_view_shift = torch.stack(camera_view_shift).cuda()\n\n        self.camera_params = [camera_view_mtx, camera_view_shift, camera_projection_mtx]\n\n    def set_camera_parameters(self, parameters):\n        self.camera_params = parameters\n'"
kaolin/graphics/dib_renderer/renderer/phongrender.py,8,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\nfrom __future__ import division\n\nfrom ..rasterizer import linear_rasterizer\nfrom ..utils import datanormalize\nfrom .fragment_shaders.frag_phongtex import fragmentshader\nfrom .vertex_shaders.perpsective import perspective_projection\nimport torch\nimport torch.nn as nn\n\n\n##################################################################\nclass PhongRender(nn.Module):\n\n    def __init__(self, height, width):\n        super(PhongRender, self).__init__()\n\n        self.height = height\n        self.width = width\n\n        # render with point normal or not\n        self.smooth = False\n\n    def set_smooth(self, pfmtx):\n        self.smooth = True\n        self.pfmtx = torch.from_numpy(pfmtx).view(\n            1, pfmtx.shape[0], pfmtx.shape[1]).cuda()\n\n    def forward(self,\n                points,\n                cameras,\n                uv_bxpx2,\n                texture_bx3xthxtw,\n                lightdirect_bx3,\n                material_bx3x3,\n                shininess_bx1,\n                ft_fx3=None):\n\n        assert lightdirect_bx3 is not None, \'When using the Phong model, light parameters must be passed\'\n        assert material_bx3x3 is not None, \'When using the Phong model, material parameters must be passed\'\n        assert shininess_bx1 is not None, \'When using the Phong model, shininess parameters must be passed\'\n\n        ##############################################################\n        # first, MVP projection in vertexshader\n        points_bxpx3, faces_fx3 = points\n\n        # use faces_fx3 as ft_fx3 if not given\n        if ft_fx3 is None:\n            ft_fx3 = faces_fx3\n\n        # camera_rot_bx3x3, camera_pos_bx3, camera_proj_3x1 = cameras\n\n        points3d_bxfx9, points2d_bxfx6, normal_bxfx3 = \\\n            perspective_projection(points_bxpx3, faces_fx3, cameras)\n\n        ################################################################\n        # normal\n\n        # decide which faces are front and which faces are back\n        normalz_bxfx1 = normal_bxfx3[:, :, 2:3]\n        # normalz_bxfx1 = torch.abs(normalz_bxfx1)\n\n        # normalize normal\n        normal1_bxfx3 = datanormalize(normal_bxfx3, axis=2)\n\n        ####################################################\n        # smooth or not\n        if self.smooth:\n            normal_bxpx3 = torch.matmul(self.pfmtx.repeat(\n                normal_bxfx3.shape[0], 1, 1), normal_bxfx3)\n            n0 = normal_bxpx3[:, faces_fx3[:, 0], :]\n            n1 = normal_bxpx3[:, faces_fx3[:, 1], :]\n            n2 = normal_bxpx3[:, faces_fx3[:, 2], :]\n            normal_bxfx9 = torch.cat((n0, n1, n2), dim=2)\n        else:\n            normal_bxfx9 = normal_bxfx3.repeat(1, 1, 3)\n\n        ############################################################\n        # second, rasterization\n        fnum = normal1_bxfx3.shape[1]\n        bnum = normal1_bxfx3.shape[0]\n\n        # we have uv, normal, eye to interpolate\n        c0 = uv_bxpx2[:, ft_fx3[:, 0], :]\n        c1 = uv_bxpx2[:, ft_fx3[:, 1], :]\n        c2 = uv_bxpx2[:, ft_fx3[:, 2], :]\n        mask = torch.ones_like(c0[:, :, :1])\n        uv_bxfx3x3 = torch.cat(\n            (c0, mask, c1, mask, c2, mask), dim=2).view(bnum, fnum, 3, -1)\n\n        # normal & eye direction\n        normal_bxfx3x3 = normal_bxfx9.view(bnum, fnum, 3, -1)\n        eyedirect_bxfx9 = -points3d_bxfx9\n        eyedirect_bxfx3x3 = eyedirect_bxfx9.view(-1, fnum, 3, 3)\n\n        feat = torch.cat(\n            (normal_bxfx3x3, eyedirect_bxfx3x3, uv_bxfx3x3), dim=3)\n        feat = feat.view(bnum, fnum, -1)\n        imfeature, improb_bxhxwx1 = linear_rasterizer(self.height, self.width,\n                                                      points3d_bxfx9, points2d_bxfx6, normalz_bxfx1, feat)\n\n        ##################################################################\n        imnormal = imfeature[:, :, :, :3]\n        imeye = imfeature[:, :, :, 3:6]\n        imtexcoords = imfeature[:, :, :, 6:8]\n        immask = imfeature[:, :, :, 8:9]\n\n        # normalize\n        imnormal1 = datanormalize(imnormal, axis=3)\n        lightdirect_bx3 = datanormalize(lightdirect_bx3, axis=1)\n        imeye1 = datanormalize(imeye, axis=3)\n\n        imrender = fragmentshader(imnormal1,\n                                  lightdirect_bx3,\n                                  imeye1,\n                                  material_bx3x3, shininess_bx1,\n                                  imtexcoords, texture_bx3xthxtw, immask)\n\n        return imrender, improb_bxhxwx1, normal1_bxfx3\n'"
kaolin/graphics/dib_renderer/renderer/shrender.py,7,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\nfrom __future__ import division\n\nfrom ..rasterizer import linear_rasterizer\nfrom ..utils import datanormalize\nfrom .fragment_shaders.frag_shtex import fragmentshader\nfrom .vertex_shaders.perpsective import perspective_projection\nimport torch\nimport torch.nn as nn\n\n\n##################################################################\nclass SHRender(nn.Module):\n\n    def __init__(self, height, width):\n        super(SHRender, self).__init__()\n\n        self.height = height\n        self.width = width\n\n        # render with point normal or not\n        self.smooth = False\n\n    def set_smooth(self, pfmtx):\n        self.smooth = True\n        self.pfmtx = pfmtx\n\n    def forward(self,\n                points,\n                cameras,\n                uv_bxpx2,\n                texture_bx3xthxtw,\n                lightparam,\n                ft_fx3=None):\n\n        assert lightparam is not None, \'When using the Spherical Harmonics model, light parameters must be passed\'\n\n        ##############################################################\n        # first, MVP projection in vertexshader\n        points_bxpx3, faces_fx3 = points\n\n        # use faces_fx3 as ft_fx3 if not given\n        if ft_fx3 is None:\n            ft_fx3 = faces_fx3\n\n        # camera_rot_bx3x3, camera_pos_bx3, camera_proj_3x1 = cameras\n\n        points3d_bxfx9, points2d_bxfx6, normal_bxfx3 = \\\n            perspective_projection(points_bxpx3, faces_fx3, cameras)\n\n        ################################################################\n        # normal\n\n        # decide which faces are front and which faces are back\n        normalz_bxfx1 = normal_bxfx3[:, :, 2:3]\n        # normalz_bxfx1 = torch.abs(normalz_bxfx1)\n\n        # normalize normal\n        normal1_bxfx3 = datanormalize(normal_bxfx3, axis=2)\n\n        ####################################################\n        # smooth or not\n        if self.smooth:\n            normal_bxpx3 = torch.matmul(self.pfmtx, normal_bxfx3)\n            n0 = normal_bxpx3[:, faces_fx3[:, 0], :]\n            n1 = normal_bxpx3[:, faces_fx3[:, 1], :]\n            n2 = normal_bxpx3[:, faces_fx3[:, 2], :]\n            normal_bxfx9 = torch.cat((n0, n1, n2), dim=2)\n        else:\n            normal_bxfx9 = normal_bxfx3.repeat(1, 1, 3)\n\n        #########################################################\n        # second, rasterization\n        fnum = normal1_bxfx3.shape[1]\n        bnum = normal1_bxfx3.shape[0]\n\n        c0 = uv_bxpx2[:, ft_fx3[:, 0], :]\n        c1 = uv_bxpx2[:, ft_fx3[:, 1], :]\n        c2 = uv_bxpx2[:, ft_fx3[:, 2], :]\n        mask = torch.ones_like(c0[:, :, :1])\n        uv_bxfx3x3 = torch.cat(\n            (c0, mask, c1, mask, c2, mask), dim=2).view(bnum, fnum, 3, -1)\n\n        # normal\n        normal_bxfx3x3 = normal_bxfx9.view(bnum, fnum, 3, -1)\n        feat = torch.cat((normal_bxfx3x3, uv_bxfx3x3), dim=3)\n        feat = feat.view(bnum, fnum, -1)\n\n        imfeat, improb_bxhxwx1 = linear_rasterizer(self.height, self.width,\n                                                   points3d_bxfx9, points2d_bxfx6, normalz_bxfx1, feat)\n        imnormal_bxhxwx3 = imfeat[:, :, :, :3]\n        imtexcoords = imfeat[:, :, :, 3:5]\n        hardmask = imfeat[:, :, :, 5:]\n\n        ####################################################\n        # fragrement shader\n        # parallel light\n        imnormal1_bxhxwx3 = datanormalize(imnormal_bxhxwx3, axis=3)\n        imrender = fragmentshader(\n            imnormal1_bxhxwx3, lightparam, imtexcoords, texture_bx3xthxtw, hardmask)\n\n        return imrender, improb_bxhxwx1, normal1_bxfx3\n'"
kaolin/graphics/dib_renderer/renderer/texrender.py,4,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\nfrom __future__ import division\n\nfrom ..rasterizer import linear_rasterizer\nfrom ..utils import datanormalize\nfrom .fragment_shaders.frag_tex import fragmentshader\nfrom .vertex_shaders.perpsective import perspective_projection\nimport torch\nimport torch.nn as nn\n\n\n##################################################################\nclass TexRender(nn.Module):\n\n    def __init__(self, height, width, filtering=\'nearest\'):\n        super(TexRender, self).__init__()\n\n        self.height = height\n        self.width = width\n        self.filtering = filtering\n\n    def forward(self,\n                points,\n                cameras,\n                uv_bxpx2,\n                texture_bx3xthxtw,\n                ft_fx3=None):\n\n        ##############################################################\n        # first, MVP projection in vertexshader\n        points_bxpx3, faces_fx3 = points\n\n        # use faces_fx3 as ft_fx3 if not given\n        if ft_fx3 is None:\n            ft_fx3 = faces_fx3\n\n        # camera_rot_bx3x3, camera_pos_bx3, camera_proj_3x1 = cameras\n\n        points3d_bxfx9, points2d_bxfx6, normal_bxfx3 = \\\n            perspective_projection(points_bxpx3, faces_fx3, cameras)\n\n        ################################################################\n        # normal\n\n        # decide which faces are front and which faces are back\n        normalz_bxfx1 = normal_bxfx3[:, :, 2:3]\n        # normalz_bxfx1 = torch.abs(normalz_bxfx1)\n\n        # normalize normal\n        normal1_bxfx3 = datanormalize(normal_bxfx3, axis=2)\n\n        ############################################################\n        # second, rasterization\n        c0 = uv_bxpx2[:, ft_fx3[:, 0], :]\n        c1 = uv_bxpx2[:, ft_fx3[:, 1], :]\n        c2 = uv_bxpx2[:, ft_fx3[:, 2], :]\n        mask = torch.ones_like(c0[:, :, :1])\n        uv_bxfx9 = torch.cat((c0, mask, c1, mask, c2, mask), dim=2)\n\n        imfeat, improb_bxhxwx1 = linear_rasterizer(\n            self.height,\n            self.width,\n            points3d_bxfx9,\n            points2d_bxfx6,\n            normalz_bxfx1,\n            uv_bxfx9\n        )\n\n        imtexcoords = imfeat[:, :, :, :2]\n        hardmask = imfeat[:, :, :, 2:3]\n\n        # fragrement shader\n        imrender = fragmentshader(imtexcoords, texture_bx3xthxtw, hardmask,\n                                  filtering=self.filtering)\n\n        return imrender, improb_bxhxwx1, normal1_bxfx3\n'"
kaolin/graphics/dib_renderer/renderer/vcrender.py,4,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\nfrom __future__ import division\n\nfrom ..rasterizer import linear_rasterizer\nfrom ..utils import datanormalize\nfrom .vertex_shaders.perpsective import perspective_projection\nimport torch\nimport torch.nn as nn\n\n\n##################################################################\nclass VCRender(nn.Module):\n\n    def __init__(self, height, width):\n        super(VCRender, self).__init__()\n\n        self.height = height\n        self.width = width\n\n    def forward(self, points, cameras, colors_bxpx3):\n\n        ##############################################################\n        # first, MVP projection in vertexshader\n        points_bxpx3, faces_fx3 = points\n\n        # camera_rot_bx3x3, camera_pos_bx3, camera_proj_3x1 = cameras\n\n        points3d_bxfx9, points2d_bxfx6, normal_bxfx3 = \\\n            perspective_projection(points_bxpx3, faces_fx3, cameras)\n\n        ################################################################\n        # normal\n\n        # decide which faces are front and which faces are back\n        normalz_bxfx1 = normal_bxfx3[:, :, 2:3]\n        # normalz_bxfx1 = torch.abs(normalz_bxfx1)\n\n        # normalize normal\n        normal1_bxfx3 = datanormalize(normal_bxfx3, axis=2)\n\n        ############################################################\n        # second, rasterization\n        c0 = colors_bxpx3[:, faces_fx3[:, 0], :]\n        c1 = colors_bxpx3[:, faces_fx3[:, 1], :]\n        c2 = colors_bxpx3[:, faces_fx3[:, 2], :]\n        mask = torch.ones_like(c0[:, :, :1])\n        color_bxfx12 = torch.cat((c0, mask, c1, mask, c2, mask), dim=2)\n\n        imfeat, improb_bxhxwx1 = linear_rasterizer(\n            self.height,\n            self.width,\n            points3d_bxfx9,\n            points2d_bxfx6,\n            normalz_bxfx1,\n            color_bxfx12\n        )\n\n        imrender = imfeat[:, :, :, :3]\n        hardmask = imfeat[:, :, :, 3:]\n\n        return imrender, improb_bxhxwx1, normal1_bxfx3\n'"
kaolin/graphics/dib_renderer/test/test_phong.py,19,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nimport torch\nimport torch.nn\nimport torch.autograd\nfrom torch.autograd import Function\n\nimport numpy as np\nimport cv2\n\n######################################################\nif __name__ == \'__main__\':\n\n    from graphics.utils.utils_mesh import loadobj, face2pfmtx, loadobjtex\n    from graphics.utils.utils_perspective import lookatnp, perspectiveprojectionnp\n    from graphics.utils.utils_sphericalcoord import get_spherical_coords_x\n\n    from graphics.render.phongrender import PhongRender\n\n    meshfile = \'./data/teapot.obj\'\n    p, f, uv, ft = loadobjtex(meshfile)\n    print(p.shape, f.shape, uv.shape, ft.shape)\n    exit()\n    pfmtx = face2pfmtx(f)\n\n    imfile = \'./data/banana.jpg\'\n    texturenp = cv2.imread(imfile)[:, :, ::-1].astype(np.float32) / 255.0\n\n    ##################################################################\n    pmax = np.max(p, axis=0, keepdims=True)\n    pmin = np.min(p, axis=0, keepdims=True)\n    pmiddle = (pmax + pmin) / 2\n    p = p - pmiddle\n\n    coef = 5\n    p = p * coef\n\n    ##########################################################\n    campos = np.array([0, 0, 1.5], dtype=np.float32)  # where camera it is\n    camcenter = np.array([0, 0, 0], dtype=np.float32)  # where camra is looking at\n    camup = np.array([-1, 1, 0], dtype=np.float32)  # y axis of camera view\n    camviewmtx, camviewshift = lookatnp(campos.reshape(3, 1), camcenter.reshape(3, 1), camup.reshape(3, 1))\n    camviewshift = -np.dot(camviewmtx.transpose(), camviewshift)\n\n    camfovy = 45 / 180.0 * np.pi\n    camprojmtx = perspectiveprojectionnp(camfovy, 1.0 * 1.0 / 1.0)\n\n    #####################################################\n    tfp_px3 = torch.from_numpy(p)\n    tfp_px3.requires_grad = True\n\n    tff_fx3 = torch.from_numpy(f)\n\n    tfuv_tx2 = torch.from_numpy(uv)\n    tfuv_tx2.requires_grad = True\n    tfft_fx3 = torch.from_numpy(ft)\n\n    tftex_thxtwx3 = torch.from_numpy(np.ascontiguousarray(texturenp))\n    tftex_thxtwx3.requires_grad = True\n\n    tfcamviewmtx = torch.from_numpy(camviewmtx)\n    tfcamshift = torch.from_numpy(camviewshift)\n    tfcamproj = torch.from_numpy(camprojmtx)\n\n    ##########################################################\n    tfp_1xpx3 = torch.unsqueeze(tfp_px3, dim=0)\n    tfuv_1xtx2 = torch.unsqueeze(tfuv_tx2, dim=0)\n    tftex_1xthxtwx3 = torch.unsqueeze(tftex_thxtwx3, dim=0)\n\n    tfcamviewmtx_1x3x3 = torch.unsqueeze(tfcamviewmtx, dim=0)\n    tfcamshift_1x3 = tfcamshift.view(-1, 3)\n    tfcamproj_3x1 = tfcamproj\n\n    bs = 10\n    tfp_bxpx3 = tfp_1xpx3.repeat([bs, 1, 1])\n    tfuv_bxtx2 = tfuv_1xtx2.repeat([bs, 1, 1])\n    tftex_bxthxtwx3 = tftex_1xthxtwx3.repeat([bs, 1, 1, 1])\n\n    tfcamviewmtx_bx3x3 = tfcamviewmtx_1x3x3.repeat([bs, 1, 1])\n    tfcamshift_bx3 = tfcamshift_1x3.repeat([bs, 1])\n    tfcameras = [tfcamviewmtx_bx3x3.cuda(),\n                 tfcamshift_bx3.cuda(),\n                 tfcamproj_3x1.cuda()]\n\n    material = np.array([[0.1, 0.1, 0.1],\n                         [1.0, 1.0, 1.0],\n                         [0.4, 0.4, 0.4]], dtype=np.float32).reshape(-1, 3, 3)\n    shininess = np.array([100], dtype=np.float32).reshape(-1, 1)\n    tfmat = torch.from_numpy(material).repeat(bs, 1, 1)\n    tfshi = torch.from_numpy(shininess).repeat(bs, 1)\n\n    lightdirect = 2 * np.random.rand(bs, 3).astype(np.float32) - 1\n    lightdirect[:, 2] += 2\n    tflight = torch.from_numpy(lightdirect)\n    tflight_bx3 = tflight\n\n    # tfcameras = None\n    tftex_bx3xthxtw = tftex_bxthxtwx3.permute([0, 3, 1, 2])\n    renderer = PhongRender(256, 256)\n    renderer.set_smooth(pfmtx)\n    tfim_bxhxwx3, _, _ = renderer.forward(points=[tfp_bxpx3.cuda(), tff_fx3.cuda()],\n                                          cameras=tfcameras,\n                                          colors=[tfuv_bxtx2.cuda(), tfft_fx3.cuda(), tftex_bx3xthxtw.cuda()],\n                                          lightdirect_bx3=tflight_bx3.cuda(),\n                                          material_bx3x3=tfmat.cuda(),\n                                          shininess_bx1=tfshi.cuda())\n\n    loss1 = torch.sum(tfim_bxhxwx3)\n    print(\'loss im {}\', format(loss1.item()))\n\n    for i in range(bs):\n        im_hxwx3 = tfim_bxhxwx3.detach().cpu().numpy()[i]\n        cv2.imshow("""", im_hxwx3[:, :, ::-1])\n        cv2.waitKey()\n\n    loss1.backward()\n\n    print(tfp_px3.grad[tfp_px3.grad > 0])\n\n    np.save(file=\'gt.npy\', arr=im_hxwx3)\n'"
kaolin/graphics/dib_renderer/test/test_sh.py,17,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nimport torch\nimport torch.nn\nimport torch.autograd\nfrom torch.autograd import Function\n\nimport numpy as np\nimport cv2\n\n######################################################\nif __name__ == \'__main__\':\n\n    from graphics.utils.utils_mesh import loadobj, face2pfmtx, loadobjtex\n    from graphics.utils.utils_perspective import lookatnp, perspectiveprojectionnp\n    from graphics.utils.utils_sphericalcoord import get_spherical_coords_x\n\n    from graphics.render.shrender import SHRender\n\n    meshfile = \'./data/banana.obj\'\n    p, f, uv, ft = loadobjtex(meshfile)\n    pfmtx = face2pfmtx(f)\n\n    \'\'\'\n    p, f = loadobj(\'2.obj\')\n    uv = get_spherical_coords_x(p)\n    uv[:, 0] = -uv[:, 0]\n    ft = f\n    \'\'\'\n\n    # make it fit pytorch coordinate\n    # uv[:, 1] = 1 - uv[:, 1]\n    # uv = uv * 2 - 1\n\n    imfile = \'./data/banana.jpg\'\n    texturenp = cv2.imread(imfile)[:, :, ::-1].astype(np.float32) / 255.0\n\n    ##################################################################\n    pmax = np.max(p, axis=0, keepdims=True)\n    pmin = np.min(p, axis=0, keepdims=True)\n    pmiddle = (pmax + pmin) / 2\n    p = p - pmiddle\n\n    coef = 5\n    p = p * coef\n\n    ##########################################################\n    campos = np.array([0, 0, 1.5], dtype=np.float32)  # where camera it is\n    camcenter = np.array([0, 0, 0], dtype=np.float32)  # where camra is looking at\n    camup = np.array([-1, 1, 0], dtype=np.float32)  # y axis of camera view\n    camviewmtx, camviewshift = lookatnp(campos.reshape(3, 1), camcenter.reshape(3, 1), camup.reshape(3, 1))\n    camviewshift = -np.dot(camviewmtx.transpose(), camviewshift)\n\n    camfovy = 45 / 180.0 * np.pi\n    camprojmtx = perspectiveprojectionnp(camfovy, 1.0 * 1.0 / 1.0)\n\n    #####################################################\n    tfp_px3 = torch.from_numpy(p)\n    tfp_px3.requires_grad = True\n\n    tff_fx3 = torch.from_numpy(f)\n\n    tfuv_tx2 = torch.from_numpy(uv)\n    tfuv_tx2.requires_grad = True\n    tfft_fx3 = torch.from_numpy(ft)\n\n    tftex_thxtwx3 = torch.from_numpy(np.ascontiguousarray(texturenp))\n    tftex_thxtwx3.requires_grad = True\n\n    tfcamviewmtx = torch.from_numpy(camviewmtx)\n    tfcamshift = torch.from_numpy(camviewshift)\n    tfcamproj = torch.from_numpy(camprojmtx)\n\n    ##########################################################\n    tfp_1xpx3 = torch.unsqueeze(tfp_px3, dim=0)\n    tfuv_1xtx2 = torch.unsqueeze(tfuv_tx2, dim=0)\n    tftex_1xthxtwx3 = torch.unsqueeze(tftex_thxtwx3, dim=0)\n\n    tfcamviewmtx_1x3x3 = torch.unsqueeze(tfcamviewmtx, dim=0)\n    tfcamshift_1x3 = tfcamshift.view(-1, 3)\n    tfcamproj_3x1 = tfcamproj\n\n    bs = 10\n    tfp_bxpx3 = tfp_1xpx3.repeat([bs, 1, 1])\n    tfuv_bxtx2 = tfuv_1xtx2.repeat([bs, 1, 1])\n    tftex_bxthxtwx3 = tftex_1xthxtwx3.repeat([bs, 1, 1, 1])\n\n    tfcamviewmtx_bx3x3 = tfcamviewmtx_1x3x3.repeat([bs, 1, 1])\n    tfcamshift_bx3 = tfcamshift_1x3.repeat([bs, 1])\n    tfcameras = [tfcamviewmtx_bx3x3.cuda(),\n                 tfcamshift_bx3.cuda(),\n                 tfcamproj_3x1.cuda()]\n\n    lightparam = np.random.rand(bs, 9).astype(np.float32)\n    lightparam[:, 0] += 2\n    tflight = torch.from_numpy(lightparam)\n    tflight_bx9 = tflight\n\n    # tfcameras = None\n    tftex_bx3xthxtw = tftex_bxthxtwx3.permute([0, 3, 1, 2])\n    renderer = SHRender(256, 256)\n    tfim_bxhxwx3, _, _ = renderer.forward(points=[tfp_bxpx3.cuda(), tff_fx3.cuda()],\n                                          cameras=tfcameras,\n                                          colors=[tfuv_bxtx2.cuda(), tfft_fx3.cuda(), tftex_bx3xthxtw.cuda()],\n                                          lightparam=tflight_bx9.cuda())\n\n    loss1 = torch.sum(tfim_bxhxwx3)\n    print(\'loss im {}\', format(loss1.item()))\n\n    for i in range(bs):\n        im_hxwx3 = tfim_bxhxwx3.detach().cpu().numpy()[i]\n        cv2.imshow("""", im_hxwx3[:, :, ::-1])\n        cv2.waitKey()\n\n    loss1.backward()\n\n    print(tfp_px3.grad[tfp_px3.grad > 0])\n\n    np.save(file=\'gt.npy\', arr=im_hxwx3)\n'"
kaolin/graphics/dib_renderer/test/test_tex.py,16,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nimport torch\nimport torch.nn\nimport torch.autograd\nfrom torch.autograd import Function\n\nimport numpy as np\nimport cv2\n\n######################################################\nif __name__ == \'__main__\':\n\n    from graphics.utils.utils_mesh import loadobj, face2pfmtx, loadobjtex\n    from graphics.utils.utils_perspective import lookatnp, perspectiveprojectionnp\n    from graphics.utils.utils_sphericalcoord import get_spherical_coords_x\n\n    from graphics.render.texrender import TexRender\n\n    meshfile = \'./data/banana.obj\'\n    p, f, uv, ft = loadobjtex(meshfile)\n    pfmtx = face2pfmtx(f)\n\n    \'\'\'\n    p, f = loadobj(\'2.obj\')\n    uv = get_spherical_coords_x(p)\n    uv[:, 0] = -uv[:, 0]\n    ft = f\n    \'\'\'\n\n    # make it fit pytorch coordinate\n    # uv[:, 1] = 1 - uv[:, 1]\n    # uv = uv * 2 - 1\n\n    imfile = \'./data/banana.jpg\'\n    texturenp = cv2.imread(imfile)[:, :, ::-1].astype(np.float32) / 255.0\n\n    ##################################################################\n    pmax = np.max(p, axis=0, keepdims=True)\n    pmin = np.min(p, axis=0, keepdims=True)\n    pmiddle = (pmax + pmin) / 2\n    p = p - pmiddle\n\n    coef = 5\n    p = p * coef\n\n    ##########################################################\n    campos = np.array([0, 0, 1.5], dtype=np.float32)  # where camera it is\n    camcenter = np.array([0, 0, 0], dtype=np.float32)  # where camra is looking at\n    camup = np.array([-1, 1, 0], dtype=np.float32)  # y axis of camera view\n    camviewmtx, camviewshift = lookatnp(campos.reshape(3, 1), camcenter.reshape(3, 1), camup.reshape(3, 1))\n    camviewshift = -np.dot(camviewmtx.transpose(), camviewshift)\n\n    camfovy = 45 / 180.0 * np.pi\n    camprojmtx = perspectiveprojectionnp(camfovy, 1.0 * 1.0 / 1.0)\n\n    #####################################################\n    tfp_px3 = torch.from_numpy(p)\n    tfp_px3.requires_grad = True\n\n    tff_fx3 = torch.from_numpy(f)\n\n    tfuv_tx2 = torch.from_numpy(uv)\n    tfuv_tx2.requires_grad = True\n    tfft_fx3 = torch.from_numpy(ft)\n\n    tftex_thxtwx3 = torch.from_numpy(np.ascontiguousarray(texturenp))\n    tftex_thxtwx3.requires_grad = True\n\n    tfcamviewmtx = torch.from_numpy(camviewmtx)\n    tfcamshift = torch.from_numpy(camviewshift)\n    tfcamproj = torch.from_numpy(camprojmtx)\n\n    ##########################################################\n    tfp_1xpx3 = torch.unsqueeze(tfp_px3, dim=0)\n    tfuv_1xtx2 = torch.unsqueeze(tfuv_tx2, dim=0)\n    tftex_1xthxtwx3 = torch.unsqueeze(tftex_thxtwx3, dim=0)\n\n    tfcamviewmtx_1x3x3 = torch.unsqueeze(tfcamviewmtx, dim=0)\n    tfcamshift_1x3 = tfcamshift.view(-1, 3)\n    tfcamproj_3x1 = tfcamproj\n\n    bs = 4\n    tfp_bxpx3 = tfp_1xpx3.repeat([bs, 1, 1])\n    tfuv_bxtx2 = tfuv_1xtx2.repeat([bs, 1, 1])\n    tftex_bxthxtwx3 = tftex_1xthxtwx3.repeat([bs, 1, 1, 1])\n\n    tfcamviewmtx_bx3x3 = tfcamviewmtx_1x3x3.repeat([bs, 1, 1])\n    tfcamshift_bx3 = tfcamshift_1x3.repeat([bs, 1])\n    tfcameras = [tfcamviewmtx_bx3x3.cuda(),\n                 tfcamshift_bx3.cuda(),\n                 tfcamproj_3x1.cuda()]\n\n    # tfcameras = None\n    tftex_bx3xthxtw = tftex_bxthxtwx3.permute([0, 3, 1, 2])\n    renderer = TexRender(256, 256)\n    tfim_bxhxwx3, _, _ = renderer.forward(points=[tfp_bxpx3.cuda(), tff_fx3.cuda()],\n                                          cameras=tfcameras,\n                                          colors=[tfuv_bxtx2.cuda(), tfft_fx3.cuda(), tftex_bx3xthxtw.cuda()])\n\n    loss1 = torch.sum(tfim_bxhxwx3)\n    print(\'loss im {}\', format(loss1.item()))\n\n    im_hxwx3 = tfim_bxhxwx3.detach().cpu().numpy()[-1]\n    cv2.imshow("""", im_hxwx3[:, :, ::-1])\n    cv2.waitKey()\n\n    loss1.backward()\n\n    print(tfp_px3.grad[tfp_px3.grad > 0])\n\n    np.save(file=\'gt.npy\', arr=im_hxwx3)\n'"
kaolin/graphics/dib_renderer/test/test_vc.py,17,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nimport torch\nimport torch.nn\nimport torch.autograd\nfrom torch.autograd import Function\n\nimport numpy as np\nimport cv2\n\n######################################################\nif __name__ == \'__main__\':\n\n    from graphics.utils.utils_mesh import loadobj, face2pfmtx, loadobjtex\n    from graphics.utils.utils_perspective import lookatnp, perspectiveprojectionnp\n    from graphics.utils.utils_sphericalcoord import get_spherical_coords_x\n\n    from graphics.render.vcrender import VCRender\n\n    meshfile = \'./data/banana.obj\'\n    p, f, uv, ft = loadobjtex(meshfile)\n    pfmtx = face2pfmtx(f)\n\n    \'\'\'\n    p, f = loadobj(\'2.obj\')\n    uv = get_spherical_coords_x(p)\n    uv[:, 0] = -uv[:, 0]\n    ft = f\n    \'\'\'\n\n    # make it fit pytorch coordinate\n    # uv[:, 1] = 1 - uv[:, 1]\n    # uv = uv * 2 - 1\n\n    imfile = \'./data/banana.jpg\'\n    texturenp = cv2.imread(imfile)[:, :, ::-1].astype(np.float32) / 255.0\n\n    ##################################################################\n    pmax = np.max(p, axis=0, keepdims=True)\n    pmin = np.min(p, axis=0, keepdims=True)\n    pmiddle = (pmax + pmin) / 2\n    p = p - pmiddle\n\n    coef = 5\n    p = p * coef\n    pnum = p.shape[0]\n\n    ##########################################################\n    campos = np.array([0, 0, 1.5], dtype=np.float32)  # where camera it is\n    camcenter = np.array([0, 0, 0], dtype=np.float32)  # where camra is looking at\n    camup = np.array([-1, 1, 0], dtype=np.float32)  # y axis of camera view\n    camviewmtx, camviewshift = lookatnp(campos.reshape(3, 1), camcenter.reshape(3, 1), camup.reshape(3, 1))\n    camviewshift = -np.dot(camviewmtx.transpose(), camviewshift)\n\n    camfovy = 45 / 180.0 * np.pi\n    camprojmtx = perspectiveprojectionnp(camfovy, 1.0 * 1.0 / 1.0)\n\n    #####################################################\n    tfp_px3 = torch.from_numpy(p)\n    tfp_px3.requires_grad = True\n\n    tff_fx3 = torch.from_numpy(f)\n\n    tfuv_tx2 = torch.from_numpy(uv)\n    tfuv_tx2.requires_grad = True\n    tfft_fx3 = torch.from_numpy(ft)\n\n    tftex_thxtwx3 = torch.from_numpy(np.ascontiguousarray(texturenp))\n    tftex_thxtwx3.requires_grad = True\n\n    tfcamviewmtx = torch.from_numpy(camviewmtx)\n    tfcamshift = torch.from_numpy(camviewshift)\n    tfcamproj = torch.from_numpy(camprojmtx)\n\n    ##########################################################\n    tfp_1xpx3 = torch.unsqueeze(tfp_px3, dim=0)\n    tfuv_1xtx2 = torch.unsqueeze(tfuv_tx2, dim=0)\n    tftex_1xthxtwx3 = torch.unsqueeze(tftex_thxtwx3, dim=0)\n\n    tfcamviewmtx_1x3x3 = torch.unsqueeze(tfcamviewmtx, dim=0)\n    tfcamshift_1x3 = tfcamshift.view(-1, 3)\n    tfcamproj_3x1 = tfcamproj\n\n    bs = 4\n    tfp_bxpx3 = tfp_1xpx3.repeat([bs, 1, 1])\n    tfuv_bxtx2 = tfuv_1xtx2.repeat([bs, 1, 1])\n    tftex_bxthxtwx3 = tftex_1xthxtwx3.repeat([bs, 1, 1, 1])\n\n    tfcamviewmtx_bx3x3 = tfcamviewmtx_1x3x3.repeat([bs, 1, 1])\n    tfcamshift_bx3 = tfcamshift_1x3.repeat([bs, 1])\n    tfcameras = [tfcamviewmtx_bx3x3.cuda(),\n                 tfcamshift_bx3.cuda(),\n                 tfcamproj_3x1.cuda()]\n\n    # tfcameras = None\n    tftex_bx3xthxtw = tftex_bxthxtwx3.permute([0, 3, 1, 2])\n    renderer = VCRender(256, 256)\n    tfim_bxhxwx3, _, _ = renderer.forward(points=[tfp_bxpx3.cuda(), tff_fx3.cuda()],\n                                          cameras=tfcameras,\n                                          colors=[torch.from_numpy(np.random.rand(bs, pnum, 3).astype(np.float32)).cuda()])\n\n    loss1 = torch.sum(tfim_bxhxwx3)\n    print(\'loss im {}\', format(loss1.item()))\n\n    im_hxwx3 = tfim_bxhxwx3.detach().cpu().numpy()[-1]\n    cv2.imshow("""", im_hxwx3[:, :, ::-1])\n    cv2.waitKey()\n\n    loss1.backward()\n\n    print(tfp_px3.grad[tfp_px3.grad > 0])\n\n    np.save(file=\'gt.npy\', arr=im_hxwx3)\n'"
kaolin/graphics/dib_renderer/utils/__init__.py,0,b'from .utils import *\nfrom .mesh import *\nfrom .perspective import *\nfrom .sphericalcoord import *\n'
kaolin/graphics/dib_renderer/utils/mesh.py,4,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nimport os\nimport torch\nimport numpy as np\n\n\n##################################################################\n# faces begin from 0!!!\ndef face2edge(facenp_fx3):\n    \'\'\'\n    facenp_fx3, int32\n    return edgenp_ex2, int32\n    \'\'\'\n    f1 = facenp_fx3[:, 0:1]\n    f2 = facenp_fx3[:, 1:2]\n    f3 = facenp_fx3[:, 2:3]\n    e1 = np.concatenate((f1, f1, f2), axis=0)\n    e2 = np.concatenate((f2, f3, f3), axis=0)\n    edgenp_ex2 = np.concatenate((e1, e2), axis=1)\n    # sort & unique\n    edgenp_ex2 = np.sort(edgenp_ex2, axis=1)\n    edgenp_ex2 = np.unique(edgenp_ex2, axis=0)\n    return edgenp_ex2\n\n\ndef face2edge2(facenp_fx3, edgenp_ex2):\n    \'\'\'\n    facenp_fx3, int32\n    edgenp_ex2, int32\n    return face_fx3, int32\n    this face is indexed by edge\n    \'\'\'\n    fnum = facenp_fx3.shape[0]\n    enum = edgenp_ex2.shape[0]\n\n    edgesort = np.sort(edgenp_ex2, axis=1)\n    edgere_fx3 = np.zeros_like(facenp_fx3)\n    for i in range(fnum):\n        for j in range(3):\n            pbe, pen = facenp_fx3[i, j], facenp_fx3[i, (j + 1) % 3]\n            if pbe > pen:\n                pbe, pen = pen, pbe\n            cond = (edgesort[:, 0] == pbe) & (edgesort[:, 1] == pen)\n            idx = np.where(cond)[0]\n            edgere_fx3[i, j] = idx\n    return edgere_fx3\n\n\ndef edge2face(facenp_fx3, edgenp_ex2):\n    \'\'\'\n    facenp_fx3, int32\n    edgenp_ex2, int32\n    return edgenp_ex2, int32\n    this edge is indexed by face\n    \'\'\'\n    fnum = facenp_fx3.shape[0]\n    enum = edgenp_ex2.shape[0]\n\n    facesort = np.sort(facenp_fx3, axis=1)\n    edgesort = np.sort(edgenp_ex2, axis=1)\n    edgere_ex2 = np.zeros_like(edgesort)\n    for i in range(enum):\n        pbe, pen = edgesort[i]\n        eid = 0\n        for j in range(fnum):\n            f1, f2, f3 = facesort[j]\n            cond1 = f1 == pbe and f2 == pen\n            cond2 = f1 == pbe and f3 == pen\n            cond3 = f2 == pbe and f3 == pen\n            if cond1 or cond2 or cond3:\n                edgere_ex2[i, eid] = j\n                eid += 1\n\n    return edgere_ex2\n\n\ndef face2pneimtx(facenp_fx3):\n    \'\'\'\n    facenp_fx3, int32\n    return pointneighbourmtx, pxp, float32\n    will normalize!\n    assume it is a good mesh\n    every point has more than one neighbour\n    \'\'\'\n    pnum = np.max(facenp_fx3) + 1\n    pointneighbourmtx = np.zeros(shape=(pnum, pnum), dtype=np.float32)\n    for i in range(3):\n        be = i\n        en = (i + 1) % 3\n        idx1 = facenp_fx3[:, be]\n        idx2 = facenp_fx3[:, en]\n        pointneighbourmtx[idx1, idx2] = 1\n        pointneighbourmtx[idx2, idx1] = 1\n    pointneicount = np.sum(pointneighbourmtx, axis=1, keepdims=True)\n    assert np.all(pointneicount > 0)\n    pointneighbourmtx /= pointneicount\n    return pointneighbourmtx\n\n\ndef face2pfmtx(facenp_fx3):\n    \'\'\'\n    facenp_fx3, int32\n    reutrn pfmtx, pxf, float32\n    \'\'\'\n    pnum = np.max(facenp_fx3) + 1\n    fnum = facenp_fx3.shape[0]\n    pfmtx = np.zeros(shape=(pnum, fnum), dtype=np.float32)\n    for i, f in enumerate(facenp_fx3):\n        pfmtx[f[0], i] = 1\n        pfmtx[f[1], i] = 1\n        pfmtx[f[2], i] = 1\n    return pfmtx\n\n\n# upsample new points\ndef meshresample(pointnp_px3, facenp_fx3, edgenp_ex2):\n    p1 = pointnp_px3[edgenp_ex2[:, 0], :]\n    p2 = pointnp_px3[edgenp_ex2[:, 1], :]\n    pmid = (p1 + p2) / 2\n    point2np_px3 = np.concatenate((pointnp_px3, pmid), axis=0)\n\n    # delete f\n    # add 4 new faces\n    face2np_fx3 = []\n    pnum = np.max(facenp_fx3) + 1\n    for f in facenp_fx3:\n        p1, p2, p3 = f\n        p12 = (edgenp_ex2 == (min(p1, p2), max(p1, p2))).all(axis=1).nonzero()[0] + pnum\n        p23 = (edgenp_ex2 == (min(p2, p3), max(p2, p3))).all(axis=1).nonzero()[0] + pnum\n        p31 = (edgenp_ex2 == (min(p3, p1), max(p3, p1))).all(axis=1).nonzero()[0] + pnum\n        face2np_fx3.append([p1, p12, p31])\n        face2np_fx3.append([p12, p2, p23])\n        face2np_fx3.append([p31, p23, p3])\n        face2np_fx3.append([p12, p23, p31])\n    face2np_fx3 = np.array(face2np_fx3, dtype=np.int64)\n    return point2np_px3, face2np_fx3\n\n\ndef mtx2tfsparse(mtx):\n    m, n = mtx.shape\n    rows, cols = np.nonzero(mtx)\n    # N = rows.shape[0]\n    # value = np.ones(shape=(N,), dtype=np.float32)\n    value = mtx[rows, cols]\n    v = torch.FloatTensor(value)\n    i = torch.LongTensor(np.stack((rows, cols), axis=0))\n    tfspmtx = torch.sparse.FloatTensor(i, v, torch.Size([m, n]))\n    return tfspmtx\n\n\n################################################################\ndef loadobj(meshfile):\n\n    v = []\n    f = []\n    meshfp = open(meshfile, \'r\')\n    for line in meshfp.readlines():\n        data = line.strip().split(\' \')\n        data = [da for da in data if len(da) > 0]\n        if len(data) != 4:\n            continue\n        if data[0] == \'v\':\n            v.append([float(d) for d in data[1:]])\n        if data[0] == \'f\':\n            data = [da.split(\'/\')[0] for da in data]\n            f.append([int(d) for d in data[1:]])\n    meshfp.close()\n\n    # torch need int64\n    facenp_fx3 = np.array(f, dtype=np.int64) - 1\n    pointnp_px3 = np.array(v, dtype=np.float32)\n    return pointnp_px3, facenp_fx3\n\n\ndef loadobjcolor(meshfile):\n\n    v = []\n    vc = []\n    f = []\n    meshfp = open(meshfile, \'r\')\n    for line in meshfp.readlines():\n        data = line.strip().split(\' \')\n        data = [da for da in data if len(da) > 0]\n        if data[0] == \'v\':\n            v.append([float(d) for d in data[1:4]])\n            if len(data) == 7:\n                vc.append([float(d) for d in data[4:7]])\n        if data[0] == \'f\':\n            data = [da.split(\'/\')[0] for da in data]\n            f.append([int(d) for d in data[1:4]])\n    meshfp.close()\n\n    # torch need int64\n    facenp_fx3 = np.array(f, dtype=np.int64) - 1\n    pointnp_px3 = np.array(v, dtype=np.float32)\n    if len(vc) > 0:\n        vc = np.array(vc, dtype=np.float32)\n    else:\n        vc = np.ones_like(pointnp_px3)\n    return pointnp_px3, facenp_fx3, vc\n\n\ndef loadobjtex(meshfile):\n\n    v = []\n    vt = []\n    f = []\n    ft = []\n    meshfp = open(meshfile, \'r\')\n    for line in meshfp.readlines():\n        data = line.strip().split(\' \')\n        data = [da for da in data if len(da) > 0]\n        if not ((len(data) == 3) or (len(data) == 4) or (len(data) == 5)):\n            continue\n        if data[0] == \'v\':\n            if len(data) == 4:\n                v.append([float(d) for d in data[1:]])\n        if data[0] == \'vt\':\n            if len(data) == 3 or len(data) == 4:\n                vt.append([float(d) for d in data[1:3]])\n        if data[0] == \'f\':\n            data = [da.split(\'/\') for da in data]\n            if len(data) == 4:\n                f.append([int(d[0]) for d in data[1:]])\n                ft.append([int(d[1]) for d in data[1:]])\n            elif len(data) == 5:\n                idx1 = [1, 2, 3]\n                data1 = [data[i] for i in idx1]\n                f.append([int(d[0]) for d in data1])\n                ft.append([int(d[1]) for d in data1])\n                idx2 = [1, 3, 4]\n                data2 = [data[i] for i in idx2]\n                f.append([int(d[0]) for d in data2])\n                ft.append([int(d[1]) for d in data2])\n    meshfp.close()\n\n    # torch need int64\n    facenp_fx3 = np.array(f, dtype=np.int64) - 1\n    ftnp_fx3 = np.array(ft, dtype=np.int64) - 1\n    pointnp_px3 = np.array(v, dtype=np.float32)\n    uvs = np.array(vt, dtype=np.float32)\n    return pointnp_px3, facenp_fx3, uvs, ftnp_fx3\n\n\ndef savemesh(pointnp_px3, facenp_fx3, fname, partinfo=None):\n\n    if partinfo is None:\n        fid = open(fname, \'w\')\n        for pidx, p in enumerate(pointnp_px3):\n            pp = p\n            fid.write(\'v %f %f %f\\n\' % (pp[0], pp[1], pp[2]))\n        for f in facenp_fx3:\n            f1 = f + 1\n            fid.write(\'f %d %d %d\\n\' % (f1[0], f1[1], f1[2]))\n        fid.close()\n    else:\n        fid = open(fname, \'w\')\n        for pidx, p in enumerate(pointnp_px3):\n            if partinfo[pidx, -1] == 0:\n                pp = p\n                color = [1, 0, 0]\n            else:\n                pp = p\n                color = [0, 0, 1]\n            fid.write(\'v %f %f %f %f %f %f\\n\' % (pp[0], pp[1], pp[2], color[0], color[1], color[2]))\n        for f in facenp_fx3:\n            f1 = f + 1\n            fid.write(\'f %d %d %d\\n\' % (f1[0], f1[1], f1[2]))\n        fid.close()\n    return\n\n\ndef savemeshcolor(pointnp_px3, facenp_fx3, fname, color_px3=None):\n\n    if color_px3 is None:\n        fid = open(fname, \'w\')\n        for pidx, p in enumerate(pointnp_px3):\n            pp = p\n            fid.write(\'v %f %f %f\\n\' % (pp[0], pp[1], pp[2]))\n        for f in facenp_fx3:\n            f1 = f + 1\n            fid.write(\'f %d %d %d\\n\' % (f1[0], f1[1], f1[2]))\n        fid.close()\n    else:\n        fid = open(fname, \'w\')\n        for pidx, p in enumerate(pointnp_px3):\n            pp = p\n            color = color_px3[pidx]\n            fid.write(\'v %f %f %f %f %f %f\\n\' % (pp[0], pp[1], pp[2], color[0], color[1], color[2]))\n        for f in facenp_fx3:\n            f1 = f + 1\n            fid.write(\'f %d %d %d\\n\' % (f1[0], f1[1], f1[2]))\n        fid.close()\n    return\n\n\ndef savemeshtes(pointnp_px3, tcoords_px2, facenp_fx3, fname):\n\n    import os\n    fol, na = os.path.split(fname)\n    na, _ = os.path.splitext(na)\n\n    matname = \'%s/%s.mtl\' % (fol, na)\n    fid = open(matname, \'w\')\n    fid.write(\'newmtl material_0\\n\')\n    fid.write(\'Kd 1 1 1\\n\')\n    fid.write(\'Ka 0 0 0\\n\')\n    fid.write(\'Ks 0.4 0.4 0.4\\n\')\n    fid.write(\'Ns 10\\n\')\n    fid.write(\'illum 2\\n\')\n    fid.write(\'map_Kd %s.png\\n\' % na)\n    fid.close()\n\n    fid = open(fname, \'w\')\n    fid.write(\'mtllib %s.mtl\\n\' % na)\n\n    for pidx, p in enumerate(pointnp_px3):\n        pp = p\n        fid.write(\'v %f %f %f\\n\' % (pp[0], pp[1], pp[2]))\n\n    for pidx, p in enumerate(tcoords_px2):\n        pp = p\n        fid.write(\'vt %f %f\\n\' % (pp[0], pp[1]))\n\n    fid.write(\'usemtl material_0\\n\')\n    for f in facenp_fx3:\n        f1 = f + 1\n        fid.write(\'f %d/%d %d/%d %d/%d\\n\' % (f1[0], f1[0], f1[1], f1[1], f1[2], f1[2]))\n    fid.close()\n\n    return\n\n\ndef save_textured_mesh(directory,\n                       file_name,\n                       vertex_pos_px3,\n                       face_fx3,\n                       tex_coord_px2,\n                       normalize_tex_coord=False,\n                       flip_vertical=False,\n                       texture_bias=0.01):\n    """"""\n    Save a textured mesh.\n    Assumes the texture is *already* saved into <directory> as <file_name>.png.\n\n    Args:\n        directory (str): The path to the folder containing the mesh to be saved.\n        file_name (str): The name of the mesh to be saved (without extension).\n            <file_name>.obj and <file_name>.mtl will be saved.\n\n        vertex_pos_px3 (numpy.ndarray): An array of shape (num_points, 3).\n            Denotes the vertex position.\n\n        face_fx3 (numpy.ndarray): An array of shape (num_faces, 3).\n            Specifies, for each face, which vertices are used.\n\n        tex_coord_px2 (numpy.ndarray): An array of shape (num_points, 2).\n            Specifies the texture coordinate of each vertex.\n            Each coordinate should be in the range [0, 1] or [-1, -1].\n            If the range is [-1, -1], set normalize_tex_coord to True.\n\n            NOTE: if this array is of the same format as specified for\n            torch.nn.functional.grid_sample(), set both normalize_tex_coord\n            and flip_vertical to True.\n\n        normalize_tex_coord (bool): Whether to normalize texture coordinates,\n            from [-1, 1] to [0, 1].\n\n        flip_vertical (bool): Whether to flip the texture coordinates vertically.\n\n        texture_bias (float): If positive, trim the edge of the texture by this\n            amount to avoid artifacts.\n    """"""\n    if os.path.splitext(file_name)[1]:\n        raise ValueError(\n            \'file_name to save_textured_mesh cannot contain extension\')\n\n    if file_name.find(\' \') != -1:\n        raise ValueError(\'file_name cannot contain space\')\n\n    obj_path = os.path.join(directory, file_name + \'.obj\')\n    mtl_path = os.path.join(directory, file_name + \'.mtl\')\n\n    with open(obj_path, \'w\') as obj_file:\n        obj_file.write(\'mtllib ./{}.mtl\\n\'.format(file_name))\n\n        for pos in vertex_pos_px3:\n            obj_file.write(\'v {} {} {}\\n\'.format(pos[0], pos[1], pos[2]))\n\n        for uv in tex_coord_px2:\n            uv = uv * 0.5 + 0.5  # normalize from [-1, 1] to [0, 1]\n            uv = uv * (1.0 - texture_bias * 2.0) + texture_bias\n            obj_file.write(\'vt {} {}\\n\'.format(\n                uv[0],\n                1.0 - uv[1] if flip_vertical else uv[1]\n            ))\n\n        obj_file.write(\'usemtl material_0\\n\')\n\n        for i in range(face_fx3.shape[0]):\n            face = face_fx3[i] + 1\n            obj_file.write(\n                \'f {0}/{0} {1}/{1} {2}/{2}\\n\'.format(face[0], face[1], face[2]))\n\n    with open(mtl_path, \'w\') as mtl_file:\n        mtl_file.write(\'\'\'newmtl material_0\nKa 0.200000 0.200000 0.200000\nKd 1.000000 1.000000 1.000000\nKs 1.000000 1.000000 1.000000\nmap_Kd {}.png\'\'\'.format(file_name))\n\n    return\n\n\ndef saveobjscale(meshfile, scale, maxratio, shift=None):\n\n    mname, prefix = os.path.splitext(meshfile)\n    mnamenew = \'%s-%.2f%s\' % (mname, maxratio, prefix)\n\n    meshfp = open(meshfile, \'r\')\n    meshfp2 = open(mnamenew, \'w\')\n    for line in meshfp.readlines():\n        data = line.strip().split(\' \')\n        data = [da for da in data if len(da) > 0]\n        if len(data) != 4:\n            meshfp2.write(line)\n            continue\n        else:\n            if data[0] == \'v\':\n                p = [scale * float(d) for d in data[1:]]\n                meshfp2.write(\'v %f %f %f\\n\' % (p[0], p[1], p[2]))\n            else:\n                meshfp2.write(line)\n                continue\n\n    meshfp.close()\n    meshfp2.close()\n\n    return\n\n\nif __name__ == \'__main__\':\n    import cv2\n\n    meshjson = \'1.obj\'\n\n    # f begin from 0!!!\n    pointnp_px3, facenp_fx3 = loadobj(meshjson)\n    assert np.max(facenp_fx3) == pointnp_px3.shape[0] - 1\n    assert np.min(facenp_fx3) == 0\n\n    pointnp_px3[:, 1] -= 0.05\n    X = pointnp_px3[:, 0]\n    Y = pointnp_px3[:, 1]\n    Z = pointnp_px3[:, 2]\n    h = 248 * (Y / Z) + 111.5\n    w = -248 * (X / Z) + 111.5\n\n    height = 224\n    width = 224\n    im = np.zeros(shape=(height, width), dtype=np.uint8)\n    for cir in zip(w, h):\n        cv2.circle(im, (int(cir[0]), int(cir[1])), 3, (255, 0, 0), -1)\n    cv2.imshow(\'\', im)\n    cv2.waitKey()\n\n    # edge, neighbour and pfmtx\n    edgenp_ex2 = face2edge(facenp_fx3)\n\n    face_edgeidx_fx3 = face2edge2(facenp_fx3, edgenp_ex2)\n\n    pneimtx = face2pneimtx(facenp_fx3)\n    pfmtx = face2pfmtx(facenp_fx3)\n\n    # save\n    savemesh(pointnp_px3, facenp_fx3, \'1s.obj\')\n'"
kaolin/graphics/dib_renderer/utils/perspective.py,0,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nimport numpy as np\n\n\ndef unit(v):\n    norm = np.linalg.norm(v)\n    if norm == 0:\n        return v\n    return v / norm\n\n\ndef lookatnp(eye_3x1, center_3x1, up_3x1):\n    # 3 variables should be length 1\n    camz = center_3x1 - eye_3x1\n    camz /= np.sqrt(np.sum(camz ** 2))\n    camx = np.cross(camz[:, 0], up_3x1[:, 0]).reshape(3, 1)\n    camy = np.cross(camx[:, 0], camz[:, 0]).reshape(3, 1)\n\n    # they are not guaranteed to be 1!!!\n    mtx = np.concatenate([unit(camx), unit(camy), -camz], axis=1).transpose()\n    shift = -np.matmul(mtx, eye_3x1)\n    return mtx, shift\n\n\ndef camera_info(param):\n    theta = np.deg2rad(param[0])\n    phi = np.deg2rad(param[1])\n\n    camY = param[3] * np.sin(phi)\n    temp = param[3] * np.cos(phi)\n    camX = temp * np.cos(theta)\n    camZ = temp * np.sin(theta)\n    cam_pos = np.array([camX, camY, camZ])\n\n    axisZ = cam_pos.copy()\n    axisY = np.array([0, 1, 0], dtype=np.float32)\n    axisX = np.cross(axisY, axisZ)\n    axisY = np.cross(axisZ, axisX)\n\n    # cam_mat = np.array([axisX, axisY, axisZ])\n    cam_mat = np.array([unit(axisX), unit(axisY), unit(axisZ)])\n\n    # for verify\n    # mtx, shift = lookatnp(cam_pos_3xb.reshape(3, 1), np.zeros(shape=(3, 1), dtype=np.float32), np.array([0,1,0], dtype=np.float32).reshape(3, 1))\n    # note, it is different from lookatnp\n    # new_p = mtx * old_p + shift\n    # new_p = cam_mat * (old_p - cam_pos)\n\n    return cam_mat, cam_pos\n\n\n#####################################################\ndef perspectiveprojectionnp(fovy, ratio=1.0, near=0.01, far=10.0):\n\n    tanfov = np.tan(fovy / 2.0)\n    # top = near * tanfov\n    # right = ratio * top\n    # mtx = [near / right, 0, 0, 0, \\\n    #          0, near / top, 0, 0, \\\n    #          0, 0, -(far+near)/(far-near), -2*far*near/(far-near), \\\n    #          0, 0, -1, 0]\n    mtx = [[1.0 / (ratio * tanfov), 0, 0, 0],\n           [0, 1.0 / tanfov, 0, 0],\n           [0, 0, -(far + near) / (far - near), -2 * far * near / (far - near)],\n           [0, 0, -1.0, 0]]\n    # return np.array(mtx, dtype=np.float32)\n    return np.array([[1.0 / (ratio * tanfov)], [1.0 / tanfov], [-1]], dtype=np.float32)\n\n\n#####################################################\ndef camera_info_batch(param_bx4):\n\n    bnum = param_bx4.shape[0]\n    cam_mat_bx3x3 = []\n    cam_pos_bx3 = []\n\n    for i in range(bnum):\n        param = param_bx4[i]\n        cam_mat, cam_pos = camera_info(param)\n        cam_mat_bx3x3.append(cam_mat)\n        cam_pos_bx3.append(cam_pos)\n\n    cam_mat_bx3x3 = np.stack(cam_mat_bx3x3, axis=0)\n    cam_pos_bx3 = np.stack(cam_pos_bx3, axis=0)\n\n    return cam_mat_bx3x3, cam_pos_bx3\n'"
kaolin/graphics/dib_renderer/utils/sphericalcoord.py,0,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nimport numpy as np\n\n\n##################################################################\n# symmetric over z axis\ndef get_spherical_coords_z(X):\n    # X is N x 3\n    rad = np.linalg.norm(X, axis=1)\n    # Inclination\n    theta = np.arccos(X[:, 2] / rad)\n    # Azimuth\n    phi = np.arctan2(X[:, 1], X[:, 0])\n\n    # Normalize both to be between [-1, 1]\n    vv = (theta / np.pi) * 2 - 1\n    uu = ((phi + np.pi) / (2 * np.pi)) * 2 - 1\n    # Return N x 2\n    return np.stack([uu, vv], 1)\n\n\n# symmetric over x axis\ndef get_spherical_coords_x(X):\n    # X is N x 3\n    rad = np.linalg.norm(X, axis=1)\n    # Inclination\n    # y == 1\n    # cos = 0\n    # y == -1\n    # cos = pi\n    theta = np.arccos(X[:, 0] / rad)\n    # Azimuth\n    phi = np.arctan2(X[:, 2], X[:, 1])\n\n    # Normalize both to be between [-1, 1]\n    uu = (theta / np.pi) * 2 - 1\n    vv = ((phi + np.pi) / (2 * np.pi)) * 2 - 1\n    # Return N x 2\n    return np.stack([uu, vv], 1)\n\n\n# symmetric spherical projection\ndef get_symmetric_spherical_tex_coords(vertex_pos,\n                                       symmetry_axis=1,\n                                       up_axis=2,\n                                       front_axis=0):\n    # vertex_pos is N x 3\n    length = np.linalg.norm(vertex_pos, axis=1)\n    # Inclination\n    theta = np.arccos(vertex_pos[:, front_axis] / length)\n    # Azimuth\n    phi = np.abs(np.arctan2(vertex_pos[:, symmetry_axis],\n                            vertex_pos[:, up_axis]))\n\n    # Normalize both to be between [-1, 1]\n    uu = (theta / np.pi) * 2 - 1\n    # vv = ((phi + np.pi) / (2 * np.pi)) * 2 - 1\n    vv = (phi / np.pi) * 2 - 1\n    # Return N x 2\n    return np.stack([uu, vv], 1)\n\n\n#########################################################################\nif __name__ == \'__main__\':\n\n    from utils.utils_mesh import loadobj, savemeshtes\n    import cv2\n\n    p, f = loadobj(\'2.obj\')\n    uv = get_spherical_coords_x(p)\n    uv[:, 0] = -uv[:, 0]\n\n    uv[:, 1] = -uv[:, 1]\n    uv = (uv + 1) / 2\n    savemeshtes(p, uv, f, \'./2_x.obj\')\n\n    tex = np.zeros(shape=(256, 512, 3), dtype=np.uint8)\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    bottomLeftCornerOfText = (10, 200)\n    fontScale = 5\n    fontColor = (0, 255, 255)\n    lineType = 2\n\n    cv2.putText(tex, \'Hello World!\',\n                bottomLeftCornerOfText,\n                font,\n                fontScale,\n                fontColor,\n                lineType)\n    cv2.imshow(\'\', tex)\n    cv2.waitKey()\n    cv2.imwrite(\'2_x.png\', np.transpose(tex, [1, 0, 2]))\n'"
kaolin/graphics/dib_renderer/utils/utils.py,2,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport torch\nimport torch.nn\n\neps = 1e-15\n\n\n##################################################\ndef datanormalize(data, axis):\n    datalen = torch.sqrt(torch.sum(data ** 2, dim=axis, keepdim=True))\n    return data / (datalen + eps)\n'"
kaolin/graphics/nmr/cuda/__init__.py,0,b''
kaolin/graphics/softras/cuda/__init__.py,0,b''
kaolin/graphics/dib_renderer/renderer/fragment_shaders/__init__.py,0,b'from .frag_phongtex import *\nfrom .frag_shtex import *\nfrom .frag_tex import *\nfrom .interpolation import *\n'
kaolin/graphics/dib_renderer/renderer/fragment_shaders/frag_phongtex.py,7,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport torch\nimport torch.nn\n\nfrom .interpolation import texinterpolation\n\n\n#####################################################\ndef fragmentshader(imnormal1_bxhxwx3,\n                   lightdirect1_bx3,\n                   eyedirect1_bxhxwx3,\n                   material_bx3x3, shininess_bx1,\n                   imtexcoord_bxhxwx2, texture_bx3xthxtw,\n                   improb_bxhxwx1,\n                   ):\n    # parallel light\n    lightdirect1_bx1x1x3 = lightdirect1_bx3.view(-1, 1, 1, 3)\n\n    # lambertian\n    cosTheta_bxhxwx1 = torch.sum(imnormal1_bxhxwx3 * lightdirect1_bx1x1x3, dim=3, keepdim=True)\n    cosTheta_bxhxwx1 = torch.clamp(cosTheta_bxhxwx1, 0, 1)\n\n    # specular\n    reflect = -lightdirect1_bx1x1x3 + 2 * cosTheta_bxhxwx1 * imnormal1_bxhxwx3\n    cosAlpha_bxhxwx1 = torch.sum(reflect * eyedirect1_bxhxwx3, dim=3, keepdim=True)\n    cosAlpha_bxhxwx1 = torch.clamp(cosAlpha_bxhxwx1, 1e-5, 1)  # should not be 0 since nan error\n    cosAlpha_bxhxwx1 = torch.pow(cosAlpha_bxhxwx1, shininess_bx1.view(-1, 1, 1, 1))  # shininess should be large than 0\n\n    # simplified model\n    # light color is [1, 1, 1]\n    MatAmbColor_bx1x1x3 = material_bx3x3[:, 0:1, :].view(-1, 1, 1, 3)\n    MatDifColor_bxhxwx3 = material_bx3x3[:, 1:2, :].view(-1, 1, 1, 3) * cosTheta_bxhxwx1\n    MatSpeColor_bxhxwx3 = material_bx3x3[:, 2:3, :].view(-1, 1, 1, 3) * cosAlpha_bxhxwx1\n\n    # tex color\n    texcolor_bxhxwx3 = texinterpolation(imtexcoord_bxhxwx2, texture_bx3xthxtw)\n\n    # ambient and diffuse rely on object color while specular doesn\'t\n    color = (MatAmbColor_bx1x1x3 + MatDifColor_bxhxwx3) * texcolor_bxhxwx3 + MatSpeColor_bxhxwx3\n    color = color * improb_bxhxwx1\n\n    return torch.clamp(color, 0, 1)\n'"
kaolin/graphics/dib_renderer/renderer/fragment_shaders/frag_shtex.py,5,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport torch\nimport torch.nn\n\nfrom .interpolation import texinterpolation\n\n\ndef fragmentshader(imnormal1_bxhxwx3,\n                   lightparam_bx9,\n                   imtexcoord_bxhxwx2,\n                   texture_bx3xthxtw,\n                   improb_bxhxwx1):\n\n    # light effect\n    x = imnormal1_bxhxwx3[:, :, :, 0:1]\n    y = imnormal1_bxhxwx3[:, :, :, 1:2]\n    z = imnormal1_bxhxwx3[:, :, :, 2:3]\n\n    # spherical harmonic parameters\n    band0 = 0.2820948 * torch.ones_like(x)\n    band10 = -0.3257350 * y\n    band11 = 0.3257350 * z\n    band12 = -0.3257350 * x\n    band20 = 0.2731371 * (x * y)\n    band21 = -0.2731371 * (y * z)\n    band22 = 0.1365686 * (z * z) - 0.0788479\n    band23 = -0.1931371 * (x * z)\n    band24 = 0.1365686 * (x * x - y * y)\n\n    bands = torch.cat((band0,\n                       band10, band11, band12,\n                       band20, band21, band22, band23, band24), dim=3)\n    coef = torch.sum(bands * lightparam_bx9.view(-1, 1, 1, 9), dim=3, keepdim=True)\n\n    # tex color\n    texcolor_bxhxwx3 = texinterpolation(imtexcoord_bxhxwx2, texture_bx3xthxtw)\n\n    # merge\n    color = coef * texcolor_bxhxwx3 * improb_bxhxwx1\n\n    return torch.clamp(color, 0, 1)\n'"
kaolin/graphics/dib_renderer/renderer/fragment_shaders/frag_tex.py,2,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport torch\nimport torch.nn\n\nfrom .interpolation import texinterpolation\n\n\n################################################\ndef fragmentshader(imtexcoord_bxhxwx2,\n                   texture_bx3xthxtw,\n                   improb_bxhxwx1,\n                   filtering=\'nearest\'):\n\n    # interpolation\n    texcolor_bxhxwx3 = texinterpolation(imtexcoord_bxhxwx2,\n                                        texture_bx3xthxtw,\n                                        filtering=filtering)\n\n    # mask\n    color = texcolor_bxhxwx3 * improb_bxhxwx1\n\n    return torch.clamp(color, 0, 1)\n'"
kaolin/graphics/dib_renderer/renderer/fragment_shaders/interpolation.py,4,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport torch\nimport torch.nn\n\n\n################################################\ndef texinterpolation(imtexcoord_bxhxwx2, texture_bx3xthxtw, filtering=\'nearest\'):\n    \'\'\'\n    Note that opengl tex coord is different from pytorch coord\n    ogl coord ranges from 0 to 1, y axis is from bottom to top and it supports circular mode(-0.1 is the same as 0.9)\n    pytorch coord ranges from -1 to 1, y axis is from top to bottom and does not support circular \n\n    filtering is the same as the mode parameter for torch.nn.functional.grid_sample.\n    \'\'\'\n\n    # convert coord mode from ogl to pytorch\n    imtexcoord_bxhxwx2 = torch.remainder(imtexcoord_bxhxwx2, 1.0)\n    imtexcoord_bxhxwx2 = imtexcoord_bxhxwx2 * 2 - 1  # [0, 1] to [-1, 1]\n    imtexcoord_bxhxwx2[:, :, :, 1] = -1.0 * imtexcoord_bxhxwx2[:, :, :, 1]  # reverse y\n\n    # sample\n    texcolor = torch.nn.functional.grid_sample(texture_bx3xthxtw,\n                                               imtexcoord_bxhxwx2,\n                                               mode=filtering)\n    texcolor = texcolor.permute(0, 2, 3, 1)\n\n    return texcolor\n'"
kaolin/graphics/dib_renderer/renderer/vertex_shaders/__init__.py,0,b'from .perpsective import *\n'
kaolin/graphics/dib_renderer/renderer/vertex_shaders/perpsective.py,5,"b'# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n\n# Permission is hereby granted, free of charge, to any person obtaining a\n# copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n# DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport torch\nimport torch.nn\n\n\n##################################################\ndef perspective_projection(points_bxpx3, faces_fx3, cameras):\n\n    # perspective, use just one camera intrinc parameter\n    camera_rot_bx3x3, camera_pos_bx3, camera_proj_3x1 = cameras\n    cameratrans_rot_bx3x3 = camera_rot_bx3x3.permute(0, 2, 1)\n\n    # follow pixel2mesh!!!\n    # new_p = cam_mat * (old_p - cam_pos)\n    points_bxpx3 = points_bxpx3 - camera_pos_bx3.view(-1, 1, 3)\n    points_bxpx3 = torch.matmul(points_bxpx3, cameratrans_rot_bx3x3)\n\n    camera_proj_bx1x3 = camera_proj_3x1.view(-1, 1, 3)\n    xy_bxpx3 = points_bxpx3 * camera_proj_bx1x3\n    xy_bxpx2 = xy_bxpx3[:, :, :2] / xy_bxpx3[:, :, 2:3]\n\n    ##########################################################\n    # 1 points\n    pf0_bxfx3 = points_bxpx3[:, faces_fx3[:, 0], :]\n    pf1_bxfx3 = points_bxpx3[:, faces_fx3[:, 1], :]\n    pf2_bxfx3 = points_bxpx3[:, faces_fx3[:, 2], :]\n    points3d_bxfx9 = torch.cat((pf0_bxfx3, pf1_bxfx3, pf2_bxfx3), dim=2)\n\n    xy_f0 = xy_bxpx2[:, faces_fx3[:, 0], :]\n    xy_f1 = xy_bxpx2[:, faces_fx3[:, 1], :]\n    xy_f2 = xy_bxpx2[:, faces_fx3[:, 2], :]\n    points2d_bxfx6 = torch.cat((xy_f0, xy_f1, xy_f2), dim=2)\n\n    ######################################################\n    # 2 normals\n    v01_bxfx3 = pf1_bxfx3 - pf0_bxfx3\n    v02_bxfx3 = pf2_bxfx3 - pf0_bxfx3\n\n    # bs cannot be 3, if it is 3, we must specify dim\n    normal_bxfx3 = torch.cross(v01_bxfx3, v02_bxfx3, dim=2)\n\n    return points3d_bxfx9, points2d_bxfx6, normal_bxfx3\n'"
