file_path,api_count,code
setup.py,1,"b""from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name='odtk',\n    version='0.2.4',\n    description='Fast and accurate single shot object detector',\n    author = 'NVIDIA Corporation',\n    packages=['retinanet', 'retinanet.backbones'],\n    ext_modules=[CUDAExtension('retinanet._C',\n        ['csrc/extensions.cpp', 'csrc/engine.cpp', 'csrc/cuda/decode.cu', 'csrc/cuda/decode_rotate.cu', 'csrc/cuda/nms.cu', 'csrc/cuda/nms_iou.cu'],\n        extra_compile_args={\n            'cxx': ['-std=c++14', '-O2', '-Wall'],\n            'nvcc': [\n                '-std=c++14', '--expt-extended-lambda', '--use_fast_math', '-Xcompiler', '-Wall',\n                '-gencode=arch=compute_60,code=sm_60', '-gencode=arch=compute_61,code=sm_61',\n                '-gencode=arch=compute_70,code=sm_70', '-gencode=arch=compute_72,code=sm_72',\n                '-gencode=arch=compute_75,code=sm_75', '-gencode=arch=compute_75,code=compute_75'\n            ],\n        },\n        libraries=['nvinfer', 'nvinfer_plugin', 'nvonnxparser'])\n    ],\n    cmdclass={'build_ext': BuildExtension.with_options(no_python_abi_suffix=True)},\n    install_requires=[\n        'torch>=1.0.0a0',\n        'torchvision',\n        'apex @ git+https://github.com/NVIDIA/apex',\n        'pycocotools @ git+https://github.com/nvidia/cocoapi.git#subdirectory=PythonAPI',\n        'pillow',\n        'requests',\n    ],\n    entry_points = {'console_scripts': ['odtk=retinanet.main:main']}\n)\n"""
retinanet/box.py,89,"b""import torch\nfrom ._C import decode as decode_cuda\nfrom ._C import iou as iou_cuda\nfrom ._C import nms as nms_cuda\nimport numpy as np\nfrom .utils import order_points, rotate_boxes\n\ndef generate_anchors(stride, ratio_vals, scales_vals, angles_vals=None):\n    'Generate anchors coordinates from scales/ratios'\n\n    scales = torch.FloatTensor(scales_vals).repeat(len(ratio_vals), 1)\n    scales = scales.transpose(0, 1).contiguous().view(-1, 1)\n    ratios = torch.FloatTensor(ratio_vals * len(scales_vals))\n\n    wh = torch.FloatTensor([stride]).repeat(len(ratios), 2)\n    ws = torch.sqrt(wh[:, 0] * wh[:, 1] / ratios)\n    dwh = torch.stack([ws, ws * ratios], dim=1)\n    xy1 = 0.5 * (wh - dwh * scales)\n    xy2 = 0.5 * (wh + dwh * scales)\n    return torch.cat([xy1, xy2], dim=1)\n\n\ndef generate_anchors_rotated(stride, ratio_vals, scales_vals, angles_vals):\n    'Generate anchors coordinates from scales/ratios/angles'\n    scales = torch.FloatTensor(scales_vals).repeat(len(ratio_vals), 1) \n    scales = scales.transpose(0, 1).contiguous().view(-1, 1)\n    ratios = torch.FloatTensor(ratio_vals * len(scales_vals))\n\n    wh = torch.FloatTensor([stride]).repeat(len(ratios), 2)\n    ws = torch.round(torch.sqrt(wh[:, 0] * wh[:, 1] / ratios))\n    dwh = torch.stack([ws, torch.round(ws * ratios)], dim=1)\n    \n    xy0 = 0.5 * (wh - dwh * scales)\n    xy2 = 0.5 * (wh + dwh * scales) - 1\n    xy1 = xy0 + (xy2 - xy0) * torch.FloatTensor([0,1])\n    xy3 = xy0 + (xy2 - xy0) * torch.FloatTensor([1,0])\n    \n    angles = torch.FloatTensor(angles_vals)\n    theta = angles.repeat(xy0.size(0),1)\n    theta = theta.transpose(0,1).contiguous().view(-1,1)\n\n    xmin_ymin = xy0.repeat(int(theta.size(0)/xy0.size(0)),1)\n    xmax_ymax = xy2.repeat(int(theta.size(0)/xy2.size(0)),1)\n    widths_heights = dwh * scales\n    widths_heights = widths_heights.repeat(int(theta.size(0)/widths_heights.size(0)),1)\n\n    u = torch.stack([torch.cos(angles), torch.sin(angles)], dim=1)\n    l = torch.stack([-torch.sin(angles), torch.cos(angles)], dim=1)\n    R = torch.stack([u, l], dim=1)\n\n    xy0R = torch.matmul(R,xy0.transpose(1,0) - stride/2 + 0.5) + stride/2 - 0.5\n    xy1R = torch.matmul(R,xy1.transpose(1,0) - stride/2 + 0.5) + stride/2 - 0.5\n    xy2R = torch.matmul(R,xy2.transpose(1,0) - stride/2 + 0.5) + stride/2 - 0.5\n    xy3R = torch.matmul(R,xy3.transpose(1,0) - stride/2 + 0.5) + stride/2 - 0.5\n    \n    xy0R = xy0R.permute(0,2,1).contiguous().view(-1,2)\n    xy1R = xy1R.permute(0,2,1).contiguous().view(-1,2)\n    xy2R = xy2R.permute(0,2,1).contiguous().view(-1,2)\n    xy3R = xy3R.permute(0,2,1).contiguous().view(-1,2)\n\n    anchors_axis = torch.cat([xmin_ymin, xmax_ymax], dim=1)\n    anchors_rotated = order_points(torch.stack([xy0R,xy1R,xy2R,xy3R],dim = 1)).view(-1,8)\n\n    return anchors_axis, anchors_rotated\n\n\ndef box2delta(boxes, anchors):\n    'Convert boxes to deltas from anchors'\n\n    anchors_wh = anchors[:, 2:] - anchors[:, :2] + 1\n    anchors_ctr = anchors[:, :2] + 0.5 * anchors_wh\n    boxes_wh = boxes[:, 2:] - boxes[:, :2] + 1\n    boxes_ctr = boxes[:, :2] + 0.5 * boxes_wh\n\n    return torch.cat([\n        (boxes_ctr - anchors_ctr) / anchors_wh,\n        torch.log(boxes_wh / anchors_wh)\n    ], 1)\n\n\ndef box2delta_rotated(boxes, anchors):\n    'Convert boxes to deltas from anchors'\n\n    anchors_wh = anchors[:, 2:4] - anchors[:, :2] + 1\n    anchors_ctr = anchors[:, :2] + 0.5 * anchors_wh\n    boxes_wh = boxes[:, 2:4] - boxes[:, :2] + 1\n    boxes_ctr = boxes[:, :2] + 0.5 * boxes_wh\n    boxes_sin = boxes[:, 4]\n    boxes_cos = boxes[:, 5]\n\n    return torch.cat([\n        (boxes_ctr - anchors_ctr) / anchors_wh,\n        torch.log(boxes_wh / anchors_wh), boxes_sin[:, None], boxes_cos[:, None]\n    ], 1)\n\n\ndef delta2box(deltas, anchors, size, stride):\n    'Convert deltas from anchors to boxes'\n\n    anchors_wh = anchors[:, 2:] - anchors[:, :2] + 1\n    ctr = anchors[:, :2] + 0.5 * anchors_wh\n    pred_ctr = deltas[:, :2] * anchors_wh + ctr\n    pred_wh = torch.exp(deltas[:, 2:]) * anchors_wh\n\n    m = torch.zeros([2], device=deltas.device, dtype=deltas.dtype)\n    M = (torch.tensor([size], device=deltas.device, dtype=deltas.dtype) * stride - 1)\n    clamp = lambda t: torch.max(m, torch.min(t, M))\n    return torch.cat([\n        clamp(pred_ctr - 0.5 * pred_wh),\n        clamp(pred_ctr + 0.5 * pred_wh - 1)\n    ], 1)\n\n\ndef delta2box_rotated(deltas, anchors, size, stride):\n    'Convert deltas from anchors to boxes'\n\n    anchors_wh = anchors[:, 2:4] - anchors[:, :2] + 1\n    ctr = anchors[:, :2] + 0.5 * anchors_wh\n    pred_ctr = deltas[:, :2] * anchors_wh + ctr\n    pred_wh = torch.exp(deltas[:, 2:4]) * anchors_wh\n    pred_sin = deltas[:, 4]\n    pred_cos = deltas[:, 5]\n\n    m = torch.zeros([2], device=deltas.device, dtype=deltas.dtype)\n    M = (torch.tensor([size], device=deltas.device, dtype=deltas.dtype) * stride - 1)\n    clamp = lambda t: torch.max(m, torch.min(t, M))\n    return torch.cat([\n        clamp(pred_ctr - 0.5 * pred_wh),\n        clamp(pred_ctr + 0.5 * pred_wh - 1),\n        torch.atan2(pred_sin, pred_cos)[:, None]\n    ], 1)\n\n\ndef snap_to_anchors(boxes, size, stride, anchors, num_classes, device, anchor_ious):\n    'Snap target boxes (x, y, w, h) to anchors'\n\n    num_anchors = anchors.size()[0] if anchors is not None else 1\n    width, height = (int(size[0] / stride), int(size[1] / stride))\n\n    if boxes.nelement() == 0:\n        return (torch.zeros([num_anchors, num_classes, height, width], device=device),\n                torch.zeros([num_anchors, 4, height, width], device=device),\n                torch.zeros([num_anchors, 1, height, width], device=device))\n\n    boxes, classes = boxes.split(4, dim=1)\n\n    # Generate anchors\n    x, y = torch.meshgrid([torch.arange(0, size[i], stride, device=device, dtype=classes.dtype) for i in range(2)])\n    xyxy = torch.stack((x, y, x, y), 2).unsqueeze(0)\n    anchors = anchors.view(-1, 1, 1, 4).to(dtype=classes.dtype)\n    anchors = (xyxy + anchors).contiguous().view(-1, 4)\n\n    # Compute overlap between boxes and anchors\n    boxes = torch.cat([boxes[:, :2], boxes[:, :2] + boxes[:, 2:] - 1], 1)\n    xy1 = torch.max(anchors[:, None, :2], boxes[:, :2])\n    xy2 = torch.min(anchors[:, None, 2:], boxes[:, 2:])\n    inter = torch.prod((xy2 - xy1 + 1).clamp(0), 2)\n    boxes_area = torch.prod(boxes[:, 2:] - boxes[:, :2] + 1, 1)\n    anchors_area = torch.prod(anchors[:, 2:] - anchors[:, :2] + 1, 1)\n    overlap = inter / (anchors_area[:, None] + boxes_area - inter)\n\n    # Keep best box per anchor\n    overlap, indices = overlap.max(1)\n    box_target = box2delta(boxes[indices], anchors)\n    box_target = box_target.view(num_anchors, 1, width, height, 4)\n    box_target = box_target.transpose(1, 4).transpose(2, 3)\n    box_target = box_target.squeeze().contiguous()\n\n    depth = torch.ones_like(overlap) * -1\n    depth[overlap < anchor_ious[0]] = 0  # background\n    depth[overlap >= anchor_ious[1]] = classes[indices][overlap >= anchor_ious[1]].squeeze() + 1  # objects\n    depth = depth.view(num_anchors, width, height).transpose(1, 2).contiguous()\n\n    # Generate target classes\n    cls_target = torch.zeros((anchors.size()[0], num_classes + 1), device=device, dtype=boxes.dtype)\n    if classes.nelement() == 0:\n        classes = torch.LongTensor([num_classes], device=device).expand_as(indices)\n    else:\n        classes = classes[indices].long()\n    classes = classes.view(-1, 1)\n    classes[overlap < anchor_ious[0]] = num_classes  # background has no class\n    cls_target.scatter_(1, classes, 1)\n    cls_target = cls_target[:, :num_classes].view(-1, 1, width, height, num_classes)\n    cls_target = cls_target.transpose(1, 4).transpose(2, 3)\n    cls_target = cls_target.squeeze().contiguous()\n\n    return (cls_target.view(num_anchors, num_classes, height, width),\n            box_target.view(num_anchors, 4, height, width),\n            depth.view(num_anchors, 1, height, width))\n\n\ndef snap_to_anchors_rotated(boxes, size, stride, anchors, num_classes, device, anchor_ious):\n    'Snap target boxes (x, y, w, h, a) to anchors'\n\n    anchors_axis, anchors_rotated = anchors\n\n    num_anchors = anchors_rotated.size()[0] if anchors_rotated is not None else 1\n    width, height = (int(size[0] / stride), int(size[1] / stride))\n\n    if boxes.nelement() == 0:\n        return (torch.zeros([num_anchors, num_classes, height, width], device=device),\n                torch.zeros([num_anchors, 6, height, width], device=device),\n                torch.zeros([num_anchors, 1, height, width], device=device))\n\n    boxes, classes = boxes.split(5, dim=1)\n    boxes_axis, boxes_rotated = rotate_boxes(boxes)\n    \n    boxes_axis = boxes_axis.to(device)\n    boxes_rotated = boxes_rotated.to(device)\n    anchors_axis = anchors_axis.to(device)\n    anchors_rotated = anchors_rotated.to(device)\n\n    # Generate anchors\n    x, y = torch.meshgrid([torch.arange(0, size[i], stride, device=device, dtype=classes.dtype) for i in range(2)])\n    xy_2corners = torch.stack((x, y, x, y), 2).unsqueeze(0)\n    xy_4corners = torch.stack((x, y, x, y, x, y, x, y), 2).unsqueeze(0)\n    anchors_axis = (xy_2corners.to(torch.float) + anchors_axis.view(-1, 1, 1, 4)).contiguous().view(-1, 4)\n    anchors_rotated = (xy_4corners.to(torch.float) + anchors_rotated.view(-1, 1, 1, 8)).contiguous().view(-1, 8)\n\n    if torch.cuda.is_available():\n        iou = iou_cuda\n\n    overlap = iou(boxes_rotated.contiguous().view(-1), anchors_rotated.contiguous().view(-1))[0]\n\n    # Keep best box per anchor\n    overlap, indices = overlap.max(1)\n    box_target = box2delta_rotated(boxes_axis[indices], anchors_axis)\n    box_target = box_target.view(num_anchors, 1, width, height, 6)\n    box_target = box_target.transpose(1, 4).transpose(2, 3)\n    box_target = box_target.squeeze().contiguous()\n\n    depth = torch.ones_like(overlap, device=device) * -1\n    depth[overlap < anchor_ious[0]] = 0  # background\n    depth[overlap >= anchor_ious[1]] = classes[indices][overlap >= anchor_ious[1]].squeeze() + 1  # objects\n    depth = depth.view(num_anchors, width, height).transpose(1, 2).contiguous()\n\n    # Generate target classes\n    cls_target = torch.zeros((anchors_axis.size()[0], num_classes + 1), device=device, dtype=boxes_axis.dtype)\n    if classes.nelement() == 0:\n        classes = torch.LongTensor([num_classes], device=device).expand_as(indices)\n    else:\n        classes = classes[indices].long()\n    classes = classes.view(-1, 1)\n    classes[overlap < anchor_ious[0]] = num_classes  # background has no class\n    cls_target.scatter_(1, classes, 1)\n    cls_target = cls_target[:, :num_classes].view(-1, 1, width, height, num_classes)\n    cls_target = cls_target.transpose(1, 4).transpose(2, 3)\n    cls_target = cls_target.squeeze().contiguous()\n\n    return (cls_target.view(num_anchors, num_classes, height, width),\n            box_target.view(num_anchors, 6, height, width),\n            depth.view(num_anchors, 1, height, width))\n\n\ndef decode(all_cls_head, all_box_head, stride=1, threshold=0.05, top_n=1000, anchors=None, rotated=False):\n    'Box Decoding and Filtering'\n\n    if rotated:\n        anchors = anchors[0]\n    num_boxes = 4 if not rotated else 6\n\n    if torch.cuda.is_available():\n        return decode_cuda(all_cls_head.float(), all_box_head.float(),\n            anchors.view(-1).tolist(), stride, threshold, top_n, rotated)\n\n    device = all_cls_head.device\n    anchors = anchors.to(device).type(all_cls_head.type())\n    num_anchors = anchors.size()[0] if anchors is not None else 1\n    num_classes = all_cls_head.size()[1] // num_anchors\n    height, width = all_cls_head.size()[-2:]\n\n    batch_size = all_cls_head.size()[0]\n    out_scores = torch.zeros((batch_size, top_n), device=device)\n    out_boxes = torch.zeros((batch_size, top_n, num_boxes), device=device)\n    out_classes = torch.zeros((batch_size, top_n), device=device)\n\n    # Per item in batch\n    for batch in range(batch_size):\n        cls_head = all_cls_head[batch, :, :, :].contiguous().view(-1)\n        box_head = all_box_head[batch, :, :, :].contiguous().view(-1, num_boxes)\n\n        # Keep scores over threshold\n        keep = (cls_head >= threshold).nonzero().view(-1)\n        if keep.nelement() == 0:\n            continue\n\n        # Gather top elements\n        scores = torch.index_select(cls_head, 0, keep)\n        scores, indices = torch.topk(scores, min(top_n, keep.size()[0]), dim=0)\n        indices = torch.index_select(keep, 0, indices).view(-1)\n        classes = (indices / width / height) % num_classes\n        classes = classes.type(all_cls_head.type())\n\n        # Infer kept bboxes\n        x = indices % width\n        y = (indices / width) % height\n        a = indices / num_classes / height / width\n        box_head = box_head.view(num_anchors, num_boxes, height, width)\n        boxes = box_head[a, :, y, x]\n\n        if anchors is not None:\n            grid = torch.stack([x, y, x, y], 1).type(all_cls_head.type()) * stride + anchors[a, :]\n            boxes = delta2box(boxes, grid, [width, height], stride)\n\n        out_scores[batch, :scores.size()[0]] = scores\n        out_boxes[batch, :boxes.size()[0], :] = boxes\n        out_classes[batch, :classes.size()[0]] = classes\n\n    return out_scores, out_boxes, out_classes\n\n\ndef nms(all_scores, all_boxes, all_classes, nms=0.5, ndetections=100):\n    'Non Maximum Suppression'\n\n    if torch.cuda.is_available():\n        return nms_cuda(all_scores.float(), all_boxes.float(), all_classes.float(), \n            nms, ndetections, False)\n\n    device = all_scores.device\n    batch_size = all_scores.size()[0]\n    out_scores = torch.zeros((batch_size, ndetections), device=device)\n    out_boxes = torch.zeros((batch_size, ndetections, 4), device=device)\n    out_classes = torch.zeros((batch_size, ndetections), device=device)\n\n    # Per item in batch\n    for batch in range(batch_size):\n        # Discard null scores\n        keep = (all_scores[batch, :].view(-1) > 0).nonzero()\n        scores = all_scores[batch, keep].view(-1)\n        boxes = all_boxes[batch, keep, :].view(-1, 4)\n        classes = all_classes[batch, keep].view(-1)\n\n        if scores.nelement() == 0:\n            continue\n\n        # Sort boxes\n        scores, indices = torch.sort(scores, descending=True)\n        boxes, classes = boxes[indices], classes[indices]\n        areas = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1).view(-1)\n        keep = torch.ones(scores.nelement(), device=device, dtype=torch.uint8).view(-1)\n\n        for i in range(ndetections):\n            if i >= keep.nonzero().nelement() or i >= scores.nelement():\n                i -= 1\n                break\n\n            # Find overlapping boxes with lower score\n            xy1 = torch.max(boxes[:, :2], boxes[i, :2])\n            xy2 = torch.min(boxes[:, 2:], boxes[i, 2:])\n            inter = torch.prod((xy2 - xy1 + 1).clamp(0), 1)\n            criterion = ((scores > scores[i]) |\n                         (inter / (areas + areas[i] - inter) <= nms) |\n                         (classes != classes[i]))\n            criterion[i] = 1\n\n            # Only keep relevant boxes\n            scores = scores[criterion.nonzero()].view(-1)\n            boxes = boxes[criterion.nonzero(), :].view(-1, 4)\n            classes = classes[criterion.nonzero()].view(-1)\n            areas = areas[criterion.nonzero()].view(-1)\n            keep[(~criterion).nonzero()] = 0\n\n        out_scores[batch, :i + 1] = scores[:i + 1]\n        out_boxes[batch, :i + 1, :] = boxes[:i + 1, :]\n        out_classes[batch, :i + 1] = classes[:i + 1]\n\n    return out_scores, out_boxes, out_classes\n\n\ndef nms_rotated(all_scores, all_boxes, all_classes, nms=0.5, ndetections=100):\n    'Non Maximum Suppression'\n\n    if torch.cuda.is_available():\n        return nms_cuda(all_scores.float(), all_boxes.float(), all_classes.float(), \n            nms, ndetections, True)\n\n    device = all_scores.device\n    batch_size = all_scores.size()[0]\n    out_scores = torch.zeros((batch_size, ndetections), device=device)\n    out_boxes = torch.zeros((batch_size, ndetections, 6), device=device)\n    out_classes = torch.zeros((batch_size, ndetections), device=device)\n\n    # Per item in batch\n    for batch in range(batch_size):\n        # Discard null scores\n        keep = (all_scores[batch, :].view(-1) > 0).nonzero()\n        scores = all_scores[batch, keep].view(-1)\n        boxes = all_boxes[batch, keep, :].view(-1, 6)\n        classes = all_classes[batch, keep].view(-1)\n        theta = torch.atan2(boxes[:, -2], boxes[:, -1])\n        boxes_theta = torch.cat([boxes[:, :-2], theta[:, None]], dim=1)\n\n        if scores.nelement() == 0:\n            continue\n\n        # Sort boxes\n        scores, indices = torch.sort(scores, descending=True)\n        boxes, boxes_theta, classes = boxes[indices], boxes_theta[indices], classes[indices]\n        areas = (boxes_theta[:, 2] - boxes_theta[:, 0] + 1) * (boxes_theta[:, 3] - boxes_theta[:, 1] + 1).view(-1)\n        keep = torch.ones(scores.nelement(), device=device, dtype=torch.uint8).view(-1)\n\n        for i in range(ndetections):\n            if i >= keep.nonzero().nelement() or i >= scores.nelement():\n                i -= 1\n                break\n\n            boxes_axis, boxes_rotated = rotate_boxes(boxes_theta, points=True)\n            overlap, inter = iou(boxes_rotated.contiguous().view(-1), boxes_rotated[i, :].contiguous().view(-1))\n            inter = inter.squeeze()\n            criterion = ((scores > scores[i]) |\n                         (inter / (areas + areas[i] - inter) <= nms) |\n                         (classes != classes[i]))\n            criterion[i] = 1\n\n            # Only keep relevant boxes\n            scores = scores[criterion.nonzero()].view(-1)\n            boxes = boxes[criterion.nonzero(), :].view(-1, 6)\n            boxes_theta = boxes_theta[criterion.nonzero(), :].view(-1, 5)\n            classes = classes[criterion.nonzero()].view(-1)\n            areas = areas[criterion.nonzero()].view(-1)\n            keep[(~criterion).nonzero()] = 0\n\n        out_scores[batch, :i + 1] = scores[:i + 1]\n        out_boxes[batch, :i + 1, :] = boxes[:i + 1, :]\n        out_classes[batch, :i + 1] = classes[:i + 1]\n\n    return out_scores, out_boxes, out_classes\n"""
retinanet/dali.py,9,"b'from contextlib import redirect_stdout\nfrom math import ceil\nimport ctypes\nimport torch\nfrom nvidia.dali import pipeline, ops, types\nfrom pycocotools.coco import COCO\n\nclass COCOPipeline(pipeline.Pipeline):\n    \'Dali pipeline for COCO\'\n\n    def __init__(self, batch_size, num_threads, path, training, annotations, world, device_id, mean, std, resize,\n                 max_size, stride, rotate_augment=False,\n                 augment_brightness=0.0,\n                 augment_contrast=0.0, augment_hue=0.0,\n                 augment_saturation=0.0):\n        super().__init__(batch_size=batch_size, num_threads=num_threads, device_id=device_id,\n                         prefetch_queue_depth=num_threads, seed=42)\n        self.path = path\n        self.training = training\n        self.stride = stride\n        self.iter = 0\n\n        self.rotate_augment = rotate_augment\n        self.augment_brightness = augment_brightness\n        self.augment_contrast = augment_contrast\n        self.augment_hue = augment_hue\n        self.augment_saturation = augment_saturation\n\n        self.reader = ops.COCOReader(annotations_file=annotations, file_root=path, num_shards=world,\n                                     shard_id=torch.cuda.current_device(),\n                                     ltrb=True, ratio=True, shuffle_after_epoch=True, save_img_ids=True)\n\n        self.decode_train = ops.ImageDecoderSlice(device=""mixed"", output_type=types.RGB)\n        self.decode_infer = ops.ImageDecoder(device=""mixed"", output_type=types.RGB)\n        self.bbox_crop = ops.RandomBBoxCrop(device=\'cpu\', ltrb=True, scaling=[0.3, 1.0],\n                                            thresholds=[0.1, 0.3, 0.5, 0.7, 0.9])\n\n        self.bbox_flip = ops.BbFlip(device=\'cpu\', ltrb=True)\n        self.img_flip = ops.Flip(device=\'gpu\')\n        self.coin_flip = ops.CoinFlip(probability=0.5)\n        self.bc = ops.BrightnessContrast(device=\'gpu\')\n        self.hsv = ops.Hsv(device=\'gpu\')\n\n        # Random number generation for augmentation\n        self.brightness_dist = ops.NormalDistribution(mean=1.0, stddev=augment_brightness)\n        self.contrast_dist = ops.NormalDistribution(mean=1.0, stddev=augment_contrast)\n        self.hue_dist = ops.NormalDistribution(mean=0.0, stddev=augment_hue)\n        self.saturation_dist = ops.NormalDistribution(mean=1.0, stddev=augment_saturation)\n\n        if rotate_augment:\n            raise RuntimeWarning(""--augment-rotate current has no effect when using the DALI data loader."")\n\n        if isinstance(resize, list): resize = max(resize)\n        self.rand_resize = ops.Uniform(range=[resize, float(max_size)])\n\n        self.resize_train = ops.Resize(device=\'gpu\', interp_type=types.DALIInterpType.INTERP_CUBIC, save_attrs=True)\n        self.resize_infer = ops.Resize(device=\'gpu\', interp_type=types.DALIInterpType.INTERP_CUBIC,\n                                       resize_longer=max_size, save_attrs=True)\n\n        padded_size = max_size + ((self.stride - max_size % self.stride) % self.stride)\n\n        self.pad = ops.Paste(device=\'gpu\', fill_value=0, ratio=1.1, min_canvas_size=padded_size, paste_x=0, paste_y=0)\n        self.normalize = ops.CropMirrorNormalize(device=\'gpu\', mean=mean, std=std, crop=(padded_size, padded_size),\n                                                 crop_pos_x=0, crop_pos_y=0)\n\n    def define_graph(self):\n\n        images, bboxes, labels, img_ids = self.reader()\n\n        if self.training:\n            crop_begin, crop_size, bboxes, labels = self.bbox_crop(bboxes, labels)\n            images = self.decode_train(images, crop_begin, crop_size)\n            resize = self.rand_resize()\n            images, attrs = self.resize_train(images, resize_longer=resize)\n\n            flip = self.coin_flip()\n            bboxes = self.bbox_flip(bboxes, horizontal=flip)\n            images = self.img_flip(images, horizontal=flip)\n\n            if self.augment_brightness or self.augment_contrast:\n                images = self.bc(images, brightness=self.brightness_dist(), contrast=self.contrast_dist())\n            if self.augment_hue or self.augment_saturation:\n                images = self.hsv(images, hue=self.hue_dist(), saturation=self.saturation_dist())\n\n        else:\n            images = self.decode_infer(images)\n            images, attrs = self.resize_infer(images)\n\n        resized_images = images\n        images = self.normalize(self.pad(images))\n\n        return images, bboxes, labels, img_ids, attrs, resized_images\n\n\nclass DaliDataIterator():\n    \'Data loader for data parallel using Dali\'\n\n    def __init__(self, path, resize, max_size, batch_size, stride, world, annotations, training=False,\n                 rotate_augment=False, augment_brightness=0.0,\n                 augment_contrast=0.0, augment_hue=0.0, augment_saturation=0.0):\n        self.training = training\n        self.resize = resize\n        self.max_size = max_size\n        self.stride = stride\n        self.batch_size = batch_size // world\n        self.mean = [255. * x for x in [0.485, 0.456, 0.406]]\n        self.std = [255. * x for x in [0.229, 0.224, 0.225]]\n        self.world = world\n        self.path = path\n\n        # Setup COCO\n        with redirect_stdout(None):\n            self.coco = COCO(annotations)\n        self.ids = list(self.coco.imgs.keys())\n        if \'categories\' in self.coco.dataset:\n            self.categories_inv = {k: i for i, k in enumerate(self.coco.getCatIds())}\n\n        self.pipe = COCOPipeline(batch_size=self.batch_size, num_threads=2,\n                                 path=path, training=training, annotations=annotations, world=world,\n                                 device_id=torch.cuda.current_device(), mean=self.mean, std=self.std, resize=resize,\n                                 max_size=max_size, stride=self.stride, rotate_augment=rotate_augment,\n                                 augment_brightness=augment_brightness,\n                                 augment_contrast=augment_contrast, augment_hue=augment_hue,\n                                 augment_saturation=augment_saturation)\n\n        self.pipe.build()\n\n    def __repr__(self):\n        return \'\\n\'.join([\n            \'    loader: dali\',\n            \'    resize: {}, max: {}\'.format(self.resize, self.max_size),\n        ])\n\n    def __len__(self):\n        return ceil(len(self.ids) // self.world / self.batch_size)\n\n    def __iter__(self):\n        for _ in range(self.__len__()):\n\n            data, ratios, ids, num_detections = [], [], [], []\n            dali_data, dali_boxes, dali_labels, dali_ids, dali_attrs, dali_resize_img = self.pipe.run()\n\n            for l in range(len(dali_boxes)):\n                num_detections.append(dali_boxes.at(l).shape[0])\n\n            pyt_targets = -1 * torch.ones([len(dali_boxes), max(max(num_detections), 1), 5])\n\n            for batch in range(self.batch_size):\n                id = int(dali_ids.at(batch)[0])\n\n                # Convert dali tensor to pytorch\n                dali_tensor = dali_data.at(batch)\n                tensor_shape = dali_tensor.shape()\n\n                datum = torch.zeros(dali_tensor.shape(), dtype=torch.float, device=torch.device(\'cuda\'))\n                c_type_pointer = ctypes.c_void_p(datum.data_ptr())\n                dali_tensor.copy_to_external(c_type_pointer)\n\n                # Calculate image resize ratio to rescale boxes\n                prior_size = dali_attrs.as_cpu().at(batch)\n                resized_size = dali_resize_img.at(batch).shape()\n                ratio = max(resized_size) / max(prior_size)\n\n                if self.training:\n                    # Rescale boxes\n                    b_arr = dali_boxes.at(batch)\n                    num_dets = b_arr.shape[0]\n                    if num_dets is not 0:\n                        pyt_bbox = torch.from_numpy(b_arr).float()\n\n                        pyt_bbox[:, 0] *= float(prior_size[1])\n                        pyt_bbox[:, 1] *= float(prior_size[0])\n                        pyt_bbox[:, 2] *= float(prior_size[1])\n                        pyt_bbox[:, 3] *= float(prior_size[0])\n                        # (l,t,r,b) ->  (x,y,w,h) == (l,r, r-l, b-t)\n                        pyt_bbox[:, 2] -= pyt_bbox[:, 0]\n                        pyt_bbox[:, 3] -= pyt_bbox[:, 1]\n                        pyt_targets[batch, :num_dets, :4] = pyt_bbox * ratio\n\n                    # Arrange labels in target tensor\n                    l_arr = dali_labels.at(batch)\n                    if num_dets is not 0:\n                        pyt_label = torch.from_numpy(l_arr).float()\n                        pyt_label -= 1  # Rescale labels to [0,79] instead of [1,80]\n                        pyt_targets[batch, :num_dets, 4] = pyt_label.squeeze()\n\n                ids.append(id)\n                data.append(datum.unsqueeze(0))\n                ratios.append(ratio)\n\n            data = torch.cat(data, dim=0)\n\n            if self.training:\n                pyt_targets = pyt_targets.cuda(non_blocking=True)\n\n                yield data, pyt_targets\n\n            else:\n                ids = torch.Tensor(ids).int().cuda(non_blocking=True)\n                ratios = torch.Tensor(ratios).cuda(non_blocking=True)\n\n                yield data, ids, ratios\n\n'"
retinanet/data.py,30,"b'import os\nimport random\nfrom contextlib import redirect_stdout\nfrom PIL import Image\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils import data\nfrom pycocotools.coco import COCO\nimport math\nfrom torchvision.transforms.functional import adjust_brightness, adjust_contrast, adjust_hue, adjust_saturation\n\n\nclass CocoDataset(data.dataset.Dataset):\n    \'Dataset looping through a set of images\'\n\n    def __init__(self, path, resize, max_size, stride, annotations=None, training=False, rotate_augment=False,\n                 augment_brightness=0.0, augment_contrast=0.0,\n                 augment_hue=0.0, augment_saturation=0.0):\n        super().__init__()\n\n        self.path = os.path.expanduser(path)\n        self.resize = resize\n        self.max_size = max_size\n        self.stride = stride\n        self.mean = [0.485, 0.456, 0.406]\n        self.std = [0.229, 0.224, 0.225]\n        self.training = training\n        self.rotate_augment = rotate_augment\n        self.augment_brightness = augment_brightness\n        self.augment_contrast = augment_contrast\n        self.augment_hue = augment_hue\n        self.augment_saturation = augment_saturation\n\n        with redirect_stdout(None):\n            self.coco = COCO(annotations)\n        self.ids = list(self.coco.imgs.keys())\n        if \'categories\' in self.coco.dataset:\n            self.categories_inv = {k: i for i, k in enumerate(self.coco.getCatIds())}\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, index):\n        \' Get sample\'\n\n        # Load image\n        id = self.ids[index]\n        if self.coco:\n            image = self.coco.loadImgs(id)[0][\'file_name\']\n        im = Image.open(\'{}/{}\'.format(self.path, image)).convert(""RGB"")\n\n        # Randomly sample scale for resize during training\n        resize = self.resize\n        if isinstance(resize, list):\n            resize = random.randint(self.resize[0], self.resize[-1])\n\n        ratio = resize / min(im.size)\n        if ratio * max(im.size) > self.max_size:\n            ratio = self.max_size / max(im.size)\n        im = im.resize((int(ratio * d) for d in im.size), Image.BILINEAR)\n\n        if self.training:\n            # Get annotations\n            boxes, categories = self._get_target(id)\n            boxes *= ratio\n\n            # Random rotation, if self.rotate_augment\n            random_angle = random.randint(0, 3) * 90\n            if self.rotate_augment and random_angle != 0:\n                # rotate by random_angle degrees.\n                im = im.rotate(random_angle)\n                x, y, w, h = boxes[:, 0].clone(), boxes[:, 1].clone(), boxes[:, 2].clone(), boxes[:, 3].clone()\n                if random_angle == 90:\n                    boxes[:, 0] = y - im.size[1] / 2 + im.size[0] / 2\n                    boxes[:, 1] = im.size[0] / 2 + im.size[1] / 2 - x - w\n                    boxes[:, 2] = h\n                    boxes[:, 3] = w\n                elif random_angle == 180:\n                    boxes[:, 0] = im.size[0] - x - w\n                    boxes[:, 1] = im.size[1] - y - h\n                elif random_angle == 270:\n                    boxes[:, 0] = im.size[0] / 2 + im.size[1] / 2 - y - h\n                    boxes[:, 1] = x - im.size[0] / 2 + im.size[1] / 2\n                    boxes[:, 2] = h\n                    boxes[:, 3] = w\n\n            # Random horizontal flip\n            if random.randint(0, 1):\n                im = im.transpose(Image.FLIP_LEFT_RIGHT)\n                boxes[:, 0] = im.size[0] - boxes[:, 0] - boxes[:, 2]\n\n            # Apply image brightness, contrast etc augmentation\n            if self.augment_brightness:\n                brightness_factor = random.normalvariate(1, self.augment_brightness)\n                brightness_factor = max(0, brightness_factor)\n                im = adjust_brightness(im, brightness_factor)\n            if self.augment_contrast:\n                contrast_factor = random.normalvariate(1, self.augment_contrast)\n                contrast_factor = max(0, contrast_factor)\n                im = adjust_contrast(im, contrast_factor)\n            if self.augment_hue:\n                hue_factor = random.normalvariate(0, self.augment_hue)\n                hue_factor = max(-0.5, hue_factor)\n                hue_factor = min(0.5, hue_factor)\n                im = adjust_hue(im, hue_factor)\n            if self.augment_saturation:\n                saturation_factor = random.normalvariate(1, self.augment_saturation)\n                saturation_factor = max(0, saturation_factor)\n                im = adjust_saturation(im, saturation_factor)\n\n            target = torch.cat([boxes, categories], dim=1)\n\n        # Convert to tensor and normalize\n        data = torch.ByteTensor(torch.ByteStorage.from_buffer(im.tobytes()))\n        data = data.float().div(255).view(*im.size[::-1], len(im.mode))\n        data = data.permute(2, 0, 1)\n\n        for t, mean, std in zip(data, self.mean, self.std):\n            t.sub_(mean).div_(std)\n\n        # Apply padding\n        pw, ph = ((self.stride - d % self.stride) % self.stride for d in im.size)\n        data = F.pad(data, (0, pw, 0, ph))\n\n        if self.training:\n            return data, target\n\n        return data, id, ratio\n\n    def _get_target(self, id):\n        \'Get annotations for sample\'\n\n        ann_ids = self.coco.getAnnIds(imgIds=id)\n        annotations = self.coco.loadAnns(ann_ids)\n\n        boxes, categories = [], []\n        for ann in annotations:\n            if ann[\'bbox\'][2] < 1 and ann[\'bbox\'][3] < 1:\n                continue\n            boxes.append(ann[\'bbox\'])\n            cat = ann[\'category_id\']\n            if \'categories\' in self.coco.dataset:\n                cat = self.categories_inv[cat]\n            categories.append(cat)\n\n        if boxes:\n            target = (torch.FloatTensor(boxes),\n                      torch.FloatTensor(categories).unsqueeze(1))\n        else:\n            target = (torch.ones([1, 4]), torch.ones([1, 1]) * -1)\n\n        return target\n\n    def collate_fn(self, batch):\n        \'Create batch from multiple samples\'\n\n        if self.training:\n            data, targets = zip(*batch)\n            max_det = max([t.size()[0] for t in targets])\n            targets = [torch.cat([t, torch.ones([max_det - t.size()[0], 5]) * -1]) for t in targets]\n            targets = torch.stack(targets, 0)\n        else:\n            data, indices, ratios = zip(*batch)\n\n        # Pad data to match max batch dimensions\n        sizes = [d.size()[-2:] for d in data]\n        w, h = (max(dim) for dim in zip(*sizes))\n\n        data_stack = []\n        for datum in data:\n            pw, ph = w - datum.size()[-2], h - datum.size()[-1]\n            data_stack.append(\n                F.pad(datum, (0, ph, 0, pw)) if max(ph, pw) > 0 else datum)\n\n        data = torch.stack(data_stack)\n\n        if self.training:\n            return data, targets\n\n        ratios = torch.FloatTensor(ratios).view(-1, 1, 1)\n        return data, torch.IntTensor(indices), ratios\n\n\nclass DataIterator():\n    \'Data loader for data parallel\'\n\n    def __init__(self, path, resize, max_size, batch_size, stride, world, annotations, training=False,\n                 rotate_augment=False, augment_brightness=0.0,\n                 augment_contrast=0.0, augment_hue=0.0, augment_saturation=0.0):\n        self.resize = resize\n        self.max_size = max_size\n\n        self.dataset = CocoDataset(path, resize=resize, max_size=max_size,\n                                   stride=stride, annotations=annotations, training=training,\n                                   rotate_augment=rotate_augment,\n                                   augment_brightness=augment_brightness,\n                                   augment_contrast=augment_contrast, augment_hue=augment_hue,\n                                   augment_saturation=augment_saturation)\n        self.ids = self.dataset.ids\n        self.coco = self.dataset.coco\n\n        self.sampler = data.distributed.DistributedSampler(self.dataset) if world > 1 else None\n        self.dataloader = data.DataLoader(self.dataset, batch_size=batch_size // world,\n                                          sampler=self.sampler, collate_fn=self.dataset.collate_fn, num_workers=2,\n                                          pin_memory=True)\n\n    def __repr__(self):\n        return \'\\n\'.join([\n            \'    loader: pytorch\',\n            \'    resize: {}, max: {}\'.format(self.resize, self.max_size),\n        ])\n\n    def __len__(self):\n        return len(self.dataloader)\n\n    def __iter__(self):\n        for output in self.dataloader:\n            if self.dataset.training:\n                data, target = output\n            else:\n                data, ids, ratio = output\n\n            if torch.cuda.is_available():\n                data = data.cuda(non_blocking=True)\n\n            if self.dataset.training:\n                if torch.cuda.is_available():\n                    target = target.cuda(non_blocking=True)\n                yield data, target\n            else:\n                if torch.cuda.is_available():\n                    ids = ids.cuda(non_blocking=True)\n                    ratio = ratio.cuda(non_blocking=True)\n                yield data, ids, ratio\n\n\nclass RotatedCocoDataset(data.dataset.Dataset):\n    \'Dataset looping through a set of images\'\n\n    def __init__(self, path, resize, max_size, stride, annotations=None, training=False, rotate_augment=False,\n                 augment_brightness=0.0, augment_contrast=0.0,\n                 augment_hue=0.0, augment_saturation=0.0, absolute_angle=False):\n        super().__init__()\n\n        self.path = os.path.expanduser(path)\n        self.resize = resize\n        self.max_size = max_size\n        self.stride = stride\n        self.mean = [0.485, 0.456, 0.406]\n        self.std = [0.229, 0.224, 0.225]\n        self.training = training\n        self.rotate_augment = rotate_augment\n        self.augment_brightness = augment_brightness\n        self.augment_contrast = augment_contrast\n        self.augment_hue = augment_hue\n        self.augment_saturation = augment_saturation\n        self.absolute_angle=absolute_angle\n\n        with redirect_stdout(None):\n            self.coco = COCO(annotations)\n        self.ids = list(self.coco.imgs.keys())\n        if \'categories\' in self.coco.dataset:\n            self.categories_inv = {k: i for i, k in enumerate(self.coco.getCatIds())}\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, index):\n        \' Get sample\'\n\n        # Load image\n        id = self.ids[index]\n        if self.coco:\n            image = self.coco.loadImgs(id)[0][\'file_name\']\n        im = Image.open(\'{}/{}\'.format(self.path, image)).convert(""RGB"")\n\n        # Randomly sample scale for resize during training\n        resize = self.resize\n        if isinstance(resize, list):\n            resize = random.randint(self.resize[0], self.resize[-1])\n\n        ratio = resize / min(im.size)\n        if ratio * max(im.size) > self.max_size:\n            ratio = self.max_size / max(im.size)\n        im = im.resize((int(ratio * d) for d in im.size), Image.BILINEAR)\n\n        if self.training:\n            # Get annotations\n            boxes, categories = self._get_target(id)\n            # boxes *= ratio\n            boxes[:, :4] *= ratio\n\n            # Random rotation, if self.rotate_augment\n            random_angle = random.randint(0, 3) * 90\n            if self.rotate_augment and random_angle != 0:\n                # rotate by random_angle degrees.\n                original_size = im.size\n                im = im.rotate(random_angle, expand=True)\n                x, y, w, h, t = boxes[:, 0].clone(), boxes[:, 1].clone(), boxes[:, 2].clone(), \\\n                                boxes[:, 3].clone(), boxes[:, 4].clone()\n                if random_angle == 90:\n                    boxes[:, 0] = y\n                    boxes[:, 1] = original_size[0] - x - w\n                    if not self.absolute_angle:\n                        boxes[:, 2] = h\n                        boxes[:, 3] = w\n                elif random_angle == 180:\n                    boxes[:, 0] = original_size[0] - x - w\n                    boxes[:, 1] = original_size[1] - y - h\n\n                elif random_angle == 270:\n                    boxes[:, 0] = original_size[1] - y - h\n                    boxes[:, 1] = x\n                    if not self.absolute_angle:\n                        boxes[:, 2] = h\n                        boxes[:, 3] = w\n\n                    pass\n\n                # Adjust theta\n                if self.absolute_angle:\n                    # This is only needed in absolute angle mode.\n                    t += math.radians(random_angle)\n                    rem = torch.remainder(torch.abs(t), math.pi)\n                    sign = torch.sign(t)\n                    t = rem * sign\n\n                boxes[:, 4] = t\n\n            # Random horizontal flip\n            if random.randint(0, 1):\n                im = im.transpose(Image.FLIP_LEFT_RIGHT)\n                boxes[:, 0] = im.size[0] - boxes[:, 0] - boxes[:, 2]\n                boxes[:, 1] = boxes[:, 1]\n                boxes[:, 4] = -boxes[:, 4]\n\n            # Apply image brightness, contrast etc augmentation\n            if self.augment_brightness:\n                brightness_factor = random.normalvariate(1, self.augment_brightness)\n                brightness_factor = max(0, brightness_factor)\n                im = adjust_brightness(im, brightness_factor)\n            if self.augment_contrast:\n                contrast_factor = random.normalvariate(1, self.augment_contrast)\n                contrast_factor = max(0, contrast_factor)\n                im = adjust_contrast(im, contrast_factor)\n            if self.augment_hue:\n                hue_factor = random.normalvariate(0, self.augment_hue)\n                hue_factor = max(-0.5, hue_factor)\n                hue_factor = min(0.5, hue_factor)\n                im = adjust_hue(im, hue_factor)\n            if self.augment_saturation:\n                saturation_factor = random.normalvariate(1, self.augment_saturation)\n                saturation_factor = max(0, saturation_factor)\n                im = adjust_saturation(im, saturation_factor)\n\n            target = torch.cat([boxes, categories], dim=1)\n\n        # Convert to tensor and normalize\n        data = torch.ByteTensor(torch.ByteStorage.from_buffer(im.tobytes()))\n        data = data.float().div(255).view(*im.size[::-1], len(im.mode))\n        data = data.permute(2, 0, 1)\n\n        for t, mean, std in zip(data, self.mean, self.std):\n            t.sub_(mean).div_(std)\n\n        # Apply padding\n        pw, ph = ((self.stride - d % self.stride) % self.stride for d in im.size)\n        data = F.pad(data, (0, pw, 0, ph))\n\n        if self.training:\n            return data, target\n\n        return data, id, ratio\n\n    def _get_target(self, id):\n        \'Get annotations for sample\'\n\n        ann_ids = self.coco.getAnnIds(imgIds=id)\n        annotations = self.coco.loadAnns(ann_ids)\n\n        boxes, categories = [], []\n        for ann in annotations:\n            if ann[\'bbox\'][2] < 1 and ann[\'bbox\'][3] < 1:\n                continue\n            final_bbox = ann[\'bbox\']\n            if len(final_bbox) == 4:\n                final_bbox.append(0.0)  # add theta of zero.\n            assert len(ann[\'bbox\']) == 5, ""Bounding box for id %i does not contain five entries."" % id\n            boxes.append(final_bbox)\n            cat = ann[\'category_id\']\n            if \'categories\' in self.coco.dataset:\n                cat = self.categories_inv[cat]\n            categories.append(cat)\n\n        if boxes:\n            target = (torch.FloatTensor(boxes),\n                      torch.FloatTensor(categories).unsqueeze(1))\n        else:\n            target = (torch.ones([1, 5]), torch.ones([1, 1]) * -1)\n\n        return target\n\n    def collate_fn(self, batch):\n        \'Create batch from multiple samples\'\n\n        if self.training:\n            data, targets = zip(*batch)\n            max_det = max([t.size()[0] for t in targets])\n            targets = [torch.cat([t, torch.ones([max_det - t.size()[0], 6]) * -1]) for t in targets]\n            targets = torch.stack(targets, 0)\n        else:\n            data, indices, ratios = zip(*batch)\n\n        # Pad data to match max batch dimensions\n        sizes = [d.size()[-2:] for d in data]\n        w, h = (max(dim) for dim in zip(*sizes))\n\n        data_stack = []\n        for datum in data:\n            pw, ph = w - datum.size()[-2], h - datum.size()[-1]\n            data_stack.append(\n                F.pad(datum, (0, ph, 0, pw)) if max(ph, pw) > 0 else datum)\n\n        data = torch.stack(data_stack)\n\n        if self.training:\n            return data, targets\n\n        ratios = torch.FloatTensor(ratios).view(-1, 1, 1)\n        return data, torch.IntTensor(indices), ratios\n\n\nclass RotatedDataIterator():\n    \'Data loader for data parallel\'\n\n    def __init__(self, path, resize, max_size, batch_size, stride, world, annotations, training=False,\n                 rotate_augment=False, augment_brightness=0.0,\n                 augment_contrast=0.0, augment_hue=0.0, augment_saturation=0.0, absolute_angle=False\n                 ):\n        self.resize = resize\n        self.max_size = max_size\n\n        self.dataset = RotatedCocoDataset(path, resize=resize, max_size=max_size,\n                                          stride=stride, annotations=annotations, training=training,\n                                          rotate_augment=rotate_augment,\n                                          augment_brightness=augment_brightness,\n                                          augment_contrast=augment_contrast, augment_hue=augment_hue,\n                                          augment_saturation=augment_saturation, absolute_angle=absolute_angle)\n        self.ids = self.dataset.ids\n        self.coco = self.dataset.coco\n\n        self.sampler = data.distributed.DistributedSampler(self.dataset) if world > 1 else None\n        self.dataloader = data.DataLoader(self.dataset, batch_size=batch_size // world,\n                                          sampler=self.sampler, collate_fn=self.dataset.collate_fn, num_workers=2,\n                                          pin_memory=True)\n\n    def __repr__(self):\n        return \'\\n\'.join([\n            \'    loader: pytorch\',\n            \'    resize: {}, max: {}\'.format(self.resize, self.max_size),\n        ])\n\n    def __len__(self):\n        return len(self.dataloader)\n\n    def __iter__(self):\n        for output in self.dataloader:\n            if self.dataset.training:\n                data, target = output\n            else:\n                data, ids, ratio = output\n\n            if torch.cuda.is_available():\n                data = data.cuda(non_blocking=True)\n\n            if self.dataset.training:\n                if torch.cuda.is_available():\n                    target = target.cuda(non_blocking=True)\n                yield data, target\n            else:\n                if torch.cuda.is_available():\n                    ids = ids.cuda(non_blocking=True)\n                    ratio = ratio.cuda(non_blocking=True)\n                yield data, ids, ratio\n'"
retinanet/infer.py,7,"b'import os\nimport json\nimport tempfile\nfrom contextlib import redirect_stdout\nimport torch\nfrom apex import amp\nfrom apex.parallel import DistributedDataParallel as DDP\nfrom pycocotools.cocoeval import COCOeval\nimport numpy as np\n\nfrom .data import DataIterator, RotatedDataIterator\nfrom .dali import DaliDataIterator\nfrom .model import Model\nfrom .utils import Profiler, rotate_box\n\n\ndef infer(model, path, detections_file, resize, max_size, batch_size, mixed_precision=True, is_master=True, world=0,\n          annotations=None, use_dali=True, is_validation=False, verbose=True, rotated_bbox=False):\n    \'Run inference on images from path\'\n\n    backend = \'pytorch\' if isinstance(model, Model) or isinstance(model, DDP) else \'tensorrt\'\n\n    # Set batch_size = 1 batch/GPU for EXPLICIT_BATCH compatibility in TRT\n    if backend is \'tensorrt\':\n        batch_size = world\n\n    stride = model.module.stride if isinstance(model, DDP) else model.stride\n\n    # Create annotations if none was provided\n    if not annotations:\n        annotations = tempfile.mktemp(\'.json\')\n        images = [{\'id\': i, \'file_name\': f} for i, f in enumerate(os.listdir(path))]\n        json.dump({\'images\': images}, open(annotations, \'w\'))\n\n    # TensorRT only supports fixed input sizes, so override input size accordingly\n    if backend == \'tensorrt\': max_size = max(model.input_size)\n\n    # Prepare dataset\n    if verbose: print(\'Preparing dataset...\')\n    if rotated_bbox:\n        if use_dali: raise NotImplementedError(""This repo does not currently support DALI for rotated bbox detections."")\n        data_iterator = RotatedDataIterator(path, resize, max_size, batch_size, stride,\n                                            world, annotations, training=False)\n    else:\n        data_iterator = (DaliDataIterator if use_dali else DataIterator)(\n            path, resize, max_size, batch_size, stride,\n            world, annotations, training=False)\n    if verbose: print(data_iterator)\n\n    # Prepare model\n    if backend is \'pytorch\':\n        # If we are doing validation during training,\n        # no need to register model with AMP again\n        if not is_validation:\n            if torch.cuda.is_available(): model = model.cuda()\n            model = amp.initialize(model, None,\n                                   opt_level=\'O2\' if mixed_precision else \'O0\',\n                                   keep_batchnorm_fp32=True,\n                                   verbosity=0)\n\n        model.eval()\n\n    if verbose:\n        print(\'   backend: {}\'.format(backend))\n        print(\'    device: {} {}\'.format(\n            world, \'cpu\' if not torch.cuda.is_available() else \'GPU\' if world == 1 else \'GPUs\'))\n        print(\'     batch: {}, precision: {}\'.format(batch_size,\n                                                     \'unknown\' if backend is \'tensorrt\' else \'mixed\' if mixed_precision else \'full\'))\n        print(\' BBOX type:\', \'rotated\' if rotated_bbox else \'axis aligned\')\n        print(\'Running inference...\')\n\n    results = []\n    profiler = Profiler([\'infer\', \'fw\'])\n    with torch.no_grad():\n        for i, (data, ids, ratios) in enumerate(data_iterator):\n            # Forward pass\n            profiler.start(\'fw\')\n            scores, boxes, classes = model(data, rotated_bbox)\n            profiler.stop(\'fw\')\n\n            results.append([scores, boxes, classes, ids, ratios])\n\n            profiler.bump(\'infer\')\n            if verbose and (profiler.totals[\'infer\'] > 60 or i == len(data_iterator) - 1):\n                size = len(data_iterator.ids)\n                msg = \'[{:{len}}/{}]\'.format(min((i + 1) * batch_size,\n                                                 size), size, len=len(str(size)))\n                msg += \' {:.3f}s/{}-batch\'.format(profiler.means[\'infer\'], batch_size)\n                msg += \' (fw: {:.3f}s)\'.format(profiler.means[\'fw\'])\n                msg += \', {:.1f} im/s\'.format(batch_size / profiler.means[\'infer\'])\n                print(msg, flush=True)\n\n                profiler.reset()\n\n    # Gather results from all devices\n    if verbose: print(\'Gathering results...\')\n    results = [torch.cat(r, dim=0) for r in zip(*results)]\n    if world > 1:\n        for r, result in enumerate(results):\n            all_result = [torch.ones_like(result, device=result.device) for _ in range(world)]\n            torch.distributed.all_gather(list(all_result), result)\n            results[r] = torch.cat(all_result, dim=0)\n\n    if is_master:\n        # Copy buffers back to host\n        results = [r.cpu() for r in results]\n\n        # Collect detections\n        detections = []\n        processed_ids = set()\n        for scores, boxes, classes, image_id, ratios in zip(*results):\n            image_id = image_id.item()\n            if image_id in processed_ids:\n                continue\n            processed_ids.add(image_id)\n\n            keep = (scores > 0).nonzero()\n            scores = scores[keep].view(-1)\n            if rotated_bbox:\n                boxes = boxes[keep, :].view(-1, 6)\n                boxes[:, :4] /= ratios\n            else:\n                boxes = boxes[keep, :].view(-1, 4) / ratios\n            classes = classes[keep].view(-1).int()\n\n            for score, box, cat in zip(scores, boxes, classes):\n                if rotated_bbox:\n                    x1, y1, x2, y2, sin, cos = box.data.tolist()\n                    theta = np.arctan2(sin, cos)\n                    w = x2 - x1 + 1\n                    h = y2 - y1 + 1\n                    seg = rotate_box([x1, y1, w, h, theta])\n                else:\n                    x1, y1, x2, y2 = box.data.tolist()\n                cat = cat.item()\n                if \'annotations\' in data_iterator.coco.dataset:\n                    cat = data_iterator.coco.getCatIds()[cat]\n                this_det = {\n                    \'image_id\': image_id,\n                    \'score\': score.item(),\n                    \'category_id\': cat}\n                if rotated_bbox:\n                    this_det[\'bbox\'] = [x1, y1, x2 - x1 + 1, y2 - y1 + 1, theta]\n                    this_det[\'segmentation\'] = [seg]\n                else:\n                    this_det[\'bbox\'] = [x1, y1, x2 - x1 + 1, y2 - y1 + 1]\n\n                detections.append(this_det)\n\n        if detections:\n            # Save detections\n            if detections_file and verbose: print(\'Writing {}...\'.format(detections_file))\n            detections = {\'annotations\': detections}\n            detections[\'images\'] = data_iterator.coco.dataset[\'images\']\n            if \'categories\' in data_iterator.coco.dataset:\n                detections[\'categories\'] = [data_iterator.coco.dataset[\'categories\']]\n            if detections_file:\n                json.dump(detections, open(detections_file, \'w\'), indent=4)\n\n            # Evaluate model on dataset\n            if \'annotations\' in data_iterator.coco.dataset:\n                if verbose: print(\'Evaluating model...\')\n                with redirect_stdout(None):\n                    coco_pred = data_iterator.coco.loadRes(detections[\'annotations\'])\n                    if rotated_bbox:\n                        coco_eval = COCOeval(data_iterator.coco, coco_pred, \'segm\')\n                    else:\n                        coco_eval = COCOeval(data_iterator.coco, coco_pred, \'bbox\')\n                    coco_eval.evaluate()\n                    coco_eval.accumulate()\n                coco_eval.summarize()\n        else:\n            print(\'No detections!\')\n'"
retinanet/loss.py,4,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FocalLoss(nn.Module):\n    'Focal Loss - https://arxiv.org/abs/1708.02002'\n\n    def __init__(self, alpha=0.25, gamma=2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, pred_logits, target):\n        pred = pred_logits.sigmoid()\n        ce = F.binary_cross_entropy_with_logits(pred_logits, target, reduction='none')\n        alpha = target * self.alpha + (1. - target) * (1. - self.alpha)\n        pt = torch.where(target == 1,  pred, 1 - pred)\n        return alpha * (1. - pt) ** self.gamma * ce\n\nclass SmoothL1Loss(nn.Module):\n    'Smooth L1 Loss'\n\n    def __init__(self, beta=0.11):\n        super().__init__()\n        self.beta = beta\n\n    def forward(self, pred, target):\n        x = (pred - target).abs()\n        l1 = x - 0.5 * self.beta\n        l2 = 0.5 * x ** 2 / self.beta\n        return torch.where(x >= self.beta, l1, l2)\n"""
retinanet/main.py,9,"b'#!/usr/bin/env python3\nimport sys\nimport os\nimport argparse\nimport random\nimport torch.cuda\nimport torch.distributed\nimport torch.multiprocessing\n\nfrom retinanet import infer, train, utils\nfrom retinanet.model import Model\nfrom retinanet._C import Engine\n\n\ndef parse(args):\n    parser = argparse.ArgumentParser(description=\'ODTK: Object Detection Toolkit.\')\n    parser.add_argument(\'--master\', metavar=\'address:port\', type=str, help=\'Address and port of the master worker\',\n                        default=\'127.0.0.1:29500\')\n\n    subparsers = parser.add_subparsers(help=\'sub-command\', dest=\'command\')\n    subparsers.required = True\n\n    devcount = max(1, torch.cuda.device_count())\n\n    parser_train = subparsers.add_parser(\'train\', help=\'train a network\')\n    parser_train.add_argument(\'model\', type=str, help=\'path to output model or checkpoint to resume from\')\n    parser_train.add_argument(\'--annotations\', metavar=\'path\', type=str, help=\'path to COCO style annotations\',\n                              required=True)\n    parser_train.add_argument(\'--images\', metavar=\'path\', type=str, help=\'path to images\', default=\'.\')\n    parser_train.add_argument(\'--backbone\', action=\'store\', type=str, nargs=\'+\', help=\'backbone model (or list of)\',\n                              default=[\'ResNet50FPN\'])\n    parser_train.add_argument(\'--classes\', metavar=\'num\', type=int, help=\'number of classes\', default=80)\n    parser_train.add_argument(\'--batch\', metavar=\'size\', type=int, help=\'batch size\', default=2 * devcount)\n    parser_train.add_argument(\'--resize\', metavar=\'scale\', type=int, help=\'resize to given size\', default=800)\n    parser_train.add_argument(\'--max-size\', metavar=\'max\', type=int, help=\'maximum resizing size\', default=1333)\n    parser_train.add_argument(\'--jitter\', metavar=\'min max\', type=int, nargs=2, help=\'jitter size within range\',\n                              default=[640, 1024])\n    parser_train.add_argument(\'--iters\', metavar=\'number\', type=int, help=\'number of iterations to train for\',\n                              default=90000)\n    parser_train.add_argument(\'--milestones\', action=\'store\', type=int, nargs=\'*\',\n                              help=\'list of iteration indices where learning rate decays\', default=[60000, 80000])\n    parser_train.add_argument(\'--schedule\', metavar=\'scale\', type=float,\n                              help=\'scale schedule (affecting iters and milestones)\', default=1)\n    parser_train.add_argument(\'--full-precision\', help=\'train in full precision\', action=\'store_true\')\n    parser_train.add_argument(\'--lr\', metavar=\'value\', help=\'learning rate\', type=float, default=0.01)\n    parser_train.add_argument(\'--warmup\', metavar=\'iterations\', help=\'numer of warmup iterations\', type=int,\n                              default=1000)\n    parser_train.add_argument(\'--gamma\', metavar=\'value\', type=float,\n                              help=\'multiplicative factor of learning rate decay\', default=0.1)\n    parser_train.add_argument(\'--override\', help=\'override model\', action=\'store_true\')\n    parser_train.add_argument(\'--val-annotations\', metavar=\'path\', type=str,\n                              help=\'path to COCO style validation annotations\')\n    parser_train.add_argument(\'--val-images\', metavar=\'path\', type=str, help=\'path to validation images\')\n    parser_train.add_argument(\'--post-metrics\', metavar=\'url\', type=str, help=\'post metrics to specified url\')\n    parser_train.add_argument(\'--fine-tune\', metavar=\'path\', type=str, help=\'fine tune a pretrained model\')\n    parser_train.add_argument(\'--logdir\', metavar=\'logdir\', type=str, help=\'directory where to write logs\')\n    parser_train.add_argument(\'--val-iters\', metavar=\'number\', type=int,\n                              help=\'number of iterations between each validation\', default=8000)\n    parser_train.add_argument(\'--with-dali\', help=\'use dali for data loading\', action=\'store_true\')\n    parser_train.add_argument(\'--augment-rotate\', help=\'use four-fold rotational augmentation\', action=\'store_true\')\n    parser_train.add_argument(\'--augment-free-rotate\', type=float, metavar=\'value value\', nargs=2, default=[0, 0],\n                              help=\'rotate images by an arbitrary angle, between min and max (in degrees)\')\n    parser_train.add_argument(\'--augment-brightness\', metavar=\'value\', type=float,\n                              help=\'adjust the brightness of the image.\', default=0.002)\n    parser_train.add_argument(\'--augment-contrast\', metavar=\'value\', type=float,\n                              help=\'adjust the contrast of the image.\', default=0.002)\n    parser_train.add_argument(\'--augment-hue\', metavar=\'value\', type=float,\n                              help=\'adjust the hue of the image.\', default=0.0002)\n    parser_train.add_argument(\'--augment-saturation\', metavar=\'value\', type=float,\n                              help=\'adjust the saturation of the image.\', default=0.002)\n    parser_train.add_argument(\'--regularization-l2\', metavar=\'value\', type=float, help=\'L2 regularization for optim\',\n                              default=0.0001)\n    parser_train.add_argument(\'--rotated-bbox\', help=\'detect rotated bounding boxes [x, y, w, h, theta]\',\n                              action=\'store_true\')\n    parser_train.add_argument(\'--anchor-ious\', metavar=\'value value\', type=float, nargs=2,\n                              help=\'anchor/bbox overlap threshold\', default=[0.4, 0.5])\n    parser_train.add_argument(\'--absolute-angle\', help=\'regress absolute angle (rather than -45 to 45 degrees.\',\n                              action=\'store_true\')\n\n    parser_infer = subparsers.add_parser(\'infer\', help=\'run inference\')\n    parser_infer.add_argument(\'model\', type=str, help=\'path to model\')\n    parser_infer.add_argument(\'--images\', metavar=\'path\', type=str, help=\'path to images\', default=\'.\')\n    parser_infer.add_argument(\'--annotations\', metavar=\'annotations\', type=str,\n                              help=\'evaluate using provided annotations\')\n    parser_infer.add_argument(\'--output\', metavar=\'file\', type=str, help=\'save detections to specified JSON file\',\n                              default=\'detections.json\')\n    parser_infer.add_argument(\'--batch\', metavar=\'size\', type=int, help=\'batch size\', default=2 * devcount)\n    parser_infer.add_argument(\'--resize\', metavar=\'scale\', type=int, help=\'resize to given size\', default=800)\n    parser_infer.add_argument(\'--max-size\', metavar=\'max\', type=int, help=\'maximum resizing size\', default=1333)\n    parser_infer.add_argument(\'--with-dali\', help=\'use dali for data loading\', action=\'store_true\')\n    parser_infer.add_argument(\'--full-precision\', help=\'inference in full precision\', action=\'store_true\')\n    parser_infer.add_argument(\'--rotated-bbox\', help=\'inference using a rotated bounding box model\',\n                              action=\'store_true\')\n\n    parser_export = subparsers.add_parser(\'export\', help=\'export a model into a TensorRT engine\')\n    parser_export.add_argument(\'model\', type=str, help=\'path to model\')\n    parser_export.add_argument(\'export\', type=str, help=\'path to exported output\')\n    parser_export.add_argument(\'--size\', metavar=\'height width\', type=int, nargs=\'+\',\n                               help=\'input size (square) or sizes (h w) to use when generating TensorRT engine\',\n                               default=[1280])\n    parser_export.add_argument(\'--batch\', metavar=\'size\', type=int, help=\'max batch size to use for TensorRT engine\',\n                               default=2)\n    parser_export.add_argument(\'--full-precision\', help=\'export in full instead of half precision\', action=\'store_true\')\n    parser_export.add_argument(\'--int8\', help=\'calibrate model and export in int8 precision\', action=\'store_true\')\n    parser_export.add_argument(\'--calibration-batches\', metavar=\'size\', type=int,\n                               help=\'number of batches to use for int8 calibration\', default=10)\n    parser_export.add_argument(\'--calibration-images\', metavar=\'path\', type=str,\n                               help=\'path to calibration images to use for int8 calibration\', default="""")\n    parser_export.add_argument(\'--calibration-table\', metavar=\'path\', type=str,\n                               help=\'path of existing calibration table to load from, or name of new calibration table\',\n                               default="""")\n    parser_export.add_argument(\'--verbose\', help=\'enable verbose logging\', action=\'store_true\')\n    parser_export.add_argument(\'--rotated-bbox\', help=\'inference using a rotated bounding box model\',\n                               action=\'store_true\')\n\n    return parser.parse_args(args)\n\n\ndef load_model(args, verbose=False):\n    if args.command != \'train\' and not os.path.isfile(args.model):\n        raise RuntimeError(\'Model file {} does not exist!\'.format(args.model))\n\n    model = None\n    state = {}\n    _, ext = os.path.splitext(args.model)\n\n    if args.command == \'train\' and (not os.path.exists(args.model) or args.override):\n        if verbose: print(\'Initializing model...\')\n        model = Model(backbones=args.backbone, classes=args.classes, rotated_bbox=args.rotated_bbox,\n                      anchor_ious=args.anchor_ious)\n        model.initialize(args.fine_tune)\n        if verbose: print(model)\n\n    elif ext == \'.pth\' or ext == \'.torch\':\n        if verbose: print(\'Loading model from {}...\'.format(os.path.basename(args.model)))\n        model, state = Model.load(filename=args.model, rotated_bbox=args.rotated_bbox)\n        if verbose: print(model)\n\n    elif args.command == \'infer\' and ext in [\'.engine\', \'.plan\']:\n        model = None\n\n    else:\n        raise RuntimeError(\'Invalid model format ""{}""!\'.format(args.ext))\n\n    state[\'path\'] = args.model\n    return model, state\n\n\ndef worker(rank, args, world, model, state):\n    \'Per-device distributed worker\'\n\n    if torch.cuda.is_available():\n        os.environ.update({\n            \'MASTER_PORT\': args.master.split(\':\')[-1],\n            \'MASTER_ADDR\': \':\'.join(args.master.split(\':\')[:-1]),\n            \'WORLD_SIZE\': str(world),\n            \'RANK\': str(rank),\n            \'CUDA_DEVICE\': str(rank)\n        })\n\n        torch.cuda.set_device(rank)\n        torch.distributed.init_process_group(backend=\'nccl\', init_method=\'env://\')\n\n        if args.batch % world != 0:\n            raise RuntimeError(\'Batch size should be a multiple of the number of GPUs\')\n\n    if model and model.angles is not None:\n        args.rotated_bbox = True\n\n    if args.command == \'train\':\n        train.train(model, state, args.images, args.annotations,\n                    args.val_images or args.images, args.val_annotations, args.resize, args.max_size, args.jitter,\n                    args.batch, int(args.iters * args.schedule), args.val_iters, not args.full_precision, args.lr,\n                    args.warmup, [int(m * args.schedule) for m in args.milestones], args.gamma,\n                    is_master=(rank == 0), world=world, use_dali=args.with_dali,\n                    metrics_url=args.post_metrics, logdir=args.logdir, verbose=(rank == 0),\n                    rotate_augment=args.augment_rotate,\n                    augment_brightness=args.augment_brightness, augment_contrast=args.augment_contrast,\n                    augment_hue=args.augment_hue, augment_saturation=args.augment_saturation,\n                    regularization_l2=args.regularization_l2, rotated_bbox=args.rotated_bbox, absolute_angle=args.absolute_angle)\n\n    elif args.command == \'infer\':\n        if model is None:\n            if rank == 0: print(\'Loading CUDA engine from {}...\'.format(os.path.basename(args.model)))\n            model = Engine.load(args.model)\n\n        infer.infer(model, args.images, args.output, args.resize, args.max_size, args.batch,\n                    annotations=args.annotations, mixed_precision=not args.full_precision,\n                    is_master=(rank == 0), world=world, use_dali=args.with_dali, verbose=(rank == 0),\n                    rotated_bbox=args.rotated_bbox)\n\n    elif args.command == \'export\':\n        onnx_only = args.export.split(\'.\')[-1] == \'onnx\'\n        input_size = args.size * 2 if len(args.size) == 1 else args.size\n\n        calibration_files = []\n        if args.int8:\n            # Get list of images to use for calibration\n            if os.path.isdir(args.calibration_images):\n                import glob\n                file_extensions = [\'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\', \'.png\', \'.PNG\']\n                for ex in file_extensions:\n                    calibration_files += glob.glob(""{}/*{}"".format(args.calibration_images, ex), recursive=True)\n                # Only need enough images for specified num of calibration batches\n                if len(calibration_files) >= args.calibration_batches * args.batch:\n                    calibration_files = calibration_files[:(args.calibration_batches * args.batch)]\n                else:\n                    print(\'Only found enough images for {} batches. Continuing anyway...\'.format(\n                        len(calibration_files) // args.batch))\n\n                random.shuffle(calibration_files)\n\n        precision = ""FP32""\n        if args.int8:\n            precision = ""INT8""\n        elif not args.full_precision:\n            precision = ""FP16""\n\n        exported = model.export(input_size, args.batch, precision, calibration_files, args.calibration_table,\n                                args.verbose, onnx_only=onnx_only)\n        if onnx_only:\n            with open(args.export, \'wb\') as out:\n                out.write(exported)\n        else:\n            exported.save(args.export)\n\n\ndef main(args=None):\n    \'Entry point for the retinanet command\'\n\n    args = parse(args or sys.argv[1:])\n\n    model, state = load_model(args, verbose=True)\n    if model: model.share_memory()\n\n    world = torch.cuda.device_count()\n    if args.command == \'export\' or world <= 1:\n        worker(0, args, 1, model, state)\n    else:\n        torch.multiprocessing.spawn(worker, args=(args, world, model, state), nprocs=world)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
retinanet/model.py,15,"b'import os.path\nimport io\nimport numpy as np\nimport math\nimport torch\nimport torch.nn as nn\n\nfrom . import backbones as backbones_mod\nfrom ._C import Engine\nfrom .box import generate_anchors, snap_to_anchors, decode, nms\nfrom .box import generate_anchors_rotated, snap_to_anchors_rotated, nms_rotated\nfrom .loss import FocalLoss, SmoothL1Loss\n\n\nclass Model(nn.Module):\n    \'RetinaNet - https://arxiv.org/abs/1708.02002\'\n\n    def __init__(self, backbones=\'ResNet50FPN\', classes=80, \n                ratios=[1.0, 2.0, 0.5], scales=[4 * 2 ** (i / 3) for i in range(3)],\n                angles=None, rotated_bbox=False, anchor_ious=[0.4, 0.5], config={}):\n        super().__init__()\n\n        if not isinstance(backbones, list):\n            backbones = [backbones]\n\n        self.backbones = nn.ModuleDict({b: getattr(backbones_mod, b)() for b in backbones})\n        self.name = \'RetinaNet\'\n        self.exporting = False\n        self.rotated_bbox = rotated_bbox\n        self.anchor_ious = anchor_ious\n\n        self.ratios = ratios\n        self.scales = scales\n        self.angles = angles if angles is not None else \\\n                    [-np.pi / 6, 0, np.pi / 6] if self.rotated_bbox else None\n        self.anchors = {}\n        self.classes = classes\n\n        self.threshold = config.get(\'threshold\', 0.05)\n        self.top_n = config.get(\'top_n\', 1000)\n        self.nms = config.get(\'nms\', 0.5)\n        self.detections = config.get(\'detections\', 100)\n\n        self.stride = max([b.stride for _, b in self.backbones.items()])\n\n        # classification and box regression heads\n        def make_head(out_size):\n            layers = []\n            for _ in range(4):\n                layers += [nn.Conv2d(256, 256, 3, padding=1), nn.ReLU()]\n            layers += [nn.Conv2d(256, out_size, 3, padding=1)]\n            return nn.Sequential(*layers)\n\n        self.num_anchors = len(self.ratios) * len(self.scales)\n        self.num_anchors = self.num_anchors if not self.rotated_bbox else (self.num_anchors * len(self.angles))\n        self.cls_head = make_head(classes * self.num_anchors)\n        self.box_head = make_head(4 * self.num_anchors) if not self.rotated_bbox \\\n                        else make_head(6 * self.num_anchors)  # theta -> cos(theta), sin(theta)\n\n        self.cls_criterion = FocalLoss()\n        self.box_criterion = SmoothL1Loss(beta=0.11)\n\n    def __repr__(self):\n        return \'\\n\'.join([\n            \'     model: {}\'.format(self.name),\n            \'  backbone: {}\'.format(\', \'.join([k for k, _ in self.backbones.items()])),\n            \'   classes: {}, anchors: {}\'.format(self.classes, self.num_anchors)\n        ])\n\n    def initialize(self, pre_trained):\n        if pre_trained:\n            # Initialize using weights from pre-trained model\n            if not os.path.isfile(pre_trained):\n                raise ValueError(\'No checkpoint {}\'.format(pre_trained))\n\n            print(\'Fine-tuning weights from {}...\'.format(os.path.basename(pre_trained)))\n            state_dict = self.state_dict()\n            chk = torch.load(pre_trained, map_location=lambda storage, loc: storage)\n            ignored = [\'cls_head.8.bias\', \'cls_head.8.weight\']\n            if self.rotated_bbox:\n                ignored += [\'box_head.8.bias\', \'box_head.8.weight\']\n            weights = {k: v for k, v in chk[\'state_dict\'].items() if k not in ignored}\n            state_dict.update(weights)\n            self.load_state_dict(state_dict)\n\n            del chk, weights\n            torch.cuda.empty_cache()\n\n        else:\n            # Initialize backbone(s)\n            for _, backbone in self.backbones.items():\n                backbone.initialize()\n\n            # Initialize heads\n            def initialize_layer(layer):\n                if isinstance(layer, nn.Conv2d):\n                    nn.init.normal_(layer.weight, std=0.01)\n                    if layer.bias is not None:\n                        nn.init.constant_(layer.bias, val=0)\n\n            self.cls_head.apply(initialize_layer)\n            self.box_head.apply(initialize_layer)\n\n        # Initialize class head prior\n        def initialize_prior(layer):\n            pi = 0.01\n            b = - math.log((1 - pi) / pi)\n            nn.init.constant_(layer.bias, b)\n            nn.init.normal_(layer.weight, std=0.01)\n\n        self.cls_head[-1].apply(initialize_prior)\n        if self.rotated_bbox:\n            self.box_head[-1].apply(initialize_prior)\n\n    def forward(self, x, rotated_bbox=None):\n        if self.training: x, targets = x\n\n        # Backbones forward pass\n        features = []\n        for _, backbone in self.backbones.items():\n            features.extend(backbone(x))\n\n        # Heads forward pass\n        cls_heads = [self.cls_head(t) for t in features]\n        box_heads = [self.box_head(t) for t in features]\n\n        if self.training:\n            return self._compute_loss(x, cls_heads, box_heads, targets.float())\n\n        cls_heads = [cls_head.sigmoid() for cls_head in cls_heads]\n\n        if self.exporting:\n            self.strides = [x.shape[-1] // cls_head.shape[-1] for cls_head in cls_heads]\n            return cls_heads, box_heads\n\n        global nms, generate_anchors\n        if self.rotated_bbox:\n            nms = nms_rotated\n            generate_anchors = generate_anchors_rotated\n\n        # Inference post-processing\n        decoded = []\n        for cls_head, box_head in zip(cls_heads, box_heads):\n            # Generate level\'s anchors\n            stride = x.shape[-1] // cls_head.shape[-1]\n            if stride not in self.anchors:\n                self.anchors[stride] = generate_anchors(stride, self.ratios, self.scales, self.angles)\n\n            # Decode and filter boxes\n            decoded.append(decode(cls_head, box_head, stride, self.threshold, \n                                self.top_n, self.anchors[stride], self.rotated_bbox))\n\n        # Perform non-maximum suppression\n        decoded = [torch.cat(tensors, 1) for tensors in zip(*decoded)]\n        return nms(*decoded, self.nms, self.detections)\n\n    def _extract_targets(self, targets, stride, size):\n        global generate_anchors, snap_to_anchors\n        if self.rotated_bbox:\n            generate_anchors = generate_anchors_rotated\n            snap_to_anchors = snap_to_anchors_rotated\n        cls_target, box_target, depth = [], [], []\n        for target in targets:\n            target = target[target[:, -1] > -1]\n            if stride not in self.anchors:\n                self.anchors[stride] = generate_anchors(stride, self.ratios, self.scales, self.angles)\n\n            anchors = self.anchors[stride]\n            if not self.rotated_bbox:\n                anchors = anchors.to(targets.device)\n            snapped = snap_to_anchors(target, [s * stride for s in size[::-1]], stride, \n                                    anchors, self.classes, targets.device, self.anchor_ious)\n            for l, s in zip((cls_target, box_target, depth), snapped): l.append(s)\n        return torch.stack(cls_target), torch.stack(box_target), torch.stack(depth)\n\n    def _compute_loss(self, x, cls_heads, box_heads, targets):\n        cls_losses, box_losses, fg_targets = [], [], []\n        for cls_head, box_head in zip(cls_heads, box_heads):\n            size = cls_head.shape[-2:]\n            stride = x.shape[-1] / cls_head.shape[-1]\n\n            cls_target, box_target, depth = self._extract_targets(targets, stride, size)\n            fg_targets.append((depth > 0).sum().float().clamp(min=1))\n\n            cls_head = cls_head.view_as(cls_target).float()\n            cls_mask = (depth >= 0).expand_as(cls_target).float()\n            cls_loss = self.cls_criterion(cls_head, cls_target)\n            cls_loss = cls_mask * cls_loss\n            cls_losses.append(cls_loss.sum())\n\n            box_head = box_head.view_as(box_target).float()\n            box_mask = (depth > 0).expand_as(box_target).float()\n            box_loss = self.box_criterion(box_head, box_target)\n            box_loss = box_mask * box_loss\n            box_losses.append(box_loss.sum())\n\n        fg_targets = torch.stack(fg_targets).sum()\n        cls_loss = torch.stack(cls_losses).sum() / fg_targets\n        box_loss = torch.stack(box_losses).sum() / fg_targets\n        return cls_loss, box_loss\n\n    def save(self, state):\n        checkpoint = {\n            \'backbone\': [k for k, _ in self.backbones.items()],\n            \'classes\': self.classes,\n            \'state_dict\': self.state_dict(),\n            \'ratios\': self.ratios,\n            \'scales\': self.scales\n        }\n        if self.rotated_bbox and self.angles:\n            checkpoint[\'angles\'] = self.angles\n\n        for key in (\'iteration\', \'optimizer\', \'scheduler\'):\n            if key in state:\n                checkpoint[key] = state[key]\n\n        torch.save(checkpoint, state[\'path\'])\n\n    @classmethod\n    def load(cls, filename, rotated_bbox=False):\n        if not os.path.isfile(filename):\n            raise ValueError(\'No checkpoint {}\'.format(filename))\n\n        checkpoint = torch.load(filename, map_location=lambda storage, loc: storage)\n        kwargs = {}\n        for i in [\'ratios\', \'scales\', \'angles\']:\n            if i in checkpoint:\n                kwargs[i] = checkpoint[i]\n        if (\'angles\' in checkpoint) or rotated_bbox:\n            kwargs[\'rotated_bbox\'] = True\n        # Recreate model from checkpoint instead of from individual backbones\n        model = cls(backbones=checkpoint[\'backbone\'], classes=checkpoint[\'classes\'], **kwargs)\n        model.load_state_dict(checkpoint[\'state_dict\'])\n\n        state = {}\n        for key in (\'iteration\', \'optimizer\', \'scheduler\'):\n            if key in checkpoint:\n                state[key] = checkpoint[key]\n\n        del checkpoint\n        torch.cuda.empty_cache()\n\n        return model, state\n\n    def export(self, size, batch, precision, calibration_files, calibration_table, verbose, onnx_only=False):\n\n        import torch.onnx.symbolic_opset10 as onnx_symbolic\n        def upsample_nearest2d(g, input, output_size, *args):\n            # Currently, TRT 5.1/6.0/7.0 ONNX Parser does not support all ONNX ops\n            # needed to support dynamic upsampling ONNX forumlation\n            # Here we hardcode scale=2 as a temporary workaround\n            scales = g.op(""Constant"", value_t=torch.tensor([1., 1., 2., 2.]))\n            return g.op(""Resize"", input, scales, mode_s=""nearest"")\n\n        onnx_symbolic.upsample_nearest2d = upsample_nearest2d\n\n        # Export to ONNX\n        print(\'Exporting to ONNX...\')\n        self.exporting = True\n        onnx_bytes = io.BytesIO()\n        zero_input = torch.zeros([1, 3, *size]).cuda()\n        extra_args = {\'opset_version\': 10, \'verbose\': verbose}\n        torch.onnx.export(self.cuda(), zero_input, onnx_bytes, **extra_args)\n        self.exporting = False\n\n        if onnx_only:\n            return onnx_bytes.getvalue()\n\n        # Build TensorRT engine\n        model_name = \'_\'.join([k for k, _ in self.backbones.items()])\n        anchors = []\n        if not self.rotated_bbox:\n            anchors = [generate_anchors(stride, self.ratios, self.scales, \n                    self.angles).view(-1).tolist() for stride in self.strides]\n        else:\n            anchors = [generate_anchors_rotated(stride, self.ratios, self.scales, \n                    self.angles)[0].view(-1).tolist() for stride in self.strides]\n        # Set batch_size = 1 batch/GPU for EXPLICIT_BATCH compatibility in TRT\n        batch = 1\n        return Engine(onnx_bytes.getvalue(), len(onnx_bytes.getvalue()), batch, precision,\n                      self.threshold, self.top_n, anchors, self.rotated_bbox, self.nms, self.detections, \n                      calibration_files, model_name, calibration_table, verbose)\n'"
retinanet/train.py,8,"b'from statistics import mean\nfrom math import isfinite\nimport torch\nfrom torch.optim import SGD, AdamW\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom apex import amp, optimizers\nfrom apex.parallel import DistributedDataParallel\nfrom .backbones.layers import convert_fixedbn_model\n\nfrom .data import DataIterator, RotatedDataIterator\nfrom .dali import DaliDataIterator\nfrom .utils import ignore_sigint, post_metrics, Profiler\nfrom .infer import infer\n\n\ndef train(model, state, path, annotations, val_path, val_annotations, resize, max_size, jitter, batch_size, iterations,\n          val_iterations, mixed_precision, lr, warmup, milestones, gamma, is_master=True, world=1, use_dali=True,\n          verbose=True, metrics_url=None, logdir=None, rotate_augment=False, augment_brightness=0.0,\n          augment_contrast=0.0, augment_hue=0.0, augment_saturation=0.0, regularization_l2=0.0001, rotated_bbox=False,\n          absolute_angle=False):\n    \'Train the model on the given dataset\'\n\n    # Prepare model\n    nn_model = model\n    stride = model.stride\n\n    model = convert_fixedbn_model(model)\n    if torch.cuda.is_available():\n        model = model.cuda()\n\n    # Setup optimizer and schedule\n    optimizer = SGD(model.parameters(), lr=lr, weight_decay=regularization_l2, momentum=0.9)\n\n    model, optimizer = amp.initialize(model, optimizer,\n                                      opt_level=\'O2\' if mixed_precision else \'O0\',\n                                      keep_batchnorm_fp32=True,\n                                      loss_scale=128.0,\n                                      verbosity=is_master)\n\n    if world > 1:\n        model = DistributedDataParallel(model)\n    model.train()\n\n    if \'optimizer\' in state:\n        optimizer.load_state_dict(state[\'optimizer\'])\n\n    def schedule(train_iter):\n        if warmup and train_iter <= warmup:\n            return 0.9 * train_iter / warmup + 0.1\n        return gamma ** len([m for m in milestones if m <= train_iter])\n\n    scheduler = LambdaLR(optimizer, schedule)\n\n    # Prepare dataset\n    if verbose: print(\'Preparing dataset...\')\n    if rotated_bbox:\n        if use_dali: raise NotImplementedError(""This repo does not currently support DALI for rotated bbox detections."")\n        data_iterator = RotatedDataIterator(path, jitter, max_size, batch_size, stride,\n                                            world, annotations, training=True, rotate_augment=rotate_augment,\n                                            augment_brightness=augment_brightness,\n                                            augment_contrast=augment_contrast, augment_hue=augment_hue,\n                                            augment_saturation=augment_saturation, absolute_angle=absolute_angle)\n    else:\n        data_iterator = (DaliDataIterator if use_dali else DataIterator)(\n            path, jitter, max_size, batch_size, stride,\n            world, annotations, training=True, rotate_augment=rotate_augment, augment_brightness=augment_brightness,\n            augment_contrast=augment_contrast, augment_hue=augment_hue, augment_saturation=augment_saturation)\n    if verbose: print(data_iterator)\n\n    if verbose:\n        print(\'    device: {} {}\'.format(\n            world, \'cpu\' if not torch.cuda.is_available() else \'GPU\' if world == 1 else \'GPUs\'))\n        print(\'     batch: {}, precision: {}\'.format(batch_size, \'mixed\' if mixed_precision else \'full\'))\n        print(\' BBOX type:\', \'rotated\' if rotated_bbox else \'axis aligned\')\n        print(\'Training model for {} iterations...\'.format(iterations))\n\n    # Create TensorBoard writer\n    if logdir is not None:\n        from tensorboardX import SummaryWriter\n        if is_master and verbose:\n            print(\'Writing TensorBoard logs to: {}\'.format(logdir))\n        writer = SummaryWriter(logdir=logdir)\n\n    profiler = Profiler([\'train\', \'fw\', \'bw\'])\n    iteration = state.get(\'iteration\', 0)\n    while iteration < iterations:\n        cls_losses, box_losses = [], []\n        for i, (data, target) in enumerate(data_iterator):\n            if iteration>=iterations:\n                break\n\n            # Forward pass\n            profiler.start(\'fw\')\n\n            optimizer.zero_grad()\n            cls_loss, box_loss = model([data, target])\n            del data\n            profiler.stop(\'fw\')\n\n            # Backward pass\n            profiler.start(\'bw\')\n            with amp.scale_loss(cls_loss + box_loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n            optimizer.step()\n\n            scheduler.step()\n\n            # Reduce all losses\n            cls_loss, box_loss = cls_loss.mean().clone(), box_loss.mean().clone()\n            if world > 1:\n                torch.distributed.all_reduce(cls_loss)\n                torch.distributed.all_reduce(box_loss)\n                cls_loss /= world\n                box_loss /= world\n            if is_master:\n                cls_losses.append(cls_loss)\n                box_losses.append(box_loss)\n\n            if is_master and not isfinite(cls_loss + box_loss):\n                raise RuntimeError(\'Loss is diverging!\\n{}\'.format(\n                    \'Try lowering the learning rate.\'))\n\n            del cls_loss, box_loss\n            profiler.stop(\'bw\')\n\n            iteration += 1\n            profiler.bump(\'train\')\n            if is_master and (profiler.totals[\'train\'] > 60 or iteration == iterations):\n                focal_loss = torch.stack(list(cls_losses)).mean().item()\n                box_loss = torch.stack(list(box_losses)).mean().item()\n                learning_rate = optimizer.param_groups[0][\'lr\']\n                if verbose:\n                    msg = \'[{:{len}}/{}]\'.format(iteration, iterations, len=len(str(iterations)))\n                    msg += \' focal loss: {:.3f}\'.format(focal_loss)\n                    msg += \', box loss: {:.3f}\'.format(box_loss)\n                    msg += \', {:.3f}s/{}-batch\'.format(profiler.means[\'train\'], batch_size)\n                    msg += \' (fw: {:.3f}s, bw: {:.3f}s)\'.format(profiler.means[\'fw\'], profiler.means[\'bw\'])\n                    msg += \', {:.1f} im/s\'.format(batch_size / profiler.means[\'train\'])\n                    msg += \', lr: {:.2g}\'.format(learning_rate)\n                    print(msg, flush=True)\n\n                if logdir is not None:\n                    writer.add_scalar(\'focal_loss\', focal_loss, iteration)\n                    writer.add_scalar(\'box_loss\', box_loss, iteration)\n                    writer.add_scalar(\'learning_rate\', learning_rate, iteration)\n                    del box_loss, focal_loss\n\n                if metrics_url:\n                    post_metrics(metrics_url, {\n                        \'focal loss\': mean(cls_losses),\n                        \'box loss\': mean(box_losses),\n                        \'im_s\': batch_size / profiler.means[\'train\'],\n                        \'lr\': learning_rate\n                    })\n\n                # Save model weights\n                state.update({\n                    \'iteration\': iteration,\n                    \'optimizer\': optimizer.state_dict(),\n                    \'scheduler\': scheduler.state_dict(),\n                })\n                with ignore_sigint():\n                    nn_model.save(state)\n\n                profiler.reset()\n                del cls_losses[:], box_losses[:]\n\n            if val_annotations and (iteration == iterations or iteration % val_iterations == 0):\n                infer(model, val_path, None, resize, max_size, batch_size, annotations=val_annotations,\n                      mixed_precision=mixed_precision, is_master=is_master, world=world, use_dali=use_dali,\n                      is_validation=True, verbose=False, rotated_bbox=rotated_bbox)\n                model.train()\n\n            if (iteration==iterations and not rotated_bbox) or (iteration>iterations and rotated_bbox):\n                break\n\n    if logdir is not None:\n        writer.close()\n'"
retinanet/utils.py,24,"b""import os.path\nimport time\nimport json\nimport warnings\nimport signal\nfrom datetime import datetime\nfrom contextlib import contextmanager\nfrom PIL import Image, ImageDraw\nimport requests\nimport numpy as np\nimport math\nimport torch\n\ndef order_points(pts):\n    pts_reorder = []\n\n    for idx, pt in enumerate(pts):\n        idx = torch.argsort(pt[:, 0])\n        xSorted = pt[idx, :]\n        leftMost = xSorted[:2, :]\n        rightMost = xSorted[2:, :]\n\n        leftMost = leftMost[torch.argsort(leftMost[:, 1]), :]\n        (tl, bl) = leftMost\n\n        D = torch.cdist(tl[np.newaxis], rightMost)[0]\n        (br, tr) = rightMost[torch.argsort(D, descending=True), :]\n        pts_reorder.append(torch.stack([tl, tr, br, bl]))\n\n    return torch.stack([p for p in pts_reorder])\n\ndef rotate_boxes(boxes, points=False):\n    '''\n    Rotate target bounding boxes \n    \n    Input:  \n        Target boxes (xmin_ymin, width_height, theta)\n    Output:\n        boxes_axis (xmin_ymin, xmax_ymax, theta)\n        boxes_rotated (xy0, xy1, xy2, xy3)\n    '''\n\n    u = torch.stack([torch.cos(boxes[:,4]), torch.sin(boxes[:,4])], dim=1)\n    l = torch.stack([-torch.sin(boxes[:,4]), torch.cos(boxes[:,4])], dim=1)\n    R = torch.stack([u, l], dim=1)\n\n    if points:\n        cents = torch.stack([(boxes[:,0]+boxes[:,2])/2, (boxes[:,1]+boxes[:,3])/2],1).transpose(1,0)\n        boxes_rotated = torch.stack([boxes[:,0],boxes[:,1], \n            boxes[:,2], boxes[:,1], \n            boxes[:,2], boxes[:,3], \n            boxes[:,0], boxes[:,3], \n            boxes[:,-2],\n            boxes[:,-1]],1)\n\n    else:\n        cents = torch.stack([boxes[:,0]+(boxes[:,2]-1)/2, boxes[:,1]+(boxes[:,3]-1)/2],1).transpose(1,0)\n        boxes_rotated = torch.stack([boxes[:,0],boxes[:,1], \n            (boxes[:,0]+boxes[:,2]-1), boxes[:,1], \n            (boxes[:,0]+boxes[:,2]-1), (boxes[:,1]+boxes[:,3]-1), \n            boxes[:,0], (boxes[:,1]+boxes[:,3]-1), \n            boxes[:,-2],\n            boxes[:,-1]],1)\n\n    xy0R = torch.matmul(R,boxes_rotated[:,:2].transpose(1,0) - cents) + cents\n    xy1R = torch.matmul(R,boxes_rotated[:,2:4].transpose(1,0) - cents) + cents\n    xy2R = torch.matmul(R,boxes_rotated[:,4:6].transpose(1,0) - cents) + cents\n    xy3R = torch.matmul(R,boxes_rotated[:,6:8].transpose(1,0) - cents) + cents\n\n    xy0R = torch.stack([xy0R[i,:,i] for i in range(xy0R.size(0))])\n    xy1R = torch.stack([xy1R[i,:,i] for i in range(xy1R.size(0))])\n    xy2R = torch.stack([xy2R[i,:,i] for i in range(xy2R.size(0))])\n    xy3R = torch.stack([xy3R[i,:,i] for i in range(xy3R.size(0))])\n\n    boxes_axis = torch.cat([boxes[:, :2], boxes[:, :2] + boxes[:, 2:4] - 1, \n        torch.sin(boxes[:,-1, None]), torch.cos(boxes[:,-1, None])], 1)\n    boxes_rotated = order_points(torch.stack([xy0R,xy1R,xy2R,xy3R],dim = 1)).view(-1,8)\n    \n    return boxes_axis, boxes_rotated\n\n\n\ndef rotate_box(bbox):\n    xmin, ymin, width, height, theta = bbox\n\n    xy1 = xmin, ymin\n    xy2 = xmin, ymin + height - 1\n    xy3 = xmin + width - 1, ymin + height - 1\n    xy4 = xmin + width - 1, ymin\n\n    cents = np.array([xmin + (width - 1) / 2, ymin + (height - 1) / 2])\n\n    corners = np.stack([xy1, xy2, xy3, xy4])\n\n    u = np.stack([np.cos(theta), -np.sin(theta)])\n    l = np.stack([np.sin(theta), np.cos(theta)])\n    R = np.vstack([u, l])\n\n    corners = np.matmul(R, (corners - cents).transpose(1, 0)).transpose(1, 0) + cents\n\n    return corners.reshape(-1).tolist()\n\n\ndef show_detections(detections):\n    'Show image with drawn detections'\n\n    for image, detections in detections.items():\n        im = Image.open(image).convert('RGBA')\n        overlay = Image.new('RGBA', im.size, (255, 255, 255, 0))\n        draw = ImageDraw.Draw(overlay)\n        detections.sort(key=lambda d: d['score'])\n        for detection in detections:\n            box = detection['bbox']\n            alpha = int(detection['score'] * 255)\n            draw.rectangle(box, outline=(255, 255, 255, alpha))\n            draw.text((box[0] + 2, box[1]), '[{}]'.format(detection['class']),\n                      fill=(255, 255, 255, alpha))\n            draw.text((box[0] + 2, box[1] + 10), '{:.2}'.format(detection['score']),\n                      fill=(255, 255, 255, alpha))\n        im = Image.alpha_composite(im, overlay)\n        im.show()\n\n\ndef save_detections(path, detections):\n    print('Writing detections to {}...'.format(os.path.basename(path)))\n    with open(path, 'w') as f:\n        json.dump(detections, f)\n\n\n@contextmanager\ndef ignore_sigint():\n    handler = signal.getsignal(signal.SIGINT)\n    signal.signal(signal.SIGINT, signal.SIG_IGN)\n    try:\n        yield\n    finally:\n        signal.signal(signal.SIGINT, handler)\n\n\nclass Profiler(object):\n    def __init__(self, names=['main']):\n        self.names = names\n        self.lasts = {k: 0 for k in names}\n        self.totals = self.lasts.copy()\n        self.counts = self.lasts.copy()\n        self.means = self.lasts.copy()\n        self.reset()\n\n    def reset(self):\n        last = time.time()\n        for name in self.names:\n            self.lasts[name] = last\n            self.totals[name] = 0\n            self.counts[name] = 0\n            self.means[name] = 0\n\n    def start(self, name='main'):\n        self.lasts[name] = time.time()\n\n    def stop(self, name='main'):\n        self.totals[name] += time.time() - self.lasts[name]\n        self.counts[name] += 1\n        self.means[name] = self.totals[name] / self.counts[name]\n\n    def bump(self, name='main'):\n        self.stop(name)\n        self.start(name)\n\n\ndef post_metrics(url, metrics):\n    try:\n        for k, v in metrics.items():\n            requests.post(url,\n                          data={'time': int(datetime.now().timestamp() * 1e9),\n                                'metric': k, 'value': v})\n    except Exception as e:\n        warnings.warn('Warning: posting metrics failed: {}'.format(e))\n"""
extras/cppapi/generate_anchors.py,0,"b'import numpy as np\nfrom retinanet.box import generate_anchors, generate_anchors_rotated\n\n# Generates anchors for export.cpp\n\n# ratios = [1.0, 2.0, 0.5]\n# scales = [4 * 2 ** (i / 3) for i in range(3)]\nratios = [0.25, 0.5, 1.0, 2.0, 4.0]\nscales = [2 * 2**(2 * i/3) for i in range(3)]\nangles = [-np.pi / 6, 0, np.pi / 6]\nstrides = [2**i for i in range(3,8)]\n\naxis = str(np.round([generate_anchors(stride, ratios, scales, \n            angles).view(-1).tolist() for stride in strides], decimals=2).tolist()\n        ).replace(\'[\', \'{\').replace(\']\', \'}\').replace(\'}, \', \'},\\n\')\n\nrot = str(np.round([generate_anchors_rotated(stride, ratios, scales, \n            angles)[0].view(-1).tolist() for stride in strides], decimals=2).tolist()\n        ).replace(\'[\', \'{\').replace(\']\', \'}\').replace(\'}, \', \'},\\n\')\n\nprint(""Axis-aligned:\\n""+axis+\'\\n\')\nprint(""Rotated:\\n""+rot)\n'"
retinanet/backbones/__init__.py,0,b'import sys\n\nfrom .resnet import *\nfrom .mobilenet import *\nfrom .fpn import *\n'
retinanet/backbones/fpn.py,2,"b""import torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.models import resnet as vrn\nfrom torchvision.models import mobilenet as vmn\n\nfrom .resnet import ResNet\nfrom .mobilenet import MobileNet\nfrom .utils import register\n\n\nclass FPN(nn.Module):\n    'Feature Pyramid Network - https://arxiv.org/abs/1612.03144'\n\n    def __init__(self, features):\n        super().__init__()\n\n        self.stride = 128\n        self.features = features\n\n        if isinstance(features, ResNet):\n            is_light = features.bottleneck == vrn.BasicBlock\n            channels = [128, 256, 512] if is_light else [512, 1024, 2048]\n        elif isinstance(features, MobileNet):\n            channels = [32, 96, 320]\n\n        self.lateral3 = nn.Conv2d(channels[0], 256, 1)\n        self.lateral4 = nn.Conv2d(channels[1], 256, 1)\n        self.lateral5 = nn.Conv2d(channels[2], 256, 1)\n        self.pyramid6 = nn.Conv2d(channels[2], 256, 3, stride=2, padding=1)\n        self.pyramid7 = nn.Conv2d(256, 256, 3, stride=2, padding=1)\n        self.smooth3 = nn.Conv2d(256, 256, 3, padding=1)\n        self.smooth4 = nn.Conv2d(256, 256, 3, padding=1)\n        self.smooth5 = nn.Conv2d(256, 256, 3, padding=1)\n\n    def initialize(self):\n        def init_layer(layer):\n            if isinstance(layer, nn.Conv2d):\n                nn.init.xavier_uniform_(layer.weight)\n                if layer.bias is not None:\n                    nn.init.constant_(layer.bias, val=0)\n        self.apply(init_layer)\n\n        self.features.initialize()\n\n    def forward(self, x):\n        c3, c4, c5 = self.features(x)\n\n        p5 = self.lateral5(c5)\n        p4 = self.lateral4(c4)\n        p4 = F.interpolate(p5, scale_factor=2) + p4\n        p3 = self.lateral3(c3)\n        p3 = F.interpolate(p4, scale_factor=2) + p3\n\n        p6 = self.pyramid6(c5)\n        p7 = self.pyramid7(F.relu(p6))\n\n        p3 = self.smooth3(p3)\n        p4 = self.smooth4(p4)\n        p5 = self.smooth5(p5)\n\n        return [p3, p4, p5, p6, p7]\n\n@register\ndef ResNet18FPN():\n    return FPN(ResNet(layers=[2, 2, 2, 2], bottleneck=vrn.BasicBlock, outputs=[3, 4, 5], url=vrn.model_urls['resnet18']))\n\n@register\ndef ResNet34FPN():\n    return FPN(ResNet(layers=[3, 4, 6, 3], bottleneck=vrn.BasicBlock, outputs=[3, 4, 5], url=vrn.model_urls['resnet34']))\n\n@register\ndef ResNet50FPN():\n    return FPN(ResNet(layers=[3, 4, 6, 3], bottleneck=vrn.Bottleneck, outputs=[3, 4, 5], url=vrn.model_urls['resnet50']))\n\n@register\ndef ResNet101FPN():\n    return FPN(ResNet(layers=[3, 4, 23, 3], bottleneck=vrn.Bottleneck, outputs=[3, 4, 5], url=vrn.model_urls['resnet101']))\n\n@register\ndef ResNet152FPN():\n    return FPN(ResNet(layers=[3, 8, 36, 3], bottleneck=vrn.Bottleneck, outputs=[3, 4, 5], url=vrn.model_urls['resnet152']))\n\n@register\ndef ResNeXt50_32x4dFPN():\n    return FPN(ResNet(layers=[3, 4, 6, 3], bottleneck=vrn.Bottleneck, outputs=[3, 4, 5], groups=32, width_per_group=4, url=vrn.model_urls['resnext50_32x4d']))\n\n@register\ndef ResNeXt101_32x8dFPN():\n    return FPN(ResNet(layers=[3, 4, 23, 3], bottleneck=vrn.Bottleneck, outputs=[3, 4, 5], groups=32, width_per_group=8, url=vrn.model_urls['resnext101_32x8d']))\n\n@register\ndef MobileNetV2FPN():\n    return FPN(MobileNet(outputs=[6, 13, 17], url=vmn.model_urls['mobilenet_v2']))\n"""
retinanet/backbones/layers.py,5,"b'import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nclass FixedBatchNorm2d(nn.Module):\n    \'BatchNorm2d where the batch statistics and the affine parameters are fixed\'\n\n    def __init__(self, n):\n        super().__init__()\n        self.register_buffer(""weight"", torch.ones(n))\n        self.register_buffer(""bias"", torch.zeros(n))\n        self.register_buffer(""running_mean"", torch.zeros(n))\n        self.register_buffer(""running_var"", torch.ones(n))\n        \n    def forward(self, x):\n        return F.batch_norm(x, running_mean=self.running_mean, running_var=self.running_var, weight=self.weight, bias=self.bias)\n\ndef convert_fixedbn_model(module):\n    \'Convert batch norm layers to fixed\'\n\n    mod = module\n    if isinstance(module, nn.BatchNorm2d):\n        mod = FixedBatchNorm2d(module.num_features)\n        mod.running_mean = module.running_mean\n        mod.running_var = module.running_var\n        if module.affine:\n            mod.weight.data = module.weight.data.clone().detach()\n            mod.bias.data = module.bias.data.clone().detach()\n    for name, child in module.named_children():\n        mod.add_module(name, convert_fixedbn_model(child))\n\n    return mod\n'"
retinanet/backbones/mobilenet.py,2,"b""import torch.nn as nn\nfrom torchvision.models import mobilenet as vmn\nimport torch.utils.model_zoo as model_zoo\n\nclass MobileNet(vmn.MobileNetV2):\n    'MobileNetV2: Inverted Residuals and Linear Bottlenecks - https://arxiv.org/abs/1801.04381'\n\n    def __init__(self, outputs=[18], url=None):\n        self.stride = 128\n        self.url = url\n        super().__init__()\n        self.outputs = outputs\n\n    def initialize(self):\n        if self.url:\n            self.load_state_dict(model_zoo.load_url(self.url))\n\n    def forward(self, x):\n        outputs = []\n        for indx, feat in enumerate(self.features[:-1]):\n            x = feat(x)\n            if indx in self.outputs:\n                outputs.append(x)\n        return outputs\n"""
retinanet/backbones/resnet.py,1,"b""import torchvision\nfrom torchvision.models import resnet as vrn\nimport torch.utils.model_zoo as model_zoo\n\nfrom .utils import register\n\nclass ResNet(vrn.ResNet):\n    'Deep Residual Network - https://arxiv.org/abs/1512.03385'\n\n    def __init__(self, layers=[3, 4, 6, 3], bottleneck=vrn.Bottleneck, outputs=[5], groups=1, width_per_group=64, url=None):\n        self.stride = 128        \n        self.bottleneck = bottleneck\n        self.outputs = outputs\n        self.url = url\n\n        kwargs = {'block': bottleneck, 'layers': layers, 'groups': groups, 'width_per_group': width_per_group}\n        super().__init__(**kwargs)\n\n    def initialize(self):\n        if self.url:\n            self.load_state_dict(model_zoo.load_url(self.url))\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        outputs = []\n        for i, layer in enumerate([self.layer1, self.layer2, self.layer3, self.layer4]):\n            level = i + 2\n            if level > max(self.outputs):\n                break\n            x = layer(x)\n            if level in self.outputs:\n                outputs.append(x)\n\n        return outputs\n\n@register\ndef ResNet18C4():\n    return ResNet(layers=[2, 2, 2, 2], bottleneck=vrn.BasicBlock, outputs=[4], url=vrn.model_urls['resnet18'])\n\n@register\ndef ResNet34C4():\n    return ResNet(layers=[3, 4, 6, 3], bottleneck=vrn.BasicBlock, outputs=[4], url=vrn.model_urls['resnet34'])\n"""
retinanet/backbones/utils.py,0,"b""import sys\nimport torchvision\n\ndef register(f):\n    all = sys.modules[f.__module__].__dict__.setdefault('__all__', [])\n    if f.__name__ in all:\n        raise RuntimeError('{} already exist!'.format(f.__name__))\n    all.append(f.__name__)\n    return f\n"""
