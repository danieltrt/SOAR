file_path,api_count,code
data.py,2,"b'""""""\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\nimport torch.utils.data as data\nimport os.path\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\n\ndef default_flist_reader(flist):\n    """"""\n    flist format: impath label\\nimpath label\\n ...(same to caffe\'s filelist)\n    """"""\n    imlist = []\n    with open(flist, \'r\') as rf:\n        for line in rf.readlines():\n            impath = line.strip()\n            imlist.append(impath)\n\n    return imlist\n\n\nclass ImageFilelist(data.Dataset):\n    def __init__(self, root, flist, transform=None,\n                 flist_reader=default_flist_reader, loader=default_loader):\n        self.root = root\n        self.imlist = flist_reader(flist)\n        self.transform = transform\n        self.loader = loader\n\n    def __getitem__(self, index):\n        impath = self.imlist[index]\n        img = self.loader(os.path.join(self.root, impath))\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img\n\n    def __len__(self):\n        return len(self.imlist)\n\n\nclass ImageLabelFilelist(data.Dataset):\n    def __init__(self, root, flist, transform=None,\n                 flist_reader=default_flist_reader, loader=default_loader):\n        self.root = root\n        self.imlist = flist_reader(os.path.join(self.root, flist))\n        self.transform = transform\n        self.loader = loader\n        self.classes = sorted(list(set([path.split(\'/\')[0] for path in self.imlist])))\n        self.class_to_idx = {self.classes[i]: i for i in range(len(self.classes))}\n        self.imgs = [(impath, self.class_to_idx[impath.split(\'/\')[0]]) for impath in self.imlist]\n\n    def __getitem__(self, index):\n        impath, label = self.imgs[index]\n        img = self.loader(os.path.join(self.root, impath))\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, label\n\n    def __len__(self):\n        return len(self.imgs)\n\n###############################################################################\n# Code from\n# https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n# Modified the original code so that it also loads images from the current\n# directory as well as the subdirectories\n###############################################################################\n\nimport torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef make_dataset(dir):\n    images = []\n    assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n\n    for root, _, fnames in sorted(os.walk(dir)):\n        for fname in fnames:\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                images.append(path)\n\n    return images\n\n\nclass ImageFolder(data.Dataset):\n\n    def __init__(self, root, transform=None, return_paths=False,\n                 loader=default_loader):\n        imgs = sorted(make_dataset(root))\n        if len(imgs) == 0:\n            raise(RuntimeError(""Found 0 images in: "" + root + ""\\n""\n                               ""Supported image extensions are: "" +\n                               "","".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n        self.return_paths = return_paths\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.return_paths:\n            return img, path\n        else:\n            return img\n\n    def __len__(self):\n        return len(self.imgs)\n'"
networks.py,29,"b'""""""\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch\nimport torch.nn.functional as F\nfrom torchvision import models\nfrom utils import weights_init\n\ntry:\n    from itertools import izip as zip\nexcept ImportError: # will be 3.x series\n    pass\n\n\n##################################################################################\n# Discriminator\n##################################################################################\n\nclass MsImageDis(nn.Module):\n    # Multi-scale discriminator architecture\n    def __init__(self, input_dim, params, fp16):\n        super(MsImageDis, self).__init__()\n        self.n_layer = params[\'n_layer\']\n        self.gan_type = params[\'gan_type\']\n        self.dim = params[\'dim\']\n        self.norm = params[\'norm\']\n        self.activ = params[\'activ\']\n        self.num_scales = params[\'num_scales\']\n        self.pad_type = params[\'pad_type\']\n        self.LAMBDA = params[\'LAMBDA\']\n        self.non_local = params[\'non_local\']\n        self.n_res = params[\'n_res\']\n        self.input_dim = input_dim\n        self.fp16 = fp16\n        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n        if not self.gan_type == \'wgan\':\n            self.cnns = nn.ModuleList()\n            for _ in range(self.num_scales):\n                Dis = self._make_net()\n                Dis.apply(weights_init(\'gaussian\'))\n                self.cnns.append(Dis)\n        else:\n             self.cnn = self.one_cnn()\n\n    def _make_net(self):\n        dim = self.dim\n        cnn_x = []\n        cnn_x += [Conv2dBlock(self.input_dim, dim, 1, 1, 0, norm=self.norm, activation=self.activ, pad_type=self.pad_type)]\n        cnn_x += [Conv2dBlock(dim, dim, 3, 1, 1, norm=self.norm, activation=self.activ, pad_type=self.pad_type)]\n        cnn_x += [Conv2dBlock(dim, dim, 3, 2, 1, norm=self.norm, activation=self.activ, pad_type=self.pad_type)]\n        for i in range(self.n_layer - 1):\n            dim2 = min(dim*2, 512)\n            cnn_x += [Conv2dBlock(dim, dim, 3, 1, 1, norm=self.norm, activation=self.activ, pad_type=self.pad_type)]\n            cnn_x += [Conv2dBlock(dim, dim2, 3, 2, 1, norm=self.norm, activation=self.activ, pad_type=self.pad_type)]\n            dim = dim2\n        if self.non_local>1:\n            cnn_x += [NonlocalBlock(dim)]\n        for i in range(self.n_res):\n            cnn_x += [ResBlock(dim, norm=self.norm, activation=self.activ, pad_type=self.pad_type, res_type=\'basic\')] \n        if self.non_local>0:\n            cnn_x += [NonlocalBlock(dim)]\n        cnn_x += [nn.Conv2d(dim, 1, 1, 1, 0)]\n        cnn_x = nn.Sequential(*cnn_x)\n        return cnn_x\n\n    def one_cnn(self):\n        dim = self.dim\n        cnn_x = []\n        cnn_x += [Conv2dBlock(self.input_dim, dim, 4, 2, 1, norm=\'none\', activation=self.activ, pad_type=self.pad_type)]\n        for i in range(5):\n            dim2 = min(dim*2, 512)\n            cnn_x += [Conv2dBlock(dim, dim2, 4, 2, 1, norm=self.norm, activation=self.activ, pad_type=self.pad_type)]\n            dim = dim2\n        cnn_x += [nn.Conv2d(dim, 1, (4,2), 1, 0)]\n        cnn_x = nn.Sequential(*cnn_x)\n        return cnn_x\n\n    def forward(self, x):\n        if not self.gan_type == \'wgan\':\n            outputs = []\n            for model in self.cnns:\n                outputs.append(model(x))\n                x = self.downsample(x)\n        else:\n             outputs = self.cnn(x)\n             outputs = torch.squeeze(outputs)\n        return outputs\n\n    def calc_dis_loss(self, model, input_fake, input_real):\n        # calculate the loss to train D\n        input_real.requires_grad_()\n        outs0 = model.forward(input_fake)\n        outs1 = model.forward(input_real)\n        loss = 0\n        reg = 0\n        Drift = 0.001\n        LAMBDA = self.LAMBDA\n\n        if self.gan_type == \'wgan\':\n            loss += torch.mean(outs0) - torch.mean(outs1)\n            # progressive gan\n            loss += Drift*( torch.sum(outs0**2) + torch.sum(outs1**2))\n            #alpha = torch.FloatTensor(input_fake.shape).uniform_(0., 1.)\n            #alpha = alpha.cuda()\n            #differences = input_fake - input_real\n            #interpolates =  Variable(input_real + (alpha*differences), requires_grad=True)\n            #dis_interpolates = self.forward(interpolates) \n            #gradient_penalty = self.compute_grad2(dis_interpolates, interpolates).mean()\n            #reg += LAMBDA*gradient_penalty \n            reg += LAMBDA* self.compute_grad2(outs1, input_real).mean() # I suggest Lambda=0.1 for wgan\n            loss = loss + reg\n            return loss, reg\n\n        for it, (out0, out1) in enumerate(zip(outs0, outs1)):\n            if self.gan_type == \'lsgan\':\n                loss += torch.mean((out0 - 0)**2) + torch.mean((out1 - 1)**2)\n                # regularization\n                reg += LAMBDA* self.compute_grad2(out1, input_real).mean()\n            elif self.gan_type == \'nsgan\':\n                all0 = Variable(torch.zeros_like(out0.data).cuda(), requires_grad=False)\n                all1 = Variable(torch.ones_like(out1.data).cuda(), requires_grad=False)\n                loss += torch.mean(F.binary_cross_entropy(F.sigmoid(out0), all0) +\n                                   F.binary_cross_entropy(F.sigmoid(out1), all1))\n                reg += LAMBDA* self.compute_grad2(F.sigmoid(out1), input_real).mean()\n            else:\n                assert 0, ""Unsupported GAN type: {}"".format(self.gan_type)\n\n        loss = loss+reg\n        return loss, reg\n\n    def calc_gen_loss(self, model, input_fake):\n        # calculate the loss to train G\n        outs0 = model.forward(input_fake)\n        loss = 0\n        Drift = 0.001\n        if self.gan_type == \'wgan\':\n            loss += -torch.mean(outs0)\n            # progressive gan\n            loss += Drift*torch.sum(outs0**2)\n            return loss\n\n        for it, (out0) in enumerate(outs0):\n            if self.gan_type == \'lsgan\':\n                loss += torch.mean((out0 - 1)**2) * 2  # LSGAN\n            elif self.gan_type == \'nsgan\':\n                all1 = Variable(torch.ones_like(out0.data).cuda(), requires_grad=False)\n                loss += torch.mean(F.binary_cross_entropy(F.sigmoid(out0), all1))\n            else:\n                assert 0, ""Unsupported GAN type: {}"".format(self.gan_type)\n        return loss\n\n    def compute_grad2(self, d_out, x_in):\n        batch_size = x_in.size(0)\n        grad_dout = torch.autograd.grad(\n            outputs=d_out.sum(), inputs=x_in,\n            create_graph=True, retain_graph=True, only_inputs=True\n        )[0]\n        grad_dout2 = grad_dout.pow(2)\n        assert(grad_dout2.size() == x_in.size())\n        reg = grad_dout2.view(batch_size, -1).sum(1)\n        return reg\n\n##################################################################################\n# Generator\n##################################################################################\n\nclass AdaINGen(nn.Module):\n    # AdaIN auto-encoder architecture\n    def __init__(self, input_dim, params, fp16):\n        super(AdaINGen, self).__init__()\n        dim = params[\'dim\']\n        n_downsample = params[\'n_downsample\']\n        n_res = params[\'n_res\']\n        activ = params[\'activ\']\n        pad_type = params[\'pad_type\']\n        mlp_dim = params[\'mlp_dim\']\n        mlp_norm = params[\'mlp_norm\']\n        id_dim = params[\'id_dim\']\n        which_dec = params[\'dec\']\n        dropout = params[\'dropout\']\n        tanh = params[\'tanh\']\n        non_local = params[\'non_local\']\n\n        # content encoder\n        self.enc_content = ContentEncoder(n_downsample, n_res, input_dim, dim, \'in\', activ, pad_type=pad_type, dropout=dropout, tanh=tanh, res_type=\'basic\')\n\n        self.output_dim = self.enc_content.output_dim\n        if which_dec ==\'basic\':        \n            self.dec = Decoder(n_downsample, n_res, self.output_dim, 3, dropout=dropout, res_norm=\'adain\', activ=activ, pad_type=pad_type, res_type=\'basic\', non_local = non_local, fp16 = fp16)\n        elif which_dec ==\'slim\':\n            self.dec = Decoder(n_downsample, n_res, self.output_dim, 3, dropout=dropout, res_norm=\'adain\', activ=activ, pad_type=pad_type, res_type=\'slim\', non_local = non_local, fp16 = fp16)\n        elif which_dec ==\'series\':\n            self.dec = Decoder(n_downsample, n_res, self.output_dim, 3, dropout=dropout, res_norm=\'adain\', activ=activ, pad_type=pad_type, res_type=\'series\', non_local = non_local, fp16 = fp16)\n        elif which_dec ==\'parallel\':\n            self.dec = Decoder(n_downsample, n_res, self.output_dim, 3, dropout=dropout, res_norm=\'adain\', activ=activ, pad_type=pad_type, res_type=\'parallel\', non_local = non_local, fp16 = fp16)\n        else:\n            (\'unkonw decoder type\')\n\n        # MLP to generate AdaIN parameters\n        self.mlp_w1 = MLP(id_dim, 2*self.output_dim, mlp_dim, 3, norm=mlp_norm, activ=activ)\n        self.mlp_w2 = MLP(id_dim, 2*self.output_dim, mlp_dim, 3, norm=mlp_norm, activ=activ)\n        self.mlp_w3 = MLP(id_dim, 2*self.output_dim, mlp_dim, 3, norm=mlp_norm, activ=activ)\n        self.mlp_w4 = MLP(id_dim, 2*self.output_dim, mlp_dim, 3, norm=mlp_norm, activ=activ)\n        \n        self.mlp_b1 = MLP(id_dim, 2*self.output_dim, mlp_dim, 3, norm=mlp_norm, activ=activ)\n        self.mlp_b2 = MLP(id_dim, 2*self.output_dim, mlp_dim, 3, norm=mlp_norm, activ=activ)\n        self.mlp_b3 = MLP(id_dim, 2*self.output_dim, mlp_dim, 3, norm=mlp_norm, activ=activ)\n        self.mlp_b4 = MLP(id_dim, 2*self.output_dim, mlp_dim, 3, norm=mlp_norm, activ=activ)\n\n        self.apply(weights_init(params[\'init\']))\n\n    def encode(self, images):\n        # encode an image to its content and style codes\n        content = self.enc_content(images)\n        return content\n\n    def decode(self, content, ID):\n        # decode style codes to an image\n        ID1 = ID[:,:2048]\n        ID2 = ID[:,2048:4096]\n        ID3 = ID[:,4096:6144]\n        ID4 = ID[:,6144:]\n        adain_params_w = torch.cat( (self.mlp_w1(ID1), self.mlp_w2(ID2), self.mlp_w3(ID3), self.mlp_w4(ID4)), 1)\n        adain_params_b = torch.cat( (self.mlp_b1(ID1), self.mlp_b2(ID2), self.mlp_b3(ID3), self.mlp_b4(ID4)), 1)\n        self.assign_adain_params(adain_params_w, adain_params_b, self.dec)\n        images = self.dec(content)\n        return images\n\n    def assign_adain_params(self, adain_params_w, adain_params_b, model):\n        # assign the adain_params to the AdaIN layers in model\n        dim = self.output_dim\n        for m in model.modules():\n            if m.__class__.__name__ == ""AdaptiveInstanceNorm2d"":\n                mean = adain_params_b[:,:dim].contiguous()\n                std = adain_params_w[:,:dim].contiguous()\n                m.bias = mean.view(-1)\n                m.weight = std.view(-1)\n                if adain_params_w.size(1)>dim :  #Pop the parameters\n                    adain_params_b = adain_params_b[:,dim:]\n                    adain_params_w = adain_params_w[:,dim:]\n\n    def get_num_adain_params(self, model):\n        # return the number of AdaIN parameters needed by the model\n        num_adain_params = 0\n        for m in model.modules():\n            if m.__class__.__name__ == ""AdaptiveInstanceNorm2d"":\n                num_adain_params += m.num_features\n        return num_adain_params\n\n\nclass VAEGen(nn.Module):\n    # VAE architecture\n    def __init__(self, input_dim, params):\n        super(VAEGen, self).__init__()\n        dim = params[\'dim\']\n        n_downsample = params[\'n_downsample\']\n        n_res = params[\'n_res\']\n        activ = params[\'activ\']\n        pad_type = params[\'pad_type\']\n\n        # content encoder\n        self.enc = ContentEncoder(n_downsample, n_res, input_dim, dim, \'in\', activ, pad_type=pad_type, dropout=0, tanh=False, res_type=\'basic\')\n        self.dec = Decoder(n_downsample, n_res, self.enc.output_dim, input_dim, res_norm=\'in\', activ=activ, pad_type=pad_type)\n\n    def forward(self, images):\n        # This is a reduced VAE implementation where we assume the outputs are multivariate Gaussian distribution with mean = hiddens and std_dev = all ones.\n        hiddens = self.encode(images)\n        if self.training == True:\n            noise = Variable(torch.randn(hiddens.size()).cuda(hiddens.data.get_device()))\n            images_recon = self.decode(hiddens + noise)\n        else:\n            images_recon = self.decode(hiddens)\n        return images_recon, hiddens\n\n    def encode(self, images):\n        hiddens = self.enc(images)\n        noise = Variable(torch.randn(hiddens.size()).cuda(hiddens.data.get_device()))\n        return hiddens, noise\n\n    def decode(self, hiddens):\n        images = self.dec(hiddens)\n        return images\n\n\n##################################################################################\n# Encoder and Decoders\n##################################################################################\n\nclass StyleEncoder(nn.Module):\n    def __init__(self, n_downsample, input_dim, dim, style_dim, norm, activ, pad_type):\n        super(StyleEncoder, self).__init__()\n        self.model = []\n        # Here I change the stride to 2. \n        self.model += [Conv2dBlock(input_dim, dim, 3, 2, 1, norm=norm, activation=activ, pad_type=pad_type)]\n        self.model += [Conv2dBlock(dim, dim, 3, 1, 1, norm=norm, activation=activ, pad_type=pad_type)]\n        for i in range(2):\n            self.model += [Conv2dBlock(dim, 2 * dim, 3, 2, 1, norm=norm, activation=activ, pad_type=pad_type)]\n            dim *= 2\n        for i in range(n_downsample - 2):\n            self.model += [Conv2dBlock(dim, dim, 3, 2, 1, norm=norm, activation=activ, pad_type=pad_type)]\n        self.model += [nn.AdaptiveAvgPool2d(1)] # global average pooling\n        self.model += [nn.Conv2d(dim, style_dim, 1, 1, 0)]\n        self.model = nn.Sequential(*self.model)\n        self.output_dim = dim\n\n    def forward(self, x):\n        return self.model(x)\n\nclass ContentEncoder(nn.Module):\n    def __init__(self, n_downsample, n_res, input_dim, dim, norm, activ, pad_type, dropout, tanh=False, res_type=\'basic\'):\n        super(ContentEncoder, self).__init__()\n        self.model = []\n        # Here I change the stride to 2.\n        self.model += [Conv2dBlock(input_dim, dim, 3, 2, 1, norm=norm, activation=activ, pad_type=pad_type)]\n        self.model += [Conv2dBlock(dim, 2*dim, 3, 1, 1, norm=norm, activation=activ, pad_type=pad_type)]\n        dim *=2 # 32dim\n        # downsampling blocks\n        for i in range(n_downsample-1):\n            self.model += [Conv2dBlock(dim, dim, 3, 1, 1, norm=norm, activation=activ, pad_type=pad_type)]\n            self.model += [Conv2dBlock(dim, 2 * dim, 3, 2, 1, norm=norm, activation=activ, pad_type=pad_type)]\n            dim *= 2\n        # residual blocks\n        self.model += [ResBlocks(n_res, dim, norm=norm, activation=activ, pad_type=pad_type, res_type=res_type)]\n        # 64 -> 128\n        self.model += [ASPP(dim, norm=norm, activation=activ, pad_type=pad_type)]\n        dim *= 2\n        if tanh:\n            self.model +=[nn.Tanh()]\n        self.model = nn.Sequential(*self.model)\n        self.output_dim = dim\n\n    def forward(self, x):\n        return self.model(x)\n\nclass ContentEncoder_ImageNet(nn.Module):\n    def __init__(self, n_downsample, n_res, input_dim, dim, norm, activ, pad_type):\n        super(ContentEncoder_ImageNet, self).__init__()\n        self.model = models.resnet50(pretrained=True)\n        # remove the final downsample\n        self.model.layer4[0].downsample[0].stride = (1,1)\n        self.model.layer4[0].conv2.stride = (1,1) \n        # (256,128) ----> (16,8)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        return x\n\nclass Decoder(nn.Module):\n    def __init__(self, n_upsample, n_res, dim, output_dim, dropout=0, res_norm=\'adain\', activ=\'relu\', pad_type=\'zero\', res_type=\'basic\', non_local=False, fp16 = False):\n        super(Decoder, self).__init__()\n        self.input_dim = dim\n        self.model = []\n        self.model += [nn.Dropout(p = dropout)]\n        self.model += [ResBlocks(n_res, dim, res_norm, activ, pad_type=pad_type, res_type=res_type)]\n        # non-local\n        if non_local>0:\n            self.model += [NonlocalBlock(dim)]\n            print(\'use non-local!\')\n        for i in range(n_upsample):\n            self.model += [nn.Upsample(scale_factor=2),\n                           Conv2dBlock(dim, dim // 2, 5, 1, 2, norm=\'ln\', activation=activ, pad_type=pad_type, fp16 = fp16)]\n            dim //= 2\n        # use reflection padding in the last conv layer\n        self.model += [Conv2dBlock(dim, dim, 3, 1, 1, norm=\'none\', activation=activ, pad_type=pad_type)]\n        self.model += [Conv2dBlock(dim, dim, 3, 1, 1, norm=\'none\', activation=activ, pad_type=pad_type)]\n        self.model += [Conv2dBlock(dim, output_dim, 1, 1, 0, norm=\'none\', activation=\'none\', pad_type=pad_type)]\n        self.model = nn.Sequential(*self.model)\n\n    def forward(self, x):\n        output = self.model(x)\n        return output\n\n##################################################################################\n# Sequential Models\n##################################################################################\nclass ResBlocks(nn.Module):\n    def __init__(self, num_blocks, dim, norm=\'in\', activation=\'relu\', pad_type=\'zero\', res_type=\'basic\'):\n        super(ResBlocks, self).__init__()\n        self.model = []\n        self.res_type = res_type\n        for i in range(num_blocks):\n            self.model += [ResBlock(dim, norm=norm, activation=activation, pad_type=pad_type, res_type=res_type)]\n        self.model = nn.Sequential(*self.model)\n\n    def forward(self, x):\n        return self.model(x)\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim, output_dim, dim, n_blk, norm=\'in\', activ=\'relu\'):\n\n        super(MLP, self).__init__()\n        self.model = []\n        self.model += [LinearBlock(input_dim, dim, norm=norm, activation=activ)]\n        for i in range(n_blk - 2):\n            self.model += [LinearBlock(dim, dim, norm=norm, activation=activ)]\n        self.model += [LinearBlock(dim, output_dim, norm=\'none\', activation=\'none\')] # no output activations\n        self.model = nn.Sequential(*self.model)\n\n    def forward(self, x):\n        return self.model(x.view(x.size(0), -1))\n\n# enlarge the ID 2time\nclass Deconv(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(Deconv, self).__init__()\n        model = []\n        model += [nn.ConvTranspose2d( input_dim, output_dim, kernel_size=(2,2), stride=2)]\n        model += [nn.InstanceNorm2d(output_dim)]\n        model += [nn.ReLU(inplace=True)]\n        model += [nn.Conv2d( output_dim, output_dim, kernel_size=(1,1), stride=1)]\n        self.model = nn.Sequential(*model)\n    def forward(self, x):\n        return self.model(x)\n\n##################################################################################\n# Basic Blocks\n##################################################################################\nclass ResBlock(nn.Module):\n    def __init__(self, dim, norm, activation=\'relu\', pad_type=\'zero\', res_type=\'basic\'):\n        super(ResBlock, self).__init__()\n\n        model = []\n        if res_type==\'basic\' or res_type==\'nonlocal\':\n            model += [Conv2dBlock(dim ,dim, 3, 1, 1, norm=norm, activation=activation, pad_type=pad_type)]\n            model += [Conv2dBlock(dim ,dim, 3, 1, 1, norm=norm, activation=\'none\', pad_type=pad_type)]\n        elif res_type==\'slim\':\n            dim_half = dim//2\n            model += [Conv2dBlock(dim ,dim_half, 1, 1, 0, norm=\'in\', activation=activation, pad_type=pad_type)]\n            model += [Conv2dBlock(dim_half, dim_half, 3, 1, 1, norm=norm, activation=activation, pad_type=pad_type)]\n            model += [Conv2dBlock(dim_half, dim_half, 3, 1, 1, norm=norm, activation=activation, pad_type=pad_type)]\n            model += [Conv2dBlock(dim_half, dim, 1, 1, 0, norm=\'in\', activation=\'none\', pad_type=pad_type)]\n        elif res_type==\'series\':\n            model += [Series2dBlock(dim ,dim, 3, 1, 1, norm=norm, activation=activation, pad_type=pad_type)]\n            model += [Series2dBlock(dim ,dim, 3, 1, 1, norm=norm, activation=\'none\', pad_type=pad_type)]\n        elif res_type==\'parallel\':\n            model += [Parallel2dBlock(dim ,dim, 3, 1, 1, norm=norm, activation=activation, pad_type=pad_type)]\n            model += [Parallel2dBlock(dim ,dim, 3, 1, 1, norm=norm, activation=\'none\', pad_type=pad_type)]\n        else:\n            (\'unkown block type\')\n        self.res_type = res_type\n        self.model = nn.Sequential(*model)\n        if res_type==\'nonlocal\':\n            self.nonloc = NonlocalBlock(dim)\n\n    def forward(self, x):\n        if self.res_type == \'nonlocal\':\n            x = self.nonloc(x)\n        residual = x\n        out = self.model(x)\n        out += residual\n        return out\n\nclass NonlocalBlock(nn.Module):\n    def __init__(self, in_dim, norm=\'in\'):\n        super(NonlocalBlock, self).__init__()\n        self.chanel_in = in_dim\n        \n        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n        self.softmax  = nn.Softmax(dim=-1) #\n    def forward(self,x):\n        """"""\n            inputs :\n                x : input feature maps( B X C X W X H)\n            returns :\n                out : self attention value + input feature \n                attention: B X N X N (N is Width*Height)\n        """"""\n        m_batchsize,C,width ,height = x.size()\n        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n        energy =  torch.bmm(proj_query, proj_key) # transpose check\n        attention = self.softmax(energy) # BX (N) X (N) \n        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n\n        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n        out = out.view(m_batchsize,C,width,height)\n        \n        out = self.gamma*out + x\n        return out\n\nclass ASPP(nn.Module):\n    # ASPP (a)\n    def __init__(self, dim, norm=\'in\', activation=\'relu\', pad_type=\'zero\'):\n        super(ASPP, self).__init__()\n        dim_part = dim//2\n        self.conv1 = Conv2dBlock(dim,dim_part, 1, 1, 0, norm=norm, activation=\'none\', pad_type=pad_type)\n\n        self.conv6 = []\n        self.conv6 += [Conv2dBlock(dim,dim_part, 1, 1, 0, norm=norm, activation=activation, pad_type=pad_type)]\n        self.conv6 += [Conv2dBlock(dim_part,dim_part, 3, 1, 3, norm=norm, activation=\'none\', pad_type=pad_type, dilation=3)]\n        self.conv6 = nn.Sequential(*self.conv6)\n\n        self.conv12 = []\n        self.conv12 += [Conv2dBlock(dim,dim_part, 1, 1, 0, norm=norm, activation=activation, pad_type=pad_type)]\n        self.conv12 += [Conv2dBlock(dim_part,dim_part, 3, 1, 6, norm=norm, activation=\'none\', pad_type=pad_type, dilation=6)]\n        self.conv12 = nn.Sequential(*self.conv12)\n\n        self.conv18 = []\n        self.conv18 += [Conv2dBlock(dim,dim_part, 1, 1, 0, norm=norm, activation=activation, pad_type=pad_type)]\n        self.conv18 += [Conv2dBlock(dim_part,dim_part, 3, 1, 9, norm=norm, activation=\'none\', pad_type=pad_type, dilation=9)]\n        self.conv18 = nn.Sequential(*self.conv18)\n\n        self.fuse = Conv2dBlock(4*dim_part,2*dim, 1, 1, 0, norm=norm, activation=\'none\', pad_type=pad_type) \n\n    def forward(self, x):\n        conv1 = self.conv1(x)\n        conv6 = self.conv6(x)\n        conv12 = self.conv12(x)\n        conv18 = self.conv18(x)\n        out = torch.cat((conv1,conv6,conv12, conv18), dim=1)\n        out = self.fuse(out)\n        return out\n\n\nclass Conv2dBlock(nn.Module):\n    def __init__(self, input_dim ,output_dim, kernel_size, stride,\n                 padding=0, norm=\'none\', activation=\'relu\', pad_type=\'zero\', dilation=1, fp16 = False):\n        super(Conv2dBlock, self).__init__()\n        self.use_bias = True\n        # initialize padding\n        if pad_type == \'reflect\':\n            self.pad = nn.ReflectionPad2d(padding)\n        elif pad_type == \'replicate\':\n            self.pad = nn.ReplicationPad2d(padding)\n        elif pad_type == \'zero\':\n            self.pad = nn.ZeroPad2d(padding)\n        else:\n            assert 0, ""Unsupported padding type: {}"".format(pad_type)\n\n        # initialize normalization\n        norm_dim = output_dim\n        if norm == \'bn\':\n            self.norm = nn.BatchNorm2d(norm_dim)\n        elif norm == \'in\':\n            self.norm = nn.InstanceNorm2d(norm_dim)\n        elif norm == \'ln\':\n            self.norm = LayerNorm(norm_dim, fp16 = fp16)\n        elif norm == \'adain\':\n            self.norm = AdaptiveInstanceNorm2d(norm_dim)\n        elif norm == \'none\' or norm == \'sn\':\n            self.norm = None\n        else:\n            assert 0, ""Unsupported normalization: {}"".format(norm)\n\n        # initialize activation\n        if activation == \'relu\':\n            self.activation = nn.ReLU(inplace=True)\n        elif activation == \'lrelu\':\n            self.activation = nn.LeakyReLU(0.2, inplace=True)\n        elif activation == \'prelu\':\n            self.activation = nn.PReLU()\n        elif activation == \'selu\':\n            self.activation = nn.SELU(inplace=True)\n        elif activation == \'tanh\':\n            self.activation = nn.Tanh()\n        elif activation == \'none\':\n            self.activation = None\n        else:\n            assert 0, ""Unsupported activation: {}"".format(activation)\n\n        # initialize convolution\n        if norm == \'sn\':\n            self.conv = SpectralNorm(nn.Conv2d(input_dim, output_dim, kernel_size, stride, dilation=dilation, bias=self.use_bias))\n        else:\n            self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride, dilation=dilation, bias=self.use_bias)\n\n    def forward(self, x):\n        x = self.conv(self.pad(x))\n        if self.norm:\n            x = self.norm(x)\n        if self.activation:\n            x = self.activation(x)\n        return x\n\n\n\nclass Series2dBlock(nn.Module):\n    def __init__(self, input_dim ,output_dim, kernel_size, stride,\n                 padding=0, norm=\'none\', activation=\'relu\', pad_type=\'zero\'):\n        super(Series2dBlock, self).__init__()\n        self.use_bias = True\n        # initialize padding\n        if pad_type == \'reflect\':\n            self.pad = nn.ReflectionPad2d(padding)\n        elif pad_type == \'replicate\':\n            self.pad = nn.ReplicationPad2d(padding)\n        elif pad_type == \'zero\':\n            self.pad = nn.ZeroPad2d(padding)\n        else:\n            assert 0, ""Unsupported padding type: {}"".format(pad_type)\n\n        # initialize normalization\n        norm_dim = output_dim\n        if norm == \'bn\':\n            self.norm = nn.BatchNorm2d(norm_dim)\n        elif norm == \'in\':\n            self.norm = nn.InstanceNorm2d(norm_dim)\n        elif norm == \'ln\':\n            self.norm = LayerNorm(norm_dim)\n        elif norm == \'adain\':\n            self.norm = AdaptiveInstanceNorm2d(norm_dim)\n        elif norm == \'none\':\n            self.norm = None\n        else:\n            assert 0, ""Unsupported normalization: {}"".format(norm)\n\n        # initialize activation\n        if activation == \'relu\':\n            self.activation = nn.ReLU(inplace=True)\n        elif activation == \'lrelu\':\n            self.activation = nn.LeakyReLU(0.2, inplace=True)\n        elif activation == \'prelu\':\n            self.activation = nn.PReLU()\n        elif activation == \'selu\':\n            self.activation = nn.SELU(inplace=True)\n        elif activation == \'tanh\':\n            self.activation = nn.Tanh()\n        elif activation == \'none\':\n            self.activation = None\n        else:\n            assert 0, ""Unsupported activation: {}"".format(activation)\n\n        # initialize convolution\n        self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride, bias=self.use_bias)\n\n        self.instance_norm = nn.InstanceNorm2d(norm_dim)\n\n    def forward(self, x):\n        x = self.conv(self.pad(x))\n        x = self.norm(x) + x\n        x = self.instance_norm(x)\n        if self.activation:\n            x = self.activation(x)\n        return x\n\nclass Parallel2dBlock(nn.Module):\n    def __init__(self, input_dim ,output_dim, kernel_size, stride,\n                 padding=0, norm=\'none\', activation=\'relu\', pad_type=\'zero\'):\n        super(Parallel2dBlock, self).__init__()\n        self.use_bias = True\n        # initialize padding\n        if pad_type == \'reflect\':\n            self.pad = nn.ReflectionPad2d(padding)\n        elif pad_type == \'replicate\':\n            self.pad = nn.ReplicationPad2d(padding)\n        elif pad_type == \'zero\':\n            self.pad = nn.ZeroPad2d(padding)\n        else:\n            assert 0, ""Unsupported padding type: {}"".format(pad_type)\n\n        # initialize normalization\n        norm_dim = output_dim\n        if norm == \'bn\':\n            self.norm = nn.BatchNorm2d(norm_dim)\n        elif norm == \'in\':\n            self.norm = nn.InstanceNorm2d(norm_dim)\n        elif norm == \'ln\':\n            self.norm = LayerNorm(norm_dim)\n        elif norm == \'adain\':\n            self.norm = AdaptiveInstanceNorm2d(norm_dim)\n        elif norm == \'none\':\n            self.norm = None\n        else:\n            assert 0, ""Unsupported normalization: {}"".format(norm)\n\n        # initialize activation\n        if activation == \'relu\':\n            self.activation = nn.ReLU(inplace=True)\n        elif activation == \'lrelu\':\n            self.activation = nn.LeakyReLU(0.2, inplace=True)\n        elif activation == \'prelu\':\n            self.activation = nn.PReLU()\n        elif activation == \'selu\':\n            self.activation = nn.SELU(inplace=True)\n        elif activation == \'tanh\':\n            self.activation = nn.Tanh()\n        elif activation == \'none\':\n            self.activation = None\n        else:\n            assert 0, ""Unsupported activation: {}"".format(activation)\n\n        # initialize convolution\n        self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride, bias=self.use_bias)\n\n        self.instance_norm = nn.InstanceNorm2d(norm_dim)\n\n    def forward(self, x):\n        x = self.conv(self.pad(x)) + self.norm(x)\n        x = self.instance_norm(x)\n        if self.activation:\n            x = self.activation(x)\n        return x\n\nclass LinearBlock(nn.Module):\n    def __init__(self, input_dim, output_dim, norm=\'none\', activation=\'relu\'):\n        super(LinearBlock, self).__init__()\n        use_bias = True\n        # initialize fully connected layer\n        self.fc = nn.Linear(input_dim, output_dim, bias=use_bias)\n\n        # initialize normalization\n        norm_dim = output_dim\n        if norm == \'bn\':\n            self.norm = nn.BatchNorm1d(norm_dim)\n        elif norm == \'in\':\n            self.norm = nn.InstanceNorm1d(norm_dim)\n        elif norm == \'ln\':\n            self.norm = LayerNorm(norm_dim)\n        elif norm == \'none\':\n            self.norm = None\n        else:\n            assert 0, ""Unsupported normalization: {}"".format(norm)\n\n        # initialize activation\n        if activation == \'relu\':\n            self.activation = nn.ReLU(inplace=True)\n        elif activation == \'lrelu\':\n            self.activation = nn.LeakyReLU(0.2, inplace=True)\n        elif activation == \'prelu\':\n            self.activation = nn.PReLU()\n        elif activation == \'selu\':\n            self.activation = nn.SELU(inplace=True)\n        elif activation == \'tanh\':\n            self.activation = nn.Tanh()\n        elif activation == \'none\':\n            self.activation = None\n        else:\n            assert 0, ""Unsupported activation: {}"".format(activation)\n\n    def forward(self, x):\n        out = self.fc(x)\n        if self.norm:\n            #reshape input\n            out = out.unsqueeze(1)\n            out = self.norm(out)\n            out = out.view(out.size(0),out.size(2))\n        if self.activation:\n            out = self.activation(out)\n        return out\n\n##################################################################################\n# VGG network definition\n##################################################################################\nclass Vgg16(nn.Module):\n    def __init__(self):\n        super(Vgg16, self).__init__()\n        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n\n        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n\n        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n\n        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n\n        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n\n    def forward(self, X):\n        h = F.relu(self.conv1_1(X), inplace=True)\n        h = F.relu(self.conv1_2(h), inplace=True)\n        h = F.max_pool2d(h, kernel_size=2, stride=2)\n\n        h = F.relu(self.conv2_1(h), inplace=True)\n        h = F.relu(self.conv2_2(h), inplace=True)\n        h = F.max_pool2d(h, kernel_size=2, stride=2)\n\n        h = F.relu(self.conv3_1(h), inplace=True)\n        h = F.relu(self.conv3_2(h), inplace=True)\n        h = F.relu(self.conv3_3(h), inplace=True)\n        h = F.max_pool2d(h, kernel_size=2, stride=2)\n\n        h = F.relu(self.conv4_1(h), inplace=True)\n        h = F.relu(self.conv4_2(h), inplace=True)\n        h = F.relu(self.conv4_3(h), inplace=True)\n\n        h = F.relu(self.conv5_1(h), inplace=True)\n        h = F.relu(self.conv5_2(h), inplace=True)\n        h = F.relu(self.conv5_3(h), inplace=True)\n        relu5_3 = h\n\n        return relu5_3\n\n##################################################################################\n# Normalization layers\n##################################################################################\nclass AdaptiveInstanceNorm2d(nn.Module):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        super(AdaptiveInstanceNorm2d, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        # weight and bias are dynamically assigned\n        self.weight = None\n        self.bias = None\n        # just dummy buffers, not used\n        self.register_buffer(\'running_mean\', torch.zeros(num_features))\n        self.register_buffer(\'running_var\', torch.ones(num_features))\n\n    def forward(self, x):\n        assert self.weight is not None and self.bias is not None, ""Please assign weight and bias before calling AdaIN!""\n        b, c = x.size(0), x.size(1)\n        running_mean = self.running_mean.repeat(b).type_as(x)\n        running_var = self.running_var.repeat(b).type_as(x)\n        # Apply instance norm\n        x_reshaped = x.contiguous().view(1, b * c, *x.size()[2:])\n        out = F.batch_norm(\n            x_reshaped, running_mean, running_var, self.weight, self.bias,\n            True, self.momentum, self.eps)\n\n        return out.view(b, c, *x.size()[2:])\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\' + str(self.num_features) + \')\'\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, num_features, eps=1e-5, affine=True, fp16=False):\n        super(LayerNorm, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n        self.fp16 = fp16\n        if self.affine:\n            self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())\n            self.beta = nn.Parameter(torch.zeros(num_features))\n    def forward(self, x):\n        shape = [-1] + [1] * (x.dim() - 1)\n        if x.type() == \'torch.cuda.HalfTensor\': # For Safety\n            mean = x.view(-1).float().mean().view(*shape)\n            std = x.view(-1).float().std().view(*shape)\n            mean = mean.half()\n            std = std.half()\n        else:\n            mean = x.view(x.size(0), -1).mean(1).view(*shape)\n            std = x.view(x.size(0), -1).std(1).view(*shape)\n\n        x = (x - mean) / (std + self.eps)\n        if self.affine:\n            shape = [1, -1] + [1] * (x.dim() - 2)\n            x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n        return x\n\n\n'"
prepare-market.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport os\nfrom shutil import copyfile\n\n# You only need to change this line to your dataset download path\ndownload_path = \'../Market\'\n\nif not os.path.isdir(download_path):\n    print(\'please change the download_path\')\n\nsave_path = download_path + \'/pytorch\'\nif not os.path.isdir(save_path):\n    os.mkdir(save_path)\n#-----------------------------------------\n#query\nquery_path = download_path + \'/query\'\nquery_save_path = download_path + \'/pytorch/query\'\nif not os.path.isdir(query_save_path):\n    os.mkdir(query_save_path)\n\nfor root, dirs, files in os.walk(query_path, topdown=True):\n    for name in files:\n        if not name[-3:]==\'jpg\':\n            continue\n        ID  = name.split(\'_\')\n        src_path = query_path + \'/\' + name\n        dst_path = query_save_path + \'/\' + ID[0] \n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n        copyfile(src_path, dst_path + \'/\' + name)\n\n#-----------------------------------------\n#multi-query\nquery_path = download_path + \'/gt_bbox\'\n# for dukemtmc-reid, we do not need multi-query\nif os.path.isdir(query_path):\n    query_save_path = download_path + \'/pytorch/multi-query\'\n    if not os.path.isdir(query_save_path):\n        os.mkdir(query_save_path)\n\n    for root, dirs, files in os.walk(query_path, topdown=True):\n        for name in files:\n            if not name[-3:]==\'jpg\':\n                continue\n            ID  = name.split(\'_\')\n            src_path = query_path + \'/\' + name\n            dst_path = query_save_path + \'/\' + ID[0]\n            if not os.path.isdir(dst_path):\n                os.mkdir(dst_path)\n            copyfile(src_path, dst_path + \'/\' + name)\n\n#-----------------------------------------\n#gallery\ngallery_path = download_path + \'/bounding_box_test\'\ngallery_save_path = download_path + \'/pytorch/gallery\'\nif not os.path.isdir(gallery_save_path):\n    os.mkdir(gallery_save_path)\n\nfor root, dirs, files in os.walk(gallery_path, topdown=True):\n    for name in files:\n        if not name[-3:]==\'jpg\':\n            continue\n        ID  = name.split(\'_\')\n        src_path = gallery_path + \'/\' + name\n        dst_path = gallery_save_path + \'/\' + ID[0]\n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n        copyfile(src_path, dst_path + \'/\' + name)\n\n#---------------------------------------\n#train_all\ntrain_path = download_path + \'/bounding_box_train\'\ntrain_save_path = download_path + \'/pytorch/train_all\'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n\nfor root, dirs, files in os.walk(train_path, topdown=True):\n    for name in files:\n        if not name[-3:]==\'jpg\':\n            continue\n        ID  = name.split(\'_\')\n        src_path = train_path + \'/\' + name\n        dst_path = train_save_path + \'/\' + ID[0]\n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n        copyfile(src_path, dst_path + \'/\' + name)\n\n\n#---------------------------------------\n#train_val\ntrain_path = download_path + \'/bounding_box_train\'\ntrain_save_path = download_path + \'/pytorch/train\'\nval_save_path = download_path + \'/pytorch/val\'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n    os.mkdir(val_save_path)\n\nfor root, dirs, files in os.walk(train_path, topdown=True):\n    for name in files:\n        if not name[-3:]==\'jpg\':\n            continue\n        ID  = name.split(\'_\')\n        src_path = train_path + \'/\' + name\n        dst_path = train_save_path + \'/\' + ID[0]\n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n            dst_path = val_save_path + \'/\' + ID[0]  #first image is used as val image\n            os.mkdir(dst_path)\n        copyfile(src_path, dst_path + \'/\' + name)\n'"
random_erasing.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nfrom __future__ import absolute_import\n\nfrom torchvision.transforms import *\n\nimport random\nimport math\n\nclass RandomErasing(object):\n    """""" Randomly selects a rectangle region in an image and erases its pixels.\n        \'Random Erasing Data Augmentation\' by Zhong et al.\n        See https://arxiv.org/pdf/1708.04896.pdf\n    Args:\n         probability: The probability that the Random Erasing operation will be performed.\n         sl: Minimum proportion of erased area against input image.\n         sh: Maximum proportion of erased area against input image.\n         r1: Minimum aspect ratio of erased area.\n         mean: Erasing value. \n    """"""\n    \n    def __init__(self, probability = 0.5, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[0.4914, 0.4822, 0.4465]):\n        self.probability = probability\n        self.mean = mean\n        self.sl = sl\n        self.sh = sh\n        self.r1 = r1\n        random.seed(7)\n\n    def __call__(self, img):\n\n        if random.uniform(0, 1) > self.probability:\n            return img\n\n        for attempt in range(100):\n            area = img.size()[1] * img.size()[2]\n       \n            target_area = random.uniform(self.sl, self.sh) * area\n            aspect_ratio = random.uniform(self.r1, 1/self.r1)\n\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w < img.size()[2] and h < img.size()[1]:\n                x1 = random.randint(0, img.size()[1] - h)\n                y1 = random.randint(0, img.size()[2] - w)\n                if img.size()[0] == 3:\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n                    img[1, x1:x1+h, y1:y1+w] = self.mean[1]\n                    img[2, x1:x1+h, y1:y1+w] = self.mean[2]\n                else:\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n                return img.detach()\n\n        return img.detach()\n'"
reIDfolder.py,0,"b'""""""\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nfrom torchvision import datasets\nimport os\nimport numpy as np\nimport random\n\nclass ReIDFolder(datasets.ImageFolder):\n\n    def __init__(self, root, transform):\n        super(ReIDFolder, self).__init__(root, transform)\n        targets = np.asarray([s[1] for s in self.samples])\n        self.targets = targets\n        self.img_num = len(self.samples)\n        print(self.img_num)\n\n    def _get_cam_id(self, path):\n        camera_id = []\n        filename = os.path.basename(path)\n        camera_id = filename.split(\'c\')[1][0]\n        return int(camera_id)-1\n\n    def _get_pos_sample(self, target, index, path):\n        pos_index = np.argwhere(self.targets == target)\n        pos_index = pos_index.flatten()\n        pos_index = np.setdiff1d(pos_index, index)\n        if len(pos_index)==0:  # in the query set, only one sample\n            return path\n        else:\n            rand = random.randint(0,len(pos_index)-1)\n        return self.samples[pos_index[rand]][0]\n\n    def _get_neg_sample(self, target):\n        neg_index = np.argwhere(self.targets != target)\n        neg_index = neg_index.flatten()\n        rand = random.randint(0,len(neg_index)-1)\n        return self.samples[neg_index[rand]]\n\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        sample = self.loader(path)\n\n        pos_path = self._get_pos_sample(target, index, path)\n        pos = self.loader(pos_path)\n\n        if self.transform is not None:\n            sample = self.transform(sample)\n            pos = self.transform(pos)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, target, pos\n\n'"
reIDmodel.py,6,"b'""""""\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import init\nfrom torchvision import models\n\n######################################################################\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\')\n    elif classname.find(\'Linear\') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_out\')\n        init.constant_(m.bias.data, 0.0)\n    elif classname.find(\'InstanceNorm1d\') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\ndef weights_init_classifier(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Linear\') != -1:\n        init.normal_(m.weight.data, std=0.001)\n        init.constant_(m.bias.data, 0.0)\n\ndef fix_bn(m):\n    classname = m.__class__.__name__\n    if classname.find(\'BatchNorm\') != -1:\n        m.eval()\n\n# Defines the new fc layer and classification layer\n# |--Linear--|--bn--|--relu--|--Linear--|\nclass ClassBlock(nn.Module):\n    def __init__(self, input_dim, class_num, droprate=0.5, relu=False, num_bottleneck=512):\n        super(ClassBlock, self).__init__()\n        add_block = []\n        add_block += [nn.Linear(input_dim, num_bottleneck)] \n        #num_bottleneck = input_dim # We remove the input_dim\n        add_block += [nn.BatchNorm1d(num_bottleneck, affine=True)]\n        if relu:\n            add_block += [nn.LeakyReLU(0.1)]\n        if droprate>0:\n            add_block += [nn.Dropout(p=droprate)]\n        add_block = nn.Sequential(*add_block)\n        add_block.apply(weights_init_kaiming)\n\n        classifier = []\n        classifier += [nn.Linear(num_bottleneck, class_num)]\n        classifier = nn.Sequential(*classifier)\n        classifier.apply(weights_init_classifier)\n\n        self.add_block = add_block\n        self.classifier = classifier\n    def forward(self, x):\n        x = self.add_block(x)\n        x = self.classifier(x)\n        return x\n\n# Define the ResNet50-based Model\nclass ft_net(nn.Module):\n\n    def __init__(self, class_num, norm=False, pool=\'avg\', stride=2):\n        super(ft_net, self).__init__()\n        if norm:\n            self.norm = True\n        else:\n            self.norm = False\n        model_ft = models.resnet50(pretrained=True)\n        # avg pooling to global pooling\n        self.part = 4\n        if pool==\'max\':\n            model_ft.partpool = nn.AdaptiveMaxPool2d((self.part,1)) \n            model_ft.avgpool = nn.AdaptiveMaxPool2d((1,1))\n        else:\n            model_ft.partpool = nn.AdaptiveAvgPool2d((self.part,1)) \n            model_ft.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        # remove the final downsample\n        if stride == 1:\n            model_ft.layer4[0].downsample[0].stride = (1,1)\n            model_ft.layer4[0].conv2.stride = (1,1)\n\n        self.model = model_ft   \n        self.classifier = ClassBlock(2048, class_num)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)  # -> 512 32*16\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        f = self.model.partpool(x) # 8 * 2048 4*1\n        x = self.model.avgpool(x)  # 8 * 2048 1*1\n        \n        x = x.view(x.size(0),x.size(1))\n        f = f.view(f.size(0),f.size(1)*self.part)\n        if self.norm:\n            fnorm = torch.norm(f, p=2, dim=1, keepdim=True) + 1e-8\n            f = f.div(fnorm.expand_as(f))\n        x = self.classifier(x)\n        return f, x\n\n# Define the AB Model\nclass ft_netAB(nn.Module):\n\n    def __init__(self, class_num, norm=False, stride=2, droprate=0.5, pool=\'avg\'):\n        super(ft_netAB, self).__init__()\n        model_ft = models.resnet50(pretrained=True)\n        self.part = 4\n        if pool==\'max\':\n            model_ft.partpool = nn.AdaptiveMaxPool2d((self.part,1))\n            model_ft.avgpool = nn.AdaptiveMaxPool2d((1,1))\n        else:\n            model_ft.partpool = nn.AdaptiveAvgPool2d((self.part,1))\n            model_ft.avgpool = nn.AdaptiveAvgPool2d((1,1))\n\n        self.model = model_ft\n\n        if stride == 1:\n            self.model.layer4[0].downsample[0].stride = (1,1)\n            self.model.layer4[0].conv2.stride = (1,1)\n\n        self.classifier1 = ClassBlock(2048, class_num, 0.5)\n        self.classifier2 = ClassBlock(2048, class_num, 0.75)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        f = self.model.partpool(x)\n        f = f.view(f.size(0),f.size(1)*self.part)\n        f = f.detach() # no gradient \n        x = self.model.avgpool(x)\n        x = x.view(x.size(0), x.size(1))\n        x1 = self.classifier1(x) \n        x2 = self.classifier2(x)  \n        x=[]\n        x.append(x1)\n        x.append(x2)\n        return f, x\n\n# Define the DenseNet121-based Model\nclass ft_net_dense(nn.Module):\n\n    def __init__(self, class_num ):\n        super().__init__()\n        model_ft = models.densenet121(pretrained=True)\n        model_ft.features.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        model_ft.fc = nn.Sequential()\n        self.model = model_ft\n        # For DenseNet, the feature dim is 1024 \n        self.classifier = ClassBlock(1024, class_num)\n\n    def forward(self, x):\n        x = self.model.features(x)\n        x = torch.squeeze(x)\n        x = self.classifier(x)\n        return x\n    \n# Define the ResNet50-based Model (Middle-Concat)\n# In the spirit of ""The Devil is in the Middle: Exploiting Mid-level Representations for Cross-Domain Instance Matching."" Yu, Qian, et al. arXiv:1711.08106 (2017).\nclass ft_net_middle(nn.Module):\n\n    def __init__(self, class_num ):\n        super(ft_net_middle, self).__init__()\n        model_ft = models.resnet50(pretrained=True)\n        # avg pooling to global pooling\n        model_ft.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        self.model = model_ft\n        self.classifier = ClassBlock(2048+1024, class_num)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        # x0  n*1024*1*1\n        x0 = self.model.avgpool(x)\n        x = self.model.layer4(x)\n        # x1  n*2048*1*1\n        x1 = self.model.avgpool(x)\n        x = torch.cat((x0,x1),1)\n        x = torch.squeeze(x)\n        x = self.classifier(x)\n        return x\n\n# Part Model proposed in Yifan Sun etal. (2018)\nclass PCB(nn.Module):\n    def __init__(self, class_num ):\n        super(PCB, self).__init__()\n\n        self.part = 4 # We cut the pool5 to 4 parts\n        model_ft = models.resnet50(pretrained=True)\n        self.model = model_ft\n        self.avgpool = nn.AdaptiveAvgPool2d((self.part,1))\n        self.dropout = nn.Dropout(p=0.5)\n        # remove the final downsample\n        self.model.layer4[0].downsample[0].stride = (1,1)\n        self.model.layer4[0].conv2.stride = (1,1)\n        self.softmax = nn.Softmax(dim=1)\n        # define 4 classifiers\n        for i in range(self.part):\n            name = \'classifier\'+str(i)\n            setattr(self, name, ClassBlock(2048, class_num, True, False, 256))\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n        \n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        x = self.avgpool(x)\n        f = x\n        f = f.view(f.size(0),f.size(1)*self.part)\n        x = self.dropout(x)\n        part = {}\n        predict = {}\n        # get part feature batchsize*2048*4\n        for i in range(self.part):\n            part[i] = x[:,:,i].contiguous()\n            part[i] = part[i].view(x.size(0), x.size(1))\n            name = \'classifier\'+str(i)\n            c = getattr(self,name)\n            predict[i] = c(part[i])\n\n        y=[]\n        for i in range(self.part):\n            y.append(predict[i])\n\n        return f, y\n\nclass PCB_test(nn.Module):\n    def __init__(self,model):\n        super(PCB_test,self).__init__()\n        self.part = 6\n        self.model = model.model\n        self.avgpool = nn.AdaptiveAvgPool2d((self.part,1))\n        # remove the final downsample\n        self.model.layer3[0].downsample[0].stride = (1,1)\n        self.model.layer3[0].conv2.stride = (1,1)\n\n        self.model.layer4[0].downsample[0].stride = (1,1)\n        self.model.layer4[0].conv2.stride = (1,1)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        x = self.avgpool(x)\n        y = x.view(x.size(0),x.size(1),x.size(2))\n        return y\n\n\n'"
train.py,14,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\nfrom utils import get_all_data_loaders, prepare_sub_folder, write_loss, get_config, write_2images, Timer\nimport argparse\nfrom trainer import DGNet_Trainer\nimport torch.backends.cudnn as cudnn\nimport torch\nimport numpy.random as random\ntry:\n    from itertools import izip as zip\nexcept ImportError: # will be 3.x series\n    pass\nimport os\nimport sys\nimport tensorboardX\nimport shutil\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--config\', type=str, default=\'configs/latest.yaml\', help=\'Path to the config file.\')\nparser.add_argument(\'--output_path\', type=str, default=\'.\', help=""outputs path"")\nparser.add_argument(\'--name\', type=str, default=\'latest_ablation\', help=""outputs path"")\nparser.add_argument(""--resume"", action=""store_true"")\nparser.add_argument(\'--trainer\', type=str, default=\'DGNet\', help=""DGNet"")\nparser.add_argument(\'--gpu_ids\',default=\'0\', type=str,help=\'gpu_ids: e.g. 0  0,1,2  0,2\')\nopts = parser.parse_args()\n\nstr_ids = opts.gpu_ids.split(\',\')\ngpu_ids = []\nfor str_id in str_ids:\n    gpu_ids.append(int(str_id))\nnum_gpu = len(gpu_ids)\n\ncudnn.benchmark = True\n\n# Load experiment setting\nif opts.resume:\n    config = get_config(\'./outputs/\'+opts.name+\'/config.yaml\')\nelse:\n    config = get_config(opts.config)\nmax_iter = config[\'max_iter\']\ndisplay_size = config[\'display_size\']\nconfig[\'vgg_model_path\'] = opts.output_path\n\n# Setup model and data loader\nif opts.trainer == \'DGNet\':\n    trainer = DGNet_Trainer(config, gpu_ids)\n    trainer.cuda()\n\nrandom.seed(7) #fix random result\ntrain_loader_a, train_loader_b, test_loader_a, test_loader_b = get_all_data_loaders(config)\ntrain_a_rand = random.permutation(train_loader_a.dataset.img_num)[0:display_size] \ntrain_b_rand = random.permutation(train_loader_b.dataset.img_num)[0:display_size] \ntest_a_rand = random.permutation(test_loader_a.dataset.img_num)[0:display_size] \ntest_b_rand = random.permutation(test_loader_b.dataset.img_num)[0:display_size] \n\ntrain_display_images_a = torch.stack([train_loader_a.dataset[i][0] for i in train_a_rand]).cuda()\ntrain_display_images_ap = torch.stack([train_loader_a.dataset[i][2] for i in train_a_rand]).cuda()\ntrain_display_images_b = torch.stack([train_loader_b.dataset[i][0] for i in train_b_rand]).cuda()\ntrain_display_images_bp = torch.stack([train_loader_b.dataset[i][2] for i in train_b_rand]).cuda()\ntest_display_images_a = torch.stack([test_loader_a.dataset[i][0] for i in test_a_rand]).cuda()\ntest_display_images_ap = torch.stack([test_loader_a.dataset[i][2] for i in test_a_rand]).cuda()\ntest_display_images_b = torch.stack([test_loader_b.dataset[i][0] for i in test_b_rand]).cuda()\ntest_display_images_bp = torch.stack([test_loader_b.dataset[i][2] for i in test_b_rand]).cuda()\n\n# Setup logger and output folders\nif not opts.resume:\n    model_name = os.path.splitext(os.path.basename(opts.config))[0]\n    train_writer = tensorboardX.SummaryWriter(os.path.join(opts.output_path + ""/logs"", model_name))\n    output_directory = os.path.join(opts.output_path + ""/outputs"", model_name)\n    checkpoint_directory, image_directory = prepare_sub_folder(output_directory)\n    shutil.copyfile(opts.config, os.path.join(output_directory, \'config.yaml\')) # copy config file to output folder\n    shutil.copyfile(\'trainer.py\', os.path.join(output_directory, \'trainer.py\')) # copy file to output folder\n    shutil.copyfile(\'reIDmodel.py\', os.path.join(output_directory, \'reIDmodel.py\')) # copy file to output folder\n    shutil.copyfile(\'networks.py\', os.path.join(output_directory, \'networks.py\')) # copy file to output folder\nelse:\n    train_writer = tensorboardX.SummaryWriter(os.path.join(opts.output_path + ""/logs"", opts.name))\n    output_directory = os.path.join(opts.output_path + ""/outputs"", opts.name)\n    checkpoint_directory, image_directory = prepare_sub_folder(output_directory)\n# Start training\niterations = trainer.resume(checkpoint_directory, hyperparameters=config) if opts.resume else 0\nconfig[\'epoch_iteration\'] = round( train_loader_a.dataset.img_num  / config[\'batch_size\'] )\nprint(\'Every epoch need %d iterations\'%config[\'epoch_iteration\'])\nnepoch = 0 \n    \nprint(\'Note that dataloader may hang with too much nworkers.\')\n\nif num_gpu>1:\n    print(\'Now you are using %d gpus.\'%num_gpu)\n    trainer.dis_a = torch.nn.DataParallel(trainer.dis_a, gpu_ids)\n    trainer.dis_b = trainer.dis_a\n    trainer = torch.nn.DataParallel(trainer, gpu_ids)\n\nwhile True:\n    for it, ((images_a,labels_a, pos_a),  (images_b, labels_b, pos_b)) in enumerate(zip(train_loader_a, train_loader_b)):\n        if num_gpu>1:\n            trainer.module.update_learning_rate()\n        else:\n            trainer.update_learning_rate()\n        images_a, images_b = images_a.cuda().detach(), images_b.cuda().detach()\n        pos_a, pos_b = pos_a.cuda().detach(), pos_b.cuda().detach()\n        labels_a, labels_b = labels_a.cuda().detach(), labels_b.cuda().detach()\n\n        with Timer(""Elapsed time in update: %f""):\n            # Main training code\n            x_ab, x_ba, s_a, s_b, f_a, f_b, p_a, p_b, pp_a, pp_b, x_a_recon, x_b_recon, x_a_recon_p, x_b_recon_p = \\\n                                                                                  trainer.forward(images_a, images_b, pos_a, pos_b)\n            if num_gpu>1:\n                trainer.module.dis_update(x_ab.clone(), x_ba.clone(), images_a, images_b, config, num_gpu)\n                trainer.module.gen_update(x_ab, x_ba, s_a, s_b, f_a, f_b, p_a, p_b, pp_a, pp_b, x_a_recon, x_b_recon, x_a_recon_p, x_b_recon_p, images_a, images_b, pos_a, pos_b, labels_a, labels_b, config, iterations, num_gpu)\n            else: \n                trainer.dis_update(x_ab.clone(), x_ba.clone(), images_a, images_b, config, num_gpu=1)\n                trainer.gen_update(x_ab, x_ba, s_a, s_b, f_a, f_b, p_a, p_b, pp_a, pp_b, x_a_recon, x_b_recon, x_a_recon_p, x_b_recon_p, images_a, images_b, pos_a, pos_b, labels_a, labels_b, config, iterations, num_gpu=1)\n\n            torch.cuda.synchronize()\n\n        # Dump training stats in log file\n        if (iterations + 1) % config[\'log_iter\'] == 0:\n            print(""\\033[1m Epoch: %02d Iteration: %08d/%08d \\033[0m"" % (nepoch, iterations + 1, max_iter), end="" "")\n            if num_gpu==1:\n                write_loss(iterations, trainer, train_writer)\n            else:\n                write_loss(iterations, trainer.module, train_writer)\n\n        # Write images\n        if (iterations + 1) % config[\'image_save_iter\'] == 0:\n            with torch.no_grad():\n                if num_gpu>1:\n                    test_image_outputs = trainer.module.sample(test_display_images_a, test_display_images_b)\n                else:\n                    test_image_outputs = trainer.sample(test_display_images_a, test_display_images_b)\n            write_2images(test_image_outputs, display_size, image_directory, \'test_%08d\' % (iterations + 1))\n            del test_image_outputs\n\n        if (iterations + 1) % config[\'image_display_iter\'] == 0:\n            with torch.no_grad():\n                if num_gpu>1:\n                    image_outputs = trainer.module.sample(train_display_images_a, train_display_images_b)\n                else:\n                    image_outputs = trainer.sample(train_display_images_a, train_display_images_b)\n            write_2images(image_outputs, display_size, image_directory, \'train_%08d\' % (iterations + 1))\n            del image_outputs\n        # Save network weights\n        if (iterations + 1) % config[\'snapshot_save_iter\'] == 0:\n            if num_gpu>1:\n                trainer.module.save(checkpoint_directory, iterations)\n            else:\n                trainer.save(checkpoint_directory, iterations)\n\n        iterations += 1\n        if iterations >= max_iter:\n            sys.exit(\'Finish training\')\n\n    # Save network weights by epoch number\n    nepoch = nepoch+1\n    if(nepoch + 1) % 10 == 0:\n        if num_gpu>1:\n            trainer.module.save(checkpoint_directory, iterations)\n        else:\n            trainer.save(checkpoint_directory, iterations)\n\n'"
trainer.py,41,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\nfrom networks import AdaINGen, MsImageDis\nfrom reIDmodel import ft_net, ft_netAB, PCB\nfrom utils import get_model_list, vgg_preprocess, load_vgg16, get_scheduler\nfrom torch.autograd import Variable\nimport torch\nimport torch.nn as nn\nimport copy\nimport os\nimport cv2\nimport numpy as np\nfrom random_erasing import RandomErasing\nimport random\nimport yaml\n\n#fp16\ntry:\n    from apex import amp\n    from apex.fp16_utils import *\nexcept ImportError:\n    print(\'This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0\')\n\n\ndef to_gray(half=False): #simple\n    def forward(x):\n        x = torch.mean(x, dim=1, keepdim=True)\n        if half:\n            x = x.half()\n        return x\n    return forward\n\ndef to_edge(x):\n    x = x.data.cpu()\n    out = torch.FloatTensor(x.size(0), x.size(2), x.size(3))\n    for i in range(x.size(0)):\n        xx = recover(x[i,:,:,:])   # 3 channel, 256x128x3\n        xx = cv2.cvtColor(xx, cv2.COLOR_RGB2GRAY) # 256x128x1\n        xx = cv2.Canny(xx, 10, 200) #256x128\n        xx = xx/255.0 - 0.5 # {-0.5,0.5}\n        xx += np.random.randn(xx.shape[0],xx.shape[1])*0.1  #add random noise\n        xx = torch.from_numpy(xx.astype(np.float32))\n        out[i,:,:] = xx\n    out = out.unsqueeze(1) \n    return out.cuda()\n\ndef scale2(x):\n    if x.size(2) > 128: # do not need to scale the input\n        return x\n    x = torch.nn.functional.upsample(x, scale_factor=2, mode=\'nearest\')  #bicubic is not available for the time being.\n    return x\n\ndef recover(inp):\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = inp * 255.0\n    inp = np.clip(inp, 0, 255)\n    inp = inp.astype(np.uint8)\n    return inp\n\ndef train_bn(m):\n    classname = m.__class__.__name__\n    if classname.find(\'BatchNorm\') != -1:\n        m.train()\n\ndef fliplr(img):\n    \'\'\'flip horizontal\'\'\'\n    inv_idx = torch.arange(img.size(3)-1,-1,-1).long().cuda()  # N x C x H x W\n    img_flip = img.index_select(3,inv_idx)\n    return img_flip\n\ndef update_teacher(model_s, model_t, alpha=0.999):\n    for param_s, param_t in zip(model_s.parameters(), model_t.parameters()):\n        param_t.data.mul_(alpha).add_(1 - alpha, param_s.data)\n\ndef predict_label(teacher_models, inputs, num_class, alabel, slabel, teacher_style=0):\n# teacher_style:\n# 0: Our smooth dynamic label\n# 1: Pseudo label, hard dynamic label\n# 2: Conditional label, hard static label \n# 3: LSRO, static smooth label\n# 4: Dynamic Soft Two-label\n# alabel is appearance label\n    if teacher_style == 0:\n        count = 0\n        sm = nn.Softmax(dim=1)\n        for teacher_model in teacher_models:\n            _, outputs_t1 = teacher_model(inputs) \n            outputs_t1 = sm(outputs_t1.detach())\n            _, outputs_t2 = teacher_model(fliplr(inputs)) \n            outputs_t2 = sm(outputs_t2.detach())\n            if count==0:\n                outputs_t = outputs_t1 + outputs_t2\n            else:\n                outputs_t = outputs_t * opt.alpha  # old model decay\n                outputs_t += outputs_t1 + outputs_t2\n            count +=2\n    elif teacher_style == 1:  # dynamic one-hot  label\n        count = 0\n        sm = nn.Softmax(dim=1)\n        for teacher_model in teacher_models:\n            _, outputs_t1 = teacher_model(inputs)\n            outputs_t1 = sm(outputs_t1.detach())  # change softmax to max\n            _, outputs_t2 = teacher_model(fliplr(inputs))\n            outputs_t2 = sm(outputs_t2.detach())\n            if count==0:\n                outputs_t = outputs_t1 + outputs_t2\n            else:\n                outputs_t = outputs_t * opt.alpha  # old model decay\n                outputs_t += outputs_t1 + outputs_t2\n            count +=2\n        _, dlabel = torch.max(outputs_t.data, 1)\n        outputs_t = torch.zeros(inputs.size(0), num_class).cuda()\n        for i in range(inputs.size(0)):\n            outputs_t[i, dlabel[i]] = 1\n    elif teacher_style == 2: # appearance label\n        outputs_t = torch.zeros(inputs.size(0), num_class).cuda()\n        for i in range(inputs.size(0)):\n            outputs_t[i, alabel[i]] = 1\n    elif teacher_style == 3: # LSRO\n        outputs_t = torch.ones(inputs.size(0), num_class).cuda()\n    elif teacher_style == 4: #Two-label\n        count = 0\n        sm = nn.Softmax(dim=1)\n        for teacher_model in teacher_models:\n            _, outputs_t1 = teacher_model(inputs)\n            outputs_t1 = sm(outputs_t1.detach())\n            _, outputs_t2 = teacher_model(fliplr(inputs))\n            outputs_t2 = sm(outputs_t2.detach())\n            if count==0:\n                outputs_t = outputs_t1 + outputs_t2\n            else:\n                outputs_t = outputs_t * opt.alpha  # old model decay\n                outputs_t += outputs_t1 + outputs_t2\n            count +=2\n        mask = torch.zeros(outputs_t.shape)\n        mask = mask.cuda()\n        for i in range(inputs.size(0)):\n            mask[i, alabel[i]] = 1\n            mask[i, slabel[i]] = 1\n        outputs_t = outputs_t*mask\n    else:\n        print(\'not valid style. teacher-style is in [0-3].\')\n\n    s = torch.sum(outputs_t, dim=1, keepdim=True)\n    s = s.expand_as(outputs_t)\n    outputs_t = outputs_t/s\n    return outputs_t\n\n######################################################################\n# Load model\n#---------------------------\ndef load_network(network, name):\n    save_path = os.path.join(\'./models\',name,\'net_last.pth\')\n    network.load_state_dict(torch.load(save_path))\n    return network\n\ndef load_config(name):\n    config_path = os.path.join(\'./models\',name,\'opts.yaml\')\n    with open(config_path, \'r\') as stream:\n        config = yaml.load(stream)\n    return config\n\n\nclass DGNet_Trainer(nn.Module):\n    def __init__(self, hyperparameters, gpu_ids=[0]):\n        super(DGNet_Trainer, self).__init__()\n        lr_g = hyperparameters[\'lr_g\']\n        lr_d = hyperparameters[\'lr_d\']\n        ID_class = hyperparameters[\'ID_class\']\n        if not \'apex\' in hyperparameters.keys():\n            hyperparameters[\'apex\'] = False\n        self.fp16 = hyperparameters[\'apex\']\n        # Initiate the networks\n        # We do not need to manually set fp16 in the network for the new apex. So here I set fp16=False.\n        self.gen_a = AdaINGen(hyperparameters[\'input_dim_a\'], hyperparameters[\'gen\'], fp16 = False)  # auto-encoder for domain a\n        self.gen_b = self.gen_a  # auto-encoder for domain b\n\n        if not \'ID_stride\' in hyperparameters.keys():\n            hyperparameters[\'ID_stride\'] = 2\n        \n        if hyperparameters[\'ID_style\']==\'PCB\':\n            self.id_a = PCB(ID_class)\n        elif hyperparameters[\'ID_style\']==\'AB\':\n            self.id_a = ft_netAB(ID_class, stride = hyperparameters[\'ID_stride\'], norm=hyperparameters[\'norm_id\'], pool=hyperparameters[\'pool\']) \n        else:\n            self.id_a = ft_net(ID_class, norm=hyperparameters[\'norm_id\'], pool=hyperparameters[\'pool\']) # return 2048 now\n\n        self.id_b = self.id_a\n        self.dis_a = MsImageDis(3, hyperparameters[\'dis\'], fp16 = False)  # discriminator for domain a\n        self.dis_b = self.dis_a # discriminator for domain b\n\n        # load teachers\n        if hyperparameters[\'teacher\'] != """":\n            teacher_name = hyperparameters[\'teacher\']\n            print(teacher_name)\n            teacher_names = teacher_name.split(\',\')\n            teacher_model = nn.ModuleList()\n            teacher_count = 0\n            for teacher_name in teacher_names:\n                config_tmp = load_config(teacher_name)\n                if \'stride\' in config_tmp:\n                    stride = config_tmp[\'stride\'] \n                else:\n                    stride = 2\n                model_tmp = ft_net(ID_class, stride = stride)\n                teacher_model_tmp = load_network(model_tmp, teacher_name)\n                teacher_model_tmp.model.fc = nn.Sequential()  # remove the original fc layer in ImageNet\n                teacher_model_tmp = teacher_model_tmp.cuda()\n                if self.fp16:\n                    teacher_model_tmp = amp.initialize(teacher_model_tmp, opt_level=""O1"")\n                teacher_model.append(teacher_model_tmp.cuda().eval())\n                teacher_count +=1\n            self.teacher_model = teacher_model\n            if hyperparameters[\'train_bn\']:\n                self.teacher_model = self.teacher_model.apply(train_bn)\n\n        self.instancenorm = nn.InstanceNorm2d(512, affine=False)\n\n        # RGB to one channel\n        if hyperparameters[\'single\']==\'edge\':\n            self.single = to_edge\n        else:\n            self.single = to_gray(False)\n\n        # Random Erasing when training\n        if not \'erasing_p\' in hyperparameters.keys():\n            self.erasing_p = 0\n        else:\n            self.erasing_p = hyperparameters[\'erasing_p\']\n        self.single_re = RandomErasing(probability = self.erasing_p, mean=[0.0, 0.0, 0.0])\n\n        if not \'T_w\' in hyperparameters.keys():\n            hyperparameters[\'T_w\'] = 1\n        # Setup the optimizers\n        beta1 = hyperparameters[\'beta1\']\n        beta2 = hyperparameters[\'beta2\']\n        dis_params = list(self.dis_a.parameters()) #+ list(self.dis_b.parameters())\n        gen_params = list(self.gen_a.parameters()) #+ list(self.gen_b.parameters())\n\n        self.dis_opt = torch.optim.Adam([p for p in dis_params if p.requires_grad],\n                                        lr=lr_d, betas=(beta1, beta2), weight_decay=hyperparameters[\'weight_decay\'])\n        self.gen_opt = torch.optim.Adam([p for p in gen_params if p.requires_grad],\n                                        lr=lr_g, betas=(beta1, beta2), weight_decay=hyperparameters[\'weight_decay\'])\n        # id params\n        if hyperparameters[\'ID_style\']==\'PCB\':\n            ignored_params = (list(map(id, self.id_a.classifier0.parameters() ))\n                            +list(map(id, self.id_a.classifier1.parameters() ))\n                            +list(map(id, self.id_a.classifier2.parameters() ))\n                            +list(map(id, self.id_a.classifier3.parameters() ))\n                            )\n            base_params = filter(lambda p: id(p) not in ignored_params, self.id_a.parameters())\n            lr2 = hyperparameters[\'lr2\']\n            self.id_opt = torch.optim.SGD([\n                 {\'params\': base_params, \'lr\': lr2},\n                 {\'params\': self.id_a.classifier0.parameters(), \'lr\': lr2*10},\n                 {\'params\': self.id_a.classifier1.parameters(), \'lr\': lr2*10},\n                 {\'params\': self.id_a.classifier2.parameters(), \'lr\': lr2*10},\n                 {\'params\': self.id_a.classifier3.parameters(), \'lr\': lr2*10}\n            ], weight_decay=hyperparameters[\'weight_decay\'], momentum=0.9, nesterov=True)\n        elif hyperparameters[\'ID_style\']==\'AB\':\n            ignored_params = (list(map(id, self.id_a.classifier1.parameters()))\n                            + list(map(id, self.id_a.classifier2.parameters())))\n            base_params = filter(lambda p: id(p) not in ignored_params, self.id_a.parameters())\n            lr2 = hyperparameters[\'lr2\']\n            self.id_opt = torch.optim.SGD([\n                 {\'params\': base_params, \'lr\': lr2},\n                 {\'params\': self.id_a.classifier1.parameters(), \'lr\': lr2*10},\n                 {\'params\': self.id_a.classifier2.parameters(), \'lr\': lr2*10}\n            ], weight_decay=hyperparameters[\'weight_decay\'], momentum=0.9, nesterov=True)\n        else:\n            ignored_params = list(map(id, self.id_a.classifier.parameters() ))\n            base_params = filter(lambda p: id(p) not in ignored_params, self.id_a.parameters())\n            lr2 = hyperparameters[\'lr2\']\n            self.id_opt = torch.optim.SGD([\n                 {\'params\': base_params, \'lr\': lr2},\n                 {\'params\': self.id_a.classifier.parameters(), \'lr\': lr2*10}\n            ], weight_decay=hyperparameters[\'weight_decay\'], momentum=0.9, nesterov=True)\n\n        self.dis_scheduler = get_scheduler(self.dis_opt, hyperparameters)\n        self.gen_scheduler = get_scheduler(self.gen_opt, hyperparameters)\n        self.id_scheduler = get_scheduler(self.id_opt, hyperparameters)\n        self.id_scheduler.gamma = hyperparameters[\'gamma2\']\n\n        #ID Loss\n        self.id_criterion = nn.CrossEntropyLoss()\n        self.criterion_teacher = nn.KLDivLoss(size_average=False)\n        # Load VGG model if needed\n        if \'vgg_w\' in hyperparameters.keys() and hyperparameters[\'vgg_w\'] > 0:\n            self.vgg = load_vgg16(hyperparameters[\'vgg_model_path\'] + \'/models\')\n            self.vgg.eval()\n            for param in self.vgg.parameters():\n                param.requires_grad = False\n\n        # save memory\n        if self.fp16:\n            # Name the FP16_Optimizer instance to replace the existing optimizer\n            assert torch.backends.cudnn.enabled, ""fp16 mode requires cudnn backend to be enabled.""\n            self.gen_a = self.gen_a.cuda()\n            self.dis_a = self.dis_a.cuda()\n            self.id_a = self.id_a.cuda()\n\n            self.gen_b = self.gen_a\n            self.dis_b = self.dis_a\n            self.id_b = self.id_a\n\n            self.gen_a, self.gen_opt = amp.initialize(self.gen_a, self.gen_opt, opt_level=""O1"")\n            self.dis_a, self.dis_opt = amp.initialize(self.dis_a, self.dis_opt, opt_level=""O1"")\n            self.id_a, self.id_opt = amp.initialize(self.id_a, self.id_opt, opt_level=""O1"")\n\n    def to_re(self, x):\n        out = torch.FloatTensor(x.size(0), x.size(1), x.size(2), x.size(3))\n        out = out.cuda()\n        for i in range(x.size(0)):\n            out[i,:,:,:] = self.single_re(x[i,:,:,:])\n        return out\n\n    def recon_criterion(self, input, target):\n        diff = input - target.detach()\n        return torch.mean(torch.abs(diff[:]))\n\n    def recon_criterion_sqrt(self, input, target):\n        diff = input - target\n        return torch.mean(torch.sqrt(torch.abs(diff[:])+1e-8))\n\n    def recon_criterion2(self, input, target):\n        diff = input - target\n        return torch.mean(diff[:]**2)\n\n    def recon_cos(self, input, target):\n        cos = torch.nn.CosineSimilarity()\n        cos_dis = 1 - cos(input, target)\n        return torch.mean(cos_dis[:])\n\n    def forward(self, x_a, x_b, xp_a, xp_b):\n        s_a = self.gen_a.encode(self.single(x_a))\n        s_b = self.gen_b.encode(self.single(x_b))\n        f_a, p_a = self.id_a(scale2(x_a))\n        f_b, p_b = self.id_b(scale2(x_b))\n        x_ba = self.gen_a.decode(s_b, f_a)\n        x_ab = self.gen_b.decode(s_a, f_b)\n        x_a_recon = self.gen_a.decode(s_a, f_a)\n        x_b_recon = self.gen_b.decode(s_b, f_b)\n        fp_a, pp_a = self.id_a(scale2(xp_a))\n        fp_b, pp_b = self.id_b(scale2(xp_b))\n        # decode the same person\n        x_a_recon_p = self.gen_a.decode(s_a, fp_a)\n        x_b_recon_p = self.gen_b.decode(s_b, fp_b)\n\n        # Random Erasing only effect the ID and PID loss.\n        if self.erasing_p > 0:\n            x_a_re = self.to_re(scale2(x_a.clone()))\n            x_b_re = self.to_re(scale2(x_b.clone()))\n            xp_a_re = self.to_re(scale2(xp_a.clone()))\n            xp_b_re = self.to_re(scale2(xp_b.clone()))\n            _, p_a = self.id_a(x_a_re)\n            _, p_b = self.id_b(x_b_re)\n            # encode the same ID different photo\n            _, pp_a = self.id_a(xp_a_re) \n            _, pp_b = self.id_b(xp_b_re)\n\n        return x_ab, x_ba, s_a, s_b, f_a, f_b, p_a, p_b, pp_a, pp_b, x_a_recon, x_b_recon, x_a_recon_p, x_b_recon_p\n\n    def gen_update(self, x_ab, x_ba, s_a, s_b, f_a, f_b, p_a, p_b, pp_a, pp_b, x_a_recon, x_b_recon, x_a_recon_p, x_b_recon_p, x_a, x_b, xp_a, xp_b, l_a, l_b, hyperparameters, iteration, num_gpu):\n        # ppa, ppb is the same person\n        self.gen_opt.zero_grad()\n        self.id_opt.zero_grad()\n \n        # no gradient\n        x_ba_copy = Variable(x_ba.data, requires_grad=False)\n        x_ab_copy = Variable(x_ab.data, requires_grad=False)\n\n        rand_num = random.uniform(0,1)\n        #################################\n        # encode structure\n        if hyperparameters[\'use_encoder_again\']>=rand_num:\n            # encode again (encoder is tuned, input is fixed)\n            s_a_recon = self.gen_b.enc_content(self.single(x_ab_copy))\n            s_b_recon = self.gen_a.enc_content(self.single(x_ba_copy))\n        else:\n            # copy the encoder\n            self.enc_content_copy = copy.deepcopy(self.gen_a.enc_content)\n            self.enc_content_copy = self.enc_content_copy.eval()\n            # encode again (encoder is fixed, input is tuned)\n            s_a_recon = self.enc_content_copy(self.single(x_ab))\n            s_b_recon = self.enc_content_copy(self.single(x_ba))\n\n        #################################\n        # encode appearance\n        self.id_a_copy = copy.deepcopy(self.id_a)\n        self.id_a_copy = self.id_a_copy.eval()\n        if hyperparameters[\'train_bn\']:\n            self.id_a_copy = self.id_a_copy.apply(train_bn)\n        self.id_b_copy = self.id_a_copy\n        # encode again (encoder is fixed, input is tuned)\n        f_a_recon, p_a_recon = self.id_a_copy(scale2(x_ba))\n        f_b_recon, p_b_recon = self.id_b_copy(scale2(x_ab))\n\n        # teacher Loss\n        #  Tune the ID model\n        log_sm = nn.LogSoftmax(dim=1)\n        if hyperparameters[\'teacher_w\'] >0 and hyperparameters[\'teacher\'] != """":\n            if hyperparameters[\'ID_style\'] == \'normal\':\n                _, p_a_student = self.id_a(scale2(x_ba_copy))\n                p_a_student = log_sm(p_a_student)\n                p_a_teacher = predict_label(self.teacher_model, scale2(x_ba_copy), num_class = hyperparameters[\'ID_class\'], alabel = l_a, slabel = l_b, teacher_style = hyperparameters[\'teacher_style\'])\n                self.loss_teacher = self.criterion_teacher(p_a_student, p_a_teacher) / p_a_student.size(0)\n\n                _, p_b_student = self.id_b(scale2(x_ab_copy))\n                p_b_student = log_sm(p_b_student)\n                p_b_teacher = predict_label(self.teacher_model, scale2(x_ab_copy), num_class = hyperparameters[\'ID_class\'], alabel = l_b, slabel = l_a, teacher_style = hyperparameters[\'teacher_style\'])\n                self.loss_teacher += self.criterion_teacher(p_b_student, p_b_teacher) / p_b_student.size(0)\n            elif hyperparameters[\'ID_style\'] == \'AB\':\n                # normal teacher-student loss\n                # BA -> LabelA(smooth) + LabelB(batchB)\n                _, p_ba_student = self.id_a(scale2(x_ba_copy))# f_a, s_b\n                p_a_student = log_sm(p_ba_student[0])\n                with torch.no_grad():\n                    p_a_teacher = predict_label(self.teacher_model, scale2(x_ba_copy), num_class = hyperparameters[\'ID_class\'], alabel = l_a, slabel = l_b, teacher_style = hyperparameters[\'teacher_style\'])\n                self.loss_teacher = self.criterion_teacher(p_a_student, p_a_teacher) / p_a_student.size(0)\n\n                _, p_ab_student = self.id_b(scale2(x_ab_copy)) # f_b, s_a\n                p_b_student = log_sm(p_ab_student[0])\n                with torch.no_grad():\n                    p_b_teacher = predict_label(self.teacher_model, scale2(x_ab_copy), num_class = hyperparameters[\'ID_class\'], alabel = l_b, slabel = l_a, teacher_style = hyperparameters[\'teacher_style\'])\n                self.loss_teacher += self.criterion_teacher(p_b_student, p_b_teacher) / p_b_student.size(0)\n\n                # branch b loss\n                # here we give different label\n                loss_B = self.id_criterion(p_ba_student[1], l_b) + self.id_criterion(p_ab_student[1], l_a)\n                self.loss_teacher = hyperparameters[\'T_w\'] * self.loss_teacher + hyperparameters[\'B_w\'] * loss_B\n        else:\n            self.loss_teacher = 0.0\n\n        # auto-encoder image reconstruction\n        self.loss_gen_recon_x_a = self.recon_criterion(x_a_recon, x_a)\n        self.loss_gen_recon_x_b = self.recon_criterion(x_b_recon, x_b)\n        self.loss_gen_recon_xp_a = self.recon_criterion(x_a_recon_p, x_a)\n        self.loss_gen_recon_xp_b = self.recon_criterion(x_b_recon_p, x_b)\n\n        # feature reconstruction\n        self.loss_gen_recon_s_a = self.recon_criterion(s_a_recon, s_a) if hyperparameters[\'recon_s_w\'] > 0 else 0\n        self.loss_gen_recon_s_b = self.recon_criterion(s_b_recon, s_b) if hyperparameters[\'recon_s_w\'] > 0 else 0\n        self.loss_gen_recon_f_a = self.recon_criterion(f_a_recon, f_a) if hyperparameters[\'recon_f_w\'] > 0 else 0\n        self.loss_gen_recon_f_b = self.recon_criterion(f_b_recon, f_b) if hyperparameters[\'recon_f_w\'] > 0 else 0\n\n        x_aba = self.gen_a.decode(s_a_recon, f_a_recon) if hyperparameters[\'recon_x_cyc_w\'] > 0 else None\n        x_bab = self.gen_b.decode(s_b_recon, f_b_recon) if hyperparameters[\'recon_x_cyc_w\'] > 0 else None\n\n        # ID loss AND Tune the Generated image\n        if hyperparameters[\'ID_style\']==\'PCB\':\n            self.loss_id = self.PCB_loss(p_a, l_a) + self.PCB_loss(p_b, l_b)\n            self.loss_pid = self.PCB_loss(pp_a, l_a) + self.PCB_loss(pp_b, l_b)\n            self.loss_gen_recon_id = self.PCB_loss(p_a_recon, l_a) + self.PCB_loss(p_b_recon, l_b)\n        elif hyperparameters[\'ID_style\']==\'AB\':\n            weight_B = hyperparameters[\'teacher_w\'] * hyperparameters[\'B_w\']\n            self.loss_id = self.id_criterion(p_a[0], l_a) + self.id_criterion(p_b[0], l_b) \\\n                         + weight_B * ( self.id_criterion(p_a[1], l_a) + self.id_criterion(p_b[1], l_b) )\n            self.loss_pid = self.id_criterion(pp_a[0], l_a) + self.id_criterion(pp_b[0], l_b) #+ weight_B * ( self.id_criterion(pp_a[1], l_a) + self.id_criterion(pp_b[1], l_b) )\n            self.loss_gen_recon_id = self.id_criterion(p_a_recon[0], l_a) + self.id_criterion(p_b_recon[0], l_b)\n        else:\n            self.loss_id = self.id_criterion(p_a, l_a) + self.id_criterion(p_b, l_b)\n            self.loss_pid = self.id_criterion(pp_a, l_a) + self.id_criterion(pp_b, l_b)\n            self.loss_gen_recon_id = self.id_criterion(p_a_recon, l_a) + self.id_criterion(p_b_recon, l_b)\n\n        #print(f_a_recon, f_a)\n        self.loss_gen_cycrecon_x_a = self.recon_criterion(x_aba, x_a) if hyperparameters[\'recon_x_cyc_w\'] > 0 else 0\n        self.loss_gen_cycrecon_x_b = self.recon_criterion(x_bab, x_b) if hyperparameters[\'recon_x_cyc_w\'] > 0 else 0\n        # GAN loss\n        if num_gpu>1:\n            self.loss_gen_adv_a = self.dis_a.module.calc_gen_loss(self.dis_a, x_ba)\n            self.loss_gen_adv_b = self.dis_b.module.calc_gen_loss(self.dis_b, x_ab)\n        else:\n            self.loss_gen_adv_a = self.dis_a.calc_gen_loss(self.dis_a, x_ba)\n            self.loss_gen_adv_b = self.dis_b.calc_gen_loss(self.dis_b, x_ab)\n        # domain-invariant perceptual loss\n        self.loss_gen_vgg_a = self.compute_vgg_loss(self.vgg, x_ba, x_b) if hyperparameters[\'vgg_w\'] > 0 else 0\n        self.loss_gen_vgg_b = self.compute_vgg_loss(self.vgg, x_ab, x_a) if hyperparameters[\'vgg_w\'] > 0 else 0\n\n        if iteration > hyperparameters[\'warm_iter\']:\n            hyperparameters[\'recon_f_w\'] += hyperparameters[\'warm_scale\']\n            hyperparameters[\'recon_f_w\'] = min(hyperparameters[\'recon_f_w\'], hyperparameters[\'max_w\'])\n            hyperparameters[\'recon_s_w\'] += hyperparameters[\'warm_scale\']\n            hyperparameters[\'recon_s_w\'] = min(hyperparameters[\'recon_s_w\'], hyperparameters[\'max_w\'])\n            hyperparameters[\'recon_x_cyc_w\'] += hyperparameters[\'warm_scale\']\n            hyperparameters[\'recon_x_cyc_w\'] = min(hyperparameters[\'recon_x_cyc_w\'], hyperparameters[\'max_cyc_w\'])\n\n        if iteration > hyperparameters[\'warm_teacher_iter\']:\n            hyperparameters[\'teacher_w\'] += hyperparameters[\'warm_scale\']\n            hyperparameters[\'teacher_w\'] = min(hyperparameters[\'teacher_w\'], hyperparameters[\'max_teacher_w\'])\n        # total loss\n        self.loss_gen_total = hyperparameters[\'gan_w\'] * self.loss_gen_adv_a + \\\n                              hyperparameters[\'gan_w\'] * self.loss_gen_adv_b + \\\n                              hyperparameters[\'recon_x_w\'] * self.loss_gen_recon_x_a + \\\n                              hyperparameters[\'recon_xp_w\'] * self.loss_gen_recon_xp_a + \\\n                              hyperparameters[\'recon_f_w\'] * self.loss_gen_recon_f_a + \\\n                              hyperparameters[\'recon_s_w\'] * self.loss_gen_recon_s_a + \\\n                              hyperparameters[\'recon_x_w\'] * self.loss_gen_recon_x_b + \\\n                              hyperparameters[\'recon_xp_w\'] * self.loss_gen_recon_xp_b + \\\n                              hyperparameters[\'recon_f_w\'] * self.loss_gen_recon_f_b + \\\n                              hyperparameters[\'recon_s_w\'] * self.loss_gen_recon_s_b + \\\n                              hyperparameters[\'recon_x_cyc_w\'] * self.loss_gen_cycrecon_x_a + \\\n                              hyperparameters[\'recon_x_cyc_w\'] * self.loss_gen_cycrecon_x_b + \\\n                              hyperparameters[\'id_w\'] * self.loss_id + \\\n                              hyperparameters[\'pid_w\'] * self.loss_pid + \\\n                              hyperparameters[\'recon_id_w\'] * self.loss_gen_recon_id + \\\n                              hyperparameters[\'vgg_w\'] * self.loss_gen_vgg_a + \\\n                              hyperparameters[\'vgg_w\'] * self.loss_gen_vgg_b + \\\n                              hyperparameters[\'teacher_w\'] * self.loss_teacher\n        if self.fp16:\n            with amp.scale_loss(self.loss_gen_total, [self.gen_opt,self.id_opt]) as scaled_loss:\n                scaled_loss.backward()\n            self.gen_opt.step()\n            self.id_opt.step()\n        else:\n            self.loss_gen_total.backward()\n            self.gen_opt.step()\n            self.id_opt.step()\n        print(""L_total: %.4f, L_gan: %.4f,  Lx: %.4f, Lxp: %.4f, Lrecycle:%.4f, Lf: %.4f, Ls: %.4f, Recon-id: %.4f, id: %.4f, pid:%.4f, teacher: %.4f""%( self.loss_gen_total, \\\n                                                        hyperparameters[\'gan_w\'] * (self.loss_gen_adv_a + self.loss_gen_adv_b), \\\n                                                        hyperparameters[\'recon_x_w\'] * (self.loss_gen_recon_x_a + self.loss_gen_recon_x_b), \\\n                                                        hyperparameters[\'recon_xp_w\'] * (self.loss_gen_recon_xp_a + self.loss_gen_recon_xp_b), \\\n                                                        hyperparameters[\'recon_x_cyc_w\'] * (self.loss_gen_cycrecon_x_a + self.loss_gen_cycrecon_x_b), \\\n                                                        hyperparameters[\'recon_f_w\'] * (self.loss_gen_recon_f_a + self.loss_gen_recon_f_b), \\\n                                                        hyperparameters[\'recon_s_w\'] * (self.loss_gen_recon_s_a + self.loss_gen_recon_s_b), \\\n                                                        hyperparameters[\'recon_id_w\'] * self.loss_gen_recon_id, \\\n                                                        hyperparameters[\'id_w\'] * self.loss_id,\\\n                                                        hyperparameters[\'pid_w\'] * self.loss_pid,\\\nhyperparameters[\'teacher_w\'] * self.loss_teacher )  )\n\n    def compute_vgg_loss(self, vgg, img, target):\n        img_vgg = vgg_preprocess(img)\n        target_vgg = vgg_preprocess(target)\n        img_fea = vgg(img_vgg)\n        target_fea = vgg(target_vgg)\n        return torch.mean((self.instancenorm(img_fea) - self.instancenorm(target_fea)) ** 2)\n\n    def PCB_loss(self, inputs, labels):\n       loss = 0.0\n       for part in inputs:\n           loss += self.id_criterion(part, labels)\n       return loss/len(inputs)\n\n    def sample(self, x_a, x_b):\n        self.eval()\n        x_a_recon, x_b_recon, x_ba1, x_ab1, x_aba, x_bab = [], [], [], [], [], []\n        for i in range(x_a.size(0)):\n            s_a = self.gen_a.encode( self.single(x_a[i].unsqueeze(0)) )\n            s_b = self.gen_b.encode( self.single(x_b[i].unsqueeze(0)) )\n            f_a, _ = self.id_a( scale2(x_a[i].unsqueeze(0)))\n            f_b, _ = self.id_b( scale2(x_b[i].unsqueeze(0)))\n            x_a_recon.append(self.gen_a.decode(s_a, f_a))\n            x_b_recon.append(self.gen_b.decode(s_b, f_b))\n            x_ba = self.gen_a.decode(s_b, f_a)\n            x_ab = self.gen_b.decode(s_a, f_b)\n            x_ba1.append(x_ba)\n            x_ab1.append(x_ab)\n            #cycle\n            s_b_recon = self.gen_a.enc_content(self.single(x_ba))\n            s_a_recon = self.gen_b.enc_content(self.single(x_ab))\n            f_a_recon, _ = self.id_a(scale2(x_ba))\n            f_b_recon, _ = self.id_b(scale2(x_ab))\n            x_aba.append(self.gen_a.decode(s_a_recon, f_a_recon))\n            x_bab.append(self.gen_b.decode(s_b_recon, f_b_recon))\n\n        x_a_recon, x_b_recon = torch.cat(x_a_recon), torch.cat(x_b_recon)\n        x_aba, x_bab = torch.cat(x_aba), torch.cat(x_bab)\n        x_ba1, x_ab1 = torch.cat(x_ba1), torch.cat(x_ab1)\n        self.train()\n\n        return x_a, x_a_recon, x_aba, x_ab1, x_b, x_b_recon, x_bab, x_ba1\n\n    def dis_update(self, x_ab, x_ba, x_a, x_b, hyperparameters, num_gpu):\n        self.dis_opt.zero_grad()\n        # D loss\n        if num_gpu>1:\n            self.loss_dis_a, reg_a = self.dis_a.module.calc_dis_loss(self.dis_a, x_ba.detach(), x_a)\n            self.loss_dis_b, reg_b = self.dis_b.module.calc_dis_loss(self.dis_b, x_ab.detach(), x_b)\n        else:\n            self.loss_dis_a, reg_a = self.dis_a.calc_dis_loss(self.dis_a, x_ba.detach(), x_a)\n            self.loss_dis_b, reg_b = self.dis_b.calc_dis_loss(self.dis_b, x_ab.detach(), x_b)\n        self.loss_dis_total = hyperparameters[\'gan_w\'] * self.loss_dis_a + hyperparameters[\'gan_w\'] * self.loss_dis_b\n        print(""DLoss: %.4f""%self.loss_dis_total, ""Reg: %.4f""%(reg_a+reg_b) )\n        if self.fp16:\n            with amp.scale_loss(self.loss_dis_total, self.dis_opt) as scaled_loss:\n                scaled_loss.backward()\n        else:\n            self.loss_dis_total.backward()\n        self.dis_opt.step()\n\n    def update_learning_rate(self):\n        if self.dis_scheduler is not None:\n            self.dis_scheduler.step()\n        if self.gen_scheduler is not None:\n            self.gen_scheduler.step()\n        if self.id_scheduler is not None:\n            self.id_scheduler.step()\n\n    def resume(self, checkpoint_dir, hyperparameters):\n        # Load generators\n        last_model_name = get_model_list(checkpoint_dir, ""gen"")\n        state_dict = torch.load(last_model_name)\n        self.gen_a.load_state_dict(state_dict[\'a\'])\n        self.gen_b = self.gen_a\n        iterations = int(last_model_name[-11:-3])\n        # Load discriminators\n        last_model_name = get_model_list(checkpoint_dir, ""dis"")\n        state_dict = torch.load(last_model_name)\n        self.dis_a.load_state_dict(state_dict[\'a\'])\n        self.dis_b = self.dis_a\n        # Load ID dis\n        last_model_name = get_model_list(checkpoint_dir, ""id"")\n        state_dict = torch.load(last_model_name)\n        self.id_a.load_state_dict(state_dict[\'a\'])\n        self.id_b = self.id_a\n        # Load optimizers\n        try:\n            state_dict = torch.load(os.path.join(checkpoint_dir, \'optimizer.pt\'))\n            self.dis_opt.load_state_dict(state_dict[\'dis\'])\n            self.gen_opt.load_state_dict(state_dict[\'gen\'])\n            self.id_opt.load_state_dict(state_dict[\'id\'])\n        except:\n            pass\n        # Reinitilize schedulers\n        self.dis_scheduler = get_scheduler(self.dis_opt, hyperparameters, iterations)\n        self.gen_scheduler = get_scheduler(self.gen_opt, hyperparameters, iterations)\n        print(\'Resume from iteration %d\' % iterations)\n        return iterations\n\n    def save(self, snapshot_dir, iterations, num_gpu=1):\n        # Save generators, discriminators, and optimizers\n        gen_name = os.path.join(snapshot_dir, \'gen_%08d.pt\' % (iterations + 1))\n        dis_name = os.path.join(snapshot_dir, \'dis_%08d.pt\' % (iterations + 1))\n        id_name = os.path.join(snapshot_dir, \'id_%08d.pt\' % (iterations + 1))\n        opt_name = os.path.join(snapshot_dir, \'optimizer.pt\')\n        torch.save({\'a\': self.gen_a.state_dict()}, gen_name)\n        if num_gpu>1:\n            torch.save({\'a\': self.dis_a.module.state_dict()}, dis_name)\n        else:\n            torch.save({\'a\': self.dis_a.state_dict()}, dis_name)\n        torch.save({\'a\': self.id_a.state_dict()}, id_name)\n        torch.save({\'gen\': self.gen_opt.state_dict(), \'id\': self.id_opt.state_dict(),  \'dis\': self.dis_opt.state_dict()}, opt_name)\n\n\n\n'"
utils.py,9,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\nfrom torch.optim import lr_scheduler\nfrom torchvision import transforms\nfrom data import ImageFilelist\nfrom reIDfolder import ReIDFolder\nimport torch\nimport os\nimport math\nimport torchvision.utils as vutils\nimport yaml\nimport numpy as np\nimport torch.nn.init as init\nimport time\n# Methods\n# get_all_data_loaders      : primary data loader interface (load trainA, testA, trainB, testB)\n# get_data_loader_list      : list-based data loader\n# get_data_loader_folder    : folder-based data loader\n# get_config                : load yaml file\n# eformat                   :\n# write_2images             : save output image\n# prepare_sub_folder        : create checkpoints and images folders for saving outputs\n# write_one_row_html        : write one row of the html file for output images\n# write_html                : create the html file.\n# write_loss\n# slerp\n# get_slerp_interp\n# get_model_list\n# load_vgg16\n# vgg_preprocess\n# get_scheduler\n# weights_init\n\ndef get_all_data_loaders(conf):\n    batch_size = conf[\'batch_size\']\n    num_workers = conf[\'num_workers\']\n    if \'new_size\' in conf:\n        new_size_a= conf[\'new_size\']\n        new_size_b = conf[\'new_size\']\n    else:\n        new_size_a = conf[\'new_size_a\']\n        new_size_b = conf[\'new_size_b\']\n    height = conf[\'crop_image_height\']\n    width = conf[\'crop_image_width\']\n\n    if \'data_root\' in conf:\n        train_loader_a = get_data_loader_folder(os.path.join(conf[\'data_root\'], \'train_all\'), batch_size, True,\n                                              new_size_a, height, width, num_workers, True)\n        test_loader_a = get_data_loader_folder(os.path.join(conf[\'data_root\'], \'query\'), batch_size, False,\n                                             new_size_a, height, width, num_workers, False)\n        train_loader_b = get_data_loader_folder(os.path.join(conf[\'data_root\'], \'train_all\'), batch_size, True,\n                                              new_size_b, height, width, num_workers, True)\n        test_loader_b = get_data_loader_folder(os.path.join(conf[\'data_root\'], \'query\'), batch_size, False,\n                                             new_size_b, height, width, num_workers, False)\n    else:\n        train_loader_a = get_data_loader_list(conf[\'data_folder_train_a\'], conf[\'data_list_train_a\'], batch_size, True,\n                                                new_size_a, height, width, num_workers, True)\n        test_loader_a = get_data_loader_list(conf[\'data_folder_test_a\'], conf[\'data_list_test_a\'], batch_size, False,\n                                                new_size_a, height, width, num_workers, False)\n        train_loader_b = get_data_loader_list(conf[\'data_folder_train_b\'], conf[\'data_list_train_b\'], batch_size, True,\n                                                new_size_b, height, width, num_workers, True)\n        test_loader_b = get_data_loader_list(conf[\'data_folder_test_b\'], conf[\'data_list_test_b\'], batch_size, False,\n                                                new_size_b, height, width, num_workers, False)\n    return train_loader_a, train_loader_b, test_loader_a, test_loader_b\n\n\ndef get_data_loader_list(root, file_list, batch_size, train, new_size=None,\n                           height=256, width=128, num_workers=4, crop=True):\n    transform_list = [transforms.ToTensor(),\n                      transforms.Normalize((0.485, 0.456, 0.406),  \n                                           (0.229, 0.224, 0.225))]\n    transform_list = [transforms.RandomCrop((height, width))] + transform_list if crop else transform_list\n    transform_list = [transforms.Pad(10, padding_mode=\'edge\')] + transform_list if train else transform_list\n    transform_list = [transforms.Resize((height, width), interpolation=3)] + transform_list if new_size is not None else transform_list\n    transform_list = [transforms.RandomHorizontalFlip()] + transform_list if train else transform_list\n    transform = transforms.Compose(transform_list)\n    dataset = ImageFilelist(root, file_list, transform=transform)\n    loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=num_workers)\n    return loader\n\ndef get_data_loader_folder(input_folder, batch_size, train, new_size=None,\n                           height=256, width=128, num_workers=4, crop=True):\n    transform_list = [transforms.ToTensor(),\n                      transforms.Normalize((0.485, 0.456, 0.406),\n                                           (0.229, 0.224, 0.225))]\n    transform_list = [transforms.RandomCrop((height, width))] + transform_list if crop else transform_list\n    transform_list = [transforms.Pad(10, padding_mode=\'edge\')] + transform_list if train else transform_list\n    transform_list = [transforms.Resize((height,width), interpolation=3)] + transform_list if new_size is not None else transform_list\n    transform_list = [transforms.RandomHorizontalFlip()] + transform_list if train else transform_list\n    transform = transforms.Compose(transform_list)\n    dataset = ReIDFolder(input_folder, transform=transform)\n    loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=num_workers)\n    return loader\n\n\ndef get_config(config):\n    with open(config, \'r\') as stream:\n        return yaml.load(stream)\n\n\ndef eformat(f, prec):\n    s = ""%.*e""%(prec, f)\n    mantissa, exp = s.split(\'e\')\n    # add 1 to digits as 1 is taken by sign +/-\n    return ""%se%d""%(mantissa, int(exp))\n\n\ndef __write_images(image_outputs, display_image_num, file_name):\n    image_outputs = [images.expand(-1, 3, -1, -1) for images in image_outputs] # expand gray-scale images to 3 channels\n    image_tensor = torch.cat([images[:display_image_num] for images in image_outputs], 0)\n    image_grid = vutils.make_grid(image_tensor.data, nrow=display_image_num, padding=0, normalize=True, scale_each=True)\n    vutils.save_image(image_grid, file_name, nrow=1)\n\n\ndef write_2images(image_outputs, display_image_num, image_directory, postfix):\n    n = len(image_outputs)\n    __write_images(image_outputs[0:n//2], display_image_num, \'%s/gen_a2b_%s.jpg\' % (image_directory, postfix))\n    __write_images(image_outputs[n//2:n], display_image_num, \'%s/gen_b2a_%s.jpg\' % (image_directory, postfix))\n\n\ndef prepare_sub_folder(output_directory):\n    image_directory = os.path.join(output_directory, \'images\')\n    if not os.path.exists(image_directory):\n        print(""Creating directory: {}"".format(image_directory))\n        os.makedirs(image_directory)\n    checkpoint_directory = os.path.join(output_directory, \'checkpoints\')\n    if not os.path.exists(checkpoint_directory):\n        print(""Creating directory: {}"".format(checkpoint_directory))\n        os.makedirs(checkpoint_directory)\n    return checkpoint_directory, image_directory\n\n\ndef write_one_row_html(html_file, iterations, img_filename, all_size):\n    html_file.write(""<h3>iteration [%d] (%s)</h3>"" % (iterations,img_filename.split(\'/\')[-1]))\n    html_file.write(""""""\n        <p><a href=""%s"">\n          <img src=""%s"" style=""width:%dpx"">\n        </a><br>\n        <p>\n        """""" % (img_filename, img_filename, all_size))\n    return\n\n\ndef write_html(filename, iterations, image_save_iterations, image_directory, all_size=1536):\n    html_file = open(filename, ""w"")\n    html_file.write(\'\'\'\n    <!DOCTYPE html>\n    <html>\n    <head>\n      <title>Experiment name = %s</title>\n      <meta http-equiv=""refresh"" content=""30"">\n    </head>\n    <body>\n    \'\'\' % os.path.basename(filename))\n    html_file.write(""<h3>current</h3>"")\n    write_one_row_html(html_file, iterations, \'%s/gen_a2b_train_current.jpg\' % (image_directory), all_size)\n    write_one_row_html(html_file, iterations, \'%s/gen_b2a_train_current.jpg\' % (image_directory), all_size)\n    for j in range(iterations, image_save_iterations-1, -1):\n        if j % image_save_iterations == 0:\n            write_one_row_html(html_file, j, \'%s/gen_a2b_test_%08d.jpg\' % (image_directory, j), all_size)\n            write_one_row_html(html_file, j, \'%s/gen_b2a_test_%08d.jpg\' % (image_directory, j), all_size)\n            write_one_row_html(html_file, j, \'%s/gen_a2b_train_%08d.jpg\' % (image_directory, j), all_size)\n            write_one_row_html(html_file, j, \'%s/gen_b2a_train_%08d.jpg\' % (image_directory, j), all_size)\n    html_file.write(""</body></html>"")\n    html_file.close()\n\n\ndef write_loss(iterations, trainer, train_writer):\n    members = [attr for attr in dir(trainer) \\\n               if not callable(getattr(trainer, attr)) and not attr.startswith(""__"") and (\'loss\' in attr or \'grad\' in attr or \'nwd\' in attr)]\n    for m in members:\n        train_writer.add_scalar(m, getattr(trainer, m), iterations + 1)\n\n\ndef slerp(val, low, high):\n    """"""\n    original: Animating Rotation with Quaternion Curves, Ken Shoemake\n    https://arxiv.org/abs/1609.04468\n    Code: https://github.com/soumith/dcgan.torch/issues/14, Tom White\n    """"""\n    omega = np.arccos(np.dot(low / np.linalg.norm(low), high / np.linalg.norm(high)))\n    so = np.sin(omega)\n    return np.sin((1.0 - val) * omega) / so * low + np.sin(val * omega) / so * high\n\n\ndef get_slerp_interp(nb_latents, nb_interp, z_dim):\n    """"""\n    modified from: PyTorch inference for ""Progressive Growing of GANs"" with CelebA snapshot\n    https://github.com/ptrblck/prog_gans_pytorch_inference\n    """"""\n\n    latent_interps = np.empty(shape=(0, z_dim), dtype=np.float32)\n    for _ in range(nb_latents):\n        low = np.random.randn(z_dim)\n        high = np.random.randn(z_dim)  # low + np.random.randn(512) * 0.7\n        interp_vals = np.linspace(0, 1, num=nb_interp)\n        latent_interp = np.array([slerp(v, low, high) for v in interp_vals],\n                                 dtype=np.float32)\n        latent_interps = np.vstack((latent_interps, latent_interp))\n\n    return latent_interps[:, :, np.newaxis, np.newaxis]\n\n\n# Get model list for resume\ndef get_model_list(dirname, key):\n    if os.path.exists(dirname) is False:\n        return None\n    gen_models = [os.path.join(dirname, f) for f in os.listdir(dirname) if\n                  os.path.isfile(os.path.join(dirname, f)) and key in f and "".pt"" in f]\n    if gen_models is None:\n        return None\n    gen_models.sort()\n    last_model_name = gen_models[-1]\n    return last_model_name\n\n\ndef load_vgg16(model_dir):\n    """""" Use the model from https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/utils.py """"""\n    if not os.path.exists(model_dir):\n        os.mkdir(model_dir)\n    if not os.path.exists(os.path.join(model_dir, \'vgg16.weight\')):\n        if not os.path.exists(os.path.join(model_dir, \'vgg16.t7\')):\n            os.system(\'wget https://www.dropbox.com/s/76l3rt4kyi3s8x7/vgg16.t7?dl=1 -O \' + os.path.join(model_dir, \'vgg16.t7\'))\n        vgglua = load_lua(os.path.join(model_dir, \'vgg16.t7\'))\n        vgg = Vgg16()\n        for (src, dst) in zip(vgglua.parameters()[0], vgg.parameters()):\n            dst.data[:] = src\n        torch.save(vgg.state_dict(), os.path.join(model_dir, \'vgg16.weight\'))\n    vgg = Vgg16()\n    vgg.load_state_dict(torch.load(os.path.join(model_dir, \'vgg16.weight\')))\n    return vgg\n\n\ndef vgg_preprocess(batch):\n    tensortype = type(batch.data)\n    (r, g, b) = torch.chunk(batch, 3, dim = 1)\n    batch = torch.cat((b, g, r), dim = 1) # convert RGB to BGR\n    batch = (batch + 1) * 255 * 0.5 # [-1, 1] -> [0, 255]\n    mean = tensortype(batch.data.size())\n    mean[:, 0, :, :] = 103.939\n    mean[:, 1, :, :] = 116.779\n    mean[:, 2, :, :] = 123.680\n    batch = batch.sub(Variable(mean)) # subtract mean\n    return batch\n\n\ndef get_scheduler(optimizer, hyperparameters, iterations=-1):\n    if \'lr_policy\' not in hyperparameters or hyperparameters[\'lr_policy\'] == \'constant\':\n        scheduler = None # constant scheduler\n    elif hyperparameters[\'lr_policy\'] == \'step\':\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=hyperparameters[\'step_size\'],\n                                        gamma=hyperparameters[\'gamma\'], last_epoch=iterations)\n    elif hyperparameters[\'lr_policy\'] == \'multistep\':\n        #50000 -- 75000 -- \n        step = hyperparameters[\'step_size\']\n        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[step, step+step//2, step+step//2+step//4],\n                                        gamma=hyperparameters[\'gamma\'], last_epoch=iterations)\n    else:\n        return NotImplementedError(\'learning rate policy [%s] is not implemented\', hyperparameters[\'lr_policy\'])\n    return scheduler\n\n\ndef weights_init(init_type=\'gaussian\'):\n    def init_fun(m):\n        classname = m.__class__.__name__\n        if (classname.find(\'Conv\') == 0 or classname.find(\'Linear\') == 0) and hasattr(m, \'weight\'):\n            # print m.__class__.__name__\n            if init_type == \'gaussian\':\n                init.normal_(m.weight.data, 0.0, 0.02)\n            elif init_type == \'xavier\':\n                init.xavier_normal_(m.weight.data, gain=math.sqrt(2))\n            elif init_type == \'kaiming\':\n                init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\')\n            elif init_type == \'orthogonal\':\n                init.orthogonal_(m.weight.data, gain=math.sqrt(2))\n            elif init_type == \'default\':\n                pass\n            else:\n                assert 0, ""Unsupported initialization: {}"".format(init_type)\n            if hasattr(m, \'bias\') and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n\n    return init_fun\n\n\nclass Timer:\n    def __init__(self, msg):\n        self.msg = msg\n        self.start_time = None\n\n    def __enter__(self):\n        self.start_time = time.time()\n\n    def __exit__(self, exc_type, exc_value, exc_tb):\n        print(self.msg % (time.time() - self.start_time))\n\n\n'"
reid_eval/evaluate_gpu.py,8,"b'\n""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport scipy.io\nimport torch\nimport numpy as np\nimport os\nimport matplotlib\nmatplotlib.use(\'agg\')\nimport matplotlib.pyplot as plt\n#######################################################################\n# Evaluate\n\ndef evaluate(qf,ql,qc,gf,gl,gc):\n    query = qf.view(-1,1)\n    score = torch.mm(gf,query)\n    score = score.squeeze(1).cpu()\n    score = score.numpy()\n    # predict index\n    index = np.argsort(score)  #from small to large\n    index = index[::-1]\n    # good index\n    query_index = np.argwhere(gl==ql)\n    #same camera\n    camera_index = np.argwhere(gc==qc)\n\n    good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n    junk_index1 = np.argwhere(gl==-1)\n    junk_index2 = np.intersect1d(query_index, camera_index)\n    junk_index = np.append(junk_index2, junk_index1) #.flatten())\n    \n    CMC_tmp = compute_mAP(index, qc, good_index, junk_index)\n    return CMC_tmp\n\n\ndef compute_mAP(index, qc, good_index, junk_index):\n    ap = 0\n    cmc = torch.IntTensor(len(index)).zero_()\n    if good_index.size==0:   # if empty\n        cmc[0] = -1\n        return ap,cmc\n\n    # remove junk_index\n    ranked_camera = gallery_cam[index]\n    mask = np.in1d(index, junk_index, invert=True)\n    mask2 = np.in1d(index, np.append(good_index,junk_index), invert=True)\n    index = index[mask]\n    ranked_camera = ranked_camera[mask]\n\n    # find good_index index\n    ngood = len(good_index)\n    mask = np.in1d(index, good_index)\n    rows_good = np.argwhere(mask==True)\n    rows_good = rows_good.flatten()\n    \n    cmc[rows_good[0]:] = 1\n    for i in range(ngood):\n        d_recall = 1.0/ngood\n        precision = (i+1)*1.0/(rows_good[i]+1)\n        if rows_good[i]!=0:\n            old_precision = i*1.0/rows_good[i]\n        else:\n            old_precision=1.0\n        ap = ap + d_recall*(old_precision + precision)/2\n\n    return ap, cmc\n\n######################################################################\nresult = scipy.io.loadmat(\'pytorch_result.mat\')\nquery_feature = torch.FloatTensor(result[\'query_f\'])\nquery_cam = result[\'query_cam\'][0]\nquery_label = result[\'query_label\'][0]\ngallery_feature = torch.FloatTensor(result[\'gallery_f\'])\ngallery_cam = result[\'gallery_cam\'][0]\ngallery_label = result[\'gallery_label\'][0]\n\nmulti = os.path.isfile(\'multi_query.mat\')\n\nif multi:\n    m_result = scipy.io.loadmat(\'multi_query.mat\')\n    mquery_feature = torch.FloatTensor(m_result[\'mquery_f\'])\n    mquery_cam = m_result[\'mquery_cam\'][0]\n    mquery_label = m_result[\'mquery_label\'][0]\n    mquery_feature = mquery_feature.cuda()\n\nquery_feature = query_feature.cuda()\ngallery_feature = gallery_feature.cuda()\n\nprint(query_feature.shape)\nalpha = [0, 0.5, -1]\nfor j in range(len(alpha)):\n    CMC = torch.IntTensor(len(gallery_label)).zero_()    \n    ap = 0.0\n    for i in range(len(query_label)):\n        qf = query_feature[i].clone()\n        if alpha[j] == -1:\n            qf[0:512] *= 0\n        else:\n            qf[512:1024] *= alpha[j]\n        ap_tmp, CMC_tmp = evaluate(qf,query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n        if CMC_tmp[0]==-1:\n            continue\n        CMC = CMC + CMC_tmp\n        ap += ap_tmp\n\n    CMC = CMC.float()\n    CMC = CMC/len(query_label) #average CMC\n    print(\'Alpha:%.2f Rank@1:%.4f Rank@5:%.4f Rank@10:%.4f mAP:%.4f\'%(alpha[j], CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n\n# multiple-query\nCMC = torch.IntTensor(len(gallery_label)).zero_()\nap = 0.0\nif multi:\n    malpha = 0.5 ######\n    for i in range(len(query_label)):\n        mquery_index1 = np.argwhere(mquery_label==query_label[i])\n        mquery_index2 = np.argwhere(mquery_cam==query_cam[i])\n        mquery_index =  np.intersect1d(mquery_index1, mquery_index2)\n        mq = torch.mean(mquery_feature[mquery_index,:], dim=0)\n        mq[512:1024] *= malpha\n        ap_tmp, CMC_tmp = evaluate(mq,query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n        if CMC_tmp[0]==-1:\n            continue\n        CMC = CMC + CMC_tmp\n        ap += ap_tmp\n    CMC = CMC.float()\n    CMC = CMC/len(query_label) #average CMC\n    print(\'multi Rank@1:%.4f Rank@5:%.4f Rank@10:%.4f mAP:%.4f\'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n'"
reid_eval/test_2label.py,19,"b'\n""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nfrom __future__ import print_function, division\n\nimport sys\nsys.path.append(\'..\')\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport time\nimport os\nimport scipy.io\nimport yaml\nfrom reIDmodel import ft_net, ft_netAB, ft_net_dense, PCB, PCB_test\n\n######################################################################\n# Options\n# --------\nparser = argparse.ArgumentParser(description=\'Training\')\nparser.add_argument(\'--gpu_ids\',default=\'0\', type=str,help=\'gpu_ids: e.g. 0  0,1,2  0,2\')\nparser.add_argument(\'--which_epoch\',default=90000, type=int, help=\'80000\')\nparser.add_argument(\'--test_dir\',default=\'../../Market/pytorch\',type=str, help=\'./test_data\')\nparser.add_argument(\'--name\', default=\'test\', type=str, help=\'save model path\')\nparser.add_argument(\'--batchsize\', default=80, type=int, help=\'batchsize\')\nparser.add_argument(\'--use_dense\', action=\'store_true\', help=\'use densenet121\' )\nparser.add_argument(\'--PCB\', action=\'store_true\', help=\'use PCB\' )\nparser.add_argument(\'--multi\', action=\'store_true\', help=\'use multiple query\' )\n\nopt = parser.parse_args()\n\nstr_ids = opt.gpu_ids.split(\',\')\nwhich_epoch = opt.which_epoch\nname = opt.name\ntest_dir = opt.test_dir\n\ngpu_ids = []\nfor str_id in str_ids:\n    id = int(str_id)\n    if id >=0:\n        gpu_ids.append(id)\n\n# set gpu ids\nif len(gpu_ids)>0:\n    torch.cuda.set_device(gpu_ids[0])\n\n######################################################################\n# Load Data\n# ---------\n#\n# We will use torchvision and torch.utils.data packages for loading the\n# data.\n#\ndata_transforms = transforms.Compose([\n        transforms.Resize((256,128), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nif opt.PCB:\n    data_transforms = transforms.Compose([\n        transforms.Resize((384,192), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n    ])\n\n\ndata_dir = test_dir\nimage_datasets = {x: datasets.ImageFolder( os.path.join(data_dir,x) ,data_transforms) for x in [\'gallery\',\'query\',\'multi-query\']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=False, num_workers=16) for x in [\'gallery\',\'query\',\'multi-query\']}\n\nclass_names = image_datasets[\'query\'].classes\nuse_gpu = torch.cuda.is_available()\n\n######################################################################\n# Load model\n#---------------------------\ndef load_network(network):\n    save_path = os.path.join(\'../outputs\',name,\'checkpoints/id_%08d.pt\'%opt.which_epoch)\n    state_dict = torch.load(save_path)\n    network.load_state_dict(state_dict[\'a\'], strict=False)\n    return network\n\n\n######################################################################\n# Extract feature\n# ----------------------\n#\n# Extract feature from  a trained model.\n#\ndef fliplr(img):\n    \'\'\'flip horizontal\'\'\'\n    inv_idx = torch.arange(img.size(3)-1,-1,-1).long()  # N x C x H x W\n    img_flip = img.index_select(3,inv_idx)\n    return img_flip\n\ndef norm(f):\n    f = f.squeeze()\n    fnorm = torch.norm(f, p=2, dim=1, keepdim=True)\n    f = f.div(fnorm.expand_as(f))\n    return f\n\ndef extract_feature(model,dataloaders):\n    features = torch.FloatTensor()\n    count = 0\n    for data in dataloaders:\n        img, label = data\n        n, c, h, w = img.size()\n        count += n\n        if opt.use_dense:\n            ff = torch.FloatTensor(n,1024).zero_()\n        else:\n            ff = torch.FloatTensor(n,1024).zero_()\n        if opt.PCB:\n            ff = torch.FloatTensor(n,2048,6).zero_() # we have six parts\n        for i in range(2):\n            if(i==1):\n                img = fliplr(img)\n            input_img = Variable(img.cuda())\n            f, x = model(input_img) \n            x[0] = norm(x[0])\n            x[1] = norm(x[1])\n            f = torch.cat((x[0],x[1]), dim=1) #use 512-dim feature\n            f = f.data.cpu()\n            ff = ff+f\n\n        ff[:, 0:512] = norm(ff[:, 0:512])\n        ff[:, 512:1024] = norm(ff[:, 512:1024])\n\n        # norm feature\n        if opt.PCB:\n            # feature size (n,2048,6)\n            # 1. To treat every part equally, I calculate the norm for every 2048-dim part feature.\n            # 2. To keep the cosine score==1, sqrt(6) is added to norm the whole feature (2048*6).\n            fnorm = torch.norm(ff, p=2, dim=1, keepdim=True) * np.sqrt(6) \n            ff = ff.div(fnorm.expand_as(ff))\n            ff = ff.view(ff.size(0), -1)\n\n        features = torch.cat((features,ff), 0)\n    return features\n\ndef get_id(img_path):\n    camera_id = []\n    labels = []\n    for path, v in img_path:\n        #filename = path.split(\'/\')[-1]\n        filename = os.path.basename(path)\n        label = filename[0:4]\n        camera = filename.split(\'c\')[1]\n        if label[0:2]==\'-1\':\n            labels.append(-1)\n        else:\n            labels.append(int(label))\n        camera_id.append(int(camera[0]))\n    return camera_id, labels\n\ngallery_path = image_datasets[\'gallery\'].imgs\nquery_path = image_datasets[\'query\'].imgs\nmquery_path = image_datasets[\'multi-query\'].imgs\n\ngallery_cam,gallery_label = get_id(gallery_path)\nquery_cam,query_label = get_id(query_path)\nmquery_cam,mquery_label = get_id(mquery_path)\n\n######################################################################\n# Load Collected data Trained model\nprint(\'-------test-----------\')\n\n###load config###\nconfig_path = os.path.join(\'../outputs\',name,\'config.yaml\')\nwith open(config_path, \'r\') as stream:\n    config = yaml.load(stream)\n\nmodel_structure = ft_netAB(config[\'ID_class\'], norm=config[\'norm_id\'], stride=config[\'ID_stride\'], pool=config[\'pool\'])\n\nif opt.PCB:\n    model_structure = PCB(config[\'ID_class\'])\n\nmodel = load_network(model_structure)\n\n# Remove the final fc layer and classifier layer\nmodel.model.fc = nn.Sequential()\nmodel.classifier1.classifier = nn.Sequential()\nmodel.classifier2.classifier = nn.Sequential()\n\n# Change to test mode\nmodel = model.eval()\nif use_gpu:\n    model = model.cuda()\n\n# Extract feature\nsince = time.time()\nwith torch.no_grad():\n    gallery_feature = extract_feature(model,dataloaders[\'gallery\'])\n    query_feature = extract_feature(model,dataloaders[\'query\'])\n    time_elapsed = time.time() - since\n    print(\'Extract features complete in {:.0f}m {:.0f}s\'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    if opt.multi:\n        mquery_feature = extract_feature(model,dataloaders[\'multi-query\'])\n    \n# Save to Matlab for check\nresult = {\'gallery_f\':gallery_feature.numpy(),\'gallery_label\':gallery_label,\'gallery_cam\':gallery_cam,\'query_f\':query_feature.numpy(),\'query_label\':query_label,\'query_cam\':query_cam}\nscipy.io.savemat(\'pytorch_result.mat\',result)\nif opt.multi:\n    result = {\'mquery_f\':mquery_feature.numpy(),\'mquery_label\':mquery_label,\'mquery_cam\':mquery_cam}\n    scipy.io.savemat(\'multi_query.mat\',result)\n\nos.system(\'python evaluate_gpu.py\')\n'"
visual_tools/show1by1.py,8,"b'""""""\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\nfrom __future__ import print_function\nimport sys\nsys.path.append(\'.\')\nfrom utils import get_config\nfrom trainer import DGNet_Trainer, to_gray\nimport argparse\nfrom torch.autograd import Variable\nimport sys\nimport torch\nimport os\nimport numpy as np\nfrom torchvision import datasets, transforms\nfrom PIL import Image\n\nname = \'E0.5new_reid0.5_w30000\'\n\nif not os.path.isdir(\'./outputs/%s\'%name):\n    assert 0, ""please change the name to your model name""\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--output_folder\', type=str, default=""./"", help=""output image path"")\nparser.add_argument(\'--input_folder\', type=str, default=""./visual_data/inputs_many_train"", help=""input image path"")\n\nparser.add_argument(\'--config\', type=str, default=\'./outputs/%s/config.yaml\'%name, help=""net configuration"")\nparser.add_argument(\'--checkpoint_gen\', type=str, default=""./outputs/%s/checkpoints/gen_00100000.pt""%name, help=""checkpoint of autoencoders"")\nparser.add_argument(\'--checkpoint_id\', type=str, default=""./outputs/%s/checkpoints/id_00100000.pt""%name, help=""checkpoint of autoencoders"")\nparser.add_argument(\'--batchsize\', default=1, type=int, help=\'batchsize\')\nparser.add_argument(\'--a2b\', type=int, default=1, help=""1 for a2b and others for b2a"")\nparser.add_argument(\'--seed\', type=int, default=10, help=""random seed"")\nparser.add_argument(\'--synchronized\', action=\'store_true\', help=""whether use synchronized style code or not"")\nparser.add_argument(\'--output_only\', action=\'store_true\', help=""whether use synchronized style code or not"")\nparser.add_argument(\'--trainer\', type=str, default=\'DGNet\', help=""DGNet"")\n\n\nopts = parser.parse_args()\n\ntorch.manual_seed(opts.seed)\ntorch.cuda.manual_seed(opts.seed)\nif not os.path.exists(opts.output_folder):\n    os.makedirs(opts.output_folder)\n\n# Load experiment setting\nconfig = get_config(opts.config)\nopts.num_style = 1\n\n# Setup model and data loader\nif opts.trainer == \'DGNet\':\n    trainer = DGNet_Trainer(config)\nelse:\n    sys.exit(""Only support DGNet"")\n\nstate_dict_gen = torch.load(opts.checkpoint_gen)\ntrainer.gen_a.load_state_dict(state_dict_gen[\'a\'], strict=False)\ntrainer.gen_b = trainer.gen_a\n\nstate_dict_id = torch.load(opts.checkpoint_id)\ntrainer.id_a.load_state_dict(state_dict_id[\'a\'])\ntrainer.id_b = trainer.id_a\n\ntrainer.cuda()\ntrainer.eval()\nencode = trainer.gen_a.encode # encode function\nstyle_encode = trainer.gen_a.encode # encode function\nid_encode = trainer.id_a # encode function\ndecode = trainer.gen_a.decode # decode function\n\ndata_transforms = transforms.Compose([\n        transforms.Resize((256,128), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nimage_datasets = datasets.ImageFolder(opts.input_folder, data_transforms)\ndataloader_content = torch.utils.data.DataLoader(image_datasets, batch_size=1, shuffle=False, num_workers=1)\ndataloader_structure = torch.utils.data.DataLoader(image_datasets, batch_size=16, shuffle=False, num_workers=1)\nimage_paths = image_datasets.imgs\n\n######################################################################\n# recover image\n# -----------------\ndef recover(inp):\n    """"""Imshow for Tensor.""""""\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = inp * 255.0\n    inp = np.clip(inp, 0, 255)\n    return inp\n\nsave_path = \'./visual_data/rainbow\'\nif not os.path.isdir(save_path):\n    os.mkdir(save_path)\n\nim = {}\ncount = 0\ndata2 = next(iter(dataloader_structure))\nbg_img, _ = data2\ngray = to_gray(False)\nbg_img = gray(bg_img)\nbg_img = Variable(bg_img.cuda())\nwith torch.no_grad():\n    for data in dataloader_content:\n        id_img, _ = data\n        id_img = Variable(id_img.cuda())\n        n, c, h, w = id_img.size()\n        # Start testing\n        s = encode(bg_img)\n        f, _ = id_encode(id_img)\n        input1 = recover(data[0].squeeze())\n        im[count] = input1\n        for i in range(s.size(0)):\n            s_tmp = s[i,:,:,:]\n            outputs = decode(s_tmp.unsqueeze(0), f)\n            tmp = recover(outputs[0].data.cpu())\n            pic = Image.fromarray(tmp.astype(\'uint8\'))\n            pic.save(\'%s/rainbow_%d_%d.jpg\'%(save_path,i,count))\n        count +=1\n\n\n'"
visual_tools/show_rainbow.py,8,"b'""""""\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\nfrom __future__ import print_function\nimport sys\nsys.path.append(\'.\')\nfrom utils import get_config\nfrom trainer import DGNet_Trainer, to_gray\nimport argparse\nfrom torch.autograd import Variable\nimport sys\nimport torch\nimport os\nimport numpy as np\nfrom torchvision import datasets, transforms\nfrom PIL import Image\n\nname = \'E0.5new_reid0.5_w30000\' \n\nif not os.path.isdir(\'./outputs/%s\'%name):\n    assert 0, ""please change the name to your model name""\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--output_folder\', type=str, default=""./"", help=""output image path"")\nparser.add_argument(\'--input_folder\', type=str, default=""./visual_data/inputs_many_test"", help=""input image path"")\n\nparser.add_argument(\'--config\', type=str, default=\'./outputs/%s/config.yaml\'%name, help=""net configuration"")\nparser.add_argument(\'--checkpoint_gen\', type=str, default=""./outputs/%s/checkpoints/gen_00100000.pt""%name, help=""checkpoint of autoencoders"")\nparser.add_argument(\'--checkpoint_id\', type=str, default=""./outputs/%s/checkpoints/id_00100000.pt""%name, help=""checkpoint of autoencoders"")\nparser.add_argument(\'--batchsize\', default=1, type=int, help=\'batchsize\')\nparser.add_argument(\'--a2b\', type=int, default=1, help=""1 for a2b and others for b2a"")\nparser.add_argument(\'--seed\', type=int, default=10, help=""random seed"")\nparser.add_argument(\'--synchronized\', action=\'store_true\', help=""whether use synchronized style code or not"")\nparser.add_argument(\'--output_only\', action=\'store_true\', help=""whether use synchronized style code or not"")\nparser.add_argument(\'--trainer\', type=str, default=\'DGNet\', help=""DGNet"")\n\n\nopts = parser.parse_args()\n\ntorch.manual_seed(opts.seed)\ntorch.cuda.manual_seed(opts.seed)\nif not os.path.exists(opts.output_folder):\n    os.makedirs(opts.output_folder)\n\n# Load experiment setting\nconfig = get_config(opts.config)\nopts.num_style = 1\n\n# Setup model and data loader\nif opts.trainer == \'DGNet\':\n    trainer = DGNet_Trainer(config)\nelse:\n    sys.exit(""Only support DGNet"")\n\nstate_dict_gen = torch.load(opts.checkpoint_gen)\ntrainer.gen_a.load_state_dict(state_dict_gen[\'a\'], strict=False)\ntrainer.gen_b = trainer.gen_a\n\nstate_dict_id = torch.load(opts.checkpoint_id)\ntrainer.id_a.load_state_dict(state_dict_id[\'a\'])\ntrainer.id_b = trainer.id_a\n\ntrainer.cuda()\ntrainer.eval()\nencode = trainer.gen_a.encode # encode function\nstyle_encode = trainer.gen_a.encode # encode function\nid_encode = trainer.id_a # encode function\ndecode = trainer.gen_a.decode # decode function\n\ndata_transforms = transforms.Compose([\n        transforms.Resize((256,128), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n\nnum = 7\nimage_datasets = datasets.ImageFolder(opts.input_folder, data_transforms)\ndataloader_content = torch.utils.data.DataLoader(image_datasets, batch_size=1, shuffle=False, num_workers=1)\ndataloader_structure = torch.utils.data.DataLoader(image_datasets, batch_size=num, shuffle=False, num_workers=1)\nimage_paths = image_datasets.imgs\n\n######################################################################\n# recover image\n# -----------------\ndef recover(inp):\n    """"""Imshow for Tensor.""""""\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = inp * 255.0\n    inp = np.clip(inp, 0, 255)\n    return inp\n\ndef pad(inp, pad = 3):\n    h = inp.shape[0]\n    w = inp.shape[1]\n    bg = np.zeros((h+2*pad, w+2*pad, inp.shape[2]))\n    bg[pad:pad+h, pad:pad+w, :] = inp\n    return bg\n\nim = {}\nnpad = 3\ncount = 0\ndata2 = next(iter(dataloader_structure))\nbg_img, _ = data2\n\ngray = to_gray(False)\nbg_img = gray(bg_img)\nbg_img = Variable(bg_img.cuda())\nwhite_col = np.ones( (256+2*npad,24,3))*255\nwith torch.no_grad():\n    for data in dataloader_content:\n        id_img, _ = data\n        id_img = Variable(id_img.cuda())\n        n, c, h, w = id_img.size()\n        # Start testing\n        s = encode(bg_img)\n        f, _ = id_encode(id_img)\n        input1 = recover(data[0].squeeze())\n        im[count] = pad(input1, pad= npad)\n        for i in range( s.size(0)):\n            s_tmp = s[i,:,:,:]\n            outputs = decode(s_tmp.unsqueeze(0), f)\n            tmp = recover(outputs[0].data.cpu())\n            tmp = pad(tmp, pad=npad)\n            im[count] = np.concatenate((im[count], white_col, tmp), axis=1)\n        count +=1\n\nfirst_row = np.ones((256+2*npad,128+2*npad,3))*255\nwhite_row = np.ones( (12,im[0].shape[1],3))*255\nfor i in range(num):\n    if i == 0:\n        pic = im[0]\n    else:\n        pic = np.concatenate((pic, im[i]), axis=0)\n    pic = np.concatenate((pic, white_row), axis=0)\n    first_row = np.concatenate((first_row, white_col, im[i][0:256+2*npad, 0:128+2*npad, 0:3]), axis=1)\n\npic = np.concatenate((first_row, white_row, pic), axis=0)\npic = Image.fromarray(pic.astype(\'uint8\'))\npic.save(\'rainbow_%d.jpg\'%num)\n\n'"
visual_tools/show_smooth.py,8,"b'""""""\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\nfrom __future__ import print_function\nimport sys\nsys.path.append(\'.\')\nfrom utils import get_config\nfrom trainer import DGNet_Trainer, to_gray\nimport argparse\nfrom torch.autograd import Variable\nimport sys\nimport torch\nimport os\nimport numpy as np\nimport imageio\nfrom torchvision import datasets, transforms\nfrom PIL import Image\n\nname = \'E0.5new_reid0.5_w30000\'\n\nif not os.path.isdir(\'./outputs/%s\'%name):\n    assert 0, ""please change the name to your model name""\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--output_folder\', type=str, default=""./"", help=""output image path"")\nparser.add_argument(\'--input_folder\', type=str, default=""./visual_data/inputs_two"", help=""input image path"")\n\nparser.add_argument(\'--config\', type=str, default=\'./outputs/%s/config.yaml\'%name, help=""net configuration"")\nparser.add_argument(\'--checkpoint_gen\', type=str, default=""./outputs/%s/checkpoints/gen_00100000.pt""%name, help=""checkpoint of autoencoders"")\nparser.add_argument(\'--checkpoint_id\', type=str, default=""./outputs/%s/checkpoints/id_00100000.pt""%name, help=""checkpoint of autoencoders"")\nparser.add_argument(\'--batchsize\', default=1, type=int, help=\'batchsize\')\nparser.add_argument(\'--a2b\', type=int, default=1, help=""1 for a2b and others for b2a"")\nparser.add_argument(\'--seed\', type=int, default=10, help=""random seed"")\nparser.add_argument(\'--synchronized\', action=\'store_true\', help=""whether use synchronized style code or not"")\nparser.add_argument(\'--output_only\', action=\'store_true\', help=""whether use synchronized style code or not"")\nparser.add_argument(\'--trainer\', type=str, default=\'DGNet\', help=""DGNet"")\n\n\nopts = parser.parse_args()\n\ntorch.manual_seed(opts.seed)\ntorch.cuda.manual_seed(opts.seed)\nif not os.path.exists(opts.output_folder):\n    os.makedirs(opts.output_folder)\n\n# Load experiment setting\nconfig = get_config(opts.config)\nopts.num_style = 1\n\n# Setup model and data loader\nif opts.trainer == \'DGNet\':\n    trainer = DGNet_Trainer(config)\nelse:\n    sys.exit(""Only support DGNet"")\n\nstate_dict_gen = torch.load(opts.checkpoint_gen)\ntrainer.gen_a.load_state_dict(state_dict_gen[\'a\'], strict=False)\ntrainer.gen_b = trainer.gen_a\n\nstate_dict_id = torch.load(opts.checkpoint_id)\ntrainer.id_a.load_state_dict(state_dict_id[\'a\'])\ntrainer.id_b = trainer.id_a\n\ntrainer.cuda()\ntrainer.eval()\nencode = trainer.gen_a.encode # encode function\nstyle_encode = trainer.gen_a.encode # encode function\nid_encode = trainer.id_a # encode function\ndecode = trainer.gen_a.decode # decode function\n\ndata_transforms = transforms.Compose([\n        transforms.Resize((256,128), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nimage_datasets = datasets.ImageFolder(opts.input_folder, data_transforms)\ndataloader_content = torch.utils.data.DataLoader(image_datasets, batch_size=2, shuffle=False, num_workers=1)\ndataloader_structure = torch.utils.data.DataLoader(image_datasets, batch_size=2, shuffle=False, num_workers=1)\nimage_paths = image_datasets.imgs\n\n######################################################################\n# recover image\n# -----------------\ndef recover(inp):\n    """"""Imshow for Tensor.""""""\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = inp * 255.0\n    inp = np.clip(inp, 0, 255)\n    return inp\n\nim = {}\ndata2 = next(iter(dataloader_structure))\nbg_img, _ = data2\ngray = to_gray(False)\nbg_img = gray(bg_img)\nbg_img = Variable(bg_img.cuda())\nff = []\ngif = []\nwith torch.no_grad():\n    for data in dataloader_content:\n        id_img, _ = data\n        id_img = Variable(id_img.cuda())\n        n, c, h, w = id_img.size()\n        # Start testing\n        s = encode(bg_img)\n        f, _ = id_encode(id_img) \n        for count in range(2):\n            input1 = recover(id_img[count].squeeze().data.cpu())\n            im[count] = input1\n            gif.append(input1)\n            for i in range(11):\n                s_tmp = s[count,:,:,:] \n                tmp_f = 0.1*i*f[0] + (1-0.1*i)*f[1]\n                tmp_f = tmp_f.view(1, -1)\n                outputs = decode(s_tmp.unsqueeze(0), tmp_f)\n                tmp = recover(outputs[0].data.cpu())\n                im[count] = np.concatenate((im[count], tmp), axis=1)\n                gif.append(tmp)\n        break\n\n# save long image\npic = np.concatenate( (im[0], im[1]) , axis=0)\npic = Image.fromarray(pic.astype(\'uint8\'))\npic.save(\'smooth.jpg\')\n\n# save gif\nimageio.mimsave(\'./smooth.gif\', gif)\n\n'"
visual_tools/show_smooth_structure.py,8,"b'""""""\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\nfrom __future__ import print_function\nimport sys\nsys.path.append(\'.\')\nfrom utils import get_config\nfrom trainer import DGNet_Trainer, to_gray\nimport argparse\nfrom torch.autograd import Variable\nimport sys\nimport torch\nimport os\nimport numpy as np\nimport imageio\nfrom torchvision import datasets, transforms\nfrom PIL import Image\n\nname = \'E0.5new_reid0.5_w30000\'\n\nif not os.path.isdir(\'./outputs/%s\'%name):\n    assert 0, ""please change the name to your model name""\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--output_folder\', type=str, default=""./"", help=""output image path"")\nparser.add_argument(\'--input_folder\', type=str, default=""./visual_data/inputs_two"", help=""input image path"")\n\nparser.add_argument(\'--config\', type=str, default=\'./outputs/%s/config.yaml\'%name, help=""net configuration"")\nparser.add_argument(\'--checkpoint_gen\', type=str, default=""./outputs/%s/checkpoints/gen_00100000.pt""%name, help=""checkpoint of autoencoders"")\nparser.add_argument(\'--checkpoint_id\', type=str, default=""./outputs/%s/checkpoints/id_00100000.pt""%name, help=""checkpoint of autoencoders"")\nparser.add_argument(\'--batchsize\', default=1, type=int, help=\'batchsize\')\nparser.add_argument(\'--a2b\', type=int, default=1, help=""1 for a2b and others for b2a"")\nparser.add_argument(\'--seed\', type=int, default=10, help=""random seed"")\nparser.add_argument(\'--synchronized\', action=\'store_true\', help=""whether use synchronized style code or not"")\nparser.add_argument(\'--output_only\', action=\'store_true\', help=""whether use synchronized style code or not"")\nparser.add_argument(\'--trainer\', type=str, default=\'DGNet\', help=""DGNet"")\n\n\nopts = parser.parse_args()\n\ntorch.manual_seed(opts.seed)\ntorch.cuda.manual_seed(opts.seed)\nif not os.path.exists(opts.output_folder):\n    os.makedirs(opts.output_folder)\n\n# Load experiment setting\nconfig = get_config(opts.config)\nopts.num_style = 1\n\n# Setup model and data loader\nif opts.trainer == \'DGNet\':\n    trainer = DGNet_Trainer(config)\nelse:\n    sys.exit(""Only support DGNet"")\n\nstate_dict_gen = torch.load(opts.checkpoint_gen)\ntrainer.gen_a.load_state_dict(state_dict_gen[\'a\'], strict=False)\ntrainer.gen_b = trainer.gen_a\n\nstate_dict_id = torch.load(opts.checkpoint_id)\ntrainer.id_a.load_state_dict(state_dict_id[\'a\'])\ntrainer.id_b = trainer.id_a\n\ntrainer.cuda()\ntrainer.eval()\nencode = trainer.gen_a.encode # encode function\nstyle_encode = trainer.gen_a.encode # encode function\nid_encode = trainer.id_a # encode function\ndecode = trainer.gen_a.decode # decode function\n\ndata_transforms = transforms.Compose([\n        transforms.Resize((256,128), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nimage_datasets = datasets.ImageFolder(opts.input_folder, data_transforms)\ndataloader_content = torch.utils.data.DataLoader(image_datasets, batch_size=2, shuffle=False, num_workers=1)\ndataloader_structure = torch.utils.data.DataLoader(image_datasets, batch_size=2, shuffle=False, num_workers=1)\nimage_paths = image_datasets.imgs\n\n######################################################################\n# recover image\n# -----------------\ndef recover(inp):\n    """"""Imshow for Tensor.""""""\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = inp * 255.0\n    inp = np.clip(inp, 0, 255)\n    return inp\n\nim = {}\ndata2 = next(iter(dataloader_structure))\nbg_img, _ = data2\ngray = to_gray(False)\nbg_img = gray(bg_img)\nbg_img = Variable(bg_img.cuda())\nff = []\ngif = []\nwith torch.no_grad():\n    for data in dataloader_content:\n        id_img, _ = data\n        id_img = Variable(id_img.cuda())\n        n, c, h, w = id_img.size()\n        # Start testing\n        s = encode(bg_img)\n        f, _ = id_encode(id_img) \n        for count in range(2):\n            input1 = recover(id_img[count].squeeze().data.cpu())\n            im[count] = input1\n            gif.append(input1)\n            for i in range(11):\n                f_tmp = f[count,:] \n                f_tmp = f_tmp.view(1,-1)\n                tmp_s = 0.1*i*s[0,:,:,:] + (1-0.1*i)*s[1,:,:,:]\n                tmp_s = tmp_s.unsqueeze(0)\n                outputs = decode(tmp_s, f_tmp)\n                tmp = recover(outputs[0].data.cpu())\n                im[count] = np.concatenate((im[count], tmp), axis=1)\n                gif.append(tmp)\n        break\n        \n# save long image\npic = np.concatenate( (im[0], im[1]) , axis=0)\npic = Image.fromarray(pic.astype(\'uint8\'))\npic.save(\'smooth-s.jpg\')\n\n# save gif\nimageio.mimsave(\'./smooth-s.gif\', gif)\n'"
visual_tools/show_swap.py,8,"b'""""""\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\nfrom __future__ import print_function\nimport sys\nsys.path.append(\'.\')\nfrom utils import get_config\nfrom trainer import DGNet_Trainer, to_gray\nimport argparse\nfrom torch.autograd import Variable\nimport sys\nimport torch\nimport os\nimport numpy as np\nfrom torchvision import datasets, transforms\nfrom PIL import Image\ntry:\n    from itertools import izip as zip\nexcept ImportError: # will be 3.x series\n    pass\n\nname = \'E0.5new_reid0.5_w30000\'\n\nif not os.path.isdir(\'./outputs/%s\'%name):\n    assert 0, ""please change the name to your model name""\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--output_folder\', type=str, default=""./"", help=""output image path"")\nparser.add_argument(\'--input_folder\', type=str, default=""./visual_data/inputs_many_test_market"", help=""input image path"")\n\nparser.add_argument(\'--config\', type=str, default=\'./outputs/%s/config.yaml\'%name, help=""net configuration"")\nparser.add_argument(\'--checkpoint_gen\', type=str, default=""./outputs/%s/checkpoints/gen_00100000.pt""%name, help=""checkpoint of autoencoders"")\nparser.add_argument(\'--checkpoint_id\', type=str, default=""./outputs/%s/checkpoints/id_00100000.pt""%name, help=""checkpoint of autoencoders"")\nparser.add_argument(\'--batchsize\', default=1, type=int, help=\'batchsize\')\nparser.add_argument(\'--a2b\', type=int, default=1, help=""1 for a2b and others for b2a"")\nparser.add_argument(\'--seed\', type=int, default=10, help=""random seed"")\nparser.add_argument(\'--synchronized\', action=\'store_true\', help=""whether use synchronized style code or not"")\nparser.add_argument(\'--output_only\', action=\'store_true\', help=""whether use synchronized style code or not"")\nparser.add_argument(\'--trainer\', type=str, default=\'DGNet\', help=""DGNet"")\n\n\nopts = parser.parse_args()\n\ntorch.manual_seed(opts.seed)\ntorch.cuda.manual_seed(opts.seed)\nif not os.path.exists(opts.output_folder):\n    os.makedirs(opts.output_folder)\n\n# Load experiment setting\nconfig = get_config(opts.config)\nopts.num_style = 1\n\n# Setup model and data loader\nif opts.trainer == \'DGNet\':\n    trainer = DGNet_Trainer(config)\nelse:\n    sys.exit(""Only support DGNet"")\n\nstate_dict_gen = torch.load(opts.checkpoint_gen)\ntrainer.gen_a.load_state_dict(state_dict_gen[\'a\'], strict=False)\ntrainer.gen_b = trainer.gen_a\n\nstate_dict_id = torch.load(opts.checkpoint_id)\ntrainer.id_a.load_state_dict(state_dict_id[\'a\'], strict=False)\ntrainer.id_b = trainer.id_a\n\ntrainer.cuda()\ntrainer.eval()\nencode = trainer.gen_a.encode # encode function\nstyle_encode = trainer.gen_a.encode # encode function\nid_encode = trainer.id_a # encode function\ndecode = trainer.gen_a.decode # decode function\n\ndata_transforms = transforms.Compose([\n        transforms.Resize((256,128), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nimage_datasets1 = datasets.ImageFolder(opts.input_folder+\'/1\', data_transforms)\nimage_datasets2 = datasets.ImageFolder(opts.input_folder+\'/2\', data_transforms)\ndataloader_content = torch.utils.data.DataLoader(image_datasets1, batch_size=1, shuffle=False, num_workers=1)\ndataloader_structure = torch.utils.data.DataLoader(image_datasets2, batch_size=1, shuffle=False, num_workers=1)\n\n######################################################################\n# recover image\n# -----------------\ndef recover(inp):\n    """"""Imshow for Tensor.""""""\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = inp * 255.0\n    inp = np.clip(inp, 0, 255)\n    return inp\n\ndef pad(inp, pad = 3):\n    h = inp.shape[0]\n    w = inp.shape[1]\n    bg = np.zeros((h+2*pad, w+2*pad, inp.shape[2]))\n    bg[pad:pad+h, pad:pad+w, :] = inp\n    return bg\n\nim = {}\nnpad = 3\ncount = 0\ngray = to_gray(False)\n\ndef generate(data, data2):\n    bg_img, _ = data2\n    bg_img = gray(bg_img)\n    bg_img = Variable(bg_img.cuda())\n    id_img, _ = data\n    id_img = Variable(id_img.cuda())\n    # Start testing\n    s = encode(bg_img)\n    f, _ = id_encode(id_img)\n    output = decode(s, f)\n    return output.squeeze().data.cpu()\n\nw = np.ones( (16, 128+2*npad,3))*255 #white row\nw2 = np.ones( (32, 128+2*npad,3))*255 #white row\nwith torch.no_grad():\n    for data, data2 in zip(dataloader_content, dataloader_structure): \n        im1 = pad(recover(data[0].squeeze()), pad= npad)\n        im2 = pad(recover(data2[0].squeeze()), pad= npad)\n        output1 = pad(recover(generate(data, data2)), pad= npad)\n        output2 = pad(recover(generate(data2, data)), pad= npad)\n        im[count] = np.concatenate((im1, w,  im2, w2, output1, w, output2), axis=0)\n        count +=1\n        print(count)\n\nwhite_col = np.ones( (im[0].shape[0], 16, 3))*255\n\nfor i in range(count):\n    if i == 0:\n        pic = im[0]\n    else:\n        pic = np.concatenate((pic, im[i]), axis=1)\n    pic = np.concatenate((pic, white_col), axis=1)\n\npic = Image.fromarray(pic.astype(\'uint8\'))\npic.save(\'swap.jpg\')\n\n'"
visual_tools/test_folder.py,8,"b'""""""\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\nfrom __future__ import print_function\nimport sys\nsys.path.append(\'.\')\nfrom trainer import DGNet_Trainer, to_gray\nfrom utils import get_config\nimport argparse\nfrom torch.autograd import Variable\nimport torchvision.utils as vutils\nimport sys\nimport torch\nimport random\nimport os\nimport numpy as np\nfrom torchvision import datasets, models, transforms\nfrom PIL import Image\nfrom shutil import copyfile\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--output_folder\', type=str, default=""../Market/pytorch/off-gan_id1/"", help=""output image path"")\nparser.add_argument(\'--output_folder2\', type=str, default=""../Market/pytorch/off-gan_bg1/"", help=""output image path"")\nparser.add_argument(\'--input_folder\', type=str, default=""../Market/pytorch/train_all/"", help=""input image path"")\n\nparser.add_argument(\'--name\', type=str, default=""E0.5new_reid0.5_w30000"", help=""model name"")\nparser.add_argument(\'--which_epoch\', default=100000, type=int, help=\'iteration\')\n\nparser.add_argument(\'--batchsize\', default=1, type=int, help=\'batchsize\')\nparser.add_argument(\'--a2b\', type=int, default=1, help=""1 for a2b and others for b2a"")\nparser.add_argument(\'--seed\', type=int, default=10, help=""random seed"")\nparser.add_argument(\'--synchronized\', action=\'store_true\', help=""whether use synchronized style code or not"")\nparser.add_argument(\'--output_only\', action=\'store_true\', help=""whether use synchronized style code or not"")\nparser.add_argument(\'--trainer\', type=str, default=\'DGNet\', help=""DGNet"")\n\n\nopts = parser.parse_args()\nopts.checkpoint_gen = ""./outputs/%s/checkpoints/gen_00%06d.pt""%(opts.name, opts.which_epoch)\nopts.checkpoint_id = ""./outputs/%s/checkpoints/id_00%06d.pt""%(opts.name, opts.which_epoch)\nopts.config = \'./outputs/%s/config.yaml\'%opts.name\n\nif not os.path.exists(opts.output_folder):\n    os.makedirs(opts.output_folder)\nelse:\n    os.system(\'rm -rf %s/*\'%opts.output_folder)\n\nif not os.path.exists(opts.output_folder2):\n    os.makedirs(opts.output_folder2)\nelse:\n    os.system(\'rm -rf %s/*\'%opts.output_folder2)\n\n# Load experiment setting\nconfig = get_config(opts.config)\n# we use config\nconfig[\'apex\'] = False\nopts.num_style = 1\n\n# Setup model and data loader\nif opts.trainer == \'DGNet\':\n    trainer = DGNet_Trainer(config)\nelse:\n    sys.exit(""Only support DGNet"")\n\nstate_dict_gen = torch.load(opts.checkpoint_gen)\ntrainer.gen_a.load_state_dict(state_dict_gen[\'a\'], strict=False)\ntrainer.gen_b = trainer.gen_a\n\nstate_dict_id = torch.load(opts.checkpoint_id)\ntrainer.id_a.load_state_dict(state_dict_id[\'a\'])\ntrainer.id_b = trainer.id_a\n\ntrainer.cuda()\ntrainer.eval()\nencode = trainer.gen_a.encode # encode function\nstyle_encode = trainer.gen_a.encode # encode function\nid_encode = trainer.id_a # encode function\ndecode = trainer.gen_a.decode # decode function\n\ndata_transforms = transforms.Compose([\n        transforms.Resize(( config[\'crop_image_height\'], config[\'crop_image_width\']), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nimage_datasets = datasets.ImageFolder(opts.input_folder, data_transforms)\ndataloader_content = torch.utils.data.DataLoader(image_datasets, batch_size=opts.batchsize, shuffle=False, pin_memory=True, num_workers=1)\ndataloader_structure = torch.utils.data.DataLoader(image_datasets, batch_size=opts.batchsize, shuffle=True, pin_memory=True, num_workers=1)\nimage_paths = image_datasets.imgs\n\n######################################################################\n# recover image\n# -----------------\ndef recover(inp):\n    """"""Imshow for Tensor.""""""\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = inp * 255.0\n    inp = np.clip(inp, 0, 255)\n    return inp\n\ndef fliplr(img):\n    \'\'\'flip horizontal\'\'\'\n    inv_idx = torch.arange(img.size(3)-1,-1,-1).long()  # N x C x H x W\n    img_flip = img.index_select(3,inv_idx)\n    return img_flip\n\n###########################################\n# ID with different background (run 10 times)\n#------------------------------------------\n\ngray = to_gray(False)\n\ntorch.manual_seed(opts.seed)\n\nwith torch.no_grad():\n    for i in range(1):\n        for data, data2, path in zip(dataloader_content, dataloader_structure, image_paths):\n            name = os.path.basename(path[0])\n            id_img, label = data\n            id_img_flip = Variable(fliplr(id_img).cuda())\n            id_img = Variable(id_img.cuda())\n            bg_img, label2 = data2\n            if config[\'single\'] == \'gray\':\n                bg_img = gray(bg_img)\n            bg_img = Variable(bg_img.cuda())\n\n            n, c, h, w = id_img.size()\n            # Start testing\n            c = encode(bg_img)\n            f, _ = id_encode(id_img)\n\n            if opts.trainer == \'DGNet\':\n                outputs = decode(c, f)\n                im = recover(outputs[0].data.cpu())\n                im = Image.fromarray(im.astype(\'uint8\'))\n                ID = name.split(\'_\')\n                dst_path = opts.output_folder + \'/%03d\'%label\n                dst_path2 = opts.output_folder2 + \'/%03d\'%label2\n                if not os.path.isdir(dst_path):\n                    os.mkdir(dst_path)\n                if not os.path.isdir(dst_path2):\n                    os.mkdir(dst_path2)\n                im.save(dst_path + \'/%03d_%03d_gan%s.jpg\'%(label2, label, name[:-4]))\n                im.save(dst_path2 + \'/%03d_%03d_gan%s.jpg\'%(label2, label, name[:-4]))\n            else:\n                pass\nprint(\'---- start fid evaluation ------\')\nos.system(\'cd ../TTUR; python fid.py ../Market/pytorch/train_all ../Market/pytorch/off-gan_id1 --gpu 0\')\n\n'"
