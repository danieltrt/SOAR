file_path,api_count,code
builder.py,2,"b'import torch\nimport models\nimport losses\nimport metrics\nimport dataloaders as dataloaders\n\n\nclass Builder(object):\n\t""""""docstring for Builder""""""\n\tdef __init__(self, opts):\n\t\tsuper(Builder, self).__init__()\n\t\tself.opts = opts\n\t\tif opts.loadModel is not None:\n\t\t\tself.states = torch.load(opts.loadModel)\n\t\telse:\n\t\t\tself.states = None\n\tdef Model(self):\n\t\tModelBuilder = getattr(models, self.opts.model)\n\t\tif self.opts.model == \'StackedHourGlass\':\n\t\t\tModel = ModelBuilder(self.opts.nChannels, self.opts.nStack, self.opts.nModules, self.opts.nReductions, self.opts.nJoints)\n\t\telif self.opts.model == \'ChainedPredictions\':\n\t\t\tModel = ModelBuilder(self.opts.modelName, self.opts.hhKernel, self.opts.ohKernel, self.opts.nJoints)\n\t\telif self.opts.model == \'DeepPose\':\n\t\t\tModel = ModelBuilder(self.opts.nJoints, self.opts.baseName)\n\t\telif self.opts.model == \'PyraNet\':\n\t\t\tModel = ModelBuilder(self.opts.nChannels, self.opts.nStack, self.opts.nModules, self.opts.nReductions, self.opts.baseWidth, self.opts.cardinality, self.opts.nJoints, self.opts.inputRes)\n\t\telif self.opts.model == \'PoseAttention\':\n\t\t\tModel = ModelBuilder(self.opts.nChannels, self.opts.nStack, self.opts.nModules, self.opts.nReductions, self.opts.nJoints, self.LRNSize, self.opts.IterSize)\n\t\telse:\n\t\t\tassert(\'Not Implemented Yet!!!\')\n\t\tif self.states is not None:\n\t\t\tModel.load_state_dict(self.states[\'model_state\'])\n\t\treturn Model\n\n\tdef Loss(self):\n\t\tinstance = losses.Loss(self.opts)\n\t\treturn getattr(instance, self.opts.model)\n\n\tdef Metric(self):\n\t\tPCKhinstance = metrics.PCKh(self.opts)\n\t\tPCKinstance = metrics.PCK(self.opts)\n\t\tif self.opts.dataset==\'MPII\':\n\t\t\treturn {\'PCK\' : getattr(PCKinstance, self.opts.model), \'PCKh\' : getattr(PCKhinstance, self.opts.model)}         \n\t\tif self.opts.dataset==\'COCO\':\n\t\t\treturn {\'PCK\' : getattr(PCKinstance, self.opts.model)}\n\t\t\t\n\tdef Optimizer(self, Model):\n\t\tTrainableParams = filter(lambda p: p.requires_grad, Model.parameters())\n\t\tOptimizer = getattr(torch.optim, self.opts.optimizer_type)(TrainableParams, lr = self.opts.LR, alpha = 0.99, eps = 1e-8)\n\t\tif self.states is not None and self.opts.loadOptim:\n\t\t\tOptimizer.load_state_dict(states[\'optimizer_state\'])\n\t\t\tif self.opts.dropPreLoaded:\n\t\t\t\tfor i,_ in enumarate(Optimizer.param_groups):\n\t\t\t\t\tOptimizer.param_groups[i][\'lr\'] /= opts.dropMagPreLoaded\n\t\treturn Optimizer\n\n\tdef DataLoaders(self):\n\t\treturn dataloaders.ImageLoader(self.opts, \'train\'), dataloaders.ImageLoader(self.opts, \'val\')\n\n\tdef Epoch(self):\n\t\tEpoch = 1\n\t\tif self.states is not None and self.opts.loadEpoch:\n\t\t\tEpoch = states[\'epoch\']\n\t\treturn Epoch\n'"
dataloaders.py,1,"b""import datasets\nfrom torch.utils.data import DataLoader\n\ndef ImageLoader(opts, split):\n\treturn DataLoader(\n\t\t\tdataset = getattr(datasets, opts.dataset)(opts, split),\n\t\t\tbatch_size = opts.data_loader_size,\n\t\t\tshuffle = opts.shuffle if split=='train' else False,\n\t\t\tpin_memory = not(opts.dont_pin_memory),\n\t\t\tnum_workers = opts.nThreads\n\t)\n"""
img.py,2,"b""import cv2\nimport ref\nimport torch\nimport numpy as np\nimport torch\nfrom math import *\n\nsigma_inp = ref.hmGaussInp\nn = sigma_inp * 6 + 1\ng_inp = np.zeros((n, n))\nfor i in range(n):\n\t\tfor j in range(n):\n\t\t\t\tg_inp[i, j] = np.exp(-((i - n / 2) ** 2 + (j - n / 2) ** 2) / (2. * sigma_inp * sigma_inp))\n\ndef GetTransform(center, scale, rot, res):\n\th = scale\n\tt = np.eye(3)\n\n\tt[0, 0] = res / h\n\tt[1, 1] = res / h\n\tt[0, 2] = res * (- center[0] / h + 0.5)\n\tt[1, 2] = res * (- center[1] / h + 0.5)\n\n\tif rot != 0:\n\t\trot = -rot\n\t\tr = np.eye(3)\n\t\tang = rot * np.math.pi / 180\n\t\ts = np.math.sin(ang)\n\t\tc = np.math.cos(ang)\n\t\tr[0, 0] = c\n\t\tr[0, 1] = - s\n\t\tr[1, 0] = s\n\t\tr[1, 1] = c\n\t\tt_ = np.eye(3)\n\t\tt_[0, 2] = - res / 2\n\t\tt_[1, 2] = - res / 2\n\t\tt_inv = np.eye(3)\n\t\tt_inv[0, 2] = res / 2\n\t\tt_inv[1, 2] = res / 2\n\t\tt = np.dot(np.dot(np.dot(t_inv,  r), t_), t)\n\n\treturn t\n\n\ndef Transform(pt, center, scale, rot, res, invert = False):\n\tpt_ = np.ones(3)\n\tpt_[0], pt_[1] = pt[0], pt[1]\n\n\tt = GetTransform(center, scale, rot, res)\n\tif invert:\n\t\tt = np.linalg.inv(t)\n\tnew_point = np.dot(t, pt_)[:2]\n\tnew_point = new_point.astype(np.int32)\n\treturn new_point\n\n\ndef getTransform3D(center, scale, rot, res):\n\th = 1.0 * scale\n\tt = np.eye(4)\n\n\tt[0][0] = res / h\n\tt[1][1] = res / h\n\tt[2][2] = res / h\n\n\tt[0][3] = res * (- center[0] / h + 0.5)\n\tt[1][3] = res * (- center[1] / h + 0.5)\n\n\tif rot != 0:\n\t\traise Exception('Not Implement')\n\n\treturn t\n\n\ndef Transform3D(pt, center, scale, rot, res, invert = False):\n\tpt_ = np.ones(4)\n\tpt_[0], pt_[1], pt_[2] = pt[0], pt[1], pt[2]\n\tt = getTransform3D(center, scale, rot, res)\n\tif invert:\n\t\tt = np.linalg.inv(t)\n\tnew_point = np.dot(t, pt_)[:3]\n\treturn new_point\n\n\ndef Crop(img, center, scale, rot, res):\n\tht, wd = img.shape[0], img.shape[1]\n\ttmpImg, newImg = img.copy(), np.zeros((res, res, 3), dtype = np.uint8)\n\n\tscaleFactor = scale / res\n\tif scaleFactor < 2:\n\t\tscaleFactor = 1\n\telse:\n\t\tnewSize = int(np.math.floor(max(ht, wd) / scaleFactor))\n\t\tnewSize_ht = int(np.math.floor(ht / scaleFactor))\n\t\tnewSize_wd = int(np.math.floor(wd / scaleFactor))\n\t\tif newSize < 2:\n\t\t\treturn torch.from_numpy(newImg.transpose(2, 0, 1).astype(np.float32) / 256.)\n\t\telse:\n\t\t\ttmpImg = cv2.resize(tmpImg, (newSize_wd, newSize_ht)) #TODO\n\t\t\tht, wd = tmpImg.shape[0], tmpImg.shape[1]\n\n\tc, s = 1.0 * center / scaleFactor, scale / scaleFactor\n\tc[0], c[1] = c[1], c[0]\n\tul = Transform((0, 0), c, s, 0, res, invert = True)\n\tbr = Transform((res, res), c, s, 0, res, invert = True)\n\n\tif scaleFactor >= 2:\n\t\tbr = br - (br - ul - res)\n\n\tpad = int(np.math.ceil((((ul - br) ** 2).sum() ** 0.5) / 2 - (br[0] - ul[0]) / 2))\n\tif rot != 0:\n\t\tul = ul - pad\n\t\tbr = br + pad\n\n\told_ = [max(0, ul[0]),   min(br[0], ht),         max(0, ul[1]),   min(br[1], wd)]\n\tnew_ = [max(0, - ul[0]), min(br[0], ht) - ul[0], max(0, - ul[1]), min(br[1], wd) - ul[1]]\n\n\tnewImg = np.zeros((br[0] - ul[0], br[1] - ul[1], 3), dtype = np.uint8)\n\t#print 'new old newshape tmpshape center', new_[0], new_[1], old_[0], old_[1], newImg.shape, tmpImg.shape, center\n\ttry:\n\t\tnewImg[new_[0]:new_[1], new_[2]:new_[3], :] = tmpImg[old_[0]:old_[1], old_[2]:old_[3], :]\n\texcept:\n\t\t#print 'ERROR: new old newshape tmpshape center', new_[0], new_[1], old_[0], old_[1], newImg.shape, tmpImg.shape, center\n\t\treturn np.zeros((3, res, res), np.uint8)\n\tif rot != 0:\n\t\tM = cv2.getRotationMatrix2D((newImg.shape[0] / 2, newImg.shape[1] / 2), rot, 1)\n\t\tnewImg = cv2.warpAffine(newImg, M, (newImg.shape[0], newImg.shape[1]))\n\t\tnewImg = newImg[pad+1:-pad+1, pad+1:-pad+1, :].copy()\n\n\tif scaleFactor < 2:\n\t\tnewImg = cv2.resize(newImg, (res, res))\n\n\treturn newImg.transpose(2, 0, 1).astype(np.float32)\n\ndef Gaussian(sigma):\n\tif sigma == 7:\n\t\treturn np.array([0.0529,  0.1197,  0.1954,  0.2301,  0.1954,  0.1197,  0.0529,\n\t\t\t\t\t\t 0.1197,  0.2709,  0.4421,  0.5205,  0.4421,  0.2709,  0.1197,\n\t\t\t\t\t\t 0.1954,  0.4421,  0.7214,  0.8494,  0.7214,  0.4421,  0.1954,\n\t\t\t\t\t\t 0.2301,  0.5205,  0.8494,  1.0000,  0.8494,  0.5205,  0.2301,\n\t\t\t\t\t\t 0.1954,  0.4421,  0.7214,  0.8494,  0.7214,  0.4421,  0.1954,\n\t\t\t\t\t\t 0.1197,  0.2709,  0.4421,  0.5205,  0.4421,  0.2709,  0.1197,\n\t\t\t\t\t\t 0.0529,  0.1197,  0.1954,  0.2301,  0.1954,  0.1197,  0.0529]).reshape(7, 7)\n\telif sigma == n:\n\t\treturn g_inp\n\telse:\n\t\traise Exception('Gaussian {} Not Implement'.format(sigma))\n\ndef DrawGaussian(img, pt, sigma, truesigma=-1):\n\timg = img.copy()\n\ttmpSize = int(np.math.ceil(3 * sigma))\n\tul = [int(np.math.floor(pt[0] - tmpSize)), int(np.math.floor(pt[1] - tmpSize))]\n\tbr = [int(np.math.floor(pt[0] + tmpSize)), int(np.math.floor(pt[1] + tmpSize))]\n\n\tif ul[0] > img.shape[1] or ul[1] > img.shape[0] or br[0] < 1 or br[1] < 1:\n\t\treturn img\n\n\tsize = 2 * tmpSize + 1\n\tg = Gaussian(size)\n\tif truesigma==0.5:\n\t\tg[0,:] *= 0\n\t\tg[-1,:] *= 0\n\t\tg[:,0] *= 0\n\t\tg[:,-1] *= 0\n\t\tg *= 1.5\n\tg_x = [max(0, -ul[0]), min(br[0], img.shape[1]) - max(0, ul[0]) + max(0, -ul[0])]\n\tg_y = [max(0, -ul[1]), min(br[1], img.shape[0]) - max(0, ul[1]) + max(0, -ul[1])]\n\n\timg_x = [max(0, ul[0]), min(br[0], img.shape[1])]\n\timg_y = [max(0, ul[1]), min(br[1], img.shape[0])]\n\n\timg[img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n\treturn img\n\ndef myDrawGaussian(img, pt, sigmaredundant):\n\tpt[0] = floor(pt[0])\n\tpt[1] = floor(pt[1])\n\tif (pt[0] < 1 or pt[1] < 1 or pt[0] > img.shape[0] or pt[1] > img.shape[1]):\n\t\treturn img\n\timg[max(pt[0]-3, 0):min(pt[0]+3, img.shape[0]), max(pt[1]-0, 0):min(pt[1]+0, img.shape[1])] = 1\n\timg[max(pt[0]-2, 0):min(pt[0]+2, img.shape[0]), max(pt[1]-1, 0):min(pt[1]+1, img.shape[1])] = 1\n\timg[max(pt[0]-2, 0):min(pt[0]+2, img.shape[0]), max(pt[1]-2, 0):min(pt[1]+2, img.shape[1])] = 1\n\timg[max(pt[0]-0, 0):min(pt[0]+0, img.shape[0]), max(pt[1]-3, 0):min(pt[1]+3, img.shape[1])] = 1\n\tif (img.sum()==0):\n\t\tprint(pt)\n\t\tassert(img.sum() != 0)\n\timg = img / img.sum()\n\treturn img\n\ndef Rnd(x):\n\twow = torch.randn(1)\n\treturn max(-2 * x, min(2 * x, float(wow) * x))\n\ndef Flip(img):\n\treturn img[:, :, ::-1].copy()\n\ndef ShuffleLR(x):\n\tfor e in ref.shuffleRef:\n\t\tx[e[0]], x[e[1]] = x[e[1]].copy(), x[e[0]].copy()\n\treturn x\n"""
losses.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Loss(object):\n        """"""docstring for Loss""""""\n        def __init__(self, opts):\n                super(Loss, self).__init__()\n                self.opts = opts\n\n        def StackedHourGlass(self, output, target, meta=None):\n                meta = 1 if self.opts.dataset == \'MPII\' else meta\n                loss = 0\n                for i in range(self.opts.nStack):\n                        loss += F.mse_loss(output[i]*meta, target*meta)\n                return loss\n\n        def PoseAttention(self, output, target, meta=None):\n                meta = 1 if self.opts.dataset == \'MPII\' else meta\n                loss = 0\n                for i in range(self.opts.nStack):\n                        loss += F.mse_loss(output[i]*meta, target*meta)\n                return loss\n\n        def PyraNet(self, output, target, meta=None):\n                meta = 1 if self.opts.dataset == \'MPII\' else meta\n                loss = 0\n                for i in range(self.opts.nStack):\n                        loss += F.mse_loss(output[i]*meta, target*meta)\n                return loss\n\n        def ChainedPredictions(self, output, target, meta=None):\n                meta = 1 if self.opts.dataset == \'MPII\' else meta\n                return F.mse_loss(output*meta, target*meta)\n\n        def DeepPose(self, output, target, meta=None):\n                meta = (target > -0.5 + 1e-8).float().reshape(-1, self.opts.nJoints, 2) if self.opts.dataset == \'MPII\' else meta[:,:,:,0]\n                return F.mse_loss(output.reshape(-1, self.opts.nJoints, 2)*meta, target.reshape(-1, self.opts.nJoints, 2)*meta)\n'"
main.py,1,"b""import torch\nimport builder\nimport trainer\n\nimport os\nimport time\nimport argparse\nfrom opts import opts\n\n\nopts = opts().parse()\ntorch.set_default_tensor_type('torch.DoubleTensor' if opts.usedouble else 'torch.FloatTensor')\nBuilder = builder.Builder(opts)\n\nModel = Builder.Model()\nOptimizer = Builder.Optimizer(Model)\nLoss = Builder.Loss()\nMetrics = Builder.Metric()\nTrainDataLoader, ValDataLoader = Builder.DataLoaders()\nEpoch = Builder.Epoch()\n\n\nModel = Model.to(opts.gpuid)\n\n# opts.saveDir = os.path.join(opts.saveDir, os.path.join(opts.model, 'logs_{}'.format(datetime.datetime.now().isoformat())))\nFile = os.path.join(opts.saveDir, 'log.txt')\n\nTrainer = trainer.Trainer(Model, Optimizer, Loss, Metrics, File, None, opts)\n\nif opts.test:\n\tTrainer.test(ValDataLoader)\n\texit()\n\nTrainer.train(TrainDataLoader, ValDataLoader, Epoch, opts.nEpochs)\n"""
metrics.py,0,"b'import torch\nimport numpy as np\neps = 1e-8\n\nclass PCKh(object):\n    """"""docstring for PCKh""""""\n    def __init__(self, opts):\n        super(PCKh, self).__init__()\n        self.opts = opts\n        self.LB = -0.5 + eps if self.opts.TargetType == \'direct\' else 0 + eps\n\n    def getPreds(self, hm):\n        assert len(hm.shape) == 4, \'Input must be a 4-D tensor\'\n        res = hm.shape[2]\n        hm = hm.reshape(hm.shape[0], hm.shape[1], hm.shape[2] * hm.shape[3])\n        idx = np.argmax(hm, axis = 2)\n        preds = np.zeros((hm.shape[0], hm.shape[1], 2))\n        preds[:, :, 0], preds[:, :, 1] = idx % res, idx / res\n        return preds\n\n    def eval(self, predictions, target, meta1, meta2, alpha=0.5):\n        batchSize = predictions.shape[0]\n        numJoints = 0\n        numCorrect = 0\n        for i in range(batchSize):\n            index1 = 0\n            index2 = 0\n            skip = 0\n            while (np.isnan(meta1[i,index1,:]).any() or (target[i,index1,:]<= self.LB).any()):\n                index1+=1\n                if index1>=15:\n                    skip = 1\n                    break\n            if skip:\n                continue\n            index2 = index1 + 1\n            while (np.isnan(meta1[i,index2,:]).any() or (meta1[i,index2, :]==meta1[i,index1,:]).all() or (target[i,index2,:]<= self.LB).any() or (target[i,index2, :]==target[i,index1,:]).all()):\n                index2+=1\n                if index2>=16:\n                    skip = 1\n                    break\n            if skip:\n                continue\n\n            # Found 2 non-nan indices\n\n            loaderDist = np.linalg.norm(target[i, index1, :] - target[i, index2, :])\n            globalDist = np.linalg.norm(meta1[i, index1, :] - meta1[i, index2, :])\n            effectiveHeadSize = meta2[i, 0] * (loaderDist/globalDist)\n\n            for j in range(16):\n                    if j==7 or j==6:\n                        continue\n                    if target[i, j, 0] >= self.LB and target[i, j, 1] >= self.LB and not(np.isnan(meta1[i, j, :]).any()):\n                            numJoints += 1\n                            if np.linalg.norm(predictions[i, j, :] - target[i, j, :]) <= alpha * effectiveHeadSize:\n                                    numCorrect += 1\n        if numJoints == 0:\n            return 1, 0\n        return float(numCorrect)/float(numJoints), numJoints\n\n    def StackedHourGlass(self, output, target, meta1, meta2, alpha=0.5):\n        predictions = self.getPreds(output[self.opts.nStack-1].detach().cpu().numpy())\n        target = self.getPreds(target.cpu().numpy())\n        return self.eval(predictions, target, meta1.cpu().numpy(), meta2.cpu().numpy(), alpha)\n\n    def PoseAttention(self, output, target, meta1, meta2, alpha=0.5):\n        predictions = self.getPreds(output[self.opts.nStack-1].detach().cpu().numpy())\n        target = self.getPreds(target.cpu().numpy())\n        return self.eval(predictions, target, meta1.cpu().numpy(), meta2.cpu().numpy(), alpha)\n\n    def PyraNet(self, output, target, meta1, meta2, alpha=0.5):\n        predictions = self.getPreds(output[self.opts.nStack-1].detach().cpu().numpy())\n        target = self.getPreds(target.cpu().numpy())\n        return self.eval(predictions, target, meta1.cpu().numpy(), meta2.cpu().numpy(), alpha)\n\n    def ChainedPredictions(self, output, target, meta1, meta2, alpha=0.5):\n        predictions = self.getPreds(output.detach().cpu().numpy())\n        target = self.getPreds(target.cpu().numpy())\n        return self.eval(predictions, target, meta1.cpu().numpy(), meta2.cpu().numpy(), alpha)\n\n    def DeepPose(self, output, target, meta1, meta2, alpha=0.5):\n        predictions = (output).reshape(-1,16,2).detach().cpu().numpy()\n        target = (target).reshape(-1,16,2).cpu().numpy()\n        return self.eval(predictions, target, meta1.cpu().numpy(), meta2.cpu().numpy(), alpha)\n\n\n\n\n\nclass PCK(object):\n    """"""docstring for PCK""""""\n    def __init__(self, opts):\n        super(PCK, self).__init__()\n        self.opts = opts\n        self.LB = -0.5 + eps if self.opts.TargetType == \'direct\' else 0 + eps\n\n    def calc_dists(self, preds, target, normalize):\n        preds = preds.astype(np.float32)\n        target = target.astype(np.float32)\n        dists = np.zeros((preds.shape[1], preds.shape[0]))\n        for n in range(preds.shape[0]):\n            for c in range(preds.shape[1]):\n                if target[n, c, 0] > 1 and target[n, c, 1] > 1:\n                    normed_preds = preds[n, c, :] / normalize[n]\n                    normed_targets = target[n, c, :] / normalize[n]\n                    dists[c, n] = np.linalg.norm(normed_preds - normed_targets)\n                else:\n                    dists[c, n] = -1\n        return dists\n\n    def dist_acc(self, dists, thr=0.5):\n         \'\'\' Return percentage below threshold while ignoring values with a -1 \'\'\'\n         dist_cal = np.not_equal(dists, -1)\n         num_dist_cal = dist_cal.sum()\n         if num_dist_cal > 0:\n             return np.less(dists[dist_cal], thr).sum() * 1.0 / num_dist_cal\n         else:\n             return -1\n\n    def get_max_preds(self, batch_heatmaps):\n        \'\'\'\n        get predictions from score maps\n        heatmaps: numpy.ndarray([batch_size, num_joints, height, width])\n        \'\'\'\n        assert isinstance(batch_heatmaps, np.ndarray), \'batch_heatmaps should be numpy.ndarray\'\n        assert batch_heatmaps.ndim == 4, \'batch_images should be 4-ndim\'\n\n        batch_size = batch_heatmaps.shape[0]\n        num_joints = batch_heatmaps.shape[1]\n        width = batch_heatmaps.shape[3]\n        heatmaps_reshaped = batch_heatmaps.reshape((batch_size, num_joints, -1))\n        idx = np.argmax(heatmaps_reshaped, 2)\n        maxvals = np.amax(heatmaps_reshaped, 2)\n\n        maxvals = maxvals.reshape((batch_size, num_joints, 1))\n        idx = idx.reshape((batch_size, num_joints, 1))\n\n        preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\n\n        preds[:, :, 0] = (preds[:, :, 0]) % width\n        preds[:, :, 1] = np.floor((preds[:, :, 1]) / width)\n\n        pred_mask = np.tile(np.greater(maxvals, 0.0), (1, 1, 2))\n        pred_mask = pred_mask.astype(np.float32)\n\n        preds *= pred_mask\n        return preds, maxvals\n\n    def eval(self, pred, target, alpha=0.5):\n        \'\'\'\n        Calculate accuracy according to PCK,\n        but uses ground truth heatmap rather than x,y locations\n        First value to be returned is average accuracy across \'idxs\',\n        followed by individual accuracies\n        \'\'\'\n        idx = list(range(16))\n        norm = 1.0\n        if True:\n         h = self.opts.outputRes\n         w = self.opts.outputRes\n         norm = np.ones((pred.shape[0], 2)) * np.array([h, w]) / 10\n        dists = self.calc_dists(pred, target, norm)\n\n        acc = np.zeros((len(idx) + 1))\n        avg_acc = 0\n        cnt = 0\n\n        for i in range(len(idx)):\n         acc[i + 1] = self.dist_acc(dists[idx[i]])\n         if acc[i + 1] >= 0:\n             avg_acc = avg_acc + acc[i + 1]\n             cnt += 1\n\n        avg_acc = avg_acc / cnt if cnt != 0 else 0\n        if cnt != 0:\n         acc[0] = avg_acc\n        return avg_acc,cnt\n\n    def StackedHourGlass(self, output, target, meta1, meta2, alpha=0.5):\n        predictions = self.get_max_preds(output[self.opts.nStack-1].detach().cpu().numpy())\n        target = self.get_max_preds(target.cpu().numpy())\n        return self.eval(predictions[0], target[0], alpha)\n\n    def PyraNet(self, output, target, meta1, meta2, alpha=0.5):\n        predictions = self.get_max_preds(output[self.opts.nStack-1].detach().cpu().numpy())\n        target = self.get_max_preds(target.cpu().numpy())\n        return self.eval(predictions[0], target[0], alpha)\n\n    def PoseAttention(self, output, target, meta1, meta2, alpha=0.5):\n        predictions = self.get_max_preds(output[self.opts.nStack-1].detach().cpu().numpy())\n        target = self.get_max_preds(target.cpu().numpy())\n        return self.eval(predictions[0], target[0], alpha)\n\n    def ChainedPredictions(self, output, target, meta1, meta2, alpha=0.5):\n        predictions = self.get_max_preds(output.detach().cpu().numpy())\n        target = self.get_max_preds(target.cpu().numpy())\n        return self.eval(predictions[0], target[0], alpha)\n\n    def DeepPose(self, output, target, meta1, meta2, alpha=0.5):\n        predictions = (0. + (output).reshape(-1,16,2).detach().cpu().numpy())*self.opts.outputRes\n        target = (0. + (target).reshape(-1,16,2).cpu().numpy())*self.opts.outputRes\n        return self.eval(predictions, target, alpha)\n'"
opts.py,0,"b'import os\nimport datetime\nimport utils as U\nimport configargparse\n\nclass opts():\n\t""""""docstring for opts""""""\n\tdef __init__(self):\n\t\tsuper(opts, self).__init__()\n\t\tself.parser = configargparse.ArgParser(default_config_files=[])\n\n\tdef init(self):\n\t\tself.parser.add(\'-test\', action=\'store_true\', help=\'Run only the validation epoch on the test dataset\')\n\t\tself.parser.add(\'-usedouble\', action=\'store_true\', help=\'Change Default tensor type to double\')\n\t\tself.parser.add(\'-DEBUG\', action=\'store_true\', help=\'To run in debug mode (visualize heatmaps, skeletons (ground truth and predicted)\')\n\t\tself.parser.add(\'-dont_pin_memory\', default=0, help=\'Whether to pin memory to gpu or not from the dataloader\')\n\n\t\tself.parser.add(\'-DataConfig\', required=True, is_config_file=True, help=\'Path to config file\')\n\t\tself.parser.add(\'-ModelConfig\', required=True, is_config_file=True, help=\'Path to config file\')\n\n\t\tself.parser.add(\'--expDir\', help=\'Experiment-Directory\')\n\t\tself.parser.add(\'--expID\', help=\'Experiment-ID\')\n\n\t\tself.parser.add(\'--visdom\', type=int, help=\'Support for visdom (currently unavailable :( \')\n\n\n######## Model Name and Load Checkpoint Parameters\n\t\tself.parser.add(\'--model\', help=\'Which model to use [DeepPose] [ChainedPredictions] [StackedHourGlass] [PyraNet] [PoseAttention]\')\n\n\t\tself.parser.add(\'--loadModel\', help=\'Path to the model to load\')\n\t\tself.parser.add(\'--loadOptim\', type=int, help=\'Whether to load Optimizer Parameters\')\n\t\tself.parser.add(\'--dropPreLoaded\', type=int, help=\'Whether to Drop Learning Rate of Loaded Optimizer\')\n\t\tself.parser.add(\'--dropMagPreLoaded\', type=float, help=\'How much learning rate to be dropped from the optimizer\')\n\t\tself.parser.add(\'--loadEpoch\', type=int, help=\'Whether to load Epoch number\')\n\n\n######## Dataloader Parameters\n\t\tself.parser.add(\'--TargetType\', help=\'TargetType for the dataloader [(direct) : Targets for Direct Regression] [(heatmap) : Targets for Heatmap Regression]\')\n\t\tself.parser.add(\'--maxTranslate\', type=float, help=\'Maximum translation as a percentage of the image width\')\n\t\tself.parser.add(\'--maxScale\', type=float, help=\'How much to scale the image or zoom in (for augmentation)\')\n\t\tself.parser.add(\'--maxRotate\', type=float, help=\'Maximum angle of rotation on either side (for augmentation)\')\n\t\tself.parser.add(\'--dataDir\', help=\'Directory for the data\')\n\t\tself.parser.add(\'--imageRes\', type=int, help=\'Size of Image Loaded\')\n\t\tself.parser.add(\'--inputRes\', type=int, help=\'Size of input to the network\')\n\t\tself.parser.add(\'--outputRes\', type=int, help=\'Size of output in case of heatmap based networks\')\n\t\tself.parser.add(\'--hmGauss\', type=int, help=\'Heatmap Gaussian Size\')\n\n\t\tself.parser.add(\'--nJoints\', type=int, help=\'Number of Joints to learn from dataset\')\n\n######## Network Parameters\n\n\t\t#### ChainedPredictions\n\t\tself.parser.add(\'--modelName\', help=\'Network Parameter\')\n\t\tself.parser.add(\'--hhKernel\', type=int, help=\'Network Parameter\')\n\t\tself.parser.add(\'--ohKernel\', type=int, help=\'Network Parameter\')\n\n\t\t#### DeepPose\n\t\tself.parser.add(\'--baseName\', help=\'Network Parameter\')\n\n\t\t#### StackedHourGlass | PyraNet | PoseAttention\n\t\tself.parser.add(\'--nChannels\', type=int, help=\'Network Parameter\')\n\t\tself.parser.add(\'--nStack\', type=int, help=\'Network Parameter\')\n\t\tself.parser.add(\'--nModules\', type=int, help=\'Network Parameter\')\n\t\tself.parser.add(\'--nReductions\', type=int, help=\'Network Parameter\')\n\n\t\t#### PyraNet\n\t\tself.parser.add(\'--baseWidth\', type=int, help=\'Network Parameter\')\n\t\tself.parser.add(\'--cardinality\', type=int, help=\'Network Parameter\')\n\n\t\t#### PoseAttention\n\t\tself.parser.add(\'--LRNSize\', type=int, help=\'Network Parameter\')\n\t\tself.parser.add(\'--IterSize\', type=int, help=\'Network Parameter\')\n\n\n######## DataLoader Parameters\n\t\tself.parser.add(\'--dataset\', help=\'MPII or COCO\')\n\t\tself.parser.add(\'--shuffle\', type=int, help=\'Shuffle the data during training\')\n\t\tself.parser.add(\'--nThreads\', type=int, help=\'How many threads to use for Dataloader\')\n\n\t\tself.parser.add(\'--data_loader_size\', type=int, help=\'Batch Size for DataLoader\')\n\t\tself.parser.add(\'--mini_batch_count\', type=int, help=\'After how many mini batches to run backprop\')\n\n\t\tself.parser.add(\'--valInterval\', type=int, help=\'After how many train epoch to run a val epoch\')\n\t\tself.parser.add(\'--saveInterval\', type=int, help=\'After how many train epochs to save model\')\n\n\t\tself.parser.add(\'--gpuid\', type=int, help=\'GPU ID for the model\')\n\t\tself.parser.add(\'--nEpochs\', type=int, help=\'Number of epochs to train\')\n\n\t\tself.parser.add(\'--optimizer_type\', help=\'Which optimizer to use in DataLoader\')\n\t\tself.parser.add(\'--optimizer_pars\', action=\'append\' , help=\'parameters for the optimizer\')\n\t\tself.parser.add(\'--LR\', type=float, help=\'Learning rate for the base resnet\')\n\n\t\tself.parser.add(\'--dropLR\', type=int, help=\'Drop LR after how many epochs\')\n\t\tself.parser.add(\'--dropMag\', type=float, help=\'Drop LR magnitude\')\n\n\t\tself.parser.add(\'--worldCoors\', help=\'World Coordinates file path (only for MPII)\')\n\t\tself.parser.add(\'--headSize\', help=\'head Size file path (only for MPII)\')\n\n\tdef parse(self):\n\t\tself.init()\n\t\tself.opt = self.parser.parse_args()\n\t\tif self.opt.DEBUG:\n\t\t\tself.opt.data_loader_size = 1\n\t\t\tself.opt.shuffle = 0\n\n\t\tself.opt.saveDir = os.path.join(os.path.join(self.opt.expDir, self.opt.expID), os.path.join(self.opt.model, \'logs_{}\'.format(datetime.datetime.now().isoformat())))\n\t\tself.opt.saveDir = os.path.join(self.opt.expDir, self.opt.model, self.opt.expID, \'logs_{}\'.format(datetime.datetime.now().isoformat()))\n\t\tU.ensure_dir(self.opt.saveDir)\n\n\t\t####### Write All Opts\n\t\targs = dict((name, getattr(self.opt, name)) for name in dir(self.opt)\n\t\t\t\t\tif not name.startswith(\'_\'))\n\n\t\tfile_name = os.path.join(self.opt.saveDir, \'opt.txt\')\n\t\twith open(file_name, \'wt\') as opt_file:\n\t\t\topt_file.write(\'==> Args:\\n\')\n\t\t\tfor k, v in sorted(args.items()):\n\t\t\t\topt_file.write(""%s: %s\\n""%(str(k), str(v)))\n\n\t\treturn self.opt\n'"
trainer.py,3,"b'import os\nimport torch\nimport pickle\nfrom progress.bar import Bar\nfrom utils import AverageMeter, adjust_learning_rate\n\nclass Trainer(object):\n        """"""docstring for Trainer""""""\n        def __init__(self, Model, Optimizer, Loss, Metrics, File, vis, opts):\n                super(Trainer, self).__init__()\n                self.model = Model\n                self.optimizer = Optimizer\n                self.Loss = Loss\n                self.metrics = Metrics\n                self.File = File\n                self.opts = opts\n                self.gpu = opts.gpuid\n                self.model = self.model\n\n        def test(self, valdataloader):\n                with torch.no_grad():\n                        self._epoch(valdataloader, -1, \'val\')\n\n        def train(self, traindataloader, valdataloader, startepoch, endepoch):\n                for epoch in range(startepoch, endepoch+1):\n\n                        train = self._epoch(traindataloader, epoch)\n\n                        if epoch%self.opts.valInterval==0:\n                                with torch.no_grad():\n                                        test = self._epoch(valdataloader, epoch, \'val\')\n                                Writer = open(self.File, \'a\')\n                                Writer.write(train + \' \' + test + \'\\n\')\n                                Writer.close()\n                        else:\n                                Writer = open(self.File, \'a\')\n                                Writer.write(train + \'\\n\')\n                                Writer.close()\n\n                        if epoch%self.opts.saveInterval==0:\n                                state = {\n                                        \'epoch\': epoch+1,\n                                        \'model_state\': self.model.state_dict(),\n                                        \'optimizer_state\' : self.optimizer.state_dict(),\n                                }\n                                path = os.path.join(self.opts.saveDir, \'model_{}.pth\'.format(epoch))\n                                torch.save(state, path)\n                        adjust_learning_rate(self.optimizer, epoch, self.opts.dropLR, self.opts.dropMag)\n                loss_final = self._epoch(valdataloader, -1, \'val\')\n                return\n\n        def initepoch(self):\n                self.loss = AverageMeter()\n                self.loss.reset()\n                for key, value in self.metrics.items():\n                        setattr(self, key, AverageMeter())\n                for key, value in self.metrics.items():\n                        getattr(self, key).reset()\n\n        def _epoch(self, dataloader, epoch, mode = \'train\'):\n                """"""\n                Training logic for an epoch\n                """"""\n                self.initepoch()\n                if mode == \'train\':\n                        self.model.train()\n                else :\n                        self.model.eval()\n\n                nIters = len(dataloader)\n                bar = Bar(\'==>\', max=nIters)\n\n                for batch_idx, (data, target, meta1, meta2) in enumerate(dataloader):\n                        model = self.model.to(self.gpu)\n                        data = data.to(self.gpu, non_blocking=True).float()\n                        target = target.to(self.gpu, non_blocking=True).float()\n                        output = model(data)\n\n                        loss = self.Loss(output, target, meta1.to(self.gpu, non_blocking=True).float().unsqueeze(-1))\n                        self.loss.update(loss.item(), data.shape[0])\n\n                        self._eval_metrics(output, target, meta1, meta2, data.shape[0])\n\n                        if self.opts.DEBUG:\n                                pass\n\n                        if mode == \'train\':\n                                loss.backward()\n                                if (batch_idx+1)%self.opts.mini_batch_count==0:\n                                        self.optimizer.step()\n                                        self.optimizer.zero_grad()\n                                        if self.opts.DEBUG:\n                                                pass\n\n                        Bar.suffix = mode + \' Epoch: [{0}][{1}/{2}]| Total: {total:} | ETA: {eta:} | Loss: {loss.avg:.6f} ({loss.val:.6f})\'.format(epoch, batch_idx+1, nIters, total=bar.elapsed_td, eta=bar.eta_td, loss=self.loss) + self._print_metrics()\n                        bar.next()\n                bar.finish()\n                return \'{:8f} \'.format(self.loss.avg) + \' \'.join([\'{:4f}\'.format(getattr(self, key).avg) for key,_ in self.metrics.items()])\n\n        def _eval_metrics(self, output, target, meta1, meta2, batchsize):\n                for key, value in self.metrics.items():\n                        value, count = value(output, target, meta1, meta2)\n                        getattr(self, key).update(value, count)\n                return\n\n        def _print_metrics(self):\n                return \'\'.join([(\'| {0}: {metric.avg:.3f} ({metric.val:.3f}) \'.format(key, metric=getattr(self, key))) for key, _ in self.metrics.items()])\n'"
utils.py,0,"b'import os\n\ndef create_plot_window(vis, xlabel, ylabel, title):\n\treturn vis.line(X=np.array([1]), Y=np.array([np.nan]), opts=dict(xlabel=xlabel, ylabel=ylabel, title=title))\n\ndef adjust_learning_rate(optimizer, epoch, dropLR, dropMag):\n\tif epoch%dropLR==0:\n\t\tlrfac = dropMag\n\telse:\n\t\tlrfac = 1\n\tfor i,param_group in enumerate(optimizer.param_groups):\n\t\tif lrfac!=1:\n\t\t\tprint(""Reducing learning rate of group %d from %f to %f""%(i,param_group[\'lr\'],param_group[\'lr\']*lrfac))\n\t\tparam_group[\'lr\'] *= lrfac\n\n\nclass AverageMeter(object):\n\t""""""Computes and stores the average and current value""""""\n\tdef __init__(self):\n\t\tself.reset()\n\n\tdef reset(self):\n\t\tself.val = 0\n\t\tself.avg = 0\n\t\tself.sum = 0\n\t\tself.count = 0\n\n\tdef update(self, val, n=1):\n\t\tself.val = val\n\t\tself.sum += val * n\n\t\tself.count += n\n\t\tself.avg = 0 if self.count is 0 else self.sum / self.count\n\ndef ensure_dir(path):\n\tif path is not None:\n\t\tif not os.path.exists(path):\n\t\t\tos.makedirs(path)'"
datasets/__init__.py,0,b'from datasets.mpii import *\nfrom datasets.coco import *'
datasets/coco.py,1,"b""import torch.utils.data as data\nfrom datasets.COCO.coco import COCODataset\n\nclass COCO(data.Dataset):\n def __init__(self, opts, split):\n\t imageSet = None\n\t isTrain = None\n\t if split == 'train':\n\t\t imageSet = 'train2017'\n\t\t isTrain = True\n\t elif split == 'val':\n\t\t imageSet = 'val2017'\n\t\t isTrain = False\n\t import torchvision.transforms as transforms\n\n\t self.stuff = COCODataset(opts, opts.dataDir, imageSet, isTrain, transforms.Compose([\n\t\t transforms.ToTensor(),\n\t ]))\n\n def __getitem__(self, index):\n\t return self.stuff.__getitem__(index)\n\n def __len__(self):\n\t return self.stuff.__len__()\n"""
datasets/img.py,2,"b""import cv2\nimport torch\nimport numpy as np\nimport torch\nfrom math import *\n\nsigma_inp = 20#ref.hmGaussInp\nn = sigma_inp * 6 + 1\ng_inp = np.zeros((n, n))\nfor i in range(n):\n\t\tfor j in range(n):\n\t\t\t\tg_inp[i, j] = np.exp(-((i - n / 2) ** 2 + (j - n / 2) ** 2) / (2. * sigma_inp * sigma_inp))\n\ndef GetTransform(center, scale, rot, res):\n\th = scale\n\tt = np.eye(3)\n\n\tt[0, 0] = res / h\n\tt[1, 1] = res / h\n\tt[0, 2] = res * (- center[0] / h + 0.5)\n\tt[1, 2] = res * (- center[1] / h + 0.5)\n\n\tif rot != 0:\n\t\trot = -rot\n\t\tr = np.eye(3)\n\t\tang = rot * np.math.pi / 180\n\t\ts = np.math.sin(ang)\n\t\tc = np.math.cos(ang)\n\t\tr[0, 0] = c\n\t\tr[0, 1] = - s\n\t\tr[1, 0] = s\n\t\tr[1, 1] = c\n\t\tt_ = np.eye(3)\n\t\tt_[0, 2] = - res / 2\n\t\tt_[1, 2] = - res / 2\n\t\tt_inv = np.eye(3)\n\t\tt_inv[0, 2] = res / 2\n\t\tt_inv[1, 2] = res / 2\n\t\tt = np.dot(np.dot(np.dot(t_inv,  r), t_), t)\n\n\treturn t\n\n\ndef Transform(pt, center, scale, rot, res, invert = False):\n\tpt_ = np.ones(3)\n\tpt_[0], pt_[1] = pt[0], pt[1]\n\n\tt = GetTransform(center, scale, rot, res)\n\tif invert:\n\t\tt = np.linalg.inv(t)\n\tnew_point = np.dot(t, pt_)[:2]\n\tnew_point = new_point.astype(np.int32)\n\treturn new_point\n\n\ndef getTransform3D(center, scale, rot, res):\n\th = 1.0 * scale\n\tt = np.eye(4)\n\n\tt[0][0] = res / h\n\tt[1][1] = res / h\n\tt[2][2] = res / h\n\n\tt[0][3] = res * (- center[0] / h + 0.5)\n\tt[1][3] = res * (- center[1] / h + 0.5)\n\n\tif rot != 0:\n\t\traise Exception('Not Implement')\n\n\treturn t\n\n\ndef Transform3D(pt, center, scale, rot, res, invert = False):\n\tpt_ = np.ones(4)\n\tpt_[0], pt_[1], pt_[2] = pt[0], pt[1], pt[2]\n\tt = getTransform3D(center, scale, rot, res)\n\tif invert:\n\t\tt = np.linalg.inv(t)\n\tnew_point = np.dot(t, pt_)[:3]\n\treturn new_point\n\n\ndef Crop(img, center, scale, rot, res):\n\tht, wd = img.shape[0], img.shape[1]\n\ttmpImg, newImg = img.copy(), np.zeros((res, res, 3), dtype = np.uint8)\n\n\tscaleFactor = scale / res\n\tif scaleFactor < 2:\n\t\tscaleFactor = 1\n\telse:\n\t\tnewSize = int(np.math.floor(max(ht, wd) / scaleFactor))\n\t\tnewSize_ht = int(np.math.floor(ht / scaleFactor))\n\t\tnewSize_wd = int(np.math.floor(wd / scaleFactor))\n\t\tif newSize < 2:\n\t\t\treturn torch.from_numpy(newImg.transpose(2, 0, 1).astype(np.float32) / 256.)\n\t\telse:\n\t\t\ttmpImg = cv2.resize(tmpImg, (newSize_wd, newSize_ht)) #TODO\n\t\t\tht, wd = tmpImg.shape[0], tmpImg.shape[1]\n\n\tc, s = 1.0 * center / scaleFactor, scale / scaleFactor\n\tc[0], c[1] = c[1], c[0]\n\tul = Transform((0, 0), c, s, 0, res, invert = True)\n\tbr = Transform((res, res), c, s, 0, res, invert = True)\n\n\tif scaleFactor >= 2:\n\t\tbr = br - (br - ul - res)\n\n\tpad = int(np.math.ceil((((ul - br) ** 2).sum() ** 0.5) / 2 - (br[0] - ul[0]) / 2))\n\tif rot != 0:\n\t\tul = ul - pad\n\t\tbr = br + pad\n\n\told_ = [max(0, ul[0]),   min(br[0], ht),         max(0, ul[1]),   min(br[1], wd)]\n\tnew_ = [max(0, - ul[0]), min(br[0], ht) - ul[0], max(0, - ul[1]), min(br[1], wd) - ul[1]]\n\n\tnewImg = np.zeros((br[0] - ul[0], br[1] - ul[1], 3), dtype = np.uint8)\n\t#print 'new old newshape tmpshape center', new_[0], new_[1], old_[0], old_[1], newImg.shape, tmpImg.shape, center\n\ttry:\n\t\tnewImg[new_[0]:new_[1], new_[2]:new_[3], :] = tmpImg[old_[0]:old_[1], old_[2]:old_[3], :]\n\texcept:\n\t\t#print 'ERROR: new old newshape tmpshape center', new_[0], new_[1], old_[0], old_[1], newImg.shape, tmpImg.shape, center\n\t\treturn np.zeros((3, res, res), np.uint8)\n\tif rot != 0:\n\t\tM = cv2.getRotationMatrix2D((newImg.shape[0] / 2, newImg.shape[1] / 2), rot, 1)\n\t\tnewImg = cv2.warpAffine(newImg, M, (newImg.shape[0], newImg.shape[1]))\n\t\tnewImg = newImg[pad+1:-pad+1, pad+1:-pad+1, :].copy()\n\n\tif scaleFactor < 2:\n\t\tnewImg = cv2.resize(newImg, (res, res))\n\n\treturn newImg.transpose(2, 0, 1).astype(np.float32)\n\ndef Gaussian(sigma):\n\tif sigma == 7:\n\t\treturn np.array([0.0529,  0.1197,  0.1954,  0.2301,  0.1954,  0.1197,  0.0529,\n\t\t\t\t\t\t 0.1197,  0.2709,  0.4421,  0.5205,  0.4421,  0.2709,  0.1197,\n\t\t\t\t\t\t 0.1954,  0.4421,  0.7214,  0.8494,  0.7214,  0.4421,  0.1954,\n\t\t\t\t\t\t 0.2301,  0.5205,  0.8494,  1.0000,  0.8494,  0.5205,  0.2301,\n\t\t\t\t\t\t 0.1954,  0.4421,  0.7214,  0.8494,  0.7214,  0.4421,  0.1954,\n\t\t\t\t\t\t 0.1197,  0.2709,  0.4421,  0.5205,  0.4421,  0.2709,  0.1197,\n\t\t\t\t\t\t 0.0529,  0.1197,  0.1954,  0.2301,  0.1954,  0.1197,  0.0529]).reshape(7, 7)\n\telif sigma == n:\n\t\treturn g_inp\n\telse:\n\t\traise Exception('Gaussian {} Not Implement'.format(sigma))\n\ndef DrawGaussian(img, pt, sigma, truesigma=-1):\n\timg = img.copy()\n\ttmpSize = int(np.math.ceil(3 * sigma))\n\tul = [int(np.math.floor(pt[0] - tmpSize)), int(np.math.floor(pt[1] - tmpSize))]\n\tbr = [int(np.math.floor(pt[0] + tmpSize)), int(np.math.floor(pt[1] + tmpSize))]\n\n\tif ul[0] > img.shape[1] or ul[1] > img.shape[0] or br[0] < 1 or br[1] < 1:\n\t\treturn img\n\n\tsize = 2 * tmpSize + 1\n\tg = Gaussian(size)\n\tif truesigma==0.5:\n\t\tg[0,:] *= 0\n\t\tg[-1,:] *= 0\n\t\tg[:,0] *= 0\n\t\tg[:,-1] *= 0\n\t\tg *= 1.5\n\tg_x = [max(0, -ul[0]), min(br[0], img.shape[1]) - max(0, ul[0]) + max(0, -ul[0])]\n\tg_y = [max(0, -ul[1]), min(br[1], img.shape[0]) - max(0, ul[1]) + max(0, -ul[1])]\n\n\timg_x = [max(0, ul[0]), min(br[0], img.shape[1])]\n\timg_y = [max(0, ul[1]), min(br[1], img.shape[0])]\n\n\timg[img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n\treturn img\n\ndef myDrawGaussian(img, pt, sigmaredundant):\n\tpt[0] = floor(pt[0])\n\tpt[1] = floor(pt[1])\n\tif (pt[0] < 1 or pt[1] < 1 or pt[0] > img.shape[0] or pt[1] > img.shape[1]):\n\t\treturn img\n\timg[max(pt[0]-3, 0):min(pt[0]+3, img.shape[0]), max(pt[1]-0, 0):min(pt[1]+0, img.shape[1])] = 1\n\timg[max(pt[0]-2, 0):min(pt[0]+2, img.shape[0]), max(pt[1]-1, 0):min(pt[1]+1, img.shape[1])] = 1\n\timg[max(pt[0]-2, 0):min(pt[0]+2, img.shape[0]), max(pt[1]-2, 0):min(pt[1]+2, img.shape[1])] = 1\n\timg[max(pt[0]-0, 0):min(pt[0]+0, img.shape[0]), max(pt[1]-3, 0):min(pt[1]+3, img.shape[1])] = 1\n\tif (img.sum()==0):\n\t\tprint(pt)\n\t\tassert(img.sum() != 0)\n\timg = img / img.sum()\n\treturn img\n\ndef Rnd(x):\n\twow = torch.randn(1)\n\treturn max(-2 * x, min(2 * x, float(wow) * x))\n\ndef Flip(img):\n\treturn img[:, :, ::-1].copy()\n\ndef ShuffleLR(x):\n\tshuffleRef = [[0, 5], [1, 4], [2, 3],\n             [10, 15], [11, 14], [12, 13]]\n\tfor e in shuffleRef:\n\t\tx[e[0]], x[e[1]] = x[e[1]].copy(), x[e[0]].copy()\n\treturn x\n"""
datasets/mpii.py,1,"b""import cv2\nimport torch\nimport h5py as  H\nimport numpy as np\nimport scipy.io as sio\nimport datasets.img as I\nimport torch.utils.data as data\nimport torchvision.transforms.functional as F\n\nclass MPII(data.Dataset):\n\tdef __init__(self, opts, split):\n\t\tprint('==> initializing 2D {} data.'.format(split))\n\n\t\tself.opts = opts\n\t\tself.split = split\n\n\t\ttags = ['imgname','part','center','scale']\n\n\t\tself.stuff1 = sio.loadmat(open(opts.worldCoors[:-4] + ('train' if split is 'train' else '') + '.mat', 'rb'))['a']\n\t\tself.stuff2 = sio.loadmat(open(opts.headSize[:-4] + ('train' if split is 'train' else '') + '.mat', 'rb'))['headSize']\n\n\t\tf = H.File('{}/mpii/pureannot/{}.h5'.format(self.opts.dataDir, split), 'r')\n\t\tannot = {}\n\t\tfor tag in tags:\n\t\t\tannot[tag] = np.asarray(f[tag]).copy()\n\t\tf.close()\n\t\tself.annot = annot\n\n\t\tself.len = len(self.annot['scale'])\n\n\t\tprint('Loaded 2D {} {} samples'.format(split, len(annot['scale'])))\n\n\tdef LoadImage(self, index):\n\t\tpath = '{}/mpii/images/{}'.format(self.opts.dataDir, ''.join(chr(int(i)) for i in self.annot['imgname'][index]))\n\t\timg = cv2.imread(path)\n\t\treturn img\n\n\tdef GetPartInfo(self, index):\n\t\tpts = self.annot['part'][index].copy()\n\t\tc = self.annot['center'][index].copy()\n\t\ts = self.annot['scale'][index]\n\t\ts = s * 200\n\t\treturn pts, c, s\n\n\tdef __getitem__(self, index):\n\t\timg = self.LoadImage(index)\n\t\tpts, c, s = self.GetPartInfo(index)\n\t\tr = 0\n\n\t\tif self.split == 'train':\n\t\t\ts = s * (2 ** I.Rnd(self.opts.maxScale))\n\t\t\tr = 0 if np.random.random() < 0.6 else I.Rnd(self.opts.maxRotate)\n\n\t\tinp = I.Crop(img, c, s, r, self.opts.inputRes) / 256.\n\n\t\tout = np.zeros((self.opts.nJoints, self.opts.outputRes, self.opts.outputRes))\n\n\t\tfor i in range(self.opts.nJoints):\n\t\t\tif pts[i][0] > 1:\n\t\t\t\tpts[i] = I.Transform(pts[i], c, s, r, self.opts.outputRes)\n\t\t\t\tout[i] = I.DrawGaussian(out[i], pts[i], self.opts.hmGauss, 0.5 if self.opts.outputRes==32 else -1)\n\n\t\tif self.split == 'train':\n\t\t\tif np.random.random() < 0.5:\n\t\t\t\tinp = I.Flip(inp)\n\t\t\t\tout = I.ShuffleLR(I.Flip(out))\n\t\t\t\tpts[:, 0] = self.opts.outputRes/4 - pts[:, 0]\n\n\t\t\tinp[0] = np.clip(inp[0] * (np.random.random() * (0.4) + 0.6), 0, 1)\n\t\t\tinp[1] = np.clip(inp[1] * (np.random.random() * (0.4) + 0.6), 0, 1)\n\t\t\tinp[2] = np.clip(inp[2] * (np.random.random() * (0.4) + 0.6), 0, 1)\n\n\t\tif self.opts.TargetType=='heatmap':\n\t\t\treturn inp, out, self.stuff1[index], self.stuff2[index]\n\t\telif self.opts.TargetType=='direct':\n\t\t\treturn inp, np.reshape((pts/self.opts.outputRes), -1) - 0.5, self.stuff1[index], self.stuff2[index]\n\n\tdef __len__(self):\n\t\treturn self.len\n"""
models/ChainedPredictions.py,4,"b'import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport models.modules.ChainedPredictions as M\n\nclass ChainedPredictions(nn.Module):\n\t""""""docstring for ChainedPredictions""""""\n\tdef __init__(self, modelName, hhKernel, ohKernel, nJoints):\n\t\tsuper(ChainedPredictions, self).__init__()\n\t\tself.nJoints = nJoints\n\t\tself.modelName = modelName\n\t\tself.resnet = getattr(torchvision.models, self.modelName)(pretrained=True)\n\t\tself.resnet.avgpool = M.Identity()\n\t\tself.resnet.fc = M.Identity()\n\t\tself.hiddenChans = 64 ### Add cases!\n\n\t\tself.hhKernel = hhKernel\n\t\tself.ohKernel = ohKernel\n\n\t\tself.init_hidden = nn.Conv2d(512, self.hiddenChans, 1)\n\t\t_deception = []\n\t\tfor i in range(self.nJoints):\n\t\t\t_deception.append(M.Deception(self.hiddenChans))\n\t\tself.deception = nn.ModuleList(_deception)\n\n\t\t_h2h = []\n\t\t_o2h = []\n\t\tfor i in range(nJoints):\n\t\t\t_o = []\n\t\t\t_h2h.append(\n\t\t\t\tnn.Sequential(\n\t\t\t\t\tnn.Conv2d(self.hiddenChans, self.hiddenChans, kernel_size=self.hhKernel, padding=self.hhKernel//2),\n\t\t\t\t\tnn.BatchNorm2d(self.hiddenChans)\n\t\t\t\t)\n\t\t\t)\n\t\t\tfor j in range(i+1):\n\t\t\t\t_o.append(nn.Sequential(\n\t\t\t\t\t\tnn.Conv2d(1, self.hiddenChans, 1),\n\t\t\t\t\t\tnn.Conv2d(self.hiddenChans, self.hiddenChans, kernel_size=self.ohKernel, stride=2, padding=self.ohKernel//2),\n\t\t\t\t\t\tnn.BatchNorm2d(self.hiddenChans),\n\t\t\t\t\t\tnn.Conv2d(self.hiddenChans, self.hiddenChans, kernel_size=self.ohKernel, stride=2, padding=self.ohKernel//2),\n\t\t\t\t\t\tnn.BatchNorm2d(self.hiddenChans),\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t_o2h.append(nn.ModuleList(_o))\n\n\t\tself.h2h = nn.ModuleList(_h2h)\n\t\tself.o2h = nn.ModuleList(_o2h)\n\n\tdef forward(self, x):\n\t\thidden = [0]*self.nJoints\n\t\toutput = [None]*self.nJoints\n\t\thidden[0] += self.resnet(x).reshape(-1, 512, 8, 8)\n\t\thidden[0] = self.init_hidden(hidden[0])\n\t\toutput[0] = self.deception[0](hidden[0])\n\n\t\tfor i in range(self.nJoints-1):\n\t\t\thidden[i+1] = self.h2h[i](hidden[i])\n\t\t\tfor j in range(i+1):\n\t\t\t\thidden[i+1] += self.o2h[i][j](output[j])\n\t\t\thidden[i+1] = torch.relu(hidden[i+1])\n\t\t\toutput[i+1] = self.deception[i+1](hidden[i+1])\n\t\treturn torch.cat(output, 1)\n'"
models/DeepPose.py,2,"b'import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport models.modules.DeepPose as M\n\nclass DeepPose(nn.Module):\n\t""""""docstring for DeepPose""""""\n\tdef __init__(self, nJoints, modelName=\'resnet50\'):\n\t\tsuper(DeepPose, self).__init__()\n\t\tself.nJoints = nJoints\n\t\tself.block = \'BottleNeck\' if (int(modelName[6:]) > 34) else \'BasicBlock\'\n\t\tself.resnet = getattr(torchvision.models, modelName)(pretrained=True)\n\t\tself.resnet.fc = nn.Linear(512 * (4 if self.block == \'BottleNeck\' else 1), self.nJoints * 2)\n\tdef forward(self, x):\n\t\treturn self.resnet(x)\n'"
models/PoseAttention.py,2,"b'import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport models.modules.PoseAttention as M\n\nclass HourglassAttention(nn.Module):\n\t""""""docstring for HourglassAttention""""""\n\tdef __init__(self, nChannels = 256, numReductions = 4, nModules = 2, poolKernel = (2,2), poolStride = (2,2), upSampleKernel = 2):\n\t\tsuper(HourglassAttention, self).__init__()\n\t\tself.numReductions = numReductions\n\t\tself.nModules = nModules\n\t\tself.nChannels = nChannels\n\t\tself.poolKernel = poolKernel\n\t\tself.poolStride = poolStride\n\t\tself.upSampleKernel = upSampleKernel\n\t\t""""""\n\t\tFor the skip connection, a Residual module (or sequence of residuaql modules)\n\t\t""""""\n\n\t\t_skip = []\n\t\tfor _ in range(self.nModules):\n\t\t\t_skip.append(M.Residual(self.nChannels, self.nChannels))\n\n\t\tself.skip = nn.Sequential(*_skip)\n\n\t\t""""""\n\t\tFirst pooling to go to smaller dimension then pass input through\n\t\tResidual Module or sequence of Modules then  and subsequent cases:\n\t\t\teither pass through Hourglass of numReductions-1\n\t\t\tor pass through Residual Module or sequence of Modules\n\t\t""""""\n\n\t\tself.mp = nn.MaxPool2d(self.poolKernel, self.poolStride)\n\n\t\t_afterpool = []\n\t\tfor _ in range(self.nModules):\n\t\t\t_afterpool.append(M.Residual(self.nChannels, self.nChannels))\n\n\t\tself.afterpool = nn.Sequential(*_afterpool)\n\n\t\tif (numReductions > 1):\n\t\t\tself.hg = HourglassAttention(self.nChannels, self.numReductions-1, self.nModules, self.poolKernel, self.poolStride)\n\t\telse:\n\t\t\t_num1res = []\n\t\t\tfor _ in range(self.nModules):\n\t\t\t\t_num1res.append(M.Residual(self.nChannels,self.nChannels))\n\n\t\t\tself.num1res = nn.Sequential(*_num1res)  # doesnt seem that important ?\n\n\t\t""""""\n\t\tNow another Residual Module or sequence of Residual Modules\n\t\t""""""\n\n\t\t_lowres = []\n\t\tfor _ in range(self.nModules):\n\t\t\t_lowres.append(M.Residual(self.nChannels,self.nChannels))\n\n\t\tself.lowres = nn.Sequential(*_lowres)\n\n\t\t""""""\n\t\tUpsampling Layer (Can we change this??????)\n\t\tAs per Newell\'s paper upsamping recommended\n\t\t""""""\n\t\tself.up = nn.Upsample(scale_factor = self.upSampleKernel)\n\n\n\tdef forward(self, x):\n\t\tout1 = x\n\t\tout1 = self.skip(out1)\n\t\tout2 = x\n\t\tout2 = self.mp(out2)\n\t\tout2 = self.afterpool(out2)\n\t\tif self.numReductions>1:\n\t\t\tout2 = self.hg(out2)\n\t\telse:\n\t\t\tout2 = self.num1res(out2)\n\t\tout2 = self.lowres(out2)\n\t\tout2 = self.up(out2)\n\n\t\treturn out2 + out1\n\n\nclass PoseAttention(nn.Module):\n\t""""""docstring for PoseAttention""""""\n\tdef __init__(self, nChannels, nStack, nModules, numReductions, nJoints, LRNSize, IterSize):\n\t\tsuper(PoseAttention, self).__init__()\n\t\tself.nChannels = nChannels\n\t\tself.nStack = nStack\n\t\tself.nModules = nModules\n\t\tself.numReductions = numReductions\n\t\tself.nJoints = nJoints\n\t\tself.LRNSize = LRNSize\n\t\tself.IterSize = IterSize\n\t\tself.nJoints = nJoints\n\n\t\tself.start = M.BnReluConv(3, 64, kernelSize = 7, stride = 2, padding = 3)\n\n\t\tself.res1 = M.Residual(64, 128)\n\t\tself.mp = nn.MaxPool2d(2, 2)\n\t\tself.res2 = M.Residual(128, 128)\n\t\tself.res3 = M.Residual(128, self.nChannels)\n\n\t\t_hourglass, _Residual, _lin1, _attiter, _chantojoints, _lin2, _jointstochan = [], [],[],[],[],[],[]\n\n\t\tfor i in range(self.nStack):\n\t\t\t_hourglass.append(HourglassAttention(self.nChannels, self.numReductions, self.nModules))\n\t\t\t_ResidualModules = []\n\t\t\tfor _ in range(self.nModules):\n\t\t\t\t_ResidualModules.append(M.Residual(self.nChannels, self.nChannels))\n\t\t\t_ResidualModules = nn.Sequential(*_ResidualModules)\n\t\t\t_Residual.append(_ResidualModules)\n\t\t\t_lin1.append(M.BnReluConv(self.nChannels, self.nChannels))\n\t\t\t_attiter.append(M.AttentionIter(self.nChannels, self.LRNSize, self.IterSize))\n\t\t\tif i<self.nStack//2:\n\t\t\t\t_chantojoints.append(\n\t\t\t\t\t\tnn.Sequential(\n\t\t\t\t\t\t\tnn.BatchNorm2d(self.nChannels),\n\t\t\t\t\t\t\tnn.Conv2d(self.nChannels, self.nJoints,1),\n\t\t\t\t\t\t)\n\t\t\t\t\t)\n\t\t\telse:\n\t\t\t\t_chantojoints.append(M.AttentionPartsCRF(self.nChannels, self.LRNSize, self.IterSize, self.nJoints))\n\t\t\t_lin2.append(nn.Conv2d(self.nChannels, self.nChannels,1))\n\t\t\t_jointstochan.append(nn.Conv2d(self.nJoints,self.nChannels,1))\n\n\t\tself.hourglass = nn.ModuleList(_hourglass)\n\t\tself.Residual = nn.ModuleList(_Residual)\n\t\tself.lin1 = nn.ModuleList(_lin1)\n\t\tself.chantojoints = nn.ModuleList(_chantojoints)\n\t\tself.lin2 = nn.ModuleList(_lin2)\n\t\tself.jointstochan = nn.ModuleList(_jointstochan)\n\n\tdef forward(self, x):\n\t\tx = self.start(x)\n\t\tx = self.res1(x)\n\t\t#print(""1"", x.mean())\n\t\tx = self.mp(x)\n\t\tx = self.res2(x)\n\t\t#print(""2"", x.mean())\n\t\tx = self.res3(x)\n\t\tout = []\n\n\t\tfor i in range(self.nStack):\n\t\t\t#print(""3"", x.mean())\n\t\t\tx1 = self.hourglass[i](x)\n\t\t\t#print(""4"", x1.mean())\n\t\t\tx1 = self.Residual[i](x1)\n\t\t\t#print(""5"", x1.mean())\n\t\t\tx1 = self.lin1[i](x1)\n\t\t\t#print(""6"", x1.mean())\n\t\t\tout.append(self.chantojoints[i](x1))\n\t\t\tx1 = self.lin2[i](x1)\n\t\t\t#print(""7"", x1.mean())\n\t\t\tx = x + x1 + self.jointstochan[i](out[i])\n\n\t\treturn (out)\n'"
models/PyraNet.py,2,"b'import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport models.modules.PyraNet as M\n\nclass PyraNetHourGlass(nn.Module):\n\t""""""docstring for PyraNetHourGlass""""""\n\tdef __init__(self, nChannels=256, numReductions=4, nModules=2, inputRes=256, baseWidth=6, cardinality=30, poolKernel=(2,2), poolStride=(2,2), upSampleKernel=2):\n\t\tsuper(PyraNetHourGlass, self).__init__()\n\t\tself.numReductions = numReductions\n\t\tself.nModules = nModules\n\t\tself.nChannels = nChannels\n\t\tself.poolKernel = poolKernel\n\t\tself.poolStride = poolStride\n\t\tself.upSampleKernel = upSampleKernel\n\n\t\tself.inputRes = inputRes\n\t\tself.baseWidth = baseWidth\n\t\tself.cardinality = cardinality\n\t\t""""""\n\t\tFor the skip connection, a residual module (or sequence of residuaql modules)\n\t\t""""""\n\t\tResidualskip = M.ResidualPyramid if numReductions > 1 else M.Residual\n\t\tResidualmain = M.ResidualPyramid if numReductions > 2 else M.Residual\n\t\t_skip = []\n\t\tfor _ in range(self.nModules):\n\t\t\t_skip.append(Residualskip(self.nChannels, self.nChannels, self.inputRes, self.baseWidth, self.cardinality))\n\n\t\tself.skip = nn.Sequential(*_skip)\n\n\t\t""""""\n\t\tFirst pooling to go to smaller dimension then pass input through\n\t\tResidual Module or sequence of Modules then  and subsequent cases:\n\t\t\teither pass through Hourglass of numReductions-1\n\t\t\tor pass through Residual Module or sequence of Modules\n\t\t""""""\n\n\t\tself.mp = nn.MaxPool2d(self.poolKernel, self.poolStride)\n\n\t\t_afterpool = []\n\t\tfor _ in range(self.nModules):\n\t\t\t_afterpool.append(Residualmain(self.nChannels, self.nChannels, self.inputRes//2, self.baseWidth, self.cardinality))\n\n\t\tself.afterpool = nn.Sequential(*_afterpool)\n\n\t\tif (numReductions > 1):\n\t\t\tself.hg = PyraNetHourGlass(self.nChannels, self.numReductions-1, self.nModules, self.inputRes//2, self.baseWidth, self.cardinality, self.poolKernel, self.poolStride, self.upSampleKernel)\n\t\telse:\n\t\t\t_num1res = []\n\t\t\tfor _ in range(self.nModules):\n\t\t\t\t_num1res.append(Residualmain(self.nChannels,self.nChannels, self.inputRes//2, self.baseWidth, self.cardinality))\n\n\t\t\tself.num1res = nn.Sequential(*_num1res)  # doesnt seem that important ?\n\n\t\t""""""\n\t\tNow another Residual Module or sequence of Residual Modules\n\t\t""""""\n\n\t\t_lowres = []\n\t\tfor _ in range(self.nModules):\n\t\t\t_lowres.append(Residualmain(self.nChannels,self.nChannels, self.inputRes//2, self.baseWidth, self.cardinality))\n\n\t\tself.lowres = nn.Sequential(*_lowres)\n\n\t\t""""""\n\t\tUpsampling Layer (Can we change this??????)\n\t\tAs per Newell\'s paper upsamping recommended\n\t\t""""""\n\t\tself.up = nn.Upsample(scale_factor = self.upSampleKernel)\n\n\n\tdef forward(self, x):\n\t\tout1 = x\n\t\tout1 = self.skip(out1)\n\t\tout2 = x\n\t\tout2 = self.mp(out2)\n\t\tout2 = self.afterpool(out2)\n\t\tif self.numReductions>1:\n\t\t\tout2 = self.hg(out2)\n\t\telse:\n\t\t\tout2 = self.num1res(out2)\n\t\tout2 = self.lowres(out2)\n\t\tout2 = self.up(out2)\n\n\t\treturn out2 + out1\n\n\nclass PyraNet(nn.Module):\n\t""""""docstring for PyraNet""""""\n\tdef __init__(self, nChannels=256, nStack=4, nModules=2, numReductions=4, baseWidth=6, cardinality=30, nJoints=16, inputRes=256):\n\t\tsuper(PyraNet, self).__init__()\n\t\tself.nChannels = nChannels\n\t\tself.nStack = nStack\n\t\tself.nModules = nModules\n\t\tself.numReductions = numReductions\n\t\tself.baseWidth = baseWidth\n\t\tself.cardinality = cardinality\n\t\tself.inputRes = inputRes\n\t\tself.nJoints = nJoints\n\n\t\tself.start = M.BnReluConv(3, 64, kernelSize = 7, stride = 2, padding = 3)\n\n\n\t\tself.res1 = M.ResidualPyramid(64, 128, self.inputRes//2, self.baseWidth, self.cardinality, 0)\n\t\tself.mp = nn.MaxPool2d(2, 2)\n\t\tself.res2 = M.ResidualPyramid(128, 128, self.inputRes//4, self.baseWidth, self.cardinality,)\n\t\tself.res3 = M.ResidualPyramid(128, self.nChannels, self.inputRes//4, self.baseWidth, self.cardinality)\n\n\t\t_hourglass, _Residual, _lin1, _chantojoints, _lin2, _jointstochan = [],[],[],[],[],[]\n\n\t\tfor _ in range(self.nStack):\n\t\t\t_hourglass.append(PyraNetHourGlass(self.nChannels, self.numReductions, self.nModules, self.inputRes//4, self.baseWidth, self.cardinality))\n\t\t\t_ResidualModules = []\n\t\t\tfor _ in range(self.nModules):\n\t\t\t\t_ResidualModules.append(M.Residual(self.nChannels, self.nChannels))\n\t\t\t_ResidualModules = nn.Sequential(*_ResidualModules)\n\t\t\t_Residual.append(_ResidualModules)\n\t\t\t_lin1.append(M.BnReluConv(self.nChannels, self.nChannels))\n\t\t\t_chantojoints.append(nn.Conv2d(self.nChannels, self.nJoints,1))\n\t\t\t_lin2.append(nn.Conv2d(self.nChannels, self.nChannels,1))\n\t\t\t_jointstochan.append(nn.Conv2d(self.nJoints,self.nChannels,1))\n\n\t\tself.hourglass = nn.ModuleList(_hourglass)\n\t\tself.Residual = nn.ModuleList(_Residual)\n\t\tself.lin1 = nn.ModuleList(_lin1)\n\t\tself.chantojoints = nn.ModuleList(_chantojoints)\n\t\tself.lin2 = nn.ModuleList(_lin2)\n\t\tself.jointstochan = nn.ModuleList(_jointstochan)\n\n\tdef forward(self, x):\n\t\tx = self.start(x)\n\t\tx = self.res1(x)\n\t\tx = self.mp(x)\n\t\tx = self.res2(x)\n\t\tx = self.res3(x)\n\t\tout = []\n\n\t\tfor i in range(self.nStack):\n\t\t\tx1 = self.hourglass[i](x)\n\t\t\tx1 = self.Residual[i](x1)\n\t\t\tx1 = self.lin1[i](x1)\n\t\t\tout.append(self.chantojoints[i](x1))\n\t\t\tx1 = self.lin2[i](x1)\n\t\t\tx = x + x1 + self.jointstochan[i](out[i])\n\n\t\treturn (out)\n\n'"
models/StackedHourGlass.py,2,"b'import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport models.modules.StackedHourGlass as M\n\n\nclass myUpsample(nn.Module):\n\t def __init__(self):\n\t\t super(myUpsample, self).__init__()\n\t\t pass\n\t def forward(self, x):\n\t\t return x[:, :, :, None, :, None].expand(-1, -1, -1, 2, -1, 2).reshape(x.size(0), x.size(1), x.size(2)*2, x.size(3)*2)\n\n\nclass Hourglass(nn.Module):\n    """"""docstring for Hourglass""""""\n    def __init__(self, nChannels = 256, numReductions = 4, nModules = 2, poolKernel = (2,2), poolStride = (2,2), upSampleKernel = 2):\n        super(Hourglass, self).__init__()\n        self.numReductions = numReductions\n        self.nModules = nModules\n        self.nChannels = nChannels\n        self.poolKernel = poolKernel\n        self.poolStride = poolStride\n        self.upSampleKernel = upSampleKernel\n        """"""\n        For the skip connection, a residual module (or sequence of residuaql modules)\n        """"""\n\n        _skip = []\n        for _ in range(self.nModules):\n            _skip.append(M.Residual(self.nChannels, self.nChannels))\n\n        self.skip = nn.Sequential(*_skip)\n\n        """"""\n        First pooling to go to smaller dimension then pass input through\n        Residual Module or sequence of Modules then  and subsequent cases:\n            either pass through Hourglass of numReductions-1\n            or pass through M.Residual Module or sequence of Modules\n        """"""\n\n        self.mp = nn.MaxPool2d(self.poolKernel, self.poolStride)\n\n        _afterpool = []\n        for _ in range(self.nModules):\n            _afterpool.append(M.Residual(self.nChannels, self.nChannels))\n\n        self.afterpool = nn.Sequential(*_afterpool)\n\n        if (numReductions > 1):\n            self.hg = Hourglass(self.nChannels, self.numReductions-1, self.nModules, self.poolKernel, self.poolStride)\n        else:\n            _num1res = []\n            for _ in range(self.nModules):\n                _num1res.append(M.Residual(self.nChannels,self.nChannels))\n\n            self.num1res = nn.Sequential(*_num1res)  # doesnt seem that important ?\n\n        """"""\n        Now another M.Residual Module or sequence of M.Residual Modules\n        """"""\n\n        _lowres = []\n        for _ in range(self.nModules):\n            _lowres.append(M.Residual(self.nChannels,self.nChannels))\n\n        self.lowres = nn.Sequential(*_lowres)\n\n        """"""\n        Upsampling Layer (Can we change this??????)\n        As per Newell\'s paper upsamping recommended\n        """"""\n        self.up = myUpsample()#nn.Upsample(scale_factor = self.upSampleKernel)\n\n\n    def forward(self, x):\n        out1 = x\n        out1 = self.skip(out1)\n        out2 = x\n        out2 = self.mp(out2)\n        out2 = self.afterpool(out2)\n        if self.numReductions>1:\n            out2 = self.hg(out2)\n        else:\n            out2 = self.num1res(out2)\n        out2 = self.lowres(out2)\n        out2 = self.up(out2)\n\n        return out2 + out1\n\n\nclass StackedHourGlass(nn.Module):\n\t""""""docstring for StackedHourGlass""""""\n\tdef __init__(self, nChannels, nStack, nModules, numReductions, nJoints):\n\t\tsuper(StackedHourGlass, self).__init__()\n\t\tself.nChannels = nChannels\n\t\tself.nStack = nStack\n\t\tself.nModules = nModules\n\t\tself.numReductions = numReductions\n\t\tself.nJoints = nJoints\n\n\t\tself.start = M.BnReluConv(3, 64, kernelSize = 7, stride = 2, padding = 3)\n\n\t\tself.res1 = M.Residual(64, 128)\n\t\tself.mp = nn.MaxPool2d(2, 2)\n\t\tself.res2 = M.Residual(128, 128)\n\t\tself.res3 = M.Residual(128, self.nChannels)\n\n\t\t_hourglass, _Residual, _lin1, _chantojoints, _lin2, _jointstochan = [],[],[],[],[],[]\n\n\t\tfor _ in range(self.nStack):\n\t\t\t_hourglass.append(Hourglass(self.nChannels, self.numReductions, self.nModules))\n\t\t\t_ResidualModules = []\n\t\t\tfor _ in range(self.nModules):\n\t\t\t\t_ResidualModules.append(M.Residual(self.nChannels, self.nChannels))\n\t\t\t_ResidualModules = nn.Sequential(*_ResidualModules)\n\t\t\t_Residual.append(_ResidualModules)\n\t\t\t_lin1.append(M.BnReluConv(self.nChannels, self.nChannels))\n\t\t\t_chantojoints.append(nn.Conv2d(self.nChannels, self.nJoints,1))\n\t\t\t_lin2.append(nn.Conv2d(self.nChannels, self.nChannels,1))\n\t\t\t_jointstochan.append(nn.Conv2d(self.nJoints,self.nChannels,1))\n\n\t\tself.hourglass = nn.ModuleList(_hourglass)\n\t\tself.Residual = nn.ModuleList(_Residual)\n\t\tself.lin1 = nn.ModuleList(_lin1)\n\t\tself.chantojoints = nn.ModuleList(_chantojoints)\n\t\tself.lin2 = nn.ModuleList(_lin2)\n\t\tself.jointstochan = nn.ModuleList(_jointstochan)\n\n\tdef forward(self, x):\n\t\tx = self.start(x)\n\t\tx = self.res1(x)\n\t\tx = self.mp(x)\n\t\tx = self.res2(x)\n\t\tx = self.res3(x)\n\t\tout = []\n\n\t\tfor i in range(self.nStack):\n\t\t\tx1 = self.hourglass[i](x)\n\t\t\tx1 = self.Residual[i](x1)\n\t\t\tx1 = self.lin1[i](x1)\n\t\t\tout.append(self.chantojoints[i](x1))\n\t\t\tx1 = self.lin2[i](x1)\n\t\t\tx = x + x1 + self.jointstochan[i](out[i])\n\n\t\treturn (out)\n'"
models/__init__.py,0,b'from models.DeepPose import *\nfrom models.ChainedPredictions import *\nfrom models.StackedHourGlass import *\nfrom models.PoseAttention import *\nfrom models.PyraNet import *\n'
datasets/COCO/JointsDataset.py,3,"b'# ------------------------------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under the MIT License.\n# Written by Bin Xiao (Bin.Xiao@microsoft.com)\n# ------------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport logging\nimport random\n\nimport cv2\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\nfrom .utils.transforms import get_affine_transform\nfrom .utils.transforms import affine_transform\nfrom .utils.transforms import fliplr_joints\nfrom datasets.img import Rnd\n\nlogger = logging.getLogger(__name__)\nclass JointsDataset(Dataset):\n    def __init__(self, opts, root, image_set, is_train, transform=None):\n        self.num_joints = 0\n        self.pixel_std = 200\n        self.flip_pairs = []\n        self.parent_ids = []\n\n        self.is_train = is_train\n        self.root = root\n        self.image_set = image_set\n\n        self.output_path = None#opts.cocoOutputDir#cfg.OUTPUT_DIR\n        self.data_format = None#cfg.DATASET.DATA_FORMAT\n\n        self.scale_factor = opts.maxScale#cfg.DATASET.SCALE_FACTOR\n        self.rotation_factor = opts.maxRotate#cfg.DATASET.ROT_FACTOR\n        self.flip = True#opts.cocoFlipping#cfg.DATASET.FLIP\n\n        self.image_size = np.asarray((opts.inputRes, opts.inputRes))#cfg.MODEL.IMAGE_SIZE\n        self.target_type = opts.TargetType#cfg.MODEL.EXTRA.TARGET_TYPE\n        self.heatmap_size = np.asarray((opts.outputRes, opts.outputRes))#cfg.MODEL.EXTRA.HEATMAP_SIZE\n        self.sigma = opts.hmGauss#opts.cocoSigma#cfg.MODEL.EXTRA.SIGMA\n\n        self.transform = transform\n        self.db = []\n\n    def _get_db(self):\n        raise NotImplementedError\n\n    def evaluate(self, cfg, preds, output_dir, *args, **kwargs):\n        raise NotImplementedError\n\n    def __len__(self,):\n        return len(self.db)\n\n    def __getitem__(self, idx):\n        db_rec = copy.deepcopy(self.db[idx])\n\n        image_file = db_rec[\'image\']\n        filename = db_rec[\'filename\'] if \'filename\' in db_rec else \'\'\n        imgnum = db_rec[\'imgnum\'] if \'imgnum\' in db_rec else \'\'\n        #print(image_file)\n        if self.data_format == \'zip\':\n            from utils import zipreader\n            data_numpy = zipreader.imread(\n                image_file, cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n        else:\n            data_numpy = cv2.imread(\n                image_file, cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n\n        if data_numpy is None:\n            logger.error(\'=> fail to read {}\'.format(image_file))\n            raise ValueError(\'Fail to read {}\'.format(image_file))\n\n        joints = db_rec[\'joints_3d\']\n        joints_vis = db_rec[\'joints_3d_vis\']\n\n        c = db_rec[\'center\']\n        s = db_rec[\'scale\']\n        score = db_rec[\'score\'] if \'score\' in db_rec else 1\n        r = 0\n\n        if self.is_train:\n            sf = self.scale_factor\n            rf = self.rotation_factor\n            #s = s * np.clip(np.random.randn()*sf + 1, 1 - sf, 1 + sf)\n            #r = np.clip(np.random.randn()*rf, -rf*2, rf*2) \\\n            #    if random.random() <= 0.6 else 0\n            s =  s **Rnd(sf)\n            r = 0 if np.random.random() < 0.6 else Rnd(rf)\n\n            if self.flip and random.random() <= 0.5:\n                data_numpy = data_numpy[:, ::-1, :]\n                joints, joints_vis = fliplr_joints(\n                    joints, joints_vis, data_numpy.shape[1], self.flip_pairs)\n                c[0] = data_numpy.shape[1] - c[0] - 1\n\n        trans = get_affine_transform(c, s, r, self.image_size)\n        input = cv2.warpAffine(\n            data_numpy,\n            trans,\n            (int(self.image_size[0]), int(self.image_size[1])),\n            flags=cv2.INTER_LINEAR)\n\n        """"""\n        if self.is_train:\n            input[:,:,0] = np.clip(input[:,:,0] * (np.random.random() * (0.4) + 0.6), 0, 255)\n            input[:,:,1] = np.clip(input[:,:,1] * (np.random.random() * (0.4) + 0.6), 0, 255)\n            input[:,:,2] = np.clip(input[:,:,2] * (np.random.random() * (0.4) + 0.6), 0, 255)\n        """"""\n\n        if self.transform:\n            input = self.transform(input)\n\n        for i in range(self.num_joints):\n            if joints_vis[i, 0] > 0.0:\n                joints[i, 0:2] = affine_transform(joints[i, 0:2], trans)\n\n        target, target_weight = self.generate_target(joints, joints_vis)\n\n        target = torch.from_numpy(target)\n        target_weight = torch.from_numpy(target_weight)\n\n        meta = {\n            \'image\': image_file,\n            \'filename\': filename,\n            \'imgnum\': imgnum,\n            \'joints\': joints,\n            \'joints_vis\': joints_vis,\n            \'center\': c,\n            \'scale\': s,\n            \'rotation\': r,\n            \'score\': score\n        }\n\n        return input, target, target_weight, meta\n\n    def select_data(self, db):\n        db_selected = []\n        for rec in db:\n            num_vis = 0\n            joints_x = 0.0\n            joints_y = 0.0\n            for joint, joint_vis in zip(\n                    rec[\'joints_3d\'], rec[\'joints_3d_vis\']):\n                if joint_vis[0] <= 0:\n                    continue\n                num_vis += 1\n\n                joints_x += joint[0]\n                joints_y += joint[1]\n            if num_vis == 0:\n                continue\n\n            joints_x, joints_y = joints_x / num_vis, joints_y / num_vis\n\n            area = rec[\'scale\'][0] * rec[\'scale\'][1] * (self.pixel_std**2)\n            joints_center = np.array([joints_x, joints_y])\n            bbox_center = np.array(rec[\'center\'])\n            diff_norm2 = np.linalg.norm((joints_center-bbox_center), 2)\n            ks = np.exp(-1.0*(diff_norm2**2) / ((0.2)**2*2.0*area))\n\n            metric = (0.2 / 16) * num_vis + 0.45 - 0.2 / 16\n            if ks > metric:\n                db_selected.append(rec)\n\n        logger.info(\'=> num db: {}\'.format(len(db)))\n        logger.info(\'=> num selected db: {}\'.format(len(db_selected)))\n        return db_selected\n\n    def generate_target(self, joints, joints_vis):\n        \'\'\'\n        :param joints:  [num_joints, 3]\n        :param joints_vis: [num_joints, 3]\n        :return: target, target_weight(1: visible, 0: invisible)\n        \'\'\'\n        target_weight = np.ones((self.num_joints, 1), dtype=np.float32)\n        target_weight[:, 0] = joints_vis[:, 0]\n\n        # assert self.target_type == \'gaussian\', \\\n        #     \'Only support gaussian map now!\'\n        if self.target_type == \'heatmap\':\n            target = np.zeros((self.num_joints,\n                               self.heatmap_size[1],\n                               self.heatmap_size[0]),\n                              dtype=np.float32)\n\n            tmp_size = self.sigma * 3\n\n            for joint_id in range(self.num_joints):\n                feat_stride = self.image_size / self.heatmap_size\n                mu_x = int(joints[joint_id][0] / feat_stride[0] + 0.5)\n                mu_y = int(joints[joint_id][1] / feat_stride[1] + 0.5)\n                # Check that any part of the gaussian is in-bounds\n                #print(mu_x, mu_y)\n\n                ul = [int(mu_x - tmp_size), int(mu_y - tmp_size)]\n                br = [int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)]\n                if ul[0] >= self.heatmap_size[0] or ul[1] >= self.heatmap_size[1] \\\n                        or br[0] < 0 or br[1] < 0:\n                    # If not, just return the image as is\n                    target_weight[joint_id] = 0\n                    continue\n\n                # # Generate gaussian\n                size = 2 * tmp_size + 1\n                x = np.arange(0, size, 1, np.float32)\n                y = x[:, np.newaxis]\n                x0 = y0 = size // 2\n                # The gaussian is not normalized, we want the center value to equal 1\n                g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * self.sigma ** 2))\n\n                # Usable gaussian range\n                g_x = max(0, -ul[0]), min(br[0], self.heatmap_size[0]) - ul[0]\n                g_y = max(0, -ul[1]), min(br[1], self.heatmap_size[1]) - ul[1]\n                # Image range\n                img_x = max(0, ul[0]), min(br[0], self.heatmap_size[0])\n                img_y = max(0, ul[1]), min(br[1], self.heatmap_size[1])\n\n                v = target_weight[joint_id]\n                if v > 0.5:\n                    target[joint_id][img_y[0]:img_y[1], img_x[0]:img_x[1]] = \\\n                        g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n\n        elif self.target_type == \'direct\':\n                return joints[:,:2]/256, target_weight\n        return target, target_weight\n'"
datasets/COCO/__init__.py,0,b'# ------------------------------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under the MIT License.\n# Written by Bin Xiao (Bin.Xiao@microsoft.com)\n# ------------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom .coco import COCODataset as coco\n'
datasets/COCO/coco.py,0,"b'# ------------------------------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under the MIT License.\n# Written by Bin Xiao (Bin.Xiao@microsoft.com)\n# ------------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport os\nimport pickle\nfrom collections import defaultdict\nfrom collections import OrderedDict\n\nimport json_tricks as json\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\n# from dataset.JointsDataset import JointsDataset\nfrom .JointsDataset import JointsDataset\nfrom .nms.nms import oks_nms\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass COCODataset(JointsDataset):\n    \'\'\'\n    ""keypoints"": {\n        0: ""nose"",\n        1: ""left_eye"",\n        2: ""right_eye"",\n        3: ""left_ear"",\n        4: ""right_ear"",\n        5: ""left_shoulder"",\n        6: ""right_shoulder"",\n        7: ""left_elbow"",\n        8: ""right_elbow"",\n        9: ""left_wrist"",\n        10: ""right_wrist"",\n        11: ""left_hip"",\n        12: ""right_hip"",\n        13: ""left_knee"",\n        14: ""right_knee"",\n        15: ""left_ankle"",\n        16: ""right_ankle""\n    },\n\t""skeleton"": [\n        [16,14],[14,12],[17,15],[15,13],[12,13],[6,12],[7,13], [6,7],[6,8],\n        [7,9],[8,10],[9,11],[2,3],[1,2],[1,3],[2,4],[3,5],[4,6],[5,7]]\n    \'\'\'\n    def __init__(self, opts, root, image_set, is_train, transform=None):\n        super().__init__(opts, root, image_set, is_train, transform)\n        self.nms_thre = 1.0#cfg.TEST.NMS_THRE\n        self.image_thre = 0.0#cfg.TEST.IMAGE_THRE\n        self.oks_thre = 0.9#cfg.TEST.OKS_THRE\n        self.in_vis_thre = 0.2#cfg.TEST.IN_VIS_THRE\n        self.bbox_file = opts.dataDir + \'/person_detection_resuts/COCO_val2017_detections_AP_H_56_person.json\'#cfg.TEST.COCO_BBOX_FILE\n        self.use_gt_bbox = True#cfg.TEST.USE_GT_BBOX\n        self.image_width = 256#cfg.MODEL.IMAGE_SIZE[0]\n        self.image_height = 256#cfg.MODEL.IMAGE_SIZE[1]\n        self.aspect_ratio = self.image_width * 1.0 / self.image_height\n        self.pixel_std = 200\n        self.coco = COCO(self._get_ann_file_keypoint())\n\n        # deal with class names\n        cats = [cat[\'name\']\n                for cat in self.coco.loadCats(self.coco.getCatIds())]\n        self.classes = [\'__background__\'] + cats\n        logger.info(\'=> classes: {}\'.format(self.classes))\n        self.num_classes = len(self.classes)\n        self._class_to_ind = dict(zip(self.classes, range(self.num_classes)))\n        self._class_to_coco_ind = dict(zip(cats, self.coco.getCatIds()))\n        self._coco_ind_to_class_ind = dict([(self._class_to_coco_ind[cls],\n                                             self._class_to_ind[cls])\n                                            for cls in self.classes[1:]])\n\n        # load image file names\n        self.image_set_index = self._load_image_set_index()\n        self.num_images = len(self.image_set_index)\n        logger.info(\'=> num_images: {}\'.format(self.num_images))\n\n        self.num_joints = 17\n        self.flip_pairs = [[1, 2], [3, 4], [5, 6], [7, 8],\n                           [9, 10], [11, 12], [13, 14], [15, 16]]\n        self.parent_ids = None\n\n        self.db = self._get_db()\n\n        if is_train and None:#cfg.DATASET.SELECT_DATA:\n            self.db = self.select_data(self.db)\n\n        logger.info(\'=> load {} samples\'.format(len(self.db)))\n\n    def _get_ann_file_keypoint(self):\n        """""" self.root / annotations / person_keypoints_train2017.json """"""\n        prefix = \'person_keypoints\' \\\n            if \'test\' not in self.image_set else \'image_info\'\n        return os.path.join(self.root, \'annotations\',\n                            prefix + \'_\' + self.image_set + \'.json\')\n\n    def _load_image_set_index(self):\n        """""" image id: int """"""\n        image_ids = self.coco.getImgIds()\n        return image_ids\n\n    def _get_db(self):\n        if self.is_train or self.use_gt_bbox:\n            # use ground truth bbox\n            gt_db = self._load_coco_keypoint_annotations()\n        else:\n            # use bbox from detection\n            gt_db = self._load_coco_person_detection_results()\n        return gt_db\n\n    def _load_coco_keypoint_annotations(self):\n        """""" ground truth bbox and keypoints """"""\n        gt_db = []\n        for index in self.image_set_index:\n            gt_db.extend(self._load_coco_keypoint_annotation_kernal(index))\n        return gt_db\n\n    def _load_coco_keypoint_annotation_kernal(self, index):\n        """"""\n        coco ann: [u\'segmentation\', u\'area\', u\'iscrowd\', u\'image_id\', u\'bbox\', u\'category_id\', u\'id\']\n        iscrowd:\n            crowd instances are handled by marking their overlaps with all categories to -1\n            and later excluded in training\n        bbox:\n            [x1, y1, w, h]\n        :param index: coco image id\n        :return: db entry\n        """"""\n        im_ann = self.coco.loadImgs(index)[0]\n        width = im_ann[\'width\']\n        height = im_ann[\'height\']\n\n        annIds = self.coco.getAnnIds(imgIds=index, iscrowd=False)\n        objs = self.coco.loadAnns(annIds)\n\n        # sanitize bboxes\n        valid_objs = []\n        for obj in objs:\n            x, y, w, h = obj[\'bbox\']\n            x1 = np.max((0, x))\n            y1 = np.max((0, y))\n            x2 = np.min((width - 1, x1 + np.max((0, w - 1))))\n            y2 = np.min((height - 1, y1 + np.max((0, h - 1))))\n            if obj[\'area\'] > 0 and x2 >= x1 and y2 >= y1:\n                # obj[\'clean_bbox\'] = [x1, y1, x2, y2]\n                obj[\'clean_bbox\'] = [x1, y1, x2-x1, y2-y1]\n                valid_objs.append(obj)\n        objs = valid_objs\n\n        rec = []\n        for obj in objs:\n            cls = self._coco_ind_to_class_ind[obj[\'category_id\']]\n            if cls != 1:\n                continue\n\n            # ignore objs without keypoints annotation\n            if max(obj[\'keypoints\']) == 0:\n                continue\n\n            joints_3d = np.zeros((self.num_joints, 3), dtype=np.float)\n            joints_3d_vis = np.zeros((self.num_joints, 3), dtype=np.float)\n            for ipt in range(self.num_joints):\n                joints_3d[ipt, 0] = obj[\'keypoints\'][ipt * 3 + 0]\n                joints_3d[ipt, 1] = obj[\'keypoints\'][ipt * 3 + 1]\n                joints_3d[ipt, 2] = 0\n                t_vis = obj[\'keypoints\'][ipt * 3 + 2]\n                if t_vis > 1:\n                    t_vis = 1\n                joints_3d_vis[ipt, 0] = t_vis\n                joints_3d_vis[ipt, 1] = t_vis\n                joints_3d_vis[ipt, 2] = 0\n\n            center, scale = self._box2cs(obj[\'clean_bbox\'][:4])\n            rec.append({\n                \'image\': self.image_path_from_index(index),\n                \'center\': center,\n                \'scale\': scale,\n                \'joints_3d\': joints_3d,\n                \'joints_3d_vis\': joints_3d_vis,\n                \'filename\': \'\',\n                \'imgnum\': 0,\n            })\n\n        return rec\n\n    def _box2cs(self, box):\n        x, y, w, h = box[:4]\n        return self._xywh2cs(x, y, w, h)\n\n    def _xywh2cs(self, x, y, w, h):\n        center = np.zeros((2), dtype=np.float32)\n        center[0] = x + w * 0.5\n        center[1] = y + h * 0.5\n\n        if w > self.aspect_ratio * h:\n            h = w * 1.0 / self.aspect_ratio\n        elif w < self.aspect_ratio * h:\n            w = h * self.aspect_ratio\n        scale = np.array(\n            [w * 1.0 / self.pixel_std, h * 1.0 / self.pixel_std],\n            dtype=np.float32)\n        if center[0] != -1:\n            scale = scale * 1.25\n\n        return center, scale\n\n    def image_path_from_index(self, index):\n        """""" example: images / train2017 / 000000119993.jpg """"""\n        file_name = \'%012d.jpg\' % index\n        if \'2014\' in self.image_set:\n            file_name = \'COCO_%s_\' % self.image_set + file_name\n\n        prefix = \'test2017\' if \'test\' in self.image_set else self.image_set\n\n        data_name = prefix + \'.zip@\' if self.data_format == \'zip\' else prefix\n\n        image_path = os.path.join(\n            self.root, \'images\', data_name, file_name)\n\n        return image_path\n\n    def _load_coco_person_detection_results(self):\n        all_boxes = None\n        with open(self.bbox_file, \'r\') as f:\n            all_boxes = json.load(f)\n\n        if not all_boxes:\n            logger.error(\'=> Load %s fail!\' % self.bbox_file)\n            return None\n\n        logger.info(\'=> Total boxes: {}\'.format(len(all_boxes)))\n\n        kpt_db = []\n        num_boxes = 0\n        for n_img in range(0, len(all_boxes)):\n            det_res = all_boxes[n_img]\n            if det_res[\'category_id\'] != 1:\n                continue\n            img_name = self.image_path_from_index(det_res[\'image_id\'])\n            box = det_res[\'bbox\']\n            score = det_res[\'score\']\n\n            if score < self.image_thre:\n                continue\n\n            num_boxes = num_boxes + 1\n\n            center, scale = self._box2cs(box)\n            joints_3d = np.zeros((self.num_joints, 3), dtype=np.float)\n            joints_3d_vis = np.ones(\n                (self.num_joints, 3), dtype=np.float)\n            kpt_db.append({\n                \'image\': img_name,\n                \'center\': center,\n                \'scale\': scale,\n                \'score\': score,\n                \'joints_3d\': joints_3d,\n                \'joints_3d_vis\': joints_3d_vis,\n            })\n\n        logger.info(\'=> Total boxes after fliter low score@{}: {}\'.format(\n            self.image_thre, num_boxes))\n        return kpt_db\n\n    # need double check this API and classes field\n    def evaluate(self, preds, output_dir, all_boxes, img_path,\n                 *args, **kwargs):\n        res_folder = os.path.join(output_dir, \'results\')\n        if not os.path.exists(res_folder):\n            os.makedirs(res_folder)\n        res_file = os.path.join(\n            res_folder, \'keypoints_%s_results.json\' % self.image_set)\n\n        # person x (keypoints)\n        _kpts = []\n        for idx, kpt in enumerate(preds):\n            _kpts.append({\n                \'keypoints\': kpt,\n                \'center\': all_boxes[idx][0:2],\n                \'scale\': all_boxes[idx][2:4],\n                \'area\': all_boxes[idx][4],\n                \'score\': all_boxes[idx][5],\n                \'image\': int(img_path[idx][-16:-4])\n            })\n        # image x person x (keypoints)\n        kpts = defaultdict(list)\n        for kpt in _kpts:\n            kpts[kpt[\'image\']].append(kpt)\n\n        # rescoring and oks nms\n        num_joints = self.num_joints\n        in_vis_thre = self.in_vis_thre\n        oks_thre = self.oks_thre\n        oks_nmsed_kpts = []\n        for img in kpts.keys():\n            img_kpts = kpts[img]\n            for n_p in img_kpts:\n                box_score = n_p[\'score\']\n                kpt_score = 0\n                valid_num = 0\n                for n_jt in range(0, num_joints):\n                    t_s = n_p[\'keypoints\'][n_jt][2]\n                    if t_s > in_vis_thre:\n                        kpt_score = kpt_score + t_s\n                        valid_num = valid_num + 1\n                if valid_num != 0:\n                    kpt_score = kpt_score / valid_num\n                # rescoring\n                n_p[\'score\'] = kpt_score * box_score\n            keep = oks_nms([img_kpts[i] for i in range(len(img_kpts))],\n                           oks_thre)\n            if len(keep) == 0:\n                oks_nmsed_kpts.append(img_kpts)\n            else:\n                oks_nmsed_kpts.append([img_kpts[_keep] for _keep in keep])\n\n        self._write_coco_keypoint_results(\n            oks_nmsed_kpts, res_file)\n        if \'test\' not in self.image_set:\n            info_str = self._do_python_keypoint_eval(\n                res_file, res_folder)\n            name_value = OrderedDict(info_str)\n            return name_value, name_value[\'AP\']\n        else:\n            return {\'Null\': 0}, 0\n\n    def _write_coco_keypoint_results(self, keypoints, res_file):\n        data_pack = [{\'cat_id\': self._class_to_coco_ind[cls],\n                      \'cls_ind\': cls_ind,\n                      \'cls\': cls,\n                      \'ann_type\': \'keypoints\',\n                      \'keypoints\': keypoints\n                      }\n                     for cls_ind, cls in enumerate(self.classes) if not cls == \'__background__\']\n\n        results = self._coco_keypoint_results_one_category_kernel(data_pack[0])\n        logger.info(\'=> Writing results json to %s\' % res_file)\n        with open(res_file, \'w\') as f:\n            json.dump(results, f, sort_keys=True, indent=4)\n        try:\n            json.load(open(res_file))\n        except Exception:\n            content = []\n            with open(res_file, \'r\') as f:\n                for line in f:\n                    content.append(line)\n            content[-1] = \']\'\n            with open(res_file, \'w\') as f:\n                for c in content:\n                    f.write(c)\n\n    def _coco_keypoint_results_one_category_kernel(self, data_pack):\n        cat_id = data_pack[\'cat_id\']\n        keypoints = data_pack[\'keypoints\']\n        cat_results = []\n\n        for img_kpts in keypoints:\n            if len(img_kpts) == 0:\n                continue\n\n            _key_points = np.array([img_kpts[k][\'keypoints\']\n                                    for k in range(len(img_kpts))])\n            key_points = np.zeros(\n                (_key_points.shape[0], self.num_joints * 3), dtype=np.float)\n\n            for ipt in range(self.num_joints):\n                key_points[:, ipt * 3 + 0] = _key_points[:, ipt, 0]\n                key_points[:, ipt * 3 + 1] = _key_points[:, ipt, 1]\n                key_points[:, ipt * 3 + 2] = _key_points[:, ipt, 2]  # keypoints score.\n\n            result = [{\'image_id\': img_kpts[k][\'image\'],\n                       \'category_id\': cat_id,\n                       \'keypoints\': list(key_points[k]),\n                       \'score\': img_kpts[k][\'score\'],\n                       \'center\': list(img_kpts[k][\'center\']),\n                       \'scale\': list(img_kpts[k][\'scale\'])\n                       } for k in range(len(img_kpts))]\n            cat_results.extend(result)\n\n        return cat_results\n\n    def _do_python_keypoint_eval(self, res_file, res_folder):\n        coco_dt = self.coco.loadRes(res_file)\n        coco_eval = COCOeval(self.coco, coco_dt, \'keypoints\')\n        coco_eval.params.useSegm = None\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        coco_eval.summarize()\n        stats_names = [\'AP\', \'Ap .5\', \'AP .75\', \'AP (M)\', \'AP (L)\', \'AR\', \'AR .5\', \'AR .75\', \'AR (M)\', \'AR (L)\']\n\n        info_str = []\n        for ind, name in enumerate(stats_names):\n            info_str.append((name, coco_eval.stats[ind]))\n\n        eval_file = os.path.join(\n            res_folder, \'keypoints_%s_results.pkl\' % self.image_set)\n\n        with open(eval_file, \'wb\') as f:\n            pickle.dump(coco_eval, f, pickle.HIGHEST_PROTOCOL)\n        logger.info(\'=> coco eval results saved to %s\' % eval_file)\n\n        return info_str\n'"
datasets/COCO/trial.py,0,"b'import coco as c\nimport torchvision.transforms as transforms\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\ndset = c.COCODataset(None, \'/home/anurag/datasets/coco\', \'train2017\', True, transforms.Compose([\n            transforms.ToTensor(),\n            #normalize,\n        ]))\n\ndef f(n):\n\ta = dset.__getitem__(n)\n\tdraw_heatmaps(a[1].numpy(), a[0].numpy(), str(n))\n\ndef draw_heatmaps(heatmaps, image, index):\n    img = image\n    #print(img.max(), img.min(), img.std(), img.mean())\n    img = np.array(255*img.transpose(1, 2, 0), dtype = np.uint8)\n    #img = cv2.resize(img, (heatmaps.shape[1], heatmaps.shape[1]))\n    #print(img.shape, img.max(), img.min(), img.mean(), img.std())\n    #print(img.shape)\n    #print(heatmaps.shape[0])\n    for i in range(heatmaps.shape[0]):\n        #current = cv2.applyColorMap(heatmaps[i, :, :], cv2.COLORMAP_JET)\n        current = heatmaps[i, :, :]\n        current = cv2.resize(current, (img.shape[0], img.shape[1]))\n        #print(current.shape)\n        #print(current.mean())\n        #print(current.std())\n        #print(img.max())\n        plt.imshow(img)\n        plt.imshow(current, alpha = 0.5)\n        plt.savefig(\'debug/\' + str(index) + \'_\' + str(i) + \'.png\')\n    print(""saved"", str(index))\n\nif __name__ == ""__main__"":\n\tprint(dset.__len__)\n\tf(1205)\n\tf(118000)'"
models/modules/ChainedPredictions.py,2,"b'import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Identity(nn.Module):\n\t\t""""""docstring for Identity""""""\n\t\tdef __init__(self):\n\t\t\t\tsuper(Identity, self).__init__()\n\n\t\tdef forward(self, x):\n\t\t\t\treturn x\n\nclass Deception(nn.Module):\n\t\t""""""docstring for Deception""""""\n\t\tdef __init__(self, hiddenChans):\n\t\t\t\tsuper(Deception, self).__init__()\n\t\t\t\tself.hiddenChans = hiddenChans\n\n\t\t\t\t_stack1 = []\n\t\t\t\t_stack2 = []\n\t\t\t\t_stack3 = []\n\n\t\t\t\tself.start = nn.Conv2d(self.hiddenChans, 32, 1)\n\n\t\t\t\t_stack1.append(nn.ConvTranspose2d(32, 32, 2, 2, 0))\n\t\t\t\t_stack1.append(nn.ConvTranspose2d(32, 32, 4, 2, 1))\n\t\t\t\t_stack1.append(nn.ConvTranspose2d(32, 32, 6, 2, 2))\n\t\t\t\t_stack1.append(nn.BatchNorm2d(32))\n\t\t\t\tself.stack1 = nn.ModuleList(_stack1)\n\n\t\t\t\t_stack2.append(nn.ConvTranspose2d(32, 32, 2, 2, 0))\n\t\t\t\t_stack2.append(nn.ConvTranspose2d(32, 32, 4, 2, 1))\n\t\t\t\t_stack2.append(nn.ConvTranspose2d(32, 32, 6, 2, 2))\n\t\t\t\t_stack2.append(nn.BatchNorm2d(32))\n\t\t\t\tself.stack2 = nn.ModuleList(_stack2)\n\n\t\t\t\tself.end = nn.Conv2d(32, 1, 3, 1, 1)\n\n\t\tdef forward(self, x):\n\t\t\t\tx = self.start(x)\n\t\t\t\tx = self.stack1[0](x) + self.stack1[1](x) + self.stack1[2](x)\n\t\t\t\tx = self.stack2[0](x) + self.stack2[1](x) + self.stack2[2](x)\n\t\t\t\tx = self.end(x)\n\t\t\t\treturn x\n\n\n'"
models/modules/DeepPose.py,2,b'import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F'
models/modules/PoseAttention.py,4,"b'import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F \n\nclass BnReluConv(nn.Module):\n\t""""""docstring for BnReluConv""""""\n\tdef __init__(self, inChannels, outChannels, kernelSize = 1, stride = 1, padding = 0):\n\t\tsuper(BnReluConv, self).__init__()\n\t\tself.inChannels = inChannels\n\t\tself.outChannels = outChannels\n\t\tself.kernelSize = kernelSize\n\t\tself.stride = stride\n\t\tself.padding = padding\n\n\t\tself.bn = nn.BatchNorm2d(self.inChannels)\n\t\tself.conv = nn.Conv2d(self.inChannels, self.outChannels, self.kernelSize, self.stride, self.padding)\n\t\tself.relu = nn.ReLU()\n\n\tdef forward(self, x):\n\t\tx = self.bn(x)\n\t\tx = self.relu(x)\n\t\tx = self.conv(x)\n\t\treturn x\n\nclass BnReluPoolConv(nn.Module):\n\t\t""""""docstring for BnReluPoolConv""""""\n\t\tdef __init__(self, inChannels, outChannels, kernelSize = 1, stride = 1, padding = 0):\n\t\t\tsuper(BnReluPoolConv, self).__init__()\n\t\t\tself.inChannels = inChannels\n\t\t\tself.outChannels = outChannels\n\t\t\tself.kernelSize = kernelSize\n\t\t\tself.stride = stride\n\t\t\tself.padding = padding\n\n\t\t\tself.bn = nn.BatchNorm2d(self.inChannels)\n\t\t\tself.conv = nn.Conv2d(self.inChannels, self.outChannels, self.kernelSize, self.stride, self.padding)\n\t\t\tself.relu = nn.ReLU()\n\n\t\tdef forward(self, x):\n\t\t\tx = self.bn(x)\n\t\t\tx = self.relu(x)\n\t\t\tx = F.max_pool2d(x, kernel_size=2, stride=2)\n\t\t\tx = self.conv(x)\n\t\t\treturn x\n\nclass ConvBlock(nn.Module):\n\t""""""docstring for ConvBlock""""""\n\tdef __init__(self, inChannels, outChannels):\n\t\tsuper(ConvBlock, self).__init__()\n\t\tself.inChannels = inChannels\n\t\tself.outChannels = outChannels\n\t\tself.outChannelsby2 = outChannels//2\n\n\t\tself.brc1 = BnReluConv(self.inChannels, self.outChannelsby2, 1, 1, 0)\n\t\tself.brc2 = BnReluConv(self.outChannelsby2, self.outChannelsby2, 3, 1, 1)\n\t\tself.brc3 = BnReluConv(self.outChannelsby2, self.outChannels, 1, 1, 0)\n\n\tdef forward(self, x):\n\t\tx = self.brc1(x)\n\t\tx = self.brc2(x)\n\t\tx = self.brc3(x)\n\t\treturn x\n\nclass PoolConvBlock(nn.Module):\n\t""""""docstring for PoolConvBlock""""""\n\tdef __init__(self, inChannels, outChannels):\n\t\tsuper(PoolConvBlock, self).__init__()\n\t\tself.inChannels = inChannels\n\t\tself.outChannels = outChannels\n\n\t\tself.brpc = BnReluPoolConv(self.inChannels, self.outChannels, 3, 1, 1)\n\t\tself.brc = BnReluConv(self.outChannels, self.outChannels, 3, 1, 1)\n\n\tdef forward(self, x):\n\t\tx = self.brpc(x)\n\t\tx = self.brc(x)\n\t\tx = F.interpolate(x, scale_factor=2)\n\t\treturn x\n\nclass SkipLayer(nn.Module):\n\t""""""docstring for SkipLayer""""""\n\tdef __init__(self, inChannels, outChannels):\n\t\tsuper(SkipLayer, self).__init__()\n\t\tself.inChannels = inChannels\n\t\tself.outChannels = outChannels\n\t\tif (self.inChannels == self.outChannels):\n\t\t\tself.conv = None\n\t\telse:\n\t\t\tself.conv = nn.Conv2d(self.inChannels, self.outChannels, 1)\n\n\tdef forward(self, x):\n\t\tif self.conv is not None:\n\t\t\tx = self.conv(x)\n\t\treturn x\n\nclass Residual(nn.Module):\n\t""""""docstring for Residual""""""\n\tdef __init__(self, inChannels, outChannels):\n\t\tsuper(Residual, self).__init__()\n\t\tself.inChannels = inChannels\n\t\tself.outChannels = outChannels\n\t\tself.cb = ConvBlock(inChannels, outChannels)\n\t\tself.skip = SkipLayer(inChannels, outChannels)\n\n\tdef forward(self, x):\n\t\tout = 0\n\t\tout = out + self.cb(x)\n\t\tout = out + self.skip(x)\n\t\treturn out\n\nclass HourGlassResidual(nn.Module):\n\t""""""docstring for HourGlassResidual""""""\n\tdef __init__(self, inChannels, outChannels):\n\t\tsuper(HourGlassResidual, self).__init__()\n\t\tself.inChannels = inChannels\n\t\tself.outChannels = outChannels\n\t\tself.cb = ConvBlock(inChannels, outChannels)\n\t\tself.pcb = PoolConvBlock(inChannels, outChannels)\n\t\tself.skip = SkipLayer(inChannels, outChannels)\n\n\n\tdef forward(self, x):\n\t\tout = 0\n\t\tout = out + self.cb(x)\n\t\tout = out + self.pcb(x)\n\t\tout = out + self.skip(x)\n\t\treturn out\n\nclass AttentionIter(nn.Module):\n\t""""""docstring for AttentionIter""""""\n\tdef __init__(self, nChannels, LRNSize, IterSize):\n\t\tsuper(AttentionIter, self).__init__()\n\t\tself.nChannels = nChannels\n\t\tself.LRNSize = LRNSize\n\t\tself.IterSize = IterSize\n\t\tself.bn = nn.BatchNorm2d(self.nChannels)\n\t\tself.U = nn.Conv2d(self.nChannels, 1, 3, 1, 1)\n\t\t# self.spConv = nn.Conv2d(1, 1, self.LRNSize, 1, self.LRNSize//2)\n\t\t# self.spConvclone = nn.Conv2d(1, 1, self.LRNSize, 1, self.LRNSize//2)\n\t\t# self.spConvclone.load_state_dict(self.spConv.state_dict())\n\t\t_spConv_ = nn.Conv2d(1, 1, self.LRNSize, 1, self.LRNSize//2)\n\t\t_spConv = []\n\t\tfor i in range(self.IterSize):\n\t\t\t_temp_ = nn.Conv2d(1, 1, self.LRNSize, 1, self.LRNSize//2)\n\t\t\t_temp_.load_state_dict(_spConv_.state_dict())\n\t\t\t_spConv.append(nn.BatchNorm2d(1))\n\t\t\t_spConv.append(_temp_)\n\t\tself.spConv = nn.ModuleList(_spConv)\n\n\tdef forward(self, x):\n\t\tx = self.bn(x)\n\t\tu = self.U(x)\n\t\tout = u\n\t\tfor i in range(self.IterSize):\n\t\t\t# if (i==1):\n\t\t\t# \tout = self.spConv(out)\n\t\t\t# else:\n\t\t\t# \tout = self.spConvclone(out)\n\t\t\tout = self.spConv[2*i](out)\n\t\t\tout = self.spConv[2*i+1](out)\n\t\t\tout = u + torch.sigmoid(out)\n\t\treturn (x * out.expand_as(x))\n\nclass AttentionPartsCRF(nn.Module):\n\t""""""docstring for AttentionPartsCRF""""""\n\tdef __init__(self, nChannels, LRNSize, IterSize, nJoints):\n\t\tsuper(AttentionPartsCRF, self).__init__()\n\t\tself.nChannels = nChannels\n\t\tself.LRNSize = LRNSize\n\t\tself.IterSize = IterSize\n\t\tself.nJoints = nJoints\n\t\t_S = []\n\t\tfor _ in range(self.nJoints):\n\t\t\t_S_ = []\n\t\t\t_S_.append(AttentionIter(self.nChannels, self.LRNSize, self.IterSize))\n\t\t\t_S_.append(nn.BatchNorm2d(self.nChannels))\n\t\t\t_S_.append(nn.Conv2d(self.nChannels, 1, 1, 1, 0))\n\t\t\t_S.append(nn.Sequential(*_S_))\n\t\tself.S = nn.ModuleList(_S)\n\n\tdef forward(self, x):\n\t\tout = []\n\t\tfor i in range(self.nJoints):\n\t\t\t#out.append(self.S[i](self.attiter(x)))\n\t\t\tout.append(self.S[i](x))\n\t\treturn torch.cat(out, 1)'"
models/modules/PyraNet.py,3,"b'import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BnReluConv(nn.Module):\n\t""""""docstring for BnReluConv""""""\n\tdef __init__(self, inChannels, outChannels, kernelSize = 1, stride = 1, padding = 0):\n\t\tsuper(BnReluConv, self).__init__()\n\t\tself.inChannels = inChannels\n\t\tself.outChannels = outChannels\n\t\tself.kernelSize = kernelSize\n\t\tself.stride = stride\n\t\tself.padding = padding\n\n\t\tself.bn = nn.BatchNorm2d(self.inChannels)\n\t\tself.relu = nn.ReLU()\n\t\tself.conv = nn.Conv2d(self.inChannels, self.outChannels, self.kernelSize, self.stride, self.padding)\n\n\tdef forward(self, x):\n\t\tx = self.bn(x)\n\t\tx = self.relu(x)\n\t\tx = self.conv(x)\n\t\treturn x\n\nclass Pyramid(nn.Module):\n\t""""""docstring for Pyramid""""""\n\tdef __init__(self, D, cardinality, inputRes):\n\t\tsuper(Pyramid, self).__init__()\n\t\tself.D = D\n\t\tself.cardinality = cardinality\n\t\tself.inputRes = inputRes\n\t\tself.scale = 2**(-1/self.cardinality)\n\t\t_scales = []\n\t\tfor card in range(self.cardinality):\n\t\t\ttemp = nn.Sequential(\n\t\t\t\t\tnn.FractionalMaxPool2d(2, output_ratio = self.scale**(card + 1)),\n\t\t\t\t\tnn.Conv2d(self.D, self.D, 3, 1, 1),\n\t\t\t\t\tnn.Upsample(size = self.inputRes)#, mode=\'bilinear\')\n\t\t\t\t)\n\t\t\t_scales.append(temp)\n\t\tself.scales = nn.ModuleList(_scales)\n\n\tdef forward(self, x):\n\t\t#print(x.shape, self.inputRes)\n\t\tout = torch.zeros_like(x)\n\t\tfor card in range(self.cardinality):\n\t\t\tout += self.scales[card](x)\n\t\treturn out\n\nclass BnReluPyra(nn.Module):\n\t""""""docstring for BnReluPyra""""""\n\tdef __init__(self, D, cardinality, inputRes):\n\t\tsuper(BnReluPyra, self).__init__()\n\t\tself.D = D\n\t\tself.cardinality = cardinality\n\t\tself.inputRes = inputRes\n\t\tself.bn = nn.BatchNorm2d(self.D)\n\t\tself.relu = nn.ReLU()\n\t\tself.pyra = Pyramid(self.D, self.cardinality, self.inputRes)\n\n\tdef forward(self, x):\n\t\tx = self.bn(x)\n\t\tx = self.relu(x)\n\t\tx = self.pyra(x)\n\t\treturn x\n\n\nclass ConvBlock(nn.Module):\n\t""""""docstring for ConvBlock""""""\n\tdef __init__(self, inChannels, outChannels):\n\t\tsuper(ConvBlock, self).__init__()\n\t\tself.inChannels = inChannels\n\t\tself.outChannels = outChannels\n\t\tself.outChannelsby2 = outChannels//2\n\n\t\tself.cbr1 = BnReluConv(self.inChannels, self.outChannelsby2, 1, 1, 0)\n\t\tself.cbr2 = BnReluConv(self.outChannelsby2, self.outChannelsby2, 3, 1, 1)\n\t\tself.cbr3 = BnReluConv(self.outChannelsby2, self.outChannels, 1, 1, 0)\n\n\tdef forward(self, x):\n\t\tx = self.cbr1(x)\n\t\tx = self.cbr2(x)\n\t\tx = self.cbr3(x)\n\t\treturn x\n\nclass PyraConvBlock(nn.Module):\n\t""""""docstring for PyraConvBlock""""""\n\tdef __init__(self, inChannels, outChannels, inputRes, baseWidth, cardinality, type = 1):\n\t\tsuper(PyraConvBlock, self).__init__()\n\t\tself.inChannels = inChannels\n\t\tself.outChannels = outChannels\n\t\tself.inputRes = inputRes\n\t\tself.baseWidth = baseWidth\n\t\tself.cardinality = cardinality\n\t\tself.outChannelsby2 = outChannels//2\n\t\tself.D = self.outChannels // self.baseWidth\n\t\tself.branch1 = nn.Sequential(\n\t\t\t\tBnReluConv(self.inChannels, self.outChannelsby2, 1, 1, 0),\n\t\t\t\tBnReluConv(self.outChannelsby2, self.outChannelsby2, 3, 1, 1)\n\t\t\t)\n\t\tself.branch2 = nn.Sequential(\n\t\t\t\tBnReluConv(self.inChannels, self.D, 1, 1, 0),\n\t\t\t\tBnReluPyra(self.D, self.cardinality, self.inputRes),\n\t\t\t\tBnReluConv(self.D, self.outChannelsby2, 1, 1, 0)\n\t\t\t)\n\t\tself.afteradd = BnReluConv(self.outChannelsby2, self.outChannels, 1, 1, 0)\n\n\tdef forward(self, x):\n\t\tx = self.branch2(x) + self.branch1(x)\n\t\tx = self.afteradd(x)\n\t\treturn x\n\nclass SkipLayer(nn.Module):\n\t""""""docstring for SkipLayer""""""\n\tdef __init__(self, inChannels, outChannels):\n\t\tsuper(SkipLayer, self).__init__()\n\t\tself.inChannels = inChannels\n\t\tself.outChannels = outChannels\n\t\tif (self.inChannels == self.outChannels):\n\t\t\tself.conv = None\n\t\telse:\n\t\t\tself.conv = nn.Conv2d(self.inChannels, self.outChannels, 1)\n\n\tdef forward(self, x):\n\t\tif self.conv is not None:\n\t\t\tx = self.conv(x)\n\t\treturn x\n\nclass Residual(nn.Module):\n\t""""""docstring for Residual""""""\n\tdef __init__(self, inChannels, outChannels, inputRes=None, baseWidth=None, cardinality=None, type=None):\n\t\tsuper(Residual, self).__init__()\n\t\tself.inChannels = inChannels\n\t\tself.outChannels = outChannels\n\t\tself.cb = ConvBlock(self.inChannels, self.outChannels)\n\t\tself.skip = SkipLayer(self.inChannels, self.outChannels)\n\n\tdef forward(self, x):\n\t\tout = 0\n\t\tout = out + self.cb(x)\n\t\tout = out + self.skip(x)\n\t\treturn out\n\nclass ResidualPyramid(nn.Module):\n\t""""""docstring for ResidualPyramid""""""\n\tdef __init__(self, inChannels, outChannels, inputRes, baseWidth, cardinality, type = 1):\n\t\tsuper(ResidualPyramid, self).__init__()\n\t\tself.inChannels = inChannels\n\t\tself.outChannels = outChannels\n\t\tself.inputRes = inputRes\n\t\tself.baseWidth = baseWidth\n\t\tself.cardinality = cardinality\n\t\tself.type = type\n\t\tself.cb = PyraConvBlock(self.inChannels, self.outChannels, self.inputRes, self.baseWidth, self.cardinality, self.type)\n\t\tself.skip = SkipLayer(self.inChannels, self.outChannels)\n\n\tdef forward(self, x):\n\t\tout = 0\n\t\tout = out + self.cb(x)\n\t\tout = out + self.skip(x)\n\t\treturn out\n'"
models/modules/StackedHourGlass.py,2,"b'import torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BnReluConv(nn.Module):\n\t\t""""""docstring for BnReluConv""""""\n\t\tdef __init__(self, inChannels, outChannels, kernelSize = 1, stride = 1, padding = 0):\n\t\t\t\tsuper(BnReluConv, self).__init__()\n\t\t\t\tself.inChannels = inChannels\n\t\t\t\tself.outChannels = outChannels\n\t\t\t\tself.kernelSize = kernelSize\n\t\t\t\tself.stride = stride\n\t\t\t\tself.padding = padding\n\n\t\t\t\tself.bn = nn.BatchNorm2d(self.inChannels)\n\t\t\t\tself.conv = nn.Conv2d(self.inChannels, self.outChannels, self.kernelSize, self.stride, self.padding)\n\t\t\t\tself.relu = nn.ReLU()\n\n\t\tdef forward(self, x):\n\t\t\t\tx = self.bn(x)\n\t\t\t\tx = self.relu(x)\n\t\t\t\tx = self.conv(x)\n\t\t\t\treturn x\n\n\nclass ConvBlock(nn.Module):\n\t\t""""""docstring for ConvBlock""""""\n\t\tdef __init__(self, inChannels, outChannels):\n\t\t\t\tsuper(ConvBlock, self).__init__()\n\t\t\t\tself.inChannels = inChannels\n\t\t\t\tself.outChannels = outChannels\n\t\t\t\tself.outChannelsby2 = outChannels//2\n\n\t\t\t\tself.cbr1 = BnReluConv(self.inChannels, self.outChannelsby2, 1, 1, 0)\n\t\t\t\tself.cbr2 = BnReluConv(self.outChannelsby2, self.outChannelsby2, 3, 1, 1)\n\t\t\t\tself.cbr3 = BnReluConv(self.outChannelsby2, self.outChannels, 1, 1, 0)\n\n\t\tdef forward(self, x):\n\t\t\t\tx = self.cbr1(x)\n\t\t\t\tx = self.cbr2(x)\n\t\t\t\tx = self.cbr3(x)\n\t\t\t\treturn x\n\nclass SkipLayer(nn.Module):\n\t\t""""""docstring for SkipLayer""""""\n\t\tdef __init__(self, inChannels, outChannels):\n\t\t\t\tsuper(SkipLayer, self).__init__()\n\t\t\t\tself.inChannels = inChannels\n\t\t\t\tself.outChannels = outChannels\n\t\t\t\tif (self.inChannels == self.outChannels):\n\t\t\t\t\t\tself.conv = None\n\t\t\t\telse:\n\t\t\t\t\t\tself.conv = nn.Conv2d(self.inChannels, self.outChannels, 1)\n\n\t\tdef forward(self, x):\n\t\t\t\tif self.conv is not None:\n\t\t\t\t\t\tx = self.conv(x)\n\t\t\t\treturn x\n\nclass Residual(nn.Module):\n\t\t""""""docstring for Residual""""""\n\t\tdef __init__(self, inChannels, outChannels):\n\t\t\t\tsuper(Residual, self).__init__()\n\t\t\t\tself.inChannels = inChannels\n\t\t\t\tself.outChannels = outChannels\n\t\t\t\tself.cb = ConvBlock(inChannels, outChannels)\n\t\t\t\tself.skip = SkipLayer(inChannels, outChannels)\n\n\t\tdef forward(self, x):\n\t\t\t\tout = 0\n\t\t\t\tout = out + self.cb(x)\n\t\t\t\tout = out + self.skip(x)\n\t\t\t\treturn out\n\nclass myUpsample(nn.Module):\n\t def __init__(self):\n\t\t super(myUpsample, self).__init__()\n\t\t pass\n\t def forward(self, x):\n\t\t return x[:, :, :, None, :, None].expand(-1, -1, -1, 2, -1, 2).reshape(x.size(0), x.size(1), x.size(2)*2, x.size(3)*2)\n'"
models/modules/__init__.py,0,b'from models.modules.DeepPose import *\nfrom models.modules.ChainedPredictions import *\nfrom models.modules.StackedHourGlass import *\nfrom models.modules.PoseAttention import *\nfrom models.modules.PyraNet import *\n'
datasets/COCO/nms/._nms.py,0,b'\x00\x05\x16\x07\x00\x02\x00\x00Mac OS X        \x00\x02\x00\x00\x00\t\x00\x00\x002\x00\x00\x0e\xb0\x00\x00\x00\x02\x00\x00\x0e\xe2\x00\x00\x01\x1e\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00ATTR\x00\x00\x00B\x00\x00\x0e\xe2\x00\x00\x00\xa0\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\xa0\x00\x00\x00\x10\x00\x00\x1acom.apple.lastuseddate#PS\x00\x00\x00\x00t\xe1\xa4[\x00\x00\x00\x00\x1e\xff\xc8\x02\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x1eThis resource fork intentionally left blank   \x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x1e\x00\x00\x00\x00\x00\x00\x00\x00\x00\x1c\x00\x1e\xff\xff'
datasets/COCO/nms/__init__.py,0,b''
datasets/COCO/nms/nms.py,0,"b'# ------------------------------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under the MIT License.\n# Modified from py-faster-rcnn (https://github.com/rbgirshick/py-faster-rcnn)\n# ------------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom .cpu_nms import cpu_nms\nfrom .gpu_nms import gpu_nms\n\n\ndef py_nms_wrapper(thresh):\n    def _nms(dets):\n        return nms(dets, thresh)\n    return _nms\n\n\ndef cpu_nms_wrapper(thresh):\n    def _nms(dets):\n        return cpu_nms(dets, thresh)\n    return _nms\n\n\ndef gpu_nms_wrapper(thresh, device_id):\n    def _nms(dets):\n        return gpu_nms(dets, thresh, device_id)\n    return _nms\n\n\ndef nms(dets, thresh):\n    """"""\n    greedily select boxes with high confidence and overlap with current maximum <= thresh\n    rule out overlap >= thresh\n    :param dets: [[x1, y1, x2, y2 score]]\n    :param thresh: retain overlap < thresh\n    :return: indexes to keep\n    """"""\n    if dets.shape[0] == 0:\n        return []\n\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n\ndef oks_iou(g, d, a_g, a_d, sigmas=None, in_vis_thre=None):\n    if not isinstance(sigmas, np.ndarray):\n        sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62, .62, 1.07, 1.07, .87, .87, .89, .89]) / 10.0\n    vars = (sigmas * 2) ** 2\n    xg = g[0::3]\n    yg = g[1::3]\n    vg = g[2::3]\n    ious = np.zeros((d.shape[0]))\n    for n_d in range(0, d.shape[0]):\n        xd = d[n_d, 0::3]\n        yd = d[n_d, 1::3]\n        vd = d[n_d, 2::3]\n        dx = xd - xg\n        dy = yd - yg\n        e = (dx ** 2 + dy ** 2) / vars / ((a_g + a_d[n_d]) / 2 + np.spacing(1)) / 2\n        if in_vis_thre is not None:\n            ind = list(vg > in_vis_thre) and list(vd > in_vis_thre)\n            e = e[ind]\n        ious[n_d] = np.sum(np.exp(-e)) / e.shape[0] if e.shape[0] != 0 else 0.0\n    return ious\n\ndef oks_nms(kpts_db, thresh, sigmas=None, in_vis_thre=None):\n    """"""\n    greedily select boxes with high confidence and overlap with current maximum <= thresh\n    rule out overlap >= thresh, overlap = oks\n    :param kpts_db\n    :param thresh: retain overlap < thresh\n    :return: indexes to keep\n    """"""\n    if len(kpts_db) == 0:\n        return []\n\n    scores = np.array([kpts_db[i][\'score\'] for i in range(len(kpts_db))])\n    kpts = np.array([kpts_db[i][\'keypoints\'].flatten() for i in range(len(kpts_db))])\n    areas = np.array([kpts_db[i][\'area\'] for i in range(len(kpts_db))])\n\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n\n        oks_ovr = oks_iou(kpts[i], kpts[order[1:]], areas[i], areas[order[1:]], sigmas, in_vis_thre)\n\n        inds = np.where(oks_ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n\n'"
datasets/COCO/nms/setup.py,0,"b'# --------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Modified from py-faster-rcnn (https://github.com/rbgirshick/py-faster-rcnn)\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nfrom setuptools import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\nimport numpy as np\n\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    # Adapted fom\n    # http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.items():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\nCUDA = locate_cuda()\n\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\next_modules = [\n    Extension(\n        ""cpu_nms"",\n        [""cpu_nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    ),\n    Extension(\'gpu_nms\',\n        [\'nms_kernel.cu\', \'gpu_nms.pyx\'],\n        library_dirs=[CUDA[\'lib64\']],\n        libraries=[\'cudart\'],\n        language=\'c++\',\n        runtime_library_dirs=[CUDA[\'lib64\']],\n        # this syntax is specific to this build system\n        # we\'re only going to use certain compiler args with nvcc and not with\n        # gcc the implementation of this trick is in customize_compiler() below\n        extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                            \'nvcc\': [\'-arch=sm_35\',\n                                     \'--ptxas-options=-v\',\n                                     \'-c\',\n                                     \'--compiler-options\',\n                                     ""\'-fPIC\'""]},\n        include_dirs = [numpy_include, CUDA[\'include\']]\n    ),\n]\n\nsetup(\n    name=\'nms\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
datasets/COCO/utils/__init__.py,0,b''
datasets/COCO/utils/transforms.py,0,"b'# ------------------------------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under the MIT License.\n# Written by Bin Xiao (Bin.Xiao@microsoft.com)\n# ------------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport cv2\n\n\ndef flip_back(output_flipped, matched_parts):\n    \'\'\'\n    ouput_flipped: numpy.ndarray(batch_size, num_joints, height, width)\n    \'\'\'\n    assert output_flipped.ndim == 4,\\\n        \'output_flipped should be [batch_size, num_joints, height, width]\'\n\n    output_flipped = output_flipped[:, :, :, ::-1]\n\n    for pair in matched_parts:\n        tmp = output_flipped[:, pair[0], :, :].copy()\n        output_flipped[:, pair[0], :, :] = output_flipped[:, pair[1], :, :]\n        output_flipped[:, pair[1], :, :] = tmp\n\n    return output_flipped\n\n\ndef fliplr_joints(joints, joints_vis, width, matched_parts):\n    """"""\n    flip coords\n    """"""\n    # Flip horizontal\n    joints[:, 0] = width - joints[:, 0] - 1\n\n    # Change left-right parts\n    for pair in matched_parts:\n        joints[pair[0], :], joints[pair[1], :] = \\\n            joints[pair[1], :], joints[pair[0], :].copy()\n        joints_vis[pair[0], :], joints_vis[pair[1], :] = \\\n            joints_vis[pair[1], :], joints_vis[pair[0], :].copy()\n\n    return joints*joints_vis, joints_vis\n\n\ndef transform_preds(coords, center, scale, output_size):\n    target_coords = np.zeros(coords.shape)\n    trans = get_affine_transform(center, scale, 0, output_size, inv=1)\n    for p in range(coords.shape[0]):\n        target_coords[p, 0:2] = affine_transform(coords[p, 0:2], trans)\n    return target_coords\n\n\ndef get_affine_transform(center,\n                         scale,\n                         rot,\n                         output_size,\n                         shift=np.array([0, 0], dtype=np.float32),\n                         inv=0):\n    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\n        #print(scale)\n        scale = np.array([scale, scale])\n\n    scale_tmp = scale * 200.0\n    src_w = scale_tmp[0]\n    dst_w = output_size[0]\n    dst_h = output_size[1]\n\n    rot_rad = np.pi * rot / 180\n    src_dir = get_dir([0, src_w * -0.5], rot_rad)\n    dst_dir = np.array([0, dst_w * -0.5], np.float32)\n\n    src = np.zeros((3, 2), dtype=np.float32)\n    dst = np.zeros((3, 2), dtype=np.float32)\n    src[0, :] = center + scale_tmp * shift\n    src[1, :] = center + src_dir + scale_tmp * shift\n    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]\n    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir\n\n    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\n    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\n\n    if inv:\n        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n    else:\n        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n\n    return trans\n\n\ndef affine_transform(pt, t):\n    new_pt = np.array([pt[0], pt[1], 1.]).T\n    new_pt = np.dot(t, new_pt)\n    return new_pt[:2]\n\n\ndef get_3rd_point(a, b):\n    direct = a - b\n    return b + np.array([-direct[1], direct[0]], dtype=np.float32)\n\n\ndef get_dir(src_point, rot_rad):\n    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n\n    src_result = [0, 0]\n    src_result[0] = src_point[0] * cs - src_point[1] * sn\n    src_result[1] = src_point[0] * sn + src_point[1] * cs\n\n    return src_result\n\n\ndef crop(img, center, scale, output_size, rot=0):\n    trans = get_affine_transform(center, scale, rot, output_size)\n\n    dst_img = cv2.warpAffine(img,\n                             trans,\n                             (int(output_size[0]), int(output_size[1])),\n                             flags=cv2.INTER_LINEAR)\n\n    return dst_img\n'"
datasets/COCO/utils/utils.py,3,"b""# ------------------------------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under the MIT License.\n# Written by Bin Xiao (Bin.Xiao@microsoft.com)\n# ------------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport logging\nimport time\nfrom pathlib import Path\n\nimport torch\nimport torch.optim as optim\n\nfrom core.config import get_model_name\n\n\ndef create_logger(cfg, cfg_name, phase='train'):\n    root_output_dir = Path(cfg.OUTPUT_DIR)\n    # set up logger\n    if not root_output_dir.exists():\n        print('=> creating {}'.format(root_output_dir))\n        root_output_dir.mkdir()\n\n    dataset = cfg.DATASET.DATASET + '_' + cfg.DATASET.HYBRID_JOINTS_TYPE \\\n        if cfg.DATASET.HYBRID_JOINTS_TYPE else cfg.DATASET.DATASET\n    dataset = dataset.replace(':', '_')\n    model, _ = get_model_name(cfg)\n    cfg_name = os.path.basename(cfg_name).split('.')[0]\n\n    final_output_dir = root_output_dir / dataset / model / cfg_name\n\n    print('=> creating {}'.format(final_output_dir))\n    final_output_dir.mkdir(parents=True, exist_ok=True)\n\n    time_str = time.strftime('%Y-%m-%d-%H-%M')\n    log_file = '{}_{}_{}.log'.format(cfg_name, time_str, phase)\n    final_log_file = final_output_dir / log_file\n    head = '%(asctime)-15s %(message)s'\n    logging.basicConfig(filename=str(final_log_file),\n                        format=head)\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n    console = logging.StreamHandler()\n    logging.getLogger('').addHandler(console)\n\n    tensorboard_log_dir = Path(cfg.LOG_DIR) / dataset / model / \\\n        (cfg_name + '_' + time_str)\n    print('=> creating {}'.format(tensorboard_log_dir))\n    tensorboard_log_dir.mkdir(parents=True, exist_ok=True)\n\n    return logger, str(final_output_dir), str(tensorboard_log_dir)\n\n\ndef get_optimizer(cfg, model):\n    optimizer = None\n    if cfg.TRAIN.OPTIMIZER == 'sgd':\n        optimizer = optim.SGD(\n            model.parameters(),\n            lr=cfg.TRAIN.LR,\n            momentum=cfg.TRAIN.MOMENTUM,\n            weight_decay=cfg.TRAIN.WD,\n            nesterov=cfg.TRAIN.NESTEROV\n        )\n    elif cfg.TRAIN.OPTIMIZER == 'adam':\n        optimizer = optim.Adam(\n            model.parameters(),\n            lr=cfg.TRAIN.LR\n        )\n\n    return optimizer\n\n\ndef save_checkpoint(states, is_best, output_dir,\n                    filename='checkpoint.pth.tar'):\n    torch.save(states, os.path.join(output_dir, filename))\n    if is_best and 'state_dict' in states:\n        torch.save(states['state_dict'],\n                   os.path.join(output_dir, 'model_best.pth.tar'))\n"""
datasets/COCO/utils/vis.py,0,"b""# ------------------------------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under the MIT License.\n# Written by Bin Xiao (Bin.Xiao@microsoft.com)\n# ------------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nimport numpy as np\nimport torchvision\nimport cv2\n\nfrom core.inference import get_max_preds\n\n\ndef save_batch_image_with_joints(batch_image, batch_joints, batch_joints_vis,\n                                 file_name, nrow=8, padding=2):\n    '''\n    batch_image: [batch_size, channel, height, width]\n    batch_joints: [batch_size, num_joints, 3],\n    batch_joints_vis: [batch_size, num_joints, 1],\n    }\n    '''\n    grid = torchvision.utils.make_grid(batch_image, nrow, padding, True)\n    ndarr = grid.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()\n    ndarr = ndarr.copy()\n\n    nmaps = batch_image.size(0)\n    xmaps = min(nrow, nmaps)\n    ymaps = int(math.ceil(float(nmaps) / xmaps))\n    height = int(batch_image.size(2) + padding)\n    width = int(batch_image.size(3) + padding)\n    k = 0\n    for y in range(ymaps):\n        for x in range(xmaps):\n            if k >= nmaps:\n                break\n            joints = batch_joints[k]\n            joints_vis = batch_joints_vis[k]\n\n            for joint, joint_vis in zip(joints, joints_vis):\n                joint[0] = x * width + padding + joint[0]\n                joint[1] = y * height + padding + joint[1]\n                if joint_vis[0]:\n                    cv2.circle(ndarr, (int(joint[0]), int(joint[1])), 2, [255, 0, 0], 2)\n            k = k + 1\n    cv2.imwrite(file_name, ndarr)\n\n\ndef save_batch_heatmaps(batch_image, batch_heatmaps, file_name,\n                        normalize=True):\n    '''\n    batch_image: [batch_size, channel, height, width]\n    batch_heatmaps: ['batch_size, num_joints, height, width]\n    file_name: saved file name\n    '''\n    if normalize:\n        batch_image = batch_image.clone()\n        min = float(batch_image.min())\n        max = float(batch_image.max())\n\n        batch_image.add_(-min).div_(max - min + 1e-5)\n\n    batch_size = batch_heatmaps.size(0)\n    num_joints = batch_heatmaps.size(1)\n    heatmap_height = batch_heatmaps.size(2)\n    heatmap_width = batch_heatmaps.size(3)\n\n    grid_image = np.zeros((batch_size*heatmap_height,\n                           (num_joints+1)*heatmap_width,\n                           3),\n                          dtype=np.uint8)\n\n    preds, maxvals = get_max_preds(batch_heatmaps.detach().cpu().numpy())\n\n    for i in range(batch_size):\n        image = batch_image[i].mul(255)\\\n                              .clamp(0, 255)\\\n                              .byte()\\\n                              .permute(1, 2, 0)\\\n                              .cpu().numpy()\n        heatmaps = batch_heatmaps[i].mul(255)\\\n                                    .clamp(0, 255)\\\n                                    .byte()\\\n                                    .cpu().numpy()\n\n        resized_image = cv2.resize(image,\n                                   (int(heatmap_width), int(heatmap_height)))\n\n        height_begin = heatmap_height * i\n        height_end = heatmap_height * (i + 1)\n        for j in range(num_joints):\n            cv2.circle(resized_image,\n                       (int(preds[i][j][0]), int(preds[i][j][1])),\n                       1, [0, 0, 255], 1)\n            heatmap = heatmaps[j, :, :]\n            colored_heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n            masked_image = colored_heatmap*0.7 + resized_image*0.3\n            cv2.circle(masked_image,\n                       (int(preds[i][j][0]), int(preds[i][j][1])),\n                       1, [0, 0, 255], 1)\n\n            width_begin = heatmap_width * (j+1)\n            width_end = heatmap_width * (j+2)\n            grid_image[height_begin:height_end, width_begin:width_end, :] = \\\n                masked_image\n            # grid_image[height_begin:height_end, width_begin:width_end, :] = \\\n            #     colored_heatmap*0.7 + resized_image*0.3\n\n        grid_image[height_begin:height_end, 0:heatmap_width, :] = resized_image\n\n    cv2.imwrite(file_name, grid_image)\n\n\ndef save_debug_images(config, input, meta, target, joints_pred, output,\n                      prefix):\n    if not config.DEBUG.DEBUG:\n        return\n\n    if config.DEBUG.SAVE_BATCH_IMAGES_GT:\n        save_batch_image_with_joints(\n            input, meta['joints'], meta['joints_vis'],\n            '{}_gt.jpg'.format(prefix)\n        )\n    if config.DEBUG.SAVE_BATCH_IMAGES_PRED:\n        save_batch_image_with_joints(\n            input, joints_pred, meta['joints_vis'],\n            '{}_pred.jpg'.format(prefix)\n        )\n    if config.DEBUG.SAVE_HEATMAPS_GT:\n        save_batch_heatmaps(\n            input, target, '{}_hm_gt.jpg'.format(prefix)\n        )\n    if config.DEBUG.SAVE_HEATMAPS_PRED:\n        save_batch_heatmaps(\n            input, output, '{}_hm_pred.jpg'.format(prefix)\n        )\n"""
datasets/COCO/utils/zipreader.py,0,"b'# ------------------------------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under the MIT License.\n# Written by Bin Xiao (Bin.Xiao@microsoft.com)\n# ------------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport zipfile\nimport xml.etree.ElementTree as ET\n\nimport cv2\nimport numpy as np\n\n_im_zfile = []\n_xml_path_zip = []\n_xml_zfile = []\n\n\ndef imread(filename, flags=cv2.IMREAD_COLOR):\n    global _im_zfile\n    path = filename\n    pos_at = path.index(\'@\')\n    if pos_at == -1:\n        print(""character \'@\' is not found from the given path \'%s\'""%(path))\n        assert 0\n    path_zip = path[0: pos_at]\n    path_img = path[pos_at + 2:]\n    if not os.path.isfile(path_zip):\n        print(""zip file \'%s\' is not found""%(path_zip))\n        assert 0\n    for i in range(len(_im_zfile)):\n        if _im_zfile[i][\'path\'] == path_zip:\n            data = _im_zfile[i][\'zipfile\'].read(path_img)\n            return cv2.imdecode(np.frombuffer(data, np.uint8), flags)\n\n    _im_zfile.append({\n        \'path\': path_zip,\n        \'zipfile\': zipfile.ZipFile(path_zip, \'r\')\n    })\n    data = _im_zfile[-1][\'zipfile\'].read(path_img)\n\n    return cv2.imdecode(np.frombuffer(data, np.uint8), flags)\n\n\ndef xmlread(filename):\n    global _xml_path_zip\n    global _xml_zfile\n    path = filename\n    pos_at = path.index(\'@\')\n    if pos_at == -1:\n        print(""character \'@\' is not found from the given path \'%s\'""%(path))\n        assert 0\n    path_zip = path[0: pos_at]\n    path_xml = path[pos_at + 2:]\n    if not os.path.isfile(path_zip):\n        print(""zip file \'%s\' is not found""%(path_zip))\n        assert 0\n    for i in range(len(_xml_path_zip)):\n        if _xml_path_zip[i] == path_zip:\n            data = _xml_zfile[i].open(path_xml)\n            return ET.fromstring(data.read())\n    _xml_path_zip.append(path_zip)\n    print(""read new xml file \'%s\'""%(path_zip))\n    _xml_zfile.append(zipfile.ZipFile(path_zip, \'r\'))\n    data = _xml_zfile[-1].open(path_xml)\n    return ET.fromstring(data.read())\n'"
