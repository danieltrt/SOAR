file_path,api_count,code
main.py,27,"b'from __future__ import print_function\nimport time\nimport sys\nimport argparse\nimport random\nimport torch\nimport gc\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom utils.metric import get_ner_fmeasure\nfrom model.seqmodel import SeqModel\nfrom utils.data import Data\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\n\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""0""\n\n\nsave_file = ""result_7.10.csv""\n\n\ndef data_initialization(data):\n    data.initial_feature_alphabets()\n    data.build_alphabet(data.train_dir)\n    data.build_alphabet(data.dev_dir)\n    data.build_alphabet(data.test_dir)\n    data.fix_alphabet()\n\n\ndef predict_check(pred_variable, gold_variable, mask_variable):\n    """"""\n        input:\n            pred_variable (batch_size, sent_len): pred tag result, in numpy format\n            gold_variable (batch_size, sent_len): gold result variable\n            mask_variable (batch_size, sent_len): mask variable\n    """"""\n    pred = pred_variable.cpu().data.numpy()\n    gold = gold_variable.cpu().data.numpy()\n    mask = mask_variable.cpu().data.numpy()\n    overlaped = (pred == gold)\n    right_token = np.sum(overlaped * mask)\n    total_token = mask.sum()\n    # print(""right: %s, total: %s""%(right_token, total_token))\n    return right_token, total_token\n\n\ndef recover_label(pred_variable, gold_variable, mask_variable, label_alphabet, word_recover):\n    """"""\n        input:\n            pred_variable (batch_size, sent_len): pred tag result\n            gold_variable (batch_size, sent_len): gold result variable\n            mask_variable (batch_size, sent_len): mask variable\n    """"""\n\n    pred_variable = pred_variable[word_recover]\n    gold_variable = gold_variable[word_recover]\n    mask_variable = mask_variable[word_recover]\n    batch_size = gold_variable.size(0)\n    seq_len = gold_variable.size(1)\n    mask = mask_variable.cpu().data.numpy()\n    pred_tag = pred_variable.cpu().data.numpy()\n    gold_tag = gold_variable.cpu().data.numpy()\n    batch_size = mask.shape[0]\n    pred_label = []\n    gold_label = []\n    for idx in range(batch_size):\n        pred = [label_alphabet.get_instance(pred_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]\n        gold = [label_alphabet.get_instance(gold_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]\n        assert(len(pred)==len(gold))\n        pred_label.append(pred)\n        gold_label.append(gold)\n    return pred_label, gold_label\n\n\ndef recover_nbest_label(pred_variable, mask_variable, label_alphabet, word_recover):\n    """"""\n        input:\n            pred_variable (batch_size, sent_len, nbest): pred tag result\n            mask_variable (batch_size, sent_len): mask variable\n            word_recover (batch_size)\n        output:\n            nbest_pred_label list: [batch_size, nbest, each_seq_len]\n    """"""\n    # exit(0)\n    pred_variable = pred_variable[word_recover]\n    mask_variable = mask_variable[word_recover]\n    batch_size = pred_variable.size(0)\n    seq_len = pred_variable.size(1)\n    nbest = pred_variable.size(2)\n    mask = mask_variable.cpu().data.numpy()\n    pred_tag = pred_variable.cpu().data.numpy()\n    batch_size = mask.shape[0]\n    pred_label = []\n    for idx in range(batch_size):\n        pred = []\n        for idz in range(nbest):\n            each_pred = [label_alphabet.get_instance(pred_tag[idx][idy][idz]) for idy in range(seq_len) if mask[idx][idy] != 0]\n            pred.append(each_pred)\n        pred_label.append(pred)\n    return pred_label\n\ndef lr_decay(optimizer, epoch, decay_rate, init_lr):\n    lr = init_lr/(1+decay_rate*epoch)\n    print("" Learning rate is set as:"", lr)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    return optimizer\n\n\n\ndef evaluate(data, model, name, nbest=None):\n    if name == ""train"":\n        instances = data.train_Ids\n    elif name == ""dev"":\n        instances = data.dev_Ids\n    elif name == \'test\':\n        instances = data.test_Ids\n    elif name == \'raw\':\n        instances = data.raw_Ids\n    else:\n        print(""Error: wrong evaluate name,"", name)\n        exit(1)\n    right_token = 0\n    whole_token = 0\n\n\n    nbest_pred_results = []\n    pred_scores = []\n    pred_results = []\n    gold_results = []\n    ## set model in eval model\n    model.eval()\n    batch_size = data.HP_batch_size\n    start_time = time.time()\n    train_num = len(instances)\n    total_batch = train_num//batch_size+1\n    for batch_id in range(total_batch):\n        start = batch_id*batch_size\n        end = (batch_id+1)*batch_size\n        if end > train_num:\n            end =  train_num\n        instance = instances[start:end]\n        if not instance:\n            continue\n        batch_word, batch_features, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask, input_label_seq_tensor = batchify_with_label(instance, data.HP_gpu, data.label_alphabet_size)\n        #batch_word, batch_features, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask ,input_label_seq_tensor = batchify_with_label(instance, data.HP_gpu,data.label_alphabet_size ,True)\n        if nbest:\n            scores, nbest_tag_seq = model.decode_nbest(batch_word,batch_features, batch_wordlen, batch_char, batch_charlen, batch_charrecover, mask, nbest,input_label_seq_tensor)\n            nbest_pred_result = recover_nbest_label(nbest_tag_seq, mask, data.label_alphabet, batch_wordrecover)\n            nbest_pred_results += nbest_pred_result\n            pred_scores += scores[batch_wordrecover].cpu().data.numpy().tolist()\n            ## select the best sequence to evalurate\n            tag_seq = nbest_tag_seq[:,:,0]\n        else:\n            #batch_word,batch_features, batch_wordlen, batch_char, batch_charlen, batch_charrecover, batch_label, mask,input_label_seq_tensor\n            tag_seq = model(batch_word, batch_features, batch_wordlen, batch_char, batch_charlen, batch_charrecover, mask,input_label_seq_tensor)\n\n\n        pred_label, gold_label = recover_label(tag_seq, batch_label, mask, data.label_alphabet, batch_wordrecover)\n\n        pred_results += pred_label\n        gold_results += gold_label\n    decode_time = time.time() - start_time\n    speed = len(instances)/decode_time\n    acc, p, r, f = get_ner_fmeasure(gold_results, pred_results, data.tagScheme)\n    if nbest:\n        return speed, acc, p, r, f, nbest_pred_results, pred_scores\n    return speed, acc, p, r, f, pred_results, pred_scores\n\n\ndef batchify_with_label(input_batch_list, gpu,label_size = 20, volatile_flag=False):\n    """"""\n        input: list of words, chars and labels, various length. [[words,chars, labels],[words,chars,labels],...]\n            words: word ids for one sentence. (batch_size, sent_len)\n            chars: char ids for on sentences, various length. (batch_size, sent_len, each_word_length)\n        output:\n            zero padding for word and char, with their batch length\n            word_seq_tensor: (batch_size, max_sent_len) Variable\n            word_seq_lengths: (batch_size,1) Tensor\n            char_seq_tensor: (batch_size*max_sent_len, max_word_len) Variable\n            char_seq_lengths: (batch_size*max_sent_len,1) Tensor\n            char_seq_recover: (batch_size*max_sent_len,1)  recover char sequence order\n            label_seq_tensor: (batch_size, max_sent_len)\n            mask: (batch_size, max_sent_len)\n    """"""\n    #label_instance\n    batch_size = len(input_batch_list)\n    words = [sent[0] for sent in input_batch_list]\n    features = [np.asarray(sent[1]) for sent in input_batch_list]\n    feature_num = len(features[0][0])\n    chars = [sent[2] for sent in input_batch_list]\n    labels = [sent[3] for sent in input_batch_list]\n\n    word_seq_lengths = torch.LongTensor(list(map(len, words)))\n    max_seq_len = word_seq_lengths.max()\n    word_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)), volatile =  volatile_flag).long()\n    label_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)),volatile =  volatile_flag).long()\n    #\n    input_label_seq_tensor = autograd.Variable(torch.zeros((batch_size, label_size)),volatile =  volatile_flag).long()\n\n    feature_seq_tensors = []\n    for idx in range(feature_num):\n        feature_seq_tensors.append(autograd.Variable(torch.zeros((batch_size, max_seq_len)),volatile =  volatile_flag).long())\n    mask = autograd.Variable(torch.zeros((batch_size, max_seq_len)),volatile =  volatile_flag).byte()\n    for idx, (seq, label, seqlen) in enumerate(zip(words, labels, word_seq_lengths)):\n        word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n        label_seq_tensor[idx, :seqlen] = torch.LongTensor(label)\n        mask[idx, :seqlen] = torch.Tensor([1]*seqlen)\n        input_label_seq_tensor[idx, :label_size] = torch.LongTensor([i for i in range(label_size)])\n        for idy in range(feature_num):\n            feature_seq_tensors[idy][idx,:seqlen] = torch.LongTensor(features[idx][:,idy])\n\n\n    word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n    word_seq_tensor = word_seq_tensor[word_perm_idx]\n    for idx in range(feature_num):\n        feature_seq_tensors[idx] = feature_seq_tensors[idx][word_perm_idx]\n\n    label_seq_tensor = label_seq_tensor[word_perm_idx]\n    mask = mask[word_perm_idx]\n    ### deal with char\n    # pad_chars (batch_size, max_seq_len)\n    pad_chars = [chars[idx] + [[0]] * (max_seq_len-len(chars[idx])) for idx in range(len(chars))]\n    length_list = [list(map(len, pad_char)) for pad_char in pad_chars]\n    max_word_len = max(map(max, length_list))\n    char_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len, max_word_len)), volatile =  volatile_flag).long()\n    char_seq_lengths = torch.LongTensor(length_list)\n    for idx, (seq, seqlen) in enumerate(zip(pad_chars, char_seq_lengths)):\n        for idy, (word, wordlen) in enumerate(zip(seq, seqlen)):\n            # print len(word), wordlen\n            char_seq_tensor[idx, idy, :wordlen] = torch.LongTensor(word)\n\n    char_seq_tensor = char_seq_tensor[word_perm_idx].view(batch_size*max_seq_len,-1)\n    char_seq_lengths = char_seq_lengths[word_perm_idx].view(batch_size*max_seq_len,)\n    char_seq_lengths, char_perm_idx = char_seq_lengths.sort(0, descending=True)\n    char_seq_tensor = char_seq_tensor[char_perm_idx]\n    _, char_seq_recover = char_perm_idx.sort(0, descending=False)\n    _, word_seq_recover = word_perm_idx.sort(0, descending=False)\n    if gpu:\n        word_seq_tensor = word_seq_tensor.cuda()\n        for idx in range(feature_num):\n            feature_seq_tensors[idx] = feature_seq_tensors[idx].cuda()\n        word_seq_lengths = word_seq_lengths.cuda()\n        word_seq_recover = word_seq_recover.cuda()\n        label_seq_tensor = label_seq_tensor.cuda()\n        char_seq_tensor = char_seq_tensor.cuda()\n        char_seq_recover = char_seq_recover.cuda()\n        input_label_seq_tensor = input_label_seq_tensor.cuda()\n        mask = mask.cuda()\n\n\n    return word_seq_tensor,feature_seq_tensors, word_seq_lengths, word_seq_recover, char_seq_tensor, char_seq_lengths, char_seq_recover, label_seq_tensor, mask ,input_label_seq_tensor\n\n\ndef train(data):\n    print(""Training model..."")\n    data.show_data_summary()\n    save_data_name = data.model_dir +"".dset""\n    data.save(save_data_name)\n    model = SeqModel(data)\n    pytorch_total_params = sum(p.numel() for p in model.parameters())\n    print (""--------pytorch total params--------"")\n    print (pytorch_total_params)\n\n    loss_function = nn.NLLLoss()\n    if data.optimizer.lower() == ""sgd"":\n        optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=data.HP_lr,\n                              momentum=data.HP_momentum, weight_decay=data.HP_l2)\n    elif data.optimizer.lower() == ""adagrad"":\n        optimizer = optim.Adagrad(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)\n    elif data.optimizer.lower() == ""adadelta"":\n        optimizer = optim.Adadelta(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)\n    elif data.optimizer.lower() == ""rmsprop"":\n        optimizer = optim.RMSprop(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)\n    elif data.optimizer.lower() == ""adam"":\n        optimizer = optim.Adam(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)\n    else:\n        print(""Optimizer illegal: %s""%(data.optimizer))\n        exit(1)\n    best_dev = -10\n    best_test = -10\n    best_epoch = -1\n    no_imprv_epoch = 0\n    # data.HP_iteration = 1\n    ## start training\n    for idx in range(data.HP_iteration):\n        epoch_start = time.time()\n        temp_start = epoch_start\n        print(""Epoch: %s/%s"" %(idx,data.HP_iteration))        #print (self.train_Ids)\n        if data.optimizer == ""SGD"":\n            optimizer = lr_decay(optimizer, idx, data.HP_lr_decay, data.HP_lr)\n        instance_count = 0\n        sample_id = 0\n        sample_loss = 0\n        total_loss = 0\n        right_token = 0\n        whole_token = 0\n        random.shuffle(data.train_Ids)\n        ## set model in train model\n        model.train()\n        model.zero_grad()\n        batch_size = data.HP_batch_size\n        batch_id = 0\n        train_num = len(data.train_Ids)\n\n        total_batch = train_num//batch_size+1\n        for batch_id in range(total_batch):\n            start = batch_id*batch_size\n            end = (batch_id+1)*batch_size\n            if end >train_num:\n                end = train_num\n            instance = data.train_Ids[start:end]\n\n            if not instance:\n                continue\n\n            #label_instance = [[i for i in range(0, data.label_alphabet_size + 1)] for _ in range(len(instance))]\n\n            batch_word, batch_features, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask ,input_label_seq_tensor= batchify_with_label(instance, data.HP_gpu,data.label_alphabet_size)\n\n            instance_count += 1\n            loss, tag_seq = model.neg_log_likelihood_loss(batch_word,batch_features, batch_wordlen, batch_char, batch_charlen, batch_charrecover, batch_label, mask,input_label_seq_tensor)\n            right, whole = predict_check(tag_seq, batch_label, mask)\n            right_token += right\n            whole_token += whole\n            sample_loss += loss.data[0]\n            total_loss += loss.data[0]\n            if end%500 == 0:\n                # temp_time = time.time()\n                # temp_cost = temp_time - temp_start\n                # temp_start = temp_time\n                #print(""     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f""%(end, temp_cost, sample_loss, right_token, whole_token,(right_token+0.)/whole_token))\n                if sample_loss > 1e8 or str(sample_loss) == ""nan"":\n                    print(""ERROR: LOSS EXPLOSION (>1e8) ! PLEASE SET PROPER PARAMETERS AND STRUCTURE! EXIT...."")\n\n\n                    exit(1)\n                sys.stdout.flush()\n                sample_loss = 0\n            loss.backward()\n            if data.whether_clip_grad:\n                from torch.nn.utils import clip_grad_norm\n                clip_grad_norm(model.parameters(), data.clip_grad)\n            optimizer.step()\n            model.zero_grad()\n        temp_time = time.time()\n        temp_cost = temp_time - temp_start\n        print(""     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f""%(end, temp_cost, sample_loss, right_token, whole_token,(right_token+0.)/whole_token))\n\n        epoch_finish = time.time()\n        epoch_cost = epoch_finish - epoch_start\n        print(""Epoch: %s training finished. Time: %.2fs, speed: %.2fst/s,  total loss: %s""%(idx, epoch_cost, train_num/epoch_cost, total_loss))\n        print(""totalloss:"", total_loss)\n        if total_loss > 1e8 or str(total_loss) == ""nan"":\n            print(""ERROR: LOSS EXPLOSION (>1e8) ! PLEASE SET PROPER PARAMETERS AND STRUCTURE! EXIT...."")\n            #exit(1)\n        # continue\n        speed, acc, p, r, f, _,_ = evaluate(data, model, ""dev"")\n        dev_finish = time.time()\n        dev_cost = dev_finish - epoch_finish\n\n        if data.seg:\n            current_score = f\n            print(""Dev: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f""%(dev_cost, speed, acc, p, r, f))\n        else:\n            current_score = acc\n            print(""Dev: time: %.2fs speed: %.2fst/s; acc: %.4f""%(dev_cost, speed, acc))\n\n\n        # ## decode test\n        speed, acc_test, p, r, f_test, _,_ = evaluate(data, model, ""test"")\n        test_finish = time.time()\n        test_cost = test_finish - dev_finish\n        if data.seg:\n            print(""Test: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f""%(test_cost, speed, acc_test, p, r, f_test))\n        else:\n            print(""Test: time: %.2fs, speed: %.2fst/s; acc: %.4f""%(test_cost, speed, acc_test))\n\n        if current_score > best_dev:\n            if data.seg:\n                best_test = f_test\n                print(""Exceed previous best f score:"", best_dev)\n            else:\n                best_test = acc_test\n                print(""Exceed previous best acc score:"", best_dev)\n            best_epoch = idx\n            # model_name = data.model_dir +\'.\'+ str(idx) + "".model""\n            # print(""Save current best model in file:"", model_name)\n            # torch.save(model.state_dict(), model_name)\n            best_dev = current_score\n            no_imprv_epoch = 0\n\n\n        else:\n            #early stop\n            no_imprv_epoch += 1\n            if no_imprv_epoch >= 10:\n                print(""early stop"")\n                print(""Current best f score in dev"", best_dev)\n                print(""Current best f score in test"", best_test)\n                break\n\n        if data.seg:\n            print (""Current best f score in dev"",best_dev)\n            print (""Current best f score in test"",best_test)\n        else:\n            print (""Current best acc score in dev"",best_dev)\n            print (""Current best acc score in test"",best_test)\n        gc.collect()\n\n\ndef load_model_decode(data, name):\n    print(""Load Model from file: "", data.model_dir)\n    model = SeqModel(data)\n    ## load model need consider if the model trained in GPU and load in CPU, or vice versa\n    # if not gpu:\n    #     model.load_state_dict(torch.load(model_dir))\n    #     # model.load_state_dict(torch.load(model_dir), map_location=lambda storage, loc: storage)\n    #     # model = torch.load(model_dir, map_location=lambda storage, loc: storage)\n    # else:\n    #     model.load_state_dict(torch.load(model_dir))\n    #     # model = torch.load(model_dir)\n    model.load_state_dict(torch.load(data.load_model_dir))\n\n    print(""Decode %s data, nbest: %s ...""%(name, data.nbest))\n    start_time = time.time()\n    speed, acc, p, r, f, pred_results, pred_scores = evaluate(data, model, name, data.nbest)\n    end_time = time.time()\n    time_cost = end_time - start_time\n    if data.seg:\n        print(""%s: time:%.2fs, speed:%.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f""%(name, time_cost, speed, acc, p, r, f))\n    else:\n        print(""%s: time:%.2fs, speed:%.2fst/s; acc: %.4f""%(name, time_cost, speed, acc))\n    return pred_results, pred_scores\n\n\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Tuning with NCRF++\')\n    # parser.add_argument(\'--status\', choices=[\'train\', \'decode\'], help=\'update algorithm\', default=\'train\')\n    parser.add_argument(\'--config\',  help=\'Configuration File\' )\n    # # POS wsj\n    parser.add_argument(\'--train_dir\', default=\'wsj_pos/train.pos\', help=\'train_file\')\n    parser.add_argument(\'--dev_dir\', default=\'wsj_pos/dev.pos\', help=\'dev_file\')\n    parser.add_argument(\'--test_dir\', default=\'wsj_pos/test.pos\', help=\'test_file\')\n    parser.add_argument(\'--model_dir\', default=\'wsj_pos/label_embedding\', help=\'model_file\')\n    parser.add_argument(\'--seg\', default=False)\n\n    parser.add_argument(\'--word_emb_dir\', default=\'glove.6B.100d.txt\', help=\'word_emb_dir\')\n    # parser.add_argument(\'--word_emb_dir\', default=\'\', help=\'word_emb_dir\')\n    parser.add_argument(\'--norm_word_emb\', default = False)\n    parser.add_argument(\'--norm_char_emb\', default = False)\n    parser.add_argument(\'--number_normalized\', default = True)\n    parser.add_argument(\'--word_emb_dim\', default=100)\n    parser.add_argument(\'--char_emb_dim\', default=30)\n\n    #NetworkConfiguration\n    parser.add_argument(\'--use_crf\', default= False)\n    parser.add_argument(\'--use_char\', default=True)\n    parser.add_argument(\'--word_seq_feature\', default=\'LSTM\')\n    parser.add_argument(\'--char_seq_feature\', default=\'LSTM\')\n\n\n\n    #TrainingSetting\n    parser.add_argument(\'--status\', default=\'train\')\n    parser.add_argument(\'--optimizer\', default=\'SGD\')\n    parser.add_argument(\'--iteration\',default = 100)\n    parser.add_argument(\'--batch_size\', default= 10)\n    parser.add_argument(\'--ave_batch_loss\', default=False)\n\n    #Hyperparameters\n    parser.add_argument(\'--cnn_layer\', default=4)\n    parser.add_argument(\'--char_hidden_dim\', default=50)\n    parser.add_argument(\'--hidden_dim\', default=400)\n    parser.add_argument(\'--dropout\', default=0.5)\n    parser.add_argument(\'--lstm_layer\', default=0)\n    parser.add_argument(\'--bilstm\', default=True)\n    parser.add_argument(\'--learning_rate\', default=0.01)\n    parser.add_argument(\'--lr_decay\',default=0.05)\n    parser.add_argument(\'--label_embedding_scale\',default = 0.0025)\n    parser.add_argument(\'--num_attention_head\', default=5)\n    #0.05\n    parser.add_argument(\'--momentum\', default=0.9)\n    parser.add_argument(\'--whether_clip_grad\', default=True)\n    parser.add_argument(\'--clip_grad\', default=5)\n    parser.add_argument(\'--l2\', default=1e-8)\n    parser.add_argument(\'--gpu\', default=True)\n    parser.add_argument(\'--seed\',default=42)\n\n\n    args = parser.parse_args()\n\n    print (args.seg)\n\n    seed_num = int(args.seed)\n    random.seed(seed_num)\n    torch.manual_seed(seed_num)\n    np.random.seed(seed_num)\n\n    data = Data()\n    #print(data.initial_feature_alphabets())\n    data.HP_gpu = torch.cuda.is_available()\n    data.read_config(args)\n    status = data.status.lower()\n    print(""Seed num:"",seed_num)\n\n    if status == \'train\':\n        print(""MODEL: train"")\n        data_initialization(data)\n        data.generate_instance(\'train\')\n        data.generate_instance(\'dev\')\n        data.generate_instance(\'test\')\n        data.build_pretrain_emb()\n        train(data)\n    elif status == \'decode\':\n        print(""MODEL: decode"")\n        data.load(data.dset_dir)\n        data.read_config(args.config)\n        # exit(0)\n        data.show_data_summary()\n        data.generate_instance(\'raw\')\n        print(""nbest: %s""%(data.nbest))\n        decode_results, pred_scores = load_model_decode(data, \'raw\')\n        if data.nbest:\n            data.write_nbest_decoded_results(decode_results, pred_scores, \'raw\')\n        else:\n            data.write_decoded_results(decode_results, \'raw\')\n    else:\n        print(""Invalid argument! Please use valid arguments! (train/test/decode)"")\n'"
main_parse.py,25,"b'# -*- coding: utf-8 -*-\n# @Author: Jie\n# @Date:   2017-06-15 14:11:08\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2018-09-05 23:05:37\n\nfrom __future__ import print_function\nimport time\nimport sys\nimport argparse\nimport random\nimport copy\nimport torch\nimport gc\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nfrom utils.metric import get_ner_fmeasure\nfrom model.seqmodel import SeqModel\nfrom utils.data import Data\n\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle as pickle\n\nseed_num = 42\nrandom.seed(seed_num)\ntorch.manual_seed(seed_num)\nnp.random.seed(seed_num)\n\n\ndef data_initialization(data):\n    data.initial_feature_alphabets()\n    data.build_alphabet(data.train_dir)\n    data.build_alphabet(data.dev_dir)\n    data.build_alphabet(data.test_dir)\n    data.fix_alphabet()\n\n\ndef predict_check(pred_variable, gold_variable, mask_variable):\n    """"""\n        input:\n            pred_variable (batch_size, sent_len): pred tag result, in numpy format\n            gold_variable (batch_size, sent_len): gold result variable\n            mask_variable (batch_size, sent_len): mask variable\n    """"""\n    pred = pred_variable.cpu().data.numpy()\n    gold = gold_variable.cpu().data.numpy()\n    mask = mask_variable.cpu().data.numpy()\n    overlaped = (pred == gold)\n    right_token = np.sum(overlaped * mask)\n    total_token = mask.sum()\n    # print(""right: %s, total: %s""%(right_token, total_token))\n    return right_token, total_token\n\n\ndef recover_label(pred_variable, gold_variable, mask_variable, label_alphabet, word_recover):\n    """"""\n        input:\n            pred_variable (batch_size, sent_len): pred tag result\n            gold_variable (batch_size, sent_len): gold result variable\n            mask_variable (batch_size, sent_len): mask variable\n    """"""\n\n    pred_variable = pred_variable[word_recover]\n    gold_variable = gold_variable[word_recover]\n    mask_variable = mask_variable[word_recover]\n    batch_size = gold_variable.size(0)\n    seq_len = gold_variable.size(1)\n    mask = mask_variable.cpu().data.numpy()\n    pred_tag = pred_variable.cpu().data.numpy()\n    gold_tag = gold_variable.cpu().data.numpy()\n    batch_size = mask.shape[0]\n    pred_label = []\n    gold_label = []\n    for idx in range(batch_size):\n        pred = [label_alphabet.get_instance(pred_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]\n        gold = [label_alphabet.get_instance(gold_tag[idx][idy]) for idy in range(seq_len) if mask[idx][idy] != 0]\n        # print(""p:"",pred, pred_tag.tolist())\n        # print(""g:"", gold, gold_tag.tolist())\n        assert (len(pred) == len(gold))\n        pred_label.append(pred)\n        gold_label.append(gold)\n    return pred_label, gold_label\n\n\ndef recover_nbest_label(pred_variable, mask_variable, label_alphabet, word_recover):\n    """"""\n        input:\n            pred_variable (batch_size, sent_len, nbest): pred tag result\n            mask_variable (batch_size, sent_len): mask variable\n            word_recover (batch_size)\n        output:\n            nbest_pred_label list: [batch_size, nbest, each_seq_len]\n    """"""\n    # print(""word recover:"", word_recover.size())\n    # exit(0)\n    pred_variable = pred_variable[word_recover]\n    mask_variable = mask_variable[word_recover]\n    batch_size = pred_variable.size(0)\n    seq_len = pred_variable.size(1)\n    print(pred_variable.size())\n    nbest = pred_variable.size(2)\n    mask = mask_variable.cpu().data.numpy()\n    pred_tag = pred_variable.cpu().data.numpy()\n    batch_size = mask.shape[0]\n    pred_label = []\n    for idx in range(batch_size):\n        pred = []\n        for idz in range(nbest):\n            each_pred = [label_alphabet.get_instance(pred_tag[idx][idy][idz]) for idy in range(seq_len) if\n                         mask[idx][idy] != 0]\n            pred.append(each_pred)\n        pred_label.append(pred)\n    return pred_label\n\n\n# def save_data_setting(data, save_file):\n#     new_data = copy.deepcopy(data)\n#     ## remove input instances\n#     new_data.train_texts = []\n#     new_data.dev_texts = []\n#     new_data.test_texts = []\n#     new_data.raw_texts = []\n\n#     new_data.train_Ids = []\n#     new_data.dev_Ids = []\n#     new_data.test_Ids = []\n#     new_data.raw_Ids = []\n#     ## save data settings\n#     with open(save_file, \'w\') as fp:\n#         pickle.dump(new_data, fp)\n#     print(""Data setting saved to file: "", save_file)\n\n\n# def load_data_setting(save_file):\n#     with open(save_file, \'r\') as fp:\n#         data = pickle.load(fp)\n#     print(""Data setting loaded from file: "", save_file)\n#     data.show_data_summary()\n#     return data\n\ndef lr_decay(optimizer, epoch, decay_rate, init_lr):\n    lr = init_lr / (1 + decay_rate * epoch)\n    print("" Learning rate is set as:"", lr)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    return optimizer\n\n\ndef evaluate(data, model, name, nbest=None):\n    if name == ""train"":\n        instances = data.train_Ids\n    elif name == ""dev"":\n        instances = data.dev_Ids\n    elif name == \'test\':\n        instances = data.test_Ids\n    elif name == \'raw\':\n        instances = data.raw_Ids\n    else:\n        print(""Error: wrong evaluate name,"", name)\n    right_token = 0\n    whole_token = 0\n    nbest_pred_results = []\n    pred_scores = []\n    pred_results = []\n    gold_results = []\n    ## set model in eval model\n    model.eval()\n    batch_size = data.HP_batch_size\n    start_time = time.time()\n    train_num = len(instances)\n    total_batch = train_num // batch_size + 1\n    for batch_id in range(total_batch):\n        start = batch_id * batch_size\n        end = (batch_id + 1) * batch_size\n        if end > train_num:\n            end = train_num\n        instance = instances[start:end]\n        if not instance:\n            continue\n        batch_word, batch_features, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask = batchify_with_label(\n            instance, data.HP_gpu, True)\n        if nbest:\n            scores, nbest_tag_seq = model.decode_nbest(batch_word, batch_features, batch_wordlen, batch_char,\n                                                       batch_charlen, batch_charrecover, mask, nbest)\n            nbest_pred_result = recover_nbest_label(nbest_tag_seq, mask, data.label_alphabet, batch_wordrecover)\n            nbest_pred_results += nbest_pred_result\n            pred_scores += scores[batch_wordrecover].cpu().data.numpy().tolist()\n            ## select the best sequence to evalurate\n            tag_seq = nbest_tag_seq[:, :, 0]\n        else:\n            tag_seq = model(batch_word, batch_features, batch_wordlen, batch_char, batch_charlen, batch_charrecover,\n                            mask)\n        # print(""tag:"",tag_seq)\n        pred_label, gold_label = recover_label(tag_seq, batch_label, mask, data.label_alphabet, batch_wordrecover)\n        pred_results += pred_label\n        gold_results += gold_label\n    decode_time = time.time() - start_time\n    speed = len(instances) / decode_time\n    acc, p, r, f = get_ner_fmeasure(gold_results, pred_results, data.tagScheme)\n    if nbest:\n        return speed, acc, p, r, f, nbest_pred_results, pred_scores\n    return speed, acc, p, r, f, pred_results, pred_scores\n\n\ndef batchify_with_label(input_batch_list, gpu, volatile_flag=False):\n    """"""\n        input: list of words, chars and labels, various length. [[words,chars, labels],[words,chars,labels],...]\n            words: word ids for one sentence. (batch_size, sent_len)\n            chars: char ids for on sentences, various length. (batch_size, sent_len, each_word_length)\n        output:\n            zero padding for word and char, with their batch length\n            word_seq_tensor: (batch_size, max_sent_len) Variable\n            word_seq_lengths: (batch_size,1) Tensor\n            char_seq_tensor: (batch_size*max_sent_len, max_word_len) Variable\n            char_seq_lengths: (batch_size*max_sent_len,1) Tensor\n            char_seq_recover: (batch_size*max_sent_len,1)  recover char sequence order\n            label_seq_tensor: (batch_size, max_sent_len)\n            mask: (batch_size, max_sent_len)\n    """"""\n    # label_inputs\n    batch_size = len(input_batch_list)\n    words = [sent[0] for sent in input_batch_list]\n    features = [np.asarray(sent[1]) for sent in input_batch_list]\n    feature_num = len(features[0][0])\n    chars = [sent[2] for sent in input_batch_list]\n    labels = [sent[3] for sent in input_batch_list]\n    word_seq_lengths = torch.LongTensor(map(len, words))\n    max_seq_len = word_seq_lengths.max()\n    word_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)), volatile=volatile_flag).long()\n    label_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len)), volatile=volatile_flag).long()\n    feature_seq_tensors = []\n    for idx in range(feature_num):\n        feature_seq_tensors.append(\n            autograd.Variable(torch.zeros((batch_size, max_seq_len)), volatile=volatile_flag).long())\n    mask = autograd.Variable(torch.zeros((batch_size, max_seq_len)), volatile=volatile_flag).byte()\n    for idx, (seq, label, seqlen) in enumerate(zip(words, labels, word_seq_lengths)):\n        word_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n        label_seq_tensor[idx, :seqlen] = torch.LongTensor(label)\n        mask[idx, :seqlen] = torch.Tensor([1] * seqlen)\n        for idy in range(feature_num):\n            feature_seq_tensors[idy][idx, :seqlen] = torch.LongTensor(features[idx][:, idy])\n    word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n    word_seq_tensor = word_seq_tensor[word_perm_idx]\n    for idx in range(feature_num):\n        feature_seq_tensors[idx] = feature_seq_tensors[idx][word_perm_idx]\n\n    label_seq_tensor = label_seq_tensor[word_perm_idx]\n    mask = mask[word_perm_idx]\n    ### deal with char\n    # pad_chars (batch_size, max_seq_len)\n    pad_chars = [chars[idx] + [[0]] * (max_seq_len - len(chars[idx])) for idx in range(len(chars))]\n    length_list = [map(len, pad_char) for pad_char in pad_chars]\n    max_word_len = max(map(max, length_list))\n    char_seq_tensor = autograd.Variable(torch.zeros((batch_size, max_seq_len, max_word_len)),\n                                        volatile=volatile_flag).long()\n    char_seq_lengths = torch.LongTensor(length_list)\n    for idx, (seq, seqlen) in enumerate(zip(pad_chars, char_seq_lengths)):\n        for idy, (word, wordlen) in enumerate(zip(seq, seqlen)):\n            # print len(word), wordlen\n            char_seq_tensor[idx, idy, :wordlen] = torch.LongTensor(word)\n\n    char_seq_tensor = char_seq_tensor[word_perm_idx].view(batch_size * max_seq_len, -1)\n    char_seq_lengths = char_seq_lengths[word_perm_idx].view(batch_size * max_seq_len, )\n    char_seq_lengths, char_perm_idx = char_seq_lengths.sort(0, descending=True)\n    char_seq_tensor = char_seq_tensor[char_perm_idx]\n    _, char_seq_recover = char_perm_idx.sort(0, descending=False)\n    _, word_seq_recover = word_perm_idx.sort(0, descending=False)\n    if gpu:\n        word_seq_tensor = word_seq_tensor.cuda()\n        for idx in range(feature_num):\n            feature_seq_tensors[idx] = feature_seq_tensors[idx].cuda()\n        word_seq_lengths = word_seq_lengths.cuda()\n        word_seq_recover = word_seq_recover.cuda()\n        label_seq_tensor = label_seq_tensor.cuda()\n        char_seq_tensor = char_seq_tensor.cuda()\n        char_seq_recover = char_seq_recover.cuda()\n        mask = mask.cuda()\n    return word_seq_tensor, feature_seq_tensors, word_seq_lengths, word_seq_recover, char_seq_tensor, char_seq_lengths, char_seq_recover, label_seq_tensor, mask\n\n\ndef train(data):\n    print(""Training model..."")\n    data.show_data_summary()\n    save_data_name = data.model_dir + "".dset""\n    data.save(save_data_name)\n    model = SeqModel(data)\n    loss_function = nn.NLLLoss()\n    if data.optimizer.lower() == ""sgd"":\n        optimizer = optim.SGD(model.parameters(), lr=data.HP_lr, momentum=data.HP_momentum, weight_decay=data.HP_l2)\n    elif data.optimizer.lower() == ""adagrad"":\n        optimizer = optim.Adagrad(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)\n    elif data.optimizer.lower() == ""adadelta"":\n        optimizer = optim.Adadelta(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)\n    elif data.optimizer.lower() == ""rmsprop"":\n        optimizer = optim.RMSprop(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)\n    elif data.optimizer.lower() == ""adam"":\n        optimizer = optim.Adam(model.parameters(), lr=data.HP_lr, weight_decay=data.HP_l2)\n    else:\n        print(""Optimizer illegal: %s"" % (data.optimizer))\n        exit(0)\n    best_dev = -10\n    # data.HP_iteration = 1\n    ## start training\n    for idx in range(data.HP_iteration):\n        epoch_start = time.time()\n        temp_start = epoch_start\n        print(""Epoch: %s/%s"" % (idx, data.HP_iteration))\n        if data.optimizer == ""SGD"":\n            optimizer = lr_decay(optimizer, idx, data.HP_lr_decay, data.HP_lr)\n        instance_count = 0\n        sample_id = 0\n        sample_loss = 0\n        total_loss = 0\n        right_token = 0\n        whole_token = 0\n        random.shuffle(data.train_Ids)\n        ## set model in train model\n        model.train()\n        model.zero_grad()\n        batch_size = data.HP_batch_size\n        batch_id = 0\n        train_num = len(data.train_Ids)\n        total_batch = train_num // batch_size + 1\n        for batch_id in range(total_batch):\n            start = batch_id * batch_size\n            end = (batch_id + 1) * batch_size\n            if end > train_num:\n                end = train_num\n            instance = data.train_Ids[start:end]\n            if not instance:\n                continue\n            batch_word, batch_features, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask, label_inputs = batchify_with_label(\n                instance, data.HP_gpu)\n            instance_count += 1\n            loss, tag_seq = model.neg_log_likelihood_loss(batch_word, batch_features, batch_wordlen, batch_char,\n                                                          batch_charlen, batch_charrecover, batch_label, mask,\n                                                          data.label_alphabet_size)\n            right, whole = predict_check(tag_seq, batch_label, mask)\n            right_token += right\n            whole_token += whole\n            sample_loss += loss.data[0]\n            total_loss += loss.data[0]\n            if end % 500 == 0:\n                temp_time = time.time()\n                temp_cost = temp_time - temp_start\n                temp_start = temp_time\n                print(""     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f"" % (\n                end, temp_cost, sample_loss, right_token, whole_token, (right_token + 0.) / whole_token))\n                sys.stdout.flush()\n                sample_loss = 0\n            loss.backward()\n            optimizer.step()\n            model.zero_grad()\n        temp_time = time.time()\n        temp_cost = temp_time - temp_start\n        print(""     Instance: %s; Time: %.2fs; loss: %.4f; acc: %s/%s=%.4f"" % (\n        end, temp_cost, sample_loss, right_token, whole_token, (right_token + 0.) / whole_token))\n        epoch_finish = time.time()\n        epoch_cost = epoch_finish - epoch_start\n        print(""Epoch: %s training finished. Time: %.2fs, speed: %.2fst/s,  total loss: %s"" % (\n        idx, epoch_cost, train_num / epoch_cost, total_loss))\n        # continue\n        speed, acc, p, r, f, _, _ = evaluate(data, model, ""dev"")\n        dev_finish = time.time()\n        dev_cost = dev_finish - epoch_finish\n\n        if data.seg:\n            current_score = f\n            print(""Dev: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f"" % (\n            dev_cost, speed, acc, p, r, f))\n        else:\n            current_score = acc\n            print(""Dev: time: %.2fs speed: %.2fst/s; acc: %.4f"" % (dev_cost, speed, acc))\n\n        if current_score > best_dev:\n            if data.seg:\n                print(""Exceed previous best f score:"", best_dev)\n            else:\n                print(""Exceed previous best acc score:"", best_dev)\n            model_name = data.model_dir + \'.\' + str(idx) + "".model""\n            print(""Save current best model in file:"", model_name)\n            torch.save(model.state_dict(), model_name)\n            best_dev = current_score\n            # ## decode test\n        speed, acc, p, r, f, _, _ = evaluate(data, model, ""test"")\n        test_finish = time.time()\n        test_cost = test_finish - dev_finish\n        if data.seg:\n            print(""Test: time: %.2fs, speed: %.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f"" % (\n            test_cost, speed, acc, p, r, f))\n        else:\n            print(""Test: time: %.2fs, speed: %.2fst/s; acc: %.4f"" % (test_cost, speed, acc))\n        gc.collect()\n\n\ndef load_model_decode(data, name):\n    print(""Load Model from file: "", data.model_dir)\n    model = SeqModel(data)\n    ## load model need consider if the model trained in GPU and load in CPU, or vice versa\n    # if not gpu:\n    #     model.load_state_dict(torch.load(model_dir))\n    #     # model.load_state_dict(torch.load(model_dir), map_location=lambda storage, loc: storage)\n    #     # model = torch.load(model_dir, map_location=lambda storage, loc: storage)\n    # else:\n    #     model.load_state_dict(torch.load(model_dir))\n    #     # model = torch.load(model_dir)\n    model.load_state_dict(torch.load(data.load_model_dir))\n\n    print(""Decode %s data, nbest: %s ..."" % (name, data.nbest))\n    start_time = time.time()\n    speed, acc, p, r, f, pred_results, pred_scores = evaluate(data, model, name, data.nbest)\n    end_time = time.time()\n    time_cost = end_time - start_time\n    if data.seg:\n        print(""%s: time:%.2fs, speed:%.2fst/s; acc: %.4f, p: %.4f, r: %.4f, f: %.4f"" % (\n        name, time_cost, speed, acc, p, r, f))\n    else:\n        print(""%s: time:%.2fs, speed:%.2fst/s; acc: %.4f"" % (name, time_cost, speed, acc))\n    return pred_results, pred_scores\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Tuning with NCRF++\')\n    parser.add_argument(\'--wordemb\', help=\'Embedding for words\', default=\'None\')\n    parser.add_argument(\'--charemb\', help=\'Embedding for chars\', default=\'None\')\n    parser.add_argument(\'--status\', choices=[\'train\', \'decode\'], help=\'update algorithm\', default=\'train\')\n    parser.add_argument(\'--savemodel\', default=""data/model/saved_model.lstmcrf."")\n    parser.add_argument(\'--savedset\', help=\'Dir of saved data setting\')\n    parser.add_argument(\'--train\', default=""data/conll03/train.bmes"")\n    parser.add_argument(\'--dev\', default=""data/conll03/dev.bmes"")\n    parser.add_argument(\'--test\', default=""data/conll03/test.bmes"")\n    parser.add_argument(\'--seg\', default=""True"")\n    parser.add_argument(\'--raw\')\n    parser.add_argument(\'--loadmodel\')\n    parser.add_argument(\'--output\')\n    args = parser.parse_args()\n    data = Data()\n\n    data.train_dir = args.train\n    data.dev_dir = args.dev\n    data.test_dir = args.test\n    data.model_dir = args.savemodel\n    data.dset_dir = args.savedset\n    print(""aaa"", data.dset_dir)\n    status = args.status.lower()\n    save_model_dir = args.savemodel\n    data.HP_gpu = torch.cuda.is_available()\n    print(""Seed num:"", seed_num)\n    data.number_normalized = True\n    data.word_emb_dir = ""../data/glove.6B.100d.txt""\n\n    if status == \'train\':\n        print(""MODEL: train"")\n        data_initialization(data)\n        data.use_char = True\n        data.HP_batch_size = 10\n        data.HP_lr = 0.015\n        data.char_seq_feature = ""CNN""\n        # data preprocessing\n        data.generate_instance(\'train\')\n        data.generate_instance(\'dev\')\n        data.generate_instance(\'test\')\n        data.build_pretrain_emb()\n        train(data)\n    elif status == \'decode\':\n        print(""MODEL: decode"")\n        data.load(data.dset_dir)\n        data.raw_dir = args.raw\n        data.decode_dir = args.output\n        data.load_model_dir = args.loadmodel\n        data.show_data_summary()\n        data.generate_instance(\'raw\')\n        print(""nbest: %s"" % (data.nbest))\n        decode_results, pred_scores = load_model_decode(data, \'raw\')\n        if data.nbest:\n            data.write_nbest_decoded_results(decode_results, pred_scores, \'raw\')\n        else:\n            data.write_decoded_results(decode_results, \'raw\')\n    else:\n        print(""Invalid argument! Please use valid arguments! (train/test/decode)"")\n\n\n\n'"
model/__init__.py,0,"b""__author__ = 'max'\n"""
model/charbigru.py,4,"b'# -*- coding: utf-8 -*-\n# @Author: Jie Yang\n# @Date:   2017-10-17 16:47:32\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2018-04-26 13:22:51\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport numpy as np\n\nclass CharBiGRU(nn.Module):\n    def __init__(self, alphabet_size, pretrain_char_embedding, embedding_dim, hidden_dim, dropout, gpu, bidirect_flag = True):\n        super(CharBiGRU, self).__init__()\n        print(""build char sequence feature extractor: GRU ..."")\n        self.gpu = gpu\n        self.hidden_dim = hidden_dim\n        if bidirect_flag:\n            self.hidden_dim = hidden_dim // 2\n        self.char_drop = nn.Dropout(dropout)\n        self.char_embeddings = nn.Embedding(alphabet_size, embedding_dim)\n        if pretrain_char_embedding is not None:\n            self.char_embeddings.weight.data.copy_(torch.from_numpy(pretrain_char_embedding))\n        else:\n            self.char_embeddings.weight.data.copy_(torch.from_numpy(self.random_embedding(alphabet_size, embedding_dim)))\n        self.char_lstm = nn.GRU(embedding_dim, self.hidden_dim, num_layers=1, batch_first=True, bidirectional=bidirect_flag)\n        if self.gpu:\n            self.char_drop = self.char_drop.cuda()\n            self.char_embeddings = self.char_embeddings.cuda()\n            self.char_lstm = self.char_lstm.cuda()\n\n\n    def random_embedding(self, vocab_size, embedding_dim):\n        pretrain_emb = np.empty([vocab_size, embedding_dim])\n        scale = np.sqrt(3.0 / embedding_dim)\n        for index in range(vocab_size):\n            pretrain_emb[index,:] = np.random.uniform(-scale, scale, [1, embedding_dim])\n        return pretrain_emb\n\n\n    def get_last_hiddens(self, input, seq_lengths):\n        """"""\n            input:\n                input: Variable(batch_size, word_length)\n                seq_lengths: numpy array (batch_size,  1)\n            output:\n                Variable(batch_size, char_hidden_dim)\n            Note it only accepts ordered (length) variable, length size is recorded in seq_lengths\n        """"""\n        batch_size = input.size(0)\n        char_embeds = self.char_drop(self.char_embeddings(input))\n        char_hidden = None\n        pack_input = pack_padded_sequence(char_embeds, seq_lengths, True)\n        char_rnn_out, char_hidden = self.char_lstm(pack_input, char_hidden)\n        char_rnn_out, _ = pad_packed_sequence(char_rnn_out)\n        return char_hidden.transpose(1,0).contiguous().view(batch_size,-1)\n\n    def get_all_hiddens(self, input, seq_lengths):\n        """"""\n            input:\n                input: Variable(batch_size,  word_length)\n                seq_lengths: numpy array (batch_size,  1)\n            output:\n                Variable(batch_size, word_length, char_hidden_dim)\n            Note it only accepts ordered (length) variable, length size is recorded in seq_lengths\n        """"""\n        batch_size = input.size(0)\n        char_embeds = self.char_drop(self.char_embeddings(input))\n        char_hidden = None\n        pack_input = pack_padded_sequence(char_embeds, seq_lengths, True)\n        char_rnn_out, char_hidden = self.char_lstm(pack_input, char_hidden)\n        char_rnn_out, _ = pad_packed_sequence(char_rnn_out)\n        return char_rnn_out.transpose(1,0)\n\n\n    def forward(self, input, seq_lengths):\n        return self.get_all_hiddens(input, seq_lengths)\n'"
model/charbilstm.py,4,"b'# -*- coding: utf-8 -*-\n# @Author: Jie Yang\n# @Date:   2017-10-17 16:47:32\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2018-04-26 13:22:34\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport numpy as np\n\nclass CharBiLSTM(nn.Module):\n    def __init__(self, alphabet_size, pretrain_char_embedding, embedding_dim, hidden_dim, dropout, gpu, bidirect_flag = True):\n        super(CharBiLSTM, self).__init__()\n        print(""build char sequence feature extractor: LSTM ..."")\n        self.gpu = gpu\n        self.hidden_dim = hidden_dim\n        if bidirect_flag:\n            self.hidden_dim = hidden_dim // 2\n        self.char_drop = nn.Dropout(dropout)\n        self.char_embeddings = nn.Embedding(alphabet_size, embedding_dim)\n        if pretrain_char_embedding is not None:\n            self.char_embeddings.weight.data.copy_(torch.from_numpy(pretrain_char_embedding))\n        else:\n            self.char_embeddings.weight.data.copy_(torch.from_numpy(self.random_embedding(alphabet_size, embedding_dim)))\n        self.char_lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=1, batch_first=True, bidirectional=bidirect_flag)\n        if self.gpu:\n            self.char_drop = self.char_drop.cuda()\n            self.char_embeddings = self.char_embeddings.cuda()\n            self.char_lstm = self.char_lstm.cuda()\n\n\n    def random_embedding(self, vocab_size, embedding_dim):\n        pretrain_emb = np.empty([vocab_size, embedding_dim])\n        scale = np.sqrt(3.0 / embedding_dim)\n        for index in range(vocab_size):\n            pretrain_emb[index,:] = np.random.uniform(-scale, scale, [1, embedding_dim])\n        return pretrain_emb\n\n\n    def get_last_hiddens(self, input, seq_lengths):\n        """"""\n            input:\n                input: Variable(batch_size, word_length)\n                seq_lengths: numpy array (batch_size,  1)\n            output:\n                Variable(batch_size, char_hidden_dim)\n            Note it only accepts ordered (length) variable, length size is recorded in seq_lengths\n        """"""\n        batch_size = input.size(0)\n        char_embeds = self.char_drop(self.char_embeddings(input))\n        char_hidden = None\n        pack_input = pack_padded_sequence(char_embeds, seq_lengths, True)\n        char_rnn_out, char_hidden = self.char_lstm(pack_input, char_hidden)\n        char_rnn_out, _ = pad_packed_sequence(char_rnn_out)\n        return char_hidden[0].transpose(1,0).contiguous().view(batch_size,-1)\n\n    def get_all_hiddens(self, input, seq_lengths):\n        """"""\n            input:\n                input: Variable(batch_size,  word_length)\n                seq_lengths: numpy array (batch_size,  1)\n            output:\n                Variable(batch_size, word_length, char_hidden_dim)\n            Note it only accepts ordered (length) variable, length size is recorded in seq_lengths\n        """"""\n        batch_size = input.size(0)\n        char_embeds = self.char_drop(self.char_embeddings(input))\n        char_hidden = None\n        pack_input = pack_padded_sequence(char_embeds, seq_lengths, True)\n        char_rnn_out, char_hidden = self.char_lstm(pack_input, char_hidden)\n        char_rnn_out, _ = pad_packed_sequence(char_rnn_out)\n        return char_rnn_out.transpose(1,0)\n\n\n    def forward(self, input, seq_lengths):\n        return self.get_all_hiddens(input, seq_lengths)\n'"
model/charcnn.py,4,"b'# -*- coding: utf-8 -*-\n# @Author: Jie Yang\n# @Date:   2017-10-17 16:47:32\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2018-04-26 13:21:40\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass CharCNN(nn.Module):\n    def __init__(self, alphabet_size, pretrain_char_embedding, embedding_dim, hidden_dim, dropout, gpu):\n        super(CharCNN, self).__init__()\n        print(""build char sequence feature extractor: CNN ..."")\n        self.gpu = gpu\n        self.hidden_dim = hidden_dim\n        self.char_drop = nn.Dropout(dropout)\n        self.char_embeddings = nn.Embedding(alphabet_size, embedding_dim)\n        if pretrain_char_embedding is not None:\n            self.char_embeddings.weight.data.copy_(torch.from_numpy(pretrain_char_embedding))\n        else:\n            self.char_embeddings.weight.data.copy_(torch.from_numpy(self.random_embedding(alphabet_size, embedding_dim)))\n        self.char_cnn = nn.Conv1d(embedding_dim, self.hidden_dim, kernel_size=3, padding=1)\n        if self.gpu:\n            self.char_drop = self.char_drop.cuda()\n            self.char_embeddings = self.char_embeddings.cuda()\n            self.char_cnn = self.char_cnn.cuda()\n\n\n    def random_embedding(self, vocab_size, embedding_dim):\n        pretrain_emb = np.empty([vocab_size, embedding_dim])\n        scale = np.sqrt(3.0 / embedding_dim)\n        for index in range(vocab_size):\n            pretrain_emb[index,:] = np.random.uniform(-scale, scale, [1, embedding_dim])\n        return pretrain_emb\n\n\n    def get_last_hiddens(self, input, seq_lengths):\n        """"""\n            input:\n                input: Variable(batch_size, word_length)\n                seq_lengths: numpy array (batch_size,  1)\n            output:\n                Variable(batch_size, char_hidden_dim)\n            Note it only accepts ordered (length) variable, length size is recorded in seq_lengths\n        """"""\n        batch_size = input.size(0)\n        char_embeds = self.char_drop(self.char_embeddings(input))\n        char_embeds = char_embeds.transpose(2,1).contiguous()\n        char_cnn_out = self.char_cnn(char_embeds)\n        char_cnn_out = F.max_pool1d(char_cnn_out, char_cnn_out.size(2)).view(batch_size, -1)\n        return char_cnn_out\n\n    def get_all_hiddens(self, input, seq_lengths):\n        """"""\n            input:\n                input: Variable(batch_size,  word_length)\n                seq_lengths: numpy array (batch_size,  1)\n            output:\n                Variable(batch_size, word_length, char_hidden_dim)\n            Note it only accepts ordered (length) variable, length size is recorded in seq_lengths\n        """"""\n        batch_size = input.size(0)\n        char_embeds = self.char_drop(self.char_embeddings(input))\n        char_embeds = char_embeds.transpose(2,1).contiguous()\n        char_cnn_out = self.char_cnn(char_embeds).transpose(2,1).contiguous()\n        return char_cnn_out\n\n\n\n    def forward(self, input, seq_lengths):\n        return self.get_all_hiddens(input, seq_lengths)\n'"
model/crf.py,34,"b'# -*- coding: utf-8 -*-\n# @Author: Jie Yang\n# @Date:   2017-12-04 23:19:38\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2018-09-24 23:15:31\nfrom __future__ import print_function\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nSTART_TAG = -2\nSTOP_TAG = -1\n\n\n# Compute log sum exp in a numerically stable way for the forward algorithm\ndef log_sum_exp(vec, m_size):\n    """"""\n    calculate log of exp sum\n    args:\n        vec (batch_size, vanishing_dim, hidden_dim) : input tensor\n        m_size : hidden_dim\n    return:\n        batch_size, hidden_dim\n    """"""\n    _, idx = torch.max(vec, 1)  # B * 1 * M\n    max_score = torch.gather(vec, 1, idx.view(-1, 1, m_size)).view(-1, 1, m_size)  # B * M\n    return max_score.view(-1, m_size) + torch.log(torch.sum(torch.exp(vec - max_score.expand_as(vec)), 1)).view(-1, m_size)  # B * M\n\nclass CRF(nn.Module):\n\n    def __init__(self, tagset_size, gpu):\n        super(CRF, self).__init__()\n        print(""build CRF..."")\n        self.gpu = gpu\n        # Matrix of transition parameters.  Entry i,j is the score of transitioning from i to j.\n        self.tagset_size = tagset_size\n        # # We add 2 here, because of START_TAG and STOP_TAG\n        # # transitions (f_tag_size, t_tag_size), transition value from f_tag to t_tag\n        init_transitions = torch.zeros(self.tagset_size+2, self.tagset_size+2)\n        init_transitions[:,START_TAG] = -10000.0\n        init_transitions[STOP_TAG,:] = -10000.0\n        init_transitions[:,0] = -10000.0\n        init_transitions[0,:] = -10000.0\n        if self.gpu:\n            init_transitions = init_transitions.cuda()\n        self.transitions = nn.Parameter(init_transitions)\n\n        # self.transitions = nn.Parameter(torch.Tensor(self.tagset_size+2, self.tagset_size+2))\n        # self.transitions.data.zero_()\n\n    def _calculate_PZ(self, feats, mask):\n        """"""\n            input:\n                feats: (batch, seq_len, self.tag_size+2)\n                masks: (batch, seq_len)\n        """"""\n        batch_size = feats.size(0)\n        seq_len = feats.size(1)\n        tag_size = feats.size(2)\n        # print feats.view(seq_len, tag_size)\n        assert(tag_size == self.tagset_size+2)\n        mask = mask.transpose(1,0).contiguous()\n        ins_num = seq_len * batch_size\n        ## be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1)\n        feats = feats.transpose(1,0).contiguous().view(ins_num,1, tag_size).expand(ins_num, tag_size, tag_size)\n        ## need to consider start\n        scores = feats + self.transitions.view(1,tag_size,tag_size).expand(ins_num, tag_size, tag_size)\n        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n        # build iter\n        seq_iter = enumerate(scores)\n        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n        # only need start from start_tag\n        partition = inivalues[:, START_TAG, :].clone().view(batch_size, tag_size, 1)  # bat_size * to_target_size\n\n        ## add start score (from start to all tag, duplicate to batch_size)\n        # partition = partition + self.transitions[START_TAG,:].view(1, tag_size, 1).expand(batch_size, tag_size, 1)\n        # iter over last scores\n        for idx, cur_values in seq_iter:\n            # previous to_target is current from_target\n            # partition: previous results log(exp(from_target)), #(batch_size * from_target)\n            # cur_values: bat_size * from_target * to_target\n\n            cur_values = cur_values + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n            cur_partition = log_sum_exp(cur_values, tag_size)\n            # print cur_partition.data\n\n                # (bat_size * from_target * to_target) -> (bat_size * to_target)\n            # partition = utils.switch(partition, cur_partition, mask[idx].view(bat_size, 1).expand(bat_size, self.tagset_size)).view(bat_size, -1)\n            mask_idx = mask[idx, :].view(batch_size, 1).expand(batch_size, tag_size)\n\n            ## effective updated partition part, only keep the partition value of mask value = 1\n            masked_cur_partition = cur_partition.masked_select(mask_idx)\n            ## let mask_idx broadcastable, to disable warning\n            mask_idx = mask_idx.contiguous().view(batch_size, tag_size, 1)\n\n            ## replace the partition where the maskvalue=1, other partition value keeps the same\n            partition.masked_scatter_(mask_idx, masked_cur_partition)\n        # until the last state, add transition score for all partition (and do log_sum_exp) then select the value in STOP_TAG\n        cur_values = self.transitions.view(1,tag_size, tag_size).expand(batch_size, tag_size, tag_size) + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n        cur_partition = log_sum_exp(cur_values, tag_size)\n        final_partition = cur_partition[:, STOP_TAG]\n        return final_partition.sum(), scores\n\n\n    def _viterbi_decode(self, feats, mask):\n        """"""\n            input:\n                feats: (batch, seq_len, self.tag_size+2)\n                mask: (batch, seq_len)\n            output:\n                decode_idx: (batch, seq_len) decoded sequence\n                path_score: (batch, 1) corresponding score for each sequence (to be implementated)\n        """"""\n        batch_size = feats.size(0)\n        seq_len = feats.size(1)\n        tag_size = feats.size(2)\n        assert(tag_size == self.tagset_size+2)\n        ## calculate sentence length for each sentence\n        length_mask = torch.sum(mask.long(), dim = 1).view(batch_size,1).long()\n        ## mask to (seq_len, batch_size)\n        mask = mask.transpose(1,0).contiguous()\n        ins_num = seq_len * batch_size\n        ## be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1)\n        feats = feats.transpose(1,0).contiguous().view(ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n        ## need to consider start\n        scores = feats + self.transitions.view(1,tag_size,tag_size).expand(ins_num, tag_size, tag_size)\n        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n\n        # build iter\n        seq_iter = enumerate(scores)\n        ## record the position of best score\n        back_points = list()\n        partition_history = list()\n        ##  reverse mask (bug for mask = 1- mask, use this as alternative choice)\n        # mask = 1 + (-1)*mask\n        mask =  (1 - mask.long()).byte()\n        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n        # only need start from start_tag\n        partition = inivalues[:, START_TAG, :].clone().view(batch_size, tag_size)  # bat_size * to_target_size\n        # print ""init part:"",partition.size()\n        partition_history.append(partition)\n        # iter over last scores\n        for idx, cur_values in seq_iter:\n            # previous to_target is current from_target\n            # partition: previous results log(exp(from_target)), #(batch_size * from_target)\n            # cur_values: batch_size * from_target * to_target\n            cur_values = cur_values + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n            ## forscores, cur_bp = torch.max(cur_values[:,:-2,:], 1) # do not consider START_TAG/STOP_TAG\n            # print ""cur value:"", cur_values.size()\n            partition, cur_bp = torch.max(cur_values, 1)\n            # print ""partsize:"",partition.size()\n            # exit(0)\n            # print partition\n            # print cur_bp\n            # print ""one best, "",idx\n            partition_history.append(partition)\n            ## cur_bp: (batch_size, tag_size) max source score position in current tag\n            ## set padded label as 0, which will be filtered in post processing\n            cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n            back_points.append(cur_bp)\n        # exit(0)\n        ### add score to final STOP_TAG\n        partition_history = torch.cat(partition_history, 0).view(seq_len, batch_size, -1).transpose(1,0).contiguous() ## (batch_size, seq_len. tag_size)\n        ### get the last position for each setences, and select the last partitions using gather()\n        last_position = length_mask.view(batch_size,1,1).expand(batch_size, 1, tag_size) -1\n        last_partition = torch.gather(partition_history, 1, last_position).view(batch_size,tag_size,1)\n        ### calculate the score from last partition to end state (and then select the STOP_TAG from it)\n        last_values = last_partition.expand(batch_size, tag_size, tag_size) + self.transitions.view(1,tag_size, tag_size).expand(batch_size, tag_size, tag_size)\n        _, last_bp = torch.max(last_values, 1)\n        pad_zero = autograd.Variable(torch.zeros(batch_size, tag_size)).long()\n        if self.gpu:\n            pad_zero = pad_zero.cuda()\n        back_points.append(pad_zero)\n        back_points  =  torch.cat(back_points).view(seq_len, batch_size, tag_size)\n\n        ## select end ids in STOP_TAG\n        pointer = last_bp[:, STOP_TAG]\n        insert_last = pointer.contiguous().view(batch_size,1,1).expand(batch_size,1, tag_size)\n        back_points = back_points.transpose(1,0).contiguous()\n        ## move the end ids(expand to tag_size) to the corresponding position of back_points to replace the 0 values\n        # print ""lp:"",last_position\n        # print ""il:"",insert_last\n        back_points.scatter_(1, last_position, insert_last)\n        # print ""bp:"",back_points\n        # exit(0)\n        back_points = back_points.transpose(1,0).contiguous()\n        ## decode from the end, padded position ids are 0, which will be filtered if following evaluation\n        decode_idx = autograd.Variable(torch.LongTensor(seq_len, batch_size))\n        if self.gpu:\n            decode_idx = decode_idx.cuda()\n        decode_idx[-1] = pointer.data\n        for idx in range(len(back_points)-2, -1, -1):\n            pointer = torch.gather(back_points[idx], 1, pointer.contiguous().view(batch_size, 1))\n            decode_idx[idx] = pointer.data\n        path_score = None\n        decode_idx = decode_idx.transpose(1,0)\n        return path_score, decode_idx\n\n\n\n    def forward(self, feats):\n    \tpath_score, best_path = self._viterbi_decode(feats)\n    \treturn path_score, best_path\n\n\n    def _score_sentence(self, scores, mask, tags):\n        """"""\n            input:\n                scores: variable (seq_len, batch, tag_size, tag_size)\n                mask: (batch, seq_len)\n                tags: tensor  (batch, seq_len)\n            output:\n                score: sum of score for gold sequences within whole batch\n        """"""\n        # Gives the score of a provided tag sequence\n        batch_size = scores.size(1)\n        seq_len = scores.size(0)\n        tag_size = scores.size(2)\n        ## convert tag value into a new format, recorded label bigram information to index\n        new_tags = autograd.Variable(torch.LongTensor(batch_size, seq_len))\n        if self.gpu:\n            new_tags = new_tags.cuda()\n        for idx in range(seq_len):\n            if idx == 0:\n                ## start -> first score\n                new_tags[:,0] =  (tag_size - 2)*tag_size + tags[:,0]\n\n            else:\n                new_tags[:,idx] =  tags[:,idx-1]*tag_size + tags[:,idx]\n\n        ## transition for label to STOP_TAG\n        end_transition = self.transitions[:,STOP_TAG].contiguous().view(1, tag_size).expand(batch_size, tag_size)\n        ## length for batch,  last word position = length - 1\n        length_mask = torch.sum(mask.long(), dim = 1).view(batch_size,1).long()\n        ## index the label id of last word\n        end_ids = torch.gather(tags, 1, length_mask - 1)\n\n        ## index the transition score for end_id to STOP_TAG\n        end_energy = torch.gather(end_transition, 1, end_ids)\n\n        ## convert tag as (seq_len, batch_size, 1)\n        new_tags = new_tags.transpose(1,0).contiguous().view(seq_len, batch_size, 1)\n        ### need convert tags id to search from 400 positions of scores\n        tg_energy = torch.gather(scores.view(seq_len, batch_size, -1), 2, new_tags).view(seq_len, batch_size)  # seq_len * bat_size\n        ## mask transpose to (seq_len, batch_size)\n        tg_energy = tg_energy.masked_select(mask.transpose(1,0))\n\n        # ## calculate the score from START_TAG to first label\n        # start_transition = self.transitions[START_TAG,:].view(1, tag_size).expand(batch_size, tag_size)\n        # start_energy = torch.gather(start_transition, 1, tags[0,:])\n\n        ## add all score together\n        # gold_score = start_energy.sum() + tg_energy.sum() + end_energy.sum()\n        gold_score = tg_energy.sum() + end_energy.sum()\n        return gold_score\n\n    def neg_log_likelihood_loss(self, feats, mask, tags):\n        # nonegative log likelihood\n        batch_size = feats.size(0)\n        forward_score, scores = self._calculate_PZ(feats, mask)\n        gold_score = self._score_sentence(scores, mask, tags)\n        # print ""batch, f:"", forward_score.data[0], "" g:"", gold_score.data[0], "" dis:"", forward_score.data[0] - gold_score.data[0]\n        # exit(0)\n        return forward_score - gold_score\n\n\n\n    def _viterbi_decode_nbest(self, feats, mask, nbest):\n        """"""\n            input:\n                feats: (batch, seq_len, self.tag_size+2)\n                mask: (batch, seq_len)\n            output:\n                decode_idx: (batch, nbest, seq_len) decoded sequence\n                path_score: (batch, nbest) corresponding score for each sequence (to be implementated)\n                nbest decode for sentence with one token is not well supported, to be optimized\n        """"""\n        batch_size = feats.size(0)\n        seq_len = feats.size(1)\n        tag_size = feats.size(2)\n        assert(tag_size == self.tagset_size+2)\n        ## calculate sentence length for each sentence\n        length_mask = torch.sum(mask.long(), dim = 1).view(batch_size,1).long()\n        ## mask to (seq_len, batch_size)\n        mask = mask.transpose(1,0).contiguous()\n        ins_num = seq_len * batch_size\n        ## be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1)\n        feats = feats.transpose(1,0).contiguous().view(ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n        ## need to consider start\n        scores = feats + self.transitions.view(1,tag_size,tag_size).expand(ins_num, tag_size, tag_size)\n        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n\n        # build iter\n        seq_iter = enumerate(scores)\n        ## record the position of best score\n        back_points = list()\n        partition_history = list()\n        ##  reverse mask (bug for mask = 1- mask, use this as alternative choice)\n        # mask = 1 + (-1)*mask\n        mask =  (1 - mask.long()).byte()\n        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n        # only need start from start_tag\n        partition = inivalues[:, START_TAG, :].clone()  # bat_size * to_target_size\n        ## initial partition [batch_size, tag_size]\n        partition_history.append(partition.view(batch_size, tag_size, 1).expand(batch_size, tag_size, nbest))\n        # iter over last scores\n        for idx, cur_values in seq_iter:\n            if idx == 1:\n                cur_values = cur_values.view(batch_size, tag_size, tag_size) + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n            else:\n                # previous to_target is current from_target\n                # partition: previous results log(exp(from_target)), #(batch_size * nbest * from_target)\n                # cur_values: batch_size * from_target * to_target\n                cur_values = cur_values.view(batch_size, tag_size, 1, tag_size).expand(batch_size, tag_size, nbest, tag_size) + partition.contiguous().view(batch_size, tag_size, nbest, 1).expand(batch_size, tag_size, nbest, tag_size)\n                ## compare all nbest and all from target\n                cur_values = cur_values.view(batch_size, tag_size*nbest, tag_size)\n                # print ""cur size:"",cur_values.size()\n            partition, cur_bp = torch.topk(cur_values, nbest, 1)\n            ## cur_bp/partition: [batch_size, nbest, tag_size], id should be normize through nbest in following backtrace step\n            # print partition[:,0,:]\n            # print cur_bp[:,0,:]\n            # print ""nbest, "",idx\n            if idx == 1:\n                cur_bp = cur_bp*nbest\n            partition = partition.transpose(2,1)\n            cur_bp = cur_bp.transpose(2,1)\n\n            # print partition\n            # exit(0)\n            #partition: (batch_size * to_target * nbest)\n            #cur_bp: (batch_size * to_target * nbest) Notice the cur_bp number is the whole position of tag_size*nbest, need to convert when decode\n            partition_history.append(partition)\n            ## cur_bp: (batch_size,nbest, tag_size) topn source score position in current tag\n            ## set padded label as 0, which will be filtered in post processing\n            ## mask[idx] ? mask[idx-1]\n            cur_bp.masked_fill_(mask[idx].view(batch_size, 1, 1).expand(batch_size, tag_size, nbest), 0)\n            # print cur_bp[0]\n            back_points.append(cur_bp)\n        ### add score to final STOP_TAG\n        partition_history = torch.cat(partition_history,0).view(seq_len, batch_size, tag_size, nbest).transpose(1,0).contiguous() ## (batch_size, seq_len, nbest, tag_size)\n        ### get the last position for each setences, and select the last partitions using gather()\n        last_position = length_mask.view(batch_size,1,1,1).expand(batch_size, 1, tag_size, nbest) - 1\n        last_partition = torch.gather(partition_history, 1, last_position).view(batch_size, tag_size, nbest, 1)\n        ### calculate the score from last partition to end state (and then select the STOP_TAG from it)\n        last_values = last_partition.expand(batch_size, tag_size, nbest, tag_size) + self.transitions.view(1, tag_size, 1, tag_size).expand(batch_size, tag_size, nbest, tag_size)\n        last_values = last_values.view(batch_size, tag_size*nbest, tag_size)\n        end_partition, end_bp = torch.topk(last_values, nbest, 1)\n        ## end_partition: (batch, nbest, tag_size)\n        end_bp = end_bp.transpose(2,1)\n        # end_bp: (batch, tag_size, nbest)\n        pad_zero = autograd.Variable(torch.zeros(batch_size, tag_size, nbest)).long()\n        if self.gpu:\n            pad_zero = pad_zero.cuda()\n        back_points.append(pad_zero)\n        back_points = torch.cat(back_points).view(seq_len, batch_size, tag_size, nbest)\n\n        ## select end ids in STOP_TAG\n        pointer = end_bp[:, STOP_TAG, :] ## (batch_size, nbest)\n        insert_last = pointer.contiguous().view(batch_size, 1, 1, nbest).expand(batch_size, 1, tag_size, nbest)\n        back_points = back_points.transpose(1,0).contiguous()\n        ## move the end ids(expand to tag_size) to the corresponding position of back_points to replace the 0 values\n        # print ""lp:"",last_position\n        # print ""il:"",insert_last[0]\n        # exit(0)\n        ## copy the ids of last position:insert_last to back_points, though the last_position index\n        ## last_position includes the length of batch sentences\n        # print ""old:"", back_points[9,0,:,:]\n        back_points.scatter_(1, last_position, insert_last)\n        ## back_points: [batch_size, seq_length, tag_size, nbest]\n        # print ""new:"", back_points[9,0,:,:]\n        # exit(0)\n        # print pointer[2]\n        \'\'\'\n        back_points: in simple demonstratration\n        x,x,x,x,x,x,x,x,x,7\n        x,x,x,x,x,4,0,0,0,0\n        x,x,6,0,0,0,0,0,0,0\n        \'\'\'\n\n        back_points = back_points.transpose(1,0).contiguous()\n        # print back_points[0]\n        ## back_points: (seq_len, batch, tag_size, nbest)\n        ## decode from the end, padded position ids are 0, which will be filtered in following evaluation\n        decode_idx = autograd.Variable(torch.LongTensor(seq_len, batch_size, nbest))\n        if self.gpu:\n            decode_idx = decode_idx.cuda()\n        decode_idx[-1] = pointer.data/nbest\n        # print ""pointer-1:"",pointer[2]\n        # exit(0)\n        # use old mask, let 0 means has token\n        for idx in range(len(back_points)-2, -1, -1):\n            # print ""pointer: "",idx,  pointer[3]\n            # print ""back:"",back_points[idx][3]\n            # print ""mask:"",mask[idx+1,3]\n            new_pointer = torch.gather(back_points[idx].view(batch_size, tag_size*nbest), 1, pointer.contiguous().view(batch_size,nbest))\n            decode_idx[idx] = new_pointer.data/nbest\n            # # use new pointer to remember the last end nbest ids for non longest\n            pointer = new_pointer + pointer.contiguous().view(batch_size,nbest)*mask[idx].view(batch_size,1).expand(batch_size, nbest).long()\n\n        # exit(0)\n        path_score = None\n        decode_idx = decode_idx.transpose(1,0)\n        ## decode_idx: [batch, seq_len, nbest]\n        # print decode_idx[:,:,0]\n        # print ""nbest:"",nbest\n        # print ""diff:"", decode_idx[:,:,0]- decode_idx[:,:,4]\n        # print decode_idx[:,0,:]\n        # exit(0)\n\n        ### calculate probability for each sequence\n        scores = end_partition[:, :, STOP_TAG]\n        ## scores: [batch_size, nbest]\n        max_scores,_ = torch.max(scores, 1)\n        minus_scores = scores - max_scores.view(batch_size,1).expand(batch_size, nbest)\n        path_score = F.softmax(minus_scores, 1)\n        ## path_score: [batch_size, nbest]\n        # exit(0)\n        return path_score, decode_idx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
model/lstm_attention.py,13,"b""from __future__ import print_function\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import *\n\n\nclass LSTM_attention(nn.Module):\n    ''' Compose with two layers '''\n    def __init__(self,lstm_hidden,bilstm_flag,data):\n        super(LSTM_attention, self).__init__()\n\n        self.lstm = nn.LSTM(lstm_hidden * 4, lstm_hidden, num_layers=1, batch_first=True, bidirectional=bilstm_flag)\n        #self.slf_attn = multihead_attention(data.HP_hidden_dim,num_heads = data.num_attention_head, dropout_rate=data.HP_dropout)\n        self.label_attn = multihead_attention(data.HP_hidden_dim, num_heads=data.num_attention_head,dropout_rate=data.HP_dropout)\n        self.droplstm = nn.Dropout(data.HP_dropout)\n        self.gpu = data.HP_gpu\n        if self.gpu:\n            self.lstm =self.lstm.cuda()\n            self.label_attn = self.label_attn.cuda()\n\n\n    def forward(self,lstm_out,label_embs,word_seq_lengths,hidden):\n        lstm_out = pack_padded_sequence(input=lstm_out, lengths=word_seq_lengths.cpu().numpy(), batch_first=True)\n        lstm_out, hidden = self.lstm(lstm_out, hidden)\n        lstm_out, _ = pad_packed_sequence(lstm_out)\n        lstm_out = self.droplstm(lstm_out.transpose(1, 0))\n        # lstm_out (seq_length * batch_size * hidden)\n        label_attention_output = self.label_attn(lstm_out, label_embs, label_embs)\n        # label_attention_output (batch_size, seq_len, embed_size)\n        lstm_out = torch.cat([lstm_out, label_attention_output], -1)\n        return lstm_out\n\nclass multihead_attention(nn.Module):\n\n    def __init__(self, num_units, num_heads=1, dropout_rate=0, gpu=True, causality=False):\n        '''Applies multihead attention.\n        Args:\n            num_units: A scalar. Attention size.\n            dropout_rate: A floating point number.\n            causality: Boolean. If true, units that reference the future are masked.\n            num_heads: An int. Number of heads.\n        '''\n        super(multihead_attention, self).__init__()\n        self.gpu = gpu\n        self.num_units = num_units\n        self.num_heads = num_heads\n        self.dropout_rate = dropout_rate\n        self.causality = causality\n        self.Q_proj = nn.Sequential(nn.Linear(self.num_units, self.num_units), nn.ReLU())\n        self.K_proj = nn.Sequential(nn.Linear(self.num_units, self.num_units), nn.ReLU())\n        self.V_proj = nn.Sequential(nn.Linear(self.num_units, self.num_units), nn.ReLU())\n        if self.gpu:\n            self.Q_proj = self.Q_proj.cuda()\n            self.K_proj = self.K_proj.cuda()\n            self.V_proj = self.V_proj.cuda()\n\n\n        self.output_dropout = nn.Dropout(p=self.dropout_rate)\n\n    def forward(self, queries, keys, values,last_layer = False):\n        # keys, values: same shape of [N, T_k, C_k]\n        # queries: A 3d Variable with shape of [N, T_q, C_q]\n        # Linear projections\n        Q = self.Q_proj(queries)  # (N, T_q, C)\n        K = self.K_proj(keys)  # (N, T_q, C)\n        V = self.V_proj(values)  # (N, T_q, C)\n        # Split and concat\n        Q_ = torch.cat(torch.chunk(Q, self.num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n        K_ = torch.cat(torch.chunk(K, self.num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n        V_ = torch.cat(torch.chunk(V, self.num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n        # Multiplication\n        outputs = torch.bmm(Q_, K_.permute(0, 2, 1))  # (h*N, T_q, T_k)\n        # Scale\n        outputs = outputs / (K_.size()[-1] ** 0.5)\n\n        # Activation\n        if last_layer == False:\n            outputs = F.softmax(outputs, dim=-1)  # (h*N, T_q, T_k)\n        # Query Masking\n        query_masks = torch.sign(torch.abs(torch.sum(queries, dim=-1)))  # (N, T_q)\n        query_masks = query_masks.repeat(self.num_heads, 1)  # (h*N, T_q)\n        query_masks = torch.unsqueeze(query_masks, 2).repeat(1, 1, keys.size()[1])  # (h*N, T_q, T_k)\n        outputs = outputs * query_masks\n        # Dropouts\n        outputs = self.output_dropout(outputs)  # (h*N, T_q, T_k)\n        if last_layer == True:\n            return outputs\n        # Weighted sum\n        outputs = torch.bmm(outputs, V_)  # (h*N, T_q, C/h)\n        # Restore shape\n        outputs = torch.cat(torch.chunk(outputs, self.num_heads, dim=0), dim=2)  # (N, T_q, C)\n        # Residual connection\n        outputs += queries\n\n        return outputs\n"""
model/seqmodel.py,4,"b'from __future__ import print_function\nfrom __future__ import absolute_import\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .wordsequence import WordSequence\nfrom .crf import CRF\n\nclass SeqModel(nn.Module):\n    def __init__(self, data):\n        super(SeqModel, self).__init__()\n        self.use_crf = data.use_crf\n        print(""build network..."")\n        print(""use_char: "", data.use_char)\n        if data.use_char:\n            print(""char feature extractor: "", data.char_feature_extractor)\n        print(""word feature extractor: "", data.word_feature_extractor)\n\n        self.gpu = data.HP_gpu\n        self.average_batch = data.average_batch_loss\n        label_size = data.label_alphabet_size\n        # data.label_alphabet_size += 2\n        self.word_hidden = WordSequence(data)\n        if self.use_crf:\n            self.crf = CRF(label_size, self.gpu)\n\n    def neg_log_likelihood_loss(self, word_inputs, feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover, batch_label, mask,input_label_seq_tensor):\n        outs = self.word_hidden(word_inputs,feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover,input_label_seq_tensor)\n        batch_size = word_inputs.size(0)\n        seq_len = word_inputs.size(1)\n        if self.use_crf:\n            total_loss = self.crf.neg_log_likelihood_loss(outs, mask, batch_label)\n            scores, tag_seq = self.crf._viterbi_decode(outs, mask)\n        else:\n            loss_function = nn.NLLLoss(ignore_index=0, size_average=False)\n            outs = outs.view(batch_size * seq_len, -1)\n            score = F.log_softmax(outs, 1)\n            total_loss = loss_function(score, batch_label.view(batch_size * seq_len))\n            _, tag_seq  = torch.max(score, 1)\n            tag_seq = tag_seq.view(batch_size, seq_len)\n        if self.average_batch:\n            total_loss = total_loss / batch_size\n        return total_loss, tag_seq\n\n\n    def forward(self, word_inputs, feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover, mask,input_label_seq_tensor):\n        outs = self.word_hidden(word_inputs,feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover,input_label_seq_tensor)\n        batch_size = word_inputs.size(0)\n        seq_len = word_inputs.size(1)\n\n        outs = outs.view(batch_size * seq_len, -1)\n        _, tag_seq  = torch.max(outs, 1)\n        tag_seq = tag_seq.view(batch_size, seq_len)\n        ## filter padded position with zero\n        tag_seq = mask.long() * tag_seq\n\n        return tag_seq'"
model/wordrep.py,8,"b'from __future__ import print_function\nfrom __future__ import absolute_import\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom .charbilstm import CharBiLSTM\nfrom .charbigru import CharBiGRU\nfrom .charcnn import CharCNN\n\n\nclass WordRep(nn.Module):\n    def __init__(self, data):\n        super(WordRep, self).__init__()\n        print(""build word representation..."")\n        self.gpu = data.HP_gpu\n        self.use_char = data.use_char\n        self.batch_size = data.HP_batch_size\n        self.char_hidden_dim = 0\n        self.char_all_feature = False\n        if self.use_char:\n            self.char_hidden_dim = data.HP_char_hidden_dim\n            self.char_embedding_dim = data.char_emb_dim\n            if data.char_feature_extractor == ""CNN"":\n                self.char_feature = CharCNN(data.char_alphabet.size(), data.pretrain_char_embedding,\n                                            self.char_embedding_dim, self.char_hidden_dim, data.HP_dropout, self.gpu)\n            elif data.char_feature_extractor == ""LSTM"":\n                self.char_feature = CharBiLSTM(data.char_alphabet.size(), data.pretrain_char_embedding,\n                                               self.char_embedding_dim, self.char_hidden_dim, data.HP_dropout, self.gpu)\n            elif data.char_feature_extractor == ""GRU"":\n                self.char_feature = CharBiGRU(data.char_alphabet.size(), data.pretrain_char_embedding,\n                                              self.char_embedding_dim, self.char_hidden_dim, data.HP_dropout, self.gpu)\n            elif data.char_feature_extractor == ""ALL"":\n                self.char_all_feature = True\n                self.char_feature = CharCNN(data.char_alphabet.size(), data.pretrain_char_embedding,\n                                            self.char_embedding_dim, self.char_hidden_dim, data.HP_dropout, self.gpu)\n                self.char_feature_extra = CharBiLSTM(data.char_alphabet.size(), data.pretrain_char_embedding,\n                                                     self.char_embedding_dim, self.char_hidden_dim, data.HP_dropout,\n                                                     self.gpu)\n            else:\n                print(\n                    ""Error char feature selection, please check parameter data.char_feature_extractor (CNN/LSTM/GRU/ALL)."")\n                exit(0)\n        self.embedding_dim = data.word_emb_dim\n        self.drop = nn.Dropout(data.HP_dropout)\n\n        self.word_embedding = nn.Embedding(data.word_alphabet.size(), self.embedding_dim)\n\n        if data.pretrain_word_embedding is not None:\n            self.word_embedding.weight.data.copy_(torch.from_numpy(data.pretrain_word_embedding))\n        else:\n            self.word_embedding.weight.data.copy_(\n                torch.from_numpy(self.random_embedding(data.word_alphabet.size(), self.embedding_dim)))\n\n        # label embedding\n        self.label_dim = data.HP_hidden_dim\n        self.label_embedding = nn.Embedding(data.label_alphabet_size, self.label_dim)\n\n        self.label_embedding.weight.data.copy_(torch.from_numpy(\n            self.random_embedding_label(data.label_alphabet_size, self.label_dim, data.label_embedding_scale)))\n\n        self.feature_num = data.feature_num\n        self.feature_embedding_dims = data.feature_emb_dims\n        self.feature_embeddings = nn.ModuleList()\n        for idx in range(self.feature_num):\n            self.feature_embeddings.append(\n                nn.Embedding(data.feature_alphabets[idx].size(), self.feature_embedding_dims[idx]))\n        for idx in range(self.feature_num):\n            if data.pretrain_feature_embeddings[idx] is not None:\n                self.feature_embeddings[idx].weight.data.copy_(torch.from_numpy(data.pretrain_feature_embeddings[idx]))\n            else:\n                self.feature_embeddings[idx].weight.data.copy_(torch.from_numpy(\n                    self.random_embedding(data.feature_alphabets[idx].size(), self.feature_embedding_dims[idx])))\n\n        if self.gpu:\n            self.drop = self.drop.cuda()\n            self.word_embedding = self.word_embedding.cuda()\n            self.label_embedding = self.label_embedding.cuda()\n            for idx in range(self.feature_num):\n                self.feature_embeddings[idx] = self.feature_embeddings[idx].cuda()\n\n    def random_embedding(self, vocab_size, embedding_dim):\n        pretrain_emb = np.empty([vocab_size, embedding_dim])\n        scale = np.sqrt(3.0 / embedding_dim)\n        for index in range(vocab_size):\n            pretrain_emb[index, :] = np.random.uniform(-scale, scale, [1, embedding_dim])\n        return pretrain_emb\n\n    def random_embedding_label(self, vocab_size, embedding_dim, scale):\n        pretrain_emb = np.empty([vocab_size, embedding_dim])\n        # scale = np.sqrt(3.0 / embedding_dim)\n        # scale = 0.025\n        for index in range(vocab_size):\n            pretrain_emb[index, :] = np.random.uniform(-scale, scale, [1, embedding_dim])\n        return pretrain_emb\n\n    def forward(self, word_inputs, feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover,\n                input_label_seq_tensor):\n        """"""\n            input:\n                word_inputs: (batch_size, sent_len)\n                features: list [(batch_size, sent_len), (batch_len, sent_len),...]\n                word_seq_lengths: list of batch_size, (batch_size,1)\n                char_inputs: (batch_size*sent_len, word_length)\n                char_seq_lengths: list of whole batch_size for char, (batch_size*sent_len, 1)\n                char_seq_recover: variable which records the char order information, used to recover char order\n                input_label_seq_tensor: (batch_size, number of label)\n            output:\n                Variable(batch_size, sent_len, hidden_dim)\n        """"""\n\n        batch_size = word_inputs.size(0)\n        sent_len = word_inputs.size(1)\n        word_embs = self.word_embedding(word_inputs)\n        word_list = [word_embs]\n        for idx in range(self.feature_num):\n            word_list.append(self.feature_embeddings[idx](feature_inputs[idx]))\n        # label embedding\n        label_embs = self.label_embedding(input_label_seq_tensor)\n        if self.use_char:\n            ## calculate char lstm last hidden\n            char_features = self.char_feature.get_last_hiddens(char_inputs, char_seq_lengths.cpu().numpy())\n            char_features = char_features[char_seq_recover]\n            char_features = char_features.view(batch_size, sent_len, -1)\n            ## concat word and char together\n            word_list.append(char_features)\n            # word_embs = torch.cat([word_embs, char_features], 2)\n            if self.char_all_feature:\n                char_features_extra = self.char_feature_extra.get_last_hiddens(char_inputs,\n                                                                               char_seq_lengths.cpu().numpy())\n                char_features_extra = char_features_extra[char_seq_recover]\n                char_features_extra = char_features_extra.view(batch_size, sent_len, -1)\n                ## concat word and char together\n                word_list.append(char_features_extra)\n        word_embs = torch.cat(word_list, 2)\n        word_represent = self.drop(word_embs)\n        # label_embs = self.drop(label_embs)\n        return word_represent, label_embs\n'"
model/wordsequence.py,3,"b'import torch.nn as nn\nimport torch\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom .wordrep import WordRep\nfrom model.lstm_attention import LSTM_attention, multihead_attention\n\n\nclass WordSequence(nn.Module):\n    def __init__(self, data):\n        super(WordSequence, self).__init__()\n        print(""build word sequence feature extractor: %s...""%(data.word_feature_extractor))\n        self.gpu = data.HP_gpu\n        self.use_char = data.use_char\n        # self.batch_size = data.HP_batch_size\n        # self.hidden_dim = data.HP_hidden_dim\n        self.droplstm = nn.Dropout(data.HP_dropout)\n        self.bilstm_flag = data.HP_bilstm\n        self.num_of_lstm_layer = data.HP_lstm_layer\n        #word embedding\n        self.wordrep = WordRep(data)\n\n        self.input_size = data.word_emb_dim\n        if self.use_char:\n            self.input_size += data.HP_char_hidden_dim\n            if data.char_feature_extractor == ""ALL"":\n                self.input_size += data.HP_char_hidden_dim\n        for idx in range(data.feature_num):\n            self.input_size += data.feature_emb_dims[idx]\n        # The LSTM takes word embeddings as inputs, and outputs hidden states\n        # with dimensionality hidden_dim.\n        if self.bilstm_flag:\n            lstm_hidden = data.HP_hidden_dim // 2\n        else:\n            lstm_hidden = data.HP_hidden_dim\n\n        self.word_feature_extractor = data.word_feature_extractor\n\n        self.lstm_first = nn.LSTM(self.input_size, lstm_hidden, num_layers=1, batch_first=True,\n                            bidirectional=self.bilstm_flag)\n        self.lstm_layer = nn.LSTM(lstm_hidden * 4, lstm_hidden, num_layers=1, batch_first=True,\n                                  bidirectional=self.bilstm_flag)\n        self.self_attention_first = multihead_attention(data.HP_hidden_dim,num_heads=data.num_attention_head, dropout_rate=data.HP_dropout, gpu=self.gpu)\n        # DO NOT Add dropout at last layer\n        self.self_attention_last = multihead_attention(data.HP_hidden_dim,num_heads=1, dropout_rate=0, gpu=self.gpu)\n        self.lstm_attention_stack =  nn.ModuleList([LSTM_attention(lstm_hidden,self.bilstm_flag,data) for _ in range(int(self.num_of_lstm_layer)-2)])\n        #highway encoding\n        #self.highway_encoding = HighwayEncoding(data,data.HP_hidden_dim,activation_function=F.relu)\n\n        if self.gpu:\n            self.droplstm = self.droplstm.cuda()\n            self.lstm_first = self.lstm_first.cuda()\n            self.lstm_layer = self.lstm_layer.cuda()\n            self.self_attention_first = self.self_attention_first.cuda()\n            self.self_attention_last = self.self_attention_last.cuda()\n            self.lstm_attention_stack = self.lstm_attention_stack.cuda()\n\n\n    def forward(self, word_inputs, feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover, input_label_seq_tensor):\n        """"""\n            input:\n                word_inputs: (batch_size, sent_len)\n                word_seq_lengths: list of batch_size, (batch_size,1)\n                char_inputs: (batch_size*sent_len, word_length)\n                char_seq_lengths: list of whole batch_size for char, (batch_size*sent_len, 1)\n                char_seq_recover: variable which records the char order information, used to recover char order\n                label_size: nubmer of label\n            output:\n                Variable(batch_size, sent_len, hidden_dim)\n        """"""\n        word_represent, label_embs = self.wordrep(word_inputs,feature_inputs, word_seq_lengths, char_inputs, char_seq_lengths, char_seq_recover,input_label_seq_tensor)\n        #word_represent shape [batch_size, seq_length, word_embedding_dim+char_hidden_dim]\n        # word_embs (batch_size, seq_len, embed_size)\n        # label_embs = self.highway_encoding(label_embs)\n        """"""\n        First LSTM layer (input word only)\n        """"""\n        lstm_out = word_represent\n        lstm_out = pack_padded_sequence(input=lstm_out, lengths=word_seq_lengths.cpu().numpy(), batch_first=True)\n        hidden = None\n        lstm_out, hidden = self.lstm_first(lstm_out, hidden)\n        lstm_out, _ = pad_packed_sequence(lstm_out)\n        # shape [seq_len, batch, hidden_size]\n        lstm_out = self.droplstm(lstm_out.transpose(1, 0))\n        attention_label = self.self_attention_first(lstm_out, label_embs, label_embs)\n        # shape [batch_size, seq_length, embedding_dim]\n        lstm_out = torch.cat([lstm_out, attention_label], -1)\n        #shape [batch_size, seq_length, embedding_dim + label_embeeding_dim]\n\n        # LAN layer\n        for layer in self.lstm_attention_stack:\n            lstm_out = layer(lstm_out, label_embs, word_seq_lengths, hidden)\n        """"""\n        Last Layer \n        Attention weight calculate loss\n        """"""\n        lstm_out = pack_padded_sequence(input=lstm_out, lengths=word_seq_lengths.cpu().numpy(), batch_first=True)\n        lstm_out, hidden = self.lstm_layer(lstm_out, hidden)\n        lstm_out, _ = pad_packed_sequence(lstm_out)\n        lstm_out = self.droplstm(lstm_out.transpose(1, 0))\n        lstm_out = self.self_attention_last(lstm_out, label_embs, label_embs, True)\n        return lstm_out\n\n'"
utils/__init__.py,0,"b""__author__ = 'max'\n"""
utils/alphabet.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Max\n# @Date:   2018-01-19 11:33:37\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2018-04-26 13:56:03\n\n\n""""""\nAlphabet maps objects to integer ids. It provides two way mapping from the index to the objects.\n""""""\nfrom __future__ import print_function\nimport json\nimport os\nimport sys\n\n\nclass Alphabet:\n    def __init__(self, name, label=False, keep_growing=True):\n        self.name = name\n        self.UNKNOWN = ""</unk>""\n        self.label = label\n        self.instance2index = {}\n        self.instances = []\n        self.keep_growing = keep_growing\n\n        # Index 0 is occupied by default, all else following.\n        self.default_index = 0\n        self.next_index = 1\n        if not self.label:\n            self.add(self.UNKNOWN)\n\n    def clear(self, keep_growing=True):\n        self.instance2index = {}\n        self.instances = []\n        self.keep_growing = keep_growing\n\n        # Index 0 is occupied by default, all else following.\n        self.default_index = 0\n        self.next_index = 1\n\n    def add(self, instance):\n        if instance not in self.instance2index:\n            self.instances.append(instance)\n            self.instance2index[instance] = self.next_index\n            self.next_index += 1\n\n    def get_index(self, instance):\n        try:\n            return self.instance2index[instance]\n        except KeyError:\n            if self.keep_growing:\n                index = self.next_index\n                self.add(instance)\n                return index\n            else:\n                return self.instance2index[self.UNKNOWN]\n\n    def get_instance(self, index):\n        if index == 0:\n            if self.label:\n                return self.instances[0]\n            # First index is occupied by the wildcard element.\n            return None\n        try:\n            return self.instances[index - 1]\n        except IndexError:\n            print(\'WARNING:Alphabet get_instance ,unknown instance, return the first label.\')\n            return self.instances[0]\n\n    def size(self):\n        # if self.label:\n        #     return len(self.instances)\n        # else:\n        return len(self.instances) + 1\n\n    def iteritems(self):\n        if sys.version_info[0] < 3:  # If using python3, dict item access uses different syntax\n            return self.instance2index.iteritems()\n        else:\n            return self.instance2index.items()\n\n    def enumerate_items(self, start=1):\n        if start < 1 or start >= self.size():\n            raise IndexError(""Enumerate is allowed between [1 : size of the alphabet)"")\n        return zip(range(start, len(self.instances) + 1), self.instances[start - 1:])\n\n    def close(self):\n        self.keep_growing = False\n\n    def open(self):\n        self.keep_growing = True\n\n    def get_content(self):\n        return {\'instance2index\': self.instance2index, \'instances\': self.instances}\n\n    def from_json(self, data):\n        self.instances = data[""instances""]\n        self.instance2index = data[""instance2index""]\n\n    def save(self, output_directory, name=None):\n        """"""\n        Save both alhpabet records to the given directory.\n        :param output_directory: Directory to save model and weights.\n        :param name: The alphabet saving name, optional.\n        :return:\n        """"""\n        saving_name = name if name else self.__name\n        try:\n            json.dump(self.get_content(), open(os.path.join(output_directory, saving_name + "".json""), \'w\'))\n        except Exception as e:\n            print(""Exception: Alphabet is not saved: "" % repr(e))\n\n    def load(self, input_directory, name=None):\n        """"""\n        Load model architecture and weights from the give directory. This allow we use old models even the structure\n        changes.\n        :param input_directory: Directory to save model and weights\n        :return:\n        """"""\n        loading_name = name if name else self.__name\n        self.from_json(json.load(open(os.path.join(input_directory, loading_name + "".json""))))\n'"
utils/data.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Jie\n# @Date:   2017-06-14 17:34:32\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2018-06-22 00:01:47\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nimport sys\nfrom .alphabet import Alphabet\nfrom .functions import *\n\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle as pickle\n\n\nSTART = ""</s>""\nUNKNOWN = ""</unk>""\nPADDING = ""</pad>""\n\nclass Data:\n    def __init__(self):\n        self.MAX_SENTENCE_LENGTH = 250\n        self.MAX_WORD_LENGTH = -1\n        self.number_normalized = True\n        self.norm_word_emb = False\n        self.norm_char_emb = False\n        self.word_alphabet = Alphabet(\'word\')\n        self.char_alphabet = Alphabet(\'character\')\n\n        self.feature_name = []\n        self.feature_alphabets = []\n        self.feature_num = len(self.feature_alphabets)\n        self.feat_config = None\n\n\n        self.label_alphabet = Alphabet(\'label\',True)\n        self.tagScheme = ""NoSeg"" ## BMES/BIO\n\n        self.seg = True\n\n        ### I/O\n        self.train_dir = None\n        self.dev_dir = None\n        self.test_dir = None\n        self.raw_dir = None\n\n        self.decode_dir = None\n        self.dset_dir = None ## data vocabulary related file\n        self.model_dir = None ## model save  file\n        self.load_model_dir = None ## model load file\n\n        self.word_emb_dir = None\n        self.char_emb_dir = None\n        self.feature_emb_dirs = []\n\n        self.train_texts = []\n        self.dev_texts = []\n        self.test_texts = []\n        self.raw_texts = []\n\n        self.train_Ids = []\n        self.dev_Ids = []\n        self.test_Ids = []\n        self.raw_Ids = []\n\n        self.pretrain_word_embedding = None\n        self.pretrain_char_embedding = None\n        self.pretrain_feature_embeddings = []\n\n        self.label_size = 0\n        self.word_alphabet_size = 0\n        self.char_alphabet_size = 0\n        self.label_alphabet_size = 0\n        self.feature_alphabet_sizes = []\n        self.feature_emb_dims = []\n        self.norm_feature_embs = []\n        self.word_emb_dim = 50\n        self.char_emb_dim = 30\n\n        ###Networks\n        self.word_feature_extractor = ""LSTM"" ## ""LSTM""/""CNN""/""GRU""/\n        self.use_char = True\n        self.char_feature_extractor = ""CNN"" ## ""LSTM""/""CNN""/""GRU""/None\n        self.use_crf = True\n        self.nbest = None\n\n        ## Training\n        self.average_batch_loss = False\n        self.optimizer = ""SGD"" ## ""SGD""/""AdaGrad""/""AdaDelta""/""RMSProp""/""Adam""\n        self.status = ""train""\n        ### Hyperparameters\n        self.HP_cnn_layer = 4\n        self.HP_iteration = 100\n        self.HP_batch_size = 10\n        self.HP_char_hidden_dim = 50\n        self.HP_hidden_dim = 200\n        self.HP_dropout = 0.5\n        self.HP_lstm_layer = 1\n        self.HP_bilstm = True\n\n        self.HP_gpu = False\n        self.HP_lr = 0.015\n        self.HP_lr_decay = 0.05\n        self.HP_clip = None\n        self.HP_momentum = 0\n        self.HP_l2 = 1e-8\n\n    def show_data_summary(self):\n        print(""++""*50)\n        print(""DATA SUMMARY START:"")\n        print("" I/O:"")\n        print(""     Tag          scheme: %s""%(self.tagScheme))\n        print(""     MAX SENTENCE LENGTH: %s""%(self.MAX_SENTENCE_LENGTH))\n        print(""     MAX   WORD   LENGTH: %s""%(self.MAX_WORD_LENGTH))\n        print(""     Number   normalized: %s""%(self.number_normalized))\n        print(""     Word  alphabet size: %s""%(self.word_alphabet_size))\n        print(""     Char  alphabet size: %s""%(self.char_alphabet_size))\n        print(""     Label alphabet size: %s""%(self.label_alphabet_size))\n        print(""     Word embedding  dir: %s""%(self.word_emb_dir))\n        print(""     Char embedding  dir: %s""%(self.char_emb_dir))\n        print(""     Word embedding size: %s""%(self.word_emb_dim))\n        print(""     Char embedding size: %s""%(self.char_emb_dim))\n        print(""     Norm   word     emb: %s""%(self.norm_word_emb))\n        print(""     Norm   char     emb: %s""%(self.norm_char_emb))\n        print(""     Train  file directory: %s""%(self.train_dir))\n        print(""     Dev    file directory: %s""%(self.dev_dir))\n        print(""     Test   file directory: %s""%(self.test_dir))\n        print(""     Raw    file directory: %s""%(self.raw_dir))\n        print(""     Dset   file directory: %s""%(self.dset_dir))\n        print(""     Model  file directory: %s""%(self.model_dir))\n        print(""     Loadmodel   directory: %s""%(self.load_model_dir))\n        print(""     Decode file directory: %s""%(self.decode_dir))\n        print(""     Train instance number: %s""%(len(self.train_texts)))\n        print(""     Dev   instance number: %s""%(len(self.dev_texts)))\n        print(""     Test  instance number: %s""%(len(self.test_texts)))\n        print(""     Raw   instance number: %s""%(len(self.raw_texts)))\n        print(""     FEATURE num: %s""%(self.feature_num))\n        for idx in range(self.feature_num):\n            print(""         Fe: %s  alphabet  size: %s""%(self.feature_alphabets[idx].name, self.feature_alphabet_sizes[idx]))\n            print(""         Fe: %s  embedding  dir: %s""%(self.feature_alphabets[idx].name, self.feature_emb_dirs[idx]))\n            print(""         Fe: %s  embedding size: %s""%(self.feature_alphabets[idx].name, self.feature_emb_dims[idx]))\n            print(""         Fe: %s  norm       emb: %s""%(self.feature_alphabets[idx].name, self.norm_feature_embs[idx]))\n        print("" ""+""++""*20)\n        print("" Model Network:"")\n        print(""     Model        use_crf: %s""%(self.use_crf))\n        print(""     Model word extractor: %s""%(self.word_feature_extractor))\n        print(""     Model       use_char: %s""%(self.use_char))\n        if self.use_char:\n            print(""     Model char extractor: %s""%(self.char_feature_extractor))\n            print(""     Model char_hidden_dim: %s""%(self.HP_char_hidden_dim))\n        print("" ""+""++""*20)\n        print("" Training:"")\n        print(""     Optimizer: %s""%(self.optimizer))\n        print(""     Iteration: %s""%(self.HP_iteration))\n        print(""     BatchSize: %s""%(self.HP_batch_size))\n        print(""     Average  batch   loss: %s""%(self.average_batch_loss))\n\n        print("" ""+""++""*20)\n        print("" Hyperparameters:"")\n\n        print(""     Hyper              lr: %s""%(self.HP_lr))\n        print(""     Hyper        lr_decay: %s""%(self.HP_lr_decay))\n        print(""     Hyper         HP_clip: %s""%(self.HP_clip))\n        print(""     Hyper        momentum: %s""%(self.HP_momentum))\n        print(""     Hyper              l2: %s""%(self.HP_l2))\n        print(""     Hyper      hidden_dim: %s""%(self.HP_hidden_dim))\n        print(""     Hyper         dropout: %s""%(self.HP_dropout))\n        print(""     Hyper      lstm_layer: %s""%(self.HP_lstm_layer))\n        print(""     Hyper          bilstm: %s""%(self.HP_bilstm))\n        print(""     Hyper             GPU: %s""%(self.HP_gpu))\n        print(""DATA SUMMARY END."")\n        print(""++""*50)\n        sys.stdout.flush()\n\n\n    def initial_feature_alphabets(self):\n        items = open(self.train_dir,\'r\').readline().strip(\'\\n\').split()\n        total_column = len(items)\n        if total_column > 2:\n            for idx in range(1, total_column-1):\n                feature_prefix = items[idx].split(\']\',1)[0]+""]""\n                self.feature_alphabets.append(Alphabet(feature_prefix))\n                self.feature_name.append(feature_prefix)\n                print(""Find feature: "", feature_prefix)\n        self.feature_num = len(self.feature_alphabets)\n        self.pretrain_feature_embeddings = [None]*self.feature_num\n        self.feature_emb_dims = [20]*self.feature_num\n        self.feature_emb_dirs = [None]*self.feature_num\n        self.norm_feature_embs = [False]*self.feature_num\n        self.feature_alphabet_sizes = [0]*self.feature_num\n        if self.feat_config:\n            for idx in range(self.feature_num):\n                if self.feature_name[idx] in self.feat_config:\n                    self.feature_emb_dims[idx] = self.feat_config[self.feature_name[idx]][\'emb_size\']\n                    self.feature_emb_dirs[idx] = self.feat_config[self.feature_name[idx]][\'emb_dir\']\n                    self.norm_feature_embs[idx] = self.feat_config[self.feature_name[idx]][\'emb_norm\']\n        # exit(0)\n\n\n    def build_alphabet(self, input_file):\n        in_lines = open(input_file,\'r\').readlines()\n        for line in in_lines:\n            if len(line) > 2:\n                pairs = line.strip().split()\n                word = pairs[0]\n                if sys.version_info[0] < 3:\n                    word = word.decode(\'utf-8\')\n                if self.number_normalized:\n                    word = normalize_word(word)\n                label = pairs[-1]\n                self.label_alphabet.add(label)\n                self.word_alphabet.add(word)\n                ## build feature alphabet\n                for idx in range(self.feature_num):\n                    feat_idx = pairs[idx+1].split(\']\',1)[-1]\n                    self.feature_alphabets[idx].add(feat_idx)\n                for char in word:\n                    self.char_alphabet.add(char)\n        self.word_alphabet_size = self.word_alphabet.size()\n        self.char_alphabet_size = self.char_alphabet.size()\n        self.label_alphabet_size = self.label_alphabet.size()\n        for idx in range(self.feature_num):\n            self.feature_alphabet_sizes[idx] = self.feature_alphabets[idx].size()\n        startS = False\n        startB = False\n        for label,_ in self.label_alphabet.iteritems():\n            if ""S-"" in label.upper():\n                startS = True\n            elif ""B-"" in label.upper():\n                startB = True\n        if startB:\n            if startS:\n                self.tagScheme = ""BMES""\n            else:\n                self.tagScheme = ""BIO""\n\n\n    def fix_alphabet(self):\n        self.word_alphabet.close()\n        self.char_alphabet.close()\n        self.label_alphabet.close()\n        for idx in range(self.feature_num):\n            self.feature_alphabets[idx].close()\n\n\n    def build_pretrain_emb(self):\n        if self.word_emb_dir:\n            print(""Load pretrained word embedding, norm: %s, dir: %s""%(self.norm_word_emb, self.word_emb_dir))\n            self.pretrain_word_embedding, self.word_emb_dim = build_pretrain_embedding(self.word_emb_dir, self.word_alphabet, self.word_emb_dim, self.norm_word_emb)\n        if self.char_emb_dir:\n            print(""Load pretrained char embedding, norm: %s, dir: %s""%(self.norm_char_emb, self.char_emb_dir))\n            self.pretrain_char_embedding, self.char_emb_dim = build_pretrain_embedding(self.char_emb_dir, self.char_alphabet, self.char_emb_dim, self.norm_char_emb)\n        for idx in range(self.feature_num):\n            if self.feature_emb_dirs[idx]:\n                print(""Load pretrained feature %s embedding:, norm: %s, dir: %s""%(self.feature_name[idx], self.norm_feature_embs[idx], self.feature_emb_dirs[idx]))\n                self.pretrain_feature_embeddings[idx], self.feature_emb_dims[idx] = build_pretrain_embedding(self.feature_emb_dirs[idx], self.feature_alphabets[idx], self.feature_emb_dims[idx], self.norm_feature_embs[idx])\n\n\n    def generate_instance(self, name):\n        self.fix_alphabet()\n        if name == ""train"":\n            self.train_texts, self.train_Ids = read_instance(self.train_dir, self.word_alphabet, self.char_alphabet, self.feature_alphabets, self.label_alphabet, self.number_normalized, self.MAX_SENTENCE_LENGTH)\n        elif name == ""dev"":\n            self.dev_texts, self.dev_Ids = read_instance(self.dev_dir, self.word_alphabet, self.char_alphabet, self.feature_alphabets, self.label_alphabet, self.number_normalized, self.MAX_SENTENCE_LENGTH)\n        elif name == ""test"":\n            self.test_texts, self.test_Ids = read_instance(self.test_dir, self.word_alphabet, self.char_alphabet, self.feature_alphabets, self.label_alphabet, self.number_normalized, self.MAX_SENTENCE_LENGTH)\n        elif name == ""raw"":\n            self.raw_texts, self.raw_Ids = read_instance(self.raw_dir, self.word_alphabet, self.char_alphabet, self.feature_alphabets, self.label_alphabet, self.number_normalized, self.MAX_SENTENCE_LENGTH)\n        else:\n            print(""Error: you can only generate train/dev/test instance! Illegal input:%s""%(name))\n\n\n    def write_decoded_results(self, predict_results, name):\n        fout = open(self.decode_dir,\'w\')\n        sent_num = len(predict_results)\n        content_list = []\n        if name == \'raw\':\n           content_list = self.raw_texts\n        elif name == \'test\':\n            content_list = self.test_texts\n        elif name == \'dev\':\n            content_list = self.dev_texts\n        elif name == \'train\':\n            content_list = self.train_texts\n        else:\n            print(""Error: illegal name during writing predict result, name should be within train/dev/test/raw !"")\n        assert(sent_num == len(content_list))\n        for idx in range(sent_num):\n            sent_length = len(predict_results[idx])\n            for idy in range(sent_length):\n                ## content_list[idx] is a list with [word, char, label]\n                fout.write(content_list[idx][0][idy].encode(\'utf-8\') + "" "" + predict_results[idx][idy] + \'\\n\')\n            fout.write(\'\\n\')\n        fout.close()\n        print(""Predict %s result has been written into file. %s""%(name, self.decode_dir))\n\n\n    def load(self,data_file):\n        f = open(data_file, \'rb\')\n        tmp_dict = pickle.load(f)\n        f.close()\n        self.__dict__.update(tmp_dict)\n\n    def save(self,save_file):\n        f = open(save_file, \'wb\')\n        pickle.dump(self.__dict__, f, 2)\n        f.close()\n\n\n\n    def write_nbest_decoded_results(self, predict_results, pred_scores, name):\n        ## predict_results : [whole_sent_num, nbest, each_sent_length]\n        ## pred_scores: [whole_sent_num, nbest]\n        fout = open(self.decode_dir,\'w\')\n        sent_num = len(predict_results)\n        content_list = []\n        if name == \'raw\':\n           content_list = self.raw_texts\n        elif name == \'test\':\n            content_list = self.test_texts\n        elif name == \'dev\':\n            content_list = self.dev_texts\n        elif name == \'train\':\n            content_list = self.train_texts\n        else:\n            print(""Error: illegal name during writing predict result, name should be within train/dev/test/raw !"")\n        assert(sent_num == len(content_list))\n        assert(sent_num == len(pred_scores))\n        for idx in range(sent_num):\n            sent_length = len(predict_results[idx][0])\n            nbest = len(predict_results[idx])\n            score_string = ""# ""\n            for idz in range(nbest):\n                score_string += format(pred_scores[idx][idz], \'.4f\')+"" ""\n            fout.write(score_string.strip() + ""\\n"")\n\n            for idy in range(sent_length):\n                try:  # Will fail with python3\n                    label_string = content_list[idx][0][idy].encode(\'utf-8\') + "" ""\n                except:\n                    label_string = content_list[idx][0][idy] + "" ""\n                for idz in range(nbest):\n                    label_string += predict_results[idx][idz][idy]+"" ""\n                label_string = label_string.strip() + ""\\n""\n                fout.write(label_string)\n            fout.write(\'\\n\')\n        fout.close()\n        print(""Predict %s %s-best result has been written into file. %s""%(name,nbest, self.decode_dir))\n\n\n    def read_config(self,args):\n\n        ## read data:\n\n\n        self.train_dir = args.train_dir\n\n        self.dev_dir = args.dev_dir\n\n        self.test_dir = args.test_dir\n\n        self.model_dir = args.model_dir\n\n        self.word_emb_dir = args.word_emb_dir\n\n        self.norm_word_emb = str2bool(args.norm_word_emb)\n\n        self.norm_char_emb = str2bool(args.norm_char_emb)\n\n        self.number_normalized = str2bool(args.number_normalized)\n\n        self.seg = args.seg\n\n        self.word_emb_dim = int(args.word_emb_dim)\n\n        self.char_emb_dim = int(args.char_emb_dim)\n\n        self.use_crf = args.use_crf\n\n        self.use_char = args.use_char\n\n        self.word_feature_extractor = args.word_seq_feature\n\n        self.char_feature_extractor = args.char_seq_feature\n\n        ## read training setting:\n\n        self.optimizer = args.optimizer\n\n        self.average_batch_loss = args.ave_batch_loss\n\n        self.status = args.status\n\n        self.HP_cnn_layer = int(args.cnn_layer)\n\n        self.HP_iteration = int(args.iteration)\n\n        self.HP_batch_size = int(args.batch_size)\n\n        self.HP_char_hidden_dim = int(args.char_hidden_dim)\n\n        self.HP_hidden_dim = int(args.hidden_dim)\n\n        self.HP_dropout = float(args.dropout)\n\n        self.HP_lstm_layer = int(args.lstm_layer)\n\n        self.HP_bilstm = args.bilstm\n\n        self.HP_gpu = args.gpu\n\n        self.HP_lr = float(args.learning_rate)\n\n        self.HP_lr_decay = float(args.lr_decay)\n\n        self.HP_momentum = float(args.momentum)\n\n        self.HP_l2 = float(args.l2)\n\n        self.clip_grad = float(args.clip_grad)\n\n        self.label_embedding_scale = float(args.label_embedding_scale)\n\n        self.num_attention_head = int(args.num_attention_head)\n\n        self.whether_clip_grad = str2bool(args.whether_clip_grad)\n\n        ##\n        # self.MAX_SENTENCE_LENGTH = int(args.MAX_SENTENCE_LENGTH)\n        #\n        # self.MAX_WORD_LENGTH = int(args.MAX_WORD_LENGTH )\n\n\n\ndef config_file_to_dict(input_file):\n    config = {}\n    fins = open(input_file,\'r\').readlines()\n    for line in fins:\n        if len(line) > 0 and line[0] == ""#"":\n            continue\n        if ""="" in line:\n            pair = line.strip().split(\'#\',1)[0].split(\'=\',1)\n            item = pair[0]\n            if item==""feature"":\n                if item not in config:\n                    feat_dict = {}\n                    config[item]= feat_dict\n                feat_dict = config[item]\n                new_pair = pair[-1].split()\n                feat_name = new_pair[0]\n                one_dict = {}\n                one_dict[""emb_dir""] = None\n                one_dict[""emb_size""] = 10\n                one_dict[""emb_norm""] = False\n                if len(new_pair) > 1:\n                    for idx in range(1,len(new_pair)):\n                        conf_pair = new_pair[idx].split(\'=\')\n                        if conf_pair[0] == ""emb_dir"":\n                            one_dict[""emb_dir""]=conf_pair[-1]\n                        elif conf_pair[0] == ""emb_size"":\n                            one_dict[""emb_size""]=int(conf_pair[-1])\n                        elif conf_pair[0] == ""emb_norm"":\n                            one_dict[""emb_norm""]=str2bool(conf_pair[-1])\n                feat_dict[feat_name] = one_dict\n                # print ""feat"",feat_dict\n            else:\n                if item in config:\n                    print(""Warning: duplicated config item found: %s, updated.""%(pair[0]))\n                config[item] = pair[-1]\n    return config\n\n\ndef str2bool(string):\n    if string == ""True"" or string == ""true"" or string == ""TRUE"":\n        return True\n    else:\n        return False'"
utils/functions.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Jie\n# @Date:   2017-06-15 14:23:06\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2018-06-10 17:49:50\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nimport sys\nimport numpy as np\n\ndef normalize_word(word):\n    new_word = """"\n    for char in word:\n        if char.isdigit():\n            new_word += \'0\'\n        else:\n            new_word += char\n    return new_word\n\n\ndef read_instance(input_file, word_alphabet, char_alphabet, feature_alphabets, label_alphabet, number_normalized, max_sent_length, char_padding_size=-1, char_padding_symbol = \'</pad>\'):\n    feature_num = len(feature_alphabets)\n    in_lines = open(input_file,\'r\').readlines()\n    instence_texts = []\n    instence_Ids = []\n    words = []\n    features = []\n    chars = []\n    labels = []\n    word_Ids = []\n    feature_Ids = []\n    char_Ids = []\n    label_Ids = []\n    for line in in_lines:\n        if len(line) > 2:\n            pairs = line.strip().split()\n\n            if sys.version_info[0] < 3:\n                word = pairs[0].decode(\'utf-8\')\n            else:\n                word = pairs[0]\n\n            if number_normalized:\n                word = normalize_word(word)\n            label = pairs[-1]\n            words.append(word)\n            labels.append(label)\n            word_Ids.append(word_alphabet.get_index(word))\n            label_Ids.append(label_alphabet.get_index(label))\n            ## get features\n            feat_list = []\n            feat_Id = []\n            for idx in range(feature_num):\n                feat_idx = pairs[idx+1].split(\']\',1)[-1]\n                feat_list.append(feat_idx)\n                feat_Id.append(feature_alphabets[idx].get_index(feat_idx))\n            features.append(feat_list)\n            feature_Ids.append(feat_Id)\n            ## get char\n            char_list = []\n            char_Id = []\n            for char in word:\n                char_list.append(char)\n            if char_padding_size > 0:\n                char_number = len(char_list)\n                if char_number < char_padding_size:\n                    char_list = char_list + [char_padding_symbol]*(char_padding_size-char_number)\n                assert(len(char_list) == char_padding_size)\n            else:\n                ### not padding\n                pass\n            for char in char_list:\n                char_Id.append(char_alphabet.get_index(char))\n            chars.append(char_list)\n            char_Ids.append(char_Id)\n        else:\n            if (len(words) > 0) and ((max_sent_length < 0) or (len(words) < max_sent_length)) :\n                instence_texts.append([words, features, chars, labels])\n                instence_Ids.append([word_Ids, feature_Ids, char_Ids,label_Ids])\n            words = []\n            features = []\n            chars = []\n            labels = []\n            word_Ids = []\n            feature_Ids = []\n            char_Ids = []\n            label_Ids = []\n    return instence_texts, instence_Ids\n\n\ndef build_pretrain_embedding(embedding_path, word_alphabet, embedd_dim=100, norm=True):\n    embedd_dict = dict()\n    if embedding_path != None:\n        embedd_dict, embedd_dim = load_pretrain_emb(embedding_path)\n    alphabet_size = word_alphabet.size()\n    scale = np.sqrt(3.0 / embedd_dim)\n    pretrain_emb = np.empty([word_alphabet.size(), embedd_dim])\n    perfect_match = 0\n    case_match = 0\n    not_match = 0\n    for word, index in word_alphabet.iteritems():\n        if word in embedd_dict:\n            if norm:\n                pretrain_emb[index,:] = norm2one(embedd_dict[word])\n            else:\n                pretrain_emb[index,:] = embedd_dict[word]\n            perfect_match += 1\n        elif word.lower() in embedd_dict:\n            if norm:\n                pretrain_emb[index,:] = norm2one(embedd_dict[word.lower()])\n            else:\n                pretrain_emb[index,:] = embedd_dict[word.lower()]\n            case_match += 1\n        else:\n            pretrain_emb[index,:] = np.random.uniform(-scale, scale, [1, embedd_dim])\n            not_match += 1\n    pretrained_size = len(embedd_dict)\n    print(""Embedding:\\n     pretrain word:%s, prefect match:%s, case_match:%s, oov:%s, oov%%:%s""%(pretrained_size, perfect_match, case_match, not_match, (not_match+0.)/alphabet_size))\n    return pretrain_emb, embedd_dim\n\ndef norm2one(vec):\n    root_sum_square = np.sqrt(np.sum(np.square(vec)))\n    return vec/root_sum_square\n\ndef load_pretrain_emb(embedding_path):\n    embedd_dim = -1\n    embedd_dict = dict()\n    with open(embedding_path, \'r\') as file:\n        for line in file:\n            line = line.strip()\n            if len(line) == 0:\n                continue\n            tokens = line.split()\n            if embedd_dim < 0:\n                embedd_dim = len(tokens) - 1\n            else:\n                assert (embedd_dim + 1 == len(tokens))\n            embedd = np.empty([1, embedd_dim])\n            embedd[:] = tokens[1:]\n            if sys.version_info[0] < 3:\n                first_col = tokens[0].decode(\'utf-8\')\n            else:\n                first_col = tokens[0]\n            embedd_dict[first_col] = embedd\n    return embedd_dict, embedd_dim\n\nif __name__ == \'__main__\':\n    a = np.arange(9.0)\n    print(a)\n    print(norm2one(a))\n'"
utils/metric.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Jie\n# @Date:   2017-02-16 09:53:19\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2017-12-19 15:23:12\n\n# from operator import add\n#\nfrom __future__ import print_function\nimport sys\n\n\n\n## input as sentence level labels\ndef get_ner_fmeasure(golden_lists, predict_lists, label_type=""BMES""):\n    sent_num = len(golden_lists)\n    golden_full = []\n    predict_full = []\n    right_full = []\n    right_tag = 0\n    all_tag = 0\n    for idx in range(0,sent_num):\n        # word_list = sentence_lists[idx]\n        golden_list = golden_lists[idx]\n        predict_list = predict_lists[idx]\n        for idy in range(len(golden_list)):\n            if golden_list[idy] == predict_list[idy]:\n                right_tag += 1\n        all_tag += len(golden_list)\n        if label_type == ""BMES"":\n            gold_matrix = get_ner_BMES(golden_list)\n            pred_matrix = get_ner_BMES(predict_list)\n        else:\n            gold_matrix = get_ner_BIO(golden_list)\n            pred_matrix = get_ner_BIO(predict_list)\n        # print ""gold"", gold_matrix\n        # print ""pred"", pred_matrix\n        right_ner = list(set(gold_matrix).intersection(set(pred_matrix)))\n        golden_full += gold_matrix\n        predict_full += pred_matrix\n        right_full += right_ner\n    right_num = len(right_full)\n    golden_num = len(golden_full)\n    predict_num = len(predict_full)\n    if predict_num == 0:\n        precision = -1\n    else:\n        precision =  (right_num+0.0)/predict_num\n    if golden_num == 0:\n        recall = -1\n    else:\n        recall = (right_num+0.0)/golden_num\n    if (precision == -1) or (recall == -1) or (precision+recall) <= 0.:\n        f_measure = -1\n    else:\n        f_measure = 2*precision*recall/(precision+recall)\n    accuracy = (right_tag+0.0)/all_tag\n    # print ""Accuracy: "", right_tag,""/"",all_tag,""="",accuracy\n    print(""gold_num = "", golden_num, "" pred_num = "", predict_num, "" right_num = "", right_num)\n    return accuracy, precision, recall, f_measure\n\n\ndef reverse_style(input_string):\n    target_position = input_string.index(\'[\')\n    input_len = len(input_string)\n    output_string = input_string[target_position:input_len] + input_string[0:target_position]\n    return output_string\n\n\ndef get_ner_BMES(label_list):\n    # list_len = len(word_list)\n    # assert(list_len == len(label_list)), ""word list size unmatch with label list""\n    list_len = len(label_list)\n    begin_label = \'B-\'\n    end_label = \'E-\'\n    single_label = \'S-\'\n    whole_tag = \'\'\n    index_tag = \'\'\n    tag_list = []\n    stand_matrix = []\n    for i in range(0, list_len):\n        # wordlabel = word_list[i]\n        current_label = label_list[i].upper()\n        if begin_label in current_label:\n            if index_tag != \'\':\n                tag_list.append(whole_tag + \',\' + str(i-1))\n            whole_tag = current_label.replace(begin_label,"""",1) +\'[\' +str(i)\n            index_tag = current_label.replace(begin_label,"""",1)\n\n        elif single_label in current_label:\n            if index_tag != \'\':\n                tag_list.append(whole_tag + \',\' + str(i-1))\n            whole_tag = current_label.replace(single_label,"""",1) +\'[\' +str(i)\n            tag_list.append(whole_tag)\n            whole_tag = """"\n            index_tag = """"\n        elif end_label in current_label:\n            if index_tag != \'\':\n                tag_list.append(whole_tag +\',\' + str(i))\n            whole_tag = \'\'\n            index_tag = \'\'\n        else:\n            continue\n    if (whole_tag != \'\')&(index_tag != \'\'):\n        tag_list.append(whole_tag)\n    tag_list_len = len(tag_list)\n\n    for i in range(0, tag_list_len):\n        if  len(tag_list[i]) > 0:\n            tag_list[i] = tag_list[i]+ \']\'\n            insert_list = reverse_style(tag_list[i])\n            stand_matrix.append(insert_list)\n    # print stand_matrix\n    return stand_matrix\n\n\ndef get_ner_BIO(label_list):\n    # list_len = len(word_list)\n    # assert(list_len == len(label_list)), ""word list size unmatch with label list""\n    list_len = len(label_list)\n    begin_label = \'B-\'\n    inside_label = \'I-\'\n    whole_tag = \'\'\n    index_tag = \'\'\n    tag_list = []\n    stand_matrix = []\n    for i in range(0, list_len):\n        # wordlabel = word_list[i]\n        current_label = label_list[i].upper()\n        if begin_label in current_label:\n            if index_tag == \'\':\n                whole_tag = current_label.replace(begin_label,"""",1) +\'[\' +str(i)\n                index_tag = current_label.replace(begin_label,"""",1)\n            else:\n                tag_list.append(whole_tag + \',\' + str(i-1))\n                whole_tag = current_label.replace(begin_label,"""",1)  + \'[\' + str(i)\n                index_tag = current_label.replace(begin_label,"""",1)\n\n        elif inside_label in current_label:\n            if current_label.replace(inside_label,"""",1) == index_tag:\n                whole_tag = whole_tag\n            else:\n                if (whole_tag != \'\')&(index_tag != \'\'):\n                    tag_list.append(whole_tag +\',\' + str(i-1))\n                whole_tag = \'\'\n                index_tag = \'\'\n        else:\n            if (whole_tag != \'\')&(index_tag != \'\'):\n                tag_list.append(whole_tag +\',\' + str(i-1))\n            whole_tag = \'\'\n            index_tag = \'\'\n\n    if (whole_tag != \'\')&(index_tag != \'\'):\n        tag_list.append(whole_tag)\n    tag_list_len = len(tag_list)\n\n    for i in range(0, tag_list_len):\n        if  len(tag_list[i]) > 0:\n            tag_list[i] = tag_list[i]+ \']\'\n            insert_list = reverse_style(tag_list[i])\n            stand_matrix.append(insert_list)\n    return stand_matrix\n\n\n\ndef readSentence(input_file):\n    in_lines = open(input_file,\'r\').readlines()\n    sentences = []\n    labels = []\n    sentence = []\n    label = []\n    for line in in_lines:\n        if len(line) < 2:\n            sentences.append(sentence)\n            labels.append(label)\n            sentence = []\n            label = []\n        else:\n            pair = line.strip(\'\\n\').split(\' \')\n            sentence.append(pair[0])\n            label.append(pair[-1])\n    return sentences,labels\n\n\ndef readTwoLabelSentence(input_file, pred_col=-1):\n    in_lines = open(input_file,\'r\').readlines()\n    sentences = []\n    predict_labels = []\n    golden_labels = []\n    sentence = []\n    predict_label = []\n    golden_label = []\n    for line in in_lines:\n        if ""##score##"" in line:\n            continue\n        if len(line) < 2:\n            sentences.append(sentence)\n            golden_labels.append(golden_label)\n            predict_labels.append(predict_label)\n            sentence = []\n            golden_label = []\n            predict_label = []\n        else:\n            pair = line.strip(\'\\n\').split(\' \')\n            sentence.append(pair[0])\n            golden_label.append(pair[1])\n            predict_label.append(pair[pred_col])\n\n    return sentences,golden_labels,predict_labels\n\n\ndef fmeasure_from_file(golden_file, predict_file, label_type=""BMES""):\n    print(""Get f measure from file:"", golden_file, predict_file)\n    print(""Label format:"",label_type)\n    golden_sent,golden_labels = readSentence(golden_file)\n    predict_sent,predict_labels = readSentence(predict_file)\n    P,R,F = get_ner_fmeasure(golden_labels, predict_labels, label_type)\n    print (""P:%sm R:%s, F:%s""%(P,R,F))\n\n\n\ndef fmeasure_from_singlefile(twolabel_file, label_type=""BMES"", pred_col=-1):\n    sent,golden_labels,predict_labels = readTwoLabelSentence(twolabel_file, pred_col)\n    P,R,F = get_ner_fmeasure(golden_labels, predict_labels, label_type)\n    print (""P:%s, R:%s, F:%s""%(P,R,F))\n\n\n\nif __name__ == \'__main__\':\n    # print ""sys:"",len(sys.argv)\n    if len(sys.argv) == 3:\n        fmeasure_from_singlefile(sys.argv[1],""BMES"",int(sys.argv[2]))\n    else:\n        fmeasure_from_singlefile(sys.argv[1],""BMES"")\n\n'"
utils/tagSchemeConverter.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Jie Yang\n# @Date:   2017-11-27 16:53:36\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2018-06-25 10:26:30\n\n\n""""""\n    convert NER/Chunking tag schemes, i.e. BIO->BIOES, BIOES->BIO, IOB->BIO, IOB->BIOES\n""""""\nfrom __future__ import print_function\n\nimport sys\n\n\ndef BIO2BIOES(input_file, output_file):\n    print(""Convert BIO -> BIOES for file:"", input_file)\n    with open(input_file,\'r\') as in_file:\n        fins = in_file.readlines()\n    fout = open(output_file,\'w\')\n    words = []\n    labels = []\n    for line in fins:\n        if len(line) < 3:\n            sent_len = len(words)\n            for idx in range(sent_len):\n                if ""-"" not in labels[idx]:\n                    fout.write(words[idx]+"" ""+labels[idx]+""\\n"")\n                else:\n                    label_type = labels[idx].split(\'-\')[-1]\n                    if ""B-"" in labels[idx]:\n                        if (idx == sent_len - 1) or (""I-"" not in labels[idx+1]):\n                            fout.write(words[idx]+"" S-""+label_type+""\\n"")\n                        else:\n                            fout.write(words[idx]+"" B-""+label_type+""\\n"")\n                    elif ""I-"" in labels[idx]:\n                        if (idx == sent_len - 1) or (""I-"" not in labels[idx+1]):\n                            fout.write(words[idx]+"" E-""+label_type+""\\n"")\n                        else:\n                            fout.write(words[idx]+"" I-""+label_type+""\\n"")\n            fout.write(\'\\n\')\n            words = []\n            labels = []\n        else:\n            pair = line.strip(\'\\n\').split()\n            words.append(pair[0])\n            labels.append(pair[-1].upper())\n    fout.close()\n    print(""BIOES file generated:"", output_file)\n\n\n\ndef BIOES2BIO(input_file, output_file):\n    print(""Convert BIOES -> BIO for file:"", input_file)\n    with open(input_file,\'r\') as in_file:\n        fins = in_file.readlines()\n    fout = open(output_file,\'w\')\n    words = []\n    labels = []\n    for line in fins:\n        if len(line) < 3:\n            sent_len = len(words)\n            for idx in range(sent_len):\n                if ""-"" not in labels[idx]:\n                    fout.write(words[idx]+"" ""+labels[idx]+""\\n"")\n                else:\n                    label_type = labels[idx].split(\'-\')[-1]\n                    if ""E-"" in labels[idx]:\n                        fout.write(words[idx]+"" I-""+label_type+""\\n"")\n                    elif ""S-"" in labels[idx]:\n                        fout.write(words[idx]+"" B-""+label_type+""\\n"")\n                    else:\n                        fout.write(words[idx]+"" ""+labels[idx]+""\\n"")     \n            fout.write(\'\\n\')\n            words = []\n            labels = []\n        else:\n            pair = line.strip(\'\\n\').split()\n            words.append(pair[0])\n            labels.append(pair[-1].upper())\n    fout.close()\n    print(""BIO file generated:"", output_file)\n\n\ndef IOB2BIO(input_file, output_file):\n    print(""Convert IOB -> BIO for file:"", input_file)\n    with open(input_file,\'r\') as in_file:\n        fins = in_file.readlines()\n    fout = open(output_file,\'w\')\n    words = []\n    labels = []\n    for line in fins:\n        if len(line) < 3:\n            sent_len = len(words)\n            for idx in range(sent_len):\n                if ""I-"" in labels[idx]:\n                    label_type = labels[idx].split(\'-\')[-1]\n                    if (idx == 0) or (labels[idx-1] == ""O"") or (label_type != labels[idx-1].split(\'-\')[-1]):\n                        fout.write(words[idx]+"" B-""+label_type+""\\n"")\n                    else:\n                        fout.write(words[idx]+"" ""+labels[idx]+""\\n"")\n                else:\n                    fout.write(words[idx]+"" ""+labels[idx]+""\\n"")\n            fout.write(\'\\n\')\n            words = []\n            labels = []\n        else:\n            pair = line.strip(\'\\n\').split()\n            words.append(pair[0])\n            labels.append(pair[-1].upper())\n    fout.close()\n    print(""BIO file generated:"", output_file)\n\n\ndef choose_label(input_file, output_file):\n    with open(input_file,\'r\') as in_file:\n        fins = in_file.readlines()\n    with open(output_file,\'w\') as fout:\n        for line in fins:\n            if len(line) < 3:\n                fout.write(line)\n            else:\n                pairs = line.strip(\'\\n\').split(\' \')\n                fout.write(pairs[0]+"" ""+ pairs[-1]+""\\n"")\n\n\nif __name__ == \'__main__\':\n    \'\'\'Convert NER tag schemes among IOB/BIO/BIOES.\n        For example: if you want to convert the IOB tag scheme to BIO, then you run as following:\n            python NERSchemeConverter.py IOB2BIO input_iob_file output_bio_file\n        Input data format is the standard CoNLL 2003 data format.\n    \'\'\'\n    if sys.argv[1].upper() == ""IOB2BIO"":\n        IOB2BIO(sys.argv[2],sys.argv[3])\n    elif sys.argv[1].upper() == ""BIO2BIOES"":\n        BIO2BIOES(sys.argv[2],sys.argv[3])\n    elif sys.argv[1].upper() == ""BIOES2BIO"":\n        BIOES2BIO(sys.argv[2],sys.argv[3])\n    elif sys.argv[1].upper() == ""IOB2BIOES"":\n        IOB2BIO(sys.argv[2],""temp"")\n        BIO2BIOES(""temp"",sys.argv[3])\n    else:\n        print(""Argument error: sys.argv[1] should belongs to \\""IOB2BIO/BIO2BIOES/BIOES2BIO/IOB2BIOES\\"""")\n'"
